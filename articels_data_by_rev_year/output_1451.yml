- Affiliation of the first author: johns hopkins university, baltimore, usa
  Affiliation of the last author: johns hopkins university, baltimore, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Every_Pixel_Counts__Joint_Learning_of_Geometry_and_Motion_with_D_Holistic_Unders\figure_1.jpg
  Figure 1 caption: (a) Two consecutive images (transparent second frame overlapped
    onto the first frame), (b) our estimated depth for the first frame, (c) our estimated
    optical flow, (d) our estimated moving object mask, (e) depth from LEGO [6] for
    the first frame, (f) optical flow estimation from Wang et al. [7], (g) segmentation
    mask from EPC [8]. By jointly learning all three geometrical cues, our results
    show significant improvement over other SoTA methods on different tasks.
  Figure 10 Link: articels_figures_by_rev_year\2019\Every_Pixel_Counts__Joint_Learning_of_Geometry_and_Motion_with_D_Holistic_Unders\figure_10.jpg
  Figure 10 caption: Odometry estimation results on four sequences of KITTI 2015 dataset.
    The two left figures (a) and (b) are results on training sequences and the right
    two results on test sequences.
  Figure 2 Link: articels_figures_by_rev_year\2019\Every_Pixel_Counts__Joint_Learning_of_Geometry_and_Motion_with_D_Holistic_Unders\figure_2.jpg
  Figure 2 caption: "The upper part of this figure shows the pipeline of our framework\
    \ (EPC++). Given a a pair of consecutive frames, i.e.,target image I t and source\
    \ image I s , the OptFlowNet is used to predict optical flow F from I t to I s\
    \ . The MotionNet predicts their relative camera pose T t\u2192s . The DepthNet\
    \ estimates the depth D t from single frame. All three informations are fed into\
    \ the Holistic 3D Motion Parser (HMP), which produce an segmentation mask for\
    \ moving object S , occlusion mask V , 3D motion maps for rigid background M b\
    \ and dynamic objects M d . The bottom part of the figure shows how different\
    \ loss terms are generated from geometrical cues. Details are shown in Section\
    \ 3.2.2."
  Figure 3 Link: articels_figures_by_rev_year\2019\Every_Pixel_Counts__Joint_Learning_of_Geometry_and_Motion_with_D_Holistic_Unders\figure_3.jpg
  Figure 3 caption: Two examples of large depth confusion. Moving object in two consecutive
    frames (a) causes large depth value confusion for our system trained with monocular
    videos, as shown in (b). This issue can be resolved by incorporating stereo training
    samples into the system (c).
  Figure 4 Link: articels_figures_by_rev_year\2019\Every_Pixel_Counts__Joint_Learning_of_Geometry_and_Motion_with_D_Holistic_Unders\figure_4.jpg
  Figure 4 caption: The architecture of DepthNet. Each rectangle represents one certain
    layer as color coded in the legend. Number on top of the rectangles indicates
    the channel size of each layer (rectangle), e.g., 32 in the left indicates both
    convolutional layers have 32 channels.
  Figure 5 Link: articels_figures_by_rev_year\2019\Every_Pixel_Counts__Joint_Learning_of_Geometry_and_Motion_with_D_Holistic_Unders\figure_5.jpg
  Figure 5 caption: Visual comparison between Godard et al. [9] and EPC++ (stereo)
    results on KITTI frames. The depth ground truths are interpolated and all images
    are reshaped for better visualization. For depths, our results have preserved
    the details of objects noticeably better (as in white circles).
  Figure 6 Link: articels_figures_by_rev_year\2019\Every_Pixel_Counts__Joint_Learning_of_Geometry_and_Motion_with_D_Holistic_Unders\figure_6.jpg
  Figure 6 caption: "Visual comparison between LEGO [6] and EPC++ (mono) results on\
    \ KITTI test frames. Thanks to the extra supervision from optical flow, our monocular\
    \ results preserve the details of the occludedde-occluded regions better, e.g.,\
    \ the structure of thin poles. Please note the \u201Clarge depth value confusion\u201D\
    \ still happens for both monocular based methods (green circle)."
  Figure 7 Link: articels_figures_by_rev_year\2019\Every_Pixel_Counts__Joint_Learning_of_Geometry_and_Motion_with_D_Holistic_Unders\figure_7.jpg
  Figure 7 caption: 'Qualitative depth estimation results on Make3D. The results are
    generated by applying EPC++, which is trained on KITTI dataset, on Make3D test
    images. From top to bottom: input test image, our depth estimation result, depth
    ground truth.'
  Figure 8 Link: articels_figures_by_rev_year\2019\Every_Pixel_Counts__Joint_Learning_of_Geometry_and_Motion_with_D_Holistic_Unders\figure_8.jpg
  Figure 8 caption: Visualization of optical flow results on KITTI 2015 training set
    images. We compared with current unsupervised SoTA method [7]. Optical flow results
    generated by EPC++ align better with the ground truth, especially on object boundaries
    (occlusion regions).
  Figure 9 Link: articels_figures_by_rev_year\2019\Every_Pixel_Counts__Joint_Learning_of_Geometry_and_Motion_with_D_Holistic_Unders\figure_9.jpg
  Figure 9 caption: 'Visualization of depth and optical flow estimation results on
    MPI-Sintel dataset using our model fine-tuned on MPI-Sintel. From top to bottom:
    input image, depth estimation result, depth ground truth, optical flow estimation
    result, optical flow ground truth.'
  First author gender probability: 0.9
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Chenxu Luo
  Name of the last author: Alan Yuille
  Number of Figures: 11
  Number of Tables: 10
  Number of authors: 7
  Paper title: 'Every Pixel Counts ++: Joint Learning of Geometry and Motion with
    3D Holistic Understanding'
  Publication Date: 2019-07-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Evaluation Metrics for Our Tasks
  Table 10 caption:
    table_text: TABLE 10 Scene Flow Performances of Different Methods on KITTI 2015
      Training Split
  Table 2 caption:
    table_text: TABLE 2 Single View Depth Estimation Results on the Eigen Test Split
  Table 3 caption:
    table_text: TABLE 3 Generalization to Make3D Dataset
  Table 4 caption:
    table_text: TABLE 4 Ablation Study of Optical Flow Estimation on KITTI 2015 Training
      Set
  Table 5 caption:
    table_text: TABLE 5 Comparison of Optical Flow Performances Between EPC++ and
      Current Unsupervised SoTA on KITTI 2012 and KITTI 2015 Datasets
  Table 6 caption:
    table_text: TABLE 6 Optical Flow Performance of Unsupervised Methods on the MPI-Sintel
      Final Training Split
  Table 7 caption:
    table_text: TABLE 7 Odometry Evaluation on Two Sequences of KITTI 2015 Dataset
  Table 8 caption:
    table_text: TABLE 8 Odometry Evaluation on KITTI Dataset Using the Metric of Average
      Translation and Rotation Errors
  Table 9 caption:
    table_text: TABLE 9 Foreground Moving Object Segmentation Performance on KITTI
      2015 Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2930258
- Affiliation of the first author: school of information technology and electrical
    engineering, university of queensland, brisbane, qld, australia
  Affiliation of the last author: school of computer science, the university of adelaide,
    adelaide, sa, australia
  Figure 1 Link: articels_figures_by_rev_year\2019\Rotation_Averaging_with_the_Chordal_Distance_Global_Minimizers_and_Strong_Dualit\figure_1.jpg
  Figure 1 caption: In many structure from motion pipelines, camera orientations are
    estimated with rotation averaging followed by recovery of camera centres (red)
    and 3D structure (blue). Here are three solutions corresponding to different local
    minima of the same rotation averaging problem.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Rotation_Averaging_with_the_Chordal_Distance_Global_Minimizers_and_Strong_Dualit\figure_2.jpg
  Figure 2 caption: A complete graph (left) and a cycle graph (right), both with 6
    vertices.
  Figure 3 Link: articels_figures_by_rev_year\2019\Rotation_Averaging_with_the_Chordal_Distance_Global_Minimizers_and_Strong_Dualit\figure_3.jpg
  Figure 3 caption: A circular graph with connections to its four closest neighbors.
  Figure 4 Link: articels_figures_by_rev_year\2019\Rotation_Averaging_with_the_Chordal_Distance_Global_Minimizers_and_Strong_Dualit\figure_4.jpg
  Figure 4 caption: "The maximal angle residual \u03B1 max (y-axis) versus the level\
    \ of available data 2b n (x-axis) for circular graphs."
  Figure 5 Link: articels_figures_by_rev_year\2019\Rotation_Averaging_with_the_Chordal_Distance_Global_Minimizers_and_Strong_Dualit\figure_5.jpg
  Figure 5 caption: Fraction of available data fracdin-2 (x-axis) versus alpha max
    (y-axis).
  Figure 6 Link: articels_figures_by_rev_year\2019\Rotation_Averaging_with_the_Chordal_Distance_Global_Minimizers_and_Strong_Dualit\figure_6.jpg
  Figure 6 caption: Images and reconstructions of the datasets in Table 2.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.94
  Name of the first author: Anders Eriksson
  Name of the last author: Tat-Jun Chin
  Number of Figures: 6
  Number of Tables: 2
  Number of authors: 4
  Paper title: 'Rotation Averaging with the Chordal Distance: Global Minimizers and
    Strong Duality'
  Publication Date: 2019-07-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Running Times on Synthetic Data, Averaged Over
      100 Runs
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 The Average Run Time and Largest Resulting Angular Residual\
      \ ( | \u03B1 ij | |\u03B1ij|) and Bound ( \u03B1 max \u03B1max) on Five Different\
      \ Real-World Datasets"
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2930051
- Affiliation of the first author: department of computer science, city university
    of hong kong, kowloon tong, hong kong
  Affiliation of the last author: department of computer science, city university
    of hong kong, kowloon tong, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2019\Visual_Tracking_via_Dynamic_Memory_Networks\figure_1.jpg
  Figure 1 caption: 'Example of template updating on the Basketball video: the control
    gate signals change along with the appearance variations. When there are large
    appearance changes, the allocation gate approaches to 1, which means a new memory
    slot is overwritten. When there are only small appearance variations in the object
    template, then the read gate is close to 1, which indicates that the most recently
    read memory slot will be updated. See Section 3.7 for detailed explanations.'
  Figure 10 Link: articels_figures_by_rev_year\2019\Visual_Tracking_via_Dynamic_Memory_Networks\figure_10.jpg
  Figure 10 caption: 'The success plots on OTB-2015 for eight challenging attributes:
    Out-of-plane rotation, occlusion, motion blur, fast motion, in-plane rotation,
    out of view, background clutter and low resolution.'
  Figure 2 Link: articels_figures_by_rev_year\2019\Visual_Tracking_via_Dynamic_Memory_Networks\figure_2.jpg
  Figure 2 caption: The pipeline of our tracking algorithm. The green rectangle is
    the candidate region for target searching. The Feature Extraction blocks for the
    object image and search image share the same architecture and parameters. An attentional
    LSTM extracts the targets information on the search feature map, which guides
    the memory reading process to retrieve a matching template. The residual template
    is combined with the initial template, to obtain a positive template. A negative
    template is read from the negative memory and combined with the positive template
    to cancel responses from distractor objects. The final template is convolved with
    the search feature map to obtain the response map. The newly predicted bounding
    box is then used to crop the objects feature map for writing to the positive memory.
    A negative template is extracted from the search feature map using the response
    score and written to negative memory.
  Figure 3 Link: articels_figures_by_rev_year\2019\Visual_Tracking_via_Dynamic_Memory_Networks\figure_3.jpg
  Figure 3 caption: 'Left: Diagram of attention network. Right: Visualization of attentional
    weights map: for each pair, (top) search images and ground-truth target box, and
    (bottom) attention maps over search image. For visualization, the attention maps
    are resized using bicubic interpolation to match the size of the original image.'
  Figure 4 Link: articels_figures_by_rev_year\2019\Visual_Tracking_via_Dynamic_Memory_Networks\figure_4.jpg
  Figure 4 caption: 'Left: The feature channels respond to target parts: Images are
    reconstructed from conv5 of the CNN used in our tracker. Each image is generated
    by accumulating reconstructed pixels from the same channel. The input image is
    shown in the top-left. Right: Channel visualizations of a retrieved template along
    with their corresponding residual gate values in the left-top corner.'
  Figure 5 Link: articels_figures_by_rev_year\2019\Visual_Tracking_via_Dynamic_Memory_Networks\figure_5.jpg
  Figure 5 caption: Example responses comparing tracking with distractor template
    canceling (MemDTC) and without (MemTrack).
  Figure 6 Link: articels_figures_by_rev_year\2019\Visual_Tracking_via_Dynamic_Memory_Networks\figure_6.jpg
  Figure 6 caption: Diagram of positive memory access mechanism, including reading
    and writing process.
  Figure 7 Link: articels_figures_by_rev_year\2019\Visual_Tracking_via_Dynamic_Memory_Networks\figure_7.jpg
  Figure 7 caption: Precision and success plots on OTB-2013 for real-time trackers.
  Figure 8 Link: articels_figures_by_rev_year\2019\Visual_Tracking_via_Dynamic_Memory_Networks\figure_8.jpg
  Figure 8 caption: Precision and success plot on OTB-2015 for real-time trackers.
  Figure 9 Link: articels_figures_by_rev_year\2019\Visual_Tracking_via_Dynamic_Memory_Networks\figure_9.jpg
  Figure 9 caption: (left) Success plot on OTB-2015 comparing our real-time methods
    with recent non-real-time trackers. (right) AUC score versus speed with recent
    trackers.
  First author gender probability: 0.66
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Tianyu Yang
  Name of the last author: Antoni B. Chan
  Number of Figures: 15
  Number of Tables: 4
  Number of authors: 2
  Paper title: Visual Tracking via Dynamic Memory Networks
  Publication Date: 2019-07-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison Results on OTB-2013 (I) and OTB-2015 (II)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results on VOT-2015
  Table 3 caption:
    table_text: TABLE 3 Results on VOT-2016
  Table 4 caption:
    table_text: TABLE 4 Results on VOT-2017
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2929034
- Affiliation of the first author: beijing advanced innovation center for big data
    and brain computing, beihang university, beijing, china
  Affiliation of the last author: department of computer science and engineering,
    michigan state university, east lansing, mi, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Learning_Continuous_Face_Age_Progression_A_Pyramid_of_GANs\figure_1.jpg
  Figure 1 caption: Demonstration of our age progression results of two subjects from
    the CACD aging database (images in the first column are input young faces of two
    subjects and the others are synthesized older appearances at different age groups).
  Figure 10 Link: articels_figures_by_rev_year\2019\Learning_Continuous_Face_Age_Progression_A_Pyramid_of_GANs\figure_10.jpg
  Figure 10 caption: Examples of continuous face aging sequences. In each row the
    leftmost image is the input; and the second, the fifth, and the rightmost images
    are the results conditioned on the discrete age labels presented to the network
    during training, while others are interpolated results.
  Figure 2 Link: articels_figures_by_rev_year\2019\Learning_Continuous_Face_Age_Progression_A_Pyramid_of_GANs\figure_2.jpg
  Figure 2 caption: Framework of the proposed age progression method. A CNN based
    generator G learns the age transformation. It takes a younger face and the conditional
    target age label as inputs, and outputs a corresponding aged likeness. The training
    critic incorporates the squared euclidean loss in the image space, the GAN loss
    that encourages generated faces to be indistinguishable from the training elderly
    faces in terms of age, and the identity preservation loss minimizing the input-output
    distance in a high-level feature representation which embeds the personalized
    characteristics.
  Figure 3 Link: articels_figures_by_rev_year\2019\Learning_Continuous_Face_Age_Progression_A_Pyramid_of_GANs\figure_3.jpg
  Figure 3 caption: The scores of four pathways are finally concatenated and jointly
    estimated by the discriminator D i ( D i is an estimator rather than a classifier;
    the Label does not need to be a single scalar).
  Figure 4 Link: articels_figures_by_rev_year\2019\Learning_Continuous_Face_Age_Progression_A_Pyramid_of_GANs\figure_4.jpg
  Figure 4 caption: Comparison of several extensions of the original adversarial training
    scheme presented in Section 3.3.1. (a) The general pipeline of original adversarial
    training; (b) Auxiliary Classifier GAN for progressive aging modeling; (c) D is
    extended to output k logits handling the degree of aging; and (d) A specific number
    of discriminators are simultaneously trained to steer the age transformation to
    different age domains.
  Figure 5 Link: articels_figures_by_rev_year\2019\Learning_Continuous_Face_Age_Progression_A_Pyramid_of_GANs\figure_5.jpg
  Figure 5 caption: Comparison of the aging results achieved by different extensions
    of the original adversarial training scheme. The fist image in each column is
    the input face image and the subsequent 3 images are the age progressed visualizations
    achieved by the methods demonstrated in Figs. 4b, 4c, and 4d, respectively.
  Figure 6 Link: articels_figures_by_rev_year\2019\Learning_Continuous_Face_Age_Progression_A_Pyramid_of_GANs\figure_6.jpg
  Figure 6 caption: Age distributions of (a) MORPH, (b) CACD, and (c) FGNET.
  Figure 7 Link: articels_figures_by_rev_year\2019\Learning_Continuous_Face_Age_Progression_A_Pyramid_of_GANs\figure_7.jpg
  Figure 7 caption: Aging effects obtained on the CACD database for 18 different subjects.
    The first image in each panel is the original face image and the subsequent 3
    images are the age progressed visualizations for that subject in the [31- 40],
    [41-50] and 50+ age clusters.
  Figure 8 Link: articels_figures_by_rev_year\2019\Learning_Continuous_Face_Age_Progression_A_Pyramid_of_GANs\figure_8.jpg
  Figure 8 caption: Aging effects obtained on the MORPH database for 18 different
    subjects.
  Figure 9 Link: articels_figures_by_rev_year\2019\Learning_Continuous_Face_Age_Progression_A_Pyramid_of_GANs\figure_9.jpg
  Figure 9 caption: Rejuvenating results achieved on CACD (the top two rows) and MORPH
    (the bottom two rows) for 24 different subjects. The first image in each panel
    is the original face image and the second is the corresponding rejuvenating result.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Hongyu Yang
  Name of the last author: Anil K. Jain
  Number of Figures: 17
  Number of Tables: 10
  Number of authors: 4
  Paper title: 'Learning Continuous Face Age Progression: A Pyramid of GANs'
  Publication Date: 2019-07-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of Recent Representative Studies on Face Age Progression
  Table 10 caption:
    table_text: TABLE 10 Quantitative Comparison with Ground-Truth
  Table 2 caption:
    table_text: TABLE 2 Generator Architecture
  Table 3 caption:
    table_text: TABLE 3 Discriminator Architecture
  Table 4 caption:
    table_text: TABLE 4 Statistics of Face Aging Databases Used for Evaluation
  Table 5 caption:
    table_text: TABLE 5 ANOVA Test Results on Aging Accuracy
  Table 6 caption:
    table_text: TABLE 6 Objective Age Estimation Results (in Years) on CACD and MORPH
  Table 7 caption:
    table_text: TABLE 7 Objective Face Verification Results on (a) CACD and (b) MORPH
  Table 8 caption:
    table_text: TABLE 8 Quantitative Evaluation Results Using One-Pathway Discriminator
      on (a) CACD and (b) MORPH
  Table 9 caption:
    table_text: TABLE 9 Quantitative Evaluation Results of the Independently Trained
      aging Models on (a) CACD and (b) MORPH
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2930985
- Affiliation of the first author: department of electrical and computer engineering,
    asri, seoul national university, seoul, seoul, korea
  Affiliation of the last author: department of electrical and computer engineering,
    asri, seoul national university, seoul, seoul, korea
  Figure 1 Link: articels_figures_by_rev_year\2019\Reconstruct_as_Far_as_You_Can_Consensus_of_NonRigid_Reconstruction_from_Feasible\figure_1.jpg
  Figure 1 caption: Overview of the proposed consensus-based reconstruction framework.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Reconstruct_as_Far_as_You_Can_Consensus_of_NonRigid_Reconstruction_from_Feasible\figure_2.jpg
  Figure 2 caption: Average reconstruction errors of STCNR on the benchmark data sets
    under various parameters. In each graph, x-axis represents the value of each parameter
    and y-axis represents the normalized reconstruction error.
  Figure 3 Link: articels_figures_by_rev_year\2019\Reconstruct_as_Far_as_You_Can_Consensus_of_NonRigid_Reconstruction_from_Feasible\figure_3.jpg
  Figure 3 caption: Reconstruction examples of the tear sequence based on STCNR.
  Figure 4 Link: articels_figures_by_rev_year\2019\Reconstruct_as_Far_as_You_Can_Consensus_of_NonRigid_Reconstruction_from_Feasible\figure_4.jpg
  Figure 4 caption: Reconstruction examples of the two cloths sequence based on STCNR.
  Figure 5 Link: articels_figures_by_rev_year\2019\Reconstruct_as_Far_as_You_Can_Consensus_of_NonRigid_Reconstruction_from_Feasible\figure_5.jpg
  Figure 5 caption: Reconstruction examples of the back sequence based on STCNR.
  Figure 6 Link: articels_figures_by_rev_year\2019\Reconstruct_as_Far_as_You_Can_Consensus_of_NonRigid_Reconstruction_from_Feasible\figure_6.jpg
  Figure 6 caption: 'Reconstruction examples of the CMU2010 (top) and CMU5501 (bottom)
    sequences. (a), (d), (g), (j): Black dots show all visible trajectories in the
    data matrix and green circles show the selected trajectories in each frame (note
    that we only used the trajectories whose lengths are longer than 15 frames). (b),
    (e), (h), (k): Reconstruction results of existing algorithms which show the second
    best performance in each case. (c), (f), (i), (l): Reconstruction results of STCNR-based
    fCNR. Red dots are ground truth and blue +s are reconstructed points.'
  Figure 7 Link: articels_figures_by_rev_year\2019\Reconstruct_as_Far_as_You_Can_Consensus_of_NonRigid_Reconstruction_from_Feasible\figure_7.jpg
  Figure 7 caption: Reconstruction examples of the VSB100 and Moseg data sets. Here,
    yellow and blue mean higher and lower depth values, respectively. Yellow circles
    in the 2D images represent the trajectories used as input.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Geonho Cha
  Name of the last author: Songhwai Oh
  Number of Figures: 7
  Number of Tables: 5
  Number of authors: 4
  Paper title: 'Reconstruct as Far as You Can: Consensus of Non-Rigid Reconstruction
    from Feasible Regions'
  Publication Date: 2019-07-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average Reconstruction Errors for the Benchmark Data Sets
      [22], [59]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average Reconstruction Errors for the Data Sets in [58] with
      Realistic Rotations
  Table 3 caption:
    table_text: TABLE 3 Average Reconstruction Errors for the Data Sets in [58] with
      Realistic Rotations and Noise
  Table 4 caption:
    table_text: TABLE 4 Average Reconstruction Errors of the Consensus-Based Reconstructor
      for the Proposed Data Sets
  Table 5 caption:
    table_text: TABLE 5 Average Reconstruction Errors of STCNR for the Multi-Body
      Data Sets
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2931317
- Affiliation of the first author: department of electrical and computer engineering,
    isfahan university of technology, isfahan, iran
  Affiliation of the last author: department of computer science, university of york,
    heslington, york, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2019\Graph_Embedding_Using_Frequency_Filtering\figure_1.jpg
  Figure 1 caption: The proposed filter function sets.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Graph_Embedding_Using_Frequency_Filtering\figure_2.jpg
  Figure 2 caption: 'The feature distances of different proposed methods in comparison
    with the edit distance. Column a: Seed graphs. Column b: FFE method. Column c:
    GeFFE by power PFO. Column d: GeFFE by correlated PFO.'
  Figure 3 Link: articels_figures_by_rev_year\2019\Graph_Embedding_Using_Frequency_Filtering\figure_3.jpg
  Figure 3 caption: "The classification accuracies of GeFFE by some different filters\
    \ and order 4 of G \u02D9 ."
  Figure 4 Link: articels_figures_by_rev_year\2019\Graph_Embedding_Using_Frequency_Filtering\figure_4.jpg
  Figure 4 caption: The classification accuracies of some different filter combinations
    using three selected combination strategies.
  Figure 5 Link: articels_figures_by_rev_year\2019\Graph_Embedding_Using_Frequency_Filtering\figure_5.jpg
  Figure 5 caption: The test of following edit distance for proposed filter functions.
  Figure 6 Link: articels_figures_by_rev_year\2019\Graph_Embedding_Using_Frequency_Filtering\figure_6.jpg
  Figure 6 caption: The similar reactions of different filters on the structurally
    similar datasets. The similar set 1 is Llow, Lmed and Lhigh and the similar set
    2 is COIL15 and ODBK50.
  Figure 7 Link: articels_figures_by_rev_year\2019\Graph_Embedding_Using_Frequency_Filtering\figure_7.jpg
  Figure 7 caption: "The classification accuracies of GeFFE by P S 4,11 filter and\
    \ different orders of G \u02D9 ."
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Hoda Bahonar
  Name of the last author: Richard C. Wilson
  Number of Figures: 7
  Number of Tables: 5
  Number of authors: 4
  Paper title: Graph Embedding Using Frequency Filtering
  Publication Date: 2019-07-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Previously Defined Invariants as the Special Cases of
      GeFFE
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Real Graph Datasets
  Table 3 caption:
    table_text: TABLE 3 The Amount of Cospectrality in Various Methods
  Table 4 caption:
    table_text: TABLE 4 The 10 First Selected Filter Responses Using Forward Selection
      on the GeFFE Filter Responses
  Table 5 caption:
    table_text: TABLE 5 The Classification Accuracies of Some Versions of GeFFE in
      Comparison with the Existing Embedding Methods
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2929519
- Affiliation of the first author: institute of high performance computing, astar,
    singapore
  Affiliation of the last author: college of computer science, sichuan university,
    chengdu, china
  Figure 1 Link: articels_figures_by_rev_year\2019\Dual_Adversarial_Transfer_for_Sequence_Labeling\figure_1.jpg
  Figure 1 caption: The general architecture of proposed models.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Dual_Adversarial_Transfer_for_Sequence_Labeling\figure_2.jpg
  Figure 2 caption: "The effect of \u03B3 on sample weights."
  Figure 3 Link: articels_figures_by_rev_year\2019\Dual_Adversarial_Transfer_for_Sequence_Labeling\figure_3.jpg
  Figure 3 caption: Comparison with different target data ratio, where AT stands for
    adversarial training, F(P)-Transfer denotes the DATNet-F(P) without AT.
  Figure 4 Link: articels_figures_by_rev_year\2019\Dual_Adversarial_Transfer_for_Sequence_Labeling\figure_4.jpg
  Figure 4 caption: The visualization of extracted features from shared bidirectional-LSTM
    layer. The left, middle, and right figures show the results when no Adversarial
    Discriminator (AD), AD, and GRAD is performed, respectively. Red points correspond
    to the source CoNLL-2003 English examples, and blue points correspond to the target
    CoNLL-2002 Spanish examples.
  Figure 5 Link: articels_figures_by_rev_year\2019\Dual_Adversarial_Transfer_for_Sequence_Labeling\figure_5.jpg
  Figure 5 caption: "Analysis of \u03B3 in GRAD on CoNLL-2002 Spanish NER."
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.87
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Joey Tianyi Zhou
  Name of the last author: Xi Peng
  Number of Figures: 5
  Number of Tables: 11
  Number of authors: 4
  Paper title: Dual Adversarial Transfer for Sequence Labeling
  Publication Date: 2019-07-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Statistics of Benchmark Sequence Labeling Datasets
  Table 10 caption:
    table_text: "TABLE 10 Analysis of Discriminator Weight \u03B1 \u03B1 in GRAD with\
      \ Varying Data Ratio \u03C1 \u03C1 (F1-Score)"
  Table 2 caption:
    table_text: TABLE 2 Statistics of Selected Named Entity Recognition Datasets from
      Pan et al. [29]
  Table 3 caption:
    table_text: TABLE 3 Comparison with State-of-the-Art Results in CoNLL and WNUT
      Datasets (F1-Score)
  Table 4 caption:
    table_text: TABLE 4 Results of Varying Cross-Language Transfer Settings in [29]
      Datasets (F1-Score)
  Table 5 caption:
    table_text: TABLE 5 Comparison with State-of-the-Art Results in Universal Dependencies
      (UD) Part-of-Speech Tagging Datasets (%)
  Table 6 caption:
    table_text: TABLE 6 Experiments on Extremely Low Resource (F1-Score)
  Table 7 caption:
    table_text: TABLE 7 Quantitative Performance Comparison Between Models with Different
      Components
  Table 8 caption:
    table_text: TABLE 8 Comparison Between At and Dropout Regularizer
  Table 9 caption:
    table_text: "TABLE 9 Analysis of Maximum Perturbation \u03F5 w T \u03B5wT in At\
      \ with Varying Data Ratio \u03C1 \u03C1 (F1-Score)"
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2931569
- Affiliation of the first author: department of image processing and computer graphics,
    university of szeged, szeged, hungary
  Affiliation of the last author: department of image processing and computer graphics,
    university of szeged, szeged, hungary
  Figure 1 Link: articels_figures_by_rev_year\2019\Absolute_Pose_Estimation_of_Central_Cameras_Using_Planar_Regions\figure_1.jpg
  Figure 1 caption: Spherical camera model.
  Figure 10 Link: articels_figures_by_rev_year\2019\Absolute_Pose_Estimation_of_Central_Cameras_Using_Planar_Regions\figure_10.jpg
  Figure 10 caption: 'Pose estimation example with (left-right) central perspective
    camera and custom Lidar data: color 2D image (original frame) with corresponding
    regions (purple); 3D data with the segmented regions (green); color information
    overlaid on 3D data using the estimated camera pose (best viewed in color).'
  Figure 2 Link: articels_figures_by_rev_year\2019\Absolute_Pose_Estimation_of_Central_Cameras_Using_Planar_Regions\figure_2.jpg
  Figure 2 caption: Examples of various amount of segmentation errors (se). First
    an omnidirectional image without se, then the same test with se =12 %, lastly
    the same template from a perspective test case with se =20 %.
  Figure 3 Link: articels_figures_by_rev_year\2019\Absolute_Pose_Estimation_of_Central_Cameras_Using_Planar_Regions\figure_3.jpg
  Figure 3 caption: "Omnidirectional rotation errors along the X , Y , and Z axis\
    \ (first row) and translation, \u03B4 error and runtime plots (second row). m\
    \ denotes median error, se2D and se3D stand for segmentation error on the 2D and\
    \ 3D regions respectively (best viewed in color)."
  Figure 4 Link: articels_figures_by_rev_year\2019\Absolute_Pose_Estimation_of_Central_Cameras_Using_Planar_Regions\figure_4.jpg
  Figure 4 caption: "Backprojection ( \u03B4 ) errors and runtime comparison for point\
    \ and triangle based spherical surface integral approximation on a 1 plane dataset\
    \ (best viewed in color)."
  Figure 5 Link: articels_figures_by_rev_year\2019\Absolute_Pose_Estimation_of_Central_Cameras_Using_Planar_Regions\figure_5.jpg
  Figure 5 caption: "Perspective pose estimation results: rotation and translation\
    \ errors, \u03B4 error and algorithm runtime plots. se2D stands for observation\
    \ segmentation error, se3D for template side segmentation error and m for median\
    \ values (best viewed in color)."
  Figure 6 Link: articels_figures_by_rev_year\2019\Absolute_Pose_Estimation_of_Central_Cameras_Using_Planar_Regions\figure_6.jpg
  Figure 6 caption: "Perspective pose estimation \u03B4 errors comparing the normalized\
    \ image plane and the spherical solutions. m stands for median values (best viewed\
    \ in color)."
  Figure 7 Link: articels_figures_by_rev_year\2019\Absolute_Pose_Estimation_of_Central_Cameras_Using_Planar_Regions\figure_7.jpg
  Figure 7 caption: Runtime comparison on test cases without segmentation errors in
    the omnidirectional and perspective case, the latter using both the normalized
    image plane and the spherical solution. m stands for median values (best viewed
    in color).
  Figure 8 Link: articels_figures_by_rev_year\2019\Absolute_Pose_Estimation_of_Central_Cameras_Using_Planar_Regions\figure_8.jpg
  Figure 8 caption: Overall rotation error in function of the number of regions and
    planes for the omnidirectional (top) and perspective (bottom) camera. m stands
    for median values (best viewed in color).
  Figure 9 Link: articels_figures_by_rev_year\2019\Absolute_Pose_Estimation_of_Central_Cameras_Using_Planar_Regions\figure_9.jpg
  Figure 9 caption: 'Histograms of the overall rotations with errors less than 1 degree
    (from left to right): for the perspective camera along the X and Z axis and for
    the omnidirectional camera along the X and Z axis. First row shows results on
    non-symmetric shapes, second row on the symmetric ones.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Robert Frohlich
  Name of the last author: Zoltan Kato
  Number of Figures: 14
  Number of Tables: 2
  Number of authors: 3
  Paper title: Absolute Pose Estimation of Central Cameras Using Planar Regions
  Publication Date: 2019-07-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparisons on High Resolution Lidar Data in Terms of the
      Mean Forward Projection Errors in Marker Points in cm
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Comparative Results with the Proposed Method (Prop), Normal\
      \ Based MI(Norm)[17] and Intensity Based MI (Int)[17] in Terms of Translation(m),\
      \ Rotation(deg) and \u03B4 \u03B4 (for Reference: \u03B4 \u03B4 for the Ground\
      \ Truth Pose Is 9.49 Percent) Errors"
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2931577
- Affiliation of the first author: department of computer science, university of hong
    kong, hong kong, china
  Affiliation of the last author: department of electronic engineering, chinese university
    of hong kong, hong kong, china
  Figure 1 Link: articels_figures_by_rev_year\2019\Switchable_Normalization_for_LearningtoNormalize_Deep_Representation\figure_1.jpg
  Figure 1 caption: (a) Shows that SN adapts to various networks and tasks by learning
    importance ratios to select normalizers. In (a), a ratio is between 0 and 1 and
    all ratios of each task sum to 1. (b) shows the top-1 accuracies of ResNet50 trained
    with SN on ImageNet and compared with BN and GN in different batch settings. The
    gradients in training are averaged over all GPUs and the statistics of normalizers
    are estimated in each GPU. For instance, all methods are compared to an ideal
    case, ideal BN, whose accuracies are 76.4 percent for all settings. This ideal
    case cannot be obtained in practice. In fact, when the minibatch size decreases,
    BNs accuracies drop significantly, while SN and GN both maintain reasonably good
    performance. SN surpasses or is comparable to both BN and GN in all settings.
  Figure 10 Link: articels_figures_by_rev_year\2019\Switchable_Normalization_for_LearningtoNormalize_Deep_Representation\figure_10.jpg
  Figure 10 caption: (a) Shows ratios of ResNet50+SN(8,2) in ImageNet. (b) Shows ratios
    of Mask R-CNN+SN(8,2) when finetuning in COCO including backbone (ResNet50), FPN,
    box stream, and mask stream.
  Figure 2 Link: articels_figures_by_rev_year\2019\Switchable_Normalization_for_LearningtoNormalize_Deep_Representation\figure_2.jpg
  Figure 2 caption: "Geometric view of directions and lengths of the filters in WN,\
    \ IN, BN, LN, and SN. These normalizers are compared in an unify way by represent\
    \ them by using WN that decomposes optimization of filters into their directions\
    \ and lengths. In this way, IN is identical to WN that sets the filter norm to\
    \ 1 (i.e., \u2225 w 1 \u2225=\u2225 w 2 \u2225=1 ) and then rescales them to \u03B3\
    \ . LN is less constrained than IN and WN to increase learning ability. BN increases\
    \ angle between filters and reduces filter length to improve generalization. SN\
    \ inherits all their benefits by learning their importance ratios."
  Figure 3 Link: articels_figures_by_rev_year\2019\Switchable_Normalization_for_LearningtoNormalize_Deep_Representation\figure_3.jpg
  Figure 3 caption: "Comparisons of learning curves. (a) Visualizes the validation\
    \ curves of SN with different settings of batch size. The bracket (\u22C5,\u22C5\
    ) denotes ( GPUs, samples per GPU). (b) Compares the top-1 train and validation\
    \ curves on ImageNet of SN, BN, and GN in the batch size of (8,32). (c) Compares\
    \ the train and validation curves of SN and GN in the batch size of (8,2)."
  Figure 4 Link: articels_figures_by_rev_year\2019\Switchable_Normalization_for_LearningtoNormalize_Deep_Representation\figure_4.jpg
  Figure 4 caption: "Importance weights versus batch sizes. The bracket (\u22C5,\u22C5\
    ) indicates ( GPUs, samples per GPU). SN doesnt have BN in (8,1)."
  Figure 5 Link: articels_figures_by_rev_year\2019\Switchable_Normalization_for_LearningtoNormalize_Deep_Representation\figure_5.jpg
  Figure 5 caption: "Selected operations of each SN layer in ResNet50. There are 53\
    \ SN layers. (a,b) show the importance weights for \u03BC and \u03C3 of (8,32),\
    \ while (c,d) show those of (8,2). The y -axis represents the importance weights\
    \ that sum to 1, while the x -axis shows different residual blocks of ResNet50.\
    \ The SN layers in different places are highlighted differently. For example,\
    \ the SN layers follow the 3\xD73 conv layers are outlined by shaded color, those\
    \ in the shortcuts are marked with \u25A0 , while those follow the 1\xD71 conv\
    \ layers are in flat color. The first SN layer follows a 7\xD77 conv layer. We\
    \ see that SN learns distinct importance weights for different normalization methods\
    \ as well as \u03BC and \u03C3 , adapting to different batch sizes, places, and\
    \ depths of a deep network."
  Figure 6 Link: articels_figures_by_rev_year\2019\Switchable_Normalization_for_LearningtoNormalize_Deep_Representation\figure_6.jpg
  Figure 6 caption: Comparisons of BN, SN with moving average, and SN with batch average,
    when training ResNet50 on ImageNet in (8,32). We see that SN with batch average
    produces more stable convergence than the other methods.
  Figure 7 Link: articels_figures_by_rev_year\2019\Switchable_Normalization_for_LearningtoNormalize_Deep_Representation\figure_7.jpg
  Figure 7 caption: "Ratios of (a) \u03BB \u03BC z and (b) \u03BB \u03C3 z in ResNet50+SN(8,32)\
    \ for each normalization layer for 100 epochs, as well as (c) their divergence\
    \ D( \u03BB \u03BC z \u2225 \u03BB \u03C3 z ) . Receptive field (RF) of each layer\
    \ is given (53 normalization layers in total). The last 6 subfigures at the 4th,\
    \ 8th, and 12th row show results of different ranges of RF including RF < 49,\
    \ 49 \u223C 99, 99 \u223C 199, 199 \u223C 299, 299 \u223C 427, and ALL (i.e.,\
    \ 7 \u223C 427)."
  Figure 8 Link: articels_figures_by_rev_year\2019\Switchable_Normalization_for_LearningtoNormalize_Deep_Representation\figure_8.jpg
  Figure 8 caption: "Hard ratios for variance ( \u03C3 ) and mean ( \u03BC ) including\
    \ BN (green), IN (blue), and LN (red). Snapshots of ResNet50 trained after 30\
    \ (top), 60 (middle), and 90 (bottom) epochs are shown. RF is given for each layer\
    \ (53 normalization layers in total). A bar with slashes denotes SN after 3\xD7\
    3 conv layer (the others are 1\xD71 conv). A black square \u25A0 indicates SN\
    \ at the shortcut. Its better to zoom in 200 percent."
  Figure 9 Link: articels_figures_by_rev_year\2019\Switchable_Normalization_for_LearningtoNormalize_Deep_Representation\figure_9.jpg
  Figure 9 caption: Average precision (AP) curves of Faster R-CNN on the 2017 val
    set of COCO. (a) Plots the results of finetuning pretrained networks. (b) Shows
    training the models from scratch.
  First author gender probability: 0.5
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.66
  Name of the first author: Ping Luo
  Name of the last author: Jingyu Li
  Number of Figures: 16
  Number of Tables: 11
  Number of authors: 5
  Paper title: Switchable Normalization for Learning-to-Normalize Deep Representation
  Publication Date: 2019-07-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparisons of Normalization Methods
  Table 10 caption:
    table_text: TABLE 10 Results of Kinetics Dataset
  Table 2 caption:
    table_text: TABLE 2 Comparisons of Top-1 Accuracies on the Validation Set of ImageNet,
      by Using ResNet50 Trained with SN, BN, and GN in Different Batch Size Settings
  Table 3 caption:
    table_text: 'TABLE 3 Top: Comparisons of Top1 Top5 Accuracy of BN and SN Trained
      with (8,32) in ImageNet'
  Table 4 caption:
    table_text: TABLE 4 Faster R-CNN for Detection in COCO Using ResNet50 and RPN
  Table 5 caption:
    table_text: TABLE 5 Faster R-CNN+FPN Using ResNet50 and FPN with 1 x x LR Schedule
  Table 6 caption:
    table_text: TABLE 6 Mask R-CNN Using ResNet50 and FPN with 2 x x LR Schedule
  Table 7 caption:
    table_text: TABLE 7 Results in ADE20K Validation Set and Cityscapes Test Set by
      Using ResNet50 with Dilated Convolutions
  Table 8 caption:
    table_text: TABLE 8 Results in Cityscapes Validation Set by Using PSPNet with
      ResNet50 as Backbone Network
  Table 9 caption:
    table_text: TABLE 9 Verification Results of MegaFace Dataset by Using Different
      Backbone Architecture
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2932062
- Affiliation of the first author: department of electrical and computer engineering,
    university of california, san diego, la jolla, ca, usa
  Affiliation of the last author: department of computer science and engineering,
    university of california, san diego, la jolla, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Selfie_Video_Stabilization\figure_1.jpg
  Figure 1 caption: 'Selfie videos have several properties that cause difficulties
    for traditional video stabilization methods: (a) Face and body significantly occludes
    the background; (b) bad feature detection caused by motion blurout of focus, insets
    show areas where feature points are hard to track accurately; (c) foreground and
    background motion mismatch, the foreground motion (red) can be different from
    background motion (blue) due to the dynamics of face and body; Our method uses
    (d) a 3D face model to analyze the motion in the foreground and (e) optical flow
    to analyze the motion in the background. The video is stabilized with respect
    to both foreground and background.'
  Figure 10 Link: articels_figures_by_rev_year\2019\Selfie_Video_Stabilization\figure_10.jpg
  Figure 10 caption: Comparison using 3 metrics suggested in Liu et al. [6]. Horizontal
    lines represent the average metric values of the corresponding methods. Note that
    a higher bar indicates a better result in these metrics.
  Figure 2 Link: articels_figures_by_rev_year\2019\Selfie_Video_Stabilization\figure_2.jpg
  Figure 2 caption: "Pipeline of our method. \u24B6: By fitting a 3D face model, we\
    \ find the head trajectory in the selfie video (Section 4); \u24B7: Optical flow\
    \ is used to track background pixels for 3 neighboring frames; \u24B8: The foreground\
    \ mask is computed from the head trajectory and is used to find the background\
    \ pixels (Section 5). The 2D projective transformation and a grid-based warp field\
    \ is estimated to remove the undesired motion of both foreground and background\
    \ (Section 6)."
  Figure 3 Link: articels_figures_by_rev_year\2019\Selfie_Video_Stabilization\figure_3.jpg
  Figure 3 caption: The vertices used as contour 3D landmarks are fixed in the fitting
    process. The fitted face is rendered and new contour 2D landmarks are detected.
    The projected vertices closest to the detected 2D contour landmarks are selected
    as 3D contour landmarks for the next iteration.
  Figure 4 Link: articels_figures_by_rev_year\2019\Selfie_Video_Stabilization\figure_4.jpg
  Figure 4 caption: Comparison of our 3D face fitting result to Shi et al. [20] and
    Thies et al. [17]. Our method achieves sufficiently good results without using
    complex structure-from-motion and shading constraints.
  Figure 5 Link: articels_figures_by_rev_year\2019\Selfie_Video_Stabilization\figure_5.jpg
  Figure 5 caption: "\u24B6: Accumulated optical flow. A large value indicates the\
    \ background area. \u24B7: Example moving standard deviation of optical flow.\
    \ Large values indicate the edges of objects in the scene."
  Figure 6 Link: articels_figures_by_rev_year\2019\Selfie_Video_Stabilization\figure_6.jpg
  Figure 6 caption: Our method tracks background pixels for 3 neighboring frames.
  Figure 7 Link: articels_figures_by_rev_year\2019\Selfie_Video_Stabilization\figure_7.jpg
  Figure 7 caption: Example video stills from our dataset. The labels represent the
    example indices in Fig. 9.
  Figure 8 Link: articels_figures_by_rev_year\2019\Selfie_Video_Stabilization\figure_8.jpg
  Figure 8 caption: Visual comparison of input video, our result, Grundmann et al.
    [2] result and Liu et al. [5] result. The video stills are scaled by the same
    factor. Our method generates results with larger field of view and does not introduce
    visible distortion. We recommend readers to watch the accompanying video for more
    visual comparison. Labels represent example indices in Fig. 9.
  Figure 9 Link: articels_figures_by_rev_year\2019\Selfie_Video_Stabilization\figure_9.jpg
  Figure 9 caption: Smoothness comparison of input video, our result, Liu et al. result
    [5] and Grundmann et al. result [2]. The horizontal axis represents the examples,
    and the height of the bar represents the smoothness value. Colored arrows are
    added where the bars overlap. The labeled properties are visualized as colored
    dots below each example.
  First author gender probability: 0.53
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jiyang Yu
  Name of the last author: Ravi Ramamoorthi
  Number of Figures: 13
  Number of Tables: 0
  Number of authors: 2
  Paper title: Selfie Video Stabilization
  Publication Date: 2019-07-30 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2931897
