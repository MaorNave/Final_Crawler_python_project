- Affiliation of the first author: institute of computer science, university of freiburg,
    freiburg im breisgau, germany
  Affiliation of the last author: university of freiburg, freiburg im breisgau, germany
  Figure 1 Link: articels_figures_by_rev_year\2021\AutoPytorch_MultiFidelity_MetaLearning_for_Efficient_and_Robust_AutoDL\figure_1.jpg
  Figure 1 caption: Illustrating Shaped MLPs (left) and Shaped ResNets (right). The
    Shaped MLP is built with 100 max. units and four layers. The Shaped ResNet is
    built with two blocks, one block per group, and 100 max. units.
  Figure 10 Link: articels_figures_by_rev_year\2021\AutoPytorch_MultiFidelity_MetaLearning_for_Efficient_and_Robust_AutoDL\figure_10.jpg
  Figure 10 caption: Effect of adding portfolios for warmstarting the search. The
    solid lines show the mean performances and the shaded areas show the standard
    deviation.
  Figure 2 Link: articels_figures_by_rev_year\2021\AutoPytorch_MultiFidelity_MetaLearning_for_Efficient_and_Robust_AutoDL\figure_2.jpg
  Figure 2 caption: Components and Pipeline of a parallel Auto-PyTorch based on a
    master-work principle.
  Figure 3 Link: articels_figures_by_rev_year\2021\AutoPytorch_MultiFidelity_MetaLearning_for_Efficient_and_Robust_AutoDL\figure_3.jpg
  Figure 3 caption: Mean validation accuracy (left) and mean relative regret (right)
    of 2000 evaluated configurations across 35 datasets. For better visualization,
    we sort by mean accuracyregret along each axis respectively.
  Figure 4 Link: articels_figures_by_rev_year\2021\AutoPytorch_MultiFidelity_MetaLearning_for_Efficient_and_Robust_AutoDL\figure_4.jpg
  Figure 4 caption: Portfolio performance as a function of portfolio size on the 2000
    configurations from LCBench.
  Figure 5 Link: articels_figures_by_rev_year\2021\AutoPytorch_MultiFidelity_MetaLearning_for_Efficient_and_Robust_AutoDL\figure_5.jpg
  Figure 5 caption: Average Spearman rank correlation for different datasets and budgets.
  Figure 6 Link: articels_figures_by_rev_year\2021\AutoPytorch_MultiFidelity_MetaLearning_for_Efficient_and_Robust_AutoDL\figure_6.jpg
  Figure 6 caption: Boxplots on the hyperparameter importance according to fANOVA
    (left) and LPI (right) on LCBench.
  Figure 7 Link: articels_figures_by_rev_year\2021\AutoPytorch_MultiFidelity_MetaLearning_for_Efficient_and_Robust_AutoDL\figure_7.jpg
  Figure 7 caption: Correlation between budgets.
  Figure 8 Link: articels_figures_by_rev_year\2021\AutoPytorch_MultiFidelity_MetaLearning_for_Efficient_and_Robust_AutoDL\figure_8.jpg
  Figure 8 caption: Average hyperparameter importance with fANOVA.
  Figure 9 Link: articels_figures_by_rev_year\2021\AutoPytorch_MultiFidelity_MetaLearning_for_Efficient_and_Robust_AutoDL\figure_9.jpg
  Figure 9 caption: Comparison of BO vs BOHB by showing the mean performance as solid
    lines and the standard deviation as shaded areas.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Lucas Zimmer
  Name of the last author: Frank Hutter
  Number of Figures: 13
  Number of Tables: 4
  Number of authors: 3
  Paper title: 'Auto-Pytorch: Multi-Fidelity MetaLearning for Efficient and Robust
    AutoDL'
  Publication Date: 2021-03-22 00:00:00
  Table 1 caption: TABLE 1 Hyperparameters and Ranges of Our Configuration Space 1
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Hyperparameters and Ranges of our Configuration Space 2
  Table 3 caption: TABLE 3 Average Spearman Rank Correlation Across Datasets and Configurations
  Table 4 caption: TABLE 4 Accuracy and Standard Deviation Across 5 Runs of Auto-PyTorch
    and Four Previous AutoML Frameworks After 1h
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3067763
- Affiliation of the first author: state key laboratory of computer-aided design (cad)
    and computer graphics (cg), zhejiang university, hangzhou, china
  Affiliation of the last author: state key laboratory of computer-aided design (cad)
    and computer graphics (cg), zhejiang university, hangzhou, china
  Figure 1 Link: articels_figures_by_rev_year\2021\High_Dimensional_Similarity_Search_With_Satellite_System_Graph_Efficiency_Scalab\figure_1.jpg
  Figure 1 caption: An illustration of the influences of sparsity on search complexity
    (a), (b), and (c) are three MSNETs on a toy dataset with different sparsity. In
    particular, (a) is built according to NSGs edge pruning strategy. Let the blue
    node be the search starting point and the red as the query. In (a) we need two
    hops via a neighbor of the blue node to reach the red, while in (b) and (c) we
    only need one hop. As a result, we need 7, 5, and 8 distance calculations to reach
    the answer on (a), (b), and (c) respectively. (b) delivers the best search performance.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\High_Dimensional_Similarity_Search_With_Satellite_System_Graph_Efficiency_Scalab\figure_2.jpg
  Figure 2 caption: An illustration of the difference between searching for an indexed
    query and an unindexed query. The graph is built according to NSGs edge pruning
    strategy. The black node denotes the search-start node, and the red node denotes
    the query node. (a) shows the search for an indexed query, while (b) shows the
    search for the NN of an unindexed query. The distance to the query determines
    the choice of the first step. Therefore, though they arrive at the same location
    finally, the routes are completely different (red lines).
  Figure 3 Link: articels_figures_by_rev_year\2021\High_Dimensional_Similarity_Search_With_Satellite_System_Graph_Efficiency_Scalab\figure_3.jpg
  Figure 3 caption: "An illustration of selecting edges for one point in a toy 2D\
    \ SSG. The black directed edges are the selected edges for p . The edges are selected\
    \ in the order of edge length. For example, according to the SSG definition, pq\
    \ \u2192 \u2208SSG because Cone( pq \u2192 ,\u03B1)\u2229S=r,q and \u03B4(p,q)<\u03B4\
    (p,r) . ps \u2192 \u2208SSG because S\u2229Cone( ps \u2192 ,\u03B1)=s ."
  Figure 4 Link: articels_figures_by_rev_year\2021\High_Dimensional_Similarity_Search_With_Satellite_System_Graph_Efficiency_Scalab\figure_4.jpg
  Figure 4 caption: ANNS performance of graph-based algorithms (in log-scale) with
    their optimal indices in high-precision region on the four datasets (top right
    is better). The x -axis is not meaningful for Serial-Scan because the results
    are accurate.
  Figure 5 Link: articels_figures_by_rev_year\2021\High_Dimensional_Similarity_Search_With_Satellite_System_Graph_Efficiency_Scalab\figure_5.jpg
  Figure 5 caption: The results of using different angles on GIST1M datasets. On SIFT1M,
    55circ is the best performing angle, but in general, the performance with 60circ
    is better than the other angles.
  Figure 6 Link: articels_figures_by_rev_year\2021\High_Dimensional_Similarity_Search_With_Satellite_System_Graph_Efficiency_Scalab\figure_6.jpg
  Figure 6 caption: The results on indexing complexity (edge selection) and searching
    complexity experiments on the GIST1M and GloVe dataset. The solid lines are the
    complexity evaluation results. The dotted lines are the automatic fitting curves
    according to the formulation in the big O notation. Due to the randomness in the
    procedure, we evaluate the algorithm on different datasets multiple times to get
    the average.
  Figure 7 Link: articels_figures_by_rev_year\2021\High_Dimensional_Similarity_Search_With_Satellite_System_Graph_Efficiency_Scalab\figure_7.jpg
  Figure 7 caption: The results on Deep100M in log-scale. -16core indicates we evaluate
    the search performance of the parallel-search version of the corresponding algorithms
    with 16 threads. -1core indicates we evaluate the search performance of the sequential-search
    version of the corresponding algorithms with only one thread. We evaluate Faiss-GPU
    version on a NVIDIA 1080Ti GPU. We draw all these curves within one figure to
    show the performance difference clearly.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Cong Fu
  Name of the last author: Deng Cai
  Number of Figures: 7
  Number of Tables: 4
  Number of authors: 3
  Paper title: 'High Dimensional Similarity Search With Satellite System Graph: Efficiency,
    Scalability, and Unindexed Query Compatibility'
  Publication Date: 2021-03-23 00:00:00
  Table 1 caption: TABLE 1 The Results of the SSGs and the MRNG on SIFT10K
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Information of Experimental Datasets
  Table 3 caption: TABLE 3 Information of the Graph-Based Indices Involved in All
    of Our Experiments
  Table 4 caption: TABLE 4 The Indexing Time of All the Graph-Based Methods
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3067706
- Affiliation of the first author: department of computer science, university of illinois,
    urbana champaign, champaign, il, usa
  Affiliation of the last author: school of artificial intelligence and automation,
    huazhong university of science and technology, wuhan, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Progressive_and_Aligned_Pose_Attention_Transfer_for_Person_Image_Generation\figure_1.jpg
  Figure 1 caption: Generated examples by our method based on different target poses.
    Zoom in for better details.
  Figure 10 Link: articels_figures_by_rev_year\2021\Progressive_and_Aligned_Pose_Attention_Transfer_for_Person_Image_Generation\figure_10.jpg
  Figure 10 caption: "Visualization of the attention masks in PATBs. \u201CBlock-1\u201D\
    \ denotes the attention mask of the first PATB and \u201CBlock-2 \u223C 9\u201D\
    \ likewise."
  Figure 2 Link: articels_figures_by_rev_year\2021\Progressive_and_Aligned_Pose_Attention_Transfer_for_Person_Image_Generation\figure_2.jpg
  Figure 2 caption: Generator architecture of the proposed method. E A and E S are
    the appearance encoder and shape encoder, respectively. Dec represents the decoder.
  Figure 3 Link: articels_figures_by_rev_year\2021\Progressive_and_Aligned_Pose_Attention_Transfer_for_Person_Image_Generation\figure_3.jpg
  Figure 3 caption: Detailed architectures of the two different versions of the core
    transfer block in our generator, namely the PATB in the left and the APATB in
    the right.
  Figure 4 Link: articels_figures_by_rev_year\2021\Progressive_and_Aligned_Pose_Attention_Transfer_for_Person_Image_Generation\figure_4.jpg
  Figure 4 caption: Qualitative comparisons on DeepFashion dataset. P G 2 , VUnet
    and Deform represent the results of [4], [7] and [5], respectively.
  Figure 5 Link: articels_figures_by_rev_year\2021\Progressive_and_Aligned_Pose_Attention_Transfer_for_Person_Image_Generation\figure_5.jpg
  Figure 5 caption: Qualitative comparisons on Market-1501 dataset. P G 2 , VUnet
    and Deform represent the results of [4], [7] and [5], respectively.
  Figure 6 Link: articels_figures_by_rev_year\2021\Progressive_and_Aligned_Pose_Attention_Transfer_for_Person_Image_Generation\figure_6.jpg
  Figure 6 caption: "GPU running time comparison ( \u03BC s) of different operations\
    \ between a single APATB and a PATB. The collected data is averaged from 1K iterations\
    \ with 1 instance per iteration. The spatial size of the input feature of both\
    \ blocks is 64\xD764 and the channel number is 256. Conv, BN, Mul, Cat, Matmul\
    \ represent convolution, batch normalization, multiplication, concatenation and\
    \ matrix multiplication operations, respectively."
  Figure 7 Link: articels_figures_by_rev_year\2021\Progressive_and_Aligned_Pose_Attention_Transfer_for_Person_Image_Generation\figure_7.jpg
  Figure 7 caption: Qualitative results of different types and numbers of the constituent
    blocks in the generator on Market-1501 and DeepFashion dataset. The images with
    pink border are generated by Resnet generator [15], [18] containing 5, 9, 13 residual
    blocks [51] from left to right respectively. And the images with blue borders
    are generated by our PATN generator containing 5, 9, 13 PATBs. The images with
    orange borders are generated by our APATN generator containing 1, 5, 9 APATBs.
  Figure 8 Link: articels_figures_by_rev_year\2021\Progressive_and_Aligned_Pose_Attention_Transfer_for_Person_Image_Generation\figure_8.jpg
  Figure 8 caption: Qualitative results of the ablation study for PATB on Market-1501
    and DeepFashion dataset.
  Figure 9 Link: articels_figures_by_rev_year\2021\Progressive_and_Aligned_Pose_Attention_Transfer_for_Person_Image_Generation\figure_9.jpg
  Figure 9 caption: Qualitative results of the ablation study for APATB on Market-1501
    and DeepFashion dataset.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Zhen Zhu
  Name of the last author: Xiang Bai
  Number of Figures: 12
  Number of Tables: 6
  Number of authors: 6
  Paper title: Progressive and Aligned Pose Attention Transfer for Person Image Generation
  Publication Date: 2021-03-23 00:00:00
  Table 1 caption: TABLE 1 Comparison With State-of-the-Art on Market-1501 and DeepFashion
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison of Model Size and Testing Speed on DeepFashion
    Dataset
  Table 3 caption: TABLE 3 User Study ( % %)
  Table 4 caption: TABLE 4 Quantitative Ablation Analysis of the Progressive Generation
  Table 5 caption: TABLE 5 Quantitative Comparison of the Ablation Study on Dissection
    of PATB and APATB
  Table 6 caption: TABLE 6 The ReID Results on Inception-v2 and ResNet-50 Using Images
    Generated by Different Methods
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3068236
- Affiliation of the first author: department of electrical and computer engineering,
    national chiao tung university, hsinchu, taiwan
  Affiliation of the last author: department of electrical and computer engineering,
    national chiao tung university, hsinchu, taiwan
  Figure 1 Link: articels_figures_by_rev_year\2021\Hierarchical_and_SelfAttended_Sequence_Autoencoder\figure_1.jpg
  Figure 1 caption: Graphical models for (a) VAE and (b) hierarchical VSAE. Solid
    lines denote the generative model (decoder) while dash lines denote the inference
    model (encoder).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Hierarchical_and_SelfAttended_Sequence_Autoencoder\figure_2.jpg
  Figure 2 caption: Variational sequence autoencoder using RNN encoder and decoder.
    z is encoded for sequence reconstruction.
  Figure 3 Link: articels_figures_by_rev_year\2021\Hierarchical_and_SelfAttended_Sequence_Autoencoder\figure_3.jpg
  Figure 3 caption: Hierarchical sequence autoencoder. Input sequence x t T t=1 is
    attended two times to infer z s , z g .
  Figure 4 Link: articels_figures_by_rev_year\2021\Hierarchical_and_SelfAttended_Sequence_Autoencoder\figure_4.jpg
  Figure 4 caption: Graphical models for inference and generation with encoder and
    decoder in A-VSAE, respectively. Green and red dash lines denote the inference
    with encoders by using a whole sequence mathbf x and a single sample mathbf xt
    , respectively. Blue solid lines denotes the generation as a decoder. Black dash
    line denotes the auxiliary prediction. Diamond and circle denote the deterministic
    and stochastic variables, respectively. Stochastic context vector widetildemathbf
    ct is used for attention.
  Figure 5 Link: articels_figures_by_rev_year\2021\Hierarchical_and_SelfAttended_Sequence_Autoencoder\figure_5.jpg
  Figure 5 caption: Annealing of hyperparameter beta in learning procedure of H-VSAE.
    Blue dashed curve shows beta value with scale in the left. Red curve shows KL
    value with scale in the right.
  Figure 6 Link: articels_figures_by_rev_year\2021\Hierarchical_and_SelfAttended_Sequence_Autoencoder\figure_6.jpg
  Figure 6 caption: 2-D latent spaces of original VSAE (left-up), H-VSAE (right-up),
    A-VSAE (left-down) and delta VSAE (right-down).
  Figure 7 Link: articels_figures_by_rev_year\2021\Hierarchical_and_SelfAttended_Sequence_Autoencoder\figure_7.jpg
  Figure 7 caption: Attention maps for reconstruction of a PTB sentence (left) and
    a Yelp sentence (right) where A-VSAE is applied.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Jen-Tzung Chien
  Name of the last author: Chun-Wei Wang
  Number of Figures: 7
  Number of Tables: 3
  Number of authors: 2
  Paper title: Hierarchical and Self-Attended Sequence Autoencoder
  Publication Date: 2021-03-23 00:00:00
  Table 1 caption: TABLE 1 Linear Interpolation of Two Sentences in Bold by Using
    Their Latent Variables
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison of NLL, KL (of Latent Code z z or z g , z s
    zg,zs), PPL, AU, Number of Parameters (Par), Human Evaluation (HE), Computation
    Time, and Classification Accuracy Over Existing Methods and Proposed Methods
  Table 3 caption: TABLE 3 Comparison of Recall, Precision and F-Measure for Document
    Summarization Over Different Methods on DUC
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3068187
- Affiliation of the first author: school of electrical engineering and telecommunications,
    the university of new south wales, sydney, nsw, australia
  Affiliation of the last author: biomedical and multimedia information technology
    (bmit) research group, school of computer science, university of sydney, darlington,
    nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2021\Deep_Cognitive_Gate_Resembling_Human_Cognition_for_Saliency_Detection\figure_1.jpg
  Figure 1 caption: '(a): Seeing phase. We see the image appearance at first glance,
    where our attentions are driven to the visual stimuli. (b): Perceiving phase.
    We then perceive the dominated scene, which helps us to understand the context
    of the image. The saliency is usually related or partially related to the dominated
    scene. (c): Cogitating phase. We further analyze the image for additional details,
    when we are hesitant on complex regions that were perceived as the dominated scene.
    In this phase, our attentions are usually attracted to the complex regions in
    multi-scale domains. The saliency can then be recognized after this process, which
    is jointly determined by human perceptual and cognitive systems.'
  Figure 10 Link: articels_figures_by_rev_year\2021\Deep_Cognitive_Gate_Resembling_Human_Cognition_for_Saliency_Detection\figure_10.jpg
  Figure 10 caption: 'Sensitivity analyses. (a): F-measure and AUC of our saliency
    model using different types of EF. (b): F-measure and AUC of our saliency model
    under different settings of lambda .'
  Figure 2 Link: articels_figures_by_rev_year\2021\Deep_Cognitive_Gate_Resembling_Human_Cognition_for_Saliency_Detection\figure_2.jpg
  Figure 2 caption: 'Images with similar scenarios. A: Pair of images with multiple
    monitors; B: Trio of images inside a cinema and, C: Trio of images of an indoor
    spa-pool. For each scenario, the state-of-the-art AttM based methods (PiCA [32],
    GBMPM [46] and PAGR [33]) detected well on the upper-layer images (A1, B1 and
    C1) but their performance deteriorated on middlelower-layer images (A2, B2-3 and
    C2-3). In contrary, our saliency model was robust to the variations of the images.'
  Figure 3 Link: articels_figures_by_rev_year\2021\Deep_Cognitive_Gate_Resembling_Human_Cognition_for_Saliency_Detection\figure_3.jpg
  Figure 3 caption: 'Challenging images selected from DUTS dataset for saliency detection
    used to illustrate the inherent problems in AttM based saliency methods. (a):
    feature map before AttMCogG; (b): heat-map of AttMCogG; (c): feature map after
    AttMCogG; (d): final saliency map. Rows A and B: the AttMs failed to suppress
    the visually distinctive but non-salient objects (false positive). Row C: the
    AttMs failed to capture the true salient object (false negative). The proposed
    CogG performed better on these challenging images for saliency detection. The
    feature maps displayed in this figure are processed by a sigmoid function for
    visualization purpose. The authors of GBMPM also used the Gate term in their method.
    As their Gate mechanism follows the AttM expression, in this paper we categorize
    it to the Attentive Module.'
  Figure 4 Link: articels_figures_by_rev_year\2021\Deep_Cognitive_Gate_Resembling_Human_Cognition_for_Saliency_Detection\figure_4.jpg
  Figure 4 caption: Illustration of domain-specific scenes and the difference of scenes
    ( DS ) calculated by our model. (a) EFs produced by DVA [73]. (b) The scenes in
    D 2 , which are the EF-centered images. (c) The scene in D 1 , which is the original
    image. If DS<0 , the area near the corresponding EF is labeled as saliency; otherwise,
    non-saliency.
  Figure 5 Link: articels_figures_by_rev_year\2021\Deep_Cognitive_Gate_Resembling_Human_Cognition_for_Saliency_Detection\figure_5.jpg
  Figure 5 caption: The procedure of generating an EF-centered image. (a) Original
    image and an EF (red node). (b) We move the context of the image such that the
    EF is located at the center of the image frame (denoted as the rectangle with
    solid line). (c) The margin of the image frame in (b), denoted as the dotted pattern,
    is padded with zeros.
  Figure 6 Link: articels_figures_by_rev_year\2021\Deep_Cognitive_Gate_Resembling_Human_Cognition_for_Saliency_Detection\figure_6.jpg
  Figure 6 caption: 'The intermediate outputs (c-f) of our method for saliency detection.
    (a): Image. (b): EFs produced by DVA [73]. (c): Saliency labels of the areas near
    the EFs (see Fig. 4 for more details of the prediction). (d): Cross-domain Scene
    Entropy (En) of the EFs. (e): The features (i.e., mathbf g(e) ) emphasizing the
    specific EF and the weights used to construct boldsymbolCog . The features displayed
    in this figure are processed via a sigmoid function for the visualization purpose.
    (f): Heat-map of boldsymbolCog . (g): Final saliency map of our method. (h): Ground
    truth.'
  Figure 7 Link: articels_figures_by_rev_year\2021\Deep_Cognitive_Gate_Resembling_Human_Cognition_for_Saliency_Detection\figure_7.jpg
  Figure 7 caption: 'Architecture of the deep CogG. The deep CogG produces the necessary
    components (highlighted in red) used to construct the boldsymbolCog . This process
    is iterated for all EFs on an image. fc-layer: fully connected layer.'
  Figure 8 Link: articels_figures_by_rev_year\2021\Deep_Cognitive_Gate_Resembling_Human_Cognition_for_Saliency_Detection\figure_8.jpg
  Figure 8 caption: 'Workflow of our saliency model and the saliency maps on four
    example images. (a): Image; (b): Saliency map inferred from mathcalC5 ; (c): heat-map
    of CogG; (d): Saliency map inferred from B5 ; (e): Final saliency map of our model.
    The salient objects are delineated using red contours on the images according
    to the ground truth. The saliency maps of the state-of-the-art methods on the
    same example images are in Fig. 9.'
  Figure 9 Link: articels_figures_by_rev_year\2021\Deep_Cognitive_Gate_Resembling_Human_Cognition_for_Saliency_Detection\figure_9.jpg
  Figure 9 caption: Saliency maps of our model and the state-of-the-art methods on
    challenging images.
  First author gender probability: 0.96
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Ke Yan
  Name of the last author: Dagan Feng
  Number of Figures: 10
  Number of Tables: 6
  Number of authors: 5
  Paper title: 'Deep Cognitive Gate: Resembling Human Cognition for Saliency Detection'
  Publication Date: 2021-03-23 00:00:00
  Table 1 caption: TABLE 1 Statistical Results of Our Method and State-of-the-Art
    Methods on Six Benchmark Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison of Our Method and EGNet After Post-Processing
    On Six Benchmark Datasets
  Table 3 caption: TABLE 3 Comparison of CogG and AttM on Benchmarking Saliency Datasets
  Table 4 caption: TABLE 4 Embedding Experiment Results (mean F-measure and E-measure)
    of the Proposed CogG on Three Benchmarking Datasets
  Table 5 caption: TABLE 5 Average Processing Time Per Image of Our Method and the
    Counterparts
  Table 6 caption: TABLE 6 Parameter Amount and Floating Point Operations (FLOPs)
    of Our Method and the Other Methods
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3068277
- Affiliation of the first author: department of computer science and engineering,
    southern university of science and technology, shenzhen, china
  Affiliation of the last author: department of computer science and engineering,
    guangdong key laboratory of brain-inspired intelligent computation, southern university
    of science and technology, shenzhen, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Learning_Rates_for_Stochastic_Gradient_Descent_With_Nonconvex_Objectives\figure_1.jpg
  Figure 1 caption: Computational errors, statistical errors and test errors versus
    the number of passes for some datasets. GradTr, GradTe and Graddiff refer to computational
    errors, test errors, and statistical errors, respectively.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Learning_Rates_for_Stochastic_Gradient_Descent_With_Nonconvex_Objectives\figure_2.jpg
  Figure 2 caption: "Test errors versus the number of passes for step sizes of the\
    \ form \u03B7 t = \u03B7 1 t \u2212\u03B8 ,\u03B8\u22080,12,1 and some datasets."
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.55
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: Yunwen Lei
  Name of the last author: Ke Tang
  Number of Figures: 2
  Number of Tables: 0
  Number of authors: 2
  Paper title: Learning Rates for Stochastic Gradient Descent With Nonconvex Objectives
  Publication Date: 2021-03-23 00:00:00
  Table 1 caption: Not Available
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3068154
- Affiliation of the first author: school of software engineering, south china university
    of technology, guangzhou, china
  Affiliation of the last author: school of software engineering, south china university
    of technology, guangzhou, china
  Figure 1 Link: articels_figures_by_rev_year\2021\DiscriminationAware_Network_Pruning_for_Deep_Model_Compression\figure_1.jpg
  Figure 1 caption: "Illustration of discrimination-aware channel pruning. Here, L\
    \ p S denotes the discrimination-aware loss (e.g., cross-entropy loss or additive\
    \ angular margin loss) in the L p th layer, L M denotes the reconstruction loss,\
    \ and L f denotes the final loss. DCP first updates the model M and learns the\
    \ parameters \u03B8 p P p=1 with L p S P p=1 and L f . Then, DCP performs channel\
    \ pruning with P+1 stages. At each stage, for example, in the p th stage, DCP\
    \ conducts the channel selection for each layer in L p\u22121 +1,\u2026, L p with\
    \ the corresponding L p S and L M ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\DiscriminationAware_Network_Pruning_for_Deep_Model_Compression\figure_2.jpg
  Figure 2 caption: Comparisons of DCP and DKP in terms of Top-1 accuracy reduction
    versus parameterFLOPs reduction. We apply DCPDKP to prune VGGNet on CIFAR-10 with
    different pruning rates.
  Figure 3 Link: articels_figures_by_rev_year\2021\DiscriminationAware_Network_Pruning_for_Deep_Model_Compression\figure_3.jpg
  Figure 3 caption: Number of FLOPs w.r.t. each layer in the pruned ResNet-56 and
    VGGNet with different stopping conditions.
  Figure 4 Link: articels_figures_by_rev_year\2021\DiscriminationAware_Network_Pruning_for_Deep_Model_Compression\figure_4.jpg
  Figure 4 caption: Visualization of the feature maps from the second layer of the
    first residual block in ResNet-18 on ILSVRC-12. Feature maps with and without
    the red bounding boxes are the pruned and selected channels, respectively.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.76
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jing Liu
  Name of the last author: Mingkui Tan
  Number of Figures: 4
  Number of Tables: 14
  Number of authors: 7
  Paper title: Discrimination-Aware Network Pruning for Deep Model Compression
  Publication Date: 2021-03-23 00:00:00
  Table 1 caption: TABLE 1 Performance Comparisons on CIFAR-10
  Table 10 caption: TABLE 10 Effect of Feature Reusing
  Table 2 caption: TABLE 2 Performance Comparisons on ILSVRC-12
  Table 3 caption: TABLE 3 Inference Acceleration of the Pruned Models on ILSVRC-12
  Table 4 caption: TABLE 4 Performance Comparisons of Different Methods on Face Recognition
  Table 5 caption: TABLE 5 Comparisons of DCP and DKP on CIFAR-10
  Table 6 caption: TABLE 6 Comparisons of DCP and DKP on ILSVRC-12
  Table 7 caption: TABLE 7 Comparisons on ResNet-18 and ResNet-50 With Different Pruning
    Rates
  Table 8 caption: "TABLE 8 Pruning Results of ResNet-56 and MobileNetV1 With Different\
    \ \u03BB \u03BB"
  Table 9 caption: TABLE 9 Effect of Efficient Single Round Fine-Tuning
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3066410
- Affiliation of the first author: zenith ai, belfast, u.k.
  Affiliation of the last author: zenith ai, belfast, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2021\Ranked_List_Loss_for_Deep_Metric_Learning\figure_1.jpg
  Figure 1 caption: Illustration of our proposed Ranked List Loss (RLL). Given a query
    and its ranked list, RLL aims to make the query closer to the positive set than
    to the negative set by a margin m . Circle and triangle represent two different
    classes. The blue circle is a query. The yellow shapes represent nontrivial examples
    while the red shapes represent trivial examples. The arrow indicates the querys
    gradient direction determined by the corresponding non-trivial examples. The final
    gradient direction of the query is a weighted combination of them. The optimisation
    target for every list is shown in the bottom. Best viewed in colour.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Ranked_List_Loss_for_Deep_Metric_Learning\figure_2.jpg
  Figure 2 caption: Illustration of different ranking-motivated structured losses.
    Different shapes (circle, triangle and square) represent different classes. For
    simplicity, only three classes are shown. The purple circle is an anchor (query).
    In triplet [44], the anchor is compared with only one negative example and one
    positive example. In N -pair-mc [49], Proxy-NCA [34] and Lifted Struct [51], one
    positive example and multiple negative classes are incorporated. N -pair-mc randomly
    selects one example per negative class. Proxy NCA pushes the anchor away from
    negative proxies instead of negative examples. The proxy is class-level and can
    represent any instance in the corresponding class. Lifted Struct uses all examples
    from all negative classes. On the contrary, our proposed Ranked List Loss not
    only exploits all negative examples, but also makes use of all positive ones.
    Best viewed in colour.
  Figure 3 Link: articels_figures_by_rev_year\2021\Ranked_List_Loss_for_Deep_Metric_Learning\figure_3.jpg
  Figure 3 caption: The overall framework of our proposed ranked list loss. In one
    mini-batch, every image acts as a query iteratively and ranks other images according
    to the similarity scores. Then in every ranked list, we mine non-trivial data
    points and weight them based on their distances to the query. Finally, the ranked
    list loss is computed for every query.
  Figure 4 Link: articels_figures_by_rev_year\2021\Ranked_List_Loss_for_Deep_Metric_Learning\figure_4.jpg
  Figure 4 caption: The optimisation objectives with hypersphere regularisation. Different
    shapes represent different classes. For simplicity, we only show three classes
    while many classes exist in practice. alpha -m denotes the diameter of each class
    hypersphere. Therefore, the distance between any two positive examples is optimised
    to be no greater than alpha -m . In addition, the distance between any two hypersphere
    boundaries is no less than m .
  Figure 5 Link: articels_figures_by_rev_year\2021\Ranked_List_Loss_for_Deep_Metric_Learning\figure_5.jpg
  Figure 5 caption: Visualisation of image retrieval on the SOP test dataset. The
    leftmost column shows queries, which rank the images in the gallery according
    to the similarity.
  Figure 6 Link: articels_figures_by_rev_year\2021\Ranked_List_Loss_for_Deep_Metric_Learning\figure_6.jpg
  Figure 6 caption: t-SNE visualisation [56] on the SOP test dataset. Best viewed
    on a monitor when zoomed in.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xinshao Wang
  Name of the last author: Neil M. Robertson
  Number of Figures: 6
  Number of Tables: 15
  Number of authors: 4
  Paper title: Ranked List Loss for Deep Metric Learning
  Publication Date: 2021-03-24 00:00:00
  Table 1 caption: TABLE 1 Comparison With the State-the-of-Art Methods on SOP
  Table 10 caption: TABLE 10 Results of Weighting Positive Examples on the SOP
  Table 2 caption: TABLE 2 Comparison With the State-the-of-Art Methods on In-Shop
    Clothes Dataset
  Table 3 caption: TABLE 3 Comparison With the State-of-the-Art Methods on CARS196
    in Terms of Recall K K (%)
  Table 4 caption: TABLE 4 Comparison With the State-of-the-Art Methods on CUB-200-2011
    in Terms of Recall K K (%)
  Table 5 caption: TABLE 5 Comparison With the State-the-of-Art Methods on SOP When
    the Deep Net Architecture is ResNet-50 [13]
  Table 6 caption: TABLE 6 Exploration on the Network Depth of an Embedding Function
    and the Concatenation of Embedding Functions on SOP
  Table 7 caption: TABLE 7 Exploration on the Network Depth of an Embedding Function
    and the Concatenation of Embedding Functions on In-Shop Clothes Dataset
  Table 8 caption: "TABLE 8 The Impact of \u03B1 \u03B1 on the SOP"
  Table 9 caption: TABLE 9 Results of Different T n Tn on the SOP in Terms of Recall
    K K (%)
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3068449
- Affiliation of the first author: "universidad polit\xE9cnica de madrid, madrid,\
    \ spain"
  Affiliation of the last author: "universidad polit\xE9cnica de madrid, madrid, spain"
  Figure 1 Link: articels_figures_by_rev_year\2021\Autoregressive_Asymmetric_Linear_Gaussian_Hidden_Markov_Models\figure_1.jpg
  Figure 1 caption: An HMM as a probabilistic graphical model.
  Figure 10 Link: articels_figures_by_rev_year\2021\Autoregressive_Asymmetric_Linear_Gaussian_Hidden_Markov_Models\figure_10.jpg
  Figure 10 caption: Test signals from BPFO.
  Figure 2 Link: articels_figures_by_rev_year\2021\Autoregressive_Asymmetric_Linear_Gaussian_Hidden_Markov_Models\figure_2.jpg
  Figure 2 caption: Graphical representation of an AR-AsLG-HMM model.
  Figure 3 Link: articels_figures_by_rev_year\2021\Autoregressive_Asymmetric_Linear_Gaussian_Hidden_Markov_Models\figure_3.jpg
  Figure 3 caption: Pseudocode for the adapted EM algorithm.
  Figure 4 Link: articels_figures_by_rev_year\2021\Autoregressive_Asymmetric_Linear_Gaussian_Hidden_Markov_Models\figure_4.jpg
  Figure 4 caption: Pseudo-code for the adapted SEM algorithm.
  Figure 5 Link: articels_figures_by_rev_year\2021\Autoregressive_Asymmetric_Linear_Gaussian_Hidden_Markov_Models\figure_5.jpg
  Figure 5 caption: Pseudo-code for the forward greedy algorithm.
  Figure 6 Link: articels_figures_by_rev_year\2021\Autoregressive_Asymmetric_Linear_Gaussian_Hidden_Markov_Models\figure_6.jpg
  Figure 6 caption: Sequences of hidden states used to construct the training signals
    for scenario 1 (a) and for scenario 2 (b).
  Figure 7 Link: articels_figures_by_rev_year\2021\Autoregressive_Asymmetric_Linear_Gaussian_Hidden_Markov_Models\figure_7.jpg
  Figure 7 caption: Sequences of hidden states used to construct the test signals.
    Sequence 1 (a) and sequence 2 (b) are used for both scenarios.
  Figure 8 Link: articels_figures_by_rev_year\2021\Autoregressive_Asymmetric_Linear_Gaussian_Hidden_Markov_Models\figure_8.jpg
  Figure 8 caption: Viterbi paths for the air quality example during the first week
    of 2016 when two hidden states are used.
  Figure 9 Link: articels_figures_by_rev_year\2021\Autoregressive_Asymmetric_Linear_Gaussian_Hidden_Markov_Models\figure_9.jpg
  Figure 9 caption: Context-specific graphs learned by AR-AsLG-HMM. (a) shows a graph
    where the air quality is good and (b), where the air quality is bad.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.99
  Name of the first author: Carlos Puerto-Santana
  Name of the last author: Concha Bielza
  Number of Figures: 13
  Number of Tables: 8
  Number of authors: 3
  Paper title: Autoregressive Asymmetric Linear Gaussian Hidden Markov Models
  Publication Date: 2021-03-25 00:00:00
  Table 1 caption: TABLE 1 Reviewed Articles and Their Contributions to Asymmetric
    HMMs and AR HMMs
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Symbols Used in Sections 3 and 4
  Table 3 caption: TABLE 3 Results for Each Testing Sequence of Scenario 1
  Table 4 caption: TABLE 4 Scores for Each Testing Sequence of Scenario 2
  Table 5 caption: TABLE 5 Scenario 1 and 2 Learning Times
  Table 6 caption: TABLE 6 Air Quality Scores When Two Hidden States are Used
  Table 7 caption: TABLE 7 Air Quality Scores When Three Hidden States are Used
  Table 8 caption: TABLE 8 Model Scores for Ball-Bearing Data
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3068799
- Affiliation of the first author: department of automation, tsinghua university,
    beijing, china
  Affiliation of the last author: department of automation, tsinghua university, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2021\StateTemporal_Compression_in_Reinforcement_Learning_With_the_RewardRestricted_Ge\figure_1.jpg
  Figure 1 caption: (a) State-temporal compression. It converts an MDP to an abstract
    SMDP. (b) State-temporal compression-based RL framework.
  Figure 10 Link: articels_figures_by_rev_year\2021\StateTemporal_Compression_in_Reinforcement_Learning_With_the_RewardRestricted_Ge\figure_10.jpg
  Figure 10 caption: (a) The key-searching task. (b) The key-making task. (c) Average
    rewards pm std over 10 runs for the key-searching task. (d) Average rewards pm
    std over 10 runs for the key-making task. The shaded areas represent the standard
    deviation.
  Figure 2 Link: articels_figures_by_rev_year\2021\StateTemporal_Compression_in_Reinforcement_Learning_With_the_RewardRestricted_Ge\figure_2.jpg
  Figure 2 caption: (a) The relationship between the relative distance and the multi-step
    metric d M . (b) The geodesic space which incorporates the environmental structures
    shown in (a).
  Figure 3 Link: articels_figures_by_rev_year\2021\StateTemporal_Compression_in_Reinforcement_Learning_With_the_RewardRestricted_Ge\figure_3.jpg
  Figure 3 caption: "The geodesic metric learning objective. Geodesic-metric network\
    \ \u03A8 w maps the state pairs within a K -step distance to neighboring points\
    \ (in a circle of radius \u03C7 ) and the state pairs beyond the K -step distance\
    \ to distant points (outside the circle of radius \u03C7 )."
  Figure 4 Link: articels_figures_by_rev_year\2021\StateTemporal_Compression_in_Reinforcement_Learning_With_the_RewardRestricted_Ge\figure_4.jpg
  Figure 4 caption: Two phases in training an RRG-RL algorithm.
  Figure 5 Link: articels_figures_by_rev_year\2021\StateTemporal_Compression_in_Reinforcement_Learning_With_the_RewardRestricted_Ge\figure_5.jpg
  Figure 5 caption: RRG-AS option.
  Figure 6 Link: articels_figures_by_rev_year\2021\StateTemporal_Compression_in_Reinforcement_Learning_With_the_RewardRestricted_Ge\figure_6.jpg
  Figure 6 caption: 2D Minecraft map.
  Figure 7 Link: articels_figures_by_rev_year\2021\StateTemporal_Compression_in_Reinforcement_Learning_With_the_RewardRestricted_Ge\figure_7.jpg
  Figure 7 caption: Doom game map.
  Figure 8 Link: articels_figures_by_rev_year\2021\StateTemporal_Compression_in_Reinforcement_Learning_With_the_RewardRestricted_Ge\figure_8.jpg
  Figure 8 caption: (a) Groud truth. (b) The learned geodesic metrics. (c) Metric
    based on raw observations. (d) Metric based on successor representation. Subfigures
    (a) to (d) visualize the four metrics between (4,8) and the other states, respectively.
    We normalized all the distance values to [0,1] with min-max normalization. Since
    larger values of the successor representation represent closer states, we used
    the successor representations negative values and normalized them as the state
    metrics.
  Figure 9 Link: articels_figures_by_rev_year\2021\StateTemporal_Compression_in_Reinforcement_Learning_With_the_RewardRestricted_Ge\figure_9.jpg
  Figure 9 caption: (a) Complex grid world. Average rewards pm std over 10 runs for
    (b) the goal-reaching task with random starting points and (c) the goal-reaching
    task with the farthest starting point. The shaded areas represent the standard
    deviations.
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Shangqi Guo
  Name of the last author: Feng Chen
  Number of Figures: 15
  Number of Tables: 2
  Number of authors: 5
  Paper title: State-Temporal Compression in Reinforcement Learning With the Reward-Restricted
    Geodesic Metric
  Publication Date: 2021-03-25 00:00:00
  Table 1 caption: TABLE 1 Error Bounds for Different Compression Types
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 Average Rewards \xB1 \xB1std for Two 2D Minecraft Tasks"
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3069005
