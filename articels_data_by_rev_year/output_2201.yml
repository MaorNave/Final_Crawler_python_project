- Affiliation of the first author: school of computer science and school of artificial
    intelligence, optics and electronics (iopen), northwestern polytechnical university,
    xian, shaanxi, china
  Affiliation of the last author: school of computer science and school of artificial
    intelligence, optics and electronics (iopen), northwestern polytechnical university,
    xian, shaanxi, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Coordinate_Descent_Method_for_k_kmeans\figure_1.jpg
  Figure 1 caption: Graphic illustration of Test 1 on four datasets. (a), (e), (i)
    and (m) show the mean convergence curves of two methods with the same initialization
    among 30 iterations in 50 runs; (b), (f), (j) and (n) show the mean objective
    function values and standard deviations of 50 runs among 30 iterations for Lloyd,
    where, the size of the intervals A, B, C represents the change of standard deviation
    in the iteration; (c), (g), (k) and (o) show the mean objective function values
    and standard deviations of 50 runs among 30 iterations for CD; (d), (h), (l) and
    (p) show the results of 50 runs of the two methods, where points in green are
    start points of Lloyd and CD, red points and purple points are convergent points
    of Lloyd and CD, respectively.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Coordinate_Descent_Method_for_k_kmeans\figure_2.jpg
  Figure 2 caption: Convergent curve of CD for the first three loops on four datasets.
    (a)-(d) are the convergent curves of CD for first three loops on data Mpeg, Binalpha,
    Ecoil and Palm, respectively. The range of the x -axis of the curve is from 1
    to 3 times the number of dataset, and the length of the x -axis of small figures
    is the number of corresponding dataset.
  Figure 3 Link: articels_figures_by_rev_year\2021\Coordinate_Descent_Method_for_k_kmeans\figure_3.jpg
  Figure 3 caption: Graphic illustration of Test 1. We choose some datasets, Mpeg
    (denoted as Mp), Palm (denoted as P), TDT2 10 (denoted as T 10), Binalpha (denoted
    as B), TDT2 4 (denoted as T 4), 20news (denoted as 20), to show the results of
    Table 2, histogram in red is the result of Lloyd and histogram in purple is the
    result of CD. (a) is the comparison result of mean objective value of two methods;
    (b) is the comparison result of standard deviation, which measures the stability
    of the results; (c) is the comparison result of minimum objective value of two
    methods. It is worth noting that the value of the objective function on the ordinate
    is normalized to between 0 and 1 for simplicity.
  Figure 4 Link: articels_figures_by_rev_year\2021\Coordinate_Descent_Method_for_k_kmeans\figure_4.jpg
  Figure 4 caption: Graphic illustration of Test 2. Four datasets Mpeg, Binalpha,
    Ecoil and Palm show the results of Test 2. (a), (c), (e) and (g) are convergence
    curves of hybrid algorithm, i.e., conducting CD after Lloyd, the convergent of
    Lloyd is also the start point of CD; (b), (d), (f) and (h) are the results of
    50 runs of the two methods, points in green, red and purple are start points,
    convergent points of Lloyd and convergent points of CD, respectively. It is worth
    noting that the convergent points of Lloyd, the points in red, are also the start
    points of CD.
  Figure 5 Link: articels_figures_by_rev_year\2021\Coordinate_Descent_Method_for_k_kmeans\figure_5.jpg
  Figure 5 caption: Graphic illustration of Test 3. Four datasets Mpeg, Binalpha,
    Ecoil and Palm show the results of Test 3. (a), (c), (e) and (g) are convergence
    curves of hybrid algorithm, i.e., conducting Lloyd after CD, the convergent of
    CD is also the start point of Lloyd; (b), (d), (f) and (h) are the results of
    50 runs of the two methods, points in green, red and purple are start points,
    convergent points of Lloyd and convergent points of CD, respectively. It is worth
    noting that the convergent points of CD, the points in purple, are also the start
    points of Lloyd.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Feiping Nie
  Name of the last author: Xuelong Li
  Number of Figures: 5
  Number of Tables: 7
  Number of authors: 6
  Paper title: Coordinate Descent Method for k k-means
  Publication Date: 2021-06-01 00:00:00
  Table 1 caption: TABLE 1 Benchmark Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison for Objective Value With Initialization in k
    k-means++
  Table 3 caption: TABLE 3 Comparison for Objective Value With Random Initialization
  Table 4 caption: TABLE 4 The p p-value of t t Test With Initialization of k k-means++
  Table 5 caption: TABLE 5 The p p-value of t t Test With Random Initialization
  Table 6 caption: TABLE 6 Hybrid Results
  Table 7 caption: TABLE 7 Speedup of Improved Version Algorithm 4 over Algorithm
    3
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3085739
- Affiliation of the first author: ai lab, deepwise healthcare, beijing, china
  Affiliation of the last author: department of computer science, university of hong
    kong, pokfulam, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2021\Act_Like_a_Radiologist_Towards_Reliable_MultiView_Correspondence_Reasoning_for_M\figure_1.jpg
  Figure 1 caption: An illustration of the relation among mammography views. Standard
    mammography screening takes a CC view and an MLO view for each breast. Figure
    (a)-(c) represent the examined view (i.e., CC view of the right breast), the contralateral
    view (i.e., CC view of the left breast) and the auxiliary view (i.e., MLO view
    of the right breast) of a specific instance. The examined view represents the
    view where the detection is performed. Figure (a) and (b) form a bilateral pair,
    and are roughly symmetric since they have similar gland background, breast shape
    and breast size. Figure (a) and (c) form an ipsilateral pair, and provide complementary
    information to represent the 3D anatomical structure. Patch (1) and (3) in the
    corresponding images refer to a mass lesion instance, while Patch (2) locates
    similarly as Patch (1) in the contralateral image. Figure (d) offers zoom-in versions
    of these patches. Figure (e) stands for an ideal projection model of mammography.
    We can see that the CC view is a top-down view of the breast, while the MLO view
    is a side view taken at a certain angle along the pectoral muscle plane.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Act_Like_a_Radiologist_Towards_Reliable_MultiView_Correspondence_Reasoning_for_M\figure_2.jpg
  Figure 2 caption: The pipeline of the proposed AGN. AGN takes multi-view backbone
    features as inputs, and outputs enhanced features of the examined view for further
    prediction. First, bipartite graph convolutional network performs reasoning across
    ipsilateral views end outputs auxiliary representations of mass lesion 3D structure.
    Second, inception graph convolutional network contrasts bilateral views and produces
    attention maps on the suspicious asymmetric areas. Finally, correspondence reasoning
    enhancement based on the defined two graphs is conducted to enhance the backbone
    features of the examined view for further detection.
  Figure 3 Link: articels_figures_by_rev_year\2021\Act_Like_a_Radiologist_Towards_Reliable_MultiView_Correspondence_Reasoning_for_M\figure_3.jpg
  Figure 3 caption: Illustration of pseudo landmarks and bipartite graph node mapping.
    (a)-(b) draw pseudo landmarks and the matched bounding boxes on CC and MLO views
    respectively. (c) illustrates how bipartite node mapping works when k=1 . Each
    mapping cell denotes the representative region of the node in the CC view.
  Figure 4 Link: articels_figures_by_rev_year\2021\Act_Like_a_Radiologist_Towards_Reliable_MultiView_Correspondence_Reasoning_for_M\figure_4.jpg
  Figure 4 caption: Detection results of AG-RCNN. Each row shows a representative
    case. Column (a)-(c) refer to the examined view, the flipped contralateral view
    and the auxiliary view with annotations. Column (d)-(e) indicate detection results
    by Mask-RCNN and AG-RCNN. Column (f) visualizes the attention area on the auxiliary
    view. Column (g) shows the attention regions of bilateral views. Column (h)-(i)
    visualize the response maps before and after correspondence reasoning enhancement.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.76
  Name of the first author: Yuhang Liu
  Name of the last author: Yizhou Yu
  Number of Figures: 4
  Number of Tables: 9
  Number of authors: 6
  Paper title: 'Act Like a Radiologist: Towards Reliable Multi-View Correspondence
    Reasoning for Mammogram Mass Detection'
  Publication Date: 2021-06-01 00:00:00
  Table 1 caption: TABLE 1 Performance on DDSM Dataset (%)
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Performance on DDSM Dataset (%)
  Table 3 caption: TABLE 3 Performance on In-House Dataset (%)
  Table 4 caption: TABLE 4 Effectiveness of Pseudo Landmarks on DDSM and In-House
    Datasets (%)
  Table 5 caption: TABLE 5 Effectiveness of Node Number on DDSM and In-House Datasets
    (%)
  Table 6 caption: TABLE 6 Effectiveness of Graph Node Mapping on DDSM and In-House
    Datasets (%)
  Table 7 caption: TABLE 7 Ablation of Components in Bipartite Graph Convolutional
    Network on DDSM and In-House Datasets (%)
  Table 8 caption: TABLE 8 Ablation of Components in Inception Graph Convolutional
    Network on DDSM and In-House Datasets (%)
  Table 9 caption: TABLE 9 Ablation of Modules on DDSM and In-House Datasets (%)
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3085783
- Affiliation of the first author: centre for artificial intelligence research, university
    of agder, grimstad, norway
  Affiliation of the last author: centre for artificial intelligence research, university
    of agder, grimstad, norway
  Figure 1 Link: articels_figures_by_rev_year\2021\On_the_Convergence_of_Tsetlin_Machines_for_the_IDENTITY_and_NOT_Operators\figure_1.jpg
  Figure 1 caption: A two-action Tsetlin automaton with 2N states.
  Figure 10 Link: articels_figures_by_rev_year\2021\On_the_Convergence_of_Tsetlin_Machines_for_the_IDENTITY_and_NOT_Operators\figure_10.jpg
  Figure 10 caption: How the positions of s2, ~s3, ~s5, ~s6 change with c , in the
    environment a>b>0.5 .
  Figure 2 Link: articels_figures_by_rev_year\2021\On_the_Convergence_of_Tsetlin_Machines_for_the_IDENTITY_and_NOT_Operators\figure_2.jpg
  Figure 2 caption: A TA team G i j consisting of 2o TAs.
  Figure 3 Link: articels_figures_by_rev_year\2021\On_the_Convergence_of_Tsetlin_Machines_for_the_IDENTITY_and_NOT_Operators\figure_3.jpg
  Figure 3 caption: TM disjunctive normal form architecture.
  Figure 4 Link: articels_figures_by_rev_year\2021\On_the_Convergence_of_Tsetlin_Machines_for_the_IDENTITY_and_NOT_Operators\figure_4.jpg
  Figure 4 caption: TM voting architecture.
  Figure 5 Link: articels_figures_by_rev_year\2021\On_the_Convergence_of_Tsetlin_Machines_for_the_IDENTITY_and_NOT_Operators\figure_5.jpg
  Figure 5 caption: "Markov chains in the noise-free case, when P(y=1|X=1)=1 , P(y=0|X=0)=1\
    \ , and P(X=1)=c , c\u2208(0,1) ."
  Figure 6 Link: articels_figures_by_rev_year\2021\On_the_Convergence_of_Tsetlin_Machines_for_the_IDENTITY_and_NOT_Operators\figure_6.jpg
  Figure 6 caption: A Markov chain for Lemma 1.
  Figure 7 Link: articels_figures_by_rev_year\2021\On_the_Convergence_of_Tsetlin_Machines_for_the_IDENTITY_and_NOT_Operators\figure_7.jpg
  Figure 7 caption: Markov chains in the noisy case, where P(y=1 | X=1) = a , and
    P(y=1 | X=0) = b ; P(X=1)=c ; a,b,c in (0,1) .
  Figure 8 Link: articels_figures_by_rev_year\2021\On_the_Convergence_of_Tsetlin_Machines_for_the_IDENTITY_and_NOT_Operators\figure_8.jpg
  Figure 8 caption: Positions of s1, ~s2, ~s3, ~s4, ~s5, ~s6 when a>b>0.5 or b>a>0.5
    , with c in (0, 1) .
  Figure 9 Link: articels_figures_by_rev_year\2021\On_the_Convergence_of_Tsetlin_Machines_for_the_IDENTITY_and_NOT_Operators\figure_9.jpg
  Figure 9 caption: Moving trend when sin (s2, s5) . In the noisy case, where P(y=1
    | X=1) = a , and P(y=1 | X=0) = b ; P(X=1)=c ; 0.5<b<a<1, ~c in (0,1) .
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xuan Zhang
  Name of the last author: Morten Goodwin
  Number of Figures: 10
  Number of Tables: 7
  Number of authors: 4
  Paper title: On the Convergence of Tsetlin Machines for the IDENTITY- and NOT Operators
  Publication Date: 2021-06-02 00:00:00
  Table 1 caption: "TABLE 1 Type I Feedback \u2014 Feedback Upon Receiving a Sample\
    \ With Label y=1 y=1, for a Single TA to Decide Whether to Include or Exclude\
    \ a Given Literal x k \xAC x k xk\xACxk into C i j Cji"
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 Type II Feedback \u2014 Feedback Upon Receiving a Sample\
    \ With Label y=0 y=0, for a Single TA to Decide Whether to Include or Exclude\
    \ a Given Literal x k \xAC x k xk\xACxk into C i j Cji"
  Table 3 caption: TABLE 3 Conditions of s s for TAs to Converge
  Table 4 caption: TABLE 4 Three Convergence Possibilities of the TM
  Table 5 caption: "TABLE 5 State Transitions per Scenarios, in the Noise-Free Case,\
    \ Where P(y=1|X=1)=1 P(y=1|X=1)=1, and P(y=0|X=0)=1 P(y=0|X=0)=1; P(X=1)=c P(X=1)=c;\
    \ c\u2208(0,1) c\u2208(0,1)"
  Table 6 caption: "TABLE 6 State Transitions per Senarios, Part 1, in the Case with\
    \ Noise, Where P(y=1|X=1)=a P(y=1|X=1)=a, and P(y=1|X=0)=b P(y=1|X=0)=b; P(X=1)=c\
    \ P(X=1)=c; a,b,c\u2208(0,1) a,b,c\u2208(0,1)"
  Table 7 caption: "TABLE 7 State Transitions per Scenarios, Part 2, in the Case with\
    \ Noise, Where P(y=1|X=1)=a P(y=1|X=1)=a, and P(y=1|X=0)=b P(y=1|X=0)=b; P(X=1)=c\
    \ P(X=1)=c; a,b,c\u2208(0,1) a,b,c\u2208(0,1)"
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3085591
- Affiliation of the first author: computer science department, purdue university,
    west lafayette, in, usa
  Affiliation of the last author: computer science department, purdue university,
    west lafayette, in, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\Importance_Weight_Estimation_and_Generalization_in_Domain_Adaptation_Under_Label\figure_1.jpg
  Figure 1 caption: Categorical Y .
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Importance_Weight_Estimation_and_Generalization_in_Domain_Adaptation_Under_Label\figure_2.jpg
  Figure 2 caption: Normed vector space Y .
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Kamyar Azizzadenesheli
  Name of the last author: Kamyar Azizzadenesheli
  Number of Figures: 2
  Number of Tables: 1
  Number of authors: 1
  Paper title: Importance Weight Estimation and Generalization in Domain Adaptation
    Under Label Shift
  Publication Date: 2021-06-02 00:00:00
  Table 1 caption: TABLE 1 Label Shift Domain Adaptation
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3086060
- Affiliation of the first author: "centro singular de investigaci\xF3n en tecnolox\xED\
    as intelixentes da univ. de santiago de compostela (citius), santiago de compostela,\
    \ spain"
  Affiliation of the last author: "centro singular de investigaci\xF3n en tecnolox\xED\
    as intelixentes da univ. de santiago de compostela (citius), santiago de compostela,\
    \ spain"
  Figure 1 Link: articels_figures_by_rev_year\2021\Fast_Support_Vector_Classification_for_LargeScale_Problems\figure_1.jpg
  Figure 1 caption: "Upper panels: examples of function A(\u03C3) of Eq. (29) for\
    \ datasets abalone, dermatology and musk. Lower panels: profiles of kappa versus\
    \ \u03C3 achieved by FSVC2 with grid-search tuning on these datasets."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Fast_Support_Vector_Classification_for_LargeScale_Problems\figure_2.jpg
  Figure 2 caption: "Kappa achieved by FSVC for each dataset using \u03C3 \u2217 =\
    \ \u03C3 max , in blue, \u03C3 \u2217 =argminA , in red, and \u03C3 \u2217 on\
    \ the left corner of A , in green."
  Figure 3 Link: articels_figures_by_rev_year\2021\Fast_Support_Vector_Classification_for_LargeScale_Problems\figure_3.jpg
  Figure 3 caption: Time of FSVC and LSVC per pattern, input and class on the large
    datasets versus dataset size.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ziad Akram-Ali-Hammouri
  Name of the last author: "Sen\xE9n Barro"
  Number of Figures: 3
  Number of Tables: 8
  Number of authors: 4
  Paper title: Fast Support Vector Classification for Large-Scale Problems
  Publication Date: 2021-06-02 00:00:00
  Table 1 caption: TABLE 1 List of the Small (top) and Large (bottom) Classification
    Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Versions of SVC and FSVC (in Bold Those Features Which
    are Different From the Original SVC or FSVC)
  Table 3 caption: TABLE 3 Kappa (in %) of SVC, SVC1, SVC2, SVC3, FSVC1, and FSVC2
    on the Small Datasets (in Bold Values Below SVC by More Than 15 Percent)
  Table 4 caption: TABLE 4 Kappa and Time Per Fold of FSVC and FSVCL (Linear Kernel)
    in High-Dimensional Datasets (Upper Part), by FSVC and FSVCA (One-vs-All) in Multi-Class
    Datasets (Middle Part), and by FSVC and FSVCLA (Linear Kernel, One-vs-All) in
    High-Dimensional Multi-Class Datasets (Lower Part)
  Table 5 caption: TABLE 5 Kappa and TimeFold of FSVC, SVC, LSVC and DKP on Small
    Datasets (in Bold Kappa Values of FSVC Below SVC by More Than 15 Points)
  Table 6 caption: TABLE 6 Kappa and Time Per Fold Achieved by FSVC and LSVC; t LSVC
    t FSVC t LSVC tFSVC, on the Large Datasets; and TrainingTest Time Per Pattern
    of FSVC
  Table 7 caption: TABLE 7 Memory Required by the Largest Data Used by FSVC
  Table 8 caption: TABLE 8 Time(in sec.fold) and Memory (in MB or GB) Required by
    FSVC and LSVC on the Large Datasets With 32 GB and 2 GB of Available Memory
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3085969
- Affiliation of the first author: school of mathematics and computing (computational
    science and engineering), yonsei university, seoul, south korea
  Affiliation of the last author: school of mathematics and computing (computational
    science and engineering), yonsei university, seoul, south korea
  Figure 1 Link: articels_figures_by_rev_year\2021\A_Fully_Automated_Method_for_D_Individual_Tooth_Identification_and_Segmentation_\figure_1.jpg
  Figure 1 caption: 'Schematic diagram of the proposed method, which consists of four
    steps: 1) Panoramic image reconstruction of the upper and lower jaws from a 3D
    CBCT image; 2) tooth identification and 2D segmentation of individual teeth in
    the panoramic images; 3) extraction of loose and tight 3D tooth ROIs using the
    detected bounding boxes and segmented tooth regions; and 4) 3D segmentation for
    individual teeth from the 3D tooth ROIs.'
  Figure 10 Link: articels_figures_by_rev_year\2021\A_Fully_Automated_Method_for_D_Individual_Tooth_Identification_and_Segmentation_\figure_10.jpg
  Figure 10 caption: Illustration of tooth identification when there are missing teeth.
    Two premolar (class P) corresponding to number 4 are missing.
  Figure 2 Link: articels_figures_by_rev_year\2021\A_Fully_Automated_Method_for_D_Individual_Tooth_Identification_and_Segmentation_\figure_2.jpg
  Figure 2 caption: Workflow of Step 1. This shows reconstruction process of upper
    jaw panoramic image P X upper and lower jaw panoramic images P X lower from a
    3D CT image X .
  Figure 3 Link: articels_figures_by_rev_year\2021\A_Fully_Automated_Method_for_D_Individual_Tooth_Identification_and_Segmentation_\figure_3.jpg
  Figure 3 caption: Concept of Step 2-1. A detection map f det predicts Y ij for each
    G ij .
  Figure 4 Link: articels_figures_by_rev_year\2021\A_Fully_Automated_Method_for_D_Individual_Tooth_Identification_and_Segmentation_\figure_4.jpg
  Figure 4 caption: Tooth identification process using the classification results
    in Step 2-1. The capital letters represent the first letters of the tooth type
    and the numbers are tooth codes.
  Figure 5 Link: articels_figures_by_rev_year\2021\A_Fully_Automated_Method_for_D_Individual_Tooth_Identification_and_Segmentation_\figure_5.jpg
  Figure 5 caption: Extraction of loose and tight 3D tooth ROIs from a detected bounding
    box and a segmented tooth region.
  Figure 6 Link: articels_figures_by_rev_year\2021\A_Fully_Automated_Method_for_D_Individual_Tooth_Identification_and_Segmentation_\figure_6.jpg
  Figure 6 caption: Tooth detection results. A PR curve represents the change in the
    precision as the recall increases for a fixed IOU threshold value, which is used
    for NMS.
  Figure 7 Link: articels_figures_by_rev_year\2021\A_Fully_Automated_Method_for_D_Individual_Tooth_Identification_and_Segmentation_\figure_7.jpg
  Figure 7 caption: Confusion matrix for tooth identification. (a) Result of the direct
    method. (b) Result of the proposed method.
  Figure 8 Link: articels_figures_by_rev_year\2021\A_Fully_Automated_Method_for_D_Individual_Tooth_Identification_and_Segmentation_\figure_8.jpg
  Figure 8 caption: Image on the left is a CBCT image that is affected by metal artifacts.
    Image on the right is a panoramic image generated from Step 1, which is not affected
    by metal-related artifacts.
  Figure 9 Link: articels_figures_by_rev_year\2021\A_Fully_Automated_Method_for_D_Individual_Tooth_Identification_and_Segmentation_\figure_9.jpg
  Figure 9 caption: Qualitative comparison for 3D individual tooth segmentation in
    a CBCT image with metal artifacts. Segmentation result of (a) Mask R-CNN, (b)
    PANet, (c) HTC, (d) ToothNet, (e) the proposed method using loose ROIs, (f) tight
    ROIs, and (g) both loose and tight ROIs.
  First author gender probability: 0.9
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.71
  Name of the first author: Tae Jun Jang
  Name of the last author: Jin Keun Seo
  Number of Figures: 10
  Number of Tables: 4
  Number of authors: 4
  Paper title: A Fully Automated Method for 3D Individual Tooth Identification and
    Segmentation in Dental CBCT
  Publication Date: 2021-06-02 00:00:00
  Table 1 caption: TABLE 1 Quantitative Evaluation for Tooth Identification Methods
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Quantitative Evaluation for 2D Tooth Segmentation Methods
  Table 3 caption: TABLE 3 Quantitative Comparison for 3D Tooth Segmentation Methods
  Table 4 caption: TABLE 4 Ablation Study for the Proposed Method
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3086072
- Affiliation of the first author: school of data science, fudan university, shanghai,
    china
  Affiliation of the last author: school of data science, fudan university, shanghai,
    china
  Figure 1 Link: articels_figures_by_rev_year\2021\How_to_Trust_Unlabeled_Data_Instance_Credibility_Inference_for_FewShot_Learning\figure_1.jpg
  Figure 1 caption: The inference process of our proposed framework. We extract features
    of each labeled and unlabeled instance, train a linear classifier with the support
    set, provide pseudo-label for the unlabeled instances, and use ICI to select the
    most trustworthy subset to expand the support set. This process is repeated until
    all the unlabeled data are included in the support set.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\How_to_Trust_Unlabeled_Data_Instance_Credibility_Inference_for_FewShot_Learning\figure_2.jpg
  Figure 2 caption: "Regularization path of \u03BB on ten samples. Red line is corresponding\
    \ to the most trustworthy sample suggested by our ICI algorithm."
  Figure 3 Link: articels_figures_by_rev_year\2021\How_to_Trust_Unlabeled_Data_Instance_Credibility_Inference_for_FewShot_Learning\figure_3.jpg
  Figure 3 caption: New images selected per class in each iteration of an inference
    episode on miniImageNet. The averaged test accuracy is on the left, while the
    test accuracy of each class is listed at the bottom of the corresponding images
    in each iteration. In each iteration, the correctly-predicted instances of each
    class are placed on the left, and vice versa on the right. For each class, we
    select 5 images at most. Note that in some iteration the number of the left unlabeled
    instances of classes is smaller than 5. The remaining images are incorrectly predicted
    in the other classes.
  Figure 4 Link: articels_figures_by_rev_year\2021\How_to_Trust_Unlabeled_Data_Instance_Credibility_Inference_for_FewShot_Learning\figure_4.jpg
  Figure 4 caption: Histogram of errors in 2000 episodes. The x -axis is the value
    of errors, while the y -axis is the number of errors.
  Figure 5 Link: articels_figures_by_rev_year\2021\How_to_Trust_Unlabeled_Data_Instance_Credibility_Inference_for_FewShot_Learning\figure_5.jpg
  Figure 5 caption: Regularization path of lambda . Red lines are correct-predicted
    instances while black lines are wrong-predicted ones. ICI will choose instances
    in the lower-left subset.
  Figure 6 Link: articels_figures_by_rev_year\2021\How_to_Trust_Unlabeled_Data_Instance_Credibility_Inference_for_FewShot_Learning\figure_6.jpg
  Figure 6 caption: "Variation of accuracy as the selected samples increases over\
    \ 2000 episodes on miniImageNet. \u201CICI (n)\u201D: select n samples per class\
    \ in each iteration."
  Figure 7 Link: articels_figures_by_rev_year\2021\How_to_Trust_Unlabeled_Data_Instance_Credibility_Inference_for_FewShot_Learning\figure_7.jpg
  Figure 7 caption: Validation accuracy with different alpha s.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Yikai Wang
  Name of the last author: Yanwei Fu
  Number of Figures: 7
  Number of Tables: 7
  Number of authors: 4
  Paper title: How to Trust Unlabeled Data? Instance Credibility Inference for Few-Shot
    Learning
  Publication Date: 2021-06-03 00:00:00
  Table 1 caption: TABLE 1 Number of Episodes Satisfying Each Assumption and Whether
    the Transductive Inference Improve the Performance
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The Averaged Accuracies With 95 percent Confidence Intervals
    Over 2000 Episodes on Several Datasets
  Table 3 caption: TABLE 3 Compare to Baselines on miniImageNet Under Several Settings
  Table 4 caption: TABLE 4 We Run 2000 Episodes, With Each Episode Training an Initial
    Classifier
  Table 5 caption: TABLE 5 Performance of ICI Using Different Classifiers on miniImageNet
    Under Several Settings
  Table 6 caption: TABLE 6 Influence of Reduced Dimension and Dimension Reduction
    Algorithms
  Table 7 caption: TABLE 7 Comparison Under Different Backbones With Exactly the Same
    Features
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3086140
- Affiliation of the first author: cedric, conservatoire national des arts et metiers,
    paris, france
  Affiliation of the last author: "valeo.ai universit\xE8, paris, france"
  Figure 1 Link: articels_figures_by_rev_year\2021\Confidence_Estimation_via_Auxiliary_Models\figure_1.jpg
  Figure 1 caption: Distributions of different confidence measures over correct and
    erroneous predictions of a given model. When ranking according to MCP (a) the
    test predictions of a convolutional model trained on CIFAR-10, we observe that
    correct ones (in green) and misclassifications (in red) overlap considerably,
    making it difficult to distinguish them. On the other hand, ranking samples according
    to TCP (b) alleviates this issue and allows a much better separation.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Confidence_Estimation_via_Auxiliary_Models\figure_2.jpg
  Figure 2 caption: "Learning confidence approach. The fixed classification network\
    \ F , with parameters w=( w E , w cls ) , is composed of a succession of convolutional\
    \ and fully-connected layers (encoder E ) followed by last classification layers\
    \ with softmax activation. The auxiliary confidence network C , with parameters\
    \ \u03B8 , builds upon the feature maps extracted by the encoder E , or its fine-tuned\
    \ version E \u2032 with parameters w E \u2032 : they are passed to ConfidNet,\
    \ a trainable multi-layer module with parameters \u03C6 . The auxiliary model\
    \ outputs a confidence score C(x;\u03B8)\u2208[0,1] , with \u03B8=\u03C6 in absence\
    \ of encoder fine-tuning and \u03B8=( w E \u2032 ,\u03C6) in case of fine-tuning."
  Figure 3 Link: articels_figures_by_rev_year\2021\Confidence_Estimation_via_Auxiliary_Models\figure_3.jpg
  Figure 3 caption: "Overview of proposed confidence learning for domain adaptation\
    \ (ConDA) in semantic segmentation. Given images in source and target domains,\
    \ we pass them to the encoder part of the segmentation network F to obtain their\
    \ feature maps. This network F is fixed during this phase and its weights are\
    \ not updated. The confidence maps are obtained by feeding these feature maps\
    \ to the trainable head of the confidence network C , which includes a multi-scale\
    \ ConfidNet module. For source-domain images, a regression loss L conf (8) is\
    \ computed to minimize the distance between C \u03B8 x s and the fixed true-class-probability\
    \ map TCP F ( x s , y s ) . An adversarial training scheme \u2013 based on discriminators\
    \ loss L D (\u03C8) (Eq. (10)) and adversarial part L adv (\u03B8) of confidence\
    \ nets loss (Eq. (12)) \u2013, is also added to enforce the consistency between\
    \ the C \u03B8 x s s and C \u03B8 x t s. Dashed arrows stand for paths that are\
    \ used only at train time."
  Figure 4 Link: articels_figures_by_rev_year\2021\Confidence_Estimation_via_Auxiliary_Models\figure_4.jpg
  Figure 4 caption: Limitations of MC Dropouts confidence measure. Two test samples
    from SVHN dataset, which are respectively misclassified (left) and correctly classified
    (right) by a given model F , illustrate these limits. The entropies of the predicted
    class distributions (averaged over Monte Carlo dropout layers and displayed as
    histograms) are equally high, at around 0.79, resulting in equally low MC Dropout
    confidence estimates. In contrast, both MCP and TCP approximated by ConfidNet
    clearly differ as expected for the two examples. Yet, ConfidNet has the best behavior,
    being the lowest for the erroneous models prediction and the highest for the correct
    one.
  Figure 5 Link: articels_figures_by_rev_year\2021\Confidence_Estimation_via_Auxiliary_Models\figure_5.jpg
  Figure 5 caption: "Qualitative results of pseudo-label selection for semantic-segmentation\
    \ adaptation. The three first columns present target-domain images of the GTA5\
    \ \u25B9 Cityscapes benchmark (a) along with their ground-truth segmentation maps\
    \ (b) and the predicted maps before self-training (c). We compare pseudo-labels\
    \ collected with MCP (d) and with ConDA (e). Green (resp. red) pixels are correct\
    \ (resp. erroneous) predictions selected by the method and black pixels are discarded\
    \ predictions. ConDA retains fewer errors while preserving approximately the same\
    \ amount of correct predictions."
  Figure 6 Link: articels_figures_by_rev_year\2021\Confidence_Estimation_via_Auxiliary_Models\figure_6.jpg
  Figure 6 caption: "Comparative quality of selected pseudo-labels. Proportion of\
    \ correct pseudo-labels (precision) for different coverages on GTA5 \u25B9 Cityscapes,\
    \ for MCP and ConDA."
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "Charles Corbi\xE8re"
  Name of the last author: "Patrick P\xE9rez"
  Number of Figures: 6
  Number of Tables: 6
  Number of authors: 6
  Paper title: Confidence Estimation via Auxiliary Models
  Publication Date: 2021-06-04 00:00:00
  Table 1 caption: TABLE 1 Comparison of Confidence Estimation Methods for Failure
    Prediction and Selective Classification
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Impact of the Choice of Training Data on the Error-Prediction
    Performance of ConfidNet
  Table 3 caption: TABLE 3 Impact of the Encoder Fine-Tuning on the Error-Prediction
    performance of ConfidNet
  Table 4 caption: TABLE 4 Comparative Performance on Semantic Segmentation With Synth-to-Real
    Unsupervised Domain Adaptation
  Table 5 caption: TABLE 5 Effect of the Loss on the Error-Detection Performance of
    ConfidNet
  Table 6 caption: TABLE 6 Ablation Study on Semantic Segmentation With Pseudo-Labelling-Based
    Adaptation
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3085983
- Affiliation of the first author: school of software engineering, south china university
    of technology, guangzhou, china
  Affiliation of the last author: tencent ai lab, tencent, shenzhen, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Towards_Accurate_and_Compact_Architectures_via_Neural_Architecture_Transformer\figure_1.jpg
  Figure 1 caption: Comparison between Neural Architecture Optimization (NAO) [20]
    and our Neural Architecture Transformer (NAT). Green blocks denote the two input
    nodes of the cell and blue blocks denote the intermediate nodes. Red blocks denote
    the connections that are changed by NAT. The accuracy and the number of parameters
    are evaluated on CIFAR-10 models.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Towards_Accurate_and_Compact_Architectures_via_Neural_Architecture_Transformer\figure_2.jpg
  Figure 2 caption: An example of the graph representation of a residual block and
    the diagram of operation transformations. (a) a residual block [15]; (b) a graph
    view of residual block; (c) transitions among three kinds of operations. N denotes
    a null operation without any computation, S denotes a skip connection, and O denotes
    some computational modules other than null and skip connections.
  Figure 3 Link: articels_figures_by_rev_year\2021\Towards_Accurate_and_Compact_Architectures_via_Neural_Architecture_Transformer\figure_3.jpg
  Figure 3 caption: "The scheme of the proposed NAT. Our NAT takes an arbitrary architecture\
    \ as input and produces the optimized architecture as the output. We use blue\
    \ arrows to represent the process of architecture optimization. Red arrows and\
    \ boxes denote the computation of reward and gradients. R(\u03B1|\u03B2) denotes\
    \ the reward that measures the performance improvement between two architectures\
    \ \u03B1 and \u03B2 ."
  Figure 4 Link: articels_figures_by_rev_year\2021\Towards_Accurate_and_Compact_Architectures_via_Neural_Architecture_Transformer\figure_4.jpg
  Figure 4 caption: Operation transition scheme of NAT++. We set the input channel
    and output channel to 128, the height and width of the input feature maps to 32.
    Here, sep denotes a separable convolution and dil denotes a dilated separable
    convolution.
  Figure 5 Link: articels_figures_by_rev_year\2021\Towards_Accurate_and_Compact_Architectures_via_Neural_Architecture_Transformer\figure_5.jpg
  Figure 5 caption: Architecture optimization results of several hand-crafted architectures.
    We provide both the views of graph (middle) and network (right).
  Figure 6 Link: articels_figures_by_rev_year\2021\Towards_Accurate_and_Compact_Architectures_via_Neural_Architecture_Transformer\figure_6.jpg
  Figure 6 caption: Effect of NATand NAT++on the average performance over 20 randomly
    sampled architectures on CIFAR-10.
  Figure 7 Link: articels_figures_by_rev_year\2021\Towards_Accurate_and_Compact_Architectures_via_Neural_Architecture_Transformer\figure_7.jpg
  Figure 7 caption: Effect of the number of layers in GCN and the value of lambda
    on the performance of NAT and NAT++.
  Figure 8 Link: articels_figures_by_rev_year\2021\Towards_Accurate_and_Compact_Architectures_via_Neural_Architecture_Transformer\figure_8.jpg
  Figure 8 caption: Comparisons of convergence between the original architectures
    and the optimized architectures by NAT and NAT++on CIFAR-10.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.88
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yong Guo
  Name of the last author: Junzhou Huang
  Number of Figures: 8
  Number of Tables: 6
  Number of authors: 8
  Paper title: Towards Accurate and Compact Architectures via Neural Architecture
    Transformer
  Publication Date: 2021-06-07 00:00:00
  Table 1 caption: TABLE 1 Comparisons of the Optimized Architectures Obtained by
    Different Methods Based on Hand-Crafted Architectures
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparisons of the Optimization Results of Hand-Crafted
    Architectures on Face Recognition Datasets
  Table 3 caption: TABLE 3 Comparisons of the Optimized Architectures Obtained by
    Different Methods on NAS Based Architectures
  Table 4 caption: TABLE 4 Average Performance of NAT and NAT++ on 20 Randomly Sampled
    Architectures on CIFAR-10
  Table 5 caption: TABLE 5 Comparisons of the Optimized Architectures Obtained by
    Different Methods on NAS-Bench-201
  Table 6 caption: TABLE 6 Effect of m m and n n on the Performance and Search Cost
    (GPU Hour) of NAT and NAT++
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3086914
- Affiliation of the first author: college of computer science and technologycollege
    of artificial intelligence, nanjing university of aeronautics and astronautics,
    nanjing, china
  Affiliation of the last author: college of computer science and technologycollege
    of artificial intelligence, nanjing university of aeronautics and astronautics,
    nanjing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\A_Concise_Yet_Effective_Model_for_NonAligned_Incomplete_MultiView_and_Missing_Mu\figure_1.jpg
  Figure 1 caption: "The global and local structures of the multiple labels. The same\
    \ sample of non-aligned views is represented by the same color, and \u201Dsky\u201D\
    , \u201Dcloud\u201D, \u2026,\u201Dfish\u201D are the labels. \u201D1\u201D in\
    \ the label matrix means that the sample is annotated with the corresponding label\
    \ whereas \u201C-1\u201D means not. All the label matrices are vertically concatenated.\
    \ The label matrix of all samples from two views has full column rank or high\
    \ rank, which refers to the global structure of multiple labels. Meanwhile, the\
    \ sub-label matrix comprised of samples that share the same individual label tends\
    \ to have low rank, e.g., the rank of sub-label matrix that share the same label\
    \ \u201Ccloud\u201D equals to 2, which corresponds to the local structure of multiple\
    \ labels."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\A_Concise_Yet_Effective_Model_for_NonAligned_Incomplete_MultiView_and_Missing_Mu\figure_2.jpg
  Figure 2 caption: The high-rankness of the entire label matrix corresponding to
    all samples and the low-rankness of the sub-label matrices corresponding to samples
    that share a single label on Corel5k dataset, which has 260 labels.
  Figure 3 Link: articels_figures_by_rev_year\2021\A_Concise_Yet_Effective_Model_for_NonAligned_Incomplete_MultiView_and_Missing_Mu\figure_3.jpg
  Figure 3 caption: The nuclear norm of the predicted entire label matrix, the mean
    value and the median value of the nuclear norm of the predicted sub-label matrices.
  Figure 4 Link: articels_figures_by_rev_year\2021\A_Concise_Yet_Effective_Model_for_NonAligned_Incomplete_MultiView_and_Missing_Mu\figure_4.jpg
  Figure 4 caption: Statistical analysis of NAIM 3 L by the Nemenyi test, the significance
    level is 0.05 and methods not connected are significantly different.
  Figure 5 Link: articels_figures_by_rev_year\2021\A_Concise_Yet_Effective_Model_for_NonAligned_Incomplete_MultiView_and_Missing_Mu\figure_5.jpg
  Figure 5 caption: The performance of four metrics on the Corel5k dataset and the
    Pascal07 dataset under different ratios of incomplete multiple views ( alpha )
    and missing multiple labels ( beta ). The Full means that the multiple views and
    multiple labels in the training set are complete. The average value and standard
    deviation are shown in each sub-figures.
  Figure 6 Link: articels_figures_by_rev_year\2021\A_Concise_Yet_Effective_Model_for_NonAligned_Incomplete_MultiView_and_Missing_Mu\figure_6.jpg
  Figure 6 caption: Results of the hyper-parameter selection on four metrics with
    different lambda . The values of lambda are scaled by log10 for clarity.
  Figure 7 Link: articels_figures_by_rev_year\2021\A_Concise_Yet_Effective_Model_for_NonAligned_Incomplete_MultiView_and_Missing_Mu\figure_7.jpg
  Figure 7 caption: The performance of all four metrics and the convergence rates
    under different mu .
  Figure 8 Link: articels_figures_by_rev_year\2021\A_Concise_Yet_Effective_Model_for_NonAligned_Incomplete_MultiView_and_Missing_Mu\figure_8.jpg
  Figure 8 caption: Convergence curves of the original objective function and the
    surrogate objective function.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xiang Li
  Name of the last author: Songcan Chen
  Number of Figures: 8
  Number of Tables: 7
  Number of authors: 2
  Paper title: A Concise Yet Effective Model for Non-Aligned Incomplete Multi-View
    and Missing Multi-Label Learning
  Publication Date: 2021-06-07 00:00:00
  Table 1 caption: TABLE 1 Statistics of the Datasets
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 Results on All Five Datasets With the Ratio of Incomplete\
    \ Multi-View \u03B1=50 \u03B1=50% and the Ratio of Missing Multi-Label \u03B2\
    =50 \u03B2=50%"
  Table 3 caption: "TABLE 3 Results on All Five Datasets With Full Labels and the\
    \ Ratio of Incomplete Multi-View \u03B1=50 \u03B1=50%"
  Table 4 caption: "TABLE 4 Results on All Five Datasets With Complete Views and the\
    \ Ratio of Missing Multi-Label \u03B2=50 \u03B2=50%"
  Table 5 caption: TABLE 5 Rank of the Predicted Entire Multi-Label Matrix of Each
    Dataset
  Table 6 caption: "TABLE 6 Results of the Variants of NAIM 3 3L With the Ratio of\
    \ Incomplete Multi-View \u03B1=50 \u03B1=50% and the Ratio of Missing Multi-Label\
    \ \u03B2=50 \u03B2=50%"
  Table 7 caption: TABLE 7 The Average Run Time of Different Methods for Computing
    the Sub-Gradient of the Trace Norm (in Seconds)
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3086895
