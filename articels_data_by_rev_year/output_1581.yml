- Affiliation of the first author: school of computer science and center for optical
    imagery analysis and learning (optimal), northwestern polytechnical university,
    xian, shaanxi, p.r. china
  Affiliation of the last author: school of computer science and center for optical
    imagery analysis and learning (optimal), and the school of cybersecurity and school
    of mechanical engineering, northwestern polytechnical university, xian, shaanxi,
    p.r. china
  Figure 1 Link: "articels_figures_by_rev_year\\2019\\Towards_Robust_Discriminative_Projections_Learning_via_NonGreedy\u2113\
    \u2113Norm_MinMax\\figure_1.jpg"
  Figure 1 caption: "(a) Illustration example of sphere surface and simplex surface\
    \ in two-dimensional space. (b) Visualization of row sparsity structure learned\
    \ by using \u2113 2,1 -norm. Each row denotes a data point and dark blue denotes\
    \ the values are close to zero."
  Figure 10 Link: "articels_figures_by_rev_year\\2019\\Towards_Robust_Discriminative_Projections_Learning_via_NonGreedy\u2113\
    \u2113Norm_MinMax\\figure_10.jpg"
  Figure 10 caption: Receiver operating characteristics (ROC) of proposed ell 2,1
    -LDA in comparison to the ROC of the other SOTA methods on NUST-RF dataset including
    indoor (a) and outdoor (b), respectively.
  Figure 2 Link: "articels_figures_by_rev_year\\2019\\Towards_Robust_Discriminative_Projections_Learning_via_NonGreedy\u2113\
    \u2113Norm_MinMax\\figure_2.jpg"
  Figure 2 caption: "Projections learned by proposed \u2113 2,1 -LDA, \u2113 2 -LDA,\
    \ and LDA- \u2113 1 on synthetic dataset with mislabeled samples (a) and outliers\
    \ (b)."
  Figure 3 Link: "articels_figures_by_rev_year\\2019\\Towards_Robust_Discriminative_Projections_Learning_via_NonGreedy\u2113\
    \u2113Norm_MinMax\\figure_3.jpg"
  Figure 3 caption: Visualization of class arithmetic mean sample and optimal weighted
    mean generated by proposed method on AR dataset, (a) occluded by black block,
    and (b) occluded by salt and pepper block.
  Figure 4 Link: "articels_figures_by_rev_year\\2019\\Towards_Robust_Discriminative_Projections_Learning_via_NonGreedy\u2113\
    \u2113Norm_MinMax\\figure_4.jpg"
  Figure 4 caption: "t-SNE 2-D mappings visualization on MNIST digits after dimension\
    \ reduction by using (a) LDA- \u2113 1 , (b) \u2113 1 -LDA, (c) \u2113 1 -DML,\
    \ (d) ILDA- \u2113 1 , (e) RLDA, and (f) \u2113 2,1 -LDA, respectively. The points\
    \ framed by dashed box and solid box represent the outliers and overlapping samples,\
    \ respectively."
  Figure 5 Link: "articels_figures_by_rev_year\\2019\\Towards_Robust_Discriminative_Projections_Learning_via_NonGreedy\u2113\
    \u2113Norm_MinMax\\figure_5.jpg"
  Figure 5 caption: "Misclassified MNIST images, the first row is digital number \u201C\
    4\u201D and and the second row is digital number \u201C9\u201D respectively."
  Figure 6 Link: "articels_figures_by_rev_year\\2019\\Towards_Robust_Discriminative_Projections_Learning_via_NonGreedy\u2113\
    \u2113Norm_MinMax\\figure_6.jpg"
  Figure 6 caption: Some original (first row) and outlier image samples with black
    block occlusion (second row) and salt and pepper occlusion (third row) in PIE
    database.
  Figure 7 Link: "articels_figures_by_rev_year\\2019\\Towards_Robust_Discriminative_Projections_Learning_via_NonGreedy\u2113\
    \u2113Norm_MinMax\\figure_7.jpg"
  Figure 7 caption: Sample images of Pubfig, OSR and NUST-RF datasets (from top to
    bottom) with normal type, mud and baboon occlusions (from left to right) respectively.
  Figure 8 Link: "articels_figures_by_rev_year\\2019\\Towards_Robust_Discriminative_Projections_Learning_via_NonGreedy\u2113\
    \u2113Norm_MinMax\\figure_8.jpg"
  Figure 8 caption: The best average recognition accuracy ( % ) of all competitors
    with different number of training samples and occlusions on PubFig dataset.
  Figure 9 Link: "articels_figures_by_rev_year\\2019\\Towards_Robust_Discriminative_Projections_Learning_via_NonGreedy\u2113\
    \u2113Norm_MinMax\\figure_9.jpg"
  Figure 9 caption: The best average recognition accuracy ( % ) of all competitors
    with different number of training samples and occlusions on OSR dataset.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Feiping Nie
  Name of the last author: Zhen Wang
  Number of Figures: 10
  Number of Tables: 3
  Number of authors: 4
  Paper title: "Towards Robust Discriminative Projections Learning via Non-Greedy\n\
    \u2113\n2,1\n\u21132,1-Norm MinMax"
  Publication Date: 2019-12-24 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Descriptions of Gray Image Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Robust Classification Results on Gray Image Sets (Best Average\
      \ Accuracy \xB1 \xB1 Standard Deviations % %) With Optimal Dimensions on Different\
      \ Number of Outliers Per Class in Training Samples"
  Table 3 caption:
    table_text: TABLE 3 Classification Accuracy of Ratio Trace LDA and Orthogonal
      Trace Ratio LDA With Optimal Weighted Mean
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2961877
- Affiliation of the first author: school of electrical and information engineering,
    university of sydney, camperdown, nsw, australia
  Affiliation of the last author: "computer vision laboratory, eth zurich, z\xFCrich,\
    \ switzerland"
  Figure 1 Link: articels_figures_by_rev_year\2019\SelfPaced_Collaborative_and_Adversarial_Network_for_Unsupervised_Domain_Adaptati\figure_1.jpg
  Figure 1 caption: Motivation of our CAN. We aim to learn domain specific features
    at lower layers and domain invariant features at higher layers, such that the
    learnt feature representation is domain invariant and the discriminability of
    features can also be well preserved.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\SelfPaced_Collaborative_and_Adversarial_Network_for_Unsupervised_Domain_Adaptati\figure_2.jpg
  Figure 2 caption: Motivation of our Self-Paced CAN (SPCAN). In order to enhance
    the discriminability of the learnt model and inspired by Self-paced Learning,
    we re-train the CAN model by iteratively selecting pseudo-labeled target samples
    in an easy-to-hard learning scheme for both imagevideo classifier (a) and domain
    classifier (b). Based on the imagevideo classifier, our SPCAN gradually selects
    from fewer target samples to more target samples with high confidence scores.
    Based on the domain classifier, our SPCAN gradually selects from fewer target
    samples to more target samples that are likely from the target domain.
  Figure 3 Link: articels_figures_by_rev_year\2019\SelfPaced_Collaborative_and_Adversarial_Network_for_Unsupervised_Domain_Adaptati\figure_3.jpg
  Figure 3 caption: The pipeline of our SPCAN method for image classification, which
    can also be similarly used for video classification. This pipeline consists of
    the CAN module (a) and the sample selection module (b). For the CAN module, we
    add multiple domain classifiers at different blocks of the CNN. We then train
    the domain classifiers by learning positive or negative weights on the losses
    to automatically extract domain-specific and domain-invariant features. As a result,
    the features learnt through domain collaborative learning at the lower layers
    will become more domain specific, while the features learnt through domain adversarial
    learning at the higher layers will become more domain invariant. For our sample
    selection module, based on the image classifier, we gradually select the samples
    from fewer target samples to more target samples to construct the set S c ; To
    construct the set S d , based on the domain classifier, we gradually select the
    samples from fewer target samples to more target samples that are likely from
    the target domain. We use both selected sets S c and S d to retrain the CAN model
    in the next training epoch. As a result, the model gradually learns more discriminant
    features for image classification as well as aligning the distributions of two
    domains.
  Figure 4 Link: articels_figures_by_rev_year\2019\SelfPaced_Collaborative_and_Adversarial_Network_for_Unsupervised_Domain_Adaptati\figure_4.jpg
  Figure 4 caption: Overview of the pseudo-labeled target sample selection process
    in our TS-SPCAN. We use the video classifier and last domain classifier of one
    stream (RGBFlow) to select the target samples for another stream (FlowRGB). mathbb
    Scrgb and mathbb Scflow are the selected pseudo-labelled target domain samples
    by using the video classifiers of the RGB and flow stream, respectively. mathbb
    Sdrgb and mathbb Sdflow are the selected target domain samples by using the last
    domain classifiers of the RGB and flow streams, respectively.
  Figure 5 Link: articels_figures_by_rev_year\2019\SelfPaced_Collaborative_and_Adversarial_Network_for_Unsupervised_Domain_Adaptati\figure_5.jpg
  Figure 5 caption: Different lambda l s learnt by using our SPCAN on the Office-31
    dataset (a) A rightarrow W and (b) W rightarrow D.
  Figure 6 Link: articels_figures_by_rev_year\2019\SelfPaced_Collaborative_and_Adversarial_Network_for_Unsupervised_Domain_Adaptati\figure_6.jpg
  Figure 6 caption: Visualization of the source and target domain samples in the 2D
    space for Office-31 (A rightarrow W) by using the t-SNE embedding method [85],
    where the representations are learnt by using the baseline method DANN (a), our
    method CAN (b) and SPCAN (c), respectively. The samples from the source and the
    target domains are shown in blue and red colors, respectively. Best viewed in
    color.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.66
  Name of the first author: Weichen Zhang
  Name of the last author: Wen Li
  Number of Figures: 6
  Number of Tables: 7
  Number of authors: 4
  Paper title: Self-Paced Collaborative and Adversarial Network for Unsupervised Domain
    Adaptation
  Publication Date: 2019-12-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Accuracies (%) of Different Methods on the Office-31 Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Accuracies (%) of Different Methods on the ImageCLEF-DA Dataset
  Table 3 caption:
    table_text: TABLE 3 Average Accuracies (%) of Different Methods on the VISDA-2017
      Dataset
  Table 4 caption:
    table_text: "TABLE 4 Accuracies (%) of the Simplified Version of SPCAN (Referred\
      \ to as sSPCAN) by Using Different Sets of Fixed \u03BB l \u03BBls on the Office-31\
      \ Dataset"
  Table 5 caption:
    table_text: TABLE 5 Accuracy (%) Comparison Between Our SPCAN and the Baseline
      Method DANN When Using Different CNN Architectures as the Backbone Networks
      on the Office-31 Dataset
  Table 6 caption:
    table_text: TABLE 6 Ablation Study of Different Variants of Our SPCAN Method on
      the Office-31 Dataset
  Table 7 caption:
    table_text: TABLE 7 Comparison of Different Methods for Video Action Recognition
      on the UCF101-10 (U) and HMDB51-10 (H) Datasets, in Which BN-Inception [88]
      is Used as the Backbone Network
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2962476
- Affiliation of the first author: college of electronics and information engineering,
    shenzhen university, shenzhen, china
  Affiliation of the last author: college of electronics and information engineering,
    shenzhen university, shenzhen, china
  Figure 1 Link: articels_figures_by_rev_year\2019\Deep_NonNegative_Matrix_Factorization_Architecture_Based_on_Underlying_Basis_Ima\figure_1.jpg
  Figure 1 caption: The factorizations of the classical shallow NMF algorithm, the
    existing deep NMF algorithm and the proposed deep NMF architecture based on the
    underlying basis images learning.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Deep_NonNegative_Matrix_Factorization_Architecture_Based_on_Underlying_Basis_Ima\figure_2.jpg
  Figure 2 caption: The illustration of the proposed deep NMF architecture based on
    underlying basis images learning.
  Figure 3 Link: articels_figures_by_rev_year\2019\Deep_NonNegative_Matrix_Factorization_Architecture_Based_on_Underlying_Basis_Ima\figure_3.jpg
  Figure 3 caption: Part face images from the 6 face databases.
  Figure 4 Link: articels_figures_by_rev_year\2019\Deep_NonNegative_Matrix_Factorization_Architecture_Based_on_Underlying_Basis_Ima\figure_4.jpg
  Figure 4 caption: The ROC curves on FERET database. The DNBMF, RDNBMF and RDNNBMF
    are the proposed algorithms. The AUC scores is shown in parentheses.
  Figure 5 Link: articels_figures_by_rev_year\2019\Deep_NonNegative_Matrix_Factorization_Architecture_Based_on_Underlying_Basis_Ima\figure_5.jpg
  Figure 5 caption: Recognition results (ACC) and ROC curves on ORL and AR databases.
    The DNBMF, RDNBMF, and RDNNBMF are the proposed algorithms. The AUC scores are
    given in parentheses.
  Figure 6 Link: articels_figures_by_rev_year\2019\Deep_NonNegative_Matrix_Factorization_Architecture_Based_on_Underlying_Basis_Ima\figure_6.jpg
  Figure 6 caption: Recognition results (ACC) on CMU, Stirling, and Yale face databases.
    The DNBMF, RDNBMF, and RDNNBMF are the proposed algorithms.
  Figure 7 Link: articels_figures_by_rev_year\2019\Deep_NonNegative_Matrix_Factorization_Architecture_Based_on_Underlying_Basis_Ima\figure_7.jpg
  Figure 7 caption: The recognition results of the proposed algorithms on different
    numbers of basic images.
  Figure 8 Link: articels_figures_by_rev_year\2019\Deep_NonNegative_Matrix_Factorization_Architecture_Based_on_Underlying_Basis_Ima\figure_8.jpg
  Figure 8 caption: The basis images learned by NMF, DSNMF, DNBMF, RDNBMF, and RDNNBMF.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.97
  Name of the first author: Yang Zhao
  Name of the last author: Jihong Pei
  Number of Figures: 8
  Number of Tables: 10
  Number of authors: 3
  Paper title: Deep Non-Negative Matrix Factorization Architecture Based on Underlying
    Basis Images Learning
  Publication Date: 2019-12-27 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Number of Underlying Basis Images of the Algorithms With
      Multi-Layer Structure
  Table 10 caption:
    table_text: TABLE 10 Mean ACC (%) Versus the Number of Underlying Basis Images
      on Stirling Face Dataset (TN=5)
  Table 2 caption:
    table_text: TABLE 2 Mean ACC (%) Versus TN on the FERET Dataset
  Table 3 caption:
    table_text: TABLE 3 Mean ACC (%) Versus TN on the ORL Dataset
  Table 4 caption:
    table_text: TABLE 4 Mean ACC (%) Versus TN on the AR Face Dataset
  Table 5 caption:
    table_text: TABLE 5 Mean ACC (%) Versus TN on the CMU Face Images Dataset
  Table 6 caption:
    table_text: TABLE 6 Mean ACC (%) Versus TN on the Stirling Face Dataset
  Table 7 caption:
    table_text: TABLE 7 Mean ACC (%) Versus TN on the Yale Face Dataset
  Table 8 caption:
    table_text: TABLE 8 Mean ACC (%) Versus the Number of Underlying Basis Images
      on Yale Face Dataset (TN=5)
  Table 9 caption:
    table_text: TABLE 9 Mean ACC (%) Versus the Number of Underlying Basis Images
      on FERET Face Dataset (TN=4)
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2962679
- Affiliation of the first author: adobe, san jose, ca, usa
  Affiliation of the last author: "universit\xE9 laval, quebec city, qc, canada"
  Figure 1 Link: articels_figures_by_rev_year\2019\Single_Day_Outdoor_Photometric_Stereo\figure_1.jpg
  Figure 1 caption: "Examples from our dataset of HDR outdoor illumination conditions.\
    \ In all, our dataset contains 3,800 different illumination conditions, captured\
    \ from 10:30 until 16:30, during 23 days, spread over ten months and at two geographical\
    \ locations. Each image is stored in the 32-bit floating point EXR format, and\
    \ shown tone mapped here for display (with \u03B3=1.6 )."
  Figure 10 Link: articels_figures_by_rev_year\2019\Single_Day_Outdoor_Photometric_Stereo\figure_10.jpg
  Figure 10 caption: Our novel CNN architecture for deep single-day outdoor PS on
    sunny days. The network operates on 16 times 16 patches mathbf Bt of the input
    image, captured at 8 time intervals t regularly spaced throughout a single day.
    The network uses convolutional (blue) and residual (red) layers before estimating
    the normals using fully-connected layers (green). Two losses are used to train
    our method, one based on the cosine distance with the ground truth mathbf hatn
    and another to constrain the norm of the output vector.
  Figure 2 Link: articels_figures_by_rev_year\2019\Single_Day_Outdoor_Photometric_Stereo\figure_2.jpg
  Figure 2 caption: "A normal n defines an integration hemisphere \u03A9 n on the\
    \ environment map. Only light emanating from this hemisphere contributes to the\
    \ shading on that patch. Thus, patches with different normals are lit differently\
    \ even if the environment map is the same."
  Figure 3 Link: articels_figures_by_rev_year\2019\Single_Day_Outdoor_Photometric_Stereo\figure_3.jpg
  Figure 3 caption: "Median confidence interval of normal estimates (red line) as\
    \ a function of mean sun visibility over the course of the day for a signal noise\
    \ \u03C3=0.5% , in bins of 10 \u2218 . Our analysis predicts that normal reconstruction\
    \ errors will likely be high if the sky is completely overcast (low sun visibility),\
    \ or completely clear (high sun visibility). Good results can thus be expected\
    \ in partially cloudy conditions, as shown in Fig. 4. The lower (upper) edge of\
    \ each blue box indicates the 25th (75th) percentile. Statistics are computed\
    \ only on normals pointing upwards to lessen ground effects."
  Figure 4 Link: articels_figures_by_rev_year\2019\Single_Day_Outdoor_Photometric_Stereo\figure_4.jpg
  Figure 4 caption: "Influence of cloud cover on the 95 percent confidence intervals\
    \ (in degrees) with \u03C3=1% . Each pair of plots show the full sphere of normals\
    \ from two different viewpoints: South (left), and North (right). Four different\
    \ types of skies are shown, based on sun visibility. For example, the top-left\
    \ plots show the confidence intervals averaged over all days with direct sun visibility\
    \ in the range 85-100 percent."
  Figure 5 Link: articels_figures_by_rev_year\2019\Single_Day_Outdoor_Photometric_Stereo\figure_5.jpg
  Figure 5 caption: 'Impact of cloud coverage on the numerical conditioning of outdoor
    PS: Clear (a) and overcast (b) days present MLVs with stronger coplanarity; in
    partly cloudy days (c) the sun is often obscured by clouds, which may lead to
    out-of-plane shifts of MLVs.'
  Figure 6 Link: articels_figures_by_rev_year\2019\Single_Day_Outdoor_Photometric_Stereo\figure_6.jpg
  Figure 6 caption: 'Cloud effect on the MLV over one day: while the sun path (orange)
    yields nearly co-planar directions of illumination, the mean light directions
    (red dots) for a normal pointing up provide a much more varied set (data from
    11062013, second row of Fig. 1).'
  Figure 7 Link: articels_figures_by_rev_year\2019\Single_Day_Outdoor_Photometric_Stereo\figure_7.jpg
  Figure 7 caption: Globes representing the coordinate system of sky probes. Each
    normal (blue arrow) defines a shaded hemisphere in the environmental map that
    does not contribute light to the computed MLVs (dots). All MLVs in two particular
    partly cloudy days (columns) were computed from real environment maps [28] for
    3 example normal vectors (rows). Relative MLV intensities are shown in the color
    bar on the left.
  Figure 8 Link: articels_figures_by_rev_year\2019\Single_Day_Outdoor_Photometric_Stereo\figure_8.jpg
  Figure 8 caption: Noise gain for normal directions n of patches visible to the camera,
    which is located South of the hypothetical target object. The colors indicate
    the shifting (coplanarity) of the associated MLVs. On both days, normals that
    are nearly horizontal are associated with nearly coplanar MLVs (smaller shifts,
    higher gains). These normals define a zero-crossing region between positive and
    negative out-of-plane shifts (mid row in Fig. 7), where sun occlusion shifts MLVs
    predominantly along the solar arc.
  Figure 9 Link: articels_figures_by_rev_year\2019\Single_Day_Outdoor_Photometric_Stereo\figure_9.jpg
  Figure 9 caption: (left) Real outdoor HDR images of owl statuette and corresponding
    HDR environment maps (top row) providing synchronized, high-fidelity estimates
    of illumination conditions. All images were acquired on 10112014 and tone-mapped
    for display only (with gamma = 1.6 ). The sun visibility was 43 percent on this
    day. We show the ground truth normals of the object (a) as well as normals recovered
    from [8] (b), along with a reference normal sphere in inset. The reconstruction
    error (c) shows sphere is shown as a color coding reference; (b) normal estimation
    error at each pixel; and (c) the error distribution, in degrees.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Yannick Hold-Geoffroy
  Name of the last author: "Jean-Fran\xE7ois Lalonde"
  Number of Figures: 16
  Number of Tables: 0
  Number of authors: 3
  Paper title: Single Day Outdoor Photometric Stereo
  Publication Date: 2019-12-27 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2962693
- Affiliation of the first author: ultra electronics tcs, montreal, canada
  Affiliation of the last author: ets montreal, montreal, canada
  Figure 1 Link: articels_figures_by_rev_year\2019\Deep_Clustering_On_the_Link_Between_Discriminative_Models_and_KMeans\figure_1.jpg
  Figure 1 caption: Evolution of the MI, I(X,K) , during the iterations of MI-ADM
    and MI-D algorithms for the FRGC data set.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Mohammed Jabi
  Name of the last author: Ismail Ben Ayed
  Number of Figures: 1
  Number of Tables: 3
  Number of authors: 4
  Paper title: 'Deep Clustering: On the Link Between Discriminative Models and K-Means'
  Publication Date: 2019-12-27 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Description of the Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Clustering Algorithms on Four Date Sets Based
      on Accuracy and Normalized Mutual Information
  Table 3 caption:
    table_text: TABLE 3 Comparison of MI-ADM and MI-D on Four Datasets (Accuracy and
      Normalized Mutual Information)
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2962683
- Affiliation of the first author: department of computer science and engineering,
    university of minnesota, minneapolis, mn, usa
  Affiliation of the last author: department of computer science and engineering,
    university of minnesota, minneapolis, mn, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Direction_Concentration_Learning_Enhancing_Congruency_in_Machine_Learning\figure_1.jpg
  Figure 1 caption: "An illustration of congruency in the saliency prediction task.\
    \ Assuming training samples are provided in a sequential manner, an incongruency\
    \ occurs since the food item is related to different saliency values across these\
    \ samples. Here, S j stands for sample j=1,2,3 , w i is the weight at time step\
    \ i , \u0394 w S j is the weight update generated with S j for w i , and the arrows\
    \ indicate updates for the model. Specifically, \u0394 w S j =\u2212\u03B7 g S\
    \ j where \u03B7 is the learning rate and g S j is the gradient w.r.t. S j . The\
    \ update of S 2 (i.e., \u0394 w S 2 ) is congruent with \u0394 w S 1 , whereas\
    \ \u0394 w S 3 is incongruent with \u0394 w S 1 and \u0394 w S 2 ."
  Figure 10 Link: articels_figures_by_rev_year\2019\Direction_Concentration_Learning_Enhancing_Congruency_in_Machine_Learning\figure_10.jpg
  Figure 10 caption: "Congruencies along the epochs in saliency prediction learning,\
    \ as defined in Eq. (4). The samples sequences for training models are determined\
    \ by independent stochastic processes in Fig. 10a and 10b, while the permuted\
    \ samples sequences are pre-determined and fixed for all models in Fig. 10c and\
    \ 10d. The baseline, GEM, and DCL are ResNeXt-29 SGD, ResNeXt-29 SGD GEM, and\
    \ ResNeXt-29 SGD DCL- \u221E -1 (see Table 6), respectively."
  Figure 2 Link: articels_figures_by_rev_year\2019\Direction_Concentration_Learning_Enhancing_Congruency_in_Machine_Learning\figure_2.jpg
  Figure 2 caption: "An illustration of model training with the proposed DCL module.\
    \ Here, 3 samples are observed in a sequential manner. The gradient generated\
    \ by S 3 is expected to be different with the gradients generated by S 1 and S\
    \ 2 . Hence, to tackle the expected violation between the update \u2212\u03B7\
    \ g S 3 and the accumulated update \u2212\u03B7 \u2211 2 i=1 g S i , the proposed\
    \ DCL method finds a corrected update \u2212\u03B7 g ~ S 3 (the pink arrow) by\
    \ solving a quadratic programming problem (5). In this way, the angle between\
    \ \u2212\u03B7 g ~ S 3 and \u2212\u03B7 \u2211 2 i=1 g S i (the blue arrow), i.e.,\
    \ \u03B1 ~ , is guaranteed to be equal to or less than \u03B1 . Note that the\
    \ gradient descent processes with or without the proposed DCL module is identical\
    \ in the test phase."
  Figure 3 Link: articels_figures_by_rev_year\2019\Direction_Concentration_Learning_Enhancing_Congruency_in_Machine_Learning\figure_3.jpg
  Figure 3 caption: "An illustration of DCL constraints with two reference points\
    \ r 0 = w 0 , r 1 = w 1 . g r 0 is the pink arrow while g r 1 is the green one.\
    \ The colored dashed line indicates the border of feasible region with regards\
    \ to \u2212 g r i ,i\u22080,1 , since Constraint (6) forces \u2212\u03B7 g ~ k\
    \ to have an angle which is smaller than or equal to 90 \u2218 w.r.t. g r 0 and\
    \ g r 1 ."
  Figure 4 Link: articels_figures_by_rev_year\2019\Direction_Concentration_Learning_Enhancing_Congruency_in_Machine_Learning\figure_4.jpg
  Figure 4 caption: "An illustration to demonstrate the concept of the effective window.\
    \ Given the spiral convergence path, \u2212\u03B7 g 10| w 0 restricts the search\
    \ direction and the minimum (i.e., the red star) and w 11 are unreachable according\
    \ to the search direction. In contrast, w 11 can be reached along the search direction\
    \ of \u2212\u03B7 g 10| w 7 . To adaptively yield appropriate accumulated gradients\
    \ that converge to the minimum, we define an effective window to periodically\
    \ update the reference."
  Figure 5 Link: articels_figures_by_rev_year\2019\Direction_Concentration_Learning_Enhancing_Congruency_in_Machine_Learning\figure_5.jpg
  Figure 5 caption: "An example demonstrating the effect of the proposed DCL method\
    \ on three optimizers, i.e., gradient descent (GD), RMSProp, and Adam. Given a\
    \ problem z=f(x,y) , we use these optimization algorithms to compute the local\
    \ minima, i.e., ( x \u2217 , y \u2217 ) that yield the minimal z \u2217 . In the\
    \ experiment, except the learning rate, the setting and hyperparameters are the\
    \ same for ALGO and ALGO DCL, where ALGO=GD, RMSProp, Adam. The proposed DCL method\
    \ encourages the convergence paths to be as straight as possible."
  Figure 6 Link: articels_figures_by_rev_year\2019\Direction_Concentration_Learning_Enhancing_Congruency_in_Machine_Learning\figure_6.jpg
  Figure 6 caption: An illustration demonstrating the difference between DCL (left)
    and GEM [33] (right). The search direction in DCL is determined by the accumulated
    gradient while the adjusted gradient (solid line) of GEM is optimized by avoiding
    the violation between the gradient (dashed line) and memory samples gradients
    (green line). Since the weights are iteratively updated and the memory samples
    are preserved, the direction of the adjusted gradient of the memory samples could
    be dynamically varying.
  Figure 7 Link: articels_figures_by_rev_year\2019\Direction_Concentration_Learning_Enhancing_Congruency_in_Machine_Learning\figure_7.jpg
  Figure 7 caption: The congruencies (Cong.) generated by the given references (Ref.)
    and samples with the baseline ResNet-50 RMSP in Table 2. The cosine similarities
    (Sim.) between referred images and sample images are provided for comparison purposes.
    Source images and the corresponding ground-truths, i.e., fixation maps, are displayed
    along with the congruencies. The first and second block are the results of subset
    that contains persons in various scenes. The third block is examples of food subset.
    The rightmost block shows subset with mixed image categories, i.e., contain objects
    of various categories in various scenes.
  Figure 8 Link: articels_figures_by_rev_year\2019\Direction_Concentration_Learning_Enhancing_Congruency_in_Machine_Learning\figure_8.jpg
  Figure 8 caption: The congruencies (Cong.) generated by the given references (Ref.)
    and samples with the baseline ResNet-101 SGD in Table 7. The images with its labels
    are displayed along with the congruencies. The cosine similarities (Sim.) between
    referred images and sample images are provided for comparison purposes. The first
    block is the results of the intra-similar-class subset consisting of images of
    tabby cat and Egyptian cat. The middle block is the results of the inter-class
    subset consisting of images of tabby cat and German shepherd dog. The value in
    bracket indicates number of images. The bottom block is the results of images
    of various labels.
  Figure 9 Link: articels_figures_by_rev_year\2019\Direction_Concentration_Learning_Enhancing_Congruency_in_Machine_Learning\figure_9.jpg
  Figure 9 caption: "Ablation study w.r.t. effective window size \u03B2 w and references\
    \ number N r . (a) and (b) are the experimental results on the SALICON validation\
    \ set, while (c) and (d) are with the Tiny ImageNet validation set. \u03B2 w =\u221E\
    \ in (b) and \u03B2 w =50 in (d)."
  First author gender probability: 0.76
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Yan Luo
  Name of the last author: Qi Zhao
  Number of Figures: 14
  Number of Tables: 12
  Number of authors: 4
  Paper title: 'Direction Concentration Learning: Enhancing Congruency in Machine
    Learning'
  Publication Date: 2019-12-31 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Saliency Prediction Performance of the Models Which are Trained
      on SALICON 2017 Training Set and Evaluated on SALICON 2017 Validation Set
  Table 10 caption:
    table_text: TABLE 10 Computational Cost of Training Models on Tiny ImageNet
  Table 2 caption:
    table_text: TABLE 2 Saliency Prediction Performance of the Models Which are Trained
      on OSIE and Tested on MIT1003
  Table 3 caption:
    table_text: TABLE 3 Performances on MNIST-R in Continual Learning Setting Using
      SGD [42] as the Optimizer
  Table 4 caption:
    table_text: TABLE 4 Performances on MNIST-P in Continual Learning Setting Using
      SGD as the Optimizer
  Table 5 caption:
    table_text: TABLE 5 Performances on iCIFAR-100 in Continual Learning Setting Using
      SGD as the Optimizer
  Table 6 caption:
    table_text: TABLE 6 Top 1 Error Rate (in % %) on CIFAR With Various Models
  Table 7 caption:
    table_text: TABLE 7 Top 1 and Top 5 Error Rate (in % %) on the Validation Set
      of Tiny ImageNet With Various Models
  Table 8 caption:
    table_text: TABLE 8 Top 1 and Top 5 1-Crop Validation Error (in % %) on ImageNet
      With SGD Optimizer
  Table 9 caption:
    table_text: TABLE 9 Top 1 and Top 5 Error Rate (in %) on the Validation Set of
      Tiny ImageNet
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2963387
- Affiliation of the first author: department of informatics and department of neuroinformatics,
    robotics and perception group, university of zurich, zurich, switzerland
  Affiliation of the last author: department of informatics and department of neuroinformatics,
    robotics and perception group, university of zurich, zurich, switzerland
  Figure 1 Link: articels_figures_by_rev_year\2019\High_Speed_and_High_Dynamic_Range_Video_with_an_Event_Camera\figure_1.jpg
  Figure 1 caption: Our network converts a spatio-temporal stream of events with microsecond
    temporal resolution (top left) into a high-quality video (top right). This enables
    synthesis of videos of high-speed phenomena such as a bullet piercing a mug (a),
    or scenes with high dynamic range (b). The reconstructions can also be used as
    input to off-the-shelf computer vision algorithms, thereby serving as an intermediate
    representation between event data and mainstream computer vision (c). The images
    in the figure were produced by the presented technique.
  Figure 10 Link: articels_figures_by_rev_year\2019\High_Speed_and_High_Dynamic_Range_Video_with_an_Event_Camera\figure_10.jpg
  Figure 10 caption: Our color reconstruction approach. Color channels are reconstructed
    independently at quarter resolution (a), then upsampled and recombined into a
    low-quality color image (b,c). The latter is combined with a high-quality grayscale
    image (d) reconstructed using all the events (ignoring the CFA). The resulting
    color image (e,f) preserves fine details that are lost in the quarter resolution
    reconstruction [45] [compare (c) and (f)].
  Figure 2 Link: articels_figures_by_rev_year\2019\High_Speed_and_High_Dynamic_Range_Video_with_an_Event_Camera\figure_2.jpg
  Figure 2 caption: 'Comparison of the output of a conventional camera and an event
    camera looking at a black disk on a rotating circle. While a conventional camera
    captures frames at a fixed rate, an event camera transmits the brightness changes
    continuously in the form of a spiral of events in space-time (red: positive events,
    blue: negative events). Figure inspired by [10].'
  Figure 3 Link: articels_figures_by_rev_year\2019\High_Speed_and_High_Dynamic_Range_Video_with_an_Event_Camera\figure_3.jpg
  Figure 3 caption: "Overview of our approach. The event stream (depicted as redblue\
    \ dots on the time axis) is split into windows \u03B5 k containing multiple events.\
    \ Each window is converted into a 3D event tensor E k and passed through the network,\
    \ together with the previous state s k\u22121 to generate a new image reconstruction\
    \ I k and updated state s k . In this example, each window \u03B5 k contains a\
    \ fixed number of events N=7 ."
  Figure 4 Link: articels_figures_by_rev_year\2019\High_Speed_and_High_Dynamic_Range_Video_with_an_Event_Camera\figure_4.jpg
  Figure 4 caption: We use a fully convolutional, UNet-like [26] architecture (a),
    composed of N E recurrent encoder layers (b), followed by N R residual blocks
    and N E decoder layers, with skip connections between symmetric layers. Encoders
    are composed of a strided convolution (stride 2) followed by a ConvLSTM [27].
    Decoder blocks perform bilinear upsampling followed by a convolution. ReLU activations
    and batch normalization [28] are used after each layer (except the last prediction
    layer, for which a sigmoid activation is used). In this diagram, N E =2 and N
    R =1 .
  Figure 5 Link: articels_figures_by_rev_year\2019\High_Speed_and_High_Dynamic_Range_Video_with_an_Event_Camera\figure_5.jpg
  Figure 5 caption: "Comparison of our method with MR and HF on sequences from [38].\
    \ Our network is able to reconstruct fine details well (textures in the first\
    \ row), while avoiding common artifacts (e.g., the \u201Cbleeding edges\u201D\
    \ in the third row)."
  Figure 6 Link: articels_figures_by_rev_year\2019\High_Speed_and_High_Dynamic_Range_Video_with_an_Event_Camera\figure_6.jpg
  Figure 6 caption: Qualitative comparison on the dataset introduced by [2]. Our method
    produces cleaner and more detailed results.
  Figure 7 Link: articels_figures_by_rev_year\2019\High_Speed_and_High_Dynamic_Range_Video_with_an_Event_Camera\figure_7.jpg
  Figure 7 caption: 'Video reconstructions of high speed physical phenomena, synthesized
    at >5,000 FPS with our approach. First two rows: shooting a garden gnome and a
    mug with a rifle. Last two rows: popping a water balloon and an air balloon with
    a needle. Our reconstructions reveal details invisible to the naked eye or a conventional
    consumer camera. In the first two rows, the trace of the bullet is clearly visible
    (the bullet itself was too fast for the event sensor to catch), and cracks in
    both objects are visible before the pieces fly apart. In the last two rows, the
    membrane of the balloons contracting away from the point where the needle hit
    is clearly visible.'
  Figure 8 Link: articels_figures_by_rev_year\2019\High_Speed_and_High_Dynamic_Range_Video_with_an_Event_Camera\figure_8.jpg
  Figure 8 caption: Reconstruction framerate for the high-speed sequences. The output
    framerate grows with the event rate and value of D , and varies between 1 kHz
    and 15 kHz.
  Figure 9 Link: articels_figures_by_rev_year\2019\High_Speed_and_High_Dynamic_Range_Video_with_an_Event_Camera\figure_9.jpg
  Figure 9 caption: "Video reconstruction under challenging lighting. First row: Hand-held,\
    \ indoor \u201Cselfie\u201D sequence. Second row: Driving sequence recorded while\
    \ driving out of a tunnel. Third row: Outdoor sequence recorded with the sensors\
    \ pointing directly at the sun on a bright day. The frames from the consumer camera\
    \ (Huawei P20 Pro) (b) suffer from under- or over-exposure, while the events (a)\
    \ capture the whole dynamic range of the scene, which our method successfully\
    \ recovers (c)."
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Henri Rebecq
  Name of the last author: Davide Scaramuzza
  Number of Figures: 16
  Number of Tables: 9
  Number of authors: 4
  Paper title: High Speed and High Dynamic Range Video with an Event Camera
  Publication Date: 2019-12-31 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison to State-of-the-Art Image Reconstruction Methods
      on the Event Camera Dataset [38]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of the Temporal Error (Eq. (2), Lower is Better)
      Between HF, MR, and Our Method
  Table 3 caption:
    table_text: TABLE 3 Classification Accuracy Compared to Recent Approaches, Including
      HATS [14], the State-of-the-Art
  Table 4 caption:
    table_text: TABLE 4 Mean Translation Error (in Meters) on the Sequences from [38]
  Table 5 caption:
    table_text: TABLE 5 Mean Inference Time Per Event Tensor (Measured on an NVIDIA
      RTX 2080 Ti GPU), and Maximum Frame Rate Achievable in Real Time, When Using
      Windows of Events of Fixed Duration
  Table 6 caption:
    table_text: "TABLE 6 Time it Takes to Process N=10,000 N=10,000 Events for HF,\
      \ MR, and Our Method (One 240\xD7180 240\xD7180 Event Tensor \u2217 )"
  Table 7 caption:
    table_text: 'TABLE 7 Ablation Study: Effect of the Temporal Loss'
  Table 8 caption:
    table_text: 'TABLE 8 Ablation Study: Effect of the Recurrent Connection'
  Table 9 caption:
    table_text: "TABLE 9 Reconstruction Quality With \u03C4=50ms \u03C4=50 ms versus\
      \ \u03C4=5ms \u03C4=5 ms Windows"
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2963386
- Affiliation of the first author: beijing institute of technology, beijing, china
  Affiliation of the last author: beijing institute of technology, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2020\LayoutGAN_Synthesizing_Graphic_Layouts_With_VectorWireframe_Adversarial_Networks\figure_1.jpg
  Figure 1 caption: Overall architecture of LayoutGAN. The generator takes as input
    graphic elements with randomly sampled class probabilities and geometric parameters
    from Uniform and Gaussian distribution respectively. An encoder embeds the input
    and feeds them into the stacked relation module, which refines the embedded features
    of each element in a coordinative manner by considering its semantic and spatial
    relations with all the other elements. Finally, a decoder decodes the refined
    features back to class probabilities and geometric parameters. The wireframe rendering
    discriminator feeds the generated results to a differentiable wireframe rendering
    layer which raterizes the input graphic elements into 2D wireframe images, upon
    which a CNN is applied for layout optimization.
  Figure 10 Link: articels_figures_by_rev_year\2020\LayoutGAN_Synthesizing_Graphic_Layouts_With_VectorWireframe_Adversarial_Networks\figure_10.jpg
  Figure 10 caption: Optimizing perturbed tangram graphic design.
  Figure 2 Link: articels_figures_by_rev_year\2020\LayoutGAN_Synthesizing_Graphic_Layouts_With_VectorWireframe_Adversarial_Networks\figure_2.jpg
  Figure 2 caption: Wireframe rendering of different polygons (point, rectangle and
    triangle). The black grids represent grids of target image. The orange dotsdotted
    lines represent the graphic element mapped onto the image grid. The blue solid
    lines represent the rasterized wireframes expressed as differentiable functions
    of graphic elements in terms of both class probilities and geometric parameters.
  Figure 3 Link: articels_figures_by_rev_year\2020\LayoutGAN_Synthesizing_Graphic_Layouts_With_VectorWireframe_Adversarial_Networks\figure_3.jpg
  Figure 3 caption: Comparisons of MNIST digits from LayoutGAN with different discriminators
    and the real data.
  Figure 4 Link: articels_figures_by_rev_year\2020\LayoutGAN_Synthesizing_Graphic_Layouts_With_VectorWireframe_Adversarial_Networks\figure_4.jpg
  Figure 4 caption: Point tracing of location refinement process (best viewed in color).
  Figure 5 Link: articels_figures_by_rev_year\2020\LayoutGAN_Synthesizing_Graphic_Layouts_With_VectorWireframe_Adversarial_Networks\figure_5.jpg
  Figure 5 caption: Visualization of document layout samples and their corresponding
    real document pages.
  Figure 6 Link: articels_figures_by_rev_year\2020\LayoutGAN_Synthesizing_Graphic_Layouts_With_VectorWireframe_Adversarial_Networks\figure_6.jpg
  Figure 6 caption: Comparisons of document layouts from DCGAN, LayoutGAN with different
    discriminators and the real data.
  Figure 7 Link: articels_figures_by_rev_year\2020\LayoutGAN_Synthesizing_Graphic_Layouts_With_VectorWireframe_Adversarial_Networks\figure_7.jpg
  Figure 7 caption: Comparisons of discriminator loss landscapes.
  Figure 8 Link: articels_figures_by_rev_year\2020\LayoutGAN_Synthesizing_Graphic_Layouts_With_VectorWireframe_Adversarial_Networks\figure_8.jpg
  Figure 8 caption: Comparisons of Clipart abstract scenes from DCGAN, LayoutGAN with
    different discriminators, and the real data.
  Figure 9 Link: articels_figures_by_rev_year\2020\LayoutGAN_Synthesizing_Graphic_Layouts_With_VectorWireframe_Adversarial_Networks\figure_9.jpg
  Figure 9 caption: Optimizing perturbed document layouts.
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Jianan Li
  Name of the last author: Tingfa Xu
  Number of Figures: 16
  Number of Tables: 6
  Number of authors: 5
  Paper title: 'LayoutGAN: Synthesizing Graphic Layouts With Vector-Wireframe Adversarial
    Networks'
  Publication Date: 2020-01-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Comparisons of Generated Digits
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Spatial Analysis of Generated Document Layouts
  Table 3 caption:
    table_text: TABLE 3 User Study Results for Clipart Abstract Scenes
  Table 4 caption:
    table_text: TABLE 4 Average Precision of Detection Results
  Table 5 caption:
    table_text: TABLE 5 Average Precision of Optimized Layouts (IoU=0.9)
  Table 6 caption:
    table_text: TABLE 6 Spatial Analysis of Optimized Webpage Layouts
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2963663
- Affiliation of the first author: department of electronic engineering and information
    science, university of science and technology of china, hefei, anhui, china
  Affiliation of the last author: wormpex ai research llc, bellevue, wa, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Explicit_Filterbank_Learning_for_Neural_Image_Style_Transfer_and_Image_Processin\figure_1.jpg
  Figure 1 caption: 'Our network architecture consists of three modules: image encoder
    E , StyleBank layer K and image decoder D .'
  Figure 10 Link: articels_figures_by_rev_year\2020\Explicit_Filterbank_Learning_for_Neural_Image_Style_Transfer_and_Image_Processin\figure_10.jpg
  Figure 10 caption: Comparison between incremental training (Left) and fresh training
    (Right). The target styles are shown on the top-left.
  Figure 2 Link: articels_figures_by_rev_year\2020\Explicit_Filterbank_Learning_for_Neural_Image_Style_Transfer_and_Image_Processin\figure_2.jpg
  Figure 2 caption: The overall network structure for the hierarchy stylebank design.
  Figure 3 Link: articels_figures_by_rev_year\2020\Explicit_Filterbank_Learning_for_Neural_Image_Style_Transfer_and_Image_Processin\figure_3.jpg
  Figure 3 caption: Reconstruction of the style elements learnt from two kinds of
    representative patches in an exemplar stylization image.
  Figure 4 Link: articels_figures_by_rev_year\2020\Explicit_Filterbank_Learning_for_Neural_Image_Style_Transfer_and_Image_Processin\figure_4.jpg
  Figure 4 caption: Learnt style elements of different StyleBank kernel sizes. (b)
    and (c) are stylization results of (3,3) and (7,7) kernels respectively. (d),
    (e) and (f) respectively show learnt style elements, original style patches and
    stylization patches.
  Figure 5 Link: articels_figures_by_rev_year\2020\Explicit_Filterbank_Learning_for_Neural_Image_Style_Transfer_and_Image_Processin\figure_5.jpg
  Figure 5 caption: k-means clustering result of feature maps(left) and corresponding
    stylization result(right).
  Figure 6 Link: articels_figures_by_rev_year\2020\Explicit_Filterbank_Learning_for_Neural_Image_Style_Transfer_and_Image_Processin\figure_6.jpg
  Figure 6 caption: 'Sparsity analysis. Top-left: means and standard deviations of
    per-channel average response; top-right: distributions of sorted means of per-channel
    average response for different model sizes ( Cmax = 32,64,128 ); bottom: corresponding
    stylization results.'
  Figure 7 Link: articels_figures_by_rev_year\2020\Explicit_Filterbank_Learning_for_Neural_Image_Style_Transfer_and_Image_Processin\figure_7.jpg
  Figure 7 caption: Illustration of the effects of two branches. The middle and right
    ones are reconstructed input image (left) with and without auto-encoder branch
    during training.
  Figure 8 Link: articels_figures_by_rev_year\2020\Explicit_Filterbank_Learning_for_Neural_Image_Style_Transfer_and_Image_Processin\figure_8.jpg
  Figure 8 caption: Stylization result of a toy image, which consists of four parts
    of different color or different texture.
  Figure 9 Link: articels_figures_by_rev_year\2020\Explicit_Filterbank_Learning_for_Neural_Image_Style_Transfer_and_Image_Processin\figure_9.jpg
  Figure 9 caption: To visualize the learned style elements, we feed different noise
    images into our network and get their corresponding stylization results. It shows
    that they contain different types of style elements, and the recovered style elements
    of different noise images are overall similiar.
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.95
  Name of the first author: Dongdong Chen
  Name of the last author: Gang Hua
  Number of Figures: 19
  Number of Tables: 4
  Number of authors: 5
  Paper title: Explicit Filterbank Learning for Neural Image Style Transfer and Image
    Processing
  Publication Date: 2020-01-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Default Network Configuration Details
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 User Study Results of Different Stylization Methods
  Table 3 caption:
    table_text: TABLE 3 Quantitative Comparison Results of Using the Proposed Filter
      Bank Learning Idea for Edge-Aware Smoothing Task
  Table 4 caption:
    table_text: TABLE 4 Quantitative Comparison Results of Using the Proposed Filter
      Bank Learning Idea for the Gray Image Denoising Task on the BSD68 Dataset
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2964205
- Affiliation of the first author: centre for imaging sciences, the university of
    manchester, united kingdom
  Affiliation of the last author: texture lab, school of mathematical and computer
    sciences, heriot-watt university, edinburgh, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2020\Perceptual_Texture_Similarity_Estimation_An_Evaluation_of_Computational_Features\figure_1.jpg
  Figure 1 caption: Dendrogram (cut at 0.337) obtained from the 8D-ISO similarity
    matrix [29], along with two representative textures [3] of each cluster.
  Figure 10 Link: articels_figures_by_rev_year\2020\Perceptual_Texture_Similarity_Estimation_An_Evaluation_of_Computational_Features\figure_10.jpg
  Figure 10 caption: The top four worst pair-of-pairs results in which the majority
    of human observers considered that the left pairs are more similar while at least
    30 of the 51 conventional feature sets did not. Note that only the central quarters
    of textures are shown. (Also note that the CNN results are discussed in Section
    8).
  Figure 2 Link: articels_figures_by_rev_year\2020\Perceptual_Texture_Similarity_Estimation_An_Evaluation_of_Computational_Features\figure_2.jpg
  Figure 2 caption: Plots of the two 334 x 334 perceptual similarity matrices available
    for Pertex [3]. (a) is as obtained directly from Halleys free-grouping experiment
    [54], while (b) shows the dimensionality reduced 8D-ISO data later derived by
    Clarke et al. [29]. The brightness of a point denotes the similarity s a,b of
    two textures and they are ordered by the results of agglomerative clustering in
    order to make clusters more obvious.
  Figure 3 Link: articels_figures_by_rev_year\2020\Perceptual_Texture_Similarity_Estimation_An_Evaluation_of_Computational_Features\figure_3.jpg
  Figure 3 caption: 'A pair of pairs set of textures: observers are required to indicate
    which pair a,b or c,d look more similar.'
  Figure 4 Link: articels_figures_by_rev_year\2020\Perceptual_Texture_Similarity_Estimation_An_Evaluation_of_Computational_Features\figure_4.jpg
  Figure 4 caption: The pipeline of the proposed evaluation protocols.
  Figure 5 Link: articels_figures_by_rev_year\2020\Perceptual_Texture_Similarity_Estimation_An_Evaluation_of_Computational_Features\figure_5.jpg
  Figure 5 caption: Means and 97.5 percent confidence bounds of the G and M scores
    calculated over the 51 feature sets, at each resolution and for each retrieval
    size, N in 10, 20, 40, 60 . These figures show the superiority of the multi-resolution
    approach.
  Figure 6 Link: articels_figures_by_rev_year\2020\Perceptual_Texture_Similarity_Estimation_An_Evaluation_of_Computational_Features\figure_6.jpg
  Figure 6 caption: Multi-series bar chart of the G and M scores obtained using Protocol
    I with 51 feature sets and N in 10, 20, 40, 60 . Note that only the multi-resolution
    approach ( r = Multi ) was used here. These results show that even at the best
    resolution, none of the feature sets perform as well as might be expected.
  Figure 7 Link: articels_figures_by_rev_year\2020\Perceptual_Texture_Similarity_Estimation_An_Evaluation_of_Computational_Features\figure_7.jpg
  Figure 7 caption: "Agreement rates (%) for the two different types of ground-truth\
    \ obtained using 51 feature sets and at six different \u201Cresolutions\u201D\
    . The black bold dashed lines show average agreement rates: (a) 57.7 percent and\
    \ (b) 53.6 percent (computed across 51 feature sets and six resolutions). For\
    \ reference, the top red bold dash-dot line in each graph shows the agreement\
    \ between the two ground-truth data sets (73.9 percent)."
  Figure 8 Link: articels_figures_by_rev_year\2020\Perceptual_Texture_Similarity_Estimation_An_Evaluation_of_Computational_Features\figure_8.jpg
  Figure 8 caption: Average agreement rates (%) for 51 feature sets (sorted in an
    ascending order) and their 95 percent confidence bounds, for six resolutions.
    The black bold dashed lines show the average agreement rates 57.7 and 53.6 percent
    (calculated over the 51 feature sets and six resolutions. For reference, the top
    red bold dash-dot line in each graph shows the agreement between the two ground-truth
    data sets (73.9 percent).
  Figure 9 Link: articels_figures_by_rev_year\2020\Perceptual_Texture_Similarity_Estimation_An_Evaluation_of_Computational_Features\figure_9.jpg
  Figure 9 caption: 'The effect of resolution: average agreement rates and 95 percent
    confidence bounds are shown for each of six resolutions computed over the 51 feature
    sets. The black bold dashed lines indicate overall averages: 57.7 percent (a)
    and 53.6 percent (b).'
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xinghui Dong
  Name of the last author: Mike J. Chantler
  Number of Figures: 12
  Number of Tables: 11
  Number of authors: 3
  Paper title: 'Perceptual Texture Similarity Estimation: An Evaluation of Computational
    Features'
  Publication Date: 2020-01-07 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Summary of 51 Computational Texture Feature Sets: (a) Histogram-Based
      and (b) Non-Histogram-Based'
  Table 10 caption:
    table_text: TABLE 10 The Receptive Field Size and Number of Filters of Six Convolutional
      Layers of VGG-VD-19 [108], and the Agreement Rates (%) Obtained Using the Local
      Mean and Global Mean Features That are Extracted From These Layers When Compared
      Against POP J POP POPJPOP
  Table 2 caption:
    table_text: TABLE 2 Summary of 12 Published Texture Databases
  Table 3 caption:
    table_text: TABLE 3 Best Feature Sets for Texture Retrieval
  Table 4 caption:
    table_text: "TABLE 4 Numbers of Feature Sets Whose G or M Scores Were Enhanced\
      \ Using the Resolutions r\u2208 r\u2208 64\xD764, 128\xD7128, 256\xD7256, 512\xD7\
      512, Multi Compared With the Scores Obtained Using the Original Resolution of\
      \ 1024\xD71024"
  Table 5 caption:
    table_text: "TABLE 5 Numbers of Feature Sets Whose Agreement Rate Was Enhanced\
      \ Using the Resolutions r\u2208 r\u2208 64\xD764, 128\xD7128, 256\xD7256, 512\xD7\
      512, Multi Compared With That Obtained Using the Original Resolution of 1024\xD7\
      1024, Together With Two Sets of Ground-Truth"
  Table 6 caption:
    table_text: "TABLE 6 Spearmans Correlation Coefficients ( \u03C1 \u03C1, \u2202\
      \ \u2202 = 0.05) and P Values: (Columns 2-7) Between the Two Sets of Curves\
      \ in Fig. 7a and 7b; and (column 8) Between the Two Curves in Fig. 8a and 8b"
  Table 7 caption:
    table_text: TABLE 7 Agreement Rates (%) Obtained Using the SSIM [126] and MS-SSIM
      [127] Quality Measures
  Table 8 caption:
    table_text: 'TABLE 8 Average Agreement Rates (%) Obtained Using Random Forests
      With 20, 50, 100, or 200 Trees Compared With the Human-Derived Ground-Truth:
      POP J POP POPJPOP'
  Table 9 caption:
    table_text: TABLE 9 Results of Three Kolmogorov-Smirnov (K-S) Tests
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2964533
