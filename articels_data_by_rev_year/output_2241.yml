- Affiliation of the first author: department of applied mathematics, beijing jiaotong
    university, beijing, china
  Affiliation of the last author: department of applied mathematics, beijing jiaotong
    university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Support_Vector_Machine_Classifier_via_L__L_SoftMargin_Loss\figure_1.jpg
  Figure 1 caption: "(a) A two dimensional training set with 200 samples. (b) Data\
    \ in (a) but with r =10% outliers. Blue stars: sampling samples in class \u2212\
    1 . Red crosses: sampling samples in class +1 . Red dashed lines: the Bayes classifier."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Support_Vector_Machine_Classifier_via_L__L_SoftMargin_Loss\figure_2.jpg
  Figure 2 caption: ACC versus r of all solvers for solving six datasets.
  Figure 3 Link: articels_figures_by_rev_year\2021\Support_Vector_Machine_Classifier_via_L__L_SoftMargin_Loss\figure_3.jpg
  Figure 3 caption: NSV versus r of all solvers for solving six datasets.
  Figure 4 Link: articels_figures_by_rev_year\2021\Support_Vector_Machine_Classifier_via_L__L_SoftMargin_Loss\figure_4.jpg
  Figure 4 caption: SWSITER versus r of all solvers for solving six datasets.
  Figure 5 Link: articels_figures_by_rev_year\2021\Support_Vector_Machine_Classifier_via_L__L_SoftMargin_Loss\figure_5.jpg
  Figure 5 caption: TNI versus r of all solvers for solving six datasets.
  Figure 6 Link: articels_figures_by_rev_year\2021\Support_Vector_Machine_Classifier_via_L__L_SoftMargin_Loss\figure_6.jpg
  Figure 6 caption: CPU versus r of all solvers for solving six datasets.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Huajun Wang
  Name of the last author: Naihua Xiu
  Number of Figures: 6
  Number of Tables: 4
  Number of authors: 5
  Paper title: Support Vector Machine Classifier via L 01 L01 Soft-Margin Loss
  Publication Date: 2021-06-24 00:00:00
  Table 1 caption: TABLE 1 Comparisons of 10 Solvers for Solving Example 5.1, Where
    L 01 L01 Stands for L 01 01ADMM
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparisons of 10 Solvers for Solving Example 5.2
  Table 3 caption: TABLE 3 Descriptions of 14 Real Datasets
  Table 4 caption: TABLE 4 Comparisons of 10 Solvers for Solving Example 5.3
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3092177
- Affiliation of the first author: department of computer science, university of york,
    york, u.k.
  Affiliation of the last author: department of computer science, university of york,
    york, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2021\Lifelong_TeacherStudent_Network_Learning\figure_1.jpg
  Figure 1 caption: The scheme of the Teacher-Student network for lifelong learning.
  Figure 10 Link: articels_figures_by_rev_year\2021\Lifelong_TeacherStudent_Network_Learning\figure_10.jpg
  Figure 10 caption: Generation and reconstruction results following the unsupervised
    lifelong learning by the proposed approach on Celeba and CACD databases.
  Figure 2 Link: articels_figures_by_rev_year\2021\Lifelong_TeacherStudent_Network_Learning\figure_2.jpg
  Figure 2 caption: Classification accuracy curves during the lifelong Teacher-Student
    learning from MNIST to SVHN databases.
  Figure 3 Link: articels_figures_by_rev_year\2021\Lifelong_TeacherStudent_Network_Learning\figure_3.jpg
  Figure 3 caption: Image generation and reconstruction results for the LTS model
    when learning MNIST and then SVHN databases.
  Figure 4 Link: articels_figures_by_rev_year\2021\Lifelong_TeacherStudent_Network_Learning\figure_4.jpg
  Figure 4 caption: Classification accuracy curves during the lifelong Teacher-Student
    learning from MNIST to MNIST-Fashion databases.
  Figure 5 Link: articels_figures_by_rev_year\2021\Lifelong_TeacherStudent_Network_Learning\figure_5.jpg
  Figure 5 caption: The generation and reconstruction results for LTS considering
    the lifelong learning from MNIST to MNIST-Fashion.
  Figure 6 Link: articels_figures_by_rev_year\2021\Lifelong_TeacherStudent_Network_Learning\figure_6.jpg
  Figure 6 caption: Semi-supervised classification results from MNIST to MNIST-Fashion.
    We use 1,000 images from MNIST database and another 10,000 from MNIST-Fashion
    as a labeled data set.
  Figure 7 Link: articels_figures_by_rev_year\2021\Lifelong_TeacherStudent_Network_Learning\figure_7.jpg
  Figure 7 caption: Generation and reconstruction results for LTS when considering
    unsupervised training with MNIST, CIFAR10, Sub-ImageNet and CelebA databases.
  Figure 8 Link: articels_figures_by_rev_year\2021\Lifelong_TeacherStudent_Network_Learning\figure_8.jpg
  Figure 8 caption: Generated images of the digit 7 after the LTS lifelong learning
    of MNIST and MNIST-Fashion database, when changing a single latent variable from
    -2 to 2.
  Figure 9 Link: articels_figures_by_rev_year\2021\Lifelong_TeacherStudent_Network_Learning\figure_9.jpg
  Figure 9 caption: Generation results in fashion item images after the LTS lifelong
    learning of MNIST and MNIST-Fashion database, when changing a single latent variable
    from -1 to 3.
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Fei Ye
  Name of the last author: Adrian G. Bors
  Number of Figures: 18
  Number of Tables: 7
  Number of authors: 2
  Paper title: Lifelong Teacher-Student Network Learning
  Publication Date: 2021-06-25 00:00:00
  Table 1 caption: TABLE 1 Classification Accuracy When Learning MNIST and SVHN Under
    the Lifelong Learning Setting
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Classification Accuracy on the MNIST and MNIST-Fashion
    Under the Lifelong Learning Setting
  Table 3 caption: TABLE 3 Semi-Supervised Classification Results on MNIST Data, When
    Considering MNIST to MNIST-Fashion Lifelong Learning
  Table 4 caption: TABLE 4 Average NLL on All Testing Samples After the Lifelong Learning
    of MNIST, CIFAR10, Sub-ImageNet, and CelebA
  Table 5 caption: TABLE 5 IS Score on 5,000 Testing Data After the Lifelong Learning
    of MNIST, CIFAR10, Sub-ImageNet, and CelebA
  Table 6 caption: TABLE 6 Average Classification Accuracy on All Testing Data After
    the Lifelong Learning of MNIST, SVHN, and CIFAR10
  Table 7 caption: TABLE 7 The Average Negative Log-Likelihood (NLL) on All Testing
    Data Samples After the Lifelong Learning of MNIST, CIFAR10, Sub-ImageNet, and
    CelebA
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3092677
- Affiliation of the first author: tklndst, college of computer science, nankai university,
    tianjin, china
  Affiliation of the last author: institute of information science, beijing jiaotong
    university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Online_Attention_Accumulation_for_Weakly_Supervised_Semantic_Segmentation\figure_1.jpg
  Figure 1 caption: Observation of our proposed approach. (a) Source images; (b-d)
    Intermediate attention maps produced by a classification network at different
    training stages; (e) Cumulative attention maps produced by combining attention
    maps in (b), (c), and (d) through a simple element-wise maximum operation. It
    can be easily observed that the discriminative regions continuously shift over
    different parts of the semantic objects. The fused attention maps in (e) can record
    most of the semantic regions compared with (b), (c), and (d). Best viewed in color.
  Figure 10 Link: articels_figures_by_rev_year\2021\Online_Attention_Accumulation_for_Weakly_Supervised_Semantic_Segmentation\figure_10.jpg
  Figure 10 caption: The evolution of attention maps during training. (a) Images;
    (b-d) Intermediate attention maps; (e) Cumulative attention maps. The attention
    maps at the first four rows are generated from the normal training process. The
    attention maps at the last four rows are generated from the normal training process
    and training process with the attention drop layer, respectively.
  Figure 2 Link: articels_figures_by_rev_year\2021\Online_Attention_Accumulation_for_Weakly_Supervised_Semantic_Segmentation\figure_2.jpg
  Figure 2 caption: The motivation of adding an attention drop layer. (a) Source images;
    (b-d) Intermediate attention maps at different training stages; (e) Cumulative
    attention maps produced by combining attention maps in (b-d) through a simple
    element-wise maximum operation. We can see that the intermediate attention maps
    focus on the same object regions, causing the cumulative attention maps hard to
    find new object regions.
  Figure 3 Link: articels_figures_by_rev_year\2021\Online_Attention_Accumulation_for_Weakly_Supervised_Semantic_Segmentation\figure_3.jpg
  Figure 3 caption: Illustration of our online attention accumulation (OAA) process.
    The attention maps are generated online from the class-aware convolutional layer.
    Our OAA utilizes these discriminative regions of attention maps at the different
    training phases and integrates them into the cumulative attention maps with a
    simple attention fusion strategy progressively.
  Figure 4 Link: articels_figures_by_rev_year\2021\Online_Attention_Accumulation_for_Weakly_Supervised_Semantic_Segmentation\figure_4.jpg
  Figure 4 caption: "Illustration of integrating an attention drop layer into our\
    \ online attention accumulation (OAA) process. The red circle denotes the regions\
    \ whose attention values are large than \u03B4 ."
  Figure 5 Link: articels_figures_by_rev_year\2021\Online_Attention_Accumulation_for_Weakly_Supervised_Semantic_Segmentation\figure_5.jpg
  Figure 5 caption: Examples produced by our mboxOAA and OAA++. Segmentation denotes
    the ground-truth segmentation masks. OAA++ newly discovers the regions in red
    boxes.
  Figure 6 Link: articels_figures_by_rev_year\2021\Online_Attention_Accumulation_for_Weakly_Supervised_Semantic_Segmentation\figure_6.jpg
  Figure 6 caption: The pipeline of our OAA + approach. The attention maps generated
    by the classification network during different training times are fused into the
    cumulative attention maps to mine the object regions as entire as possible. The
    obtained cumulative attention maps are utilized as pixel-level supervision to
    train the integral attention model, which further advances the attention maps
    quality.
  Figure 7 Link: articels_figures_by_rev_year\2021\Online_Attention_Accumulation_for_Weakly_Supervised_Semantic_Segmentation\figure_7.jpg
  Figure 7 caption: Visual comparisons among different attention maps produced by
    CAM [25], OAA, OAA++, and mboxOAA+ . OAA++ denotes OAA with the attention drop
    layer. mboxOAA+ denotes the integral attention model. Best viewed in color.
  Figure 8 Link: articels_figures_by_rev_year\2021\Online_Attention_Accumulation_for_Weakly_Supervised_Semantic_Segmentation\figure_8.jpg
  Figure 8 caption: Comparison of pseudo segmentation labels. (a) Source images; (b)
    Ground-truth segmentation labels; (c) Pseudo segmentation labels based on attention
    maps generated from the final model trained with the attention drop layer; (d)
    Pseudo segmentation labels based on attention maps generated from our OAA with
    attention drop layer.
  Figure 9 Link: articels_figures_by_rev_year\2021\Online_Attention_Accumulation_for_Weakly_Supervised_Semantic_Segmentation\figure_9.jpg
  Figure 9 caption: Qualitative segmentation results on the PASCAL VOC 2012 validation
    set using attention maps generated by our OAA and mboxOAA+ , respectively. We
    also show several failure cases on the bottom row.
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Peng-Tao Jiang
  Name of the last author: Yunchao Wei
  Number of Figures: 13
  Number of Tables: 14
  Number of authors: 5
  Paper title: Online Attention Accumulation for Weakly Supervised Semantic Segmentation
  Publication Date: 2021-06-25 00:00:00
  Table 1 caption: TABLE 1 Quantitative Comparisons to Previous State-of-the-Art Approaches
  Table 10 caption: TABLE 10 Segmentation Results Using the Attention Drop Layer With
    Different Training Iterations
  Table 2 caption: TABLE 2 Comparison of the Weakly Supervised Semantic Segmentation
    Methods on PASCAL VOC 2012 Validation Set
  Table 3 caption: TABLE 3 Comparison of the Weakly Supervised Semantic Segmentation
    Methods on PASCAL VOC 2012 Test Set
  Table 4 caption: TABLE 4 Comparisons of mIoU Scores on the PASCAL VOC 2012 Validation
    Set When Using Different Settings
  Table 5 caption: TABLE 5 Segmentation Results Using the Hybrid Loss With Different
    Thresholds for N c + N+c
  Table 6 caption: TABLE 6 Segmentation Results Using the Hybrid Loss Function With
    Different Training Iterations for the Segmentation Network
  Table 7 caption: TABLE 7 Comparisons of mIoU Scores on PASCAL VOC 2012 Validation
    Set When Using a Different Number of Training Images
  Table 8 caption: TABLE 8 Ablation on the Hyper-Parameters of the Attention Drop
    Layer
  Table 9 caption: TABLE 9 Comparisons of the Variants of OAA With the Attention Drop
    Layer on PASCAL VOC 2012 Validation Sets
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3092573
- Affiliation of the first author: department of computer science and engineering,
    university of notre dame, notre dame, in, usa
  Affiliation of the last author: department of computer science and engineering,
    university of notre dame, notre dame, in, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\Measuring_Human_Perception_to_Improve_Handwritten_Document_Transcription\figure_1.jpg
  Figure 1 caption: In this work we introduce a novel loss formulation for deep learning
    that incorporates measurements of human vision, which can be applied to different
    processing pipelines for handwritten document transcription.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Measuring_Human_Perception_to_Improve_Handwritten_Document_Transcription\figure_2.jpg
  Figure 2 caption: "An example of how the psychophysical penalty z i is calculated\
    \ when the priority ( \u03C1 ) is on easy or hard examples."
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Samuel Grieggs
  Name of the last author: Walter J. Scheirer
  Number of Figures: 2
  Number of Tables: 5
  Number of authors: 8
  Paper title: Measuring Human Perception to Improve Handwritten Document Transcription
  Publication Date: 2021-06-25 00:00:00
  Table 1 caption: TABLE 1 Examples of Off-the-Shelf Tools That Could be Used for
    a Handwritten Latin Transcription Task
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 A Comparison of the Average Results on the Validation Set
    of Each Dataset for Each Set of Experiments
  Table 3 caption: TABLE 3 A Comparison of the Average Results on the Test Set of
    Each Dataset for Each Set of Experiments
  Table 4 caption: TABLE 4 Average Results for the Control Experiments on the IAM
    Dataset Using the CRNN Architecture
  Table 5 caption: TABLE 5 Comparison to Other Work on Handwritten Document Transcription
    With Language Model Post-Correction
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3092688
- Affiliation of the first author: shenzhen institute of advanced technology, chinese
    academy of sciences, shenzhen, china
  Affiliation of the last author: beijing institute of technology, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\A_SelfSupervised_Gait_Encoding_Approach_With_LocalityAwareness_for_D_Skeleton_Ba\figure_1.jpg
  Figure 1 caption: Gait-based person Re-ID using 3D skeleton data.
  Figure 10 Link: articels_figures_by_rev_year\2021\A_SelfSupervised_Gait_Encoding_Approach_With_LocalityAwareness_for_D_Skeleton_Ba\figure_10.jpg
  Figure 10 caption: Confusion matrices of KS20 and BIWI. Note that abscissa and ordinate
    denote the ground-truth and predicted IDs respectively. The position in the ath
    column and bth row indicates that the testing samples belonging to the ath ID
    is predicted as the bth ID, while the corresponding value is the proportion of
    such samples to samples in the whole testing set. Full results are provided in
    the supplementary material, available online.
  Figure 2 Link: articels_figures_by_rev_year\2021\A_SelfSupervised_Gait_Encoding_Approach_With_LocalityAwareness_for_D_Skeleton_Ba\figure_2.jpg
  Figure 2 caption: "Flow diagram of our model: (1) Gait Encoder (yellow) encodes\
    \ each skeleton frame S I t into an encoded gait state h t . (2) Locality-aware\
    \ attention mechanism (green) first computes the basic attention alignment score\
    \ a t (\u22C5) , so as to measure the content-based correlation between each encoded\
    \ gait state and the decoded gait state h t from Gait Decoder (purple). Then,\
    \ the locality mask l t (\u22C5) provides an objective a ~ t (\u22C5)= a t (\u22C5\
    ) l t (\u22C5) , which guides our model to learn locality-aware alignment scores\
    \ a \xAF \xAF \xAF t (\u22C5) by the locality-aware attention alignment loss L\
    \ A . Next, h 1 \u2026 h f are weighted by a \xAF \xAF \xAF t (\u22C5) to compute\
    \ the context vector c t . c t and h t are fed into the concatenation layer f\
    \ att (\u22C5) to produce an attentional state vector h \xAF \xAF \xAF \xAF t\
    \ . Finally, h \xAF \xAF \xAF \xAF t is fed into the full connected layer f F\
    \ (\u22C5) to output t th skeleton S \xAF \xAF \xAF \xAF t and Gait Decoder for\
    \ later decoding. (3) c t is used to build Attention-based Gait Encodings (AGEs)\
    \ v t , which are concatenated and fed into f C (\u22C5) to perform locality-aware\
    \ contrastive learning (red). (4) The Contrastive Attention-based Gait Encodings\
    \ (CAGEs) v \xAF \xAF \xAF t learned by contrastive learning are fed into a recognition\
    \ network for person Re-ID (blue)."
  Figure 3 Link: articels_figures_by_rev_year\2021\A_SelfSupervised_Gait_Encoding_Approach_With_LocalityAwareness_for_D_Skeleton_Ba\figure_3.jpg
  Figure 3 caption: 'Schematic diagrams of three pretext tasks: Reverse reconstruction
    (top), prediction (middle), sorting (bottom). The original sequence boldsymbolS=(S1,ldots,Sf)
    is the input (StextI1,ldots,StextIf) for reverse reconstruction and prediction,
    and a random shuffle of boldsymbolS is the input for sorting.'
  Figure 4 Link: articels_figures_by_rev_year\2021\A_SelfSupervised_Gait_Encoding_Approach_With_LocalityAwareness_for_D_Skeleton_Ba\figure_4.jpg
  Figure 4 caption: Visualization of the BAS (left) and LAS (right) attention matrices
    that represent average attention alignment scores. Note that the abscissa and
    ordinate denote indices of input skeletons and output skeletons respectively.
    The LA alignment improves the learning of locality by assigning larger alignment
    scores near the clinodiagonal line.
  Figure 5 Link: articels_figures_by_rev_year\2021\A_SelfSupervised_Gait_Encoding_Approach_With_LocalityAwareness_for_D_Skeleton_Ba\figure_5.jpg
  Figure 5 caption: Reconstruction loss curves when using no attention, BAS, MBAS
    or LAS for skeleton reconstruction (note that here we compare different attention
    mechanisms without using contrastive learning). Using LAS achieves better reverse
    reconstruction with smaller reconstruction loss.
  Figure 6 Link: articels_figures_by_rev_year\2021\A_SelfSupervised_Gait_Encoding_Approach_With_LocalityAwareness_for_D_Skeleton_Ba\figure_6.jpg
  Figure 6 caption: "Skeleton reconstruction loss when using no attention, BAS, MBAS\
    \ or LAS for skeleton reconstruction. The comparison between applying LCL (\u201C\
    LCL\u201D) and not applying LCL (\u201CNo LCL\u201D) is reported."
  Figure 7 Link: articels_figures_by_rev_year\2021\A_SelfSupervised_Gait_Encoding_Approach_With_LocalityAwareness_for_D_Skeleton_Ba\figure_7.jpg
  Figure 7 caption: Examples of RGB images and 3D skeletons in BIWI (first row), IAS-Lab
    (second row) and CASIA B (third row). Note that the last skeleton sequence is
    estimated from RGB images of CASIA B.
  Figure 8 Link: articels_figures_by_rev_year\2021\A_SelfSupervised_Gait_Encoding_Approach_With_LocalityAwareness_for_D_Skeleton_Ba\figure_8.jpg
  Figure 8 caption: Rank-1 accuracy on different datasets when using no attention,
    BAS, MBAS or LAS for model learning. f denotes the sequence length.
  Figure 9 Link: articels_figures_by_rev_year\2021\A_SelfSupervised_Gait_Encoding_Approach_With_LocalityAwareness_for_D_Skeleton_Ba\figure_9.jpg
  Figure 9 caption: Rank-1 accuracy and nAUC comparison between the original model
    and the transferred model on different datasets. Note that the abscissa and ordinate
    denote target datasets and source datasets (for training gait encoding models)
    respectively.
  First author gender probability: 0.93
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Haocong Rao
  Name of the last author: Bin Hu
  Number of Figures: 11
  Number of Tables: 9
  Number of authors: 8
  Paper title: A Self-Supervised Gait Encoding Approach With Locality-Awareness for
    3D Skeleton Based Person Re-Identification
  Publication Date: 2021-06-28 00:00:00
  Table 1 caption: TABLE 1 Statistics of Different Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison With Existing Skeleton-Based Methods (11-16)
  Table 3 caption: TABLE 3 Re-ID Performance Comparison on Cross-View Splits (CVS)
    of KS20 Dataset
  Table 4 caption: TABLE 4 Ablation Study of Our Model
  Table 5 caption: "TABLE 5 Performance Comparison of Different Pretext Tasks and\
    \ the Proposed Enhanced Approach Under Two Re-ID Manners (\u201CAP\u201D: Average\
    \ Prediction. \u201CSC\u201D: Sequence-Level Concatenation)"
  Table 6 caption: "TABLE 6 Performance of Our Approach When Setting Different Contrasting\
    \ Intervals for Learning (\u201CInterval=1\u201D Indicates Contrasting Adjacent\
    \ Sequences)"
  Table 7 caption: "TABLE 7 Performance of Our Approach When Setting Different Temperatures\
    \ ( \u03C4=0.05,0.1,0.5,0.8,1 \u03C4=0.05,0.1,0.5,0.8,1) for the LCL Scheme on\
    \ Different Datasets"
  Table 8 caption: TABLE 8 Rank-1 Accuracy on Different Views of CASIA B Under CVE
    Setup
  Table 9 caption: TABLE 9 Rank-1 Matching Rate on CASIA B Compared With Appearance-Based
    Methods Under CME Setup
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3092833
- Affiliation of the first author: mitsubishi electric research labs (merl), cambridge,
    ma, usa
  Affiliation of the last author: research school of engineering, australian national
    university, canberra, act, australia
  Figure 1 Link: articels_figures_by_rev_year\2021\Generalized_OneClass_Learning_Using_Pairs_of_Complementary_Classifiers\figure_1.jpg
  Figure 1 caption: Visualizations of decision regions using various GODS formulations
    on synthetic data. Figures (a, b, c) show subspaces found by BODS and GODS on
    various data distributions (see Section 7.5). The colors identify hyperplanes
    within a classifier in the complementary pair. Figure (d) shows KODS decision
    regions for ring-shaped data (black dots) using an RBF kernel. Figures (e, f)
    are the decision regions of the classifiers; W 1 bounding data from outside and
    W 2 from inside, together they define the region in (d). Figure (g,h,i) show 3D
    points and the decision surfaces of the two classifiers.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Generalized_OneClass_Learning_Using_Pairs_of_Complementary_Classifiers\figure_2.jpg
  Figure 2 caption: (a), (b) graphically illustrate our BODS and GODS formulations.
    Our one-class variants learn discriminative hyperplanes (BODS) or orthonormal
    frames (GODS) that maximize the margin with the data distribution while minimizing
    the volume of the one-class region captured (via minimizing the distance dis t
    2 between hyperplanes or frames). In Fig. 2c, we show a detailed depiction of
    the objectives in GODS using a single data point x . See Section 4.2 for more
    details.
  Figure 3 Link: articels_figures_by_rev_year\2021\Generalized_OneClass_Learning_Using_Pairs_of_Complementary_Classifiers\figure_3.jpg
  Figure 3 caption: Frames from our Dash-Cam-Pose dataset. The left-top frame has
    poses in-position (one-class), while the rest of the frames are from videos labeled
    out-of-position.
  Figure 4 Link: articels_figures_by_rev_year\2021\Generalized_OneClass_Learning_Using_Pairs_of_Complementary_Classifiers\figure_4.jpg
  Figure 4 caption: Some examples from JHMDB (first column), UCF-Crime (second column),
    and USCD Ped2 (third column) datasets, with respective categories.
  Figure 5 Link: articels_figures_by_rev_year\2021\Generalized_OneClass_Learning_Using_Pairs_of_Complementary_Classifiers\figure_5.jpg
  Figure 5 caption: "Performance of GODS with F1 , F1 \xAF \xAF \xAF \xAF \xAF \xAF\
    \ and AUC for different initialization methods and manifold assumptions."
  Figure 6 Link: articels_figures_by_rev_year\2021\Generalized_OneClass_Learning_Using_Pairs_of_Complementary_Classifiers\figure_6.jpg
  Figure 6 caption: "Performance of BODS with F1 and F1 \xAF \xAF \xAF \xAF \xAF \xAF\
    \ for increasing \u03B7 ."
  Figure 7 Link: articels_figures_by_rev_year\2021\Generalized_OneClass_Learning_Using_Pairs_of_Complementary_Classifiers\figure_7.jpg
  Figure 7 caption: Performance of GODS for an increasing number of subspaces.
  Figure 8 Link: articels_figures_by_rev_year\2021\Generalized_OneClass_Learning_Using_Pairs_of_Complementary_Classifiers\figure_8.jpg
  Figure 8 caption: Performance of KODS in different kernel type on various datasets
    for an increasing number of subspaces.
  Figure 9 Link: articels_figures_by_rev_year\2021\Generalized_OneClass_Learning_Using_Pairs_of_Complementary_Classifiers\figure_9.jpg
  Figure 9 caption: "(a\u2013d) show optimization convergence on JHMDB dataset. (a-b)\
    \ GODS (10) against the choice of optimization schemes, (c-d) KODS (18) under\
    \ different kernels (using CG optimizer). (e) compares running time."
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Anoop Cherian
  Name of the last author: Jue Wang
  Number of Figures: 9
  Number of Tables: 5
  Number of authors: 2
  Paper title: Generalized One-Class Learning Using Pairs of Complementary Classifiers
  Publication Date: 2021-06-28 00:00:00
  Table 1 caption: TABLE 1 Comparisons of GODS Variants on Vision Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Average Performances on the Dash-Cam-Pose and JHMDB Datasets
  Table 3 caption: TABLE 3 Performances on UCF-Crime Dataset (Left) and UCSD Ped2
    Dataset (Right)
  Table 4 caption: TABLE 4 Performances on UCI Datasets
  Table 5 caption: TABLE 5 Comparisons of GODS Variants on UCI Datasets
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3092999
- Affiliation of the first author: department of computer science, illinois institute
    of technology, chicago, il, usa
  Affiliation of the last author: department of computer science, illinois institute
    of technology, chicago, il, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\Saying_the_Unseen_Video_Descriptions_via_Dialog_Agents\figure_1.jpg
  Figure 1 caption: Unseen video description task via interpretable knowledge transfer
    between dialog agents. The task setup includes three phases, and the ultimate
    goal is for Q-BOT to describe the unseen video mainly based on the dialog. The
    input description is also presented for reference. The difference between the
    input descriptions for A-BOT and the final descriptions given by Q-BOT reveals
    the actual knowledge gap due to the lack of direct access to the original video
    data.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Saying_the_Unseen_Video_Descriptions_via_Dialog_Agents\figure_2.jpg
  Figure 2 caption: Different inputs for Q-BOT and A-BOT in shaded orange and blue
    boxes, respectively. For Q-BOT, we extract the first and the last frames from
    the video clip and process the extracted frames using a pre-trained semantic segmentation
    model. We then input semantic segmented frames to Q-BOT, which only provides a
    sketchy perception of the general environment and does not reveal any sensitive
    information. In contrast, A-BOT has access to all the information, including entire
    videos, audio signals, and descriptions.
  Figure 3 Link: articels_figures_by_rev_year\2021\Saying_the_Unseen_Video_Descriptions_via_Dialog_Agents\figure_3.jpg
  Figure 3 caption: QA-Cooperative network at QA round i . History dialog H is a common
    input for both agents. The model components for Q-BOT are in orange color boxes,
    while those for A-BOT are in blue. The dashed lines represent the processing of
    the question and answer candidates uniquely for the discriminative setting, while
    the solid lines are operations for both generative and discriminative settings.
  Figure 4 Link: articels_figures_by_rev_year\2021\Saying_the_Unseen_Video_Descriptions_via_Dialog_Agents\figure_4.jpg
  Figure 4 caption: Example of qualitative results. We present the input of Q-BOT,
    different video descriptions, and the internal dialog. The descriptions given
    by our Q-BOT include more details compared to multiple baselines. The color intensities
    in the figure represent attention weights. More examples can be found in Appendix,
    available in the online supplemental material.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.54
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.76
  Name of the first author: Ye Zhu
  Name of the last author: Yan Yan
  Number of Figures: 4
  Number of Tables: 3
  Number of authors: 4
  Paper title: 'Saying the Unseen: Video Descriptions via Dialog Agents'
  Publication Date: 2021-06-29 00:00:00
  Table 1 caption: TABLE 1 Notations for the Unseen Video Description Task
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Quantitative Experimental Results of the Unseen Video Description
    Task
  Table 3 caption: TABLE 3 Quantitative Results for Ablation Studies on Model Components,
    Data Modalities, QA Pairs, Beam Width, and Cluster Numbers
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3093360
- Affiliation of the first author: faculty of engineering, bar ilan university, ramat
    gan, israel
  Affiliation of the last author: facebook ai, menlo park, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\DeepFake_Detection_Based_on_Discrepancies_Between_Faces_and_Their_Context\figure_1.jpg
  Figure 1 caption: 'Detecting swapped faces by comparing faces and their context.
    Two example fake (swapped) faces from DFD [1]. Left: The arm of the eyeglasses
    does not extend from face to context. Right: An apparent identity mismatch between
    face and context. We show how these and similar discrepancies can be used as powerful
    signals for automatic detection of swapped faces.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\DeepFake_Detection_Based_on_Discrepancies_Between_Faces_and_Their_Context\figure_2.jpg
  Figure 2 caption: Affected regions of different manipulation methods. (a) + (b)
    Face2Face [2] and NeuralTextures [3]; (c) + (d) Deepfake [4] variants of FaceForensics++
    [5] and DFD [1]; (e) FaceSwap [6]; (f) FSGAN [7]. In all cases, faces are manipulated
    but their context is left unchanged.
  Figure 3 Link: articels_figures_by_rev_year\2021\DeepFake_Detection_Based_on_Discrepancies_Between_Faces_and_Their_Context\figure_3.jpg
  Figure 3 caption: LFW verification accuracy for identification networks trained
    on different face regions. (a) Results obtained by representing faces with the
    final layers of the Xception architectures. (b) Faces represented using the activations
    of the penultimate layers of Xception. In the latter case, face versus context
    do not match well for the same person, since the two networks were trained independently.
    Our approach, therefore, uses the final layers of the networks, representing subject
    pseudo-probabilities, when comparing the two (top).
  Figure 4 Link: articels_figures_by_rev_year\2021\DeepFake_Detection_Based_on_Discrepancies_Between_Faces_and_Their_Context\figure_4.jpg
  Figure 4 caption: Method overview. Following initial preprocessing, we obtain regions
    for the face, If , and its context, Ic . The two are processed by the face identification
    networks, Ef and Ec , respectively. A separate network, Es , considers the input
    image, I , seeking apparent swapping artifacts to decide if it is a face swapping
    result. The pseudo-probability vectors of the two face identification networks
    are subtracted and, jointly with the representations obtained from the method
    type network, Es , are passed to the final classifier, D .
  Figure 5 Link: articels_figures_by_rev_year\2021\DeepFake_Detection_Based_on_Discrepancies_Between_Faces_and_Their_Context\figure_5.jpg
  Figure 5 caption: Extending FaceForensics++ with unseen methods. Examples shown
    for the same source target face pair, using the 3D-based methods, FaceSwap [6]
    and Nirkin et al. [18], and the GAN-based methods, Deepfakes [4] and FSGAN [7].
    Despite using the same image pairs in all four examples, the results are different,
    each exhibiting its own artifacts.
  Figure 6 Link: articels_figures_by_rev_year\2021\DeepFake_Detection_Based_on_Discrepancies_Between_Faces_and_Their_Context\figure_6.jpg
  Figure 6 caption: Results on our two variations of FaceForensics++ videos. (a) Generalization
    results with FSGAN generated swaps [7]. (b) Generalization results with swaps
    generated by Nirkin et al. [18]. See Section 5.3 for more details.
  Figure 7 Link: articels_figures_by_rev_year\2021\DeepFake_Detection_Based_on_Discrepancies_Between_Faces_and_Their_Context\figure_7.jpg
  Figure 7 caption: Qualitative detection results. Examples taken from the DFDC collection.
    (a) Fakes detected by our method, but undetected by a leading baseline, XceptionNet
    fake detector [60]. (b) Fakes detected by XceptionNet but missed by our approach.
    (c) Fakes missed by both methods. See Section 5.4 for more details.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.96
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.74
  Name of the first author: Yuval Nirkin
  Name of the last author: Tal Hassner
  Number of Figures: 7
  Number of Tables: 5
  Number of authors: 4
  Paper title: DeepFake Detection Based on Discrepancies Between Faces and Their Context
  Publication Date: 2021-06-29 00:00:00
  Table 1 caption: TABLE 1 Face Recognition Accuracy on VGGFace2
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Face Swap Detection Results
  Table 3 caption: TABLE 3 FaceForensics++ Image Benchmark Results
  Table 4 caption: TABLE 4 Generalization Ablation
  Table 5 caption: TABLE 5 Image Laundry Ablation
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3093446
- Affiliation of the first author: simon fraser university, burnaby, bc, canada
  Affiliation of the last author: simon fraser university, burnaby, bc, canada
  Figure 1 Link: articels_figures_by_rev_year\2021\Learning_Mesh_Representations_via_Binary_Space_Partitioning_Tree_Networks\figure_1.jpg
  Figure 1 caption: (a) 3D shape auto-encoding by our network, BSP-Net, quickly reconstructs
    a compact, i.e., low-poly, mesh, which can be easily textured. The mesh edges
    reproduce sharp details in the input (e.g., edges of the legs), yet still approximate
    smooth geometry (e.g., circular table-top). (b) Current implicit models regress
    an indicator function, which needs to be iso-surfaced, resulting in over-tessellated
    meshes which only approximate sharp details with smooth surfaces.
  Figure 10 Link: articels_figures_by_rev_year\2021\Learning_Mesh_Representations_via_Binary_Space_Partitioning_Tree_Networks\figure_10.jpg
  Figure 10 caption: Structured SVR by BSP-Net reconstructs each shape with corresponding
    convexes. Convexes belonging to the same semantic parts are manually grouped and
    assigned the same color, resulting in semantic part correspondence.
  Figure 2 Link: articels_figures_by_rev_year\2021\Learning_Mesh_Representations_via_Binary_Space_Partitioning_Tree_Networks\figure_2.jpg
  Figure 2 caption: "An illustration of the neural BSP-tree for shape construction.\
    \ See Fig. 3 for definitions of P , T , C , and S \u2217 ."
  Figure 3 Link: articels_figures_by_rev_year\2021\Learning_Mesh_Representations_via_Binary_Space_Partitioning_Tree_Networks\figure_3.jpg
  Figure 3 caption: Network architecture of BSP-Net, corresponding to Fig. 2.
  Figure 4 Link: articels_figures_by_rev_year\2021\Learning_Mesh_Representations_via_Binary_Space_Partitioning_Tree_Networks\figure_4.jpg
  Figure 4 caption: "Evaluation in 2D \u2013 auto-encoder trained on the synthetic\
    \ 2D dataset. We show auto-encoding results and highlight mistakes made in Stage\
    \ 1 with red circles, which are resolved in Stage 2. We further show the effect\
    \ of enabling the (optional) overlap loss. Notice that in the visualization we\
    \ use different (possibly repeating) colors to indicate different convexes."
  Figure 5 Link: articels_figures_by_rev_year\2021\Learning_Mesh_Representations_via_Binary_Space_Partitioning_Tree_Networks\figure_5.jpg
  Figure 5 caption: "Examples of L 2 output \u2013 a few convexes from the first shape\
    \ in Fig. 4 and the planes used. Note how many planes are unused."
  Figure 6 Link: articels_figures_by_rev_year\2021\Learning_Mesh_Representations_via_Binary_Space_Partitioning_Tree_Networks\figure_6.jpg
  Figure 6 caption: "The architecture of BSP-FC. Compared to the original BSP-Net\
    \ architecture in Fig. 3, we use the same network structure up to layer L 1 ,\
    \ and then, instead of primitive selection and assembly, we continue with multiple\
    \ fully-connected (FC) layers to reconstruct the 3D shape. Also, we replace the\
    \ MLP P \u03C9 in BSP-Net by a linear mapping. All new changes are highlighted\
    \ in dark blue color."
  Figure 7 Link: articels_figures_by_rev_year\2021\Learning_Mesh_Representations_via_Binary_Space_Partitioning_Tree_Networks\figure_7.jpg
  Figure 7 caption: "Segmentation and correspondence \u2013 semantics implied from\
    \ autoencoding by BSP-Net. Colors shown here are the result of a manual grouping\
    \ of learned convexes. The color assignment was performed on a few shapes: once\
    \ a convex is colored in one shape, we can propagate the color to the other shapes\
    \ by using the learnt convex ID."
  Figure 8 Link: articels_figures_by_rev_year\2021\Learning_Mesh_Representations_via_Binary_Space_Partitioning_Tree_Networks\figure_8.jpg
  Figure 8 caption: Segmentation and reconstruction Qualitative.
  Figure 9 Link: articels_figures_by_rev_year\2021\Learning_Mesh_Representations_via_Binary_Space_Partitioning_Tree_Networks\figure_9.jpg
  Figure 9 caption: "Single-view 3D reconstruction \u2013 comparison to AtlasNet [28],\
    \ IM-NET [11], and OccNet [15]. Middle column shows mesh tessellations of the\
    \ reconstruction; last column shows the edge sampling used in the ECD metric."
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Zhiqin Chen
  Name of the last author: Hao Zhang
  Number of Figures: 14
  Number of Tables: 7
  Number of authors: 3
  Paper title: Learning Mesh Representations via Binary Space Partitioning Tree Networks
  Publication Date: 2021-06-29 00:00:00
  Table 1 caption: 'TABLE 1 Surface Reconstruction: Comparison for 3D Shape Auto-Encoding'
  Table 10 caption: Not Available
  Table 2 caption: 'TABLE 2 Segmentation: Comparison in Per-Label IoU'
  Table 3 caption: 'TABLE 3 Single View Reconstruction (SVR): Comparison to State-of-the-Art
    Neural Models'
  Table 4 caption: 'TABLE 4 Low-Poly Analysis: Dataset-Averaged Metrics for SVR'
  Table 5 caption: TABLE 5 Results of Ablation Study on Shape Auto-Encoding
  Table 6 caption: TABLE 6 Results of Ablation Study on Single View Reconstruction
  Table 7 caption: TABLE 7 Comparing General Implicit Field Decoders on 3D Shape Auto-Encoding
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3093440
- Affiliation of the first author: school of artificial intelligence, jilin university,
    changchun, jilin, china
  Affiliation of the last author: australian artificial intelligence institute, university
    of technology sydney, nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2021\Distribution_Disagreement_via_Lorentzian_Focal_Representation\figure_1.jpg
  Figure 1 caption: "Lorentzian version space ( A \u2032 BC , D \u2032 EF ) versus\
    \ euclidean version space ( ABC , DEF ). Lorentzian norm shrinks the volume of\
    \ euclidean version space by shifting euclidean centroids (A,D) into Lorentzian\
    \ focal points (A \u2032 , D \u2032 ) that are close to the boundary region. As\
    \ such, sampling from regions AB A \u2032 C and DE D \u2032 F is ineffective,\
    \ which means any AL model in the Lorentzian version space has a tighter bound\
    \ on label complexity (i.e., a smaller number of labels are needed to achieve\
    \ a desired error threshold)."
  Figure 10 Link: articels_figures_by_rev_year\2021\Distribution_Disagreement_via_Lorentzian_Focal_Representation\figure_10.jpg
  Figure 10 caption: Training losses of each epoch of training ResNet20 with different
    AL outputs on CIFAR-10, CIFAR-100, and SVHN.
  Figure 2 Link: articels_figures_by_rev_year\2021\Distribution_Disagreement_via_Lorentzian_Focal_Representation\figure_2.jpg
  Figure 2 caption: "Generalization test over MNIST dataset using \u03B8 and \u03B8\
    \ G ."
  Figure 3 Link: articels_figures_by_rev_year\2021\Distribution_Disagreement_via_Lorentzian_Focal_Representation\figure_3.jpg
  Figure 3 caption: "Euclidean, Gaussian kernelized, Poincar\xE9 and Lorentzian centroids\
    \ on a noisy spherical Gaussian dataset. The minimization or maximization on the\
    \ centroids are performed with 30 iterations against k -medoids algorithm."
  Figure 4 Link: articels_figures_by_rev_year\2021\Distribution_Disagreement_via_Lorentzian_Focal_Representation\figure_4.jpg
  Figure 4 caption: "Test accuracies of euclidean, squared euclidean, Gaussian kernelized,\
    \ Poincar\xE9 and Lorentzian centroids with varying parameters."
  Figure 5 Link: articels_figures_by_rev_year\2021\Distribution_Disagreement_via_Lorentzian_Focal_Representation\figure_5.jpg
  Figure 5 caption: "Geometric centroids of euclidean, Poincar\xE9, and focal points\
    \ of squared Lorentzian distances in a given set of points. Lorentzian norm updates\
    \ the focal points of the embedded half-sphere toward the surface as the parameter\
    \ B decreases w.r.t. Eq. (15)."
  Figure 6 Link: articels_figures_by_rev_year\2021\Distribution_Disagreement_via_Lorentzian_Focal_Representation\figure_6.jpg
  Figure 6 caption: Illustration of tree-likeness splitting. The first layer nodes
    are k global focal points. A binary tree splitting strategy begins from the second
    layer of the tree.
  Figure 7 Link: articels_figures_by_rev_year\2021\Distribution_Disagreement_via_Lorentzian_Focal_Representation\figure_7.jpg
  Figure 7 caption: "Test accuracies of euclidean, squared euclidean, Gaussian kernelized,\
    \ Poincar\xE9 centroids and our Lorentzian focal points with varying parameters\
    \ on MNIST."
  Figure 8 Link: articels_figures_by_rev_year\2021\Distribution_Disagreement_via_Lorentzian_Focal_Representation\figure_8.jpg
  Figure 8 caption: Test accuracies of training ResNet20 with different AL outputs
    on CIFAR-10, CIFAR-100, and SVHN.
  Figure 9 Link: articels_figures_by_rev_year\2021\Distribution_Disagreement_via_Lorentzian_Focal_Representation\figure_9.jpg
  Figure 9 caption: Test accuracies of each epoch of training ResNet20 with different
    AL outputs on CIFAR-10, CIFAR-100, and SVHN.
  First author gender probability: 0.83
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Xiaofeng Cao
  Name of the last author: Ivor W. Tsang
  Number of Figures: 14
  Number of Tables: 7
  Number of authors: 2
  Paper title: Distribution Disagreement via Lorentzian Focal Representation
  Publication Date: 2021-06-30 00:00:00
  Table 1 caption: "TABLE 1 Mean \xB1 \xB1standard Deviation of the Breakpoints of\
    \ the Generalization Test Over MNIST Using \u03B8 \u03B8 and \u03B8 G \u03B8G"
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 Label Complexities of the Generalization Test Over MNIST\
    \ Using \u03B8 \u03B8 and \u03B8 G \u03B8G"
  Table 3 caption: TABLE 3 Summary of Distance Metrics and Centroid Expressions in
    the Euclidean Space, RKHS, and Hyperbolic Space
  Table 4 caption: "TABLE 4 Mean \xB1 \xB1Standard Deviation of the Test Accuracies\
    \ of the Breakpoints on CIFAR-10, CIFAR-100 and SVHN After 80 Epochs Over 3 Runs"
  Table 5 caption: "TABLE 5 Mean \xB1 \xB1standard Deviation of the Training Losses\
    \ on CIFAR-10, CIFAR-100, and SVHN After 80 Epochs Over 3 Runs"
  Table 6 caption: "TABLE 6 Mean \xB1 \xB1standard Deviation of the Test Accuracies\
    \ of Different Baselines With Batch Settings on CIFAR-10"
  Table 7 caption: TABLE 7 Time Computations of Selecting 10,000 Examples From CIFAR-10
    Using Different Baselines
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3093590
