- Affiliation of the first author: dgene (prev. plex-vr), santa clara, ca, usa
  Affiliation of the last author: school of information science and technology, shanghaitech
    university, shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2019\Structure_From_Motion_on_XSlit_Cameras\figure_1.jpg
  Figure 1 caption: 'XSlit ray geometry. Top: Ray geometry of a single XSlit camera.
    Bottom: XSlit images captured from different viewpoints are correlated by a fundamental
    matrix F .'
  Figure 10 Link: articels_figures_by_rev_year\2019\Structure_From_Motion_on_XSlit_Cameras\figure_10.jpg
  Figure 10 caption: Error curve w.r.t the accuracy of initial pose.
  Figure 2 Link: articels_figures_by_rev_year\2019\Structure_From_Motion_on_XSlit_Cameras\figure_2.jpg
  Figure 2 caption: 'The degenerated cases of the generalized epipolar constraint.
    Top: the locally-central case, where for every 3D point the projection center
    is fixed across views, and the axial case, where all image rays must intersect
    a common line, identified by [25] and [26]. Bottom: The XSlit degeneracy where
    corresponding rays across views must lie on a doubly ruled surface.'
  Figure 3 Link: articels_figures_by_rev_year\2019\Structure_From_Motion_on_XSlit_Cameras\figure_3.jpg
  Figure 3 caption: 'An example of our feature matching technique. Left: We disturb
    the Gaussian kernel in SIFT using affine deformation and apply them to patches
    in XSlit image pairs. Middle: A better match can be found by non-uniform kernels.
    Right: Projected curves using the fundamental matrix calculated from correspondences
    produced by our method.'
  Figure 4 Link: articels_figures_by_rev_year\2019\Structure_From_Motion_on_XSlit_Cameras\figure_4.jpg
  Figure 4 caption: 'Feature matching evaluation. Top: Subset of feature points generated
    by our algorithm. The overlaid parallelograms illustrate the affine transformations
    used to produce features; Bottom-left: Precision curves in comparison with state-of-the-arts;
    Bottom-right: Numbers of valid features (correctly matched) and all matched features
    w.r.t. different viewing angles.'
  Figure 5 Link: articels_figures_by_rev_year\2019\Structure_From_Motion_on_XSlit_Cameras\figure_5.jpg
  Figure 5 caption: XSlit camera exhibits depth dependent aspect ratio and depth dependent
    line slopes.
  Figure 6 Link: articels_figures_by_rev_year\2019\Structure_From_Motion_on_XSlit_Cameras\figure_6.jpg
  Figure 6 caption: 'Two error metrics for XSlit bundle adjustment. (a) left: Re-projection
    error Ed : distance between the projected and observed points; (b) left: Depth-dependent
    slope (DDS) error Er : difference between the projected and observed slopes; Right:
    Bundle adjustment (BA) comparison without (a) and with DDS (b) error metric.'
  Figure 7 Link: articels_figures_by_rev_year\2019\Structure_From_Motion_on_XSlit_Cameras\figure_7.jpg
  Figure 7 caption: Comparison with Li et al.s method [25] under 1000 random tests
    with Gaussian noise. The figure shows the histograms of translation error and
    rotation error. The horizontal and vertical axes correspond to error and tests
    number respectively. The top row shows the error of Li et al.s method [25] and
    the bottom row is ours.
  Figure 8 Link: articels_figures_by_rev_year\2019\Structure_From_Motion_on_XSlit_Cameras\figure_8.jpg
  Figure 8 caption: The robustness of our algorithm w.r.t the noise ratio and the
    point-to-camera distance.
  Figure 9 Link: articels_figures_by_rev_year\2019\Structure_From_Motion_on_XSlit_Cameras\figure_9.jpg
  Figure 9 caption: Error curve of Li et al.s method, non-linear optimization method
    and our method w.r.t to noise. Li et al.s method does not work as it only handles
    ambiguities in axial and central cameras. Without reliable prior, non-linear optimization
    method can not produce accurate estimation.
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.99
  Name of the first author: Wei Yang
  Name of the last author: Jingyi Yu
  Number of Figures: 17
  Number of Tables: 3
  Number of authors: 7
  Paper title: Structure From Motion on XSlit Cameras
  Publication Date: 2019-12-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparisons of Our Feature Matching Technique Versus the State-of-the-Art
      Methods on XSlit Image Pair
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Recovered XSlit Camera Poses Shown in Fig. 17
  Table 3 caption:
    table_text: TABLE 3 Percentage of the Correct Points in the Reconstructed Point
      Cloud
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2957119
- Affiliation of the first author: idiap research institute, martigny, switzerland
  Affiliation of the last author: idiap research institute, martigny, switzerland
  Figure 1 Link: articels_figures_by_rev_year\2019\A_Differential_Approach_for_Gaze_Estimation\figure_1.jpg
  Figure 1 caption: Examples of variability factors. (a) Head pose shape variabilities
    induce different frontal head pose definition and hence variabilities in eye images.
    (images from Pinterest.com). (b) Variabilities across subjects of the difference
    between the visual axis (unobserved, defining gaze) and the optical axis (defined
    by iris center, observed) introduces gaze prediction uncertainties (image from
    [18]).
  Figure 10 Link: articels_figures_by_rev_year\2019\A_Differential_Approach_for_Gaze_Estimation\figure_10.jpg
  Figure 10 caption: Average angular error (degree) on EYEDIAP dataset with different
    Gaussian kernel bandwidth sigma (in degree).
  Figure 2 Link: articels_figures_by_rev_year\2019\A_Differential_Approach_for_Gaze_Estimation\figure_2.jpg
  Figure 2 caption: Appearance comparison. Columns (a) to (d) show right eye images
    from different persons from the EYEDIAP dataset [23]. Row (1) to (3) correspond
    to gaze directions with the same pitch (5 degrees) and a yaw of 5, 10, 15 degrees
    respectively.
  Figure 3 Link: articels_figures_by_rev_year\2019\A_Differential_Approach_for_Gaze_Estimation\figure_3.jpg
  Figure 3 caption: Approach overview. During training, random pairs of samples from
    the same eye are used to train a differential network. At test time, given a set
    of reference samples, gaze differences are computed and used to infer the gaze
    of the input image. To gain higher accuracy, the differential network can be adapted
    via fine- tuning using the pairs of reference samples.
  Figure 4 Link: articels_figures_by_rev_year\2019\A_Differential_Approach_for_Gaze_Estimation\figure_4.jpg
  Figure 4 caption: Baseline CNN structure for gaze estimation.
  Figure 5 Link: articels_figures_by_rev_year\2019\A_Differential_Approach_for_Gaze_Estimation\figure_5.jpg
  Figure 5 caption: Scatter plot of the network regression ( X -axis) and labelled
    groundtruth ( Y -axis) of the yaw (left plot) and pitch (right plot) angles for
    an individual eye taken in the (a) EYEDIAP dataset; and (b) MPIIGaze dataset.
  Figure 6 Link: articels_figures_by_rev_year\2019\A_Differential_Approach_for_Gaze_Estimation\figure_6.jpg
  Figure 6 caption: The designed differential network.
  Figure 7 Link: articels_figures_by_rev_year\2019\A_Differential_Approach_for_Gaze_Estimation\figure_7.jpg
  Figure 7 caption: 'Histogram and cumulative histogram of angular errors (in degree)
    due to the random selection of the calibration images, for (a) a given user and
    (b) all users of the MPIIGaze dataset, and for different methods: Diff-NN (green
    curve), Lin-Ad (blue curve), Baseline (red; in (b) the average result per user
    is used for the plot).'
  Figure 8 Link: articels_figures_by_rev_year\2019\A_Differential_Approach_for_Gaze_Estimation\figure_8.jpg
  Figure 8 caption: The average prediction error of the yaw in function of the absolute
    difference in yaw between the reference and test samples (green curve), and similarly
    for the pitch and gaze.
  Figure 9 Link: articels_figures_by_rev_year\2019\A_Differential_Approach_for_Gaze_Estimation\figure_9.jpg
  Figure 9 caption: Comparison of average angular error in degree (average of the
    left right eyes) for different methods in function of the number of reference
    images on EYEDIAP dataset. Note that the Baseline method does not require calibration
    data.
  First author gender probability: 0.95
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Gang Liu
  Name of the last author: Jean-Marc Odobez
  Number of Figures: 10
  Number of Tables: 3
  Number of authors: 4
  Paper title: A Differential Approach for Gaze Estimation
  Publication Date: 2019-12-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average Angular Error (degree) on Three Public Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average Angular Error (degree) on EYEDIAP Dataset for Different
      Systems (See Text)
  Table 3 caption:
    table_text: "TABLE 3 Run-times (in ms) Between the Baseline and Our Diff-NN Method,\
      \ Using Mini-Batch (Diff-NN \u2217 ) Computation or Not"
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2957373
- Affiliation of the first author: center for research on intelligent system and engineering,
    institute of automation, chinese academy of sciences, beijing, china
  Affiliation of the last author: center for research on intelligent system and engineering
    and national laboratory of pattern recognition, institute of automation, chinese
    academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2019\GOTk_A_Large_HighDiversity_Benchmark_for_Generic_Object_Tracking_in_the_Wild\figure_1.jpg
  Figure 1 caption: 'Screenshots of some representative videos collected and annotated
    in GOT-10k. Each video is attached with two semantic labels: object and motion
    classes. The object classes in GOT-10k are populated based on the semantic hierarchy
    of WordNet [1], where we expand five subtrees: animal, vehicle, person, passive
    motion object and object part to cover the majority of both natural and artificial
    moving objects in real-world. The motion classes are mostly backboned by WordNet,
    with only 6 exceptions: camera motion, dragon-lion dance, parkour, powered cartwheeling,
    taking off, and uneven bars, which are not included in WordNet. GOT-10k populates
    563 object classes and 87 motion classes in total. From the screenshots, we can
    also find that introducing motion labels to data collection significantly improves
    the variety of our dataset.'
  Figure 10 Link: articels_figures_by_rev_year\2019\GOTk_A_Large_HighDiversity_Benchmark_for_Generic_Object_Tracking_in_the_Wild\figure_10.jpg
  Figure 10 caption: Class-wise performance of baseline trackers on GOT-10k. We divide
    the test videos of GOT-10k into several subsets according to the object or motion
    classes and evaluate AO scores on each subset. The thick red curves represent
    the average performance over baseline trackers. (a) AO w.r.t. object classes.
    (b) AO w.r.t. motion classes. (c) AO w.r.t. motion classes of person. Note for
    the person class, motion classes in training and test sets are not overlapped.
    Best viewed with zooming in.
  Figure 2 Link: articels_figures_by_rev_year\2019\GOTk_A_Large_HighDiversity_Benchmark_for_Generic_Object_Tracking_in_the_Wild\figure_2.jpg
  Figure 2 caption: 'BAR CHART: Number of object classes in different tracking (blue)
    and video image detection (red) datasets. Among all the compared datasets, GOT-10k
    offers an unprecedentedly wider coverage of moving object classes. TABLE: Statistics
    of five subtrees populated in GOT-10k. All 563 object classes in GOT-10k are expanded
    from the five subtrees. The table shows the number of tracking targets, the scale
    of manually annotated object bounding boxes, sub-class counts, and average video
    length per each subtree.'
  Figure 3 Link: articels_figures_by_rev_year\2019\GOTk_A_Large_HighDiversity_Benchmark_for_Generic_Object_Tracking_in_the_Wild\figure_3.jpg
  Figure 3 caption: Screenshots taken from the YouTube-BB [26] and ImageNet-VID [23]
    datasets. Many of their videos contain noisy segments such as incomplete objects
    and shot changes, making them less optimal for the tracking task.
  Figure 4 Link: articels_figures_by_rev_year\2019\GOTk_A_Large_HighDiversity_Benchmark_for_Generic_Object_Tracking_in_the_Wild\figure_4.jpg
  Figure 4 caption: The number of videos per each group of object classes. In the
    collection stage, we categorize all potential object classes into 121 groups that
    we would like to ensure each being collected, as described in Section 3.1. The
    plot shows the final distribution of these groups in GOT-10k, with OTB2015 dataset
    as a comparison.
  Figure 5 Link: articels_figures_by_rev_year\2019\GOTk_A_Large_HighDiversity_Benchmark_for_Generic_Object_Tracking_in_the_Wild\figure_5.jpg
  Figure 5 caption: (a)-(d) Per-frame occlusion labeling in popular tracking datasets
    and GOT-10k. (a) The OTB dataset provides no frame-wise labeling of occlusion,
    while (b) The VOT dataset offers a binary label for each frame indicating whether
    or not the target is occluded. (c) The NUSPRO dataset distinguishes between partial
    and complete occlusions, while (d) GOT-10k further offers a more continuous labeling
    of target occlusion status. (e) Cumulative distribution of objects visible ratios
    in GOT-10k. The regions that are occluded or truncated by image boundary correspond
    to the invisible parts. In around 15.43 percent of frames, the targets are occludedtruncated
    (with less than 90 percent visible), and in approximately 1.86 percent of frames,
    they are heavily occludedtruncated (with less than 45 percent visible). In about
    0.43 percent of frames, the targets are absent (being fully occluded or out-of-view).
  Figure 6 Link: articels_figures_by_rev_year\2019\GOTk_A_Large_HighDiversity_Benchmark_for_Generic_Object_Tracking_in_the_Wild\figure_6.jpg
  Figure 6 caption: The impact of different test data configurations on evaluation
    stability, assessed by the standard deviation of the ranking of 25 trackers. The
    average standard deviation over trackers is emphasized by the thick red curves.
    A higher standard deviation indicates a less reliable evaluation. Better viewed
    with zooming in.
  Figure 7 Link: articels_figures_by_rev_year\2019\GOTk_A_Large_HighDiversity_Benchmark_for_Generic_Object_Tracking_in_the_Wild\figure_7.jpg
  Figure 7 caption: Dataset splits of GOT-10k. Except for the person class, all object
    classes between training and test videos are not overlapped; while for persons,
    the motion classes between training and test are not overlapped. The validation
    set is randomly sampled from training videos with uniform distribution across
    different object classes.
  Figure 8 Link: articels_figures_by_rev_year\2019\GOTk_A_Large_HighDiversity_Benchmark_for_Generic_Object_Tracking_in_the_Wild\figure_8.jpg
  Figure 8 caption: Success curves of 39 baseline trackers on GOT-10k (420 sequences)
    and OTB2015 (100 sequences) benchmarks, ranked by their mean average overlap (mAO)
    scores. For clarity, only the top 25 trackers are shown.
  Figure 9 Link: articels_figures_by_rev_year\2019\GOTk_A_Large_HighDiversity_Benchmark_for_Generic_Object_Tracking_in_the_Wild\figure_9.jpg
  Figure 9 caption: 'Performance of baseline trackers under 6 challenging attributes:
    occlusiontruncation, scale variation, aspect ratio variation, fast motion, illumination
    variation and low resolution targets. Each attribute is quantified as a continuous
    difficulty indicator, which can be directly computed from the annotations, as
    introduced in Section 4.4. The thick red curves represent the average performance
    over trackers. The final AO score of each tracker at the legend is computed by
    averaging the results on the top 20 percent hardest frames. For clarity, only
    the top 25 trackers are shown in the plots.'
  First author gender probability: 0.61
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.61
  Name of the first author: Lianghua Huang
  Name of the last author: Kaiqi Huang
  Number of Figures: 11
  Number of Tables: 9
  Number of authors: 3
  Paper title: 'GOT-10k: A Large High-Diversity Benchmark for Generic Object Tracking
    in the Wild'
  Publication Date: 2019-12-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of GOT-10k Against Other Visual Tracking and Video
      Object Detection Datasets in Terms of Scale, Diversity, Experiment Setting,
      and Other Properties
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Attribute Annotations of Different Datasets
  Table 3 caption:
    table_text: TABLE 3 Quality Control Pipeline for Video Collection
  Table 4 caption:
    table_text: TABLE 4 Quality Control Pipeline for Trajectory Annotation
  Table 5 caption:
    table_text: TABLE 5 Overall Tracking Results of Baseline Trackers on GOT-10k
  Table 6 caption:
    table_text: TABLE 6 Comparison of the Performance of Deep Trackers on Seen and
      Unseen Object Classes
  Table 7 caption:
    table_text: TABLE 7 Comparison of the Tracking Performance of Deep Trackers Trained
      on Class-Balanced and Class-Imbalanced Data
  Table 8 caption:
    table_text: TABLE 8 Comparison of the Evaluation Stability on Balanced and Imbalanced
      Test Data, Assessed by the Standard Deviation of the Ranking of 25 Baseline
      Trackers
  Table 9 caption:
    table_text: TABLE 9 Cross Dataset Evaluation of Deep Trackers
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2957464
- Affiliation of the first author: apple inc., cupertino, ca, usa
  Affiliation of the last author: department of electrical and computer engineering,
    umiacs, university of maryland, college park, md, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Deep_Regionlets_Blended_Representation_and_Deep_Learning_for_Generic_Object_Dete\figure_1.jpg
  Figure 1 caption: Architecture of the Deep Regionlets detection approach. It consists
    of a Region Selection Network (RSN) and a deep regionlet learning module. A region
    is one part of the detection window. The region selection network performs non-rectangular
    region selection from the detection window proposal generated by the object proposal
    network. The deep regionlet learning module learns the feature selection schema
    (corresponding to functionality of the original regionlet) through a spatial transformation
    network and a gating network. The entire pipeline is end-to-end trainable. For
    clarity of visualization, object proposal network is not displayed here.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Deep_Regionlets_Blended_Representation_and_Deep_Learning_for_Generic_Object_Dete\figure_2.jpg
  Figure 2 caption: Illustration of structural relationships among the detection bounding
    box, feature extraction regions and regionlets. The yellow box is a detection
    bounding box and R is a feature extraction region shown as a purple rectangle
    with filled dots inside the bounding box. Inside R , two small sub-regions denoted
    as r 1 and r 2 are from regionlets.
  Figure 3 Link: articels_figures_by_rev_year\2019\Deep_Regionlets_Blended_Representation_and_Deep_Learning_for_Generic_Object_Dete\figure_3.jpg
  Figure 3 caption: "Initialization of one set of projective transformation parameters\
    \ and affine transformation parameters. Normalized projective transformation parameters\
    \ \u0398 0 =[ 1 3 ,0,\u2212 2 3 ;0, 1 3 , 2 3 ;0,0,1] ( \u03B8 i \u2208[\u2212\
    1,1] ) and affine transformation parameters \u0398 \u2217 0 =[ 1 3 ,0,\u2212 2\
    \ 3 ;0, 1 3 , 2 3 ] ( \u03B8 \u2217 i \u2208[\u22121,1] ) selects the top-left\
    \ region in the 3\xD73 evenly divided detection bounding box, shown as the purple\
    \ rectangle."
  Figure 4 Link: articels_figures_by_rev_year\2019\Deep_Regionlets_Blended_Representation_and_Deep_Learning_for_Generic_Object_Dete\figure_4.jpg
  Figure 4 caption: Design of the gating network. f denotes the non-negative gate
    function (i.e., sigmoid).
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.62
  Name of the first author: Hongyu Xu
  Name of the last author: Rama Chellappa
  Number of Figures: 4
  Number of Tables: 8
  Number of authors: 6
  Paper title: 'Deep Regionlets: Blended Representation and Deep Learning for Generic
    Object Detection'
  Publication Date: 2019-12-05 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Ablation Study on the Improvement of the Proposed Deep Regionlets
      Method Over Traditional Regionlets [66], [67] and its Extension DNP [80]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Ablation Study of Each Component in Deep Regionlet Approach
  Table 3 caption:
    table_text: TABLE 3 Results of Ablation Studies When a RSN Selects Different Number
      of Regions and Regionlets are Learned at Different Level of Density
  Table 4 caption:
    table_text: TABLE 4 Detection Results on PASCAL VOC2007 Using VGG16 as Backbone
      Architecture
  Table 5 caption:
    table_text: TABLE 5 Detection Results on PASCAL VOC2007 Test Set
  Table 6 caption:
    table_text: TABLE 6 Complete Object Detection Results on PASCAL VOC 2007 test
      Set for Each Object Category
  Table 7 caption:
    table_text: "TABLE 7 Detection Results on VOC2012 test Set Using Training Data\
      \ \u201C07++12\u201D: The Union Set of 2007 trainvaltest and 2012 trainval."
  Table 8 caption:
    table_text: TABLE 8 Object Detection Results on MS COCO 2017 test-dev using ResNet-101
      [24] as Backbone Acchitecture
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2957780
- Affiliation of the first author: bielefeld university, bielefeld, germany
  Affiliation of the last author: faculty of information systems and applied computer
    science, university of bamberg, bamberg, germany
  Figure 1 Link: articels_figures_by_rev_year\2019\Automatic_Detection_of_Pain_from_Facial_Expressions_A_Survey\figure_1.jpg
  Figure 1 caption: General steps involved in developing an automatic pain detection
    system based on facial expressions. The boxes marked in gray highlight the elements
    that are covered in detail in this survey.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.99
  Name of the first author: Teena Hassan
  Name of the last author: Ute Schmid
  Number of Figures: 1
  Number of Tables: 7
  Number of authors: 7
  Paper title: 'Automatic Detection of Pain from Facial Expressions: A Survey'
  Publication Date: 2019-12-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of Datasets Containing Facial Expressions of Pain
      That are Available upon Request via Email to First Author or through a Website
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Summary of Datasets Whose Availability is Unknown at the Time
      of Writing This Survey
  Table 3 caption:
    table_text: TABLE 3 Summary of the Learning Approaches That Have Been Developed
      and Tested for Automatic Pain Detection from Facial Expressions
  Table 4 caption:
    table_text: TABLE 4 Summary of Spatial Representations Extracted Directly from
      Facial Images for Automatic Pain Detection
  Table 5 caption:
    table_text: TABLE 5 Summary of Spatiotemporal and Mixed Representations Extracted
      Directly from Facial Images for Automatic Pain Detection
  Table 6 caption:
    table_text: TABLE 6 Summary of Indirect Representations of Facial Images and Image
      Sequences Used in Two-Step Automatic Pain Detection Approaches Either Alone
      or in Combination with Direct Representations
  Table 7 caption:
    table_text: TABLE 7 Summary of Machine Learning Methods Used in the Automatic
      Pain Detection Approaches
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2958341
- Affiliation of the first author: school of computer science, wuhan university, wuhan,
    hubei, china
  Affiliation of the last author: university of oxford,, oxford, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2019\Learning_Regional_Attraction_for_Line_Segment_Detection\figure_1.jpg
  Figure 1 caption: Illustrative examples of different methods on an image (a). (b)
    shows our detected line segments. (c) and (d) present the locally estimated edge
    map and the result of MCMLSD [3]. (e) and (f) present the gradient magnitude and
    the result of LSD [2]. (g) and (h) display the deep edge map and the result of
    Deep Wireframe Parser (DWP) [12]. The rightmost column shows the close-up (in
    red) of detection results by different methods, which highlights the better accuracy
    of our proposed method. Best viewed in color.
  Figure 10 Link: articels_figures_by_rev_year\2019\Learning_Regional_Attraction_for_Line_Segment_Detection\figure_10.jpg
  Figure 10 caption: Visualized interpretation of the learned network. The top row
    displays some examples of images and the bottom row displays corresponding saliency
    maps obtained by Guided Backpropagation [53].
  Figure 2 Link: articels_figures_by_rev_year\2019\Learning_Regional_Attraction_for_Line_Segment_Detection\figure_2.jpg
  Figure 2 caption: An illustration of the proposed regional attraction and line segment
    detection system. In the training phase, the annotated line segments of an image
    are equivalently represented by an attraction field map (AFM). Then, the image
    and corresponding AFM are feed into the encoder-decoder network for learning.
    In the inference phase, a testing image is passed into the trained network to
    obtain the AFM prediction. After removing the outliers and squeezing the predictions,
    the system outputs a set of line segments.
  Figure 3 Link: articels_figures_by_rev_year\2019\Learning_Regional_Attraction_for_Line_Segment_Detection\figure_3.jpg
  Figure 3 caption: A toy example illustrating a line segment map with 3 line segments,
    including (a) the region-partition map with 3 regions, (b) selected attraction
    vectors, and (c) the squeeze module for obtaining line segments.
  Figure 4 Link: articels_figures_by_rev_year\2019\Learning_Regional_Attraction_for_Line_Segment_Detection\figure_4.jpg
  Figure 4 caption: Verification of the duality between line segment maps and attraction
    field maps, and its scale invariance.
  Figure 5 Link: articels_figures_by_rev_year\2019\Learning_Regional_Attraction_for_Line_Segment_Detection\figure_5.jpg
  Figure 5 caption: Distribution of magnitudes for the size-normalized attraction
    vectors in the training split of the Wireframe dataset.
  Figure 6 Link: articels_figures_by_rev_year\2019\Learning_Regional_Attraction_for_Line_Segment_Detection\figure_6.jpg
  Figure 6 caption: The PR curves of different line segment detection methods on the
    Wireframe dataset [12] and YorkUrban dataset [4].
  Figure 7 Link: articels_figures_by_rev_year\2019\Learning_Regional_Attraction_for_Line_Segment_Detection\figure_7.jpg
  Figure 7 caption: 'Some results of line segment detection of different approaches
    on the Wireframe [12] dataset. From top to bottom: LSD [2], MCMLSD [3], Linelet
    [18], DWP [12], AFM [33] with the a-trous Residual U-Net and AFM++ proposed in
    this paper.'
  Figure 8 Link: articels_figures_by_rev_year\2019\Learning_Regional_Attraction_for_Line_Segment_Detection\figure_8.jpg
  Figure 8 caption: 'Some results of line segment detection of different approaches
    on the YorkUrban [4] datasets. From top to bottom: LSD [2], MCMLSD [3], Linelet
    [18], DWP [12], AFM [33] with the a-trous Residual U-Net and AFM++ proposed in
    this paper.'
  Figure 9 Link: articels_figures_by_rev_year\2019\Learning_Regional_Attraction_for_Line_Segment_Detection\figure_9.jpg
  Figure 9 caption: Line segments detected on images of different resolutions.
  First author gender probability: 0.94
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Nan Xue
  Name of the last author: Philip H.S. Torr
  Number of Figures: 10
  Number of Tables: 3
  Number of authors: 7
  Paper title: Learning Regional Attraction for Line Segment Detection
  Publication Date: 2019-12-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Network Architectures We Use for Attraction Field Learning
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of the F-measure With the State-of-the-Art Methods
      on the Wireframe and YorkUrban Datasets
  Table 3 caption:
    table_text: TABLE 3 Performance Change by Increasing Image Resolution, Using a
      Better Optimization Method and Adding the Outlier Removal Module
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2958642
- Affiliation of the first author: korea advanced institute of science and technology
    (kaist), yuseong-gu, daejeon, south korea
  Affiliation of the last author: korea advanced institute of science and technology
    (kaist), yuseong-gu, daejeon, south korea
  Figure 1 Link: articels_figures_by_rev_year\2019\Recurrent_Temporal_Aggregation_Framework_for_Deep_Video_Inpainting\figure_1.jpg
  Figure 1 caption: Overview of the proposed meta-architecture. The skip connections,
    recurrent feedback, and the used objective functions are denoted by gray, red,
    and black arrows, respectively. During inference, we apply the model in an auto-regressive
    manner to obtain output sequences.
  Figure 10 Link: articels_figures_by_rev_year\2019\Recurrent_Temporal_Aggregation_Framework_for_Deep_Video_Inpainting\figure_10.jpg
  Figure 10 caption: Comparison with state-of-the-art methods. Input video with mask
    boundaries in red (row-1). Video inpainting by frame-by-frame image inpainting
    [5] (row-2), CombCN [9] (row-3), optimization-based method [8] (row-4), and the
    results by our VINet (row-5). Best viewed when zoomed-in.
  Figure 2 Link: articels_figures_by_rev_year\2019\Recurrent_Temporal_Aggregation_Framework_for_Deep_Video_Inpainting\figure_2.jpg
  Figure 2 caption: 'Overview of BVDNet: Blind video decaptioning network. We propose
    a hybrid encoder-decoder model, where the aggregation encoder stream takes multiple
    input frames and the decoder reconstructs the middle frame. The temporal-pooling
    skip connections carry low-level information. By a residual learning algorithm,
    our model directly learns to recover the corrupted pixels in the input. The output
    is then fed into a feedback connection for a recurrent learning to the next time
    step.'
  Figure 3 Link: articels_figures_by_rev_year\2019\Recurrent_Temporal_Aggregation_Framework_for_Deep_Video_Inpainting\figure_3.jpg
  Figure 3 caption: "Overview of VINet. Our proposed VINet takes in multiple input\
    \ frames ( X t\u22126 , X t\u22123 , X t , X t+3 , X t+6 ) and the previously\
    \ generated frame ( Y t\u22121 ), and generates the inpainted frame ( Y t ) as\
    \ well as the flow map ( W t\u21D2t\u22121 ). We employ both flow sub-networks\
    \ and mask sub-networks at 4 scales (18, 14, 12, and 1) to aggregate and synthesize\
    \ feature points progressively. For temporal consistency, we use a recurrent feedback\
    \ along with two losses: Flow loss and warp loss. The orange arrows denote the\
    \ \xD72 upsampling for residual flow learning as in [29] for 5 reference streams,\
    \ while the thinner orange arrow denotes only the stream from Y t\u22121 . The\
    \ mask sub-networks are omitted in the figure for the simplicity."
  Figure 4 Link: articels_figures_by_rev_year\2019\Recurrent_Temporal_Aggregation_Framework_for_Deep_Video_Inpainting\figure_4.jpg
  Figure 4 caption: 'The impact of each loss terms. (a) An input center frame. (b-d)
    The reconstructed frames with: (b) L 1 loss, (c) L 1 + L grad. loss (d) L 1 ++
    L grad. + L SSIM loss (e) Ground truth frame. Best viewed when zoomed-in.'
  Figure 5 Link: articels_figures_by_rev_year\2019\Recurrent_Temporal_Aggregation_Framework_for_Deep_Video_Inpainting\figure_5.jpg
  Figure 5 caption: The impact of recurrence on temporal consistency. For each sample,
    we visualize four consecutive input frames in the top row. In the bottom rows
    are the zoomed-in views of our results without recurrence (2nd row), with recurrence
    (3rd row), and the ground truth frames (4th row). Without the recurrence, the
    change in the subtitles leads to temporally flickering artifacts (a).
  Figure 6 Link: articels_figures_by_rev_year\2019\Recurrent_Temporal_Aggregation_Framework_for_Deep_Video_Inpainting\figure_6.jpg
  Figure 6 caption: "Visualization of learned feature activation. E and D denote the\
    \ encoder and decoder layers, respectively. For the visualization, we average\
    \ each feature maps along the channel axis, and up-sample to 128\xD7128 pixel.\
    \ The fractional numbers denote the spatial resolutions. We observe hierarchical\
    \ attention operations across layers. In the early encoder layers (b, c), low-level\
    \ features such as background textures (e.g., around the subtitles) are aggregated\
    \ along the time dimension. The latter decoder layers (d, e) then gradually focus\
    \ on the exact target regions."
  Figure 7 Link: articels_figures_by_rev_year\2019\Recurrent_Temporal_Aggregation_Framework_for_Deep_Video_Inpainting\figure_7.jpg
  Figure 7 caption: Qualitative decaptioning results. For each example, the top rows
    are the input sequences and the bottom rows are the decaptioning results using
    our full model. For visualization, we determine the time interval between the
    frames to be 0.1 seconds. Our model performs well on various types of subtitles
    with complex background variations and also is able to separate the non-caption
    texts in a video.
  Figure 8 Link: articels_figures_by_rev_year\2019\Recurrent_Temporal_Aggregation_Framework_for_Deep_Video_Inpainting\figure_8.jpg
  Figure 8 caption: User study on our BVDNet results versus ground truth.
  Figure 9 Link: articels_figures_by_rev_year\2019\Recurrent_Temporal_Aggregation_Framework_for_Deep_Video_Inpainting\figure_9.jpg
  Figure 9 caption: Visualization of the learned feature composition. Input frames
    are on the odd rows, and corresponding feature flows referential to the middle
    stream, and the inpainting results are on the even rows. VINet successfully aligns
    and integrates the reference features onto the target frame to fill in the large
    and complex hole region.
  First author gender probability: 0.7
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.88
  Name of the first author: Dahun Kim
  Name of the last author: In So Kweon
  Number of Figures: 12
  Number of Tables: 7
  Number of authors: 4
  Paper title: Recurrent Temporal Aggregation Framework for Deep Video Inpainting
  Publication Date: 2019-12-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Ablation Studies on Architectural Design, Loss Functions,
      and Recurrence Stream
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Ablation Studies on Additional GAN Loss and Without Residual
      Learning
  Table 3 caption:
    table_text: 'TABLE 3 The Ablation Studies on the Hyperparamter: Number of Input
      Frames'
  Table 4 caption:
    table_text: TABLE 4 Temporal Errors (Warping Errors) of BVDNet With and Without
      the Temporal Consistency Constraint
  Table 5 caption:
    table_text: TABLE 5 Final Performances of the Top Entries in the ECCV ChaLearn
      2018 LAP Inpainting Challenge Track2 Test Phase
  Table 6 caption:
    table_text: TABLE 6 Flow Warping Errors
  Table 7 caption:
    table_text: TABLE 7 FID Scores
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2958083
- Affiliation of the first author: national engineering laboratory for integrated
    aero-space-ground-ocean big data application technology, school of computer science,
    northwestern polytechnical university, xian, china
  Affiliation of the last author: national engineering laboratory for integrated aero-space-ground-ocean
    big data application technology, school of computer science, northwestern polytechnical
    university, xian, china
  Figure 1 Link: articels_figures_by_rev_year\2019\A_Performance_Evaluation_of_Correspondence_Grouping_Methods_for_D_Rigid_Data_Mat\figure_1.jpg
  Figure 1 caption: Illustration of local feature-based matching paradigm, where the
    objective of 3D correspondence grouping is to search inliers from an initial correspondence
    set with outliers between two shapes.
  Figure 10 Link: articels_figures_by_rev_year\2019\A_Performance_Evaluation_of_Correspondence_Grouping_Methods_for_D_Rigid_Data_Mat\figure_10.jpg
  Figure 10 caption: Computational efficiency with regards to different sizes of the
    initial correspondence set.
  Figure 2 Link: articels_figures_by_rev_year\2019\A_Performance_Evaluation_of_Correspondence_Grouping_Methods_for_D_Rigid_Data_Mat\figure_2.jpg
  Figure 2 caption: Sample views (visualized in mesh representation) from (a) B3R
    [17], (b) U3OR [25], [48], and (c) U3M [14] datasets.
  Figure 3 Link: articels_figures_by_rev_year\2019\A_Performance_Evaluation_of_Correspondence_Grouping_Methods_for_D_Rigid_Data_Mat\figure_3.jpg
  Figure 3 caption: 'Spatial distributions of corresponding keypoints (detector: H3D
    [9]; descriptor: SHOT [11]) on two example matching pairs for each dataset. Green
    and red dots represent keypoints with correct and false correspondences, respectively.'
  Figure 4 Link: articels_figures_by_rev_year\2019\A_Performance_Evaluation_of_Correspondence_Grouping_Methods_for_D_Rigid_Data_Mat\figure_4.jpg
  Figure 4 caption: "Information in terms of inlier ratio and the number of inliers\
    \ for the initial correspondences when confronted with different challenges. Note\
    \ that the effects of (a) noise and (b) density variation are examined on the\
    \ B3R dataset; the effects of (c) clutter, (d) occlusion, (f) varying threshold\
    \ \u03F5 , and (g) varying the number of inilier matches are tested on the U3OR\
    \ dataset; the effects of (e) partial overlap are tested on the U3M dataset; the\
    \ effects of varying detector-descriptor combinations are evaluated on all datasets.\
    \ Details about the generation of the initial correspondence set in each challenge\
    \ case are presented in Section 4.3."
  Figure 5 Link: articels_figures_by_rev_year\2019\A_Performance_Evaluation_of_Correspondence_Grouping_Methods_for_D_Rigid_Data_Mat\figure_5.jpg
  Figure 5 caption: Comparison of (left) a clean rigid data and its (middle) noisy
    and (right) down-sampled copies. The standard deviation of noise is 0.5 pr and
    the down-sampling ratio is 0.7.
  Figure 6 Link: articels_figures_by_rev_year\2019\A_Performance_Evaluation_of_Correspondence_Grouping_Methods_for_D_Rigid_Data_Mat\figure_6.jpg
  Figure 6 caption: Precision, recall, and F-score performance of selected methods
    in Section 4 on experimental datasets with different challenges (Section 4.3).
  Figure 7 Link: articels_figures_by_rev_year\2019\A_Performance_Evaluation_of_Correspondence_Grouping_Methods_for_D_Rigid_Data_Mat\figure_7.jpg
  Figure 7 caption: Visual results of the grouped inlier sets by evaluated methods
    on sample data from the experimental datasets.
  Figure 8 Link: articels_figures_by_rev_year\2019\A_Performance_Evaluation_of_Correspondence_Grouping_Methods_for_D_Rigid_Data_Mat\figure_8.jpg
  Figure 8 caption: Rigid registration performance of selected methods in Section
    4 regarding various application scenarios and challenges (Section 4.3).
  Figure 9 Link: articels_figures_by_rev_year\2019\A_Performance_Evaluation_of_Correspondence_Grouping_Methods_for_D_Rigid_Data_Mat\figure_9.jpg
  Figure 9 caption: Precision results of GC and RANSAC on each matching pair from
    the U3OR dataset.
  First author gender probability: 0.66
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.63
  Name of the first author: Jiaqi Yang
  Name of the last author: Yanning Zhang
  Number of Figures: 10
  Number of Tables: 2
  Number of authors: 4
  Paper title: A Performance Evaluation of Correspondence Grouping Methods for 3D
    Rigid Data Matching
  Publication Date: 2019-12-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Parameters Used Through the Evaluation
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Computational Complexity of Evaluated Methods
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2960234
- Affiliation of the first author: computer science department, university of freiburg,
    freiburg im breisgau, germany
  Affiliation of the last author: computer science department, university of freiburg,
    freiburg im breisgau, germany
  Figure 1 Link: articels_figures_by_rev_year\2019\SemiSupervised_Semantic_Segmentation_With_High_and_LowLevel_Consistency\figure_1.jpg
  Figure 1 caption: An image from the PASCAL VOC dataset (a) and its ground-truth
    segmentation mask (b). Prediction (c) is obtained with supervised training on
    5% labeled samples. Using the other 95% unlabeled images, our GAN-based branch
    improves the shape estimation (d). The second branch adds high-level consistency
    by removing false positives (e). (f) shows the output when training on 100% pixel-wise
    labeled samples.
  Figure 10 Link: articels_figures_by_rev_year\2019\SemiSupervised_Semantic_Segmentation_With_High_and_LowLevel_Consistency\figure_10.jpg
  Figure 10 caption: Qualitative results on the Cityscapes dataset using 18 labeled
    samples. Failures of our approach. We compare our (Ours) results with the fully-supervised
    baseline (Base) which is trained only on the labeled subset of data.
  Figure 2 Link: articels_figures_by_rev_year\2019\SemiSupervised_Semantic_Segmentation_With_High_and_LowLevel_Consistency\figure_2.jpg
  Figure 2 caption: 'Semi-supervised Semantic Segmentation: The proposed semi-supervised
    learning (SSL) approach improves over the baselines when only little labeled data
    is available using unlabeled data, especially with less than 5% labeled samples.
    Performance is shown on the PASCAL VOC dataset without (a) and with (b) COCO pre-training.'
  Figure 3 Link: articels_figures_by_rev_year\2019\SemiSupervised_Semantic_Segmentation_With_High_and_LowLevel_Consistency\figure_3.jpg
  Figure 3 caption: Overview of our proposed semi-supervised segmentation approach.
    The s4GAN branch is a GAN-based model which improves the low-level details in
    the segmentation prediction. The MLMT branch performs semi-supervised multi-label
    classification to exploit class-level information for removing false-positive
    predictions from the segmentation map.
  Figure 4 Link: articels_figures_by_rev_year\2019\SemiSupervised_Semantic_Segmentation_With_High_and_LowLevel_Consistency\figure_4.jpg
  Figure 4 caption: Qualitative results obtained using our semi-supervised segmentation
    approach on the PASCAL VOC dataset with 5% labeled data without COCO pre-training.
  Figure 5 Link: articels_figures_by_rev_year\2019\SemiSupervised_Semantic_Segmentation_With_High_and_LowLevel_Consistency\figure_5.jpg
  Figure 5 caption: Qualitative results on the PASCAL-Context dataset using 18 labeled
    samples. Our approach produces improved results compared to the baseline. We compare
    our (Ours) results with the fully-supervised baseline which is trained only on
    the labeled subset of data.
  Figure 6 Link: articels_figures_by_rev_year\2019\SemiSupervised_Semantic_Segmentation_With_High_and_LowLevel_Consistency\figure_6.jpg
  Figure 6 caption: Qualitative results on the Cityscapes dataset using 18 labeled
    samples without COCO pre-training. The proposed semi-supervised approach produces
    improved results compared to the baseline. We compare our (Ours) results with
    the fully-supervised baseline (Base) which is trained only on the labeled subset
    of data.
  Figure 7 Link: articels_figures_by_rev_year\2019\SemiSupervised_Semantic_Segmentation_With_High_and_LowLevel_Consistency\figure_7.jpg
  Figure 7 caption: Ablation study on the PASCAL VOC dataset showing the contribution
    of the MLMT (d) and the s4GAN (e) branches individually. The s4GAN and the MLMT
    branches together show a complementary behaviour fixing both low and high-level
    artifacts (g). These results are obtained using 5% labeled data.
  Figure 8 Link: articels_figures_by_rev_year\2019\SemiSupervised_Semantic_Segmentation_With_High_and_LowLevel_Consistency\figure_8.jpg
  Figure 8 caption: Failure cases. Sometimes, our approach can lead to under-segmentation
    of objects with multiple protrusions, shown in the top row. Bottom row shows a
    case where an ambiguous foreground object is falsely marked as one of the classes.
  Figure 9 Link: articels_figures_by_rev_year\2019\SemiSupervised_Semantic_Segmentation_With_High_and_LowLevel_Consistency\figure_9.jpg
  Figure 9 caption: Qualitative results on the PASCAL-Context dataset using 18 labeled
    samples. Failure of our approach. We compare our (Ours) results with the fully-supervised
    baseline which is trained only on the labeled subset of data.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Sudhanshu Mittal
  Name of the last author: Thomas Brox
  Number of Figures: 12
  Number of Tables: 7
  Number of authors: 3
  Paper title: Semi-Supervised Semantic Segmentation With High- and Low-Level Consistency
  Publication Date: 2019-12-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Semi-Supervised Semantic Segmentation Results on the PASCAL
      VOC Dataset Without and With COCO Pre-Training
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results on PASCAL VOC without COCO Pre-Training Using Different
      Backbone Architectures
  Table 3 caption:
    table_text: TABLE 3 Semi-Supervised Semantic Segmentation Results on the PASCAL-Context
      Dataset Without COCO Pre-Training
  Table 4 caption:
    table_text: TABLE 4 Semi-Supervised Semantic Segmentation Results on the Cityscapes
      Dataset Without COCO Pre-Training
  Table 5 caption:
    table_text: TABLE 5 Ablation Study of the Contribution of Each Branch
  Table 6 caption:
    table_text: TABLE 6 Ablation Study of Different GAN Loss Terms for the Generator
      on the PASCAL VOC Dataset
  Table 7 caption:
    table_text: TABLE 7 Semi-Supervised Semantic Segmentation Results on the PASCAL
      VOC Dataset Using Extra Weak Image-Level Annotations
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2960224
- Affiliation of the first author: department of diagnostic imaging, st. jude childrens
    research hospital, memphis, tn, usa
  Affiliation of the last author: department of computer and information science,
    university of mississippi, university, ms, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Estimating_FeatureLabel_Dependence_Using_Gini_Distance_Statistics\figure_1.jpg
  Figure 1 caption: "Estimates of the generalized Gini distance covariance and generalized\
    \ Gini distance correlation for different kernel parameters using 2000 iid samples:\
    \ (a) independent case; (b) dependent case. Three critical values are shown in\
    \ (b). They are calculated for significance levels 0.01, 0.05, and 0.15, respectively.\
    \ In terms of the uniform convergence bounds, the optimal value of the kernel\
    \ parameter \u03C3 is defined by the minimizer (or maximizer) of the test statistics\
    \ under H 0 (or H 1 )."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Estimating_FeatureLabel_Dependence_Using_Gini_Distance_Statistics\figure_2.jpg
  Figure 2 caption: "Permutation tests of the generalized Gini distance covariance\
    \ and generalized Gini distance correlation for different kernel parameters using\
    \ 200 iid samples and 5000 random permutations: (a) independent case; (b) dependent\
    \ case. The 95 percentile curves define the critical values for \u03B1=0.05 .\
    \ They are calculated from the permuted data. Test statistics higher (lower) than\
    \ the critical value suggest accepting H 1 ( H 0 )."
  Figure 3 Link: articels_figures_by_rev_year\2019\Estimating_FeatureLabel_Dependence_Using_Gini_Distance_Statistics\figure_3.jpg
  Figure 3 caption: 'Simulation results using normal distribution with K = 3: (a)
    Type I error; (b) Type II error.'
  Figure 4 Link: articels_figures_by_rev_year\2019\Estimating_FeatureLabel_Dependence_Using_Gini_Distance_Statistics\figure_4.jpg
  Figure 4 caption: 'The MNIST dataset. (a) Test accuracy using the top k selected
    features. (b) Test statistics of features in descending order. (c) Visualization
    of the top k pixels selected. White: selected. Black: not selected. Test accuracy
    using the selected pixels is labeled on the top of each image.'
  Figure 5 Link: articels_figures_by_rev_year\2019\Estimating_FeatureLabel_Dependence_Using_Gini_Distance_Statistics\figure_5.jpg
  Figure 5 caption: The breast cancer dataset. (a) Test accuracy using the top k selected
    genes. (b) Number of PAM50 genes in the selected top k genes.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.64
  Name of the first author: Silu Zhang
  Name of the last author: Yixin Chen
  Number of Figures: 5
  Number of Tables: 5
  Number of authors: 5
  Paper title: Estimating Feature-Label Dependence Using Gini Distance Statistics
  Publication Date: 2019-12-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of Related Work on Feature Relevance
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Models of Different Distribution Families
  Table 3 caption:
    table_text: "TABLE 3 Power ( \u03B1=0.05 \u03B1=0.05) and AUC"
  Table 4 caption:
    table_text: TABLE 4 Data set summary
  Table 5 caption:
    table_text: TABLE 5 Classification Accuracies Using Top k Dependent Features
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2960358
