- Affiliation of the first author: school of computer science and engineering, nanjing
    university of science and technology, nanjing, china
  Affiliation of the last author: university of california, irvine, ca
  Figure 1 Link: articels_figures_by_rev_year\2016\TriClustered_Tensor_Completion_for_SocialAware_Image_Tag_Refinement\figure_1.jpg
  Figure 1 caption: Exemplary images from Flickr with the associated tags and users.
    The user information can assist the image tag refinement. The tags (red font)
    are the completed tags after the refinement.
  Figure 10 Link: articels_figures_by_rev_year\2016\TriClustered_Tensor_Completion_for_SocialAware_Image_Tag_Refinement\figure_10.jpg
  Figure 10 caption: Average performance of different methods for tag refinement.
  Figure 2 Link: articels_figures_by_rev_year\2016\TriClustered_Tensor_Completion_for_SocialAware_Image_Tag_Refinement\figure_2.jpg
  Figure 2 caption: Illustration of image groups. Although users generally upload
    images with multiple interests, the visual difference can obviously cluster them
    into serval groups.
  Figure 3 Link: articels_figures_by_rev_year\2016\TriClustered_Tensor_Completion_for_SocialAware_Image_Tag_Refinement\figure_3.jpg
  Figure 3 caption: 'The framework of tri-clustering Tensor Completion for social-aware
    image tag refinement. (a) Relation discovery: find the intra-relations among the
    homogeneous data; (b) Tri-clustering and sub-tensor selection: divide the original
    tensor into sub-tensors via tri-clustering, which can simultaneously cluster the
    users, images and tags, and select the denser sub-tensors; and (c) sub-tensors
    completion and tag refinement.'
  Figure 4 Link: articels_figures_by_rev_year\2016\TriClustered_Tensor_Completion_for_SocialAware_Image_Tag_Refinement\figure_4.jpg
  Figure 4 caption: The statistics on the real-world NUS-WIDE-USER dataset.
  Figure 5 Link: articels_figures_by_rev_year\2016\TriClustered_Tensor_Completion_for_SocialAware_Image_Tag_Refinement\figure_5.jpg
  Figure 5 caption: Visualization for the proposed tri-clustering and the existing
    methods [39] , [40]. The proposed tri-clustering is end-to-end.
  Figure 6 Link: articels_figures_by_rev_year\2016\TriClustered_Tensor_Completion_for_SocialAware_Image_Tag_Refinement\figure_6.jpg
  Figure 6 caption: Performance in terms of F-score by varying the cluster numbers
    I I , I T and I U .
  Figure 7 Link: articels_figures_by_rev_year\2016\TriClustered_Tensor_Completion_for_SocialAware_Image_Tag_Refinement\figure_7.jpg
  Figure 7 caption: Convergence curve of the proposed tri-clustering optimization
    algorithm.
  Figure 8 Link: articels_figures_by_rev_year\2016\TriClustered_Tensor_Completion_for_SocialAware_Image_Tag_Refinement\figure_8.jpg
  Figure 8 caption: Parameter tuning results of parameters beta and lambda .
  Figure 9 Link: articels_figures_by_rev_year\2016\TriClustered_Tensor_Completion_for_SocialAware_Image_Tag_Refinement\figure_9.jpg
  Figure 9 caption: Visualizations of the obtained tri-clusters by the tri-clustering
    method. The data surrounded by the same solid line box belong to the same sub-tensor.
    Here, we list no more than 10 tags for each image and no more than nine images
    for each tri-cluster due to the space limitation.
  First author gender probability: 0.93
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jinhui Tang
  Name of the last author: Ramesh Jain
  Number of Figures: 14
  Number of Tables: 1
  Number of authors: 7
  Paper title: Tri-Clustered Tensor Completion for Social-Aware Image Tag Refinement
  Publication Date: 2016-09-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 NUS-WIDE-USER Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2608882
- Affiliation of the first author: centre for quantum computation and intelligent
    systems, university of technology sydney, sydney, nsw, australia
  Affiliation of the last author: machine learning department, carnegie mellon university,
    pittsburgh, pa
  Figure 1 Link: articels_figures_by_rev_year\2016\Semantic_Pooling_for_Complex_Event_Analysis_in_Untrimmed_Videos\figure_1.jpg
  Figure 1 caption: "Two Internet video examples, where the same event \u201CRock\
    \ Climbing\u201D happened in very different time frames. The number in each frame\
    \ indicates its saliency score, which describes how this keyframe is relevant\
    \ to the specified event. We use this saliency information to prioritize the video\
    \ shot representations."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Semantic_Pooling_for_Complex_Event_Analysis_in_Untrimmed_Videos\figure_2.jpg
  Figure 2 caption: Each input video is divided into multiple shots, and each event
    has a short textual description. CNN is used to extract features (Section 3.1).
    ImageNet concept names and skip-gram model are used to derive a probability vector
    (Section 3.2) and a relevance vector (Section 3.3), which are combined to yield
    the new semantic saliency and used for prioritizing shots in the classifier training
    (Section 3.4).
  Figure 3 Link: articels_figures_by_rev_year\2016\Semantic_Pooling_for_Complex_Event_Analysis_in_Untrimmed_Videos\figure_3.jpg
  Figure 3 caption: "Qualitative analysis of the prioritization effect. A positive\
    \ test video from event \u201CHorse Riding Competition\u201D is used as an example.\
    \ The first row shows the original video shots; the second row depicts prioritized\
    \ video shots, having its weight (in norm) on the bottom left and semantic saliency\
    \ on the bottom right; and the third row presents the most salient concepts detected\
    \ in these shots."
  Figure 4 Link: articels_figures_by_rev_year\2016\Semantic_Pooling_for_Complex_Event_Analysis_in_Untrimmed_Videos\figure_4.jpg
  Figure 4 caption: "Performance sensitivity w.r.t. \u03BB , \u03B3 and initialization\
    \ on the TRECVID MEDTest 2014 dataset."
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.85
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xiaojun Chang
  Name of the last author: Eric P. Xing
  Number of Figures: 4
  Number of Tables: 9
  Number of authors: 4
  Paper title: Semantic Pooling for Complex Event Analysis in Untrimmed Videos
  Publication Date: 2016-09-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance (mAP) w.r.t. Different Configurations on the TRECVID
      MEDTest2014 (100Ex), MEDTest2013 (100Ex) and CCVsub Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance (mAP) w.r.t. Different Configurations on the TRECVID
      MEDTest2014 (100Ex), MEDTest2013 (100Ex) and CCVsub Datasets
  Table 3 caption:
    table_text: TABLE 3 mAP Comparison Against State-of-the-Art Alternatives That
      Use a Single Type of Feature on the TRECVID MEDTest 2014, MEDTest 2013 and CCVsub
      Datasets
  Table 4 caption:
    table_text: TABLE 4 mAP Comparison Against State-of-the-Art Systems That Fuse
      Multiple Types of Features on the TRECVID MEDTest 2014 and MEDTest 2013 Datasets
  Table 5 caption:
    table_text: TABLE 5 mAP Comparison of Different Isotonic Regularizers on TRECVID
      MEDTest 2014, MEDTest 2013 and CCVsub
  Table 6 caption:
    table_text: TABLE 6 Comparison Against Ranking SVM (RSVM) [74]
  Table 7 caption:
    table_text: TABLE 7 Video Statistics and Recounting Accuracy
  Table 8 caption:
    table_text: TABLE 8 Event Recounting Results on MED13 and MED14
  Table 9 caption:
    table_text: TABLE 9 Mean F 1 Score (m F 1 ) on MED14, MED13, and CCVsub Datasets
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2608901
- Affiliation of the first author: department of electronic engineering, shanghai
    jiao tong university, shanghai, china
  Affiliation of the last author: microsoft research, redmond, wa
  Figure 1 Link: articels_figures_by_rev_year\2016\A_TubeandDropletBased_Approach_for_Representing_and_Analyzing_Motion_Trajectorie\figure_1.jpg
  Figure 1 caption: '(a) An example of ambiguous trajectories: The orange, red, and
    green curves labeled by C P 1 - C P 3 indicate three trajectory classes. The black
    curves labeled by Trajectory A and B are two trajectories from C P 1 and C P 2
    , respectively. (b) Complete contextual motion patterns of trajectories A and
    B in (a): Contextual motion patterns are described by the motion flows of all
    trajectory points in the neighborhood of A and B (Note: We use color to differentiate
    motion flows from different trajectory classes only for a clearer illustration.
    In our approach, we do not differentiate motion flows'' classes and directly use
    all motion flows in the neighborhood of an input trajectory to model its contextual
    motion pattern). (c) Use our 3D tube to model A and B ''s complete contextual
    motion patterns. (Best viewed in color.)'
  Figure 10 Link: articels_figures_by_rev_year\2016\A_TubeandDropletBased_Approach_for_Representing_and_Analyzing_Motion_Trajectorie\figure_10.jpg
  Figure 10 caption: '(a) Trajectories from different groundtruth clusters in VMT
    dataset (color curves) and five input trajectories (black curves labeled by A
    - E ). (b) Upper: 3D tubes for trajectories A , B , and C in (a); lower: Water
    droplets for A , B , and C . (c) Comparison of water droplet results between clean
    trajectories ( B , C ) and noisy trajectories ( D , E ) in (a) (note that B ,
    D are from one cluster and C , E are from another cluster). (d) Upper: Droplets
    for trajectory A ''s 3D tube in Fig. 10b under different lambda 1 where lambda
    2=0.1 ; lower: Droplets for trajectory A ''s 3D tube in Fig. 10b under different
    lambda 2 where lambda 1=2 . (Best viewed in color.)'
  Figure 2 Link: articels_figures_by_rev_year\2016\A_TubeandDropletBased_Approach_for_Representing_and_Analyzing_Motion_Trajectorie\figure_2.jpg
  Figure 2 caption: The framework of the proposed approach. It constructs a scene-specific
    thermal transfer field via trajectory training data; for a test trajectory sample,
    it builds a 3D tube based on the constructed thermal transfer field, and generates
    a feature vector via a droplet process. The obtained feature vector is applicable
    to various trajectory analysis applications. (Best viewed in color).
  Figure 3 Link: articels_figures_by_rev_year\2016\A_TubeandDropletBased_Approach_for_Representing_and_Analyzing_Motion_Trajectorie\figure_3.jpg
  Figure 3 caption: "(a) Input trajectories (black solid curves) and given trajectories\
    \ (blue dashed curves). (b) Thermal transfer fields constructed from the blue\
    \ dashed curves in (a). Figures from left to right correspond to thermal transfer\
    \ fields in directions upward ( y \u2212 ), downward ( y + ), leftward ( x \u2212\
    \ ), rightward ( x + ), respectively. (Best viewed in color.)"
  Figure 4 Link: articels_figures_by_rev_year\2016\A_TubeandDropletBased_Approach_for_Representing_and_Analyzing_Motion_Trajectorie\figure_4.jpg
  Figure 4 caption: 'Examples of equipotential lines. (a) The process of iterative
    thermal diffusion. (b)-(d) Lower figures: The thermal diffusion maps and the equipotential
    lines for trajectory points p a , p b , and p c in Fig. 3a; upper figures: The
    thermal diffusion maps and the equipotential lines displayed in 3D. (Best viewed
    in color.)'
  Figure 5 Link: articels_figures_by_rev_year\2016\A_TubeandDropletBased_Approach_for_Representing_and_Analyzing_Motion_Trajectorie\figure_5.jpg
  Figure 5 caption: 'Examples of 3D tubes and water droplet results. First row: results
    by expanding trajectories in Fig. 3a into 3D spatio-temporal curves; second row:
    3D tube representations for the black input trajectories A - D in Fig. 3a; third
    row: water droplet results derived from 3D tubes. (Note: In the middle row of
    (a)-(d), the thickness of a tube is represented by different colors where yellow
    indicates thick and red indicates narrow. Best viewed in color.)'
  Figure 6 Link: articels_figures_by_rev_year\2016\A_TubeandDropletBased_Approach_for_Representing_and_Analyzing_Motion_Trajectorie\figure_6.jpg
  Figure 6 caption: "The water droplet process. (a) Illustrations of the input water\
    \ droplet, P b,n , P c,n , \u03B8 b,n , and routes of water droplet points w c\
    \ , w b when passing through a 3D tube. (b) The resulting 3D water droplet (left)\
    \ and the simplified 2D droplet by only considering d t b (right)."
  Figure 7 Link: articels_figures_by_rev_year\2016\A_TubeandDropletBased_Approach_for_Representing_and_Analyzing_Motion_Trajectorie\figure_7.jpg
  Figure 7 caption: 'An example of 3D skeleton sequence from MSR-Action3D dataset.
    Left: The 3D trajectory of ''horizontal wave'' action of a ''left hand'' body
    point. Right: The skeleton sequence for ''horizontal wave'' action.'
  Figure 8 Link: articels_figures_by_rev_year\2016\A_TubeandDropletBased_Approach_for_Representing_and_Analyzing_Motion_Trajectorie\figure_8.jpg
  Figure 8 caption: An example of thermal transfer fields for 3D trajectories. (a)
    Given 3D trajectories. (c)-(h) Constructed thermal transfer fields for the given
    3D trajectories in (a). Note that (c)-(h) correspond to the thermal transfer fields
    in directions y- , x- , z- , y+ , x+ , z+ , respectively. (b) An illustration
    of details inside a 3D thermal transfer field when cutting a high-value volume
    in a transfer field.
  Figure 9 Link: articels_figures_by_rev_year\2016\A_TubeandDropletBased_Approach_for_Representing_and_Analyzing_Motion_Trajectorie\figure_9.jpg
  Figure 9 caption: An example of equipotential surfaces (left) and the resulting
    water droplet sphere (right) for 3D trajectories.
  First author gender probability: 0.64
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.57
  Name of the first author: Weiyao Lin
  Name of the last author: Zicheng Liu
  Number of Figures: 12
  Number of Tables: 5
  Number of authors: 7
  Paper title: A Tube-and-Droplet-Based Approach for Representing and Analyzing Motion
    Trajectories
  Publication Date: 2016-09-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Cluster Learning Accuracy for Different Methods on VMT Dataset
      (%)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Cluster Learning Accuracy with Different Noise or Trajectory
      Break Levels on VMT Dataset (%)
  Table 3 caption:
    table_text: TABLE 3 Running Time of Different Steps for Table 2
  Table 4 caption:
    table_text: TABLE 4 Trajectory Classification and Abnormality Detection Results
      on CROSS (%)
  Table 5 caption:
    table_text: TABLE 5 Recognition Accuracy Comparison on MSR-Action3D Dataset (%)
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2608884
- Affiliation of the first author: intel labs, santa clara, ca
  Affiliation of the last author: korea advanced institute of science and technology,
    daejeon, republic of korea
  Figure 1 Link: articels_figures_by_rev_year\2016\Robust_Multiview_Photometric_Stereo_Using_Planar_Mesh_Parameterization\figure_1.jpg
  Figure 1 caption: Overview of our method. Structure-from-motion is used to calibrate
    cameras and multiview stereo is used to recover a coarse mesh. After parameterizing
    the coarse mesh, multiview photometric stereo and mesh refinement are performed
    in a parameterized 2D space.
  Figure 10 Link: articels_figures_by_rev_year\2016\Robust_Multiview_Photometric_Stereo_Using_Planar_Mesh_Parameterization\figure_10.jpg
  Figure 10 caption: Two imaging setups for real world object capture. (left) Automated
    setup. It consists of a rotation stage, a light array and two cameras. (right)
    Manual setup. It only requires a camera and a light bulb. In both setups, several
    images are captured under varying lighting for a particular camera orientation.
  Figure 2 Link: articels_figures_by_rev_year\2016\Robust_Multiview_Photometric_Stereo_Using_Planar_Mesh_Parameterization\figure_2.jpg
  Figure 2 caption: Example of surface normal map and displacement map estimation.
    (a) Input image. (b) Initial normal map obtained from a base mesh, in U . (c)
    Disambiguated normals from photometric stereo in U . Here, unit 3D vectors have
    been linearly mapped to RGB. (d) Estimated displacement map.
  Figure 3 Link: articels_figures_by_rev_year\2016\Robust_Multiview_Photometric_Stereo_Using_Planar_Mesh_Parameterization\figure_3.jpg
  Figure 3 caption: Illustration of the mapping of pixel intensities to the observation
    matrix O . It shows surface normal can be estimated by solving dense block matrices
    (Section 3.3.1) or by applying factorization-based method (Section 3.3.2).
  Figure 4 Link: articels_figures_by_rev_year\2016\Robust_Multiview_Photometric_Stereo_Using_Planar_Mesh_Parameterization\figure_4.jpg
  Figure 4 caption: "Evaluation of the normal estimation algorithms on the five synthetic\
    \ capture setups (shown in (a)) and the three different BRDFs (shown in (b)).\
    \ A red dot in (a) indicates a point light source. In (c), each group of bar plots\
    \ shows the Accuracy metric ( \xD7 10 \u22123 ) for the three surface normal estimation\
    \ methods. The Factorization with regularization method is most stable and accurate\
    \ in various capture configurations and BRDFs (see Section 5.1.1 for more details).\
    \ These results were obtained with the Bunny model with perturbation level 3 as\
    \ the base mesh. The Accuracy metric of the base mesh is shown by the red line."
  Figure 5 Link: articels_figures_by_rev_year\2016\Robust_Multiview_Photometric_Stereo_Using_Planar_Mesh_Parameterization\figure_5.jpg
  Figure 5 caption: Close-ups of the Bunny model refined by the three surface normal
    estimation methods and Hernandez et al. [7]. The model is rendered with the Alum-bronze
    BRDF and the input images were captured in the Hemi. Dense (HD) light and camera
    configuration shown in Fig. 4.
  Figure 6 Link: articels_figures_by_rev_year\2016\Robust_Multiview_Photometric_Stereo_Using_Planar_Mesh_Parameterization\figure_6.jpg
  Figure 6 caption: Comparisons of the light directions estimated by our approach
    and that of Hernandez et al. [7]. The directional errors for the three synthetic
    datasets are displayed. The spherical heat maps, which is only used by Hernandez
    et al. [7] , indicate the number of inlier points as a function of light direction.
    The center of the circle corresponds to the ground truth light direction. The
    weak consensus of the heat map (particularly in Alum-bronze) indicates weak confidence
    on the estimated light direction because there are many probable light directions
    that have many inlier points. Our estimated light directions (yellow dots) and
    results of Hernandez et al. [7] (green dots) are overlaid on the heat maps for
    comparisons.
  Figure 7 Link: articels_figures_by_rev_year\2016\Robust_Multiview_Photometric_Stereo_Using_Planar_Mesh_Parameterization\figure_7.jpg
  Figure 7 caption: Cumulative error distributions of the Accuracy metric for three
    synthetic dataset; Bunny , Gargoyle, and Happy-Buddha. The graph corresponds to
    the mesh perturbation test in Table 2 when perturbation level is 3. On Bunny and
    Gargoyle, our method is consistently the most accurate. On Happy-Buddha, the accuracy
    is very similar to Hernandez et al. [7].
  Figure 8 Link: articels_figures_by_rev_year\2016\Robust_Multiview_Photometric_Stereo_Using_Planar_Mesh_Parameterization\figure_8.jpg
  Figure 8 caption: Perturbed Happy-Buddha model (denoted as input) and refinement
    results by three methods; Hernandez [7], Nehab [6], and ours. The result corresponds
    to Mesh perturbation test in Table 2 where perturbation level is 3.
  Figure 9 Link: articels_figures_by_rev_year\2016\Robust_Multiview_Photometric_Stereo_Using_Planar_Mesh_Parameterization\figure_9.jpg
  Figure 9 caption: Comparison with Nehab et al. [6] on the computational cost and
    accuracy of the refined mesh. Mesh resolutions, accuracies, and computation times
    are shown under the sub-figures. Our method does not require tuning the mesh resolution
    because it is automatically determined by the input image resolution.
  First author gender probability: 0.94
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.88
  Name of the first author: Jaesik Park
  Name of the last author: In So Kweon
  Number of Figures: 17
  Number of Tables: 2
  Number of authors: 5
  Paper title: Robust Multiview Photometric Stereo Using Planar Mesh Parameterization
  Publication Date: 2016-09-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Our Method with Prior Techniques [6], [7]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison on Synthetic Dataset
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2608944
- Affiliation of the first author: school of information and communication engineering,
    dalian university of technology, dalian, liaoning, china
  Affiliation of the last author: school of engineering, university of california,
    merced, ca
  Figure 1 Link: articels_figures_by_rev_year\2016\Ranking_Saliency\figure_1.jpg
  Figure 1 caption: Main steps of the proposed algorithm. We rank the similarity of
    image elements with foreground or background cues via graph-based manifold ranking.
    In this first stage, the boundary priors are used as query where the relevances
    of regions to each image side are ranked, and then merged to generate a saliency
    map. In the second stage, all regions with the soft labels are used as queries
    for the foreground salient objects. The saliency of each node is computed based
    on its relevance to foreground queries. In the third stage, the saliency probabilistic
    measures computed in the second stage are used as mid-level features to construct
    a new image manifold, based on which the regions are ranked to generate the final
    saliency map.
  Figure 10 Link: articels_figures_by_rev_year\2016\Ranking_Saliency\figure_10.jpg
  Figure 10 caption: "Precision-recall curves on the ASD dataset by the proposed algorithm\
    \ with different design options. (a) Ranking with normalized and unnormalized\
    \ Laplacian matrices. (b) Graph construction with different constraints. (c) SC\
    \ approach. (d) Ranking using A with zero and nonzero diagonal elements. (e) Results\
    \ with different value of \u03B3 ."
  Figure 2 Link: articels_figures_by_rev_year\2016\Ranking_Saliency\figure_2.jpg
  Figure 2 caption: Proposed multi-scale graph model. For the intra-layer links at
    the superpixel-based layer, we extend the scope of node connection, which is shown
    as the dark cyan nodes. Specifically, we define that the dark cyan nodes sharing
    a common boundary (shown in thick line) with the dark blue nodes are connected
    to the red node. In addition, the red line along the four sides indicates that
    all the boundary nodes are connected with each other.
  Figure 3 Link: articels_figures_by_rev_year\2016\Ranking_Saliency\figure_3.jpg
  Figure 3 caption: 'Saliency measure via ranking. From top to bottom: input images,
    results without and with setting the diagonal elements of A to 0.'
  Figure 4 Link: articels_figures_by_rev_year\2016\Ranking_Saliency\figure_4.jpg
  Figure 4 caption: 'Saliency maps based on top boundary prior and geodesic connection
    constraints of superpixels at the image boundary. Top: Input images. Middle: Results
    without enforcing the geodesic distance constraints. Bottom: Results with geodesic
    distance constraints. The geodesic constraints help detect salient objects when
    they appear near the image boundaries or in scenes with complex backgrounds.'
  Figure 5 Link: articels_figures_by_rev_year\2016\Ranking_Saliency\figure_5.jpg
  Figure 5 caption: Saliency maps using different queries. (a) Input images. (b) Results
    of using all the boundary nodes together as queries. (c) Results of integrating
    four maps from each side. (d) Results of ranking with foreground queries.
  Figure 6 Link: articels_figures_by_rev_year\2016\Ranking_Saliency\figure_6.jpg
  Figure 6 caption: Examples in which the salient objects appear at the image boundary.
    (a) Input images. (b) Saliency maps using all the boundary nodes together as queries.
    (c) Four saliency maps by different boundary priors. (d) Integration of four maps.
    (e) Saliency maps after the second stage.
  Figure 7 Link: articels_figures_by_rev_year\2016\Ranking_Saliency\figure_7.jpg
  Figure 7 caption: Two examples in which imprecise salient queries are selected in
    the second stage. (a) Input images. (b) Saliency maps of the first stage. (c)
    Saliency maps of the second stage. (d) Final saliency maps. Although some nodes
    are mistakenly highlighted in the first stage, salient objects can be detected
    by the proposed algorithm based on intra-object relevance of foreground and background
    regions.
  Figure 8 Link: articels_figures_by_rev_year\2016\Ranking_Saliency\figure_8.jpg
  Figure 8 caption: Analysis of learned relevances between nodes in the affinity matrix
    A . (a) Intra-object r oo , intra-background r bb , and object-background r ob
    relevance values. (b) Saliency detection results using the row-sum of the sub-matrix
    of A corresponding to superpixel nodes. (c) The ratios computed with mid-level
    features are much larger than the corresponding ones computed with low-level features
    for most of the images. (d) The variances computed with mid-level features are
    smaller than those computed with low-level features.
  Figure 9 Link: articels_figures_by_rev_year\2016\Ranking_Saliency\figure_9.jpg
  Figure 9 caption: 'Examples showing the benefits of the multi-scale analysis. From
    top to down: input images, results of using single-layer graph [60], results of
    the second stage with multi-layer graph, results of the third stage. The objects
    can more completely stand out from the backgrounds by the proposed method.'
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Lihe Zhang
  Name of the last author: Ming-Hsuan Yang
  Number of Figures: 15
  Number of Tables: 3
  Number of authors: 5
  Paper title: Ranking Saliency
  Publication Date: 2016-09-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of the Proposed Algorithm with Multi-Scale Graph
      and Early Method with Single-Scale Graph-Based Manifold Ranking (MR) [60]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Comparisons on the MIT300 Dataset
  Table 3 caption:
    table_text: TABLE 3 Comparison of Average Execution Time (Seconds per Image)
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2609426
- Affiliation of the first author: "department of electrical engineering, link\xF6\
    ping university, link\xF6ping, sweden"
  Affiliation of the last author: "department of electrical engineering, link\xF6\
    ping university, link\xF6ping, sweden"
  Figure 1 Link: articels_figures_by_rev_year\2016\Discriminative_Scale_Space_Tracking\figure_1.jpg
  Figure 1 caption: Comparison of our discriminative scale space tracker (in red)
    with the standard DCF based translation tracker (in green). Example frames are
    shown from the liqour (top row), walking (middle row) and dog (bottom row) sequences.
    The standard DCF tracker is not able to handle the scale changes in these sequences.
    In contrast, our approach accurately estimates the target size, thereby significantly
    increasing both the accuracy and robustness of the tracker.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Discriminative_Scale_Space_Tracking\figure_2.jpg
  Figure 2 caption: Visualization of the joint scale space filter approach. The training
    sample (yellow box) is extracted from a scale pyramid constructed around the target
    center.
  Figure 3 Link: articels_figures_by_rev_year\2016\Discriminative_Scale_Space_Tracking\figure_3.jpg
  Figure 3 caption: Visualization of training samples used to update our DSST and
    fDSST approaches. The translation filter sample (a) is extracted from a rectangular
    patch centered around the target. To update the scale filter (b), we first sample
    patches of the target appearance at a set of different scales. Each such patch
    is then mapped to a feature vector. The training sample used for updating the
    scale filter is set to the collection of these feature vectors.
  Figure 4 Link: articels_figures_by_rev_year\2016\Discriminative_Scale_Space_Tracking\figure_4.jpg
  Figure 4 caption: Success plot showing a comparison of the different DCF-based tracking
    approaches over all the 50 videos in the OTB dataset. The legend of the success
    plot contains the area-under-the-curve (AUC) score for each method. The best results
    are obtained with our DSST method, improving the baseline translation DCF tracker
    by 6.6 percent in AUC.
  Figure 5 Link: articels_figures_by_rev_year\2016\Discriminative_Scale_Space_Tracking\figure_5.jpg
  Figure 5 caption: Impact of the number of subspace dimensions for the translation
    filter in the fDSST. Tracking performance on the OTB dataset in terms of area-under-the-curve
    (AUC) score is plotted for different choices of the dimensionality d ~ . We use
    18 dimensions (red) in our experiments.
  Figure 6 Link: articels_figures_by_rev_year\2016\Discriminative_Scale_Space_Tracking\figure_6.jpg
  Figure 6 caption: Success plot showing the performance of our fDSST compared to
    several state-of-the-art approaches on the OTB dataset. The area-under-the-curve
    (AUC) score for each tracker is reported in the legend. For the OPE (a), only
    the top ten trackers are displayed in the legend for clarity. Our method outperforms
    the second best tracker (SAMF) with 2.6 percent in AUC. We also compare the temporal
    (b) and spatial (c) robustness of our approach with the top five trackers. In
    both cases, our approach provides promising performance compared to existing tracking
    approaches.
  Figure 7 Link: articels_figures_by_rev_year\2016\Discriminative_Scale_Space_Tracking\figure_7.jpg
  Figure 7 caption: A qualitative comparison of our approach with four state-of-the-art
    trackers. Tracking results are shown on four example videos from the OTB dataset.
    The videos show challenging situations, such as scale variations (a), out-of-plane
    rotation (b), illumination variations (c) and partial occlusions (d). Our approach
    performs favorably compared to the existing tracker in these challenging situations.
  Figure 8 Link: articels_figures_by_rev_year\2016\Discriminative_Scale_Space_Tracking\figure_8.jpg
  Figure 8 caption: Ranking plots for the baseline and region noise experiments in
    the VOT 2014 dataset. The accuracy and robustness rank are plotted along the vertical
    and horizontal axis respectively. Our fDSST approach (denoted by the red circle)
    achieves superior results in both experiments.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Martin Danelljan
  Name of the last author: Michael Felsberg
  Number of Figures: 8
  Number of Tables: 7
  Number of authors: 4
  Paper title: Discriminative Scale Space Tracking
  Publication Date: 2016-09-15 00:00:00
  Table 1 caption:
    table_text: TABLE 1 A Comparison of Our DSST Approach with the Baseline Translation
      Filter and the DCF Based Scale Adaptive Trackers Discussed in Section 4
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 A Baseline Comparison on VOT 2014 of Our DSST Approach with
      the Translation-Only DCF and the Scale Adaptive DCF Trackers Discussed in Section
      4
  Table 3 caption:
    table_text: TABLE 3 A Comparison of Our Discriminative Scale Space Trackers (DSST
      and fDSST)
  Table 4 caption:
    table_text: TABLE 4 A Comparison of Our fDSST Approach with 19 State-of-the-Art
      Trackers
  Table 5 caption:
    table_text: TABLE 5 Attribute-Based Comparison with State-of-the-Art Trackers
      on the OTB Dataset
  Table 6 caption:
    table_text: TABLE 6 A Comparison of Our fDSST with Participating Methods in the
      VOT 2014 Challenge
  Table 7 caption:
    table_text: TABLE 7 Attribute Analysis on the VOT 2014 Dataset
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2609928
- Affiliation of the first author: departments of management science and mathematics
    and statistics, lancaster university, lancaster, united kingdom
  Affiliation of the last author: departments of management science and mathematics
    and statistics, lancaster university, lancaster, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2016\Clustering_by_Minimum_Cut_Hyperplanes\figure_1.jpg
  Figure 1 caption: Optimal hyperplanes based on NCut (left) and RatioCut (right)
    from the same 100 initialisations.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Clustering_by_Minimum_Cut_Hyperplanes\figure_2.jpg
  Figure 2 caption: Regret distributions across all 17 benchmark datasets.
  Figure 3 Link: articels_figures_by_rev_year\2016\Clustering_by_Minimum_Cut_Hyperplanes\figure_3.jpg
  Figure 3 caption: Run time analysis from Gaussian mixtures. The plot shows the medians
    and interquartile ranges from 50 replications for each value of n . The number
    of clusters is fixed at 5.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: David P. Hofmeyr
  Name of the last author: David P. Hofmeyr
  Number of Figures: 3
  Number of Tables: 4
  Number of authors: 1
  Paper title: Clustering by Minimum Cut Hyperplanes
  Publication Date: 2016-09-15 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Details of Benchmark Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 100 \xD7 Purity on Benchmark Datasets"
  Table 3 caption:
    table_text: "TABLE 3 100 \xD7 V-Measure on Benchmark Datasets"
  Table 4 caption:
    table_text: TABLE 4 Run Time on Benchmark Datasets (in Seconds)
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2609929
- Affiliation of the first author: university of delaware, newark, de
  Affiliation of the last author: university of delaware, newark, de
  Figure 1 Link: articels_figures_by_rev_year\2016\Saliency_Detection_on_Light_Field\figure_1.jpg
  Figure 1 caption: Light field versus traditional saliency detection. Similar foreground
    and background or complex background imposes challenges on state-of-the-art algorithms
    (e.g., RC [6], DRFI [7]). Using light field as inputs, our saliency detection
    scheme is able to robustly handle these cases.
  Figure 10 Link: articels_figures_by_rev_year\2016\Saliency_Detection_on_Light_Field\figure_10.jpg
  Figure 10 caption: Visual comparisons of different saliency detection algorithms
    versus ours on our light field dataset.
  Figure 2 Link: articels_figures_by_rev_year\2016\Saliency_Detection_on_Light_Field\figure_2.jpg
  Figure 2 caption: (a) A Lytro light field camera can capture a light field towards
    the scene in a single shot. The results can be then used to synthesize a focal
    stack and further a all-focus image. (b) Focus stack (the first row) and its corresponding
    focus regions (second row).
  Figure 3 Link: articels_figures_by_rev_year\2016\Saliency_Detection_on_Light_Field\figure_3.jpg
  Figure 3 caption: Processing pipeline of our saliency detection algorithm for light
    fields.
  Figure 4 Link: articels_figures_by_rev_year\2016\Saliency_Detection_on_Light_Field\figure_4.jpg
  Figure 4 caption: Focusness detection comparison of UFO [13] versus ours. (a) Focusness
    detection results comparsion. (b) PRCs comparison.
  Figure 5 Link: articels_figures_by_rev_year\2016\Saliency_Detection_on_Light_Field\figure_5.jpg
  Figure 5 caption: Focusness detection result on focus stack. (a) All focus image.
    (b) Focusness map on the nearest objects. (c) Focusness map on objects at the
    middle of depth range. (c) Focusness map on the furthest objects.
  Figure 6 Link: articels_figures_by_rev_year\2016\Saliency_Detection_on_Light_Field\figure_6.jpg
  Figure 6 caption: 'Separating the foreground and background using focusness cues.
    Left: The computed foreground likelihood score (FLS) and the background likelihood
    score (BLS) computed on different focal slices. Right: Examples on computing objectness
    measure (up) and background measure (bottom). Green curve is corresponding filter
    (U-shape or Gaussian); blue curve is sample Dx Dy ; red curve is the scaled distribution
    by the filter.'
  Figure 7 Link: articels_figures_by_rev_year\2016\Saliency_Detection_on_Light_Field\figure_7.jpg
  Figure 7 caption: Saliency results using all-focus images (the first and third rows)
    and partial-focus images (the second and forth rows).
  Figure 8 Link: articels_figures_by_rev_year\2016\Saliency_Detection_on_Light_Field\figure_8.jpg
  Figure 8 caption: PRC comparisons on our light field dataset. (a) Results of regular
    image based algorithms. (b) Results of depthmap based algorithms. (c) Using different
    cues in our approach.
  Figure 9 Link: articels_figures_by_rev_year\2016\Saliency_Detection_on_Light_Field\figure_9.jpg
  Figure 9 caption: Comparison of average time taken for different saliency detection
    methods.
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.53
  Name of the first author: Nianyi Li
  Name of the last author: Jingyi Yu
  Number of Figures: 14
  Number of Tables: 0
  Number of authors: 5
  Paper title: Saliency Detection on Light Field
  Publication Date: 2016-09-16 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2610425
- Affiliation of the first author: department of computer science, university of central
    florida, orlando, fl
  Affiliation of the last author: department of computer science, university of central
    florida, orlando, fl
  Figure 1 Link: articels_figures_by_rev_year\2016\Linear_Subspace_Ranking_Hashing_for_CrossModal_Retrieval\figure_1.jpg
  Figure 1 caption: An example of the empirical loss (3) and its softmax approximation
    (12) as a function of the training iteration number. The update rules in (15)
    are used with mini-batch size set to 500. The algorithm converges in less then
    50 iterations.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Linear_Subspace_Ranking_Hashing_for_CrossModal_Retrieval\figure_2.jpg
  Figure 2 caption: Example of textual queries on the labelme image database. From
    (a) to (b), the query keywords are 'building', 'tree', 'sea' and 'mountain' respectively.
    The code length of LSRH is set to 30 bits and the top 30 results are shown. Images
    in the grid are ordered from left to right and top to bottom based on the Hamming
    distance of their hash codes to the hash code of the textual query.
  Figure 3 Link: articels_figures_by_rev_year\2016\Linear_Subspace_Ranking_Hashing_for_CrossModal_Retrieval\figure_3.jpg
  Figure 3 caption: Example of image annotations by using images to query tags. The
    code length of LSRH is set to 30 bits and approximately 30 of the most relevant
    tags are shown. The ground truth for each image is shown in green. The order of
    the tags is based on the Hamming distance between hash codes.
  Figure 4 Link: articels_figures_by_rev_year\2016\Linear_Subspace_Ranking_Hashing_for_CrossModal_Retrieval\figure_4.jpg
  Figure 4 caption: Top-100 precision of different methods on all datasets, with the
    hash code varying from 16 to 64 bits. The first row shows the results of text-query-image
    and the second row shows that of image-query-text.
  Figure 5 Link: articels_figures_by_rev_year\2016\Linear_Subspace_Ranking_Hashing_for_CrossModal_Retrieval\figure_5.jpg
  Figure 5 caption: Top-k precision of all methods with 32-bit hash code and k varies
    from 100 to 1,000. The first row shows the results of text-query-image and the
    second row shows that of image-query-text. The proposed LSRH outperforms all the
    baselines.
  Figure 6 Link: articels_figures_by_rev_year\2016\Linear_Subspace_Ranking_Hashing_for_CrossModal_Retrieval\figure_6.jpg
  Figure 6 caption: Precision-recall curves with 32-bit hash code. The first row shows
    the results of text-query-image and the second row shows that of image-query-text.
    Larger area under the curve indicates better performance. LSRH achieves the best
    performance.
  Figure 7 Link: articels_figures_by_rev_year\2016\Linear_Subspace_Ranking_Hashing_for_CrossModal_Retrieval\figure_7.jpg
  Figure 7 caption: Comparison of different strategies in generating multiple hash
    codes. The results are obtained with 32-bit hash code on MIRFlickr and NUSWIDE.
  Figure 8 Link: articels_figures_by_rev_year\2016\Linear_Subspace_Ranking_Hashing_for_CrossModal_Retrieval\figure_8.jpg
  Figure 8 caption: Top-k precision of LSRH trained with different loss functions.
    The length of the hash code is set to 32 bits and k varies from 100 to 1,000.
    The first and second rows show the results of text-query-image and image-query-text
    respectively.
  Figure 9 Link: articels_figures_by_rev_year\2016\Linear_Subspace_Ranking_Hashing_for_CrossModal_Retrieval\figure_9.jpg
  Figure 9 caption: The mAP with 60-bit hash code under different choices of subspace
    dimensions. K = 4 leads to the best overall performance.
  First author gender probability: 0.93
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Kai Li
  Name of the last author: Kien A. Hua
  Number of Figures: 9
  Number of Tables: 5
  Number of authors: 4
  Paper title: Linear Subspace Ranking Hashing for Cross-Modal Retrieval
  Publication Date: 2016-09-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Cross-Modal mAP Results of the Proposed LSRH and Compared
      Baselines on All of the Benchmark Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Statistics of Benchmark Datasets
  Table 3 caption:
    table_text: TABLE 3 Performance Results of the Proposed LSRH and Baselines in
      Terms of Top-K Precision, Precision-Recall and TrainingTesting Time on All of
      the Benchmark Datasets
  Table 4 caption:
    table_text: TABLE 4 mAP and Top-100 Precision of LSRH Using Different Loss Functions
  Table 5 caption:
    table_text: TABLE 5 Training Time of 32-Bit Hash Code on MIRFlickr and NUSWIDE
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2610969
- Affiliation of the first author: pattern recognition and computer vision laboratory
    (perceive lab), department of electrical, electronics and computer engineering,
    university of catania, catania, italy
  Affiliation of the last author: pattern recognition and computer vision laboratory
    (perceive lab), department of electrical, electronics and computer engineering,
    university of catania, catania, italy
  Figure 1 Link: articels_figures_by_rev_year\2016\Gamifying_Video_Object_Segmentation\figure_1.jpg
  Figure 1 caption: In-game screenshot of the user interface.
  Figure 10 Link: articels_figures_by_rev_year\2016\Gamifying_Video_Object_Segmentation\figure_10.jpg
  Figure 10 caption: 'Failure cases: User clicks were extremely inaccurate resulting
    in wrong object segmentations.'
  Figure 2 Link: articels_figures_by_rev_year\2016\Gamifying_Video_Object_Segmentation\figure_2.jpg
  Figure 2 caption: (Left to right) Initially users tend mainly to click only over
    the left person. As soon as it receives many clicks it gets blurred (blurring
    salient objects as those that accumulated enough users- clicks;), thus making
    players shift attention towards the right person.
  Figure 3 Link: articels_figures_by_rev_year\2016\Gamifying_Video_Object_Segmentation\figure_3.jpg
  Figure 3 caption: "Due to users' reaction times, clicks may be delayed with respect\
    \ to the \u201Cintended\u201D frame. It is possible to notice that this phenomenon\
    \ may be more or less evident even within the same image, depending not only on\
    \ the user but also on the objects in the scene."
  Figure 4 Link: articels_figures_by_rev_year\2016\Gamifying_Video_Object_Segmentation\figure_4.jpg
  Figure 4 caption: 'Output examples for superclick identification: blue dots are
    users'' clicks while green regions show the yielded segmentation masks. Segmentation
    refinement is carried out by including temporal constraints.'
  Figure 5 Link: articels_figures_by_rev_year\2016\Gamifying_Video_Object_Segmentation\figure_5.jpg
  Figure 5 caption: Qualitative comparison between segmentations obtained when excluding
    (first column, see Fig. 4) and including temporal smoothing (second column).
  Figure 6 Link: articels_figures_by_rev_year\2016\Gamifying_Video_Object_Segmentation\figure_6.jpg
  Figure 6 caption: Average F 1 measure over different datasets as cumulative users'
    play time increases. The results for a certain value T of cumulative play time
    are obtained by considering the clicks from the first T12 games played on the
    system, where 12 is the duration in minutes of each game.
  Figure 7 Link: articels_figures_by_rev_year\2016\Gamifying_Video_Object_Segmentation\figure_7.jpg
  Figure 7 caption: Single user clicks versus multiple players' ones. (Left) The seven
    clicks of a single user in different game sessions show the tendency of players
    to click always on the same image parts. (Right) Clicks from multiple users (color-coded
    w.r.t. users in the figure) allows, instead, to identify multiple objects and
    their parts.
  Figure 8 Link: articels_figures_by_rev_year\2016\Gamifying_Video_Object_Segmentation\figure_8.jpg
  Figure 8 caption: Interaction times versus segmentation accuracy. The figure shows
    that with the proposed approach we get a fairly good segmentation quality just
    after 30 seconds. When we allowed users to spend more time on the annotation task,
    the method in [28] achieved the best performance with a POM of about 0.85 with
    an interaction time of about 1,400 seconds.
  Figure 9 Link: articels_figures_by_rev_year\2016\Gamifying_Video_Object_Segmentation\figure_9.jpg
  Figure 9 caption: 'Output segmentations when using only points within objects (i.e.,
    taken from ground truth segmentation masks) of interest: Blue dots are ground
    truth points while green regions show the yielded segmentation masks.'
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Concetto Spampinato
  Name of the last author: Daniela Giordano
  Number of Figures: 10
  Number of Tables: 8
  Number of authors: 3
  Paper title: Gamifying Video Object Segmentation
  Publication Date: 2016-09-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average Segmentation Accuracy for the Video Categories Employed
      in Our Game in Terms of Precision, Recall and F-Measure, When We Employ Only
      Superclick Extraction (First Column) and When We Refine the Output Segmentation
      by Means of Spatio-Temporal Linking Between Superclicks in Consecutive Frames
      (Second Column)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average Segmentation Accuracy for the Video Categories Employed
      in Our Game in Terms of Precision, Recall and F-Measure, for the Single-Player
      and the Many-Players Scenarios
  Table 3 caption:
    table_text: TABLE 3 Average Segmentation Accuracy in Terms of Precision, Recall
      and F-Measure, for the Single-Player and the Many-Players Scenarios, with Respect
      to the Number of Objects in a Frame
  Table 4 caption:
    table_text: TABLE 4 Average Segmentation Accuracy over Different Datasets in Terms
      of Precision, Recall and F-Measure, for Different Click Delay Values
  Table 5 caption:
    table_text: TABLE 5 Average Segmentation Accuracy over Different Benchmarks in
      Terms of Precision, Recall and F-Measure, for Different Ranges of Click Quality
  Table 6 caption:
    table_text: TABLE 6 Comparison in Terms of Segmentation Accuracy-Measured as Average
      POM in Percentage, Respectively, Achieved within the First 50 Secs of Annotation
      (First Row Indicated with PO M 50secs ) and the Maximum Value PO M max (with
      the Related Interaction Times Shown in the Third Row)-Between Our Approach and
      Other Interactive Video Annotation Methods on a Subset of 10 Frames Extracted
      from SegTrack v2 Dataset
  Table 7 caption:
    table_text: TABLE 7 POM in Percentage for the Youtube-Objects Dataset
  Table 8 caption:
    table_text: TABLE 8 Videos Used for Creating Game Levels
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2610973
