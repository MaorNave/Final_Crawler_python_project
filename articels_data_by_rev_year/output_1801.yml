- Affiliation of the first author: school of computer science and engineering, chung-ang
    university, seoul, south korea
  Affiliation of the last author: school of computer science and engineering, chung-ang
    university, seoul, south korea
  Figure 1 Link: articels_figures_by_rev_year\2020\SphereGAN_Sphere_Generative_Adversarial_Network_Based_on_Geometric_Moment_Matchi\figure_1.jpg
  Figure 1 caption: Diagram of proposed SphereGAN. The generator network takes random
    noise as input and generates fake data. The discriminator network takes real (fake)
    data as an input and outputs real (fake) feature vectors to the n -dimensional
    euclidean feature space (gray plane). The feature vectors of the fake and real
    samples are indicated by blue and pink circles in the plane, respectively. Through
    geometric-aware transformations (Section 3.2), these feature vectors are mapped
    into the n -dimensional hypersphere (gray sphere). SphereGAN calculates the centered
    geometric moments of the mapped points at the north pole of the hypersphere. The
    discriminator network of SphereGAN attempts to maximize the discrepancy between
    the geometric moments of real and fake samples, whereas the generator network
    attempts to interfere with the discriminator network by minimizing the moment
    discrepancies. Then, using a two-player minmax game, the generator and discriminator
    networks enhance their performance.
  Figure 10 Link: articels_figures_by_rev_year\2020\SphereGAN_Sphere_Generative_Adversarial_Network_Based_on_Geometric_Moment_Matchi\figure_10.jpg
  Figure 10 caption: 3D point clouds generated by SphereGAN from ShapeNet dataset.
  Figure 2 Link: articels_figures_by_rev_year\2020\SphereGAN_Sphere_Generative_Adversarial_Network_Based_on_Geometric_Moment_Matchi\figure_2.jpg
  Figure 2 caption: "Illustration of inverse of stereographic projection on euclidean\
    \ plane \u03A0 \u22121 : R 2 \u2192 S 2 \u2212N . The line p\u2212q and the curve\
    \ a\u2212b denote the geodesics on R 2 and S 2 , respectively. \u03A0 \u22121\
    \ (p)=a, \u03A0 \u22121 (q)=b ."
  Figure 3 Link: articels_figures_by_rev_year\2020\SphereGAN_Sphere_Generative_Adversarial_Network_Based_on_Geometric_Moment_Matchi\figure_3.jpg
  Figure 3 caption: "IS on the CIFAR-10 dataset using ConvNet according to different\
    \ moment matching modes and hypersphere dimensions. Red, yellow, and blue bars\
    \ indicate moment modes: \u2211 1 d r , \u2211 3 1 d r , and \u2211 5 1 d r ,\
    \ respectively. The horizontal axis denotes hypersphere dimensions: S n :n=16,64,256,1024\
    \ ."
  Figure 4 Link: articels_figures_by_rev_year\2020\SphereGAN_Sphere_Generative_Adversarial_Network_Based_on_Geometric_Moment_Matchi\figure_4.jpg
  Figure 4 caption: Norm of gradient for discriminator networks of SphereGAN and WGAN-GP.
  Figure 5 Link: articels_figures_by_rev_year\2020\SphereGAN_Sphere_Generative_Adversarial_Network_Based_on_Geometric_Moment_Matchi\figure_5.jpg
  Figure 5 caption: Averaged computational time over 100 iterations for different
    GAN variants. The yellow and red bars indicate that the averaged computational
    time when the updating ratios of the generator and discriminator are 1:1 and 1:5
    , respectively.
  Figure 6 Link: articels_figures_by_rev_year\2020\SphereGAN_Sphere_Generative_Adversarial_Network_Based_on_Geometric_Moment_Matchi\figure_6.jpg
  Figure 6 caption: Loss of generator and discriminator networks for SphereGAN+SpectralNorm.
  Figure 7 Link: articels_figures_by_rev_year\2020\SphereGAN_Sphere_Generative_Adversarial_Network_Based_on_Geometric_Moment_Matchi\figure_7.jpg
  Figure 7 caption: Images generated by SphereGAN from LSUN-bedroom (left) and STL-10
    (right) datasets.
  Figure 8 Link: articels_figures_by_rev_year\2020\SphereGAN_Sphere_Generative_Adversarial_Network_Based_on_Geometric_Moment_Matchi\figure_8.jpg
  Figure 8 caption: Qualitative comparison between SphereGAN and WGAN-GP from the
    CIFAR-10 dataset.
  Figure 9 Link: articels_figures_by_rev_year\2020\SphereGAN_Sphere_Generative_Adversarial_Network_Based_on_Geometric_Moment_Matchi\figure_9.jpg
  Figure 9 caption: Qualitative results of the proposed method for high resolution
    images.
  First author gender probability: 0.94
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Sung Woo Park
  Name of the last author: Junseok Kwon
  Number of Figures: 11
  Number of Tables: 10
  Number of authors: 2
  Paper title: 'SphereGAN: Sphere Generative Adversarial Network Based on Geometric
    Moment Matching and its Applications'
  Publication Date: 2020-08-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Generator Network for the CIFAR-10 Dataset
  Table 10 caption:
    table_text: TABLE 10 Unsupervised 3D Point Cloud Generation on ShapeNet Dataset
  Table 2 caption:
    table_text: TABLE 2 Discriminator Network for the CIFAR-10 Dataset
  Table 3 caption:
    table_text: TABLE 3 Generator Network for the STL-10 Dataset
  Table 4 caption:
    table_text: TABLE 4 Discriminator Network for the STL-10 Dataset
  Table 5 caption:
    table_text: TABLE 5 Unsupervised Image Generation Results on the CIFAR-10 Dataset
  Table 6 caption:
    table_text: TABLE 6 Unsupervised Image Generation Results on the STL-10 Dataset
  Table 7 caption:
    table_text: TABLE 7 Unsupervised Image Generation Results on the LSUN Bedroom
      Dataset
  Table 8 caption:
    table_text: TABLE 8 Generator Network for the ShapeNet Dataset
  Table 9 caption:
    table_text: TABLE 9 Discriminator Network for the ShapeNet Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3015948
- Affiliation of the first author: inria, paris, france
  Affiliation of the last author: inria, paris, france
  Figure 1 Link: articels_figures_by_rev_year\2020\NCNet_Neighbourhood_Consensus_Networks_for_Estimating_Image_Correspondences\figure_1.jpg
  Figure 1 caption: Overview of the proposed method. A fully convolutional neural
    network is used to extract dense image descriptors f A and f B for images I A
    and I B , respectively. Scores for all pairs of individual feature matches f A
    ij and f B kl are stored in the 4-D correlation map c ijkl (here shown as a 3-D
    illustration). These matches are further processed by the proposed soft-nearest
    neighbour filtering and neighbourhood consensus network to produce the final set
    of output correspondences.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\NCNet_Neighbourhood_Consensus_Networks_for_Estimating_Image_Correspondences\figure_2.jpg
  Figure 2 caption: Semantic keypoint transfer. The annotated (ground truth) keypoints
    in the left image are automatically transferred to the right image using the dense
    correspondences between the two images obtained from our NCNet.
  Figure 3 Link: articels_figures_by_rev_year\2020\NCNet_Neighbourhood_Consensus_Networks_for_Estimating_Image_Correspondences\figure_3.jpg
  Figure 3 caption: Dense semantic alignment. The first two columns show the source
    and target images, respectively. The right-most column shows the result of transforming
    the source image by bilinear interpolation using the matches obtained by NCNet
    such that the result is aligned to the target image. Note that no global geometric
    model is used for the warping.
  Figure 4 Link: articels_figures_by_rev_year\2020\NCNet_Neighbourhood_Consensus_Networks_for_Estimating_Image_Correspondences\figure_4.jpg
  Figure 4 caption: HPatches benchmark results. We report the Mean Matching Accuracy
    (MMA) as a function of the tolerance threshold for the illumination and viewpoint
    subsets, and well as the overall results.
  Figure 5 Link: articels_figures_by_rev_year\2020\NCNet_Neighbourhood_Consensus_Networks_for_Estimating_Image_Correspondences\figure_5.jpg
  Figure 5 caption: Correspondences on HPatches images. The top row shows the image
    pair; the middle row shows the matches obtained directly from the correlation
    map c (before NCNet filtering); the bottom row shows the matches obtained by the
    proposed method (after NCNet filtering). Correspondences have been coloured as
    inliers (green) and outliers (red) w.r.t. the ground-truth homography using a
    threshold of 5px. Each image shows 100 randomly sampled matches from the top 2000
    matches. The proposed NCNet method tends to obtain a larger fraction of correct
    matches which span a larger portion of the image with respect to the raw matches
    before NCNet filtering.
  Figure 6 Link: articels_figures_by_rev_year\2020\NCNet_Neighbourhood_Consensus_Networks_for_Estimating_Image_Correspondences\figure_6.jpg
  Figure 6 caption: Correspondences and poses on InLoc. Each row shows (a-b) the correspondences
    used for pose estimation in the case of the proposed NCNet method (blue) against
    those of the baseline InLoc method (red); and (c) the resulting obtained poses
    for the proposed NCNet (blue) and InLoc baseline (red) compared to the ground-truth
    pose (green). In both cases the InLoc baseline produces many mismatches due to
    repetitive structures (ceiling lamps in the top, and columns in the bottom example)
    that result in a large pose error. On the other hand, NCNet obtains mostly correct
    matches resulting in a small pose error.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Ignacio Rocco
  Name of the last author: Josef Sivic
  Number of Figures: 6
  Number of Tables: 4
  Number of authors: 6
  Paper title: 'NCNet: Neighbourhood Consensus Networks for Estimating Image Correspondences'
  Publication Date: 2020-08-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Results for Semantic Matching on Different Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Indoor Localization Methods
  Table 3 caption:
    table_text: TABLE 3 Homography Estimation on HPatches
  Table 4 caption:
    table_text: TABLE 4 Ablation Studies on the PF-Pascal Dataset
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3016711
- Affiliation of the first author: integrated services networks, xidian university,
    xian, china
  Affiliation of the last author: faculty of engineering and information technologies,
    ubtech sydney artificial intelligence centre and the school of information technologies,
    university of sydney, darlington, nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2020\Enhanced_Tensor_RPCA_and_its_Application\figure_1.jpg
  Figure 1 caption: The relationship graph of our model with the related work.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Enhanced_Tensor_RPCA_and_its_Application\figure_2.jpg
  Figure 2 caption: Correct recovery for varying rank and sparsity. (a)-(d) are ETRPCA
    under different weights; (e) is TRPCA, (f) is TPSSV.
  Figure 3 Link: articels_figures_by_rev_year\2020\Enhanced_Tensor_RPCA_and_its_Application\figure_3.jpg
  Figure 3 caption: Background modeling from videos. Original frames (first row),
    RPCA (1a and 1b), TRPCA (2a and 2b), WRPCA (3a and 3b),TPSSV (4a and 4b), and
    ETRPCA (5a and 5b).
  Figure 4 Link: articels_figures_by_rev_year\2020\Enhanced_Tensor_RPCA_and_its_Application\figure_4.jpg
  Figure 4 caption: Comparison of the PSNR values of RPCA, WRPCA, TRPCA, TPSSV, and
    ETRPCA for image denoising on 50 images.
  Figure 5 Link: articels_figures_by_rev_year\2020\Enhanced_Tensor_RPCA_and_its_Application\figure_5.jpg
  Figure 5 caption: Comparison of the SSIM values of RPCA, WRPCA, TRPCA, TPSSV, and
    ETRPCA for image denoising on 50 images.
  Figure 6 Link: articels_figures_by_rev_year\2020\Enhanced_Tensor_RPCA_and_its_Application\figure_6.jpg
  Figure 6 caption: Recovery performance comparison on the Berkeley images. (a) Original
    image; (b) observed image; (c)-(h) recovered images by RPCA, WRPCA, TRPCA, TPSSV,
    and ETRPCA, respectively.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.95
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Quanxue Gao
  Name of the last author: Dacheng Tao
  Number of Figures: 6
  Number of Tables: 4
  Number of authors: 6
  Paper title: Enhanced Tensor RPCA and its Application
  Publication Date: 2020-08-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Correct Recovery for Random Problems of Varying Sizes ( r=ran
      k t (D 0 )=0.1n,m=|| E 0 | | 0 =0.1 n 3 r=rankt(D0)=0.1n,m=||E0||0=0.1n3)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Correct Recovery for Random Problems of Varying Sizes ( r=ran
      k t (D 0 )=0.1n,m=|| E 0 | | 0 =0.2 n 3 r=rankt(D0)=0.1n,m=||E0||0=0.2n3)
  Table 3 caption:
    table_text: "TABLE 3 Correct Recovery With Varying p p and \u03C9 \u03C9 ( n=100\
      \ n=100, r=ran k t (D 0 )=0.1n,m=|| E 0 | | 0 =0.2 n 3 r=rankt(D0)=0.1n,m=||E0||0=0.2n3)"
  Table 4 caption:
    table_text: TABLE 4 Correct Recovery Under Different Distributions ( r=ran k t
      (D 0 )=0.1n,m=|| E 0 | | 0 =0.1 n 3 ,n=100 r=rankt(D0)=0.1n,m=||E0||0=0.1n3,n=100)
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3017672
- Affiliation of the first author: school of electrical engineering and computer science,
    oregon state university, corvallis, or, usa
  Affiliation of the last author: school of electrical engineering and computer science,
    oregon state university, corvallis, or, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Incomplete_Label_Multiple_Instance_Multiple_Label_Learning\figure_1.jpg
  Figure 1 caption: 'Weak supervision paradigms. PLL settings where training can be:
    (a) single-instance single-label learning - for each instance, a set of candidate
    labels are given, only one of which is correct, (b) single-instance multiple-label
    learning - each instance is given a set of candidate labels, in which a subset
    of the candidate set is correct. ILL settings where training can be: (c) single-instance
    multiple-label learning with missing labels - each instance is given a subset
    of the ground truth label set, (d) multiple-instance multiple-label learning with
    missing labels - each bag of instances is given a subset of the ground truth label
    set. Notations in this figure: frame and shading represent the provided and ground
    truth label, respectively; green, red and gray represent for positive, negative,
    and unknown label, respectively.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Incomplete_Label_Multiple_Instance_Multiple_Label_Learning\figure_2.jpg
  Figure 2 caption: "Fully labeled SIMLMIML learning versus incomplete label SIMLMIML\
    \ learning. In (a) the fully labeled SIMLMIML setting, each element of vector\
    \ Y i indicates the presence of a given class by 1 or its absence by 0. In (b)\
    \ the incomplete label SIMLMIML setting, each element of vector Y i indicates\
    \ the presence of a given class by 1, or its absence by 0, or the unavailable\
    \ of a class by \u22121 ."
  Figure 3 Link: articels_figures_by_rev_year\2020\Incomplete_Label_Multiple_Instance_Multiple_Label_Learning\figure_3.jpg
  Figure 3 caption: The general SIML model.
  Figure 4 Link: articels_figures_by_rev_year\2020\Incomplete_Label_Multiple_Instance_Multiple_Label_Learning\figure_4.jpg
  Figure 4 caption: (a) The general MIML model and (b) a reformulation of (a) as a
    chain for computational efficiency of the E-step in the EM algorithm. Shaded nodes
    are observed.
  Figure 5 Link: articels_figures_by_rev_year\2020\Incomplete_Label_Multiple_Instance_Multiple_Label_Learning\figure_5.jpg
  Figure 5 caption: "Incomplete label learning model: (a) the original MIML model,\
    \ (b) a reformulated model, which includes the state reduced instance label hidden\
    \ variables z b1 ,\u2026, z b n b , and (c) a reformulated model, which includes\
    \ the state reduced instance label hidden variables z b1 ,\u2026, z b n b reorganized\
    \ into a chain structure using variables Z 1 b ,\u2026, Z n b b . Shaded nodes\
    \ are observed (i.e., x b1 ,\u2026, x b n b ) and striped nodes are partially-observed\
    \ (i.e., Y b )."
  Figure 6 Link: articels_figures_by_rev_year\2020\Incomplete_Label_Multiple_Instance_Multiple_Label_Learning\figure_6.jpg
  Figure 6 caption: Running time (marker) and computational complexity (solid line)
    for FullEM (in red) and for MML (in blue) as a function of (a) the number of positive
    labels in the bag | S + b | , (b) the number of instances in the bag n b , (c)
    the number of classes C , and (d) the instance feature vector dimension d . Nominal
    values are | S + b |=3 , C=10 , d=30 , and n b =10.
  Figure 7 Link: articels_figures_by_rev_year\2020\Incomplete_Label_Multiple_Instance_Multiple_Label_Learning\figure_7.jpg
  Figure 7 caption: Bag accuracy as a function of the proportion of missing labels
    for 5 algorithms evaluated on 11 datasets (a)-(k).
  Figure 8 Link: articels_figures_by_rev_year\2020\Incomplete_Label_Multiple_Instance_Multiple_Label_Learning\figure_8.jpg
  Figure 8 caption: Instance accuracy as a function of the proportion of missing labels
    for 4 algorithms across 5 datasets (a)-(e).
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.75
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Tam Nguyen
  Name of the last author: Raviv Raich
  Number of Figures: 8
  Number of Tables: 7
  Number of authors: 2
  Paper title: Incomplete Label Multiple Instance Multiple Label Learning
  Publication Date: 2020-08-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Notations Used in This Paper
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 The Average Number of Labels Per Bag | S + b | \xAF \xAF\
      \ \xAF \xAF \xAF \xAF \xAF \xAF \xAF |Sb+|\xAF, the Maximum Number of Labels\
      \ Per Bag | S + b | max |Sb+|max, the Total Number of Classes C C, the Dimension\
      \ of the Instance Feature Vector d d, and the Number of Bags B B for Each MIML\
      \ Dataset: Scene, Reuters, MSRA [29], Corel5k-25 and Corel5k-100 Both are Derived\
      \ From Corek5k [30], MNIST-MIML is Generated From MNIST [31], HJA Bird, MSCV2,\
      \ Voc12, Letter Caroll, Letter Frost [32], OSU Wrist [33], Alipr [34], UIUC\
      \ Sport [35], NUS-WIDE [36], MLSP [26], MIR [37], and Protein [38]"
  Table 3 caption:
    table_text: 'TABLE 3 Computational Complexity During an EM-Iteration Excluding
      the 10 percent of the Bags With Highest | S + b | |Sb+| for Different Datasets:
      M-Step (second column), E-Step Posterior Calculation (Third Column), and E-step
      Posterior Calculation When Half of the Labels are Missing (Last Column)'
  Table 4 caption:
    table_text: "TABLE 4 Dataset Groups by Size Based on \u2211 B b=1 n b Cd \u2211\
      b=1BnbCd"
  Table 5 caption:
    table_text: "TABLE 5 Bag-Level Evaluation Metrics ( \xB1 \xB1 90% 90% Confidence\
      \ Interval) for 5 Algorithms Across 11 Datasets at \u03C1=0.6 \u03C1=0.6"
  Table 6 caption:
    table_text: "TABLE 6 The Average Instance-Level AUC ( \xB1 \xB1 90% 90% Confidence\
      \ Interval) on the Five Datasets That Provide Instance-Level Label Ground Truth\
      \ Information at \u03C1=0.6 \u03C1=0.6"
  Table 7 caption:
    table_text: "TABLE 7 The Average Running Time (ms) for 5 Algorithms Under Three\
      \ Scenarios: Fully-Labeled Data, Missing Label Data With Missing Labels Replaced\
      \ by 0, and Missing Label Data With Missing Labels Marked by \u22121 -1"
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3017456
- Affiliation of the first author: school of information systems, singapore management
    university, singapore
  Affiliation of the last author: "department of computer vision and machine learning,\
    \ max planck institute for informatics, saarbr\xFCcken, germany"
  Figure 1 Link: articels_figures_by_rev_year\2020\MetaTransfer_Learning_Through_Hard_Tasks\figure_1.jpg
  Figure 1 caption: 'The pipeline of our proposed few-shot learning method, including:
    (a) DNN pre-training on large-scale data, i.e., using the entire training dataset;
    and (b) meta-transfer learning (MTL) that learns the parameters of Scaling and
    Shifting (SS), on the basis of pre-trained feature extractor (Section 4.1). The
    learning process is scheduled by the proposed HT meta-batch (Section 4.2) and
    regularized by meta-gradient regularization (Section 4.3). In (c), it is meta-test
    on unseen task whose processing consists of a base-learner (classifier) Fine-Tuning
    (FT) stage and a final evaluation stage, described in the last paragraph in Section
    3. Input data are along with arrows. Modules with names in bold get updated at
    corresponding phases.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\MetaTransfer_Learning_Through_Hard_Tasks\figure_2.jpg
  Figure 2 caption: "Two kinds of meta operations on pre-trained weights. (a) Parameter-level\
    \ Fine-Tuning (FT) is a conventional meta-train operation used in related works\
    \ such as MAML [5], ProtoNets [33] and RelationNets [34]. Its update works for\
    \ all neuron parameters, W and b . (b) Our neuron-level Scaling and Shifting (SS)\
    \ operations in MTL. They reduce the number of learning parameters and avoid overfitting\
    \ problems. In addition, they keep large-scale trained parameters (in yellow)\
    \ frozen, preventing \u201Ccatastrophic forgetting\u201D [31], [32]."
  Figure 3 Link: articels_figures_by_rev_year\2020\MetaTransfer_Learning_Through_Hard_Tasks\figure_3.jpg
  Figure 3 caption: "The computation flow of online hard task sampling. During an\
    \ HT meta-batch phase, the meta-training first goes through K random tasks then\
    \ continues on re-sampled K \u2032 hard tasks."
  Figure 4 Link: articels_figures_by_rev_year\2020\MetaTransfer_Learning_Through_Hard_Tasks\figure_4.jpg
  Figure 4 caption: The 5-way, 1-shot meta-validation accuracy plots on the FC100,
    using FT (pre-trained ResNet-12 and MAML [5]) and our MTL on different pre-trained
    networks. Red curve uses the original meta-batch [5] and others use our proposed
    HT meta-batch.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.51
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Qianru Sun
  Name of the last author: Bernt Schiele
  Number of Figures: 4
  Number of Tables: 8
  Number of authors: 5
  Paper title: Meta-Transfer Learning Through Hard Tasks
  Publication Date: 2020-08-21 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 The 5-Way, 1-Shot and 5-Shot Classification Accuracy ( %\
      \ %) on miniImageNet, for Choosing the Best Architecture of Base-Learner (i.e.,\
      \ the Classifier \u03B8 \u03B8)"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The 5-Way, 1-Shot and 5-Shot Classification Accuracy ( % %)
      on miniImageNet Datasets
  Table 3 caption:
    table_text: TABLE 3 The 5-Way, 1-Shot and 5-Shot Classification Accuracy ( % %)
      on tieredImageNet and FC100 Datasets
  Table 4 caption:
    table_text: TABLE 4 The 5-Way, 1-Shot and 5-Shot Classification Accuracy ( % %)
      on miniImageNet and tieredImageNet Datasets
  Table 5 caption:
    table_text: TABLE 5 Semi-Supervised 5-Way, 1-Shot and 5-Shot Classification Accuracy
      (%) on miniImageNet and tieredImageNet
  Table 6 caption:
    table_text: TABLE 6 The 5-Way, 1-Shot and 5-Shot Classification Accuracy (%) Using
      Ablative Models, on Two Datasets
  Table 7 caption:
    table_text: TABLE 7 Ablation Results for the 5-Way, 1-Shot and 5-Shot Classification
      Accuracy ( % %) on miniImageNet and tieredImageNet Datasets
  Table 8 caption:
    table_text: TABLE 8 Statistical Values of SS Parameters, i.e., to See How Much
      Network Parameters Drifted After the Meta-Training Using SS
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3018506
- Affiliation of the first author: department of electrical engineering, the city
    college, city university of new york, new york, ny, usa
  Affiliation of the last author: department of electrical engineering, the city college,
    city university of new york, new york, ny, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Unambiguous_Text_Localization_Retrieval_and_Recognition_for_Cluttered_Scenes\figure_1.jpg
  Figure 1 caption: An example of unambiguous text localization, retrieval, and recognition.
    Given a cluttered scene image and candidate text bounding boxes (in white, detected
    by the proposed DTLN), the proposed CRTR model is applied to retrieve a specific
    text instance (in color) based on a natural language description, which can be
    further transcribed to machine-readable character codes. The CRTR model scores
    and ranks candidate boxes based on text attributes, spatial configurations, and
    context information.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Unambiguous_Text_Localization_Retrieval_and_Recognition_for_Cluttered_Scenes\figure_2.jpg
  Figure 2 caption: The architecture of the proposed Dense Text Localization Network
    (DTLN) and Context Reasoning Text Retrieval (CRTR) Models. For an input image,
    the DTLN model directly decodes the CNN features into a variable length set of
    text instance candidates. The CRTR model pools the information from three different
    LSTM models, and jointly scores and ranks the candidate text regions which are
    generated by DTLN. The rightmost recurrent text recognition module helps further
    extend the applicability of DTLN and CRTR models, by transcribing the retrieved
    text regions to precise literal character codes for verification.
  Figure 3 Link: articels_figures_by_rev_year\2020\Unambiguous_Text_Localization_Retrieval_and_Recognition_for_Cluttered_Scenes\figure_3.jpg
  Figure 3 caption: Example results of scene text localization. The green bounding
    boxes contain correct detections; The red bounding boxes contain false positives;
    The yellow bounding boxes contain false negatives.
  Figure 4 Link: articels_figures_by_rev_year\2020\Unambiguous_Text_Localization_Retrieval_and_Recognition_for_Cluttered_Scenes\figure_4.jpg
  Figure 4 caption: Text region retrieval and recognition results of the proposed
    Context Reasoning Text Retrieval (CRTR) model on the COCO-TextRef dataset. At
    first, red boxes are employed to denote context concepts. Then green boxes are
    added to identify the successfully retrieved text regions associated with the
    context concepts. The remaining text regions are marked by yellow boxes. The input
    queries and output recognition results are listed below each image.
  Figure 5 Link: articels_figures_by_rev_year\2020\Unambiguous_Text_Localization_Retrieval_and_Recognition_for_Cluttered_Scenes\figure_5.jpg
  Figure 5 caption: "Challenging samples of scene text from COCO-TextRef dataset,\
    \ which are correctly recognized by the recurrent text recognition model: \u201C\
    AMER,\u201D \u201CYALLE,\u201D \u201Cplus,\u201D \u201CFAIRHAVEN,\u201D \u201C\
    ASTED,\u201D \u201CLIBERTY,\u201D \u201CFORCE,\u201D \u201CCLEARANCE,\u201D \u201C\
    193,\u201D \u201CSWEET,\u201D \u201CSauza,\u201D \u201CPROCTOR,\u201D \u201CKawasaki,\u201D\
    \ \u201CPatcham,\u201D and \u201CTELEPHONE\u201D."
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.86
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.74
  Name of the first author: Xuejian Rong
  Name of the last author: Yingli Tian
  Number of Figures: 5
  Number of Tables: 5
  Number of authors: 3
  Paper title: Unambiguous Text Localization, Retrieval, and Recognition for Cluttered
    Scenes
  Publication Date: 2020-08-21 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance Comparison Between Our Proposed Framework With
      Previous Scene Text Localization Approaches on ICDAR 2013 [80] and SVT Datasets
      [81] in Terms of the Measures of PASCAL Eval [82] and DetEval [83]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 This Table Presents the Top-1 Precision of Our Method Compared
      With Previous Methods on Annotated Ground Truth Bounding Boxes on the COCO-TextRef
      Dataset
  Table 3 caption:
    table_text: TABLE 3 This Table Presents the Top-1, Top-5 Recalls of Our Method
      Compared With Previous Methods With Detected Text Regions Generated by the Proposed
      DTLN Method on the COCO-TextRef Dataset
  Table 4 caption:
    table_text: TABLE 4 Text Recognition Results (f-Measure Value) on ICDAR 2013,
      ICDAR 2015, and SVT Datasets With the Recurrent Recognition Model
  Table 5 caption:
    table_text: 'TABLE 5 Refined Detection Results With Recognition on ICDAR 2013
      Dataset: Precision (P), Recall (R), and f-Measure (F)'
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3018491
- Affiliation of the first author: department of computer science, peking university,
    beijing, china
  Affiliation of the last author: center on frontiers of computing studies, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2020\Locally_Connected_Network_for_Monocular_D_Human_Pose_Estimation\figure_1.jpg
  Figure 1 caption: The human pose graph used in this work which consists of 17 nodes
    and 20 edges.
  Figure 10 Link: articels_figures_by_rev_year\2020\Locally_Connected_Network_for_Monocular_D_Human_Pose_Estimation\figure_10.jpg
  Figure 10 caption: MPJPE errors of different methods in the cross-action experiment.
    FCN [6] and LCN are respectively trained on each of the 15 actions and tested
    on all of them. The X -axis shows the action class used in training.
  Figure 2 Link: articels_figures_by_rev_year\2020\Locally_Connected_Network_for_Monocular_D_Human_Pose_Estimation\figure_2.jpg
  Figure 2 caption: Comparison of a single layer of FCN, GCN, and LCN. The input is
    a graph with three nodes and two edges. (a) In a single FCN layer, input features
    of different nodes are mixed making every output feature dependent on all nodes.
    (b) In a single GCN layer, the output features of a node only depend on its connected
    nodes. For example, when we compute the features for the node B (top branch),
    it only takes the features of nodes A and B . Different nodes share the same filter
    T . (c) In a single LCN layer, each node has a different filter. We will discuss
    them in detail in the paper.
  Figure 3 Link: articels_figures_by_rev_year\2020\Locally_Connected_Network_for_Monocular_D_Human_Pose_Estimation\figure_3.jpg
  Figure 3 caption: Overview of our end-to-end 3D pose estimator. For an input image,
    it first predicts a heatmap for each joint followed by a spatial integration step
    to compute 2D joint positions in the image. Then the 2D pose is fed to LCN to
    estimate the corresponding 3D pose. The two networks are jointly trained end-to-end.
  Figure 4 Link: articels_figures_by_rev_year\2020\Locally_Connected_Network_for_Monocular_D_Human_Pose_Estimation\figure_4.jpg
  Figure 4 caption: "Illustration of an LCN layer. Suppose we have 3 nodes whose features\
    \ are represented as the blue bars. The weight matrix W and structure matrix S\u2208\
    \ R 3 M input \xD73 M output are evenly divided into 3\xD73 blocks. \u2299 denotes\
    \ the element-wise product operator. We fill zeros in the structure matrix at\
    \ appropriate locations to remove the dependence between the corresponding pairs\
    \ of joints. For example, output features of node 1 only depend on the input features\
    \ of nodes 1 and 3. While in the weight matrix W , all weight elements are independent\
    \ and learned end-to-end via back-propagation."
  Figure 5 Link: articels_figures_by_rev_year\2020\Locally_Connected_Network_for_Monocular_D_Human_Pose_Estimation\figure_5.jpg
  Figure 5 caption: Structure of the LCN pose estimation network. The input is a 2D
    pose and the output is a 3D pose. It consists of B LCN units. Each LCN unit consists
    of one LCN layer followed by batch normalization, LeakyRelu, and dropout.
  Figure 6 Link: articels_figures_by_rev_year\2020\Locally_Connected_Network_for_Monocular_D_Human_Pose_Estimation\figure_6.jpg
  Figure 6 caption: "(a-e) The computed 2D poses when we use different \u03B1 values\
    \ to normalize heatmaps. (f) 2D pose obtained by finding maximum heatmap response\
    \ in each heatmap. (g) Ground-truth heatmap."
  Figure 7 Link: articels_figures_by_rev_year\2020\Locally_Connected_Network_for_Monocular_D_Human_Pose_Estimation\figure_7.jpg
  Figure 7 caption: Structure matrices learned by LCN (2-NN)-Learn (a) and LCN-Learn
    (b), respectively. The X - and Y -axes denote joint indices. See Fig. 1 for joint
    definition.
  Figure 8 Link: articels_figures_by_rev_year\2020\Locally_Connected_Network_for_Monocular_D_Human_Pose_Estimation\figure_8.jpg
  Figure 8 caption: Impact of network depth (left) and a variety of structure matrix
    construction methods (right). The 2D pose estimator is finetuned on the H36M dataset.
  Figure 9 Link: articels_figures_by_rev_year\2020\Locally_Connected_Network_for_Monocular_D_Human_Pose_Estimation\figure_9.jpg
  Figure 9 caption: MPJPE error increase for the rest of the joints when some joints
    are disconnected.
  First author gender probability: 0.85
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Hai Ci
  Name of the last author: Yizhou Wang
  Number of Figures: 13
  Number of Tables: 6
  Number of authors: 4
  Paper title: Locally Connected Network for Monocular 3D Human Pose Estimation
  Publication Date: 2020-08-24 00:00:00
  Table 1 caption:
    table_text: TABLE 1 MPJPE Errors of Our Approach and FCN [6] Using Different Coordinate
      Systems on the H36M Dataset Under Protocol 1
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 MPJPE Errors on the H36M Dataset Under Protocol 1
  Table 3 caption:
    table_text: TABLE 3 MPJPE Errors of the State-of-the-Art Methods on the H36M Dataset
  Table 4 caption:
    table_text: TABLE 4 3D Pose Estimation Results of the State-of-the-art Approaches
      on the MPI-INF-3DHP Dataset
  Table 5 caption:
    table_text: TABLE 5 Change in MPJPE Error When Different Body Parts Are Cut From
      the Pose Graph
  Table 6 caption:
    table_text: TABLE 6 Ablative Study on Joint Training
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3019139
- Affiliation of the first author: diagnostic image analysis group, radboud institute
    for health sciences, radboud university medical center, nijmegen, ga, the netherlands
  Affiliation of the last author: diagnostic image analysis group, radboud institute
    for health sciences, radboud university medical center, nijmegen, ga, the netherlands
  Figure 1 Link: articels_figures_by_rev_year\2020\Streaming_Convolutional_Neural_Networks_for_EndtoEnd_Learning_With_MultiMegapixe\figure_1.jpg
  Figure 1 caption: "Schematic overview of streaming in a one-dimensional case, using\
    \ a 1\xD73 kernel. We can calculate the same gradients with smaller input and\
    \ output shapes, saving memory. Colored in red are incomplete gradients of the\
    \ input tile, illustrating that if we chain two of these one-dimensional convolutions\
    \ together, more overlap is needed (four values instead of the two pictured).\
    \ During streaming, the gradients of the convolutional kernel are not reset between\
    \ tiles and are summed with the previous gradients. After the three tiles have\
    \ been backpropagated, the kernel gradients are equal to the normal case. Gradient\
    \ checkpointing is also used to save additional memory, but is omitted here for\
    \ clarity. Numbers denote the order of serial steps taken by the respective algorithms."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Streaming_Convolutional_Neural_Networks_for_EndtoEnd_Learning_With_MultiMegapixe\figure_2.jpg
  Figure 2 caption: "This graph shows the linear relationship between the size of\
    \ the tile and the memory required to stream an input of size 2048\xD72048. Since\
    \ the amount of overlap is minimal in this example (16 pixels), the time does\
    \ not increase until 81 tiles are needed. Performance shown of three 2D-convolutional\
    \ layers with respectively 3, 64, and 3 output channels and kernel-size 3 on an\
    \ RTX 2080Ti GPU. All numbers are averaged over 10 runs."
  Figure 3 Link: articels_figures_by_rev_year\2020\Streaming_Convolutional_Neural_Networks_for_EndtoEnd_Learning_With_MultiMegapixe\figure_3.jpg
  Figure 3 caption: "This graph shows the linear relationship with time required when\
    \ increasing the input size (keeping the tile size constant). With streaming,\
    \ there is significantly less memory required (up to 97 percent). The memory increase\
    \ with streaming is due to the size of the input and output gradient. The amount\
    \ of memory required to train with images above 2048 \xD7 2048 without streaming\
    \ is estimated. Performance shown of three 2D-convolutional layers with respectively\
    \ 3, 64, and 3 output channels and kernel-size 3 on an RTX 2080Ti GPU. All numbers\
    \ are averaged over 10 runs."
  Figure 4 Link: articels_figures_by_rev_year\2020\Streaming_Convolutional_Neural_Networks_for_EndtoEnd_Learning_With_MultiMegapixe\figure_4.jpg
  Figure 4 caption: "Network trained from the same initialization using conventional\
    \ training and streaming (dividing the input in 400 tiles of 32 \xD7 32). Showing\
    \ identical loss and accuracy curves."
  Figure 5 Link: articels_figures_by_rev_year\2020\Streaming_Convolutional_Neural_Networks_for_EndtoEnd_Learning_With_MultiMegapixe\figure_5.jpg
  Figure 5 caption: Saliency maps for test set images of the TUPAC16 experiment using
    the best performing models. The TUPAC16 network shows highlights in cell-dense
    and cancerous regions. There is a trend in which the higher the input solution
    of the model, the less it focuses on healthy tissue. Also, higher resolution models
    focus on more locations of the tissue.
  Figure 6 Link: articels_figures_by_rev_year\2020\Streaming_Convolutional_Neural_Networks_for_EndtoEnd_Learning_With_MultiMegapixe\figure_6.jpg
  Figure 6 caption: Saliency maps for images of the tuning set of the CAMELYON17 experiment.
    The highest resolution model, trained on image-level labels shows highlights corresponding
    to the ground truth pixel-level annotation of a breast cancer metastasis. The
    lower resolution models have lower probability for the ground truth class and
    show little correspondence to the location of the metastases. The last row shows
    a micro metastasis for which models failed to recognize.
  Figure 7 Link: articels_figures_by_rev_year\2020\Streaming_Convolutional_Neural_Networks_for_EndtoEnd_Learning_With_MultiMegapixe\figure_7.jpg
  Figure 7 caption: Resolution examples of resized whole slide images. Example slide
    of TUPAC16 showing tumor, illustrating the increasing detail with increasing input
    size.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Hans Pinckaers
  Name of the last author: Geert Litjens
  Number of Figures: 7
  Number of Tables: 5
  Number of authors: 3
  Paper title: Streaming Convolutional Neural Networks for End-to-End Learning With
    Multi-Megapixel Images
  Publication Date: 2020-08-26 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Network Architecture for Imagenette Experiment
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Network Architecture for TUPAC16 Experiments
  Table 3 caption:
    table_text: 'TABLE 3 TUPAC16: Performance of the Models on the Independent Test
      Test, Spearmans Rho Correlation Coefficient'
  Table 4 caption:
    table_text: 'TABLE 4 TUPAC16: Leaderboard'
  Table 5 caption:
    table_text: TABLE 5 CAMELYON17 Results on the Independent Test Set (AUC)
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3019563
- Affiliation of the first author: intelligent systems lab, intel labs, munich, germany
  Affiliation of the last author: intelligent systems lab, intel labs, santa clara,
    ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Towards_Robust_Monocular_Depth_Estimation_Mixing_Datasets_for_ZeroShot_CrossData\figure_1.jpg
  Figure 1 caption: 'We show how to leverage training data from multiple, complementary
    sources for single-view depth estimation, in spite of varying and unknown depth
    range and scale. Our approach enables strong generalization across datasets. Top:
    input images. Middle: inverse depth maps predicted by the presented approach.
    Bottom: corresponding point clouds rendered from a novel view-point. Point clouds
    rendered via Open3D [4]. Input images from the Microsoft COCO dataset [5], which
    was not seen during training.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Towards_Robust_Monocular_Depth_Estimation_Mixing_Datasets_for_ZeroShot_CrossData\figure_2.jpg
  Figure 2 caption: Sample images from the 3D movies dataset. We show images from
    some of the films in the training set together with their inverse depth maps.
    Sky regions and invalid pixels are masked out. Each image is taken from a different
    film. 3D movies provide a massive source of diverse data.
  Figure 3 Link: articels_figures_by_rev_year\2020\Towards_Robust_Monocular_Depth_Estimation_Mixing_Datasets_for_ZeroShot_CrossData\figure_3.jpg
  Figure 3 caption: Relative performance of different loss functions (higher is better)
    with the best performing loss L ssitrim + L reg used as reference. All our four
    proposed losses (white area) outperform current state-of-the-art losses (gray
    area).
  Figure 4 Link: articels_figures_by_rev_year\2020\Towards_Robust_Monocular_Depth_Estimation_Mixing_Datasets_for_ZeroShot_CrossData\figure_4.jpg
  Figure 4 caption: Relative performance of different encoders across datasets (higher
    is better). ImageNet performance of an encoder is predictive of its performance
    in monocular depth estimation.
  Figure 5 Link: articels_figures_by_rev_year\2020\Towards_Robust_Monocular_Depth_Estimation_Mixing_Datasets_for_ZeroShot_CrossData\figure_5.jpg
  Figure 5 caption: Comparison of models trained on different combinations of datasets
    using pareto-optimal mixing. Images from Microsoft COCO [5].
  Figure 6 Link: articels_figures_by_rev_year\2020\Towards_Robust_Monocular_Depth_Estimation_Mixing_Datasets_for_ZeroShot_CrossData\figure_6.jpg
  Figure 6 caption: Qualitative comparison of our approach to the four best competitors
    on images from the Microsoft COCO dataset [5].
  Figure 7 Link: articels_figures_by_rev_year\2020\Towards_Robust_Monocular_Depth_Estimation_Mixing_Datasets_for_ZeroShot_CrossData\figure_7.jpg
  Figure 7 caption: Qualitative results on the DIW test set.
  Figure 8 Link: articels_figures_by_rev_year\2020\Towards_Robust_Monocular_Depth_Estimation_Mixing_Datasets_for_ZeroShot_CrossData\figure_8.jpg
  Figure 8 caption: "Results on paintings and drawings. Top row: A Friend in Need,\
    \ Cassius Marcellus Coolidge, and Bathers at Asni\xE9res, Georges Pierre Seurat.\
    \ Bottom row: Mittagsrast, Vincent van Gogh, and Vector drawing of central street\
    \ of old european town, Vilnius, Misha."
  Figure 9 Link: articels_figures_by_rev_year\2020\Towards_Robust_Monocular_Depth_Estimation_Mixing_Datasets_for_ZeroShot_CrossData\figure_9.jpg
  Figure 9 caption: Failure cases. Subtle failures in relative depth arrangement or
    missing details are highlighted in green.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: "Ren\xE9 Ranftl"
  Name of the last author: Vladlen Koltun
  Number of Figures: 9
  Number of Tables: 11
  Number of authors: 5
  Paper title: 'Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot
    Cross-Dataset Transfer'
  Publication Date: 2020-08-27 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Datasets Used in Our Work
  Table 10 caption:
    table_text: "TABLE 10 Relative Performance of State of the Art Methods With Respect\
      \ to Our Best Model (Top Row) \u2013 Higher is Better"
  Table 2 caption:
    table_text: TABLE 2 List of Films and the Number of Extracted Frames in the 3D
      Movies Dataset After Automatic Processing
  Table 3 caption:
    table_text: TABLE 3 Relative Performance With Respect to the Baseline in Percent
      When Fine-Tuning on Different Single Training Sets (Higher is Better)
  Table 4 caption:
    table_text: "TABLE 4 Absolute Performance When Fine-Tuning on Different Single\
      \ Training Sets \u2013 Lower is Better"
  Table 5 caption:
    table_text: TABLE 5 Combinations of Datasets Used for Training
  Table 6 caption:
    table_text: "TABLE 6 Relative Performance of Naive Dataset Mixing With Respect\
      \ to the RW Baseline (Top Row) \u2013 Higher is Better"
  Table 7 caption:
    table_text: "TABLE 7 Absolute Performance of Naive Dataset Mixing \u2013 Lower\
      \ is Better"
  Table 8 caption:
    table_text: "TABLE 8 Relative Performance of Dataset Mixing With Multi-Objective\
      \ Optimization With Respect to the RW Baseline (Top Row) \u2013 Higher is Better"
  Table 9 caption:
    table_text: "TABLE 9 Absolute Performance of Dataset Mixing With Multi-Objective\
      \ Optimization \u2013 Lower is Better"
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3019967
- Affiliation of the first author: department of national laboratory of pattern recognition,
    institute of automation, chinese academy of science, beijing, china
  Affiliation of the last author: school of artificial intelligence, university of
    chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2020\DATA_Differentiable_ArchiTecture_Approximation_With_Distribution_Guided_Sampling\figure_1.jpg
  Figure 1 caption: "A conceptual visualization for the searching process with M=2\
    \ . (a) First, an architecture (i.e., directed acyclic graph) consisting of four\
    \ ordered nodes is predefined. (b) In the searching process, with three candidate\
    \ primitive operations (i.e., green, orange and cyan lines), the binary function\
    \ f(\u22C5) is employed to generate a path according to corresponding probabilities\
    \ in a differentiable manner for M times. The sampled paths are ensembled to generate\
    \ a new architecture. (c) Finally, the details of the cell can be generated according\
    \ to the learned architecture probabilities."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\DATA_Differentiable_ArchiTecture_Approximation_With_Distribution_Guided_Sampling\figure_2.jpg
  Figure 2 caption: A visualized comparison between Softmax, Gumbel-Softmax and ensemble
    Gumbel-Softmax ( M=2 ). For a probability vector p=[0.5,0.5] , Gumbel-Softmax
    solely pertains to sample only two binary codes with the same probability, i.e.,
    P([1,0])=P([0,1])=0.5 . In contrast, our ensemble Gumbel-Softmax is capable of
    sampling more diversified binary codes, i.e., [1,0], [1,1] and [0,1]. Furthermore,
    the probabilities of sampling these binary codes are logical. Typically, it is
    conceptually intuitive that the probability of sampling [1,1] is larger than the
    probabilities of sampling the others since the probabilities in p=[0.5,0.5] are
    equal to each other.
  Figure 3 Link: articels_figures_by_rev_year\2020\DATA_Differentiable_ArchiTecture_Approximation_With_Distribution_Guided_Sampling\figure_3.jpg
  Figure 3 caption: 'Architectures learned on different tasks with DATA, where the
    five operations: separable convolution, dilated convolution, max pooling, average
    pooling and identity mappling are represented by sc, dc, mp, ap and id in short.
    Normal cells and reduction cells learned on classification task with (a) (b) M
    = 4 and (c) (d) M = 7. (e) (f) Normal cells and reduction cells learned on one-shot
    learning task, (g) (h) five-shot learning task and (i) (j) unsuperwised clustering
    task. (k) Recurrent cell learned on language modeling task.'
  Figure 4 Link: articels_figures_by_rev_year\2020\DATA_Differentiable_ArchiTecture_Approximation_With_Distribution_Guided_Sampling\figure_4.jpg
  Figure 4 caption: The sampling process of DATA is actually transforming the learned
    distribution into binary distribution, inducting the distribution discrepancy
    between derived architecture and the supernet.
  Figure 5 Link: articels_figures_by_rev_year\2020\DATA_Differentiable_ArchiTecture_Approximation_With_Distribution_Guided_Sampling\figure_5.jpg
  Figure 5 caption: Evolution of normal cells during searching. The children architectures
    obtained at the (a) 25-th, (b) 50-th, (c) 75-th, (d) 100-th epoches.
  Figure 6 Link: articels_figures_by_rev_year\2020\DATA_Differentiable_ArchiTecture_Approximation_With_Distribution_Guided_Sampling\figure_6.jpg
  Figure 6 caption: Obtained architecture distributions in the searching process.
    The distributions of norm cell and reduction cell are shown in the first row and
    the second row respectively.
  Figure 7 Link: articels_figures_by_rev_year\2020\DATA_Differentiable_ArchiTecture_Approximation_With_Distribution_Guided_Sampling\figure_7.jpg
  Figure 7 caption: Searching process of DATA for convolutional architecture on CIFAR-10.
    (a) The validation accuracy of supernet in the searching process. (b) The visualized
    comparison of searched architectures between different search epochs. (c) The
    visualized effect of architecture distribution constrain (ADC) on validation accuracy
    in the searching process. (d) The visualized effect of sampling times on supernet
    in during searching, bold curves denote training accuracy and thin curves denote
    validation accuracy.
  Figure 8 Link: articels_figures_by_rev_year\2020\DATA_Differentiable_ArchiTecture_Approximation_With_Distribution_Guided_Sampling\figure_8.jpg
  Figure 8 caption: (a) Performance of child architectures obtained with different
    sampling times. (b) Effect of different regulations on mean accuracy and sampling
    variance of supernet. (c) Validation accuracy and sampling variance of supernet
    under different L1 regulation weights during searching. (d) Performance of child
    architectures under different number of searching epochs. (e) Performance of child
    networks obtained with different data divisions, where m:n represents the ratio
    of data division for optimizing weights and architecture probabilities. (f) Effect
    of different searching steps for network weights and probabilities parameters,
    where m:n represents optimizing network weights for m steps followed by n steps
    for architecture probabilities for every m+n steps. (g) Frequency of the operations
    selected by the obtained architectures. (h) Frequency of the incorporation of
    every two operations applied by the obtained architectures.
  Figure 9 Link: articels_figures_by_rev_year\2020\DATA_Differentiable_ArchiTecture_Approximation_With_Distribution_Guided_Sampling\figure_9.jpg
  Figure 9 caption: Architecture distributions obtained under the effects of different
    kinds of regulations on the CIFAR-10 dataset. We visualize the distributions of
    norm cells in the first row while the reduction cells in the second row. (a) No
    regulation. (b) L1 loss function. (c) L2 loss function. (d) KL-divergence loss
    function. (e) Entropy loss function.
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.78
  Name of the first author: Xinbang Zhang
  Name of the last author: Chunhong Pan
  Number of Figures: 9
  Number of Tables: 11
  Number of authors: 7
  Paper title: 'DATA: Differentiable ArchiTecture Approximation With Distribution
    Guided Sampling'
  Publication Date: 2020-08-31 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Image Classification Architectures on CIFAR-10
  Table 10 caption:
    table_text: TABLE 10 Sensitivity to Initialization on CIFAR-10
  Table 2 caption:
    table_text: TABLE 2 Validation Accuracy Gap on CIFAR-10 (Lower Error Rate is Better)
  Table 3 caption:
    table_text: TABLE 3 Comparison With Classifiers on ImageNet in the Mobile Setting
      (Lower Test Error is Better)
  Table 4 caption:
    table_text: TABLE 4 Semantic Segmentation on the PASCAL VOC-2012
  Table 5 caption:
    table_text: TABLE 5 Comparison of Few Shot Learning Algorithms on MiniImageNet
      (Architectures With are Obtained on CIFAR-10 Classification Task)
  Table 6 caption:
    table_text: TABLE 6 Comparison of Unsupervised Clustering Methods on CIFAR-10
      (Architectures With are Obtained on CIFAR-10 Classification Task)
  Table 7 caption:
    table_text: TABLE 7 Comparison With State-of-the-Art Language Models on PTB
  Table 8 caption:
    table_text: TABLE 8 Comparison With State-of-the-Art Language Models on WT2
  Table 9 caption:
    table_text: TABLE 9 Sensitivity to the Number of Operations for DARTS [27] on
      CIFAR-10
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3020315
