- Affiliation of the first author: "department of computer science, universidad de\
    \ la sabana, ch\xEDa, cundinamarca, colombia"
  Affiliation of the last author: department of computer science, paderborn university,
    paderborn, germany
  Figure 1 Link: articels_figures_by_rev_year\2021\Predicting_Machine_Learning_Pipeline_Runtimes_in_the_Context_of_Automated_Machin\figure_1.jpg
  Figure 1 caption: The predictor resides inside the evaluation module and is, hence,
    isolated from the search engine.
  Figure 10 Link: articels_figures_by_rev_year\2021\Predicting_Machine_Learning_Pipeline_Runtimes_in_the_Context_of_Automated_Machin\figure_10.jpg
  Figure 10 caption: Wasted computation times, numbers of successful evaluations,
    and hours spent in successful evaluations.
  Figure 2 Link: articels_figures_by_rev_year\2021\Predicting_Machine_Learning_Pipeline_Runtimes_in_the_Context_of_Automated_Machin\figure_2.jpg
  Figure 2 caption: Runtimes of DecisionTable.
  Figure 3 Link: articels_figures_by_rev_year\2021\Predicting_Machine_Learning_Pipeline_Runtimes_in_the_Context_of_Automated_Machin\figure_3.jpg
  Figure 3 caption: Learning curves of the three considered regressors for train time
    (left) and prediction time (right).
  Figure 4 Link: articels_figures_by_rev_year\2021\Predicting_Machine_Learning_Pipeline_Runtimes_in_the_Context_of_Automated_Machin\figure_4.jpg
  Figure 4 caption: "Runtime prediction performances of a random forest with 100 trees\
    \ on datasets of size 50000\xD71000 . All 170 source datasets are considered,\
    \ and a range of different parametrizations is considered for each base algorithm.\
    \ Left and right column: Prediction results for training times and prediction\
    \ times respectively. Rows group the ground truth runtimes into different bins\
    \ in order to ease interpretation of the boxplots. Colors indicate degrees of\
    \ acceptability for the prediction error."
  Figure 5 Link: articels_figures_by_rev_year\2021\Predicting_Machine_Learning_Pipeline_Runtimes_in_the_Context_of_Automated_Machin\figure_5.jpg
  Figure 5 caption: 'Top figure: Number of rejected executions. Bottom figures: (1)
    + (2) Number of correctlyincorrectly rejected executions relative to the number
    of total rejects. (3) + (4) number of correctlyincorrectly permitted executions
    relative to the number of total accepts.'
  Figure 6 Link: articels_figures_by_rev_year\2021\Predicting_Machine_Learning_Pipeline_Runtimes_in_the_Context_of_Automated_Machin\figure_6.jpg
  Figure 6 caption: "Absolute (Abs) and relative (Rel) comparison of prediction performances\
    \ between the standard model M \u03B8 and the models M \u2212\u03B8 for default\
    \ parametrization (Def) and models M \u03B8+d for posteriors (Pos). Blue indicates\
    \ improvements, white is neutral, and red indicates deteriorations."
  Figure 7 Link: articels_figures_by_rev_year\2021\Predicting_Machine_Learning_Pipeline_Runtimes_in_the_Context_of_Automated_Machin\figure_7.jpg
  Figure 7 caption: Errors in the predicted number of features after pre-processor
    application with linear regression.
  Figure 8 Link: articels_figures_by_rev_year\2021\Predicting_Machine_Learning_Pipeline_Runtimes_in_the_Context_of_Automated_Machin\figure_8.jpg
  Figure 8 caption: Spy-wrapped base learners b 1 ,.., b k leak feature transformations
    of the meta-learner and base learner runtimes.
  Figure 9 Link: articels_figures_by_rev_year\2021\Predicting_Machine_Learning_Pipeline_Runtimes_in_the_Context_of_Automated_Machin\figure_9.jpg
  Figure 9 caption: 'Top 2 plots: Ratios of base learner trainprediction time and
    total meta-learner trainprediction time respectively. Bottom 2 plots: Absrel.
    difference of total true meta-learner runtime and the time predicted by (2) (in
    seconds).'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.58
  Name of the first author: Felix Mohr
  Name of the last author: "Eyke H\xFCllermeier"
  Number of Figures: 10
  Number of Tables: 1
  Number of authors: 4
  Paper title: Predicting Machine Learning Pipeline Runtimes in the Context of Automated
    Machine Learning
  Publication Date: 2021-02-04 00:00:00
  Table 1 caption: TABLE 1 Comparison of Overall Error Rates in ML-Plan
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3056950
- Affiliation of the first author: department electrical engineering, center for processing
    speech and images, leuven, belgium
  Affiliation of the last author: department electrical engineering, center for processing
    speech and images, leuven, belgium
  Figure 1 Link: articels_figures_by_rev_year\2021\A_Continual_Learning_Survey_Defying_Forgetting_in_Classification_Tasks\figure_1.jpg
  Figure 1 caption: A tree diagram illustrating the different continual learning families
    of methods and the different branches within each family. The leaves enlist example
    methods.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\A_Continual_Learning_Survey_Defying_Forgetting_in_Classification_Tasks\figure_2.jpg
  Figure 2 caption: Parameter isolation and regularization-based methods (top) and
    replay methods (bottom) on Tiny Imagenet for the base model with random ordering,
    reporting average accuracy (forgetting) in the legend.
  Figure 3 Link: articels_figures_by_rev_year\2021\A_Continual_Learning_Survey_Defying_Forgetting_in_Classification_Tasks\figure_3.jpg
  Figure 3 caption: RecogSeq dataset sequence results, reporting average accuracy
    (forgetting).
  Figure 4 Link: articels_figures_by_rev_year\2021\A_Continual_Learning_Survey_Defying_Forgetting_in_Classification_Tasks\figure_4.jpg
  Figure 4 caption: Continual learning methods accuracy plots for three different
    orderings of the iNaturalist dataset.
  Figure 5 Link: articels_figures_by_rev_year\2021\A_Continual_Learning_Survey_Defying_Forgetting_in_Classification_Tasks\figure_5.jpg
  Figure 5 caption: Main setup of related machine learning fields, illustrating the
    differences with general continual learning settings.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.96
  Name of the first author: Matthias De Lange
  Name of the last author: Tinne Tuytelaars
  Number of Figures: 5
  Number of Tables: 10
  Number of authors: 8
  Paper title: 'A Continual Learning Survey: Defying Forgetting in Classification
    Tasks'
  Publication Date: 2021-02-05 00:00:00
  Table 1 caption: TABLE 1 The Balanced Tiny Imagenet, and Unbalanced iNaturalist
    and RecogSeq Dataset Characteristics
  Table 10 caption: TABLE 10 Summary of Our Main Findings
  Table 2 caption: TABLE 2 The RecogSeq Dataset Sequence Details
  Table 3 caption: TABLE 3 Models Used for Tiny Imagenet, iNaturalist, and RecogSeq
    Experiments
  Table 4 caption: TABLE 4 Parameter Isolation and Regularization-Based Methods results
    on Tiny Imagenet for Different Models With Random (Top), Easy to Hard (Middle)
    and Hard to Easy (Bottom) Ordering of Tasks, Reporting Average Accuracy (Forgetting)
  Table 5 caption: TABLE 5 Replay Methods Results on Tiny Imagenet for Different Models
    wzith Random (top), Easy to Hard (Middle) and Hard to Easy (Bottom) Ordering of
    Tasks, Reporting Average Accuracy (Forgetting)
  Table 6 caption: 'TABLE 6 Parameter Isolation and Regularization-Based Methods:
    Dropout and Weight Decay for Tiny Imagenet Models'
  Table 7 caption: "TABLE 7 Replay methods: Dropout ( p=0.5 p=0.5) and Weight Decay\
    \ ( \u03BB= 10 \u22124 \u03BB=10-4) Regularization for Tiny Imagenet Models"
  Table 8 caption: TABLE 8 The Unbalanced iNaturalist Task Sequence Details for Random,
    Related, and Unrelated Orderings
  Table 9 caption: TABLE 9 Qualitative Comparison of the Compared Continual Learning
    Methods
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3057446
- Affiliation of the first author: department of computer science and engineering,
    michigan state university, east lansing, mi, usa
  Affiliation of the last author: department of computer science and engineering,
    michigan state university, east lansing, mi, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\InfantID_Fingerprints_for_Global_Good\figure_1.jpg
  Figure 1 caption: Face images (top row) and corresponding left thumb fingerprints
    (bottom row) of six different infants under 3 months of age. Face images were
    captured by a Xiaomi MI A1 smartphone camera and fingerprint images were captured
    by the 1,900 ppi RaspiReader designed by Engelsma et al. [1], [2] at the Saran
    Ashram Hospital, a charitable organization in Dayalbagh, Agra, India.
  Figure 10 Link: articels_figures_by_rev_year\2021\InfantID_Fingerprints_for_Global_Good\figure_10.jpg
  Figure 10 caption: View of the manual minutiae markupediting software used to markup
    minutiae locations on a subset of infant fingerprint images. These markups were
    later used as ground truth to train our high resolution infant minutiae extractor.
    The fingerprint on the left (blue annotations) is coarsely annotated with Verifinger
    v10 SDK to help speed up the annotation process. The fingerprint on the right
    (red annotations) shows the manually edited minutiae.
  Figure 2 Link: articels_figures_by_rev_year\2021\InfantID_Fingerprints_for_Global_Good\figure_2.jpg
  Figure 2 caption: Face images (top row) and corresponding left thumb fingerprints
    (bottom row) of an infant, Meena Kumari, acquired on (a) December 16, 2018 (Meena
    was 3 months old), (b) December 18, 2018 (3 months, 2 days old), (c) March 5,
    2019 (6 months old), and (d) September 17, 2019 (12 months old) at Saran Ashram
    Hospital, Dayalbagh, India. Note that as Meena ages, fingerprint details emerge
    such as visible pores. This level of detail is enabled by our 1,900 ppi reader.
  Figure 3 Link: articels_figures_by_rev_year\2021\InfantID_Fingerprints_for_Global_Good\figure_3.jpg
  Figure 3 caption: Overview of the Infant-Prints system.
  Figure 4 Link: articels_figures_by_rev_year\2021\InfantID_Fingerprints_for_Global_Good\figure_4.jpg
  Figure 4 caption: "Prototype of the 1,900 ppi compact (1\u201D x 2\u201D x 3\u201D\
    ), ergonomic fingerprint reader. An infants finger is placed on the glass prism\
    \ with the operator applying slight pressure on the finger. The capture time is\
    \ 500 milliseconds. The prototype can be assembled in less than 2 hours. See the\
    \ video at: https:www.youtube.comwatch?v=f8tYbE9Cwd0."
  Figure 5 Link: articels_figures_by_rev_year\2021\InfantID_Fingerprints_for_Global_Good\figure_5.jpg
  Figure 5 caption: An infants fingerprints are acquired via (a) a 500 ppi commercial
    reader (Digital Persona U.are.U 4500) and (b) our custom 1,900 RaspiReader. The
    captured fingerprint images of the right thumb from the commercial reader and
    the Infant-Prints reader for a 13 day old infant are shown in (c) and (d), respectively.
    Manually annotated minutiae are shown in red circles (location) with a tail (orientation).
    Blue arrows denote pores on the ridges.
  Figure 6 Link: articels_figures_by_rev_year\2021\InfantID_Fingerprints_for_Global_Good\figure_6.jpg
  Figure 6 caption: (a) Prototype of our 1,900 ppi contactless fingerprint reader.
    During capture, an infants finger is placed on top of a small, contactless, rectangular
    opening (annotated in red) on the reader (the size of this opening can be adjusted
    with different sized slots). A camera captures the infants fingerprint from behind
    the rectangular opening. Examples of a processed (segmented, contrast enhanced),
    contactless infant thumb-print (2 months old) is shown in (b) and the same infants
    thumb-print acquired via contact-based fingerprint reader in (c).
  Figure 7 Link: articels_figures_by_rev_year\2021\InfantID_Fingerprints_for_Global_Good\figure_7.jpg
  Figure 7 caption: Infant fingerprint collection at Saran Ashram hospital, Dayalbagh,
    India. Pediatrician, Dr. Anjoo Bhatnagar, explaining longitudinal fingerprint
    study to the mothers while the authors are acquiring an infants fingerprints in
    her clinic. Parents also sign a consent form approved by the Institutional Review
    Board (IRB) of our organizations.
  Figure 8 Link: articels_figures_by_rev_year\2021\InfantID_Fingerprints_for_Global_Good\figure_8.jpg
  Figure 8 caption: "Overview of the minutiae extraction algorithm. An input fingerprint\
    \ of any size ( n\xD7m ) is passed to the minutiae extraction network (Table 3).\
    \ The network outputs a n\xD7m\xD712 minutiae map H which encodes the minutiae\
    \ locations and orientations of the input fingerprint. Finally, the minutiae map\
    \ is converted to a minutiae set ( x 1 , y 1 , \u03B8 1 ),\u2026,( x N , y N ,\
    \ \u03B8 N ) of N minutiae."
  Figure 9 Link: articels_figures_by_rev_year\2021\InfantID_Fingerprints_for_Global_Good\figure_9.jpg
  Figure 9 caption: "An example infant fingerprint patch (a) and the corresponding\
    \ minutiae map (b). Note, we only show 3 channels of the 12 channel minutiae map\
    \ here for illustrative purposes (red channel is the first channel, green is the\
    \ fifth channel, and blue is the ninth channel). Given the full 12 channels of\
    \ the minutiae map in (b), we can compute the minutiae locations (x,y) and orientations\
    \ \u03B8 of the 1,900 ppi fingerprint patch in (a)."
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Joshua J. Engelsma
  Name of the last author: Anil K. Jain
  Number of Figures: 17
  Number of Tables: 12
  Number of authors: 6
  Paper title: 'Infant-ID: Fingerprints for Global Good'
  Publication Date: 2021-02-08 00:00:00
  Table 1 caption: TABLE 1 Related Work on Child Fingerprint Recognition
  Table 10 caption: TABLE 10 Ablated Fingerprint Reader Authentication Results
  Table 2 caption: TABLE 2 Infant Longitudinal Fingerprint Dataset Statistics
  Table 3 caption: TABLE 3 Minutiae Extraction Network
  Table 4 caption: "TABLE 4 Infant Authentication Accuracy ( 0\u22123 0-3 Months at\
    \ Enrollment With 3 Month Time Lapse Between Enrollment and Authentication)"
  Table 5 caption: "TABLE 5 Infant Search Accuracy ( 0\u22123 0-3 Months at Enrollment\
    \ With 3 Month Time Lapse Between Enrollment and Search)"
  Table 6 caption: "TABLE 6 Ablated Infant Authentication Accuracy ( 0\u22123 0-3\
    \ Months at Enrollment With 3 Month Time Lapse Between Enrollment and Authentication)"
  Table 7 caption: TABLE 7 Ablated Verifinger Performance
  Table 8 caption: TABLE 8 Ablated COTS Latent Matcher (LM) Performance
  Table 9 caption: TABLE 9 Ablated DeepPrint Performance
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3057634
- Affiliation of the first author: department of computer science, university of york,
    york, u.k.
  Affiliation of the last author: department of computer science, university of york,
    york, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2021\Outdoor_Inverse_Rendering_From_a_Single_Image_Using_Multiview_SelfSupervision\figure_1.jpg
  Figure 1 caption: Sample output from our inverse rendering network. From a single
    image (col. 1), we estimate albedo, shadow and normal maps and illumination (col.
    2-5); re-rendering of our shape with (col. 6) frontal, white point source and
    with (col. 7) estimated spherical harmonic lighting.
  Figure 10 Link: articels_figures_by_rev_year\2021\Outdoor_Inverse_Rendering_From_a_Single_Image_Using_Multiview_SelfSupervision\figure_10.jpg
  Figure 10 caption: 'Qualitative results on BigTime data [43]. Each consecutive pair
    of rows shows results for two different frames from a time-lapse sequence. Col.
    1: input, col. 2-4: albedo, col. 5-7: shading, col. 8-10: reconstruction. For
    image we show comparison between InverseRenderNet [11], Nestmeyer et al. [72],
    and ours.'
  Figure 2 Link: articels_figures_by_rev_year\2021\Outdoor_Inverse_Rendering_From_a_Single_Image_Using_Multiview_SelfSupervision\figure_2.jpg
  Figure 2 caption: At inference time, our network regresses shadow, diffuse albedo
    and normal maps from a single, uncontrolled image. These are used to infer shadow
    free and shading only images from which we compute least squares optimal spherical
    harmonic lighting coefficients. At training time, we introduce self-supervision
    via an appearance loss computed using a differentiable renderer and the estimated
    quantities.
  Figure 3 Link: articels_figures_by_rev_year\2021\Outdoor_Inverse_Rendering_From_a_Single_Image_Using_Multiview_SelfSupervision\figure_3.jpg
  Figure 3 caption: The mask for ground plane is obtained by PSPNet [63]. We then
    define the normal for masked ground plane by the normal of fitted camera plane.
  Figure 4 Link: articels_figures_by_rev_year\2021\Outdoor_Inverse_Rendering_From_a_Single_Image_Using_Multiview_SelfSupervision\figure_4.jpg
  Figure 4 caption: Statistical illumination model. The central image shows the mean
    illumination. The two diagonals and the vertical show the first three principal
    components.
  Figure 5 Link: articels_figures_by_rev_year\2021\Outdoor_Inverse_Rendering_From_a_Single_Image_Using_Multiview_SelfSupervision\figure_5.jpg
  Figure 5 caption: Cross-rendering loss. A cross-rendered image is generated by relighting
    the albedo and normal predictions from one image and the lighting prediction from
    the other. Before rendering, we rotate the lighting to align it with the new view
    given relative camera poses. The rendering is shadow free, so the shadow is removed
    from ground truth relighting image. Both shadow and ground truth relighting image
    are cross-projected from the view from which new lighting is taken.
  Figure 6 Link: articels_figures_by_rev_year\2021\Outdoor_Inverse_Rendering_From_a_Single_Image_Using_Multiview_SelfSupervision\figure_6.jpg
  Figure 6 caption: Albedo consistency loss. Given the depth map and camera parameters
    from performing MVS over image collections, the albedo prediction from one view
    can be cross-projected to another view. For each image, we compute the albedo
    consistency loss by measuring the difference between albedo prediction and cross-projected
    albedo prediction from other views within each batch.
  Figure 7 Link: articels_figures_by_rev_year\2021\Outdoor_Inverse_Rendering_From_a_Single_Image_Using_Multiview_SelfSupervision\figure_7.jpg
  Figure 7 caption: Qualitative results for reconstruction. The first and fourth rows
    are albedo predictions, and second and fifth rows are shading predictions from
    labelled methods. The reconstruction results for each method is composed by albedo
    and shading and is shown on third and last rows.
  Figure 8 Link: articels_figures_by_rev_year\2021\Outdoor_Inverse_Rendering_From_a_Single_Image_Using_Multiview_SelfSupervision\figure_8.jpg
  Figure 8 caption: 'Qualitative results for albedo consistency. For each consecutive
    pair of rows we show results for two overlapping images of the same scene. Albedo
    predictions are shown in col. 2-5 which are then cross-projected to the viewpoint
    of the other image in the pair in col. 6-9. Results shown for col. 2 and 6: BigTime
    [43], col. 3 and 7: InverseRenderNet [11], col. 4 and 8: Nestmeyer et al. [72],
    col. 5 and 9: ours.'
  Figure 9 Link: articels_figures_by_rev_year\2021\Outdoor_Inverse_Rendering_From_a_Single_Image_Using_Multiview_SelfSupervision\figure_9.jpg
  Figure 9 caption: Qualitative results for our inverse rendering benchmark. We show
    comparison against InverseRenderNet [11], BigTime [43], and SIRFS [1].
  First author gender probability: 0.54
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ye Yu
  Name of the last author: William A. P. Smith
  Number of Figures: 18
  Number of Tables: 6
  Number of authors: 2
  Paper title: Outdoor Inverse Rendering From a Single Image Using Multiview Self-Supervision
  Publication Date: 2021-02-09 00:00:00
  Table 1 caption: TABLE 1 Quantitative Inverse Rendering Results on MegaDepth Dataset
    [28]
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Reconstruction Error and Reflectance Consistency for MegaDepth
    [28] and BigTime Dataset [43]
  Table 3 caption: TABLE 3 Quantitative Results on IIW Benchmark Using WHDR Percentage
    (Lower is Better)
  Table 4 caption: TABLE 4 Quantitative Surface Normal Prediction Errors on the DIODE
    Dataset [74]
  Table 5 caption: TABLE 5 Quantitative Results for Illumination Estimation
  Table 6 caption: TABLE 6 Quantitative Comparison for Ablation Study
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3058105
- Affiliation of the first author: shanghai key laboratory of intelligent information
    processing, school of computer science, fudan university, shanghai, china
  Affiliation of the last author: institute of science and technology for brain-inspired
    intelligence, fudan university, shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2021\GaitSet_CrossView_Gait_Recognition_Through_Utilizing_Gait_As_a_Deep_Set\figure_1.jpg
  Figure 1 caption: From top-left to bottom-right are silhouettes of a completed period
    of a subject in the CASIA-B gait dataset.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\GaitSet_CrossView_Gait_Recognition_Through_Utilizing_Gait_As_a_Deep_Set\figure_2.jpg
  Figure 2 caption: "The framework of GaitSet [26] . SP represents set pooling. Trapezoids\
    \ represent convolution and pooling blocks and those in the same column have the\
    \ same configurations, as shown by the rectangles with capital letters. Note that\
    \ although the blocks in MGP have the same configurations as those in the main\
    \ pipeline, the parameters are shared only across blocks in the main pipeline\
    \ \u2013 not with those in MGP. HPP represents horizontal pyramid pooling [27]."
  Figure 3 Link: articels_figures_by_rev_year\2021\GaitSet_CrossView_Gait_Recognition_Through_Utilizing_Gait_As_a_Deep_Set\figure_3.jpg
  Figure 3 caption: "Seven different instantiations of Set Pooling (SP). 11C and cat\
    \ represent the 1\xD71 convolutional layer and the concatenate operation, respectively.\
    \ Here, n represents the number of feature maps in a set, and c , h and w denote\
    \ the number of channels, the height and width of a feature map, respectively.\
    \ a. Three basic statistical SP and two joint SP. b. Pixel-wise attention SP.\
    \ c. Frame-wise attention SP."
  Figure 4 Link: articels_figures_by_rev_year\2021\GaitSet_CrossView_Gait_Recognition_Through_Utilizing_Gait_As_a_Deep_Set\figure_4.jpg
  Figure 4 caption: The structure of horizontal pyramid mapping [26].
  Figure 5 Link: articels_figures_by_rev_year\2021\GaitSet_CrossView_Gait_Recognition_Through_Utilizing_Gait_As_a_Deep_Set\figure_5.jpg
  Figure 5 caption: The accuracy change process on OU-MVLP.
  Figure 6 Link: articels_figures_by_rev_year\2021\GaitSet_CrossView_Gait_Recognition_Through_Utilizing_Gait_As_a_Deep_Set\figure_6.jpg
  Figure 6 caption: Relationships between recognition accuracy and the HPM output
    dimension. From left to right are the individual results on the CASIA-B NM, BG,
    and CL subsets. The relationships vary with different training strategies, as
    shown by the different lines in each graph.
  Figure 7 Link: articels_figures_by_rev_year\2021\GaitSet_CrossView_Gait_Recognition_Through_Utilizing_Gait_As_a_Deep_Set\figure_7.jpg
  Figure 7 caption: Average rank-1 accuracies with constraints of silhouette volume
    on the CASIA-B dataset using the LT setting. The accuracy values are averaged
    on all 11 views excluding identical-view cases, and the final reported results
    are averaged across ten experimental repetitions.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.86
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Hanqing Chao
  Name of the last author: Jianfeng Feng
  Number of Figures: 7
  Number of Tables: 9
  Number of authors: 5
  Paper title: 'GaitSet: Cross-View Gait Recognition Through Utilizing Gait As a Deep
    Set'
  Publication Date: 2021-02-09 00:00:00
  Table 1 caption: TABLE 1 Batch Size ( B S ) (BS), Learning Rate ( L R ) (LR), and
    Training Iterations (Iter) on OU-MVLP and the Three Settings of CASIA-B (CASIA-ST,
    CASIA-MT, and CASIA-LT)
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Averaged Rank-1 Accuracies on CASIA-B Under Three Different
    Experimental Settings, Excluding Identical-View Cases
  Table 3 caption: 'TABLE 3 Averaged Rank-1 Accuracies on OU-MVLP, Excluding Identical-View
    Cases. GEINet: [18]. 3in+2diff: [4]'
  Table 4 caption: TABLE 4 Ablation Experiments Conducted on CASIA-B Using Setting
    LT
  Table 5 caption: TABLE 5 The Impact of Different HPM Scales and HPM Weight Independence
    Experiments Conducted on CASIA-B Using the Setting on LT
  Table 6 caption: TABLE 6 Different Loss Functions Conducted on CASIA-B Using Setting
    LT
  Table 7 caption: TABLE 7 The Recognition Accuracy After Dimensional Reduction With
    the New FC
  Table 8 caption: TABLE 8 Multiview Experiments Conducted on CASIA-B Using the LT
    Setting
  Table 9 caption: TABLE 9 Multiple Walking Condition Experiments Conducted on CASIA-B
    Using the LT Setting
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3057879
- Affiliation of the first author: cuhk t stone robotics institute, hong kong centre
    for logistics robotics, chinese university of hong kong (cuhk), hong kong
  Affiliation of the last author: cuhk t stone robotics institute, hong kong centre
    for logistics robotics, chinese university of hong kong (cuhk), hong kong
  Figure 1 Link: articels_figures_by_rev_year\2021\SelfSupervised_Video_Representation_Learning_by_Uncovering_SpatioTemporal_Statis\figure_1.jpg
  Figure 1 caption: Main idea of the proposed approach. Given a video sequence, we
    design a pretext task to uncover the summaries derived from spatio-temporal statistics
    for self-supervised video representation learning. Specifically, each video frame
    is first divided into several spatial regions using different partitioning patterns
    like the grid shown in the figure. Then the derived statistical labels, such as
    the region with the largest motion and its direction (the red patch), the most
    diverged region in appearance and its dominant color (the blue patch), and the
    most stable region in appearance and its dominant color (the green patch), are
    employed as supervision signals to guide the representation learning.
  Figure 10 Link: articels_figures_by_rev_year\2021\SelfSupervised_Video_Representation_Learning_by_Uncovering_SpatioTemporal_Statis\figure_10.jpg
  Figure 10 caption: 'Attention visualization. For each sample from top to bottom:
    A frame from a video clip, activation based attention map of conv5 layer on the
    frame by using [81], summarized motion boundaries Mu , and summarized motion boundaries
    Mv computed from the video clip.'
  Figure 2 Link: articels_figures_by_rev_year\2021\SelfSupervised_Video_Representation_Learning_by_Uncovering_SpatioTemporal_Statis\figure_2.jpg
  Figure 2 caption: Illustration of extracting statistical labels in a three-frame
    video clip. Detailed explanation is in Section 3.1.
  Figure 3 Link: articels_figures_by_rev_year\2021\SelfSupervised_Video_Representation_Learning_by_Uncovering_SpatioTemporal_Statis\figure_3.jpg
  Figure 3 caption: Three different partitioning patterns used to divide video frames
    into different spatial regions. Each spatial block is assigned with a number to
    represent its location.
  Figure 4 Link: articels_figures_by_rev_year\2021\SelfSupervised_Video_Representation_Learning_by_Uncovering_SpatioTemporal_Statis\figure_4.jpg
  Figure 4 caption: "Framework of the proposed approach. Given a video clip, 14 motion\
    \ statistical labels and 13 appearance statistical labels are to be predicted.\
    \ The motion statistical labels are computed from the summarized motion boundaries\
    \ M u and M v . The appearance statistical labels are computed from the input\
    \ video clip. For each local motion pattern, 4 ground-truth labels are generated:\
    \ p u , o u \u2014the spatial location of the largest magnitude based on M u and\
    \ its corresponding dominant orientation; p v , o v \u2014the spatial location\
    \ of the largest magnitude based on M v and its corresponding dominant orientation.\
    \ Two global motion statistical labels are I u , I v \u2014the frame indices of\
    \ the largest magnitude sum w.r.t. m u and m v . For each local appearance pattern,\
    \ 4 ground-truth labels are generated: p l , c l \u2014the spatial location of\
    \ the largest color diversity and its corresponding dominant color; p s , c s\
    \ \u2014the spatial location of the smallest color diversity and its corresponding\
    \ dominant color. The global appearance statistical label is C \u2014the dominant\
    \ color of the whole video."
  Figure 5 Link: articels_figures_by_rev_year\2021\SelfSupervised_Video_Representation_Learning_by_Uncovering_SpatioTemporal_Statis\figure_5.jpg
  Figure 5 caption: Action recognition accuracy in terms of four initialization methods
    w.r.t three backbone networks on UCF101 and HMDB51 datasets.
  Figure 6 Link: articels_figures_by_rev_year\2021\SelfSupervised_Video_Representation_Learning_by_Uncovering_SpatioTemporal_Statis\figure_6.jpg
  Figure 6 caption: 'Correlation between pretext task error and downstream task accuracy.
    Left: representative value obtained from the best performed model using different
    backbone networks. Right: Complete evolution using R(2+1)D as the backbone network.'
  Figure 7 Link: articels_figures_by_rev_year\2021\SelfSupervised_Video_Representation_Learning_by_Uncovering_SpatioTemporal_Statis\figure_7.jpg
  Figure 7 caption: Action recognition accuracy in terms of different pre-training
    datasets w.r.t. three backbone networks on UCF101 and HMDB51.
  Figure 8 Link: articels_figures_by_rev_year\2021\SelfSupervised_Video_Representation_Learning_by_Uncovering_SpatioTemporal_Statis\figure_8.jpg
  Figure 8 caption: "Action recognition accuracy in terms of different pre-training\
    \ dataset scales of K-400 shown in (a) linear-scale (position \u201C0\u201D indicates\
    \ random initialization), and (b) log-scale w.r.t three backbone networks on UCF101\
    \ and HMDB51 datasets."
  Figure 9 Link: articels_figures_by_rev_year\2021\SelfSupervised_Video_Representation_Learning_by_Uncovering_SpatioTemporal_Statis\figure_9.jpg
  Figure 9 caption: Three video samples of the curriculum learning strategy. From
    left to right, the difficulty to predict the motion statistical labels of each
    video clip is increasing. For each sample, the top three images are the first,
    middle, and last frames of a video clip. In the bottom row, the first two images
    are the corresponding optical flows and the last image is the summarized motion
    boundaries MuMv with the maximum magnitude sum.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.57
  Name of the first author: Jiangliu Wang
  Name of the last author: Yun-hui Liu
  Number of Figures: 12
  Number of Tables: 9
  Number of authors: 6
  Paper title: Self-Supervised Video Representation Learning by Uncovering Spatio-Temporal
    Statistics
  Publication Date: 2021-02-10 00:00:00
  Table 1 caption: TABLE 1 Ablation Experiments on Spatio-Temporal Statistics Component
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Evaluation of Different Backbone Networks on UCF101 and
    HMDB51 Datasets
  Table 3 caption: TABLE 3 Evaluation of Curriculum Learning Strategy.
  Table 4 caption: TABLE 4 Evaluation of Different Training Targets on UCF101 and
    HMDB51 Datasets w.r.t Three Backbone Networks
  Table 5 caption: TABLE 5 Comparison With State-of-the-Art Self-Supervised Learning
    Approaches on the Action Recognition Task
  Table 6 caption: TABLE 6 Comparison With State-of-the-Art Self-Supervised Learning
    Methods on the Video Retrieval Task With the UCF101 Dataset
  Table 7 caption: TABLE 7 Comparison With State-of-the-Art Self-Supervised Learning
    Methods on the Video Retrieval Task With the HMDB51 Dataset
  Table 8 caption: TABLE 8 Comparison With State-of-the-Art Hand-Crafted Methods and
    Self-Supervised Representation Learning Methods on the Dynamic Scene Recognition
    Task
  Table 9 caption: TABLE 9 Comparison With Different Hand-Crafted Features and Fully-Supervised
    Models on the ASLAN Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3057833
- Affiliation of the first author: hamlyn centre for robotic surgery, department of
    surgery and cancer, imperial college london, london, u.k.
  Affiliation of the last author: hamlyn centre for robotic surgery, department of
    surgery and cancer, imperial college london, london, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2021\SeeThrough_Vision_With_Unsupervised_Scene_Occlusion_Reconstruction\figure_1.jpg
  Figure 1 caption: Overview of the proposed generator G model. Solid lines are forward
    propagation and dotted lines are skip connections. All connections related to
    strea m 1 are colour coded in magenta, and similarly for strea m 2 in green.
  Figure 10 Link: articels_figures_by_rev_year\2021\SeeThrough_Vision_With_Unsupervised_Scene_Occlusion_Reconstruction\figure_10.jpg
  Figure 10 caption: Occlusion recovery, on natural scenes from the Davis 2017 dataset.
    The rows a display the original candidate frame, b the input masked frame, and
    c the reconstructed output.
  Figure 2 Link: articels_figures_by_rev_year\2021\SeeThrough_Vision_With_Unsupervised_Scene_Occlusion_Reconstruction\figure_2.jpg
  Figure 2 caption: The generator G model. In the encoder stream, the red, green and
    yellow blocks denote convolution filters of size 7x7x3, 5x5x3 and 3x3x3, respectively.
    The blue blocks between the skip connections of strea m 1 are bottle neck layers.
    In the decoder, all convolution filters are 2D. The solid lines show forward propagation
    and the dotted lines are skip connections.
  Figure 3 Link: articels_figures_by_rev_year\2021\SeeThrough_Vision_With_Unsupervised_Scene_Occlusion_Reconstruction\figure_3.jpg
  Figure 3 caption: The decoder block. All connections from strea m 1 are colour coded
    in magenta and for strea m 2 in green. All the layers with learnable weights are
    2D and are yellow.
  Figure 4 Link: articels_figures_by_rev_year\2021\SeeThrough_Vision_With_Unsupervised_Scene_Occlusion_Reconstruction\figure_4.jpg
  Figure 4 caption: Occlusion reconstruction for MIS sequences where, rows a display
    the original candidate frame, b the input masked frame and c the reconstructed
    output. Sequence Ref shows the reference frames for Sequences 1,2 & 3. Sequences
    4-5 were occluded by random synthetic occlusions on the entire video with OIRs
    ranging from 60-70 & 30-40 percent respectively.
  Figure 5 Link: articels_figures_by_rev_year\2021\SeeThrough_Vision_With_Unsupervised_Scene_Occlusion_Reconstruction\figure_5.jpg
  Figure 5 caption: Comparison of qualitative results on a sample of consecutive frames
    from the EndoVis2018 data, presented in each column. a the original candidate
    frame, b the input masked frame and c the reconstructed output from the proposed
    model,d output from [12] and e output from [13].
  Figure 6 Link: articels_figures_by_rev_year\2021\SeeThrough_Vision_With_Unsupervised_Scene_Occlusion_Reconstruction\figure_6.jpg
  Figure 6 caption: Performance evaluation of different occlusion recovery methods
    under varying OIR(%) on MIS data from the Hamlyn database.
  Figure 7 Link: articels_figures_by_rev_year\2021\SeeThrough_Vision_With_Unsupervised_Scene_Occlusion_Reconstruction\figure_7.jpg
  Figure 7 caption: "Loss Ablation results, comprising of L1 error (top left), PSNR\
    \ (centre), SSIM (top right), IC score (bottom left) and Temporal SSIM error (bottom\
    \ right). The results were attained from the hamlyn test dataset. The metrics\
    \ shown were calculated between the generated output and the ground truth from\
    \ the hamlyn test set. \u2212NoAdver. being the proposed model with no adversarial\
    \ component, \u2212NoPerc. denotes no perceptual component, \u2212NoStyle denotes\
    \ no style component, \u2212NoWarp denotes no warping loss component and finally,\
    \ \u2212Proposed is the proposed model with the all the proposed loss components."
  Figure 8 Link: articels_figures_by_rev_year\2021\SeeThrough_Vision_With_Unsupervised_Scene_Occlusion_Reconstruction\figure_8.jpg
  Figure 8 caption: Qualitative results for different loss function combinations.
    (a) input masked image, (b) ground truth, (c) proposed loss, (d) no style loss,
    (e) no perceptual loss, (f) no warping loss, and (g) no adversarial loss.
  Figure 9 Link: articels_figures_by_rev_year\2021\SeeThrough_Vision_With_Unsupervised_Scene_Occlusion_Reconstruction\figure_9.jpg
  Figure 9 caption: Glove Effect visualisation, where rows a display the original
    candidate frame and b the output of the glove effect, where the contours of the
    tool and the end effector are purposely visible.
  First author gender probability: 0
  Gender of the first author: Not Available
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Samyakh Tukra
  Name of the last author: Stamatia Giannarou
  Number of Figures: 10
  Number of Tables: 1
  Number of authors: 3
  Paper title: See-Through Vision With Unsupervised Scene Occlusion Reconstruction
  Publication Date: 2021-02-10 00:00:00
  Table 1 caption: TABLE 1 Performance Evaluation on Random Masks of OIR 30-40 percent
    on the Davis2017 Test Data
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3058410
- Affiliation of the first author: "technische universit\xE4t m\xFCnchen, m\xFCnchen,\
    \ germany"
  Affiliation of the last author: microsoft, redmond, wa, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\DomainSpecific_Priors_and_Meta_Learning_for_FewShot_FirstPerson_Action_Recogniti\figure_1.jpg
  Figure 1 caption: We leverage hand-object-action oriented visual cues in first-person
    views that decouple foreground action from background appearance, and meta-learning
    to enable few-shot transfer of action representations. Visual cues are learned
    from disparate image datasets and applied to first-person action video datasets,
    thus demonstrating the strength of our decoupled approach.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\DomainSpecific_Priors_and_Meta_Learning_for_FewShot_FirstPerson_Action_Recogniti\figure_2.jpg
  Figure 2 caption: 'System overview: The hand detector provides extended hand-context
    (red boxes) and activates object cells (dark green) laid out in a fixed grid (light
    green). Training proceeds in three stages: (i) visual cue extractors, trained
    from disparate image datasets; (ii) Bi-Directional GRU and Attention layer, trained
    from source domain videos with action labels; and (iii) transfer learning: we
    experiment with fine-tuning, KNN and meta learning.'
  Figure 3 Link: articels_figures_by_rev_year\2021\DomainSpecific_Priors_and_Meta_Learning_for_FewShot_FirstPerson_Action_Recogniti\figure_3.jpg
  Figure 3 caption: 'Architecture overview: Feats are 1-d vectors and, depending on
    the experiment, are a concatenation of different visual cues. FC represents a
    fully connected layer; LN represents layer normalization. The embedding layer
    ensures that the number of parameters of the temporal model stays fixed, even
    if different visual cues are used.'
  Figure 4 Link: articels_figures_by_rev_year\2021\DomainSpecific_Priors_and_Meta_Learning_for_FewShot_FirstPerson_Action_Recogniti\figure_4.jpg
  Figure 4 caption: Qualitative results for our inter-class experiments on EPIC. Frames
    are selected uniformly at random; different rows correspond to different activities.
    The first three rows show correct predictions; the last three rows highlight failure
    cases. Even when wrong, our predictions still provide a reasonable explanation
    of the scene.
  Figure 5 Link: articels_figures_by_rev_year\2021\DomainSpecific_Priors_and_Meta_Learning_for_FewShot_FirstPerson_Action_Recogniti\figure_5.jpg
  Figure 5 caption: "We visualize the right-hand feature extraction regions in image\
    \ space. We subdivide the image into 4\xD76 cells. Green cells show our object\
    \ feature extraction regions. The red box shows the hand feature extraction region.\
    \ The changes in the red box size depend on the hand size in the image."
  Figure 6 Link: articels_figures_by_rev_year\2021\DomainSpecific_Priors_and_Meta_Learning_for_FewShot_FirstPerson_Action_Recogniti\figure_6.jpg
  Figure 6 caption: Convergence of models at test-time as a function of the number
    of updates of the model parameters. All training strategies use the full set of
    visual cues and identical models.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.77
  Name of the first author: Huseyin Coskun
  Name of the last author: Harpreet S. Sawhney
  Number of Figures: 6
  Number of Tables: 9
  Number of authors: 7
  Paper title: Domain-Specific Priors and Meta Learning for Few-Shot First-Person
    Action Recognition
  Publication Date: 2021-02-10 00:00:00
  Table 1 caption: TABLE 1 Visual Cue Networks Training Details
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Network-Specific Details for Domain-Specific Visual Cue
    Extraction
  Table 3 caption: TABLE 3 Grasp Accuracy (in %) on the GUN71 Dataset [67]
  Table 4 caption: TABLE 4 Classification Accuracy (in %) for Transfer Learning Experiments
    With K-Nearest Neighbors
  Table 5 caption: 'TABLE 5 Classification Accuracy (in %) for Transfer Learning Experiments:
    Here we Fine Tune All the Network Parameters of Our Temporal Model (RNN + attention
    layer + final logits) and Baselines'
  Table 6 caption: TABLE 6 Classification Accuracy (in %) for Transfer Learning Experiments
    for Which we Fine Tune Only the Final Logit Layers
  Table 7 caption: TABLE 7 Classification Accuracy (in %) on the EPIC Dataset Test
    Subjects
  Table 8 caption: TABLE 8 Classification Accuracy (in %) for Comparison of Different
    Transfer Learning Strategies on 1-Shot 5-Class, 5-Shot 5-Class, and 10-Shot 5-Class
    Action Recognition Experiments on EPIC
  Table 9 caption: TABLE 9 Classification Accuracy (in %) for five Way Classification
    Task on EPIC and EGTEA Datasets
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3058606
- Affiliation of the first author: state key laboratory of robotics, shenyang institute
    of automation, institutes for robotics and intelligent manufacturing, chinese
    academy of sciences, shenyang, china
  Affiliation of the last author: state key laboratory of robotics, shenyang institute
    of automation, institutes for robotics and intelligent manufacturing, chinese
    academy of sciences, shenyang, china
  Figure 1 Link: articels_figures_by_rev_year\2021\What_and_How_Generalized_Lifelong_Spectral_Clustering_via_Dual_Memory\figure_1.jpg
  Figure 1 caption: Illustration of our generalized lifelong spectral clustering (GL
    2 SC) model, where different shapes in matrices X t and W t are from different
    clusters. When observing a new clustering task t , the knowledge or experience
    is iteratively transferred from orthogonal basis memory and feature embedding
    memory to encode the new task.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\What_and_How_Generalized_Lifelong_Spectral_Clustering_via_Dual_Memory\figure_2.jpg
  Figure 2 caption: 'Examples of CORe50 [51] dataset. From left to right: mobile phone,
    marker, glasses, and remote control.'
  Figure 3 Link: articels_figures_by_rev_year\2021\What_and_How_Generalized_Lifelong_Spectral_Clustering_via_Dual_Memory\figure_3.jpg
  Figure 3 caption: Impact of the learned task number on WebKB4 (Top) and 20NewsGroups
    (Bottom) datasets in terms of Purity and NMI metrics, where the horizontal and
    vertical axes are the learned tasks number and clustering performance, respectively.
    The initial clustering performance for each task (except for the first task) on
    each dataset is attained using stSC algorithm.
  Figure 4 Link: articels_figures_by_rev_year\2021\What_and_How_Generalized_Lifelong_Spectral_Clustering_via_Dual_Memory\figure_4.jpg
  Figure 4 caption: "p -value of t -test between our GL 2 SC model and others on WebKB4,\
    \ CORe50, Reuters, 20NewsGroups datasets. We pre-process using \u2212log(p) so\
    \ that the large value illustrated in the above figure represents the more significance\
    \ of our model compared with others."
  Figure 5 Link: articels_figures_by_rev_year\2021\What_and_How_Generalized_Lifelong_Spectral_Clustering_via_Dual_Memory\figure_5.jpg
  Figure 5 caption: "Parameter \u03BB analysis of our GL 2 SC model on (a) WebKB4,\
    \ (b) CORe50, (c) Reuters, and (d) 20NewsGroups datasets, where lines with different\
    \ colors denote clustering performance in terms of different metrics."
  Figure 6 Link: articels_figures_by_rev_year\2021\What_and_How_Generalized_Lifelong_Spectral_Clustering_via_Dual_Memory\figure_6.jpg
  Figure 6 caption: 'Layer size analysis of our GL 2 SC model in terms of Purity and
    NMI. From left to right: WebKB4, CORe50, Reuters and 20NewsGroups datasets.'
  Figure 7 Link: articels_figures_by_rev_year\2021\What_and_How_Generalized_Lifelong_Spectral_Clustering_via_Dual_Memory\figure_7.jpg
  Figure 7 caption: (a) Comparisons with our extended conference L 2 SC model (Two-layers)
    on WebKB4 and 20NewsGroups datasets. (b) Effect of task order of our GL 2 SC model
    on WebKB4, Reuters and 20NewsGroups datasets.
  Figure 8 Link: articels_figures_by_rev_year\2021\What_and_How_Generalized_Lifelong_Spectral_Clustering_via_Dual_Memory\figure_8.jpg
  Figure 8 caption: Convergence analysis of our proposed GL 2 SC model on (a) WebKB4,
    (b) CORe50, (c) Reuters and (d) 20NewsGroups datasets, where lines with different
    colors represent different tasks in each dataset.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.7
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.95
  Name of the first author: Gan Sun
  Name of the last author: Haibin Yu
  Number of Figures: 8
  Number of Tables: 6
  Number of authors: 6
  Paper title: 'What and How: Generalized Lifelong Spectral Clustering via Dual Memory'
  Publication Date: 2021-02-11 00:00:00
  Table 1 caption: TABLE 1 Details of the Used Four Real Datasets Including Task ID,
    Clustering Categories, and Feature Dimensions
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 Comparison Performance on the WebKB4 Dataset in Terms\
    \ of Three Different Metrics (Mean \xB1 \xB1 Standard Deviation)"
  Table 3 caption: "TABLE 3 Comparison Performance on the CORe50 Dataset in Terms\
    \ of Three Different Metrics (Mean \xB1 \xB1 Standard Deviation)"
  Table 4 caption: "TABLE 4 Comparison Performance on the Reuters Dataset in Terms\
    \ of Three Different Metrics (Mean \xB1 \xB1 Standard Deviation)"
  Table 5 caption: "TABLE 5 Comparison Performance on the 20NewsGroups Dataset in\
    \ Terms of Three Different Metrics (Mean \xB1 \xB1 Standard Deviation)"
  Table 6 caption: TABLE 6 Runtime (seconds) on a Standard CPU of all the Competing
    Models
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3058852
- Affiliation of the first author: school of computer science and engineering, sun
    yat-sen university, guangzhou, china
  Affiliation of the last author: school of computer science and engineering, sun
    yat-sen university, guangzhou, china
  Figure 1 Link: articels_figures_by_rev_year\2021\APANet_AutoPath_Aggregation_for_Future_Instance_Segmentation_Prediction\figure_1.jpg
  Figure 1 caption: Video-based instance segmentation versus future instance segmentation
    prediction. (a) presents the process of instance segmentation, in which instance
    segmentation results are generated for the observed frames. (b) shows the process
    of future instance segmentation prediction, the goal of which is to produce instance
    segmentation results for unobserved future frames based on the observed past frames.
  Figure 10 Link: articels_figures_by_rev_year\2021\APANet_AutoPath_Aggregation_for_Future_Instance_Segmentation_Prediction\figure_10.jpg
  Figure 10 caption: Evaluation on the optimization for our future instance segmentation
    prediction system.
  Figure 2 Link: articels_figures_by_rev_year\2021\APANet_AutoPath_Aggregation_for_Future_Instance_Segmentation_Prediction\figure_2.jpg
  Figure 2 caption: 'An overview of the proposed framework for future instance segmentation
    prediction. Our framework consists of three blocks: a pyramid feature encoder,
    a future feature prediction block, and a pyramid feature decoder. We use gray
    dash lines to indicate the auto-path connections. The operations employed in each
    auto-path connection will be automatically determined by neural architecture search
    (NAS) [3]. The sets of possible operations for the auto-path connections in top-down
    and bottom-up directions are different, as shown at the bottom of the figure.
    Different candidate operations are indicated by different colors.'
  Figure 3 Link: articels_figures_by_rev_year\2021\APANet_AutoPath_Aggregation_for_Future_Instance_Segmentation_Prediction\figure_3.jpg
  Figure 3 caption: "Our proposed auto-path aggregation network (APANet) with one\
    \ cell. The left of the figure shows a cell in our APANet and the right of the\
    \ figure represents the architecture of our auto-path connections (i.e., the AP\
    \ block in the APANet cell). P l t , Q l t , and C l t denote the input feature,\
    \ output feature, and hidden state, respectively, for the l th ConvLSTM at time\
    \ step t . AP denotes an auto-path connection between two different ConvLSTM cells\
    \ (see Eq. (3)). \u2A00 denotes element-wise multiplication, and + is the addition\
    \ operation. The parameter \u03B1 is determined according to the input hidden\
    \ state, which enables our method to adaptively adjust the architecture for aggregation\
    \ in accordance with the observed videosframes."
  Figure 4 Link: articels_figures_by_rev_year\2021\APANet_AutoPath_Aggregation_for_Future_Instance_Segmentation_Prediction\figure_4.jpg
  Figure 4 caption: A graphic illustration of the employed 3-stage training strategy.
    In this figure, different operations are marked with different colors. For better
    illustration, we only present the aggregation of 2-level features. Best viewed
    in color.
  Figure 5 Link: articels_figures_by_rev_year\2021\APANet_AutoPath_Aggregation_for_Future_Instance_Segmentation_Prediction\figure_5.jpg
  Figure 5 caption: Some visualization results of the learned auto-path architectures.
    The left of the figure presents the architecture for the same video at varied
    time steps and the right gives the architecture for different samples at the same
    time step. As shown, the architecture of the learned auto-path connections differs
    slightly for the same sample with different time steps. In contrast, it differs
    significantly for different samples. Different operations are marked with different
    colors for better visualization. When zero operation is selected, we do not present
    any connection between the two ConvLSTMs in the figure. Best viewed in color.
  Figure 6 Link: articels_figures_by_rev_year\2021\APANet_AutoPath_Aggregation_for_Future_Instance_Segmentation_Prediction\figure_6.jpg
  Figure 6 caption: The statistical information for the operations selected in our
    APANet on the Cityscapes set. Figure (a) and (b) provide the top 4 operations
    that have been selected in our top-down and bottom-up auto-path connections, respectively.
    (c) shows the statistics for withwithout attention for non-zero operations. (d)
    presents the statistics for zero non-zero operations. Best viewed in color.
  Figure 7 Link: articels_figures_by_rev_year\2021\APANet_AutoPath_Aggregation_for_Future_Instance_Segmentation_Prediction\figure_7.jpg
  Figure 7 caption: 'Some visualized results for the mid-term future instance segmentation
    prediction. From left to right: The ground-truth frames, the prediction results
    for the regions indicated by red-dashed boxes obtained by the F2F [2], our preliminary
    work [94] and our method, respectively. As shown, our method produces the best
    future instance segmentation prediction results for deformed and occluded moving
    objects, some of which are highlighted with red dashed boxes in ground-truth frames.
    The results illustrate that collaboratively predicting multi-level pyramid features
    with selective and adaptive information aggregation is beneficial for future instance
    segmentation prediction. Best viewed in color.'
  Figure 8 Link: articels_figures_by_rev_year\2021\APANet_AutoPath_Aggregation_for_Future_Instance_Segmentation_Prediction\figure_8.jpg
  Figure 8 caption: Qualitative results for the learned attention maps mathbf Av rightarrow
    l , as expressed in Eq. (4). The colors used to visualize the attention values
    range from red to blue in rainbow order, where red color indicates a high attention
    value and blue color indicates a low attention value. Best viewed in color.
  Figure 9 Link: articels_figures_by_rev_year\2021\APANet_AutoPath_Aggregation_for_Future_Instance_Segmentation_Prediction\figure_9.jpg
  Figure 9 caption: Visualization results of our approaches withwithout adaption.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jian-Fang Hu
  Name of the last author: Wei-Shi Zheng
  Number of Figures: 10
  Number of Tables: 11
  Number of authors: 6
  Paper title: 'APANet: Auto-Path Aggregation for Future Instance Segmentation Prediction'
  Publication Date: 2021-02-11 00:00:00
  Table 1 caption: "TABLE 1 Candidates of \u201COperation X\u201D for Different Pathways"
  Table 10 caption: "TABLE 10 Effects of the Parameter \u03BB \u03BB(Eq. (9)), Which\
    \ is Used to Control the Balance Between the Prediction Loss and the Segmentation\
    \ Loss"
  Table 2 caption: TABLE 2 Comparison Results on the Cityscapes Validation Set
  Table 3 caption: TABLE 3 Comparison Results on the Inria 3DMovie Dataset v2
  Table 4 caption: TABLE 4 Comparison Results on the BDD100K Dataset
  Table 5 caption: TABLE 5 The Benefits of Introducing Connections
  Table 6 caption: TABLE 6 Evaluation on the Influence of Adaptive Learning
  Table 7 caption: TABLE 7 Comparison Results With Our Preliminary Work [94]
  Table 8 caption: TABLE 8 Comparison Results on the Cityscapes Validation Set for
    Semantic Segmentation Prediction
  Table 9 caption: TABLE 9 Influence of the Number of Pyramid Feature Levels
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3058679
