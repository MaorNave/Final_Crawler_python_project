- Affiliation of the first author: computer vision laboratory, epfl, lausanne, switzerland
  Affiliation of the last author: computer vision laboratory, epfl, lausanne, switzerland
  Figure 1 Link: articels_figures_by_rev_year\2017\Reconstructing_Evolving_Tree_Structures_in_Time_Lapse_Sequences_by_Enforcing_Tim\figure_1.jpg
  Figure 1 caption: Key algorithmic steps, best viewed in color. (a) Maximum intensity
    projection of one of three in vivo image-stacks of a neural network taken at one
    week intervals. (b) Corresponding tubularity image. (c) Maxima of tubularity selected
    as graph nodes in two different stacks. Those shown in green have been determined
    to correspond to the same location in both, while those in red or blue appear
    in only one. (d) Connecting neighboring nodes by high-tubularity paths produces
    a spatial graph in each image. High-quality paths are shown as red while low quality
    ones appear as blue. (e) Connecting the corresponding vertices across images turns
    the spatial graphs into a single spatio-temporal one and solving the resulting
    QMIP problem yields two temporally consistent trees. (f) The red tree from the
    first image can be deformed and superposed on the blue tree in the second one,
    making the changes highlighted in red easy to detect.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Reconstructing_Evolving_Tree_Structures_in_Time_Lapse_Sequences_by_Enforcing_Tim\figure_2.jpg
  Figure 2 caption: Establishing correspondences. Initialization. A set of corresponding
    points with possible inconsistencies is found in each image using high-tubularity
    locations and NCC. Iteration 1. A set of corresponding points (shown in green)
    with the highest tubularity likelihoods has been selected, which are then used
    to instantiate a GPR that maps the remaining red points in image I n (top) to
    the red locations in image I n+1 (bottom). The white circles denote the covariances
    associated to these locations. The blue points in image I n+1 that are close enough
    to them and correlate well with the original red points in image I n are taken
    to form new correspondences. Iterations 2 and 3. They are added to the set of
    correspondences, shown in green. The process is then repeated.
  Figure 3 Link: articels_figures_by_rev_year\2017\Reconstructing_Evolving_Tree_Structures_in_Time_Lapse_Sequences_by_Enforcing_Tim\figure_3.jpg
  Figure 3 caption: Axon delineation. First row. Input images. Second row. Ground
    truth. Third row. Delineations without temporal consistency. Fourth row. Delineations
    with temporal consistency enforced. The red, green, and blue branches are much
    better modeled. Half of the yellow branch is mistakenly attached to the blue one
    because, where the two branches almost cross, the image is extremely blurry. However,
    this error would be easier to correct during a post-processsing state because
    the topology is at least consistent in all frames.
  Figure 4 Link: articels_figures_by_rev_year\2017\Reconstructing_Evolving_Tree_Structures_in_Time_Lapse_Sequences_by_Enforcing_Tim\figure_4.jpg
  Figure 4 caption: Road delineation. First row. Images acquired in 2009, 2011, and
    2012. Second row. Delineations without temporal consistency. Third row. Delineations
    with temporal consistency enforced. Note the added road fragments within the overlaid
    yellow ellipse.
  Figure 5 Link: articels_figures_by_rev_year\2017\Reconstructing_Evolving_Tree_Structures_in_Time_Lapse_Sequences_by_Enforcing_Tim\figure_5.jpg
  Figure 5 caption: Road delineation. First row. Images acquired in 2002, 2015, and
    2016. New houses and roads have been built between 2002 and 2015. Second row.
    Tubularity images. Note that the 2016 one is noisier, making delineation more
    difficult. Third row. Delineations without temporal consistency. Fourth row. Delineations
    with temporal consistency enforced. Note the added road fragments within the overlaid
    yellow ellipses and the removed one within the overlaid yellow box.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: "Przemys\u0142aw G\u0142owacki"
  Name of the last author: Pascal Fua
  Number of Figures: 5
  Number of Tables: 1
  Number of authors: 9
  Paper title: Reconstructing Evolving Tree Structures in Time Lapse Sequences by
    Enforcing Time-Consistency
  Publication Date: 2017-03-10 00:00:00
  Table 1 caption:
    table_text: TABLE 1 DIADEM Scores [17] for the Brain Images Datasets (Denoted
      BR i ) and Road Images Datasets (Denoted RD i )
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2680444
- Affiliation of the first author: department of computer science and technology,
    tsinghua university, beijing, china
  Affiliation of the last author: department of computer science and technology, tsinghua
    university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2017\Spectral_Learning_for_Supervised_Topic_Models\figure_1.jpg
  Figure 1 caption: A graphical illustration of supervised LDA. See text for details.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Spectral_Learning_for_Supervised_Topic_Models\figure_2.jpg
  Figure 2 caption: Interaction between response variable y and word vector x in the
    3rd-order tensor M 3 .
  Figure 3 Link: articels_figures_by_rev_year\2017\Spectral_Learning_for_Supervised_Topic_Models\figure_3.jpg
  Figure 3 caption: "Reconstruction errors of two spectral methods when each document\
    \ contains 250 words. X axis denotes the training size n in log domain with base\
    \ 2 (i.e., n= 2 k ,k\u22088,...,15 ). Error bars denote the standard deviations\
    \ measured on 3 independent trials under each setting."
  Figure 4 Link: articels_figures_by_rev_year\2017\Spectral_Learning_for_Supervised_Topic_Models\figure_4.jpg
  Figure 4 caption: "Reconstruction errors of two spectral methods when each document\
    \ contains 500 words. X axis denotes the training size n in log domain with base\
    \ 2 (i.e., n= 2 k ,k\u22088,...,15 ). Error bars denote the standard deviations\
    \ measured on 3 independent trials under each setting."
  Figure 5 Link: articels_figures_by_rev_year\2017\Spectral_Learning_for_Supervised_Topic_Models\figure_5.jpg
  Figure 5 caption: "Mean square errors and negative per-word log-likelihood of Alg.\
    \ 1 and Gibbs sLDA. Each document contains M=500 words. The X axis denotes the\
    \ training size ( times 103 ). The \u201Cref. model\u201D denotes the one with\
    \ the underlying true parameters."
  Figure 6 Link: articels_figures_by_rev_year\2017\Spectral_Learning_for_Supervised_Topic_Models\figure_6.jpg
  Figure 6 caption: pR2 scores and negative per-word log-likelihood on the Hotel Review
    dataset. The X axis indicates the number of topics. Error bars indicate the standard
    deviation of 5-fold cross-validation.
  Figure 7 Link: articels_figures_by_rev_year\2017\Spectral_Learning_for_Supervised_Topic_Models\figure_7.jpg
  Figure 7 caption: pR2 scores and negative per-word log-likelihood on Amazon dataset.
    The X axis indicates the number of topics. Error bars indicate the standard deviation
    of 5-fold cross-validation. Vocabulary size V = 5,000
  Figure 8 Link: articels_figures_by_rev_year\2017\Spectral_Learning_for_Supervised_Topic_Models\figure_8.jpg
  Figure 8 caption: pR2 scores and negative per-word log-likelihood on Amazon dataset.
    The X axis indicates the number of topics. Error bars indicate the standard deviation
    of 5-fold cross-validation. Vocabulary size V = 10,000
  Figure 9 Link: articels_figures_by_rev_year\2017\Spectral_Learning_for_Supervised_Topic_Models\figure_9.jpg
  Figure 9 caption: Running time of our method w.r.t the number of threads. Both x
    and y axes are plotted in log scale with base e .
  First author gender probability: 0.95
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Yong Ren
  Name of the last author: Jun Zhu
  Number of Figures: 9
  Number of Tables: 3
  Number of authors: 3
  Paper title: Spectral Learning for Supervised Topic Models
  Publication Date: 2017-03-15 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Estimated \u03B7 by the Two Spectral Methods (Sorted for\
      \ Ease of Comparison)"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Probabilities and Ranks (in Brackets) of Some Non-Neutral
      Words
  Table 3 caption:
    table_text: TABLE 3 Running time (Seconds) of Our Spectral Learning Methods and
      Gibbs Sampling
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2682085
- Affiliation of the first author: microsoft research asia, beijing, china
  Affiliation of the last author: amazon, seattle, wa
  Figure 1 Link: articels_figures_by_rev_year\2017\Collaborative_Active_Visual_Recognition_from_Crowds_A_Distributed_Ensemble_Appro\figure_1.jpg
  Figure 1 caption: The number of images per category for the 10 classes from ImageNet.
  Figure 10 Link: articels_figures_by_rev_year\2017\Collaborative_Active_Visual_Recognition_from_Crowds_A_Distributed_Ensemble_Appro\figure_10.jpg
  Figure 10 caption: Recognition performance on real crowd-sourced labels on the face
    gender image dataset in the active learning pool and the hold-out testing dataset.
  Figure 2 Link: articels_figures_by_rev_year\2017\Collaborative_Active_Visual_Recognition_from_Crowds_A_Distributed_Ensemble_Appro\figure_2.jpg
  Figure 2 caption: "Recognition performance with clean labels without noise. The\
    \ vertical bar indicated the standard deviation of mAP values on the curve. CAL\
    \ stands for the proposed collaborative active learning algorithm. MIAL refers\
    \ to the multiple independent active learning algorithm discarding the cross labeler\
    \ loss function L l ij (\u22C5) in Eq. (1). SVM-MIAL indicates the multiple independent\
    \ active learning algorithm using hinge loss like SVM. And CRL, MIRL, SVM-MIRL\
    \ are the corresponding random learning counterparts, respectively. Unlike CAL\
    \ and CRL that use the same exact m ensemble classifiers defined in Eq. (11),\
    \ CAL-All and CRL-All are the corresponding active learning and random learning\
    \ with the ensemble classifiers which combine all the K individual classifiers\
    \ from all the K labelers together."
  Figure 3 Link: articels_figures_by_rev_year\2017\Collaborative_Active_Visual_Recognition_from_Crowds_A_Distributed_Ensemble_Appro\figure_3.jpg
  Figure 3 caption: The recognition performance on the active learning pool with different
    levels of label noises, and the hold-out testing dataset, respectively.
  Figure 4 Link: articels_figures_by_rev_year\2017\Collaborative_Active_Visual_Recognition_from_Crowds_A_Distributed_Ensemble_Appro\figure_4.jpg
  Figure 4 caption: Recognition performance with 5 irresponsible labelers (with 50
    percent label noise) and the rest responsible labelers (with 5 percent label noise).
  Figure 5 Link: articels_figures_by_rev_year\2017\Collaborative_Active_Visual_Recognition_from_Crowds_A_Distributed_Ensemble_Appro\figure_5.jpg
  Figure 5 caption: "Detect irresponsible labelers by ranking the labeler quality\
    \ in an increasing order on \u201CMeerkat, meerkat\u201D class with 5 irresponsible\
    \ labelers, i.e., labeler 15, 16, 17, 18 and 19."
  Figure 6 Link: articels_figures_by_rev_year\2017\Collaborative_Active_Visual_Recognition_from_Crowds_A_Distributed_Ensemble_Appro\figure_6.jpg
  Figure 6 caption: Label quality measures with different number of irresponsible
    labelers. The label quality responsible labelers are averaged with variance bar
    overlayed. The irresponsible labelers are plotted alone.
  Figure 7 Link: articels_figures_by_rev_year\2017\Collaborative_Active_Visual_Recognition_from_Crowds_A_Distributed_Ensemble_Appro\figure_7.jpg
  Figure 7 caption: Recognition performance with real crowd-sourced labels on five
    ImageNet categories in the active learning pool and the hold-out testing dataset.
  Figure 8 Link: articels_figures_by_rev_year\2017\Collaborative_Active_Visual_Recognition_from_Crowds_A_Distributed_Ensemble_Appro\figure_8.jpg
  Figure 8 caption: Recognition performance on real crowd-sourced labels on a face
    gender image dataset.
  Figure 9 Link: articels_figures_by_rev_year\2017\Collaborative_Active_Visual_Recognition_from_Crowds_A_Distributed_Ensemble_Appro\figure_9.jpg
  Figure 9 caption: Recognition performance with real crowd-sourced labels on five
    ImageNet categories in the active learning pool and the hold-out testing dataset.
  First author gender probability: 0.95
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.76
  Name of the first author: Gang Hua
  Name of the last author: Yan Gao
  Number of Figures: 11
  Number of Tables: 0
  Number of authors: 4
  Paper title: 'Collaborative Active Visual Recognition from Crowds: A Distributed
    Ensemble Approach'
  Publication Date: 2017-03-15 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2682082
- Affiliation of the first author: department of electrical and electronic engineering,
    imperial college london, london, united kingdom
  Affiliation of the last author: department of electrical and electronic engineering,
    imperial college london, london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2017\Binary_Online_Learned_Descriptors\figure_1.jpg
  Figure 1 caption: In contrast to typical approaches that use the same measurements
    for every patch, we adapt the descriptor online to each patch. The blue line ends
    indicate the selected binary tests from a common superset based on the measurements
    from the synthesized views of each patch. Note that although the final descriptor
    is different for each patch, it consists of a subset of a fixed set of dimensions.
    This allows efficient sequential matching and common database storage.
  Figure 10 Link: articels_figures_by_rev_year\2017\Binary_Online_Learned_Descriptors\figure_10.jpg
  Figure 10 caption: (left) Matching performance for variants of descriptor and distance
    measures, (right) symmetric and asymmetric masking variants.
  Figure 2 Link: articels_figures_by_rev_year\2017\Binary_Online_Learned_Descriptors\figure_2.jpg
  Figure 2 caption: The effect of the randomness on BRIEF intensity tests on the performance.
    The top row of patches represents the query images, and the 4 bottom rows show
    the retrieval results from the dataset, where in each row the respective BRIEF
    descriptor is used. The true positives are shown in green, and the false positives
    are shown in red. Note that different BRIEF descriptors created with different
    random seeds, exhibit non-consistent behaviour.
  Figure 3 Link: articels_figures_by_rev_year\2017\Binary_Online_Learned_Descriptors\figure_3.jpg
  Figure 3 caption: Distribution of pairwise intensity test that flip their sign under
    5 and 10 degrees rotation of a patch, estimated for 10k patches. For comparison,
    we also plot positive (intra) and negative (inter-class) distance distributions
    for real patch pairs.
  Figure 4 Link: articels_figures_by_rev_year\2017\Binary_Online_Learned_Descriptors\figure_4.jpg
  Figure 4 caption: The effect of altering the intensity tests used in the online
    learned detector of TLD [6]. (Left) The three different fern classifiers we used
    in our experiments. (Right) Results using the benchmark of [21]. Note the varying
    performance of the fern classifiers based on different intensity tests.
  Figure 5 Link: articels_figures_by_rev_year\2017\Binary_Online_Learned_Descriptors\figure_5.jpg
  Figure 5 caption: Distribution of intensity differences f k for two intensity tests
    for intra and inter class variations. Intra class is generated by affine transformation
    of patches and interclass is estimated on large number of different patches. Left
    distribution represents a less stable test with low f k value. Interclass distribution
    is not centred on zero due to rotation alignment of the dominant gradient.
  Figure 6 Link: articels_figures_by_rev_year\2017\Binary_Online_Learned_Descriptors\figure_6.jpg
  Figure 6 caption: (left) Probability of bit flip Pk(flip) w.r.t. intensity difference
    fk . The larger the difference the less likely to flip. (right) Error rates for
    different methods of choosing subsets of binary intensity tests, i.e., low and
    high intensity differences compared to random as well as our proposed online selection
    based on affine transformed patches.
  Figure 7 Link: articels_figures_by_rev_year\2017\Binary_Online_Learned_Descriptors\figure_7.jpg
  Figure 7 caption: Histogram of numbers of stable dimensions for positive (left)
    and negative (right) pairs of patches mathbf p1 and mathbf p2 . The distribution
    is more compact and the numbers of stable dimensions are similar for positive
    patch pairs in contrast to the negative ones.
  Figure 8 Link: articels_figures_by_rev_year\2017\Binary_Online_Learned_Descriptors\figure_8.jpg
  Figure 8 caption: Histogram of numbers of stable dimensions under various geometric
    transformations of patches including translations, scaling and rotations. We observe
    that 90 percent of tests are stable only for very small transformations, i.e.,
    up to 5 degree rotation, 1.05 scaling or 2 pixel translation, but nearly 50 percent
    of tests fail with translation by 5 pixels, scaling of 1.15 or rotation of 30
    degree, which shows high sensitivity of binary intensity tests to geometric patch
    deformations.
  Figure 9 Link: articels_figures_by_rev_year\2017\Binary_Online_Learned_Descriptors\figure_9.jpg
  Figure 9 caption: Matching error FPR95 for Yosemite 100k with respect to various
    affine examples in intra-class optimization of binary tests. Small affine transformations
    and few examples are sufficient to achieve low error rate.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Vassileios Balntas
  Name of the last author: Krystian Mikolajczyk
  Number of Figures: 15
  Number of Tables: 5
  Number of authors: 3
  Paper title: Binary Online Learned Descriptors
  Publication Date: 2017-03-20 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Example Cases of Test (in) Stability
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Recall Results for 10 Sequences of the Recently Published
      Tracking Evaluation Benchmark [21]
  Table 3 caption:
    table_text: TABLE 3 Performance of the Masked Hamming Distance, for 1000 Pairs
      of Patches
  Table 4 caption:
    table_text: TABLE 4 Comparison of Efficiency Per Operation for Various Feature
      Descriptors
  Table 5 caption:
    table_text: TABLE 5 FPR95 Results for Several State of the Art Binary descriptors
      and Deep Learning CNN Based Descriptors, i.e., DeepBit and DeepCompare
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2679193
- Affiliation of the first author: singapore university of technology and design,
    singapore
  Affiliation of the last author: singapore university of technology and design, singapore
  Figure 1 Link: articels_figures_by_rev_year\2017\Embedding_Based_on_Function_Approximation_for_Large_Scale_Image_Search\figure_1.jpg
  Figure 1 caption: "The convergence of the Algorithm 1. Number of anchor points n=8\
    \ ; \u03BC= 10 \u22122 . 1M descriptors are used for training."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Embedding_Based_on_Function_Approximation_for_Large_Scale_Image_Search\figure_2.jpg
  Figure 2 caption: "Impact of number of anchor points on the Holidays dataset for\
    \ different embedding methods: VLAD, Fisher, Temb, VLAT and the proposed FAemb,\
    \ F-FAemb. Given n , the dimension of VLAD, Temb, Fisher is 128\xD7(n\u22121)\
    \ ; the dimension of VLAT, FAemb, and F-FAemb is 45\xD746 2 \xD7(n\u22121) ."
  Figure 3 Link: articels_figures_by_rev_year\2017\Embedding_Based_on_Function_Approximation_for_Large_Scale_Image_Search\figure_3.jpg
  Figure 3 caption: "Impact of number of anchor points on the Oxford5k dataset for\
    \ different embedding methods: VLAD, Fisher, Temb, VLAT and the proposed FAemb,\
    \ F-FAemb. Given n , the dimension of VLAD, Temb, Fisher is 128\xD7(n\u22121)\
    \ ; the dimension of VLAT, FAemb, and F-FAemb is 45\xD746 2 \xD7(n\u22121) ."
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.91
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.71
  Name of the first author: Thanh-Toan Do
  Name of the last author: Ngai-Man Cheung
  Number of Figures: 3
  Number of Tables: 9
  Number of authors: 2
  Paper title: Embedding Based on Function Approximation for Large Scale Image Search
  Publication Date: 2017-03-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Computational Complexity (5th Row) and CPU Timing in Milliseconds
      (6th Row) to Embed a Local Descriptor
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Comparison between the Implementation of VLAD, VLAT, Fisher
      in This Paper and Their Improved Versions [12], [18] on Holidays Dataset
  Table 3 caption:
    table_text: TABLE 3 The Difference between Compared Embedding Methods
  Table 4 caption:
    table_text: TABLE 4 Comparison with the State of the Art on Holidays and Oxford5k
      Datasets
  Table 5 caption:
    table_text: TABLE 5 Comparison with the State of the Art on Oxford105k Dataset
  Table 6 caption:
    table_text: TABLE 6 Comparison with the State of the Art on Short Vector Representation
  Table 7 caption:
    table_text: TABLE 7 mAP Comparison between F-FAemb and Temb in Binary Representation
      with Varying Code Lengths on Three Datasets (Holidays, Oxford5k and Oxford105k)
  Table 8 caption:
    table_text: TABLE 8 Comparison with State-of-the-Art CNN Deep Learning-Based Image
      Retrieval
  Table 9 caption:
    table_text: TABLE 9 Best Results of F-FAemb (with IntermediateShort Representation)
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2686861
- Affiliation of the first author: the department of computer science and technology,
    tsinghua university, haidian qu, beijing, china
  Affiliation of the last author: school of computer science and engineering, nanyang
    technological university, singapore, singapore
  Figure 1 Link: articels_figures_by_rev_year\2017\Intrinsic_Manifold_SLIC_A_Simple_and_Efficient_Method_for_Computing_ContentSensi\figure_1.jpg
  Figure 1 caption: Superpixels obtained by normalized cuts (NC) [6], the Felzenszwalb-Huttenlocher
    (FH) method [7], superpixel lattices (SL) [8], the graph-cut-based energy optimization
    method (GraphCut) [9], turboPixels [10], VCells [11], simple linear iterative
    clustering (SLIC) [12], structure-sensitive superpixels (SSS) [14], manifold SLIC
    (MSLIC) [13] and our method. The user-specified number of superpixels is 300,
    and the actual numbers are shown in brackets. Only SL and intrinsic MSLIC can
    match the user's input. Both FH and SL generate under-segmentations in content-rich
    regions due to the lack of a compactness constraint, whereas the other methods
    produce regular superpixels. It is worth noting that only SSS, MSLIC and IMSLIC
    can produce content-sensitive superpixels. IMSLIC outperforms the other methods
    in terms of under-segmentation error, boundary recall and achievable segmentation
    accuracy. See Section 6 for a detailed comparison and evaluation. Images are provided
    in high resolution for zoom-in examination.
  Figure 10 Link: articels_figures_by_rev_year\2017\Intrinsic_Manifold_SLIC_A_Simple_and_Efficient_Method_for_Computing_ContentSensi\figure_10.jpg
  Figure 10 caption: The compactness metric of four representative methods on the
    BSDS500 and NYUV2 benchmarks for Kin [200,700] .
  Figure 2 Link: articels_figures_by_rev_year\2017\Intrinsic_Manifold_SLIC_A_Simple_and_Efficient_Method_for_Computing_ContentSensi\figure_2.jpg
  Figure 2 caption: Measuring area on the 2-manifold M embedded in R 5 .
  Figure 3 Link: articels_figures_by_rev_year\2017\Intrinsic_Manifold_SLIC_A_Simple_and_Efficient_Method_for_Computing_ContentSensi\figure_3.jpg
  Figure 3 caption: "Overview of a uniform tessellation on a synthetic greyscale image\
    \ I . (a) We represent I as a 2-manifold embedded in R 3 , denoted by M=\u03A6\
    (u,v)\u2282 R 3 , whose area elements are a good measure of content density in\
    \ I . (b) A uniform tessellation of K(=16) seeds s i K i=1 in I leads to a non-uniform\
    \ tessellation on M . (c) A uniform Voronoi tessellation on M , in which all cells\
    \ are similar in size and conform to the geometric features on the surface, induces\
    \ the content-sensitive superpixels in the image domain."
  Figure 4 Link: articels_figures_by_rev_year\2017\Intrinsic_Manifold_SLIC_A_Simple_and_Efficient_Method_for_Computing_ContentSensi\figure_4.jpg
  Figure 4 caption: "Restricted Voronoi tessellation (RVT) and geodesic Voronoi tessellation\
    \ (GVT). (a) Given a set of generators G= g 1 , g 2 ,\u22EF, g 6 on a 2-manifold\
    \ M , the restricted Voronoi tessellation RVT has a Voronoi cell V( g 1 ) consisting\
    \ of two disjoint components. (b) In the RVT, Voronoi cell V( g 1 ) is bounded\
    \ by five trimmed planes, which are bisectors of g 1 , g j , j=2,3,\u22EF,6 ,\
    \ respectively. (c) With the same set G , all Voronoi cells in the geodesic Voronoi\
    \ tessellation (GVT) are simply connected."
  Figure 5 Link: articels_figures_by_rev_year\2017\Intrinsic_Manifold_SLIC_A_Simple_and_Efficient_Method_for_Computing_ContentSensi\figure_5.jpg
  Figure 5 caption: (a) A 400times 400 grey image. (b) The RCVT on the 2-manifold
    mathcal M=Phi (I) . (c) The Voronoi cells consisting of disjoint components in
    RCVT are shown by colours. (d) Superpixels computed by MSLIC [13] without the
    split-and-merge postprocessing. (e) The GCVT proposed in this paper, in which
    each of its Voronoi cells is simply connected. (f) Superpixels output from intrinsic
    MSLIC proposed in this paper.
  Figure 6 Link: articels_figures_by_rev_year\2017\Intrinsic_Manifold_SLIC_A_Simple_and_Efficient_Method_for_Computing_ContentSensi\figure_6.jpg
  Figure 6 caption: Comparison of different initializations with K=300 . The commonly
    used quality measures on these superpixels resulting from different initializations
    are summarized in Table 2.
  Figure 7 Link: articels_figures_by_rev_year\2017\Intrinsic_Manifold_SLIC_A_Simple_and_Efficient_Method_for_Computing_ContentSensi\figure_7.jpg
  Figure 7 caption: Evaluation of eleven representative algorithms and our method
    (IMSLIC) on the BSDS500 benchmark for Kin [200, 700] . Superpixels produced by
    IMSLIC, VCells, and GGM have the lowest under-segmentation error (for K> 300 ),
    the highest boundary recall ratio (for all K ) and the best ASA values (for K>
    300 ). SEEDS, FH, SLIC, SL, and MSLIC are the five fastest algorithms and IMSLIC
    is significantly faster than the other six algorithms. IMSLIC achieves a good
    balance of under-segmentation error, boundary recall, ASA and run time.
  Figure 8 Link: articels_figures_by_rev_year\2017\Intrinsic_Manifold_SLIC_A_Simple_and_Efficient_Method_for_Computing_ContentSensi\figure_8.jpg
  Figure 8 caption: Comparison of random and uniform initializations in IMSLIC on
    the BSDS500 benchmark for Kin [200,700] . The average results of 100 random initializations
    are reported and the bars indicate pm 1 the mean of standard deviations (see text
    for details).
  Figure 9 Link: articels_figures_by_rev_year\2017\Intrinsic_Manifold_SLIC_A_Simple_and_Efficient_Method_for_Computing_ContentSensi\figure_9.jpg
  Figure 9 caption: Quantitative comparison of compactness of GGM and IMSLIC. The
    results of IMSLIC exhibit content sensitivity and a regular superpixel shape,
    leading to good compactness. Superpixels in GGM show strongly curved boundaries,
    leading to bad compactness.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.86
  Name of the first author: Yong-Jin Liu
  Name of the last author: Ying He
  Number of Figures: 15
  Number of Tables: 3
  Number of authors: 4
  Paper title: 'Intrinsic Manifold SLIC: A Simple and Efficient Method for Computing
    Content-Sensitive Superpixels'
  Publication Date: 2017-03-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Major Notations Used in This Paper
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Commonly Used Quality Measures (See Section 6 for Details
      on USE, ASA and BR) for the Superpixels Shown in Fig. 6. The Number of Output
      Superpixels Is Denoted as outputsp. Only IMSLIC Can Produce Exactly the Same
      Number of Superpixels Required by the User. Starting with Any of Three Random
      Initializations, IMSLIC Always Outperforms MSLIC. IMSLIC with Random Initializations
      Also Outperforms IMSLIC and MSLIC with Uniform Initializations
  Table 3 caption:
    table_text: TABLE 3 The Statistical Data Pertaining to the Exact Numbers of Superpixels
      Produced by VCells on the Images in BSDS500
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2686857
- Affiliation of the first author: department of center for research in computer vision,
    university of central florida, orlando, fl
  Affiliation of the last author: department of center for research in computer vision,
    university of central florida, orlando, fl
  Figure 1 Link: articels_figures_by_rev_year\2017\Binary_Quadratic_Programing_for_Online_Tracking_of_Hundreds_of_People_in_Extreme\figure_1.jpg
  Figure 1 caption: This figure shows one of our test sequences. The yellow boxes
    show the targets that are tracked in this sequence. On the borders we show the
    close-up versions of some of the targets. As can be seen targets are very small
    and there are very few discriminative appearance cues. Moreover targets look very
    similar which confuses most trackers.
  Figure 10 Link: articels_figures_by_rev_year\2017\Binary_Quadratic_Programing_for_Online_Tracking_of_Hundreds_of_People_in_Extreme\figure_10.jpg
  Figure 10 caption: This figure shows tracks (show on the left) and quantitative
    results of our method (shown on the right) with competitive approaches of FF [12],
    CTM [13] , MSBP [55], PNMC [11] and MS [56]. For each sequence we show the qualitative
    results of all tracks and on the right we show the quantitative comparison. Each
    plot shows the tracking accuracy versus pixel error.
  Figure 2 Link: articels_figures_by_rev_year\2017\Binary_Quadratic_Programing_for_Online_Tracking_of_Hundreds_of_People_in_Extreme\figure_2.jpg
  Figure 2 caption: This figure motivates the spatial proximity constraint used in
    our formulation. The top figure shows the two targets with very similar appearance.
    The bottom figures demonstrate the tracking results of our method, with and without
    proximity constraint. As can be seen when spatial proximity constraint is not
    used, the tracker gets confused and tracks of one target jump to the near by one
    with similar appearance and motion. However, we are able to correctly track the
    two targets when we use the spatial proximity constraint as shown in bottom right
    figure.
  Figure 3 Link: articels_figures_by_rev_year\2017\Binary_Quadratic_Programing_for_Online_Tracking_of_Hundreds_of_People_in_Extreme\figure_3.jpg
  Figure 3 caption: This figure is a graph illustration of the contextual constraints
    used in our formulation. The figure on the left shows the tracks and the figure
    on the right shows the constraints between the targets in yellow box. Each target
    is a node in the graph and all targets are connected with an edge. In practice
    there is a connection between every pair of targets, but here for simplicity we
    are showing only some of the connections. The figure illustrates two groups walking
    in opposite directions as well as two individuals. Each edge is assigned different
    cost depending on the grouping information and distance of targets from each other.
    The red edges that connect people in the same group encode the grouping and proximity
    constraints. The blue lines contain the neighborhood motion information and exist
    only between targets with coherent motion. The yellow edge only contains the spatial
    proximity information between two nearby targets.
  Figure 4 Link: articels_figures_by_rev_year\2017\Binary_Quadratic_Programing_for_Online_Tracking_of_Hundreds_of_People_in_Extreme\figure_4.jpg
  Figure 4 caption: An example of groups that move with coherent motion in several
    sequences (each color corresponds to one group). These groups are used to incorporate
    the neighborhood motion effect in our optimization.
  Figure 5 Link: articels_figures_by_rev_year\2017\Binary_Quadratic_Programing_for_Online_Tracking_of_Hundreds_of_People_in_Extreme\figure_5.jpg
  Figure 5 caption: An example of our minimum spanning tree group structure model.
    The figure on the left shows the groups found in frame 638 of Galleria1 sequence
    (nearby tracks with similar color correspond to one group). The figure on the
    right illustrates the model created for the group inside the yellow box. p i denotes
    the location of the target and e i j denotes the relative distance between the
    two targets i and j .
  Figure 6 Link: articels_figures_by_rev_year\2017\Binary_Quadratic_Programing_for_Online_Tracking_of_Hundreds_of_People_in_Extreme\figure_6.jpg
  Figure 6 caption: A comparison of our speed-up sampling technique versus dense sampling
    of candidate locations. (a) shows the targets to be tracked. (b) illustrates the
    candidates sampled densely in the neighborhood of each target (each yellow dot
    represent one candidate location). (c) shows the selected extrema locations (each
    candidate is shown with a cross). (d) visualizes the final candidates that survived
    using the proposed speed-up technique (each candidate is shown with a small dot).
  Figure 7 Link: articels_figures_by_rev_year\2017\Binary_Quadratic_Programing_for_Online_Tracking_of_Hundreds_of_People_in_Extreme\figure_7.jpg
  Figure 7 caption: This figure illustrates the run-time versus the number of extremas
    ( m ).
  Figure 8 Link: articels_figures_by_rev_year\2017\Binary_Quadratic_Programing_for_Online_Tracking_of_Hundreds_of_People_in_Extreme\figure_8.jpg
  Figure 8 caption: This figure shows tracks obtained using our method for a few challenging
    targets. We also compare our results with the one in [11]. For some sequences
    the proposed method gives tracks very close to the ground truth tracks. Since
    green is superimposed on yellow, due to that tracks in yellow may not be that
    visible.
  Figure 9 Link: articels_figures_by_rev_year\2017\Binary_Quadratic_Programing_for_Online_Tracking_of_Hundreds_of_People_in_Extreme\figure_9.jpg
  Figure 9 caption: This figure shows tracks (shown on the left) and quantitative
    results of our method (shown on the right). We compare our method with PNMC [11]
    which achieves the best results on the 9 high-density crowd sequences.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Afshin Dehghan
  Name of the last author: Mubarak Shah
  Number of Figures: 14
  Number of Tables: 4
  Number of authors: 2
  Paper title: Binary Quadratic Programing for Online Tracking of Hundreds of People
    in Extremely Crowded Scenes
  Publication Date: 2017-03-24 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Results of Our Method in Terms of Tracking Accuracy
      When Pixel Threshold Is Set to 15
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Result of Our Method in Terms of Tracking Accuracy
      When Pixel Threshold Is Set to Five on All Sequences
  Table 3 caption:
    table_text: TABLE 3 Quantitative Comparison of Tracking Accuracy Using Different
      Terms in Cost Function of Eq. (1)
  Table 4 caption:
    table_text: TABLE 4 Quantitative Comparison, in Terms of Tracking Accuracy, of
      Our Method with the Tracker of [11] When Pixel Threshold Is Set to 15 on Two
      New Sequences of Galleria1 and Galleria2
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2687462
- Affiliation of the first author: department of medical biophysics, university of
    western ontario, london, on, canada
  Affiliation of the last author: department of medical biophysics, university of
    western ontario, london, on, canada
  Figure 1 Link: articels_figures_by_rev_year\2017\MultiTarget_Regression_via_Robust_LowRank_Learning\figure_1.jpg
  Figure 1 caption: The architecture of the proposed multi-layer multi-target regression
    (MMR).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\MultiTarget_Regression_via_Robust_LowRank_Learning\figure_2.jpg
  Figure 2 caption: The convergence of the proposed alternating optimization algorithm
    on the two representative datasets, i.e., EDM and SCM1d with 2 and 16 targets,
    respectively. J(A,S) is the value of the objective function and aMSE is the average
    mean square error.
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.76
  Name of the first author: Xiantong Zhen
  Name of the last author: Shuo Li
  Number of Figures: 2
  Number of Tables: 2
  Number of authors: 4
  Paper title: Multi-Target Regression via Robust Low-Rank Learning
  Publication Date: 2017-03-28 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Statistics of the 18 Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Comparison with State-of-the-Art Algorithms on 18 Real-World
      Datasets in Terms of aRRMSE (%)
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2688363
- Affiliation of the first author: department of electrical and computer engineering,
    national university of singapore, singapore
  Affiliation of the last author: key laboratory of machine perception (moe), peking
    university, beijing shi, china
  Figure 1 Link: articels_figures_by_rev_year\2017\A_Unified_Alternating_Direction_Method_of_Multipliers_by_Majorization_Minimizati\figure_1.jpg
  Figure 1 caption: "Plots of (a) |f( x k )\u2212f( x \u2217 )| ; (b) residual \u2225\
    \ \u2225 A x k \u2212b \u2225 \u2225 ; (c) relative error \u2225 \u2225 x k \u2212\
    \ x \u2217 \u2225 \u2225 \u2225 x \u2217 \u2225 on the basis pursuit problem."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\A_Unified_Alternating_Direction_Method_of_Multipliers_by_Majorization_Minimizati\figure_2.jpg
  Figure 2 caption: "Plots of (a) sorted \u2225 A i \u2225 2 2 ,i=1,\u2026,n in descending\
    \ order; (b) L B 1 + L B 2 versus n 1 ; (c) f( x k ) versus k ; (d) residual \u2225\
    \ \u2225 A x k \u2212b \u2225 \u2225 versus k based on different variable partitions\
    \ (corresponding to different n 1 ); (e) dual residual \u2225 \u2225 x k+1 \u2212\
    \ x k \u2225 \u2225 versus k , and (f) f( x 1000 ) versus n 1 ."
  Figure 3 Link: articels_figures_by_rev_year\2017\A_Unified_Alternating_Direction_Method_of_Multipliers_by_Majorization_Minimizati\figure_3.jpg
  Figure 3 caption: "Comparison of L-ADMM-PS (3), M-ADMM (3) and M-ADMM (2) on different\
    \ choices of \u03BB : (a) \u03BB=0.001 ; (b) \u03BB=0.1 and (c) \u03BB=10 . Top\
    \ row: plots of f( x k ) versus CPU time; middle row: Plots of \u2225 \u2225 A\
    \ x k \u2212b \u2225 \u2225 versus CPU time. (d) Subspace clustering accuracy\
    \ versus \u03BB . In (a)-(c), for better visualization, we plot the objective\
    \ value in a relatively smaller range in sub-figures."
  Figure 4 Link: articels_figures_by_rev_year\2017\A_Unified_Alternating_Direction_Method_of_Multipliers_by_Majorization_Minimizati\figure_4.jpg
  Figure 4 caption: Images used for nonnegative matrix completion.
  Figure 5 Link: articels_figures_by_rev_year\2017\A_Unified_Alternating_Direction_Method_of_Multipliers_by_Majorization_Minimizati\figure_5.jpg
  Figure 5 caption: 'Top row: The observed noisy image (left), recovered image by
    L-ADMM-PS (middle), and recovered image by M-ADMM (right). Bottom row: plots of
    f(mathbf xk) versus CPU time (left), plots of leftVert mathbf Amathbf xk-mathbf
    brightVert versus CPU time (middle), and PSNR values versus CPU time (right).'
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.95
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.64
  Name of the first author: Canyi Lu
  Name of the last author: Zhouchen Lin
  Number of Figures: 5
  Number of Tables: 4
  Number of authors: 4
  Paper title: A Unified Alternating Direction Method of Multipliers by Majorization
    Minimization
  Publication Date: 2017-03-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Previous Gauss-Seidel ADMMs Are Special Cases of Algorithm
      1 with Different f 1 and G 1
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Previous Jacobian ADMMs Are Special Cases of Algorithm 2 with
      Different f i and G i
  Table 3 caption:
    table_text: TABLE 3 Comparison of L-ADMM-PS (3) and M-ADMM (3) and M-ADMM (2)
      for Latent LRR on the Hopkins 155 Dataset
  Table 4 caption:
    table_text: TABLE 4 Numerical Comparison on the Image Inpainting
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2689021
- Affiliation of the first author: san diego state university (sdsu), san diego, ca
  Affiliation of the last author: university of california at los angeles (ucla),
    los angeles, ca
  Figure 1 Link: articels_figures_by_rev_year\2017\SingleView_D_Scene_Reconstruction_and_Parsing_by_Attribute_Grammar\figure_1.jpg
  Figure 1 caption: A typical result of our approach. (a) Input image overlaid with
    parallel lines, where each color indicates a family of parallel lines; (b) surface
    normal map, where each color indicates a unique normal orientation; (c) a synthesized
    image from a novel viewpoint; and (d) depth map, where darker pixels are closer
    to the camera.
  Figure 10 Link: articels_figures_by_rev_year\2017\SingleView_D_Scene_Reconstruction_and_Parsing_by_Attribute_Grammar\figure_10.jpg
  Figure 10 caption: Convergence of Algorithm 1. (a) Convergence curve calculated
    for the image in Fig. 9, i.e., the energy at each iteration. The solid red line
    indicates the optimal energy calculated from the ground-truth annotation. (b)
    Convergence curve calculated on 50 images from the dataset LMW-A, which plots
    the average energy at each iteration. We plot the average optimal energy (red
    line) of these images for comparisons.
  Figure 2 Link: articels_figures_by_rev_year\2017\SingleView_D_Scene_Reconstruction_and_Parsing_by_Attribute_Grammar\figure_2.jpg
  Figure 2 caption: "Parsing an image using attribute grammar. Left: global geometric\
    \ attributes are associated with the root node (scene) of the parse graph, including\
    \ focal length of camera, and Cartesian Coordinate System defined by Manhattan\
    \ frames. Right: parse graph augmented with local geometric attributes, such as\
    \ surface normal orientations and vanishing points (VPs) associated with a surface,\
    \ or multiple vanishing points for a building. R 1 ,\u2026, R 5 are the five grammar\
    \ rules used for scene decomposition."
  Figure 3 Link: articels_figures_by_rev_year\2017\SingleView_D_Scene_Reconstruction_and_Parsing_by_Attribute_Grammar\figure_3.jpg
  Figure 3 caption: Illustration of surface normal. A planar region often contains
    two sets of orthogonal parallel lines converging at two vanishing points ( x 1
    , y 1 ) and ( x 2 , y 2 ) , respectively. f is the camera focal length. Thus the
    surface normal is the cross-product of the two Manhattan axes ( x 1 , y 1 ,f)
    and ( x 2 , y 2 ,f) in the camera coordinate (taking camera position O as the
    origin).
  Figure 4 Link: articels_figures_by_rev_year\2017\SingleView_D_Scene_Reconstruction_and_Parsing_by_Attribute_Grammar\figure_4.jpg
  Figure 4 caption: Illustration of hierarchical scene entities in the attribute planar
    representation and their associated geometric attributes. Each entity is also
    entitled with a semantic attribute, e.g., building, sky etc.
  Figure 5 Link: articels_figures_by_rev_year\2017\SingleView_D_Scene_Reconstruction_and_Parsing_by_Attribute_Grammar\figure_5.jpg
  Figure 5 caption: 'Illustration of the five grammar rules. Each rule is associated
    with a set of geometric attributes that impose constraints over the parent node
    and children nodes. Layout rule: a children planar surface is supporting the other
    n children entities; siding rule: two children planar surfaces of the same semantic
    label are spatially connected; affinity rule: two children planar surfaces have
    the same appearance; mesh rule: multiple children surfaces appear in a 2D mesh
    structure; instance rule: links a terminal node and its image representation.'
  Figure 6 Link: articels_figures_by_rev_year\2017\SingleView_D_Scene_Reconstruction_and_Parsing_by_Attribute_Grammar\figure_6.jpg
  Figure 6 caption: Illustration of piece-wise linear spline model for the contact
    boundaries of composite surfaces. Each spline consists of several control points
    and the straight lines between these points. Note that each straight line correlates
    with one planar region in the composite surface.
  Figure 7 Link: articels_figures_by_rev_year\2017\SingleView_D_Scene_Reconstruction_and_Parsing_by_Attribute_Grammar\figure_7.jpg
  Figure 7 caption: 'Diffusion and jump dynamics. (a) An initial parse graph that
    includes a root node and terminal nodes; (b) jump dynamic: birth (from left-hand
    to right-hand) or death (from right-hand to left-hand) of non-terminal nodes;
    (c) diffusion dynamic: re-grouping superpixels.'
  Figure 8 Link: articels_figures_by_rev_year\2017\SingleView_D_Scene_Reconstruction_and_Parsing_by_Attribute_Grammar\figure_8.jpg
  Figure 8 caption: Focal length estimation. (a) Input image overlaid with parallel
    families of edges (colored), each of which corresponds to a vanishing point on
    the image plane; (b) focal length estimated by orthogonal pairs of vanishing points;
    (c) focal length estimated by non-orthogonal pairs of vanishing points. The true
    focal length is 500 (red dotted lines).
  Figure 9 Link: articels_figures_by_rev_year\2017\SingleView_D_Scene_Reconstruction_and_Parsing_by_Attribute_Grammar\figure_9.jpg
  Figure 9 caption: 'Results of normal map estimation over iterations in Algorithm
    1. Row-1: input image (left), surface normal map (middle) obtained during the
    stage-2, and that during the stage 3 (right). Row-2: three normal maps obtained
    during the stage 3 after 300, 500 and 1,000 iterations, respectively. Each color
    indicates a unique normal orientation.'
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.61
  Name of the first author: Xiaobai Liu
  Name of the last author: Song-Chun Zhu
  Number of Figures: 15
  Number of Tables: 3
  Number of authors: 3
  Paper title: Single-View 3D Scene Reconstruction and Parsing by Attribute Grammar
  Publication Date: 2017-03-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Definitions of Grammar Rules and Their Geometric Attributes
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Numerical Comparisons on Normal Orientation Estimation
  Table 3 caption:
    table_text: TABLE 3 Numerical Comparisons on Region Labeling
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2689007
