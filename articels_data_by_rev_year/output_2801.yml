- Affiliation of the first author: college of computer, national university of defense
    technology, changsha, china
  Affiliation of the last author: college of computer, national university of defense
    technology, changsha, china
  Figure 1 Link: articels_figures_by_rev_year\2022\SimpleMKKM_Simple_Multiple_Kernel_KMeans\figure_1.jpg
  Figure 1 caption: The objective value of SimpleMKKM-C with the variation of iterations
    on all benchmarks. SimpleMKKM-C adopts an alternate optimization strategy to solve
    the objective of SimpleMKKM.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\SimpleMKKM_Simple_Multiple_Kernel_KMeans\figure_2.jpg
  Figure 2 caption: Clustering comparison of the learned H by MKKM and the proposed
    SimpleMKKM with iterations.
  Figure 3 Link: articels_figures_by_rev_year\2022\SimpleMKKM_Simple_Multiple_Kernel_KMeans\figure_3.jpg
  Figure 3 caption: Clustering performance of aforementioned algorithms with different
    number of samples on Caltech102.
  Figure 4 Link: articels_figures_by_rev_year\2022\SimpleMKKM_Simple_Multiple_Kernel_KMeans\figure_4.jpg
  Figure 4 caption: Clustering performance of aforementioned algorithms with different
    number of base kernels on Caltech102.
  Figure 5 Link: articels_figures_by_rev_year\2022\SimpleMKKM_Simple_Multiple_Kernel_KMeans\figure_5.jpg
  Figure 5 caption: The kernel weights learned by different algorithms. SimpleMKKM
    maintains reduced sparsity compared to several competitors. Other datasets omitted
    due to space limit.
  Figure 6 Link: articels_figures_by_rev_year\2022\SimpleMKKM_Simple_Multiple_Kernel_KMeans\figure_6.jpg
  Figure 6 caption: The iterative objective curves of SimpleMKKM under ten different
    initialization on Flo17, Flo102, PFold, CCV, Digit, and Cal-30. Though with different
    initialization, the objective value at stopping point is the same.
  Figure 7 Link: articels_figures_by_rev_year\2022\SimpleMKKM_Simple_Multiple_Kernel_KMeans\figure_7.jpg
  Figure 7 caption: Running time of different algorithms on 11 benchmark datasets
    (in second). The experiments are conducted on a PC with Intel(R) Core(TM)-i7-5820
    3.3 GHz CPU and 32G RAM in MATLAB environment. SimpleMKKM is comparably fast to
    alternatives while providing superior performance and requiring no hyper-parameter
    tuning.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xinwang Liu
  Name of the last author: Xinwang Liu
  Number of Figures: 7
  Number of Tables: 3
  Number of authors: 1
  Paper title: 'SimpleMKKM: Simple Multiple Kernel K-Means'
  Publication Date: 2022-08-16 00:00:00
  Table 1 caption: TABLE 1 Specification of Our 11 Benchmark Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Clustering Performance of SimpleMKKM and Ten Baselines
    on Five Benchmarks in Terms of ACC, NMI, Purity, and Rand Index
  Table 3 caption: TABLE 3 Empirical Comparison of SimpleMKKM With MKKM, MKKM-R and
    SimpleMKKM-C on All Datasets
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3198638
- Affiliation of the first author: department of operational research and planning,
    naval university of engineering, wuhan, china
  Affiliation of the last author: science and technology on information systems engineering
    laboratory, national university of defense technology, changsha, hunan, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Debiased_Scene_Graph_Generation_for_Dual_Imbalance_Learning\figure_1.jpg
  Figure 1 caption: The scene graph generation faces the dual imbalance problem. (a)
    The long-tailed distribution of foreground relationships on VG150 [3]. (b) The
    long-tailed distribution of foreground relationships on VrR-VG.(c)The background-foreground
    imbalance on VrR-VG and VG150. (d) R100 and mR100 of our DSDI and VCTree-TDE on
    SGCls task on VG150. (e) mR100 of DSDI and VCTree-TDE on PredCls on VG150 and
    VrR-VG at different background-foreground scales.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Debiased_Scene_Graph_Generation_for_Dual_Imbalance_Learning\figure_2.jpg
  Figure 2 caption: An overall pipeline of our DSDI. Given an image, the Faster RCNN
    and BiTreeLSTM are firstly adopted to generate the proposals (a) and objects (b).
    Then, the unbiased relationship logits are learned from the causal graph and causal
    intervention (c). Finally, the BR loss is used for debiased training under dual
    imbalanced data (d).
  Figure 3 Link: articels_figures_by_rev_year\2022\Debiased_Scene_Graph_Generation_for_Dual_Imbalance_Learning\figure_3.jpg
  Figure 3 caption: "The proposed causal graph of content ( C n ) and context ( C\
    \ x ) in SGG and the causal intervention P( r i\u2192j |do(O)) ."
  Figure 4 Link: articels_figures_by_rev_year\2022\Debiased_Scene_Graph_Generation_for_Dual_Imbalance_Learning\figure_4.jpg
  Figure 4 caption: A case study of the differences between P( o k | o i =book, o
    j =table) and P( o k |do(O)) on VG150 dataset.
  Figure 5 Link: articels_figures_by_rev_year\2022\Debiased_Scene_Graph_Generation_for_Dual_Imbalance_Learning\figure_5.jpg
  Figure 5 caption: An illustration about the foreground features space distribution
    for biased SGG methods (a) and DSDI with BR loss (b). The blue and yellow areas
    represent the features spaces of background and foreground, respectively.
  Figure 6 Link: articels_figures_by_rev_year\2022\Debiased_Scene_Graph_Generation_for_Dual_Imbalance_Learning\figure_6.jpg
  Figure 6 caption: (a) R100 of our DSDI and VCTree-TDE [2] on PredCls task on VG150
    dataset. (b) The number of errors for some head classes on PredCls task on VG150
    test set. (c) The number of rightness for some tail classes on PredCls task on
    VG150 test set. (d) R100 of our DSDI and VCTree-TDE on PredCls task on VrR-VG
    dataset. (e) The proportion of classifier weights Vert wi Vert for each class
    of our DSDI and VCTree.
  Figure 7 Link: articels_figures_by_rev_year\2022\Debiased_Scene_Graph_Generation_for_Dual_Imbalance_Learning\figure_7.jpg
  Figure 7 caption: 'The qualitative examples of scene graphs from VCTree-TDE dag
    (yellow) and our DSDI (green) on VG150 dataset. Green edges are true positives,
    red edges are misclassified foreground relationships and grey edges are foreground
    relationships classified as background. Specially, the blue dotted edges are spurious
    correlations at R50 setting. Top: the results with some spurious correlations.
    Mid: the results with some tail classes. Bottom: bounded rationality and language
    bias in annotations (red), resulting in the decrease of RK performance.'
  Figure 8 Link: articels_figures_by_rev_year\2022\Debiased_Scene_Graph_Generation_for_Dual_Imbalance_Learning\figure_8.jpg
  Figure 8 caption: 'The qualitative examples of scene graphs from VCTree-TDE dag
    (yellow) and our DSDI (green) on VrR-VG dataset. Green edges are true positives,
    red edges are misclassified foreground relationships and grey edges are foreground
    relationships classified as background. Top: the compared results between VCTree-TDE
    and DSDI. Bottom: bounded rationality and language bias in annotations (red),
    resulting in the decrease of RK performance.'
  Figure 9 Link: articels_figures_by_rev_year\2022\Debiased_Scene_Graph_Generation_for_Dual_Imbalance_Learning\figure_9.jpg
  Figure 9 caption: The predicate-level performance of R100 for our DSDI and VCTree
    on PredCls task on VG150 dataset.
  First author gender probability: 0.9
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Hao Zhou
  Name of the last author: Jun Lei
  Number of Figures: 9
  Number of Tables: 9
  Number of authors: 5
  Paper title: Debiased Scene Graph Generation for Dual Imbalance Learning
  Publication Date: 2022-08-16 00:00:00
  Table 1 caption: TABLE 1 The mRK (%) of DSDI Compared to the State-of-the-Art Methods
    on VG150 With Constraints
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The mRK (%) of DSDI Compared to the State-of-the-Art Methods
    on VG150 Without Constraints
  Table 3 caption: TABLE 3 The RK and mRK (%)Of DSDI Compared to State-of-the-Art
    Debiased Methods on VG150 Dataset With Constraints
  Table 4 caption: TABLE 4 The RK and mRK (%) of DSDI Compared to State-of-the-Art
    Debiased Methods on VG150 Dataset Without Constraints
  Table 5 caption: TABLE 5 The RK and mRK (%)Of Our DSDI Compared to State-of-the-Art
    Debiased Methods on VrR-VG Dataset With Constraints
  Table 6 caption: TABLE 6 The RK and mRK (%) of Our DSDI Compared to State-of-the-Art
    Debiased Methods on VrR-VG Dataset Without Constraints
  Table 7 caption: 'TABLE 7 Ablation Studies of the Proposed Modules: BR Loss and
    Causal Intervention Tree (CIT) on VG150'
  Table 8 caption: TABLE 8 The mRK Results of Our DSDI Compared With the Traditional
    Imbalance Learning Strategies
  Table 9 caption: 'TABLE 9 The mRK Results of Three Different and Representative
    Models: Motifs, VTransE and VCTree Equipped With Our BR Loss'
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3198965
- Affiliation of the first author: department of artificial intelligence, xiamen university,
    xiamen, china
  Affiliation of the last author: department of information engineering and computer
    science, university of trento, trento, italy
  Figure 1 Link: articels_figures_by_rev_year\2022\Towards_Robust_Person_ReIdentification_by_Defending_Against_Universal_Attackers\figure_1.jpg
  Figure 1 caption: Schematic illustration of our MetaAttack and MetaDefense. (a)
    MetaAttack has a good universality, which can mislead models trained on different
    domains with the same model, the same domain with different models, and different
    domains with different models. (b) MetaDefense is to learn a robust re-ID model
    that can defense various attacks achieved by different types of algorithms.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Towards_Robust_Person_ReIdentification_by_Defending_Against_Universal_Attackers\figure_2.jpg
  Figure 2 caption: "The process of the proposed combinatorial universal attack. Given\
    \ an image, we first perturb it by the functional color attack. Then, we produce\
    \ the final adversarial example by adding the learned universal perturbation \u03B4\
    \ to the color-attacked image."
  Figure 3 Link: articels_figures_by_rev_year\2022\Towards_Robust_Person_ReIdentification_by_Defending_Against_Universal_Attackers\figure_3.jpg
  Figure 3 caption: "The overall framework of the proposed \u201CMetaAttack\u201D\
    \ and \u201CMetaDefense\u201D. Overall: in both \u201CMetaAttack\u201D and \u201C\
    MetaDefense\u201D, we regard the real dataset as meta-train and the virtual dataset\
    \ as meta-test. A meta-learning strategy is applied to learn the universal attack\
    \ (MetaAttack) and the robust re-ID model (MetaDefense). (a) MetaAttack: (1) We\
    \ first generate adversarial samples for the meta-train with the original attack\
    \ and obtain a temporary attack with the meta-train loss L mtr . (2) We then generate\
    \ adversarial samples for the meta-test with the obtained temporary attack and\
    \ calculate the meta-test loss L mte . (3) The original attack is updated with\
    \ the combination of L mtr and L mte . In this way, the proposed MetaAttack encourages\
    \ the attack to capture more underlying variations across domains, and thus to\
    \ have better universality. (b) MetaDefense: Similar to MetaAttack, we train the\
    \ re-ID model with three steps. The difference is that we fix the attack and update\
    \ the re-ID model with corresponding defense losses. With our MetaDefense, the\
    \ re-ID model is enforced to be robust to unseen adversarial examples."
  Figure 4 Link: articels_figures_by_rev_year\2022\Towards_Robust_Person_ReIdentification_by_Defending_Against_Universal_Attackers\figure_4.jpg
  Figure 4 caption: "Examples of corrupted queries with different \u03F5 ."
  Figure 5 Link: articels_figures_by_rev_year\2022\Towards_Robust_Person_ReIdentification_by_Defending_Against_Universal_Attackers\figure_5.jpg
  Figure 5 caption: Examples of the perturbations and corrupted queries for three
    attacks trained with our method. We use IDE as the training method and Market
    as the source data.
  Figure 6 Link: articels_figures_by_rev_year\2022\Towards_Robust_Person_ReIdentification_by_Defending_Against_Universal_Attackers\figure_6.jpg
  Figure 6 caption: Visualization of ranking lists under cross-dataset attack. We
    train the adversarial perturbation with PCB model trained on Market and attack
    the PCB model trained on Duke. The queries are evaluated on the Duke dataset.
  Figure 7 Link: articels_figures_by_rev_year\2022\Towards_Robust_Person_ReIdentification_by_Defending_Against_Universal_Attackers\figure_7.jpg
  Figure 7 caption: "Visualization of ranking lists for two query images under attack\
    \ \u201CCol.+Del.\u201D disturbing a normally trained model \u201CNormal\u201D\
    \ and the proposed metric-preserving model \u201CM(Col.+Del.)\u201D ."
  Figure 8 Link: articels_figures_by_rev_year\2022\Towards_Robust_Person_ReIdentification_by_Defending_Against_Universal_Attackers\figure_8.jpg
  Figure 8 caption: "Visualization of incorrect ranking lists for query images under\
    \ \u201CM(Col.+Del.)\u201D against \u201CCol.+Del.\u201D."
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.67
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Fengxiang Yang
  Name of the last author: Nicu Sebe
  Number of Figures: 8
  Number of Tables: 19
  Number of authors: 10
  Paper title: Towards Robust Person Re-Identification by Defending Against Universal
    Attackers
  Publication Date: 2022-08-16 00:00:00
  Table 1 caption: TABLE 1 Performance of Source Attack With Three Methods (Color
    Attack, Delta Attack and Combinatorial Attack)
  Table 10 caption: TABLE 10 Attack Results of Using two Virtual Datasets
  Table 2 caption: TABLE 2 Performance of Cross-Model Attack With Three Methods
  Table 3 caption: TABLE 3 Performance of Cross-Dataset Attack With Three Methods
    (Color Attack, Delta Attack and Combinatorial Attack)
  Table 4 caption: TABLE 4 The Results of Cross-Model & Dataset Attack
  Table 5 caption: TABLE 5 Comparison Between the Proposed Attack (Metaattack With
    Combinatorial Attack) and the State-of-the-Arts (MisRank [12] and UAP-Retrieval
    [18]) Under the Settings of Source Attack and Cross-Dataset Attack
  Table 6 caption: TABLE 6 Ablation Study on Meta-Learning and Meta-Test Sets
  Table 7 caption: TABLE 7 Comparison of Using Different Training Datasets
  Table 8 caption: "TABLE 8 Sensitivity Analysis of \u03F5 \u03B5"
  Table 9 caption: TABLE 9 Comparison of SSIM Scores Between Different Attack Methods
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3199013
- Affiliation of the first author: school of automation, southeast university, nanjing,
    china
  Affiliation of the last author: school of automation, southeast university, nanjing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2022\ObjectOccluded_Human_Shape_and_Pose_Estimation_With_Probabilistic_Latent_Consist\figure_1.jpg
  Figure 1 caption: Given an object-occluded human image (a), our method estimates
    visible features (b) and then reconstructs a complete human mesh (c). The different
    views of the mesh are shown in (d).
  Figure 10 Link: articels_figures_by_rev_year\2022\ObjectOccluded_Human_Shape_and_Pose_Estimation_With_Probabilistic_Latent_Consist\figure_10.jpg
  Figure 10 caption: Relationship between the reconstruction accuracy and the occlusion
    ratio.
  Figure 2 Link: articels_figures_by_rev_year\2022\ObjectOccluded_Human_Shape_and_Pose_Estimation_With_Probabilistic_Latent_Consist\figure_2.jpg
  Figure 2 caption: Overview of the proposed two-branch network. The training phase
    contains two stages. In the first stage (green background), the UV map inpainting
    branch (a) is trained, and both the network parameters of the encoder and decoder
    are fixed. In the second stage (pink background), the occluded color image, its
    saliency map and visibility heatmap (b) are concatenated and fed to the color
    image encoder (c). The corresponding partial UV map is encoded by the fixed inpainting
    network and used for supervising the color image encoder in the latent space (d).
    At the inference phase (blue background), a single color image is passed through
    the visible feature estimation sub-net (b), the occluded human reconstruction
    sub-net (c), and the decoder of the inpainting network to regress a UV position
    map. The output mesh is directly re-sampled from the UV position map.
  Figure 3 Link: articels_figures_by_rev_year\2022\ObjectOccluded_Human_Shape_and_Pose_Estimation_With_Probabilistic_Latent_Consist\figure_3.jpg
  Figure 3 caption: The illustration of UV representation. The vertex position can
    be recorded in the RGB channel of the 2D UV position map via the UV coordinate
    map.
  Figure 4 Link: articels_figures_by_rev_year\2022\ObjectOccluded_Human_Shape_and_Pose_Estimation_With_Probabilistic_Latent_Consist\figure_4.jpg
  Figure 4 caption: "The representation of the object-occluded human. Given an occluded\
    \ human image (a) and the corresponding occlusion segmentation (b), we render\
    \ the fitted human body model onto the 2D image plane (c). With the occlusion\
    \ segmentation (b), we can get occluded mesh (d). We store the normalized x,y,z\
    \ coordinates of the vertices as r,g,b color values in the UV map for the visible\
    \ part. For the occlusion part, we set the r,g,b values of the UV map to \u2212\
    0.5 (e)."
  Figure 5 Link: articels_figures_by_rev_year\2022\ObjectOccluded_Human_Shape_and_Pose_Estimation_With_Probabilistic_Latent_Consist\figure_5.jpg
  Figure 5 caption: Overview of visible feature estimation sub-network. We propose
    a sub-network to estimate human saliency maps (b) and visibility heatmaps (c)
    from occluded RGB images (a). The saliency map (b) aims to reduce invalid information
    such as background and occlusion. The heatmap (c) can be used to align the regressed
    mesh to the RGB image, which also improves shape estimation performance. We use
    different scales of masks and heatmaps as intermediate supervision.
  Figure 6 Link: articels_figures_by_rev_year\2022\ObjectOccluded_Human_Shape_and_Pose_Estimation_With_Probabilistic_Latent_Consist\figure_6.jpg
  Figure 6 caption: Samples of the 3DOH50K. Each image (a) in our dataset includes
    SMPL parameters (b), 2D and 3D keypoints (c) and accurate segmentation (d).
  Figure 7 Link: articels_figures_by_rev_year\2022\ObjectOccluded_Human_Shape_and_Pose_Estimation_With_Probabilistic_Latent_Consist\figure_7.jpg
  Figure 7 caption: Qualitative results of our approach. The left 3 columns show the
    input images (a), recovered meshes (b), and different views of the meshes (c)
    on our 3DOH50K. The right 3 columns are the examples on 3DPW[16], UP-3D[6] and
    Synthetic Occlusion Human3.6M[12] respectively (d). (e,f) show the corresponding
    results. The results are obtained by fitting the SMPL model to the regressed meshes.
    More results without SMPL fitting are shown in the Figs. 14 and 16.
  Figure 8 Link: articels_figures_by_rev_year\2022\ObjectOccluded_Human_Shape_and_Pose_Estimation_With_Probabilistic_Latent_Consist\figure_8.jpg
  Figure 8 caption: Comparison with state-of-the-art. (a) is the input images. (b)
    is the reconstructed results of our method. (c) (d) (e) are SPIN [68], GraphCMR
    [69] and HMR [1] respectively. Our method could obtain more visually appealing
    results.
  Figure 9 Link: articels_figures_by_rev_year\2022\ObjectOccluded_Human_Shape_and_Pose_Estimation_With_Probabilistic_Latent_Consist\figure_9.jpg
  Figure 9 caption: Comparison between heatmap branch (b) and camera branch (c) in
    mesh rendering. (a) is the input images. The camera branch (c) is easily affected
    by occlusion, while the heatmap branch (b) not.
  First author gender probability: 0.61
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Buzhen Huang
  Name of the last author: Yangang Wang
  Number of Figures: 16
  Number of Tables: 5
  Number of authors: 3
  Paper title: Object-Occluded Human Shape and Pose Estimation With Probabilistic
    Latent Consistency
  Publication Date: 2022-08-17 00:00:00
  Table 1 caption: TABLE 1 Comparison Among Different Public Datasets Related to 3D
    Pose Estimation
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparisons With the State-of-the-Art Methods on Human3.6M
    and MPI-INF-3DHP
  Table 3 caption: TABLE 3 Comparisons With the State-of-the-Art Methods on 3DPW and
    Our 3DOH50K
  Table 4 caption: TABLE 4 Ablation on Proposed Modules and Different Structures on
    Synthetic Occlusion Human3.6M
  Table 5 caption: TABLE 5 Ablation on Proposed Modules and Different Structures on
    3DOH50K With Real Object Occlusions
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3199449
- Affiliation of the first author: "mila & diro, universit\xE9 de montr\xE9al, montreal,\
    \ qc, canada"
  Affiliation of the last author: servicenow, santa clara, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\A_Survey_of_SelfSupervised_and_FewShot_Object_Detection\figure_1.jpg
  Figure 1 caption: 'A taxonomy of object detection methods reviewed in this survey.
    We categorize them based on the following hierarchy: methods using supervised
    backbone pretraining, methods using self-supervised pretraining of backbone and
    detection heads, and self-supervised backbone pretraining methods. In parallel,
    we also tag (shaded rectangles) those methods depending on whether they have been
    benchmarked on regular object detection, few-shotlow-shot object detection, and
    ImageNet classification. As discussed in Section 5, many self-supervised classification
    methods have also been used to initialize object detection backbones and evaluated
    on object detection benchmarks. DETReg [6], which is a self-supervised object
    detection method, obtained state-of-the-art FSOD results on MS COCO and uses self-supervised
    pretraining of the entire architecture.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\A_Survey_of_SelfSupervised_and_FewShot_Object_Detection\figure_2.jpg
  Figure 2 caption: A Faster R-CNN with Feature Pyramid Network. The input image is
    fed to the backbone network, then the feature pyramid network (light yellow) computes
    multi-scale features. The region proposal network proposes candidate boxes, which
    are filtered with non-maximum suppression (NMS). Features for the remaining boxes
    are pooled with RoIAlign and fed to the box head, which predicts object category
    and refined box coordinates. Finally, redundant and low-quality predictions are
    removed with NMS. Blue labels are class names in the detectron2 implementation.
    Figure courtesy of Hiroto Honda. https:medium.comhirotoschwertdigging-into-detectron-2-47b2e794fabd.
  Figure 3 Link: articels_figures_by_rev_year\2022\A_Survey_of_SelfSupervised_and_FewShot_Object_Detection\figure_3.jpg
  Figure 3 caption: The DETR object detector. The image is fed to the backbone, then
    positional encodings are added to the features and fed to the transformer encoder.
    The decoder takes as input object query embeddings, cross-attends to the encoded
    representation while performing self-attention on the transformed query embeddings,
    and outputs a fixed number of object detections, which are finally thresholded,
    without need for NMS [9]. Image courtesy of Carion et al. [9].
  Figure 4 Link: articels_figures_by_rev_year\2022\A_Survey_of_SelfSupervised_and_FewShot_Object_Detection\figure_4.jpg
  Figure 4 caption: Few-shot object detection protocol, as proposed by Kang et al.
    [54]. During base-training, the method is trained on base classes. Then during
    few-shot finetuning, the model is finetuned or conditioned on the support set.
    Finally, during few-shot evaluation, the method is evaluated on base and novel
    class detection.
  Figure 5 Link: articels_figures_by_rev_year\2022\A_Survey_of_SelfSupervised_and_FewShot_Object_Detection\figure_5.jpg
  Figure 5 caption: Importance of evaluating over several episodes. The nAP, nAP50
    and nAP75 of PASCAL VOC Split-1 are averaged using a variable number of episodes.
    Note how the means and variances only become stable after around 20 episodes.
    Figure courtesy of [110].
  Figure 6 Link: articels_figures_by_rev_year\2022\A_Survey_of_SelfSupervised_and_FewShot_Object_Detection\figure_6.jpg
  Figure 6 caption: 'Top: precisionk and recallk as a function of k the number of
    boxes considered. Bottom: precisionk and interpolated-precisionk as functions
    of the recallk (precision-recall curve).'
  Figure 7 Link: articels_figures_by_rev_year\2022\A_Survey_of_SelfSupervised_and_FewShot_Object_Detection\figure_7.jpg
  Figure 7 caption: 'DINOs attention maps. Since DINO is based on a visual transformer,
    the attention maps corresponding to the [CLS] token can be plotted. Despite being
    trained with no supervision, different attention heads are found to segment different
    objects. Source: Caron et al. [11].'
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.87
  Name of the first author: Gabriel Huang
  Name of the last author: "Pau Rodr\xEDguez"
  Number of Figures: 7
  Number of Tables: 4
  Number of authors: 5
  Paper title: A Survey of Self-Supervised and Few-Shot Object Detection
  Publication Date: 2022-08-17 00:00:00
  Table 1 caption: TABLE 1 Common FSOD Benchmarks
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Example Computation of Precisionk and Recallk
  Table 3 caption: TABLE 3 Few-Shot Object Detection Methods With Results on PASCAL
    VOC and MS COCO
  Table 4 caption: TABLE 4 Comparison of Self-Supervised Object Detection Methods
    Pretrained on Unlabeled ImageNet
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3199617
- Affiliation of the first author: graduate school of information science and technology,
    osaka university, osaka, japan
  Affiliation of the last author: graduate school of information science and technology,
    osaka university, osaka, japan
  Figure 1 Link: articels_figures_by_rev_year\2022\Discrete_Search_Photometric_Stereo_for_Fast_and_Accurate_Shape_Estimation\figure_1.jpg
  Figure 1 caption: An overview of our algorithm, discrete search photometric stereo
    (DSPS). It precomputes a database from discretized surface normals and reference
    BRDFs in a scene-independent manner. During inference from input images, surface
    normals of any scene are efficiently recovered by searching the precomputed database.
  Figure 10 Link: articels_figures_by_rev_year\2022\Discrete_Search_Photometric_Stereo_for_Fast_and_Accurate_Shape_Estimation\figure_10.jpg
  Figure 10 caption: Precomputation time of our three methods on a CPU and GPU for
    varying light configurations.
  Figure 2 Link: articels_figures_by_rev_year\2022\Discrete_Search_Photometric_Stereo_for_Fast_and_Accurate_Shape_Estimation\figure_2.jpg
  Figure 2 caption: Starting from the appearance tensor T that represents appearances
    for a comprehensive set of light directions, surface normals, and BRDFs, we slice
    out a sampled appearance matrix D i for a set of known light directions and a
    hypothesized surface normal n i . The column space of D i is the space of appearances
    over all possible materials for the hypothesized normal under the known light
    directions.
  Figure 3 Link: articels_figures_by_rev_year\2022\Discrete_Search_Photometric_Stereo_for_Fast_and_Accurate_Shape_Estimation\figure_3.jpg
  Figure 3 caption: "Geometric interpretation of the measurement reconstruction error.\
    \ The reconstruction error of measurements \u2225 Z i m \u2225 2 2 can be seen\
    \ as distance between the measurement vector m and the subspace spanned by D i\
    \ in the L \u2032 -dimensional space \u03A9 ."
  Figure 4 Link: articels_figures_by_rev_year\2022\Discrete_Search_Photometric_Stereo_for_Fast_and_Accurate_Shape_Estimation\figure_4.jpg
  Figure 4 caption: Ground truth surface normals and example images of PrincipledPS
    dataset.
  Figure 5 Link: articels_figures_by_rev_year\2022\Discrete_Search_Photometric_Stereo_for_Fast_and_Accurate_Shape_Estimation\figure_5.jpg
  Figure 5 caption: (a) CPU computation time of our methods and HS17. (b) GPU computation
    time of DSPS-D and CNN-PS.
  Figure 6 Link: articels_figures_by_rev_year\2022\Discrete_Search_Photometric_Stereo_for_Fast_and_Accurate_Shape_Estimation\figure_6.jpg
  Figure 6 caption: Angular error maps for BALL, COW, and READING objects from the
    DiLiGenT dataset [43] with all the 96 lights. See the supplementary materials
    for more objects.
  Figure 7 Link: articels_figures_by_rev_year\2022\Discrete_Search_Photometric_Stereo_for_Fast_and_Accurate_Shape_Estimation\figure_7.jpg
  Figure 7 caption: Difference in the angular error maps between our DSPS-DE with
    and without shadow masking. Blue indicates pixels, where surface normal estimation
    is improved by shadow masking, and red indicates the opposite.
  Figure 8 Link: articels_figures_by_rev_year\2022\Discrete_Search_Photometric_Stereo_for_Fast_and_Accurate_Shape_Estimation\figure_8.jpg
  Figure 8 caption: Relationship between the accuracy of surface normal estimation
    and the number of BRDFs in the appearance tensor in DSPS-D. The solid line shows
    the mean angular error of the ten trials, and the colored area shows the maximum
    and minimum angular errors of the trials.
  Figure 9 Link: articels_figures_by_rev_year\2022\Discrete_Search_Photometric_Stereo_for_Fast_and_Accurate_Shape_Estimation\figure_9.jpg
  Figure 9 caption: (a) Mean angular errors and (b) Computation time of our methods
    with varying number of surface normal candidates. This experiment is performed
    on the MERL sphere dataset with 100 lights.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Kenji Enomoto
  Name of the last author: Yasuyuki Matsushita
  Number of Figures: 10
  Number of Tables: 7
  Number of authors: 5
  Paper title: Discrete Search Photometric Stereo for Fast and Accurate Shape Estimation
  Publication Date: 2022-08-17 00:00:00
  Table 1 caption: TABLE 1 Comparison of Exemplar-Based Photometric Stereo Methods
    and Their Properties
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparisons on the MERL Sphere Dataset With Light Configuration
    10 Sets
  Table 3 caption: TABLE 3 Comparisons on the PrincipledPS Dataset
  Table 4 caption: TABLE 4 Comparisons on the DiLiGenT Dataset With 96 and 10 Lights
  Table 5 caption: TABLE 5 Mean Angular Errors of our DSPS-DE With K K Times Shadow
    Masking on the DiLiGenT Dataset
  Table 6 caption: TABLE 6 Mean Angular Errors and Standard Deviations (Mean Angular
    ErrorStandard Deviation) on the Corrupted MERL Sphere Datasets With 100 Lights
  Table 7 caption: TABLE 7 Increases of Angular Errors due to Discretized Lights
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3198729
- Affiliation of the first author: state key laboratory of information security, institute
    of information engineering, chinese academy of sciences, beijing, china
  Affiliation of the last author: school of computer science and technology, university
    of chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Optimizing_Partial_Area_Under_the_Topk_Curve_Theory_and_Practice\figure_1.jpg
  Figure 1 caption: Label ambiguity on the Places-365 dataset [1]. On one hand, the
    semantic similarity between Mountain and Valley makes it easy to make wrong predictions
    even for humans. On the other hand, many instances are inherently relevant with
    multiple classes such as Mountain and Sky.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Optimizing_Partial_Area_Under_the_Topk_Curve_Theory_and_Practice\figure_2.jpg
  Figure 2 caption: 'Comparisons of TOP-k and AUTKC : (a) TOP-k focuses on the performance
    at a single point of the TOP-k curve; (b) AUTKC-W considers the entire area under
    the TOP-k curve; (c) AUTKC-P emphasizes the partial area with k ranging in [ K
    1 , K 2 ] . We denote AUTKC-P as AUTKC in the rest discussion for convenience.'
  Figure 3 Link: articels_figures_by_rev_year\2022\Optimizing_Partial_Area_Under_the_Topk_Curve_Theory_and_Practice\figure_3.jpg
  Figure 3 caption: "The limitation (L1) of TOP-k . f 1 (x) is an ideal prediction,\
    \ while f 2 (x) is a bad prediction since it ranks the irrelevant label Beach\
    \ higher than the ground-truth label Lake. We expect err(y, f 2 (x))>err(y, f\
    \ 1 (x)) , but er r k (y, f 1 (x))=er r k (y, f 2 (x))=0 when k\u22652 ."
  Figure 4 Link: articels_figures_by_rev_year\2022\Optimizing_Partial_Area_Under_the_Topk_Curve_Theory_and_Practice\figure_4.jpg
  Figure 4 caption: The limitation (L2) of TOP-k . f and g perform inconsistently
    at different k . It is difficult to select which model to deploy unless the participant
    knows the k of interest ahead. In some scenarios, the k of interest changes dynamically,
    and optimizing the performance at a specific k is not a reasonable strategy.
  Figure 5 Link: articels_figures_by_rev_year\2022\Optimizing_Partial_Area_Under_the_Topk_Curve_Theory_and_Practice\figure_5.jpg
  Figure 5 caption: "The comparison between AUTKC Bayes optimality and TOP-k Bayes\
    \ optimality. AUTKC optimality requires that Lake\u227B Sky \u2013 \u2013 \u2013\
    \ \u2013 \u227B Cloud \u2013 \u2013 \u2013 \u2013 \u2013 \u2013 \u227BBeach\u227B\
    others , while TOP-k optimality does not."
  Figure 6 Link: articels_figures_by_rev_year\2022\Optimizing_Partial_Area_Under_the_Topk_Curve_Theory_and_Practice\figure_6.jpg
  Figure 6 caption: The normalized top- k accuracy gain with respect to CE on the
    four datasets. The scattered points represent the average performance of each
    method type, and the shadow represents the standard deviation.
  Figure 7 Link: articels_figures_by_rev_year\2022\Optimizing_Partial_Area_Under_the_Topk_Curve_Theory_and_Practice\figure_7.jpg
  Figure 7 caption: Case study on Places-365. For each input, we select the annotated
    label, ambiguous labels and some irrelevant labels to visualize the ranking result
    produced by different types of method. In each ranking list, the annotated label
    and the ambiguous labels are highlighted with green and blue box, respectively.
  Figure 8 Link: articels_figures_by_rev_year\2022\Optimizing_Partial_Area_Under_the_Topk_Curve_Theory_and_Practice\figure_8.jpg
  Figure 8 caption: Sensitivity analysis of AUTKC optimization methods on K , where
    the experiments are conducted on Places-365. For each box, K is fixed as the y
    -axis value, and the scattered points represent the performance with different
    hyperparameters.
  Figure 9 Link: articels_figures_by_rev_year\2022\Optimizing_Partial_Area_Under_the_Topk_Curve_Theory_and_Practice\figure_9.jpg
  Figure 9 caption: Sensitivity analysis of AUTKC optimization methods on Ew , where
    the experiments are conducted on Cifar-100. For each box, Ew is fixed as the y
    -axis value, and the scattered points represent the performance with different
    hyperparameters.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.89
  Name of the first author: Zitai Wang
  Name of the last author: Qingming Huang
  Number of Figures: 9
  Number of Tables: 3
  Number of authors: 6
  Paper title: 'Optimizing Partial Area Under the Top-k Curve: Theory and Practice'
  Publication Date: 2022-08-18 00:00:00
  Table 1 caption: TABLE 1 Some Important Notations Used in This Paper
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 The Prior Arts of TOP-k TOP-k Optimization, Where 1 \xAF\
    \ \xAF \xAF y :=1\u2212 e y 1\xAFy:=1-ey"
  Table 3 caption: "TABLE 3 The Empirical Results of AUTKC \u2191 K AUTKC\u2191K on\
    \ the Four Datasets"
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3199970
- Affiliation of the first author: school of computer science and technology, university
    of science and technology of china, hefei, china
  Affiliation of the last author: school of computer science and technology, university
    of science and technology of china, hefei, china
  Figure 1 Link: articels_figures_by_rev_year\2022\MultiTarget_Markov_Boundary_Discovery_Theory_Algorithm_and_Application\figure_1.jpg
  Figure 1 caption: Examples to illustrate the basic concepts in this paper. (1) MB
    of a target (e.g., Common Cold) contains parents (Frigid Weather, Specific gene
    sequence of Rhinovirus, and Same gene sequence of the two viruses), children (Coughing,
    Fatigue, and Allergy), and spouses (Pollen) of the target. (2) For multiple targets,
    common MB variables simultaneously influence multiple targets while target-specific
    MB variables influence a single target. For example, Coughing is the common MB
    variable of COVID-19 and Common Cold, while Respiratory Distress is the target-specific
    MB variable of COVID-19.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\MultiTarget_Markov_Boundary_Discovery_Theory_Algorithm_and_Application\figure_2.jpg
  Figure 2 caption: A simple example of Equivalent information. The response variable
    is T , and all variables take values 0,1 . Variables A and B , highlighted with
    the same color, contain equivalent information about T .
  Figure 3 Link: articels_figures_by_rev_year\2022\MultiTarget_Markov_Boundary_Discovery_Theory_Algorithm_and_Application\figure_3.jpg
  Figure 3 caption: An example of common MB variables and target-specific MB variables
    without multiple MBs.
  Figure 4 Link: articels_figures_by_rev_year\2022\MultiTarget_Markov_Boundary_Discovery_Theory_Algorithm_and_Application\figure_4.jpg
  Figure 4 caption: An example of common MB variables and target-specific MB variables
    with multiple MBs.
  Figure 5 Link: articels_figures_by_rev_year\2022\MultiTarget_Markov_Boundary_Discovery_Theory_Algorithm_and_Application\figure_5.jpg
  Figure 5 caption: A toy example to illustrate the efficiency of CTMB.
  Figure 6 Link: articels_figures_by_rev_year\2022\MultiTarget_Markov_Boundary_Discovery_Theory_Algorithm_and_Application\figure_6.jpg
  Figure 6 caption: The Hamming Loss , Ranking Loss , FMacro , and FMicro of CLFS
    and other state-of-the-art algorithms on six real-world data sets.
  Figure 7 Link: articels_figures_by_rev_year\2022\MultiTarget_Markov_Boundary_Discovery_Theory_Algorithm_and_Application\figure_7.jpg
  Figure 7 caption: Spider web diagrams showing the stability obtained on six multi-label
    data sets with Hamming Loss , Ranking Loss , FMacro , and FMicro of CLFS and other
    state-of-the-art multi-label feature selection algorithms.
  Figure 8 Link: articels_figures_by_rev_year\2022\MultiTarget_Markov_Boundary_Discovery_Theory_Algorithm_and_Application\figure_8.jpg
  Figure 8 caption: The graphical representation of the Tellegen-Watson-Clark model
    [46], where the six labels in Emotions data set are amazed-surprised, happy-pleased,
    relaxing-calm, quiet-still, sad-lonely, and angry-aggressive.
  Figure 9 Link: articels_figures_by_rev_year\2022\MultiTarget_Markov_Boundary_Discovery_Theory_Algorithm_and_Application\figure_9.jpg
  Figure 9 caption: 'The Identified relationship between each selected feature and
    each label in the Emotions data set. In the grid, each column corresponds to a
    feature selected by CLFS, and each row corresponds to a label ( L1 : amazed-surprised,
    L2 : happy-pleased, L3 : relaxing-calm, L4 : quiet-still, L5 : sad-lonely, L6
    : angry-aggressive). The shaded cell indicates that the corresponding feature
    affects the corresponding label.'
  First author gender probability: 0.79
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.64
  Name of the first author: Xingyu Wu
  Name of the last author: Huanhuan Chen
  Number of Figures: 9
  Number of Tables: 6
  Number of authors: 4
  Paper title: 'Multi-Target Markov Boundary Discovery: Theory, Algorithm, and Application'
  Publication Date: 2022-08-18 00:00:00
  Table 1 caption: TABLE 1 Experiment Parameters
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Average Precision and Recall of Searched Common and Target-Specific
    MB Variables With Respect to the Percentage of the Targets That Have Multiple
    MBs
  Table 3 caption: TABLE 3 Average Precision and Recall of Searched Common and Target-Specific
    MB Variables With Respect to the Percentage of the Targets That Have Direct Relationships
    With Each Other
  Table 4 caption: TABLE 4 Details of the Multi-Label Data Sets
  Table 5 caption: TABLE 5 Experiment Time ( lg(Time) lg(Time)) of Each Algorithm
  Table 6 caption: TABLE 6 Performance of CLFS and Comparing SHAP Methods
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3199784
- Affiliation of the first author: faculty of electronic information and electrical
    engineering, dalian university of technology, dalian, china
  Affiliation of the last author: department of computer science, city university
    of hong kong, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2022\LargeField_Contextual_Feature_Learning_for_Glass_Detection\figure_1.jpg
  Figure 1 caption: Problems with glass in existing vision tasks. In depth prediction,
    existing method [1] wrongly predicts the depth of the scene behind the glass,
    instead of the depth of the glass (1st row of (b)). For instance segmentation,
    Mask RCNN [2] only segments the instances behind the glass, not aware that they
    are actually behind the glass (2nd row of (b)). Besides, if we directly apply
    an existing singe-image reflection removal (SIRR) method [3] to an image that
    is only partially covered by glass, the non-glass region can be corrupted (3rd
    row of (b)). Our method can detect the glass (c) and then help correct these failure
    cases (d).
  Figure 10 Link: articels_figures_by_rev_year\2022\LargeField_Contextual_Feature_Learning_for_Glass_Detection\figure_10.jpg
  Figure 10 caption: More glass detection results of our GDNet-B on images beyond
    the GDD testing set.
  Figure 2 Link: articels_figures_by_rev_year\2022\LargeField_Contextual_Feature_Learning_for_Glass_Detection\figure_2.jpg
  Figure 2 caption: Example glass imagemask pairs in our glass detection dataset (GDD).
    It shows that GDD covers diverse types of glass in daily-life scenes.
  Figure 3 Link: articels_figures_by_rev_year\2022\LargeField_Contextual_Feature_Learning_for_Glass_Detection\figure_3.jpg
  Figure 3 caption: Statistics of our dataset. We show that GDD has glass with reasonable
    property distributions in terms of type, location and area.
  Figure 4 Link: articels_figures_by_rev_year\2022\LargeField_Contextual_Feature_Learning_for_Glass_Detection\figure_4.jpg
  Figure 4 caption: The pipeline of the proposed GDNet-B. First, we use the pre-trained
    ResNeXt-101 [75] as a multi-level feature extractor (MFE) to obtain features of
    different levels. Second, we embed four LCFI modules to the last four layers of
    MFE, to learn large-field contextual features at different levels. Third, the
    outputs of the last three LCFI modules are concatenated and fused via an attention
    module [76] to generate high-level large-field contextual features. An attention
    map is then learned from these high-level large-field contextual features and
    used to guide the low-level large-field contextual features, i.e., the output
    of the first LCFI module, to focus more on glass regions. Fourth, we apply two
    BFE modules on the high-levelattentive low-level large-field contextual features
    to further perceive and integrate boundary cues. Finally, we combine high-level
    and attentive low-level large-field contextual features by concatenation and attention
    [76] operations to produce the final glass detection map.
  Figure 5 Link: articels_figures_by_rev_year\2022\LargeField_Contextual_Feature_Learning_for_Glass_Detection\figure_5.jpg
  Figure 5 caption: The structure of the proposed LCFI module. The input features
    are passed through four parallel LCFI blocks, and the outputs of all LCFI blocks
    are fused to generate multi-scale large-field contextual features. In each LCFI
    block (red dashed box), input features are fed to two parallel spatially separable
    convolutions with opposite convolution orders to obtain large-field contextual
    features with different characteristics. The output of the current LCFI block
    is then fed to the next LCFI block to be further processed in a larger field.
  Figure 6 Link: articels_figures_by_rev_year\2022\LargeField_Contextual_Feature_Learning_for_Glass_Detection\figure_6.jpg
  Figure 6 caption: The structure of the proposed BFE module. The input features are
    passed through four parallel branches, and the outputs of all branches are fused
    to generate glass boundary features, which are then used to predict the glass
    boundary map and complement the input features.
  Figure 7 Link: articels_figures_by_rev_year\2022\LargeField_Contextual_Feature_Learning_for_Glass_Detection\figure_7.jpg
  Figure 7 caption: Ablation analysis on the effectiveness of the LCFI module.
  Figure 8 Link: articels_figures_by_rev_year\2022\LargeField_Contextual_Feature_Learning_for_Glass_Detection\figure_8.jpg
  Figure 8 caption: Visual comparison of GDNet-B to the state-of-the-art methods on
    the proposed GDD testing set.
  Figure 9 Link: articels_figures_by_rev_year\2022\LargeField_Contextual_Feature_Learning_for_Glass_Detection\figure_9.jpg
  Figure 9 caption: "Ablation analysis on the effectiveness of the BFE module. \u201C\
    HL map\u201D and \u201CLL map\u201D are the output boundary maps of the BFE module\
    \ applied on the high-level features and attentive low-level features in GDNet-B,\
    \ respectively."
  First author gender probability: 0.99
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Haiyang Mei
  Name of the last author: Rynson W. H. Lau
  Number of Figures: 15
  Number of Tables: 6
  Number of authors: 6
  Paper title: Large-Field Contextual Feature Learning for Glass Detection
  Publication Date: 2022-08-19 00:00:00
  Table 1 caption: TABLE 1 Quantitative Comparison to the State-of-the-art Methods
    on the GDD Testing Set
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Ablation Studies on the Effectiveness of the LCFI Module
  Table 3 caption: TABLE 3 Ablation Studies on the Effectiveness of the BFE Module
  Table 4 caption: TABLE 4 Ablation Studies on the Impact of Different Multi-Level
    Feature Extractors
  Table 5 caption: TABLE 5 Quantitative Comparison to State-of-the-art Methods on
    the MSD [9] Testing Set
  Table 6 caption: "TABLE 6 Comparison of the Proposed Method on the Saliency Detection\
    \ Task With 17 State-of-the-art Methods on Five Benchmark Datasets in Terms of\
    \ the Structure-Measure S \u03B1 S\u03B1 (Larger is Better), the Adaptive E-Measure\
    \ E a \u03D5 E\u03D5a (Larger is Better), the Weighted F-Measure F w \u03B2 F\u03B2\
    w (Larger is Better), and the Mean Absolute error M M (Smaller is Better)"
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3181973
- Affiliation of the first author: wangxuan institute of computer technology, peking
    university, beijing, china
  Affiliation of the last author: department of electrical and computer engineering,
    northwestern university, evanston, il, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\Discriminative_SelfPaced_GroupMetric_Adaptation_for_Online_Visual_Identification\figure_1.jpg
  Figure 1 caption: Taking P-RID task as an example, the normalized pair-wise distance
    distributions of both training and testing samples based on the well-trained state-of-the-art
    HA-CNN and MLFN networks on the Market1501 and DukeMTMC-reID datasets are presented.
    The results demonstrate the severe training-testing data distribution shifting
    issue, where the extremely challenging hard negative distractors (in blue box)
    will significantly influence the retrieval accuracy (the Original top-10 retrieval
    results). Even using the state-of-the-art online re-ranking method [8] (RR), the
    ground-truth (in red box) still has a lower rank than the distractors. Our method
    succeeds in handling the distractors so that the true-match is successfully re-ranked
    to the top position in the list (Ours).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Discriminative_SelfPaced_GroupMetric_Adaptation_for_Online_Visual_Identification\figure_2.jpg
  Figure 2 caption: "The online testing query and gallery samples are first fed into\
    \ the offline learned network to extract feature descriptors. The proposed frequent\
    \ sharing-subset (SSSet) mining algorithm is performed to generate multiple sharing-subsets\
    \ which are further utilized by the self-paced SSSet selection algorithm to iteratively\
    \ determine which SSSets will be involved for each learning round. The \u201C\
    easy\u201D SSSets will be processed first so that the model can be accordingly\
    \ trained from scratch. When the model can handle these \u201Ceasy\u201D SSSets\
    \ well, the \u201Charder\u201D ones will be gradually involved to further improve\
    \ the effectiveness of the model. Within each learning round, the selected SSSets\
    \ will be fed into the proposed online group-metric adaptation model for local\
    \ discriminant enhancement (The same sample can be contained by multiple SSSets\
    \ since it may share different visual similarity relationships with different\
    \ samples). Such learning continues until all the obtained SSSets are processed.\
    \ Finally, by fusing the learned group-metrics for each query and gallery sample,\
    \ our final ranking list is obtained by a bi-directional retrieval matching."
  Figure 3 Link: articels_figures_by_rev_year\2022\Discriminative_SelfPaced_GroupMetric_Adaptation_for_Online_Visual_Identification\figure_3.jpg
  Figure 3 caption: "The pipeline of our proposed unsupervised frequent sharing-subset\
    \ mining algorithm. Given the extracted features of testing samples, the affinity\
    \ matrix A can be computed. To keep only the most reliable sharing relationships,\
    \ a threshold \u0398 is used for filtering so that a binary index map B is obtained.\
    \ Then each non-zero row B j of B can be considered as an element t i in the set\
    \ T . Moreover, T is utilized to build a CFI-Tree that is the input for the off-the-shelf\
    \ FP-Close mining algorithm to mine all the frequent SSSets."
  Figure 4 Link: articels_figures_by_rev_year\2022\Discriminative_SelfPaced_GroupMetric_Adaptation_for_Online_Visual_Identification\figure_4.jpg
  Figure 4 caption: A CFI-Tree is constructed based on T . The same identity may be
    contained by multiple t i so that there may be multiple nodes for the same identity.
  Figure 5 Link: articels_figures_by_rev_year\2022\Discriminative_SelfPaced_GroupMetric_Adaptation_for_Online_Visual_Identification\figure_5.jpg
  Figure 5 caption: The visualization of rank improvement on Market1501 (a) and DukeMTMC-reID
    (b) based on the baseline [34]. For each case, its top-5 (left to right) matches
    are presented and the true-match is labeled by the red box.
  Figure 6 Link: articels_figures_by_rev_year\2022\Discriminative_SelfPaced_GroupMetric_Adaptation_for_Online_Visual_Identification\figure_6.jpg
  Figure 6 caption: "The influence of \u03BB on (top) CUHK03, (mid) Market1501 and\
    \ (bottom) DukeMTMC-reID based on HA-CNN."
  Figure 7 Link: articels_figures_by_rev_year\2022\Discriminative_SelfPaced_GroupMetric_Adaptation_for_Online_Visual_Identification\figure_7.jpg
  Figure 7 caption: The comparison of computational cost of OL and our SPGMA method
    on the Market1501, DukeMTMC-reID, and MSMT17 datasets based on the HA-CNN, MLFN
    and DenseNet121 baselines. The total number of learned online adaptation metrics
    are demonstrated.
  Figure 8 Link: articels_figures_by_rev_year\2022\Discriminative_SelfPaced_GroupMetric_Adaptation_for_Online_Visual_Identification\figure_8.jpg
  Figure 8 caption: The affinity matrix refinement by our SPGMA method on the Market1501
    dataset based on the HA-CNN, MLFN and DenseNet121 baselines.
  Figure 9 Link: articels_figures_by_rev_year\2022\Discriminative_SelfPaced_GroupMetric_Adaptation_for_Online_Visual_Identification\figure_9.jpg
  Figure 9 caption: The affinity matrix refinement by our SPGMA method on the DukeMTMC-reID
    datasets based on the HA-CNN, MLFN and DenseNet121 baselines.
  First author gender probability: 0.69
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.86
  Name of the first author: Jiahuan Zhou
  Name of the last author: Ying Wu
  Number of Figures: 9
  Number of Tables: 7
  Number of authors: 3
  Paper title: Discriminative Self-Paced Group-Metric Adaptation for Online Visual
    Identification
  Publication Date: 2022-08-19 00:00:00
  Table 1 caption: TABLE 1 The Statistics of P-RID Benchmarks
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Compared with the State-of-the-Art P-RID Methods on CUHK03,
    Market1501, and DukeMTMC-reID Datasets
  Table 3 caption: TABLE 3 Compared with the State-of-the-Arts on MSMT17
  Table 4 caption: TABLE 4 The Influence of Each Component in Our Algorithm
  Table 5 caption: TABLE 5 Compared with State-of-the-Art Online P-RID Re-Ranking
    Methods
  Table 6 caption: TABLE 6 Cross-Dataset Validation Results with Our Model on Market1501
    and DukeMTMC-reID
  Table 7 caption: TABLE 7 Comparison Results on Oxford, Paris, R ROxford and R RParis
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3200036
