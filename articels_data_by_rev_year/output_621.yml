- Affiliation of the first author: department of engineering, university of cambridge,
    cambridge, united kingdom
  Affiliation of the last author: cmla-ens cachan, cachan, france
  Figure 1 Link: articels_figures_by_rev_year\2016\Joint_A_Contrario_Ellipse_and_Line_Detection\figure_1.jpg
  Figure 1 caption: 'Left: region chaining. Middle and right: parameters defining
    the polygonal and the elliptical candidates.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Joint_A_Contrario_Ellipse_and_Line_Detection\figure_2.jpg
  Figure 2 caption: 'From left to right: original image, gradient orientations and
    seed pixel, region growing starting from the seed pixel. Far right: the final
    support region and the corresponding candidate rectangle.'
  Figure 3 Link: articels_figures_by_rev_year\2016\Joint_A_Contrario_Ellipse_and_Line_Detection\figure_3.jpg
  Figure 3 caption: "Left: Three pixels \u03B4 -aligned with an oriented line segment.\
    \ Centre: Pixels aligned with a polygon. Right: Aligned pixel in ellipse case."
  Figure 4 Link: articels_figures_by_rev_year\2016\Joint_A_Contrario_Ellipse_and_Line_Detection\figure_4.jpg
  Figure 4 caption: 'Examples of test images used to compare mathrmNFAc and mathrmNFAd
    . Left: A polygon with 6 sides with a circumscribed circle radius of 60 pixels;
    the noise variance is 0.1. Right: A polygon with 8 sides and a circle with radius
    of 40 pixels; the noise variance is 0.075.'
  Figure 5 Link: articels_figures_by_rev_year\2016\Joint_A_Contrario_Ellipse_and_Line_Detection\figure_5.jpg
  Figure 5 caption: 'Left: A nine-sides polygon with circumscribed circle of 25 pixels
    radius, and a circle with 25 pixels radius. This is an example where mathrmNFAc
    is not able to choose the correct polygonal interpretation. Right: An octagon
    and its circumscribed circle; all the pixels aligned with the polygon are also
    aligned with its circumscribed circle.'
  Figure 6 Link: articels_figures_by_rev_year\2016\Joint_A_Contrario_Ellipse_and_Line_Detection\figure_6.jpg
  Figure 6 caption: '1st row: samples of synthetic images, altered by Gaussian noise,
    included in Dataset 1. The noise level increases from left to right. The rightmost
    image contains pure Gaussian noise; 2nd row: ELSDc result (red: detected elliptical
    arcs); 3rd row: Prasad result (red: detected ellipses); 4th row: Etemadi result
    (bolded curve: detected circles, regular line: detected line segments); 5th row:
    Hough circle result (red: detected circles). Even if the images contain circles,
    ELSDc and Prasad report ellipses, which in most of the cases are very close to
    circles, as expected.'
  Figure 7 Link: articels_figures_by_rev_year\2016\Joint_A_Contrario_Ellipse_and_Line_Detection\figure_7.jpg
  Figure 7 caption: '1st row: sample pattern images [51], 2nd row: ELSDc result (red:
    detected ellipse arcs, blue: detected polygons), 3rd row: Prasad result (red:
    detected ellipses), 4th row: Hough ellipse result (red: detected ellipses).'
  Figure 8 Link: articels_figures_by_rev_year\2016\Joint_A_Contrario_Ellipse_and_Line_Detection\figure_8.jpg
  Figure 8 caption: Precision and Recall obtained by ELSDc, Prasad, Etemadi, and Hough
    circle detector, on synthetic images.
  Figure 9 Link: articels_figures_by_rev_year\2016\Joint_A_Contrario_Ellipse_and_Line_Detection\figure_9.jpg
  Figure 9 caption: "Results on natural images. 1st row: original image; 2nd row:\
    \ Canny edges \u2013 see text for parameters used; 3rd row ELSDc result (red:\
    \ detected ellipses, blue: detected polygons); 4th row: Prasad (red: detected\
    \ ellipses); 5th row: Etemadi (bold curves: detected circle arcs, regular lines:\
    \ detected line segments); 6th row: Hough circle detector (red: detected circles)."
  First author gender probability: 0.99
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "Viorica P\u0103tr\u0103ucean"
  Name of the last author: Rafael Grompone von Gioi
  Number of Figures: 9
  Number of Tables: 3
  Number of authors: 3
  Paper title: Joint A Contrario Ellipse and Line Detection
  Publication Date: 2016-04-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Success Rate in Primitive Validation Using the Two a Contrario
      Criteria NFAc and NFAd (See Text)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Success Rate in Model Selection When Using NFAc and NFAd .
      The True Primitive is a Polygon With Different Number of Sides and Different
      Circumscribed Circle Radius. The Competing Models are the Ground Truth Polygon
      and a Fitted Circle.
  Table 3 caption:
    table_text: TABLE 3 Evaluation of ELSDc, Prasad, and Hough Detectors on Calibration
      Pattern Images (See Text)
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2558150
- Affiliation of the first author: department of automation and beijing key laboratory
    of multi-dimension & multi-scale computational photography, tsinghua university,
    beijing, china
  Affiliation of the last author: tsinghua berkeley shenzhen institute, shenzhen,
    china
  Figure 1 Link: articels_figures_by_rev_year\2016\FrequencyDomain_Transient_Imaging\figure_1.jpg
  Figure 1 caption: Demonstration of a transient image. A transient image is a temporal
    sequence of images (a) which visualizes the propagation of light during an ultra-short
    time interval (b) (for example 4 ns), while a conventional image (c) is static.
  Figure 10 Link: articels_figures_by_rev_year\2016\FrequencyDomain_Transient_Imaging\figure_10.jpg
  Figure 10 caption: Reconstructed transient images from the synthetic data sets.
    Heide et al.'s result is from the 10 MHz data set. Note that our results have
    no artifacts (frame 180, 200, 220).
  Figure 2 Link: articels_figures_by_rev_year\2016\FrequencyDomain_Transient_Imaging\figure_2.jpg
  Figure 2 caption: Comparison of two types of transient imaging systems. (a) An ultrafast
    camera samples in the time domain. (b) A multi-frequency ToF camera samples in
    the frequency domain.
  Figure 3 Link: articels_figures_by_rev_year\2016\FrequencyDomain_Transient_Imaging\figure_3.jpg
  Figure 3 caption: "Operating principle of multi-frequency ToF cameras. The light\
    \ source s l (\u03C9t) and the sensor gain s s (\u03C9t+\u03D5) are modulated\
    \ by the same signal generator and locked at a programmable phase \u03D5 . The\
    \ incident light s r (\u03C9t) carries the transient image information of the\
    \ scene. The output image H is formed by multiplying the incident light by the\
    \ sensor gain and integrating over an exposure time."
  Figure 4 Link: articels_figures_by_rev_year\2016\FrequencyDomain_Transient_Imaging\figure_4.jpg
  Figure 4 caption: "Rectangular frequency window functions r L (\u03C4) and r H (\u03C4\
    ) (refer to (18))."
  Figure 5 Link: articels_figures_by_rev_year\2016\FrequencyDomain_Transient_Imaging\figure_5.jpg
  Figure 5 caption: 'The schematic diagram of transient imaging in the frequency domain.
    The process of transient imaging involves three steps: image acquisition, data
    rectification, and transient image reconstruction. The process of camera calibration
    yields the correlation function of a ToF camera for data rectification.'
  Figure 6 Link: articels_figures_by_rev_year\2016\FrequencyDomain_Transient_Imaging\figure_6.jpg
  Figure 6 caption: Data rectification at a pixel. The plot of the measured data is
    irregular (blue plot). The plot becomes sinusoid after extracting the fundamental
    component (green plot) and rectifying the amplitude and the phase (red plot).
  Figure 7 Link: articels_figures_by_rev_year\2016\FrequencyDomain_Transient_Imaging\figure_7.jpg
  Figure 7 caption: Demonstration of low frequency spectrum recovery using the proposed
    double iDFTs algorithm.
  Figure 8 Link: articels_figures_by_rev_year\2016\FrequencyDomain_Transient_Imaging\figure_8.jpg
  Figure 8 caption: 'Correlation matrices at a pixel. Top left: data measured in a
    calibration process. Top right: fitted values from parameterized correlation function.
    Bottom left: error between measured data and fitted values. Bottom right: comparison
    of measured data and fitted values.'
  Figure 9 Link: articels_figures_by_rev_year\2016\FrequencyDomain_Transient_Imaging\figure_9.jpg
  Figure 9 caption: 'Amplitude of the fundamental component and the harmonic components
    of a correlation function. Left: the amplitude of the fundamental component. Right:
    the relative amplitudes of the harmonic components. Even order harmonic components
    (solid plots) are very small due to symmetric light signals. Non-zero n -order
    harmonic components contains n times as high frequency information as the fundamental
    component.'
  First author gender probability: 0.64
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.76
  Name of the first author: Jingyu Lin
  Name of the last author: Qionghai Dai
  Number of Figures: 21
  Number of Tables: 2
  Number of authors: 4
  Paper title: Frequency-Domain Transient Imaging
  Publication Date: 2016-04-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Methods of Transient Imaging
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Computational Complexity of the Algorithms
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2560814
- Affiliation of the first author: school of information and communication engineering,
    dalian university of technology, china
  Affiliation of the last author: department of computing, the hong kong polytechnic
    university, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2016\Local_LogEuclidean_Multivariate_Gaussian_Descriptor_and_Its_Application_to_Image\figure_1.jpg
  Figure 1 caption: Overview of the local log-euclidean multivariate Gaussian (L 2
    EMG) descriptor. (a) Given an input image, we compute raw features and then extract
    Gaussians at a dense grid using these features; finally we embed the Gaussians
    in the linear space to obtain vectorized L 2 EMG descriptors. We show that the
    space of n -dimensional Gaussians N(n) can be equipped with a Lie group structure,
    equivalent to a subgroup, denoted by A + (n+1) , of the upper triangular matrix
    group. It can be directly embedded in a linear space A(n+1) by the matrix logarithm
    which is called DE-LogE, or indirectly by IE-LogE which first maps A + (n+1) into
    the space Sy m + (n+1) of SPD matrices and then into the linear space Sym(n+1)
    , as shown in (b) and (c), respectively. The embedding processes preserve the
    algebraic and topological structures. Consequently, we can handle Gaussians with
    Euclidean operations instead of Riemannian ones. See Section 4 for details.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Local_LogEuclidean_Multivariate_Gaussian_Descriptor_and_Its_Application_to_Image\figure_2.jpg
  Figure 2 caption: Image derivative operators or filters [54] used in our paper.
  Figure 3 Link: articels_figures_by_rev_year\2016\Local_LogEuclidean_Multivariate_Gaussian_Descriptor_and_Its_Application_to_Image\figure_3.jpg
  Figure 3 caption: Comparison of different embedding methods on VOC 2007.
  Figure 4 Link: articels_figures_by_rev_year\2016\Local_LogEuclidean_Multivariate_Gaussian_Descriptor_and_Its_Application_to_Image\figure_4.jpg
  Figure 4 caption: Impact of dimensionality reduction on L 2 EMG and L 2 ECM with
    FV on VOC 2007.
  Figure 5 Link: articels_figures_by_rev_year\2016\Local_LogEuclidean_Multivariate_Gaussian_Descriptor_and_Its_Application_to_Image\figure_5.jpg
  Figure 5 caption: Comparison of the distributions of dimension 30, 60 and 90 (from
    left to right) of L 2 EMG (Diag.) and L 2 EMG (Full) on VOC 2007.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.55
  Name of the first author: Peihua Li
  Name of the last author: Lei Zhang
  Number of Figures: 5
  Number of Tables: 6
  Number of authors: 4
  Paper title: Local Log-Euclidean Multivariate Gaussian Descriptor and Its Application
    to Image Classification
  Publication Date: 2016-04-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Analogy of Matrix-Valued and Function-Valued Image Representation
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Embedding Methods for Gaussian Distributions
  Table 3 caption:
    table_text: TABLE 3 Classification Accuracy (mAP, %) of L 2 EMG Versus Varying
      Combination of Raw Features on VOC 2007
  Table 4 caption:
    table_text: TABLE 4 Effect (in Terms of mAP, %) of Patch Size and Sampling Step
      on L 2 EMG and L 2 ECM on VOC2007
  Table 5 caption:
    table_text: TABLE 5 Classification Accuracy (mAP, %) of Competing Descriptors
      Using Various Coding Methods on VOC 2007
  Table 6 caption:
    table_text: TABLE 6 Comparison with State-of-the-Art Methods on a Variety of Benchmark
      Datasets
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2560816
- Affiliation of the first author: "instituto universitario para el desarrollo tecnol\xF3\
    gico y la innovaci\xF3n en comunicaciones, universidad de las palmas de gran canaria,\
    \ las palmas, spain"
  Affiliation of the last author: "laboratoire d'imagerie, de vision et d'intelligence\
    \ artificielle, universit\xE9 du qu\xE9bec, 1100, rue notre-dame ouest, room a-3600,\
    \ montr\xE9al, qc, canada"
  Figure 1 Link: articels_figures_by_rev_year\2016\Generation_of_Duplicated_OffLine_Signature_Images_for_Verification_Systems\figure_1.jpg
  Figure 1 caption: Hexagonal cell unit distortion with sinusoidal transformation.
    Original grid on the left and distorted one on the right.
  Figure 10 Link: articels_figures_by_rev_year\2016\Generation_of_Duplicated_OffLine_Signature_Images_for_Verification_Systems\figure_10.jpg
  Figure 10 caption: ROC plots training with the first five real signatures plus the
    duplicates for MCYT-75.
  Figure 2 Link: articels_figures_by_rev_year\2016\Generation_of_Duplicated_OffLine_Signature_Images_for_Verification_Systems\figure_2.jpg
  Figure 2 caption: General overview of the off-line signature duplicator.
  Figure 3 Link: articels_figures_by_rev_year\2016\Generation_of_Duplicated_OffLine_Signature_Images_for_Verification_Systems\figure_3.jpg
  Figure 3 caption: "Visual examples of intra-component variability in different repetitions\
    \ of a scanned image. Note how loops in letters \u201Cd\u201D and \u201Ca\u201D\
    \ are opened and closed without losing their original continuity."
  Figure 4 Link: articels_figures_by_rev_year\2016\Generation_of_Duplicated_OffLine_Signature_Images_for_Verification_Systems\figure_4.jpg
  Figure 4 caption: Visual examples of labeling in handwriting signatures. The non-connected
    components are highlighted by assigning different colors to each signature component.
  Figure 5 Link: articels_figures_by_rev_year\2016\Generation_of_Duplicated_OffLine_Signature_Images_for_Verification_Systems\figure_5.jpg
  Figure 5 caption: Cognitive inspired model to create a duplicated off-line signature
    using only one real off-line specimen.
  Figure 6 Link: articels_figures_by_rev_year\2016\Generation_of_Duplicated_OffLine_Signature_Images_for_Verification_Systems\figure_6.jpg
  Figure 6 caption: Procedure to evaluate the human-like variability of the duplicator.
    Following the novel scheme branch, performance in B is expected to be better than
    performance in A.
  Figure 7 Link: articels_figures_by_rev_year\2016\Generation_of_Duplicated_OffLine_Signature_Images_for_Verification_Systems\figure_7.jpg
  Figure 7 caption: 'At the top, from left to right the values used to illustrate
    the intra-component variability for the four signatures were respectively: alpha
    A = 30, 5, 5, 5 ; alpha P = 0.8, 0.8, 0.5, 0.5 and alpha S = 0, 0, 0, 0.5 . Similarly,
    at the bottom, from left to right, the values used to defined the inter-component
    variability were respectively, sigma x1 = 10, 0, 40, 40 and sigma y1 = 0, 4, 4,
    16 . Symbols checkmark and times indicate natural and unnatural writing styles
    respectively. (a) Intra-component. (b) Inter-component.'
  Figure 8 Link: articels_figures_by_rev_year\2016\Generation_of_Duplicated_OffLine_Signature_Images_for_Verification_Systems\figure_8.jpg
  Figure 8 caption: Examples of multiple signatures from only one original signature.
    The first column shows the original signature and the rest the duplicated samples.
    Some details in the signature variability are highlighted with gray spots.
  Figure 9 Link: articels_figures_by_rev_year\2016\Generation_of_Duplicated_OffLine_Signature_Images_for_Verification_Systems\figure_9.jpg
  Figure 9 caption: ROC plots training with the first five real signatures plus the
    duplicates for GPDS-300.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Moises Diaz
  Name of the last author: Robert Sabourin
  Number of Figures: 12
  Number of Tables: 6
  Number of authors: 4
  Paper title: Generation of Duplicated Off-Line Signature Images for Verification
    Systems
  Publication Date: 2016-04-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 State-of-the-Art on Duplicated Signature Generation
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Configuration of the Duplicator Parameters
  Table 3 caption:
    table_text: TABLE 3 Equal Error Rate (First) and Area Under Curve (Second) Results
      in % for the GPDS-300 Off-Line Signature DB for Four Validations
  Table 4 caption:
    table_text: TABLE 4 Equal Error Rate (First) and Area Under Curve (Second) Results
      in % for the MCYT-75 Off-Line Signature DB for Four Validations
  Table 5 caption:
    table_text: TABLE 5 Proportion of Beneficiary Users per Unit for GPDS-300 and
      MCYT-75 Databases
  Table 6 caption:
    table_text: TABLE 6 Equal Error Rate (First) and Area Under Curve (Second) Results
      in % for Comparison between Affine and Cognitive Duplication Methods for GPDS-300
      and MCYT-75 Databases
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2560810
- Affiliation of the first author: department of electronic engineering, the chinese
    university of hong kong, shatin, hong kong
  Affiliation of the last author: department of electronic engineering, the chinese
    university of hong kong, shatin, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2016\LRegularized_StationaryTime_Estimation_for_Crowd_Analysis\figure_1.jpg
  Figure 1 caption: Four major types of stationary group activities to be detection
    based on our proposed stationary-time estimation algorithm. (a) People join a
    group from different directions at different times. When all people arrive, the
    whole group moves to the same destination. (b) A group of people enters the view
    together, stay for a period of time, and leave together. (c) After staying at
    a place for a while, people move to another location and become stationary again.
    (d) People in a group have their own activities, taking photos for example.
  Figure 10 Link: articels_figures_by_rev_year\2016\LRegularized_StationaryTime_Estimation_for_Crowd_Analysis\figure_10.jpg
  Figure 10 caption: "Reconstruction results when adding noise to the whole synthetic\
    \ video. Only one x\u2212t plane of the inputoutput video is shown. Input frames\
    \ with different noise levels are in the first row. Corresponding reconstruction\
    \ results using average filtering, L 0 norm constraint of first order gradient,\
    \ L 1 norm constraint of second order gradient, and L 0 norm constraint of second\
    \ order gradient, are in the following four rows. Noise level ranges from 0 to\
    \ 0.5, shown in different columns. Black rectangles simulate foreground pedestrians\
    \ of different sizes."
  Figure 2 Link: articels_figures_by_rev_year\2016\LRegularized_StationaryTime_Estimation_for_Crowd_Analysis\figure_2.jpg
  Figure 2 caption: The emergence and dispersal of stationary groups might cause dynamic
    variations of traffic patterns. Stationary groups and main traffic flows are marked
    in red and blue.
  Figure 3 Link: articels_figures_by_rev_year\2016\LRegularized_StationaryTime_Estimation_for_Crowd_Analysis\figure_3.jpg
  Figure 3 caption: Averaged stationary-time distribution over 4 hours of a train
    station scene. Stationary groups tend to emerge and stay long around the information
    booth and in front of the ticketing windows.
  Figure 4 Link: articels_figures_by_rev_year\2016\LRegularized_StationaryTime_Estimation_for_Crowd_Analysis\figure_4.jpg
  Figure 4 caption: Estimating a 3D stationary-time map for a video sequence. Results
    from a few frames are shown. The period of time since each pixel has been stationary
    up to each frame is represented by the intensity level. Brighter pixels correspond
    to longer stationary-times.
  Figure 5 Link: articels_figures_by_rev_year\2016\LRegularized_StationaryTime_Estimation_for_Crowd_Analysis\figure_5.jpg
  Figure 5 caption: Challenges of stationary-time estimation. Three example cases
    show that results from background subtraction are erroneous. (a) Two foreground
    objects with spatio-temporal overlap. (b) Local movement of objects also leads
    to estimation errors. (c) If a foreground pixel is misclassified as background
    in one frame, stationary-time resets to 0 , which is wrong. In (c3), mis-classification
    happens in the middle, making time reset.
  Figure 6 Link: articels_figures_by_rev_year\2016\LRegularized_StationaryTime_Estimation_for_Crowd_Analysis\figure_6.jpg
  Figure 6 caption: Illustration of the foreground encoding that separates foreground
    objects close or overlapped in the spatio-temporal space. (a) 3 frames from the
    same region. After person B arrives, A leaves. (b) Temporal slice image along
    the yellow line, where A and B overlap. (c) Foreground pixels assigned with three
    different codewords. They are well separated. (d) Foreground codewords (colored)
    and estimated stationary-time (gray-scale) of input frames. The learned codebook
    D with M=3 are shown on the right. Each codeword d i is represented by one rectangle.
    The R, G, B color values are shown as the colors of the rectangles while the X,
    Y coordinates are illustrated as the locations of the rectangles.
  Figure 7 Link: articels_figures_by_rev_year\2016\LRegularized_StationaryTime_Estimation_for_Crowd_Analysis\figure_7.jpg
  Figure 7 caption: "By utilizing the sparse prior, we estimate \u03B1 better from\
    \ noisy andor locally moving objects. (a) Stages of two pedestrians arriving,\
    \ staying, locally moving, and leaving (horizontal-axis: time; vertical-axis:\
    \ scanline pixels highlighted by the orange dashed line). (b) Pixels with non-zero\
    \ \u2202 x,t values. (c) Pixels with non-zero \u2202 x + \u2202 t values. (d)\
    \ Our foreground encoding result with \u2202 x,t in (b). (e) Erroneous encoding\
    \ result with \u2202 x + \u2202 t in (c)."
  Figure 8 Link: articels_figures_by_rev_year\2016\LRegularized_StationaryTime_Estimation_for_Crowd_Analysis\figure_8.jpg
  Figure 8 caption: "Illustration of convergence of \u03B1 in one x\u2212t plane.\
    \ Initial estimation and following updates in different iterations are shown.\
    \ Noise is gradually removed."
  Figure 9 Link: articels_figures_by_rev_year\2016\LRegularized_StationaryTime_Estimation_for_Crowd_Analysis\figure_9.jpg
  Figure 9 caption: Annotated stationary-time maps on (a) the Grand Central dataset
    [47] and (b) the dataset collected by us.
  First author gender probability: 0.85
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Shuai Yi
  Name of the last author: Hongsheng Li
  Number of Figures: 19
  Number of Tables: 5
  Number of authors: 5
  Paper title: 'L

    0

    Regularized Stationary-Time Estimation for Crowd Analysis'
  Publication Date: 2016-04-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Details of Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results of Stationary-Time Estimation on Dataset I
  Table 3 caption:
    table_text: TABLE 3 Results of Stationary-Time Estimation on Dataset II
  Table 4 caption:
    table_text: TABLE 4 Results of Stationary-Time Estimation by Modifying Different
      Parameters
  Table 5 caption:
    table_text: TABLE 5 Activity Detection Results (False Positive Mis-Detection)
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2560807
- Affiliation of the first author: department of computer science, zhejiang university,
    hangzhou, china
  Affiliation of the last author: department of computer science, zhejiang university,
    hangzhou, china
  Figure 1 Link: articels_figures_by_rev_year\2016\A_D_Feature_Descriptor_Recovered_from_a_Single_D_Palmprint_Image\figure_1.jpg
  Figure 1 caption: The 2D palmprint image is essentially the projection of the palmprint
    surface. The direction of illumination L is always from the front side of the
    palmprint imaging.
  Figure 10 Link: articels_figures_by_rev_year\2016\A_D_Feature_Descriptor_Recovered_from_a_Single_D_Palmprint_Image\figure_10.jpg
  Figure 10 caption: The ROC curves from the CASIA palmprint database.
  Figure 2 Link: articels_figures_by_rev_year\2016\A_D_Feature_Descriptor_Recovered_from_a_Single_D_Palmprint_Image\figure_2.jpg
  Figure 2 caption: For a small region on palmprint, if we add the normal vector of
    each pixel point, the summation result will almost be vertical. This figure illuminates
    the summation of points on arbitrary palm line.
  Figure 3 Link: articels_figures_by_rev_year\2016\A_D_Feature_Descriptor_Recovered_from_a_Single_D_Palmprint_Image\figure_3.jpg
  Figure 3 caption: Different colors represent different subsets in a filter. In (a)-(c)
    the crossing in the filter increases from 1 to 3 while in (d)-(f) the directions
    of filters are varied when the crossing is fixed to 2.
  Figure 4 Link: articels_figures_by_rev_year\2016\A_D_Feature_Descriptor_Recovered_from_a_Single_D_Palmprint_Image\figure_4.jpg
  Figure 4 caption: The feature code of palmprint. (a) Original gray level image.
    (b) The feature code.
  Figure 5 Link: articels_figures_by_rev_year\2016\A_D_Feature_Descriptor_Recovered_from_a_Single_D_Palmprint_Image\figure_5.jpg
  Figure 5 caption: "(a) Typical matching distance distribution for different \u03BB\
    \ . (b) When p= 5 6 , the inter-class distance distribution and three intra-class\
    \ distance distribution with different \u03C9 ."
  Figure 6 Link: articels_figures_by_rev_year\2016\A_D_Feature_Descriptor_Recovered_from_a_Single_D_Palmprint_Image\figure_6.jpg
  Figure 6 caption: "(a) If \u03C9 1 =0.95 , the inter-class and intra-class distance\
    \ distribution with \u03BB=2 and \u03BB = 6 respectively. (b) If \u03C9 1 =0.98\
    \ , the inter-class and intra-class distance distribution with \u03BB = 2 and\
    \ \u03BB = 6 respectively."
  Figure 7 Link: articels_figures_by_rev_year\2016\A_D_Feature_Descriptor_Recovered_from_a_Single_D_Palmprint_Image\figure_7.jpg
  Figure 7 caption: (a) The ROC curves and (b) CMC curves of different methods from
    the PolyU 2D3D contactless palmprint database.
  Figure 8 Link: articels_figures_by_rev_year\2016\A_D_Feature_Descriptor_Recovered_from_a_Single_D_Palmprint_Image\figure_8.jpg
  Figure 8 caption: (a) The ROC curves and (b) CMC curves of different methods from
    the IITD palmprint database.
  Figure 9 Link: articels_figures_by_rev_year\2016\A_D_Feature_Descriptor_Recovered_from_a_Single_D_Palmprint_Image\figure_9.jpg
  Figure 9 caption: (a) The ROC curves and (b) CMC curves of different methods from
    the PolyU palmprint database.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.95
  Name of the first author: Qian Zheng
  Name of the last author: Gang Pan
  Number of Figures: 12
  Number of Tables: 6
  Number of authors: 3
  Paper title: A 3D Feature Descriptor Recovered from a Single 2D Palmprint Image
  Publication Date: 2016-05-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The EER and Rank-One Recognition Rate (Accuracy) of Different
      Methods from PolyU 2D3D Palmprint Database
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The EER and Rank-One Recognition Rate (Accuracy) of Different
      Methods from the IITD Palmprint Database
  Table 3 caption:
    table_text: TABLE 3 The EER and Rank-One Recognition Rate (Accuracy) of Different
      Methods from PolyU Palmprint Database
  Table 4 caption:
    table_text: TABLE 4 The EER(%) Comparison from CASIA Palmprint Database
  Table 5 caption:
    table_text: TABLE 5 Feature Extraction and Matching Time (ms) of Different Methods
  Table 6 caption:
    table_text: TABLE 6 Identification Rate Comparison with the State-of-the-Art Methods
      from the Extended Yale Face Database B
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2509968
- Affiliation of the first author: department of computer and information sciences,
    temple university, philadelphia, pa
  Affiliation of the last author: department of computer science and information systems,
    birkbeck college, london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2016\Salient_Object_Detection_via_Structured_Matrix_Decomposition\figure_1.jpg
  Figure 1 caption: Typical challenging examples for LR-based salient object detection
    algorithms. The resulting saliency maps of previous solutions (LRR [27], ULR [26]
    and SLR [28]) are scattered and incomplete, while our algorithm (SMD) overcomes
    these difficulties and performs close to the ground truth (GT).
  Figure 10 Link: articels_figures_by_rev_year\2016\Salient_Object_Detection_via_Structured_Matrix_Decomposition\figure_10.jpg
  Figure 10 caption: "Left three: Comparison of SMD with other LR-based methods (SLR\
    \ [28], ULR [26] and LRR [27]). The superscript ' \u2217 ' indicates methods without\
    \ using high-level priors. Rightmost: Comparison of rank distribution estimated\
    \ on the produced background feature matrices."
  Figure 2 Link: articels_figures_by_rev_year\2016\Salient_Object_Detection_via_Structured_Matrix_Decomposition\figure_2.jpg
  Figure 2 caption: 'Rank statistics of feature matrices extracted from image background
    over five datasets: MSRA10K [48], [69], DUT-OMRON [61], SOD [70], iCoSeg [71]
    and ECSSD [21].'
  Figure 3 Link: articels_figures_by_rev_year\2016\Salient_Object_Detection_via_Structured_Matrix_Decomposition\figure_3.jpg
  Figure 3 caption: "The construction of an index tree from an image. (A): The hierarchical\
    \ segmentation of an input image. The digits are the indexes of patches. (B):\
    \ An index tree constructed over the indices of image patches 1,2,\u2026,8 . Depth\
    \ 1 (Root): G 1 1 =1,2,3,4,5,6,7,8 . Depth 2: G 2 1 =1,2,3,4 , G 2 2 =5,6 , G\
    \ 2 3 =7,8 . Depth 3: G 3 1 =1,2 , G 3 2 =3,4 , G 3 3 =5 , G 3 4 =6 ."
  Figure 4 Link: articels_figures_by_rev_year\2016\Salient_Object_Detection_via_Structured_Matrix_Decomposition\figure_4.jpg
  Figure 4 caption: "The pairwise similarity matrices of feature vectors before and\
    \ after imposing Laplacian regularization. The upper-left block in the matrices\
    \ represents the similarities of foreground patches, while the bottom-right block\
    \ indicates the similarities of background patches. Matrices with subscript '\
    \ \u2217 ' are the results after imposing Laplacian regularization."
  Figure 5 Link: articels_figures_by_rev_year\2016\Salient_Object_Detection_via_Structured_Matrix_Decomposition\figure_5.jpg
  Figure 5 caption: Framework of the SMD model for salient object detection.
  Figure 6 Link: articels_figures_by_rev_year\2016\Salient_Object_Detection_via_Structured_Matrix_Decomposition\figure_6.jpg
  Figure 6 caption: Illustration of index tree construction based on the graph-based
    clustering. Each image indicates one layer in the index tree, while each patch
    represents one node.
  Figure 7 Link: articels_figures_by_rev_year\2016\Salient_Object_Detection_via_Structured_Matrix_Decomposition\figure_7.jpg
  Figure 7 caption: Quantitative comparison on five datasets in terms of PR and F
    -measure curves.
  Figure 8 Link: articels_figures_by_rev_year\2016\Salient_Object_Detection_via_Structured_Matrix_Decomposition\figure_8.jpg
  Figure 8 caption: Visual comparisons of saliency maps of the best methods. Our segmentation
    results (SMD-Seg), which are produced by simple adaptive thresholding on the saliency
    maps (SMD), are close to ground truth.
  Figure 9 Link: articels_figures_by_rev_year\2016\Salient_Object_Detection_via_Structured_Matrix_Decomposition\figure_9.jpg
  Figure 9 caption: 'Left and Middle: The evaluation of performance contribution of
    each component in our SMD model with respect to PR and ROC metrics. Right: The
    projection distance distribution of images in MSRA10K dataset before and after
    imposing the Laplacian regularization.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Houwen Peng
  Name of the last author: Stephen J. Maybank
  Number of Figures: 13
  Number of Tables: 4
  Number of authors: 6
  Paper title: Salient Object Detection via Structured Matrix Decomposition
  Publication Date: 2016-05-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of the Benchmark Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Summary of All Evaluated Detection Methods
  Table 3 caption:
    table_text: TABLE 3 Results on Five Datasets in Terms of WF, AUC, OR and MAE
  Table 4 caption:
    table_text: TABLE 4 The Objective Function of Different Models Related to SMD
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2562626
- Affiliation of the first author: "isit - cnrsuniversit\xE9 d'auvergne, clermont-ferrand,\
    \ france"
  Affiliation of the last author: "isit - cnrsuniversit\xE9 d'auvergne, clermont-ferrand,\
    \ france"
  Figure 1 Link: articels_figures_by_rev_year\2016\A_Stable_Analytical_Framework_for_Isometric_ShapefromTemplate_by_Surface_Integra\figure_1.jpg
  Figure 1 caption: Differential geometric modeling of Shape-from-Template.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\A_Stable_Analytical_Framework_for_Isometric_ShapefromTemplate_by_Surface_Integra\figure_2.jpg
  Figure 2 caption: Differential geometric modeling of Shape-from-Template with the
    locally isometric flattening. The new space depicted in the middle is locally
    isometric to the 3D template and the deformed surface. This property is required
    to construct the type-II SfT PDE.
  Figure 3 Link: articels_figures_by_rev_year\2016\A_Stable_Analytical_Framework_for_Isometric_ShapefromTemplate_by_Surface_Integra\figure_3.jpg
  Figure 3 caption: SfT type-I solutions (left) and type-II solutions (right) for
    different projection models and amount of perspective.
  Figure 4 Link: articels_figures_by_rev_year\2016\A_Stable_Analytical_Framework_for_Isometric_ShapefromTemplate_by_Surface_Integra\figure_4.jpg
  Figure 4 caption: Direct-depth method and stable type-I and type-II methods for
    SfT. The existing solution represents the results from the direct-depth method
    and the proposed solutions represent the results from the stable methods.
  Figure 5 Link: articels_figures_by_rev_year\2016\A_Stable_Analytical_Framework_for_Isometric_ShapefromTemplate_by_Surface_Integra\figure_5.jpg
  Figure 5 caption: Plots for the synthetic dataset. We show the depth errors in the
    first row and the normal errors in the second row.
  Figure 6 Link: articels_figures_by_rev_year\2016\A_Stable_Analytical_Framework_for_Isometric_ShapefromTemplate_by_Surface_Integra\figure_6.jpg
  Figure 6 caption: Plots for the KINECT Paper dataset and the Zooming dataset.
  Figure 7 Link: articels_figures_by_rev_year\2016\A_Stable_Analytical_Framework_for_Isometric_ShapefromTemplate_by_Surface_Integra\figure_7.jpg
  Figure 7 caption: Rendered 3D results with error coded texture maps for the Zooming
    dataset. We use the normal error to generate the texture maps.
  Figure 8 Link: articels_figures_by_rev_year\2016\A_Stable_Analytical_Framework_for_Isometric_ShapefromTemplate_by_Surface_Integra\figure_8.jpg
  Figure 8 caption: Rendered 3D results with error coded texture maps for the Cushion
    and the Can datasets.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.91
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: Ajad Chhatkuli
  Name of the last author: Toby Collins
  Number of Figures: 8
  Number of Tables: 2
  Number of authors: 4
  Paper title: A Stable Analytical Framework for Isometric Shape-from-Template by
    Surface Integration
  Publication Date: 2016-05-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Mean Depth Errors in Real Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Compared Methods and Their Characteristics
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2562622
- Affiliation of the first author: department of computer science, university of texas
    at austin, texas, tx
  Affiliation of the last author: department of computer science, university of texas
    at austin, texas, tx
  Figure 1 Link: articels_figures_by_rev_year\2016\Efficient_Activity_Detection_in_Untrimmed_Video_with_MaxSubgraph_Search\figure_1.jpg
  Figure 1 caption: Our approach constructs a space-time video graph, and efficiently
    finds the subgraph that maximizes an activity classifier's score. The detection
    result can take on non-cubic shapes (see dotted shapes in top frames), as demanded
    by the action.
  Figure 10 Link: articels_figures_by_rev_year\2016\Efficient_Activity_Detection_in_Untrimmed_Video_with_MaxSubgraph_Search\figure_10.jpg
  Figure 10 caption: Accuracy versus computation time in temporal search. We compare
    our T-Subgraph (which produces the optimal detection output for a fixed time)
    to the standard T-Sliding method (which produces its detection output based on
    exhaustive search of a pool of candidate windows). Here we increase T-Sliding's
    accuracy and run-time by increasing that pool of windows.
  Figure 2 Link: articels_figures_by_rev_year\2016\Efficient_Activity_Detection_in_Untrimmed_Video_with_MaxSubgraph_Search\figure_2.jpg
  Figure 2 caption: "Overview and limitations of standard approaches to activity detection.\
    \ (a) A sliding window search is most commonly employed, but it is computationally\
    \ expensive to search all possible windows. Furthermore, searching only in the\
    \ temporal dimension can mislead a classifier when there is substantial clutter\
    \ or irrelevant features in the scene, i.e., the objects relevant to the activity\
    \ occupy a small portion of the frame. (b) Efficient subvolume search techniques\
    \ can greatly improve efficiency and generalize sliding windows to restricted\
    \ spatial areas, yet existing methods are limited to cubic subvolumes. This creates\
    \ problems with the subject of the activity moves within the frame. (c) A general\
    \ alternative to sliding window search is to detect activities based on tracked\
    \ people (or other objects). This lets the system follow the region of interest\
    \ as it moves over time and works well for \u201Cperson-centric\u201D activities\
    \ like waving or jumping. However, the tracking-based approach risks losing important\
    \ contextual information about an activity (e.g., as depicted here, the activity\
    \ \u201Cget into car\u201D demands a representation of surrounding objects, not\
    \ just the tracked person)."
  Figure 3 Link: articels_figures_by_rev_year\2016\Efficient_Activity_Detection_in_Untrimmed_Video_with_MaxSubgraph_Search\figure_3.jpg
  Figure 3 caption: Pipeline of our max-subgraph search approach to activity detection.
    To detect an action in a video, we first divide the test video into a 3D array
    of spatio-temporal volumes. These are the graph nodes, weighted by their respective
    features' classifier scores (Section 3.1). The edges in the graph connect nodes
    that are adjacent in space-time (Section 3.3), with optional additional connectivity
    between more distant nodes in order to tolerate occlusions or noisy features (i.e.,
    as defined for the T-Jump variant). Then we search for the maximum weighted subgraph
    using an efficient branch-and-cut solution ( Section 3.4). Finally, the resulting
    subgraph yields the subset of connected space-time nodes where the action most
    likely appears. This is our detection result, as shown backprojected into the
    original video in the rightmost image above.
  Figure 4 Link: articels_figures_by_rev_year\2016\Efficient_Activity_Detection_in_Untrimmed_Video_with_MaxSubgraph_Search\figure_4.jpg
  Figure 4 caption: Schematic of the data comprising our high-level descriptors. After
    detecting people and other objects in the video frames, we form semi-local neighborhoods
    around each detected object that summarize the space-time layout of other nearby
    detections. To map those neighborhoods into discrete and discriminative visual
    words, we apply a random forest trained for the action labels (Section 3.2.2).
    Here, the left images depict the detected objects surrounding the person detected
    in bounding box C in the center frame. The right text box displays the information
    exposed to the random forest feature quantizer, in terms of the neighboring detections
    and their relative spatial and temporal distance from that person box C.
  Figure 5 Link: articels_figures_by_rev_year\2016\Efficient_Activity_Detection_in_Untrimmed_Video_with_MaxSubgraph_Search\figure_5.jpg
  Figure 5 caption: The two node structures we consider. (a) A temporal only graph
    simply breaks the video into slabs of frames. Max subgraph search on this graph
    is equivalent to sliding window in terms of results, but is faster. (b) Spatio-temporal
    graphs further break the frames into spatial cubes, allowing both spatial and
    temporal localization of the activity in irregular subvolume shapes, at the cost
    of a denser input graph.
  Figure 6 Link: articels_figures_by_rev_year\2016\Efficient_Activity_Detection_in_Untrimmed_Video_with_MaxSubgraph_Search\figure_6.jpg
  Figure 6 caption: "The two linking strategies we consider. (a) The neighbors only\
    \ graph links temporally adjacent (shown here) and optionally spatially adjacent\
    \ (not shown) nodes. (b) The temporal \u201Cjump\u201D linking strategy also incorporates\
    \ edges between non-adjacent nodes, so that the output detection can realize a\
    \ good connected detection result in spite of intermittent noisyocclusion features\
    \ on certain nodes. Here, the numbers shown on nodes indicate weights; white nodes\
    \ indicate those that would be selected under either linking strategy (see text)."
  Figure 7 Link: articels_figures_by_rev_year\2016\Efficient_Activity_Detection_in_Untrimmed_Video_with_MaxSubgraph_Search\figure_7.jpg
  Figure 7 caption: Our two stage subgraph search approximates the ST-Subgraph search,
    allowing efficient spatio-temporal detection even with long test sequences. First
    we extract the standard space-time cuboid nodes (left). Then, we generate a series
    of simpler graphs in time (stage 1, top right), and solve for the maximum connected
    subgraph in each one. This yields a detection region and score for each simpler
    graph. Finally, we create a graph based on temporal nodes only, which are weighted
    by the output scores of the previous stage (stage 2, bottom right). The nodes
    selected in both stages serve as the final output. Best viewed in color.
  Figure 8 Link: articels_figures_by_rev_year\2016\Efficient_Activity_Detection_in_Untrimmed_Video_with_MaxSubgraph_Search\figure_8.jpg
  Figure 8 caption: "Sketch of the candidate subvolume types considered by different\
    \ methods, ordered approximately from least to most flexible. T-Sliding or T-Subgraph:\
    \ The status quo sliding window search (and the proposed T-Subgraph without jumps)\
    \ finds the full-frame subvolume believed to contain the activity (leftmost image).\
    \ ST-Cube-Sliding: A variant that performs sliding window on different spatial\
    \ portions of the frame, with the restriction of cuboid subvolumes. ST-Cube-Subvolume:\
    \ A branch-and-bound search strategy from existing work [8] that considers all\
    \ possible cube-shaped subvolumes\u2014not just the grid-based subset considered\
    \ by ST-Cube-Sliding. T-Jump-Subgraph: The proposed method using temporal nodes\
    \ (slabs of frames) only, with additional allowance of temporal \u201Cgap(s)\u201D\
    \ in the output detections. ST-Subgraph The most general form of the proposed\
    \ method, where we use both spatial and temporal nodes, allowing irregular, non-cubic\
    \ detection results."
  Figure 9 Link: articels_figures_by_rev_year\2016\Efficient_Activity_Detection_in_Untrimmed_Video_with_MaxSubgraph_Search\figure_9.jpg
  Figure 9 caption: Qualitative example showing how our T-Jump method can perform
    robust detection. The five colored cubes represent the weighted node computed
    from the extracted features and learned classifier. For the second to fourth nodes,
    the classifier generates negative weights due to the occlusion. Using T-Sliding
    or T-Subgraph, the detection output does not cover the first and last cubes due
    to the negative weights from three cubes in the middle. In contrast, using our
    T-Jump method, it can skip over the intervening negative weights. This makes the
    detection framework more robust to noise from occlusion. Best viewed in color.
  First author gender probability: 0.86
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Chao Yeh Chen
  Name of the last author: Kristen Grauman
  Number of Figures: 12
  Number of Tables: 13
  Number of authors: 2
  Paper title: Efficient Activity Detection in Untrimmed Video with Max-Subgraph Search
  Publication Date: 2016-05-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Properties of the Four Datasets
  Table 10 caption:
    table_text: TABLE 10 Mean Temporal Overlap Accuracy on the MSR Dataset
  Table 2 caption:
    table_text: TABLE 2 Mean Overlap Accuracy for the UCF Sports Data
  Table 3 caption:
    table_text: TABLE 3 Search Time for the UCF Sports Data
  Table 4 caption:
    table_text: TABLE 4 Mean Overlap Accuracy on Uncropped Hollywood Data
  Table 5 caption:
    table_text: TABLE 5 Search Time on Uncropped Hollywood Data
  Table 6 caption:
    table_text: TABLE 6 Recognition Accuracy on Hollywood as Test Input Varies
  Table 7 caption:
    table_text: TABLE 7 Mean Overlap Accuracy on Hollywood for Low-Level Features
      versus the Object-Based High-Level Descriptors
  Table 8 caption:
    table_text: TABLE 8 Recognition Accuracyc on THUMOS 2014 Data
  Table 9 caption:
    table_text: TABLE 9 Search Time on THUMOS 2014 Data
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2564404
- Affiliation of the first author: school of arts, media, & engineering and department
    of electrical, computer, and energy engineering, arizona state university, tempe,
    az
  Affiliation of the last author: department of statistics, florida state university,
    tallahasse, fl
  Figure 1 Link: articels_figures_by_rev_year\2016\Elastic_Functional_Coding_of_Riemannian_Trajectories\figure_1.jpg
  Figure 1 caption: "Row wise from top \u2013 S 1 , S 2 , Warped action S 2 ~ , Warped\
    \ mean, Unwarped mean. The TSRVF can enable more accurate estimation of statistical\
    \ quantities such as average of two actions S 1 , S 2 ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Elastic_Functional_Coding_of_Riemannian_Trajectories\figure_2.jpg
  Figure 2 caption: Dimensionality reduction for Riemannian trajectories.
  Figure 3 Link: articels_figures_by_rev_year\2016\Elastic_Functional_Coding_of_Riemannian_Trajectories\figure_3.jpg
  Figure 3 caption: Representing the warped trajectories on a manifold as a vector
    field, allows us to use existing algorithms to perform dimensionality reduction
    efficiently, while also respecting the geometric and temporal constraints.
  Figure 4 Link: articels_figures_by_rev_year\2016\Elastic_Functional_Coding_of_Riemannian_Trajectories\figure_4.jpg
  Figure 4 caption: Eigenvalue decay for MSRActions3D [41], UTKinect [42], and Florence3D
    [43] datasets obtained with RF-PCA. UTKinect and Florence3D have 10 and 9 different
    classes respectively, as a result the corresponding eigenvalue decay saturates
    at around 10 dimensions. MSRActions consists of 20 classes and the decay saturates
    later at around 20 .
  Figure 5 Link: articels_figures_by_rev_year\2016\Elastic_Functional_Coding_of_Riemannian_Trajectories\figure_5.jpg
  Figure 5 caption: The stroke rehabilitation system [29], that uses a 14 marker configuration
    to provide feedback on motor function for stroke patients. A typical evaluation
    protocol requires a therapist to observe a specified movement to give a score
    indicating the quality of movement.
  Figure 6 Link: articels_figures_by_rev_year\2016\Elastic_Functional_Coding_of_Riemannian_Trajectories\figure_6.jpg
  Figure 6 caption: The RF-PCA is able to accurately predict movement quality as compared
    to an expert therapist which can improve home-based systems for stroke rehabilitation.
  Figure 7 Link: articels_figures_by_rev_year\2016\Elastic_Functional_Coding_of_Riemannian_Trajectories\figure_7.jpg
  Figure 7 caption: "Diverse action sampling using Precis [50] by sampling in RF-PCA\
    \ space in mathbb R10 on a highly skewed dataset. K-medoids picks more samples\
    \ (marked) from classes that have a higher representation, while Precis remains\
    \ invariant to it. The K-medoids and diverse clustering operations are performed\
    \ sim 500 times faster in the RF-PCA space. Fig. 7c shows a 2D axis sampled in\
    \ the latent space. It's clearly seen that even in only 2 dimensions, some action\
    \ information (\u201Cstyle\u201D) is discernible."
  Figure 8 Link: articels_figures_by_rev_year\2016\Elastic_Functional_Coding_of_Riemannian_Trajectories\figure_8.jpg
  Figure 8 caption: Robustness experiments for different factors as measured by their
    effect on recognition accuracy. Experiments in Fig. 8a and Fig. 8b are performed
    on the Grassmann manifold, & Fig. 8c shows results on the SE(3) times SE(3) cdots
    SE(3) manifold. It can be clearly seen that the RFPCA representation affords significant
    robustness in the presence of noise, and remains more robust to different sampling
    rates than unwarped trajectories.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Rushil Anirudh
  Name of the last author: Anuj Srivastava
  Number of Figures: 8
  Number of Tables: 4
  Number of authors: 4
  Paper title: Elastic Functional Coding of Riemannian Trajectories
  Publication Date: 2016-05-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Recognition Performance on the Florence3D Actions Dataset
      [43] for Different Feature Spaces
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Recognition Performance on the UTKinect Actions Dataset [42]
  Table 3 caption:
    table_text: TABLE 3 Recognition Performance on the MSRActions3D Dataset [41] Following
      the Protocol of [44] by Testing on 20 Classes, with All Possible Combinations
      of Test Train Subjects
  Table 4 caption:
    table_text: TABLE 4 Visual Speech Recognition Performance on the OuluVS Database
      [47] on 1,000 Videos Using the Subject Dependent Testing (SDT)
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2564409
