- Affiliation of the first author: department of computer, control, and management
    engineering, sapienza university of rome, rome, italy
  Affiliation of the last author: fondazione bruno kessler, trento, italy
  Figure 1 Link: articels_figures_by_rev_year\2019\Inferring_Latent_Domains_for_Unsupervised_Deep_Domain_Adaptation\figure_1.jpg
  Figure 1 caption: The idea behind the proposed framework. We introduce a novel deep
    architecture which, given a set of images, automatically discover multiple latent
    domains and use this information to align the distributions of the internal CNN
    feature representations of sources and target domains for the purpose of domain
    adaptation. In this way, more accurate target classifiers can be learned. Image
    better seen at magnification.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Inferring_Latent_Domains_for_Unsupervised_Deep_Domain_Adaptation\figure_2.jpg
  Figure 2 caption: Schematic representation of our method applied to the AlexNet
    architecture (left) and of an mDA-layer (right).
  Figure 3 Link: articels_figures_by_rev_year\2019\Inferring_Latent_Domains_for_Unsupervised_Deep_Domain_Adaptation\figure_3.jpg
  Figure 3 caption: Distribution of the assignments produced by the domain prediction
    branch for each latent domain in all possible settings of the PACS dataset. Different
    colors denote different source domains.
  Figure 4 Link: articels_figures_by_rev_year\2019\Inferring_Latent_Domains_for_Unsupervised_Deep_Domain_Adaptation\figure_4.jpg
  Figure 4 caption: Top-6 images associated to each latent domain for the different
    sourcestarget combinations. Each row corresponds to a different latent domain.
  Figure 5 Link: articels_figures_by_rev_year\2019\Inferring_Latent_Domains_for_Unsupervised_Deep_Domain_Adaptation\figure_5.jpg
  Figure 5 caption: 'Distribution of the assignments produced by the domain prediction
    branch in all possible multi-target settings of the PACS dataset. Different colors
    denote different source domains (red: Art, yellow: Cartoon, blue: Photo, green:
    Sketch).'
  Figure 6 Link: articels_figures_by_rev_year\2019\Inferring_Latent_Domains_for_Unsupervised_Deep_Domain_Adaptation\figure_6.jpg
  Figure 6 caption: 'Distribution of the assignments produced by the domain prediction
    branch trained with the additional constraint on the entropy loss in all possible
    multi-target settings of the PACS dataset. Different colors denote different source
    domains (red: Art, yellow: Cartoon, blue: Photo, green: Sketch).'
  Figure 7 Link: articels_figures_by_rev_year\2019\Inferring_Latent_Domains_for_Unsupervised_Deep_Domain_Adaptation\figure_7.jpg
  Figure 7 caption: 'Distribution of the assignments produced by the domain prediction
    branch for each latent domain in all target settings of the Digits-five dataset.
    Different colors denote different source domains (black: MNIST, blue: MNIST-m,
    green: USPS, red: SVHN, yellow: Synthetic numbers).'
  Figure 8 Link: articels_figures_by_rev_year\2019\Inferring_Latent_Domains_for_Unsupervised_Deep_Domain_Adaptation\figure_8.jpg
  Figure 8 caption: Office31 dataset. Performance at varying number of domain labels
    ( % ) for source samples.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Massimiliano Mancini
  Name of the last author: Elisa Ricci
  Number of Figures: 8
  Number of Tables: 7
  Number of authors: 5
  Paper title: Inferring Latent Domains for Unsupervised Deep Domain Adaptation
  Publication Date: 2019-08-08 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Digits Datasets: Comparison of Different Models in the Multi-Source
      Scenario'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Digits-Five [18] Setting, Comparison of Different Single Source
      and Multi-Source DA Models
  Table 3 caption:
    table_text: 'TABLE 3 PACS Dataset: Comparison of Different Methods Using the ResNet
      Architecture'
  Table 4 caption:
    table_text: 'TABLE 4 PACS Dataset: Comparison of Different Methods Using the ResNet
      Architecture on the Multi-Source Multi-Target Setting'
  Table 5 caption:
    table_text: 'TABLE 5 Office-31 Dataset: Comparison of Different Methods Using
      AlexNet'
  Table 6 caption:
    table_text: 'TABLE 6 Office-31: Comparison with State-of-the-Art Algorithms'
  Table 7 caption:
    table_text: 'TABLE 7 Office-Caltech Dataset: Comparison with State-of-the-Art
      Algorithms'
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2933829
- Affiliation of the first author: faculty of engineering and information technologies,
    ubtech sydney artificial intelligence centre and school of computer science, university
    of sydney, camperdown, nsw, australia
  Affiliation of the last author: faculty of engineering and information technologies,
    ubtech sydney artificial intelligence centre and school of computer science, university
    of sydney, camperdown, nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2019\Matching_Seqlets_An_Unsupervised_Approach_for_Locality_Preserving_Sequence_Match\figure_1.jpg
  Figure 1 caption: "Matching two human action sequences using (a) individual frames\
    \ and (b) seqlets. A seqlet is denoted as a bounding box enclosing skeletons.\
    \ The upper row depicts a \u201Cpick up\u201D action while the lower depicts a\
    \ \u201Cwalking\u201D. They belong to different classes and therefore fewer matchings,\
    \ optimally zero, are desirable. Frame-to-frame approach in this case produces\
    \ three matchings while our seqlet-based one produces none."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Matching_Seqlets_An_Unsupervised_Approach_for_Locality_Preserving_Sequence_Match\figure_2.jpg
  Figure 2 caption: Examples of SBM matching. (a) Matching two happy expression sequences.
    (b) Matching two similar Chinese characters. The left parts of the two characters
    are different but the right parts are same. SBM automatically groups the strokes
    of the right part and match them.
  Figure 3 Link: articels_figures_by_rev_year\2019\Matching_Seqlets_An_Unsupervised_Approach_for_Locality_Preserving_Sequence_Match\figure_3.jpg
  Figure 3 caption: Illustration of our matching process. We show two action sequences
    that comprise seven and nine frames respectively. We use a black bounding box
    to denote a seqlet, and a green box to denote a merged one. Each selection edge
    h ij is colored blue, and each matching edge s ij is colored yellow. Note that,
    for a more concise presentation, we show only a pair of matching edges. A green
    skeleton denotes a keyframe. Also, we highlight a selected h ij using red and
    a selected s ij using black. By the end of the first iteration, Seqlets 8 and
    6 are selected for sequence 1, and Seqlets 12, 9, and 10 are selected for sequence
    2. Meanwhile, Seqlet 8 of sequence 1 is matched with Seqlet 12 of sequence 2.
    In Iteration 2, we update the keyframes and conduct the joint clustering-matching
    again. This process is repeated until convergence.
  Figure 4 Link: articels_figures_by_rev_year\2019\Matching_Seqlets_An_Unsupervised_Approach_for_Locality_Preserving_Sequence_Match\figure_4.jpg
  Figure 4 caption: Comparative results on the MSR-Action3D, MSRDailyActivity3D, SAD,
    Cohn-Kanade and Similar Online Chinese Character dataset. NM, 1-NN, 3-NN, and
    5-NN denote the results evaluated using classification accuracy, while NMMAP and
    MAP denote the NM and 1-NN results using MAP, respectively.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.61
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jiayan Qiu
  Name of the last author: Dacheng Tao
  Number of Figures: 4
  Number of Tables: 2
  Number of authors: 4
  Paper title: 'Matching Seqlets: An Unsupervised Approach for Locality Preserving
    Sequence Matching'
  Publication Date: 2019-08-14 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 The Influence of \u03B1 1 \u03B11, \u03B1 2 \u03B12, \u03B1\
      \ 3 \u03B13 and ( \u03B2 1 , \u03B2 2 \u03B21,\u03B22) on the MSR-Action3D Dataset"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Influences of the Terms in SBM
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2934052
- Affiliation of the first author: electrical and electronics engineering, eskisehir
    osmangazi university, eskisehir, turkey
  Affiliation of the last author: electrical and electronics engineering, eskisehir
    osmangazi university, eskisehir, turkey
  Figure 1 Link: articels_figures_by_rev_year\2019\Polyhedral_Conic_Classifiers_for_Computer_Vision_Applications_and_Open_Set_Recog\figure_1.jpg
  Figure 1 caption: "A decision hyperplane returned by an SVM successfully separates\
    \ its training classes, dogs (positive) and people (negative). However it also\
    \ assigns instances of new classes such as cats, horses, fish and chairs to the\
    \ dog class, sometimes with higher confidence scores than for dogs themselves.\
    \ The problem is the too large acceptance region \u2013 SVM only tries to separate\
    \ dogs and people, not to bound the dog class. A tighter (e.g., polyhedral or\
    \ ellipsoidal) decision boundary improves classification, reducing mis-classifications\
    \ caused by unforeseen classes and outliers."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Polyhedral_Conic_Classifiers_for_Computer_Vision_Applications_and_Open_Set_Recog\figure_2.jpg
  Figure 2 caption: 'Geometric interpretation of four separation types: linear, h
    -polyhedral, polyhedral conic and max-min separation.'
  Figure 3 Link: articels_figures_by_rev_year\2019\Polyhedral_Conic_Classifiers_for_Computer_Vision_Applications_and_Open_Set_Recog\figure_3.jpg
  Figure 3 caption: "Visualization of PCC classifiers for 2D synthetic data: The positive\
    \ acceptance regions are \u201Dkite-like\u201D octahedroids containing the points\
    \ for which a linear hyperplane lies above an L 1 cone.(a): 2D positive (yellow)\
    \ and negative (blue) samples, (b)-(e): views of positive-class acceptance regions\
    \ from different angles in 3D, (f): Resulting kite-like acceptance region in 2D\
    \ space."
  Figure 4 Link: articels_figures_by_rev_year\2019\Polyhedral_Conic_Classifiers_for_Computer_Vision_Applications_and_Open_Set_Recog\figure_4.jpg
  Figure 4 caption: 2D synthetic data and the decision boundaries returned by the
    proposed classifiers. Brighter pixels in decision boundaries correspond to higher
    scores.
  Figure 5 Link: articels_figures_by_rev_year\2019\Polyhedral_Conic_Classifiers_for_Computer_Vision_Applications_and_Open_Set_Recog\figure_5.jpg
  Figure 5 caption: 'Visualization of the convergence of the cone vertex point estimation
    algorithm: Starting from (0,0), the method converges to the optimal point in 6
    iterations.'
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Hakan Cevikalp
  Name of the last author: Halil Saglamlar
  Number of Figures: 5
  Number of Tables: 14
  Number of authors: 2
  Paper title: Polyhedral Conic Classifiers for Computer Vision Applications and Open
    Set Recognition
  Publication Date: 2019-08-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average Precision (%) on the Synthetic Dataset
  Table 10 caption:
    table_text: TABLE 10 Datasets from the UCI Repository
  Table 2 caption:
    table_text: TABLE 2 Average Precision (%) for Various Face Detectors on the FDDB
      and ESOGU Faces Datasets
  Table 3 caption:
    table_text: TABLE 3 Average Precision (%) on the INRIA Person Dataset
  Table 4 caption:
    table_text: TABLE 4 Object Detection Results (%) on MS COCO Dataset
  Table 5 caption:
    table_text: TABLE 5 Results for PaSC Face Verification Experiments
  Table 6 caption:
    table_text: TABLE 6 Average Precision Scores (%) on PASCAL VOC 2007 Classification
      Datasets
  Table 7 caption:
    table_text: TABLE 7 Classification Rates (%) on the CIFAR-10 Dataset
  Table 8 caption:
    table_text: TABLE 8 Classification Rates (%) for the Multi-Class Visual Object
      Classification Experiments
  Table 9 caption:
    table_text: TABLE 9 Accuracies on the MS-COCO Dataset on 80 Classes for Top-3
      Highest Ranked Labels
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2934455
- Affiliation of the first author: hikvision research institute, santa clara, ca,
    usa
  Affiliation of the last author: department of statistics, university of california,
    los angeles, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Learning_EnergyBased_SpatialTemporal_Generative_ConvNets_for_Dynamic_Patterns\figure_1.jpg
  Figure 1 caption: "Synthesizing dynamic textures with both spatial and temporal\
    \ stationarity. For each category, the first row displays the frames of the observed\
    \ sequence, and the second and third rows display the corresponding frames of\
    \ two synthesized sequences generated by the learning algorithm. The observed\
    \ and the synthesized videos are of the size 224 \xD7 224 pixels \xD7 50 or 70\
    \ frames."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Learning_EnergyBased_SpatialTemporal_Generative_ConvNets_for_Dynamic_Patterns\figure_2.jpg
  Figure 2 caption: "Synthesizing dynamic textures with only temporal stationarity.\
    \ For each category, the first row displays 6 frames of the observed sequence,\
    \ and the second and third rows display the corresponding frames of two synthesized\
    \ sequences generated by the learning algorithm. The observed and the synthesized\
    \ videos are of the size 224 \xD7 224 pixels \xD7 70 frames. (a) Flashing lights.\
    \ (b) Water spray. (c) Fountain. (d) Spring water. (e) Burning fire heating a\
    \ pot. (f) Burning fire in a stove. (g) Waterfall in a mountain. (h) Water spray\
    \ in a fountain."
  Figure 3 Link: articels_figures_by_rev_year\2019\Learning_EnergyBased_SpatialTemporal_Generative_ConvNets_for_Dynamic_Patterns\figure_3.jpg
  Figure 3 caption: 'Comparison on synthesizing dynamic texture of waterfall. From
    top to bottom: segments of the observed sequence, synthesized sequence by our
    method, and synthesized sequence by the method of [1].'
  Figure 4 Link: articels_figures_by_rev_year\2019\Learning_EnergyBased_SpatialTemporal_Generative_ConvNets_for_Dynamic_Patterns\figure_4.jpg
  Figure 4 caption: "Limited time pairwise comparison results. Each curve shows the\
    \ \u201Cfooling\u201D rateuser error (realism) over different exposure times.\
    \ The number of pairwise comparisons is 36. The number of participants is 20."
  Figure 5 Link: articels_figures_by_rev_year\2019\Learning_EnergyBased_SpatialTemporal_Generative_ConvNets_for_Dynamic_Patterns\figure_5.jpg
  Figure 5 caption: "Learning from 30 observed burning fire videos with mini-batch\
    \ implementation. The batch size is 10. For each mini-batch, the number of parallel\
    \ chains for synthesis is 13. The observed and synthesized videos are of the size\
    \ 100 \xD7100 pixels \xD7 30 frames. (a) displays one frame for each of 30 observed\
    \ sequences. (b) displays the corresponding frame for each of the synthesized\
    \ sequences. (c) shows three examples (one example per row) of synthesized video\
    \ sequences."
  Figure 6 Link: articels_figures_by_rev_year\2019\Learning_EnergyBased_SpatialTemporal_Generative_ConvNets_for_Dynamic_Patterns\figure_6.jpg
  Figure 6 caption: "Learning from 30 observed flowing cloud videos with mini-batch\
    \ implementation. The batch size is 10. For each mini-batch, the number of parallel\
    \ chains for synthesis is 13. The observed and synthesized videos are of the size\
    \ 100 \xD7 100 pixels \xD7 30 frames. (a) displays one frame for each of 30 observed\
    \ sequences. (b) displays the corresponding frame for each of the synthesized\
    \ sequences. (c) shows three examples (one example per row) of synthesized video\
    \ sequences."
  Figure 7 Link: articels_figures_by_rev_year\2019\Learning_EnergyBased_SpatialTemporal_Generative_ConvNets_for_Dynamic_Patterns\figure_7.jpg
  Figure 7 caption: "Synthesizing action patterns without spatial or temporal stationarity.\
    \ For each action video sequence, 6 continuous frames are shown. (a) Running cows.\
    \ Frames of 2 of 5 training sequences are displayed. The corresponding frames\
    \ of 2 of 8 synthesized sequences generated by the learning algorithm are displayed.\
    \ (b) Running tigers. Frames of 2 observed training sequences are displayed. The\
    \ corresponding frames of 2 of 4 synthesized sequences are displayed. (c) Running\
    \ llamas. Frames of 2 observed training sequences are displayed. The corresponding\
    \ frames of 2 of 5 synthesized sequences are displayed. The observed sequences\
    \ are of the size 100\xD7100 pixels \xD770 frames."
  Figure 8 Link: articels_figures_by_rev_year\2019\Learning_EnergyBased_SpatialTemporal_Generative_ConvNets_for_Dynamic_Patterns\figure_8.jpg
  Figure 8 caption: "Learning energy-based spatial-temporal generative ConvNets from\
    \ occluded video sequences. For each experiment, the first row shows a segment\
    \ of the observed sequence, the second row shows a segment of the occluded sequence\
    \ with black masks, and the third row shows the corresponding segment of the recovered\
    \ sequence. The observed occluded sequences are of the size 150\xD7150 pixels\
    \ \xD770 frames. (a) Type 1: salt and pepper mask. (b) Type 2: single region mask.\
    \ (c) Type 3: 50 percent missing frames."
  Figure 9 Link: articels_figures_by_rev_year\2019\Learning_EnergyBased_SpatialTemporal_Generative_ConvNets_for_Dynamic_Patterns\figure_9.jpg
  Figure 9 caption: "Background inpainting for videos. For each experiment, the first\
    \ column displays 4 frames of the original video. The second column shows the\
    \ corresponding frames with black masks occluding the target to be removed. The\
    \ third column shows the inpainting result by our algorithm. (a) A moving boat\
    \ in the lake ( 130\xD7174 pixels \xD7150 frames). (b) A walking person in front\
    \ of fountain ( 130\xD7230 pixels \xD7104 frames)."
  First author gender probability: 0.95
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.86
  Name of the first author: Jianwen Xie
  Name of the last author: Ying Nian Wu
  Number of Figures: 9
  Number of Tables: 2
  Number of authors: 3
  Paper title: Learning Energy-Based Spatial-Temporal Generative ConvNets for Dynamic
    Patterns
  Publication Date: 2019-08-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 A Comparison of Different Dynamic Texture Models on SSIM
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Recovery Errors in Occlusion Experiments
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2934852
- Affiliation of the first author: markableai inc, brooklyn, ny, usa
  Affiliation of the last author: markableai inc, brooklyn, ny, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Saliency_Prediction_in_the_Deep_Learning_Era_Successes_and_Limitations\figure_1.jpg
  Figure 1 caption: Some images with fixation maps and prediction maps from the SALICON
    model [16]. Maps in the second and third rows belong to either Model (M) or Humans
    (H). Try to guess which one is which (in the second row). Zoom on the bottom-right
    caption to see the answer.
  Figure 10 Link: articels_figures_by_rev_year\2019\Saliency_Prediction_in_the_Deep_Learning_Era_Successes_and_Limitations\figure_10.jpg
  Figure 10 caption: Performance of models over LEDOV dataset [38]. Xu et al. is the
    model in [57]. Models using the motion are marked with . Deep models are marked
    with . Results are compiled from Jiang et al. [38].
  Figure 2 Link: articels_figures_by_rev_year\2019\Saliency_Prediction_in_the_Deep_Learning_Era_Successes_and_Limitations\figure_2.jpg
  Figure 2 caption: New crowd-sourcing methodologies to collect attention data including
    TurkerGaze [19], SALICON [22], and BubbleView [23].
  Figure 3 Link: articels_figures_by_rev_year\2019\Saliency_Prediction_in_the_Deep_Learning_Era_Successes_and_Limitations\figure_3.jpg
  Figure 3 caption: Recent fixation datasets over still images. Please refer to the
    MIT Saliency Benchmark and the text for details.
  Figure 4 Link: articels_figures_by_rev_year\2019\Saliency_Prediction_in_the_Deep_Learning_Era_Successes_and_Limitations\figure_4.jpg
  Figure 4 caption: Recent fixation datasets over videos. Please refer to the MIT
    Saliency Benchmark and the text for details.
  Figure 5 Link: articels_figures_by_rev_year\2019\Saliency_Prediction_in_the_Deep_Learning_Era_Successes_and_Limitations\figure_5.jpg
  Figure 5 caption: A) Common structure of deep saliency models. Pre-trained models
    for object recognition are adopted and fine-tuned for saliency recognition, and
    are applied to different tasks, B) A timeline of deep-learning based visual saliency
    since 2014.
  Figure 6 Link: articels_figures_by_rev_year\2019\Saliency_Prediction_in_the_Deep_Learning_Era_Successes_and_Limitations\figure_6.jpg
  Figure 6 caption: 'A) Image versus video saliency. Rudoy et al. [59] showed the
    same image to observers twice: once in isolation for 3 seconds (left) and once
    embedded in a video (right). As it can be seen, video fixation map is more concentrated
    on a single object whereas image saliency is dispersed. See also [89]. Figure
    from [59].'
  Figure 7 Link: articels_figures_by_rev_year\2019\Saliency_Prediction_in_the_Deep_Learning_Era_Successes_and_Limitations\figure_7.jpg
  Figure 7 caption: Saliency maps of 90 models over a sample image. Models are sorted
    according to AUC-Judd score. Red boxes indicate baselines.
  Figure 8 Link: articels_figures_by_rev_year\2019\Saliency_Prediction_in_the_Deep_Learning_Era_Successes_and_Limitations\figure_8.jpg
  Figure 8 caption: In Bylinskii et al. [7], we showed that a small set of 10 images
    is enough to rank models. These images contain people at varying scales and text
    amidst distracting textures. This figure shows some images for which deep models
    perform the best while classical non-deep models suffer the most. Refer to Fig.
    2 in [7] to see all 10 images.
  Figure 9 Link: articels_figures_by_rev_year\2019\Saliency_Prediction_in_the_Deep_Learning_Era_Successes_and_Limitations\figure_9.jpg
  Figure 9 caption: 'Performance of models over DHF1K dataset [37]. Models using the
    motion feature are marked with . Deep models are marked with . Bottom-right: Dynamic
    saliency prediction performance over time, evaluated on the DHF1K test set. Results
    are compiled from our work in [37].'
  First author gender probability: 0.94
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.94
  Name of the first author: Ali Borji
  Name of the last author: Ali Borji
  Number of Figures: 23
  Number of Tables: 3
  Number of authors: 1
  Paper title: 'Saliency Prediction in the Deep Learning Era: Successes and Limitations'
  Publication Date: 2019-08-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance of Static Saliency Models over the MIT300 dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance of Static Saliency Models over the CAT2000 Dataset
  Table 3 caption:
    table_text: TABLE 3 Performance of Dynamic Saliency Models over the SALICON Test
      set
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2935715
- Affiliation of the first author: media analytics and computing lab, department of
    artificial intelligence, school of informatics, xiamen university, china
  Affiliation of the last author: department of computer science, university of rochester,
    rochester, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\SemiSupervised_Adversarial_Monocular_Depth_Estimation\figure_1.jpg
  Figure 1 caption: Our semi-supervised adversarial framework. We try to leverage
    a vast amount of unannotated images together with a small number of image-depth
    pairs to train a depth estimation network. The generator receives gradients from
    two discriminator networks to update its parameters. Unlike traditional losses
    such as L 1 , L 2 and Huber norm, the loss from two discriminators feedback can
    preserve better details with less requirement on the amount of ground truth.
  Figure 10 Link: articels_figures_by_rev_year\2019\SemiSupervised_Adversarial_Monocular_Depth_Estimation\figure_10.jpg
  Figure 10 caption: Qualitative results on KITTI dataset. The columns(from left to
    right) are RGB images, ground truth depth maps, and results by our semi-supervised
    model, respectively. Pixels with distance larger than 80m are masked out.
  Figure 2 Link: articels_figures_by_rev_year\2019\SemiSupervised_Adversarial_Monocular_Depth_Estimation\figure_2.jpg
  Figure 2 caption: A basic encoder-decoder generator architecture. Encoder extracts
    features while reducing the spatial size, which is usually done by basic convolutions
    or convBlocks [23], [36]. Decoder gradually upsamples the extracted features to
    a size similar with the input image, which is usually implemented by deconvolution
    or naive interpolation.
  Figure 3 Link: articels_figures_by_rev_year\2019\SemiSupervised_Adversarial_Monocular_Depth_Estimation\figure_3.jpg
  Figure 3 caption: The architecture of the pair discriminator (PatchGAN [39]), which
    consists of five layers of convolution with increasing sizes of receptive fields
    in each layer. The input image and the predicted depth map in the last layer are
    split to patches and each patch is inspected as real or fake by the discriminator.
  Figure 4 Link: articels_figures_by_rev_year\2019\SemiSupervised_Adversarial_Monocular_Depth_Estimation\figure_4.jpg
  Figure 4 caption: Qualitative results on NYU Depth test set. All losses are applied
    to the same model architecture with the same learning strategy using 500 image-depth
    pairs. Our semi-supervised models are able to produce finer depth maps than the
    compared approaches.
  Figure 5 Link: articels_figures_by_rev_year\2019\SemiSupervised_Adversarial_Monocular_Depth_Estimation\figure_5.jpg
  Figure 5 caption: The convergence curves of different loss functions during model
    training. After about 15 epochs, all losses tend to stop decreasing and we stop
    training when convergence is observed.
  Figure 6 Link: articels_figures_by_rev_year\2019\SemiSupervised_Adversarial_Monocular_Depth_Estimation\figure_6.jpg
  Figure 6 caption: Performance curves with respect to different numbers of additional
    RGB images used on NYU Depth dataset. In all experiment, 500 image-depth pairs
    are used. Our semi-supervised framework can effectively boost the performance
    by leveraging extra unlabeled RGB images.
  Figure 7 Link: articels_figures_by_rev_year\2019\SemiSupervised_Adversarial_Monocular_Depth_Estimation\figure_7.jpg
  Figure 7 caption: Some qualitative results tested on wild images using the model
    trained on NYUDv2 with 1k ground truth image pairs.
  Figure 8 Link: articels_figures_by_rev_year\2019\SemiSupervised_Adversarial_Monocular_Depth_Estimation\figure_8.jpg
  Figure 8 caption: Qualitative results on Make3D dataset. The rows (from up to bottom)
    are RGB images, ground truth depth maps, and results by our supervised model,
    respectively. Pixels with distance larger than 70m are masked out.
  Figure 9 Link: articels_figures_by_rev_year\2019\SemiSupervised_Adversarial_Monocular_Depth_Estimation\figure_9.jpg
  Figure 9 caption: Performance curves with respect to different numbers of training
    image-depth pairs. We can see that our semi-supervised framework can boost the
    performance when the number of labeled training samples is less than 1 K.
  First author gender probability: 0.88
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.75
  Name of the first author: Rongrong Ji
  Name of the last author: Jiebo Luo
  Number of Figures: 12
  Number of Tables: 8
  Number of authors: 9
  Paper title: Semi-Supervised Adversarial Monocular Depth Estimation
  Publication Date: 2019-08-20 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Receptive Field Size of Neurons in Each Feature Layer
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Evaluation of Different Loss Functions on NYU Depth Test Set
  Table 3 caption:
    table_text: TABLE 3 Comparisons with the GAN Loss Variant
  Table 4 caption:
    table_text: TABLE 4 Quantitative Comparisons of Models Trained by Supervised Learning
      and Our Semi-Supervised Framework
  Table 5 caption:
    table_text: TABLE 5 Error Analysis Against Different Semantic Areas
  Table 6 caption:
    table_text: TABLE 6 Comparison on the Make3D Dataset
  Table 7 caption:
    table_text: TABLE 7 Comparison on the KITTI Dataset
  Table 8 caption:
    table_text: TABLE 8 Adaptation Results from the KITTI to the Make3D
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2936024
- Affiliation of the first author: moe key laboratory of system control and information
    processing, institute of image processing and pattern recognition and institute
    of medical robotics, shanghai jiao tong university, shanghai, p.r. china
  Affiliation of the last author: moe key laboratory of system control and information
    processing, institute of image processing and pattern recognition and institute
    of medical robotics, shanghai jiao tong university, shanghai, p.r. china
  Figure 1 Link: articels_figures_by_rev_year\2019\Adversarial_Attack_Type_I_Cheat_Classifiers_by_Significant_Changes\figure_1.jpg
  Figure 1 caption: "Relationship between Type I and Type II adversarial attacks on\
    \ ROC curve of f 1 . Through viewing number \u201C3\u201D as true sample and others\
    \ as false samples, Type II attack aims to decrease the true positive rate (TPR),\
    \ while Type I attack tries to increase the false positive rate (FPR)."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Adversarial_Attack_Type_I_Cheat_Classifiers_by_Significant_Changes\figure_2.jpg
  Figure 2 caption: "f 2 is the oracle classifier which uses x(1) and x(2) to distinguish\
    \ samples shown by green crosses and yellow circles. But trained on these samples,\
    \ f 1 gets 100 percent accuracy by considering x(1) and x(3) . Here x(3) is an\
    \ unnecessary feature such that shifting a true positive along x(3) cannot be\
    \ observed by the oracle but makes the result of f 1 change (Type II attack, the\
    \ blue arrow). For Type I attack (the green arrow), the sample is moved along\
    \ x(2) (the missing feature) and an example different to x in the view of the\
    \ oracle but f 1 (x)= f 1 ( x \u2032 ) is generated."
  Figure 3 Link: articels_figures_by_rev_year\2019\Adversarial_Attack_Type_I_Cheat_Classifiers_by_Significant_Changes\figure_3.jpg
  Figure 3 caption: "Illustration of Type I adversarial attack on the sphere dataset\
    \ [24]. The attacking direction is project of the radial vector onto the surface\
    \ \u25BD f 1 ( x t )=0 , where x t is the sample generated at the t th step."
  Figure 4 Link: articels_figures_by_rev_year\2019\Adversarial_Attack_Type_I_Cheat_Classifiers_by_Significant_Changes\figure_4.jpg
  Figure 4 caption: The framework for generating Type I adversarial example from the
    latent space. In this paper, SVAE is proposed for encoding.
  Figure 5 Link: articels_figures_by_rev_year\2019\Adversarial_Attack_Type_I_Cheat_Classifiers_by_Significant_Changes\figure_5.jpg
  Figure 5 caption: "Illustration of supervised image class transition by SVAE on\
    \ (a) MNIST and (b) CelebA datasets. The target class for digital image i is i+1\
    \ (the target of \u201C9\u201D is \u201C0\u201D) and for malefemale face images\
    \ is the opposites."
  Figure 6 Link: articels_figures_by_rev_year\2019\Adversarial_Attack_Type_I_Cheat_Classifiers_by_Significant_Changes\figure_6.jpg
  Figure 6 caption: "Illustration of the log loss terms of the attacker f 2 , the\
    \ discriminator f dis and the MLP f 1 by setting y \u2032 =5 and y=4 in (8). The\
    \ lower loss of f 1 , the more it is believe the generated image belongs to class\
    \ \u201C4\u201D. Until convergence, f 1 still classifies it as \u201C4\u201D with\
    \ 99.41 percent confidence."
  Figure 7 Link: articels_figures_by_rev_year\2019\Adversarial_Attack_Type_I_Cheat_Classifiers_by_Significant_Changes\figure_7.jpg
  Figure 7 caption: (a) Adversarial examples with the probabilities according to iteration
    steps on MNIST dataset. (b) Adversarial example pairs with euclidean distance
    on the top measured by FaceNet. Note that 1.242 is the distance threshold adopted
    by FaceNet to judge whether two images are from the same person.
  Figure 8 Link: articels_figures_by_rev_year\2019\Adversarial_Attack_Type_I_Cheat_Classifiers_by_Significant_Changes\figure_8.jpg
  Figure 8 caption: Type I attack performance on the latent space of AC-GAN. For each
    pair of the digits, the attacked LeNet correctly recognizes the left digit but
    incorrectly thinks the right one belongs to the same class.
  Figure 9 Link: articels_figures_by_rev_year\2019\Adversarial_Attack_Type_I_Cheat_Classifiers_by_Significant_Changes\figure_9.jpg
  Figure 9 caption: "Type I attack on the latent variables of StyleGAN. The left column\
    \ shows the generated faces corresponding to the starting latent variable, the\
    \ right column shows the attack results, and the intermediate images are displayed\
    \ in the middle. The image size is 1024\xD71024 . For all the 6 rows, the distances\
    \ judged by FaceNet between the left and the right are: 0.6780, 0.6321, 0.7632,\
    \ 1.0482, 0.6448, and 1.0449 (from top to bottom), which are all below 1.242.\
    \ Though the appearances have been changed significantly, the faces in each row\
    \ are still identified as a same person by FaceNet."
  First author gender probability: 0.93
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.66
  Name of the first author: Sanli Tang
  Name of the last author: Jie Yang
  Number of Figures: 9
  Number of Tables: 3
  Number of authors: 5
  Paper title: 'Adversarial Attack Type I: Cheat Classifiers by Significant Changes'
  Publication Date: 2019-08-20 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Detection Rates on MNIST Test Set, Type I Adversarial Examples
      and Type II Adversarial Examples with Different Thresholds
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Detection Rates on CelebA Test Set Through the Feature Squeezing
      Defense Method
  Table 3 caption:
    table_text: TABLE 3 Classification Accuracy of FaceNet Strengthened by Different
      Attack Methods on Adversarial Examples
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2936378
- Affiliation of the first author: department of pathology, diagnostic image analysis
    group, radboud university medical center, nijmegen, the netherlands
  Affiliation of the last author: department of pathology, diagnostic image analysis
    group, radboud university medical center, nijmegen, the netherlands
  Figure 1 Link: articels_figures_by_rev_year\2019\Neural_Image_Compression_for_Gigapixel_Histopathology_Image_Analysis\figure_1.jpg
  Figure 1 caption: 'Gigapixel neural image compression. Left: a gigapixel histopathology
    whole-slide image is divided into a set of patches mapped to a set of low-dimensional
    embedding vectors using a neural network (the encoder). Center: these embeddings
    are stored keeping the spatial arrangement of the original patches. Right: the
    resulting array is a compressed representation of the gigapixel image. M and N
    : size of the gigapixel image; P : size of the square patches; C : size of the
    embedding vectors; and S : stride used to sample the patches. Typically: M=N=50,000
    and P=S=C=128 .'
  Figure 10 Link: articels_figures_by_rev_year\2019\Neural_Image_Compression_for_Gigapixel_Histopathology_Image_Analysis\figure_10.jpg
  Figure 10 caption: 'Grad-CAM visualization applied to several WSIs from Camelyon16.
    Top: the first five images represent the saliency maps for CNNs trained with 5
    different encoders, respectively. The sixth and seventh images are the reference
    standard (manual annotations) and RGB thumbnail of the WSI, respectively. Dark
    blue represents low saliency, whereas yellow indicates high saliency. Bottom-left:
    failure case where the BiGAN model failed to recognize the tumor area. Bottom-right:
    failure case where the BiGAN model attended to a region with no tumor cells.'
  Figure 2 Link: articels_figures_by_rev_year\2019\Neural_Image_Compression_for_Gigapixel_Histopathology_Image_Analysis\figure_2.jpg
  Figure 2 caption: 'Variational Autoencoder. Top: the encoder maps a patch to an
    embedding vector depending on a noise vector while the decoder reconstructs the
    original patch from the embedding vector. Bottom: pairs of real and reconstructed
    patch samples using C=128 .'
  Figure 3 Link: articels_figures_by_rev_year\2019\Neural_Image_Compression_for_Gigapixel_Histopathology_Image_Analysis\figure_3.jpg
  Figure 3 caption: 'Contrastive training. Top: pairs of patches are extracted from
    gigapixel images. Pairs labeled as same originate from the same spatial location
    whereas different are extracted from either adjacent locations or different images.
    Bottom: scheme of a Siamese network trained for binary classification using the
    previous pairs.'
  Figure 4 Link: articels_figures_by_rev_year\2019\Neural_Image_Compression_for_Gigapixel_Histopathology_Image_Analysis\figure_4.jpg
  Figure 4 caption: 'Adversarial Feature Learning. Top: three networks play a minimax
    game where the discriminator distinguishes between actual or generated image-embedding
    pairs, while the generator and the encoder fool the discriminator by producing
    increasingly more realistic images and embeddings. Bottom: real and generated
    patch samples using C=128 .'
  Figure 5 Link: articels_figures_by_rev_year\2019\Neural_Image_Compression_for_Gigapixel_Histopathology_Image_Analysis\figure_5.jpg
  Figure 5 caption: 'Example of an image from the synthetic dataset. Left: ground
    truth mask depicting the tilted and non-tilted rectangles that simulate lesions
    in grey and white, respectively. Center: image containing instances of MNIST digits;
    classes are defined by the rectangles or selected randomly. Right: all digits
    within the tilted rectangle boundary (in green) belong to the same class (number
    two), which corresponds to the image label as well.'
  Figure 6 Link: articels_figures_by_rev_year\2019\Neural_Image_Compression_for_Gigapixel_Histopathology_Image_Analysis\figure_6.jpg
  Figure 6 caption: 'Experimental results with synthetic data and image-level labels.
    Default hyper-parameter choice unless specified otherwise is: supervised encoder,
    code size 16, stride 9 pixels, and usage of 100 percent of training data.'
  Figure 7 Link: articels_figures_by_rev_year\2019\Neural_Image_Compression_for_Gigapixel_Histopathology_Image_Analysis\figure_7.jpg
  Figure 7 caption: Grad-CAM visualization applied to randomly selected synthetic
    test images. Left images within the pairs correspond to the ground truth masks
    (unseen by the model), and right ones to the saliency heatmaps. Note that areas
    corresponding to the grey tilted rectangles (responsible for the image-level labels)
    are highly salient with respect to the rest of the image.
  Figure 8 Link: articels_figures_by_rev_year\2019\Neural_Image_Compression_for_Gigapixel_Histopathology_Image_Analysis\figure_8.jpg
  Figure 8 caption: 'Experimental results with respect to lesion size in Camelyon16
    all test set using multiple encoders. Solid lines: average probability of samples
    with positive labels; dashed lines: average probability of samples with negative
    labels (no lesion).'
  Figure 9 Link: articels_figures_by_rev_year\2019\Neural_Image_Compression_for_Gigapixel_Histopathology_Image_Analysis\figure_9.jpg
  Figure 9 caption: 'Hyper-parameter value analysis performed in Camelyon16 data using
    the supervised encoder. Evaluated on unseen images from the first data partition
    out of the 4-fold cross-validation sets. Left: varying code size using a fix stride
    of 128 pixels; center: varying stride while using a fix code size of 128 elements;
    and right: varying the number of WSIs used during training.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: David Tellez
  Name of the last author: Francesco Ciompi
  Number of Figures: 11
  Number of Tables: 3
  Number of authors: 4
  Paper title: Neural Image Compression for Gigapixel Histopathology Image Analysis
  Publication Date: 2019-08-22 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Patch-Level Classification Performance (Accuracy)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Predicting the Presence of Metastasis at WSI Level (AUC)
  Table 3 caption:
    table_text: TABLE 3 Predicting Tumor Proliferation Speed at WSI Level (Spearman
      Corr.)
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2936841
- Affiliation of the first author: research school of engineering, the australian
    national university, canberra, act, australia
  Affiliation of the last author: mistubishi electric research labs (merl), cambridge,
    ma, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Discriminative_Video_Representation_Learning_Using_Support_Vector_Classifiers\figure_1.jpg
  Figure 1 caption: A illustration of our discriminative pooling scheme. Our main
    idea is to learn a representation for the positive bag (left) of CNN features
    from the video of interest. To extract useful features from this video, we use
    a negative bag (right) of features from videos that are known to contain irrelevantnoise
    features. The representation learning problem is cast as a binary (non)-linear
    classification problem in an SVM setting; the hyperplane found via the optimization
    (which is a linear combination of support vectors) is used as the representation
    of the positive bag, which we call the SVM pooled descriptor.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Discriminative_Video_Representation_Learning_Using_Support_Vector_Classifiers\figure_2.jpg
  Figure 2 caption: Illustration of our SVM Pooling scheme. (i) Extraction of frames
    from videos, (ii) Converting frames f into feature x , (iii) Learning decision
    boundary w from feature x , and (iv) Using w as video descriptor.
  Figure 3 Link: articels_figures_by_rev_year\2019\Discriminative_Video_Representation_Learning_Using_Support_Vector_Classifiers\figure_3.jpg
  Figure 3 caption: "An illustration of our overall idea. (a) the input data points,\
    \ and the plausible hyperplanes satisfying some \u03B7 constraint, (b) when noise\
    \ X \u2212 is introduced (green dots), it helps identify noisy featuresdata dimensions,\
    \ towards producing a hyperplane w that classifies useful data from noise, while\
    \ satisfying the \u03B7 constraint."
  Figure 4 Link: articels_figures_by_rev_year\2019\Discriminative_Video_Representation_Learning_Using_Support_Vector_Classifiers\figure_4.jpg
  Figure 4 caption: "Two possible ways to insert SVM pooling layer within a standard\
    \ CNN architecture. In the first option (top), we insert the SVMP layer between\
    \ fully connected layers, while in the latter we include it before the final classifier\
    \ layer. The choice of L\u22121 layer for the former is arbitrary. We also show\
    \ the corresponding partial gradients with respect to weights of the layer penultimate\
    \ to the SVM pooling layer. Except for the gradients \u2202SVMP(X) \u2202X , other\
    \ gradients are the standard ones. Here Z \u2113 represents the weights of the\
    \ \u2113 th layer of the network."
  Figure 5 Link: articels_figures_by_rev_year\2019\Discriminative_Video_Representation_Learning_Using_Support_Vector_Classifiers\figure_5.jpg
  Figure 5 caption: Analysis of the parameters used in our scheme. All experiments
    use VGG features from fc6 dense layer. See text for details.
  Figure 6 Link: articels_figures_by_rev_year\2019\Discriminative_Video_Representation_Learning_Using_Support_Vector_Classifiers\figure_6.jpg
  Figure 6 caption: 'T-SNE plots of positive (blue) and negative bags (red) when using
    negatives from: (a) Thumos, (b) UCF101, and (c) white noise.'
  Figure 7 Link: articels_figures_by_rev_year\2019\Discriminative_Video_Representation_Learning_Using_Support_Vector_Classifiers\figure_7.jpg
  Figure 7 caption: T-SNE visualizations of SVMP and other pooling methods on sequences
    from the HMDB51 dataset (10 classes used). From left to right, Average Pooling,
    Max Pooling, and SVMP.
  Figure 8 Link: articels_figures_by_rev_year\2019\Discriminative_Video_Representation_Learning_Using_Support_Vector_Classifiers\figure_8.jpg
  Figure 8 caption: Visualizations of various pooled descriptors.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Jue Wang
  Name of the last author: Anoop Cherian
  Number of Figures: 8
  Number of Tables: 9
  Number of authors: 2
  Paper title: Discriminative Video Representation Learning Using Support Vector Classifiers
  Publication Date: 2019-08-26 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison between Algorithms 1 and 2 in HMDB-51 Split-1
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of SVMP Descriptors using Various CNN Features
      on HMDB Split-1
  Table 3 caption:
    table_text: TABLE 3 Comparison between SVMP and NSVMP on Split-1
  Table 4 caption:
    table_text: TABLE 4 Comparison to Standard Pooling Methods on Split-1
  Table 5 caption:
    table_text: TABLE 5 Recognition Rates on Split-1 of JHMDB and UCF-101
  Table 6 caption:
    table_text: TABLE 6 Comparisons on Kinetics-600 Dataset Using I3D Feature
  Table 7 caption:
    table_text: TABLE 7 Comparisons on Charades Dataset
  Table 8 caption:
    table_text: TABLE 8 Accuracy Comparison on Different Subsets of HMDB-51(H) and
      UCF-101(U) Split-1 using I3D+ Features
  Table 9 caption:
    table_text: TABLE 9 Comparison to the State of the Art in Each Dataset, Following
      the Official Evaluation Protocol for Each Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2937292
- Affiliation of the first author: school of electronic information and communications,
    huazhong university of science and technology, wuhan, china
  Affiliation of the last author: school of electronic information and communications,
    huazhong university of science and technology, wuhan, china
  Figure 1 Link: articels_figures_by_rev_year\2019\Mask_TextSpotter_An_EndtoEnd_Trainable_Neural_Network_for_Spotting_Text_with_Arb\figure_1.jpg
  Figure 1 caption: 'Illustrations of different text spotting methods. The left presents
    horizontal text spotting methods [39], [42]; The middle indicates oriented text
    spotting methods [7], [41], [46]; The right is our proposed method. Green bounding
    box: detection result; Red text in green background: recognition result.'
  Figure 10 Link: articels_figures_by_rev_year\2019\Mask_TextSpotter_An_EndtoEnd_Trainable_Neural_Network_for_Spotting_Text_with_Arb\figure_10.jpg
  Figure 10 caption: Failure cases. Failure cases are in the red boxes for better
    visualization.
  Figure 2 Link: articels_figures_by_rev_year\2019\Mask_TextSpotter_An_EndtoEnd_Trainable_Neural_Network_for_Spotting_Text_with_Arb\figure_2.jpg
  Figure 2 caption: Architecture of Mask TextSpotter. The solid arrows mean the data
    flow both in training and inference period. The dashed arrows in blue and in red
    indicate the data flow in training stage and inference stage, respectively. The
    details of the character segmentation and the spatial attentional module are illustrated
    in Fig. 3.
  Figure 3 Link: articels_figures_by_rev_year\2019\Mask_TextSpotter_An_EndtoEnd_Trainable_Neural_Network_for_Spotting_Text_with_Arb\figure_3.jpg
  Figure 3 caption: Architecture of the standalone recognition model. We use a feature-pyramid
    structure with ResNet-50. Note that both the two modules can provide the recognition
    results along with their confidence score, we select the final recognition result
    with a higher confidence score dynamically. The solid arrows mean the steps in
    both the training and the inference period; the dashed arrows indicate the steps
    only in the inference period.
  Figure 4 Link: articels_figures_by_rev_year\2019\Mask_TextSpotter_An_EndtoEnd_Trainable_Neural_Network_for_Spotting_Text_with_Arb\figure_4.jpg
  Figure 4 caption: 'Label generation of the text instance segmentation and the character
    segmentation. Left: the blue box is a proposal yielded by RPN, the red polygon
    and yellow boxes are ground truth polygon and character boxes, the green box is
    the horizontal rectangle which covers the polygon with minimal area. Right: the
    text instance map (top) and the character map (bottom).'
  Figure 5 Link: articels_figures_by_rev_year\2019\Mask_TextSpotter_An_EndtoEnd_Trainable_Neural_Network_for_Spotting_Text_with_Arb\figure_5.jpg
  Figure 5 caption: Illustration of the pixel voting algorithm. We use the original
    image crop represents the corresponding RoI feature for better visualization.
  Figure 6 Link: articels_figures_by_rev_year\2019\Mask_TextSpotter_An_EndtoEnd_Trainable_Neural_Network_for_Spotting_Text_with_Arb\figure_6.jpg
  Figure 6 caption: Illustration of the edit distance and our proposed weighted edit
    distance. The red characters are the characters will be deleted, inserted and
    replaced. Green characters mean the candidate characters. p c index is the character
    probability, index is the character index and c is the current character.
  Figure 7 Link: articels_figures_by_rev_year\2019\Mask_TextSpotter_An_EndtoEnd_Trainable_Neural_Network_for_Spotting_Text_with_Arb\figure_7.jpg
  Figure 7 caption: Visualization results of ICDAR 2013 (the first column), ICDAR
    2015 (the second column) and Total-Text (the last two columns). The dashed red
    bounding boxes are the false negatives.
  Figure 8 Link: articels_figures_by_rev_year\2019\Mask_TextSpotter_An_EndtoEnd_Trainable_Neural_Network_for_Spotting_Text_with_Arb\figure_8.jpg
  Figure 8 caption: 'Qualitative comparisons on Total-Text without lexicon. Top: results
    of TextBoxes [42]; Bottom: results of ours.'
  Figure 9 Link: articels_figures_by_rev_year\2019\Mask_TextSpotter_An_EndtoEnd_Trainable_Neural_Network_for_Spotting_Text_with_Arb\figure_9.jpg
  Figure 9 caption: Visualization results of the character segmentation maps and the
    spatial attention weights. Best viewed in color.
  First author gender probability: 0.55
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Minghui Liao
  Name of the last author: Xiang Bai
  Number of Figures: 10
  Number of Tables: 9
  Number of authors: 6
  Paper title: 'Mask TextSpotter: An End-to-End Trainable Neural Network for Spotting
    Text with Arbitrary Shapes'
  Publication Date: 2019-08-26 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Detection Results on ICDAR2013 and ICDAR2015
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results on ICDAR2013
  Table 3 caption:
    table_text: TABLE 3 Results on ICDAR2015
  Table 4 caption:
    table_text: TABLE 4 Detection and End-to-End Results on COCO-Text
  Table 5 caption:
    table_text: TABLE 5 Results on Total-Text
  Table 6 caption:
    table_text: TABLE 6 Results on MLT Dataset
  Table 7 caption:
    table_text: TABLE 7 Ablation Experimental Results
  Table 8 caption:
    table_text: TABLE 8 Experiments on Backbone and RoI Size of the Mask Branch
  Table 9 caption:
    table_text: TABLE 9 Scene Text Recognition Results
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2937086
