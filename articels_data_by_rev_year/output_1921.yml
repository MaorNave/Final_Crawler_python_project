- Affiliation of the first author: moe key lab of artificial intelligence, ai institute,
    shanghai jiao tong university, shanghai, china
  Affiliation of the last author: school of electronic engieering and computer science,
    university of california at merced, merced, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Deep_Object_Tracking_With_Shrinkage_Loss\figure_1.jpg
  Figure 1 caption: Qualitative results of different loss functions for learning one-stage
    regression trackers on the Bolt2 sequence [12]. The proposed regression tracker
    (DSLT) with shrinkage loss performs much better than that with the L2 and L3 losses.
  Figure 10 Link: articels_figures_by_rev_year\2020\Deep_Object_Tracking_With_Shrinkage_Loss\figure_10.jpg
  Figure 10 caption: Overall performance on the VOT-2016 [13] using expected average
    overlap graph. The proposed DSLT method ranks second.
  Figure 2 Link: articels_figures_by_rev_year\2020\Deep_Object_Tracking_With_Shrinkage_Loss\figure_2.jpg
  Figure 2 caption: 'Overview of the proposed deep regression network for tracking.
    Left: Fixed feature extractor (ResNet50 [47]). Right: Regression network trained
    in the first frame and updated frame-by-frame. We apply residual connections to
    both convolutional layers and output response maps, and use a bilinear interpolation
    layer for upsampling. The proposed network effectively exploits multi-level semantic
    abstraction across convolutional layers (Section 3.3). Our shrinkage loss helps
    to break the performance bottleneck of one-stage regression trackers caused by
    data imbalance and accelerates the convergence of network training (Section 3.2).'
  Figure 3 Link: articels_figures_by_rev_year\2020\Deep_Object_Tracking_With_Shrinkage_Loss\figure_3.jpg
  Figure 3 caption: "(a) Input patch. (b) The corresponding soft labels Y generated\
    \ by Gaussian function for training. (c) The output regression map P . (d) The\
    \ histogram of the absolute difference |P\u2212Y| . Note that easy samples with\
    \ small absolute difference scores dominate the training data. See Section 3.2\
    \ for details."
  Figure 4 Link: articels_figures_by_rev_year\2020\Deep_Object_Tracking_With_Shrinkage_Loss\figure_4.jpg
  Figure 4 caption: (a) Modulating factors in Eq. (5) with different hyper-parameters.
    (b) Comparison between the square loss ( L 2 ), L 3 loss and the proposed shrinkage
    loss for regression learning. The proposed shrinkage loss only decreases the loss
    from easy samples ( l<0.5 ) and keeps the loss from hard samples ( l>0.5 ) unchanged.
    See Section 3.2 for details
  Figure 5 Link: articels_figures_by_rev_year\2020\Deep_Object_Tracking_With_Shrinkage_Loss\figure_5.jpg
  Figure 5 caption: Different schemes to fuse convolutional layers (Section 3.3).
    ECO [10] independently learns correlation filters over multiple convolutional
    layers. CREST [3] learns a base and a residual regression network over a single
    convolutional layer. We first fuse multiple convolutional layers using residual
    connection and then perform regression learning. Our regression network makes
    full use of multi-level semantics across multiple convolutional layers rather
    than merely integrating response maps as ECO and CREST.
  Figure 6 Link: articels_figures_by_rev_year\2020\Deep_Object_Tracking_With_Shrinkage_Loss\figure_6.jpg
  Figure 6 caption: (a) Modulating factors in Eq. (12) with different hyper-parameters
    (Section 3.4). (b) Comparison between the BCE loss, focal loss and the proposed
    shrinkage loss for Siamese network learning. The proposed shrinkage loss only
    decreases the loss from easy samples ( l<0.5 ) and keeps the loss from hard samples
    ( l>0.5 ) almost unchanged (pink) or even greater (red) for hard samples.
  Figure 7 Link: articels_figures_by_rev_year\2020\Deep_Object_Tracking_With_Shrinkage_Loss\figure_7.jpg
  Figure 7 caption: Response maps with different loss functions. (a) The region of
    interest. (b) Ground truth map. (c), (d) and (e) show the response maps of SiameseFC
    with the original loss, focal loss and the proposed shrinkage loss. See Section
    3.4 for details.
  Figure 8 Link: articels_figures_by_rev_year\2020\Deep_Object_Tracking_With_Shrinkage_Loss\figure_8.jpg
  Figure 8 caption: Overall performance on the OTB-2013 [90] and OTB-2015 [12] datasets
    using one-pass evaluation (OPE). The legend of distance precision contains the
    threshold scores at 20 pixels while the legend of overlap success contains area-under-the-curve
    score for each tracker. Our tracker performs well against state-of-the-art methods
  Figure 9 Link: articels_figures_by_rev_year\2020\Deep_Object_Tracking_With_Shrinkage_Loss\figure_9.jpg
  Figure 9 caption: Overall performance on the UAV-123 [91] dataset using one-pass
    evaluation (OPE). The legend of distance precision contains the threshold scores
    at 20 pixels while the legend of overlap success contains area-under-the-curve
    score for each tracker. The proposed DSLT method ranks first among the regression
    based methods.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xiankai Lu
  Name of the last author: Ming-Hsuan Yang
  Number of Figures: 18
  Number of Tables: 3
  Number of authors: 6
  Paper title: Deep Object Tracking With Shrinkage Loss
  Publication Date: 2020-11-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Overall Performance on the VOT-2016 [13] in Comparison With
      the Top 10 Trackers
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Overall Performance on the VOT-2018 [24] Dataset in Comparison
      With the State-of-the-Art Trackers
  Table 3 caption:
    table_text: TABLE 3 Ablation Studies on the Combined Pool of the OTB-2015 and
      UAV-123 Datasets Under the Regression and Classification Cases
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3041332
- Affiliation of the first author: university of washington, seattle, wa, usa
  Affiliation of the last author: university of washington, seattle, wa, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\DiCENet_DimensionWise_Convolutions_for_Efficient_Networks\figure_1.jpg
  Figure 1 caption: Separable convolutions versus the DiCE unit.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\DiCENet_DimensionWise_Convolutions_for_Efficient_Networks\figure_2.jpg
  Figure 2 caption: Convolution-wise distribution of FLOPs for different networks
    with similar accuracy. The size of pie charts is scaled with respect to MobileNetv2s
    FLOPs. In DiCENet, efficient convs correspond to dimension-wise convolutions while
    in other networks, they correspond to depth-wise convolutions.
  Figure 3 Link: articels_figures_by_rev_year\2020\DiCENet_DimensionWise_Convolutions_for_Efficient_Networks\figure_3.jpg
  Figure 3 caption: DiCE unit efficiently encodes the spatial and channel-wise information
    in the input tensor X using dimension-wise convolutions (DimConv) and dimension-wise
    fusion (DimFuse) to produce an output tensor Y . For simplicity, we show kernel
    corresponding to each dimension independently. However, in practice, these three
    kernels are executed simultaneously, leading to faster run-time. See Section 3.4
    and 6 for more details.
  Figure 4 Link: articels_figures_by_rev_year\2020\DiCENet_DimensionWise_Convolutions_for_Efficient_Networks\figure_4.jpg
  Figure 4 caption: DiCE unit in different architecture designs for the task of image
    classification on the ImageNet dataset. Green and boxes are with and without stride,
    respectively. Here, N i =3,7,3 for i=1,2,3 . See supplementary material, which
    can be found on the Computer Society Digital Library at http:doi.ieeecomputersociety.org10.1109TPAMI.2020.3041871,
    for detailed architecture specification at different complexity levels.
  Figure 5 Link: articels_figures_by_rev_year\2020\DiCENet_DimensionWise_Convolutions_for_Efficient_Networks\figure_5.jpg
  Figure 5 caption: DiCE unit for arbitrary sized input.
  Figure 6 Link: articels_figures_by_rev_year\2020\DiCENet_DimensionWise_Convolutions_for_Efficient_Networks\figure_6.jpg
  Figure 6 caption: Implementation of dimension-wise convolution ( DimConv ). In (a),
    each kernel is applied to a pixel (represented by red dot) independently. In (b),
    all kernels are applied to a pixel simultaneously, allowing us to aggregate the
    information from tensor efficiently. Convolutional kernels are highlighted in
    color (depth-, width-, and height-wise).
  Figure 7 Link: articels_figures_by_rev_year\2020\DiCENet_DimensionWise_Convolutions_for_Efficient_Networks\figure_7.jpg
  Figure 7 caption: Impact of different components in the training of DiCENet . With
    +, we indicate that the component is added to the previous configuration. Here,
    UnOpt represents the un-optimized DiCENet trained for 150 epochs with a batch
    size of 512 with cross entropy and Opt represents DiCENet with custom CUDA kernel.
    300 ep denotes that model is trained for 300 epochs, CE-LS denotes that label-smooth
    cross-entropy is used, EMA denoted that exponential moving average is used, and
    LBSz denotes that large batch size (2048 images) is used for training. Here, inference
    time is measured on NVIDIA GTX 1080 Ti GPU and is an average across 100 trials
    for a batch of 32 RGB images, each with a spatial dimension of 224times 224 .
  Figure 8 Link: articels_figures_by_rev_year\2020\DiCENet_DimensionWise_Convolutions_for_Efficient_Networks\figure_8.jpg
  Figure 8 caption: Semantic segmentation results on the PASCAL VOC 2012 validation
    set. Here, mIOU represents mean intersection over union.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Sachin Mehta
  Name of the last author: Mohammad Rastegari
  Number of Figures: 8
  Number of Tables: 7
  Number of authors: 3
  Paper title: 'DiCENet: Dimension-Wise Convolutions for Efficient Networks'
  Publication Date: 2020-12-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison Between the DiCE Unit and Separable Convolutions
      on the ImageNet Dataset Across Different Architectures
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Evaluating DiCE Unit on the ImageNet Dataset
  Table 3 caption:
    table_text: TABLE 3 Impact of Replacing All Pointwise Convolutions With DimFuse.
      Here, DWise Denotes Depth-Wise Convolution
  Table 4 caption:
    table_text: TABLE 4 Results on the ImageNet Dataset
  Table 5 caption:
    table_text: TABLE 5 Inference Speed
  Table 6 caption:
    table_text: TABLE 6 Object Detection Results of SSD [17] With Different Backbones
      on PASCAL VOC 2007 and MS-COCO
  Table 7 caption:
    table_text: TABLE 7 Multi-Object Classification Results on the MS-COCO Dataset
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3041871
- Affiliation of the first author: multimedia and interactive computing lab, school
    of computer science and engineering, nanyang technological university, singapore,
    singapore
  Affiliation of the last author: data science & ai department, faculty of it, monash
    university, clayton, vic, australia
  Figure 1 Link: articels_figures_by_rev_year\2020\AutoEncoding_and_Distilling_Scene_Graphs_for_Image_Captioning\figure_1.jpg
  Figure 1 caption: "Illustration of incorporating the auto-encoding of scene graphs\
    \ (blue arrows) into the conventional encoder-decoder framework for image captioning\
    \ (red arrows), where the language inductive bias is encoded in the trainable\
    \ shared dictionary D . In D\u2192S , the dark dot line from the blue arrow to\
    \ the red arrow denotes the Knowledge Distillation strategy, which teaches the\
    \ encoder-decoder to reason as the auto-encoder. Word colors correspond to nodes\
    \ in image and sentence scene graphs."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\AutoEncoding_and_Distilling_Scene_Graphs_for_Image_Captioning\figure_2.jpg
  Figure 2 caption: 'Top: the conventional encoder-decoder. Bottom: our proposed encoder-decoder,
    where the novel SGAE embeds the language inductive bias in the dictionary set,
    which is shared to the image captioner, and in the decoder S -RNN of SGAE, whose
    knowledge is distilled into the decoder I -RNN of the image captioner.'
  Figure 3 Link: articels_figures_by_rev_year\2020\AutoEncoding_and_Distilling_Scene_Graphs_for_Image_Captioning\figure_3.jpg
  Figure 3 caption: "Attentional Graph Convolutional Network. In particular, it is\
    \ a spatial convolutional network, where the colored neighborhood is \u201Cconvolved\u201D\
    \ for the resultant embedding. The dashed lines in (b) and (c) mean these knowledge\
    \ flows are modulated by the soft attention weights."
  Figure 4 Link: articels_figures_by_rev_year\2020\AutoEncoding_and_Distilling_Scene_Graphs_for_Image_Captioning\figure_4.jpg
  Figure 4 caption: "The visualization of the re-encoder function R . The black dashed\
    \ block shows the operation of this function. The top part demonstrates how \u201C\
    imagination\u201D is achieved by re-encoding: the green line shows the generated\
    \ phrase by re-encoding, while the red line shows the one without re-encoding.\
    \ For convenience, we only visualize the attribute dictionary D A here."
  Figure 5 Link: articels_figures_by_rev_year\2020\AutoEncoding_and_Distilling_Scene_Graphs_for_Image_Captioning\figure_5.jpg
  Figure 5 caption: The sketch of Knowledge Distillation, where Con denotes the concatenation
    operation.
  Figure 6 Link: articels_figures_by_rev_year\2020\AutoEncoding_and_Distilling_Scene_Graphs_for_Image_Captioning\figure_6.jpg
  Figure 6 caption: Qualitative examples of different methods. The comparing methods
    are Base, the reimplemented version of Up-Down [3], MAGCN, the reimplemented version
    of GCN-LSTM [42], SGAE, the upgraded version of our preliminary work [30], and
    our SGAE-KD. For each figure, the image scene graph is pruned to avoid clutter.
    The id refers to the image id in MS-COCO. Word colors correspond to nodes in the
    detected scene graphs.
  Figure 7 Link: articels_figures_by_rev_year\2020\AutoEncoding_and_Distilling_Scene_Graphs_for_Image_Captioning\figure_7.jpg
  Figure 7 caption: Captions generated by using different language corpora. The id
    refers to the image id in MS-COCO.
  Figure 8 Link: articels_figures_by_rev_year\2020\AutoEncoding_and_Distilling_Scene_Graphs_for_Image_Captioning\figure_8.jpg
  Figure 8 caption: Two examples of unpaired image captioning. The id refers to the
    image id in MS-COCO.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Xu Yang
  Name of the last author: Jianfei Cai
  Number of Figures: 8
  Number of Tables: 7
  Number of authors: 3
  Paper title: Auto-Encoding and Distilling Scene Graphs for Image Captioning
  Publication Date: 2020-12-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Performances of Various Methods on MS-COCO Karpathy Split
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Performances of Using Different Language Corpora
  Table 3 caption:
    table_text: TABLE 3 Results of Human Evaluation, Where Each Row Compares Two Methods
  Table 4 caption:
    table_text: TABLE 4 The Performances of the Sentence Reconstruction Using Different
      Scene Graphs
  Table 5 caption:
    table_text: TABLE 5 The Performances of Various Methods on MS-COCO Karpathy Split
      Trained by Cross-Entropy Loss Only
  Table 6 caption:
    table_text: TABLE 6 The Performances of Various Methods on the Online MS-COCO
      Test Server
  Table 7 caption:
    table_text: TABLE 7 The Performances of Unpaired Image Captioning
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3042192
- Affiliation of the first author: national engineering research center for multimedia
    software, school of computer science, wuhan university, wuhan, china
  Affiliation of the last author: electronic information school, wuhan university,
    wuhan, china
  Figure 1 Link: articels_figures_by_rev_year\2020\A_Progressive_Fusion_Generative_Adversarial_Network_for_Realistic_and_Consistent\figure_1.jpg
  Figure 1 caption: Different merging schemes for time-based video SR. (a) Architecture
    of DRVSR [11]. (b) Architecture of FRVSR [12].
  Figure 10 Link: articels_figures_by_rev_year\2020\A_Progressive_Fusion_Generative_Adversarial_Network_for_Realistic_and_Consistent\figure_10.jpg
  Figure 10 caption: Cropped frames 009-013 of calendar from Vid4 [9] dataset.
  Figure 2 Link: articels_figures_by_rev_year\2020\A_Progressive_Fusion_Generative_Adversarial_Network_for_Realistic_and_Consistent\figure_2.jpg
  Figure 2 caption: Different merging schemes [9] for space-based video SR, when adopting
    five frames as input. (a) Direct fusion strategy fuses multiple frames into one
    part in the beginning. (b) Slow fusion strategy fuses frames into smaller groups
    gradually and into one piece. (c) A 3D convolution not only convolutes frames
    in the space dimension, but also in the time dimension.
  Figure 3 Link: articels_figures_by_rev_year\2020\A_Progressive_Fusion_Generative_Adversarial_Network_for_Realistic_and_Consistent\figure_3.jpg
  Figure 3 caption: Differences of non-local operation and ME&MC. Non-local operation
    tries to obtain the response at position x i by computing the weighted average
    of relationships of all possible positions x j [13]. ME&MC tries to compensate
    neighboring frames to the reference frame. More specific explanations can be found
    in Section 3.2.
  Figure 4 Link: articels_figures_by_rev_year\2020\A_Progressive_Fusion_Generative_Adversarial_Network_for_Realistic_and_Consistent\figure_4.jpg
  Figure 4 caption: "The architecture of our whole network (left) and the structure\
    \ of one PFRB (right). Note that the input number of the residual blocks can be\
    \ arbitrary, and we show the case taking five frames as input. The \u201C \u2A01\
    \ \u201D refers to element-wise adding. Black, blue, green, gray, yellow and red\
    \ arrows denote concatenation, normal convolution with 3\xD73 kernel, normal convolution\
    \ with 1\xD71 kernel, hybrid convolution, down-sample, and up-sample function,\
    \ respectively."
  Figure 5 Link: articels_figures_by_rev_year\2020\A_Progressive_Fusion_Generative_Adversarial_Network_for_Realistic_and_Consistent\figure_5.jpg
  Figure 5 caption: "Structure of one NLRB. We show the feature maps as their shapes\
    \ like T\xD7H\xD7W\xD7C , and they are reshaped if noted. The \u201C \u2A02 \u201D\
    \ represents matrix multiplication and \u201C \u2A01 \u201D represents element-wise\
    \ add."
  Figure 6 Link: articels_figures_by_rev_year\2020\A_Progressive_Fusion_Generative_Adversarial_Network_for_Realistic_and_Consistent\figure_6.jpg
  Figure 6 caption: A progressive fusion GAN framework for video SR, where the generator
    is implemented based on the progressive fusion model in Fig. 4 and the discriminator
    is composed of ESRGAN [40].
  Figure 7 Link: articels_figures_by_rev_year\2020\A_Progressive_Fusion_Generative_Adversarial_Network_for_Realistic_and_Consistent\figure_7.jpg
  Figure 7 caption: Training process for different models.
  Figure 8 Link: articels_figures_by_rev_year\2020\A_Progressive_Fusion_Generative_Adversarial_Network_for_Realistic_and_Consistent\figure_8.jpg
  Figure 8 caption: Temporal profiles of calendar from Vid4 [9] dataset.
  Figure 9 Link: articels_figures_by_rev_year\2020\A_Progressive_Fusion_Generative_Adversarial_Network_for_Realistic_and_Consistent\figure_9.jpg
  Figure 9 caption: Various numerical results for different models. (a) Training process
    for models without parameter sharing. (b) Training process for models with parameter
    sharing. (c) Training process for models incorporating multi-scale structure and
    hybrid convolutions. (d) Training process for models with different number of
    input frames.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.97
  Name of the first author: Peng Yi
  Name of the last author: Jiayi Ma
  Number of Figures: 14
  Number of Tables: 4
  Number of authors: 6
  Paper title: A Progressive Fusion Generative Adversarial Network for Realistic and
    Consistent Video Super-Resolution
  Publication Date: 2020-12-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance, Parameters, Calculation, and Testing Time Costs
      of Models With Different Fusion Strategies
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 PSNR (dB) SSIM of Different Video SR Models on Vid4 Testing
      Dataset [9] by the Upscaling Factor of 4
  Table 3 caption:
    table_text: TABLE 3 PSNR (dB) SSIM of Different Video SR Models on UDM10 Testing
      Dataset [16] by the Upscaling Factor of 4
  Table 4 caption:
    table_text: TABLE 4 PSNR (dB) SSIM of Different Video SR Models on Vimeo-90K Testing
      Dataset [31] by the Upscaling Factor of 4
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3042298
- Affiliation of the first author: media analytics and computing laboratory, department
    of artificial intelligence, school of informatics, xiamen university, xiamen,
    fujian, china
  Affiliation of the last author: school of computer science, faculty of engineering,
    university of sydney, darlington, nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2020\Fast_ClassWise_Updating_for_Online_Hashing\figure_1.jpg
  Figure 1 caption: Comparisons of the existing supervised online hashing methods
    [24], [26], [27], [30], [33], [34], [35], [36], and ours in terms of adaptivity.
    Existing methods reach the full performance only at the final training stage (solid
    rectangle box), which cannot be fast adapted online, and require a large number
    of training batches to learn up-to-date hash functions (dashed rectangle box).
    Differently, the proposed FCOH method can achieve superior adaptivity online at
    the earlier stages with much less training data.
  Figure 10 Link: articels_figures_by_rev_year\2020\Fast_ClassWise_Updating_for_Online_Hashing\figure_10.jpg
  Figure 10 caption: mAP performance with respect to different sizes of training instances
    on MNIST.
  Figure 2 Link: articels_figures_by_rev_year\2020\Fast_ClassWise_Updating_for_Online_Hashing\figure_2.jpg
  Figure 2 caption: "An illustration of the proposed class-wise updating. For an arriving\
    \ stream batch, the updating is divided into several independent subprocesses,\
    \ i.e., (a), (b), (c), and (d). In each subprocess, FCOH aims to discriminate\
    \ a particular class from the others, which is conducted sequentially ((a) \u2192\
    \ (b) \u2192 (c) \u2192 (d)). By this, the neighborhood information is well embedded\
    \ in the Hamming space."
  Figure 3 Link: articels_figures_by_rev_year\2020\Fast_ClassWise_Updating_for_Online_Hashing\figure_3.jpg
  Figure 3 caption: Analysis on the class-wise updating. The blue and green curves
    represent the loss for the first class and the second class. The orange curve
    stands for the overall loss. W t 1 is the start point for the t th updating stage.
    Without class-wise updating, W t 1 is updated for only once and the renewed weights
    are W t 2 , which is not a good choice for both the first class and second class.
    With class-wise updating, the hash weights are updated gradient-by-gradient. It
    first updates hash functions using L 1t and obtains W t 3 , based on which L 2t
    is used and then W t 4 is obtained, serving as the final weights for the t -stage
    and shows an overall better result for class 1 and class 2.
  Figure 4 Link: articels_figures_by_rev_year\2020\Fast_ClassWise_Updating_for_Online_Hashing\figure_4.jpg
  Figure 4 caption: mAP performance with respect to different sizes of training instances
    on CIFAR-10.
  Figure 5 Link: articels_figures_by_rev_year\2020\Fast_ClassWise_Updating_for_Online_Hashing\figure_5.jpg
  Figure 5 caption: PrecisionK curves of compared algorithms on CIFAR-10.
  Figure 6 Link: articels_figures_by_rev_year\2020\Fast_ClassWise_Updating_for_Online_Hashing\figure_6.jpg
  Figure 6 caption: AUC curves for mAP and PrecisionK on CIFAR-10.
  Figure 7 Link: articels_figures_by_rev_year\2020\Fast_ClassWise_Updating_for_Online_Hashing\figure_7.jpg
  Figure 7 caption: mAP performance with respect to different sizes of training instances
    on Places205.
  Figure 8 Link: articels_figures_by_rev_year\2020\Fast_ClassWise_Updating_for_Online_Hashing\figure_8.jpg
  Figure 8 caption: PrecisionK curves of compared algorithms on Places205.
  Figure 9 Link: articels_figures_by_rev_year\2020\Fast_ClassWise_Updating_for_Online_Hashing\figure_9.jpg
  Figure 9 caption: AUC curves for mAP and PrecisionK on Places205.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Mingbao Lin
  Name of the last author: Dacheng Tao
  Number of Figures: 15
  Number of Tables: 7
  Number of authors: 7
  Paper title: Fast Class-Wise Updating for Online Hashing
  Publication Date: 2020-12-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 List of Notations Used in This Paper
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Parameter Configurations on Three Benchmarks
  Table 3 caption:
    table_text: TABLE 3 mAP and PrecisionH2 Comparisons on CIFAR-10 With 8, 16, 32,
      48, 64, and 128 Bits
  Table 4 caption:
    table_text: TABLE 4 mAP1,000 and PrecisionH2 Comparisons on Places205 With 8,
      16, 32, 48, 64 and 128 Bits
  Table 5 caption:
    table_text: TABLE 5 mAP and PrecisionH2 Comparisons on MNIST With 8, 16, 32, 48,
      64 and 128 Bits
  Table 6 caption:
    table_text: TABLE 6 mAP(1,000) Results of 32-Bit for Different Variants of the
      Proposed Method on the Three Datasets
  Table 7 caption:
    table_text: TABLE 7 Time Consumption on Hash Function Updating and Hash Table
      Updating on the Three Benchmarks Under 32-Bit Hashing Codes
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3042193
- Affiliation of the first author: cripac, nlpr, institute of automation, chinese
    academy of sciences, beijing, china
  Affiliation of the last author: cripac, nlpr, institute of automation, chinese academy
    of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Learning_D_Human_Shape_and_Pose_From_Dense_Body_Parts\figure_1.jpg
  Figure 1 caption: Illustration of our main ideas. (a) A human image with a parametric
    body model. (b) Comparison of the raw RGB image, silhouette, segmentation, and
    IUV map. (c) Local visual cues are crucial for the perception of joint rotations.
    (d) Our DaNet learns 3D human shape and pose from IUV maps with decomposed perception,
    aggregated refinement, and part-based dropout strategies.
  Figure 10 Link: articels_figures_by_rev_year\2020\Learning_D_Human_Shape_and_Pose_From_Dense_Body_Parts\figure_10.jpg
  Figure 10 caption: 'Comparison of different dropping out strategies in challenging
    cases. From left to right: input images, estimated IUV maps, results of models
    trained without dropping, with Dropout, DropBlock, and PartDrop strategies.'
  Figure 2 Link: articels_figures_by_rev_year\2020\Learning_D_Human_Shape_and_Pose_From_Dense_Body_Parts\figure_2.jpg
  Figure 2 caption: Illustration of the preparation of ground truth IUV maps. (a)(b)(c)
    show the Index , U , and V values defined in DensePose [39], respectively. Note
    that the original Index values (range from 1 to 24) are also normalized into the
    [0,1] interval. (d) Generation of ground truth IUV Maps for 3D human body models.
  Figure 3 Link: articels_figures_by_rev_year\2020\Learning_D_Human_Shape_and_Pose_From_Dense_Body_Parts\figure_3.jpg
  Figure 3 caption: Overview of the proposed decompose-and-aggregate network (DaNet).
  Figure 4 Link: articels_figures_by_rev_year\2020\Learning_D_Human_Shape_and_Pose_From_Dense_Body_Parts\figure_4.jpg
  Figure 4 caption: Visualization of (a) global, (b) partial, and (c) simplified partial
    IUV maps.
  Figure 5 Link: articels_figures_by_rev_year\2020\Learning_D_Human_Shape_and_Pose_From_Dense_Body_Parts\figure_5.jpg
  Figure 5 caption: "Joint-centric RoI pooling. (a) The RoI pooling is implemented\
    \ as an STN. (b) The evolution of \u03B1 k s of different body joints over learning\
    \ iterations."
  Figure 6 Link: articels_figures_by_rev_year\2020\Learning_D_Human_Shape_and_Pose_From_Dense_Body_Parts\figure_6.jpg
  Figure 6 caption: Comparison of different dropping out strategy. (a) Original IUV
    map. (b)(c)(d) PartDrop (ours), DropBlock [60] and Dropout [58] drop IUV values
    in part-wise, block-wise, and unit-wise manners, respectively. The corresponding
    binary masks are shown on the top row.
  Figure 7 Link: articels_figures_by_rev_year\2020\Learning_D_Human_Shape_and_Pose_From_Dense_Body_Parts\figure_7.jpg
  Figure 7 caption: Illustration of the aggregated refinement module. (a) Three steps
    of the proposed refinement strategy. (b) The kinematic tree with K=24 joints in
    the SMPL model. The pelvis joint with 0 index is the root node of the tree. Joints
    belonging to the same kinematic chain are linked by the line with the same color.
    (c)(d)(e) Adjacency matrices of the graphs used in three steps for the feature
    collection, refinement, and conversion.
  Figure 8 Link: articels_figures_by_rev_year\2020\Learning_D_Human_Shape_and_Pose_From_Dense_Body_Parts\figure_8.jpg
  Figure 8 caption: Qualitative comparison of reconstruction results on the UP-3D
    dataset.
  Figure 9 Link: articels_figures_by_rev_year\2020\Learning_D_Human_Shape_and_Pose_From_Dense_Body_Parts\figure_9.jpg
  Figure 9 caption: Qualitative comparison of reconstruction results on the COCO dataset.
  First author gender probability: 0.67
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Hongwen Zhang
  Name of the last author: Zhenan Sun
  Number of Figures: 18
  Number of Tables: 10
  Number of authors: 5
  Paper title: Learning 3D Human Shape and Pose From Dense Body Parts
  Publication Date: 2020-12-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Comparison With State-of-the-Art Methods on the
      Human3.6M Dataset
  Table 10 caption:
    table_text: TABLE 10 Ablation Study of Using Learnable Graph Edge and PartDrop
      Strategies on the Human3.6M Dataset
  Table 2 caption:
    table_text: TABLE 2 Quantitative Comparison of MPJPE-PA Across Different Actions
      on the Human3.6M Dataset
  Table 3 caption:
    table_text: TABLE 3 Quantitative Comparison of PVE With State-of-the-Art Methods
      on the UP-3D Dataset
  Table 4 caption:
    table_text: TABLE 4 Quantitative Comparison of Keypoint Localization AP With State-of-the-Art
      Methods on the COCO Validation Set
  Table 5 caption:
    table_text: TABLE 5 Quantitative Comparison With State-of-the-Art Methods on the
      3DPW Dataset
  Table 6 caption:
    table_text: TABLE 6 Comparison of Running Time (ms) With State-of-the-Art Models
  Table 7 caption:
    table_text: TABLE 7 Performance of Approaches Adopting Different Intermediate
      Representations on the Human3.6M Dataset
  Table 8 caption:
    table_text: TABLE 8 Performance of Approaches Using Different Perception Strategies
      on the Human3.6M Dataset
  Table 9 caption:
    table_text: TABLE 9 Performance of Approaches Using Different Feature Refinement
      Strategies on the Human3.6M Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3042341
- Affiliation of the first author: department of electronic engineering, tsinghua
    university, beijing, china
  Affiliation of the last author: department of electronic engineering, tsinghua university,
    beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2020\BuildingFusion_SemanticAware_Structural_BuildingScale_D_Reconstruction\figure_1.jpg
  Figure 1 caption: Our results on the Lab dataset. (a) shows the colored geometry;
    (b) shows the semantic labels, following the same label coloring as Fig. 11; (c)
    shows the scene structure, where different colors represent different rooms. Grey
    means the place does not belong to any room.
  Figure 10 Link: articels_figures_by_rev_year\2020\BuildingFusion_SemanticAware_Structural_BuildingScale_D_Reconstruction\figure_10.jpg
  Figure 10 caption: Comparison of the reconstruction results with [4]. The similar
    frames that caused LCD failures for [4] are shown at the bottom right.
  Figure 2 Link: articels_figures_by_rev_year\2020\BuildingFusion_SemanticAware_Structural_BuildingScale_D_Reconstruction\figure_2.jpg
  Figure 2 caption: Overview of the hierarchical data structure of our method.
  Figure 3 Link: articels_figures_by_rev_year\2020\BuildingFusion_SemanticAware_Structural_BuildingScale_D_Reconstruction\figure_3.jpg
  Figure 3 caption: System pipeline of our collaborative reconstruction architecture.
  Figure 4 Link: articels_figures_by_rev_year\2020\BuildingFusion_SemanticAware_Structural_BuildingScale_D_Reconstruction\figure_4.jpg
  Figure 4 caption: The process of the room-level LCD, including room detection and
    segmentation (Section 3.3.1), instance feature learning (Section 3.3.2), similarity
    measure and candidate selection (Section 3.3.3), and geometry check and post-optimization
    (Section 3.3.4).
  Figure 5 Link: articels_figures_by_rev_year\2020\BuildingFusion_SemanticAware_Structural_BuildingScale_D_Reconstruction\figure_5.jpg
  Figure 5 caption: The architecture of the Embedding-Net.
  Figure 6 Link: articels_figures_by_rev_year\2020\BuildingFusion_SemanticAware_Structural_BuildingScale_D_Reconstruction\figure_6.jpg
  Figure 6 caption: Visualization of the geometry check results.
  Figure 7 Link: articels_figures_by_rev_year\2020\BuildingFusion_SemanticAware_Structural_BuildingScale_D_Reconstruction\figure_7.jpg
  Figure 7 caption: Different parts of the Lab dataset that are collaboratively reconstructed
    by different agents.
  Figure 8 Link: articels_figures_by_rev_year\2020\BuildingFusion_SemanticAware_Structural_BuildingScale_D_Reconstruction\figure_8.jpg
  Figure 8 caption: RecallN comparisons of our room-level LCD with baselines.
  Figure 9 Link: articels_figures_by_rev_year\2020\BuildingFusion_SemanticAware_Structural_BuildingScale_D_Reconstruction\figure_9.jpg
  Figure 9 caption: Nearest neighbour search of embeddings on the object test set.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Tian Zheng
  Name of the last author: Lu Fang
  Number of Figures: 18
  Number of Tables: 6
  Number of authors: 5
  Paper title: 'BuildingFusion: Semantic-Aware Structural Building-Scale 3D Reconstruction'
  Publication Date: 2020-12-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of the Results of Our Room-Level LCD With Different
      Types of Baselines (%)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Overview of the Datasets Used for Instance Segmentation, Embedding
      Learning and Room-level LCD Evaluation
  Table 3 caption:
    table_text: TABLE 3 Comparison of the Results of Our Embedding-Net Trained Under
      Different Output Dimension Settings
  Table 4 caption:
    table_text: TABLE 4 Details of Our Building-Scale Reconstruction Datasets
  Table 5 caption:
    table_text: TABLE 5 Comparison of Localization Error on Public Datasets (cm)
  Table 6 caption:
    table_text: TABLE 6 Server Execution Time on the Lab Dataset (ms)
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3042881
- Affiliation of the first author: school of computer science and engineering, south
    china university of technology, guangzhou, guangdong, china
  Affiliation of the last author: school of computer science and engineering, south
    china university of technology, guangzhou, guangdong, china
  Figure 1 Link: articels_figures_by_rev_year\2020\CrowdGAN_IdentityFree_Interactive_Crowd_Video_Generation_and_Beyond\figure_1.jpg
  Figure 1 caption: Examples of our interactive crowd video generation. Given a few
    consecutive frames of a crowd video, different future frames can be synthesized
    under the guidance of user-specified positions of pedestrians in the crowd, which
    are represented by points shown in each top-left subfigure. As observed, our method
    is able to generate various crowd behaviors without identity annotations, e.g.,
    walking towards two circles (first row) or a cluster (second row).
  Figure 10 Link: articels_figures_by_rev_year\2020\CrowdGAN_IdentityFree_Interactive_Crowd_Video_Generation_and_Beyond\figure_10.jpg
  Figure 10 caption: Progressive fine-tuning evaluation on the FDST dataset. K is
    the number of phases used in fine-tuning. T indicates the prediction steps of
    generation.
  Figure 2 Link: articels_figures_by_rev_year\2020\CrowdGAN_IdentityFree_Interactive_Crowd_Video_Generation_and_Beyond\figure_2.jpg
  Figure 2 caption: Overview of the proposed method. The generator is composed of
    three modules, Spatio-temporal Transfer Generator (STG), Point-aware Flow Predictor
    (PFP), and a self-selective fusion module. STG infers the position and shape of
    each individual based on the spatio-temporal information, while PFP preserves
    identity appearance by warping pixels according to the guidance map. The self-selective
    fusion module adaptively combines two outputs, leading to a continuous and identity-preserved
    video.
  Figure 3 Link: articels_figures_by_rev_year\2020\CrowdGAN_IdentityFree_Interactive_Crowd_Video_Generation_and_Beyond\figure_3.jpg
  Figure 3 caption: Architecture of our Spatio-temporal Transfer Generator.
  Figure 4 Link: articels_figures_by_rev_year\2020\CrowdGAN_IdentityFree_Interactive_Crowd_Video_Generation_and_Beyond\figure_4.jpg
  Figure 4 caption: "Visualization of the average attention maps. \u201C Mapi \u201D\
    \ denotes the calculated average mask of the i th attention operation during forward\
    \ inference. The darker color means the value is closer to 1 and vice versa (best\
    \ viewed when zoomed in)."
  Figure 5 Link: articels_figures_by_rev_year\2020\CrowdGAN_IdentityFree_Interactive_Crowd_Video_Generation_and_Beyond\figure_5.jpg
  Figure 5 caption: An example of the proposed self-selective fusion scheme.
  Figure 6 Link: articels_figures_by_rev_year\2020\CrowdGAN_IdentityFree_Interactive_Crowd_Video_Generation_and_Beyond\figure_6.jpg
  Figure 6 caption: Illustration of our progressive fine-tuning strategy.
  Figure 7 Link: articels_figures_by_rev_year\2020\CrowdGAN_IdentityFree_Interactive_Crowd_Video_Generation_and_Beyond\figure_7.jpg
  Figure 7 caption: Qualitative comparison on FDST dataset [55] between our method
    and related works. All the results (except for RetrospectiveGAN) are from the
    methods with the fine-tuned setting. The proposed method is able to generate longer
    and plausible crowd video with the individual appearance preserved, unlike the
    other methods suffering the severe blurry results. We highlight the details of
    generated results with our method by rectangles of different colors.
  Figure 8 Link: articels_figures_by_rev_year\2020\CrowdGAN_IdentityFree_Interactive_Crowd_Video_Generation_and_Beyond\figure_8.jpg
  Figure 8 caption: Long-term comparison results on FDST dataset between our method
    and related works.
  Figure 9 Link: articels_figures_by_rev_year\2020\CrowdGAN_IdentityFree_Interactive_Crowd_Video_Generation_and_Beyond\figure_9.jpg
  Figure 9 caption: Qualitative comparison of the results generated by CrowdGAN and
    its ablative variants.
  First author gender probability: 0.74
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Liangyu Chai
  Name of the last author: Shengfeng He
  Number of Figures: 16
  Number of Tables: 3
  Number of authors: 5
  Paper title: 'CrowdGAN: Identity-Free Interactive Crowd Video Generation and Beyond'
  Publication Date: 2020-12-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Evaluations of Multi-Step Crowd Generation on
      Testing Set of FDST [55]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Ablation Study on FDST Dataset Where T is the Time Steps of
      Prediction
  Table 3 caption:
    table_text: TABLE 3 Data augmentation Comparisons on the Mall [57] (Upper Half)
      and FDST [55] (Bottom Half)
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3043372
- Affiliation of the first author: sun yat-sen university, guangzhou, china
  Affiliation of the last author: sun yat-sen university, guangzhou, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Graphonomy_Universal_Image_Parsing_via_Graph_Reasoning_and_Transfer\figure_1.jpg
  Figure 1 caption: "With huge different granularity and quantity of semantic labels,\
    \ image parsing is isolated into multiple level tasks that hinder the model generation\
    \ capability and data annotation utilization. For example, the head region on\
    \ a dataset is further annotated into several fine-grained concepts on another\
    \ dataset, such as hat, hair, and face. However, different semantic parts still\
    \ have some intrinsic and hierarchical relations (e.g., Head includes the face.\
    \ Face is next to hair), which can be encoding as intra-graph and inter-graph\
    \ connections for better information propagation. To alleviate the label discrepancy\
    \ issue and take advantage of their semantic correlations, we introduce a learning\
    \ framework, named as \u201CGraphonomy,\u201D which models the global semantic\
    \ coherency in multiple domains via graph transfer learning to achieve multiple\
    \ levels of human parsing tasks. For clarity, we only show a portion of labels\
    \ and connections."
  Figure 10 Link: articels_figures_by_rev_year\2020\Graphonomy_Universal_Image_Parsing_via_Graph_Reasoning_and_Transfer\figure_10.jpg
  Figure 10 caption: Visualized comparison of panopic scene segmentation results on
    ADE20K dataset [50].
  Figure 2 Link: articels_figures_by_rev_year\2020\Graphonomy_Universal_Image_Parsing_via_Graph_Reasoning_and_Transfer\figure_2.jpg
  Figure 2 caption: The overview of our Graphonomy that tackles the universal parsing
    via graph reasoning and transfer. The parsing model can be trained across domains
    (e.g.relevant but different tasks or datasets) with discrepant semantic labels.
  Figure 3 Link: articels_figures_by_rev_year\2020\Graphonomy_Universal_Image_Parsing_via_Graph_Reasoning_and_Transfer\figure_3.jpg
  Figure 3 caption: Illustration of our Graphonomy that tackles universal human parsing
    via graph reasoning and transfer for achieving multiple levels of human parsing.
    The annotations from different datasets can be thus integrated for training the
    universal parsing model. The image features extracted by deep convolutional networks
    are projected into a semantic graph with its nodes and edges defined according
    to prior knowledge (i.e., human body structures). The global reasoning is implemented
    within the graph of each domain by the Intra-Graph Reasoning module for enhancing
    the discriminability of visual features. While the graphs across domains are further
    fused via the Inter-Graph Transfer module, in which the hierarchical label correlation
    is employed for alleviating the label discrepancy across different datasets. During
    training, our Graphonomy can take advantage of annotated data with different granularity.
    For inference, the trained model can generate different levels of human parsing
    results taking an arbitrary image as input.
  Figure 4 Link: articels_figures_by_rev_year\2020\Graphonomy_Universal_Image_Parsing_via_Graph_Reasoning_and_Transfer\figure_4.jpg
  Figure 4 caption: Examples of the definite connections between each two human body
    parts, which is the foundation to encode the relations between two semantic nodes
    in the graph for reasoning. Two nodes co-relates if they are connected by a white
    line.
  Figure 5 Link: articels_figures_by_rev_year\2020\Graphonomy_Universal_Image_Parsing_via_Graph_Reasoning_and_Transfer\figure_5.jpg
  Figure 5 caption: Illustration of applying our Graphonomy to panoptic scene segmentation.
    Each task (i.e., instance-level thing segmentation or pixel-wise segmentation
    of background stuff) is treated as one domain, and our framework exploits the
    semantics-aware dependencies across domains in an explicit way. The implementation
    of graph construct is modified based on the version used for human parsing, and
    the other components are basically kept. By analogy, our framework can be easily
    extneded into other similar scene understanding problem involving multiple co-related
    tasks.
  Figure 6 Link: articels_figures_by_rev_year\2020\Graphonomy_Universal_Image_Parsing_via_Graph_Reasoning_and_Transfer\figure_6.jpg
  Figure 6 caption: An example of the generated semantics-aware graph across the two
    tasks, i.e., instance-aware object segmentation and the stuff segmentation. The
    input image is shown at the bottom, and the graphs of the two tasks are shown
    in the up right and left, respectively.
  Figure 7 Link: articels_figures_by_rev_year\2020\Graphonomy_Universal_Image_Parsing_via_Graph_Reasoning_and_Transfer\figure_7.jpg
  Figure 7 caption: Examples of different levels of human parsing results generated
    by our universal human parsing model. We can observe that our model is able to
    generates precise and fine-grained results for different levels of human parsing
    tasks by distilling universal semantic graph representation.
  Figure 8 Link: articels_figures_by_rev_year\2020\Graphonomy_Universal_Image_Parsing_via_Graph_Reasoning_and_Transfer\figure_8.jpg
  Figure 8 caption: Visualized comparison of human parsing results on PASCAL-Person-Part
    dataset [48] (Left) and CIHP dataset [45] (Right).
  Figure 9 Link: articels_figures_by_rev_year\2020\Graphonomy_Universal_Image_Parsing_via_Graph_Reasoning_and_Transfer\figure_9.jpg
  Figure 9 caption: Visualized results predicted on (a) ATR dataset [9] (a) and (b)
    MHP dataset [7].
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Liang Lin
  Name of the last author: Xiaodan Liang
  Number of Figures: 10
  Number of Tables: 13
  Number of authors: 5
  Paper title: 'Graphonomy: Universal Image Parsing via Graph Reasoning and Transfer'
  Publication Date: 2020-12-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Human Parsing Performance With Several State-of-the-Art
      Methods on PASCAL-Person-Part Dataset [48]
  Table 10 caption:
    table_text: TABLE 10 Comparison of Human Parsing Performance With the Different
      Backbone on Different Datasets
  Table 2 caption:
    table_text: TABLE 2 Human Parsing Result on ATR Dataset [10]
  Table 3 caption:
    table_text: TABLE 3 Performance Comparison With State-of-the-Art Methods on CIHP
      Dataset [45]
  Table 4 caption:
    table_text: TABLE 4 Ablation Experiments on PASCAL-Person-Part Dataset [48]
  Table 5 caption:
    table_text: TABLE 5 Evaluation Results of Our Graphonomy When Training on Different
      Number of Data on PASCAL-Person-Part Dataset [48], in Terms of Mean IoU(%)
  Table 6 caption:
    table_text: TABLE 6 Comparison of Human Parsing Performance on MHP Dataset [7]
  Table 7 caption:
    table_text: TABLE 7 Evaluation Results of Our Graphonomy Training in an Incremental
      Way to Extend the Model Capacity, in Terms of Mean IoU(%)
  Table 8 caption:
    table_text: TABLE 8 Comparison of Human Parsing Performance With Several Graph
      Convolution Layers of Our Proposed Intra-Graph Reasoning on PASCAL-Person-Part
      Dataset [48]
  Table 9 caption:
    table_text: TABLE 9 Evaluations of Human Parsing on Transferring From Different
      Source Datasets
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3043268
- Affiliation of the first author: "laboratoire mia, math\xE9matiques, image et applications,\
    \ la rochelle universit\xE9, la rochelle, france"
  Affiliation of the last author: "laboratoire mia, math\xE9matiques, image et applications,\
    \ la rochelle universit\xE9, la rochelle, france"
  Figure 1 Link: articels_figures_by_rev_year\2020\Graph_Moving_Object_Segmentation\figure_1.jpg
  Figure 1 caption: 'Comparisons of the visual results of the proposed Graph Moving
    Object Segmentation (GraphMOS) and Graph Video Object Segmentation (GraphVOS)
    algorithms with existing SOTA methods on five MOS and two VOS challenging video
    sequences taken from CDNet2014 [7], UCSD [14], DAVIS2016 [15], and Youtube-VOS
    [16] datasets. The compared methods for MOS are: PAWCS [17], IUTIS-5 [18], BSUV-Net
    [19], ROSL [20], and DECOLOR [21]; while the compared methods for VOS are: OSVOS
    S [22], RGMP [23], OnAVOS [24], STM [25], and Siam R-CNN [26]. Our proposed algorithms
    perform significantly better than the compared methods in these challenging sequences.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Graph_Moving_Object_Segmentation\figure_2.jpg
  Figure 2 caption: The pipeline of the MOS algorithm with the reconstruction of graph
    signals. The algorithm uses background initialization and superpixel segmentation
    [74], [75]. Each superpixel represents a node in a graph, and the representation
    of each node is obtained with motion, intensity, texture, and deep features. The
    ground-truth is used to decide if a node is a moving (green nodes) or a static
    object (blue nodes). Black nodes correspond to the non-labeled images in the dataset.
    Finally, some nodes are sampled and the semi-supervised algorithm reconstructs
    all the labels in the graph.
  Figure 3 Link: articels_figures_by_rev_year\2020\Graph_Moving_Object_Segmentation\figure_3.jpg
  Figure 3 caption: Results of the semantic, instance, and superpixel segmentation
    using DeepLab [78], Mask R-CNN [79], and SLIC [74] methods on the sequence fall
    taken from the CDNet2014 dataset. The green-colored cars in (b), instances in
    different colors in (c), and homogeneous regions in (d) represent the nodes of
    the graph.
  Figure 4 Link: articels_figures_by_rev_year\2020\Graph_Moving_Object_Segmentation\figure_4.jpg
  Figure 4 caption: Procedure to represent the nodes of the graph with a Mask R-CNN
    as backbone. Each mask of the segmented image represents a node in the graph,
    and the representation of the node is achieved with intensity, optical flow, texture,
    and deep features.
  Figure 5 Link: articels_figures_by_rev_year\2020\Graph_Moving_Object_Segmentation\figure_5.jpg
  Figure 5 caption: "Example of the sets involved in the construction of the graph\
    \ signal when the segmentation algorithm is a Mask R-CNN. Yellow pixels are GT\
    \ t \u2229 P v , red pixels are P v \u2212 GT t , and the green pixels are GT\
    \ t \u2212 P v ."
  Figure 6 Link: articels_figures_by_rev_year\2020\Graph_Moving_Object_Segmentation\figure_6.jpg
  Figure 6 caption: Example of elementary frequencies obtained from the Laplacian
    matrix on a sensor network of N=500 . Each graph shows a frequency lambda i with
    its corresponding eigenvector. The lowest frequency is lambda 1=0 , corresponding
    to a constant graph signal, i.e., the Laplacian quadratic form of eigenvector
    mathbf u1 is given such that mathbf u1mathsf Tmathbf Lu1=lambda 1=0 .
  Figure 7 Link: articels_figures_by_rev_year\2020\Graph_Moving_Object_Segmentation\figure_7.jpg
  Figure 7 caption: 'Results of the experiment related to the sample complexity. Left:
    power spectrum of the graph signal hat mathbfy2 related the moving objects, right:
    classification error versus sample size of the semi-supervised learning algorithm.'
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Jhony H. Giraldo
  Name of the last author: Thierry Bouwmans
  Number of Figures: 7
  Number of Tables: 12
  Number of authors: 3
  Paper title: Graph Moving Object Segmentation
  Publication Date: 2020-12-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of the Parameters Used in Our Experiments for Each
      Dataset
  Table 10 caption:
    table_text: TABLE 10 Performance Comparison in Terms of Average F-Measure Score
      of Superpixel and Block-Based Segmentation for Graph Construction Methods
  Table 2 caption:
    table_text: TABLE 2 Comparison of the Qualitative Results of GraphMOS and GraphVOS
      Algorithms on CDNet2014, UCSD, DAVIS2016, and Youtube-VOS Datasets With Existing
      SOTA Methods
  Table 3 caption:
    table_text: TABLE 3 Comparisons of Average F-Measure on CDNet2014 Dataset
  Table 4 caption:
    table_text: TABLE 4 Comparison of F-Measure Results Over the Sequences of I2R
      Dataset
  Table 5 caption:
    table_text: TABLE 5 Comparison of F-Measure Results Over the Sequences of SBI2015
      Dataset
  Table 6 caption:
    table_text: TABLE 6 Comparison of F-Measure Results Over the Videos of UCSD Background
      Subtraction Dataset
  Table 7 caption:
    table_text: TABLE 7 Comparison of J J and F F Metrics on the DAVIS2016 Validation
      Set
  Table 8 caption:
    table_text: TABLE 8 Comparison of J J and F F Metrics on the Youtube-VOS Validation
      Set
  Table 9 caption:
    table_text: TABLE 9 Performance Comparisons in Terms of Average F-Measure Score
      for Different Segmentation Methods Used for Graph Construction
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3042093
