- Affiliation of the first author: department of computer science and engineering,
    michigan state university, east lansing, mi
  Affiliation of the last author: department of computer science and engineering,
    michigan state university, east lansing, mi
  Figure 1 Link: articels_figures_by_rev_year\2016\Adaptive_D_Face_Reconstruction_from_Unconstrained_Photo_Collections\figure_1.jpg
  Figure 1 caption: The proposed system reconstructs a detailed 3D face model of the
    individual, adapting to the number and quality of photos provided.
  Figure 10 Link: articels_figures_by_rev_year\2016\Adaptive_D_Face_Reconstruction_from_Unconstrained_Photo_Collections\figure_10.jpg
  Figure 10 caption: Sample rendering used for human perception experiment.
  Figure 2 Link: articels_figures_by_rev_year\2016\Adaptive_D_Face_Reconstruction_from_Unconstrained_Photo_Collections\figure_2.jpg
  Figure 2 caption: Overview of face reconstruction. Given a photo collection, we
    apply landmark alignment and use a 3DMM to create a personalized template. Then
    a coarse-to-fine process alternates between normal estimation and surface reconstruction.
  Figure 3 Link: articels_figures_by_rev_year\2016\Adaptive_D_Face_Reconstruction_from_Unconstrained_Photo_Collections\figure_3.jpg
  Figure 3 caption: The landmark marching process. (a) internal (green) landmarks
    and external (red) defined paths; (b) estimated face and pose; (c) face with roll
    rotation removed; (d) landmarks without marching; and (e) landmarks after marching
    corresponding to 2D image alignment.
  Figure 4 Link: articels_figures_by_rev_year\2016\Adaptive_D_Face_Reconstruction_from_Unconstrained_Photo_Collections\figure_4.jpg
  Figure 4 caption: Effect on albedo estimation with (a) and without (b) dependability.
    Skin should have a consistent albedo, but without dependability the cheek shows
    ghosting effects from misalignment.
  Figure 5 Link: articels_figures_by_rev_year\2016\Adaptive_D_Face_Reconstruction_from_Unconstrained_Photo_Collections\figure_5.jpg
  Figure 5 caption: Raw image, synthetic image under estimated lighting conditions,
    and SSIM used for local selection. Brighter indicates higher SSIM.
  Figure 6 Link: articels_figures_by_rev_year\2016\Adaptive_D_Face_Reconstruction_from_Unconstrained_Photo_Collections\figure_6.jpg
  Figure 6 caption: The mean curvature normal indicates how a vertex deviates from
    the average location of its immediate neighbors, which can be evaluated as the
    Laplacian of the position. The mean curvature H j can be evaluated through n .
  Figure 7 Link: articels_figures_by_rev_year\2016\Adaptive_D_Face_Reconstruction_from_Unconstrained_Photo_Collections\figure_7.jpg
  Figure 7 caption: Qualitative evaluation of 16 individuals from Internet photo collections.
    Note the diversity in ages, ethnicities and genders.
  Figure 8 Link: articels_figures_by_rev_year\2016\Adaptive_D_Face_Reconstruction_from_Unconstrained_Photo_Collections\figure_8.jpg
  Figure 8 caption: Synthetic data with lighting (top), pose (middle), and expression
    (bottom) variation.
  Figure 9 Link: articels_figures_by_rev_year\2016\Adaptive_D_Face_Reconstruction_from_Unconstrained_Photo_Collections\figure_9.jpg
  Figure 9 caption: Qualitative comparison on celebrities. The proposed approach incorporates
    more of the sides of the face and neck.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.84
  Name of the first author: Joseph Roth
  Name of the last author: Xiaoming Liu
  Number of Figures: 17
  Number of Tables: 6
  Number of authors: 3
  Paper title: Adaptive 3D Face Reconstruction from Unconstrained Photo Collections
  Publication Date: 2016-12-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Overview of Face Reconstruction Approaches
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Notations
  Table 3 caption:
    table_text: TABLE 3 Synthetic Surface-to-Surface Error
  Table 4 caption:
    table_text: TABLE 4 Local Selection Error
  Table 5 caption:
    table_text: TABLE 5 SSIM Radius Error
  Table 6 caption:
    table_text: TABLE 6 Personal Collection Adaptability
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2636829
- Affiliation of the first author: general electric global research, 1 research circle,
    niskayuna, ny
  Affiliation of the last author: department of electrical and computer engineering,
    university of maryland, college park, md
  Figure 1 Link: articels_figures_by_rev_year\2016\Submodular_Attribute_Selection_for_Visual_Recognition\figure_1.jpg
  Figure 1 caption: Exemplar images of two classes and their associated attribute
    sets from the Animals with Attributes dataset and UCF101 dataset.
  Figure 10 Link: articels_figures_by_rev_year\2016\Submodular_Attribute_Selection_for_Visual_Recognition\figure_10.jpg
  Figure 10 caption: Exemplar frames of four action classes from the Olympic Sports
    dataset and UCF101 dataset respectively.
  Figure 2 Link: articels_figures_by_rev_year\2016\Submodular_Attribute_Selection_for_Visual_Recognition\figure_2.jpg
  Figure 2 caption: An example of entropy rate-based attribute selection. (a) Three
    examples of vector r corresponding to three different subsets. (b)(c)(d) The corresponding
    undirected graphs constructed from three examples of vector r . We show the role
    of the entropy rate in selecting attributes which have large and similar discrimination
    capability for each pair of classes. The circles with numbers denote the corresponding
    class vertices and the numbers next to the edge denote the edge weights, which
    is a measure of the discrimination capability of selected attribute subset. The
    self-loops are not displayed. Before selection, w 1,2 =5, w 1,3 =4, w 1,4 =6,
    w 2,3 =6, w 2,4 =4, w 3,4 =5 , so w 1 = w 2 = w 3 = w 4 =15 . The stationary distribution
    is r=[ 1 4 , 1 4 , 1 4 , 1 4 ] . For the selected S 1 , w 1,1 =12, w 1,2 = w 1,3
    = w 1,4 =1 and P 1,1 = 4 5 , P 1,2 = P 1,3 = P 1,4 = 1 15 . Other w i,j can be
    calculated in the same way and the entropy rate is calculated according to (7)
    for the selected S 1 . The entropy rate of the graph with large edge weights in
    (c) has a higher objective value than that of a graph with smaller edge weights
    in (b). The entropy rate of graph with equal edge weights in (c) has a higher
    objective value than that of the graph with different edge weights in (d).
  Figure 3 Link: articels_figures_by_rev_year\2016\Submodular_Attribute_Selection_for_Visual_Recognition\figure_3.jpg
  Figure 3 caption: The coverage graph constructed based on Table 4. We show the role
    of weighted maximum coverage term in selecting attributes which have large coverage
    weights. Two numbers separated by a forward backslash in the top circles denote
    a pair of classes, while the bottom circles denote different attributes. The number
    next to one edge is the coverage weight associated with the class pair when covered
    by the corresponding attribute. The edge which provides maximum coverage weight
    for each class pair is in red color. We consider three attribute subsets S 1 =
    a 1 , a 2 , S 2 = a 1 , a 3 , S 3 = a 1 , a 4 . S 2 has a higher objective value
    than S 1 and S 3 because the sum of maximum coverage weights for all class pairs
    obtained using attributes from subset S 2 is largest.
  Figure 4 Link: articels_figures_by_rev_year\2016\Submodular_Attribute_Selection_for_Visual_Recognition\figure_4.jpg
  Figure 4 caption: Recognition results by different submodular methods on the AwA
    dataset. The number of training images percategory is 15.
  Figure 5 Link: articels_figures_by_rev_year\2016\Submodular_Attribute_Selection_for_Visual_Recognition\figure_5.jpg
  Figure 5 caption: Recognition results by different submodular methods on the AwA
    datset. The number of training images percategory is 25.
  Figure 6 Link: articels_figures_by_rev_year\2016\Submodular_Attribute_Selection_for_Visual_Recognition\figure_6.jpg
  Figure 6 caption: Recognition results by different submodular methods on the AwA
    datset. The number of training images percategory is 50.
  Figure 7 Link: articels_figures_by_rev_year\2016\Submodular_Attribute_Selection_for_Visual_Recognition\figure_7.jpg
  Figure 7 caption: The effect of lambda on the performance of the proposed approach
    on the AwA dataset when the number of training images percategory is 15, 25 and
    50 respectively.
  Figure 8 Link: articels_figures_by_rev_year\2016\Submodular_Attribute_Selection_for_Visual_Recognition\figure_8.jpg
  Figure 8 caption: Recognition results by different submodular methods on the aPascal
    dataset.
  Figure 9 Link: articels_figures_by_rev_year\2016\Submodular_Attribute_Selection_for_Visual_Recognition\figure_9.jpg
  Figure 9 caption: Sparse codes of class 6 and 16 before and after selection respectively.
    (a),(c) The sparse codes in red correspond to the sub-dictionary D6 . (b),(d)
    The sparse codes in read correspond to the sub-dictionary D16 .
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.62
  Name of the first author: Jingjing Zheng
  Name of the last author: Rama Chellappa
  Number of Figures: 12
  Number of Tables: 13
  Number of authors: 3
  Paper title: Submodular Attribute Selection for Visual Recognition
  Publication Date: 2016-12-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 An Example of Attribute Assignment Matrix
  Table 10 caption:
    table_text: TABLE 10 Recognition Results of Different Approaches
  Table 2 caption:
    table_text: TABLE 2 An Example of Attribute Contribution Matrix
  Table 3 caption:
    table_text: TABLE 3 Three Examples of Vector r Corresponding to Three Different
      Selected Subsets
  Table 4 caption:
    table_text: TABLE 4 Attribute Contribution Matrix
  Table 5 caption:
    table_text: TABLE 5 Recognition Accuracy on the AwA Dataset Using Human-Labeled
      Attributes
  Table 6 caption:
    table_text: TABLE 6 Recognition Accuracy on the AwA Dataset Using Data-Driven
      Attributes
  Table 7 caption:
    table_text: TABLE 7 Recognition Accuracy on the AwA Dataset Using the Mixed Attribute
      Set
  Table 8 caption:
    table_text: TABLE 8 Recognition Accuracy of Different Comparing Methods on the
      AwA Dataset
  Table 9 caption:
    table_text: TABLE 9 Recognition Results of Different Attribute-Based Representations
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2636827
- Affiliation of the first author: school of mathematical sciences, tel aviv university,
    ramat aviv, israel
  Affiliation of the last author: school of mathematical sciences, tel aviv university,
    ramat aviv, israel
  Figure 1 Link: articels_figures_by_rev_year\2016\CrossValidated_Variable_Selection_in_TreeBased_Methods_Improves_Predictive_Perfo\figure_1.jpg
  Figure 1 caption: Predictive performance of the two CART variants and ALOOF for
    the model defined in (1).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\CrossValidated_Variable_Selection_in_TreeBased_Methods_Improves_Predictive_Perfo\figure_2.jpg
  Figure 2 caption: Tree-based modeling in the presence of an uninformative categorical
    feature. The plot on the left is the MSE while the plot on the right is the elapsed
    time. The legend applies for both plots
  Figure 3 Link: articels_figures_by_rev_year\2016\CrossValidated_Variable_Selection_in_TreeBased_Methods_Improves_Predictive_Perfo\figure_3.jpg
  Figure 3 caption: Modeling complexity (represented in df ) and its corresponding
    MSE in the presence of uninformative feature with K different categories. The
    straight line at the bottom is to emphasize that the MSE achieve by ALOOF is uniformly
    lower than CART.
  Figure 4 Link: articels_figures_by_rev_year\2016\CrossValidated_Variable_Selection_in_TreeBased_Methods_Improves_Predictive_Perfo\figure_4.jpg
  Figure 4 caption: Variable importance distribution boxplots for CART RF, CIT RF
    and ALOOF RF. The left column corresponds to subsampling while the right column
    is bootstrap sampling.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Amichai Painsky
  Name of the last author: Saharon Rosset
  Number of Figures: 4
  Number of Tables: 4
  Number of authors: 2
  Paper title: Cross-Validated Variable Selection in Tree-Based Methods Improves Predictive
    Performance
  Publication Date: 2016-12-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Misclassification Results on the Melbourne Grant Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Regression Real-World Data Experiments on Small Datasets with
      Large- K Variables
  Table 3 caption:
    table_text: TABLE 3 Regression Real-World Experiments
  Table 4 caption:
    table_text: TABLE 4 Classification Real-World Data Experiments
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2636831
- Affiliation of the first author: cooperative medianet innovation center and the
    shanghai key laboratory of multimedia processing and transmissions, shanghai jiao
    tong university, shanghai, china
  Affiliation of the last author: school of information technologies and the faculty
    of engineering and information technologies, university of sydney, darlington,
    nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2016\WeblySupervised_FineGrained_Visual_Categorization_via_Deep_Domain_Adaptation\figure_1.jpg
  Figure 1 caption: "The proposed semi-supervised method via web data. A strongly\
    \ supervised dataset is introduced to \u201Cteach\u201D web images how to learn\
    \ properly."
  Figure 10 Link: articels_figures_by_rev_year\2016\WeblySupervised_FineGrained_Visual_Categorization_via_Deep_Domain_Adaptation\figure_10.jpg
  Figure 10 caption: 'Visualization of classification using the proposed method with
    a root and two parts: head and body. (a) Test image with a ground-truth label
    of 80. (b) Activation map for the three detectors. (c) Located part bounding boxes.
    The top 6 nearest neighbors for the detected parts from the training images are
    shown in (d)-(f). The original part-based R-CNN method using training data only
    misclassifies the test image into class 81, as shown in (d). Green boxes demonstrate
    the image patches of label 80, and red boxes for label 81. After re-fine-tuning
    part-CNNs with the augmented training set, the new feature representations guarantee
    that the test image is correctly classified. (e) Nearest neighbors from the strongly
    supervised training set only using the new feature representations. (f) Results
    after putting weakly supervised images into the training set. Yellow boxes indicate
    images in the weakly supervised dataset with label 80. (g) and (h) show typical
    training images from class 80 (GreenKingfisher) and 81 (PiedKingfisher), respectively.'
  Figure 2 Link: articels_figures_by_rev_year\2016\WeblySupervised_FineGrained_Visual_Categorization_via_Deep_Domain_Adaptation\figure_2.jpg
  Figure 2 caption: Demonstration of the properties of web images for fine-grained
    object recognition.
  Figure 3 Link: articels_figures_by_rev_year\2016\WeblySupervised_FineGrained_Visual_Categorization_via_Deep_Domain_Adaptation\figure_3.jpg
  Figure 3 caption: Flowchart of the proposed algorithm. The employed strongly supervised
    algorithm for initialization is shown in the right, which provides multiple forms
    of domain knowledge to facilitate webly supervised learning procedure. The learning
    process could be conducted several iterations via self-paced learning.
  Figure 4 Link: articels_figures_by_rev_year\2016\WeblySupervised_FineGrained_Visual_Categorization_via_Deep_Domain_Adaptation\figure_4.jpg
  Figure 4 caption: "Detection results on weakly supervised images. Green frames indicate\
    \ the detected bounding box for the part \u201Cbody\u201D. Image labels in the\
    \ top two rows are correctly classified; the bottom two rows show cases in which\
    \ classification has failed. Beyond the classification results, part patches in\
    \ rows 1 and 3 are associated with high detection scores, while rows 2 and 4 have\
    \ low detection scores. We propose to use both the classification and detection\
    \ scores to select valid part patches and augment the training data."
  Figure 5 Link: articels_figures_by_rev_year\2016\WeblySupervised_FineGrained_Visual_Categorization_via_Deep_Domain_Adaptation\figure_5.jpg
  Figure 5 caption: 'Examples of detected part patches from web images selected as
    valid training patches. From top to bottom: whole object, head, body. The leftmost
    five columns show top-scoring detections, while the right two columns show patches
    with the lowest detection scores.'
  Figure 6 Link: articels_figures_by_rev_year\2016\WeblySupervised_FineGrained_Visual_Categorization_via_Deep_Domain_Adaptation\figure_6.jpg
  Figure 6 caption: Demonstration of the impact of different CNN architectures.
  Figure 7 Link: articels_figures_by_rev_year\2016\WeblySupervised_FineGrained_Visual_Categorization_via_Deep_Domain_Adaptation\figure_7.jpg
  Figure 7 caption: Number of web images per category used for training versus classification
    accuracy. Red lines show results for the proposed method; blue lines indicate
    the baseline webly-supervised method using only image labels and web images. We
    also show results using only the strongly supervised dataset as reference.
  Figure 8 Link: articels_figures_by_rev_year\2016\WeblySupervised_FineGrained_Visual_Categorization_via_Deep_Domain_Adaptation\figure_8.jpg
  Figure 8 caption: "Classification accuracy using different forms of knowledge transfer\
    \ in multiple experimental settings. \u201CStrong+Weak(100)\u201D means training\
    \ models using the original CUB-200-2011 dataset and a web dataset with 100 images\
    \ per category. \u201CWeak\u201D, \u201Cimage\u201D, and \u201Cpart\u201D indicate\
    \ the three different forms of knowledge transfer detailed in the main text."
  Figure 9 Link: articels_figures_by_rev_year\2016\WeblySupervised_FineGrained_Visual_Categorization_via_Deep_Domain_Adaptation\figure_9.jpg
  Figure 9 caption: "Visualization of detection and classification results on the\
    \ CUB-200-2011 test set (rows 1-3), web images (rows 4-6), and the NAbirds dataset\
    \ (rows 6-9). The last row in each group shows examples with inaccurate detection\
    \ results. For classification results, green marks indicate successful cases,\
    \ while red ones denote misclassifications. Object bounding boxes together with\
    \ \u201Chead\u201D and \u201Cbody\u201D are shown by rectangles in green, red,\
    \ and blue, respectively. Specifically, part detections with low scores are not\
    \ visualized. Note that some web images are regarded as noise by our method (blue\
    \ signs). Viewing digitally with zoom is recommended."
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Zhe Xu
  Name of the last author: Dacheng Tao
  Number of Figures: 10
  Number of Tables: 7
  Number of authors: 4
  Paper title: Webly-Supervised Fine-Grained Visual Categorization via Deep Domain
    Adaptation
  Publication Date: 2016-12-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Part Localization Accuracy in Terms of PCP on the CUB-200-2011
      Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparisons of Methods on the CUB-200-2011 Dataset
  Table 3 caption:
    table_text: TABLE 3 CUB-200-2011 Ablation Study to Investigate Different Fine-Tuning,
      Classifier, Detector, and Denoising Strategies
  Table 4 caption:
    table_text: TABLE 4 Effect of Self-Paced Learning at Multiple Iterations
  Table 5 caption:
    table_text: TABLE 5 Demonstration of the Impact of Less Strongly Supervised Images
  Table 6 caption:
    table_text: TABLE 6 Comparison of Classification Accuracy on the NAbirds Dataset
  Table 7 caption:
    table_text: TABLE 7 Comparison of Classification Accuracy on the Clothing Dataset
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2637331
- Affiliation of the first author: "wearable computing lab, eth z\xFCrich, switzerland"
  Affiliation of the last author: "wearable computing lab, eth z\xFCrich, switzerland"
  Figure 1 Link: articels_figures_by_rev_year\2016\Supporting_OneTime_Point_Annotations_for_Gesture_Recognition\figure_1.jpg
  Figure 1 caption: "Data processing flow of the proposed boundary fixing approach.\
    \ An example is shown on the right. Three annotated gestures \u201Copen door\u201D\
    , \u201Cdrink\u201D, and \u201Ctoggle switch\u201D with their one-time point annotations\
    \ indicated as black arrows are given in the training data. In this example, the\
    \ maximum length of \u201Cdrink\u201D gesture is given and it is used to initialize\
    \ the boundaries of the \u201Cdrink\u201D gesture. \u220B represents non-motion\
    \ symbols."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Supporting_OneTime_Point_Annotations_for_Gesture_Recognition\figure_2.jpg
  Figure 2 caption: "Illustration of an alignment between two strings \u201CBBCED\u201D\
    \ and \u201CACFFFD\u201D and the corresponding warping path starting from the\
    \ first indices at the top left to the last indices at the bottom right. In this\
    \ example, two symbols \u201CB\u201D of the first string are warped and so are\
    \ three symbols \u201CF\u201D of the second string."
  Figure 3 Link: articels_figures_by_rev_year\2016\Supporting_OneTime_Point_Annotations_for_Gesture_Recognition\figure_3.jpg
  Figure 3 caption: "Illustration of BoundarySearch algorithm between two \u201Cdrink\u201D\
    \ gestures with incorrectly initialized boundaries \u201CTXDRINKUPTOGGLES\u201D\
    \ and \u201COGGLESWDRINKA\u201D, and the corresponding dependency graph. The initialized\
    \ boundaries wrongly cover some parts of \u201Ctoggle switch\u201D gesture. Here\
    \ we assume the distance between two symbols is their difference in encoded English\
    \ alphabets (A-Z are converted to 0-25) and the penalty p is 0.3. In the dependency\
    \ graph, the horizontal or vertical brown arrows show warpings in one string,\
    \ meanwhile the diagonal arrows show the alignments between elements of two strings.\
    \ Two black arrows indicate the one-time point annotations of the gestures. Two\
    \ clusters of connected scores show the longest matching substrings (\u201CDRINKUP\u201D\
    \ with \u201CDRINKA\u201D, and \u201COGGLES\u201D with \u201COGGLESW\u201D). The\
    \ targeted cluster on the bottom left with the highest similarity score M = 5\
    \ spans the marked annotation. Hence, the fixed boundaries of two gestures are\
    \ shown in green bars."
  Figure 4 Link: articels_figures_by_rev_year\2016\Supporting_OneTime_Point_Annotations_for_Gesture_Recognition\figure_4.jpg
  Figure 4 caption: Illustrations of boundary jitter in fixed annotations. GT stands
    for ground truth. The blue dash-dotted lines indicate the correct boundary of
    a gesture.
  Figure 5 Link: articels_figures_by_rev_year\2016\Supporting_OneTime_Point_Annotations_for_Gesture_Recognition\figure_5.jpg
  Figure 5 caption: "The analysis on jitter levels of annotations before and after\
    \ fixing in both cases NoLen and MaxLen. (a) Shows extend levels of gesture boundaries\
    \ initialized around one-time point annotations before being fixed. (b) Shows\
    \ jitter levels of fixed annotations. The box in the box plot covers from the\
    \ 25th to the 75th percentile (i.e., 50 percent) of jitter levels. Blue lines\
    \ show the medians and black lines show the minimum as well as the 98th percentile.\
    \ The red star in (b) indicates the maximum level of jitter. The dashed boxes\
    \ emphasize one example of improvement in jitter levels from before to after fixing\u2014\
    in the Opportunity, extend levels can exceed 1,000 percent before fixing, but\
    \ after fixing they reduce to a maximum 200 percent."
  Figure 6 Link: articels_figures_by_rev_year\2016\Supporting_OneTime_Point_Annotations_for_Gesture_Recognition\figure_6.jpg
  Figure 6 caption: Instance-based noise distribution in fixed annotations in Fixed-NoLen
    and Fixed-MaxLen in the Opportunity, HCI and Skoda data sets. Blue lines split
    a boundary jitter part from good and delete types.
  Figure 7 Link: articels_figures_by_rev_year\2016\Supporting_OneTime_Point_Annotations_for_Gesture_Recognition\figure_7.jpg
  Figure 7 caption: The performance of WarpingLCSS method on HCI data set with Fixed-NoLen
    annotations as number of clusters k varies.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.0
  Gender of the first author: Not Available
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Long-Van Nguyen-Dinh
  Name of the last author: "Gerhard Tr\xF6ster"
  Number of Figures: 7
  Number of Tables: 5
  Number of authors: 3
  Paper title: Supporting One-Time Point Annotations for Gesture Recognition
  Publication Date: 2016-12-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Gestures in Opportunity, Skoda, and HCI Data Sets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quality of Annotations on Opportunity, HCI and Skoda Data
      Sets
  Table 3 caption:
    table_text: TABLE 3 Performance of WarpingLCSS and SVM on Fixed annotations in
      Both Cases of NoLen and MaxLen and on Baselines
  Table 4 caption:
    table_text: TABLE 4 Quality of Annotations on ChaLearn Data
  Table 5 caption:
    table_text: TABLE 5 Recognition Performance on ChaLearn Data
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2637350
- Affiliation of the first author: department of electrical and computer engineering,
    university of florida, gainesville, fl
  Affiliation of the last author: department of electrical and computer engineering,
    university of florida, gainesville, fl
  Figure 1 Link: articels_figures_by_rev_year\2016\PreCapture_Privacy_for_Small_Vision_Sensors\figure_1.jpg
  Figure 1 caption: Optical elements used for defocus. We use either lensless or lenslet
    designs in this paper for optical defocus. The figure shows that any lenslet sensor
    of diameter d and image distance u can be modeled as a lensless sensor of height
    u and pinhole size d , and therefore we use only the lensless version in our theory.
  Figure 10 Link: articels_figures_by_rev_year\2016\PreCapture_Privacy_for_Small_Vision_Sensors\figure_10.jpg
  Figure 10 caption: Programmable optics for pre-capture privacy. In (I), we show
    a ray diagram for our programmable optics-based pre-capture privacy framework.
    In (II), we show the required mappings between the various system components.
    In (III), we demonstrate how to reduce the volume occupied by the display and
    beamsplitter, determined by lbeam and lmask . For the perspective case, we show
    that there exists two configurations with identical, minimum volume.
  Figure 2 Link: articels_figures_by_rev_year\2016\PreCapture_Privacy_for_Small_Vision_Sensors\figure_2.jpg
  Figure 2 caption: 'Face recognition rate versus simulated optical defocus. We quantified
    optical defocus privacy empirically by convolving face images from the FERET database
    [80] with a Gaussian filters of standard deviations 2,4,8,16,32,64,128,256 , to
    simulate optical defocus, and performing face recognition on filtered images.
    Three face recognition algorithms were tested: Principle Components Analysis,
    Linear Discriminant Analysis, and Elastic Bunch Graph Matching.'
  Figure 3 Link: articels_figures_by_rev_year\2016\PreCapture_Privacy_for_Small_Vision_Sensors\figure_3.jpg
  Figure 3 caption: Optical knapsack algorithm. A traditional knapsack solution for
    packing optical elements might fail if the elements covered the same portion of
    the visual field. Our optical knapsack solution takes into account the angular
    coverage of each sensor and maintains the pseudo-polynomial nature of the original
    dynamic programming knapsack solution [63].
  Figure 4 Link: articels_figures_by_rev_year\2016\PreCapture_Privacy_for_Small_Vision_Sensors\figure_4.jpg
  Figure 4 caption: Edge detection application with optical packing. Wide angle optical
    edge detection has been shown [49] by subtracting sensor measurements from two
    different lensless apertures. [49]'s approach in (I) is unable to utilize the
    full sensor size because it requires each image to come from one sensor. In contrast,
    our optical knapsack technique can pack the sensor plane with multiple optical
    elements (II) and synthesize, in software, a wider field of view. (II) demonstrates
    how the angular support of multiple elements vary over the visual field, and how
    different measurements from multiple apertures are combined to create a mosaicked
    image with a larger eFOV. We perform edge detection using both the configuration
    from [49] and our packed sensor on a simple scene consisting of a white blob on
    a dark background. When the target is directly in front of the sensor (III), both
    optical configurations produce reasonable edge maps. At a particular slanted angle
    (in this case, around 15 degrees due to vignetting) [49]'s approach (IV) does
    not view the target (images show sensor noise) and no edges are detected. The
    edges are still visible for our design, demonstrating its larger field of view.
  Figure 5 Link: articels_figures_by_rev_year\2016\PreCapture_Privacy_for_Small_Vision_Sensors\figure_5.jpg
  Figure 5 caption: Privacy preserving depth sensing and full-body motion tracking.
    We designed a fully 3D printed privacy sleeve for the Microsoft Kinect V2 and
    that allows accurate depth sensing and motion tracking. The sleeve has a removable
    3D printed cover for the color camera and a 3D printed lens for the IR sensor.
    As shown in (I), without the privacy sleeve, faces can clearly be identified in
    both the RGB and IR sensor images. In contrast, as shown in (II), our privacy
    sleeve performs optical black-out out for the RGB sensor and optical defocus for
    the IR sensor, yet the native Kinect tracking software from Microsoft still performs
    accurate depth sensing and motion tracking. Close-ups of the 3d printed privacy
    sleeve and lens are shown in (III). A plot comparing the measured depth of a person's
    head and chest with and with out our privacy sleeve for the documented depth range
    of the Kinect, is given in (IV).
  Figure 6 Link: articels_figures_by_rev_year\2016\PreCapture_Privacy_for_Small_Vision_Sensors\figure_6.jpg
  Figure 6 caption: Defocus with close-range IR overexposure. We designed a sensor
    overexposes nearby faces to remove the minimum depth requirement for defocus privacy.
    The setup consisted of an IR light source adjacent to a defocused webcam, with
    sensitivity in to NIR light, placed in an unilluminated office ( Figs. 6 II and
    6 III). With our setup, faces were overexposed for distances less than zmin=1.8textm
    and were appropriately exposed, but defocused for distances greater than 2.3textm
    . Fig. 6 I shows a sequence of four private images of a person standing 3.0, 2.4,
    1.8, 0.6 m (ordered left to right) from the sensor. Fig. 6 IV shows an image of
    a person standing 0.6m without the overexposing IR light source. Since 0.6m <
    zmin , this image is not private.
  Figure 7 Link: articels_figures_by_rev_year\2016\PreCapture_Privacy_for_Small_Vision_Sensors\figure_7.jpg
  Figure 7 caption: Privacy preserving people tracking. We fitted a FLIR One Thermal
    sensor with an IR Lens to enable privacy preserving people tracking via pre-capture
    optical Gaussian blurring. (I) shows the FLIR One and the IR Lens. (II) shows
    and image of a face taken with and without the IR Lens fitted to the FLIR One.
    Using this system, we were able to easily perform people tracking by searching
    for high intensity blobs in the optically de-identified thermal images (III).
    Our defocusing optics preserve approximate temperatures for objects larger than
    the largest target feature. We measured the temperatures of a human head and the
    background wall at various distances from the sensor, with and without our defocusing
    optics, using the native FLIR One thermal calibration. A graph of the measured
    temperature with and without our defocusing optics versus depth is shown in (IV).
  Figure 8 Link: articels_figures_by_rev_year\2016\PreCapture_Privacy_for_Small_Vision_Sensors\figure_8.jpg
  Figure 8 caption: Scale and position from anisotropically defocused linear sensors.
    Building on [23], we show a pre-capture privacy preserving scale detection two
    dimensional scene analysis with anisotropically defocused linear sensors. Our
    implementation consisted of to two Lu171 Lumenera monochrome sensors (Fig. 9)
    each fitted with the 3D printed lens holder and 6mm focal cylindrical lens, shown
    in (I). Only a single row of pixels from each sensor was used in the analysis
    to simulate using linear sensors. (II) shows how two dimensional scale and position
    can be extracted from the local extrema of the sensor outputs, the horizontal
    and vertical brightness distributions of the scene. Using only the horizontally
    oriented sensor from this setup, we calculated the horizontal scale of a person
    standing lbrace 1.2, 1.8, 2.4, 3.0, 3.7 rbrace meters away from the sensor. (III)
    shows the measured versus expected scale for each distance. The mean and standard
    deviation of the absolute error in the measured versus expected scale were 0.56
    and 0.46 pix respectively.
  Figure 9 Link: articels_figures_by_rev_year\2016\PreCapture_Privacy_for_Small_Vision_Sensors\figure_9.jpg
  Figure 9 caption: Privacy preserving angular blob detection. Our privacy preserving
    optical blob detector uses a Lumenera Lu-171 sensor and 3D printedlaser cut optics.
    The sensor was divided into multiple elements, where each performs pre-capture
    optical defocus filtering of different aperture radii. Therefore, a single frame
    contains a gaussian pyramid which can be used for angular blob detection. Using
    our prototype we measured the angular scale range of white circles of varying
    sizes on black background located 20.32 cm from the sensor. A graph of the measured
    angular scale range and the expected angular scale versus circle radius is the
    bottom row of the figure.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Francesco Pittaluga
  Name of the last author: Sanjeev Jagannatha Koppal
  Number of Figures: 11
  Number of Tables: 0
  Number of authors: 2
  Paper title: Pre-Capture Privacy for Small Vision Sensors
  Publication Date: 2016-12-08 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2637354
- Affiliation of the first author: national tsing hua university, hsinchu city, taiwan
  Affiliation of the last author: university of washington, seattle, wa
  Figure 1 Link: articels_figures_by_rev_year\2016\Summarizing_Unconstrained_Videos_Using_Salient_Montages\figure_1.jpg
  Figure 1 caption: 'Given a video (click to watch on YouTube: link) captured by a
    head-mounted camera (top row), we first automatically identify montageable moments
    (highlighted by the color-coded bounding boxes) containing the salient person
    (the little girl in pink) and ignore irrelevant frames. A set of salient montages
    ordered by our novel montageability scores is generated automatically. Here we
    show four typical examples.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Summarizing_Unconstrained_Videos_Using_Salient_Montages\figure_2.jpg
  Figure 2 caption: Detailed system overview. Top panel shows the steps toward identifying
    salient tracklets ( Sections 3.1 and 3.2), where each tracklet consists of color
    coded hypotheses. Bottom panel shows the steps toward generating a salient montage
    ( Section 4). The color in the pixel-level labeling L indicates the source images
    indexed by color as well.
  Figure 3 Link: articels_figures_by_rev_year\2016\Summarizing_Unconstrained_Videos_Using_Salient_Montages\figure_3.jpg
  Figure 3 caption: Process to generate estimated person probability mask from bounding
    box (a) and initial mask (b) to the final person mask (c). Note that white means
    high probability and dark means low probability. Note that the foreground segmentation
    does not need to be perfect, since the seam-term will also help to avoid cutting
    foreground with high seam cost.
  Figure 4 Link: articels_figures_by_rev_year\2016\Summarizing_Unconstrained_Videos_Using_Salient_Montages\figure_4.jpg
  Figure 4 caption: 'Our data and annotations: The top row shows challenging frames
    from ego-centric videos: Fast camera motion, wearer self-occlusion, and navigation
    moment. The middle row shows the ground truth person annotations (bounding boxes
    with person identity indices) and ground truth gaze (green dots). The bottom row
    shows ground truth salient people (red bounding boxes) and ground truth gaze (green
    dots). When bounding boxes overlap (bottom row, middle frame) we resolve ambiguity
    by minimizing identity switches.'
  Figure 5 Link: articels_figures_by_rev_year\2016\Summarizing_Unconstrained_Videos_Using_Salient_Montages\figure_5.jpg
  Figure 5 caption: Distribution of good, reasonable, and bad montages, where x -axis
    is the number of montages. On top of each bar, we show the total length of videos
    in each dataset. The mean average precision (mAP) comparison between our method
    and the baseline (BL) method for retrieving good montages is overlaid on the bar
    plot.
  Figure 6 Link: articels_figures_by_rev_year\2016\Summarizing_Unconstrained_Videos_Using_Salient_Montages\figure_6.jpg
  Figure 6 caption: Good or reasonable montages from the first-person outing dataset.
    In each example, we show the montage and the selected frames overlaid with the
    hypotheses indicated by red bounding boxes.
  Figure 7 Link: articels_figures_by_rev_year\2016\Summarizing_Unconstrained_Videos_Using_Salient_Montages\figure_7.jpg
  Figure 7 caption: Good or reasonable montages from the YouTube outing dataset. In
    each example, we show the montage and the selected frames overlaid with the hypotheses
    indicated by red bounding boxes.
  Figure 8 Link: articels_figures_by_rev_year\2016\Summarizing_Unconstrained_Videos_Using_Salient_Montages\figure_8.jpg
  Figure 8 caption: Good or reasonable montages from the YouTube motion dataset. In
    each example, we show the montage and the selected frames overlaid with the hypotheses
    indicated by red bounding boxes.
  Figure 9 Link: articels_figures_by_rev_year\2016\Summarizing_Unconstrained_Videos_Using_Salient_Montages\figure_9.jpg
  Figure 9 caption: Failure cases. In each example, we show both the montage (top
    row) and the selected frames (bottom row) overlaid with the person hypotheses
    indicated by red (correct ones) and blue (missing or incorrect ones) boxes. We
    also highlight the causes of failures using blue circles.
  First author gender probability: 0.58
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Min Sun
  Name of the last author: Steve Seitz
  Number of Figures: 9
  Number of Tables: 1
  Number of authors: 4
  Paper title: Summarizing Unconstrained Videos Using Salient Montages
  Publication Date: 2016-12-09 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 (a) Tracking Results: Mean Average Precision (mAP) Comparison
      of Different Human Detection and Tracking System'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2623699
- Affiliation of the first author: school of computer science, the university of adelaide,
    adelaide, sa, australia
  Affiliation of the last author: school of computer science, the university of adelaide,
    adelaide, sa, australia
  Figure 1 Link: articels_figures_by_rev_year\2016\CrossConvolutionalLayer_Pooling_for_Image_Recognition\figure_1.jpg
  Figure 1 caption: Illustration of the proposed method. Our method performs the pooling
    operation by using two consecutive convolutional layers. Local features x t i
    are extracted from the t th convolutional layer, and each of the feature maps
    at the (t+1) th convolutional layer is used to perform weighted sum-pooling for
    x t i . The concatenation of all pooling results is used as the image representation.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\CrossConvolutionalLayer_Pooling_for_Image_Recognition\figure_2.jpg
  Figure 2 caption: Visualization of feature maps extracted from the conv5-4 layer
    of the VGG Net. Three feature maps and their activations on three different images
    are shown. Each row represents the feature map corresponding to the same filter.
    Warmer color indicates higher activation values.
  Figure 3 Link: articels_figures_by_rev_year\2016\CrossConvolutionalLayer_Pooling_for_Image_Recognition\figure_3.jpg
  Figure 3 caption: 'This figure demonstrates the image style mismatch issue when
    using fully-connected layer activations as regional descriptors. Top row: input
    images that a DCNN ''sees'' at the training stage. Bottom row: input images that
    a DCNN ''sees'' at the test stage.'
  Figure 4 Link: articels_figures_by_rev_year\2016\CrossConvolutionalLayer_Pooling_for_Image_Recognition\figure_4.jpg
  Figure 4 caption: Performance of cross-layer pooling on image retrieval. For our
    method, the feature maps of the conv5-4 layer with top k average activations are
    selected as pooling channels.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Lingqiao Liu
  Name of the last author: Anton van den Hengel
  Number of Figures: 4
  Number of Tables: 9
  Number of authors: 3
  Paper title: Cross-Convolutional-Layer Pooling for Image Recognition
  Publication Date: 2016-12-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Results on MIT-67
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Results on Birds-200
  Table 3 caption:
    table_text: TABLE 3 Comparison of Results on PASCAL VOC 2007
  Table 4 caption:
    table_text: TABLE 4 Comparison of Results on Pascal VOC 2007 for Each of 20 Classes
  Table 5 caption:
    table_text: TABLE 5 Comparison of Results Obtained by Using Cross-Layer Pooling
      with Fully-Connected Layers
  Table 6 caption:
    table_text: TABLE 6 Comparison of Results Obtained by Using Different Convolutional
      Layers
  Table 7 caption:
    table_text: "TABLE 7 The Impact of PCA, \u2113 2 Normalization and Power Normalization"
  Table 8 caption:
    table_text: TABLE 8 Comparison of Alternative Pooling Methods
  Table 9 caption:
    table_text: TABLE 9 Results Obtained by Using Feature Sign Quantization
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2637921
- Affiliation of the first author: carnegie mellon university, pittsburgh, pa
  Affiliation of the last author: carnegie mellon university, pittsburgh, pa
  Figure 1 Link: articels_figures_by_rev_year\2016\KroneckerMarkov_Prior_for_Dynamic_D_Reconstruction\figure_1.jpg
  Figure 1 caption: We define the Kronecker Shape-Trajectory GMRF as resulting from
    the Kronecker product of independent shape and trajectory precision matrices,
    describing shape-only and trajectory-only GMRF models of deformation respectively.
  Figure 10 Link: articels_figures_by_rev_year\2016\KroneckerMarkov_Prior_for_Dynamic_D_Reconstruction\figure_10.jpg
  Figure 10 caption: The matrix normal model allows us to compute the expected value
    and spatiotemporal covariance of missing data. For this 30 frame sequence, points
    have been removed completely from frames 10-20. Observed points are marked by
    red dots. We infer missing values and visualize the mean and 95 percent confidence
    bound.
  Figure 2 Link: articels_figures_by_rev_year\2016\KroneckerMarkov_Prior_for_Dynamic_D_Reconstruction\figure_2.jpg
  Figure 2 caption: "Human spatiotemporal point cloud data exhibits a Kronecker structured\
    \ covariance matrix, allowing us to model the distribution over sequences as matrix\
    \ normal. (Left) The spatiotemporal covariance computed from 5,402 vectorized\
    \ sequences shows a distinct block structure, highlighted in the inset. (Right)\
    \ The corresponding covariance of the matrix normal model, where the full (3FP)\xD7\
    (3FP) matrix is separable into two smaller covariance matrices, the F\xD7F trajectory\
    \ (row) and 3P\xD73P shape (column) covariances respectively. Here, F=30 frames\
    \ and P=16 points."
  Figure 3 Link: articels_figures_by_rev_year\2016\KroneckerMarkov_Prior_for_Dynamic_D_Reconstruction\figure_3.jpg
  Figure 3 caption: Empirical spatiotemporal covariance matrix for a subset of face
    motion capture data ( P=10 , F=10 ), shown for two possible vectorizations of
    the matrix X . (Left) The row-major arrangement shows blocks that are approximately
    scaled versions of the spatial or row covariance. (Right) The column-major arrangement
    shows more clearly the trajectory or column covariance.
  Figure 4 Link: articels_figures_by_rev_year\2016\KroneckerMarkov_Prior_for_Dynamic_D_Reconstruction\figure_4.jpg
  Figure 4 caption: "(Left) Empirical and predicted model parameter distributions.\
    \ (a) Top, the empirical trajectory precision matrix. Below, the DCT \u22122 matrix\
    \ from Section 3.2.3. (b) Each plot corresponds to a coefficient C i,j in the\
    \ matrix C . The red curve shows the predicted standard normal pdf, the histogram\
    \ shows the empirical distribution. (c) Distribution of singular values for empirical\
    \ shape covariances (black), compared to the predicted fall-off induced by p(B)\
    \ (red)."
  Figure 5 Link: articels_figures_by_rev_year\2016\KroneckerMarkov_Prior_for_Dynamic_D_Reconstruction\figure_5.jpg
  Figure 5 caption: Inference of missing data with learned distribution parameters.
    Subscript tr indicates an orthonormal truncation method.
  Figure 6 Link: articels_figures_by_rev_year\2016\KroneckerMarkov_Prior_for_Dynamic_D_Reconstruction\figure_6.jpg
  Figure 6 caption: Inferring missing data under three different occlusion patterns
    when the shape distribution is unknown. The graphs show mean euclidean error in
    the reconstruction under the occlusion models discussed in Section 6.2. The bottom
    two results correspond to the method of Section 4.2. We investigate two different
    arrangements for the data matrix, 3Ftimes P and Ftimes 3P , which capture different
    correlations of the data. For this experiment, 3Ftimes P usually offered better
    performance, which we report on our method. The data is from dense human motion
    capture originally intended to measure non-rigid skin deformation while running
    in place.
  Figure 7 Link: articels_figures_by_rev_year\2016\KroneckerMarkov_Prior_for_Dynamic_D_Reconstruction\figure_7.jpg
  Figure 7 caption: "Multiview reconstruction on the \u201CRock Climbing\u201D sequence\
    \ from [20]. Annotated labels are shown in white. (Left) Qualitative comparison.\
    \ The top row shows a result on the full data (104 camera snapshots of 45 points).\
    \ All methods perform similarly for fully observed frames. The bottom row shows\
    \ a result on a simulated occlusion (see text). (Center) Reconstructed 3D trajectories\
    \ of the points, side view of the climbing wall. The arrows denote the direction\
    \ of motion of the climber. (Right) x,y,z -plot of the mean trajectories of the\
    \ imputed points."
  Figure 8 Link: articels_figures_by_rev_year\2016\KroneckerMarkov_Prior_for_Dynamic_D_Reconstruction\figure_8.jpg
  Figure 8 caption: 'Reconstructing a dynamic face from a frontal view. The top row
    shows frames from a video with superimposed detected 2D landmarks (green circles).
    We reconstruct the face in full 3D using Eq. (20) and show the reprojection onto
    three other (held out) views for comparison (yellow). Bottom: ground truth (black),
    MND (red), trace-norm (blue).'
  Figure 9 Link: articels_figures_by_rev_year\2016\KroneckerMarkov_Prior_for_Dynamic_D_Reconstruction\figure_9.jpg
  Figure 9 caption: Reconstructing a baseball motion sequence. Black lines indicate
    observed points, red lines are inferred trajectories. Two motion trail diagrams
    of 30-frame overlapping parts of a baseball swing are shown. The graphs show a
    close up reconstruction for different subsets of the points.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Tomas Simon
  Name of the last author: Yaser Sheikh
  Number of Figures: 10
  Number of Tables: 3
  Number of authors: 4
  Paper title: Kronecker-Markov Prior for Dynamic 3D Reconstruction
  Publication Date: 2016-12-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Linear Methods for Structure Reconstruction
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison on Zero-Noise Standard NRSfM Sequences Using Normalized
      Mean 3D Error [1], [14]
  Table 3 caption:
    table_text: TABLE 3 Comparison with the PND Method of Lee et al. [16] for 0, 30,
      and 60 Percent Missing Data
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2638904
- Affiliation of the first author: collaborative innovation center of high performance
    computing, national university of defense technology, changsha, china
  Affiliation of the last author: school of science and engineering (computing), university
    of dundee, dundee, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2016\Jointly_Learning_Heterogeneous_Features_for_RGBD_Activity_Recognition\figure_1.jpg
  Figure 1 caption: "Visualization of HOG features for two activity snapshots from\
    \ RGB (gray) channel and depth channel, respectively. As shown, the HOG features\
    \ from both channels of the same activity unveil similar \u201Cgist\u201D structure\
    \ of that activity, e.g., the \u201Cgist\u201D of looking down in reading, and\
    \ cup-to-mouth in drinking."
  Figure 10 Link: articels_figures_by_rev_year\2016\Jointly_Learning_Heterogeneous_Features_for_RGBD_Activity_Recognition\figure_10.jpg
  Figure 10 caption: Effects of parameter M on the system performance.
  Figure 2 Link: articels_figures_by_rev_year\2016\Jointly_Learning_Heterogeneous_Features_for_RGBD_Activity_Recognition\figure_2.jpg
  Figure 2 caption: "A graphic illustration of our joint learning framework. In this\
    \ framework, all the i-transforms (e.g., four i-transforms, \u0398 i i=1,2,3,4\
    \ ) shared structures and specific structures are jointly learned for the purpose\
    \ of recognition on RGB and Depth channels."
  Figure 3 Link: articels_figures_by_rev_year\2016\Jointly_Learning_Heterogeneous_Features_for_RGBD_Activity_Recognition\figure_3.jpg
  Figure 3 caption: Two signals (left) and their TPF features (middle and right).
    The TPF features of the gradient signal (right) is more distinctive than the TPF
    of the original signal (middle) when differentiating the input signals.
  Figure 4 Link: articels_figures_by_rev_year\2016\Jointly_Learning_Heterogeneous_Features_for_RGBD_Activity_Recognition\figure_4.jpg
  Figure 4 caption: Confusion matrix of JOULE on MSR daily dataset.
  Figure 5 Link: articels_figures_by_rev_year\2016\Jointly_Learning_Heterogeneous_Features_for_RGBD_Activity_Recognition\figure_5.jpg
  Figure 5 caption: Confusion matrix of JOULE on CAD 60 set.
  Figure 6 Link: articels_figures_by_rev_year\2016\Jointly_Learning_Heterogeneous_Features_for_RGBD_Activity_Recognition\figure_6.jpg
  Figure 6 caption: Confusion matrix of JOULE on composable activities dataset.
  Figure 7 Link: articels_figures_by_rev_year\2016\Jointly_Learning_Heterogeneous_Features_for_RGBD_Activity_Recognition\figure_7.jpg
  Figure 7 caption: Confusion matrices of JOULE on SYSU 3D HOI set under setting-1
    (a) and setting-2 (b).
  Figure 8 Link: articels_figures_by_rev_year\2016\Jointly_Learning_Heterogeneous_Features_for_RGBD_Activity_Recognition\figure_8.jpg
  Figure 8 caption: Snapshots of activities in SYSU 3D HOI set, one sample per class.
    The rows headed with RGB show the samples in RGB channel and the rows underneath
    headed with Depth show the corresponding depth channel superimposed with skeleton
    data. Best viewed in color.
  Figure 9 Link: articels_figures_by_rev_year\2016\Jointly_Learning_Heterogeneous_Features_for_RGBD_Activity_Recognition\figure_9.jpg
  Figure 9 caption: Illustration of the convergence of our method. The vertical axis
    indicates the value of objective function and the horizontal axis is the number
    of iterations.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.63
  Name of the first author: Jian-Fang Hu
  Name of the last author: Jianguo Zhang
  Number of Figures: 12
  Number of Tables: 10
  Number of authors: 4
  Paper title: Jointly Learning Heterogeneous Features for RGB-D Activity Recognition
  Publication Date: 2016-12-15 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison on the MSR Daily Activity Dataset
  Table 10 caption:
    table_text: "TABLE 10 Comparison of Transfer-JOULE and JOULE, and the Effects\
      \ of \u03C1 , Where \u2192 Indicates the Direction of Transfer (%)"
  Table 2 caption:
    table_text: TABLE 2 Comparison on the CAD 60 Dataset
  Table 3 caption:
    table_text: TABLE 3 Comparison on the Composable Activities Dataset
  Table 4 caption:
    table_text: TABLE 4 Comparison of 3D HOI Dataset with Relevant Datasets
  Table 5 caption:
    table_text: TABLE 5 Comparison on the SYSU 3D HOI Dataset
  Table 6 caption:
    table_text: TABLE 6 Accuracy (%) of Our Methods with and without TPF on Gradient
  Table 7 caption:
    table_text: "TABLE 7 Effects of Parameter \u03BB on Recognition (%)"
  Table 8 caption:
    table_text: "TABLE 8 Effects of Parameter \u03B3 on Recognition Accuracy (%)"
  Table 9 caption:
    table_text: TABLE 9 Effects of Jointly Learning in Different Channels
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2640292
