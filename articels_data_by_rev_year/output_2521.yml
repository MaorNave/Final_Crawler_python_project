- Affiliation of the first author: college of artificial intelligence, dalian maritime
    university, dalian, china
  Affiliation of the last author: school of electronics, electrical engineering and
    computer science, queens university belfast, belfast, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2022\SEEM_A_Sequence_Entropy_EnergyBased_Model_for_Pedestrian_Trajectory_AllThenOne_P\figure_1.jpg
  Figure 1 caption: Architecture of the SEEM model. The overall architecture has two
    core parts, a generator network and an energy network. The Generator network is
    trained to produce multi-modal trajectories. The Energy network is used to determine
    which trajectories are most representative of pedestrian behavior.
  Figure 10 Link: articels_figures_by_rev_year\2022\SEEM_A_Sequence_Entropy_EnergyBased_Model_for_Pedestrian_Trajectory_AllThenOne_P\figure_10.jpg
  Figure 10 caption: Performance of the full SEEM model (SEEM) and the SEEM model
    without the PDC module included (SEEM-PDC) when different fractions of the available
    training data are used to train the models. Results are presented for thor = 12
    for each of the ETH and UCY datasets. The x -axis in each plot is the percentage
    of the available training data used to train the models, and the y -axis shows
    the corresponding ADE (top row) and FDE (bottom row) values (in meters).
  Figure 2 Link: articels_figures_by_rev_year\2022\SEEM_A_Sequence_Entropy_EnergyBased_Model_for_Pedestrian_Trajectory_AllThenOne_P\figure_2.jpg
  Figure 2 caption: "Flow diagram showing the process for maximizing the mutual information\
    \ through f -divergence. A network \u03C3(T( Y ,z)) is introduced, where Y and\
    \ its corresponding z are regarded as a positive sample pair, while Y and randomly\
    \ drawn z ~ are regarded as negative samples."
  Figure 3 Link: articels_figures_by_rev_year\2022\SEEM_A_Sequence_Entropy_EnergyBased_Model_for_Pedestrian_Trajectory_AllThenOne_P\figure_3.jpg
  Figure 3 caption: Diversity of the predicted trajectories for the Binary Tree case
    study. The black dashed line in each plot indicates the ground truth ratio.
  Figure 4 Link: articels_figures_by_rev_year\2022\SEEM_A_Sequence_Entropy_EnergyBased_Model_for_Pedestrian_Trajectory_AllThenOne_P\figure_4.jpg
  Figure 4 caption: Evolution of f -divergence with training for the SocialGAN-P,
    SoPhie, Energy, and SEEM models.
  Figure 5 Link: articels_figures_by_rev_year\2022\SEEM_A_Sequence_Entropy_EnergyBased_Model_for_Pedestrian_Trajectory_AllThenOne_P\figure_5.jpg
  Figure 5 caption: Diversity of the predicted trajectories for the Trigeminal Tree
    case study with balanced datasets.
  Figure 6 Link: articels_figures_by_rev_year\2022\SEEM_A_Sequence_Entropy_EnergyBased_Model_for_Pedestrian_Trajectory_AllThenOne_P\figure_6.jpg
  Figure 6 caption: Diversity of the predicted trajectories for the Trigeminal Tree
    case study with unbalanced datasets.
  Figure 7 Link: articels_figures_by_rev_year\2022\SEEM_A_Sequence_Entropy_EnergyBased_Model_for_Pedestrian_Trajectory_AllThenOne_P\figure_7.jpg
  Figure 7 caption: Qualitative comparison of SEEM and a number of baseline models
    in three different settings with multiple pedestrians. For a better view of the
    diversity achieved by each model, trajectory distributions are depicted for only
    one or two people in each scene.
  Figure 8 Link: articels_figures_by_rev_year\2022\SEEM_A_Sequence_Entropy_EnergyBased_Model_for_Pedestrian_Trajectory_AllThenOne_P\figure_8.jpg
  Figure 8 caption: 'Examples of the diversity of trajectory predictions obtained
    with selected models for three types of pedestrian interaction: Avoidance (row
    1); Following (row 2); Merging (row 3). Four successive frames are displayed for
    each interaction. The colored regions highlight the diversity of future trajectories
    generated by each model for a selected pedestrian: SocialGAN-P (red); SoPhie (yellow);
    Energy (blue); SEEM (green).'
  Figure 9 Link: articels_figures_by_rev_year\2022\SEEM_A_Sequence_Entropy_EnergyBased_Model_for_Pedestrian_Trajectory_AllThenOne_P\figure_9.jpg
  Figure 9 caption: A comparison of the ADE and FDE values obtained with SEEM for
    selected baseline models. The x -axis labels 1-5 correspond to the ETH, HOTEL,
    UNIV, ZARA1 and ZARA2 test datasets, respectively.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Dafeng Wang
  Name of the last author: "Se\xE1n McLoone"
  Number of Figures: 15
  Number of Tables: 7
  Number of authors: 6
  Paper title: 'SEEM: A Sequence Entropy Energy-Based Model for Pedestrian Trajectory
    All-Then-One Prediction'
  Publication Date: 2022-02-01 00:00:00
  Table 1 caption: TABLE 1 Summary of Main Notations
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Implementation Parameters
  Table 3 caption: TABLE 3 Diverse Trajectory Validation for the Binary Tree Case
    Study
  Table 4 caption: TABLE 4 Diverse Trajectory Validation for the Trigeminal Tree Case
    Study
  Table 5 caption: TABLE 5 KDE NLL Evaluation of SEEM and Baselines for ETH and UCY
  Table 6 caption: TABLE 6 Performance for t obs =8 tobs=8 and t pred =12 tpred=12
    in Meters on TrajNet++
  Table 7 caption: TABLE 7 Quantitative Prediction Results for the ETH and UCY Datasets
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3147639
- Affiliation of the first author: department of computer science and technology,
    bnrist, moe-key laboratory of pervasive computing, tsinghua university, beijing,
    china
  Affiliation of the last author: school of computer science and informatics, cardiff
    university, cardiff, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2022\Quality_Metric_Guided_Portrait_Line_Drawing_Generation_From_Unpaired_Training_Da\figure_1.jpg
  Figure 1 caption: 'Comparison with state-of-the-art methods: (a) input face photo;
    (b)-(c) style transfer methods: Gatys [4] and Linear Style Transfer [7]; (f)-(h)
    single-modal image-to-image translation methods: DualGAN [8], CycleGAN [6], UNIT
    [9]; (d) multi-modal image-to-image translation methods MUNIT [10]; (e) our previous
    conference version (Ours-pre) [3] that outputs three styles; (i) a portrait generation
    method APDrawingGAN++ [2] using paired training data; (j) our method. Note that
    our method only uses unpaired training data. Due to this essential difference,
    we only compare APDrawingGAN++ with our method in Appendix A5.2, which can be
    found on the Computer Society Digital Library at http:doi.ieeecomputersociety.org10.1109TPAMI.2022.3147570.'
  Figure 10 Link: articels_figures_by_rev_year\2022\Quality_Metric_Guided_Portrait_Line_Drawing_Generation_From_Unpaired_Training_Da\figure_10.jpg
  Figure 10 caption: 'Comparison with three single-modal unpaired image-to-image translation
    methods: DualGAN [8], CycleGAN [6], UNIT [9]. All methods are trained using the
    same training corpus with both real and synthesized drawings.'
  Figure 2 Link: articels_figures_by_rev_year\2022\Quality_Metric_Guided_Portrait_Line_Drawing_Generation_From_Unpaired_Training_Da\figure_2.jpg
  Figure 2 caption: Representative images of the three styles in our collected web
    portrait line drawing data. (a) The first style is from Yann Legendre and Charles
    Burns who use thin parallel lines to draw shadows. (b) The second style is from
    Kathryn Rathke who draws facial features using simple flowing lines and uses few
    dark regions. (c) The third style is from vectorportal.com where continuous thick
    lines and large dark regions are utilized. Close up views are presented alongside
    for better comparison of the three styles.
  Figure 3 Link: articels_figures_by_rev_year\2022\Quality_Metric_Guided_Portrait_Line_Drawing_Generation_From_Unpaired_Training_Da\figure_3.jpg
  Figure 3 caption: Samples of collected (including generated and artist) portrait
    line drawings of target style 2 for quality metric model training. The drawings
    from top to bottom have decreasing quality.
  Figure 4 Link: articels_figures_by_rev_year\2022\Quality_Metric_Guided_Portrait_Line_Drawing_Generation_From_Unpaired_Training_Da\figure_4.jpg
  Figure 4 caption: CycleGAN reconstructs the input photo from generated drawings
    using a strict cycle-consistency loss, which can potentially embed invisible reconstruction
    information anywhere in the whole drawings. A nonlinear monotonic mapping of the
    gray values is applied in a local region around the nose to visualize the embedded
    reconstruction information.
  Figure 5 Link: articels_figures_by_rev_year\2022\Quality_Metric_Guided_Portrait_Line_Drawing_Generation_From_Unpaired_Training_Da\figure_5.jpg
  Figure 5 caption: "Our GAN model uses an asymmetric cycle structure, which consists\
    \ of a photo to drawing generator G , a drawing to photo generator F , a drawing\
    \ discriminator D D and a photo discriminator D P . We use a relaxed cycle-consistency\
    \ loss between reconstructed face photo F(G(p,s)) and input photo p , while enforcing\
    \ a strict cycle-consistency loss between reconstructed drawing G(F(d)) and input\
    \ drawing d . We also introduce local drawing discriminators D ln , D le , D ll\
    \ for the nose, eyes and lips and a truncation loss. Our model deals with multi-style\
    \ generation by inserting a style feature vector into the generator and adding\
    \ a style loss. A quality loss based on the quality metric model (Section 3) further\
    \ encourages generation of \u201Cgood looking\u201D APDrawings. The detailed architecture\
    \ is illustrated in Appendix A2, available in the online supplemental material."
  Figure 6 Link: articels_figures_by_rev_year\2022\Quality_Metric_Guided_Portrait_Line_Drawing_Generation_From_Unpaired_Training_Da\figure_6.jpg
  Figure 6 caption: 'Results of interpolating between style feature vectors: (a) input
    photos, (b)-(d) results of three target styles, (e)-(f) results of interpolating
    target styles. Close-up views are shown by the side.'
  Figure 7 Link: articels_figures_by_rev_year\2022\Quality_Metric_Guided_Portrait_Line_Drawing_Generation_From_Unpaired_Training_Da\figure_7.jpg
  Figure 7 caption: "Examples of \u201Cnew\u201D style generation. Given a target\
    \ \u201Cnew\u201D style portrait drawing (i.e., style unseen in training data)\
    \ in (a), we find a proper style feature vector that generates APDrawings similar\
    \ to the target, by optimizing a histogram based style loss. The optimization\
    \ process is shown in (c) and the final generated APDrawing is shown in (d). The\
    \ style loss change during optimization is shown in (e). Style feature vectors\
    \ used for generation are shown under each generated APDrawing. Close-up views\
    \ are shown by the side."
  Figure 8 Link: articels_figures_by_rev_year\2022\Quality_Metric_Guided_Portrait_Line_Drawing_Generation_From_Unpaired_Training_Da\figure_8.jpg
  Figure 8 caption: "Visualizing interpretable units. The rows from top to bottom\
    \ show units that best match \u201Cupper lip\u201D, \u201Ceye\u201D, and \u201C\
    nose\u201D categories, respectively. The IoU is measured over the full test set\
    \ of 154 images. For each unit, eight images with top IoU are shown, and the masks\
    \ of thresholding the upsampled feature map ( Fuuparrow > tu,r ) are outlined\
    \ in yellow."
  Figure 9 Link: articels_figures_by_rev_year\2022\Quality_Metric_Guided_Portrait_Line_Drawing_Generation_From_Unpaired_Training_Da\figure_9.jpg
  Figure 9 caption: Comparison with two state-of-the-art neural style transfer methods,
    i.e., Gatys [4] and LinearStyleTransfer [7].
  First author gender probability: 0.58
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ran Yi
  Name of the last author: Paul L. Rosin
  Number of Figures: 14
  Number of Tables: 5
  Number of authors: 4
  Paper title: Quality Metric Guided Portrait Line Drawing Generation From Unpaired
    Training Data
  Publication Date: 2022-02-01 00:00:00
  Table 1 caption: TABLE 1 User Study Results
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Analysis of Variance (ANOVA) Results for Pairwise Comparisons
  Table 3 caption: "TABLE 3 Fr\xE9chet Inception Distance (FID) of Our Method and\
    \ Four Multi-Modal Image Translation Methods"
  Table 4 caption: TABLE 4 The Scores Predicted by Quality Metric Model M M on the
    Results of Different Methods
  Table 5 caption: "TABLE 5 Fr\xE9chet Inception Distance (FID) of the Ablation Studies"
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3147570
- Affiliation of the first author: school of computer science and technology, xian
    jiaotong university, xian, china
  Affiliation of the last author: department of computer science and engineering,
    university of texas at arlington, arlington, tx, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\Learning_Representations_by_Graphical_Mutual_Information_Estimation_and_Maximiza\figure_1.jpg
  Figure 1 caption: A high-level overview of Deep InfoMax (left), Deep Graph InfoMax
    (DGI) (middle), and Graphical Mutual Information (GMI) (right). Note that graphs
    with topology and features are more complicated than images that involve features
    only, thus GMI ought to maximize the MI of both features and edges between inputs
    (i.e., an input graph) and outputs (i.e., an output graph depicted by hidden vectors)
    of the encoder. Note that GMI considers the structural information from a local
    perspective, while GMI++ captures both local and global topological properties.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Learning_Representations_by_Graphical_Mutual_Information_Estimation_and_Maximiza\figure_2.jpg
  Figure 2 caption: Illustration of Davis southern women social network and heatmaps
    of its normalized adjacency matrix A and PPMI matrix P , respectively.
  Figure 3 Link: articels_figures_by_rev_year\2022\Learning_Representations_by_Graphical_Mutual_Information_Estimation_and_Maximiza\figure_3.jpg
  Figure 3 caption: Visualization of detection results on Books dataset from ANOMALOUS
    (b), Dominant (c), and our GMI (d). Nodes with higher scores have darker colors,
    and the top 30 nodes with the highest scores are marked in red. Subfigures (a)
    visualizes Books dataset and the true anomalies are marked with red.
  Figure 4 Link: articels_figures_by_rev_year\2022\Learning_Representations_by_Graphical_Mutual_Information_Estimation_and_Maximiza\figure_4.jpg
  Figure 4 caption: Visualization of PolBlogs and detection results. Nodes with larger
    residuals have darker colors. As can be observed, most of the nodes are lighter
    in color, and only a few are darker. This is consistent with the fact that anomalies
    are rare objects deviating from the patterns of majority in datasets.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Zhen Peng
  Name of the last author: Junzhou Huang
  Number of Figures: 4
  Number of Tables: 8
  Number of authors: 7
  Paper title: Learning Representations by Graphical Mutual Information Estimation
    and Maximization
  Publication Date: 2022-02-01 00:00:00
  Table 1 caption: TABLE 1 Statistics of the Datasets Used in Each Task
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Classification Accuracies (With Std Dev) in Percent on
    Transductive Tasks and Micro-Averaged F1 Scores on Inductive Tasks
  Table 3 caption: TABLE 3 Comparison With Different Objective Functions in Terms
    of Classification Accuracies (With Std Dev) in Percent on Cora, Citeseer, and
    PubMed, and Micro-Averaged F1 Scores in Percent on Reddit and PPI
  Table 4 caption: TABLE 4 AUC Scores (With Std Dev) in Percent for Link Prediction
  Table 5 caption: TABLE 5 Visualization of t-SNE Embeddings From Raw Features, DGI,
    a Learned FMI, GMI, and GMI++ on Cora and Citeseer
  Table 6 caption: TABLE 6 The Silhouette Coefficient Score for the Clustering of
    Node Embeddings
  Table 7 caption: TABLE 7 AUC Scores in Percent for Anomaly Detection
  Table 8 caption: TABLE 8 Three Anomalies Identified by Our Method on PolBlogs
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3147886
- Affiliation of the first author: pca lab, key laboratory of intelligent perception
    and systems for high-dimensional information of ministry of education, school
    of computer science and engineering, nanjing university of science and technology,
    nanjing, china
  Affiliation of the last author: department of computer and information science,
    state key laboratory of internet of things for smart city, university of macau,
    macau, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Consistency_and_Diversity_Induced_Human_Motion_Segmentation\figure_1.jpg
  Figure 1 caption: 'Comparison of different transfer subspace learning algorithms:
    (a) low-dimensional feature reconstruction based transfer subspace learning, and
    (b) multi-level feature reconstruction based transfer subspace learning. Different
    shapes denote the data points from the source or target domain, and the two colors
    denote different classes.'
  Figure 10 Link: articels_figures_by_rev_year\2022\Consistency_and_Diversity_Induced_Human_Motion_Segmentation\figure_10.jpg
  Figure 10 caption: Segmentation results on MAD (Keck as source) via using different
    numbers of layers and dimensionalities of dictionary atoms.
  Figure 2 Link: articels_figures_by_rev_year\2022\Consistency_and_Diversity_Induced_Human_Motion_Segmentation\figure_2.jpg
  Figure 2 caption: Overview of multi-level transfer subspace learning framework for
    human motion segmentation. Our model first factorizes the source and target data
    into multi-layer implicit feature spaces using a deep NMF model, in which multi-level
    transfer subspace learning (TSL) is carried out in different spaces (i.e., layers)
    to capture multi-level information. Then, a multi-mutual consistency learning
    strategy is presented to reduce the difference in feature distribution between
    the two domains. After that, we construct a novel affinity matrix by fusing multi-level
    representation coefficients. Finally, the Normalized Cuts algorithm can be applied
    to the learned affinity matrix to obtain the segmentation (clustering) results.
  Figure 3 Link: articels_figures_by_rev_year\2022\Consistency_and_Diversity_Induced_Human_Motion_Segmentation\figure_3.jpg
  Figure 3 caption: Comparison segmentation results of using different coefficient
    representations from the first layer, last layer, and multi-layer fusion to construct
    the affinity matrix on the Weiz dataset.
  Figure 4 Link: articels_figures_by_rev_year\2022\Consistency_and_Diversity_Induced_Human_Motion_Segmentation\figure_4.jpg
  Figure 4 caption: Sampling frames from five human motion benchmark datasets, i.e.,
    (a) Keck, (b) MAD, (c) Weiz, (d) UT, and (e) YouTube.
  Figure 5 Link: articels_figures_by_rev_year\2022\Consistency_and_Diversity_Induced_Human_Motion_Segmentation\figure_5.jpg
  Figure 5 caption: Visualization of clustering results on a sample video of the Weiz
    dataset. The ten colors denote ten different temporal clusters.
  Figure 6 Link: articels_figures_by_rev_year\2022\Consistency_and_Diversity_Induced_Human_Motion_Segmentation\figure_6.jpg
  Figure 6 caption: The segmentation performance in terms of NMI and ACC when using
    different numbers of sequential neighbors tau .
  Figure 7 Link: articels_figures_by_rev_year\2022\Consistency_and_Diversity_Induced_Human_Motion_Segmentation\figure_7.jpg
  Figure 7 caption: 'Parameter sensitivity study and convergence analysis on the Keck
    dataset using HOG features: (a) Sensitivity analysis for parameters beta and gamma
    , (b) Sensitivity analysis for parameters alpha and gamma , (c) Sensitivity analysis
    for parameters alpha and beta , and (d) Convergence curves.'
  Figure 8 Link: articels_figures_by_rev_year\2022\Consistency_and_Diversity_Induced_Human_Motion_Segmentation\figure_8.jpg
  Figure 8 caption: 'Parameter sensitivity study on MAD: (a) Sensitivity analysis
    for parameters beta and gamma , (b) Sensitivity analysis for parameters alpha
    and gamma , and (c) Sensitivity analysis for parameters alpha and beta .'
  Figure 9 Link: articels_figures_by_rev_year\2022\Consistency_and_Diversity_Induced_Human_Motion_Segmentation\figure_9.jpg
  Figure 9 caption: Segmentation results based on (a) using different action numbers
    and (b) using different frame ratios.
  First author gender probability: 0.87
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Tao Zhou
  Name of the last author: Jianbing Shen
  Number of Figures: 12
  Number of Tables: 4
  Number of authors: 7
  Paper title: Consistency and Diversity Induced Human Motion Segmentation
  Publication Date: 2022-02-01 00:00:00
  Table 1 caption: TABLE 1 Main Notations Used in the Proposed Model
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Clustering Results of Compared Methods in Terms of NMI
    and ACC on Four Human Motion Datasets
  Table 3 caption: TABLE 3 Clustering Results of Compared Methods in Terms of NMI
    and ACC on Three Human Motion Datasets
  Table 4 caption: TABLE 4 Comparisons on MAD When Using one and two Datasets as the
    Source
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3147841
- Affiliation of the first author: australian institute for machine learning, university
    of adelaide, adelaide, sa, australia
  Affiliation of the last author: australian institute for machine learning, university
    of adelaide, adelaide, sa, australia
  Figure 1 Link: articels_figures_by_rev_year\2022\PACBayes_MetaLearning_With_Implicit_TaskSpecific_Posteriors\figure_1.jpg
  Figure 1 caption: "Meta-learning is an extension of hyper-parameter optimisation,\
    \ where the meta-parameter \u03B8 is shared across all tasks. The solid arrows\
    \ denote forward pass, while the dashed arrows indicate parameter inference, and\
    \ rectangles illustrate the plate notations. The training subset ( x (t) ij ,\
    \ y (t) ij ) m (t) i j=1 of task T i and the meta-parameter \u03B8 are used to\
    \ learn the task-specific parameter \u03BB i , corresponding to the lower-level\
    \ optimisation in (3). The obtained \u03BB i is then used to evaluate the error\
    \ on the validation subset ( x (v) ij , y (v) ij ) m (v) i j=1 to learn the meta-parameter\
    \ \u03B8 , corresponding to the upper-level optimisation in (3)."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\PACBayes_MetaLearning_With_Implicit_TaskSpecific_Posteriors\figure_2.jpg
  Figure 2 caption: "SImPa and MAML are compared in a regression problem when training\
    \ is based on multi-modal data \u2013 half of the tasks are generated from sinusoidal\
    \ functions, and the other half are from linear functions. The shaded area is\
    \ the prediction made by SImPa \xB1 standard deviation."
  Figure 3 Link: articels_figures_by_rev_year\2022\PACBayes_MetaLearning_With_Implicit_TaskSpecific_Posteriors\figure_3.jpg
  Figure 3 caption: Quantitative comparison between various probabilistic meta-learning
    approaches averaged over 1000 unseen tasks shows that SImPa has a comparable MSE
    error and the smallest calibration error.
  Figure 4 Link: articels_figures_by_rev_year\2022\PACBayes_MetaLearning_With_Implicit_TaskSpecific_Posteriors\figure_4.jpg
  Figure 4 caption: "Calibration of the \u201Cstandard\u201D 4-block CNN trained with\
    \ different meta-learning methods on 5-way 1-shot classification tasks on mini-ImageNet."
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Cuong Nguyen
  Name of the last author: Gustavo Carneiro
  Number of Figures: 4
  Number of Tables: 1
  Number of authors: 3
  Paper title: PAC-Bayes Meta-Learning With Implicit Task-Specific Posteriors
  Publication Date: 2022-02-01 00:00:00
  Table 1 caption: TABLE 1 The Few-Shot 5-Way Classification Accuracy Results (in
    Percentage, With 95% Confidence Interval) of SImPa Averaged Over 1 Million Tasks
    on Omniglot (Top), and 600 Tasks on Mini-ImageNet (Middle-Top and Middle-Bottom)
    and Tiered-ImageNet (Bottom) Datasets
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3147798
- Affiliation of the first author: mathematical institute, utrecht university, cs
    utrecht, the netherlands
  Affiliation of the last author: institute of mathematics, technical university of
    berlin, berlin, germany
  Figure 1 Link: articels_figures_by_rev_year\2022\Solving_Inverse_Problems_With_Deep_Neural_Networks__Robustness_Included\figure_1.jpg
  Figure 1 caption: Schematic network reconstruction pipelines of UNet , TiraFL (top),
    and ItNet (bottom).
  Figure 10 Link: articels_figures_by_rev_year\2022\Solving_Inverse_Problems_With_Deep_Neural_Networks__Robustness_Included\figure_10.jpg
  Figure 10 caption: "Case Study C \u2013 fastMRI. Individual reconstructions of a\
    \ central slice of a randomly selected volume from the validation set for different\
    \ levels of adversarial noise. The reconstructed images are displayed in the window\
    \ [0.05,4.50], which is also used for the computation of the PSNR and SSIM. For\
    \ error plots and the results of text UNet and text ItNet , we refer to Fig. S9,\
    \ available online. The ground truth image boldsymbolx0 is shown at the bottom\
    \ right."
  Figure 2 Link: articels_figures_by_rev_year\2022\Solving_Inverse_Problems_With_Deep_Neural_Networks__Robustness_Included\figure_2.jpg
  Figure 2 caption: "Scenario A1 \u2013 CS with 1D signals. (a) shows the adversarial\
    \ noise-to-error curve for the randomly selected signal of Fig. 3. (b) shows the\
    \ corresponding Gaussian noise-to-error curve, where the mean and standard deviation\
    \ are computed over 200 draws of e . (c) and (d) display the respective curves\
    \ averaged over 50 signals from the test set. For the sake of clarity, we have\
    \ omitted the standard deviations for UNet and TiraFL , which behave similarly."
  Figure 3 Link: articels_figures_by_rev_year\2022\Solving_Inverse_Problems_With_Deep_Neural_Networks__Robustness_Included\figure_3.jpg
  Figure 3 caption: "Scenario A1 \u2013 CS with 1D signals. Individual reconstructions\
    \ of a randomly selected signal from the test set for different levels of adversarial\
    \ noise. The ground truth signal is visualized by a dashed line."
  Figure 4 Link: articels_figures_by_rev_year\2022\Solving_Inverse_Problems_With_Deep_Neural_Networks__Robustness_Included\figure_4.jpg
  Figure 4 caption: "Scenario A2 \u2013 CS with MNIST. (a) shows the adversarial noise-to-error\
    \ curve for the randomly selected digit 3 of Fig. 5. (b) shows the corresponding\
    \ Gaussian noise-to-error curve, where the mean and standard deviation are computed\
    \ over 200 draws of e . (c) and (d) display the respective curves averaged over\
    \ 50 signals from the test set."
  Figure 5 Link: articels_figures_by_rev_year\2022\Solving_Inverse_Problems_With_Deep_Neural_Networks__Robustness_Included\figure_5.jpg
  Figure 5 caption: "Scenario A2 \u2013 CS with MNIST. Individual reconstructions\
    \ of two randomly selected digits from the test set for different levels of adversarial\
    \ noise. The reconstructed digits and their error plots (with relative ell 2 -error)\
    \ are displayed in the windows [0,1] and [0, 0.6] , respectively. The horizontal\
    \ line artifacts in the text TV[eta ] -solutions are due to the fact that the\
    \ MNIST images are treated as vectorized 1D signals. Remarkably, although relying\
    \ on 1D convolutional filters, the NN-based reconstructions do not suffer from\
    \ these artifacts."
  Figure 6 Link: articels_figures_by_rev_year\2022\Solving_Inverse_Problems_With_Deep_Neural_Networks__Robustness_Included\figure_6.jpg
  Figure 6 caption: "Scenario B1 \u2013 Fourier meas. with ellipses. (a) shows the\
    \ adversarial noise-to-error curve for the randomly selected image of Fig. 7.\
    \ (b) shows the corresponding Gaussian noise-to-error curve, where the mean and\
    \ (almost imperceptible) standard deviation are computed over 50 draws of boldsymbole\
    \ . (c) and (d) display the respective curves averaged over 50 images from the\
    \ test set. For the sake of clarity, we have omitted the standard deviations for\
    \ text UNet and text TiraFL , which behave similarly."
  Figure 7 Link: articels_figures_by_rev_year\2022\Solving_Inverse_Problems_With_Deep_Neural_Networks__Robustness_Included\figure_7.jpg
  Figure 7 caption: "Scenario B1 \u2013 Fourier meas. with ellipses. Individual reconstructions\
    \ of a randomly selected image from the test set for different levels of adversarial\
    \ noise. The reconstructed images are displayed in the window [0,0.9], which is\
    \ also used for the computation of the PSNR and SSIM. For error plots and the\
    \ results of text UNet and text TiraFL , we refer to Fig. S5, available online.\
    \ The bottom right figure concerns the transferability of adversarial noise: it\
    \ shows the reconstruction text TV[eta ](boldsymbolytextadv) , where boldsymbolytextadv\
    \ is the perturbation found for text ItNet with 8% noise; see Fig. S8, available\
    \ online, for additional experiments. The ground truth image boldsymbolx0 has\
    \ been omitted, as it is visually indistinguishable from the noiseless reconstruction\
    \ by text TV[eta ] ."
  Figure 8 Link: articels_figures_by_rev_year\2022\Solving_Inverse_Problems_With_Deep_Neural_Networks__Robustness_Included\figure_8.jpg
  Figure 8 caption: "Scenario B2 \u2013 Radon meas. with ellipses. Individual reconstructions\
    \ of a randomly selected image from the test set for different levels of adversarial\
    \ noise. The reconstructed images are displayed in the window [0,1], which is\
    \ also used for the computation of the PSNR and SSIM. The bottom right figure\
    \ shows the FBP inversion of the 2%-adversarial perturbation found for text UNet\
    \ . The ground truth image boldsymbolx0 has been omitted, as it is visually indistinguishable\
    \ from the noiseless reconstruction by text TV[eta ] ."
  Figure 9 Link: articels_figures_by_rev_year\2022\Solving_Inverse_Problems_With_Deep_Neural_Networks__Robustness_Included\figure_9.jpg
  Figure 9 caption: "Case Study C \u2013 fastMRI. (a) shows the adversarial noise-to-error\
    \ curve for the randomly selected image of Fig. 10. (b) shows the corresponding\
    \ Gaussian noise-to-error curve, where the mean and (almost imperceptible) standard\
    \ deviation are computed over 50 draws of boldsymbole . (c) and (d) display the\
    \ respective curves averaged over 30 images from the validation set. For the sake\
    \ of clarity, we have omitted the standard deviations for text UNet and text TiraFL\
    \ , which behave similarly."
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Martin Genzel
  Name of the last author: "Maximilian M\xE4rz"
  Number of Figures: 16
  Number of Tables: 0
  Number of authors: 3
  Paper title: "Solving Inverse Problems With Deep Neural Networks \u2013 Robustness\
    \ Included?"
  Publication Date: 2022-02-04 00:00:00
  Table 1 caption: Not Available
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3148324
- Affiliation of the first author: school of mechanical, electrical and information
    engineering, shandong university, weihai, china
  Affiliation of the last author: bnrist, thuibcs, blbci, kliss, school of software,
    tsinghua universitty, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\STORM_StructureBased_Overlap_Matching_for_Partial_Point_Cloud_Registration\figure_1.jpg
  Figure 1 caption: The overall pipeline of STORM. Given the input point clouds, STORM
    first performs structure-based feature extraction. The multiple densely-connected
    EdgeConv layers and a Transformer layer are employed to map the input points to
    a feature space. Then, we perform overlap prediction to detects the points in
    common between the input point clouds. After that, a Transformer layer is employed
    to perform feature refinement for overlap region and generate pointwise features
    containing structural information. Finally, we perform pose estimation based on
    the generated pointwise features. Specifically, we generate virtual partial correspondences
    based on feature similarity. The correspondences are formulated as an affinity
    matrix, and R and t can be obtained by SVD. Utilizing R and t aligns the input
    point clouds together.
  Figure 10 Link: articels_figures_by_rev_year\2022\STORM_StructureBased_Overlap_Matching_for_Partial_Point_Cloud_Registration\figure_10.jpg
  Figure 10 caption: The red points denote that the points lay in overlap. (a) The
    input point clouds. (b) The ground truth of overlap. (c) The overlap predicted
    by the model with the proposed overlap prediction module. (d) The overlap predicted
    by the model with the Top-K mechanism.
  Figure 2 Link: articels_figures_by_rev_year\2022\STORM_StructureBased_Overlap_Matching_for_Partial_Point_Cloud_Registration\figure_2.jpg
  Figure 2 caption: "The detail of overlap prediction in our method. Given pointwise\
    \ features, our overlap prediction module first predicts a vector \u03C0 denoting\
    \ class probabilities. Then, Gumbel-Softmax is performed to generate a distribution\
    \ which describes the latent common structure of the input point clouds. Lastly,\
    \ we sample K times from the distribution and generate K points representing the\
    \ overlap."
  Figure 3 Link: articels_figures_by_rev_year\2022\STORM_StructureBased_Overlap_Matching_for_Partial_Point_Cloud_Registration\figure_3.jpg
  Figure 3 caption: The experimental results of compared methods on point clouds with
    noise and lower overlaps in terms of RMSE(R) .
  Figure 4 Link: articels_figures_by_rev_year\2022\STORM_StructureBased_Overlap_Matching_for_Partial_Point_Cloud_Registration\figure_4.jpg
  Figure 4 caption: The experimental results of compared methods on point clouds with
    noise and lower overlaps in terms of RMSE(t) .
  Figure 5 Link: articels_figures_by_rev_year\2022\STORM_StructureBased_Overlap_Matching_for_Partial_Point_Cloud_Registration\figure_5.jpg
  Figure 5 caption: The precision and recall of the learned points in overlap region
    on point clouds with different overlap ratios.
  Figure 6 Link: articels_figures_by_rev_year\2022\STORM_StructureBased_Overlap_Matching_for_Partial_Point_Cloud_Registration\figure_6.jpg
  Figure 6 caption: The distribution of inliers and outliers in the points ranking
    by the predicted probabilities based on structural information.
  Figure 7 Link: articels_figures_by_rev_year\2022\STORM_StructureBased_Overlap_Matching_for_Partial_Point_Cloud_Registration\figure_7.jpg
  Figure 7 caption: Qualitative registration examples. (a) Point clouds with approximately
    0.69 overlap rate, noise and outliers. (b) Point clouds with approximately 0.47
    overlap rate and noise. (c) Point clouds with approximately 0.32 overlap rate
    and noise.
  Figure 8 Link: articels_figures_by_rev_year\2022\STORM_StructureBased_Overlap_Matching_for_Partial_Point_Cloud_Registration\figure_8.jpg
  Figure 8 caption: The visualization of the relevance of the learned point embeddings
    using t-SNE [57].
  Figure 9 Link: articels_figures_by_rev_year\2022\STORM_StructureBased_Overlap_Matching_for_Partial_Point_Cloud_Registration\figure_9.jpg
  Figure 9 caption: Qualitative registration examples on the Stanford 3D Scanning
    Repository [58]. The first row is the input point clouds, and the second row is
    the point clouds aligned by STORM.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Yujie Wang
  Name of the last author: Yue Gao
  Number of Figures: 12
  Number of Tables: 10
  Number of authors: 6
  Paper title: 'STORM: Structure-Based Overlap Matching for Partial Point Cloud Registration'
  Publication Date: 2022-02-04 00:00:00
  Table 1 caption: TABLE 1 Experimental Results on Clean Point Clouds With Approximately
    0.69 Overlap Rate
  Table 10 caption: TABLE 10 The Quantitative Results of Registration Methods on the
    Outdoor KITTI Dataset
  Table 2 caption: TABLE 2 Experimental Results on Point Clouds With Noise and Approximately
    0.69 Overlap Ratio
  Table 3 caption: TABLE 3 Experimental Results on Point Clouds With Noise and Outliers
  Table 4 caption: TABLE 4 Experimental Results on Partial Point Clouds Truncated
    by the Partial Manner in RPM-Net
  Table 5 caption: TABLE 5 The Inference Time Comparison of Different Methods (in
    microseconds)
  Table 6 caption: TABLE 6 The Computation Cost of STORM for Different Sizes of the
    Input Point Clouds
  Table 7 caption: TABLE 7 Results of Ablation Study
  Table 8 caption: TABLE 8 Experimental Results With Respect to Different Numbers
    of Sampling Points Predicted by the Proposed Overlap Prediction Module on Registration
    Accuracy
  Table 9 caption: TABLE 9 Experimental Results With Respect to the Loss Function
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3148308
- Affiliation of the first author: "bosch center for artificial intelligence, renningen,\
    \ baden-w\xFCrttemberg, germany"
  Affiliation of the last author: university of siegen, siegen, germany
  Figure 1 Link: articels_figures_by_rev_year\2022\HigherOrder_Multicuts_for_Geometric_Model_Fitting_and_Motion_Segmentation\figure_1.jpg
  Figure 1 caption: (a) An example of a graph decomposition and its encodings. Switching
    green and red labels will produce a different encoding for the same decomposition.
    Dashed lines, in turn, constitute a multicut of a graph and uniquely define its
    decomposition. (b) An example of a lifted graph decomposition and it encoding.
    Blue lines denote lifted edges, that connect vertices which are not direct neighbors
    in the graph. (c) An example of 3rd-order costs, that consider three nodes at
    a time (light blue triangles) for a better join cut decision. If a higher-order
    cost does not correspond to a clique in the graph, we add lifted edges.
  Figure 10 Link: articels_figures_by_rev_year\2022\HigherOrder_Multicuts_for_Geometric_Model_Fitting_and_Motion_Segmentation\figure_10.jpg
  Figure 10 caption: The articulated motion causes over-segmentation in [4]. The Lifted
    AOMC performs better.
  Figure 2 Link: articels_figures_by_rev_year\2022\HigherOrder_Multicuts_for_Geometric_Model_Fitting_and_Motion_Segmentation\figure_2.jpg
  Figure 2 caption: '(a) Line fitting: We fit a line using tls into a set of points
    and assume that the latter are independently drawn from a 1D Gaussian centered
    on the line and orthogonal to it. (b) Homography Estimation: For a pair of images
    with annotated correspondences (yellow line) we directly model the distance r
    between the corresponding and projected point. In the example above, a point from
    the second image corresponds to a red dot, but its projection via the estimated
    homography is a bit off (blue dot). Uncertainty model corresponds to a 2D isotropic
    Gaussian centered at the red point.'
  Figure 3 Link: articels_figures_by_rev_year\2022\HigherOrder_Multicuts_for_Geometric_Model_Fitting_and_Motion_Segmentation\figure_3.jpg
  Figure 3 caption: Qualitative results on synthetic line fitting data from Toldo
    and Fusiello [21]. Colored dots denote points assigned to the same line model.
    Gray dots denote points assigned to noisebackground.
  Figure 4 Link: articels_figures_by_rev_year\2022\HigherOrder_Multicuts_for_Geometric_Model_Fitting_and_Motion_Segmentation\figure_4.jpg
  Figure 4 caption: "(a) Mean me and standard deviation of our method on Star11 averaged\
    \ over 20 runs as functions of the sub-sampling rate of the cost edges; \u03C3\
    \ is fixed. Already at 5% of all the ( 1100 3 ) possible cost edges the mean error\
    \ becomes almost optimal and the standard deviation drops to 0. (b) Plotted are\
    \ sorted absolute costs per cluster in the optimal solutions for Stairs4, Star5,\
    \ and Star11. For each problem the corresponding 4, 5, and 11 most expensive clusters\
    \ are visually clearly separated form the lower cost clusters, the latter being\
    \ lines fit into the outliers."
  Figure 5 Link: articels_figures_by_rev_year\2022\HigherOrder_Multicuts_for_Geometric_Model_Fitting_and_Motion_Segmentation\figure_5.jpg
  Figure 5 caption: Some visualizations of our results on the Adelaide Robust Model
    Fitting data set [91]. Only one image of the pair is shown. Colored boxes denote
    points assigned to the same geometric model. Yellow dots denote points assigned
    to noisebackground.
  Figure 6 Link: articels_figures_by_rev_year\2022\HigherOrder_Multicuts_for_Geometric_Model_Fitting_and_Motion_Segmentation\figure_6.jpg
  Figure 6 caption: Samples of our Lifted AOMC segmentations densified by [58]. Even
    for articulated motion, our segmentations show little over-segmentation.
  Figure 7 Link: articels_figures_by_rev_year\2022\HigherOrder_Multicuts_for_Geometric_Model_Fitting_and_Motion_Segmentation\figure_7.jpg
  Figure 7 caption: The scaling motion of the white horse moving towards the camera
    causes over-segmentation with a simple motion model [4]. With the proposed Lifted
    AOMC, this can be avoided.
  Figure 8 Link: articels_figures_by_rev_year\2022\HigherOrder_Multicuts_for_Geometric_Model_Fitting_and_Motion_Segmentation\figure_8.jpg
  Figure 8 caption: The two cars in front move to the same direction, leading an assignment
    to the same cluster with the non-lifted multicut approach [4]. The Lifted AOMC
    can assign the different cars to distinct segments.
  Figure 9 Link: articels_figures_by_rev_year\2022\HigherOrder_Multicuts_for_Geometric_Model_Fitting_and_Motion_Segmentation\figure_9.jpg
  Figure 9 caption: Due to camera motion the person and the wall are assigned to the
    same cluster with the non-lifted multicut approach [4]. The Lifted AOMC allows
    for correct segmentation.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Evgeny Levinkov
  Name of the last author: Margret Keuper
  Number of Figures: 13
  Number of Tables: 7
  Number of authors: 4
  Paper title: Higher-Order Multicuts for Geometric Model Fitting and Motion Segmentation
  Publication Date: 2022-02-07 00:00:00
  Table 1 caption: TABLE 1 Quantitative Results on Synthetic Line Fitting Data From
    Toldo and Fusiello [21]
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Misclassification Errors on the Adelaide Robust Model Fitting
    Data Set [91]
  Table 3 caption: TABLE 3 Segmentation Results on the FBMS-59 Dataset on Set A (top)
    and Set B (Bottom)
  Table 4 caption: TABLE 4 Segmentation Results on FBMS-59 on Set A (top) and Set
    B (Bottom)
  Table 5 caption: TABLE 5 Segmentation Results of the Proposed Model Lifted AOMC
    on the FBMS-59 Dataset on Set A (top) and Set B (Bottom) for Different Optical
    Flow Methods
  Table 6 caption: TABLE 6 Results for Densified Segmentations on FBMS-59 Using Annotations
    and Metrics as in [73] for Freely Moving 3D Objects
  Table 7 caption: TABLE 7 Evaluation on DAVIS 16 16
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3148795
- Affiliation of the first author: integrative sciences and engineering programme,
    nus graduate school, national university of singapore, singapore
  Affiliation of the last author: school of computing, national university of singapore,
    singapore
  Figure 1 Link: articels_figures_by_rev_year\2022\Fair_Representation_Guaranteeing_Approximate_Multiple_Group_Fairness_for_Unknown\figure_1.jpg
  Figure 1 caption: Sample images (from top to bottom) of MPI3D [59], LFW [62], VGGFace2
    [60], and CelebA [63].
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Fair_Representation_Guaranteeing_Approximate_Multiple_Group_Fairness_for_Unknown\figure_2.jpg
  Figure 2 caption: Our approach on correlated MPI3D dataset with varying level of
    correlation. Color is the sensitive attribute. Shape is the target attribute that
    correlates with color. Size and bkgd color are independent target attributes.
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xudong Shen
  Name of the last author: Mohan Kankanhalli
  Number of Figures: 2
  Number of Tables: 6
  Number of authors: 3
  Paper title: 'Fair Representation: Guaranteeing Approximate Multiple Group Fairness
    for Unknown Tasks'
  Publication Date: 2022-02-07 00:00:00
  Table 1 caption: TABLE 1 Seven Group Fairness Notions Are Considered
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Summary of Our Theoretical Results
  Table 3 caption: TABLE 3 Our Approach on Adult Dataset Compared with the Baseline
    (i.e., Representations Learned with Only Reconstruction) and Existing Fair Representation
    Learning Methods
  Table 4 caption: TABLE 4 Our Approach on Uncorrelated MPI3D Dataset Compared with
    Representations Learned with Only Reconstruction and Existing Fair Representation
    Learning Methods
  Table 5 caption: TABLE 5 Our Approach on Face Datasets Compared with the Baseline
    and Existing Gender-blind Face Representation Learning Methods
  Table 6 caption: TABLE 6 The Learned Face Representation Evaluated on CelebA Dataset
    Compared with Representations Learned with ArcFace Only (AO)
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3148905
- Affiliation of the first author: college of engineering and computer science, australian
    national university, canberra, act, australia
  Affiliation of the last author: tencent, shenzhen, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Enhanced_SpatioTemporal_Interaction_Learning_for_Video_Deraining_Faster_and_Bett\figure_1.jpg
  Figure 1 caption: The PSNR versus runtime of the state-of-the-art deep video deraining
    methods and our method on the NTURain dataset.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Enhanced_SpatioTemporal_Interaction_Learning_for_Video_Deraining_Faster_and_Bett\figure_2.jpg
  Figure 2 caption: Our proposed Enhanced Spatio-Temporal Interaction Networks (ESTINet).
    The input rainy frames are fed into SICM to extract the spatial cue, which is
    further forwarded into STIM to extract spatio-temporal features. Finally, the
    proposed ESTM takes the extracted features as input to capture the spatio-tempoal
    consistency and generate the final results.
  Figure 3 Link: articels_figures_by_rev_year\2022\Enhanced_SpatioTemporal_Interaction_Learning_for_Video_Deraining_Faster_and_Bett\figure_3.jpg
  Figure 3 caption: "The illustration of the ResNet-based Encoder-Decoder backbone\
    \ (SICM) to extract spatial representations from frames. The input is a single\
    \ rainy frame, while the output is its spatial features. \u201CUp\u201D means\
    \ the upsampling operation."
  Figure 4 Link: articels_figures_by_rev_year\2022\Enhanced_SpatioTemporal_Interaction_Learning_for_Video_Deraining_Faster_and_Bett\figure_4.jpg
  Figure 4 caption: "The comparison illustration between LSTM and the Interaction-CBLSTM\
    \ backbone in STIM to learn temporal correlations. The input ( f t ) are the spatial\
    \ features extracted from SICM illustrated in Fig. 3. The output of \u201CConv\u201D\
    \ is two kinds of spatio-temporal features which mean bidirectional sequences,\
    \ which are further fed into a CNN to obtain the forward sequence. The blue and\
    \ red arrows in the Interaction-CBLSTM mean the different orders of input frames."
  Figure 5 Link: articels_figures_by_rev_year\2022\Enhanced_SpatioTemporal_Interaction_Learning_for_Video_Deraining_Faster_and_Bett\figure_5.jpg
  Figure 5 caption: "The illustration of the Enhanced Spatio-Temporal Model (ESTM)\
    \ to refine the deraining videos. \u201COutput1\u201D represents the coarse deraining\
    \ results, and \u201COutput2\u201D indicates the refined results."
  Figure 6 Link: articels_figures_by_rev_year\2022\Enhanced_SpatioTemporal_Interaction_Learning_for_Video_Deraining_Faster_and_Bett\figure_6.jpg
  Figure 6 caption: 'Exemplar results on the RainSynLight25 dataset. From left to
    right: input, results of [4], [35], [3] and ours. All results are attained without
    alignment. Best viewed in color.'
  Figure 7 Link: articels_figures_by_rev_year\2022\Enhanced_SpatioTemporal_Interaction_Learning_for_Video_Deraining_Faster_and_Bett\figure_7.jpg
  Figure 7 caption: 'Exemplar results on the NTURain dataset. From left to right:
    input, results of [4], [35], [3] and ours. All results are attained without alignment.
    Best viewed in color.'
  Figure 8 Link: articels_figures_by_rev_year\2022\Enhanced_SpatioTemporal_Interaction_Learning_for_Video_Deraining_Faster_and_Bett\figure_8.jpg
  Figure 8 caption: Deraining results on the real-world rainy sequences. The top and
    bottom rows are the input sequences and the output sequences from out proposed
    model, respectively. Best viewed in color.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.73
  Name of the first author: Kaihao Zhang
  Name of the last author: Wei Liu
  Number of Figures: 8
  Number of Tables: 3
  Number of authors: 5
  Paper title: 'Enhanced Spatio-Temporal Interaction Learning for Video Deraining:
    Faster and Better'
  Publication Date: 2022-02-07 00:00:00
  Table 1 caption: TABLE 1 Performance Comparison With State-of-the-art Methods on
    the RainSynLight25, RainSynHeavy25 and NTURain Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Speed Comparison With Current Methods
  Table 3 caption: TABLE 3 Performance Comparison of Different Architectures on the
    NTURain Dataset
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3148707
