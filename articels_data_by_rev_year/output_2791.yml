- Affiliation of the first author: signal processing and speech communication laboratory,
    graz university of technology, graz, austria
  Affiliation of the last author: signal processing and speech communication laboratory,
    graz university of technology, graz, austria
  Figure 1 Link: articels_figures_by_rev_year\2022\SelfGuided_Belief_Propagation__A_Homotopy_Continuation_Method\figure_1.jpg
  Figure 1 caption: "Illustration of the model-classes specified in Section 2.2.1:\
    \ (a) frustrated; (b) balanced, and (c) its equivalent attractive model; and (d)\
    \ unidirectional model. Solid lines depict attractive edges and dashed lines depict\
    \ repulsive ones. The signs in the vertices are equal to the signs of the corresponding\
    \ local fields \u03B8 i ."
  Figure 10 Link: articels_figures_by_rev_year\2022\SelfGuided_Belief_Propagation__A_Homotopy_Continuation_Method\figure_10.jpg
  Figure 10 caption: "Images for salt and pepper noise in (a) and (c) and for Gaussian\
    \ noise in (b) and (d). All images have the respective signal to noise ratio (SNR)\
    \ above. The evolution of SBP for denoising with Gaussian noise is illustrated\
    \ in (d) for \u03B6 m \u22080,0.2,0.35,0.68 ; for illustration purposes we show\
    \ a cropped version (depicted in red in the original image). BP failed to denoise\
    \ the image in (d) in any meaningful way (SNR <1 dB)."
  Figure 2 Link: articels_figures_by_rev_year\2022\SelfGuided_Belief_Propagation__A_Homotopy_Continuation_Method\figure_2.jpg
  Figure 2 caption: "Evolution of the Bethe free energy F B for Example 1. We consider\
    \ a complete graph with N=4 variables with homogeneous potentials, attractive\
    \ edges, and \u03B8 i =0 . F B is evaluated for P ~ ( x i )= P ~ for (a) J =0.1\
    \ , (b) J =0.55 , and (c) J =0.7 ."
  Figure 3 Link: articels_figures_by_rev_year\2022\SelfGuided_Belief_Propagation__A_Homotopy_Continuation_Method\figure_3.jpg
  Figure 3 caption: "Evolution of the Bethe free energy F B for Example 2. We consider\
    \ a complete graph with N=4 variables with homogeneous potentials, attractive\
    \ edges, and \u03B8 i >0 . F B is evaluated for P ~ ( x i )= P ~ for (a) J =0.25\
    \ , (b) J =0.62 , and (c) J =0.75 ."
  Figure 4 Link: articels_figures_by_rev_year\2022\SelfGuided_Belief_Propagation__A_Homotopy_Continuation_Method\figure_4.jpg
  Figure 4 caption: 'Example 3: SBP proceeds along the smooth solution path and obtains
    accurate marginals despite instability of the terminal fixed point.'
  Figure 5 Link: articels_figures_by_rev_year\2022\SelfGuided_Belief_Propagation__A_Homotopy_Continuation_Method\figure_5.jpg
  Figure 5 caption: "MSE (orange) and MSE B (blue) and their standard deviation (shaded\
    \ area) over the cumulative number of iterations. Results are averaged over 100\
    \ grid graphs ( 5\xD75 ); \u03B8 i =0.4 and J ij \u2208\u22121,1 ."
  Figure 6 Link: articels_figures_by_rev_year\2022\SelfGuided_Belief_Propagation__A_Homotopy_Continuation_Method\figure_6.jpg
  Figure 6 caption: "MSE and number of iterations for: SBP all (blue), BP \u2218 (orange),\
    \ and BP \u2218 D (green); \u03B8 i \u223CU(\u22120.5,0.5) and (a) J ij \u223C\
    U(0,\u03B2) (attractive model); (b) J ij \u223CU(\u2212\u03B2,\u03B2) (general\
    \ model). In terms of accuracy, SBP is superior in all scenarios, while increasing\
    \ the number of iterations only slightly."
  Figure 7 Link: articels_figures_by_rev_year\2022\SelfGuided_Belief_Propagation__A_Homotopy_Continuation_Method\figure_7.jpg
  Figure 7 caption: "Convergence ratio for BP (orange) and BP D (green) for grid graphs\
    \ (solid) and complete graphs (dashed) with general potentials \u03B8 i \u223C\
    U(\u22120.5,0.5) and J ij \u223CU(\u2212\u03B2,\u03B2) ."
  Figure 8 Link: articels_figures_by_rev_year\2022\SelfGuided_Belief_Propagation__A_Homotopy_Continuation_Method\figure_8.jpg
  Figure 8 caption: "(a) Scaling parameter \u03B6 m , (b) MSE, and (c) number of iterations\
    \ over the maximum number of steps M for a 10\xD710 grid graph with \u03B8 i \u223C\
    U(\u22120.5,0.5) and J ij \u223CU(\u2212\u03B2,\u03B2) for \u03B2=0.5 (blue),\
    \ \u03B2=1.5 (red), \u03B2=2.5 (black), \u03B2=3.5 (green), and \u03B2=4.5 (orange).\
    \ Irrespective of \u03B2 , SBP has an optimal accuracy run-time trade-off for\
    \ M\u224510 . Finer steps increase the run-time but do not improve the marginal\
    \ accuracy much."
  Figure 9 Link: articels_figures_by_rev_year\2022\SelfGuided_Belief_Propagation__A_Homotopy_Continuation_Method\figure_9.jpg
  Figure 9 caption: "Number of iterations with adaptive step size (solid) and without\
    \ (dashed) for a grid graph with potentials \u03B8 i \u223CU(\u22120.5,0.5) and\
    \ J ij \u223CU(\u2212\u03B2,\u03B2) . We consider M=10 (blue) and M=100 (red).\
    \ The adaptive step size is particularly advantageous if the step size is chosen\
    \ too fine."
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Christian Knoll
  Name of the last author: Franz Pernkopf
  Number of Figures: 12
  Number of Tables: 1
  Number of authors: 3
  Paper title: "Self-Guided Belief Propagation \u2013 A Homotopy Continuation Method"
  Publication Date: 2022-08-08 00:00:00
  Table 1 caption: "TABLE 1 Results for General Models With J ij \u2208\u22121,1 Jij\u2208\
    -1,1 on Grid Graphs ( N=25 N=25 and N=100 N=100), Complete Graphs ( N=10 N=10),\
    \ and Random Graphs ( N=10 N=10)"
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3196140
- Affiliation of the first author: chinese university of hong kong, hong kong sar,
    china
  Affiliation of the last author: chinese university of hong kong, hong kong sar,
    china
  Figure 1 Link: articels_figures_by_rev_year\2022\ZoomNAS_Searching_for_WholeBody_Human_Pose_Estimation_in_the_Wild\figure_1.jpg
  Figure 1 caption: The pipeline of ZoomNet. ZoomNet adopts the top-down paradigm
    that first detects human instances and then estimates keypoints. A single neural
    network consisting of three modules, i.e., BodyNet, FaceHead, and HandHead, is
    proposed to localize whole-body keypoints for each person. BodyNet predicts bodyfoot
    keypoints and facehand boxes. FaceHeadHandHead zoom in to the facehand areas and
    estimate facehand keypoints at a higher resolution. This design handles the scale
    variance of different body parts of a person instance.
  Figure 10 Link: articels_figures_by_rev_year\2022\ZoomNAS_Searching_for_WholeBody_Human_Pose_Estimation_in_the_Wild\figure_10.jpg
  Figure 10 caption: WBH is challenging as it contains more complex hand poses except
    the standard hand gestures.
  Figure 2 Link: articels_figures_by_rev_year\2022\ZoomNAS_Searching_for_WholeBody_Human_Pose_Estimation_in_the_Wild\figure_2.jpg
  Figure 2 caption: ZoomNAS applies neural architecture search to ZoomNet for efficient
    whole-body pose estimation. The architectural configurations of BodyNetFaceHeadHandHead,
    including depth, channel, group number, and input resolution, are searched simultaneously.
    ZoomNAS also searches the connections between BodyNet and FaceHeadHandHead, i.e.,
    feature stage and RoI expansion. The computational complexity is allocated among
    different sub-modules automatically.
  Figure 3 Link: articels_figures_by_rev_year\2022\ZoomNAS_Searching_for_WholeBody_Human_Pose_Estimation_in_the_Wild\figure_3.jpg
  Figure 3 caption: "The discovered architecture of ZoomNAS. (Depth, Channel, Group)\
    \ are listed for each stage. Only the branch with the highest resolution are illustrated,\
    \ and other branches are omitted for clarity. \u201CKpts\u201D means keypoints."
  Figure 4 Link: articels_figures_by_rev_year\2022\ZoomNAS_Searching_for_WholeBody_Human_Pose_Estimation_in_the_Wild\figure_4.jpg
  Figure 4 caption: Definitions of 133 keypoints on COCO-WholeBody.
  Figure 5 Link: articels_figures_by_rev_year\2022\ZoomNAS_Searching_for_WholeBody_Human_Pose_Estimation_in_the_Wild\figure_5.jpg
  Figure 5 caption: 'Annotation examples. Line 1: Different colors are used to distinguish
    different types of bounding boxes, i.e., body (green), face (purple), left hand
    (blue) and right hand (red). Line 2 and 3: Face keypoints. Line 4 and 5: Hand
    keypoints.'
  Figure 6 Link: articels_figures_by_rev_year\2022\ZoomNAS_Searching_for_WholeBody_Human_Pose_Estimation_in_the_Wild\figure_6.jpg
  Figure 6 caption: (a) The box size of bodyfacehand (b) The average keypoint distance
    of bodyfacehand. We notice the large scale variance of different parts of the
    same person.
  Figure 7 Link: articels_figures_by_rev_year\2022\ZoomNAS_Searching_for_WholeBody_Human_Pose_Estimation_in_the_Wild\figure_7.jpg
  Figure 7 caption: Distribution of the center location over the whole image (a) body,
    (b) face, and (c) hand.
  Figure 8 Link: articels_figures_by_rev_year\2022\ZoomNAS_Searching_for_WholeBody_Human_Pose_Estimation_in_the_Wild\figure_8.jpg
  Figure 8 caption: The normalized standard deviation of manual annotation for each
    keypoint. Body keypoints have the larger manual annotation variance than face
    and hand keypoints.
  Figure 9 Link: articels_figures_by_rev_year\2022\ZoomNAS_Searching_for_WholeBody_Human_Pose_Estimation_in_the_Wild\figure_9.jpg
  Figure 9 caption: WBF is challenging as it contains (a) many blurry face images
    and (b) large head pose variations.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Lumin Xu
  Name of the last author: Xiaogang Wang
  Number of Figures: 11
  Number of Tables: 12
  Number of authors: 7
  Paper title: 'ZoomNAS: Searching for Whole-Body Human Pose Estimation in the Wild'
  Publication Date: 2022-08-08 00:00:00
  Table 1 caption: TABLE 1 Overview of Some Popular Public Datasets for 2D Keypoint
    Estimation
  Table 10 caption: TABLE 10 Effect of Automatic Computation Allocation
  Table 2 caption: TABLE 2 The Search Spaces of ZoomNAS
  Table 3 caption: TABLE 3 Compare the Numbers of Valid FaceHand Samples in the Training
    Set of COCO-WholeBody V0.5 and V1.0
  Table 4 caption: TABLE 4 Whole-Body Pose Estimation Results on COCO-WholeBody V1.0
    Dataset
  Table 5 caption: TABLE 5 Benchmarking Results on WBF Dataset
  Table 6 caption: TABLE 6 Cross-Dataset Evalutation for WBF Dataset
  Table 7 caption: "TABLE 7 Cross-Dataset Generalization Results Measured by PCK \u2191\
    \ \u2191AUC \u2191 \u2191EPE \u2193 \u2193"
  Table 8 caption: TABLE 8 Benchmarking Results on WBH Dataset
  Table 9 caption: TABLE 9 Effect of Searching Connections Between BodyNet and FaceHeadHandHead
    on COCO-WholeBody V1.0 dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3197352
- Affiliation of the first author: school of electronic information and electrical
    engineering, and moe key lab of artificial intelligence, shanghai jiao tong university,
    shanghai, china
  Affiliation of the last author: school of electronic information and electrical
    engineering, and moe key lab of artificial intelligence, shanghai jiao tong university,
    shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Detecting_Rotated_Objects_as_Gaussian_Distributions_and_its_D_Generalization\figure_1.jpg
  Figure 1 caption: 'Detection results comparison (top: 2-D, bottom: 3-D) at the boundary
    condition (i.e., horizontal or vertical rotation) between Smooth L1 loss-based
    (left) and the Gaussian-based (right) detectors. See illustration in Fig. 2 for
    the Gaussian-based bounding box detection.'
  Figure 10 Link: articels_figures_by_rev_year\2022\Detecting_Rotated_Objects_as_Gaussian_Distributions_and_its_D_Generalization\figure_10.jpg
  Figure 10 caption: Orientation degeneration cases. Note that the red and blue boxescubes
    share the same Gaussian distribution representation.
  Figure 2 Link: articels_figures_by_rev_year\2022\Detecting_Rotated_Objects_as_Gaussian_Distributions_and_its_D_Generalization\figure_2.jpg
  Figure 2 caption: A schematic diagram of modeling a rotating bounding 2-D (top)
    and 3-D (bottom) box by a Gaussian distribution instead of the BBox.
  Figure 3 Link: articels_figures_by_rev_year\2022\Detecting_Rotated_Objects_as_Gaussian_Distributions_and_its_D_Generalization\figure_3.jpg
  Figure 3 caption: 'Two classic definitions of rotated BBoxes. Left: OpenCV Definition
    D oc [2], [18], Right: Long Edge Definition D le [17], [34].'
  Figure 4 Link: articels_figures_by_rev_year\2022\Detecting_Rotated_Objects_as_Gaussian_Distributions_and_its_D_Generalization\figure_4.jpg
  Figure 4 caption: 'High-precision detection by Smooth L1 loss, GWD, BCD and KLD
    (left to right). Datasets: DOTA (top) [53], HRSC2016 (bottom) [54].'
  Figure 5 Link: articels_figures_by_rev_year\2022\Detecting_Rotated_Objects_as_Gaussian_Distributions_and_its_D_Generalization\figure_5.jpg
  Figure 5 caption: 'Behavior comparison of different losses: Smooth L1 Loss, IoU
    Loss and Gaussian-based loss (e.g., GWD) in different detection cases.'
  Figure 6 Link: articels_figures_by_rev_year\2022\Detecting_Rotated_Objects_as_Gaussian_Distributions_and_its_D_Generalization\figure_6.jpg
  Figure 6 caption: Boundary discontinuity under two BBox definitions (top), and illustration
    of the square-like problem (bottom).
  Figure 7 Link: articels_figures_by_rev_year\2022\Detecting_Rotated_Objects_as_Gaussian_Distributions_and_its_D_Generalization\figure_7.jpg
  Figure 7 caption: Illustration of the sensitivity of GWD to large errors. GWD is
    very sensitive to large errors and requires a suitable transformation for normalization.
    The x -axis (offset) represents the displacement in pixels.
  Figure 8 Link: articels_figures_by_rev_year\2022\Detecting_Rotated_Objects_as_Gaussian_Distributions_and_its_D_Generalization\figure_8.jpg
  Figure 8 caption: 'Behavior of L 2 -norm, GWD and KLD versus parameters when the
    targeted height varies, as generated by a simulation test. Left: the gradient
    landscape of (Delta x,Delta y,Delta w,Delta h) ; Right: gradient curve of Delta
    theta .'
  Figure 9 Link: articels_figures_by_rev_year\2022\Detecting_Rotated_Objects_as_Gaussian_Distributions_and_its_D_Generalization\figure_9.jpg
  Figure 9 caption: The loss of L 2 -norm, GWD and KLD versus scaling factor, generated
    by a simulation test. Only the value of KLD is invariant to the scaling factor
    s . Th bounding boxes denote different scales of objects.
  First author gender probability: 0.72
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.85
  Name of the first author: Xue Yang
  Name of the last author: Junchi Yan
  Number of Figures: 11
  Number of Tables: 16
  Number of authors: 8
  Paper title: Detecting Rotated Objects as Gaussian Distributions and its 3-D Generalization
  Publication Date: 2022-08-08 00:00:00
  Table 1 caption: TABLE 1 Ablation Test of GWD-Based Regression Loss Form and Hyperparameter
    on DOTA
  Table 10 caption: TABLE 10 Comparison Between Different Solutions for Inconsistency
    Between Metric and Loss (IML), Boundary Discontinuity (BD) and Square-Like Problem
    (SLP) on DOTA Dataset
  Table 2 caption: TABLE 2 Ablation of KLD Regression Losses Using RetinaNet as Based
    Detector
  Table 3 caption: TABLE 3 Ablation Study of Normalization
  Table 4 caption: TABLE 4 Ablation Study Under Different BBox Definitions
  Table 5 caption: TABLE 5 High-Precision Detection Experiment Under Different Regression
    Loss
  Table 6 caption: TABLE 6 Performance Comparison on the KITTI val Split
  Table 7 caption: TABLE 7 Performance Comparison on Waymo Open Dataset val Set With
    202 Sequences for 3-D Object Detection
  Table 8 caption: TABLE 8 Ablation of Heading Post-Processing on Waymo Open Dataset
    val Set
  Table 9 caption: TABLE 9 More Ablation Experiments on More Datasets (MLT and UCAS-AOD)
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3197152
- Affiliation of the first author: department of computer science and engineering,
    the chinese university of hong kong, hong kong
  Affiliation of the last author: department of computer science and engineering,
    the chinese university of hong kong, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2022\DSGN_Exploiting_VisualSpatial_Relation_for_StereoBased_D_Detectors\figure_1.jpg
  Figure 1 caption: Overview of the proposed DSGN++ framework. The whole framework
    consists of six components. (a) 2D image extraction network for extracting stereo
    features. (b) Volume construction process by Depth-wise Plane Sweeping. (c) 3D
    CNN for front-view and top-view feature extraction. (d) Dual-view flow integration
    followed by 3D CNN. (e) Front-surface depth head for supervising depth signals
    in the front-view. (f) 3D detection head that detects objects in birds eye view.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\DSGN_Exploiting_VisualSpatial_Relation_for_StereoBased_D_Detectors\figure_2.jpg
  Figure 2 caption: Depth-wise plane sweeping. Assume the constructed volume is a
    4D tensor in the space along (x,y,depth,channel) axes. We visualize the depth-channel
    plane of 3D featured volume where the graduated color indicates channel orders.
    Depth-wise volume is constructed by jointly sweeping the depth planes and slicing
    the features along the channel dimension. Cyclic slicing reorders the channels
    to ensure channel consistency across nearby depth planes.
  Figure 3 Link: articels_figures_by_rev_year\2022\DSGN_Exploiting_VisualSpatial_Relation_for_StereoBased_D_Detectors\figure_3.jpg
  Figure 3 caption: Comparison of stereo information flows. Prior stereo detectors
    adapt 2D features to plane-sweep volume (green cube) or 3D-geometry volume (golden
    cube). Differently, dual-view stereo volume (DSV) aggregates both spaced features
    in 3D space and enforces geometric learning in the front view that is fit for
    visual sensors.
  Figure 4 Link: articels_figures_by_rev_year\2022\DSGN_Exploiting_VisualSpatial_Relation_for_StereoBased_D_Detectors\figure_4.jpg
  Figure 4 caption: Comparison of average voxel occupancy per category within the
    plane-sweep and 3D-geometry volumes. We set the maximum voxel numbers to 600 for
    visualization of distant regions.
  Figure 5 Link: articels_figures_by_rev_year\2022\DSGN_Exploiting_VisualSpatial_Relation_for_StereoBased_D_Detectors\figure_5.jpg
  Figure 5 caption: Joint Stereo-LiDAR copy-paste strategy. For binocular training
    samples, the source object patches are cropped with their calibrated projections
    lbrace PSleft, PSrightrbrace . The augmented scene uses projections lbrace PTleft,
    PTrightrbrace . Object patches are pasted bilinearly to binocular images.
  Figure 6 Link: articels_figures_by_rev_year\2022\DSGN_Exploiting_VisualSpatial_Relation_for_StereoBased_D_Detectors\figure_6.jpg
  Figure 6 caption: Qualitative results on the KITTI val set. Green boxes represents
    ground-truth and red boxes denotes our predictions. The left-view images are shown
    in the left column and the BEV point clouds images are shown on the right side.
    Some failure cases are shown at the bottom of the table. Please zoom in to observe
    the prediction details. Redline shown in the birds eye view is 50 meters away
    from the sensor.
  Figure 7 Link: articels_figures_by_rev_year\2022\DSGN_Exploiting_VisualSpatial_Relation_for_StereoBased_D_Detectors\figure_7.jpg
  Figure 7 caption: Comparison of foreground object localization error. Red dotted
    line computes the regressed line. The localization error computes the average
    depth estimation error within the 3D object boxes at the respective depth ranges.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.91
  Name of the first author: Yilun Chen
  Name of the last author: Jiaya Jia
  Number of Figures: 7
  Number of Tables: 7
  Number of authors: 5
  Paper title: 'DSGN++: Exploiting Visual-Spatial Relation for Stereo-Based 3D Detectors'
  Publication Date: 2022-08-08 00:00:00
  Table 1 caption: TABLE 1 Performance Comparison on the Official KITTI test Server
    (Car)
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Performance Comparison on the Official KITTI Test Server
    (Pedestrian and Cyclist)
  Table 3 caption: TABLE 3 Performance Comparison on the KITTI val Set in Various
    Modality Settings
  Table 4 caption: TABLE 4 Main Ablation Studies on the KITTI val Set
  Table 5 caption: "TABLE 5 Effects of Expanded Channels and Smoothness Factor \u03B1\
    \ \u03B1 for Depth-Wise Plane Sweeping"
  Table 6 caption: TABLE 6 Hyper-Parameter Choices for Stereo-LiDAR Copy-Paste
  Table 7 caption: TABLE 7 Inference Time Comparison and the Efficient Implementation
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3197236
- Affiliation of the first author: school of mechanical, electrical and information
    engineering, shandong university, weihai, shandong, china
  Affiliation of the last author: department of computer science, hong kong baptist
    university, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2022\Complex_Network_Evolution_Model_Based_on_Turing_Pattern_Dynamics\figure_1.jpg
  Figure 1 caption: Turing pattern examples. (a) A network with communities. (b) A
    pattern on the surface of animal skin. As the communities in the network shown
    by (a) are similar to the spots on the surface of animal skin shown by (b), the
    network can be regarded as a pattern composed of such spots like communities.
    Based on this assumption, using the reaction-diffusion model of Turing pattern
    dynamics to simulate the generation mechanism of the pattern composed of communities,
    is actually to model the emergence and evolution mechanism of communities in the
    network.
  Figure 10 Link: articels_figures_by_rev_year\2022\Complex_Network_Evolution_Model_Based_on_Turing_Pattern_Dynamics\figure_10.jpg
  Figure 10 caption: Illustration of parameter R , C , alpha and gamma having little
    effect on the emergence of communities. (a)-(d) The modularity matrix for RC ranging
    from 5 to 15 with varepsilon =0.1 , 0.3, 0.7, 0.9 respectively. The color of one
    square is determined by the modularity of the network generated by the RDQL model
    with parameter R and C . (e) The modularity as a function of parameter alpha for
    different varepsilon values (0.1, 0.3, 0.5, 0.7, 0.9) marked by different colors
    respectively. (f) The modularity as a function of parameter gamma for different
    values of parameter varepsilon .
  Figure 2 Link: articels_figures_by_rev_year\2022\Complex_Network_Evolution_Model_Based_on_Turing_Pattern_Dynamics\figure_2.jpg
  Figure 2 caption: An example from the co-author network for explaining why we leverage
    Q-learning to develop a reaction-diffusion model in the low-dimensional space.
    (a) A co-author network in the computer science field. (b) The low-dimensional
    representation of the network shown in (a). (c) Three important factors that should
    be considered in modeling the reaction-diffusion process of authors in the low-dimensional
    space. (d) Three functions of Q-learning that are helpful to implement the three
    factors presented in (c).
  Figure 3 Link: articels_figures_by_rev_year\2022\Complex_Network_Evolution_Model_Based_on_Turing_Pattern_Dynamics\figure_3.jpg
  Figure 3 caption: "An example of explaining the RDQL model. (a) An initial network\
    \ in the low-dimensional space. The row number R and column number C are 3, and\
    \ the total number of states is 33. There are 3 agents in each region and this\
    \ example contains 27 agents in total. The initial values of the utility matrix\
    \ are 0. (b) The reaction process and diffusion process at time=2 . In diffusion\
    \ process, each agent moves its position based on its utility matrix and \u03B5\
    \ -greedy. In reaction process, the topology structure of the network and the\
    \ utility matrix of each agent are updated. (c)-(g) The reaction process and diffusion\
    \ process at time=17 , 35, 61, 89. In this example, agent i marked by red color\
    \ separately performs six different behaviors at six time steps."
  Figure 4 Link: articels_figures_by_rev_year\2022\Complex_Network_Evolution_Model_Based_on_Turing_Pattern_Dynamics\figure_4.jpg
  Figure 4 caption: "The low-dimensional representations of the networks generated\
    \ by the RDQL model with \u03B5=0.3 , \u03B1=0.5 and \u03B3=0.3 , network size\
    \ = 500 ( R=C=10 ) at (a) time=1 , (b) time=3 , (c) time=5 , (d) time=10 , (e)\
    \ time=20 , (f) time=30 , (g) time=50 , (h) time=70 , (i) time=100 , (j) time=130\
    \ , (k) time=160 , (l) time=200 . This figure clearly shows the formation and\
    \ evolution of communities in the network."
  Figure 5 Link: articels_figures_by_rev_year\2022\Complex_Network_Evolution_Model_Based_on_Turing_Pattern_Dynamics\figure_5.jpg
  Figure 5 caption: "The topology structures of the networks generated by the RDQL\
    \ model with \u03B5=0.3 , \u03B1=0.5 and \u03B3=0.3 , network size=500 ( R=C=10\
    \ ) at (a) time=1 , (b) time=3 , (c) time=5 , (d) time=10 , (e) time=20 , (f)\
    \ time=30 , (g) time=50 , (h) time=70 , (i) time=100 , (j) time=130 , (k) time=160\
    \ , (l) time=200 . Each subfigure (high dimension) in this figure corresponds\
    \ to a subfigure (low dimension) in Fig. 4 at the same time step."
  Figure 6 Link: articels_figures_by_rev_year\2022\Complex_Network_Evolution_Model_Based_on_Turing_Pattern_Dynamics\figure_6.jpg
  Figure 6 caption: "The impact of parameter \u03B5 on the formation of communities\
    \ in complex networks. Low-dimensional representations of the networks generated\
    \ by the RDQL model with \u03B1=0.5 , \u03B3=0.3 , network size=500 , time=200\
    \ , and (a) \u03B5=0.1 , (b) \u03B5=0.2 , (c) \u03B5=0.3 , (d) \u03B5=0.4 , (e)\
    \ \u03B5=0.5 , (f) \u03B5=0.6 , (g) \u03B5=0.7 , (h) \u03B5=0.8 , (i) \u03B5=0.9\
    \ , (j) \u03B5=1.0 . (k) Network modularity as a function of parameter \u03B5\
    \ for different values of network size : 300 (green), 500 (red), and 700 (blue).\
    \ Each point in red curve corresponds to one network representation shown in (a)-(j).\
    \ Distinct modularity phases are evident at a certain \u03B5 threshold range between\
    \ 0.6 and 0.7."
  Figure 7 Link: articels_figures_by_rev_year\2022\Complex_Network_Evolution_Model_Based_on_Turing_Pattern_Dynamics\figure_7.jpg
  Figure 7 caption: "The topological properties of the networks generated by the RDQL\
    \ model, validation results on real data, and running time. (a)-(b) Degree distributions\
    \ of generated networks for different sizes (300, 500, 700) with weight threshold\
    \ 1.5 and 1.0, respectively. (c) Illustration of the clustering coefficient as\
    \ a function of parameter \u03B5 for different values of network size . (d) Average\
    \ path length as a function of the network size for different \u03B5 values (0.3,\
    \ 0.5, 0.7). (e) The joint degree distribution of the network for \u03B5 =0.7.\
    \ (f) Degree correlation coefficients for different values of \u03B5 . (g)-(h)\
    \ Comparison between the distribution of research position switch length in generated\
    \ networks and that in real networks at 1982-1983 and 1983-1984 respectively.\
    \ (i)-(j) Cumulative running time as a function of time step for the network high-dimensional\
    \ representation and low-dimensional representation respectively."
  Figure 8 Link: articels_figures_by_rev_year\2022\Complex_Network_Evolution_Model_Based_on_Turing_Pattern_Dynamics\figure_8.jpg
  Figure 8 caption: "The performance of the heuristic model. (a)-(l) The low dimensional\
    \ representations of the networks generated by the heuristic model at different\
    \ time steps. (m) The state change proportion as a function of time step for the\
    \ heuristic nd RDQL models. (n-w) The degree distributions of the networks generated\
    \ by the heuristic model at time 3 ( \u03B5 =0.97) and 200 ( \u03B5 =0.13)."
  Figure 9 Link: articels_figures_by_rev_year\2022\Complex_Network_Evolution_Model_Based_on_Turing_Pattern_Dynamics\figure_9.jpg
  Figure 9 caption: The performances of the NW, BA and SBM models in terms of degree
    distribution (a)-(c), clustering coefficient (d)-(f), APL (g)-(i) and network
    structure (j)-(l).
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.57
  Name of the first author: Dong Li
  Name of the last author: Jiming Liu
  Number of Figures: 13
  Number of Tables: 2
  Number of authors: 3
  Paper title: Complex Network Evolution Model Based on Turing Pattern Dynamics
  Publication Date: 2022-08-08 00:00:00
  Table 1 caption: TABLE 1 The Comparisons Between Classic Network Models and RDQL
    Model
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Important Parameters in the RDQL Model
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3197276
- Affiliation of the first author: college of computer science, sichuan university,
    chengdu, china
  Affiliation of the last author: college of computer science, sichuan university,
    chengdu, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Dual_Contrastive_Prediction_for_Incomplete_MultiView_Representation_Learning\figure_1.jpg
  Figure 1 caption: "Our key observation and basic idea. In the figure, X 1 and X\
    \ 2 denote two views of a given dataset, and the corresponding representations\
    \ are denoted by Z 1 and Z 2 , respectively. The information contained in X 1\
    \ = A1 \u222A A2 \u222A A3 and X 2 = A2 \u222A A3 \u222A A5 are represented by\
    \ the solid and dotted rectangles, respectively. The area under the red rectangular\
    \ box ( Y= A3 \u222A A4) indicates the task-relevant information. Specifically,\
    \ A1 (H( X 1 | X 2 )) and A5 (H( X 2 | X 1 )) indicate the view-specific information\
    \ of X 1 and X 2 . A2 (I( X 1 ; X 2 |Y)) and A3 (I( X 1 ; X 2 ;Y)) together denote\
    \ the mutual information of X 1 and X 2 , where A3 contains task-relevant information\
    \ while A2 is task-agnostic. A4 (H(Y| X 1 , X 2 )) indicates the task-relevant\
    \ information that is unavailable from the input data. From information theory,\
    \ we propose to quantify the cross-view consistency and cross-view recoverability\
    \ using the mutual information I( Z 1 ; Z 2 ) (red area) and the conditional entropy\
    \ H( Z i | Z j ) (grey area), respectively. To learn a consistent representation\
    \ and recover the missing views, we maximize the mutual information I( Z 1 ; Z\
    \ 2 ) while minimizing the conditional entropy H( Z i | Z j ) . Subtly, the two\
    \ objectives could mutually boost and jointly optimizing both could achieve a\
    \ sufficient (i.e., A3 \u2208 Z i ) and minimal (i.e., (A1 \u222A A5) \u2209 Z\
    \ i and A2 \u2208 Z i ) representation."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Dual_Contrastive_Prediction_for_Incomplete_MultiView_Representation_Learning\figure_2.jpg
  Figure 2 caption: Overview of DCP. In the figure, we use RGB and depth data as a
    showcase. As shown, DCP consists of three joint losses, i.e., within-view reconstruction,
    dual cross-view contrastive learning, and cross-view dual prediction. Specifically,
    the within-view reconstruction loss projects each view into a view-specific subspace
    through an autoencoder. Dual contrastive learning objectives are constituted by
    the instance-level and category-level contrastive learning. In short, the instance-level
    contrastive learning loss aims to maximize the mutual information I( Z 1 ; Z 2
    ) for enhancing the cross-view consistency. The category-level contrastive learning
    loss aims to minimize the distance between an anchor (obtained by concatenating
    the view-specific representations) and a real within-class positive, while maximizing
    the distance between the anchor and a negative from the misclassified class. The
    dual prediction loss aims to recover one view from another view through the dual
    prediction G (1) and G (2) .
  Figure 3 Link: articels_figures_by_rev_year\2022\Dual_Contrastive_Prediction_for_Incomplete_MultiView_Representation_Learning\figure_3.jpg
  Figure 3 caption: Contrastive prediction for more than two views. The solid and
    dotted circles denote the available views and missing views, respectively. The
    lines indicate training with L icl and L pre , and the arrows indicate the recovery
    of missing view .
  Figure 4 Link: articels_figures_by_rev_year\2022\Dual_Contrastive_Prediction_for_Incomplete_MultiView_Representation_Learning\figure_4.jpg
  Figure 4 caption: "Clustering performance comparisons on (a) Caltech101-20 and (b)\
    \ Noisy MNIST with different missing rates ( \u03B7 )."
  Figure 5 Link: articels_figures_by_rev_year\2022\Dual_Contrastive_Prediction_for_Incomplete_MultiView_Representation_Learning\figure_5.jpg
  Figure 5 caption: "Classification performance comparisons on (a) Caltech101-20 and\
    \ (b) Noisy MNIST with different missing rates ( \u03B7 )."
  Figure 6 Link: articels_figures_by_rev_year\2022\Dual_Contrastive_Prediction_for_Incomplete_MultiView_Representation_Learning\figure_6.jpg
  Figure 6 caption: Parameter analysis on Caltech101-20.
  Figure 7 Link: articels_figures_by_rev_year\2022\Dual_Contrastive_Prediction_for_Incomplete_MultiView_Representation_Learning\figure_7.jpg
  Figure 7 caption: Clustering results of our method with the increasing alpha on
    Caltech101-20. The x-axis denotes the parameter alpha , the left and right y-axis
    denote clustering performance and information entropy, respectively.
  Figure 8 Link: articels_figures_by_rev_year\2022\Dual_Contrastive_Prediction_for_Incomplete_MultiView_Representation_Learning\figure_8.jpg
  Figure 8 caption: Data recovery on the Noisy MNIST dataset. Row 1 and 4 are complete
    views, Row 2 and 5 are missing views, and Row 3 and 6 are the recovered results
    from the complete view.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.64
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Yijie Lin
  Name of the last author: Xi Peng
  Number of Figures: 8
  Number of Tables: 5
  Number of authors: 6
  Paper title: Dual Contrastive Prediction for Incomplete Multi-View Representation
    Learning
  Publication Date: 2022-08-08 00:00:00
  Table 1 caption: TABLE 1 Multi-View Clustering Performance With More Than Two Views
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Multi-View Classification Performance With More Than Two
    Views
  Table 3 caption: "TABLE 3 Ablation Study on the Caltech101-20 Dataset, Where \u201C\
    \u2713\u201D Indicates the Used Component"
  Table 4 caption: "TABLE 4 Human Action Recognition Performance on the UWA Dataset,\
    \ Where \u201C\u2013\u201D Denotes the Method Cannot Handle Such Scenarios"
  Table 5 caption: TABLE 5 Human Action Recognition Performance on the DHA Dataset
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3197238
- Affiliation of the first author: department of artificial intelligence, korea university,
    seoul, south korea
  Affiliation of the last author: department of artificial intelligence and the department
    of brain and cognitive engineering, korea university, seoul, south korea
  Figure 1 Link: articels_figures_by_rev_year\2022\LearnExplainReinforce_Counterfactual_Reasoning_and_its_Guidance_to_Reinforce_an_\figure_1.jpg
  Figure 1 caption: "Schematic of our proposed learn-explain-reinforce (LEAR) \u2020\
    \ framework. The explanation unit is a variation of conditional GAN that can synthesize\
    \ a counterfactual map conditioned on an arbitrary target label. The reinforcement\
    \ unit provides adequate guidance from the produced counterfactual map for reinforcing\
    \ the performance of diagnostic model R . We also introduce a simple iterative\
    \ optimization scheme that enables simultaneous improvement of the explanation\
    \ and diagnostic performance."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\LearnExplainReinforce_Counterfactual_Reasoning_and_its_Guidance_to_Reinforce_an_\figure_2.jpg
  Figure 2 caption: "Schematic overview of the counterfactual map generation to induce\
    \ the cause of dementia diagnosed from the backbone network. It has major components:\
    \ counterfactual map generator and reasoning evaluator. The counterfactual map\
    \ generator synthesizes a counterfactual map M X,t conditioned on arbitrary target\
    \ label t or posterior probability t \u2032 obtained from the diagnostic model\
    \ R(X) , while the reasoning evaluator works towards enforcing target label attributes\
    \ to the synthesized map. Note that \u201C \u2295 \u201D is the operator for channel-wise\
    \ concatenation and \u201C + \u201D is the operator for element-wise addition.\
    \ X and X \xAF are two random instances drawn from the same data distribution,\
    \ i.e., X, X \xAF \u223C P X ."
  Figure 3 Link: articels_figures_by_rev_year\2022\LearnExplainReinforce_Counterfactual_Reasoning_and_its_Guidance_to_Reinforce_an_\figure_3.jpg
  Figure 3 caption: Detailed view of the counterfactual map generator (CMG). A target
    label t is tiled and channel-wise concatenated to the skip connection. This enables
    the CMG to condition the counterfactual maps to be conditioned on an arbitrary
    target condition.
  Figure 4 Link: articels_figures_by_rev_year\2022\LearnExplainReinforce_Counterfactual_Reasoning_and_its_Guidance_to_Reinforce_an_\figure_4.jpg
  Figure 4 caption: "Schematic overview of the explanation-guided attention (XGA)\
    \ module with a guidance map. A guidance map is a supervision for the XGA module\
    \ that assists in focusing on regions of pathological and morphological changes\
    \ caused by dementia on the whole-brain. XGA module learns and integrates locally\
    \ subtle changes and globally discriminative structural changes that can optionally\
    \ be supervised by the guidance map. Note that \u201C \u2299 \u201D is the operator\
    \ for element-wise multiplication and \u201C + \u201D is the operator for element-wise\
    \ addition."
  Figure 5 Link: articels_figures_by_rev_year\2022\LearnExplainReinforce_Counterfactual_Reasoning_and_its_Guidance_to_Reinforce_an_\figure_5.jpg
  Figure 5 caption: Examples of counterfactual maps for the MNIST dataset. The resulting
    synthesized image is an addition between an input and its corresponding counterfactual
    map (blue and yellow denote, respectively, subtraction and addition of the respective
    pixel values, i.e., deletion and addition of areas to be a target-labeled digit)
    conditioned on a target label.
  Figure 6 Link: articels_figures_by_rev_year\2022\LearnExplainReinforce_Counterfactual_Reasoning_and_its_Guidance_to_Reinforce_an_\figure_6.jpg
  Figure 6 caption: Example of counterfactual maps for the ADNI dataset (Subject ID
    024S0985, Image ID on top left corner). Purple, green, and orange boxes visualize
    ventricular, cortex, and hippocampus regions, respectively.
  Figure 7 Link: articels_figures_by_rev_year\2022\LearnExplainReinforce_Counterfactual_Reasoning_and_its_Guidance_to_Reinforce_an_\figure_7.jpg
  Figure 7 caption: "Example of counterfactual map conditioned on interpolated target\
    \ labels (Subject ID 123S0106, Image ID on top left corner). The purple boxes\
    \ correspond to the ventricular region. Parentheses \u22C5 and [\u22C5] for condition\
    \ indicate the posterior probability and a target condition, respectively. The\
    \ +- signs above the gray arrows denote, respectively, NCC(+) and NCC(-). Refer\
    \ to Supplementary Fig. S1, available online, for a more detailed interpolation\
    \ result of disease progression."
  Figure 8 Link: articels_figures_by_rev_year\2022\LearnExplainReinforce_Counterfactual_Reasoning_and_its_Guidance_to_Reinforce_an_\figure_8.jpg
  Figure 8 caption: Counterfactual map and CAM visualization of XGA-injected ResNet18
    on the CN versus MCI versus AD scenario with self-iterative training. The values
    at the bottom of brain images (Subject ID 005S0223) are the models softmax activated
    logits.
  Figure 9 Link: articels_figures_by_rev_year\2022\LearnExplainReinforce_Counterfactual_Reasoning_and_its_Guidance_to_Reinforce_an_\figure_9.jpg
  Figure 9 caption: Reinforced counterfactual map visualization by using iterative
    optimization on trained ResNet18 (Subject ID 131S0123, Image ID on top left corner).
    The purple, orange, and green boxes correspond to the ventricular, hippocampus,
    and cortex regions, respectively.
  First author gender probability: 0.9
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Kwanseok Oh
  Name of the last author: Heung-Il Suk
  Number of Figures: 9
  Number of Tables: 6
  Number of authors: 3
  Paper title: 'Learn-Explain-Reinforce: Counterfactual Reasoning and its Guidance
    to Reinforce an Alzheimers Disease Diagnosis Model'
  Publication Date: 2022-08-10 00:00:00
  Table 1 caption: TABLE 1 Recent Studies Categorized Into Visual Explanation, Use
    of Attention Mechanism, Explanation-Guided Methods, and Ability to Reinforce Visual
    Explanation
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Normalized Cross-Correlation (NCC) Scores With Comparison
    Methods on the ADNI Dataset
  Table 3 caption: TABLE 3 Normalized Cross-Correlation (NCC) Scores in an Ablation
    Study of the Loss Terms in Eq. (9)
  Table 4 caption: TABLE 4 Comparison of Performance (ACC) Among the Backbone, Augmentation,
    and the Attention With Guidance on ADNI Dataset
  Table 5 caption: TABLE 5 Comparison of Performance on the Multi-Class (i.e., CN
    versus MCI versus AD) Classification Scenario on the ADNI Dataset
  Table 6 caption: TABLE 6 Comparison of Performance (ACC) Among Various Iterations
    on the ADNI Dataset
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3197845
- Affiliation of the first author: delft university of technology, delft, netherlands
  Affiliation of the last author: delft university of technology, delft, netherlands
  Figure 1 Link: articels_figures_by_rev_year\2022\Improved_Generalization_in_SemiSupervised_Learning_A_Survey_of_Theoretical_Resul\figure_1.jpg
  Figure 1 caption: Data generation process used in [11].
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Improved_Generalization_in_SemiSupervised_Learning_A_Survey_of_Theoretical_Resul\figure_2.jpg
  Figure 2 caption: "Simple functional causal model [16]. The effect E is caused by\
    \ C given a deterministic mapping \u03F1 . E and C are influenced by noise variables\
    \ N E and N C , respectively."
  Figure 3 Link: articels_figures_by_rev_year\2022\Improved_Generalization_in_SemiSupervised_Learning_A_Survey_of_Theoretical_Resul\figure_3.jpg
  Figure 3 caption: "The shapes shown in (a) and (b) are two different embeddings\
    \ of a circle in the Euclidean plane. One half of the circle is labeled +1 , while\
    \ the other half is labeled as \u22121 . Everything outside the circle is labeled\
    \ +1 ."
  Figure 4 Link: articels_figures_by_rev_year\2022\Improved_Generalization_in_SemiSupervised_Learning_A_Survey_of_Theoretical_Resul\figure_4.jpg
  Figure 4 caption: "A schematic proof why the hypothesis set H c has an infinite\
    \ VC-dimension. The embedded circle, its upper half assigning points to +1 and\
    \ its lower to \u22121 , can label the seven points correctly."
  Figure 5 Link: articels_figures_by_rev_year\2022\Improved_Generalization_in_SemiSupervised_Learning_A_Survey_of_Theoretical_Resul\figure_5.jpg
  Figure 5 caption: The idea of (a) a positive and (b) a negative gamma -margin.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Alexander Mey
  Name of the last author: Marco Loog
  Number of Figures: 5
  Number of Tables: 0
  Number of authors: 2
  Paper title: 'Improved Generalization in Semi-Supervised Learning: A Survey of Theoretical
    Results'
  Publication Date: 2022-08-15 00:00:00
  Table 1 caption: Not Available
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3198175
- Affiliation of the first author: inria, centre rennes - bretagne atlantique, rennes
    cedex, france
  Affiliation of the last author: inria, centre rennes - bretagne atlantique, rennes
    cedex, france
  Figure 1 Link: articels_figures_by_rev_year\2022\EMDriven_Unsupervised_Learning_for_Efficient_Motion_Segmentation\figure_1.jpg
  Figure 1 caption: "Flowchart of the proposed CNN method for the training (top) and\
    \ inference (bottom) steps. Training step: First, we segment the optical flow\
    \ field f with the neural network g \u03D5 . Then, we get the optimal parametric\
    \ motion models f \u03B8 \u2217 k k=1,\u2026,K within each probabilistic segmentation\
    \ masks g \u03D5 (f ) k k=1,\u2026,K using (13). Finally, we update the parameters\
    \ \u03D5 of the neural network using (14), where the loss function is defined\
    \ in (12). This training step is performed iteratively over each batch B (of size\
    \ 1 in this illustration). Inference step: We directly apply the trained network\
    \ g \u03D5 \u2217 to any new unseen optical flow field f to obtain the probabilistic\
    \ segmentation masks g \u03D5 \u2217 (f ) k k=1,\u2026,K . There is no estimation\
    \ of the motion models f \u03B8 \u2217 k k=1,\u2026,K in the inference step in\
    \ contrast to the training step. For the sake of visualization, optical flows\
    \ and polynomial motion models are represented with the HSV color code, but actually,\
    \ the flow field f used as input of the neural network is taken as a 2D vector\
    \ field. We have a two-channel input. Optical flow coding: correspondence between\
    \ the arrow visualization of the optical flow field and the HSV color map."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\EMDriven_Unsupervised_Learning_for_Efficient_Motion_Segmentation\figure_2.jpg
  Figure 2 caption: "Illustration of the computation graph for the training of our\
    \ network. m\u225C g \u03D5 (f) denotes the set of arrays (as many as masks) collecting\
    \ the probability for each site of the input flow field to belong to each mask.\
    \ \u03B8 is the set of the motion model parameters, and \u03D5 the set of the\
    \ network parameters. In our method, we are alternatively optimising w.r.t. \u03B8\
    \ (optimisation 1) and \u03D5 (optimisation 2)."
  Figure 3 Link: articels_figures_by_rev_year\2022\EMDriven_Unsupervised_Learning_for_Efficient_Motion_Segmentation\figure_3.jpg
  Figure 3 caption: 'Examples of motion segmentation results obtained with our method
    with two masks, on the videos bmx-trees, breakdance-flare, scooter-black, kite-surf,
    blackswan, parkour, dogs02 and cuttlefish 1, of the DAVIS2016, FBMS and MoCA datasets.
    First row: a frame of the video with the ground-truth superimposed in yellow.
    Second row: the input flow field displayed with the HSV color code [3] that is
    depicted in Fig. 1. Third row: the segmentation produced by our method superimposed
    in green on the corresponding image.'
  Figure 4 Link: articels_figures_by_rev_year\2022\EMDriven_Unsupervised_Learning_for_Efficient_Motion_Segmentation\figure_4.jpg
  Figure 4 caption: 'Results obtained with our method for four masks ( K=4 ) regarding
    multiple motion segmentation. First row: one image of the video. Second row: input
    optical flow displayed with the HSV color code that is depicted in Fig. 1. Third
    row: motion segmentation maps with four masks, one colour per mask (the four masks
    may be not present if not necessary). We adopt the same color code for all segmentation
    maps (dark blue: mask 1, light blue: mask 2, green: mask 3, yellow: mask 4). Examples
    are drawn from DAVIS2016, FBMS59 and SegTrackV2 datasets. Videos in lexicographic
    order: mallard-fly, hockey, libby, swing, hummingbird, cars5, cars4, people2.'
  Figure 5 Link: articels_figures_by_rev_year\2022\EMDriven_Unsupervised_Learning_for_Efficient_Motion_Segmentation\figure_5.jpg
  Figure 5 caption: 'Illustration of the implicit temporal consistency ensured by
    our method with four masks. For each group, first row: input optical flow fields
    displayed with the HSV color code, second row: OFS maps. The color code is the
    same for all segmentation maps (dark blue: mask 1, light blue: mask 2, green:
    mask 3, yellow: mask 4). Examples are drawn from DAVIS2016 (from top to bottom):
    libby, dance-twirl, car-roundabout.'
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Etienne Meunier
  Name of the last author: Patrick Bouthemy
  Number of Figures: 5
  Number of Tables: 3
  Number of authors: 3
  Paper title: EM-Driven Unsupervised Learning for Efficient Motion Segmentation
  Publication Date: 2022-08-15 00:00:00
  Table 1 caption: TABLE 1 Results on DAVIS2016 Validation Dataset, SegTrackV2, FBMS59
    Validation Dataset and MoCA for Several Unsupervised and Supervised Methods (Scores
    Taken From [19], [58], [57] and [59])
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Ablation Study for Different Components of Our Method
  Table 3 caption: TABLE 3 Results on DAVIS2016 Validation Dataset, SegTrackV2, FBMS59
    Validation Dataset and MoCA Obtained With Our Method for Several Numbers of Masks
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3198480
- Affiliation of the first author: school of computer science and technology, east
    china normal university, shanghai, china
  Affiliation of the last author: school of computer science and technology, east
    china normal university, shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Multiview_Unsupervised_Shapelet_Learning_for_Multivariate_Time_Series_Clustering\figure_1.jpg
  Figure 1 caption: The framework of the proposed multiview unsupervised shapelet
    learning with adaptive neighbors (MUSLA) model. MUSLA not only treats shapelet-transformed
    representations learned from shapelets of different lengths as different views
    (e.g., S1 and S2 , S3 and S4 , and S5 and S6 are analyzed in different views),
    but also learns the importance of each view and the neighbor graph matrix among
    multiview representations when candidate multivariate shapelets of different lengths
    are determined. Besides, it uses the nearest neighbor graph structure constraint
    across different views to learn salient multivariate shapelets of different lengths.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Multiview_Unsupervised_Shapelet_Learning_for_Multivariate_Time_Series_Clustering\figure_2.jpg
  Figure 2 caption: Performance comparisons of USLA with isometric multivariate shapelets
    and USLA with multivariate shapelets of different lengths on benchmark datasets.
  Figure 3 Link: articels_figures_by_rev_year\2022\Multiview_Unsupervised_Shapelet_Learning_for_Multivariate_Time_Series_Clustering\figure_3.jpg
  Figure 3 caption: Performance comparisons of MUSLA and USLA with multivariate shapelets
    of different lengths on benchmark datasets.
  Figure 4 Link: articels_figures_by_rev_year\2022\Multiview_Unsupervised_Shapelet_Learning_for_Multivariate_Time_Series_Clustering\figure_4.jpg
  Figure 4 caption: Variations in the performance of USLA with respect to the number
    and the length of the multivariate shapelets.
  Figure 5 Link: articels_figures_by_rev_year\2022\Multiview_Unsupervised_Shapelet_Learning_for_Multivariate_Time_Series_Clustering\figure_5.jpg
  Figure 5 caption: "Variations in the performance of USLA with respect to the length\
    \ of the multivariate shapelets and \u03BC ."
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.94
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.89
  Name of the first author: Nan Zhang
  Name of the last author: Shiliang Sun
  Number of Figures: 5
  Number of Tables: 5
  Number of authors: 2
  Paper title: Multiview Unsupervised Shapelet Learning for Multivariate Time Series
    Clustering
  Publication Date: 2022-08-16 00:00:00
  Table 1 caption: TABLE 1 General Notations
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Details of the Ten Multivariate Time Series Benchmark Datasets
  Table 3 caption: TABLE 3 Performance Comparisons of MUSLA and Contrast Algorithms
    in Terms of RI
  Table 4 caption: TABLE 4 Performance Comparisons of MUSLA and Contrast Algorithms
    in Terms of NMI
  Table 5 caption: TABLE 5 Running Time (Seconds) of MUSLA and Contrast Algorithms
    on Benchmark Datasets
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3198411
