- Affiliation of the first author: key laboratory of behavior sciences, institute
    of psychology, chinese academy of sciences, beijing, china
  Affiliation of the last author: state key laboratory of brain and cognitive science,
    institute of psychology, chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\CASME_A_Third_Generation_Facial_Spontaneous_MicroExpression_Database_With_Depth_\figure_1.jpg
  Figure 1 caption: Neural tracts for FE. Voluntary and involuntary expressions are
    controlled by the pyramidal tract (blue trajectory) and extrapyramidal tract (red
    trajectory), respectively. In particular, both the pyramidal system and the extrapyramidal
    downward transmission begin in the precentral gyrus of the cerebral cortex. In
    the pyramidal tract, the cortical nucleus tract (blue trajectory) is a neural
    pathway that controls facial muscles and bones and is responsible for voluntary
    expression. Meantime, for the extrapyramidal tract, the complete pathway (red
    trajectory) travels from the cerebral cortex through the brainstem to areas such
    as the red nucleus and substantia nigra, then through cranial nerves and down
    to the facial nucleus, responsible for unconscious expressions.
  Figure 10 Link: articels_figures_by_rev_year\2022\CASME_A_Third_Generation_Facial_Spontaneous_MicroExpression_Database_With_Depth_\figure_10.jpg
  Figure 10 caption: The CIT questions during the interrogation are related to key
    clues in the red blocks and the room layout in Fig. 11a.
  Figure 2 Link: articels_figures_by_rev_year\2022\CASME_A_Third_Generation_Facial_Spontaneous_MicroExpression_Database_With_Depth_\figure_2.jpg
  Figure 2 caption: ME analysis (MEA) process, including MES and MER, respectively
    locating the moment when the ME occurs in the video and classifying the ME video
    clip.
  Figure 3 Link: articels_figures_by_rev_year\2022\CASME_A_Third_Generation_Facial_Spontaneous_MicroExpression_Database_With_Depth_\figure_3.jpg
  Figure 3 caption: MEA research trend. The number of articles on MEA is increasing
    yearly, mainly in the area of MER (bottom column). MES research has not yet attracted
    sufficient attention (top column).
  Figure 4 Link: articels_figures_by_rev_year\2022\CASME_A_Third_Generation_Facial_Spontaneous_MicroExpression_Database_With_Depth_\figure_4.jpg
  Figure 4 caption: Lab configuration. The subject wore 3D Active Glasses to watch
    3D ME videos. The subject sat at a chair and adjusted the table to the appropriate
    height. A BenQ TH6370 projector was located under the table and projected to a
    screen that was placed 3.5 meters away from the subject. The subject used a Bluetooth
    keyboard to respond. The control computer was placed at the ride side of the subject.
  Figure 5 Link: articels_figures_by_rev_year\2022\CASME_A_Third_Generation_Facial_Spontaneous_MicroExpression_Database_With_Depth_\figure_5.jpg
  Figure 5 caption: "Experimental procedure. First, the subjects were required to\
    \ press the \u201Dspace\u201D key to start a ME video clip, which was played only\
    \ once. Then the subjects answered three questions including emotional valence,\
    \ emotional type, and emotional intensity according to their own feelings. The\
    \ practice phase consisted of 3 trials, and the formal experiment consisted of\
    \ 30 trials."
  Figure 6 Link: articels_figures_by_rev_year\2022\CASME_A_Third_Generation_Facial_Spontaneous_MicroExpression_Database_With_Depth_\figure_6.jpg
  Figure 6 caption: Schematic diagram of results. The figure shows the mean values
    of emotion valence, emotion type, and emotion intensity under 2D and 3D viewing
    conditions. means p<0.05, means p<0.01, indicating significant difference.
  Figure 7 Link: articels_figures_by_rev_year\2022\CASME_A_Third_Generation_Facial_Spontaneous_MicroExpression_Database_With_Depth_\figure_7.jpg
  Figure 7 caption: Depth contours on the face during a facial ME. On the subjects
    face, from onset to apex frames, an AU 14 (Dimpler) occurs at the corner of the
    mouth. In both lower face frames (onset and apex) with depth contour, the same
    colors have the same depth values, varying from 510 mm to 545 mm. In the corners
    of the mouth and cheek regions, the contour lines have different shapes in the
    two frames (as indicated by yellow arrows). This difference confirms that the
    FE of the face can be reflected in the depth information.
  Figure 8 Link: articels_figures_by_rev_year\2022\CASME_A_Third_Generation_Facial_Spontaneous_MicroExpression_Database_With_Depth_\figure_8.jpg
  Figure 8 caption: 'The left block shows the recording environment for CAS(ME) 3
    database. The upper right block illustrates the samples of RGB image and depth
    map (Subject spNO.216 in Part A: Surprise with AU R1+R2); and the lower right
    block displays the samples of voice and electrodermal activity (EDA) signals (Subject
    MC11 in Part C: negative emotion with AU4).'
  Figure 9 Link: articels_figures_by_rev_year\2022\CASME_A_Third_Generation_Facial_Spontaneous_MicroExpression_Database_With_Depth_\figure_9.jpg
  Figure 9 caption: Labeled ME samples distribution in Part A.
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Jingting Li
  Name of the last author: Xiaolan Fu
  Number of Figures: 16
  Number of Tables: 7
  Number of authors: 9
  Paper title: 'CAS(ME)3: A Third Generation Facial Spontaneous Micro-Expression Database
    With Depth Information and High Ecological Validity'
  Publication Date: 2022-05-13 00:00:00
  Table 1 caption: TABLE 1 The Comparison Result for 2D and 3D Artificial MER Performance
    Analysis
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The Overview of CAS(ME) 3 3
  Table 3 caption: TABLE 3 Quantitative Report of the Reference to the Emotional Variety
    in Part B
  Table 4 caption: TABLE 4 Multi-Modality Signals, Including Physiological Signals
    and Voice Signal
  Table 5 caption: TABLE 5 MES Performance (F1-Score) Comparison Among Different ME
    Databases
  Table 6 caption: TABLE 6 MER Performance Comparison Among Different ME Databases
  Table 7 caption: TABLE 7 Multimodal Analysis on MER for Part C
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3174895
- Affiliation of the first author: inception institute of artificial intelligence
    (iiai), abu dhabi, united arab emirates
  Affiliation of the last author: terminus group, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\ResNetLDDMM_Advancing_the_LDDMM_Framework_Using_Deep_Residual_Networks\figure_1.jpg
  Figure 1 caption: Qualitative comparison of ResNet-LDDMM (third column) to state-of-the-art
    approaches including Functional Maps (FM) framework and Supervised and Unsupervised
    FMNet on human-like artistic models (the six last registration results are previously
    reported by Halimi et al. in [25]).
  Figure 10 Link: articels_figures_by_rev_year\2022\ResNetLDDMM_Advancing_the_LDDMM_Framework_Using_Deep_Residual_Networks\figure_10.jpg
  Figure 10 caption: 'Quantitative evaluation (Geodesic Error Curves) on SHREC2020
    and comparison with existing approaches including variants of the Functional Maps
    framework, as reported in [59]. Considered posesdeformations, from left to right:
    twist, indent, inflate, stretch, and overall.'
  Figure 2 Link: articels_figures_by_rev_year\2022\ResNetLDDMM_Advancing_the_LDDMM_Framework_Using_Deep_Residual_Networks\figure_2.jpg
  Figure 2 caption: "Overview of our joint ResNet-LDDMM diffeomorphic registration\
    \ framework \u2013 the ResNet architecture is shown on the left, comprising of\
    \ an ensemble of successive two-layers ReLU networks followed by a dimensionality\
    \ reduction layer (the ensemble is called a building block). Each predicts a time-dependent\
    \ velocity field f l (., \u03B8 l ) . On the right, we show the flow of obtaining\
    \ velocity fields and diffeomorphisms \u03A6 l applied to the source shape, including\
    \ the end-point \u03A6 L , achieved by integration. In Computational Anatomy (e.g.,\
    \ [42]), it is assumed that observations of the same anatomical organ are elements\
    \ of a common shape Orbit, so that [ q S ] and [ q T ] coincide. The Networks\
    \ width m is the number of filters."
  Figure 3 Link: articels_figures_by_rev_year\2022\ResNetLDDMM_Advancing_the_LDDMM_Framework_Using_Deep_Residual_Networks\figure_3.jpg
  Figure 3 caption: "A pictorial summary of our diffeomorphic registration results.\
    \ Top: a geodesic path connecting a source hand shape to a target shape; Bottom:\
    \ the inverse path (source and target change roles); Center-right: both flows\
    \ of the time-dependent velocity fields f l (., \u03B8 l ) , building blocks of\
    \ our ResNet-LDDMM. Center-left: final euclidean displacements and registration\
    \ results."
  Figure 4 Link: articels_figures_by_rev_year\2022\ResNetLDDMM_Advancing_the_LDDMM_Framework_Using_Deep_Residual_Networks\figure_4.jpg
  Figure 4 caption: Geodesic paths connecting source (left) and target (right) hand
    shapes modulo deformations. Last row provides and example of a wrong transformation
    (index and middle fingers interchanged).
  Figure 5 Link: articels_figures_by_rev_year\2022\ResNetLDDMM_Advancing_the_LDDMM_Framework_Using_Deep_Residual_Networks\figure_5.jpg
  Figure 5 caption: 'Geodesic paths connecting source (first column) and target anatomical
    shapes (last column) of different subjects. From top to bottom: whole heart, liver,
    femur 1-to-2 (left) followed by femur 2-to-1 (right), heart valve, cortex and
    LV+RV (left and right ventricles of pathological hearts).'
  Figure 6 Link: articels_figures_by_rev_year\2022\ResNetLDDMM_Advancing_the_LDDMM_Framework_Using_Deep_Residual_Networks\figure_6.jpg
  Figure 6 caption: "LDDMM versus ResNet-LDDMM \u2013 registration results of hand\
    \ shapes with fingers moving in opposite directions."
  Figure 7 Link: articels_figures_by_rev_year\2022\ResNetLDDMM_Advancing_the_LDDMM_Framework_Using_Deep_Residual_Networks\figure_7.jpg
  Figure 7 caption: Four examples to compare ResNet-LDDMM (ours) to LDDMM.
  Figure 8 Link: articels_figures_by_rev_year\2022\ResNetLDDMM_Advancing_the_LDDMM_Framework_Using_Deep_Residual_Networks\figure_8.jpg
  Figure 8 caption: 'Deformable registration achieved by ResNet-LDDMM (geodesic paths
    and computed velocity fields): Difficult examples where source and target shapes
    have different topology (highlighted in red circles).'
  Figure 9 Link: articels_figures_by_rev_year\2022\ResNetLDDMM_Advancing_the_LDDMM_Framework_Using_Deep_Residual_Networks\figure_9.jpg
  Figure 9 caption: 'Evaluation on SHREC2019 and comparison with existing methods
    (from [58]). From left to right : test-set0 (articulated deformations), test-set1
    (near-isometric deformations), test-set2 (non-isometric deformations), test-set3
    (topological and geometric changes), all test-sets.'
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Boulbaba Ben Amor
  Name of the last author: Ling Shao
  Number of Figures: 10
  Number of Tables: 0
  Number of authors: 3
  Paper title: 'ResNet-LDDMM: Advancing the LDDMM Framework Using Deep Residual Networks'
  Publication Date: 2022-05-13 00:00:00
  Table 1 caption: Not Available
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3174908
- Affiliation of the first author: wangxuan institute of computer technology, peking
    university, beijing, china
  Affiliation of the last author: wangxuan institute of computer technology, peking
    university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Deep_Point_Set_Resampling_via_Gradient_Fields\figure_1.jpg
  Figure 1 caption: "A conceptual illustration of the proposed deep point set resampling\
    \ method. We first estimate the global gradient field of the degradation-convolved\
    \ distribution \u2207 x log[(p\u2217h)(x)] from the input degraded point cloud.\
    \ Then, we perform gradient ascent using the estimated gradient field to converge\
    \ points to the underlying surface for point cloud restoration."
  Figure 10 Link: articels_figures_by_rev_year\2022\Deep_Point_Set_Resampling_via_Gradient_Fields\figure_10.jpg
  Figure 10 caption: "A gradient ascent trajectory of our point cloud upsampling every\
    \ other 10 steps. The first row is obtained by \u201DOurs-Naive,\u201D while the\
    \ second row is from \u201DOurs-Gen\u201D."
  Figure 2 Link: articels_figures_by_rev_year\2022\Deep_Point_Set_Resampling_via_Gradient_Fields\figure_2.jpg
  Figure 2 caption: "An overview of the proposed network architecture. Note that points\
    \ x c j \u2208 N r (x) are sampled from the context point cloud."
  Figure 3 Link: articels_figures_by_rev_year\2022\Deep_Point_Set_Resampling_via_Gradient_Fields\figure_3.jpg
  Figure 3 caption: "An illustration of the proposed network architecture. h [\u2217\
    ] represents the feature dimension, where h c , h r and h out represent the dimension\
    \ of the context feature, relative feature and aggregated feature, respectively."
  Figure 4 Link: articels_figures_by_rev_year\2022\Deep_Point_Set_Resampling_via_Gradient_Fields\figure_4.jpg
  Figure 4 caption: A toy example to illustrate the comparison between resampling
    results without regularization and those with regularization. An appropriate regularization
    leads to sharper boundaries.
  Figure 5 Link: articels_figures_by_rev_year\2022\Deep_Point_Set_Resampling_via_Gradient_Fields\figure_5.jpg
  Figure 5 caption: An illustration of the difference between the proposed global
    gradient field estimation and the local one in [22]. In (a), boldsymbolxi and
    boldsymbolx represent a point in the input point cloud and some 3D coordinate
    nearby boldsymbolxi , respectively. Li denotes the training objective for the
    local score function of boldsymbolxi in [22]. In (b), boldsymbolx represents some
    3D coordinate in mathbb R3 , and boldsymbolxj in mathcal Nr(boldsymbolx) denotes
    boldsymbolxj is in the neighborhood of boldsymbolx with radius r . mathcal S is
    a distribution of boldsymbolx in the mathbb R3 space, and boldsymbolg(boldsymbolx)
    represents the estimated gradient field of boldsymbolx .
  Figure 6 Link: articels_figures_by_rev_year\2022\Deep_Point_Set_Resampling_via_Gradient_Fields\figure_6.jpg
  Figure 6 caption: Visual comparison of point cloud denoising methods under (a) Isotropic
    Gaussian noise, (b) simulated LiDAR noise. Points colored yellower are farther
    away from the ground truth surface.
  Figure 7 Link: articels_figures_by_rev_year\2022\Deep_Point_Set_Resampling_via_Gradient_Fields\figure_7.jpg
  Figure 7 caption: Visual comparison of denoising results on the real-world dataset
    Paris-rue-Madame [54].
  Figure 8 Link: articels_figures_by_rev_year\2022\Deep_Point_Set_Resampling_via_Gradient_Fields\figure_8.jpg
  Figure 8 caption: A gradient ascent trajectory of our point cloud denoising every
    other 10 steps.
  Figure 9 Link: articels_figures_by_rev_year\2022\Deep_Point_Set_Resampling_via_Gradient_Fields\figure_9.jpg
  Figure 9 caption: An illustration of the estimated gradient vectors at different
    iterations. The actual lengths of gradient vectors are all magnified by 14 times
    for ease of visualization.
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Haolan Chen
  Name of the last author: Wei Hu
  Number of Figures: 12
  Number of Tables: 6
  Number of authors: 4
  Paper title: Deep Point Set Resampling via Gradient Fields
  Publication Date: 2022-05-16 00:00:00
  Table 1 caption: TABLE 1 Comparison Among Competitive Denoising Algorithms Under
    Isotropic Gaussian Noise
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison Among Competitive Denoising AlgorithmsUnder
    Various Types of Noise
  Table 3 caption: "TABLE 3 Ablation Studies on the Effectiveness of the Global Gradient\
    \ Field (\u201DGlobal\u201D), the Continuity of Our Model, and Resampling With\
    \ Regularization"
  Table 4 caption: TABLE 4 Evaluation of the Generalizability of Our Model by Reducing
    the Size of the Training Set
  Table 5 caption: TABLE 5 Comparison Among Our Model and State-of-The-Art Point Cloud
    Upsampling Methods
  Table 6 caption: TABLE 6 Ablation Studies to Examine the Effect of the Coarse Generator
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3175183
- Affiliation of the first author: technologies of vision lab, fondazione bruno kessler,
    trento, italy
  Affiliation of the last author: technologies of vision lab, fondazione bruno kessler,
    trento, italy
  Figure 1 Link: articels_figures_by_rev_year\2022\Learning_General_and_Distinctive_D_Local_Deep_Descriptors_for_Point_Cloud_Regist\figure_1.jpg
  Figure 1 caption: 'GeDis training pipeline. Two overlapping point clouds are aligned
    using a ground-truth transformation ( T ). From the overlap region (grey) we randomly
    sample b patch centroids (black). We use a Siamese approach to train two deep
    neural networks with shared parameters concurrently. Prior to the deep network
    processing, we perform the following operations: (i) for each centre (black) a
    patch (red) with radius r is extracted and the corresponding local reference frame
    is computed using a subset of the points of the patch; (ii) this patch is canonicalised
    using the local reference frame and n points are randomly sampled from the patch
    (green points); (iii) the coordinates of these n points are represented relative
    to the patch centre and normalised to obtain patches with unitary radius; (iv)
    these n points are given to the deep networks as input to learn the descriptor.
    The L2-normalisation layer is used to output descriptors with unitary norm. We
    train the deep network using the hardest-contrastive loss [10].'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Learning_General_and_Distinctive_D_Local_Deep_Descriptors_for_Point_Cloud_Regist\figure_2.jpg
  Figure 2 caption: Feature-matching recall as a function (left) of the inlier-ratio
    and (right) of the inlier-distance thresholds [5].
  Figure 3 Link: articels_figures_by_rev_year\2022\Learning_General_and_Distinctive_D_Local_Deep_Descriptors_for_Point_Cloud_Regist\figure_3.jpg
  Figure 3 caption: Colour-coded distances (red=large, blue=small) between a descriptor
    and all the other descriptors. The queried descriptor is located on the chairs
    seat in the centre of the image. We used descriptor radii equal to .6 (top) and
    3 (bottom). Each figure reports the training iteration (iter) and the average
    distance with respect to all the other descriptors.
  Figure 4 Link: articels_figures_by_rev_year\2022\Learning_General_and_Distinctive_D_Local_Deep_Descriptors_for_Point_Cloud_Regist\figure_4.jpg
  Figure 4 caption: Qualitative registration results on (a) ETH, (b,d) KITTI and (c,e)
    3DMatch datasets using GeDi descriptors trained on 3DMatch. The point clouds are
    in their original reference frame before registration (left-hand side column)
    and in a common reference frame after registration (right-hand side column). We
    show correct and incorrect registration results, and highlight regions of interest
    to facilitate the analysis of the results.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Fabio Poiesi
  Name of the last author: Davide Boscaini
  Number of Figures: 4
  Number of Tables: 8
  Number of authors: 2
  Paper title: Learning General and Distinctive 3D Local Deep Descriptors for Point
    Cloud Registration
  Publication Date: 2022-05-16 00:00:00
  Table 1 caption: "TABLE 1 Feature-Matching Recall ( \u039E \u039E): 3DMatch (training)\
    \ \u2192 \u2192 ETH (testing)"
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 Relative Translational Error (RTE) and Relative Rotation\
    \ Error (RRE): 3DMatch (training) \u2192 \u2192 KITTI (testing)"
  Table 3 caption: "TABLE 3 Feature-Matching Recall ( \u039E \u039E): KITTI (training)\
    \ \u2192 \u2192 3DMatch (testing)"
  Table 4 caption: "TABLE 4 Feature-Matching Recall ( \u039E \u039E): 3DMatch (training)\
    \ \u2192 \u2192 3DMatch (testing)"
  Table 5 caption: "TABLE 5 Relative Translational Error (RTE) and Relative Rotation\
    \ Error (RRE): KITTI (training) \u2192 \u2192 KITTI (testing)"
  Table 6 caption: "TABLE 6 Ablation Study Using the Feature-Matching Recall ( \u039E\
    \ \u039E) as a Function of the Number of Sampled Points on the 3DMatch dataset\
    \ [9]"
  Table 7 caption: TABLE 7 Ablation Study on Different GeDis Implementation settings
  Table 8 caption: TABLE 8 RRE Statistics Before and After QNet Computed on the Ablation
    Dataset
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3175371
- Affiliation of the first author: beijing university of posts and telecommunications,
    beijing, china
  Affiliation of the last author: beijing university of posts and telecommunications,
    beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\OPOM_Customized_Invisible_Cloak_Towards_Face_Privacy_Protection\figure_1.jpg
  Figure 1 caption: Illustration of person-specific (class-wise) universal privacy
    masks. A regular user can generate one privacy mask only once, and then apply
    it to all his or her photographs and videos. In the training process for mask
    generation, the proposed OPOM method optimizes each training sample in the direction
    away from the feature subspace of the source identity. To make full use of the
    limited training images, several modeling methods, including affine hulls, class
    centers and convex hulls, have been investigated to obtain a better description
    of the feature subspace of the source identities. In the testing process, with
    the customized mask, the protected images will not be recognized as the original
    identity. Compared with image-specific protection, person-specific masks protect
    the privacy of regular users in a friendlier way.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\OPOM_Customized_Invisible_Cloak_Towards_Face_Privacy_Protection\figure_2.jpg
  Figure 2 caption: With better approximation methods (red) for the feature subspace
    of each identity in the mask generation process, both individual universality
    and model transferability can be improved, which will lead to more effective privacy
    protection. Experimental results on two datasets (Privacy-Commons and Privacy-Celebrities)
    are shown.
  Figure 3 Link: articels_figures_by_rev_year\2022\OPOM_Customized_Invisible_Cloak_Towards_Face_Privacy_Protection\figure_3.jpg
  Figure 3 caption: "Some protected images in the Privacy-Celebrities dataset with\
    \ masks ( \u03B5=8 ), generated by OPOM-AffineHull, OPOM-ClassCenter, and OPOM-ConvexHull\
    \ respectively."
  Figure 4 Link: articels_figures_by_rev_year\2022\OPOM_Customized_Invisible_Cloak_Towards_Face_Privacy_Protection\figure_4.jpg
  Figure 4 caption: Protection against Commercial APIs (Amazon [62], Microsoft [63],
    Baidu [64] and Face++ [65]). Fifty identities in the Privacy-Commons dataset,
    each with 5 test images are used for the face verification test. The normalized
    average similarityconfidence scores are shown (lower is better). The original
    scores are listed above the bar.
  Figure 5 Link: articels_figures_by_rev_year\2022\OPOM_Customized_Invisible_Cloak_Towards_Face_Privacy_Protection\figure_5.jpg
  Figure 5 caption: Some failure cases, as well as the corresponding training samples
    and successful easily protected samples for analysis. Each row represents an identity.
    The privacy masks generated with OPOM can generalize to different testing images
    to some degree. However, if there are obvious differences between the testing
    images and the training samples, such as, large poses, different illuminations,
    and occlusions, the mask protection tends to break down.
  Figure 6 Link: articels_figures_by_rev_year\2022\OPOM_Customized_Invisible_Cloak_Towards_Face_Privacy_Protection\figure_6.jpg
  Figure 6 caption: Comparison of universal, person-specific (class-wise) and image-specific
    masks in terms of effectiveness and efficiency.
  Figure 7 Link: articels_figures_by_rev_year\2022\OPOM_Customized_Invisible_Cloak_Towards_Face_Privacy_Protection\figure_7.jpg
  Figure 7 caption: Application of OPOM in video privacy protection, Sherlock [66].
    The first row shows the original frames. The second row shows the modified frames
    with the privacy masks of Sherlock generated from other images of actors (Benedict
    Cumberbatch). The cosine similarity between the deep features of the detected
    face in the video and the deep features of the corresponding character (Sherlock
    Holmes) is used to show the effectiveness.
  Figure 8 Link: articels_figures_by_rev_year\2022\OPOM_Customized_Invisible_Cloak_Towards_Face_Privacy_Protection\figure_8.jpg
  Figure 8 caption: Application of OPOM in video privacy protection, Sherlock [66].
    The privacy masks are trained with other face images of actors (Benedict Cumberbatch
    and Martin Freeman). The average cosine similarity between the deep features of
    the detected face in the video and the deep features of the corresponding characters
    (Sherlock Holmes and Doctor John Watson) is used to demonstrate the effectiveness.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.81
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Yaoyao Zhong
  Name of the last author: Weihong Deng
  Number of Figures: 8
  Number of Tables: 6
  Number of authors: 2
  Paper title: 'OPOM: Customized Invisible Cloak Towards Face Privacy Protection'
  Publication Date: 2022-05-19 00:00:00
  Table 1 caption: TABLE 1 Comparison of Methods to Generate Privacy Masks With Adversarial
    Examples to Protect Images Against Malicious Face Recognition Systems
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 Comparison of Different Methods to Generate Person-Specific\
    \ Privacy Masks ( \u03B5=8 \u025B=8) From a Single Source Model to Protect Face\
    \ Images Against Black-Box Models"
  Table 3 caption: "TABLE 3 Comparison of Different Methods to Generate Person-Specific\
    \ Privacy Masks ( \u03B5=8 \u025B=8) From a Single Source Model to Protect Face\
    \ Images Against Black-Box Models. We report Top-1 and Top-5 protection success\
    \ rate under 1:N identification setting of the Privacy-Celebrities dataset. The\
    \ higher protection success rate is better"
  Table 4 caption: "TABLE 4 Comparison of Different Methods Combined With the Momentum\
    \ Boosting Method [32] and DFANet [34] to Generate More Transferable Person-Specific\
    \ Privacy Masks ( \u03B5=8 \u025B=8) From a Single Source Model to Protect Face\
    \ Images Against Black-Box Models"
  Table 5 caption: "TABLE 5 Comparison of Different Methods Combined With the Momentum\
    \ Boosting Method [32] and DFANet [34] to Generate More Transferable Person-Specific\
    \ Privacy Masks ( \u03B5=8 \u025B=8) From a Single Source Model to Protect Face\
    \ Images Against Black-Box Models"
  Table 6 caption: "TABLE 6 Diverse Person-Specific Privacy Masks ( \u03B5=8 \u025B\
    =8) From a Single Source Model to Protect Face Images Against Black-Box Models"
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3175602
- Affiliation of the first author: school of electronic, electrical, and communication
    engineering, university of chinese academy of sciences, beijing, china
  Affiliation of the last author: school of electronic, electrical, and communication
    engineering, university of chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Dynamic_Support_Network_for_FewShot_Class_Incremental_Learning\figure_1.jpg
  Figure 1 caption: The forgetting and over-fitting issues in FSCIL.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Dynamic_Support_Network_for_FewShot_Class_Incremental_Learning\figure_2.jpg
  Figure 2 caption: "Dynamic support network (DSN). (a) A scale-fixed network with\
    \ a feature space of limited representation capacity. Embedding examples of new\
    \ classes into the feature space causes distribution crumbling. (b) DSN refers\
    \ to an adaptively updating network with compressive node expansion. When the\
    \ network is trained upon new classes, it tentatively expands network nodes to\
    \ learn new class features and then compresses the redundant nodes to provide\
    \ compact feature representation. Meanwhile, the old class distribution in this\
    \ feature space is recalled to regularize incremental learning. DSN dynamically\
    \ \u201Csupports\u201D the feature space and feature distribution to avoid class\
    \ confusion."
  Figure 3 Link: articels_figures_by_rev_year\2022\Dynamic_Support_Network_for_FewShot_Class_Incremental_Learning\figure_3.jpg
  Figure 3 caption: Network expansion and compression.
  Figure 4 Link: articels_figures_by_rev_year\2022\Dynamic_Support_Network_for_FewShot_Class_Incremental_Learning\figure_4.jpg
  Figure 4 caption: Distribution recalling for feature space support. (a) In the current
    learning session, the new class distribution is estimated by the old class distributions.
    (b) In the next learning session, feature vectors are sampled from old classes
    to regularize the incremental learning.
  Figure 5 Link: articels_figures_by_rev_year\2022\Dynamic_Support_Network_for_FewShot_Class_Incremental_Learning\figure_5.jpg
  Figure 5 caption: Feature spaces of the baseline approach and our DSN approach.
    Adding new classes to the feature space projected by the baseline causes distribution
    crumbling, which can be solved by DSN.
  Figure 6 Link: articels_figures_by_rev_year\2022\Dynamic_Support_Network_for_FewShot_Class_Incremental_Learning\figure_6.jpg
  Figure 6 caption: Evolution of the indicator vector ( boldsymbolalpha ) for self-activated
    node compression. (Lighter color denotes larger values).
  Figure 7 Link: articels_figures_by_rev_year\2022\Dynamic_Support_Network_for_FewShot_Class_Incremental_Learning\figure_7.jpg
  Figure 7 caption: Activation ratio of the extended nodes during training.
  Figure 8 Link: articels_figures_by_rev_year\2022\Dynamic_Support_Network_for_FewShot_Class_Incremental_Learning\figure_8.jpg
  Figure 8 caption: Confusion matrices between old classes (0-189) and new classed
    (190-199). (Best viewed in color with zoom).
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.88
  Name of the first author: Boyu Yang
  Name of the last author: Qixiang Ye
  Number of Figures: 8
  Number of Tables: 7
  Number of authors: 7
  Paper title: Dynamic Support Network for Few-Shot Class Incremental Learning
  Publication Date: 2022-05-19 00:00:00
  Table 1 caption: "TABLE 1 Performance Comparison on CUB200 Using the Resnet18 Backbone.\
    \ \u201CPD\u201D Denotes the Performance Drop. \u201CPR\u201D Denotes the Performance\
    \ Retention"
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Performance Comparison on CIFAR100 Using the ResNet18 Backbone
  Table 3 caption: TABLE 3 Performance Comparison on miniImageNet Using the ResNet18
    Backbone
  Table 4 caption: "TABLE 4 Ablation Study of DSN on CUB Dataset Using the Resnet18\
    \ Backbone. \u201Cimages\u201D Denotes the Old Class Recalling Using the Old Class\
    \ Images. \u201Cfeatures\u201D Denotes the Old Class Recalling Using the Sampled\
    \ Features"
  Table 5 caption: "TABLE 5 Comparison of Compressive Node Expansion on Different\
    \ Network Layers. \u201CFC\u201D Denotes the Fully Connected Layer"
  Table 6 caption: "TABLE 6 Comparison of Distribution Estimation Strategies. \u201C\
    DE\u201D Estimates New Class Distributions Using Base Class Distributions (Classes\
    \ of the 0- th th Session). \u201CDEO\u201D Estimates New Class Distributions\
    \ Using Old Class Distributions (Classes From 0- th th to (t\u22121) (t-1)- th\
    \ th Sessions)"
  Table 7 caption: TABLE 7 Comparison of Distribution Sampling Strategies
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3175849
- Affiliation of the first author: school of computer and communication engineering,
    university of science and technology beijing (ustb), beijing, china
  Affiliation of the last author: school of computer and communication engineering,
    institute of artificial intelligence, university of science and technology beijing
    (ustb), beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Arbitrary_Shape_Text_Detection_via_Segmentation_With_Probability_Maps\figure_1.jpg
  Figure 1 caption: 'Differences of annotations: (a) box annotations for object detection,
    (b) pixel-level annotations for instance segmentation, (c) polygon annotations
    for curved text detection. (c) text instance masks obtained by polygon annotations,
    and (d) text masks generated by polygon annotations. General instance segmentation
    datasets usually provide two annotations: object box and pixel-level mask. However,
    scene text detection datasets only provide coarse-grained annotations.'
  Figure 10 Link: articels_figures_by_rev_year\2022\Arbitrary_Shape_Text_Detection_via_Segmentation_With_Probability_Maps\figure_10.jpg
  Figure 10 caption: Comparisons of segmentation results. (a) Ground-truth annotation
    boundaries; (b) Segmentation results of text instances supervised by binary maps;
    (c) The generated contours from (b) via threshold filtering (Algorithm 1); (d)
    Segmentation results of text instances supervised by probability maps; (e) The
    generated contours from (d) via Algorithm 1.
  Figure 2 Link: articels_figures_by_rev_year\2022\Arbitrary_Shape_Text_Detection_via_Segmentation_With_Probability_Maps\figure_2.jpg
  Figure 2 caption: Representative detection results (enclosed by green contours)
    of arbitrary texts on challenging scene images from public datasets.
  Figure 3 Link: articels_figures_by_rev_year\2022\Arbitrary_Shape_Text_Detection_via_Segmentation_With_Probability_Maps\figure_3.jpg
  Figure 3 caption: Framework of the proposed method. The backbone extracts the shared
    features, and 16 chancel fusion features are fed into the iterative model to iteratively
    generate the prediction of probability maps.
  Figure 4 Link: articels_figures_by_rev_year\2022\Arbitrary_Shape_Text_Detection_via_Segmentation_With_Probability_Maps\figure_4.jpg
  Figure 4 caption: Architecture of the backbone. To capture multi-scale text instances,
    we adopt ResNet-50 as the backbone which is combined with a multi-level fusion
    strategy. The up-sample is performed by deconvolution. The 14, 18, 116, and 132
    represent different scales of the feature maps. The 16, 32, 64, and 256 represent
    the number of output feature maps. The H, W, C represent the width, height, and
    the number of channels of input image, respectively.
  Figure 5 Link: articels_figures_by_rev_year\2022\Arbitrary_Shape_Text_Detection_via_Segmentation_With_Probability_Maps\figure_5.jpg
  Figure 5 caption: Illustration of the relation between probability distribution
    and the distance to the boundary. (a) Some sampling points and boundaries (green
    contour) are annotated in labels, (b) Distance map, and (c) Boundaries of different
    sizes, where the green contour is annotated in labels. The number on the contour
    represents its probability value.
  Figure 6 Link: articels_figures_by_rev_year\2022\Arbitrary_Shape_Text_Detection_via_Segmentation_With_Probability_Maps\figure_6.jpg
  Figure 6 caption: 'Illustration of different probability distribution curves generated
    by SAF , BF and LF : (a) Cross-section curves of different mapping function for
    text instance, (b) Probability distribution curves for text instance with scale
    30, and (c) Probability distribution curves for text instance with scale 100.
    The thb refers to binary threshold in Algorithm 1 and Algorithm 2.The horizontal
    axis represents the distance from the text pixel to the boundary, and the vertical
    axis represents the probability value.'
  Figure 7 Link: articels_figures_by_rev_year\2022\Arbitrary_Shape_Text_Detection_via_Segmentation_With_Probability_Maps\figure_7.jpg
  Figure 7 caption: The generation of different probability map labels, where the
    annotation of text polygon is visualized in red lines, and lbrace G0, G1,...,Gnrbrace
    are the corresponding results using SAF with different alphas.
  Figure 8 Link: articels_figures_by_rev_year\2022\Arbitrary_Shape_Text_Detection_via_Segmentation_With_Probability_Maps\figure_8.jpg
  Figure 8 caption: The structure of iterative module, where f0 represents the feature
    extracted by backbone, and the set lbrace P1,P2,...,Pi,...,Pnrbrace is the prediction
    of probability maps.
  Figure 9 Link: articels_figures_by_rev_year\2022\Arbitrary_Shape_Text_Detection_via_Segmentation_With_Probability_Maps\figure_9.jpg
  Figure 9 caption: Ablation study on iterative steps (n) and the step of alpha value
    (k). The results are only based on Total-Text dataset without pre-training.
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.96
  Name of the first author: Shi-Xue Zhang
  Name of the last author: Xu-Cheng Yin
  Number of Figures: 13
  Number of Tables: 5
  Number of authors: 5
  Paper title: Arbitrary Shape Text Detection via Segmentation With Probability Maps
  Publication Date: 2022-05-20 00:00:00
  Table 1 caption: TABLE 1 Ablation Study for Probability Maps (PMS) and Iterative
    Module (IM) on Total-Text and MSRA-TD500
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Experimental Results on Total-Text, CTW-1500, and MSRA-TD500
  Table 3 caption: TABLE 3 Experimental Results on ICDAR17-MLT
  Table 4 caption: TABLE 4 Experimental Results on ICDAR2015
  Table 5 caption: TABLE 5 Analysis of Time Consumption
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3176122
- Affiliation of the first author: department of computer science and engineering,
    michigan state university, east lansing, mi, usa
  Affiliation of the last author: department of computer science and engineering,
    michigan state university, east lansing, mi, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\Spoof_Trace_Disentanglement_for_Generic_Face_AntiSpoofing\figure_1.jpg
  Figure 1 caption: "The proposed approach can detect spoof faces, disentangle the\
    \ spoof traces, and reconstruct the live counterparts. It can be applied to diverse\
    \ spoof types and estimate distinct traces (e.g., Moir\xE9 pattern in replay attack,\
    \ artificial eyebrow and wax in makeup attack, color distortion in print attack,\
    \ and specular highlights in 3D mask attack). Zoom in for details."
  Figure 10 Link: articels_figures_by_rev_year\2022\Spoof_Trace_Disentanglement_for_Generic_Face_AntiSpoofing\figure_10.jpg
  Figure 10 caption: Examples of the spoof data synthesis. The first row are the source
    spoof faces, the first column are the target live faces, and the remaining are
    the synthesized spoof faces from the live face with the corresponding spoof traces.
    .
  Figure 2 Link: articels_figures_by_rev_year\2022\Spoof_Trace_Disentanglement_for_Generic_Face_AntiSpoofing\figure_2.jpg
  Figure 2 caption: The comparison of different deep-learning based face anti-spoofing.
    (a) direct FAS only provides a binary decision of spoofness; (b) auxiliary FAS
    can provide simple interpretation of spoofness. M denotes the auxiliary task,
    such as depth map estimation; (c) generative FAS can provide more intuitive interpretation
    of spoofness, but only for a limited number of spoof attacks; (d) the proposed
    method can provide spoof trace estimation for generic face spoof attacks.
  Figure 3 Link: articels_figures_by_rev_year\2022\Spoof_Trace_Disentanglement_for_Generic_Face_AntiSpoofing\figure_3.jpg
  Figure 3 caption: "The overall training workflow of Spoof Trace Disentanglement\
    \ Network (STDN+). Gen denotes the Generator, and it takes the input faces, estimate\
    \ the pseudo depth map M , and disentangle the spoof trace components P, I P ,\
    \ T A . With the spoof trace components, live reconstruction G(\u22C5) and spoof\
    \ synthesis G \u2212 (\u22C5) can be done based on Eqn. (5) and Eqn. (9). Disc\
    \ denotes the Multi-scale Discriminators."
  Figure 4 Link: articels_figures_by_rev_year\2022\Spoof_Trace_Disentanglement_for_Generic_Face_AntiSpoofing\figure_4.jpg
  Figure 4 caption: "The proposed STDN+ network architecture. Except the last layer,\
    \ each conv and transposed conv is concatenated with a batch normalizstion layer\
    \ and a leaky ReLU layer. k3c64s2 indicates the kernel size of 3\xD73 , the convolution\
    \ channel of 64 and the stride of 2."
  Figure 5 Link: articels_figures_by_rev_year\2022\Spoof_Trace_Disentanglement_for_Generic_Face_AntiSpoofing\figure_5.jpg
  Figure 5 caption: 'The visualization of image decomposition for different input
    faces: (a) live face (b) 3D mask attack (c) replay attack (d) print attack.'
  Figure 6 Link: articels_figures_by_rev_year\2022\Spoof_Trace_Disentanglement_for_Generic_Face_AntiSpoofing\figure_6.jpg
  Figure 6 caption: The online 3D warping layer. (a) Given the corresponding dense
    offset, we warp the spoof trace and add them to the target live face to create
    a new spoof. E.g. pixel (x,y) with offset (3,5) is warped to pixel (x+3,y+5) in
    the new image. (b) To obtain a dense offsets from the spare offsets of the selected
    face shape vertices, Delaunay triangulation interpolation is adopted.
  Figure 7 Link: articels_figures_by_rev_year\2022\Spoof_Trace_Disentanglement_for_Generic_Face_AntiSpoofing\figure_7.jpg
  Figure 7 caption: 'Preliminary mask P 0 for the negative term in inpainting mask
    loss. White pixels denote 1 and black pixels denote 0. White indicates the area
    should not be inpainted. P 0 for: (a) print, replay; (b) 3D mask and makeup; (c)
    partial attacks that cover the eye portion; (d) partial attacks that cover the
    mouth portion.'
  Figure 8 Link: articels_figures_by_rev_year\2022\Spoof_Trace_Disentanglement_for_Generic_Face_AntiSpoofing\figure_8.jpg
  Figure 8 caption: Examples of each spoof trace components. (a) the input sample
    faces. (b) B . (c) C . (d) T . (e) P . (f) the final live counterpart reconstruction
    and zoom-in details. (g) results from [29]. (h) results from Step1+Step2 with
    a single trace representation. .
  Figure 9 Link: articels_figures_by_rev_year\2022\Spoof_Trace_Disentanglement_for_Generic_Face_AntiSpoofing\figure_9.jpg
  Figure 9 caption: "Examples of spoof trace disentanglement on SiW (a-h) and SiW-M\
    \ (i-x). (a)-(d) items are print attacks and (e)-(h) items are replay attacks.\
    \ (i)-(x) items are live, print, replay, half mask, silicone mask, paper mask,\
    \ transparent mask, obfuscation makeup, impersonation makeup, cosmetic makeup,\
    \ paper glasses, partial paper, funny eye glasses, and mannequin head. The first\
    \ column is the input face, the second column is the overall spoof trace ( I\u2212\
    \ I ), the third column is the reconstructed live. ."
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.84
  Name of the first author: Yaojie Liu
  Name of the last author: Xiaoming Liu
  Number of Figures: 13
  Number of Tables: 9
  Number of authors: 2
  Paper title: Spoof Trace Disentanglement for Generic Face Anti-Spoofing
  Publication Date: 2022-05-20 00:00:00
  Table 1 caption: TABLE 1 The Evaluation on Four Protocols in OULU-NPU
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The Evaluation on Three Protocols in SiW Dataset
  Table 3 caption: TABLE 3 Cross Testing on CASIA-MFSD Vs. Replay-Attack
  Table 4 caption: TABLE 4 AUC ( % %) of the Model Testing on CASIA, Replay, and MSU-MFSD
  Table 5 caption: 'TABLE 5 The Evaluation and Ablation Study on SiW-M Protocol I:
    Known Spoof Detection'
  Table 6 caption: 'TABLE 6 The Evaluation on SiW-M Protocol II: Unknown Spoof Detection'
  Table 7 caption: TABLE 7 Confusion Matrices of Spoof Mediums Classification Based
    on Spoof Traces
  Table 8 caption: TABLE 8 Confusion Matrices of 6-Class Spoof Traces Classification
    on SiW-M Database
  Table 9 caption: 'TABLE 9 The Evaluation on SiW-M Protocol III: Openset Spoof Detection'
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3176387
- Affiliation of the first author: cas key laboratory of gipas, university of science
    and technology of china, hefei, china
  Affiliation of the last author: cas key laboratory of gipas, university of science
    and technology of china, hefei, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Masked_Contrastive_Representation_Learning_for_Reinforcement_Learning\figure_1.jpg
  Figure 1 caption: "The training framework of M-CURL. Inputs are first processed\
    \ by an CNN encoder \u03B8 to extract representations. The left part is a policy\
    \ network \u03C9 for decision making. The right part is for masked contrastive\
    \ training, which consists of a non-linear layer \u03C8 and Transformer module\
    \ \u03C6 to reconstruct the masked input."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Masked_Contrastive_Representation_Learning_for_Reinforcement_Learning\figure_2.jpg
  Figure 2 caption: "Training strategy of M-CURL. \u201CSG\u201D stands for \u201C\
    stop gradient\u201D."
  Figure 3 Link: articels_figures_by_rev_year\2022\Masked_Contrastive_Representation_Learning_for_Reinforcement_Learning\figure_3.jpg
  Figure 3 caption: Results of CURL and our method on DMControl benchmark. Our method
    outperforms CURL on 14 out of 16 selected environments. The shaded region represents
    the standard deviation of the average return over 5 trials, and curves are smoothed
    uniformly for visual clarity.
  Figure 4 Link: articels_figures_by_rev_year\2022\Masked_Contrastive_Representation_Learning_for_Reinforcement_Learning\figure_4.jpg
  Figure 4 caption: Aggregate metrics on Atari100 K based on 26 games with 95% CIs.
    Higher mean, median and IQM scores and lower optimality gap are better. The CIs
    are estimated using the percentile bootstrap with stratified sampling. All results
    except M-CURL are cited from [59]. The results of M-CURL are from 10 runs with
    different seeds. .
  Figure 5 Link: articels_figures_by_rev_year\2022\Masked_Contrastive_Representation_Learning_for_Reinforcement_Learning\figure_5.jpg
  Figure 5 caption: Performance profiles on Atari100 K based on score distributions
    (left, recommended by [59]) and average score distributions (right).
  Figure 6 Link: articels_figures_by_rev_year\2022\Masked_Contrastive_Representation_Learning_for_Reinforcement_Learning\figure_6.jpg
  Figure 6 caption: Ablation study on Hopper-Stand environment.
  Figure 7 Link: articels_figures_by_rev_year\2022\Masked_Contrastive_Representation_Learning_for_Reinforcement_Learning\figure_7.jpg
  Figure 7 caption: Results of ablation study.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.61
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jinhua Zhu
  Name of the last author: Houqiang Li
  Number of Figures: 7
  Number of Tables: 3
  Number of authors: 8
  Paper title: Masked Contrastive Representation Learning for Reinforcement Learning
  Publication Date: 2022-05-20 00:00:00
  Table 1 caption: TABLE 1 Results of Our Method (Implemented on Top of Eff. Rainbow
    [26]) and Baselines on 26 Environments From Atari 2600 Games at 100 K Timesteps
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison of Training Time (1 K Interaction Steps) Between
    CURL and M-CURL
  Table 3 caption: TABLE 3 Comparison With Other Data Augmentation Methods on DMControl
    Benchmark
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3176413
- Affiliation of the first author: institute of artificial intelligence, beihang university,
    beijing, china
  Affiliation of the last author: institute of artificial intelligence, beihang university,
    beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Adversarial_Sticker_A_Stealthy_Attack_Method_in_the_Physical_World\figure_1.jpg
  Figure 1 caption: Examples of pasting stickers on the face and traffic sign in our
    life, respectively.
  Figure 10 Link: articels_figures_by_rev_year\2022\Adversarial_Sticker_A_Stealthy_Attack_Method_in_the_Physical_World\figure_10.jpg
  Figure 10 caption: The predicted probabilities of ground-truth labels before and
    after attacks in the physical environment. The numbers next to the vertical line
    represent the difference in the probability on each model.
  Figure 2 Link: articels_figures_by_rev_year\2022\Adversarial_Sticker_A_Stealthy_Attack_Method_in_the_Physical_World\figure_2.jpg
  Figure 2 caption: Some examples of stickers used in our experiments.
  Figure 3 Link: articels_figures_by_rev_year\2022\Adversarial_Sticker_A_Stealthy_Attack_Method_in_the_Physical_World\figure_3.jpg
  Figure 3 caption: Examples reflecting the regional aggregation of positions for
    successful attacks. The top row shows the distribution of these positions, and
    the bottom row shows the probabilities of the ground-truth label t and predicted
    wrong label t versus the distance to the center.
  Figure 4 Link: articels_figures_by_rev_year\2022\Adversarial_Sticker_A_Stealthy_Attack_Method_in_the_Physical_World\figure_4.jpg
  Figure 4 caption: The process of bending and rotating the sticker (the yellow dot
    indicates the highest point of the pasting area).
  Figure 5 Link: articels_figures_by_rev_year\2022\Adversarial_Sticker_A_Stealthy_Attack_Method_in_the_Physical_World\figure_5.jpg
  Figure 5 caption: The transformation process of the sticker pattern. The top row
    is the schematic diagram of variable annotation involved in the deformation, the
    second row shows the change of sticker patterns on the 2D X-Y plane, and the third
    row is the stereogram of the corresponding sticker.
  Figure 6 Link: articels_figures_by_rev_year\2022\Adversarial_Sticker_A_Stealthy_Attack_Method_in_the_Physical_World\figure_6.jpg
  Figure 6 caption: Failed attack results when k is small (e.g., k=5 ). The image
    in the red box is the adversarial example. We can find that the retrieved images
    are still relevant to adversarial query image after the attack even though its
    original top- k images are subverted.
  Figure 7 Link: articels_figures_by_rev_year\2022\Adversarial_Sticker_A_Stealthy_Attack_Method_in_the_Physical_World\figure_7.jpg
  Figure 7 caption: Illustration for the proposed iterative query image retrieval.
  Figure 8 Link: articels_figures_by_rev_year\2022\Adversarial_Sticker_A_Stealthy_Attack_Method_in_the_Physical_World\figure_8.jpg
  Figure 8 caption: Examples of attacks using different stickers. For each group,
    the three images correspond to the un-attacked original image, the image after
    attacks, and the image corresponding to the predicted wrong class after attacks.
    The black text denotes the predicted correct name and the red text denotes the
    predicted wrong name after attacks.
  Figure 9 Link: articels_figures_by_rev_year\2022\Adversarial_Sticker_A_Stealthy_Attack_Method_in_the_Physical_World\figure_9.jpg
  Figure 9 caption: Comparisons of our meaningful Adv-sticker with other attack methods
    (Adv-hat [9], Adv-glasses [8], [10]) for FR systems. Our approach uses real stickers
    without relying on the generated perturbation patterns and printed accessories.
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.59
  Name of the first author: Xingxing Wei
  Name of the last author: Jie Yu
  Number of Figures: 18
  Number of Tables: 9
  Number of authors: 3
  Paper title: 'Adversarial Sticker: A Stealthy Attack Method in the Physical World'
  Publication Date: 2022-05-23 00:00:00
  Table 1 caption: TABLE 1 The Results of Attacks on the Face Classification Task
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The Distribution Percentage of the Three Stickers in Different
    Positions When the Attack is Successful
  Table 3 caption: TABLE 3 Comparisons of the Fooling Rate and Average Time With Two
    SOTA Physical Methods for Face Recognition Systems in the Black-Box Setting
  Table 4 caption: TABLE 4 The Fooling Rate (FR) and the Number of Queries (NQ) of
    the Ablation Study
  Table 5 caption: TABLE 5 The Percentage of Video Frames Successfully Attacked When
    Different Subjects Continuously Change Their Face Postures in the Physical Environment
  Table 6 caption: TABLE 6 The Fooling Rate (FR) and the Number of Queries (NQ) After
    Adversarial Training, and the Changes Compared to the Undefended Situations (Shown
    in Brackets)
  Table 7 caption: TABLE 7 The Results of Attacks on the Face Identification Task
  Table 8 caption: TABLE 8 The Attack Results for Image Retrieval Using Adversarial
    Stickers
  Table 9 caption: TABLE 9 The Quantitative Attack Performance for Traffic Sign Recognition
    Using an Adversarial Sticker
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3176760
