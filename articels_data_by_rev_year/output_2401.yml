- Affiliation of the first author: smart data analytics, university of bonn, bonn,
    germany
  Affiliation of the last author: smart data analytics, university of bonn, bonn,
    germany
  Figure 1 Link: articels_figures_by_rev_year\2021\Bringing_Light_Into_the_Dark_A_LargeScale_Evaluation_of_Knowledge_Graph_Embeddin\figure_1.jpg
  Figure 1 caption: 'Exemplary KG: Nodes represent entities and edges their respective
    relations.'
  Figure 10 Link: articels_figures_by_rev_year\2021\Bringing_Light_Into_the_Dark_A_LargeScale_Evaluation_of_Knowledge_Graph_Embeddin\figure_10.jpg
  Figure 10 caption: Overall hits10 results for FB15K-237 where box-plots summarize
    the results across different combinations of interaction models, training approaches,
    loss functions, and the explicit usage of inverse relations.
  Figure 2 Link: articels_figures_by_rev_year\2021\Bringing_Light_Into_the_Dark_A_LargeScale_Evaluation_of_Knowledge_Graph_Embeddin\figure_2.jpg
  Figure 2 caption: An example embedding of the entities and relations from the knowledge
    graph portrayed by Fig. 2.
  Figure 3 Link: articels_figures_by_rev_year\2021\Bringing_Light_Into_the_Dark_A_LargeScale_Evaluation_of_Knowledge_Graph_Embeddin\figure_3.jpg
  Figure 3 caption: Visualization of different training approaches for the relation
    worksat in the KG in Fig. 1. Red color indicates positive examples, i.e. true
    triples present in the KG. Dark blue color denotes triples used as negative examples
    in LCWA. Light blue color sampling candidates for negative examples in sLCWA.
    Yellow color indicates triples that are not considered.
  Figure 4 Link: articels_figures_by_rev_year\2021\Bringing_Light_Into_the_Dark_A_LargeScale_Evaluation_of_Knowledge_Graph_Embeddin\figure_4.jpg
  Figure 4 caption: Overall hits10 results for Kinships where box-plots summarize
    the best results across different configurations, i.e., combinations of interaction
    models, training approaches, loss functions, and the explicit usage of inverse
    relations.
  Figure 5 Link: articels_figures_by_rev_year\2021\Bringing_Light_Into_the_Dark_A_LargeScale_Evaluation_of_Knowledge_Graph_Embeddin\figure_5.jpg
  Figure 5 caption: Impact of training approach on the performance for a fixed interaction
    model and loss function for the Kinships dataset based on Adam.
  Figure 6 Link: articels_figures_by_rev_year\2021\Bringing_Light_Into_the_Dark_A_LargeScale_Evaluation_of_Knowledge_Graph_Embeddin\figure_6.jpg
  Figure 6 caption: Impact of explicitly modeling inverse relations on the performance
    for a fixed loss function for the Kinships dataset.
  Figure 7 Link: articels_figures_by_rev_year\2021\Bringing_Light_Into_the_Dark_A_LargeScale_Evaluation_of_Knowledge_Graph_Embeddin\figure_7.jpg
  Figure 7 caption: Overall hits10 results for WN18RR where box-plots summarize the
    results across different combinations of interaction models, training approaches,
    loss functions, and the explicit usage of inverse relations.
  Figure 8 Link: articels_figures_by_rev_year\2021\Bringing_Light_Into_the_Dark_A_LargeScale_Evaluation_of_Knowledge_Graph_Embeddin\figure_8.jpg
  Figure 8 caption: Impact of training approach on the performance for a fixed interaction
    model and loss function for the WN18RR dataset.
  Figure 9 Link: articels_figures_by_rev_year\2021\Bringing_Light_Into_the_Dark_A_LargeScale_Evaluation_of_Knowledge_Graph_Embeddin\figure_9.jpg
  Figure 9 caption: Impact of explicitly modeling inverse relations on the performance
    for a fixed loss function for the WN18RR dataset.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Mehdi Ali
  Name of the last author: Jens Lehmann
  Number of Figures: 15
  Number of Tables: 3
  Number of authors: 9
  Paper title: 'Bringing Light Into the Dark: A Large-Scale Evaluation of Knowledge
    Graph Embedding Models Under a Unified Framework'
  Publication Date: 2021-11-04 00:00:00
  Table 1 caption: TABLE 1 Existing Benchmark Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Frequency of Detected Relation Patterns Across the Benchmark
    Datasets
  Table 3 caption: TABLE 3 Summary of Main Insights Over All Datasets
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3124805
- Affiliation of the first author: department of electrical engineering, technion
    institute of technology, haifa, israel
  Affiliation of the last author: department of computer science, technion institute
    of technology, haifa, israel
  Figure 1 Link: articels_figures_by_rev_year\2021\AdaLISTA_Learned_Solvers_Adaptive_to_Varying_Models\figure_1.jpg
  Figure 1 caption: Ada-LISTA architecture as an iterative model (top), and its unfolded
    version for three iterations (bottom). The input dictionary D is embedded in the
    architecture, while the matrices W 1 , W 2 are free to be learned. For clarity,
    the sample indices of y,D, x 3 have been omitted.
  Figure 10 Link: articels_figures_by_rev_year\2021\AdaLISTA_Learned_Solvers_Adaptive_to_Varying_Models\figure_10.jpg
  Figure 10 caption: Patch-wise validation error versus unfoldings.
  Figure 2 Link: articels_figures_by_rev_year\2021\AdaLISTA_Learned_Solvers_Adaptive_to_Varying_Models\figure_2.jpg
  Figure 2 caption: MSE performance under column permutations.
  Figure 3 Link: articels_figures_by_rev_year\2021\AdaLISTA_Learned_Solvers_Adaptive_to_Varying_Models\figure_3.jpg
  Figure 3 caption: MSE performance for noisy dictionaries with decreasing SNR values.
  Figure 4 Link: articels_figures_by_rev_year\2021\AdaLISTA_Learned_Solvers_Adaptive_to_Varying_Models\figure_4.jpg
  Figure 4 caption: MSE performance for random dictionaries with increasing cardinality.
  Figure 5 Link: articels_figures_by_rev_year\2021\AdaLISTA_Learned_Solvers_Adaptive_to_Varying_Models\figure_5.jpg
  Figure 5 caption: MSE performance under column permutations and noisy inputs.
  Figure 6 Link: articels_figures_by_rev_year\2021\AdaLISTA_Learned_Solvers_Adaptive_to_Varying_Models\figure_6.jpg
  Figure 6 caption: MSE performance for noisy dictionaries and noisy inputs.
  Figure 7 Link: articels_figures_by_rev_year\2021\AdaLISTA_Learned_Solvers_Adaptive_to_Varying_Models\figure_7.jpg
  Figure 7 caption: MSE performance under random dictionaries and noisy inputs.
  Figure 8 Link: articels_figures_by_rev_year\2021\AdaLISTA_Learned_Solvers_Adaptive_to_Varying_Models\figure_8.jpg
  Figure 8 caption: MSE performance for the experiment of noisy dictionaries with
    SNR of 20dB . Robust-ALISTA represents the clean version of the algorithm proposed
    in [21]. As can be seen, our proposed Ada-LISTA method significantly outperforms
    this algorithm while almost reaching the Oracle-LITSA performances.
  Figure 9 Link: articels_figures_by_rev_year\2021\AdaLISTA_Learned_Solvers_Adaptive_to_Varying_Models\figure_9.jpg
  Figure 9 caption: 'Image inpainting results. Left to right: original image, corrupted
    image, ISTA, FISTA, and Ada-LISTA.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Aviad Aberdam
  Name of the last author: Michael Elad
  Number of Figures: 11
  Number of Tables: 1
  Number of authors: 3
  Paper title: 'Ada-LISTA: Learned Solvers Adaptive to Varying Models'
  Publication Date: 2021-11-04 00:00:00
  Table 1 caption: TABLE 1 PSNR Results for Image Inpainting With 50% 50% Missing
    Pixels and K=20 K=20 Unfoldings
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3125041
- Affiliation of the first author: center for research in intelligent systems, university
    of california, riverside, riverside, ca, usa
  Affiliation of the last author: center for research in intelligent systems, university
    of california, riverside, riverside, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\Privacy_Preserving_Defense_For_Black_Box_Classifiers_Against_OnLine_Adversarial_\figure_1.jpg
  Figure 1 caption: Overall framework of our approach. Faces are masked to hide the
    identity of individuals.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Privacy_Preserving_Defense_For_Black_Box_Classifiers_Against_OnLine_Adversarial_\figure_2.jpg
  Figure 2 caption: Illustration of (a) KD-1 using the Fashion MNIST dataset, (b)
    KD-2 using the MIO-TCD dataset, and (c) KD-3 using the MS-Celeb dataset.
  Figure 3 Link: articels_figures_by_rev_year\2021\Privacy_Preserving_Defense_For_Black_Box_Classifiers_Against_OnLine_Adversarial_\figure_3.jpg
  Figure 3 caption: (a) - (d) show the average amount of purification versus the number
    of iterations of purification, (e) - (h) show the average Aleatoric uncertainties
    and, (i) - (l) show the average Epistemic uncertainties for the GTSRB [80], MIO-TCD
    [60], Tiny-ImageNet [84], and MS-Celeb [59] datasets, respectively.
  Figure 4 Link: articels_figures_by_rev_year\2021\Privacy_Preserving_Defense_For_Black_Box_Classifiers_Against_OnLine_Adversarial_\figure_4.jpg
  Figure 4 caption: Example of images from the (a) Fashion-MNIST [78], (b) CIFAR-10
    [79], (c) GTSRB [80], (d) MIO-TCD [60], (e) Tiny-ImageNet [84], and (f) MS-Celeb
    [59] datasets, respectively. For ethical concerns and in order to preserve the
    identities of the celebrities in the MS-Celeb dataset we have masked the top portion
    of the faces.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.65
  Name of the first author: Rajkumar Theagarajan
  Name of the last author: Bir Bhanu
  Number of Figures: 4
  Number of Tables: 17
  Number of authors: 2
  Paper title: Privacy Preserving Defense For Black Box Classifiers Against On-Line
    Adversarial Attacks
  Publication Date: 2021-11-08 00:00:00
  Table 1 caption: TABLE 1 Summary of Related Work for White Box Adversarial Defense
  Table 10 caption: TABLE 10 Performance Comparison of Our Defense on the Tiny ImageNet
    Dataset Using the KD-2 Approach
  Table 2 caption: TABLE 2 Summary of Related Work for Black Box Adversarial Defense
  Table 3 caption: TABLE 3 Summary of the Related Work for Knowledge Distillation
  Table 4 caption: TABLE 4 CNN Architectures
  Table 5 caption: TABLE 5 Summary of the Datasets Used in This Paper
  Table 6 caption: TABLE 6 Performance Evaluation and Comparison of Our KD Approaches
    With Respect to the Teacher (Black Box) Classifier
  Table 7 caption: TABLE 7 Performance Evaluation of Adversarial Detection Using Bayesian
    Uncertainties
  Table 8 caption: TABLE 8 Performance Evaluation of Purifying Non-Adversarial Images
  Table 9 caption: TABLE 9 Performance Comparison of Our Defense on the GTSRB Dataset
    Using the KD-1 Approach With Z = 25% and 50%
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3125931
- Affiliation of the first author: samsung ai center, toronto, on, canada
  Affiliation of the last author: samsung ai center, toronto, on, canada
  Figure 1 Link: articels_figures_by_rev_year\2021\A_Little_Bit_More_BitplaneWise_BitDepth_Recovery\figure_1.jpg
  Figure 1 caption: The ground truth N -bit high-bit-depth (HBD) image, its q -bit
    quantized low-bit-depth (LBD) version, and our recovered N -bit HBD output are
    shown. Our proposed bit-depth recovery algorithm restores the q -bit quantized
    input image to its original bit depth of N by recovering the lost (N - q) bits
    (shown in red), one bitplane at a time. The green box denotes a zoomed-in region
    of the image. While (q,N) =(8,10) or (8,12) are common target use cases in HBD
    displays or photo editing applications, most printers and monitors (and even this
    papers PDF format) are designed for 8-bit. Therefore, for the purpose of this
    example, we use (q,N) =(4,8) so that the quantization effects can be visually
    observed in print and on screen.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\A_Little_Bit_More_BitplaneWise_BitDepth_Recovery\figure_2.jpg
  Figure 2 caption: Bit-depth recovery from a bitplane perspective. Existing DNN methods
    are trained to predict a residual image which when added to the input q -bit quantized
    LBD image produces the desired N -bit HBD output image. Interpreted as bitplanes,
    recovering the residual is equivalent to recovering the (N - q) bitplanes lost
    during quantization. As opposed to existing methods that predict the full residual
    in a single shot, we propose to recover it one bitplane at a time using a multi-level-supervised
    training strategy. Specifically, we train (N - q) separate networks in a supervised
    fashion where the inputtarget pairs for each network are obtained by applying
    the appropriate level of quantization to the ground truth HBD image.
  Figure 3 Link: articels_figures_by_rev_year\2021\A_Little_Bit_More_BitplaneWise_BitDepth_Recovery\figure_3.jpg
  Figure 3 caption: "Overview of our proposed approach. We train (N - q) separate\
    \ DNNs to recover the (N - q) bitplanes lost during quantization. Here, N is the\
    \ bit depth of the desired HBD image, and q the bit depth of the input LBD image.\
    \ The input-target pairs ( I q+k\u22121 , B N\u2212(q+k) ) , k =1 to (N - q) ,\
    \ for training each network can be computed directly from the HBD ground truth\
    \ image O . The target B N\u2212(q+k) is a binary map, and the problem reduces\
    \ to one of binary image segmentation, which we optimize using a binary cross\
    \ entropy loss. At test time, the q -bit quantized input LBD image I q is passed\
    \ through the trained networks sequentially, with each network increasing the\
    \ bit depth by one until the image is restored to its desired bit depth of N .\
    \ In particular, each individual network predicts the next bitplane, which is\
    \ then weighted by the corresponding bit position, and added back to the input\
    \ image to increment its bit depth by one. This estimated image forms the input\
    \ to the next network, and so forth."
  Figure 4 Link: articels_figures_by_rev_year\2021\A_Little_Bit_More_BitplaneWise_BitDepth_Recovery\figure_4.jpg
  Figure 4 caption: A diagram of our network architecture. We train (N - q) networks,
    one for each bitplane lost during quantization. Compared to existing DNN architectures
    for bit-depth recovery that use encoder-decoder modules, dense feature concatenation
    layers, multiple streams, and more, our architecture is a standard ResNet-style
    [22] structure with a series of residual blocks. Even with just four residual
    blocks (i.e., D =4), we can achieve state-of-the-art results based on our bitplane-wise
    training framework.
  Figure 5 Link: articels_figures_by_rev_year\2021\A_Little_Bit_More_BitplaneWise_BitDepth_Recovery\figure_5.jpg
  Figure 5 caption: Qualitative comparisons on the natural image Kodak dataset [27]
    and the animated image ESPL v2 dataset [28] for 3 to 8 bit recovery.
  Figure 6 Link: articels_figures_by_rev_year\2021\A_Little_Bit_More_BitplaneWise_BitDepth_Recovery\figure_6.jpg
  Figure 6 caption: PSNR (dB) versus number of model parameters. Our parameters is
    the total for all bitplanes.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Abhijith Punnappurath
  Name of the last author: Michael S. Brown
  Number of Figures: 6
  Number of Tables: 7
  Number of authors: 2
  Paper title: 'A Little Bit More: Bitplane-Wise Bit-Depth Recovery'
  Publication Date: 2021-11-08 00:00:00
  Table 1 caption: "TABLE 1 Results on Sintel Dataset [24]. The Best Results are Reported\
    \ in Bold and Red. The Second, Third, and Fourth Best-Performing Methods are Shown\
    \ in Green, Blue, and Yellow, Respectively. Numbers Copied Directly from the Original\
    \ Papers are Marked with a \u2020 \u2020 Symbol."
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Results on MIT-Adobe FiveK Dataset [25]
  Table 3 caption: TABLE 3 Results on TESTIMAGES 1200 Dataset [26]
  Table 4 caption: TABLE 4 Results on Kodak Dataset [27]. NR Denotes that a Score
    was Not Reported in the Original Paper.
  Table 5 caption: TABLE 5 Results on ESPL v2 Dataset [28]
  Table 6 caption: TABLE 6 Two Ablation Studies Where we Examine the Effects of Changing
    (i) The Target, and (ii) The Loss Function. Results Reported are for 4 to 8 Bit
    Recovery Using the D4 Model. Best Results are in Bold.
  Table 7 caption: TABLE 7 TESTIMAGES 1200 Dataset [26] for 4 to 8-Bit Recovery, With
    D D=16. To Compute Metrics, We use as Ground Truth the Image Quantized upto the
    Corresponding Bit Depth. Performance Improves as each Lost Bitplane (from 5 to
    8) is Recovered.
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3125692
- Affiliation of the first author: department of computing, imperial college london,
    london, u.k.
  Affiliation of the last author: department of computing, imperial college london,
    london, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2021\AvatarMe_Facial_Shape_and_BRDF_Inference_With_Photorealistic_RenderingAware_GANs\figure_1.jpg
  Figure 1 caption: 'From left to right: Input image from LFW [1]; AvatarMe++ predicted
    reflectance (diffuse albedo, diffuse normals, specular albedo and specular normals);
    Rendered predictions with our photorealistic differentiable rendering; Rendered
    reconstruction in different environments.'
  Figure 10 Link: articels_figures_by_rev_year\2021\AvatarMe_Facial_Shape_and_BRDF_Inference_With_Photorealistic_RenderingAware_GANs\figure_10.jpg
  Figure 10 caption: Consistency of AvatarMe++ on varying conditions, from the Digital
    Emily Project [80]. We calculate on average 30.94 PSNR and 0.0007 MSE between
    our results. Compared to the ground truth from [80], we achieve on average 0.0083
    MSE and 20.13 PSNR on albedo and and 0.011 MSE and 24.02 PSNR on normals.
  Figure 2 Link: articels_figures_by_rev_year\2021\AvatarMe_Facial_Shape_and_BRDF_Inference_With_Photorealistic_RenderingAware_GANs\figure_2.jpg
  Figure 2 caption: Example of a captured subject data using [31], [32], registered
    and projected to a standard UV topology.
  Figure 3 Link: articels_figures_by_rev_year\2021\AvatarMe_Facial_Shape_and_BRDF_Inference_With_Photorealistic_RenderingAware_GANs\figure_3.jpg
  Figure 3 caption: "Summary of the AvatarMe++ method: Given an \u201Cin-the-wild\u201D\
    \ image I , we first fit a 3D Morphable Model (3DMM) to acquire the shape S O\
    \ , texture T and shape normals N O in UV space. Then, we upscale the texture\
    \ T using a state-of-the-art super resolution network \u03B6 , trained on synthetic\
    \ data rendered in the textures T domain. A deep network G is then used to transform\
    \ the upscaled texture T and normals N O ) to reflectance maps, namely the diffuse\
    \ albedo A D , specular albedo A S , diffuse normals N D and specular normals\
    \ N S . The deep image-translation network is trained on high-resolution captured\
    \ facial BRDF, which we have made public as RealFaceDB. To train AvatarMe++, we\
    \ define a photorealistic differentiable rendering module R , with subsurface-scattering\
    \ and self-occlusion approximation. During training, R is used to create synthetic\
    \ data pairs, by rendering the captured data in the targets environment L and\
    \ random ones. The loss L used during training, is comprised of an adversarial\
    \ loss L GAN , a feature-matching loss L FM and our photorealistic differentiable\
    \ loss L R . The complete high resolution (up to 6k\xD74 k) BRDF maps can be used\
    \ for photorealistic rendering, while the specular normals N S can be used to\
    \ enhance the 3DMMs geometry."
  Figure 4 Link: articels_figures_by_rev_year\2021\AvatarMe_Facial_Shape_and_BRDF_Inference_With_Photorealistic_RenderingAware_GANs\figure_4.jpg
  Figure 4 caption: 'The impact of our rendering modifications, to the Pytorch3D Mesh
    Renderer [20]. Top row: rasterized mesh with rendered texture, bottom: detail.
    Our improvements in realisticity, also improve the networks ability to recover
    the albedo and specular highlights.'
  Figure 5 Link: articels_figures_by_rev_year\2021\AvatarMe_Facial_Shape_and_BRDF_Inference_With_Photorealistic_RenderingAware_GANs\figure_5.jpg
  Figure 5 caption: Prediction and ground truth for our self-occlusion autoencoder
    O (Section 4.6.4) for randomly sampled sets of 3 light sources, as input.
  Figure 6 Link: articels_figures_by_rev_year\2021\AvatarMe_Facial_Shape_and_BRDF_Inference_With_Photorealistic_RenderingAware_GANs\figure_6.jpg
  Figure 6 caption: 'AvatarMe++ training methodology: For each iteration we render
    reflectance patches mathbfR=[mathbfAD,AS,ND,NS] from the captured data (RealFaceDB),
    using our differentiable renderer mathcal R (Section 4.6) and occlusion autoencoder
    (Section 4.6.4). The rendering parameters tildemathbf Li sim mathcal N(mathbf
    Lmathbfd, sigma), quad i=1dots nL are sampled from a distribution with mean the
    target 3DMM environment mathbf Lmathbfd . We pass one rendered patch hatmathbfT=mathcalR(mathbf
    R, tildemathbf Lmathbf i) to our main network mathcal G (Section 4.7) and produce
    the reflectance patches mathbf R . The training mathbf R and generated mathbf
    R reflectance patches are used for the adversarial loss mathcal LGAN and feature-matching
    loss mathcal LFM . Moreover, the consistency of the predicted patches is improved
    by including the down-scaled input texture mathbf T(w) in mathcal G s input, and
    the down-scaled diffuse albedo mathbf AD(w) in the mathcal G s target. Additionally,
    we render the training and predicted data, with each tildemathbf Li and define
    the rendering loss mathcal LR , as the average loss for each random environment.'
  Figure 7 Link: articels_figures_by_rev_year\2021\AvatarMe_Facial_Shape_and_BRDF_Inference_With_Photorealistic_RenderingAware_GANs\figure_7.jpg
  Figure 7 caption: Reflectance maps produced by our method AvatarMe++, against state-of-the-art
    methods. Reconstructions of [23] are provided by the authors and [13] are acquired
    using their open-sourced models.
  Figure 8 Link: articels_figures_by_rev_year\2021\AvatarMe_Facial_Shape_and_BRDF_Inference_With_Photorealistic_RenderingAware_GANs\figure_8.jpg
  Figure 8 caption: Qualitative comparison of rendered reconstructions from a frontal
    and challenging side image. [23] results are provided by the authors and [13]
    results are acquired from their open-sourced models.
  Figure 9 Link: articels_figures_by_rev_year\2021\AvatarMe_Facial_Shape_and_BRDF_Inference_With_Photorealistic_RenderingAware_GANs\figure_9.jpg
  Figure 9 caption: 'Comparison of AvatarMe++ predicted reflectance with ground truth.
    Left: Patch of rendered test subject in target domain, Top row: predicted reflectance
    with our method, Bottom row: ground truth.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Alexandros Lattas
  Name of the last author: Stefanos Zafeiriou
  Number of Figures: 17
  Number of Tables: 3
  Number of authors: 6
  Paper title: 'AvatarMe++: Facial Shape and BRDF Inference With Photorealistic Rendering-Aware
    GANs'
  Publication Date: 2021-11-08 00:00:00
  Table 1 caption: "TABLE 1 Quantitative Comparisons With State-of-the-Art, Between\
    \ 6 Reconstructions of the Same Subject, From Different \u201Cin-the-wild\u201C\
    \ Images, and Ground Truth Using [32]"
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Method Components Ablation
  Table 3 caption: TABLE 3 Ablation Study for Rendering Loss Hyper-Parameters
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3125598
- Affiliation of the first author: faculty of computing, harbin institute of technology,
    heilongjiang, china
  Affiliation of the last author: school of data science, chinese university of hong
    kong (shenzhen), shenzhen, guangdong, china
  Figure 1 Link: articels_figures_by_rev_year\2021\MODENN_A_Shallow_Broad_Neural_Network_Model_Based_on_MultiOrder_Descartes_Expans\figure_1.jpg
  Figure 1 caption: The data processing flow chart of the multi-order descartes expansion
    neural network.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\MODENN_A_Shallow_Broad_Neural_Network_Model_Based_on_MultiOrder_Descartes_Expans\figure_2.jpg
  Figure 2 caption: The architecture of the multi-order Descartes expansion neural
    network (MODENN).
  Figure 3 Link: articels_figures_by_rev_year\2021\MODENN_A_Shallow_Broad_Neural_Network_Model_Based_on_MultiOrder_Descartes_Expans\figure_3.jpg
  Figure 3 caption: Samples in XOR problem, the 2 classes are shown by blue circles
    and red triangles.
  Figure 4 Link: articels_figures_by_rev_year\2021\MODENN_A_Shallow_Broad_Neural_Network_Model_Based_on_MultiOrder_Descartes_Expans\figure_4.jpg
  Figure 4 caption: Training and testing samples in Circle-in-block problem.
  Figure 5 Link: articels_figures_by_rev_year\2021\MODENN_A_Shallow_Broad_Neural_Network_Model_Based_on_MultiOrder_Descartes_Expans\figure_5.jpg
  Figure 5 caption: "Weight distribution of different models in XOR problem and CIB\
    \ problem, darker colors and thicker lines represent larger weight amplitude.\
    \ \u03B8 is the threshold of the output neuron."
  Figure 6 Link: articels_figures_by_rev_year\2021\MODENN_A_Shallow_Broad_Neural_Network_Model_Based_on_MultiOrder_Descartes_Expans\figure_6.jpg
  Figure 6 caption: 'Comparison of the training process of the MLP verses 2-5 order
    MODENNs for different problems. The blue curves are MODENNs, and the red are MLPs.
    The color goes darker as the complexity of the model increases. (a) The mean square
    loss attenuation percentage curves for XOR problem: the percentage ratio of the
    current epoch loss to the initial loss. (b) The mean square loss curves for Circle-in-block
    problem. (c) The accuracy curves for Iris problem. (d) The mean square loss curves
    for Iris problem.'
  Figure 7 Link: articels_figures_by_rev_year\2021\MODENN_A_Shallow_Broad_Neural_Network_Model_Based_on_MultiOrder_Descartes_Expans\figure_7.jpg
  Figure 7 caption: 'Iris data distribution in different dimensions: (a) oringinal
    dimension (b) important DE dimensions according to weight amplitude.'
  Figure 8 Link: articels_figures_by_rev_year\2021\MODENN_A_Shallow_Broad_Neural_Network_Model_Based_on_MultiOrder_Descartes_Expans\figure_8.jpg
  Figure 8 caption: Accuracy on MNIST for different MODENNs using different amount
    and order of MODE terms.
  Figure 9 Link: articels_figures_by_rev_year\2021\MODENN_A_Shallow_Broad_Neural_Network_Model_Based_on_MultiOrder_Descartes_Expans\figure_9.jpg
  Figure 9 caption: Comparison of the training process of 5-MLP and MODENN on Pavia
    University. (a)accuracy curves (b) crossentropy curves.
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Haifeng Li
  Name of the last author: David Zhang
  Number of Figures: 9
  Number of Tables: 10
  Number of authors: 5
  Paper title: 'MODENN: A Shallow Broad Neural Network Model Based on Multi-Order
    Descartes Expansion'
  Publication Date: 2021-11-08 00:00:00
  Table 1 caption: TABLE 1 XOR Problems Dataset for ANN Construction
  Table 10 caption: TABLE 10 The Results of the Experiments on Pavia University
  Table 2 caption: TABLE 2 Results on iris Problem for MLP and Different Order MODENNs
  Table 3 caption: TABLE 3 The Performance of the MLP and the 2-MODENN After Pruning
    Different Proportion Weights
  Table 4 caption: TABLE 4 The Retained Mode Terms After Network Pruning Without Performance
    Decay
  Table 5 caption: TABLE 5 Results on Adult Problem for MODENNs and MLPs
  Table 6 caption: TABLE 6 Comparison of MLPs and MODENNs on MNIST
  Table 7 caption: TABLE 7 Weight Pruning Result of 2-Order MODENN According to its
    Weight Amplitude Distribution on MNIST
  Table 8 caption: TABLE 8 Pruning Comparison of MODENN and MLPs on MNIST
  Table 9 caption: TABLE 9 Weight Amplitude Comparison of MODENN and ResNet on MNIST
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3125690
- Affiliation of the first author: school of artificial intelligence, optics and electronics
    (iopen), northwestern polytechnical university, xian, china
  Affiliation of the last author: school of artificial intelligence, optics and electronics
    (iopen), northwestern polytechnical university, xian, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Adaptive_Graph_AutoEncoder_for_General_Data_Clustering\figure_1.jpg
  Figure 1 caption: Framework of AdaGAE. k 0 is the initial sparsity. First, we construct
    a sparse graph via the generative model defined in Eq. (7). The learned graph
    is employed to apply the GAE designed for the weighted graphs. After training
    the GAE, we update the graph from the learned embedding with a larger sparsity,
    k . With the new graph, we re-train the GAE. These steps are repeated until the
    convergence.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Adaptive_Graph_AutoEncoder_for_General_Data_Clustering\figure_2.jpg
  Figure 2 caption: Visualization of the learning process of AdaGAE on USPS. Figure
    (b)-(i) show the embedding learned by AdaGAE at the i th epoch, while the raw
    features and the final results are shown in Figure (a) and (j), respectively.
    An epoch corresponds to an update of the graph.
  Figure 3 Link: articels_figures_by_rev_year\2021\Adaptive_Graph_AutoEncoder_for_General_Data_Clustering\figure_3.jpg
  Figure 3 caption: 't-SNE visualization on UMIST and USPS: The first and second line
    illustrate results on UMIST and USPS, respectively. Clearly, AdaGAE projects most
    semblable samples into the analogous embedding. Note that the cohesive embedding
    is preferable for clustering.'
  Figure 4 Link: articels_figures_by_rev_year\2021\Adaptive_Graph_AutoEncoder_for_General_Data_Clustering\figure_4.jpg
  Figure 4 caption: "Parameter sensitivity of \u03BB on UMIST and USPS. On UMIST,\
    \ the second term improves the performance distinctly. Besides, if \u03BB is not\
    \ too large, AdaGAE will obtain good results."
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Xuelong Li
  Name of the last author: Rui Zhang
  Number of Figures: 4
  Number of Tables: 4
  Number of authors: 3
  Paper title: Adaptive Graph Auto-Encoder for General Data Clustering
  Publication Date: 2021-11-08 00:00:00
  Table 1 caption: TABLE 1 Information of Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 ACC (%)
  Table 3 caption: "TABLE 3 k 0 k0: Initial Sparsity; \u03B3 \u03B3: Learning Rate;\
    \ t i ti: Number of Iterations to Update GAE; Struct: Neurons of Each Layer Used\
    \ in AdaGAE"
  Table 4 caption: TABLE 4 NMI (%)
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3125687
- Affiliation of the first author: institute of big data science and industry, the
    key laboratory of computational intelligence and chinese information processing
    of ministry of education, shanxi university, taiyuan, shanxi, china
  Affiliation of the last author: key laboratory of computational intelligence and
    chinese information processing of ministry of education, shanxi university, taiyuan,
    shanxi, china
  Figure 1 Link: articels_figures_by_rev_year\2021\AF_An_AssociationBased_Fusion_Method_for_MultiModal_Classification\figure_1.jpg
  Figure 1 caption: 'Some association relationship forms existing between feature
    variables, are measured by different methods: Pearson Correlation (PC), Normalized
    Mutual Information (NMI), Neighborhood Similarity (NS) [3], Distance Correlation
    (DC) [4], and Maximal Information Coefficient (MIC) [5].'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\AF_An_AssociationBased_Fusion_Method_for_MultiModal_Classification\figure_2.jpg
  Figure 2 caption: The differences between our proposed AF and the existing MMC methods.
  Figure 3 Link: articels_figures_by_rev_year\2021\AF_An_AssociationBased_Fusion_Method_for_MultiModal_Classification\figure_3.jpg
  Figure 3 caption: The scatter plot on Iris in the original space and the AF space.
  Figure 4 Link: articels_figures_by_rev_year\2021\AF_An_AssociationBased_Fusion_Method_for_MultiModal_Classification\figure_4.jpg
  Figure 4 caption: The difference of associations between features from original
    space and fused space on image feature space on Wiki.
  Figure 5 Link: articels_figures_by_rev_year\2021\AF_An_AssociationBased_Fusion_Method_for_MultiModal_Classification\figure_5.jpg
  Figure 5 caption: "Comparison between A and B (control algorithms, A and B denote\
    \ the AF version and original algorithms with the best performance, and they are\
    \ remarked with red star and blue star, respectively) against other comparing\
    \ algorithms with the Nemenyi test. Algorithms are not connected with A (red line)\
    \ and B (blue line) in the CD diagram are considered to have significantly different\
    \ performance from the control algorithm (significance level \u03B1=0.05 )."
  Figure 6 Link: articels_figures_by_rev_year\2021\AF_An_AssociationBased_Fusion_Method_for_MultiModal_Classification\figure_6.jpg
  Figure 6 caption: Performance of algorithm with different fusion strategies.
  Figure 7 Link: articels_figures_by_rev_year\2021\AF_An_AssociationBased_Fusion_Method_for_MultiModal_Classification\figure_7.jpg
  Figure 7 caption: 'A DMMC architecture. FC: n denotes a fully-connected layer with
    n neurons, where n in lbrace 64, 128rbrace .'
  Figure 8 Link: articels_figures_by_rev_year\2021\AF_An_AssociationBased_Fusion_Method_for_MultiModal_Classification\figure_8.jpg
  Figure 8 caption: Performance comparison on associated data and orthogonal data.
  Figure 9 Link: articels_figures_by_rev_year\2021\AF_An_AssociationBased_Fusion_Method_for_MultiModal_Classification\figure_9.jpg
  Figure 9 caption: Performance comparison among different association measures.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Xinyan Liang
  Name of the last author: Jiye Liang
  Number of Figures: 9
  Number of Tables: 20
  Number of authors: 5
  Paper title: 'AF: An Association-Based Fusion Method for Multi-Modal Classification'
  Publication Date: 2021-11-09 00:00:00
  Table 1 caption: TABLE 1 Notations
  Table 10 caption: TABLE 10 Classification Performance Comparison Between LR U U
    and AF U U
  Table 2 caption: TABLE 2 Original Dataset D D
  Table 3 caption: TABLE 3 Boosting Dataset E p Ep
  Table 4 caption: TABLE 4 Dataset C p Cp in the AF Space
  Table 5 caption: TABLE 5 Statistics of Benchmark Datasets
  Table 6 caption: TABLE 6 Description of Multiple Features Dataset
  Table 7 caption: TABLE 7 Description of NUS-WIDE-OBJECT Dataset
  Table 8 caption: TABLE 8 Classification Performance Comparison Between LR B B and
    AF B B
  Table 9 caption: TABLE 9 Classification Performance Comparison Between LR C C and
    AF C C
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3125995
- Affiliation of the first author: mit csail, cambridge, ma, usa
  Affiliation of the last author: mit csail, cambridge, ma, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\MultiMoments_in_Time_Learning_and_Interpreting_Models_for_MultiAction_Video_Unde\figure_1.jpg
  Figure 1 caption: (a) Multi-Moments in Time 2 million action labels for 1 million
    3 second videos (b) Multi-Regions Localizing multiple visual regions involved
    in recognizing simultaneous actions, like running and bicycling (c) Action Regions
    Spatial localization of actions in single frames for network interpretation (d)
    Action Concepts Interpretable action features learned by a trained model (i.e.,
    jogging).
  Figure 10 Link: articels_figures_by_rev_year\2021\MultiMoments_in_Time_Learning_and_Interpreting_Models_for_MultiAction_Video_Unde\figure_10.jpg
  Figure 10 caption: 'Visualization of learned action concepts: Different features
    learn different representations of the same action. For example, units 684 and
    1417 can both be interpreted as learning the concept of burning (top left). However,
    unit 684 learns to correlate smoke with the action while unit 1417 correlates
    it with a flame.'
  Figure 2 Link: articels_figures_by_rev_year\2021\MultiMoments_in_Time_Learning_and_Interpreting_Models_for_MultiAction_Video_Unde\figure_2.jpg
  Figure 2 caption: An example of the path of generating new candidate verbs from
    previously annotated classes.
  Figure 3 Link: articels_figures_by_rev_year\2021\MultiMoments_in_Time_Learning_and_Interpreting_Models_for_MultiAction_Video_Unde\figure_3.jpg
  Figure 3 caption: The label distribution of our proposed Multi-Moments in Time dataset
    compared to the Moments in Time dataset.
  Figure 4 Link: articels_figures_by_rev_year\2021\MultiMoments_in_Time_Learning_and_Interpreting_Models_for_MultiAction_Video_Unde\figure_4.jpg
  Figure 4 caption: Lists of vocabulary differences in M-MiT compared to MiT. We combined
    similar classes, removed ambiguousnoisy classes and added some new classes found
    during annotation.
  Figure 5 Link: articels_figures_by_rev_year\2021\MultiMoments_in_Time_Learning_and_Interpreting_Models_for_MultiAction_Video_Unde\figure_5.jpg
  Figure 5 caption: 'Region Separation: Example showing single class CAM images and
    the separation of relevant features in a multi-class CAM image. The red regions
    specify important areas of the image used by the model to infer the detected action.'
  Figure 6 Link: articels_figures_by_rev_year\2021\MultiMoments_in_Time_Learning_and_Interpreting_Models_for_MultiAction_Video_Unde\figure_6.jpg
  Figure 6 caption: 'Multi-CAM Examples: Multi-class CAM images for a variety of scenes
    with simultaneous actions. Action labels are placed near the important image regions
    used by the model for identifying each specific action. This area is signified
    by the red overlay with blue edges separating image regions distinct to each detected
    action showing that our model is able to localize multiple actions present in
    each scene despite not being trained for localization.'
  Figure 7 Link: articels_figures_by_rev_year\2021\MultiMoments_in_Time_Learning_and_Interpreting_Models_for_MultiAction_Video_Unde\figure_7.jpg
  Figure 7 caption: ResNet block-wise interpretability Visualize how different semantic
    concepts - objects, scenes and actions emerge across residual blocks of the ResNet-50
    network.
  Figure 8 Link: articels_figures_by_rev_year\2021\MultiMoments_in_Time_Learning_and_Interpreting_Models_for_MultiAction_Video_Unde\figure_8.jpg
  Figure 8 caption: 'Localized action regions: Bounding boxes annotated around 549
    different action categories in 67,468 image frames each selected from unique videos
    in the Moments in Time, Kinetics, and Something-Something datasets.'
  Figure 9 Link: articels_figures_by_rev_year\2021\MultiMoments_in_Time_Learning_and_Interpreting_Models_for_MultiAction_Video_Unde\figure_9.jpg
  Figure 9 caption: Graph of learned action concepts ordered by the number of features
    associated with each concept.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.99
  Name of the first author: Mathew Monfort
  Name of the last author: Aude Oliva
  Number of Figures: 11
  Number of Tables: 5
  Number of authors: 10
  Paper title: 'Multi-Moments in Time: Learning and Interpreting Models for Multi-Action
    Video Understanding'
  Publication Date: 2021-11-09 00:00:00
  Table 1 caption: 'TABLE 1 Validation Results: Performance of the Baseline Models
    With Different Loss Functions on the Multi-Label Validation Set'
  Table 10 caption: Not Available
  Table 2 caption: 'TABLE 2 MiT versus M-MiT Model Validation Results: Performance
    of Models Trained on MiT and M-MiT on a Subset of the M-MiT Validation Set Containing
    Only Classes Shared Between MiT and M-MiT'
  Table 3 caption: 'TABLE 3 Loss Function Comparison: We Validate Our Proposed wLSEP
    Loss Function on Four Different Multi-Label Datasets'
  Table 4 caption: TABLE 4 Dataset Transfer Performance Using ResNet-50 I3D Models
    Pretrained on Kinetics, Moments in Time (MiT) and the Proposed Multi-Moments in
    Time Dataset (M-MiT)
  Table 5 caption: TABLE 5 Comparison of the Number of Concepts and Interpretable
    Features Identified by NetDissect Given the Broden Dataset, the Action Region
    Dataset and the Combined Dataset on Block 4 of a ResNet-50 Trained for Action
    Recognition
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3126682
- Affiliation of the first author: school of computer science, beijing institute of
    technology, beijing, china
  Affiliation of the last author: university of kentucky, lexington, ky, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\Graph_Neural_Network_and_Spatiotemporal_Transformer_Attention_for_D_Video_Object\figure_1.jpg
  Figure 1 caption: A representative challenging case in autonomous driving scenarios.
    A typical single-frame 3D object detector, e.g. [3], suffers from false negatives,
    due to occlusion (top row). In contrast, our 3D video object detector predicts
    correctly (bottom row). Red and grey denote the predicted and ground truth boxes,
    respectively.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Graph_Neural_Network_and_Spatiotemporal_Transformer_Attention_for_D_Video_Object\figure_2.jpg
  Figure 2 caption: "Schematic of our point cloud-based 3D video object detection\
    \ framework. The input T\xD7M frames are first divided into T groups of short-term\
    \ data with each merging M frames. Then, the short-term encoding module extracts\
    \ the BEV features for each short-term data with a Grid Message Passing Network\
    \ (GMPNet) followed by a CNN backbone. Afterwards, the long-term aggregation module\
    \ further captures the dependencies in these short-term features with an Attentive\
    \ Spatiotemporal Transformer GRU (AST-GRU). Finally, the detection head receives\
    \ the enhanced memory features and produces the detection results."
  Figure 3 Link: articels_figures_by_rev_year\2021\Graph_Neural_Network_and_Spatiotemporal_Transformer_Attention_for_D_Video_Object\figure_3.jpg
  Figure 3 caption: Illustration of one iteration step for message propagation, where
    h i is the state of node v i . In step s , the neighbors for h 1 are h 2 , h 3
    , h 4 (within the gray dashed line), presenting the grids of the top car. After
    receiving messages from the neighbors, the receptive field of h 1 is enlarged
    in step s+1 . This indicates the relations with the bottom car are modeled after
    message propagation.
  Figure 4 Link: articels_figures_by_rev_year\2021\Graph_Neural_Network_and_Spatiotemporal_Transformer_Attention_for_D_Video_Object\figure_4.jpg
  Figure 4 caption: Illustration of our proposed AST-GRU in the online mode. It consists
    of a spatial transformer attention (STA) module and a temporal transformer attention
    (TTA) module. AST-GRU captures the dependencies from a long-term perspective and
    produces the enhanced memory features H t T t=1 .
  Figure 5 Link: articels_figures_by_rev_year\2021\Graph_Neural_Network_and_Spatiotemporal_Transformer_Attention_for_D_Video_Object\figure_5.jpg
  Figure 5 caption: Qualitative results of 3D video object detection. We compare our
    algorithm (the right three frames in each case) with the single-frame 3D object
    detector (the left ones). The red and grey boxes indicate the predictions and
    ground-truths, respectively.
  Figure 6 Link: articels_figures_by_rev_year\2021\Graph_Neural_Network_and_Spatiotemporal_Transformer_Attention_for_D_Video_Object\figure_6.jpg
  Figure 6 caption: Ablation study for GMPNet. We fix one parameter and vary the other
    to ablate the importance of iteration step S and neighbor node number K .
  Figure 7 Link: articels_figures_by_rev_year\2021\Graph_Neural_Network_and_Spatiotemporal_Transformer_Attention_for_D_Video_Object\figure_7.jpg
  Figure 7 caption: Ablation study for the input length of the short-term and long-term
    modules. In (a) and (b), different keyframes T or non-keyframe sweeps M are used
    to evaluate the performance.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.96
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Junbo Yin
  Name of the last author: Ruigang Yang
  Number of Figures: 7
  Number of Tables: 3
  Number of authors: 5
  Paper title: Graph Neural Network and Spatiotemporal Transformer Attention for 3D
    Video Object Detection From Point Clouds
  Publication Date: 2021-11-09 00:00:00
  Table 1 caption: TABLE 1 Quantitative Detection Results on the nuScenes 3D Object
    Detection Benchmark
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Ablation Study of Our 3D Video Object Detector
  Table 3 caption: TABLE 3 Detailed Analysis of the Input Choices and the Layer Number
    in TTA Module
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3125981
