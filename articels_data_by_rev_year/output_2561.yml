- Affiliation of the first author: "vision for autonomous systems group, mpi for informatics,\
    \ saarbr\xFCcken, germany"
  Affiliation of the last author: "computer vision lab, eth z\xFCrich, zrich, switzerland"
  Figure 1 Link: articels_figures_by_rev_year\2022\Binaural_SoundNet_Predicting_Semantics_Depth_and_Motion_With_Binaural_Sounds\figure_1.jpg
  Figure 1 caption: "An illustration of our four tasks: (a) super-resolved binaural\
    \ sounds \u2013 from azimuth angle 0 \u2218 (red) to other angles such as 90 \u2218\
    \ (blue), (b) auditory semantic prediction for three sound-making object classes,\
    \ (c) auditory depth prediction and (d) auditory motion prediction."
  Figure 10 Link: articels_figures_by_rev_year\2022\Binaural_SoundNet_Predicting_Semantics_Depth_and_Motion_With_Binaural_Sounds\figure_10.jpg
  Figure 10 caption: Qualitative results of all the four tasks. Better view in color.
  Figure 2 Link: articels_figures_by_rev_year\2022\Binaural_SoundNet_Predicting_Semantics_Depth_and_Motion_With_Binaural_Sounds\figure_2.jpg
  Figure 2 caption: 'Sensor and dataset: a) data capture locations, b) our custom
    rig and c) abstract depiction of our recording setup with sensor orientations
    and microphone ids.'
  Figure 3 Link: articels_figures_by_rev_year\2022\Binaural_SoundNet_Predicting_Semantics_Depth_and_Motion_With_Binaural_Sounds\figure_3.jpg
  Figure 3 caption: The diagram of our method for the four considered tasks. The encoder
    is shared by all tasks and each task has its own decoder.
  Figure 4 Link: articels_figures_by_rev_year\2022\Binaural_SoundNet_Predicting_Semantics_Depth_and_Motion_With_Binaural_Sounds\figure_4.jpg
  Figure 4 caption: The diagram of our two alternative encoders.
  Figure 5 Link: articels_figures_by_rev_year\2022\Binaural_SoundNet_Predicting_Semantics_Depth_and_Motion_With_Binaural_Sounds\figure_5.jpg
  Figure 5 caption: Qualitative results of auditory semantic prediction by our approach
    with input audio signal of different amplification ratios (i.e. 0,0.5,1, and 2)
    at inference stage. We show the input spectrograms (Spec) and the multiplier of
    amplitude for the signal from (c) to (f) and corresponding semantic prediction
    in (g) to (j). We show the visual scene and ground truth semantic prediction in
    (a) and (b) respectively.
  Figure 6 Link: articels_figures_by_rev_year\2022\Binaural_SoundNet_Predicting_Semantics_Depth_and_Motion_With_Binaural_Sounds\figure_6.jpg
  Figure 6 caption: 'Results (mIoU) of semantic prediction: different sets of microphones
    used as inputs under Ours(B) in (a) and (b) and varying number of output microphones
    for the auxiliary task S 3 R, under Ours(B:S) and Ours(B:SD), in (c).'
  Figure 7 Link: articels_figures_by_rev_year\2022\Binaural_SoundNet_Predicting_Semantics_Depth_and_Motion_With_Binaural_Sounds\figure_7.jpg
  Figure 7 caption: Subjective assessment of the generated binaural sounds. The results
    are the percentages of times each result is chosen by users as the preferred one.
  Figure 8 Link: articels_figures_by_rev_year\2022\Binaural_SoundNet_Predicting_Semantics_Depth_and_Motion_With_Binaural_Sounds\figure_8.jpg
  Figure 8 caption: Qualitative results of auditory semantic prediction by our approach.
    First column shows the visual scene, the second for the computed background image,
    the third for the object masks predicted by our approach, and the fourth for the
    ground truth.
  Figure 9 Link: articels_figures_by_rev_year\2022\Binaural_SoundNet_Predicting_Semantics_Depth_and_Motion_With_Binaural_Sounds\figure_9.jpg
  Figure 9 caption: An illustration of samples of semantic segmentation masks, depth
    predictions and motion estimates generated from our dataset using the pre-trained
    state of the art vision models.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Dengxin Dai
  Name of the last author: Luc Van Gool
  Number of Figures: 10
  Number of Tables: 3
  Number of authors: 4
  Paper title: 'Binaural SoundNet: Predicting Semantics, Depth and Motion With Binaural
    Sounds'
  Publication Date: 2022-03-03 00:00:00
  Table 1 caption: TABLE 1 Results of Auditory Semantic Prediction
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Results of Auditory Depth Prediction and Motion Prediction
  Table 3 caption: TABLE 3 The Results of Our S 3 3R Task
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3155643
- Affiliation of the first author: department of electrical and computer engineering,
    augmented cognition lab, northeastern university, boston, ma, usa
  Affiliation of the last author: department of electrical and computer engineering,
    augmented cognition lab, northeastern university, boston, ma, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\SimultaneouslyCollected_Multimodal_Lying_Pose_Dataset_Enabling_InBed_Human_Pose_\figure_1.jpg
  Figure 1 caption: Our Multimodal in-bed pose data collection setup, (a) in a regular
    bedroom, (b) in a simulated hospital room.
  Figure 10 Link: articels_figures_by_rev_year\2022\SimultaneouslyCollected_Multimodal_Lying_Pose_Dataset_Enabling_InBed_Human_Pose_\figure_10.jpg
  Figure 10 caption: PCKh pose estimation performance of state-of-the-arts on LWIR
    and depth modalities under hospital setting.
  Figure 2 Link: articels_figures_by_rev_year\2022\SimultaneouslyCollected_Multimodal_Lying_Pose_Dataset_Enabling_InBed_Human_Pose_\figure_2.jpg
  Figure 2 caption: 'SLP image data samples from in-bed supine and side postures:
    (a-f) show images captured using an RGB webcam, (g-l) show images captured using
    an LWIR camera, (m-r) shows images captured using a depth camera, and (s-x) shows
    images captured using a pressure mat. These images are taken from the participants
    without cover and with two different types (one thin and one thick) of covers.'
  Figure 3 Link: articels_figures_by_rev_year\2022\SimultaneouslyCollected_Multimodal_Lying_Pose_Dataset_Enabling_InBed_Human_Pose_\figure_3.jpg
  Figure 3 caption: Pose ambiguities in LWIR images with their corresponding RGB images,
    (a) false leg pose (in red) caused by the heat residue in the LWIR image,(b) false
    arm pose (in red) due to the cuddled limbs. The correct limb poses are given in
    green.
  Figure 4 Link: articels_figures_by_rev_year\2022\SimultaneouslyCollected_Multimodal_Lying_Pose_Dataset_Enabling_InBed_Human_Pose_\figure_4.jpg
  Figure 4 caption: 'Demos of PM ground truth generation via physical hyperparameter
    tuning (PHPT) of guideline III in: (a) a supine pose, and (b) a right lying pose.
    Red dash line shows direct annotation, intuitively.'
  Figure 5 Link: articels_figures_by_rev_year\2022\SimultaneouslyCollected_Multimodal_Lying_Pose_Dataset_Enabling_InBed_Human_Pose_\figure_5.jpg
  Figure 5 caption: 'SLP cross domain alignment design: (a) alignment markers design,
    and (b) automatic center extraction in RGB imaging.'
  Figure 6 Link: articels_figures_by_rev_year\2022\SimultaneouslyCollected_Multimodal_Lying_Pose_Dataset_Enabling_InBed_Human_Pose_\figure_6.jpg
  Figure 6 caption: 'Distribution of the measured person-specific parameters: (a)
    height (cm), (b) weight (kg), (c) tailor measurements (cm).'
  Figure 7 Link: articels_figures_by_rev_year\2022\SimultaneouslyCollected_Multimodal_Lying_Pose_Dataset_Enabling_InBed_Human_Pose_\figure_7.jpg
  Figure 7 caption: 'Truncated histogram of normalized distance from the gold standard
    labels (using LWIR-G123) for labels generated using: (a) LWIR-G1, and (b) LWIR-G3.
    A Gaussian curve is fitted with green vertical lines as the mean and 3 standard
    deviation bounds.'
  Figure 8 Link: articels_figures_by_rev_year\2022\SimultaneouslyCollected_Multimodal_Lying_Pose_Dataset_Enabling_InBed_Human_Pose_\figure_8.jpg
  Figure 8 caption: PCKh pose estimation performance of state-of-the-arts on LWIR,
    depth, and PM modalities under home setting.
  Figure 9 Link: articels_figures_by_rev_year\2022\SimultaneouslyCollected_Multimodal_Lying_Pose_Dataset_Enabling_InBed_Human_Pose_\figure_9.jpg
  Figure 9 caption: Some qualitative results of in-bed pose estimation based on Sun,
    CVPR19[20]. RGB images are given with the ground truth pose along side the inference
    results from depth, LWIR, and PM modalities.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Shuangjun Liu
  Name of the last author: Sarah Ostadabbas
  Number of Figures: 13
  Number of Tables: 5
  Number of authors: 6
  Paper title: 'Simultaneously-Collected Multimodal Lying Pose Dataset: Enabling In-Bed
    Human Pose Monitoring'
  Publication Date: 2022-03-03 00:00:00
  Table 1 caption: TABLE 1 SLP Dataset Compared to the State-of-the-Art 2D Human Pose
    Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 SLP Dataset Composition
  Table 3 caption: TABLE 3 PCKh0.5 Performance, When State-of-the-Art Models are Trained
    on MPII [33] (All in RGB Modality) and SLP (Its Individual LWIR, PM, and Depth
    Modalities) Datasets and Tested on Corresponding DatasetModality, Respectively
  Table 4 caption: TABLE 4 Mean and Variance of Two Human Pose Estimation Models TrainedTested
    on MPII [33], COCO [36], and SLP Datasets
  Table 5 caption: TABLE 5 PCKh0.5 Performance of the State-of-the-Art Pose Estimation
    Models Tested on Test-Portion of the MPII and SLP Datasets
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3155712
- Affiliation of the first author: huawei cloud, shenzhen, china
  Affiliation of the last author: school of artificial intelligence and automation,
    huazhong university of science and technology, wuhan, china
  Figure 1 Link: articels_figures_by_rev_year\2022\RealTime_Scene_Text_Detection_With_Differentiable_Binarization_and_Adaptive_Scal\figure_1.jpg
  Figure 1 caption: The comparisons of several recent scene text detection methods
    on the MSRA-TD500 dataset [59], in terms of both accuracy (F-measure) and speed.
    Our method achieves the ideal tradeoff between effectiveness and efficiency.
  Figure 10 Link: articels_figures_by_rev_year\2022\RealTime_Scene_Text_Detection_With_Differentiable_Binarization_and_Adaptive_Scal\figure_10.jpg
  Figure 10 caption: Some visualization results of DBNet and DBNet++ on text instances
    of various shapes, including curved text, vertical text and multi-oriented text.
    For each unit, the top is the result of DBNet; the bottom is the result of DBNet++.
    More results are shown in the supplementary, which can be found on the Computer
    Society Digital Library at http:doi.ieeecomputersociety.org10.1109TPAMI.2022.3155612.
  Figure 2 Link: articels_figures_by_rev_year\2022\RealTime_Scene_Text_Detection_With_Differentiable_Binarization_and_Adaptive_Scal\figure_2.jpg
  Figure 2 caption: A traditional pipeline (blue flow) and our pipeline (red flow).
    Dashed arrows are the inference only operators; solid arrows indicate differentiable
    operators in both training and inference.
  Figure 3 Link: articels_figures_by_rev_year\2022\RealTime_Scene_Text_Detection_With_Differentiable_Binarization_and_Adaptive_Scal\figure_3.jpg
  Figure 3 caption: Architecture of our proposed DBNet++, where the Adaptive Scale
    Fusion module is shown in Fig. 4.
  Figure 4 Link: articels_figures_by_rev_year\2022\RealTime_Scene_Text_Detection_With_Differentiable_Binarization_and_Adaptive_Scal\figure_4.jpg
  Figure 4 caption: Illustration of the adaptive scale fusion module.
  Figure 5 Link: articels_figures_by_rev_year\2022\RealTime_Scene_Text_Detection_With_Differentiable_Binarization_and_Adaptive_Scal\figure_5.jpg
  Figure 5 caption: Numerical comparisons of different functions and derivatives.
  Figure 6 Link: articels_figures_by_rev_year\2022\RealTime_Scene_Text_Detection_With_Differentiable_Binarization_and_Adaptive_Scal\figure_6.jpg
  Figure 6 caption: The derivative of losses in Eqs. (10) and (11).
  Figure 7 Link: articels_figures_by_rev_year\2022\RealTime_Scene_Text_Detection_With_Differentiable_Binarization_and_Adaptive_Scal\figure_7.jpg
  Figure 7 caption: The threshold map withwithout supervision.
  Figure 8 Link: articels_figures_by_rev_year\2022\RealTime_Scene_Text_Detection_With_Differentiable_Binarization_and_Adaptive_Scal\figure_8.jpg
  Figure 8 caption: Label generation. The annotation of text polygon is visualized
    in red lines. The shrunk and dilated polygon are displayed in blue and green lines,
    respectively.
  Figure 9 Link: articels_figures_by_rev_year\2022\RealTime_Scene_Text_Detection_With_Differentiable_Binarization_and_Adaptive_Scal\figure_9.jpg
  Figure 9 caption: Some visualization results on text instances of various shapes,
    including curved text, multi-oriented text, vertical text, and long text lines.
    For each unit, the top right is the threshold map; the bottom right is the probability
    map.
  First author gender probability: 0.59
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Minghui Liao
  Name of the last author: Xiang Bai
  Number of Figures: 11
  Number of Tables: 9
  Number of authors: 5
  Paper title: Real-Time Scene Text Detection With Differentiable Binarization and
    Adaptive Scale Fusion
  Publication Date: 2022-03-03 00:00:00
  Table 1 caption: TABLE 1 Detection Results With Different Settings of Deformable
    Convolution, Differentiable Binarization and Adaptive Scale Fusion Module
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Detection Results With Different Settings of ASF
  Table 3 caption: TABLE 3 Effect of Supervising the Threshold Map on the MLT-2017
    Dataset
  Table 4 caption: TABLE 4 Comparisons With Multi-Scale Feature Fusion and Context
    Enhancement Modules in Semantic Segmentation Methods
  Table 5 caption: TABLE 5 Detection Results on the Total-Text Dataset
  Table 6 caption: TABLE 6 Detection Results on the CTW1500 Dataset
  Table 7 caption: TABLE 7 Detection Results on the ICDAR 2015 Dataset
  Table 8 caption: TABLE 8 Detection Results on the MSRA-TD500 Dataset
  Table 9 caption: TABLE 9 Detection Results on the MLT-2019 Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3155612
- Affiliation of the first author: centre for vision, speech and signal processing
    (cvssp), university of surrey, guildford, u.k.
  Affiliation of the last author: centre for vision, speech and signal processing
    (cvssp), university of surrey, guildford, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2022\Deep_OrderPreserving_Learning_With_Adaptive_Optimal_Transport_Distance\figure_1.jpg
  Figure 1 caption: "The real-world applications of the LDL framework. (a) Facial\
    \ age estimation \u2013 due to similarity between neighbouring ages, the label\
    \ is a Gaussian distribution function for a facial image at the age of 25. (b)\
    \ Image aesthetic assessment \u2013 the label is a mixture distribution function\
    \ for a given image, specified by crowd opinions, ranging from 0 to 10, on the\
    \ aesthetic impact of the image."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Deep_OrderPreserving_Learning_With_Adaptive_Optimal_Transport_Distance\figure_2.jpg
  Figure 2 caption: Optimal transportation plan that matches the the source probability
    vector into the target probability vector. Target probability vector is modelled
    a Gaussian label distribution for a subject at the age of 30. (Best viewed in
    colour).
  Figure 3 Link: articels_figures_by_rev_year\2022\Deep_OrderPreserving_Learning_With_Adaptive_Optimal_Transport_Distance\figure_3.jpg
  Figure 3 caption: "The curvature of the proposed ground metric function with respect\
    \ to \u03C1 and \u03C4 . \u03C1=2 and \u03C1=1 reproduce the most widely-used\
    \ existing ground metric g w ."
  Figure 4 Link: articels_figures_by_rev_year\2022\Deep_OrderPreserving_Learning_With_Adaptive_Optimal_Transport_Distance\figure_4.jpg
  Figure 4 caption: 'Heat map of accuracy of two DNN models, Left: Age estimation,
    Right: Image aesthetic assessment, (The optimal point is represented using a red
    star).'
  Figure 5 Link: articels_figures_by_rev_year\2022\Deep_OrderPreserving_Learning_With_Adaptive_Optimal_Transport_Distance\figure_5.jpg
  Figure 5 caption: "Comparison of different loss functions between two label distributions\
    \ with respect to \u0394l= l A \u2212 l B . l A and l B are the indices of the\
    \ peak of label distributions, respectively."
  Figure 6 Link: articels_figures_by_rev_year\2022\Deep_OrderPreserving_Learning_With_Adaptive_Optimal_Transport_Distance\figure_6.jpg
  Figure 6 caption: Sample images from the AVA dataset.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.94
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Ali Akbari
  Name of the last author: Josef Kittler
  Number of Figures: 6
  Number of Tables: 6
  Number of authors: 4
  Paper title: Deep Order-Preserving Learning With Adaptive Optimal Transport Distance
  Publication Date: 2022-03-07 00:00:00
  Table 1 caption: TABLE 1 Performance of the Proposed Method in Age Estimation on
    the MORPH Database (Partial RS Protocol)
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Performance of the Proposed Method in Age Estimation on
    the MORPH Database (RS and Partial RS Protocols)
  Table 3 caption: TABLE 3 Performance of the Proposed Method in Age Estimation on
    the MORPH Database (S1S2+S3 and S2S1+S3 Protocols)
  Table 4 caption: TABLE 4 Evaluation on the AgeDB Database (RS Protocol)
  Table 5 caption: TABLE 5 Performance of the Proposed Method in Predicting the AVA
    Quality Ratings
  Table 6 caption: TABLE 6 Performance of Different Loss Functions in Predicting the
    Age Labels on the Morph Dataset (Left) and the AVA Quality Ratings (Right)
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3156885
- Affiliation of the first author: department of electrical and computer engineering
    and digital technology center, university of minnesota, minneapolis, mn, usa
  Affiliation of the last author: department of electrical and computer engineering
    and digital technology center, university of minnesota, minneapolis, mn, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\Incremental_Ensemble_Gaussian_Processes\figure_1.jpg
  Figure 1 caption: "Schematic diagrams for IE-GP, SIE-GP, and DIE-GP. Notice the\
    \ difference in the subscripts of w and \u03B8 , and the implementation of the\
    \ prediction and correction steps."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Incremental_Ensemble_Gaussian_Processes\figure_2.jpg
  Figure 2 caption: nMSE plots on (a) Toms hardware; (b) SARCOS; and, (c) Air quality
    datasets.
  Figure 3 Link: articels_figures_by_rev_year\2022\Incremental_Ensemble_Gaussian_Processes\figure_3.jpg
  Figure 3 caption: Normalized running times for regression on (a) Toms hardware;
    (b) SARCOS; and, (c) Air quality datasets.
  Figure 4 Link: articels_figures_by_rev_year\2022\Incremental_Ensemble_Gaussian_Processes\figure_4.jpg
  Figure 4 caption: Cumulative classification errors on (a) Banana, (b) Musk, and
    (c) Ionosphere datasets.
  Figure 5 Link: articels_figures_by_rev_year\2022\Incremental_Ensemble_Gaussian_Processes\figure_5.jpg
  Figure 5 caption: Normalized running times for classification on (a) Banana, (b)
    Musk, and (c) Ionosphere datasets.
  Figure 6 Link: articels_figures_by_rev_year\2022\Incremental_Ensemble_Gaussian_Processes\figure_6.jpg
  Figure 6 caption: Classification error versus runtime plots for dimensionality reduction
    on the (a) MNIST, (b) oil, and (c) USPS datasets.
  Figure 7 Link: articels_figures_by_rev_year\2022\Incremental_Ensemble_Gaussian_Processes\figure_7.jpg
  Figure 7 caption: Visualization of the embedding attained by IE-GPLVM on the USPS
    dataset. Colors represent different digits.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.95
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Qin Lu
  Name of the last author: Georgios B. Giannakis
  Number of Figures: 7
  Number of Tables: 1
  Number of authors: 3
  Paper title: Incremental Ensemble Gaussian Processes
  Publication Date: 2022-03-07 00:00:00
  Table 1 caption: TABLE 1 Statistics of the Datasets
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3157197
- Affiliation of the first author: department of electrical engineering, eindhoven
    university of technology, eindhoven, the netherlands
  Affiliation of the last author: department of electrical engineering, eindhoven
    university of technology, eindhoven, the netherlands
  Figure 1 Link: articels_figures_by_rev_year\2022\A_Review_of_the_Gumbelmax_Trick_and_its_Extensions_for_Discrete_Stochasticity_in\figure_1.jpg
  Figure 1 caption: "Illustration of inverse transform sampling for drawing a sample\
    \ from Cat(\u03C0) . A sample from the Uniform distribution is converted to one\
    \ of the categoriesclasses of the categorical distribution via its inverse cumulative\
    \ density function. The chance that class i is being sampled equals \u03C0 i ;\
    \ the width of the corresponding box in this illustration."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\A_Review_of_the_Gumbelmax_Trick_and_its_Extensions_for_Discrete_Stochasticity_in\figure_2.jpg
  Figure 2 caption: Categorization of the different applications in deep learning
    in which Gumbel-based gradient estimators have been applied in order to facilitate
    training via backpropagation through discrete stochastic nodes. Visualizations
    in the bottom show where different models exhibit discretized data, indicated
    by the dashed gray boxes.
  Figure 3 Link: articels_figures_by_rev_year\2022\A_Review_of_the_Gumbelmax_Trick_and_its_Extensions_for_Discrete_Stochasticity_in\figure_3.jpg
  Figure 3 caption: The Gumbel (double Exponential) distribution directly relates
    to the UniformBeta and Exponential distribution via a single, respectively double,
    negative logarithmic relation. The green ellipses indicate the probability distribution
    of the corresponding random variable in the cell. To prevent clutter, we here
    define U:= U (i) , X:= X (i) , and G:= G (i) , being an i th independent standard
    Uniform, Exponential, and Gumbel random variable, respectively.
  Figure 4 Link: articels_figures_by_rev_year\2022\A_Review_of_the_Gumbelmax_Trick_and_its_Extensions_for_Discrete_Stochasticity_in\figure_4.jpg
  Figure 4 caption: "Drawing a sample from a categorical distribution can be done\
    \ via different paths. The most suitable path depends on the context in which\
    \ one would like to draw a sample. One can start from either the unnormalized\
    \ log-probabilities a (and a Boltzmann temperature T ), or the normalized probabilities\
    \ \u03C0 . The green ellipses in the upper right corner of certain nodes indicate\
    \ that the node represents a random variable following the distribution indicated\
    \ in the ellipse. The different partition functions Z D all normalize their corresponding\
    \ node, their input is omitted for readability reasons. The yellow square boxes\
    \ are used to refer to certain paths in the text. A Jupyter Notebook that accompanies\
    \ this figure is publicly available. 4"
  Figure 5 Link: articels_figures_by_rev_year\2022\A_Review_of_the_Gumbelmax_Trick_and_its_Extensions_for_Discrete_Stochasticity_in\figure_5.jpg
  Figure 5 caption: Sampling Gumbels and the corresponding argmax I and maximum M
    , can be done in a bottom-up or top-down approach. The former finds the operatorname(arg)max
    from the perturbed logits, starting from the Gumbels. Top-down sampling, on the
    other hand, starts from either M andor I , and conditionally samples the corresponding
    perturbed logits. The maximum and argmax are independent ( perp !!! perp ), allowing
    for independent sampling of either of them in case only the other entity is known.
    The top-down procedure can be applied in parallel for the entire domain D (depicted)
    or sequentially (not visualized).
  Figure 6 Link: articels_figures_by_rev_year\2022\A_Review_of_the_Gumbelmax_Trick_and_its_Extensions_for_Discrete_Stochasticity_in\figure_6.jpg
  Figure 6 caption: 'Overview of sampling algorithms for different scenarios: single
    (discrete or soft) or multiple sample(s) from (un)structured distributions. For
    all cases, both defaultconventional and Gumbel-based algorithms are given. The
    green ellipses indicate the distribution from which samples of the indicated algorithms
    originate. In case of empty ellipses, it is unknown which distribution the samples
    follow. A superscript z indicates that the algorithm requires normalized probabilities,
    i.e., partition function Z should be known.'
  Figure 7 Link: articels_figures_by_rev_year\2022\A_Review_of_the_Gumbelmax_Trick_and_its_Extensions_for_Discrete_Stochasticity_in\figure_7.jpg
  Figure 7 caption: Illustration of Gumbel-top- k sampling. Figure adapted from [14].
    Gumbel-top- k perturbs all logits with i.i.d. Gumbel samples, and returns the
    index of the k perturbed logits with the highest values. These indices yield k
    independent samples without replacement from operatornameCat(boldsymboltheta )
    .
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Iris A. M. Huijben
  Name of the last author: Ruud J. G. van Sloun
  Number of Figures: 7
  Number of Tables: 0
  Number of authors: 4
  Paper title: A Review of the Gumbel-max Trick and its Extensions for Discrete Stochasticity
    in Machine Learning
  Publication Date: 2022-03-07 00:00:00
  Table 1 caption: Not Available
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3157042
- Affiliation of the first author: school of artificial intelligence, university of
    chinese academy of sciences (ucas), beijing, china
  Affiliation of the last author: school of artificial intelligence, university of
    chinese academy of sciences (ucas), beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Constructing_Stronger_and_Faster_Baselines_for_SkeletonBased_Action_Recognition\figure_1.jpg
  Figure 1 caption: "The overall pipeline of our approach, where \u2295 represents\
    \ concatenation operation, GAP and FC denote the Global Average Pooling operation\
    \ and Fully Connected layer, respectively. (Best viewed in color.)."
  Figure 10 Link: articels_figures_by_rev_year\2022\Constructing_Stronger_and_Faster_Baselines_for_SkeletonBased_Action_Recognition\figure_10.jpg
  Figure 10 caption: The subfigures (a) and (b) describe the attention maps of two
    randomly selected samples respectively, where the four parts of each subfigure
    are calculated by the attention modules in four stages of EfficientGCN-B4 model.
    A small square with darker color denotes a higher attention weight for the corresponding
    spatial temporal joint. All these attention maps are performed on X-sub benchmark.
    (Best viewed in color.).
  Figure 2 Link: articels_figures_by_rev_year\2022\Constructing_Stronger_and_Faster_Baselines_for_SkeletonBased_Action_Recognition\figure_2.jpg
  Figure 2 caption: Model size versus model accuracy on the cross-subject benchmark
    of NTU 60 dataset. (Best viewed in color.).
  Figure 3 Link: articels_figures_by_rev_year\2022\Constructing_Stronger_and_Faster_Baselines_for_SkeletonBased_Action_Recognition\figure_3.jpg
  Figure 3 caption: The demonstration of input data. (a) is the relative positions,
    (b) is the motion velocities, and (c) demonstrates the 3D lengths and the 3D angles
    of a bone. (Best viewed in color.).
  Figure 4 Link: articels_figures_by_rev_year\2022\Constructing_Stronger_and_Faster_Baselines_for_SkeletonBased_Action_Recognition\figure_4.jpg
  Figure 4 caption: Standard convolution versus separable convolution for skeleton-based
    action recognition, where Cin and Cout denote the numbers of input and output
    channels, Df and Dk denote the sizes of feature map and convolutional kernel,
    and represents convolutional operation.(Best viewed in color.).
  Figure 5 Link: articels_figures_by_rev_year\2022\Constructing_Stronger_and_Faster_Baselines_for_SkeletonBased_Action_Recognition\figure_5.jpg
  Figure 5 caption: The overview of the proposed EfficientGCN model, where the two
    numbers in each block denote input and output channels, Q is the number of action
    classes, oplus and odot represent concatenation and element-wise product, and
    2 represents a stride of 2. (Best viewed in color.).
  Figure 6 Link: articels_figures_by_rev_year\2022\Constructing_Stronger_and_Faster_Baselines_for_SkeletonBased_Action_Recognition\figure_6.jpg
  Figure 6 caption: The details of various convolutional layers, where Cin and Cout
    denote the numbers of input and output channels, rrd and rep are employed to reduce
    or expand the inner channels. (Best viewed in color.).
  Figure 7 Link: articels_figures_by_rev_year\2022\Constructing_Stronger_and_Faster_Baselines_for_SkeletonBased_Action_Recognition\figure_7.jpg
  Figure 7 caption: The overview of the proposed ST-JointAtt module, where C,T,V denote
    the numbers of input channels, frames and joints respectively, rrd=4 is utilized
    to compact the features, otimes represents the outer-product, BN denotes the batch
    normalization, HardSwish [43] and Sigmoid are both activated functions. (Best
    viewed in color.).
  Figure 8 Link: articels_figures_by_rev_year\2022\Constructing_Stronger_and_Faster_Baselines_for_SkeletonBased_Action_Recognition\figure_8.jpg
  Figure 8 caption: Model size versus model accuracy of different width and depth
    scaling hyper-parameters ( alpha and beta ). (Best viewed in color.).
  Figure 9 Link: articels_figures_by_rev_year\2022\Constructing_Stronger_and_Faster_Baselines_for_SkeletonBased_Action_Recognition\figure_9.jpg
  Figure 9 caption: Confusion matrices of EfficientGCN-B0 (top) and EfficientGCN-B4
    (bottom) with failure actions (less than 80% accuracy on X-sub benchmark), where
    the numbers in coordinate axes represent the indexes of each action category,
    the red and green rectangles denote two groups of similar actions. (Best viewed
    in color.).
  First author gender probability: 0.67
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Yi-Fan Song
  Name of the last author: Liang Wang
  Number of Figures: 11
  Number of Tables: 8
  Number of authors: 4
  Paper title: Constructing Stronger and Faster Baselines for Skeleton-Based Action
    Recognition
  Publication Date: 2022-03-07 00:00:00
  Table 1 caption: "TABLE 1 Comparisons of Different TC Layer Types on X-Sub Benchmark\
    \ in Accuracy (%), FLOPs ( \xD7 10 9 \xD7109) and Parameter Number ( \xD7 10 6\
    \ \xD7106)"
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 Comparisons of Different Attention Modules on X-Sub Benchmark\
    \ in Accuracy (%), FLOPs ( \xD7 10 9 \xD7109) and Parameter Number ( \xD7 10 6\
    \ \xD7106)"
  Table 3 caption: "TABLE 3 Comparisons of Different Inputs on X-Sub Benchmark in\
    \ Accuracy (%), FLOPs ( \xD7 10 9 \xD7109) and Parameter Number ( \xD7 10 6 \xD7\
    106)"
  Table 4 caption: "TABLE 4 Comparisons of Different Fusion Stages on X-Sub Benchmark\
    \ in Accuracy (%), FLOPs ( \xD7 10 9 \xD7109) and Parameter Number ( \xD7 10 6\
    \ \xD7106)"
  Table 5 caption: TABLE 5 Comparisons With SOTA Methods on NTU 60 Dataset in Accuracy
    (%)
  Table 6 caption: TABLE 6 Comparisons With SOTA Methods on NTU 120 Dataset in Accuracy
    (%)
  Table 7 caption: "TABLE 7 Comparisons With SOTA Methods on X-Sub Benchmark in Accuracy\
    \ (%), FLOPs ( \xD7 10 9 \xD7109) and Parameter Number ( \xD7 10 6 \xD7106)"
  Table 8 caption: TABLE 8 Comparisons With Other Person ReID Methods in Rank-1 Accuracy
    (%)
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3157033
- Affiliation of the first author: school of artificial intelligence, optics and electronics
    (iopen), northwestern polytechnical university, xian, china
  Affiliation of the last author: school of artificial intelligence, optics and electronics
    (iopen), northwestern polytechnical university, xian, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Matrix_Completion_via_NonConvex_Relaxation_and_Adaptive_Correlation_Learning\figure_1.jpg
  Figure 1 caption: An illustration of singular values of a natural image.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Matrix_Completion_via_NonConvex_Relaxation_and_Adaptive_Correlation_Learning\figure_2.jpg
  Figure 2 caption: Two images from MSRC-v2 which are used for image recovery.
  Figure 3 Link: articels_figures_by_rev_year\2022\Matrix_Completion_via_NonConvex_Relaxation_and_Adaptive_Correlation_Learning\figure_3.jpg
  Figure 3 caption: "Recovered images under different kinds of noise. For random noise,\
    \ the missing rate, \u03F5 , is set as 0.5, while the missing block is 50\xD7\
    50 for the block mask. The colorful image is obtained by recovering images from\
    \ three channels individually."
  Figure 4 Link: articels_figures_by_rev_year\2022\Matrix_Completion_via_NonConvex_Relaxation_and_Adaptive_Correlation_Learning\figure_4.jpg
  Figure 4 caption: "Experiemntal results on MovieLens-1M with different \u03F5 .\
    \ Note that the nuclear norm consumes more than 3000s on this dataset."
  Figure 5 Link: articels_figures_by_rev_year\2022\Matrix_Completion_via_NonConvex_Relaxation_and_Adaptive_Correlation_Learning\figure_5.jpg
  Figure 5 caption: "Recovered images employing noisy model with different \u03B3\
    \ . r is the average rank of recovered images from three channels. The mask rate,\
    \ \u03F5 , is set as 0.5."
  Figure 6 Link: articels_figures_by_rev_year\2022\Matrix_Completion_via_NonConvex_Relaxation_and_Adaptive_Correlation_Learning\figure_6.jpg
  Figure 6 caption: "Convergence of Algorithm 2 on Image-1 and MovieLens-100K with\
    \ \u03F5=0.5 . Clearly, NCARL converges rapidly on both datasets, especially Image-1."
  Figure 7 Link: articels_figures_by_rev_year\2022\Matrix_Completion_via_NonConvex_Relaxation_and_Adaptive_Correlation_Learning\figure_7.jpg
  Figure 7 caption: The influence of sparsity k to MSE on Image-1 and MovieLens-100K
    with epsilon =0.5 .
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.83
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Hongyuan Zhang
  Name of the last author: Rui Zhang
  Number of Figures: 7
  Number of Tables: 4
  Number of authors: 2
  Paper title: Matrix Completion via Non-Convex Relaxation and Adaptive Correlation
    Learning
  Publication Date: 2022-03-07 00:00:00
  Table 1 caption: TABLE 1 MSE and Consuming Seconds on Three Synthetic Matrices
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 MSE on Two Noiseless Datasets
  Table 3 caption: TABLE 3 Consuming Seconds of Various Models
  Table 4 caption: "TABLE 4 Ablation Experiments of NCARL ( \u03F5=0.5 \u03B5=0.5):\
    \ Correlation Represents the Correlation Preserving Term and Adaptive Denotes\
    \ the Adaptive Learning Mechanism"
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3157083
- Affiliation of the first author: computer science department, universidade federal
    de minas gerais, belo horizonte, mg, brazil
  Affiliation of the last author: computer science department, universidade federal
    de minas gerais, belo horizonte, mg, brazil
  Figure 1 Link: articels_figures_by_rev_year\2022\TextDriven_Video_Acceleration_A_WeaklySupervised_Reinforcement_Learning_Method\figure_1.jpg
  Figure 1 caption: 'Overview of our methodology. It is composed of two main steps:
    i) we employ our Extended Visually-guided Document Attention Network (VDAN+) to
    build a cross-modal embedding space that provides the representative embeddings
    e D and e v to the user document and the video segment, respectively; ii) we train
    a reinforcement learning agent to select which frames to remove executing actions
    to increase, decrease, or keep the current skip rate given the embeddings e D
    and e v , the encoded position in the video ( e p ), and the encoded average relative
    skip rate ( e s ).'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\TextDriven_Video_Acceleration_A_WeaklySupervised_Reinforcement_Learning_Method\figure_2.jpg
  Figure 2 caption: "Average output speed-up rate error in the YouCook2 dataset. For\
    \ target speed-up rates from 2 to 20\xD7 , SAFFA generates videos with a small\
    \ relative error, indicating its accuracy in the speed-up objective."
  Figure 3 Link: articels_figures_by_rev_year\2022\TextDriven_Video_Acceleration_A_WeaklySupervised_Reinforcement_Learning_Method\figure_3.jpg
  Figure 3 caption: Qualitative results. The colored bars (right) represent the frames
    selected by each method for each instructional video, and the contiguous black
    blocks represent the ground-truth segments. Note that, in general, our agent performs
    a denser frame sampling in the regions representing a instruction step and a sparser
    one in the other regions (see the images outlined in green and the instructions).
    Exceptions are the regions where our agent decides to increase its skip rate to
    attend the target speed-up or matches the video snippet with the instructions
    (images outlined in red).
  Figure 4 Link: articels_figures_by_rev_year\2022\TextDriven_Video_Acceleration_A_WeaklySupervised_Reinforcement_Learning_Method\figure_4.jpg
  Figure 4 caption: Qualitative results for VDAN+. The Figure shows VDAN+ document
    embeddings from the VaTeX validation set projected into two dimensions via t-SNE
    [61] and colored according to their alignment with a given input video. Note that
    the documents with the highest alignment with the input video (blue points) are
    composed of semantically related sentences, and these sentences present the higher
    attention weights (Att.).
  Figure 5 Link: articels_figures_by_rev_year\2022\TextDriven_Video_Acceleration_A_WeaklySupervised_Reinforcement_Learning_Method\figure_5.jpg
  Figure 5 caption: VDAN+ Versus VDAN [14]. The Receiver Operating Characteristic
    (ROC) curves in the YouCook2 when using all dot products as thresholds. VDAN+
    is superior in deciding if a framesegment is relevant.
  Figure 6 Link: articels_figures_by_rev_year\2022\TextDriven_Video_Acceleration_A_WeaklySupervised_Reinforcement_Learning_Method\figure_6.jpg
  Figure 6 caption: Robustness to Automatic Speech Recognition (ASR). The image illustrates
    the difference between using the annotated recipe and the ASR text as input. Note
    that the distribution of the selected frames is similar for either input (colored
    bars represent the selected frames).
  Figure 7 Link: articels_figures_by_rev_year\2022\TextDriven_Video_Acceleration_A_WeaklySupervised_Reinforcement_Learning_Method\figure_7.jpg
  Figure 7 caption: Limitations. The colored bars, contiguous black blocks, and images
    outlined in red represent, respectively, the selected frames, the ground-truth,
    and the regions where the agent took a wrong decision.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Washington Ramos
  Name of the last author: Erickson R. Nascimento
  Number of Figures: 7
  Number of Tables: 2
  Number of authors: 7
  Paper title: 'Text-Driven Video Acceleration: A Weakly-Supervised Reinforcement
    Learning Method'
  Publication Date: 2022-03-07 00:00:00
  Table 1 caption: TABLE 1 Comparison With Baselines
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Ablation Study
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3157198
- Affiliation of the first author: 4paradigm inc. beijing, china
  Affiliation of the last author: department of computer science, hong kong university
    of science and technology, hong kong, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Bilinear_Scoring_Function_Search_for_Knowledge_Graph_Learning\figure_1.jpg
  Figure 1 caption: The forms of R (r) for representative BLMs (best viewed in color).
    Different colors correspond to different parts of [ r 1 , r 2 , r 3 , r 4 ] (red
    for r 1 , blue for r 2 , yellow for r 3 , gray for r 4 ). Solid lines mean positive
    values, while dashed lines mean negative values. The empty parts have value zero.
  Figure 10 Link: articels_figures_by_rev_year\2022\Bilinear_Scoring_Function_Search_for_Knowledge_Graph_Learning\figure_10.jpg
  Figure 10 caption: MRRs of structures as estimated by the parameter-sharing approach
    and stand-alone approach.
  Figure 2 Link: articels_figures_by_rev_year\2022\Bilinear_Scoring_Function_Search_for_Knowledge_Graph_Learning\figure_2.jpg
  Figure 2 caption: A graphical illustration of the proposed form of f A (h,r,t) .
  Figure 3 Link: articels_figures_by_rev_year\2022\Bilinear_Scoring_Function_Search_for_Knowledge_Graph_Learning\figure_3.jpg
  Figure 3 caption: "Illustration of R (r) for Analogy (Fig. 3a) and three example\
    \ equivalent structures based on the conditions in Proposition 3. Fig. 3b permutes\
    \ the index [1,2,3,4] of rows and columns in A to [3,4,1,2]; Fig. 3c permutes\
    \ the values [1,2,3,4] in A to [3,4,1,2]; Fig. 3d flips the signs of values [1,2,3,4]\
    \ in A to [\u22121,2,\u22123,4] ."
  Figure 4 Link: articels_figures_by_rev_year\2022\Bilinear_Scoring_Function_Search_for_Knowledge_Graph_Learning\figure_4.jpg
  Figure 4 caption: Convergence of the testing MRR versus running time on the KG completion
    task.
  Figure 5 Link: articels_figures_by_rev_year\2022\Bilinear_Scoring_Function_Search_for_Knowledge_Graph_Learning\figure_5.jpg
  Figure 5 caption: Graphical illustration of the BLMs obtained by AutoBLM (top) and
    AutoBLM+ (bottom) on the KG completion task (Section 5.1.2). Different colors
    correspond to different parts of [boldsymbolr1 (red), boldsymbolr2 (blue), boldsymbolr3
    (yellow), boldsymbolr4 (gray)]. Solid lines mean positive values, while dashed
    lines mean negative values. The empty parts have value zero.
  Figure 6 Link: articels_figures_by_rev_year\2022\Bilinear_Scoring_Function_Search_for_Knowledge_Graph_Learning\figure_6.jpg
  Figure 6 caption: Comparison of different search algorithms.
  Figure 7 Link: articels_figures_by_rev_year\2022\Bilinear_Scoring_Function_Search_for_Knowledge_Graph_Learning\figure_7.jpg
  Figure 7 caption: Comparison of the effect of filter.
  Figure 8 Link: articels_figures_by_rev_year\2022\Bilinear_Scoring_Function_Search_for_Knowledge_Graph_Learning\figure_8.jpg
  Figure 8 caption: Effectiveness of the performance predictor.
  Figure 9 Link: articels_figures_by_rev_year\2022\Bilinear_Scoring_Function_Search_for_Knowledge_Graph_Learning\figure_9.jpg
  Figure 9 caption: Comparison of different K values.
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yongqi Zhang
  Name of the last author: James T. Kwok
  Number of Figures: 10
  Number of Tables: 12
  Number of authors: 3
  Paper title: Bilinear Scoring Function Search for Knowledge Graph Learning
  Publication Date: 2022-03-07 00:00:00
  Table 1 caption: TABLE 1 Notations Used in the Paper
  Table 10 caption: "TABLE 10 Testing Performance of H3 and MRR on Multi-Hop Query\
    \ Task. Results of \u22C6 \u2605s are Copied from [4]"
  Table 2 caption: TABLE 2 Popular Properties in KG Relations
  Table 3 caption: "TABLE 3 The Representative BLM Scoring Functions. For Each Scoring\
    \ Function We Show the Definitions, Expressiveness in Definition 1, the Ability\
    \ to Model All Common Relation Patterns in Table 2 (\u201CRP\u201D for Short),\
    \ and the Number of Parameters"
  Table 4 caption: "TABLE 4 Example r r (Resp. r \u02D8 r\u02D8) for the Two Conditions\
    \ in Proposition 1"
  Table 5 caption: TABLE 5 Statistics of the KG Completion Datasets
  Table 6 caption: "TABLE 6 Testing Performance of MRR, H1 and H10 on KG Completion.\
    \ The Best Model is Highlighted in Bold and the Second Best is Underlined. \u201C\
    \u2013\u201D Means That Results are Not Reported in Those Papers or Their Code\
    \ on That datametric is Not Available. CompGCN Uses the Entire KG in Each Iteration\
    \ and So Runs Out of Memory on the Larger Data Sets of WN18, FB15 k and YAGO3-10"
  Table 7 caption: TABLE 7 Testing MRR on Applying the BLMs Obtained From a Source
    Dataset (row) to a Target Dataset (column). Bold Numbers Indicate the Best Performance
    Each Dataset for the Models Searched by AutoBLM and AutoBLM+ Respectively
  Table 8 caption: TABLE 8 Testing MRR and Number of Parameters on Ogbl-Biokg and
    ogbl-Wikikg2. the Best Performance is Indicated in Boldface
  Table 9 caption: TABLE 9 Running Time (In Minutes) of Different Components in Algorithm
    4
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3157321
