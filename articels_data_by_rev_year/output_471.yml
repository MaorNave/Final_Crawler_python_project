- Affiliation of the first author: isti-cnr (italian national research council), pisa,
    italy
  Affiliation of the last author: department of computer science, university of verona,
    verona, italy
  Figure 1 Link: articels_figures_by_rev_year\2015\Scale_Space_Graph_Representation_and_Kernel_Matching_for_Non_Rigid_and_Textured_\figure_1.jpg
  Figure 1 caption: 'Top: sketch of the scale-space representation of a one dimensional
    function f(x) , (a) f(x) computed at the initial scale s 1 , (b) f(x) computed
    at an intermediate scale s 2 and (c) f(x) computed at the final scale s 3 , with
    s 1 < s 2 < s 3 . The local maxima are enumerated and indicated with diamond markers.
    Bottom: The three steps of the graph definition: (d) creation of the nodes and
    the edges, the number inside each node indicates its corresponding local maximum;
    (e) node pruning, node number 7 has a one-child-no-sibling configuration and it
    will be deleted; (f) final insertion of the artificial root node.'
  Figure 10 Link: articels_figures_by_rev_year\2015\Scale_Space_Graph_Representation_and_Kernel_Matching_for_Non_Rigid_and_Textured_\figure_10.jpg
  Figure 10 caption: "Tier Images of the \u201CShape-only\u201D classification."
  Figure 2 Link: articels_figures_by_rev_year\2015\Scale_Space_Graph_Representation_and_Kernel_Matching_for_Non_Rigid_and_Textured_\figure_2.jpg
  Figure 2 caption: Examples of shapes from the SHREC'14 dataset.
  Figure 3 Link: articels_figures_by_rev_year\2015\Scale_Space_Graph_Representation_and_Kernel_Matching_for_Non_Rigid_and_Textured_\figure_3.jpg
  Figure 3 caption: (a),(b),(c),(e),(f),(g) Salient points and related basins of attractions,
    visualized in different colors, at three selected time values samples ( t 2 ,
    t 4 , t 10 ) on two example models; (d),(h) their corresponding TreeSha graphs.
  Figure 4 Link: articels_figures_by_rev_year\2015\Scale_Space_Graph_Representation_and_Kernel_Matching_for_Non_Rigid_and_Textured_\figure_4.jpg
  Figure 4 caption: "Retrieval scores (NN, FT, ST and ADR) for \u201CTextured-Shape\u201D\
    \ SHREC'13 dataset varying \u03C1 parameter."
  Figure 5 Link: articels_figures_by_rev_year\2015\Scale_Space_Graph_Representation_and_Kernel_Matching_for_Non_Rigid_and_Textured_\figure_5.jpg
  Figure 5 caption: 'Best Retrieval scores (NN, FT, ST and ADR) for subset of kernel
    functions, ordered by cardinality. The kernel functions are indicated as follows:
    Color Histogram (H), Area (A), Mean Color (M), Degree (D), Edge(E), Shape Diameter
    Function (S), Geodesic Distance (G), Curvature (C) and Edge Geodesic distance
    (Eg).'
  Figure 6 Link: articels_figures_by_rev_year\2015\Scale_Space_Graph_Representation_and_Kernel_Matching_for_Non_Rigid_and_Textured_\figure_6.jpg
  Figure 6 caption: "\u201CTextured-Shape\u201D Precision-Recall plots for six example\
    \ classes and averaged over all models for both textured datasets."
  Figure 7 Link: articels_figures_by_rev_year\2015\Scale_Space_Graph_Representation_and_Kernel_Matching_for_Non_Rigid_and_Textured_\figure_7.jpg
  Figure 7 caption: "Tier Images of the \u201CTextured-Shape\u201D classification."
  Figure 8 Link: articels_figures_by_rev_year\2015\Scale_Space_Graph_Representation_and_Kernel_Matching_for_Non_Rigid_and_Textured_\figure_8.jpg
  Figure 8 caption: "\u201CShape-only\u201D Precision-Recall plots for six example\
    \ classes and averaged over all models for SHREC'13 SHREC'14 datasets."
  Figure 9 Link: articels_figures_by_rev_year\2015\Scale_Space_Graph_Representation_and_Kernel_Matching_for_Non_Rigid_and_Textured_\figure_9.jpg
  Figure 9 caption: Precision-Recall plots for six example classes and averaged over
    all models for SHREC'11 dataset.
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.82
  Name of the first author: Valeria Garro
  Name of the last author: Andrea Giachetti
  Number of Figures: 11
  Number of Tables: 8
  Number of authors: 2
  Paper title: Scale Space Graph Representation and Kernel Matching for Non Rigid
    and Textured 3D Shape Retrieval
  Publication Date: 2015-09-10 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 \u201CTextured-Shape\u201D Retrieval Performances Obtained\
      \ with the TreeSha Representation and the Complete Set of Graph Kernels Described\
      \ in Section 4, Compared with the Scores of the Best Performing Methods in the\
      \ SHREC'13 Contest (A2,G2,Gi,V2) and with PHOG"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 \u201CTextured-Shape\u201D Retrieval Performances Obtained\
      \ with the TreeSha Representation and the Complete Set of Graph Kernels Described\
      \ in Section 4, Compared with the Scores of the Best Performing Methods in the\
      \ SHREC'14 Contest (GG2,Gi2,Gi3,Ve1)"
  Table 3 caption:
    table_text: "TABLE 3 \u201CShape-Only\u201D Retrieval Performances Obtained with\
      \ the TreeSha Representation and All the Geometric Graph Kernels Described in\
      \ Section 4, Compared with the Scores of the Best Performing Methods in the\
      \ SHREC'13 Contest (A2,G1,Gi,V2)"
  Table 4 caption:
    table_text: "TABLE 4 \u201CShape-Only\u201D Retrieval Performances Obtained with\
      \ the TreeSha Representation and All the Geometric Graph Kernels Described in\
      \ Section 4, Compared with the Scores of the Best Performing Methods in the\
      \ SHREC'14 Contest (Gi3,LBG4,TA,Ve2)"
  Table 5 caption:
    table_text: TABLE 5 Retrieval Performances Obtained with the TreeSha Representation
      and All the Geometric Graph Kernels Described in Section 4, Compared with the
      Scores of the Best Performing Methods in the SHREC'11 Contest [58] (HKS, FOG
      + MRR, MLSF, MDS-CM-BOF, ShapeDNA, SD-GDM-MeshSIFT) and HAPT [15]
  Table 6 caption:
    table_text: 'TABLE 6 Computational Performances of the Graph Kernel Matching Step:
      the Values Are Referring to the Comparison of the Whole Datasets, i.e., All
      Models Are Compared to the Remaining Ones of the Respective Dataset, the Average
      Time to Compare a Pair of Models Is Indicated between Parentheses'
  Table 7 caption:
    table_text: "TABLE 7 Retrieval and Computational Performances of TreeSha and TreeShaMod\
      \ Applied to the SHREC'13 Dataset with \u201CTextured-Shape\u201D Classification"
  Table 8 caption:
    table_text: "TABLE 8 Retrieval and Computational Performances of TreeSha and TreeShaMod\
      \ Applied to the SHREC'13 Dataset with \u201CShape-Only\u201D Classification"
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2477823
- Affiliation of the first author: electrical engineering department, tel-aviv university,
    tel-aviv, ramat-aviv, israel
  Affiliation of the last author: electrical engineering department, tel-aviv university,
    tel-aviv, ramat-aviv, israel
  Figure 1 Link: articels_figures_by_rev_year\2015\Coherency_Sensitive_Hashing\figure_1.jpg
  Figure 1 caption: "Adapted from [6]: An array of the 64 Walsh-Hadamard kernels of\
    \ dimension 8 , ordered with increasing sequency in each column and row. For each\
    \ kernel, white represents the value 1 , and black represents the value \u2212\
    1 . The 'snake' ordering (shown) forms a Gray-Code sequence of the kernels."
  Figure 10 Link: articels_figures_by_rev_year\2015\Coherency_Sensitive_Hashing\figure_10.jpg
  Figure 10 caption: "Mapping errors ordered by patch energy. x-axis. Patches of the\
    \ source image are divided into 10 deciles, according to their energy level (mean\
    \ gradient magnitude). y-axis. the difference between PatchMatch and CSH mapping\
    \ errors, averaged over each of the deciles. On the lower end, the first decile\
    \ represents patches with low energy in the range [0,14] on which PatchMatch error\
    \ (mean L2 patch distances) is slightly lower (two graylevels), while at the tenth\
    \ decile (high energy in the range [155,255] )\u2014CSH error is significantly\
    \ lower (over 11 graylevels)."
  Figure 2 Link: articels_figures_by_rev_year\2015\Coherency_Sensitive_Hashing\figure_2.jpg
  Figure 2 caption: 'Spatial decomposition of LSH versus CSH. Points (patches) are
    projected to (solid) lines (green, red). Dashed lines represent the respective
    binning. Each projection line defines a hash function h i , which assigns a bin
    number to the point. The final hash function g is a concatenation of the bin numbers,
    given by each h i and points in the same ''cell'' (gray area) share a hash code.
    There are three main differences between LSH and CSH hashing: (i) LSH uses random
    oriented lines, while CSH uses fixed orthogonal (WH) lines. (ii) LSH uses an equal
    number of bins per projection, while CSH allocates more bins (code bits) to more
    ''informative'' projections. (iii) LSH bins are evenly spaced, while CSH bins
    are variably spaced aiming at a uniform spread of samples.'
  Figure 3 Link: articels_figures_by_rev_year\2015\Coherency_Sensitive_Hashing\figure_3.jpg
  Figure 3 caption: "BitKernel allocations. The table specifies the number of bins\
    \ (and the number of bits, in parenthesis) allocated to each of the eight WH kernels\
    \ used in the hashing scheme. Each row specifies the allocations for a different\
    \ size of patch: 8\xD78 and 16\xD716 (which use the same allocation), 4\xD74 and\
    \ 2\xD72 . Each column represents a specific WH kernel, with the following convention:\
    \ YCbCr stands for the channel the kernel is applied on. The pair of subscripts\
    \ relate to the location of the WH kernel in the 2d arrangement, like the one\
    \ shown for 8\xD78 patches, in Fig. 1. Notice that the total length of the hash\
    \ code will be 18 bits for patch dimensions of 8 or 16 and slightly shorter for\
    \ the smaller patch sizes."
  Figure 4 Link: articels_figures_by_rev_year\2015\Coherency_Sensitive_Hashing\figure_4.jpg
  Figure 4 caption: "Hashing by binning of kernel projections. The four histograms\
    \ illustrate the different distributions obtained by projecting all patches of\
    \ an image on different WH kernels (histograms are scaled to fit the figure).\
    \ In each, the 8\xD78 kernel is shown with its name (see Fig. 3 for the convention)\
    \ and the number of bins. Examples\u2014 Top-left. (DC)-kernel applied on the\
    \ Y channel produces a very dispersed distribution, which is divided into a large\
    \ number of 32 bins (defined by the red bin edges, located at fixed percentiles).\
    \ Besides the extreme two bins (due to random shift), each of the 30 other bins\
    \ contains 132 of the image patches. Top-right. The DC distribution on a color\
    \ channel is typically less informative compared to the Y channel. The distribution\
    \ is more centralized and hence, is divided into less (only four) bins. Bottom-right.\
    \ As the frequency of the kernel increases, the distribution concentrates around\
    \ zero."
  Figure 5 Link: articels_figures_by_rev_year\2015\Coherency_Sensitive_Hashing\figure_5.jpg
  Figure 5 caption: Candidate types for a patch. In each of the sub-figures, Image
    A is on the left, image B is on the right and the hash table in use is in the
    center. Arrows relating to a pixel actually relate to the patch whose top left
    corner is at the pixel. Red arrows represent the hashing (notice their direction),
    while green arrows point to the patch's current best known representative. The
    highlighted pixels (patches) in image B on the right are the candidates of the
    highlighted pixel (patch) in image A on the left. If the width of the hash table
    is defined to be w (i.e., it stores w representative patches from each of the
    two images) then the total number of candidates is between 4k and 4k+2 (types
    1 and 3 each contribute w candidates, while type 2 appears both in leftright and
    topbottom configurations and contributes w or w+1 in each configuration). In our
    implementation (and this illustration) we use w=2 .
  Figure 6 Link: articels_figures_by_rev_year\2015\Coherency_Sensitive_Hashing\figure_6.jpg
  Figure 6 caption: "WH projections used for L2 distance approximation. The illustration\
    \ specifies the subsets of WH kernels, used for lower bounding L2 patches distances\
    \ according to Equation (3), for patches of dimensions 16, 8, 4 and 2. The matrices\
    \ depict 2d arrays of WH kernels, where kernels marked red are those used for\
    \ all patch channels (YCbCr), while the blue ones are used for the Y channel only.\
    \ For example, in the 8\xD78 case, out of 192 kernels ( 8\u22C58\u22C53 ), we\
    \ use only 23 (15 for Y and 4 for each of Cb and Cr). Notice that the set of projections\
    \ used for hash code construction is a strict subset of this selection. The Grey-Code-Kernels\
    \ method for efficient projection computation, requires that each projection should\
    \ be computed after either of the projection above or to the left of it. This\
    \ is clearly possible with the proposed set of chosen kernels."
  Figure 7 Link: articels_figures_by_rev_year\2015\Coherency_Sensitive_Hashing\figure_7.jpg
  Figure 7 caption: The Video Pairs data set (8 out of the 133 pairs).
  Figure 8 Link: articels_figures_by_rev_year\2015\Coherency_Sensitive_Hashing\figure_8.jpg
  Figure 8 caption: ErrorTime tradeoffs of PatchMatch and CSH. Averages are over the
    133 image pairs of the data set. Markers on the lines indicate the time it took
    each algorithm to complete an iteration, and errors are average L2 distances between
    patches. Lower error rates (such as those reached by CSH on its third iteration)
    are reached more than four times faster by CSH compared to PatchMatch. Notice
    that the CSH errors are significantly closer to the ground truth average error
    (denoted by the solid red line).
  Figure 9 Link: articels_figures_by_rev_year\2015\Coherency_Sensitive_Hashing\figure_9.jpg
  Figure 9 caption: Efficiency of different hash functions. Each curve (one per hash-function)
    describes the CDF of query-to-bin distances. For a specific error level (x-axis)
    it shows the fraction of query patches (y-axis), whose average distance from target
    patches with the same code is below that error level. The three CSH alternatives
    do relatively well in general (the higher the curve the better) and are especially
    competitive in the lower range of error levels (see enlarged region), where good
    matches are generated. The K-means based CSH-AKM (green) is generally more accurate
    than its counterparts (CSH and CSH-AP), however it produces many less low-error
    matches and is therefore inferior in the CSH framework. The baseline LSH [22]
    as well as ITQ [22] are clearly less accurate while the state-of-the-art PQ [22]
    and OPQ [13] are more accurate than the CSH alternatives, but their runtimes (shown
    in the legend, in seconds, for hashing the patches of a pair of images) are several
    orders of magnitude slower.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.72
  Name of the first author: Simon Korman
  Name of the last author: Shai Avidan
  Number of Figures: 19
  Number of Tables: 1
  Number of authors: 2
  Paper title: Coherency Sensitive Hashing
  Publication Date: 2015-09-10 00:00:00
  Table 1 caption:
    table_text: ''
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2477814
- Affiliation of the first author: "clermont universit\xE9, universit\xE9 blaise pascal,\
    \ cnrs, umr 6158, limos, aubi\xE8re, france"
  Affiliation of the last author: nicta, canberra, australia
  Figure 1 Link: "articels_figures_by_rev_year\\2015\\Learning_SVM_in_Kre\u012Dn_Spaces\\\
    figure_1.jpg"
  Figure 1 caption: The three solutions of the illustrative example.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: "articels_figures_by_rev_year\\2015\\Learning_SVM_in_Kre\u012Dn_Spaces\\\
    figure_2.jpg"
  Figure 2 caption: Illustration of the solutions provided by minimizing or stabilizing
    the cost function of a 2d indefinite problem with a linear kernel.
  Figure 3 Link: "articels_figures_by_rev_year\\2015\\Learning_SVM_in_Kre\u012Dn_Spaces\\\
    figure_3.jpg"
  Figure 3 caption: "Training 2D checkers on 96 training points with different methods.\
    \ The least eigenvalue of the tanh kernel is \u221213.11 ."
  Figure 4 Link: "articels_figures_by_rev_year\\2015\\Learning_SVM_in_Kre\u012Dn_Spaces\\\
    figure_4.jpg"
  Figure 4 caption: "Training 2D checkers on 400 training points with different methods.\
    \ The least eigenvalue of the tanh kernel is \u221259.73 ."
  Figure 5 Link: "articels_figures_by_rev_year\\2015\\Learning_SVM_in_Kre\u012Dn_Spaces\\\
    figure_5.jpg"
  Figure 5 caption: This figure shows the training and testing time for various methods
    applicable to indefinite matrices for kernel based classification. The slope of
    each curve indicates the complexity of the algorithms. The results are obtained
    for training set size from 1,000 to 10,000, on checkerboard datasets, which are
    2d and separable. The tanh kernel was used with parameters [-1, 1]. The test set's
    size is 1,000.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.91
  Name of the first author: "Ga\xEBlle Loosli"
  Name of the last author: Cheng Soon Ong
  Number of Figures: 5
  Number of Tables: 4
  Number of authors: 3
  Paper title: "Learning SVM in Kre\u012Dn Spaces"
  Publication Date: 2015-09-10 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Possible Values of \u03B1 i"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average Gain of Stabilization over Minimization in Terms of
      Margins, Along with the Average Difference of Cost Function Optimal Value
  Table 3 caption:
    table_text: TABLE 3 This Tables Gives an Overview of the Different Datasets Used
      in Our Study
  Table 4 caption:
    table_text: TABLE 4 Error Rates (Standard Deviations) Are Obtained on Average,
      Based of 20 Random Splits of Each Dataset
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2477830
- Affiliation of the first author: eecs department at the university of california,
    berkeley, in berkeley, ca
  Affiliation of the last author: cse department, university of california, san diego,
    la jolla, ca
  Figure 1 Link: articels_figures_by_rev_year\2015\Depth_Estimation_and_Specular_Removal_for_Glossy_Surfaces_Using_Point_and_Line_C\figure_1.jpg
  Figure 1 caption: 'Depth Estimation for Glossy Surfaces. Our input is a light-field
    image. We use PBRT [13] to synthesize a red wood textured glossy sphere with specular
    reflectance K s =[1,1,1] and roughness =0.001 with four light sources of different
    colors (a). We use two photoconsistency metrics: point-consistency and line-consistency.
    By using point-consistency, we obtain depth measures suitable for diffuse only
    surfaces, but exhibit erroneous depth (b) and high confidence (c) at glossy regions
    due to overfitting data. By using the light-source color estimation (d), we seek
    a depth where the colors from different viewpoints represent a line, with direction
    corresponding to light-source color, which we call line-consistency. The new depth
    measurement gives correct depths at specular edges (e), but exhibits low confidence
    values everywhere else. We highlighted the difference of the edges by highlighting
    in white. The comparison between the two can be seen in Fig. 3. We use both of
    the cues to perform a depth regularization that produces an optimal result by
    exploiting the advantages of both cues (g). With the analysis, we can also extract
    a specular-free image (h) and an estimated specular image (i). In this paper,
    we provide the theoretical background of using the two metrics. Note: The specular
    colors are enhanced for easier visibility throughout the paper. For depth maps,
    cool to warm colors represent closer to farther respectively, and for confidence
    maps, less confident to more confident respectively, with a scale between 0 and
    1.'
  Figure 10 Link: articels_figures_by_rev_year\2015\Depth_Estimation_and_Specular_Removal_for_Glossy_Surfaces_Using_Point_and_Line_C\figure_10.jpg
  Figure 10 caption: Our Results. We compare our depth estimation results against
    Lytro software, Heber and Pock [39], Tao et al. 14 [6], Tao et al. 13 [5], and
    Wanner et al. [4]; we compare our specular removal results against Mallick et
    al. [10], Yoon et al. [11], and Tao et al. 14 [6]. On the top, we have an example
    of a smooth cat with texture and speckles. Our final depth is smooth and contains
    plausible shape details on the paw. We also can see that we remove the large specularities
    on the stomach of the cat. In the second example, we have a figurine of a princess
    holding her skirt with the left arm tilted behind her body. We can see that our
    depth estimation exhibits less errors at the large patches of specularities whereas
    the Lytro result shows erroneous patches. Our depth algorithm is able to recover
    the contours of the dress and resolve depth where the left arm is behind the body.
    Our algorithm removes the large specularities on the bow. With the third example
    of Chip and Dale, we show that our depth result resembles the shape of the figurine.
    The method is able to recover the shape of the feet on the right and does not
    exhibit specularity depth artifacts. The specularities throw off previous methods.
    Our result also successfully removes the glossy regions throughout the whole figurine.
    The red box indicated in the input image is where our insets are cropped from.
  Figure 2 Link: articels_figures_by_rev_year\2015\Depth_Estimation_and_Specular_Removal_for_Glossy_Surfaces_Using_Point_and_Line_C\figure_2.jpg
  Figure 2 caption: Synthetic data for dimension analysis of different types of BRDF
    with one light source. The synthetic data is generated by PBRT [13] to simulate
    the Lytro camera. Note that for the general diffuse surface, we use a view-dependent
    spectral reflectance k d for the sphere. The center images are linearly scaled
    for display. The scatter plots are pixel intensities in RGB color space from 49
    different views, imaging the location where the red arrow points. All pixel values
    are scaled to [0,1] . Synthetic data shows that when the light field image is
    refocused to the correct depth, the result corresponds to our dimension analysis
    ( Table 1).
  Figure 3 Link: articels_figures_by_rev_year\2015\Depth_Estimation_and_Specular_Removal_for_Glossy_Surfaces_Using_Point_and_Line_C\figure_3.jpg
  Figure 3 caption: Point-consistency versus Line-Consistency. For point-consistency,
    diffuse edges exhibit high confidence and meaningful depth. However, for specular
    regions, the error is large with high confidence. With line-consistency, diffuse
    regions register depth values that are noisy and lower confidence. For specular
    regions, line-consistency is accurate at the edges with high confidence. For both,
    it is important to note that the metrics have high confidence where edges are
    present. Therefore, saturated pixels, often observed in large specular patches,
    and smooth surfaces require data propagation. Although saturated specular regions
    still have high errors, we can see that line-consistency has a much lower confidence
    than the point-consistency metric.
  Figure 4 Link: articels_figures_by_rev_year\2015\Depth_Estimation_and_Specular_Removal_for_Glossy_Surfaces_Using_Point_and_Line_C\figure_4.jpg
  Figure 4 caption: "Line Estimation. With the same input image scene as Fig. 1 and\
    \ sampled point (a), we plot the the angular pixels at the point-consistency depth,\
    \ I \u03B1 p (u,v) (b). By using the angular pixels, we can estimate the light\
    \ source color (estimated line shown in blue) accurately (ground truth shown in\
    \ red). With point-consistency, we reduce the influence of colors from neighboring\
    \ points but still have enough color variation to estimate the light-source color\
    \ (c). Without using the point-consistency set of angular pixels, we can see that\
    \ neighborhood pixels from the sphere throw off the line estimations (shown in\
    \ dotted green lines) (d)."
  Figure 5 Link: articels_figures_by_rev_year\2015\Depth_Estimation_and_Specular_Removal_for_Glossy_Surfaces_Using_Point_and_Line_C\figure_5.jpg
  Figure 5 caption: "Estimating Multiple Light-Sources. By using our L \u2217 estimation\
    \ on the scene with two highly glossy cans with two light sources (a), we can\
    \ see that the estimated L \u2217 (b) is consistent with the ground truth (e).\
    \ The RMSE for the green and red light-sources are 0.063 and 0.106 respectively.\
    \ Even with semi-glossy crayons (c), the light source estimation is consistent\
    \ (d). The RMSE for the green and red light-sources are 0.1062 and 0.0495 respectively.\
    \ We took photos directly of the light sources for ground truth."
  Figure 6 Link: articels_figures_by_rev_year\2015\Depth_Estimation_and_Specular_Removal_for_Glossy_Surfaces_Using_Point_and_Line_C\figure_6.jpg
  Figure 6 caption: "Light-Source Estimation. With input image (a), we estimate the\
    \ light-source color, L , for each pixel as shown in (b). We use the k-means clustering\
    \ method to estimate the light-source color, L \u2217 of the scene (c). The light\
    \ source colors match the ground-truth, starting from top left to bottom right,\
    \ with RMSE of 0.0676, 0.0790, 0.0115, and 0.0555. In Section 4.6.1, we show how\
    \ we measure the specular intensity (d) of each pixel to estimate specular-free\
    \ images."
  Figure 7 Link: articels_figures_by_rev_year\2015\Depth_Estimation_and_Specular_Removal_for_Glossy_Surfaces_Using_Point_and_Line_C\figure_7.jpg
  Figure 7 caption: Specular removal. With just using the angular information, we
    are able to reduce speckles. However, with large specular regions such as the
    one from the sphere, the specular removal from angular information can only remove
    partially (reducing the size of the specular highlight). Therefore, spatial Poisson
    reconstruction hole filling is needed to completely remove large saturated specular
    regions.
  Figure 8 Link: articels_figures_by_rev_year\2015\Depth_Estimation_and_Specular_Removal_for_Glossy_Surfaces_Using_Point_and_Line_C\figure_8.jpg
  Figure 8 caption: Qualitative and Quantitative Synthetic Results. We added Gaussian
    noise with zero mean and variance as the variable parameter to the input image
    of Fig. 1. We compute the RMSE of our results against the ground truth diffuse
    image and depth map. On the left, even with high noise, we can see that our diffuse
    and specular separation closely resembles the ground truth. In both cases, the
    algorithm is able to extract all four specular regions. For depth maps, we can
    see that the depth estimation at high noise still reasonably resembles the ground
    truth sphere. On the right, we can see that these qualitative results reflect
    the quantitative result. We see that our results outperform prior works by a significant
    margin.
  Figure 9 Link: articels_figures_by_rev_year\2015\Depth_Estimation_and_Specular_Removal_for_Glossy_Surfaces_Using_Point_and_Line_C\figure_9.jpg
  Figure 9 caption: Flat Glossy Surface Results. We have a completely flat glossy
    surface with specular sequins throughout the image that we placed directly perpendicular
    to the camera (a). For the ground truth, the depth should be flat (b). We can
    see that our final result is also smooth and flat (c). The line-consistency provides
    the smoothness, but has some errors in the non-glossy regions (d). The point-consistency
    is thrown off by some of the glossy regions of the image (e). With the Lytro's
    depth estimation, we also see that the specular regions throw-off the depth estimation
    (f).
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Michael W. Tao
  Name of the last author: Ravi Ramamoorthi
  Number of Figures: 12
  Number of Tables: 1
  Number of authors: 5
  Paper title: Depth Estimation and Specular Removal for Glossy Surfaces Using Point
    and Line Consistency with Light-Field Cameras
  Publication Date: 2015-09-10 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Dimension Analysis of Different Types of BRDF with One Light
      Source
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2477811
- Affiliation of the first author: "institute of computer science and applied mathematics,\
    \ university of bern, nebr\xFCckstrasse 10, bern, switzerland"
  Affiliation of the last author: "institute of computer science and applied mathematics,\
    \ university of bern, nebr\xFCckstrasse 10, bern, switzerland"
  Figure 1 Link: articels_figures_by_rev_year\2015\A_Clearer_Picture_of_Total_Variation_Blind_Deconvolution\figure_1.jpg
  Figure 1 caption: "Illustration of the convolution operators (8) and (9) assuming\
    \ a blur support of 3\xD73 . a) With the use of (8) we assume that the blurry\
    \ image f and the sharp image u have the same support, therefore we must choose\
    \ how the values at the boundaries of u are defined (red pixels); b) With (9)\
    \ we assume that f has a smaller support than u , therefore the pixels of f are\
    \ completely defined by the pixels of u ."
  Figure 10 Link: articels_figures_by_rev_year\2015\A_Clearer_Picture_of_Total_Variation_Blind_Deconvolution\figure_10.jpg
  Figure 10 caption: Example of blind-deconvolution image and blur (bottom-right insert)
    restoration.
  Figure 2 Link: articels_figures_by_rev_year\2015\A_Clearer_Picture_of_Total_Variation_Blind_Deconvolution\figure_2.jpg
  Figure 2 caption: "Illustration of Proposition 4.5 and Proposition 4.4 (best viewed\
    \ in color). The original step function is denoted by a solid-blue line. The TV\
    \ signal (green-solid) is obtained by solving arg min u \u2225u\u2212f \u2225\
    \ 2 2 +\u03BBJ(u) . In (a) we show how the TV denoising algorithm reduces the\
    \ contrast of a blurred step function (red-dotted). In (b) we illustrate Proposition\
    \ 4.4: If the constraints on the blur are enforced, any blur different from the\
    \ Dirac delta increases the distance between the input blurry signal and the blurry\
    \ TV signal (black-solid). In (c) we illustrate Proposition 4.5: In the second\
    \ step of the PAM algorithm, estimating a blur kernel without a normalization\
    \ constraint is equivalent to scaling the TV signal."
  Figure 3 Link: articels_figures_by_rev_year\2015\A_Clearer_Picture_of_Total_Variation_Blind_Deconvolution\figure_3.jpg
  Figure 3 caption: "Illustration of Proposition 4.1 (best viewed in color). In this\
    \ example we show a 1D experiment where we blur a step function with k 0 =[0.4;0.3;0.3]\
    \ . We visualize the cost function of eq. (10) for three different values of the\
    \ parameter \u03BB . Since the blur integrates to 1 , only two of the three components\
    \ are free to take values on a triangular domain (the upper-left triangle in each\
    \ image). We denote with a yellow triangle the true blur k 0 and with white dots\
    \ the intermediate blurs estimated during the minimization via the PAM algorithm.\
    \ Blue pixels have lower values than the red pixels. Dirac delta blurs are located\
    \ at the three corners of each triangle. At these locations, as well as at the\
    \ true blur, there are local minima. Notice how for a sufficiently large lambda\
    \ the path of the estimated blur on the rightmost image ascends and then descends\
    \ a hill in the cost functional."
  Figure 4 Link: articels_figures_by_rev_year\2015\A_Clearer_Picture_of_Total_Variation_Blind_Deconvolution\figure_4.jpg
  Figure 4 caption: "Illustration of Proposition 4.5 (best viewed in color). Each\
    \ row represents the visualization of the cost function for a particular value\
    \ of the parameter \u03BB . Each column shows the cost function for three different\
    \ blur normalizations: ||k| | 1 =1, 1.5, and 2.5 . We denote the scaled true blur\
    \ k 0 =[0.2, 0.5, 0.3] (with ||k| | 1 =1 ) with a red triangle and with a red\
    \ dot the cost function minimum. The color coding is such that: blue < yellow\
    \ < red; each row shares the same color coding for cross comparison."
  Figure 5 Link: articels_figures_by_rev_year\2015\A_Clearer_Picture_of_Total_Variation_Blind_Deconvolution\figure_5.jpg
  Figure 5 caption: "Illustration of configurations of \u03B4 1 and \u03B4 2 for which\
    \ it is not possible to estimate a sharp signal in problem (18) (cases a) and\
    \ b) ) and in problem (24) (case c) ). The region of feasible solutions is where\
    \ \u03B4 1 + \u03B4 2 \u22641 is satisfied and is denoted by the white region;\
    \ the red lines denote the configurations that can not lead to a sharp signal\
    \ for: a) a signal as in Proposition 4.2 such that L 1 =3 and L 2 =3 ; b) a signal\
    \ as in Proposition 4.2 such that L 1 =15 and L 2 =24 ; c) a filtered signal as\
    \ in Proposition 4.3."
  Figure 6 Link: articels_figures_by_rev_year\2015\A_Clearer_Picture_of_Total_Variation_Blind_Deconvolution\figure_6.jpg
  Figure 6 caption: Comparison of PAM algorithm with different boundary conditions.
  Figure 7 Link: articels_figures_by_rev_year\2015\A_Clearer_Picture_of_Total_Variation_Blind_Deconvolution\figure_7.jpg
  Figure 7 caption: As in Fig. 6, but using a filtered version of the images for the
    blur estimation.
  Figure 8 Link: articels_figures_by_rev_year\2015\A_Clearer_Picture_of_Total_Variation_Blind_Deconvolution\figure_8.jpg
  Figure 8 caption: Comparison between the PAM algorithm and recent state-of-the-art
    algorithms on the dataset [37].
  Figure 9 Link: articels_figures_by_rev_year\2015\A_Clearer_Picture_of_Total_Variation_Blind_Deconvolution\figure_9.jpg
  Figure 9 caption: Comparison between the PAM algorithm and recent state-of-the-art
    algorithms on the dataset [32].
  First author gender probability: 0.95
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Daniele Perrone
  Name of the last author: Paolo Favaro
  Number of Figures: 12
  Number of Tables: 1
  Number of authors: 2
  Paper title: A Clearer Picture of Total Variation Blind Deconvolution
  Publication Date: 2015-09-10 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Formulas of U 1 and U 2 for \u03BB\u2208[ \u03BB l min ,\
      \ \u03BB l max ) , \u03BB\u2208[ \u03BB c min , \u03BB c max ) and \u03BB\u2208\
      [ \u03BB r min , \u03BB r max ) Used in Proposition 4.2"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2477819
- Affiliation of the first author: department of information engineering and computer
    science, university of trento, italy
  Affiliation of the last author: department of information engineering and computer
    science, university of trento, italy
  Figure 1 Link: articels_figures_by_rev_year\2015\A_MultiTask_Learning_Framework_for_Head_Pose_Estimation_under_Target_Motion\figure_1.jpg
  Figure 1 caption: HPE under target motion. Face crops corresponding to three different
    positions of a target exhibiting the same 3D head pose are shown in the bottom
    inset. Yellow and red arrows respectively denote head pose and motion direction.
    Significant changes in facial appearance can be observed as the target moves closer
    to the camera. These appearance differences severely impede performance of traditional
    head pose estimation (or classification) methods. Figure is best viewed in color
    and under zoom.
  Figure 10 Link: articels_figures_by_rev_year\2015\A_MultiTask_Learning_Framework_for_Head_Pose_Estimation_under_Target_Motion\figure_10.jpg
  Figure 10 caption: Sensitivity analysis. Classification accuracy on varying regularization
    parameters lambda 1 and lambda 2 when lambda s and lambda theta are fixed (left);
    lambda s and lambda theta with lambda 1 and lambda 2 fixed (right).
  Figure 2 Link: articels_figures_by_rev_year\2015\A_MultiTask_Learning_Framework_for_Head_Pose_Estimation_under_Target_Motion\figure_2.jpg
  Figure 2 caption: FEGA-MTL HPE framework overview assuming three camera views. Blue
    and red blocks correspond to the training and test phases respectively. FEGA-MTL
    can be trained using annotated ('Training' box on top-row center) or unlabeled
    images, where motion direction serves as a weak label for head pose (top-row right),
    enabling its use in both supervised and unsupervised settings. Figure is best
    viewed in color and under zoom.
  Figure 3 Link: articels_figures_by_rev_year\2015\A_MultiTask_Learning_Framework_for_Head_Pose_Estimation_under_Target_Motion\figure_3.jpg
  Figure 3 caption: (From left to right) Method to predict appearance distortion induced
    due to translation of the head sphere Z k from p k to p (exemplified with respect
    to camera C3), appearance similarity map computed around p k with U=3 and U=4
    camera views, and learned grid clusters for the three-camera setup (figure best
    viewed in color).
  Figure 4 Link: articels_figures_by_rev_year\2015\A_MultiTask_Learning_Framework_for_Head_Pose_Estimation_under_Target_Motion\figure_4.jpg
  Figure 4 caption: 'Exemplar target trajectory from DPOSE [7]: blue dots correspond
    to samples retained after the filtering process, while red ones are discarded.
    The two sets of head crops correspond to filtering windows associated with two
    samples. In the dotted blue rectangle, high similarity among the warped crops
    imply consistent head and body movements, and thus this sample is used for training.
    In the red rectangle, warps based on trajectory-based analysis differ considerably,
    and this sample is rejected (best viewed under zoom).'
  Figure 5 Link: articels_figures_by_rev_year\2015\A_MultiTask_Learning_Framework_for_Head_Pose_Estimation_under_Target_Motion\figure_5.jpg
  Figure 5 caption: 'Synthetic data experiments: (left) Comparison with several MTL
    methods: classification accuracy. Higher numbers indicate better performance.
    (middle) Comparison with several MTL methods for the regression problem. Lower
    numbers indicate better performance. (right) S matrix for regression task comprising
    task clusters.'
  Figure 6 Link: articels_figures_by_rev_year\2015\A_MultiTask_Learning_Framework_for_Head_Pose_Estimation_under_Target_Motion\figure_6.jpg
  Figure 6 caption: 'DPOSE dataset: Comparison with state-of-the-art head pose classification
    methods.'
  Figure 7 Link: articels_figures_by_rev_year\2015\A_MultiTask_Learning_Framework_for_Head_Pose_Estimation_under_Target_Motion\figure_7.jpg
  Figure 7 caption: (Left to right) Classification accuracies with Graph-guided MTL
    methods using 4, 3, 2 and single-view information.
  Figure 8 Link: articels_figures_by_rev_year\2015\A_MultiTask_Learning_Framework_for_Head_Pose_Estimation_under_Target_Motion\figure_8.jpg
  Figure 8 caption: (Top) Head pose classification results for a target moving freely
    within a three-camera setup are shown two-by-two. The learned clusters, as seen
    from a fourth view, are shown on the bottom-left inset. Cluster corresponding
    to the target position (denoted using a stick model) is highlighted. (Bottom)
    Head pose classification results for the PARTY sequence involving mobile targets
    (best viewed under zoom).
  Figure 9 Link: articels_figures_by_rev_year\2015\A_MultiTask_Learning_Framework_for_Head_Pose_Estimation_under_Target_Motion\figure_9.jpg
  Figure 9 caption: 'DPOSE dataset: Head pose classification accuracy obtained with
    different methods employing different features and features combination.'
  First author gender probability: 0.76
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Yan Yan
  Name of the last author: Nicu Sebe
  Number of Figures: 10
  Number of Tables: 8
  Number of authors: 6
  Paper title: A Multi-Task Learning Framework for Head Pose Estimation under Target
    Motion
  Publication Date: 2015-09-10 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Synthetic Data Generation for Classification
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 DPOSE Dataset: Comparing Head Pose Classification Accuracy
      with Competing MTL Methods'
  Table 3 caption:
    table_text: 'TABLE 3 PARTY Dataset: Head Pose Classification Accuracy'
  Table 4 caption:
    table_text: 'TABLE 4 DPOSE Dataset: HPE Accuracy with Varying Grid Sizes'
  Table 5 caption:
    table_text: 'TABLE 5 DPOSE Dataset: Accuracy with Different Prediction Strategies'
  Table 6 caption:
    table_text: 'TABLE 6 PARTY Dataset: Head Localization versus Classification Accuracy'
  Table 7 caption:
    table_text: 'TABLE 7 DPOSE Dataset: Computation Time Comparison'
  Table 8 caption:
    table_text: TABLE 8 FEGA-MTL Classification Accuracy Obtained with Different Training
      Sets
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2477843
- Affiliation of the first author: ibm research
  Affiliation of the last author: ibm t.j. watson research center
  Figure 1 Link: articels_figures_by_rev_year\2015\MultiGraph_Matching_via_Affinity_Optimization_with_Graduated_Consistency_Regular\figure_1.jpg
  Figure 1 caption: Comparison of CAO, CAO cst , CAO 2nd , CAO-C and CAO-C inv on
    the synthetic random graphs. RRWM [16] is used as the pairwise matcher to generate
    the initial configuration ('RRWM'). 'CAO' denotes CAO dismissing the step of L9-13,
    so for other methods (best viewed in color). Refer to Table 1 for settings.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\MultiGraph_Matching_via_Affinity_Optimization_with_Graduated_Consistency_Regular\figure_2.jpg
  Figure 2 caption: Performance on synthetic random graphs by varying the maximum
    iteration threshold T for T 0 =2 used in the algorithm charts (best viewed in
    color). Refer to Table 2 for settings.
  Figure 3 Link: articels_figures_by_rev_year\2015\MultiGraph_Matching_via_Affinity_Optimization_with_Graduated_Consistency_Regular\figure_3.jpg
  Figure 3 caption: 'In case of few outliers: evaluation for CAO, CAO-C and the two
    variants CAO-UC and CAO-PC, and state of the art on the synthetic random graph
    dataset by varying the disturbance level (top row), and by varying the number
    of considered graphs (bottom row). Refer to Table 1 for settings (best viewed
    in color).'
  Figure 4 Link: articels_figures_by_rev_year\2015\MultiGraph_Matching_via_Affinity_Optimization_with_Graduated_Consistency_Regular\figure_4.jpg
  Figure 4 caption: 'In case of few outliers: evaluation for CAO, CAO-C and the two
    variants CAO-UC and CAO-PC, and state of the art on real images (best viewed in
    color). Refer to Table 5 for settings.'
  Figure 5 Link: articels_figures_by_rev_year\2015\MultiGraph_Matching_via_Affinity_Optimization_with_Graduated_Consistency_Regular\figure_5.jpg
  Figure 5 caption: 'In the presence of more outliers: random point test for our methods
    driven by consistency (solid) and affinity (dashed) inlier eliciting mechanism
    (best viewed in color). Refer to Table 6 for settings.'
  Figure 6 Link: articels_figures_by_rev_year\2015\MultiGraph_Matching_via_Affinity_Optimization_with_Graduated_Consistency_Regular\figure_6.jpg
  Figure 6 caption: 'In the presence of more outliers: real image test for our methods
    driven by consistency (solid) and affinity (dashed) inlier eliciting mechanism
    (best viewed in color). Refer to Table 7 for settings.'
  Figure 7 Link: articels_figures_by_rev_year\2015\MultiGraph_Matching_via_Affinity_Optimization_with_Graduated_Consistency_Regular\figure_7.jpg
  Figure 7 caption: Examples of visual results on public dataset. Top three rows correspond
    to the second row in Fig. 6, and the bottom three rows refer to the second row
    in Fig. 4. Inlier correspondences are drawn by green and red for correct and wrong
    matchings respectively. Outliers are colored in white (best viewed in color).
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.76
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Junchi Yan
  Name of the last author: Stephen M. Chu
  Number of Figures: 7
  Number of Tables: 7
  Number of authors: 5
  Paper title: Multi-Graph Matching via Affinity Optimization with Graduated Consistency
    Regularization
  Publication Date: 2015-09-10 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Settings for Synthetic Test in Fig. 1, Fig. 3
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Settings for Synthetic Test in Fig. 2
  Table 3 caption:
    table_text: TABLE 3 Main Parameters for Experimental Settings
  Table 4 caption:
    table_text: TABLE 4 Complexity Comparison of the State of the Art
  Table 5 caption:
    table_text: TABLE 5 Parameter Settings for Real-Image Test in Fig. 4
  Table 6 caption:
    table_text: TABLE 6 Settings for Random Point Set Test in Fig. 5
  Table 7 caption:
    table_text: TABLE 7 Settings of Willow-ObjectClass Test in Fig. 6
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2477832
- Affiliation of the first author: department of electronic engineering, tsinghua
    university, beijing, china
  Affiliation of the last author: "d\xE9partment d'informatique, universit\xE9 de\
    \ sherbrooke, sherbrooke, qc, canada"
  Figure 1 Link: articels_figures_by_rev_year\2015\A_Deep_and_Autoregressive_Approach_for_Topic_Modeling_of_Multimodal_Data\figure_1.jpg
  Figure 1 caption: "Illustration of a single hidden layer SupDocNADE model for multimodal\
    \ image data. Visual words, annotation words and class label y are modeled as\
    \ p(v,y)=p(y|v) \u220F i p( v i | v 1 ,\u2026, v i\u22121 ) . All conditionals\
    \ p(y|v) and p( v i | v 1 ,\u2026, v i\u22121 ) are modeled using neural networks\
    \ with shared weights. Each predictive word conditional p( v i | v 1 ,\u2026,\
    \ v i\u22121 ) (noted v i for brevity) follows a tree decomposition where each\
    \ leaf is a possible word. At test time, the annotation words are not used (trated\
    \ with a dotted box) to compute the image's topic feature representation."
  Figure 10 Link: articels_figures_by_rev_year\2015\A_Deep_and_Autoregressive_Approach_for_Topic_Modeling_of_Multimodal_Data\figure_10.jpg
  Figure 10 caption: The illustration of multimodal retrieval results for SupDeepDocNADE.
    Both the query input and retrieved results contain image and text modalities.
    The annotations (text modality) are shown under the image. The query input is
    shown in the first column, and the 4 most similar imageannotation pairs according
    to SupDeepDocNADE are shown in the following columns, ranked by similarity from
    left to right.
  Figure 2 Link: articels_figures_by_rev_year\2015\A_Deep_and_Autoregressive_Approach_for_Topic_Modeling_of_Multimodal_Data\figure_2.jpg
  Figure 2 caption: "Illustration of the deep extension of Supervised DocNADE model.\
    \ At the training phase, the input v (visual and annotation words) is first shuffled\
    \ randomly based on an ordering o and then randomly split into two parts, v o\
    \ <d and v o \u2265d . Then we compute each of the conditionals in Equation (21)\
    \ and use backpropagation to optimize the parameters of the model. To deal with\
    \ the imbalance between the visual and annotation words, the histogram of v o\
    \ <d and v o \u2265d is weighted by \u03C9(\u03C1) . At test time, all the words\
    \ in v are fed to the model to compute a discriminative deep representation. Besides\
    \ the visual and annotation words, global features f are also leveraged by the\
    \ model."
  Figure 3 Link: articels_figures_by_rev_year\2015\A_Deep_and_Autoregressive_Approach_for_Topic_Modeling_of_Multimodal_Data\figure_3.jpg
  Figure 3 caption: "Classification performance comparison on LabelMe (even) and UIUC-Sports\
    \ (odd). On the left, we compare the classification performance of SupDocNADE,\
    \ DocNADE and sLDA. On the right, we compare the performance between different\
    \ variants of SupDocNADE. The \u201C \u03BB varies \u201D means the unsupervised\
    \ weight \u03BB in Equation (12) is chosen by cross-validation."
  Figure 4 Link: articels_figures_by_rev_year\2015\A_Deep_and_Autoregressive_Approach_for_Topic_Modeling_of_Multimodal_Data\figure_4.jpg
  Figure 4 caption: Predicted class and annotation by SupDocNADE on LabelMe data set.
    We list some correctly (top row) and incorrectly (bottom row) classified images.
    The predicted (in blue) and ground-truth (in black) class labels and annotation
    words are presented under each image.
  Figure 5 Link: articels_figures_by_rev_year\2015\A_Deep_and_Autoregressive_Approach_for_Topic_Modeling_of_Multimodal_Data\figure_5.jpg
  Figure 5 caption: Visualization of learned representations. Class labels are colored
    in red. For each class, we list four visual words (each represented by 16 image
    patches) and five annotation words that are strongly associated with each class.
    See Section 6.1.4 for more details.
  Figure 6 Link: articels_figures_by_rev_year\2015\A_Deep_and_Autoregressive_Approach_for_Topic_Modeling_of_Multimodal_Data\figure_6.jpg
  Figure 6 caption: Performance of SupDeepDocNADE w.r.t the number of epochs pretrained
    on unlabeled data.
  Figure 7 Link: articels_figures_by_rev_year\2015\A_Deep_and_Autoregressive_Approach_for_Topic_Modeling_of_Multimodal_Data\figure_7.jpg
  Figure 7 caption: Illustration of some failed examples of SupDeepDocNADE. The reasons
    for failure are listed on the left-side of each row. For each reason, we list
    three examples. The text below each image is the confidence of either the wrongly
    predicted class (the top row) or the ground truth class (the middle and bottom
    rows). The maximum value of confidence is 1 and minimum is 0 .
  Figure 8 Link: articels_figures_by_rev_year\2015\A_Deep_and_Autoregressive_Approach_for_Topic_Modeling_of_Multimodal_Data\figure_8.jpg
  Figure 8 caption: Comparison between different annotation weights.
  Figure 9 Link: articels_figures_by_rev_year\2015\A_Deep_and_Autoregressive_Approach_for_Topic_Modeling_of_Multimodal_Data\figure_9.jpg
  Figure 9 caption: The illustration of generated texts from images by SupDeepDocNADE.
    The input for this task is the image modality only and the output is the generated
    text. We put the ground truth annotations in the second column and illustrate
    the top words generated using SupDeepDocNADE in the third column. If there is
    no ground truth annotations, the corresponding part is left blank. We can see
    that SupDeepDocNADE can generate meaningful annotations from images.
  First author gender probability: 0.96
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yin Zheng
  Name of the last author: Hugo Larochelle
  Number of Figures: 10
  Number of Tables: 2
  Number of authors: 3
  Paper title: A Deep and Autoregressive Approach for Topic Modeling of Multimodal
    Data
  Publication Date: 2015-09-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance Comparison of SupDocNADE with Different Models
      on LabelMe and UIUC-Sports Data Sets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance Comparison on MIR Flickr Data Set
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2476802
- Affiliation of the first author: department of electrical and computer engineering,
    mcmaster university, hamilton, ontario, canada
  Affiliation of the last author: department of electrical and computer engineering,
    university of toronto, toronto, ontario, canada
  Figure 1 Link: articels_figures_by_rev_year\2015\Local_Feature_Selection_for_Data_Classification\figure_1.jpg
  Figure 1 caption: "Block diagram of the proposed method where each training sample\
    \ x (i) i=1,\u2026,N is considered to be a representative point of its neighboring\
    \ region and an optimal feature set (possibly different in size and membership)\
    \ is selected for that region. Feature sets of all representative points are used\
    \ for classification of a query datum x q . The detail of the feature selection\
    \ and classification is presented in Sections 3.1 and 3.2, respectively."
  Figure 10 Link: articels_figures_by_rev_year\2015\Local_Feature_Selection_for_Data_Classification\figure_10.jpg
  Figure 10 caption: "Histogram of distances between relaxed solutions and their corresponding\
    \ binary solutions for data set \u201CProstate\u201D where \u03B1 is set to the\
    \ typical value of 5."
  Figure 2 Link: articels_figures_by_rev_year\2015\Local_Feature_Selection_for_Data_Classification\figure_2.jpg
  Figure 2 caption: "The polyhedron P in the case of a 3-D original feature space,\
    \ i.e., the data dimension M is 3, where \u03B1 is set to 2. It is a unit cube\
    \ (defined by 0\u2264 f (i) m \u22641,m=1,\u2026,3 ) in which two regions, i.e.,\
    \ blue and red pyramids, are removed. The blue pyramid is the intersection between\
    \ unit cube and the half space 1 T f (i) <1 , and the red pyramid is the intersection\
    \ between the half space 1 T f (i) >\u03B1 and the unit cube."
  Figure 3 Link: articels_figures_by_rev_year\2015\Local_Feature_Selection_for_Data_Classification\figure_3.jpg
  Figure 3 caption: Illustration of the synthetic data set in terms of its relevant
    features x 1 and x 2 , after feature values are transformed into their z-scores.
  Figure 4 Link: articels_figures_by_rev_year\2015\Local_Feature_Selection_for_Data_Classification\figure_4.jpg
  Figure 4 caption: "Percentage of correct feature selection over four successive\
    \ iterations of the proposed algorithm for the synthetic data set, where the samples\
    \ are contaminated with a varying number of irrelevant features. The parameter\
    \ \u03B1 is set to 2."
  Figure 5 Link: articels_figures_by_rev_year\2015\Local_Feature_Selection_for_Data_Classification\figure_5.jpg
  Figure 5 caption: "Selected features for \u201CDNA\u201D data set. The height corresponding\
    \ to each feature index indicates what percentage of representative points select\
    \ the respective feature as a discriminative feature, where \u03B1 is set to a\
    \ typical value of 5."
  Figure 6 Link: articels_figures_by_rev_year\2015\Local_Feature_Selection_for_Data_Classification\figure_6.jpg
  Figure 6 caption: "Classification error rate of the proposed method for data set\
    \ \u201CSonar\u201D where the parameter \u03B1 ranges from 1 to the maximum possible\
    \ value of M=160."
  Figure 7 Link: articels_figures_by_rev_year\2015\Local_Feature_Selection_for_Data_Classification\figure_7.jpg
  Figure 7 caption: "Averaged cardinality of the optimal feature sets f \u2217 (i)\
    \ i=1,\u2026,N versus the parameter \u03B1 where \u03B1 ranges from 1 to the maximum\
    \ possible value of M=160."
  Figure 8 Link: articels_figures_by_rev_year\2015\Local_Feature_Selection_for_Data_Classification\figure_8.jpg
  Figure 8 caption: "Classification error rate of the proposed method for data set\
    \ \u201CColon\u201D where the parameter \u03B3 ranges from 0 to 1."
  Figure 9 Link: articels_figures_by_rev_year\2015\Local_Feature_Selection_for_Data_Classification\figure_9.jpg
  Figure 9 caption: "Selected features for \u201CALLAML\u201D data set. The height\
    \ of each feature index indicates what percentage of representative points select\
    \ the respective feature as a discriminative feature where \u03B1 is set to the\
    \ typical value of 5."
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Narges Armanfard
  Name of the last author: Majid Komeili
  Number of Figures: 11
  Number of Tables: 2
  Number of authors: 3
  Paper title: Local Feature Selection for Data Classification
  Publication Date: 2015-09-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Minimum Classification Error (in Percent) and Standard Deviation
      (in Percent) of the Different Algorithms
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Characteristics of the Real-World Data Sets Used in the Experiments
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2478471
- Affiliation of the first author: fondazione toscana gabriele monasterio, pisa, italy
  Affiliation of the last author: fondazione toscana gabriele monasterio, pisa, italy
  Figure 1 Link: articels_figures_by_rev_year\2015\Contour_Tracking_with_a_SpatioTemporal_Intensity_Moment\figure_1.jpg
  Figure 1 caption: "The figure shows how vector bt varies when varying the displacement\
    \ \u0394s and the aperture \u03C3 1 if \u03C3 2 =4\u03C0 . The straight line represents\
    \ the equations b t =2\u0394s ."
  Figure 10 Link: articels_figures_by_rev_year\2015\Contour_Tracking_with_a_SpatioTemporal_Intensity_Moment\figure_10.jpg
  Figure 10 caption: From the top left hand corner to the bottom right hand corner,
    the starting contour and the contours which are located by [57] on the 10th, 20th,
    30th, 40th and 50th frames are shown.
  Figure 2 Link: articels_figures_by_rev_year\2015\Contour_Tracking_with_a_SpatioTemporal_Intensity_Moment\figure_2.jpg
  Figure 2 caption: "The figure shows how vector bt varies when varying the displacement\
    \ \u0394s and the aperture \u03C31 if \u03C3 2 =4\u03C0 and d=4 pixels. The plot\
    \ shows that (7) is equal to zero when \u0394s=d . According to this property\
    \ the iterative localization procedure converges to the respective edge point\
    \ of f(x,t + \u0394t) even if the starting point is not an edge point."
  Figure 3 Link: articels_figures_by_rev_year\2015\Contour_Tracking_with_a_SpatioTemporal_Intensity_Moment\figure_3.jpg
  Figure 3 caption: "Let d 1 =4 pixels, the plot shows how the curves do not intersect\
    \ the \u0394s axis at the point \u0394s= d 1 as they do in Fig. 2. Therefore,\
    \ the localization procedure does not converge to a point of the smoothstep discontinuity\
    \ of frame f(x,t + \u0394t)."
  Figure 4 Link: articels_figures_by_rev_year\2015\Contour_Tracking_with_a_SpatioTemporal_Intensity_Moment\figure_4.jpg
  Figure 4 caption: The figure shows how the expected value of vector bt changes in
    the presence of Gaussian noise.
  Figure 5 Link: articels_figures_by_rev_year\2015\Contour_Tracking_with_a_SpatioTemporal_Intensity_Moment\figure_5.jpg
  Figure 5 caption: "The figure shows the plot of (19) when A=10 pixels and \u0394\
    s=4 pixels. The plot shows that the estimated displacement varies a lot when it\
    \ is computed at different points of a rect discontinuity even though the velocity\
    \ w is constant."
  Figure 6 Link: articels_figures_by_rev_year\2015\Contour_Tracking_with_a_SpatioTemporal_Intensity_Moment\figure_6.jpg
  Figure 6 caption: From the top left hand corner to the bottom right hand corner,
    the starting contour and the contours which are located by bt on the 10th, 20th,
    30th, 40th and 50th frames are shown.
  Figure 7 Link: articels_figures_by_rev_year\2015\Contour_Tracking_with_a_SpatioTemporal_Intensity_Moment\figure_7.jpg
  Figure 7 caption: From the top left hand corner to the bottom right hand corner,
    the starting contour and the contours which are located by of on the 10th, 20th,
    30th, 40th and 50th frames are shown.
  Figure 8 Link: articels_figures_by_rev_year\2015\Contour_Tracking_with_a_SpatioTemporal_Intensity_Moment\figure_8.jpg
  Figure 8 caption: From the top left hand corner to the bottom right hand corner,
    the starting contour and the contours which are located by L&K on the 10th, 20th,
    30th, 40th and 50th frames are shown.
  Figure 9 Link: articels_figures_by_rev_year\2015\Contour_Tracking_with_a_SpatioTemporal_Intensity_Moment\figure_9.jpg
  Figure 9 caption: From the top left hand corner to the bottom right hand corner,
    the starting contour and the contours which are located by [54] on the 10th, 20th,
    30th, 40th and 50th frames are shown.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Marcello Demi
  Name of the last author: Marcello Demi
  Number of Figures: 21
  Number of Tables: 6
  Number of authors: 1
  Paper title: Contour Tracking with a Spatio-Temporal Intensity Moment
  Publication Date: 2015-09-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Contour Tracking on a Test Sequence
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Contour Tracking on a Test Sequence
  Table 3 caption:
    table_text: TABLE 3 Contour Tracking on a Test Sequence
  Table 4 caption:
    table_text: TABLE 4 Contour Tracking on a Test Sequence
  Table 5 caption:
    table_text: TABLE 5 Contour Tracking on a Test Sequence
  Table 6 caption:
    table_text: TABLE 6 Contour Tracking on a Test Sequence
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2478438
