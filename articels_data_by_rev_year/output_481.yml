- Affiliation of the first author: "epita research and development laboratory (lrde),\
    \ universit\xE9 paris-est, equipe a3si, esiee paris (fr-93160 noisy-le-grand,\
    \ france), france"
  Affiliation of the last author: "laboratoire d'informatique gaspard-monge, universit\xE9\
    \ paris-est, equipe a3si, esiee paris (fr-93160 noisy-le-grand, france)"
  Figure 1 Link: articels_figures_by_rev_year\2015\Connected_Filtering_on_TreeBased_ShapeSpaces\figure_1.jpg
  Figure 1 caption: Classical connected operators (black path) and the proposed shape-space
    filtering scheme (black+red path).
  Figure 10 Link: articels_figures_by_rev_year\2015\Connected_Filtering_on_TreeBased_ShapeSpaces\figure_10.jpg
  Figure 10 caption: Some blood vessel segmentation results on the STARE database.
    On the right side, black pixels are true positive, white pixels are true negative,
    cyan pixels are false positive and red pixels are false negative.
  Figure 2 Link: articels_figures_by_rev_year\2015\Connected_Filtering_on_TreeBased_ShapeSpaces\figure_2.jpg
  Figure 2 caption: (a) Evolution of the circularity attribute on two branches (containing
    respectively the red and blue shapes) of the tree of shapes [10]; (b to e) Some
    shapes; (f) Result of attribute thresholding; (g) Result of a shaping.
  Figure 3 Link: articels_figures_by_rev_year\2015\Connected_Filtering_on_TreeBased_ShapeSpaces\figure_3.jpg
  Figure 3 caption: 'An example of the workflow of shape-space filtering framework
    with TT being the Min-tree of T . Circles with a capital letter inside: nodes
    of C ; Blue values: first attribute A ; Circles without a letter inside: nodes
    of CC ; Red values: second attribute AA ; Dashed circles: filtered nodes with
    a given threshold value 2.'
  Figure 4 Link: articels_figures_by_rev_year\2015\Connected_Filtering_on_TreeBased_ShapeSpaces\figure_4.jpg
  Figure 4 caption: Comparison between shape-based lower leveling and attribute thresholding,
    with A being the compactness. Images (c, e) are obtained with the subtractive
    rule and images (d, f) with the average [see text].
  Figure 5 Link: articels_figures_by_rev_year\2015\Connected_Filtering_on_TreeBased_ShapeSpaces\figure_5.jpg
  Figure 5 caption: Comparison of shape-based lower leveling to attribute thresholding;
    the attribute function is the average of the gradient magnitude on the shape contour.
  Figure 6 Link: articels_figures_by_rev_year\2015\Connected_Filtering_on_TreeBased_ShapeSpaces\figure_6.jpg
  Figure 6 caption: 'Comparison between extinction-based shapings and attribute thresholding.
    (b-d): Using one shape attribute; (e-f): Using a combination of shape attributes.'
  Figure 7 Link: articels_figures_by_rev_year\2015\Connected_Filtering_on_TreeBased_ShapeSpaces\figure_7.jpg
  Figure 7 caption: Comparison between shaping and attribute thresholding, with as
    attribute the context-based energy estimator [34].
  Figure 8 Link: articels_figures_by_rev_year\2015\Connected_Filtering_on_TreeBased_ShapeSpaces\figure_8.jpg
  Figure 8 caption: Steps for the blood vessel segmentation in retinal image using
    elongation-based upper leveling. Images in (c) and (d) are reversed. In (e), black
    pixels are true positives, white pixels are true negatives, cyan pixels are false
    positives and red pixels are false negatives. tmin is set to 0.05 , and d0 is
    set to 0.09 .
  Figure 9 Link: articels_figures_by_rev_year\2015\Connected_Filtering_on_TreeBased_ShapeSpaces\figure_9.jpg
  Figure 9 caption: Illustration of blood vessel segmentation on the DRIVE database.
    In the segmented results, black pixels are true positives, white pixels are true
    negatives, cyan pixels are false positives and red pixels are false negatives.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Yongchao Xu
  Name of the last author: Laurent Najman
  Number of Figures: 10
  Number of Tables: 3
  Number of authors: 3
  Paper title: Connected Filtering on Tree-Based Shape-Spaces
  Publication Date: 2015-09-22 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Benchmark of Several Blood Vessel Segmentation Approaches
      on DRIVE Database
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE A1 Benchmark of Different Blood Vessel Segmentation Approaches
      on the DRIVE Database
  Table 3 caption:
    table_text: TABLE A2 Benchmark of Different Blood Vessel Segmentation Approaches
      on the STARE Database
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2441070
- Affiliation of the first author: "signal theory and communications, universitat\
    \ polit\xE8cnica de catalunya, barcelonatech, barcelona, spain"
  Affiliation of the last author: "signal theory and communications, universitat polit\xE8\
    cnica de catalunya, barcelonatech, barcelona, spain"
  Figure 1 Link: articels_figures_by_rev_year\2015\Supervised_Evaluation_of_Image_Segmentation_and_Object_Proposal_Techniques\figure_1.jpg
  Figure 1 caption: 'Quantitative meta-measure principles: How good are the evaluation
    measures at ranking the second-row partitions better than the third-row ones?'
  Figure 10 Link: articels_figures_by_rev_year\2015\Supervised_Evaluation_of_Image_Segmentation_and_Object_Proposal_Techniques\figure_10.jpg
  Figure 10 caption: 'State-of-the-art examples. Top row: An image from BSDS500, the
    five ground-truth partitions done by different humans, and the partitions obtained
    by the two baseline techniques. Bottom row: Partitions obtained by the nine representative
    SoA segmentation techniques, each of them at its ODS with respect to F b .'
  Figure 2 Link: articels_figures_by_rev_year\2015\Supervised_Evaluation_of_Image_Segmentation_and_Object_Proposal_Techniques\figure_2.jpg
  Figure 2 caption: 'F measure versus Jaccard index: J and F are functionally related.'
  Figure 3 Link: articels_figures_by_rev_year\2015\Supervised_Evaluation_of_Image_Segmentation_and_Object_Proposal_Techniques\figure_3.jpg
  Figure 3 caption: 'The three interpretations of image partition: Clustering of the
    pixel set (region-based), two-class clustering of the pairs of pixels (pairs-of-pixels-based),
    and detection of true pixel contours (boundary-based).'
  Figure 4 Link: articels_figures_by_rev_year\2015\Supervised_Evaluation_of_Image_Segmentation_and_Object_Proposal_Techniques\figure_4.jpg
  Figure 4 caption: 'Region classification example: Regions are classified into object,
    part candidates, fragmentation candidates, and noise.'
  Figure 5 Link: articels_figures_by_rev_year\2015\Supervised_Evaluation_of_Image_Segmentation_and_Object_Proposal_Techniques\figure_5.jpg
  Figure 5 caption: 'SIHD example: Distribution of F b values for the same-image pairs
    of partitions () and different-image pairs ( ). In gray rectangles, four representative
    pairs of partitions: a pair of correctly classified as different image (up-left)
    and as same image (up-right); and a pair incorrectly classified as different image
    (down-left) and as same image (down-right).'
  Figure 6 Link: articels_figures_by_rev_year\2015\Supervised_Evaluation_of_Image_Segmentation_and_Object_Proposal_Techniques\figure_6.jpg
  Figure 6 caption: 'SABD example: Correct and incorrect judgments by a segmentation
    evaluation measure.'
  Figure 7 Link: articels_figures_by_rev_year\2015\Supervised_Evaluation_of_Image_Segmentation_and_Object_Proposal_Techniques\figure_7.jpg
  Figure 7 caption: 'SISD example: Correct and incorrect judgments by a segmentation
    evaluation measure.'
  Figure 8 Link: articels_figures_by_rev_year\2015\Supervised_Evaluation_of_Image_Segmentation_and_Object_Proposal_Techniques\figure_8.jpg
  Figure 8 caption: 'Object-based J versus F b : Complementary examples where the
    behavior of the two measures differs.'
  Figure 9 Link: articels_figures_by_rev_year\2015\Supervised_Evaluation_of_Image_Segmentation_and_Object_Proposal_Techniques\figure_9.jpg
  Figure 9 caption: 'Object proposal evaluation: Pixel- and boundary-based measures
    ( J and F ~ b ): mean, median and recall.'
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Jordi Pont-Tuset
  Name of the last author: Ferran Marques
  Number of Figures: 13
  Number of Tables: 2
  Number of authors: 2
  Paper title: Supervised Evaluation of Image Segmentation and Object Proposal Techniques
  Publication Date: 2015-09-23 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Measure Structure Overview: Based on the Three Interpretations
      of Image Partition'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Measure Comparison in Terms of Quant. Meta-Meas
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2481406
- Affiliation of the first author: nec laboratories america, at cupertino, ca
  Affiliation of the last author: nec laboratories america, at cupertino, ca
  Figure 1 Link: articels_figures_by_rev_year\2015\The_Information_Available_to_a_Moving_Observer_on_Shape_with_Unknown_Isotropic_B\figure_1.jpg
  Figure 1 caption: Illustration of the imaging setup and notations.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\The_Information_Available_to_a_Moving_Observer_on_Shape_with_Unknown_Isotropic_B\figure_2.jpg
  Figure 2 caption: (a) Constraints on surface estimation in the orthographic case
    may be derived for BRDFs that depend on a reflection angle y in the plane of the
    light source and the camera. (b) BRDF for common material types, such as Lambertian
    (top left), metals (top right), plastics (bottom left) and any isotropic BRDF
    under colocated illumination (bottom right) satisfy this property.
  Figure 3 Link: articels_figures_by_rev_year\2015\The_Information_Available_to_a_Moving_Observer_on_Shape_with_Unknown_Isotropic_B\figure_3.jpg
  Figure 3 caption: (a) One of three simulated orthographic images (two motions),
    with unknown lighting and non-Lambertian BRDF dependent on source and view angles.
    (b) Level curves estimated using Proposition 6. (c) Surface reconstructed after
    interpolation.
  Figure 4 Link: articels_figures_by_rev_year\2015\The_Information_Available_to_a_Moving_Observer_on_Shape_with_Unknown_Isotropic_B\figure_4.jpg
  Figure 4 caption: (a) One of three synthetic images (two motions), with unknown
    half-angle BRDF and known lighting, under orthographic projection. (b) Characteristic
    curves estimated using Proposition 7. (c) Surface reconstructed after interpolation.
  Figure 5 Link: articels_figures_by_rev_year\2015\The_Information_Available_to_a_Moving_Observer_on_Shape_with_Unknown_Isotropic_B\figure_5.jpg
  Figure 5 caption: "(a) One of four synthetic images (three motions), with unknown\
    \ non-Lambertian BRDF and lighting, under perspective projection. (b,c) \u03C0\
    \ 1 and \u03C0 2 recovered using (55), as discussed in Section 6 . (d) Depth estimated\
    \ using Proposition 8. Mean reconstruction error is 1.1 percent."
  Figure 6 Link: articels_figures_by_rev_year\2015\The_Information_Available_to_a_Moving_Observer_on_Shape_with_Unknown_Isotropic_B\figure_6.jpg
  Figure 6 caption: "Trends in the error made by neglecting \u2207 x \u03C1 . The\
    \ units for the x -axis are pixels and those for the y -axis are meters. In each\
    \ plot, we observe that the error in approximation is higher in the surface regions\
    \ where the BRDF sharpness is high or where the surface normal changes more sharply.\
    \ Along each column, we observe that the error in approximation is higher for\
    \ a sharper BRDF."
  Figure 7 Link: articels_figures_by_rev_year\2015\The_Information_Available_to_a_Moving_Observer_on_Shape_with_Unknown_Isotropic_B\figure_7.jpg
  Figure 7 caption: Reconstruction using Proposition 8 for a non-Lambertian ball with
    an unknown BRDF, under unknown light source.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Manmohan Chandraker
  Name of the last author: Manmohan Chandraker
  Number of Figures: 7
  Number of Tables: 1
  Number of authors: 1
  Paper title: The Information Available to a Moving Observer on Shape with Unknown,
    Isotropic BRDFs
  Publication Date: 2015-09-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 A Unified Look at Frameworks on General BRDF Shape Recovery
      from Differential Motions of Light Source, Object or Camera
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2481415
- Affiliation of the first author: department of electrical and computer engineering,
    the ohio state university, columbus, oh
  Affiliation of the last author: department of electrical and computer engineering,
    the ohio state university, columbus, oh
  Figure 1 Link: articels_figures_by_rev_year\2015\Labeled_Graph_Kernel_for_Behavior_Analysis\figure_1.jpg
  Figure 1 caption: 'From left to right: A sign language concept can be described
    as a labeled graph, which specifies handshapes, motions, andor places-of-articulation.
    Each video of a sign language concept is a labeled graph. The labeled graphs corresponding
    to all sample videos form the training set. Our approach is very general and can
    be plugged into any kernel-based classification method. When using Support Vector
    Machines, for instance, we obtained a Labeled Graph Support Vector Machine algorithm
    which is used to discriminate between samples of different concepts in this set
    as shown in the image above. The classification of a test graph is readily given
    by this LGSVM algorithm or the kernel-based classifier of our choosing.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Labeled_Graph_Kernel_for_Behavior_Analysis\figure_2.jpg
  Figure 2 caption: Shown here are the seven possible temporal relationships between
    two events. Here, k specifies the action and i and j specify the event.
  Figure 3 Link: articels_figures_by_rev_year\2015\Labeled_Graph_Kernel_for_Behavior_Analysis\figure_3.jpg
  Figure 3 caption: To determine place-of-articulation of a sign, we discretize the
    image space into twenty regions. The limits of these regions are defined by the
    skeleton points of the signer. For example, as seen in the figure above, the skeleton
    point representing the head of the signer divides the space into left and right
    columns, the average shoulder position separates the top two rows from the bottom
    three as well as the left- and right-most columns, and the bottom-most points
    of the torso separate the bottom row from the top four.
  Figure 4 Link: articels_figures_by_rev_year\2015\Labeled_Graph_Kernel_for_Behavior_Analysis\figure_4.jpg
  Figure 4 caption: The classification accuracies shown above are obtained using the
    same classification algorithm used in previous sections, but the data (features)
    now include additive Gaussian noise. The x -axis specifies the variance of the
    Gaussian used in the noise model. The y -axis shows the classification accuracy.
    We see that realistic levels of noise do not have much of an effect on the derived
    algorithm. Note that in the 6 DMG dataset the noise is added to both modalities
    and, hence, this corresponds to twice the amount of noise tested in other datasets.
  Figure 5 Link: articels_figures_by_rev_year\2015\Labeled_Graph_Kernel_for_Behavior_Analysis\figure_5.jpg
  Figure 5 caption: The classification accuracies shown above are obtained using the
    same classification algorithm used in previous sections, but the data (features)
    now include additive Uniform noise. The x -axis specifies the length of support
    of the Uniform Distribution used in the noise model. The y -axis shows the classification
    accuracy. We see that realistic levels of noise do not have much of an effect
    on the derived algorithm. Note that in the 6 DMG dataset the noise is added to
    both modalities and, hence, this corresponds to twice the amount of noise tested
    in other datasets.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.67
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ruiqi Zhao
  Name of the last author: Aleix M. Martinez
  Number of Figures: 5
  Number of Tables: 9
  Number of authors: 2
  Paper title: Labeled Graph Kernel for Behavior Analysis
  Publication Date: 2015-09-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Classification Accuracies on the Auslan Database for the Proposed
      Approach as Well as Several State-of-the-Art Methods
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Most Discriminant Paths for Three Auslan Signs
  Table 3 caption:
    table_text: TABLE 3 Classification Accuracies on the DGS Dataset for the Proposed
      Approach as Well as Several State-of-the-Art Methods
  Table 4 caption:
    table_text: TABLE 4 Discriminant Paths for a Few Sample Concepts in the DGS Dataset
  Table 5 caption:
    table_text: TABLE 5 Classification Accuracies on the ASLLVD Dataset for the Proposed
      Approach as Well as the Random Walk Kernel
  Table 6 caption:
    table_text: TABLE 6 Discriminant Paths in the ASSLVD Dataset
  Table 7 caption:
    table_text: TABLE 7 Most Discriminant Paths for a Couple of the Actions in the
      UCF Dataset
  Table 8 caption:
    table_text: TABLE 8 Classification Accuracies for the 6 DMG Dataset
  Table 9 caption:
    table_text: TABLE 9 Results on the UCF Human Action Database
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2481404
- Affiliation of the first author: mpi for intelligent systems, spemannstr. 38, tuebingen,
    germany
  Affiliation of the last author: mpi for intelligent systems, spemannstr. 38, tuebingen,
    germany
  Figure 1 Link: articels_figures_by_rev_year\2015\Learning_to_Deblur\figure_1.jpg
  Figure 1 caption: Architecture of our proposed blind deblurring network. First the
    feature extraction module transforms the image to a learned gradient-like representation
    suitable for kernel estimation. Next, the kernel is estimated by division in Fourier
    space, then similarly the latent image. The next stages, each consisting of these
    three operations, operate on both the blurry image and the latent image.
  Figure 10 Link: articels_figures_by_rev_year\2015\Learning_to_Deblur\figure_10.jpg
  Figure 10 caption: Intermediate result of a feature image before (left) and after
    (right) the first tanh-layer. The saturation effect creates sharp edges.
  Figure 2 Link: articels_figures_by_rev_year\2015\Learning_to_Deblur\figure_2.jpg
  Figure 2 caption: 'Intermediary outputs of the first stage (see Fig. 1) of a single-stage
    NN with the following architecture: eight convolution filters; tanh nonlinearity;
    linear recombination to eight images; tanh nonlinearity; linear recombination
    to two sharp and two blurry feature images; kernel estimation.'
  Figure 3 Link: articels_figures_by_rev_year\2015\Learning_to_Deblur\figure_3.jpg
  Figure 3 caption: Deeper networks are better at kernel prediction. One stage for
    kernel prediction consists of a convolutional layer, two hidden layers and a kernel
    estimation module.
  Figure 4 Link: articels_figures_by_rev_year\2015\Learning_to_Deblur\figure_4.jpg
  Figure 4 caption: The performance of the network for kernel estimation depends on
    ?> the architecture. More filters in the convolutional layer and more hidden layers
    are better.
  Figure 5 Link: articels_figures_by_rev_year\2015\Learning_to_Deblur\figure_5.jpg
  Figure 5 caption: The performance of the network depends on the number of the predicted
    gradient-like images used in the kernel estimation module. Predefining y ~ i to
    x- and y-gradients and not learning these representations slows down training.
  Figure 6 Link: articels_figures_by_rev_year\2015\Learning_to_Deblur\figure_6.jpg
  Figure 6 caption: 'Examples of blurs sampled from a Gaussian process (left: 33 px,
    right: 17 px).'
  Figure 7 Link: articels_figures_by_rev_year\2015\Learning_to_Deblur\figure_7.jpg
  Figure 7 caption: "Learned filters of the convolution layer for each of the three\
    \ iterations within a single scale of a trained NN for kernel size 17 \xD7 17\
    \ pixels. See text for details."
  Figure 8 Link: articels_figures_by_rev_year\2015\Learning_to_Deblur\figure_8.jpg
  Figure 8 caption: Visualization of the effect of the first stage of a network with
    two predicted output images on toy example with disks blurred with Gaussians of
    varying size (top row) and motion blurred Lena image (bottom row). While for the
    circles in y i ~ the different sizes of the Gaussian blurs are clearly visible,
    the NN replaces them in x i ~ with shapes of comparable sharpness.
  Figure 9 Link: articels_figures_by_rev_year\2015\Learning_to_Deblur\figure_9.jpg
  Figure 9 caption: 'Distribution of the intensity values of the feature images at
    different steps during their creation (averaged over 200 different images): (1)
    after the convolution layer (2) after the first tanh nonlinearity, where it is
    dominated by extreme values (3) the final blurry and sharp feature images, after
    recombination and a second nonlinearity.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Christian J. Schuler
  Name of the last author: "Bernhard Sch\xF6lkopf"
  Number of Figures: 20
  Number of Tables: 2
  Number of authors: 4
  Paper title: Learning to Deblur
  Publication Date: 2015-09-23 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 The Method Is Very Fast: Runtime in Seconds for Kernel Estimation
      with Varying Image Size on an Intel i5 in Matlab'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 PSNRs of Content Agnostic versus Content Specific Training
      on Images of the Category Valley (Top) and Blackboard (Bottom)
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2481418
- Affiliation of the first author: department of signal processing and acoustics,
    aalto university, espoo, uusimaa, finland
  Affiliation of the last author: "institute of communication acoustics, ruhr-universit\xE4\
    t bochum, bochum, nrw, germany"
  Figure 1 Link: articels_figures_by_rev_year\2015\Uncertain_LDA_Including_Observation_Uncertainties_in_Discriminative_Transforms\figure_1.jpg
  Figure 1 caption: "Assuming there exists an optimal x \u2217 l to represent an acoustic\
    \ event in low dimensional space, there are several factors leading to arrive\
    \ at uncertain description of one observation (indexed by l in this example) represented\
    \ as x \u2013 \u2013 l \u223CN( \u03BC l , \u03A3 l ) ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Uncertain_LDA_Including_Observation_Uncertainties_in_Discriminative_Transforms\figure_2.jpg
  Figure 2 caption: An example of comparing LDA, ULDA and PLDA in terms of finding
    the most discriminant projection. The dotted lines represent discriminant direction
    found by availability of only two samples (with respective uncertainty shown as
    ellipse around each data point) per class. The full lines indicate the projections
    found using 10 times more data from the same classes. The LDA projection can be
    considered optimal when many samples are available for estimating scatter matrices.
    Having many samples per class, the ULDA transform coincides with LDA (blue full
    line). When the training data is scarce, ULDA (dotted green line) provides a better
    resemblance of optimal LDA (blue full line).
  Figure 3 Link: articels_figures_by_rev_year\2015\Uncertain_LDA_Including_Observation_Uncertainties_in_Discriminative_Transforms\figure_3.jpg
  Figure 3 caption: Task grammar defined in the CHiME speech recognition challenge
    [43]. An example sentence would be Lay blue at A five please.
  Figure 4 Link: articels_figures_by_rev_year\2015\Uncertain_LDA_Including_Observation_Uncertainties_in_Discriminative_Transforms\figure_4.jpg
  Figure 4 caption: A block diagram of a typical state-of-the-art automatic speech
    recognition system. Three flows are indicating three different configurations
    for employing uncertainty of observations in ASR. HMM stands for hidden Markov
    model and the abbreviations Conv, MI and UD are used in Table 1 to report word
    recognition error rates for each configuration using LDA or ULDA for dimensionality
    reduction.
  Figure 5 Link: articels_figures_by_rev_year\2015\Uncertain_LDA_Including_Observation_Uncertainties_in_Discriminative_Transforms\figure_5.jpg
  Figure 5 caption: 'Schematic block diagram of a typical state-of-the-art speaker
    verification system [51], [55]. Abbreviations stand for; UBM: universal background
    model, WCCN: within-class covariance normalization and PLDA: probabilistic LDA.'
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Rahim Saeidi
  Name of the last author: Dorothea Kolossa
  Number of Figures: 5
  Number of Tables: 3
  Number of authors: 3
  Paper title: 'Uncertain LDA: Including Observation Uncertainties in Discriminative
    Transforms'
  Publication Date: 2015-09-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Keyword Recognition Error Rate in Percent for a Range of Approaches,
      Computed on the CHiME [46] Test Set
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Number of Speakers, Speech Segments and Trials in the Modified
      I4U File List for Male Speakers in DEV Set
  Table 3 caption:
    table_text: TABLE 3 Speaker Recognition Results for Uncertainty Caused by Duration
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2481420
- Affiliation of the first author: centre for intelligent machines, mcgill university,
    montreal, qc h3a 2a7, canada
  Affiliation of the last author: centre for intelligent machines, mcgill university,
    montreal, qc h3a 2a7, canada
  Figure 1 Link: articels_figures_by_rev_year\2015\Hierarchical_SpatioTemporal_Probabilistic_Graphical_Model_with_Multiple_Feature_\figure_1.jpg
  Figure 1 caption: 'The McGill Real-world Face Video (McGillFaces) Database [1]:
    (a) Sample video frames from some of the subjects in McGillFaces database. (b)
    Cropped facial areas depicting the challenges of the real-world environments present
    in this database.'
  Figure 10 Link: articels_figures_by_rev_year\2015\Hierarchical_SpatioTemporal_Probabilistic_Graphical_Model_with_Multiple_Feature_\figure_10.jpg
  Figure 10 caption: ROC curves of the proposed model for gender and facial hair classification.
  Figure 2 Link: articels_figures_by_rev_year\2015\Hierarchical_SpatioTemporal_Probabilistic_Graphical_Model_with_Multiple_Feature_\figure_2.jpg
  Figure 2 caption: Same subject with two different facial poses. The left image shows
    frontal view and is perceived to be female by humans. The right image depicting
    a small pitch angle change is perceived by humans to be male. Note that the subject
    is a female (image courtesy of [56]).
  Figure 3 Link: articels_figures_by_rev_year\2015\Hierarchical_SpatioTemporal_Probabilistic_Graphical_Model_with_Multiple_Feature_\figure_3.jpg
  Figure 3 caption: 'Illustration of detected local features and corresponding descriptors
    on a tracked face from a sample video frame. Note that only a subset of features
    is shown for better visibility: (a) densely sampled patch-based features (e.g.,
    SIFT descriptors [46]), (b) detected facial edge points and corresponding Geometric
    Blur descriptors [50], (c) detected Boundary Preserving Local Regions and corresponding
    PHOG+gPb descriptors [48].'
  Figure 4 Link: articels_figures_by_rev_year\2015\Hierarchical_SpatioTemporal_Probabilistic_Graphical_Model_with_Multiple_Feature_\figure_4.jpg
  Figure 4 caption: 'Flowchart of the proposed framework: (a) data collection, preprocessing
    and training steps, (b) testing step.'
  Figure 5 Link: articels_figures_by_rev_year\2015\Hierarchical_SpatioTemporal_Probabilistic_Graphical_Model_with_Multiple_Feature_\figure_5.jpg
  Figure 5 caption: An overview of the three levels of the proposed hierarchical graphical
    model. Level 1 models the relationship between different facial features and the
    corresponding facial class estimates. Level 2 infers the single frame facial class
    trait distribution using the facial class estimates inferred from Level 1. Lastly,
    Level 3 leverages the temporal information to estimate the facial class for a
    given video sequence.
  Figure 6 Link: articels_figures_by_rev_year\2015\Hierarchical_SpatioTemporal_Probabilistic_Graphical_Model_with_Multiple_Feature_\figure_6.jpg
  Figure 6 caption: The Level 1 and Level 2 of the proposed hierarchical graphical
    model for a given video frame with a time index t . The patch, edge and region
    based visual words are represented via red nodes. The yellow nodes of Y patch
    , Y edge , and Y region represent trait distributions for the patch, edge and
    region representations, respectively. Y patch , Y edge , and Y region represent
    the patch, edge and region based trait classifier estimates. The pink cone plotted
    in Level 2 represents the fully connected graph containing the unary, pairwise
    and tripled clique potentials. The blue node Y t is the trait distribution at
    the t th frame. The boxes show the potential functions used to model the relationship
    between the corresponding two nodes.
  Figure 7 Link: articels_figures_by_rev_year\2015\Hierarchical_SpatioTemporal_Probabilistic_Graphical_Model_with_Multiple_Feature_\figure_7.jpg
  Figure 7 caption: 'Examples of qualitative results for gender trait classification
    on subjects from the McGillFaces database. (a) Example video frames from successful
    cases. The ground truth gender labels starting from the top row: lbrace male,
    female, female, male rbrace . (b) Example video frames from extreme failure cases.
    The ground truth gender labels starting from the top row: lbrace female, male,
    female, male rbrace . Note that top left corner shows the signs for single frame
    trait decisions from the proposed model, i.e., female (pink) and male (blue).'
  Figure 8 Link: articels_figures_by_rev_year\2015\Hierarchical_SpatioTemporal_Probabilistic_Graphical_Model_with_Multiple_Feature_\figure_8.jpg
  Figure 8 caption: Example tracked and aligned face images from the McGillFaces database,
    and the corresponding quality labels, i.e., occlusion, blurriness, non-frontalness
    ( > 30circ in yaw or pitch) and non-uniform illumination.
  Figure 9 Link: articels_figures_by_rev_year\2015\Hierarchical_SpatioTemporal_Probabilistic_Graphical_Model_with_Multiple_Feature_\figure_9.jpg
  Figure 9 caption: 'Examples of qualitative results for facial hair trait classification
    on subjects from the McGillFaces database. (a) Example video frames from successful
    cases. The ground truth facial hair labels starting from the top row: hair, no-hair,
    hair . (b) Example video frames from extreme failure cases. The ground truth facial
    hair labels starting from the top row: no-hair, hair, no-hair . Note that top
    left corner shows the signs for single frame trait decisions from the proposed
    model, i.e. with facial hair (green) and without facial hair (red).'
  First author gender probability: 0.98
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.74
  Name of the first author: Meltem Demirkus
  Name of the last author: Tal Arbel
  Number of Figures: 12
  Number of Tables: 6
  Number of authors: 4
  Paper title: Hierarchical Spatio-Temporal Probabilistic Graphical Model with Multiple
    Feature Fusion for Binary Facial Attribute Classification in Real-World Face Videos
  Publication Date: 2015-09-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Some of the Leading Publicly Available Real-World Face Databases
      in the Computer Vision Field and Their Comparison
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Performance of the Proposed Graphical Model for Gender
      Classification
  Table 3 caption:
    table_text: TABLE 3 Comparison of the Gender Classification Accuracies from Different
      Approaches
  Table 4 caption:
    table_text: TABLE 4 The Percentages of the TrackedLocated Faces from McGillFaces
      Database, Which Contain Faces with Occlusion, Blur, Non-Frontal Head Pose, Non-Uniformly
      Illumination and Their Combinations. The Single-Frame Based Gender Classification
      Performance of the Proposed Graphical Model for these Low Quality Frames
  Table 5 caption:
    table_text: TABLE 5 The Performance of the Proposed Graphical Model for the Task
      of Facial Hair Classification
  Table 6 caption:
    table_text: TABLE 6 Comparison of the Facial Hair Classification Accuracies from
      Different Approaches
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2481396
- Affiliation of the first author: howard hughes medical institute, salk institute
    for biological studies, 10010 north torrey pines road, la jolla, ca
  Affiliation of the last author: division of biological sciences, university of california
    at san diego, la jolla, ca
  Figure 1 Link: articels_figures_by_rev_year\2015\Correlated_Percolation_Fractal_Structures_and_ScaleInvariant_Distribution_of_Clu\figure_1.jpg
  Figure 1 caption: "Percolation in 2d square lattice of size L = 8. The \u201Coccupied\u201D\
    \ sites are shown in color, sites in the same cluster are colored uniformly and\
    \ different colors denotes clusters of difference sizes. In this example, there\
    \ are three clusters of size 1 ( n 1 =364 ), one cluster of size 2 ( n 2 =164\
    \ ), two clusters of size 3 ( n 3 =264 ), and there is a percolating cluster of\
    \ size 17 ( n 17 =164,M(L=8)=17 ) shown in red. See Fig. 7 for the scaling of\
    \ M(L) and n s in images."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Correlated_Percolation_Fractal_Structures_and_ScaleInvariant_Distribution_of_Clu\figure_2.jpg
  Figure 2 caption: "Samples from the percolation model before and after the phase\
    \ transition. The system size is 1,024\xD71,536 pixels, the same as the van Hateren\
    \ database. The white pixels are empty sites, which are occupied with black pixels\
    \ at random. The occupation probability is given under each sample. The phase\
    \ transition on the square lattice happens at the occupation probability p c =0.59274621\
    \ . In (a) and (b) ( p< p c , disordered) the largest cluster is singled out in\
    \ blue. In (c) and (d) ( p> p c , ordered) the percolating cluster is singled\
    \ out in red."
  Figure 3 Link: articels_figures_by_rev_year\2015\Correlated_Percolation_Fractal_Structures_and_ScaleInvariant_Distribution_of_Clu\figure_3.jpg
  Figure 3 caption: "The binary representation for the \u039B\u2212 bit integer value\
    \ I in the range [0, 2 \u039B \u22121] . B \u03BB \u22080,1 is found iteratively\
    \ by calculating \u230A(I\u2212 \u2211 \u03BB\u22121 l=1 2 \u039B\u2212l B l )\
    \ 2 \u039B\u2212\u03BB \u230B starting from \u03BB=1 , \u230A \u230B is the floor\
    \ function. The map from the analog to binary for the first three bits is shown\
    \ schematically here, where each bit divide the interval in half iteratively starting\
    \ from the most significant bit B 1 ."
  Figure 4 Link: articels_figures_by_rev_year\2015\Correlated_Percolation_Fractal_Structures_and_ScaleInvariant_Distribution_of_Clu\figure_4.jpg
  Figure 4 caption: "Example of an image I from the van Hateren database of natural\
    \ images [6]. Its median thresholded image \u0398 , and the bit planes B 3 , B\
    \ 4 ,\u2026, B 10 are shown."
  Figure 5 Link: articels_figures_by_rev_year\2015\Correlated_Percolation_Fractal_Structures_and_ScaleInvariant_Distribution_of_Clu\figure_5.jpg
  Figure 5 caption: This figure singles out the percolating clusters (shown in red)
    of the bit planes B 5 and B 6 . For bit planes B 7 and B 8 which are not percolating
    the largest cluster is singled out in blue. Notice that in comparison with Fig.
    2 , the percolating clusters are much more structured, which highlights the fact
    that this is a correlated percolation phase transition.
  Figure 6 Link: articels_figures_by_rev_year\2015\Correlated_Percolation_Fractal_Structures_and_ScaleInvariant_Distribution_of_Clu\figure_6.jpg
  Figure 6 caption: "Plot of the average percolation order parameter \u27E8P\u27E9\
    \ for natural images for binary and general base decomposition b=2,3,\u2026,11\
    \ (color-coded in the figure) as a function of \u03BB . The \u03BB in base b is\
    \ transformed to base 2 by the relation \u230A log 2 ( I max )+1\u230B\u2212 log\
    \ 2 ( b \u230A log b ( I max )+1\u230B\u2212\u03BB ) . The standard deviation\
    \ is indicated by the error bars. The black curve is the best fit near the phase\
    \ transition P=\u03B8( \u03BB c \u2212\u03BB) ( \u03BB c \u2212\u03BB ) \u03B2\
    \ C (where \u03B8 is the step function) was obtained for \u03BB c =6.1, \u03B2\
    =0.6, C=1.6 . Percolation order parameter was also calculated for the ensemble\
    \ \u0398 and is given in black at the position \u03BB=5.7 obtained from the average\
    \ median values in the van Hateren database."
  Figure 7 Link: articels_figures_by_rev_year\2015\Correlated_Percolation_Fractal_Structures_and_ScaleInvariant_Distribution_of_Clu\figure_7.jpg
  Figure 7 caption: "(a) Plot of the size of the percolating clusters M(L) as a function\
    \ of L , obtained by taking random samples of size L\xD7L from bit planes \u03BB\
    =4,5,6 , and for the ensemble \u0398 (see Fig. 4). The fractal dimension D : M(L)\u221D\
    \ L D obtained from the linear fit in the log-log scale is given in Table 1. (b)\
    \ Plot of n s (the number of s-clusters per lattice site) as a function of s .\
    \ The exponent \u03C4 : n s \u221D s \u2212\u03C4 obtained from the linear fit\
    \ in the log-log scale is given in Table 1."
  Figure 8 Link: articels_figures_by_rev_year\2015\Correlated_Percolation_Fractal_Structures_and_ScaleInvariant_Distribution_of_Clu\figure_8.jpg
  Figure 8 caption: "B \u2217(n) 6 is the layer B 6 of Fig. 4 after dissolving clusters\
    \ smaller than 2 n into their surroundings. C \u2217(n) 6 are the corresponding\
    \ edges between the clusters. They are demonstrated for n=6 and n=10 ."
  Figure 9 Link: articels_figures_by_rev_year\2015\Correlated_Percolation_Fractal_Structures_and_ScaleInvariant_Distribution_of_Clu\figure_9.jpg
  Figure 9 caption: "Example of an image in the Berkeley segmentation database. The\
    \ top figure highlights in red the boundaries C \u2217(6) 1 of the clusters in\
    \ the first layer B 1 after dissolving clusters of size smaller than 2 6 . The\
    \ bottom figure is the same but for layer two B 2 . This figure highlights the\
    \ limitations of the correspondence between clusters and image segments at the\
    \ present stage."
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Saeed Saremi
  Name of the last author: Terrence J. Sejnowski
  Number of Figures: 9
  Number of Tables: 1
  Number of authors: 2
  Paper title: Correlated Percolation, Fractal Structures, and Scale-Invariant Distribution
    of Clusters in Natural Images
  Publication Date: 2015-09-23 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Exponents D and \u03C4"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2481402
- Affiliation of the first author: department of computer science, tokyo institute
    of technology, japan
  Affiliation of the last author: department of computer science, tokyo institute
    of technology, japan
  Figure 1 Link: articels_figures_by_rev_year\2015\Fast_Coding_of_Feature_Vectors_Using_NeighbortoNeighbor_Search\figure_1.jpg
  Figure 1 caption: "Neighbor-to-neighbor search. NTN search assigns a code to an\
    \ input vector from a neighbor vector to a neighbor vector. A typical example\
    \ of a neighbor vector is a descriptor x j adjacent to a descriptor x j\u2212\
    1 where image descriptors are densely sampled from an image. The red path on the\
    \ image shows the ordering of computation."
  Figure 10 Link: articels_figures_by_rev_year\2015\Fast_Coding_of_Feature_Vectors_Using_NeighbortoNeighbor_Search\figure_10.jpg
  Figure 10 caption: The computational cost reduction by NTN-VQ for different images.
    Eight images are from PASCAL VOC 2007. The reduction rate of the coding cost by
    NTN-VQ and ANN-VQ for each image is reported.
  Figure 2 Link: articels_figures_by_rev_year\2015\Fast_Coding_of_Feature_Vectors_Using_NeighbortoNeighbor_Search\figure_2.jpg
  Figure 2 caption: "Algorithm overview. (a) Initialization step: distance from an\
    \ input vector x 1 to each codeword is calculated. (b) STEP 1: the next input\
    \ vector x 2 which minimizes \u0394 12 is selected from neighbor vectors. (c)\
    \ STEP 2-1: d 2 k \u2217 is calculated where k \u2217 is the code for x 1 . (d)\
    \ STEP 2-2: a lower bound d \u2013 \u2013 21 = d 11 \u2212\u03B4 \u0394 12 is\
    \ calculated where \u03B4 is a parameter, calculation of d 21 is skipped if d\
    \ \u2013 \u2013 21 \u2265 d 2 k \u2217 . (e),(f),(g): STEP 1, 2-1, and 2-2 for\
    \ x j (j>2) , respectively. In (g), accumulated distance \u0394 ij between x i\
    \ and x j is used to obtain a lower bound d \u2013 \u2013 jk = d ik \u2212\u03B4\
    \ \u0394 ij in Eq. (4)."
  Figure 3 Link: articels_figures_by_rev_year\2015\Fast_Coding_of_Feature_Vectors_Using_NeighbortoNeighbor_Search\figure_3.jpg
  Figure 3 caption: Values of Gaussian probability p jk . The indexes of Gaussian
    components on the horizontal axis are sorted by the probability values, e.g.,
    the maximum value of p jk over k is plotted on k =1 . Results for the top 25 of
    512 components are used for illustration. On average, five components have a probability
    value larger than 0.01. This result is obtained on randomly sampled 100 thousand
    descriptors on PASCAL VOC 2007 training images.
  Figure 4 Link: articels_figures_by_rev_year\2015\Fast_Coding_of_Feature_Vectors_Using_NeighbortoNeighbor_Search\figure_4.jpg
  Figure 4 caption: "Distribution of p ik and p jk (i<j) . Calculation of a Gaussian\
    \ probability p jk is skipped for k\u2208 U ik ."
  Figure 5 Link: articels_figures_by_rev_year\2015\Fast_Coding_of_Feature_Vectors_Using_NeighbortoNeighbor_Search\figure_5.jpg
  Figure 5 caption: Cumulative histogram of delta . Statistics of the true delta in
    Eq. (8) on PASCAL VOC 2007 training images is reported for NTN-VQ.
  Figure 6 Link: articels_figures_by_rev_year\2015\Fast_Coding_of_Feature_Vectors_Using_NeighbortoNeighbor_Search\figure_6.jpg
  Figure 6 caption: 'A histogram of descriptors. Red bars: descriptors that have the
    same visual word as a neighbor descriptor. White bars: all descriptors. SIFT descriptors
    are extracted from every 4 pixels at 5 scales on the PASCAL VOC 2007 training
    images. The codebook size is 512. 61.3 percent of two adjacent descriptors have
    the same visual word.'
  Figure 7 Link: articels_figures_by_rev_year\2015\Fast_Coding_of_Feature_Vectors_Using_NeighbortoNeighbor_Search\figure_7.jpg
  Figure 7 caption: 'Speed-accuracy trade-off for different values of delta . Trade-off
    between coding time and Mean AP is reported. All plots are for delta = 1.0, 0.9,
    ldots 0.1, 0.09, ldots, 0.01 . VQ: standard hard vector quantization, NTN-VQ:
    neighbor-to-neighbor search for VQ, GMM: standard Gaussian mixture model, NTN-GMM:
    NTN search for a GMM, NTN-LM-GMM: NTN-GMM with the log-max approximation.'
  Figure 8 Link: articels_figures_by_rev_year\2015\Fast_Coding_of_Feature_Vectors_Using_NeighbortoNeighbor_Search\figure_8.jpg
  Figure 8 caption: 'Comparison with RAND-VQ. Trade-off between coding time and Mean
    AP is reported. NTN-VQ: neighbor-to-neighbor search for VQ, this is the same plot
    as Fig. 7, ANN-VQ: approximate nearest neighbor search [20], RF-VQ: random forests
    [38], [39], RAND-VQ: NTN-VQ in which a neighbor vector is replaced by a randomly
    sampled vector.'
  Figure 9 Link: articels_figures_by_rev_year\2015\Fast_Coding_of_Feature_Vectors_Using_NeighbortoNeighbor_Search\figure_9.jpg
  Figure 9 caption: 'Comparison of the accumulated distance and the direct distance.
    VQ error rate in NTN-VQ for different values of delta is reported. All plots are
    for delta = 1.0, 0.9, ldots 0.1, 0.09, ldots 0.01 . Accumulated distance: Delta
    ij is defined by Eq. (6). Direct distance: Delta ij is replaced by the direct
    distance Vert xi - xjVert . Pre-computed direct distance: the direct distance
    is used but distance calculations for it are not counted.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Nakamasa Inoue
  Name of the last author: Koichi Shinoda
  Number of Figures: 12
  Number of Tables: 9
  Number of authors: 2
  Paper title: Fast Coding of Feature Vectors Using Neighbor-to-Neighbor Search
  Publication Date: 2015-09-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of Notation
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Speed Comparison at the Fixed Accuracy Level
  Table 3 caption:
    table_text: TABLE 3 Speed Comparison Using Different Types of Neighbor Vectors
      on PASCAL VOC 2007
  Table 4 caption:
    table_text: TABLE 4 Accuracy and Speed Comparison Using Different Strides in Dense
      Sampling
  Table 5 caption:
    table_text: TABLE 5 Speed Comparison on TRECVID 2010
  Table 6 caption:
    table_text: TABLE 6 Speed Comparison Using Different Types of Neighbor Vectors
      on TRECVID 2010
  Table 7 caption:
    table_text: TABLE 7 Comparison with Harris-Laplace Detector
  Table 8 caption:
    table_text: TABLE 8 Average Precision by Objects on the PASCAL VOC 2007 Classification
      Challenge
  Table 9 caption:
    table_text: TABLE 9 Average Precision on the TRECVID 2010 Semantic Indexing Task
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2481390
- Affiliation of the first author: university of california, riverside
  Affiliation of the last author: university of california, riverside
  Figure 1 Link: articels_figures_by_rev_year\2015\Distributed_MultiTarget_Tracking_and_Data_Association_in_Vision_Networks\figure_1.jpg
  Figure 1 caption: "In this figure, there are six sensing nodes, C 1 , C 2 ,\u2026\
    , C 6 observing an area (black rectangle) consisting of four targets. The solid\
    \ blue lines show the communication channels between different nodes. This figure\
    \ also depicts the presence of \u201Cna\xEFve\u201D nodes. For example, C 3 ,\
    \ C 5 , C 6 get direct measurements about the black target which it shares with\
    \ its immediate network neighbors. However, C 1 does not have direct access to\
    \ measurements of that target and thus is na\xEFve w.r.t. that target's state."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Distributed_MultiTarget_Tracking_and_Data_Association_in_Vision_Networks\figure_2.jpg
  Figure 2 caption: 'We show the performance comparison for both MTIC and EMTIC algorithm
    in a simulation setup. The parameters evaluated are: (a,b) varying amount of clutter,
    (c,d) varying number of cameras, (e,f) varying number of targets, (g,h) convergence
    over consensus iterations, (i,j) varying proximity of tracks, and (k,l) varying
    degree of the network graph. The top and bottom figures are generated from the
    linear and non-linear model respectively.'
  Figure 3 Link: articels_figures_by_rev_year\2015\Distributed_MultiTarget_Tracking_and_Data_Association_in_Vision_Networks\figure_3.jpg
  Figure 3 caption: Difference in convergence speed at different timesteps for (a)
    linear model and (b) non-linear model.
  Figure 4 Link: articels_figures_by_rev_year\2015\Distributed_MultiTarget_Tracking_and_Data_Association_in_Vision_Networks\figure_4.jpg
  Figure 4 caption: Example scenarios from EPFL dataset ((a)-(d)) and our dataset
    ((e)-(h)) on which the proposed EMTIC algorithm were evaluated by varying the
    number and proximity of targets.
  Figure 5 Link: articels_figures_by_rev_year\2015\Distributed_MultiTarget_Tracking_and_Data_Association_in_Vision_Networks\figure_5.jpg
  Figure 5 caption: Performance comparison between EMTIC and EJPDAF on EPFL (top row)
    and our dataset (bottom row). The subfigures demonstrate the convergence over
    consensus iterations(K) with (a,b) varying number of targets at fixed number of
    cameras and (c,d) varying number of cameras at fixed number targets. (Plots are
    best viewable in color.)
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Ahmed T. Kamal
  Name of the last author: Amit K. Roy-Chowdhury
  Number of Figures: 5
  Number of Tables: 1
  Number of authors: 4
  Paper title: Distributed Multi-Target Tracking and Data Association in Vision Networks
  Publication Date: 2015-10-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of the State Estimation Equations of the KCF, ICF,
      MTIC, EKCF, EICF and EMTIC for One Particular Target and a Single Consensus
      Iteration Step
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2484339
