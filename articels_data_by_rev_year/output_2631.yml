- Affiliation of the first author: lucerne university of applied sciences and arts,
    rotkreuz, switzerland
  Affiliation of the last author: lucerne university of applied sciences and arts,
    rotkreuz, switzerland
  Figure 1 Link: articels_figures_by_rev_year\2022\Latent_Gaussian_Model_Boosting\figure_1.jpg
  Figure 1 caption: Example of locations for training and test data for the spatial
    data. Test and Testext refers to locations of the interpolation and extrapolation
    test data sets, respectively.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Latent_Gaussian_Model_Boosting\figure_2.jpg
  Figure 2 caption: "Comparison of the LaGaBoost and LogitBoost algorithms for grouped\
    \ data with varying number of samples per group and spatial data with varying\
    \ range parameters \u03C1 . The top row shows the relative decrease in test error\
    \ of the LaGaBoost versus the LogitBoost algorithm visualized using violin plots.\
    \ The red rhombi representing means over the simulation runs. The bottom row shows\
    \ the average test error of the two algorithms."
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Fabio Sigrist
  Name of the last author: Fabio Sigrist
  Number of Figures: 2
  Number of Tables: 6
  Number of authors: 1
  Paper title: Latent Gaussian Model Boosting
  Publication Date: 2022-04-19 00:00:00
  Table 1 caption: TABLE 1 Results for the GroupedHigh-Cardinality Categorical Variable
    Data and a Binary Bernoulli Likelihood
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Results for the Spatial Data and a Binary Bernoulli Likelihood
  Table 3 caption: TABLE 3 Results for the GroupedHigh-Cardinality Categorical Variable
    Data and a Poisson Likelihood
  Table 4 caption: TABLE 4 Results for the Spatial Data and a Poisson Likelihood
  Table 5 caption: TABLE 5 Summary of Real-World Data Sets
  Table 6 caption: TABLE 6 Results for the Real-World Data Sets
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3168152
- Affiliation of the first author: faculty of engineering, information and systems,
    university of tsukuba, tsukuba, japan
  Affiliation of the last author: school of electrical engineering and computer science,
    kth royal institute of technology, stockholm, sweden
  Figure 1 Link: articels_figures_by_rev_year\2022\Discriminant_Feature_Extraction_by_Generalized_Difference_Subspace\figure_1.jpg
  Figure 1 caption: 'Basis concept of difference subspace: (a) difference vector,
    (b) canonical angles, vectors and difference subspace.'
  Figure 10 Link: articels_figures_by_rev_year\2022\Discriminant_Feature_Extraction_by_Generalized_Difference_Subspace\figure_10.jpg
  Figure 10 caption: Comparison of gFDA and GDS projection in terms of the distribution
    of eigenvalues and our Fisher-like criterion.
  Figure 2 Link: articels_figures_by_rev_year\2022\Discriminant_Feature_Extraction_by_Generalized_Difference_Subspace\figure_2.jpg
  Figure 2 caption: Direct sum decomposition of sum subspace S 2 into principal component
    subspace M 2 and difference subspace D 2 .
  Figure 3 Link: articels_figures_by_rev_year\2022\Discriminant_Feature_Extraction_by_Generalized_Difference_Subspace\figure_3.jpg
  Figure 3 caption: Conceptual diagram of generalized difference subspace for c subspaces.
  Figure 4 Link: articels_figures_by_rev_year\2022\Discriminant_Feature_Extraction_by_Generalized_Difference_Subspace\figure_4.jpg
  Figure 4 caption: The simplification process from FDA toward gFDA. the simplification
    process from Eq. (5) to Eq. (21) is summarized.
  Figure 5 Link: articels_figures_by_rev_year\2022\Discriminant_Feature_Extraction_by_Generalized_Difference_Subspace\figure_5.jpg
  Figure 5 caption: "Discrimination mechanism of gFDA, which consists of two processes:\
    \ 1) whitening: all the orthogonal basis vectors \u03D5 c i of three class subspaces\
    \ are orthogonalized to each other in the normalized space N , 2) PCA: only three\
    \ first basis vectors \u03D5 c 1 3 c=1 of each class are extracted and the difference\
    \ vectors Z ij 1 between them are calculated. Then, the orthogonal basis vectors,\
    \ d 1 and d 2 , of the discriminant H are obtained by applying PCA to a set of\
    \ the three difference vectors. Finally, all the data are projected onto the discriminant\
    \ space H ."
  Figure 6 Link: articels_figures_by_rev_year\2022\Discriminant_Feature_Extraction_by_Generalized_Difference_Subspace\figure_6.jpg
  Figure 6 caption: 'Effectiveness of normalization: (a) and (b) show the projections
    of data of 3 classes in the normalized spaces and discriminant space, respectively,
    where all data of each class are completely contained within its class subspace.
    (c) and (d) show the projections of data of the 3classes, where some component
    of data are not covered by the class subspaces.'
  Figure 7 Link: articels_figures_by_rev_year\2022\Discriminant_Feature_Extraction_by_Generalized_Difference_Subspace\figure_7.jpg
  Figure 7 caption: The projections of face classes from the Yale face database by
    FDA (left) and gFDA (right), where a row represents the case of 2, 3 or 4 classes.
  Figure 8 Link: articels_figures_by_rev_year\2022\Discriminant_Feature_Extraction_by_Generalized_Difference_Subspace\figure_8.jpg
  Figure 8 caption: Connection between gFDA and GDS projection.
  Figure 9 Link: articels_figures_by_rev_year\2022\Discriminant_Feature_Extraction_by_Generalized_Difference_Subspace\figure_9.jpg
  Figure 9 caption: Visualization of the projections by gFDA and GDS projection in
    the cases of two, three and four classes from the Yale face database.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Kazuhiro Fukui
  Name of the last author: Atsuto Maki
  Number of Figures: 15
  Number of Tables: 8
  Number of authors: 5
  Paper title: Discriminant Feature Extraction by Generalized Difference Subspace
  Publication Date: 2022-04-19 00:00:00
  Table 1 caption: TABLE 1 Comparison of the Three Discriminant Spaces by FDA, sFDA
    and gFDA
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Mean Performances (%) of Different Methods in Terms of
    Recognition Rate and Equal Error Rate (%)
  Table 3 caption: TABLE 3 Mean Performances (%) of Different Methods on CMU Dataset
  Table 4 caption: TABLE 4 Performances of Different Methods on ALOI300, Which are
    Ranked in Descending Order of the Mean Recognition Rates
  Table 5 caption: TABLE 5 Performances of Different Methods When Using Image Sets
    of Five Objects for Learning in Terms of Recognition Rate and EER (%)
  Table 6 caption: TABLE 6 Performance Comparison of FDA Based Methods With ResNet18
    on MNIST
  Table 7 caption: TABLE 7 Performances of Our Kernel Methods With Data Compression
    and Typical DNNs Without Data Augmentation on MNIST
  Table 8 caption: TABLE 8 Performances of Different Methods on CIFAR10
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3168557
- Affiliation of the first author: department of computer science, hong kong baptist
    university, hong kong sar, china
  Affiliation of the last author: school of electrical and electronic engineering,
    nanyang technological university, singapore
  Figure 1 Link: articels_figures_by_rev_year\2022\Benchmarking_SingleImage_Reflection_Removal_Algorithms\figure_1.jpg
  Figure 1 caption: The physical (left), mathematical (middle) image formation models,
    and exemplar images (right) for three kinds of reflections. The physical models
    for the in-focus reflection and out-of-focus reflection ignore the refractive
    effect of glass. For simplicity, we omit the lens and use only one ray for the
    physical model of the ghosting effect. I , B , R , and kernel k are defined in
    Section 2.2. The red boxes in exemplar images denote the regions with in-focus
    reflection, out-of-focus reflection, and ghosting effect, respectively.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Benchmarking_SingleImage_Reflection_Removal_Algorithms\figure_2.jpg
  Figure 2 caption: Three examples with low-transmitted reflections (labelled by red
    boxes) caused by the regional property. The third one is captured through tinted
    glass.
  Figure 3 Link: articels_figures_by_rev_year\2022\Benchmarking_SingleImage_Reflection_Removal_Algorithms\figure_3.jpg
  Figure 3 caption: Examples from the outdoor dataset with four types of reflections.
    We show the results obtained by NR17 [32], YW19 [35], CoRRN [13], ZN18 [15], ERRNet
    [14], and IBCLN [19]. We adjust the contrast of the result obtained by IBCLN [19]
    in low-transmitted reflection (fifth row) to show the dark-hole effect lablled
    by blue box in this case.
  Figure 4 Link: articels_figures_by_rev_year\2022\Benchmarking_SingleImage_Reflection_Removal_Algorithms\figure_4.jpg
  Figure 4 caption: Examples from the indoor dataset with regular settings. We show
    the results obtained by NR17 [32], YW19 [35], CoRRN [13], ZN18 [15], ERRNet [14],
    and IBCLN [19].
  Figure 5 Link: articels_figures_by_rev_year\2022\Benchmarking_SingleImage_Reflection_Removal_Algorithms\figure_5.jpg
  Figure 5 caption: Examples with four types of reflections from the in-the-wild dataset.
    We show the results obtained by NR17 [32], YW19 [35], CoRRN [13], ZN18 [15], ERRNet
    [14], and IBCLN [19]. The last example is with the low-transmitted reflection
    and color distortion.
  Figure 6 Link: articels_figures_by_rev_year\2022\Benchmarking_SingleImage_Reflection_Removal_Algorithms\figure_6.jpg
  Figure 6 caption: "Examples from the outdoor dataset and the in-the-wild dataset,\
    \ respectively. We show the results obtained by NR17 [32], YW19 [35], CoRRN [13],\
    \ ZN18 [15], and ERRNet [14]. The red boxes in the first part indicate the regions\
    \ with low-transmitted reflections. The yellow boxes in the second part indicate\
    \ the regions with out-of-focus reflections. \u201CM strategy t\u201D denotes\
    \ the mixture strategy with tinted images. \u201CM strategy wo t\u201D denotes\
    \ the mixture strategy without tinted images."
  Figure 7 Link: articels_figures_by_rev_year\2022\Benchmarking_SingleImage_Reflection_Removal_Algorithms\figure_7.jpg
  Figure 7 caption: One example captured through four different obscure glass. The
    first row denotes the image before processing and the second row contains the
    images after processing.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.93
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: Renjie Wan
  Name of the last author: Alex C. Kot
  Number of Figures: 7
  Number of Tables: 10
  Number of authors: 6
  Paper title: Benchmarking Single-Image Reflection Removal Algorithms
  Publication Date: 2022-04-19 00:00:00
  Table 1 caption: TABLE 1 Summary of Non-Learning-Based Single-Image Reflection Removal
    Methods
  Table 10 caption: TABLE 10 The Results Obtained by CoRRN [13], ZN18 [15], ERRNet
    [14], and IBCLN [19] Based on by the Synthetic Strategy, Mixed Strategy With Images
    Captured Through Tinted Glass (Mixed Strategy With t-image), Mixed Strategy Without
    Images Captured Through Tinted Glass (Mixed Strategy wo t-image), and Learning-Based
    Strategy
  Table 2 caption: TABLE 2 Summary of Deep-Learning-Based Reflection Removal Methods
  Table 3 caption: TABLE 3 Summary of Motion-Based Reflection Removal Methods
  Table 4 caption: TABLE 4 Details of the Proposed Dataset
  Table 5 caption: TABLE 5 Summary for Existing Reflection Removal Datasets
  Table 6 caption: TABLE 6 The Average Error Metric Values on the Outdoor and in-the-Wild
    Dataset
  Table 7 caption: TABLE 7 Benchmark Results Using Indoor Dataset With F-Variance
    and T-Variance
  Table 8 caption: TABLE 8 The Overall Evaluations for the Categorized Outdoor Dataset
  Table 9 caption: TABLE 9 The Overall Evaluations for the Categorized in-the-Wild
    Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3168560
- Affiliation of the first author: tsinghua university, beijing, china
  Affiliation of the last author: tsinghua university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\DeepCloth_Neural_Garment_Representation_for_Shape_and_Style_Editing\figure_1.jpg
  Figure 1 caption: Our neural garment representation framework, DeepCloth, enables
    garment reconstruction (b), shape editing (c) (from close to open, from short
    to long, from loose to tight, etc.) and animation (d,e) (with garment-specific
    dynamics even after significant topological editing of the garment) given a 3D
    scan with arbitrary pose (a).
  Figure 10 Link: articels_figures_by_rev_year\2022\DeepCloth_Neural_Garment_Representation_for_Shape_and_Style_Editing\figure_10.jpg
  Figure 10 caption: 'The ablation study of garment editing in PCA space and the original
    latent space. Top: editing parameters in PCA space; bottom: editing parameters
    in original latent space. Different garment shape styles can be edited easily
    by shifting the PCA vectors. In contrast, directly editing the original latent
    space vectors can hardly yield meaningful garment shape editing results.'
  Figure 2 Link: articels_figures_by_rev_year\2022\DeepCloth_Neural_Garment_Representation_for_Shape_and_Style_Editing\figure_2.jpg
  Figure 2 caption: 'The demonstration of our DeepCloth framework. From left to right:
    garment representation, garment reconstruction and garment animation module.'
  Figure 3 Link: articels_figures_by_rev_year\2022\DeepCloth_Neural_Garment_Representation_for_Shape_and_Style_Editing\figure_3.jpg
  Figure 3 caption: The demonstration on coupling the UV-map representation of the
    garment. (a) 2D representation for the T-posed garment for Section 4. (b) 2D representation
    for the randomly posed garment for Section 7.
  Figure 4 Link: articels_figures_by_rev_year\2022\DeepCloth_Neural_Garment_Representation_for_Shape_and_Style_Editing\figure_4.jpg
  Figure 4 caption: The basic structure of our proposed ParamNet, which encodes our
    UV-based garment representation into garment latent space.
  Figure 5 Link: articels_figures_by_rev_year\2022\DeepCloth_Neural_Garment_Representation_for_Shape_and_Style_Editing\figure_5.jpg
  Figure 5 caption: 'The demonstration of garment shape transition under UV-mask representations:
    (a) garment shape transition with the proposed distance-transformed masks, (b)
    garment shape transition with binary masks. Note that without the distance-transform
    operation, there will be an unnatural transition between different garment shapes.'
  Figure 6 Link: articels_figures_by_rev_year\2022\DeepCloth_Neural_Garment_Representation_for_Shape_and_Style_Editing\figure_6.jpg
  Figure 6 caption: 'The demonstration of different garment shape inference methods.
    From left to right: (a) input 3D scan, (b) segmented garment, (c) garment animation
    result with garment shape inferred by 3DInferNet, (d) garment animation result
    with garment shape directly obtained from the scan.'
  Figure 7 Link: articels_figures_by_rev_year\2022\DeepCloth_Neural_Garment_Representation_for_Shape_and_Style_Editing\figure_7.jpg
  Figure 7 caption: The basic structure of our 3DInferNet, which infers the garment
    shape parameters from the input 3D scans.
  Figure 8 Link: articels_figures_by_rev_year\2022\DeepCloth_Neural_Garment_Representation_for_Shape_and_Style_Editing\figure_8.jpg
  Figure 8 caption: The basic structure of our AnimNet, which generates garment dynamics
    under arbitrary human poses and shapes.
  Figure 9 Link: articels_figures_by_rev_year\2022\DeepCloth_Neural_Garment_Representation_for_Shape_and_Style_Editing\figure_9.jpg
  Figure 9 caption: 'The demonstration of different styles garment animation. From
    left to right: front-opening T-shirt animation, front-closing T-shirt animation,
    and skinning results with front-opening T-shirt. The results clarify that AnimNet
    learns garment dynamics from the encoded garment styles.'
  First author gender probability: 0.79
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.89
  Name of the first author: Zhaoqi Su
  Name of the last author: Yebin Liu
  Number of Figures: 23
  Number of Tables: 2
  Number of authors: 4
  Paper title: 'DeepCloth: Neural Garment Representation for Shape and Style Editing'
  Publication Date: 2022-04-19 00:00:00
  Table 1 caption: TABLE 1 Mean Vertex-to-Vertex Error (mm) of Our AnimNet Method
    and PointNet-Based Method for Different Garment Types, With Quantitative Evaluations
    on SMPL-UV Dresses
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Mean Vertex-to-Vertex Error (mm) of Our AnimNet Method
    and TailorNet [8] Method on the TailorNet Dataset for Different Garment Types
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3168569
- Affiliation of the first author: department of electrical engineering, national
    tsing hua university, hsinchu, taiwan
  Affiliation of the last author: department of electrical engineering, national tsing
    hua university, hsinchu, taiwan
  Figure 1 Link: articels_figures_by_rev_year\2022\Monocular_QuasiDense_D_Object_Tracking\figure_1.jpg
  Figure 1 caption: Monocular quasi-dense detection and tracking in 3D. Our dynamic
    3D tracking pipeline predicts 3D bounding box association of observed target from
    quasi-dense object proposals in image sequences captured by a monocular camera
    with an ego-motion sensor.
  Figure 10 Link: articels_figures_by_rev_year\2022\Monocular_QuasiDense_D_Object_Tracking\figure_10.jpg
  Figure 10 caption: Qualitative results on testing set of KITTI datasets. Our VeloLSTM
    continues to predict the 3D object state of unmatched tracklets after they disappear.
    With motion-based data association, our tracker successfully recovers tracked
    vehicle (cyan-colored box) from object occlusions after the vehicle re-appeared.
    We show predicted 3D bounding boxes and trajectories colored with tracking IDs.
  Figure 2 Link: articels_figures_by_rev_year\2022\Monocular_QuasiDense_D_Object_Tracking\figure_2.jpg
  Figure 2 caption: Overview of our monocular quasi-dense 3D tracking framework. Our
    online approach processes monocular frames to estimate and track regions of interest
    (RoIs) in 3D (a). For each RoI, we learn the 3D layout (i.e., depth, orientation,
    dimensions, a projection of 3D center) estimation and instance-level feature embedding
    (b). With the 3D layout, our VeloLSTM helps to predict object states, and our
    3D tracker produces robust linking across frames leveraging 3D location and motion-aware
    association (c). VeloLSTM further refines the 3D estimation by fusing object motion
    features of the previous frames (d).
  Figure 3 Link: articels_figures_by_rev_year\2022\Monocular_QuasiDense_D_Object_Tracking\figure_3.jpg
  Figure 3 caption: The illustration of the quasi-dense similarity learning. We leverage
    quasi-dense object proposals (all circles), instead of traditional sparse ground
    truth (solid circles), to train a discriminative feature space by comparing the
    region proposal pairs between the key frame and the reference frame. The quasi-dense
    instance similarity loss pulls the feature embedding of different object away
    from its paired target proposal and draws the embedding of same object pairs together
    in a high dimensional space.
  Figure 4 Link: articels_figures_by_rev_year\2022\Monocular_QuasiDense_D_Object_Tracking\figure_4.jpg
  Figure 4 caption: 3D projected centers are preferred over 2D bounding boxes. A visible
    2D center projection point may wrongly locate the object away from the ground
    plane in the 3D coordinates and would inevitably suffer from more severe scenarios,
    occluded or truncated. The center shift causes object misalignment for GT and
    predicted tracks and harms 3D IoU AP performance.
  Figure 5 Link: articels_figures_by_rev_year\2022\Monocular_QuasiDense_D_Object_Tracking\figure_5.jpg
  Figure 5 caption: Illustration of 3D location-aware data association. Given the
    tracklets and detections, we project them onto world coordinates by their object
    state. For each detection of interest (DoI), we calculate similarities of the
    object state between DoI and each tracklet. The location of the object in the
    world coordinates naturally provides higher probabilities to tracklets near the
    DoI.
  Figure 6 Link: articels_figures_by_rev_year\2022\Monocular_QuasiDense_D_Object_Tracking\figure_6.jpg
  Figure 6 caption: Demonstration of motion-aware data association. The green tracklet
    is visible all the time, while the yellow tracklet is occluded by the blue tracklet
    at frame T-1 . Given pseudo motion vectors (dotted arrows from a tracklet to all
    detection candidates) and the temporal-aggregated motion vector (dashed arrows)
    by the tracklet, our motion-based data association scheme is encouraged to match
    reappeared candidates in trajectories.
  Figure 7 Link: articels_figures_by_rev_year\2022\Monocular_QuasiDense_D_Object_Tracking\figure_7.jpg
  Figure 7 caption: Workflow of 3D object state estimation, data association and VeloLSTM.
    We detailed the data flow between 3D estimation and data association steps. P-LSTM
    predicts the tracklet state from the velocity history in parallel to 3D detection.
    After the data association, U-LSTM refines matched tracklets by balancing observation
    and prediction tracklet state.
  Figure 8 Link: articels_figures_by_rev_year\2022\Monocular_QuasiDense_D_Object_Tracking\figure_8.jpg
  Figure 8 caption: Statistical summary of our dataset in comparison of KITTI [3],
    VKITTI [32], VIPER [66], and Cityscapes [58].
  Figure 9 Link: articels_figures_by_rev_year\2022\Monocular_QuasiDense_D_Object_Tracking\figure_9.jpg
  Figure 9 caption: Qualitative results on testing set of nuScenes (First row), Waymo
    Open (Second row) datasets. Our proposed quasi-dense 3D tracking pipeline estimates
    accurate 3D extent and robustly associates tracking trajectories from a monocular
    image. We show predicted 3D bounding boxes and trajectories colored with tracking
    IDs.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.58
  Name of the first author: Hou-Ning Hu
  Name of the last author: Min Sun
  Number of Figures: 10
  Number of Tables: 11
  Number of authors: 6
  Paper title: Monocular Quasi-Dense 3D Object Tracking
  Publication Date: 2022-04-19 00:00:00
  Table 1 caption: TABLE 1 Ablation Study of Tracking Performance With Different Methods
    on nuScenes Validation Set
  Table 10 caption: TABLE 10 Performance of 3D Estimation on Object Detection IoU
    mAP
  Table 2 caption: TABLE 2 Tracking Performance Comparison With Different Designs
    of Affinity Matrix on nuScenes Validation Set
  Table 3 caption: TABLE 3 Ablation for QD-3DT versus Mono3DT on NuScenes
  Table 4 caption: TABLE 4 Importance of Using Projection of 3D Bounding Box Center
    Estimation on KITTI Validation Subset
  Table 5 caption: TABLE 5 Comparison of Location Refinement With Different Motion
    Model on nuScenes Validation Set
  Table 6 caption: TABLE 6 Tracking Performance on the Testing Set of nuScenes Tracking
    Benchmark [61]
  Table 7 caption: TABLE 7 3D Detection and Tracking Performance in Vehicle Level
    2 Difficulty on the Testing Set of Waymo Dataset
  Table 8 caption: TABLE 8 3D Detection and Tracking Performance in Vehicle Level
    2 Difficulty on the Validation Set of Waymo Dataset
  Table 9 caption: TABLE 9 Centroid-Based Evaluation Across Datasets
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3168781
- Affiliation of the first author: school of computing, national university of singapore,
    singapore
  Affiliation of the last author: department of computer science and engineering,
    university of minnesota, minneapolis, mn, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\Emotional_Attention_From_Eye_Tracking_to_Computational_Modeling\figure_1.jpg
  Figure 1 caption: Example images from EMOtional attention dataset (EMOd), along
    with emotions that observers indicated as strongly elicited by the images and
    colormaps visualizing human attention.
  Figure 10 Link: articels_figures_by_rev_year\2022\Emotional_Attention_From_Eye_Tracking_to_Computational_Modeling\figure_10.jpg
  Figure 10 caption: Emotional objects are predicted as being more salient than neutral
    objects by CASNet II, which is consistent with the emotion prioritization effect
    of human observers.
  Figure 2 Link: articels_figures_by_rev_year\2022\Emotional_Attention_From_Eye_Tracking_to_Computational_Modeling\figure_2.jpg
  Figure 2 caption: User interface of (a) EMOd object-labeling platform, and (b) EMOd
    image-annotation platform.
  Figure 3 Link: articels_figures_by_rev_year\2022\Emotional_Attention_From_Eye_Tracking_to_Computational_Modeling\figure_3.jpg
  Figure 3 caption: Emotion-eliciting objects receive more (a) and earlier (b) human
    attention than neutral objects. In all figures in this paper, error bars represent
    standard error. Images in (c) and (d) illustrate how emotion-eliciting objects
    (outlined in blue), such as the crying face and broken card, are more salient
    and draw attention earlier than neutralless emotional stimuli (outlined in gray).
    In each pair of numbers, the first number is the attention score and the second
    number indicates attention shift rank.
  Figure 4 Link: articels_figures_by_rev_year\2022\Emotional_Attention_From_Eye_Tracking_to_Computational_Modeling\figure_4.jpg
  Figure 4 caption: (a) Human observers fixated first on emotion-eliciting objects
    more than neutral objects, but the attention prioritization quickly diminishes.
    (b) Viewers fixated on the emotion-eliciting objects (i.e., food (1) and crocodiles
    mouth (2)) before the neutral human body (3).
  Figure 5 Link: articels_figures_by_rev_year\2022\Emotional_Attention_From_Eye_Tracking_to_Computational_Modeling\figure_5.jpg
  Figure 5 caption: "(a) Emotion prioritization is stronger for human-related objects:\
    \ those being touched, gazed upon, or with motion or sound. (b-c) Examples of\
    \ gazed-upon objects and their respective attention scores. The emotion-eliciting\
    \ gazed-upon object\u2014the injection point on the crying childs arm (b) has\
    \ a higher attention score than the neutral gazed-upon object\u2014the box of\
    \ dye in the ladys hand (c)."
  Figure 6 Link: articels_figures_by_rev_year\2022\Emotional_Attention_From_Eye_Tracking_to_Computational_Modeling\figure_6.jpg
  Figure 6 caption: "Correlation of number of fixations with attributes \u201Caesthetics,\u201D\
    \ \u201Cawe,\u201D \u201Cinformativeness,\u201D and \u201Chaving focused objects\u201D\
    \ in scenes of vehicle (top) and indoors (bottom). In the left graph of each set,\
    \ the x -axis stands for the ratings of respective attribute, the red line is\
    \ the linear regression line of image points, and each dot represents one image."
  Figure 7 Link: articels_figures_by_rev_year\2022\Emotional_Attention_From_Eye_Tracking_to_Computational_Modeling\figure_7.jpg
  Figure 7 caption: The architecture of the proposed DNN (CASNet II). An Atrous Spatial
    Pyramid Pooling (ASPP) structure with four branches (inside the gray dashed rectangle)
    is used to capture the contextual information for each pixel at multiple resolution
    scales. A channel weighting subnetwork (inside the dashed orange rectangle) computes
    a set of 1024-dimensional feature weights for each image (instead of only one
    for the whole image) to capture the relative importance of the semantic features
    of a particular image. The gray dashed arrows illustrate how the relative saliency
    of different regions within an image are modified through the subnetwork.
  Figure 8 Link: articels_figures_by_rev_year\2022\Emotional_Attention_From_Eye_Tracking_to_Computational_Modeling\figure_8.jpg
  Figure 8 caption: Qualitative results generated by our saliency model in comparison
    with state-of-the-art methods. Our model (CASNet II) outperforms other models
    in both location and order, by taking into consideration contextual information
    (e.g., encoding relative importance of occurring faces in the first two rows,
    objects in the third and fourth row, and highlighting areas of interest in scene
    images in the last three rows). Due to space limit, we only show examples from
    nine DNN-based models, which are top performers on EMOd dataset.
  Figure 9 Link: articels_figures_by_rev_year\2022\Emotional_Attention_From_Eye_Tracking_to_Computational_Modeling\figure_9.jpg
  Figure 9 caption: 'Examples of how our models gradually improve the relative saliency
    among different objects in a scene, to closely match human emotion prioritization.
    The last two columns visualize the difference between predictions from CASNet
    II and CASNet I (difference image 1), and CASNet I and N-CASNet (difference image
    2): colors close to orangered indicate increased saliency after applying the subnetwork
    for contextual saliency, whereas colors close to bluegreen indicate decreased
    saliency.'
  First author gender probability: 0.9
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Shaojing Fan
  Name of the last author: Qi Zhao
  Number of Figures: 12
  Number of Tables: 9
  Number of authors: 6
  Paper title: 'Emotional Attention: From Eye Tracking to Computational Modeling'
  Publication Date: 2022-04-21 00:00:00
  Table 1 caption: TABLE 1 Descriptions of Semantic Attributes of Objects Labeled
    in EMOd Dataset
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 List of 33 Scene-Level Attributes in the EMOd Dataset
  Table 3 caption: TABLE 3 Results on the EMOd Dataset
  Table 4 caption: TABLE 4 Results on the NUSEF Dataset
  Table 5 caption: TABLE 5 Results on the CAT2000 Dataset
  Table 6 caption: TABLE 6 Results on the MIT1003 Dataset
  Table 7 caption: TABLE 7 Results on the OSIE Dataset
  Table 8 caption: TABLE 8 Normalized Means of all Z-Scored Metrics (AUC-Judd, AUC-Borji,
    sAUC, NSS, IG, CC, SIM, KL)
  Table 9 caption: TABLE 9 Normalized Means of all Z-Scored Metrics (AUC-Judd, AUC-Borji,
    sAUC, NSS, IG, CC, SIM, KL) of Four Model Versions on Five Benchmark Datasets
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3169234
- Affiliation of the first author: center for artificial intelligence and natural
    sciences, korea institute for advanced study (kias), seoul, south korea
  Affiliation of the last author: department of industrial engineering, seoul national
    university, seoul, south korea
  Figure 1 Link: articels_figures_by_rev_year\2022\GradDiv_Adversarial_Robustness_of_Randomized_Neural_Networks_via_Gradient_Divers\figure_1.jpg
  Figure 1 caption: "Illustration of the proposed Gradient Diversity (GradDiv) regularization.\
    \ GradDiv encourages a randomized neural network to have dispersed loss gradients.\
    \ Given a fixed input X , we plot the loss gradients with respect to the input\
    \ for sample models from the randomized neural network. Each point on the sphere\
    \ represents the loss gradient direction \u2207 X l f (X) \u2225 \u2207 X l f\
    \ (X)\u2225 for each sample model f . Thus, clustered points indicate aligned\
    \ sample gradients. The xy -plane is spanned by the sample mean vectors u (black\
    \ line) and u reg (red line) of the sample gradients for Baseline (black dots)\
    \ and Baseline+GradDiv (red dots), respectively. The length of sample mean vector\
    \ is called the Mean Resultant Length (MRL)."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\GradDiv_Adversarial_Robustness_of_Randomized_Neural_Networks_via_Gradient_Divers\figure_2.jpg
  Figure 2 caption: "The effectiveness of the PGD attack using proxy gradients instead\
    \ of the actual gradients. The proxy gradients are obtained by rotating the true\
    \ gradients by \u03B8 with a random axis at every iterations of the attack. The\
    \ loss difference caused by the proxy gradients (red) decreases, and the accuracy\
    \ under the PGD attack (blue) increases as \u03B8 increases to 90 \u2218 . Note\
    \ that the graph of the loss difference (red) appears similar to the cosine curve,\
    \ empirically demonstrating (11)."
  Figure 3 Link: articels_figures_by_rev_year\2022\GradDiv_Adversarial_Robustness_of_Randomized_Neural_Networks_via_Gradient_Divers\figure_3.jpg
  Figure 3 caption: The change in the concentration measures (16) (left), (17) (middle),
    and (19) (right) during training on MNIST. The two leftmost vertical lines in
    each graph indicate the end of the warm-up and ramp-up periods, and the third
    vertical line indicates when the learning rate has decayed.
  Figure 4 Link: articels_figures_by_rev_year\2022\GradDiv_Adversarial_Robustness_of_Randomized_Neural_Networks_via_Gradient_Divers\figure_4.jpg
  Figure 4 caption: "Top: The scatter plot of the gradient samples on the unit circle.\
    \ The dots indicate the gradient samples from the randomized neural network and\
    \ the lines indicate the sample mean vectors of the gradients with the length\
    \ of the sample MRL \u03C1 (black: Adv-BNN, red: Adv-BNN+GradDiv). Bottom: The\
    \ density plots of the loss increase under the EOT-PGD attack. GradDiv effectively\
    \ reduces the effects of the proxy-gradient-based attack as indicated in (12)."
  Figure 5 Link: articels_figures_by_rev_year\2022\GradDiv_Adversarial_Robustness_of_Randomized_Neural_Networks_via_Gradient_Divers\figure_5.jpg
  Figure 5 caption: The density plots of the estimated concentration parameters hatkappa
    for the test examples of MNIST (top), CIFAR10 (middle), and STL10 (bottom). GradDiv
    effectively reduces hatkappa .
  Figure 6 Link: articels_figures_by_rev_year\2022\GradDiv_Adversarial_Robustness_of_Randomized_Neural_Networks_via_Gradient_Divers\figure_6.jpg
  Figure 6 caption: The test accuracy against the EOT-PGD attack on MNIST (top) and
    STL10 (bottom).
  Figure 7 Link: articels_figures_by_rev_year\2022\GradDiv_Adversarial_Robustness_of_Randomized_Neural_Networks_via_Gradient_Divers\figure_7.jpg
  Figure 7 caption: Transferability among sample models of Adv-BNN+GradDiv (left)
    and Adv-BNN (right). Lighter colors indicate higherbetter test accuracy, i.e.,
    lower transferability.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.83
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.83
  Name of the first author: Sungyoon Lee
  Name of the last author: Jaewook Lee
  Number of Figures: 7
  Number of Tables: 1
  Number of authors: 3
  Paper title: 'GradDiv: Adversarial Robustness of Randomized Neural Networks via
    Gradient Diversity Regularization'
  Publication Date: 2022-04-21 00:00:00
  Table 1 caption: "TABLE 1 The Test Accuracy (mean \xB1 std% for Stochastic Defenses)\
    \ Against a Set of Adversarial Attacks With \u03F5= 8 255 \u03B5=8255 on the CIFAR10\
    \ Dataset"
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3169217
- Affiliation of the first author: school of communication and information engineering,
    shanghai university, shanghai, china
  Affiliation of the last author: school of communication and information engineering,
    shanghai university, shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Maximum_Block_Energy_Guided_Robust_Subspace_Clustering\figure_1.jpg
  Figure 1 caption: "Performance of our method with varying \u03B1 and k on Hopkins\
    \ 155."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Maximum_Block_Energy_Guided_Robust_Subspace_Clustering\figure_2.jpg
  Figure 2 caption: "Performance of BEM(LSR-Z) and BEM(BDR-Z) with different corrupted\
    \ ratios on Hopkins 155 based on the optimal \u03B1 ."
  Figure 3 Link: articels_figures_by_rev_year\2022\Maximum_Block_Energy_Guided_Robust_Subspace_Clustering\figure_3.jpg
  Figure 3 caption: "Performance of BEM(LSR-Z) and BEM(BDR-Z) with different corrupted\
    \ ratios on Extended Yale B based on the optimal \u03B1 ."
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.67
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Yalan Qin
  Name of the last author: Guorui Feng
  Number of Figures: 3
  Number of Tables: 5
  Number of authors: 4
  Paper title: Maximum Block Energy Guided Robust Subspace Clustering
  Publication Date: 2022-04-22 00:00:00
  Table 1 caption: "TABLE 1 Choices of \u03A9(C),||.| | l \u03A9(C),||.||l and \u03A8\
    \ \u03A8 of the Existing Representative Methods"
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Clustering Error (%) on Hopkins 155
  Table 3 caption: TABLE 3 Clustering Error (%) on Extended Yale B
  Table 4 caption: TABLE 4 Clustering Error (%) on USPS
  Table 5 caption: TABLE 5 Running Time (%) on Different Datasets
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3168882
- Affiliation of the first author: school of data science, shanghai key lab of intelligent
    information processing, fudan university, shanghai, china
  Affiliation of the last author: hong kong university of science and technology,
    hong kong
  Figure 1 Link: articels_figures_by_rev_year\2022\Exploring_Structural_Sparsity_of_Deep_Networks_Via_Inverse_Scale_Spaces\figure_1.jpg
  Figure 1 caption: "Visualization of solution path and filter patterns in the third\
    \ convolutional layer (i.e., conv.c5) of LetNet-5, trained on MNIST. The left\
    \ figure shows the magnitude changes for each filter of the models trained by\
    \ DessiLBI and SGD, where x -axis and y -axis indicate the training epochs, and\
    \ filter magnitudes ( \u2113 2 -norm), respectively. The DessiLBI path of filters\
    \ selected in the support of \u0393 are drawn in blue color, while the red color\
    \ curves represent the filters that are not important and outside the support\
    \ of \u0393 . We visualize the corresponding learned filters by [104] at 20 (purple),\
    \ 40 (green), and 80 (black) epochs, which are shown in the right figure with\
    \ the corresponding color bounding boxes, i.e., purple, green, and black, respectively.\
    \ It shows that our DessiLBI enjoys a sparse selection of filters without sacrificing\
    \ accuracy (see Table 1)."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Exploring_Structural_Sparsity_of_Deep_Networks_Via_Inverse_Scale_Spaces\figure_2.jpg
  Figure 2 caption: The sparsified network structure of VGG16 and ResNet56 by our
    DessiLBI pruning layers. The green line shows the structure of HRank [118].
  Figure 3 Link: articels_figures_by_rev_year\2022\Exploring_Structural_Sparsity_of_Deep_Networks_Via_Inverse_Scale_Spaces\figure_3.jpg
  Figure 3 caption: DessiLBI with early stopping finds sparse subnet whose test accuracy
    (stars) after retraining is comparable or even better than winning tickets found
    by one shot pruning and iterative pruning in [31] (reproduced based on released
    code of [41]). All the three lottery ticket pruning methods outperform Iterative-Pruning-A
    [116] and Iterative-Pruning-B [117] (reproduced based on our own implementation).
  Figure 4 Link: articels_figures_by_rev_year\2022\Exploring_Structural_Sparsity_of_Deep_Networks_Via_Inverse_Scale_Spaces\figure_4.jpg
  Figure 4 caption: The winning ticket found by DessiLBI could generalize to different
    datasets. Each figure shows the trade-off of accuracy and sparsity on a target
    dataset. The red curve represents retraining after random pruning and the shaded
    area shows the standard error of five experiments. Stars with different colors
    show the performance of winning tickets found by DessiLBI on different source
    datasets. It is clear that on all target datasets, the winning tickets generated
    from alternative source datasets exhibit similar performance to that generated
    from the target dataset.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: Yanwei Fu
  Name of the last author: Yuan Yao
  Number of Figures: 4
  Number of Tables: 5
  Number of authors: 7
  Paper title: Exploring Structural Sparsity of Deep Networks Via Inverse Scale Spaces
  Publication Date: 2022-04-22 00:00:00
  Table 1 caption: TABLE 1 Top-1Top-5 accuracy(%) on ImageNet-2012
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The Performance of the Sparse Structure Under Weight Pruning
    Setting Found Via Our Method
  Table 3 caption: TABLE 3 The Performance of the Sparse Structure Under Filter Pruning
    Setting Found Via Our Method
  Table 4 caption: TABLE 4 Filter Configuration of Model Pruned by L1-Prune and Model
    Learned by Our Growing Method on CIFAR10 Dataset. L 1 L1-P is the L 1 L1 Pruning
  Table 5 caption: TABLE 5 Comparison Between AutoGrow [124] and Our Growing Method
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3168881
- Affiliation of the first author: inria, universite cote dazur, valbonne, france
  Affiliation of the last author: toyota motor europe, zaventem, belgium
  Figure 1 Link: articels_figures_by_rev_year\2022\Toyota_Smarthome_Untrimmed_RealWorld_Untrimmed_Videos_for_Activity_Detection\figure_1.jpg
  Figure 1 caption: 'Overview of the challenges in TSU. On the left part, we present
    challenges related to spontaneous behaviours: For the first two examples, we present
    the activity following a strict script on the left, and the same activity performed
    spontaneously in TSU on the right: i) Different from using drawer performed quickly,
    once per video [13], in TSU, using drawer may be repeated several times in a video,
    and the subject may keep several drawers open at the same time to facilitate finding
    things. In [14], subject uses shortly the telephone while looking at the camera.
    In contrast in TSU, the subject is deeply involved with his telephone and the
    activity may last several minutes instead of few seconds. In TSU, subject may
    stayed seated or stand up to cut the bread in an easier manner. Besides the spontaneous
    behaviours, we also illustrate on the right part the following real-world challenges:
    (1) Camera framing: subject is not in the middle of the image and can be even
    outside the field of view. (2) Object-based activities: similar activities can
    be performed while interacting with different objects. (3) Multi-views: activities
    look different in appearance from different view points. (4) Composite activity:
    composite activities can be split into several elementary activities (e.g. While
    having breakfast, we may cut bread, spread butter and eat at the table). Moreover,
    these complex composite activities can last a long period of time. Large variations
    of appearance make the recognition challenging, requiring to understand the composition
    of elementary activities to better recognize the composite activities. (5) Concurrent
    activities: activities can be performed concurrently (e.g. take note while having
    a phone call). (6) High temporal variation: in the same untrimmed video, we may
    have related short activities (e.g. taking on glasses) and long activities (e.g.
    playing tablet). Different instances of the same activity class can also be short
    or long (e.g. writing) corresponding to high intra-class temporal variance.'
  Figure 10 Link: articels_figures_by_rev_year\2022\Toyota_Smarthome_Untrimmed_RealWorld_Untrimmed_Videos_for_Activity_Detection\figure_10.jpg
  Figure 10 caption: Qualitative analysis of the detection result and the attention
    map. On the top, we visualize the attention map Ai for 5 layers. On the bottom,
    we present the corresponding ground truth and detection performance for an example
    video.
  Figure 2 Link: articels_figures_by_rev_year\2022\Toyota_Smarthome_Untrimmed_RealWorld_Untrimmed_Videos_for_Activity_Detection\figure_2.jpg
  Figure 2 caption: 'Available modalities in Toyota Smarthome Untrimmed. Note: in
    the sub-figure of RGB modality, we also mark the 2D skeleton joints.'
  Figure 3 Link: articels_figures_by_rev_year\2022\Toyota_Smarthome_Untrimmed_RealWorld_Untrimmed_Videos_for_Activity_Detection\figure_3.jpg
  Figure 3 caption: "An example of annotation on TSU dataset. \u2190 and \u2192 indicate\
    \ respectively the start and end of an activity."
  Figure 4 Link: articels_figures_by_rev_year\2022\Toyota_Smarthome_Untrimmed_RealWorld_Untrimmed_Videos_for_Activity_Detection\figure_4.jpg
  Figure 4 caption: 'On the top row, we divide the 51 activities in TSU into (a) composite
    and (b) elementary activities. Then, we analyze the activities along four properties:
    (c) highly related composite and elementary activities, (d) pose-based activities,
    (e) similar motionactivities, and (f) activities with subtle motion.'
  Figure 5 Link: articels_figures_by_rev_year\2022\Toyota_Smarthome_Untrimmed_RealWorld_Untrimmed_Videos_for_Activity_Detection\figure_5.jpg
  Figure 5 caption: 'On top row (from left to right): we provide the 7 camera locations
    (C: camera); activity distribution along the different (a) environments, (b) duration
    and (c) temporal variance. Remark: (a) is per activity instance, (b),(c) are per
    activity class. On bottom row: we provide the (d) instance frequency and corresponding
    (e) temporal variance heat map (e.g. the lighter the larger variance), (f) distribution
    of performing environment for each activity.'
  Figure 6 Link: articels_figures_by_rev_year\2022\Toyota_Smarthome_Untrimmed_RealWorld_Untrimmed_Videos_for_Activity_Detection\figure_6.jpg
  Figure 6 caption: Spatial Distribution of the person location in normalized image
    coordinates for 3 datasets, dark regions correspond to high frequency areas of
    the person position. The green bounding boxes embrace the high frequency locations.
    From the size of the bounding box, we find that TSU exhibits the largest spatial
    scatter, indicating the low camera framing property.
  Figure 7 Link: articels_figures_by_rev_year\2022\Toyota_Smarthome_Untrimmed_RealWorld_Untrimmed_Videos_for_Activity_Detection\figure_7.jpg
  Figure 7 caption: On the left, we present the overview of the AGNet. In this figure,
    Bottleneck indicates the 1D convolution that processes the features across time
    and which kernel size is 1. On the right, we present the computation flow for
    one block. In each block, k is the kernel size and d is the dilation.
  Figure 8 Link: articels_figures_by_rev_year\2022\Toyota_Smarthome_Untrimmed_RealWorld_Untrimmed_Videos_for_Activity_Detection\figure_8.jpg
  Figure 8 caption: Average Precision for the activities in Fine-grained TSU. The
    classes are sorted by their size. The mAP is marked by a red line. We can see
    that while there is a slight trend for smaller classes to have lower accuracy,
    many classes do not follow that trend.
  Figure 9 Link: articels_figures_by_rev_year\2022\Toyota_Smarthome_Untrimmed_RealWorld_Untrimmed_Videos_for_Activity_Detection\figure_9.jpg
  Figure 9 caption: 'Frame-based mAP of the AGNet using different modalities: (1)
    Top 10 activities where the 3D skeleton stream outperforms the RGB stream for
    the CV protocol. (2) Top 10 activities where the RGB stream outperforms the 3D
    Skeleton stream for the CS protocol.'
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Rui Dai
  Name of the last author: Gianpiero Francesca
  Number of Figures: 15
  Number of Tables: 8
  Number of authors: 7
  Paper title: 'Toyota Smarthome Untrimmed: Real-World Untrimmed Videos for Activity
    Detection'
  Publication Date: 2022-04-25 00:00:00
  Table 1 caption: TABLE 1 Untrimmed Dataset Comparison Along the Seven Real-World
    Challenges
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison Between the Two Versions of Toyota Smarthome
  Table 3 caption: TABLE 3 Per-Frame mAP (%) on the Fine-Grained TSU Dataset
  Table 4 caption: TABLE 4 Event-Based mAP (%) for Different IoU Thresholds for the
    Fine-Grained TSU dataset
  Table 5 caption: TABLE 5 Balanced TSU
  Table 6 caption: TABLE 6 View 1 and View 2 Indicate the Two Views for the Synchronized
    Videos
  Table 7 caption: TABLE 7 Address the Camera Framing Challenge
  Table 8 caption: 'TABLE 8 Per-Frame mAP (%) on Charades, Evaluated With the Charades
    Localization Setting. Note: Cited Papers May Not Be the Original Paper but the
    One Providing This mAP Results'
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3169976
