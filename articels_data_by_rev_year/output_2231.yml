- Affiliation of the first author: cvlab, epfl, lausanne, switzerland
  Affiliation of the last author: "encov-institut pascal-cnrsuniversit\xE9 clermont\
    \ auvergne, clermont-ferrand, france"
  Figure 1 Link: articels_figures_by_rev_year\2021\Robust_Isometric_NonRigid_StructureFromMotion\figure_1.jpg
  Figure 1 caption: Reconstruction of an image from the Tshirt dataset using automatic
    correspondences. This dataset was introduced in [9] with manually clicked correspondences.
    With these perfect correspondences, both [9], [30] achieved good 3D reconstruction
    quality. However, they both fail when the correspondences are obtained automatically
    using ASIFT, because these contain mismatches. Our robust method, on the other
    hand, handles both the manual and automatic correspondences well.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Robust_Isometric_NonRigid_StructureFromMotion\figure_2.jpg
  Figure 2 caption: Notation, illustrated in a two-view NRSfM example.
  Figure 3 Link: articels_figures_by_rev_year\2021\Robust_Isometric_NonRigid_StructureFromMotion\figure_3.jpg
  Figure 3 caption: Solution from [31] and our methods for three scenarios.
  Figure 4 Link: articels_figures_by_rev_year\2021\Robust_Isometric_NonRigid_StructureFromMotion\figure_4.jpg
  Figure 4 caption: "The Cylinder dataset. (top) Shape and depth errors (400 points,\
    \ 7 images, 1 pixel noise, correspondence errors between 0-50 percent at 100 pixels).\
    \ (middle) Analysis of RR-NRSfM: TNR shown across perturbation amplitudes and\
    \ ROC analysis in the error range of 76\u2212100 pixels. (bottom) Shape (Es) and\
    \ depth (Ed) error for FR-NRSfM with various sizes of M withwithout the influence\
    \ of missing data. The mean computation time for M as 5, 7 and 10 are 2.5s, 3s\
    \ and 5s respectively. The performance becomes stable for M\u22657 ."
  Figure 5 Link: articels_figures_by_rev_year\2021\Robust_Isometric_NonRigid_StructureFromMotion\figure_5.jpg
  Figure 5 caption: Visual ROC for Rug and Tshirt. RR-NRSfM performs MAD+MVR+OR. The
    use of MAD and OR drastically reduces the falsely classified 3D points. In Rug,
    OR identifies less points when used with MAD while in Tshirt, it identifies more
    incorrect 3D points.
  Figure 6 Link: articels_figures_by_rev_year\2021\Robust_Isometric_NonRigid_StructureFromMotion\figure_6.jpg
  Figure 6 caption: "(a) Summary of methods. For each dataset, the depth (Ed) and\
    \ shape (Es) errors are reported. \u2013 represents the methods that did not complete\
    \ in 24 hours. XX represents the methods which failed due to missing data. (b)\
    \ Ablation study and (c) ROC analysis of our robust pipeline using FR-NRSfM as\
    \ a base method."
  Figure 7 Link: articels_figures_by_rev_year\2021\Robust_Isometric_NonRigid_StructureFromMotion\figure_7.jpg
  Figure 7 caption: Shape and depth errors for the cushion (top) and the paper (bottom)
    in the NRSfM use-case. The table summarises the mean shape errors (Es) and mean
    depth errors (Ed) for FR-NRSFM (FR), FS-NRSFM (FS), RR-NRSFM (RR) and RS-NRSFM
    (RS) over the entire data.
  Figure 8 Link: articels_figures_by_rev_year\2021\Robust_Isometric_NonRigid_StructureFromMotion\figure_8.jpg
  Figure 8 caption: Reconstruction of Cushion and Paper from the uniformly sampled
    NRSfM use-case sequence using RR-NRSfM. A grid computed from reconstructed points
    is shown to display the geometry of the 3D object. The color of the grid points
    represent the relative depth of the grid points, blue being closest to the camera
    and yellow being the farthest. Ed represents the mean depth error measured in
    mm .
  Figure 9 Link: articels_figures_by_rev_year\2021\Robust_Isometric_NonRigid_StructureFromMotion\figure_9.jpg
  Figure 9 caption: The eagle-ray footage with RR-NRSfM reconstruction. The red arrow
    indicates the tail and the orange line indicates the spine of the eagle-ray.
  First author gender probability: 0.97
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Shaifali Parashar
  Name of the last author: Adrien Bartoli
  Number of Figures: 9
  Number of Tables: 1
  Number of authors: 3
  Paper title: Robust Isometric Non-Rigid Structure-From-Motion
  Publication Date: 2021-06-16 00:00:00
  Table 1 caption: TABLE 1 The Sylvester Matrix for the Degenerate Cases of Cubics
    A A and B B
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3089923
- Affiliation of the first author: school of software engineering, south china university
    of technology, guangzhou, guangdong, china
  Affiliation of the last author: mit-ibm watson ai laboratory, cambridge, ma, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\Graph_Convolutional_Module_for_Temporal_Action_Localization_in_Videos\figure_1.jpg
  Figure 1 caption: Schematic depiction of our approach. We apply graph convolutional
    networks to model the interactions between action units and boost the temporal
    action localization performance.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Graph_Convolutional_Module_for_Temporal_Action_Localization_in_Videos\figure_2.jpg
  Figure 2 caption: Schematic of our method. (a) Given a set of action units (e.g.,
    proposals in two-stage methods and segments in one-stage methods), our graph convolutional
    module (GCM) instantiates the nodes in the graph by each action unit. Then, we
    establish three kinds of edges among nodes to model the relations between action
    units and employ GCNs on the constructed graph. Lastly, our GCM module outputs
    relation-aware features. (b) For two-stage action localization methods, our GCM
    can be used in the second stage to enhance the proposal features, which are used
    for action classification and boundary regression. (c) For one-stage action localization
    methods, our GCM can be exploited to enhance the video features before the anchor
    layer.
  Figure 3 Link: articels_figures_by_rev_year\2021\Graph_Convolutional_Module_for_Temporal_Action_Localization_in_Videos\figure_3.jpg
  Figure 3 caption: Visualization results of the graph constructed by our GCM on THUMOS14.
    The temporal boundary of the input proposal is not precise (i.e., some portions
    of the corresponding ground truth have not been detected). Our proposed GCM helps
    to aggregates contextual information from other proposals and lastly predicts
    the action category correctly and refines the temporal boundary of the input proposal
    precisely.
  Figure 4 Link: articels_figures_by_rev_year\2021\Graph_Convolutional_Module_for_Temporal_Action_Localization_in_Videos\figure_4.jpg
  Figure 4 caption: Action localization results on THUMOS14 with different backbones,
    measured by mAPtIoU=0.5.
  Figure 5 Link: articels_figures_by_rev_year\2021\Graph_Convolutional_Module_for_Temporal_Action_Localization_in_Videos\figure_5.jpg
  Figure 5 caption: Qualitative results on THUMOS14 dataset. Our proposed GCM helps
    SSN to predict a more precise temporal boundary.
  Figure 6 Link: articels_figures_by_rev_year\2021\Graph_Convolutional_Module_for_Temporal_Action_Localization_in_Videos\figure_6.jpg
  Figure 6 caption: 'Examples of failure cases. Top: Our method predicts the beginning
    portion of Pole Vault as Javelin Throw since these two actions have similar contents
    (i.e., an athlete running with a pole). Bottom: Our method mis-classifies the
    action Cliff Diving into the action Diving without recognizing the background
    cliff.'
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.75
  Name of the first author: Runhao Zeng
  Name of the last author: Chuang Gan
  Number of Figures: 6
  Number of Tables: 12
  Number of authors: 7
  Paper title: Graph Convolutional Module for Temporal Action Localization in Videos
  Publication Date: 2021-06-17 00:00:00
  Table 1 caption: "TABLE 1 Action Localization Results on THUMOS14, Measured by mAP\
    \ (%) at Different tIoU Thresholds \u03B1 \u03B1"
  Table 10 caption: TABLE 10 Comparison of Different Sampling Sizes and Training Time
    for Each Iteration on THUMOS14, Measured by mAPtIoU=0.5
  Table 2 caption: TABLE 2 Action Localization Results on ActivityNet v1.3 (val),
    Measured by mAP (%) at Different tIoU Thresholds and the Average mAP of IoU Thresholds
    From 0.5 to 0.95
  Table 3 caption: TABLE 3 Ablation Study of GCM on CBR and R-C3D, Measured by mAP
    (%) When tIoU=0.5 on THUMOS14
  Table 4 caption: TABLE 4 Comparison Between Our Model and the MLP Baseline on THUMOS14,
    Measured by mAP (%) When tIoU=0.5
  Table 5 caption: TABLE 5 Comparison With MLP Baseline in Terms of Runtime, Computation
    Complexity in FLOPs, and Action Localization mAP on THUMOS14
  Table 6 caption: TABLE 6 Comparison Between Our Model and Mean-Pooling (MP) on THUMOS14,
    Measured by mAP (%) When tIoU=0.5
  Table 7 caption: TABLE 7 Comparison of Different Types of Edge Functions on THUMOS14,
    Measured by mAP (%) When tIoU=0.5
  Table 8 caption: TABLE 8 Comparisons Between our GCM and the Baseline Using Learned
    Weights on THUMOS14
  Table 9 caption: TABLE 9 Comparison of Three Types of Edge on THUMOS14, Measured
    by mAP (%) When tIoU=0.5
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3090167
- Affiliation of the first author: department of information engineering, university
    of florence, florence, italy
  Affiliation of the last author: department of information engineering, university
    of florence, florence, italy
  Figure 1 Link: articels_figures_by_rev_year\2021\A_Sparse_and_Locally_Coherent_Morphable_Face_Model_for_Dense_Semantic_Correspond\figure_1.jpg
  Figure 1 caption: "Previous solutions, e.g., PCA [59] and others, interpret each\
    \ 3D face as a training sample, and learn a set of k<N components (top row). In\
    \ our solution (bottom row), we analyze each of the 3m coordinate displacements\
    \ v \u2032 i independently. A set of k\u226A3m primary directions d i is extracted\
    \ from V \u2032 . The coefficients c i identify a way to expand the k learned\
    \ directions back to the 3m coordinates and deform the 3D model."
  Figure 10 Link: articels_figures_by_rev_year\2021\A_Sparse_and_Locally_Coherent_Morphable_Face_Model_for_Dense_Semantic_Correspond\figure_10.jpg
  Figure 10 caption: Re-indexed models mathbf tprime with transferred landmarks mathbf
    tprime (Lidx) obtained using NICP [49] (a), the standard Nearest-Neighbor (b),
    and our proposed mean point association (c). The ground-truth scans are shown
    in (d). Our solution maintains a semantic consistency in case of large expressions
    and noise, especially in the mouth region.
  Figure 2 Link: articels_figures_by_rev_year\2021\A_Sparse_and_Locally_Coherent_Morphable_Face_Model_for_Dense_Semantic_Correspond\figure_2.jpg
  Figure 2 caption: "Examples of learned components C modeling structural traits of\
    \ the face related to the identity (a-b) as well as expressions (c-d). In (e)\
    \ the same component as in (d) learned without \u2113 2 regularization."
  Figure 3 Link: articels_figures_by_rev_year\2021\A_Sparse_and_Locally_Coherent_Morphable_Face_Model_for_Dense_Semantic_Correspond\figure_3.jpg
  Figure 3 caption: "Large differences between template and target induce a misalignment\
    \ (red=high, green=low) of local regions (left). Local Mean-Point association\
    \ and outliers rejection (right): for each Voronoi region R j (blue polygon),\
    \ the point S j (blue dot) is associated to the centroid t \xAF j (green dot)\
    \ of the points t R j (red crosses). The local and global rejection thresholds\
    \ (red and yellow dotted lines) are iteratively updated."
  Figure 4 Link: articels_figures_by_rev_year\2021\A_Sparse_and_Locally_Coherent_Morphable_Face_Model_for_Dense_Semantic_Correspond\figure_4.jpg
  Figure 4 caption: Overview of the proposed 3DMM fitting and dense semantic transfer
    approach. First, the template and target are rigidly aligned. Then, s is iteratively
    deformed guided by our mean-point association procedure for point-to-point correspondence
    estimation. Finally, the dense annotation of the 3DMM is transferred to the target.
    Red points denote the detected outliers that are iteratively refined to let the
    template smoothly adapt to the target shape, while rejecting noisy local regions
    (e.g., points inside the mouth).
  Figure 5 Link: articels_figures_by_rev_year\2021\A_Sparse_and_Locally_Coherent_Morphable_Face_Model_for_Dense_Semantic_Correspond\figure_5.jpg
  Figure 5 caption: 'CoMA dataset: Qualitative comparison of some reconstructions
    obtained with the Neural3DMM [26], PCA and our SLC solution (better viewed in
    digital). Error heatmaps show that SLC results in more accurate reconstructions.'
  Figure 6 Link: articels_figures_by_rev_year\2021\A_Sparse_and_Locally_Coherent_Morphable_Face_Model_for_Dense_Semantic_Correspond\figure_6.jpg
  Figure 6 caption: 'CoMA dataset: Ablation study against CoMA [25], Neural3DMM [26],
    DL [30] and PCA. The effect of varying the latent vector size, i.e., number of
    components (a), cumulative per-vertex error distribution using z=k=512 (b), and
    learning the models on a reduced training set (360 scans instead of approx 19,000
    ) (c) are reported.'
  Figure 7 Link: articels_figures_by_rev_year\2021\A_Sparse_and_Locally_Coherent_Morphable_Face_Model_for_Dense_Semantic_Correspond\figure_7.jpg
  Figure 7 caption: 'CoMA dataset: Testing on unseen identities and expressions. Top
    row: SLC reconstructed samples. Bottom row: average per-vertex euclidean error
    is reported for each of the 12 expressions ( x -axis).'
  Figure 8 Link: articels_figures_by_rev_year\2021\A_Sparse_and_Locally_Coherent_Morphable_Face_Model_for_Dense_Semantic_Correspond\figure_8.jpg
  Figure 8 caption: "BU-3DFE dataset: Cumulative per-vertex error distribution comparing\
    \ against (a) PCA and DL, (b) Neural3DMM. Legends report \u201CMethod T - k \u201D\
    \ where the subscript T refers either to the complete training set \u201CA\u201D\
    , or the reduced training set \u201CR\u201D (10 percent of the samples). k is\
    \ the number of components."
  Figure 9 Link: articels_figures_by_rev_year\2021\A_Sparse_and_Locally_Coherent_Morphable_Face_Model_for_Dense_Semantic_Correspond\figure_9.jpg
  Figure 9 caption: Cumulative landmark localization error distributions for (a) FRGCv2.0,
    (b) Bosphorus, and (c) FaceWarehouse.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Claudio Ferrari
  Name of the last author: Alberto Del Bimbo
  Number of Figures: 14
  Number of Tables: 4
  Number of authors: 4
  Paper title: A Sparse and Locally Coherent Morphable Face Model for Dense Semantic
    Correspondence Across Heterogeneous 3D Faces
  Publication Date: 2021-06-22 00:00:00
  Table 1 caption: TABLE 1 Average Times for Dense Registration and Models Learning
  Table 10 caption: Not Available
  Table 2 caption: 'TABLE 2 FRGCv2.0: Landmark Localization Error (mm)'
  Table 3 caption: 'TABLE 3 Bosphorus: Landmarks Localization Error (mm)'
  Table 4 caption: "TABLE 4 FaceWarehouse: Mean \xB1 \xB1 Std of Landmark Localization\
    \ Error (mm)"
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3090942
- Affiliation of the first author: national engineering laboratory for integrated
    aero-space-ground-ocean big data application technology, school of computer science
    and engineering, northwestern polytechnical university, xian, china
  Affiliation of the last author: department of radiology and bric, university of
    north carolina at chapel hill, chapel hill, nc, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\DiseaseImageSpecific_Learning_for_DiagnosisOriented_Neuroimage_Synthesis_With_In\figure_1.jpg
  Figure 1 caption: 'Illustration of our disease-image-specific deep learning (DSDL)
    framework. Two major components are included: (a) two single-modality Disease-image-Specific
    Networks (DSNet) for classification and learning disease-image specificity for
    MRI (i.e., P A = F A + C A ) and PET (i.e., P B = F B + C B ), respectively, and
    (b) a Feature-consistency Generative Adversarial Network (FGAN) for missing image
    synthesis, encouraging feature maps (e.g., generated by F A ) of a synthetic image
    and its real image to be consistent. Note that F A and F B in (b) are initialized
    by those learned in (a) and kept frozen in FGAN. Based on complete (after imputation
    via FGAN) paired MRI and PET scans, we further develop a multi-modality DSNet
    (i.e., P AB =[ F A , F B ]+ C AB ) for brain disease identification by concatenating
    feature maps of MRI and PET (c). In (a) and (c), the backbone feature extractors
    (e.g., F A and F B ) are followed by a spatial cosine module (e.g., C A , C B
    , and C AB ) for classification.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\DiseaseImageSpecific_Learning_for_DiagnosisOriented_Neuroimage_Synthesis_With_In\figure_2.jpg
  Figure 2 caption: "Illustration of the proposed disease-image-specific network (DSNet)\
    \ for disease classification and modeling disease-image specificity. (a) two single-modality\
    \ DSNets (i.e., P A and P B ) using MRI and PET data, receptively, with each containing\
    \ a backbone (i.e., F A or F B ) for feature extraction and a classifier (i.e.,\
    \ C A or C B ) for classification. (b) a multi-modality DSNet (mDSNet) that use\
    \ paired MRI and PET data as input (i.e., P AB ), with two parallel backbones\
    \ (i.e., F A for MRI and F B for PET) and a classifier (i.e., C AB based on concatenation\
    \ of features maps generated from two modalities). The backbones (i.e., F A and\
    \ F B ) in single-modality DSNet and mDSNet share the same network architecture\
    \ but have different input modalities, containing 5 convolutional layers (size:\
    \ 3\xD73\xD73 ) with instance normalization and \u201Crelu\u201D activation. Also,\
    \ feature maps of the first 4 convolutional layers are max-pooled, while the feature\
    \ map of the last layer in each backbone is average-pooled with the stride of\
    \ 2\xD72\xD72 . Here, K denote the elements in the feature map generated by F\
    \ A or F B and K=4\xD75\xD74 in this work."
  Figure 3 Link: articels_figures_by_rev_year\2021\DiseaseImageSpecific_Learning_for_DiagnosisOriented_Neuroimage_Synthesis_With_In\figure_3.jpg
  Figure 3 caption: Illustration of our feature-consistency generative adversarial
    network (FGAN) for image synthesis. It contains (1) two feature-consistency components
    (i.e., MRI-based and PET-based components) to encourage feature maps of a synthetic
    image to be consistent with those of its corresponding real image, and (2) a generative
    adversarial learning component to synthesize images under the constraints of feature
    consistency (i.e., L f ) and distribution consistency (i.e., L a ). Note that
    F A and F B in two feature-consistency components have same architecture but are
    learned in MRI-based and PET-based DSNet models, respectively, through which the
    disease-image specificity learned in DSNets will be employed in the image synthesis
    process, encouraging FGAN to focus on those disease-relevant regions in each modality.
    Also, the adversarial components, i.e., D A and D B ), are used to constrain the
    synthetic MRI and PET scans follow the same data distribution of those real MRI
    and PET scans, respectively. Besides, two generators (i.e., G A and G B ) are
    learned to construct bi-directional mappings between two imaging modalities.
  Figure 4 Link: articels_figures_by_rev_year\2021\DiseaseImageSpecific_Learning_for_DiagnosisOriented_Neuroimage_Synthesis_With_In\figure_4.jpg
  Figure 4 caption: 'PET and MRI scans synthesized by four methods for two typical
    subjects (Roster IDs: 4386, 4997) in ADNI-2, along with their corresponding ground-truth
    images. All six image synthesis models are trained on ADNI-1.'
  Figure 5 Link: articels_figures_by_rev_year\2021\DiseaseImageSpecific_Learning_for_DiagnosisOriented_Neuroimage_Synthesis_With_In\figure_5.jpg
  Figure 5 caption: Quality (a) and classification performance (b) of synthetic images
    generated by five FGAN variants using the feature-consistency constraint on feature
    maps at only one single Conv layer (e.g., the i th layer l i ) in F A and F B
    .
  Figure 6 Link: articels_figures_by_rev_year\2021\DiseaseImageSpecific_Learning_for_DiagnosisOriented_Neuroimage_Synthesis_With_In\figure_6.jpg
  Figure 6 caption: Performance of the generative component of FGAN in image synthesis
    versus the numbers of training epochs. The model was trained on ADNI-1 with only
    single loss and tested on ADNI-2. Four loss functions were evaluated, including
    the adversarial loss ( L a ), cycle-consistency loss ( L c ), voxel-wise-consistency
    loss( L v ), and feature-consistency loss ( L f ).
  Figure 7 Link: articels_figures_by_rev_year\2021\DiseaseImageSpecific_Learning_for_DiagnosisOriented_Neuroimage_Synthesis_With_In\figure_7.jpg
  Figure 7 caption: The inter-class averaged dissimilarities (ICAD) and rescaled contribution
    coefficients of 80 brain region in synthetic MRI and PET images. For each modality,
    the top six rows are the ICAD of the synthesized images generated by GAN, cycGAN,
    voxGAN, FGAN, FcycGAN, and FvoxGAN, respectively, the 7th row is the ICAD of real
    images, and the 8th row is the rescaled value of contribution coefficient. It
    shows that the disease-relevant regions are different across two modalities.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yongsheng Pan
  Name of the last author: Dinggang Shen
  Number of Figures: 7
  Number of Tables: 6
  Number of authors: 4
  Paper title: Disease-Image-Specific Learning for Diagnosis-Oriented Neuroimage Synthesis
    With Incomplete Multi-Modality Data
  Publication Date: 2021-06-22 00:00:00
  Table 1 caption: TABLE 1 The Demographic and Clinical Information of Studied Subjects
    With Four Categories (Cat.) From Three Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Results(% except PSNR) of Image Synthesis Achieved by Six
    Different Methods for MRI and PET Scans of Subjects in ADNI-2, With the Models
    Trained on ADNI-1
  Table 3 caption: TABLE 3 Diagnosis Results (%) Achieved by Six Different Methods,
    With Classification Models Trained on ADNI-1 and Tested on ADNI2
  Table 4 caption: "TABLE 4 Diagnosis Results (%) Achieved by Six Different Methods\
    \ Using all Subjects With Only Real MRI Scans (denoted as \u201C-M\u201D) and\
    \ With Both Real MRI Images and Synthetic PET Images (generated by FGAN) in AC\
    \ versus CN. classification"
  Table 5 caption: TABLE 5 Comparison of Our mDSNet on AIBL While Using Only ADNI-1
    or Using Both ADNI-1 and ADNI-2 for Training in Stage (3)
  Table 6 caption: TABLE 6 Hypotheses and Obtained p p-values in Our Statistical Significance
    Analysis
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3091214
- Affiliation of the first author: bnrist, thuicbs, kliss, school of software, tsinghua
    university, beijing, china
  Affiliation of the last author: bnrist, thuicbs, kliss, school of software, tsinghua
    university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\ViewAware_GeometryStructure_Joint_Learning_for_SingleView_D_Shape_Reconstruction\figure_1.jpg
  Figure 1 caption: The VGSNet can reconstruct a 3D shape in the point cloud form
    from a single-view image by joint learning 2D image, 3D geometry and structure
    features. Fine-grained part geometry and structure are successfully reconstructed
    on a detailed level that no baseline methods are able to achieve.
  Figure 10 Link: articels_figures_by_rev_year\2021\ViewAware_GeometryStructure_Joint_Learning_for_SingleView_D_Shape_Reconstruction\figure_10.jpg
  Figure 10 caption: "Ablation study on applying GNN for encoding structure features.\
    \ Training with explicit structure learning by GNN produces more detailed part\
    \ geometry and structure than other methods. FPS is applied to the result of \u201C\
    with GNN\u201D to allow for a fair visual comparison. For \u201Cwith GNN (Encoder)\u201D\
    , the results may even underperform the \u201Cwithout GNN\u201D since the non-GNN\
    \ based decoder used is not trained to reconstruct from the structure-aware feature\
    \ extracted by the GNN based encoder."
  Figure 2 Link: articels_figures_by_rev_year\2021\ViewAware_GeometryStructure_Joint_Learning_for_SingleView_D_Shape_Reconstruction\figure_2.jpg
  Figure 2 caption: 'Mapping between images (camera views) and the 3D shapes. Left:
    images captured by cameras at different viewpoints are mapped to the same shape
    at the canonical pose (same geometry coordinates). Right: each image is just mapped
    to one transformed 3D shape (different geometry coordinates for these two shapes).'
  Figure 3 Link: articels_figures_by_rev_year\2021\ViewAware_GeometryStructure_Joint_Learning_for_SingleView_D_Shape_Reconstruction\figure_3.jpg
  Figure 3 caption: The multimodal joint VAE branch for VGSNet. The parts (highlighted
    in red) from pairs of view-aligned image and 3D shape are fed to the joint encoder
    to compute a view-aware z VGS . The joint decoder transforms z VGS back to part
    images and 3D shapes recursively. The Shape structure of part relations is explicitly
    modeled by the hierarchical shape graph. The final output shape is a union of
    the reconstructed part point clouds.
  Figure 4 Link: articels_figures_by_rev_year\2021\ViewAware_GeometryStructure_Joint_Learning_for_SingleView_D_Shape_Reconstruction\figure_4.jpg
  Figure 4 caption: 'Training and inference stage of VGSNet. The network jointly trains
    two branches: a multimodal joint autoencoder for reconstructing the view-aligned
    3D shapes and images, and an auxiliary image encoder to map the image feature
    to the multimodal latent code. Single-view reconstruction is achieved at the inference
    stage by using the joint decoder to generate a point cloud Shape from the image
    feature extracted by the auxiliary image encoder.'
  Figure 5 Link: articels_figures_by_rev_year\2021\ViewAware_GeometryStructure_Joint_Learning_for_SingleView_D_Shape_Reconstruction\figure_5.jpg
  Figure 5 caption: The architecture of the joint part encoder (left) and joint part
    decoder (right). In the encoder, given a view-aligned pair of shape part p i and
    its part image, the 2D image feature f p i img extracted by ResNet-18 is aggregated
    to the shape feature f p i G extracted by PointNet to form a multimodal feature
    vector f p i VG . In the decoder, the graph node feature f p j VGS , which is
    generated by the joint graph decoder for a predicted part p j , is first disentangled
    into image feature f p j img and 3D shape feature f p j G . While f p j img is
    used for image reconstruction, multimodal fusion is employed to combine the image
    and 3D shape features and generate a multimodal feature f p j M for 3D shape reconstruction.
  Figure 6 Link: articels_figures_by_rev_year\2021\ViewAware_GeometryStructure_Joint_Learning_for_SingleView_D_Shape_Reconstruction\figure_6.jpg
  Figure 6 caption: Illustration of how various losses are applied on the VGSNet to
    ensure joint learning of image view, shape geometry and structure. Note that we
    do not show the feature distillation loss L fd for learning the auxiliary image
    encoder to avoid the clutteredness of the figure.
  Figure 7 Link: articels_figures_by_rev_year\2021\ViewAware_GeometryStructure_Joint_Learning_for_SingleView_D_Shape_Reconstruction\figure_7.jpg
  Figure 7 caption: Qualitative comparison on PartNet-color. The VGSNet significantly
    outperforms other methods by producing more fine-grained 3D geometry and structures,
    while the baselines may fail dramatically, such as having large missing regions
    and holes.
  Figure 8 Link: articels_figures_by_rev_year\2021\ViewAware_GeometryStructure_Joint_Learning_for_SingleView_D_Shape_Reconstruction\figure_8.jpg
  Figure 8 caption: Qualitative comparison on PartNet-texture. The baseline methods
    show improved performance comparing to the results on PartNet-color in Fig. 7.
    Meanwhile, the VGSNet produces the best results with more geometry and structure
    details (e.g., the bars in chairs back) on this challenging dataset, showing the
    generalization ability of the network.
  Figure 9 Link: articels_figures_by_rev_year\2021\ViewAware_GeometryStructure_Joint_Learning_for_SingleView_D_Shape_Reconstruction\figure_9.jpg
  Figure 9 caption: Qualitative comparison with StructureNet on PartNet-texture. The
    joint 2D-3D embedding is applied to StructureNet for single-view reconstruction.
    Note that although StructureNet can generate results with relatively better symmetry
    and part adjacency, it may predict inaccurate geometry (first row) or incorrect
    structure from the image, e.g., wrong number of bars in the chairs back (second
    row) or even wrong shape for the chair (third row). In comparison, VGSNet can
    produce satisfied reconstruction in both geometry and structure.
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Xuancheng Zhang
  Name of the last author: Yue Gao
  Number of Figures: 14
  Number of Tables: 5
  Number of authors: 6
  Paper title: View-Aware Geometry-Structure Joint Learning for Single-View 3D Shape
    Reconstruction
  Publication Date: 2021-06-22 00:00:00
  Table 1 caption: "TABLE 1 Quantitative Comparison of Average CD- L 2 L2 (unit: 10\
    \ \u22123 10-3) and Average F-Score (unit: 10 \u22123 10-3) for Each Category\
    \ in PartNet-Color Testing Set"
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 Quantitative Comparison of Average CD- L 2 L2 (unit: 10\
    \ \u22123 10-3) and Average F-Score (Unit: 10 \u22123 10-3) for Each Category\
    \ in PartNet-texture Testing Set"
  Table 3 caption: TABLE 3 Quantitative Comparison With StructureNet for the Geometry
    and Structure Prediction on PartNet-Texture
  Table 4 caption: "TABLE 4 Quantitative Ablation Study on Applying GNN for Structure\
    \ Learning. Average CD- L 2 L2 (Unit: 10 \u22123 10-3) for Each Category in PartNet-texture\
    \ Testing Set is Reported."
  Table 5 caption: "TABLE 5 Quantitative Ablation Study on Different Multimodal Feature\
    \ Fusion Schemes. Average CD- L 2 L2 (Unit: 10 \u22123 10-3) for Each Category\
    \ in PartNet-texture Testing Set is Reported."
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3090917
- Affiliation of the first author: neuroscience and intelligent media institute (nimi),
    communication university of china, beijing, china
  Affiliation of the last author: national laboratory of pattern recognition (nlpr),
    center for biometrics and security research (cbsr), institute of automation, chinese
    academy of sciences (casia), beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\MetaTeacher_For_Face_AntiSpoofing\figure_1.jpg
  Figure 1 caption: 'The first row: some examples of the live and spoof (funny eye,
    paper mask, transparent mask, impersonate, and paper glasses) faces from SiW-M[19].
    The second row: the exhausted annotated pixel-wise mask labels for the live and
    spoof faces in the first row. The third row: the normalized pixel-wise supervision
    provided by the proposed meta-teacher. The colors ranging from blue to red denote
    the float numbers ranging from 0 to 1. Compared with handcrafted labels, the meta-teacher
    can explore reasonable pixel-wise supervision to train the PA detector without
    using human effort.'
  Figure 10 Link: articels_figures_by_rev_year\2021\MetaTeacher_For_Face_AntiSpoofing\figure_10.jpg
  Figure 10 caption: The ablation meta-teachers predictions for spoof faces from OULU-NPU.
    The colors ranging from blue to red denote the float values from zero to one.
  Figure 2 Link: articels_figures_by_rev_year\2021\MetaTeacher_For_Face_AntiSpoofing\figure_2.jpg
  Figure 2 caption: Using meta-teacher to supervise the PA detectors learning.
  Figure 3 Link: articels_figures_by_rev_year\2021\MetaTeacher_For_Face_AntiSpoofing\figure_3.jpg
  Figure 3 caption: "The training framework of MT-FAS. Each training iteration contains\
    \ a lower-level and a higher-level learning. In the lower-level (black dotted\
    \ arrow), the mini training data batch is simultaneously fed into the meta-teacher\
    \ M T t and the PA detector. Given training data, M T t provides pixel-wise supervision\
    \ for the detector. Be supervised by M T t , the detector with weight \u03B8 is\
    \ optimized to turn to newly detector with weight \u03B8 \u2217 (\u03C9) . The\
    \ optimizer in the lower-level learning is gradient descent with learning rate\
    \ \u03B1 . In the higher-level (the red arrow), the detector is evaluated on the\
    \ mini validation data batch using the pixel-wise answer provided by another meta-teacher\
    \ M T v . Then, the meta-teacher M T t is optimized using the detectors validation\
    \ loss L \u03A6 v ( \u03B8 \u2217 (\u03C9) . M T v is momentum updated via Eq.\
    \ (11). The PA detectors weight \u03B8 is updated by copying the weight \u03B8\
    \ \u2217 (\u03C9) , as the black arrow shows."
  Figure 4 Link: articels_figures_by_rev_year\2021\MetaTeacher_For_Face_AntiSpoofing\figure_4.jpg
  Figure 4 caption: Network structure of FAS-DR-Light. Each orange cube is the convolution
    layer and the number on it means the number of filters. The Pool layer is Max-pooling.
  Figure 5 Link: articels_figures_by_rev_year\2021\MetaTeacher_For_Face_AntiSpoofing\figure_5.jpg
  Figure 5 caption: Examples from OULU-NPU[59], CASIA-MFSD[60], Idiap Replay-Attack[61],
    MSU-MFSD[62], and SiW-M[19]. In each dataset, the faces in the leftmost column
    are live faces while all the other faces are spoof faces.
  Figure 6 Link: articels_figures_by_rev_year\2021\MetaTeacher_For_Face_AntiSpoofing\figure_6.jpg
  Figure 6 caption: Network structure of FAS-DR and FAS-DR-BC. FAS-DR is the network
    without the layers in the dashed box while FAS-DR-BC contains all layers in this
    figure. Each orange cube is the convolution layer and the number on it means the
    number of filters. The convolution layers in the dashed box use 2x2 stride while
    the other convolution layers use 1x1 stride. The Pool layer is Max-pooling with
    2x2 stride and GAP is the global average pooling.
  Figure 7 Link: articels_figures_by_rev_year\2021\MetaTeacher_For_Face_AntiSpoofing\figure_7.jpg
  Figure 7 caption: "The impact of the parameter \u03B3 towards the meta-teacher and\
    \ the trained FAS-DR detector. The x -axis denotes the value of \u03B3 and the\
    \ y -axis denotes ACER on protocol 1 of OULU-NPU."
  Figure 8 Link: articels_figures_by_rev_year\2021\MetaTeacher_For_Face_AntiSpoofing\figure_8.jpg
  Figure 8 caption: "(a) The architecture of the ResNet-8 backbone. (b) The inner\
    \ structure of the residual block. Pool: Maxpooling with stride of 2 and pooling\
    \ size of 2. Orange cube: convolution layer. BN: batch-normalization. Note that\
    \ the number on orange cube denotes the number of filters of the convolution layer.\
    \ The convolution layer on the shortcut path uses 1\xD71 stride while all the\
    \ other convolution layers use 3\xD73 stride. N is set to 16 in this work."
  Figure 9 Link: articels_figures_by_rev_year\2021\MetaTeacher_For_Face_AntiSpoofing\figure_9.jpg
  Figure 9 caption: The outputs of the meta-teacher for spoof faces from the three
    benchmarks. The colors ranging from blue to red denote the float values from zero
    to one.
  First author gender probability: 0.67
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Yunxiao Qin
  Name of the last author: Zhen Lei
  Number of Figures: 11
  Number of Tables: 6
  Number of authors: 6
  Paper title: Meta-Teacher For Face Anti-Spoofing
  Publication Date: 2021-06-22 00:00:00
  Table 1 caption: TABLE 1 Experimental Results on OULU-NPU [59]
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Experimental Results on SiW-M [19] With Leave-One-Attack-Out
    Protocol
  Table 3 caption: TABLE 3 Experimental Results on the Domain-Generalization Benchmark
  Table 4 caption: TABLE 4 Comparison Between the Meta-Teacher and Other Teachers
    on Protocol 4 of OULU-NPU [59]
  Table 5 caption: TABLE 5 Comparison Between the Meta-Teacher and Other Teachers
    on the Domain-Generalization Benchmark
  Table 6 caption: TABLE 6 Ablation Experimental Results on Protocol 1 of OULU-NPU
    [59]
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3091167
- Affiliation of the first author: bytedance ai lab, beijing, china
  Affiliation of the last author: institute for artificial intelligence, tsinghua
    university (thuai), beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Recent_Advances_in_Large_Margin_Learning\figure_1.jpg
  Figure 1 caption: "An example of the adversarial example which is misclassified\
    \ as purse by a ResNet-50 trained on ImageNet. We enlarge the perturbation by\
    \ 10\xD7 for better illustration in the middle picture."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Recent_Advances_in_Large_Margin_Learning\figure_2.jpg
  Figure 2 caption: The concerned classification margin is closely related to adversarial
    robustness and generalization ability, and we will introduce methods that aim
    to enlarge the margins for DNNs, by local linearization [35], [36], [37], certification
    [38], [39], [40], or relying on a margin in the decision space [41], [42], [43]
    in combination with some (possibly implicit) Lipschitz constraints.
  Figure 3 Link: articels_figures_by_rev_year\2021\Recent_Advances_in_Large_Margin_Learning\figure_3.jpg
  Figure 3 caption: "Different choices of of the key components result in different\
    \ penalty on instance-specific margins. More specifically, (a) min considers only\
    \ the worst case in the training set, (b) \u2211 combined with an identity function\
    \ treat all training samples equally, (c) and (d) incorporates the aggregation\
    \ function and shrinkage function, respectively."
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.63
  Name of the first author: Yiwen Guo
  Name of the last author: Changshui Zhang
  Number of Figures: 3
  Number of Tables: 1
  Number of authors: 2
  Paper title: Recent Advances in Large Margin Learning
  Publication Date: 2021-06-23 00:00:00
  Table 1 caption: TABLE 1 Large Margin Classification for Face Recognition
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3091717
- Affiliation of the first author: shanghai jiao tong university, shanghai, china
  Affiliation of the last author: peking university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Signed_Graph_Metric_Learning_via_Gershgorin_Disc_Perfect_Alignment\figure_1.jpg
  Figure 1 caption: GDPA-based optimization framework. BCD=Block coordinate descent.
    FW=Frank-Wolfe. LOBPCG=Locally optimal block preconditioned conjugate gradient.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Signed_Graph_Metric_Learning_via_Gershgorin_Disc_Perfect_Alignment\figure_2.jpg
  Figure 2 caption: Existing linear distance metric learning methods are classified
    based on contributions in modeling, optimization or joint modelingoptimization.
  Figure 3 Link: articels_figures_by_rev_year\2021\Signed_Graph_Metric_Learning_via_Gershgorin_Disc_Perfect_Alignment\figure_3.jpg
  Figure 3 caption: "Illustration of Gershgorin discs for matrix M in (5) (left),\
    \ and aligned discs for B=SM S \u22121 (right)."
  Figure 4 Link: articels_figures_by_rev_year\2021\Signed_Graph_Metric_Learning_via_Gershgorin_Disc_Perfect_Alignment\figure_4.jpg
  Figure 4 caption: Examples of a 3-node balanced signed graph (left) and an unbalanced
    signed graph (right).
  Figure 5 Link: articels_figures_by_rev_year\2021\Signed_Graph_Metric_Learning_via_Gershgorin_Disc_Perfect_Alignment\figure_5.jpg
  Figure 5 caption: Example of two 5-node balanced graphs. Node 1 has turned from
    blue to red from the left to the right.
  Figure 6 Link: articels_figures_by_rev_year\2021\Signed_Graph_Metric_Learning_via_Gershgorin_Disc_Perfect_Alignment\figure_6.jpg
  Figure 6 caption: Running time on Madelon of optimization methods PD-cone, HBNB,
    and SGML on objective functions MCML, DEML, LSML, LMNN and GLR. Labelled numbers
    denote the speed gain (faster (positive) or slower (negative)) of SGML over PD-cone.
  Figure 7 Link: articels_figures_by_rev_year\2021\Signed_Graph_Metric_Learning_via_Gershgorin_Disc_Perfect_Alignment\figure_7.jpg
  Figure 7 caption: Running time on Colon-cancer of optimization methods PD-cone,
    HBNB, and SGML on objective functions MCML, DEML, LSML, LMNN and GLR. Labelled
    numbers denote the speed gain (faster (positive) or slower (negative)) of SGML
    over PD-cone.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.75
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Cheng Yang
  Name of the last author: Wei Hu
  Number of Figures: 7
  Number of Tables: 7
  Number of authors: 3
  Paper title: Signed Graph Metric Learning via Gershgorin Disc Perfect Alignment
  Publication Date: 2021-06-23 00:00:00
  Table 1 caption: "TABLE 1 LMNN Objective Q( M \u2217 ) Q(M) by Minimizing Q(M)+\u03C4\
    ||M| | \u2217 Q(M)+\u03C4||M|| Using PD-Cone, and Objective Q(M) Q(M) by Minimizing\
    \ Q(M) Q(M) Directly Using SGML, for Sonar Dataset (60 Original Features and Ten\
    \ PCA-Transformed Features)"
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Tested Convex and (Partially) Differentiable Objective
    Functions Q(M) Q(M)s
  Table 3 caption: TABLE 3 Optimization Parameters and Convergence Thresholds
  Table 4 caption: TABLE 4 Converged Objective Values
  Table 5 caption: TABLE 5 Continuation of Table 4 on Other Datasets
  Table 6 caption: TABLE 6 Classification Accuracy (%) With a Ten-Nearest Neighbor
    Classifier
  Table 7 caption: TABLE 7 Continuation of Table 6 on Other Datasets
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3091682
- Affiliation of the first author: department of engineering science, visual geometry
    group, university of oxford, oxford, u.k.
  Affiliation of the last author: department of engineering science, visual geometry
    group, university of oxford, oxford, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2021\AutoNovel_Automatically_Discovering_and_Learning_Novel_Visual_Categories\figure_1.jpg
  Figure 1 caption: Novel category discovery. Given labelled images from a few known
    categories (e.g., dog and cat), our objective is to automatically partition unlabelled
    images from new categories (e.g., monkey and bird) into proper clusters.
  Figure 10 Link: articels_figures_by_rev_year\2021\AutoNovel_Automatically_Discovering_and_Learning_Novel_Visual_Categories\figure_10.jpg
  Figure 10 caption: Performance of different methods with different cluster number
    on ImageNet A . The ground truth is 30. We vary the cluster number from 20 to
    100.
  Figure 2 Link: articels_figures_by_rev_year\2021\AutoNovel_Automatically_Discovering_and_Learning_Novel_Visual_Categories\figure_2.jpg
  Figure 2 caption: Overview of the AutoNovel learning pipeline for novel category
    discovery. The first step is to learn an unbiased image representation via self-supervision
    using both labelled and unlabelled data. This learns well the early layers of
    the representation. The second step is to fine-tune only the last few layers of
    the representation using supervision on the labelled subset of the data. The final
    step is to use the fine-tuned representation, via ranking statistics, to induce
    clusters in the unlabelled data, while maintaining a good representation on the
    labelled set.
  Figure 3 Link: articels_figures_by_rev_year\2021\AutoNovel_Automatically_Discovering_and_Learning_Novel_Visual_Categories\figure_3.jpg
  Figure 3 caption: Ranking statistics. In this example, we consider top-3 ranks.
    As the top-3 ranks of z i and z j are the same, s ij =1 . While the top-3 ranks
    of z j and z k are the different, so s jk =0 .
  Figure 4 Link: articels_figures_by_rev_year\2021\AutoNovel_Automatically_Discovering_and_Learning_Novel_Visual_Categories\figure_4.jpg
  Figure 4 caption: Joint learning on labelled and unlabelled data.
  Figure 5 Link: articels_figures_by_rev_year\2021\AutoNovel_Automatically_Discovering_and_Learning_Novel_Visual_Categories\figure_5.jpg
  Figure 5 caption: Data split for category number estimation.
  Figure 6 Link: articels_figures_by_rev_year\2021\AutoNovel_Automatically_Discovering_and_Learning_Novel_Visual_Categories\figure_6.jpg
  Figure 6 caption: Performance evolution w.r.t. k for ranking statistics. We report
    results for k=1,2,3,5,7,10,15,20,50 .
  Figure 7 Link: articels_figures_by_rev_year\2021\AutoNovel_Automatically_Discovering_and_Learning_Novel_Visual_Categories\figure_7.jpg
  Figure 7 caption: Evolution of the t-SNE during the training of CIFAR-10. Performed
    on unlabelled data (i.e., instances of dog, frog, horse, ship, truck). Colors
    of data points denote their ground-truth labels.
  Figure 8 Link: articels_figures_by_rev_year\2021\AutoNovel_Automatically_Discovering_and_Learning_Novel_Visual_Categories\figure_8.jpg
  Figure 8 caption: "t-SNE on CIFAR10: impact of incremental Learning. Colors of data\
    \ points denote their ground-truth labels (\u201Cold\u201D classes 0-4; \u201C\
    new\u201D classes 5-9). We observe a bigger overlap in (a) between the \u201C\
    old\u201D class 3 and the \u201Cnew\u201D class 5 when not incorporating Incremental\
    \ Learning."
  Figure 9 Link: articels_figures_by_rev_year\2021\AutoNovel_Automatically_Discovering_and_Learning_Novel_Visual_Categories\figure_9.jpg
  Figure 9 caption: 'Confusion matrix on unlabelled classes of CIFAR10. Left: our
    method; Right: our method w I.L.'
  First author gender probability: 0.93
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.82
  Name of the first author: Kai Han
  Name of the last author: Andrea Vedaldi
  Number of Figures: 12
  Number of Tables: 13
  Number of authors: 4
  Paper title: 'AutoNovel: Automatically Discovering and Learning Novel Visual Categories'
  Publication Date: 2021-06-24 00:00:00
  Table 1 caption: TABLE 1 Data Splits in the Experiments
  Table 10 caption: TABLE 10 Different Methods for Pairwise Pseudo Labels
  Table 2 caption: TABLE 2 Ablation Study of AutoNovel
  Table 3 caption: TABLE 3 Novel Category Discovery Results on CIFAR10, CIFAR100,
    and SVHN
  Table 4 caption: TABLE 4 Novel Category Discovery Results on OmniGlot and ImageNet
  Table 5 caption: TABLE 5 Incremental Learning With the Novel Categories
  Table 6 caption: TABLE 6 Category Number Estimation on OmniGlot
  Table 7 caption: TABLE 7 Category Number Estimation Results
  Table 8 caption: TABLE 8 Novel Category Discovery With an Unknown Class Number C
    u Cu
  Table 9 caption: TABLE 9 Transferring From ImageNet to CIFAR10CIFAR100SVHN
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3091944
- Affiliation of the first author: faculty of engineering, ben-gurion university of
    the negev, beersheba, israel
  Affiliation of the last author: faculty of engineering, bar ilan university, ramat-gan,
    israel
  Figure 1 Link: articels_figures_by_rev_year\2021\Joint_Detection_and_Matching_of_Feature_Points_in_Multimodal_Images\figure_1.jpg
  Figure 1 caption: The multisensor patch matching problem. The matched optical (left)
    and IR (right) images differ by significant appearance changes due to the dissimilar
    physical characteristics captured by the different sensors. The images are part
    of the LWIR-RGB dataset [11]. The feature points in both images were detected
    using the proposed scheme.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Joint_Detection_and_Matching_of_Feature_Points_in_Multimodal_Images\figure_2.jpg
  Figure 2 caption: 'The proposed hybrid matching model. The model consists of two
    sub-networks: a siamese subnetwork and an asymmetric subnetwork with non-shared
    weights. The siamese branch consists of a pair of W s CNNs and is trained by the
    loss L s . The asymmetric branch consists of the W x and W y CNNs and is trained
    by the loss L A . The symmetric and asymmetric representations are merged and
    trained by the loss L H .'
  Figure 3 Link: articels_figures_by_rev_year\2021\Joint_Detection_and_Matching_of_Feature_Points_in_Multimodal_Images\figure_3.jpg
  Figure 3 caption: "Backtracking a 2\xD72 max-pooling layer. In the forward pass\
    \ (left to right), multiple spatial locations are mapped by the max-pooling layer\
    \ to a single spatial location. In the backtracking pass (right to left), the\
    \ maximum over all channels in a particular spatial location (red dot), is the\
    \ one propagated back."
  Figure 4 Link: articels_figures_by_rev_year\2021\Joint_Detection_and_Matching_of_Feature_Points_in_Multimodal_Images\figure_4.jpg
  Figure 4 caption: "Feature detection results. We report the Probability of Correct\
    \ Keypoint [42] and compare with handcrafted detectors and the D2-Net [35]. (a)\
    \ VIS-NIR dataset results. The images and detection maps are 680\xD71024 and 78\xD7\
    121, respectively. (b) CUHK dataset results. The images and detection maps are\
    \ 250\xD7200 and 24\xD718, respectively. (c) VEDAI dataset results. The images\
    \ and detection maps are 512\xD7512 and 57\xD757, respectively."
  Figure 5 Link: articels_figures_by_rev_year\2021\Joint_Detection_and_Matching_of_Feature_Points_in_Multimodal_Images\figure_5.jpg
  Figure 5 caption: Qualitative feature detection results. For each approach we show
    the 200 detections having the largest detection scores. The inliers are marked
    in green, while the outliers are marked in red. Please zoom-in to view the detected
    points.
  Figure 6 Link: articels_figures_by_rev_year\2021\Joint_Detection_and_Matching_of_Feature_Points_in_Multimodal_Images\figure_6.jpg
  Figure 6 caption: Qualitative feature detection results. For each approach we show
    the 200 detections having the largest detection scores. The inliers are marked
    in green, while the outliers are marked in red. Please zoom-in to view the detected
    points.
  Figure 7 Link: articels_figures_by_rev_year\2021\Joint_Detection_and_Matching_of_Feature_Points_in_Multimodal_Images\figure_7.jpg
  Figure 7 caption: Qualitative feature detection and matching results. For each approach
    we match the top 200 detections in the IR image to top 400 detections in the VIS
    image, based on the L 2 distance of their descriptors. The inliers that are correctly
    matched within a r=5 are marked in green, while the outliers are marked in red.
  Figure 8 Link: articels_figures_by_rev_year\2021\Joint_Detection_and_Matching_of_Feature_Points_in_Multimodal_Images\figure_8.jpg
  Figure 8 caption: Qualitative feature detection and matching results. For each approach
    we match the top 200 detections in the IR image to top 400 detections in the VIS
    image, based on the L 2 distance of their descriptors. The inliers that are correctly
    matched within a r=5 are marked in green, while the outliers are marked in red.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.67
  Name of the first author: Elad Ben Baruch
  Name of the last author: Yosi Keller
  Number of Figures: 8
  Number of Tables: 5
  Number of authors: 2
  Paper title: Joint Detection and Matching of Feature Points in Multimodal Images
  Publication Date: 2021-06-24 00:00:00
  Table 1 caption: TABLE 1 The CNN Architecture of the Sub-Networks Using the Contrastive
    Loss
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The CNN Architecture of the Sub-Networks Using the Cross-Entropy
    Loss
  Table 3 caption: TABLE 3 Patch Matching Results Evaluated Using the VIS-NIR Dataset
  Table 4 caption: TABLE 4 Patch Matching Results Evaluated Using the UVIS-NIR, VEDAI,
    and CUHK Datasets
  Table 5 caption: TABLE 5 Ablation Results Evaluated Using the VIS-NIR Dataset, Where
    the Patches Were Extracted Using a Uniform Lattice Layout as in En et al. [20]
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3092289
