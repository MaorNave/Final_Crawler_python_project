- Affiliation of the first author: national laboratory of pattern recognition, institute
    of automation, chinese academy of sciences, beijing, china
  Affiliation of the last author: department of computer science and information systems,
    birkbeck college, malet street, london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2014\Single_and_Multiple_Object_Tracking_Using_a_MultiFeature_Joint_Sparse_Representa\figure_1.jpg
  Figure 1 caption: An illustration of representation with object templates and trivial
    templates for a candidate observation.
  Figure 10 Link: articels_figures_by_rev_year\2014\Single_and_Multiple_Object_Tracking_Using_a_MultiFeature_Joint_Sparse_Representa\figure_10.jpg
  Figure 10 caption: The results of tracking an outdoor pedestrian in a noisy and
    cluttered environment.
  Figure 2 Link: articels_figures_by_rev_year\2014\Single_and_Multiple_Object_Tracking_Using_a_MultiFeature_Joint_Sparse_Representa\figure_2.jpg
  Figure 2 caption: The results of tracking a woman who is occluded by two men walking
    across her.
  Figure 3 Link: articels_figures_by_rev_year\2014\Single_and_Multiple_Object_Tracking_Using_a_MultiFeature_Joint_Sparse_Representa\figure_3.jpg
  Figure 3 caption: The results of tracking a face occluded by a book.
  Figure 4 Link: articels_figures_by_rev_year\2014\Single_and_Multiple_Object_Tracking_Using_a_MultiFeature_Joint_Sparse_Representa\figure_4.jpg
  Figure 4 caption: The results of tracking a car which undergoes large changes in
    scale and appearance.
  Figure 5 Link: articels_figures_by_rev_year\2014\Single_and_Multiple_Object_Tracking_Using_a_MultiFeature_Joint_Sparse_Representa\figure_5.jpg
  Figure 5 caption: The results of tracking a boat in a lake.
  Figure 6 Link: articels_figures_by_rev_year\2014\Single_and_Multiple_Object_Tracking_Using_a_MultiFeature_Joint_Sparse_Representa\figure_6.jpg
  Figure 6 caption: The results of tracking a car running in a dark night.
  Figure 7 Link: articels_figures_by_rev_year\2014\Single_and_Multiple_Object_Tracking_Using_a_MultiFeature_Joint_Sparse_Representa\figure_7.jpg
  Figure 7 caption: The results of tracking a woman who is partially occluded by cars.
  Figure 8 Link: articels_figures_by_rev_year\2014\Single_and_Multiple_Object_Tracking_Using_a_MultiFeature_Joint_Sparse_Representa\figure_8.jpg
  Figure 8 caption: The results of tracking a girl's head which undergoes large changes
    in appearance and severe occlusions.
  Figure 9 Link: articels_figures_by_rev_year\2014\Single_and_Multiple_Object_Tracking_Using_a_MultiFeature_Joint_Sparse_Representa\figure_9.jpg
  Figure 9 caption: The results of tracking a pedestrian with a small apparent size
    and gradual pose changes.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Weiming Hu
  Name of the last author: Stephen Maybank
  Number of Figures: 22
  Number of Tables: 5
  Number of authors: 4
  Paper title: Single and Multiple Object Tracking Using a Multi-Feature Joint Sparse
    Representation
  Publication Date: 2014-09-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Multi-Feature Joint Sparse Representation for an Observation
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Tracking Algorithm
  Table 3 caption:
    table_text: TABLE 3 The Mean of the RMSEs for All the Frames in Each of the Five
      Sequences
  Table 4 caption:
    table_text: TABLE 4 Quantitative Comparisons between Our Algorithm, Zhang's Algorithm,
      and Yang's Algorithm for the Third Sequence
  Table 5 caption:
    table_text: TABLE 5 Quantitative Results of Our Algorithm, Zhang's Algorithm,
      and Yang's Algorithm for the Fourth Sequence
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2353628
- Affiliation of the first author: department of electrical engineering and computer
    science, university of california at berkeley, berkeley, ca
  Affiliation of the last author: department of electrical engineering and computer
    science, university of california at berkeley, berkeley, ca
  Figure 1 Link: articels_figures_by_rev_year\2014\Generalized_Sparselet_Models_for_RealTime_Multiclass_Object_Recognition\figure_1.jpg
  Figure 1 caption: Overview diagram of object detection with sparselets. Once we
    evaluate the image with learned sparselets, the reconstruction phase can be done
    via efficient sparse matrix vector multiplications.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\Generalized_Sparselet_Models_for_RealTime_Multiclass_Object_Recognition\figure_2.jpg
  Figure 2 caption: 'Computation graph for a multiclass problem with K = 3 . Let the
    sparselet size be m and the number of blocks be p = 2 . We define bf w= (bf w1sf
    T , bf w2sf T , bf w3sf T )sf T in mathbb RKpm . Each per-class classifier bf
    wk in mathbb Rpm is partitioned into p blocks such that bf wk = (bf bk1sf T ,
    bf bk2sf T )sf T . An input vector bf x in mathbb Rpm is partitioned into subvectors
    such that bf x= (bf c1sf T , bf c2sf T )sf T . The feature map bf Phi(bf x,k)
    in mathbb RKpm is defined as: bf Phi(bf x,1) = (bf xsf T ,0,ldots ,0)sf T ; bf
    Phi(bf x,2) = (0,ldots ,0,bf xsf T ,0,ldots ,0)sf T ; bf Phi(bf x,3) = (0,ldots
    ,0,bf xsf T )sf T . The edges in the graph encode the dot products computed while
    solving rmargmaxk in lbrace 1,2,3rbrace bf wsf T bf Phi(bf x,k) .'
  Figure 3 Link: articels_figures_by_rev_year\2014\Generalized_Sparselet_Models_for_RealTime_Multiclass_Object_Recognition\figure_3.jpg
  Figure 3 caption: (Left) 128 of the 256 sparselets learned from 20 DPMs trained
    on the PASCAL VOC 2007 dataset. (Right) The top 16 sparselets activated for the
    motorbike category.
  Figure 4 Link: articels_figures_by_rev_year\2014\Generalized_Sparselet_Models_for_RealTime_Multiclass_Object_Recognition\figure_4.jpg
  Figure 4 caption: Reconstruction error for all 20 object categories from PASCAL
    2007 dataset as sparselet parameters are varied. The precomputation time is fixed
    in the top figure and the representation space is fixed on the bottom. Object
    categories are sorted by the reconstruction error by 6times 6 in the top figure
    and by 1times 1 in the bottom figure.
  Figure 5 Link: articels_figures_by_rev_year\2014\Generalized_Sparselet_Models_for_RealTime_Multiclass_Object_Recognition\figure_5.jpg
  Figure 5 caption: "Mean average precision (mAP) vs. sparsity for object detection\
    \ on the PASCAL 2007 dataset (left) and for nine classes from ImageNet (right).\
    \ The dictionary learned from the PASCAL detectors was used for the novel ImageNet\
    \ classes. \u201COriginal\u201D is the original linear model; \u201CReconstructive\
    \ sparselets\u201D is the baseline method from [8]; the remaining methods correspond\
    \ to discriminative learning [9] with each of the regularizers described in Section\
    \ 4.2."
  Figure 6 Link: articels_figures_by_rev_year\2014\Generalized_Sparselet_Models_for_RealTime_Multiclass_Object_Recognition\figure_6.jpg
  Figure 6 caption: Average classification accuracy vs. speedup factor for Caltech-101,
    256.
  Figure 7 Link: articels_figures_by_rev_year\2014\Generalized_Sparselet_Models_for_RealTime_Multiclass_Object_Recognition\figure_7.jpg
  Figure 7 caption: Run time comparison for DPM implementation on GPU, reconstructive
    sparselets and discriminatively activated sparselets in contrast to CPU cascade.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.9
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Hyun Oh Song
  Name of the last author: Trevor Darrell
  Number of Figures: 7
  Number of Tables: 1
  Number of authors: 6
  Paper title: Generalized Sparselet Models for Real-Time Multiclass Object Recognition
  Publication Date: 2014-09-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Statistics of Average Precision for All 20 Classes over Five
      Trials of Constructing the Dictionary from Five Randomly Chosen Classes (Five
      Different Dictionaries Per Class)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2353631
- Affiliation of the first author: "institute of information theory and automation\
    \ of the ascr, pod vod\xE1renskou v\u011B\u017E\xED 4, czech republic"
  Affiliation of the last author: "institute of information theory and automation\
    \ of the ascr, pod vod\xE1renskou v\u011B\u017E\xED 4, czech republic"
  Figure 1 Link: articels_figures_by_rev_year\2014\Projection_Operators_and_Moment_Invariants_to_Image_Blurring\figure_1.jpg
  Figure 1 caption: Blurred template (a) to be matched against a database (b). A typical
    situation where the convolution invariants can be employed.
  Figure 10 Link: articels_figures_by_rev_year\2014\Projection_Operators_and_Moment_Invariants_to_Image_Blurring\figure_10.jpg
  Figure 10 caption: The 7-fold PSF captured as the image of a LED diode.
  Figure 2 Link: articels_figures_by_rev_year\2014\Projection_Operators_and_Moment_Invariants_to_Image_Blurring\figure_2.jpg
  Figure 2 caption: "Top row: Examples of the real out-of-focus blur PSF at circular\
    \ aperture (left), at polygonal aperture formed by the diaphragm blades (middle),\
    \ and the ring-shaped PSF of a catadioptric objective (right). Bottom row: images\
    \ degraded by out-of-focus blur. The shape of the respective PSF can be observed\
    \ as an image of bright points in the out-of-focus background (this effect is\
    \ in photography called \u201Dbokeh\u201D).3"
  Figure 3 Link: articels_figures_by_rev_year\2014\Projection_Operators_and_Moment_Invariants_to_Image_Blurring\figure_3.jpg
  Figure 3 caption: "Performance of the projection operators. (a) original image f\
    \ , its projections (b) P 2 f , (c) P 4 f and (d) P \u221E f ."
  Figure 4 Link: articels_figures_by_rev_year\2014\Projection_Operators_and_Moment_Invariants_to_Image_Blurring\figure_4.jpg
  Figure 4 caption: The structure (a) of the invariant matrix and (b) of the PSF moment
    matrix. The gray elements are zero for any f and h . The white elements stand
    for non-trivial invariants in (a) and non-zero moments of the PSF in (b). Note
    the complementarity of both matrices (except the (0,0) element).
  Figure 5 Link: articels_figures_by_rev_year\2014\Projection_Operators_and_Moment_Invariants_to_Image_Blurring\figure_5.jpg
  Figure 5 caption: "The ratio between the invariants of the blurred image and of\
    \ the original. Blur mask of N=2 (elongated rectangle in this case). K 2 is a\
    \ perfect invariant while K 3 and K 4 vary significantly. The size of the original\
    \ was cca 1,000 \xD7 1,000 pixels."
  Figure 6 Link: articels_figures_by_rev_year\2014\Projection_Operators_and_Moment_Invariants_to_Image_Blurring\figure_6.jpg
  Figure 6 caption: "The leaves used in the experiment\u2014one sample per each species.\
    \ Top row: Aesculus hippocastanum (leaflet of palmately compound leaf), Ailanthus\
    \ altissima (leaflet of pinnately compound leaf), Betula papyrifera, Chaenomeles\
    \ japonica, Cornus alba. Middle row: Cotoneaster integerrimus, Deutzia scabra,\
    \ Fagus sylvatica, Fraxinus excelsior (leaflet of pinnately compound leaf). Bottom\
    \ row: Gymnocladus dioicus (leaflet of pinnately compound leaf), Juglans regia\
    \ (leaflet of pinnately compound leaf), Lonicera involucrata, Prunus laurocerasus\
    \ and Staphylea pinnata (leaflet of pinnately compound leaf)."
  Figure 7 Link: articels_figures_by_rev_year\2014\Projection_Operators_and_Moment_Invariants_to_Image_Blurring\figure_7.jpg
  Figure 7 caption: Examples of the blurred leaves.
  Figure 8 Link: articels_figures_by_rev_year\2014\Projection_Operators_and_Moment_Invariants_to_Image_Blurring\figure_8.jpg
  Figure 8 caption: "The feature space of two non-invariant moments c11 and c22 .\
    \ Legend: \u2013 Aesculus hippocastanum, \u2013 Ailanthus altissima, \u2013 Betula\
    \ papyrifera, \u2013 Cornus alba, \u2013 Cotoneaster integerrimus, \u2013 Deutzia\
    \ scabra, \u2013 Euonymus europaea, \u2013 Fagus sylvatica, \u2013 Fraxinus excelsior,\
    \ \u2013 Gymnocladus dioicus, \u2013 Chaenomeles japonica, \u2013 Juglans regia,\
    \ \u2013 Lonicera involucrata, \u2013 Prunus laurocerasus, \u2013 Staphylea pinnata."
  Figure 9 Link: articels_figures_by_rev_year\2014\Projection_Operators_and_Moment_Invariants_to_Image_Blurring\figure_9.jpg
  Figure 9 caption: "The feature space of real parts of the invariant mathcal Rmathnormal\
    \ eKinfty (3,0) and mathcal Rmathnormal eKinfty (4,0) . Each cluster is formed\
    \ by ten blurred images and one training image of the leaf. Legend: \u2013 Aesculus\
    \ hippocastanum, \u2013 Ailanthus altissima, \u2013 Betula papyrifera, \u2013\
    \ Cornus alba, \u2013 Cotoneaster integerrimus, \u2013 Deutzia scabra, \u2013\
    \ Euonymus europaea, \u2013 Fagus sylvatica, \u2013 Fraxinus excelsior, \u2013\
    \ Gymnocladus dioicus, \u2013 Chaenomeles japonica, \u2013 Juglans regia, \u2013\
    \ Lonicera involucrata, \u2013 Prunus laurocerasus, \u2013 Staphylea pinnata."
  First author gender probability: 0.91
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Jan Flusser
  Name of the last author: "Barbara Zitov\xE1"
  Number of Figures: 15
  Number of Tables: 1
  Number of authors: 4
  Paper title: Projection Operators and Moment Invariants to Image Blurring
  Publication Date: 2014-09-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Number of Non-Trivial N -Fold Blur Invariants Up to the
      Order r
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2353644
- Affiliation of the first author: pattern analysis and computer vision (pavis), istituto
    italiano di tecnologia, genova, italy
  Affiliation of the last author: department of computer science, university of verona,
    verona, italy
  Figure 1 Link: articels_figures_by_rev_year\2014\Joint_IndividualGroup_Modeling_for_Tracking\figure_1.jpg
  Figure 1 caption: Models for joint individual-group tracking.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\Joint_IndividualGroup_Modeling_for_Tracking\figure_2.jpg
  Figure 2 caption: Graphical model that represents a Dirichlet Process Mixture Model.
  Figure 3 Link: articels_figures_by_rev_year\2014\Joint_IndividualGroup_Modeling_for_Tracking\figure_3.jpg
  Figure 3 caption: Computation of the state estimate Fboldsymbol tildeTheta t that
    deals with discrete labels.
  Figure 4 Link: articels_figures_by_rev_year\2014\Joint_IndividualGroup_Modeling_for_Tracking\figure_4.jpg
  Figure 4 caption: Qualitative results on the real FM dataset (S04 and S14) comparing
    the DP 2 -JIGT and the data association-based baseline. Better printed in color.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.96
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Loris Bazzani
  Name of the last author: Vittorio Murino
  Number of Figures: 4
  Number of Tables: 4
  Number of authors: 4
  Paper title: Joint Individual-Group Modeling for Tracking
  Publication Date: 2014-09-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Results on the Synthetic FM Dataset Excluding the Queue Sequences
      (See Text for the Details)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results on the Real FM Dataset Excluding the Queue Sequences
      (See the Text for the Details)
  Table 3 caption:
    table_text: TABLE 3 Results of Individual Tracking for the Real FM Dataset Excluding
      the Queue Sequences (See the Text for the Details)
  Table 4 caption:
    table_text: TABLE 4 Group Results on the BIWI Dataset
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2353641
- Affiliation of the first author: pattern recognition and intelligent system lab.,
    beijing university of posts and telecommunications, beijing, china
  Affiliation of the last author: pattern recognition and intelligent system lab.,
    beijing university of posts and telecommunications, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2014\Variational_Bayesian_Matrix_Factorization_for_Bounded_Support_Data\figure_1.jpg
  Figure 1 caption: Graphical model of the BG-NMF. All the dashed circles in the graphical
    figure represent variables. Apk , Bpk , and Hkt are assumed to be gamma distributed.
    Xpt is assumed to be beta distributed with parameter apt and bpt . Arrows show
    the relationship between variables. The variables in the box are independent from
    each other.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\Variational_Bayesian_Matrix_Factorization_for_Bounded_Support_Data\figure_2.jpg
  Figure 2 caption: "Comparison of lnx and \u03C8(x) ."
  Figure 3 Link: articels_figures_by_rev_year\2014\Variational_Bayesian_Matrix_Factorization_for_Bounded_Support_Data\figure_3.jpg
  Figure 3 caption: "Illustration of the convergence of the BG-NMF algorithm. We randomly\
    \ selected 80 images from the Olivetti faces database [46] and downsampled them\
    \ to size 32\xD732 . We set K=10 , 20 , and 30 . The algorithm could always converge\
    \ after about 60\u223C80 rounds of iterations. The objective function is numerically\
    \ calculated by generating samples from the posterior distributions."
  Figure 4 Link: articels_figures_by_rev_year\2014\Variational_Bayesian_Matrix_Factorization_for_Bounded_Support_Data\figure_4.jpg
  Figure 4 caption: "Synthetic source separation example. Each column is rearranged\
    \ into a 6\xD76 matrix for visual clarity. The order of the matrices has also\
    \ been rearranged for the purpose of easy comparison. See Section 4.1 for more\
    \ details."
  Figure 5 Link: articels_figures_by_rev_year\2014\Variational_Bayesian_Matrix_Factorization_for_Bounded_Support_Data\figure_5.jpg
  Figure 5 caption: Source separation with images. See Section 4.1.
  Figure 6 Link: articels_figures_by_rev_year\2014\Variational_Bayesian_Matrix_Factorization_for_Bounded_Support_Data\figure_6.jpg
  Figure 6 caption: Examples from the missing pixels prediction. See Section 4.2.
  Figure 7 Link: articels_figures_by_rev_year\2014\Variational_Bayesian_Matrix_Factorization_for_Bounded_Support_Data\figure_7.jpg
  Figure 7 caption: "RPBMM clustering results based on the transposed BG-NMF pseudo-basis\
    \ matrix. The 1st bar from the top is the normal (grey) and breast cancer (black)\
    \ examples. The clustering results are shown in the 2nd bar from the top. The\
    \ heatmap shows the pseudo-basis matrix of the BG-NMF. The heatmap itself has\
    \ been standardized so that blue indicates high level ( \u223C1 ), and yellow\
    \ denotes low level ( \u223C0 )."
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Zhanyu Ma
  Name of the last author: Jun Guo
  Number of Figures: 7
  Number of Tables: 3
  Number of authors: 6
  Paper title: Variational Bayesian Matrix Factorization for Bounded Support Data
  Publication Date: 2014-09-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparisons of the Approximation Accuracy of the LIB Function
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparisons of BG-NMF, PMF, BPMF, and Sparse PCA
  Table 3 caption:
    table_text: TABLE 3 RPBMM Clustering Details for all the 136 Samples over 14 BG-NMF
      Pseudo-Basis Vectors
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2353639
- Affiliation of the first author: department of computing, imperial college london,
    london, 180 queen's gate, uk
  Affiliation of the last author: department of computing, imperial college london,
    london, 180 queen's gate, uk
  Figure 1 Link: articels_figures_by_rev_year\2014\ContextSensitive_Dynamic_Ordinal_Regression_for_Intensity_Estimation_of_Facial_A\figure_1.jpg
  Figure 1 caption: Relationship between the scale of facial appearance change and
    intensity levels when evidence of an AU is present [3].
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\ContextSensitive_Dynamic_Ordinal_Regression_for_Intensity_Estimation_of_Facial_A\figure_2.jpg
  Figure 2 caption: The proposed cs-CORF model. The input to the model are the time-varying
    CFE covariates ( xijr ) and the constant (on the sequence level) CRE covariates
    ( xiu ), used to model the context-questions how and who, respectively. These
    effects are linearly related to the latent variable zi , contaminated by Gaussian
    noise with zero mean and variance defined as the sum of the CRE ( sigma u2(xiu)
    ) and CFE ( sigma r2(xiir) ) heteroscedastic variance, as well as sigma o2 that
    accounts for unexplained variance in the data. The latent variable zi is non-linearly
    mapped to the ordinal labels yi via the probit link function, used to define the
    node potentials of the cs-CORF model. The context question when is modeled by
    encoding the first-order temporal dependences between the intensity levels via
    the edge potentials of the model.
  Figure 3 Link: articels_figures_by_rev_year\2014\ContextSensitive_Dynamic_Ordinal_Regression_for_Intensity_Estimation_of_Facial_A\figure_3.jpg
  Figure 3 caption: Examples of AUs available in Shoulder-Pain and DISFA datasets.
    The images are obtained from http:www.cs.cmu.edu~facefacs.htm.
  Figure 4 Link: articels_figures_by_rev_year\2014\ContextSensitive_Dynamic_Ordinal_Regression_for_Intensity_Estimation_of_Facial_A\figure_4.jpg
  Figure 4 caption: Distribution of intensity levels in the AU data used from the
    Shoulder-pain (left) and DISFA ( right) datasets.
  Figure 5 Link: articels_figures_by_rev_year\2014\ContextSensitive_Dynamic_Ordinal_Regression_for_Intensity_Estimation_of_Facial_A\figure_5.jpg
  Figure 5 caption: The intensity estimation of pain from two example sequences of
    facial expressions from the Shoulder-pain dataset, attained by cs-CORF(w+h) and
    base CORF(w). The upper row shows true (dashed blue) and predicted ( solid red)
    labels by the two models. The middle row shows the ordinal projections of the
    inputs ( solid black), with their standard deviation sigma ( grey), and the scaled
    thresholds (dashed red). For cs-CORF(w+h), we also plot the context-induced 'bias'
    (solid blue). The bottom row shows the probability of each pain intensity level
    per frame.
  Figure 6 Link: articels_figures_by_rev_year\2014\ContextSensitive_Dynamic_Ordinal_Regression_for_Intensity_Estimation_of_Facial_A\figure_6.jpg
  Figure 6 caption: The (normalized) confusion matrices computed from the true and
    predicted intensity labels, the latter being obtained by the denoted models, for
    AU6 and AU25 from the DISFA dataset. Note that the lower the OCI score, the better
    performance.
  Figure 7 Link: articels_figures_by_rev_year\2014\ContextSensitive_Dynamic_Ordinal_Regression_for_Intensity_Estimation_of_Facial_A\figure_7.jpg
  Figure 7 caption: The true (dashed blue) and predicted (solid red) intensity of
    AU6 and AU25 from the DISFA dataset. The sequences shown are obtained by concatenation
    of several exemplary sequences corresponding to different test subjects. The scores
    shown at the top of each figure are computed from the depicted sequences. For
    RVM, we also include the continuous estimation of AU intensity (dashed black).
  Figure 8 Link: articels_figures_by_rev_year\2014\ContextSensitive_Dynamic_Ordinal_Regression_for_Intensity_Estimation_of_Facial_A\figure_8.jpg
  Figure 8 caption: 'Cross-dataset registration: (a) DISFA to Shoulder-pain, and (b)
    Shoulder-pain to DISFA. The reference face is calculated as the average of the
    points registered within the datasets (red) that are used to train the models.
    The registered points of the test dataset (black) are obtained by using an affine
    transform that maps the test points to the reference face of the training set.'
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.99
  Name of the first author: Ognjen Rudovic
  Name of the last author: Maja Pantic
  Number of Figures: 8
  Number of Tables: 3
  Number of authors: 3
  Paper title: Context-Sensitive Dynamic Ordinal Regression for Intensity Estimation
    of Facial Action Units
  Publication Date: 2014-09-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average Performance of the Models Tested on 23 Intensity Estimation
      Problems (pain + 10 AUs from Shoulder-Pain Dataset and 12 AUs from DISFA Dataset)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Performance of the Models on Intensity Estimation of pain
      (P) and 11 AUs from the Sholder-Pain Dataset, and 12 AUs from the DISFA Dataset
  Table 3 caption:
    table_text: TABLE 3 Cross-Datasets Evaluation of the Models on Seven AUs Present
      in Both Datasets
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2356192
- Affiliation of the first author: "finland and the department of mathematics, \xE5\
    bo akademi university, aalto university, turku, finland"
  Affiliation of the last author: "department of mathematics, \xE5bo akademi university,\
    \ turku, finland"
  Figure 1 Link: articels_figures_by_rev_year\2014\A_Bayesian_Predictive_Model_for_Clustering_Data_of_Mixed_Discrete_and_Continuous\figure_1.jpg
  Figure 1 caption: Impurity profile of amphetamine sample.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\A_Bayesian_Predictive_Model_for_Clustering_Data_of_Mixed_Discrete_and_Continuous\figure_2.jpg
  Figure 2 caption: Comparison between real (left panel) and simulated (right panel)
    amphetamine data. Rows correspond to individual samples and columns to features.
    Zero values are shown in blue and the highest values in red.
  Figure 3 Link: articels_figures_by_rev_year\2014\A_Bayesian_Predictive_Model_for_Clustering_Data_of_Mixed_Discrete_and_Continuous\figure_3.jpg
  Figure 3 caption: Clustering solution for amphetamine data.
  Figure 4 Link: articels_figures_by_rev_year\2014\A_Bayesian_Predictive_Model_for_Clustering_Data_of_Mixed_Discrete_and_Continuous\figure_4.jpg
  Figure 4 caption: Obtained cluster means for labels 0, 1 and 2 of the MNIST test
    dataset.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Paul Blomstedt
  Name of the last author: Jukka Corander
  Number of Figures: 4
  Number of Tables: 2
  Number of authors: 5
  Paper title: A Bayesian Predictive Model for Clustering Data of Mixed Discrete and
    Continuous Type
  Publication Date: 2014-09-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average ARI (sd's in Parentheses) for Simulated Datasets with
      20 Clusters and a Varying Number of Features
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average ARI (sd's in Parentheses) for Second Simulation Experiment
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2359431
- Affiliation of the first author: department of electrical and computer engineering,
    the university of texas at austin, austin, tx
  Affiliation of the last author: department of electrical and computer engineering,
    the university of texas at austin, austin, tx
  Figure 1 Link: articels_figures_by_rev_year\2014\SemiSupervised_Affinity_Propagation_with_Soft_InstanceLevel_Constraints\figure_1.jpg
  Figure 1 caption: Soft-constraint semi-supervised affinity propagation. The cannot-link
    and must-link factor nodes are not present in the classical AP formulation. In
    this graph, the pair (i,k) is in the set of cannot-link constraints and the pair
    (i,m) is in the set of must-link constraints, i,j,k,l,m in lbrace 1,2,ldots ,Nrbrace
    , where N denotes the number of instances. Here Sij is the similarity between
    i th and j th instance, and cij indicates whether the j th instance is the exemplar
    for the i th instance.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\SemiSupervised_Affinity_Propagation_with_Soft_InstanceLevel_Constraints\figure_2.jpg
  Figure 2 caption: Messages in soft-constraint semi-supervised affinity propagation.
    The alpha , rho , eta , and beta messages are as same as those in the classical
    AP derived for the binary factor graph. Here i,j,k,l,m in lbrace 1,2,ldots ,Nrbrace
    , where N is the number of instances.
  Figure 3 Link: articels_figures_by_rev_year\2014\SemiSupervised_Affinity_Propagation_with_Soft_InstanceLevel_Constraints\figure_3.jpg
  Figure 3 caption: Modified Rand index for SCSSAP (o), Givoni and Frey's SSAP (blue),
    Leone et al.'s SSAP (orange), and unsupervised AP (red). The darkness of the SCSSAP
    curve indicates magnitude of the penalty parameter, where the darkest curve is
    for exp (-q)=0 .
  Figure 4 Link: articels_figures_by_rev_year\2014\SemiSupervised_Affinity_Propagation_with_Soft_InstanceLevel_Constraints\figure_4.jpg
  Figure 4 caption: Modified Rand index for SCSSAP, Givoni and Frey's SSAP (blue),
    Leone et al.'s SSAP (orange), and unsupervised AP (red) in the presence of 5 percent
    (o) and 10 percent ( ) noisy constraints. The darkness of the SCSSAP curve indicates
    the magnitude of the penalty parameter, where the darkest curve is for exp (-q)=0
    .
  Figure 5 Link: articels_figures_by_rev_year\2014\SemiSupervised_Affinity_Propagation_with_Soft_InstanceLevel_Constraints\figure_5.jpg
  Figure 5 caption: Modified Rand index for SCSSAP, Givoni and Frey's SSAP (blue),
    Leone et al.'s SSAP (orange), and unsupervised AP (red) in the presence of 10%
    (o) and 20% ( ) noisy labels. The darkness of the SCSSAP curve indicates the magnitude
    of the penalty parameter, where the darkest curve is for exp (-q)=0 .
  Figure 6 Link: articels_figures_by_rev_year\2014\SemiSupervised_Affinity_Propagation_with_Soft_InstanceLevel_Constraints\figure_6.jpg
  Figure 6 caption: SCSSAP (circles) and constrained EM (lines) modified Rand index
    for datasets without noise (black), 5 percent constraint noise (red), and 10 percent
    label noise (blue).
  Figure 7 Link: articels_figures_by_rev_year\2014\SemiSupervised_Affinity_Propagation_with_Soft_InstanceLevel_Constraints\figure_7.jpg
  Figure 7 caption: SCSSAP without metric learning (black), with barg in the objective
    (blue) and with hatg in the objective (green). Empty circles correspond to a single
    metric for the entire dataset, while filled circles correspond to cluster-specific
    metrics.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Natalia M. Arzeno
  Name of the last author: Haris Vikalo
  Number of Figures: 7
  Number of Tables: 0
  Number of authors: 2
  Paper title: Semi-Supervised Affinity Propagation with Soft Instance-Level Constraints
  Publication Date: 2014-09-19 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2359454
- Affiliation of the first author: school of electrical and electronics engineering,
    nanyang technological university, nanyang link, singapore
  Affiliation of the last author: school of electrical and electronics engineering,
    nanyang technological university, nanyang link, singapore
  Figure 1 Link: articels_figures_by_rev_year\2014\Sparse_and_Dense_Hybrid_Representation_via_Dictionary_Decomposition_for_Face_Rec\figure_1.jpg
  Figure 1 caption: 'Image decomposition: a face image projected to the class-specific
    subspace, the non-class-specific subspace and its residual random sparse noise.'
  Figure 10 Link: articels_figures_by_rev_year\2014\Sparse_and_Dense_Hybrid_Representation_via_Dictionary_Decomposition_for_Face_Rec\figure_10.jpg
  Figure 10 caption: Examples of images of Extended Yale B database with different
    levels of block occlusion. From left to right, 20, 30, 40 and 50 percent of images
    are occluded, respectively.
  Figure 2 Link: articels_figures_by_rev_year\2014\Sparse_and_Dense_Hybrid_Representation_via_Dictionary_Decomposition_for_Face_Rec\figure_2.jpg
  Figure 2 caption: 'Comparison of SRC ( 1 th row) and the proposed SDR ( 2 nd row)
    on AR database mathbf D of 100 persons with three images per person: (a) query
    image; (b) SRC coefficients of mathbf D and SDR coefficients of mathbf A and mathbf
    B ; (c) top 16 images in mathbf D of SRC and in mathbf A and mathbf B of SDR;
    (d) residuals. Sparse coefficients of the correct person are in red lines and
    its training samples are in yellow rectangles. Images are normalized into the
    same mean and ell 2 -norm.'
  Figure 3 Link: articels_figures_by_rev_year\2014\Sparse_and_Dense_Hybrid_Representation_via_Dictionary_Decomposition_for_Face_Rec\figure_3.jpg
  Figure 3 caption: 'Sample images produced by the proposed SLR: images of a person
    in mathbf D corrupted by random noise (a); their initial components assigned to
    mathbf A (b) and mathbf B (c); final images in mathbf A (d), mathbf Bmathbf X
    (e) and mathbf E (f).'
  Figure 4 Link: articels_figures_by_rev_year\2014\Sparse_and_Dense_Hybrid_Representation_via_Dictionary_Decomposition_for_Face_Rec\figure_4.jpg
  Figure 4 caption: 'Examples of RPCA and the proposed SLR: training images (a); low-rank
    (b) and sparse (c) images from RPCA; class-specific (d), non-class-specific (e)
    and sparse (f) images decomposed by SLR.'
  Figure 5 Link: articels_figures_by_rev_year\2014\Sparse_and_Dense_Hybrid_Representation_via_Dictionary_Decomposition_for_Face_Rec\figure_5.jpg
  Figure 5 caption: Normalized images from FERET database.
  Figure 6 Link: articels_figures_by_rev_year\2014\Sparse_and_Dense_Hybrid_Representation_via_Dictionary_Decomposition_for_Face_Rec\figure_6.jpg
  Figure 6 caption: Sample images in Multi-PIE database with variations of (a) illumination,
    (b) expression, (c) pose.
  Figure 7 Link: articels_figures_by_rev_year\2014\Sparse_and_Dense_Hybrid_Representation_via_Dictionary_Decomposition_for_Face_Rec\figure_7.jpg
  Figure 7 caption: Face recognition rate versus number of training illuminations
    per subject on Multi-PIE database.
  Figure 8 Link: articels_figures_by_rev_year\2014\Sparse_and_Dense_Hybrid_Representation_via_Dictionary_Decomposition_for_Face_Rec\figure_8.jpg
  Figure 8 caption: Face recognition rate versus number of training expressions per
    subject on Multi-PIE database.
  Figure 9 Link: articels_figures_by_rev_year\2014\Sparse_and_Dense_Hybrid_Representation_via_Dictionary_Decomposition_for_Face_Rec\figure_9.jpg
  Figure 9 caption: Face recognition rate versus percentage of uniform noise on AR
    database.
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Xudong Jiang
  Name of the last author: Jian Lai
  Number of Figures: 11
  Number of Tables: 6
  Number of authors: 2
  Paper title: Sparse and Dense Hybrid Representation via Dictionary Decomposition
    for Face Recognition
  Publication Date: 2014-09-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Face Recognition Rate on Extended Yale B
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Face Recognition Rate on the Multi-PIE Face Database with
      Pose Variation.
  Table 3 caption:
    table_text: TABLE 3 Recognition Rate on All Undisguised Images of AR Dataset
  Table 4 caption:
    table_text: TABLE 4 Recognition Rate of Fewer Training Samples of AR Dataset
  Table 5 caption:
    table_text: TABLE 5 Face Recognition Rate on FERET Database
  Table 6 caption:
    table_text: TABLE 6 Face Recognition Rate on the AR Database
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2359453
- Affiliation of the first author: school of electrical and computer engineering,
    cornell university, ithaca, ny
  Affiliation of the last author: school of electrical and computer engineering, cornell
    university, ithaca, ny
  Figure 1 Link: articels_figures_by_rev_year\2014\D_Reasoning_from_Blocks_to_Stability\figure_1.jpg
  Figure 1 caption: (a) The input RGB-D image. (b) Initial segmentation from RGB-D
    data. (c) A 3D bounding box is fit to the 3D point clouds of each segment, and
    several features are extracted for reasoning about stability. Unstable boxes are
    labeled in red. (d) The segmentation is updated based on the stability analysis
    and produces a better segmentation and a stable box representation.
  Figure 10 Link: articels_figures_by_rev_year\2014\D_Reasoning_from_Blocks_to_Stability\figure_10.jpg
  Figure 10 caption: (a) 3D oriented bounding boxes can be ill-fit because of noise,
    and this may lead to incorrect support relation inference. For example, between
    object A and B, a partial on-top support is proposed, although it should have
    been a surface on-top support. (b) After stability reasoning, we adjust the higher
    box if it is only supported from beneath, and then correct the support relation
    accordingly.
  Figure 2 Link: articels_figures_by_rev_year\2014\D_Reasoning_from_Blocks_to_Stability\figure_2.jpg
  Figure 2 caption: An overview of our algorithm.
  Figure 3 Link: articels_figures_by_rev_year\2014\D_Reasoning_from_Blocks_to_Stability\figure_3.jpg
  Figure 3 caption: (a) A bounding box fit based on minimum volume may not be a good
    representation for RGB-D images, where only partially observed 3D data is available.
    (b) A better fit box not only occupies a small volume, but also has many 3D points
    near the box surface. Data points are projected to 2D for illustration.
  Figure 4 Link: articels_figures_by_rev_year\2014\D_Reasoning_from_Blocks_to_Stability\figure_4.jpg
  Figure 4 caption: (a) To fit the 3D points, we use RANSAC to find the first plane
    S 1 . (3D points are projected on 2D for a simpler illustration, and the plane
    S 1 is presented as red line). (b) For the 3D points that do not belong to S 1
    , we fit another plane S 2 to them, enforcing that S 2 is perpendicular to S 1
    .
  Figure 5 Link: articels_figures_by_rev_year\2014\D_Reasoning_from_Blocks_to_Stability\figure_5.jpg
  Figure 5 caption: Given the camera position and a proposed bounding box, we determine
    the visible surfaces of the box, shown as a solid parallel black line to the box
    surface. (a) This box may give a compact fit, but most of the points lie on the
    hidden surfaces. (b) With a better box fit, most of the points lie on the visible
    surfaces of the two boxes.
  Figure 6 Link: articels_figures_by_rev_year\2014\D_Reasoning_from_Blocks_to_Stability\figure_6.jpg
  Figure 6 caption: (a) Well-fit boxes should not intersect much with neighboring
    boxes. (b) If two segments are merged incorrectly, e.g., the two books in the
    image, then the new box fit to the segment is likely to intersect with neighboring
    boxes, e.g., the box shown in red.
  Figure 7 Link: articels_figures_by_rev_year\2014\D_Reasoning_from_Blocks_to_Stability\figure_7.jpg
  Figure 7 caption: 'Separating axis theorem in 2D: (a) in order to separate two boxes,
    we choose an axis perpendicular to any of the edges, and project all the vertices
    to this rotated axis. (b) If two bounding boxes are separate, there exists an
    axis that has a zero overlap distance ( D in the image). We examine all the possible
    axis (in this case four possibilities, two for each box), and choose the minimum
    overlap distance. This gives the orientation and the minimum distance required
    to separate two boxes.'
  Figure 8 Link: articels_figures_by_rev_year\2014\D_Reasoning_from_Blocks_to_Stability\figure_8.jpg
  Figure 8 caption: '(a) to (c): Three different supporting relations: (a) surface
    on-top support (black arrow); (b) partial on-top support (red arrow); (c) side
    support (blue arrow). Different supporting relations give different supporting
    areas as plotted in red dashed circles. (d) to (e): stability reasoning: (e) considering
    only the top two boxes, the center of the gravity (in black dashed line) intersects
    the supporting area (in red dashed circle), and appears (locally) stable. (e)
    When proceeding further down, the new center of the gravity does not intersect
    the supporting area, and the configuration is found to be unstable. (f) to (g)
    supporting area with multi-support: (f) one object can be supported by multiple
    other objects. (g) The supporting area projected on the ground is the convex hull
    of all the supporting areas.'
  Figure 9 Link: articels_figures_by_rev_year\2014\D_Reasoning_from_Blocks_to_Stability\figure_9.jpg
  Figure 9 caption: (a) Near-touching objects, e.g., objects A and C do not necessarily
    support one another. (b) After stability reasoning, we find that object A can
    be fully supported by object B beneath it through a surface on-top support. Therefore,
    we delete the unnecessary side support between A and C.
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.64
  Name of the first author: Zhaoyin Jia
  Name of the last author: Tsuhan Chen
  Number of Figures: 20
  Number of Tables: 4
  Number of authors: 4
  Paper title: 3D Reasoning from Blocks to Stability
  Publication Date: 2014-09-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Features for Single and Pairwise Potentials
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average Angle Error on the Bounding Box Orientation Given
      Ground-Truth Segmentations
  Table 3 caption:
    table_text: TABLE 3 Supporting Relation Accuracy for Different Datasets
  Table 4 caption:
    table_text: TABLE 4 Pixel-Wise Segmentation Score
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2359435
