- Affiliation of the first author: school of computer science, fudan university, shanghai,
    china
  Affiliation of the last author: department of electrical engineering, columbia university,
    new york, ny
  Figure 1 Link: articels_figures_by_rev_year\2017\Exploiting_Feature_and_Class_Relationships_in_Video_Categorization_with_Regulari\figure_1.jpg
  Figure 1 caption: Illustration of the proposed rDNN framework for video categorization.
    See texts for more explanations.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Exploiting_Feature_and_Class_Relationships_in_Video_Categorization_with_Regulari\figure_2.jpg
  Figure 2 caption: 'Popular neural network structures: (a) Is the standard one-vs-all
    training scheme using a single type of feature; (b) is the popular structure used
    in multi-class learning with a single type of feature; (c) is the one-vs-all training
    scheme using multiple types of features; and (d) processes multiple features separately
    and then performs fusion using a middle layer [31].'
  Figure 3 Link: articels_figures_by_rev_year\2017\Exploiting_Feature_and_Class_Relationships_in_Video_Categorization_with_Regulari\figure_3.jpg
  Figure 3 caption: "The learned class relationship matrix \u03A9 on FCVID and example\
    \ frames of a few category groups. Many of the found groups contain categories\
    \ that share visualauditory commonalities but do not necessarily co-occur."
  Figure 4 Link: articels_figures_by_rev_year\2017\Exploiting_Feature_and_Class_Relationships_in_Video_Categorization_with_Regulari\figure_4.jpg
  Figure 4 caption: "(a) The learned class relationship matrix \u03A9 on CCV. (b)\
    \ Confusion matrix on CCV."
  Figure 5 Link: articels_figures_by_rev_year\2017\Exploiting_Feature_and_Class_Relationships_in_Video_Categorization_with_Regulari\figure_5.jpg
  Figure 5 caption: Performance of different numbers of training samples. We plot
    the results of the DNN baseline without regularization, rDNN-F, rDNN-C and the
    rDNN exploiting both types of relationships. The best mAP on the three datasets
    (the rDNN approach using all the training samples) are 66.9, 73.5 and 76.0 percent
    respectively.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.85
  Name of the first author: Yu-Gang Jiang
  Name of the last author: Shih-Fu Chang
  Number of Figures: 5
  Number of Tables: 4
  Number of authors: 5
  Paper title: Exploiting Feature and Class Relationships in Video Categorization
    with Regularized Deep Neural Networks
  Publication Date: 2017-02-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance Comparison (mAP) Using Individual and Multiple
      Features with Various Fusion Methods
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance Comparison (mAP) with DMF and DASD, which Focus
      on the Use of the Class Relationships
  Table 3 caption:
    table_text: TABLE 3 Performance of the Entire Framework (the Last Row) Using Both
      Kinds of Relationships, in Comparison with Single-Relationship Results and the
      Basic Deep Networks with Various Numbers of Hidden Layers
  Table 4 caption:
    table_text: TABLE 4 Comparison with State of the Arts
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2670560
- Affiliation of the first author: nanyang technological university, 50 nanyang ave,
    singapore
  Affiliation of the last author: nanyang technological university, 50 nanyang ave,
    singapore
  Figure 1 Link: articels_figures_by_rev_year\2017\ShadingBased_Surface_Detail_Recovery_Under_General_Unknown_Illumination\figure_1.jpg
  Figure 1 caption: "At each point v i on the surface, the vertex overall illumination\
    \ vector L( v i ) represents the overall effect of all incident lights such as\
    \ l 1 , l 2 ,\u2026, l m from different directions and n( v i ) represents the\
    \ normal of the surface at v i ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\ShadingBased_Surface_Detail_Recovery_Under_General_Unknown_Illumination\figure_2.jpg
  Figure 2 caption: 'Visualization of L(v) , which is mapped to RGB, over the 3D models
    lit by different light probes. First row: the original light probe map from [43].
    Second row: the distribution of light sources in the cube map, where each green
    box indicates the location of one light source. Third and fourth rows: bunny and
    budda models with color mapped L(v) .'
  Figure 3 Link: articels_figures_by_rev_year\2017\ShadingBased_Surface_Detail_Recovery_Under_General_Unknown_Illumination\figure_3.jpg
  Figure 3 caption: Reconstruction with and without visual hull constraints.
  Figure 4 Link: articels_figures_by_rev_year\2017\ShadingBased_Surface_Detail_Recovery_Under_General_Unknown_Illumination\figure_4.jpg
  Figure 4 caption: Reconstruction of four objects (bird, hands, owl and budda) with
    uniform albedo.
  Figure 5 Link: articels_figures_by_rev_year\2017\ShadingBased_Surface_Detail_Recovery_Under_General_Unknown_Illumination\figure_5.jpg
  Figure 5 caption: Reconstruction of four objects (pilot, elephant, bunny, vegetables)
    with non-uniform albedo.
  Figure 6 Link: articels_figures_by_rev_year\2017\ShadingBased_Surface_Detail_Recovery_Under_General_Unknown_Illumination\figure_6.jpg
  Figure 6 caption: Reconstruction of synthetic budda model using heavily distorted
    initial meshes.
  Figure 7 Link: articels_figures_by_rev_year\2017\ShadingBased_Surface_Detail_Recovery_Under_General_Unknown_Illumination\figure_7.jpg
  Figure 7 caption: The reconstructions without (left) and with (right) visual hull
    constraints.
  Figure 8 Link: articels_figures_by_rev_year\2017\ShadingBased_Surface_Detail_Recovery_Under_General_Unknown_Illumination\figure_8.jpg
  Figure 8 caption: 'Left: the reconstruction result of [11] without visual hull constraints;
    Right: the reconstruction result of our method with visual hull constraints.'
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.74
  Name of the first author: Di Xu
  Name of the last author: Tat-Jen Cham
  Number of Figures: 8
  Number of Tables: 3
  Number of authors: 6
  Paper title: Shading-Based Surface Detail Recovery Under General Unknown Illumination
  Publication Date: 2017-02-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Reconstruction Errors for the Input Meshes Generated by [57],
      the Reconstruction Meshes of [11], and Our Optimized Meshes
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Evaluation of Our Recovered L(v) for the Two
      Synthetic Models Under the Four Light Probe Images Against the Ground Truth
  Table 3 caption:
    table_text: 'TABLE 3 Statistics of DTU Robot Image Data Sets in the Experiments:
      Model Sizes, Percentage of the Constrained Vertices from Visual Hull, and Runtime'
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2671458
- Affiliation of the first author: key lab of network security and cryptology, fujian
    normal university, fujian, p.r. china
  Affiliation of the last author: key lab of network security and cryptology, fujian
    normal university, fujian, p.r. china
  Figure 1 Link: articels_figures_by_rev_year\2017\SeparabilityOriented_Subclass_Discriminant_Analysis\figure_1.jpg
  Figure 1 caption: Illustration of local scatterness and global scatterness of a
    class of data.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\SeparabilityOriented_Subclass_Discriminant_Analysis\figure_2.jpg
  Figure 2 caption: Illustration of S w and S sw .
  Figure 3 Link: articels_figures_by_rev_year\2017\SeparabilityOriented_Subclass_Discriminant_Analysis\figure_3.jpg
  Figure 3 caption: Illustration of S b and S sb .
  Figure 4 Link: articels_figures_by_rev_year\2017\SeparabilityOriented_Subclass_Discriminant_Analysis\figure_4.jpg
  Figure 4 caption: Sample images from the face databases.
  Figure 5 Link: articels_figures_by_rev_year\2017\SeparabilityOriented_Subclass_Discriminant_Analysis\figure_5.jpg
  Figure 5 caption: "Number of wins by SSDA-1, SSDA-2 and SSDA-3 for different K max\
    \ on all 10 UCI data sets and 3 face databases\u2014AR, ORL and LFW."
  Figure 6 Link: articels_figures_by_rev_year\2017\SeparabilityOriented_Subclass_Discriminant_Analysis\figure_6.jpg
  Figure 6 caption: 'Iris data: subclass structures sought after by SDA, MSDA and
    SSDA-1. Every circle represents one cluster, which is centred at the mean of the
    cluster and has a radius as the distance between the centre and furtherest point
    in the cluster.'
  Figure 7 Link: articels_figures_by_rev_year\2017\SeparabilityOriented_Subclass_Discriminant_Analysis\figure_7.jpg
  Figure 7 caption: Visualisation of data in the first few dimensions of the LDA space.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.73
  Name of the first author: Huan Wan
  Name of the last author: Xin Wei
  Number of Figures: 7
  Number of Tables: 10
  Number of authors: 4
  Paper title: Separability-Oriented Subclass Discriminant Analysis
  Publication Date: 2017-02-22 00:00:00
  Table 1 caption:
    table_text: TABLE 1 General Information About Ten UCI Data Sets Used in Experiments
  Table 10 caption:
    table_text: TABLE 10 Dunn Index for a Classifying of Data in the Original Data
      Space, and an LDA Space by LDASSDA-1SDAMSDA on All Ten UCI Data Sets and the
      ARLFWORL Face Data Sets
  Table 2 caption:
    table_text: TABLE 2 General Information About the Eight Imbalanced Data Sets Used
      in the Experiments
  Table 3 caption:
    table_text: "TABLE 3 EMA \xB1 SEM of All Methods on Ten UCI Data Sets"
  Table 4 caption:
    table_text: TABLE 4 Run Time, in Second, of All Methods on Ten UCI Data Sets
  Table 5 caption:
    table_text: "TABLE 5 EMA \xB1 SEM of All Methods on Eight Imbalanced Data Sets"
  Table 6 caption:
    table_text: TABLE 6 Run Time, in Second, of All Methods on Eight Imbalanced Data
      Sets
  Table 7 caption:
    table_text: "TABLE 7 EMA \xB1 SEM of All Methods on Four Face Databases"
  Table 8 caption:
    table_text: TABLE 8 Run Time, in Second, of All Methods on Four Face Databases
  Table 9 caption:
    table_text: "TABLE 9 EMA \xB1 SEM of SDA with NN-Clustering, SDA with SSDA Clustering,\
      \ and SSDA on 10 UCI Database"
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2672557
- Affiliation of the first author: "universit\xE9 de reims champagne-ardenne, crestic,\
    \ reims, france"
  Affiliation of the last author: "universit\xE9 de reims champagne-ardenne, crestic,\
    \ reims, france"
  Figure 1 Link: articels_figures_by_rev_year\2017\Curvilinear_Structure_Analysis_by_Ranking_the_Orientation_Responses_of_Path_Oper\figure_1.jpg
  Figure 1 caption: An illustration of the RORPO intensity feature (b) applied on
    a 3D synthetic image (a) with a plane and a curvilinear structure. Both images
    are seen with a maximum intensity projection (MIP). RORPO successfully removes
    the plane structure, decreases the background intensity while preserving the curvilinear
    structure.
  Figure 10 Link: articels_figures_by_rev_year\2017\Curvilinear_Structure_Analysis_by_Ranking_the_Orientation_Responses_of_Path_Oper\figure_10.jpg
  Figure 10 caption: Number of path opening orientations detecting (a) tubes and (b)
    planes, within the set of 7 orientations illustrated in Fig. 5.
  Figure 2 Link: articels_figures_by_rev_year\2017\Curvilinear_Structure_Analysis_by_Ranking_the_Orientation_Responses_of_Path_Oper\figure_2.jpg
  Figure 2 caption: In 3D, (a) a blob structure, (b) a planar structure, and (c) a
    curvilinear structure, in blue. Arrows show sampling along some orientations;
    a green (resp. red) arrow represents a high (resp. low) response of an oriented
    filter. The blob presents a high response in every orientation; the planar structure
    presents a high response in 4 out of the 8 orientations; the curvilinear structure
    only presents a high response in 1 orientation.
  Figure 3 Link: articels_figures_by_rev_year\2017\Curvilinear_Structure_Analysis_by_Ranking_the_Orientation_Responses_of_Path_Oper\figure_3.jpg
  Figure 3 caption: Comparison on image (a) of a classical opening with line structuring
    elements (b) and a path opening (c) with the same SE length of 10 pixels. The
    bottom right line is globally vertical but presents local tortuosity. Only the
    path opening is able to detect such structure.
  Figure 4 Link: articels_figures_by_rev_year\2017\Curvilinear_Structure_Analysis_by_Ranking_the_Orientation_Responses_of_Path_Oper\figure_4.jpg
  Figure 4 caption: "(a) An example of adjacency relation \u2192 . (b\u2013d) The\
    \ connectedness \u0393 which is a periodic repetition of (a) over X . (b) A path\
    \ of length 3 (resp. 4) is depicted in red (resp. blue). (c) A binary image defined\
    \ on X . (d) The result of a path opening of length 4 on the image of (c)."
  Figure 5 Link: articels_figures_by_rev_year\2017\Curvilinear_Structure_Analysis_by_Ranking_the_Orientation_Responses_of_Path_Oper\figure_5.jpg
  Figure 5 caption: The 7 orientations of mathcal C . The center of the cube, the
    centers of faces, and corners represent points of the image. The arrows represent
    the adjacency relation. The red arrows are the vectors estar and dstar representing
    the global orientation of each cone.
  Figure 6 Link: articels_figures_by_rev_year\2017\Curvilinear_Structure_Analysis_by_Ranking_the_Orientation_Responses_of_Path_Oper\figure_6.jpg
  Figure 6 caption: 2D illustration of filter response ranking. A structure present
    in one orientation (the vertical line) remains in RF1 ; structures present in
    two orientations (the oblique lines) remain in RF1 and RF2 ; the disc is present
    in all the orientations and thus remains in all the RF s.
  Figure 7 Link: articels_figures_by_rev_year\2017\Curvilinear_Structure_Analysis_by_Ranking_the_Orientation_Responses_of_Path_Oper\figure_7.jpg
  Figure 7 caption: At each point of an image (red square), 7 path opening responses
    ALGamma (I(x)) , associated to each orientation Cstar are computed (top right
    values). After ranking, the orientations of interest can be the first one, two
    or three path opening orientations (in the figure the first two orientations).
  Figure 8 Link: articels_figures_by_rev_year\2017\Curvilinear_Structure_Analysis_by_Ranking_the_Orientation_Responses_of_Path_Oper\figure_8.jpg
  Figure 8 caption: Illustration of our proposed simplified robust path opening (see
    Section 5.1 ). The disconnected curvilinear structure is fully preserved while
    the top noise structure is removed.
  Figure 9 Link: articels_figures_by_rev_year\2017\Curvilinear_Structure_Analysis_by_Ranking_the_Orientation_Responses_of_Path_Oper\figure_9.jpg
  Figure 9 caption: The path length also depends on the scale of the curvature. (a)
    A thin structure of length Lr presenting small-scale and large-scale curvature.
    (b) A path length of length L1 < Lr along one orientation detects a first part
    of the structure. (c) A path length of length L2 < Lr along another orientation
    detects a second part of the structure. (d) The two detected parts overlap.
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "Odyss\xE9e Merveille"
  Name of the last author: Nicolas Passat
  Number of Figures: 17
  Number of Tables: 4
  Number of authors: 4
  Paper title: Curvilinear Structure Analysis by Ranking the Orientation Responses
    of Path Operators
  Publication Date: 2017-02-22 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Filtering Performances on Synthetic Images, for Various Levels\
      \ of Gaussian White Noise \u2013 MCC Scores"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Filtering Performances on Synthetic Images, for Various Levels\
      \ of Noise (Gaussian)\u2013TPR FPR Scores"
  Table 3 caption:
    table_text: TABLE 3 Comparison of RORPO, OOF and FV Results on a Coronary CT Scan
  Table 4 caption:
    table_text: "TABLE 4 Direction Error (in Degrees) on Synthetic Images, for Various\
      \ Levels of Noise (Gaussian)\u2013Mean Values and Standard Deviation into Brackets"
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2672972
- Affiliation of the first author: department of biomedical engineering, eindhoven
    university of technology (tue), eindhoven, az, the netherlands
  Affiliation of the last author: department of biomedical engineering, eindhoven
    university of technology (tue), eindhoven, az, the netherlands
  Figure 1 Link: articels_figures_by_rev_year\2017\Template_Matching_via_Densities_on_the_RotoTranslation_Group\figure_1.jpg
  Figure 1 caption: "A retinal image f of the optic nerve head and a volume rendering\
    \ of the orientation score U f (obtained via a wavelet transform W \u03C8 )."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Template_Matching_via_Densities_on_the_RotoTranslation_Group\figure_2.jpg
  Figure 2 caption: "In orientation scores U f , constructed from an image f via the\
    \ orientation score transform W \u03C8 , we make use of a left-invariant derivative\
    \ frame \u2202 \u03BE , \u2202 \u03B7 , \u2202 \u03B8 that is aligned with the\
    \ orientation \u03B8 corresponding to each layer in the score. Three slices and\
    \ the corresponding left-invariant frames are shown separately (at \u03B8\u2208\
    0, \u03C0 4 , 3\u03C0 4 )."
  Figure 3 Link: articels_figures_by_rev_year\2017\Template_Matching_via_Densities_on_the_RotoTranslation_Group\figure_3.jpg
  Figure 3 caption: "Overview of trained templates for ONH detection, and their responses\
    \ to a challenging retinal image. (a) The example input image with true ONH location\
    \ in blue. (b) The R 2 -type templates (top row) and their responses to the input\
    \ image (bottom row). (c) The maximum intensity projections (over \u03B8 ) of\
    \ the SE(2) -type templates (top row) and their responses to the input image (bottom\
    \ row). Detected ONH locations are indicated with colored circles (green = correct,\
    \ red = incorrect)."
  Figure 4 Link: articels_figures_by_rev_year\2017\Template_Matching_via_Densities_on_the_RotoTranslation_Group\figure_4.jpg
  Figure 4 caption: "Overview of trained templates for fovea detection, and their\
    \ responses to a challenging retinal image. (a) The example input image with true\
    \ fovea location in blue. (b) The R 2 -type templates (top row) and their responses\
    \ to the input image (bottom row). (c) The maximum intensity projections (over\
    \ \u03B8 ) of the SE(2) -type templates (top row) and their responses to the input\
    \ image (bottom row). Detected fovea locations are indicated with colored circles\
    \ (green = correct, red = incorrect)."
  Figure 5 Link: articels_figures_by_rev_year\2017\Template_Matching_via_Densities_on_the_RotoTranslation_Group\figure_5.jpg
  Figure 5 caption: Overview of trained templates for right-eye pupil detection, and
    their responses to a challenging image from the BioID database. (a) The example
    input image with true pupil locations (blue circle with a radius that corresponds
    to a normalized error threshold of 0.1, see Eq. 39. The white square indicates
    the periocular image region for the right eye. (b) The mathbb R2 -type templates
    (top row) and their responses to the input image (bottom row). (c) The maximum
    intensity projections (over theta ) of the SE(2) -type templates (top row) and
    their responses to the input image (bottom row). Detected pupil locations are
    indicated with colored circles (green = correct, red = incorrect, based on a normalized
    error threshold of 0.1). (d) Accuracy curves generated by varying thresholds on
    the normalized error, in comparison with the two most recent methods from literature.
    (e) Accuracy (at a normalized error threshold of 0.1) comparison with pupil detection
    methods from literature.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Erik Johannes Bekkers
  Name of the last author: Remco Duits
  Number of Figures: 5
  Number of Tables: 3
  Number of authors: 4
  Paper title: Template Matching via Densities on the Roto-Translation Group
  Publication Date: 2017-02-24 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Average Template Matching Results ( \xB1 Standard Deviation)\
      \ for Optic Nerve Head Detection in Five-Fold Cross Validation, Number of Failed\
      \ Detections in Parentheses"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Comparison to State of the Art: Optic Nerve Head Detection
      Success Rates, the Number of Fails (in Parentheses), and Computation Times'
  Table 3 caption:
    table_text: 'TABLE 3 Comparison to State of the Art: Fovea Detection Success Rates,
      the Number of Fails (in Parentheses), and Computation Times'
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2652452
- Affiliation of the first author: australian national university, canberra, act,
    australia
  Affiliation of the last author: microsoft research asia, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2017\Multiview_Rectification_of_Folded_Documents\figure_1.jpg
  Figure 1 caption: Our technique recovers a ridge-aware 3D reconstruction of the
    document surface from a sparse 3D point cloud. The final rectified image is then
    obtained via robust conformal mapping.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Multiview_Rectification_of_Folded_Documents\figure_2.jpg
  Figure 2 caption: Developable surfaces with underlying rulers (lines with zero Gaussian
    curvature) and fold lines (ridges) shown as dotted and solids lines respectively.
    Examples of (a) smooth parallel rulers, (b) smooth rulers not parallel to each
    other and (c) rulers and ridges in arbitrary directions.
  Figure 3 Link: articels_figures_by_rev_year\2017\Multiview_Rectification_of_Folded_Documents\figure_3.jpg
  Figure 3 caption: Example of ridge-aware 3D surface reconstruction.
  Figure 4 Link: articels_figures_by_rev_year\2017\Multiview_Rectification_of_Folded_Documents\figure_4.jpg
  Figure 4 caption: Vertices of a triangle in a local coordinate basis.
  Figure 5 Link: articels_figures_by_rev_year\2017\Multiview_Rectification_of_Folded_Documents\figure_5.jpg
  Figure 5 caption: Rectification results from combination of methods. Acronyms RA
    and Po denote our ridge-aware method and Poisson reconstruction respectively.
    L1 denotes our ell 1 conformal mapping method with non-local constraints; L2 indicates
    LSCM [29] and Geo indicates geodesic unwrapping [30].
  Figure 6 Link: articels_figures_by_rev_year\2017\Multiview_Rectification_of_Folded_Documents\figure_6.jpg
  Figure 6 caption: '[Our results] Original images are shown in rows 1 and 3 and our
    rectification results are shown in rows 2 and 4.'
  Figure 7 Link: articels_figures_by_rev_year\2017\Multiview_Rectification_of_Folded_Documents\figure_7.jpg
  Figure 7 caption: Distortion metrics for datasets shown in Fig. 6. Abbreviations
    are consistent with Fig. 5 and the text.
  Figure 8 Link: articels_figures_by_rev_year\2017\Multiview_Rectification_of_Folded_Documents\figure_8.jpg
  Figure 8 caption: Comparison with Periollat et al.'s method [28].
  Figure 9 Link: articels_figures_by_rev_year\2017\Multiview_Rectification_of_Folded_Documents\figure_9.jpg
  Figure 9 caption: (a) Comparison of the global distortion metric between our method
    (top) and Zhang et al. [30] and Brown et al.[29] with varying point density and
    noise. Here lower values indicate higher accuracy. (b) Frequency distribution
    of local distortion metrics for the associated experiments. Our method is more
    accurate when input point are sparser or have more noise.
  First author gender probability: 0.77
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Shaodi You
  Name of the last author: Katsushi Ikeuchi
  Number of Figures: 9
  Number of Tables: 0
  Number of authors: 5
  Paper title: Multiview Rectification of Folded Documents
  Publication Date: 2017-03-02 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2675980
- Affiliation of the first author: department of electronic engineering and information
    science, university of science and technology of china, hefei, china
  Affiliation of the last author: department of computer science, university of texas
    at san antonio, san antonio, tx
  Figure 1 Link: articels_figures_by_rev_year\2017\Collaborative_Index_Embedding_for_Image_Retrieval\figure_1.jpg
  Figure 1 caption: The general framework of our approach. Each column in the index
    matrix corresponds to a visual word histogram of a database image. The details
    are discussed in Section 3.
  Figure 10 Link: articels_figures_by_rev_year\2017\Collaborative_Index_Embedding_for_Image_Retrieval\figure_10.jpg
  Figure 10 caption: The mAP performance of CNN index and SIFT index (without matching
    verification) on the (a) Holidays dataset and (b) UKBench dataset in different
    iterations. The queries are taken from the original index while the searched image
    database are represented with the updated index.
  Figure 2 Link: articels_figures_by_rev_year\2017\Collaborative_Index_Embedding_for_Image_Retrieval\figure_2.jpg
  Figure 2 caption: A toy example of neighborhood embedding. Each point corresponds
    to an image in the SIFT feature space or CNN feature space. Two relevant images
    (images 1 and 2), which are nearest neighbor of each other in the SIFT space,
    will be pulled closer in the CNN feature space. That is implicitly achieved by
    updating the image index matrices of CNN feature.
  Figure 3 Link: articels_figures_by_rev_year\2017\Collaborative_Index_Embedding_for_Image_Retrieval\figure_3.jpg
  Figure 3 caption: "A toy example of the alternated index update. The operation \u201C\
    \ \u2295 \u201D denotes our index embedding optimization. The index matrices before\
    \ and after the \u201Ccopy\u201D operation are identical."
  Figure 4 Link: articels_figures_by_rev_year\2017\Collaborative_Index_Embedding_for_Image_Retrieval\figure_4.jpg
  Figure 4 caption: (a) The sparseness of deep CNN feature with respect to different
    energy ratio settings. The mAP performance of image retrieval on the (b) Holidays
    dataset and (c) UKBench dataset with respect to different energy ratio settings
    on deep CNN feature.
  Figure 5 Link: articels_figures_by_rev_year\2017\Collaborative_Index_Embedding_for_Image_Retrieval\figure_5.jpg
  Figure 5 caption: "(a) The retrieval accuracy (mAP) with deep CNN feature on the\
    \ Holidays dataset with respect to \u03B1 , with \u03B2 set as 1.0. (b) The retrieval\
    \ accuracy (mAP) with SIFT feature on the Holidays dataset with respect to \u03B2\
    \ , with \u03B1 set as 0.4."
  Figure 6 Link: articels_figures_by_rev_year\2017\Collaborative_Index_Embedding_for_Image_Retrieval\figure_6.jpg
  Figure 6 caption: The top-1 accuracy and top-2 accuracy with the CNN index and the
    SIFT index on the UKBench dataset in different iterations. The BoW vector of the
    query is taken from the original index for both CNN and SIFT features.
  Figure 7 Link: articels_figures_by_rev_year\2017\Collaborative_Index_Embedding_for_Image_Retrieval\figure_7.jpg
  Figure 7 caption: Sample results of two queries (on the left of the vertical dashed
    line) from the Holidays dataset with CNN index, SIFT index, and the SIFT-embedded
    CNN index. Images on the right of the vertical dashed line in each row are the
    top-4 retrieval results of the corresponding index denoted on the rightmost side.
    The false positives are highlighted by red dashed bounding boxes. The CNN features
    are extracted with the AlexNet which is trained on the ImageNet classification
    task.
  Figure 8 Link: articels_figures_by_rev_year\2017\Collaborative_Index_Embedding_for_Image_Retrieval\figure_8.jpg
  Figure 8 caption: "The inverted file size of CNN index and SIFT index on the (a)\
    \ Holidays dataset (image resolution: 1,024\xD7768 ) and (b) UKBench dataset (image\
    \ resolution: 640\xD7480 ), in different iterations."
  Figure 9 Link: articels_figures_by_rev_year\2017\Collaborative_Index_Embedding_for_Image_Retrieval\figure_9.jpg
  Figure 9 caption: The mAP performance of CNN index and SIFT index (with matching
    verification) on the (a) Holidays dataset and (b) UKBench dataset in different
    iterations. The queries are taken from the original index while the searched image
    database are represented with the updated index.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Wengang Zhou
  Name of the last author: Qi Tian
  Number of Figures: 12
  Number of Tables: 2
  Number of authors: 4
  Paper title: Collaborative Index Embedding for Image Retrieval
  Publication Date: 2017-03-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance Comparison of Different CNN Features Extracted
      from AlexNet, VGGNet-16, and VGGNet-19, Respectively
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Our Collaborative Index Embedding (CIE) Approach
      with the State-of-the-Art Algorithms in Terms of the Retrieval Accuracy, Involved
      Visual Features, and On-Line Memory Cost per Image
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2676779
- Affiliation of the first author: "institut de robotica i inform\xE0tica industrial,\
    \ barcelona, spain"
  Affiliation of the last author: "institut de robotica i inform\xE0tica industrial,\
    \ barcelona, spain"
  Figure 1 Link: articels_figures_by_rev_year\2017\Boosted_Random_Ferns_for_Object_Detection\figure_1.jpg
  Figure 1 caption: 'Results for single- and multi-view object detection. Left: 2D
    detection results on public datasets of cars, motorbikes and horses. The green
    rectangles indicate the location and scale at which an object was detected. The
    level contours show the distribution of the centers for the 300 ferns used to
    build the classifier. Ferns concentrate on regions that have been found to be
    more discriminative. Note how these locations are salient regions that one would
    naturally choose as discriminative, like the wheels of a car or the head of a
    horse. Right: Multi-view detection results on cars and bikes datasets. The pie-like
    circles on the top-left of each image indicate the estimated (green) and ground
    truth (blue) orientation of the object.'
  Figure 10 Link: articels_figures_by_rev_year\2017\Boosted_Random_Ferns_for_Object_Detection\figure_10.jpg
  Figure 10 caption: Average detection times (in Matlab) given by BRFs on the UIUC
    cars dataset with and without feature sharing for varying feature sizes and total
    number of weak learners.
  Figure 2 Link: articels_figures_by_rev_year\2017\Boosted_Random_Ferns_for_Object_Detection\figure_2.jpg
  Figure 2 caption: "HOG-based fern. Given an image window x , a Histogram of Oriented\
    \ Gradients (HOG) is locally computed in a subwindow s , centered at location\
    \ u . The HOG computation is carried out by calculating gradients within s and\
    \ casting votes for a particular spatial partition and orientation bin. Votes\
    \ are weighted according to the gradient magnitude. For this example, a 2\xD7\
    2 spatial grid and 4 orientation bins are considered. The resulting HOG-descriptor\
    \ is then a concatenation of local and adjacent distributions of oriented gradients.\
    \ The features f we consider in this paper, compare the value of two of these\
    \ bins chosen at random during training. The fern f is made from several such\
    \ individual features."
  Figure 3 Link: articels_figures_by_rev_year\2017\Boosted_Random_Ferns_for_Object_Detection\figure_3.jpg
  Figure 3 caption: "Example showing the response of ferns densely computed over the\
    \ image but using only three different vectors of histogram bin indices \u03B8\
    \ 1 , \u03B8 2 and \u03B8 3 , and M=8 . The fern response is color coded, where\
    \ each color indicates a particular output value z\u22080,\u2026,255 ."
  Figure 4 Link: articels_figures_by_rev_year\2017\Boosted_Random_Ferns_for_Object_Detection\figure_4.jpg
  Figure 4 caption: "Class-conditional probabilities of two ferns sharing the same\
    \ feature parameters \u03B8 r and tested at two different locations u 1 and u\
    \ 2 . Similarity between these distributions is computed using the Bhattacharyya\
    \ coefficient Q ."
  Figure 5 Link: articels_figures_by_rev_year\2017\Boosted_Random_Ferns_for_Object_Detection\figure_5.jpg
  Figure 5 caption: '2D classification results of BRFs, RFs [38], Random Forest [6]
    and GentleBoost classifier [13] . First column: Classification results on 500
    positive and negative testing samples. Correctly classified samples are shown
    in cyan (positive samples) and red (negative samples). Misclassified samples are
    shown in black. Second column: Confidence maps provided by the classifiers over
    the 2D feature space. Third column: Uncertainty maps where brighter regions correspond
    to uncertain classification values. Red contours indicate uncertainty of 90 percent.
    Fourth column: Score distributions for the positive and negative classes.'
  Figure 6 Link: articels_figures_by_rev_year\2017\Boosted_Random_Ferns_for_Object_Detection\figure_6.jpg
  Figure 6 caption: 'Classification performance of RFs, BRFs, RForest and GBoost in
    the scenario of Fig. 5 for different values of weak learners (WLs) and tree depth
    ( D ). First column: Mean EERs on the precision-recall curve. Second column: Hellinger
    distances between classes. Third and fourth columns: Computational times for training
    and testing the classifiers.'
  Figure 7 Link: articels_figures_by_rev_year\2017\Boosted_Random_Ferns_for_Object_Detection\figure_7.jpg
  Figure 7 caption: Number of features used by the RForest and BRFs.
  Figure 8 Link: articels_figures_by_rev_year\2017\Boosted_Random_Ferns_for_Object_Detection\figure_8.jpg
  Figure 8 caption: Equal Error Rate (EER) and Hellinger distance (HD), as a function
    of the training set size ( N ).
  Figure 9 Link: articels_figures_by_rev_year\2017\Boosted_Random_Ferns_for_Object_Detection\figure_9.jpg
  Figure 9 caption: In the left column, spatial layout of ferns for different object
    categories. In the right column, distribution of weak learners for each class.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Michael Villamizar
  Name of the last author: Francesc Moreno-Noguer
  Number of Figures: 21
  Number of Tables: 8
  Number of authors: 4
  Paper title: Boosted Random Ferns for Object Detection
  Publication Date: 2017-03-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Equal Error Rates Achieved by BRFs on Various Datasets Using
      Signed Binary Comparisons of Pixel Intensities (INT) or Signed Binary Comparisons
      of Cells of the Local Histogram of Oriented Gradients (HOG)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Not Boosting (RFs) versus Boosting (BRFs) on
      Three Distinct Object Datasets
  Table 3 caption:
    table_text: TABLE 3 Comparison of Shared versus Independent Ferns
  Table 4 caption:
    table_text: TABLE 4 Execution Times (in Matlab) and PR-EER Rates for Various Pool
      Sizes of BRFs ( R )
  Table 5 caption:
    table_text: TABLE 5 BRFs Recognition Rates Compared to the State of the Art for
      Object Detection in the Chosen Object Datasets
  Table 6 caption:
    table_text: TABLE 6 Performance Comparison of Cascaded BRFs versus Other Methods
      for Rotation Invariant Object Detection
  Table 7 caption:
    table_text: TABLE 7 Multi-View Object Detection Rates and View Pose Recognition
      Accuracy on the 3DObject Dataset
  Table 8 caption:
    table_text: TABLE 8 Multi-View Object Detection Rates and View Pose Recognition
      Accuracy on the EPFL and KITTI Cars Datasets
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2676778
- Affiliation of the first author: department of electrical and computer engineering,
    national chiao tung university, hsinchu, taiwan
  Affiliation of the last author: department of electrical and computer engineering,
    national chiao tung university, hsinchu, taiwan
  Figure 1 Link: articels_figures_by_rev_year\2017\Deep_Unfolding_for_Topic_Models\figure_1.jpg
  Figure 1 caption: Graphical representation for (a) LDA and (b) sLDA. Class labels
    y= y d are included in sLDA.
  Figure 10 Link: articels_figures_by_rev_year\2017\Deep_Unfolding_for_Topic_Models\figure_10.jpg
  Figure 10 caption: Comparison of PMI of using VI-LDA, VI-sLDA, BP-LDA, BP-sLDA,
    DUI-LDA and DUI-sLDA. L=5 and K=50 are fixed. MultiSent and 20 Newsgroups datasets
    are used.
  Figure 2 Link: articels_figures_by_rev_year\2017\Deep_Unfolding_for_Topic_Models\figure_2.jpg
  Figure 2 caption: "(a) Forward pass and (b) backward pass in deep unfolding inference.\
    \ In forward pass, each input x n is used to calculate the intermediate variables\
    \ \u03A8 n in L iterations or layers. Interesting output y \u02C6 n is estimated\
    \ and used to evaluate the end performance J . This is run for all training samples\
    \ x= x n to obtain outputs y \u02C6 = y \u02C6 n and \u03A8 (l) = \u03A8 (l) n\
    \ for L layers. Using the gradient of evaluation function \u2207 \u0398 J , in\
    \ backward pass, model parameters are updated from output layer \u0398 (L) back\
    \ to input layer \u0398 (1) ."
  Figure 3 Link: articels_figures_by_rev_year\2017\Deep_Unfolding_for_Topic_Models\figure_3.jpg
  Figure 3 caption: "Schematic illustration for bi-level optimization in VI and DUI.\
    \ Model parameters \u0398 and intermediate variables \u03A8 are shown in two horizons.\
    \ Blue contours show the approximate or secondary objective F \u0398 ( x n ,\u03A8\
    ) which is optimized via VI procedure. Red contours indicates the original or\
    \ primary objective J \u0398 ( y \u02C6 n ) with y \u02C6 n = G \u0398 ( x n ,\u03A8\
    ) for optimization of end performance in DUI procedure. The markers of blue circle\
    \ and red triangle indicate the optimum points of variational and original objectives,\
    \ respectively."
  Figure 4 Link: articels_figures_by_rev_year\2017\Deep_Unfolding_for_Topic_Models\figure_4.jpg
  Figure 4 caption: "Mapping between the forward passes in DNN and DUI at test time.\
    \ Using DUI, each observation x n is applied to calculate the unfolded intermediate\
    \ variables in different layers from \u03A8 (l\u22121) n to \u03A8 (l) n according\
    \ to an inference objective function f \u0398 (l) (\u22C5) . The output y \u02C6\
    \ n is obtained via an estimator G \u0398 (L) (\u22C5) using input x n and the\
    \ newest parameters \u03A8 (L\u22121) n in the deepest layer. The intermediate\
    \ variables \u03A8 (l) n are calculated by using the estimated parameter \u0398\
    \ (l) in current layer l . Parameters \u0398 (l) are untied in different layers\
    \ l . In output layer, an end performance measure y \u02C6 n is calculated."
  Figure 5 Link: articels_figures_by_rev_year\2017\Deep_Unfolding_for_Topic_Models\figure_5.jpg
  Figure 5 caption: Forward pass in DUI for calculating the end performance measures
    mathcal J of (a) supervised topic model and (b) unsupervised topic model. Model
    parameters boldsymbol Theta(l) are shown in solid circles while the intermediate
    parameters boldsymbol Psi(l) are shown in dashed circles. Solid lines show the
    propagation of intermediate parameters and model parameters along different layer
    l while dash lines show the information from observations.
  Figure 6 Link: articels_figures_by_rev_year\2017\Deep_Unfolding_for_Topic_Models\figure_6.jpg
  Figure 6 caption: Perplexity versus number of steps by using VI-LDA and DUI-LDA
    under different L and fixed K=50 . MultiSent dataset is used.
  Figure 7 Link: articels_figures_by_rev_year\2017\Deep_Unfolding_for_Topic_Models\figure_7.jpg
  Figure 7 caption: Variance of gradients versus number of steps by using VI-LDA and
    DUI-LDA with L=5 . MultiSent dataset is used.
  Figure 8 Link: articels_figures_by_rev_year\2017\Deep_Unfolding_for_Topic_Models\figure_8.jpg
  Figure 8 caption: Comparison of perplexity of using MU and EG algorithms in DUI-LDA
    under different L and fixed K=50 . MultiSent dataset is used.
  Figure 9 Link: articels_figures_by_rev_year\2017\Deep_Unfolding_for_Topic_Models\figure_9.jpg
  Figure 9 caption: Comparison of perplexity of using VI-LDA, BP-LDA and DUI-LDA under
    different K and fixed L=5 . BP-LDAs using MU and EG algorithms are compared. 20
    Newsgroups dataset is used.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.83
  Name of the first author: Jen-Tzung Chien
  Name of the last author: Chao-Hsi Lee
  Number of Figures: 12
  Number of Tables: 1
  Number of authors: 2
  Paper title: Deep Unfolding for Topic Models
  Publication Date: 2017-03-02 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Values of the Paired t -Test for Comparison of DUI Over VI\
      \ (D \u2192 V) and BP (D \u2192 B) Under Different K or L in Different Experiments"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2677439
- Affiliation of the first author: department of electrical engineering, kaist, daejeon,
    republic of korea
  Affiliation of the last author: department of electrical engineering, kaist, daejeon,
    republic of korea
  Figure 1 Link: articels_figures_by_rev_year\2017\Fast_Randomized_Singular_Value_Thresholding_for_LowRank_Optimization\figure_1.jpg
  Figure 1 caption: Basic idea of our method. Instead of applying SVT to the large
    original matrix, if we can obtain the same result by applying SVT to small matrix
    with few additional efforts, the complexity could be significantly drop. As shown
    in Proposition 1, once a large matrix is decomposed into a core matrix and orthonormal
    matrices (the left and right thin matrices in the illustration), by applying SVT
    to the small core matrix we can obtain the same result with the direct SVT computation
    on the large matrix.
  Figure 10 Link: articels_figures_by_rev_year\2017\Fast_Randomized_Singular_Value_Thresholding_for_LowRank_Optimization\figure_10.jpg
  Figure 10 caption: Comparisons according to varying sliding window lengths. (a)
    Sampled input image. (b) The exact PSVT based result with the sliding window length
    5. (c-e) The PSVT by FRSVT based results with the sliding window length lbrace
    3,5,10rbrace respectively. [Top] Low-rank images, mathbf L . [Bottom] Absolute
    of sparse images, mathrmabs(mathbf S) .
  Figure 2 Link: articels_figures_by_rev_year\2017\Fast_Randomized_Singular_Value_Thresholding_for_LowRank_Optimization\figure_2.jpg
  Figure 2 caption: Illustration of singular value decaying. [Left] Gaussian random,
    video and image samples. [Right] Decaying graphs of singular values. The Red,
    Green, Blue lines represent the graphs of a Gaussian random matrix, video, and
    image, respectively. The spectrum of visual data decays significantly faster.
  Figure 3 Link: articels_figures_by_rev_year\2017\Fast_Randomized_Singular_Value_Thresholding_for_LowRank_Optimization\figure_3.jpg
  Figure 3 caption: 'Angular difference of subspaces between subsequent iterations.
    R: ground-truth rank of input data, OR: oversampling rate, pk .'
  Figure 4 Link: articels_figures_by_rev_year\2017\Fast_Randomized_Singular_Value_Thresholding_for_LowRank_Optimization\figure_4.jpg
  Figure 4 caption: "SVT comparisons among SVD methods. [Top] Experiments on Matlab\
    \ 2010a. [Bottom] Experiments on Matlab 2014a. Accuracy is measured by normalized\
    \ root mean squared error (NRMSE), \u2225 A \u2217 \u2212 A k \u2225 F \u2225\
    \ A \u2217 \u2225 F , where A \u2217 = S \u03C4 ( D GT ) , A k = S \u03C4 ( D\
    \ k ) , D GT is the input data, and D k is approximated by each method. For the\
    \ low-rank matrix test, we generate input data matrices whose rank correspond\
    \ to the target rank by multiplying two Gaussian random matrices with m\xD7r and\
    \ r\xD7n , while in other tests full rank matrices are used. In (b, d, h, j),\
    \ the theoretical minimum error bound by rank truncation in S \u03C4 ( D k ) is\
    \ provided for guidance, and it is defined as \u2211 j>k \u03C3 i ( A \u2217 )\
    \ , where \u03C3 i ( A \u2217 ) is computed by Matlab built-in SVD. For the low-rank\
    \ case in (f,l), the theoretical minimum error is zero, so we omit the theoretical\
    \ bound."
  Figure 5 Link: articels_figures_by_rev_year\2017\Fast_Randomized_Singular_Value_Thresholding_for_LowRank_Optimization\figure_5.jpg
  Figure 5 caption: 'RPCA comparisons. X -axis: Number of iterations, Y -axis: Stopping
    criterion. Except Mu et al. is based on the exact ALM method, all the other methods
    are based on the inexact ALM.'
  Figure 6 Link: articels_figures_by_rev_year\2017\Fast_Randomized_Singular_Value_Thresholding_for_LowRank_Optimization\figure_6.jpg
  Figure 6 caption: 'Convergence time plots of RPCA. X -axis: Elapsed time (sec.),
    Y -axis: Stopping criterion. Except Mu et al. is based on the exact ALM method,
    all the other methods are based on the inexact ALM. As a representative example,
    matrices of size 6,000times 6,000 are used, but the results can be generalized
    to other size matrices.'
  Figure 7 Link: articels_figures_by_rev_year\2017\Fast_Randomized_Singular_Value_Thresholding_for_LowRank_Optimization\figure_7.jpg
  Figure 7 caption: 'Varying parameter tests for (a) over-sampling and (b) power iteration
    in RPCA problem. X -axis: Number of iterations, Y -axis: NRMSE of low-rank solution
    mathbf L as fracVert mathbf L-mathbf LGTVert FVert mathbf LGTVert F .'
  Figure 8 Link: articels_figures_by_rev_year\2017\Fast_Randomized_Singular_Value_Thresholding_for_LowRank_Optimization\figure_8.jpg
  Figure 8 caption: Computation time comparison on the subspace clustering application
    [4]. Computation times are measured for face clustering on Extended Yale Database
    B [44].
  Figure 9 Link: articels_figures_by_rev_year\2017\Fast_Randomized_Singular_Value_Thresholding_for_LowRank_Optimization\figure_9.jpg
  Figure 9 caption: Qualitative results for the weather artifact removal. (a) Sample
    input images mathbf O . (b) Low-rank images mathbf Z .
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.88
  Name of the first author: Tae-Hyun Oh
  Name of the last author: In So Kweon
  Number of Figures: 14
  Number of Tables: 7
  Number of authors: 4
  Paper title: Fast Randomized Singular Value Thresholding for Low-Rank Optimization
  Publication Date: 2017-03-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of Notations
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Computational Complexity of Algorithm 1
  Table 3 caption:
    table_text: TABLE 3 Lists of Comparison Methods and Details for SVT Tests
  Table 4 caption:
    table_text: "TABLE 4 Parameter Settings for Convergence Evaluation of RPCA ( p\
      \ : Over-Sampling Rate, AP: Adaptive Rank Prediction, \u03B7 : The Number of\
      \ Power Iterations)"
  Table 5 caption:
    table_text: TABLE 5 Quantitative Comparison on RPCA
  Table 6 caption:
    table_text: TABLE 6 Examples of NNM Related Objective Functions
  Table 7 caption:
    table_text: TABLE 7 Comparisons of the Subspace Segmentation Algorithms on Hopkins155
      [43] (Motion) and Extended Yale Database B [44] (Face)
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2677440
