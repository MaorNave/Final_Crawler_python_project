- Affiliation of the first author: department of electrical engineering and computer
    science, seoul national university, seoul, south korea
  Affiliation of the last author: department of electrical engineering and computer
    science, seoul national university, seoul, south korea
  Figure 1 Link: articels_figures_by_rev_year\2017\Dynamic_Video_Deblurring_Using_a_Locally_Adaptive_Blur_Model\figure_1.jpg
  Figure 1 caption: (a) A real blurry frame in a dynamic scene. (b) Our deblurring
    result. (c) Our color coded optical flow result. (d) Our (Gaussian) defocus blur
    map.
  Figure 10 Link: articels_figures_by_rev_year\2017\Dynamic_Video_Deblurring_Using_a_Locally_Adaptive_Blur_Model\figure_10.jpg
  Figure 10 caption: (a) Ground truth sharp frames. (b) Generated blurry frames. Spatially
    varying blurs by object motions and camera shakes are synthesized realistically.
  Figure 2 Link: articels_figures_by_rev_year\2017\Dynamic_Video_Deblurring_Using_a_Locally_Adaptive_Blur_Model\figure_2.jpg
  Figure 2 caption: (a) A blurry frame from a dynamic scene. (b) Deblurring result
    by Cho et al. [19]. (c) Our result.
  Figure 3 Link: articels_figures_by_rev_year\2017\Dynamic_Video_Deblurring_Using_a_Locally_Adaptive_Blur_Model\figure_3.jpg
  Figure 3 caption: (a) Two light sources. (b) A light streak of the focused light
    source with camera motion. (c) A light streak of the defocused light source with
    camera motion.
  Figure 4 Link: articels_figures_by_rev_year\2017\Dynamic_Video_Deblurring_Using_a_Locally_Adaptive_Blur_Model\figure_4.jpg
  Figure 4 caption: Blurring process underlying in the proposed method.
  Figure 5 Link: articels_figures_by_rev_year\2017\Dynamic_Video_Deblurring_Using_a_Locally_Adaptive_Blur_Model\figure_5.jpg
  Figure 5 caption: (a) A sharp patch. (b) A patch blurred by defocus blur (Gaussian
    blur with standard deviation 5). (c) A patch blurred by defocus blur (Gaussian
    blur with standard deviation 5) and motion blur (linear kernel with length 11).
    (d) Comparisons of fidelities at the centers of the blurry patches by changing
    the scale of defocus blur. The ground truth scale of the defocus blur is 5 and
    the arrows indicate peaks estimated by ML estimator.
  Figure 6 Link: articels_figures_by_rev_year\2017\Dynamic_Video_Deblurring_Using_a_Locally_Adaptive_Blur_Model\figure_6.jpg
  Figure 6 caption: Defocus and motion blur kernels. (a) Gaussian defocus blur kernel
    with standard deviation boldsymbol sigma i(mathbf x) at a pixel location mathbf
    x . (b) Bidirectional optical flows and corresponding piece-wise linear motion
    blur kernel at a pixel location mathbf x .
  Figure 7 Link: articels_figures_by_rev_year\2017\Dynamic_Video_Deblurring_Using_a_Locally_Adaptive_Blur_Model\figure_7.jpg
  Figure 7 caption: 'Left to right: Blurry frames, ground truth blur maps, blur maps
    from [32], and our blur maps. (a) A blurry frame has spatially varying Gaussian
    defocus blur (i.e., sharp foreground and blurry background). (b) A blurry frame
    has spatially varying Gaussian defocus blur as well as motion blur.'
  Figure 8 Link: articels_figures_by_rev_year\2017\Dynamic_Video_Deblurring_Using_a_Locally_Adaptive_Blur_Model\figure_8.jpg
  Figure 8 caption: (a) A blurry frame of a video in dynamic scene. (b) Locally varying
    kernel using homography. (c) Our pixel-wise varying motion blur kernel using bidirectional
    optical flows.
  Figure 9 Link: articels_figures_by_rev_year\2017\Dynamic_Video_Deblurring_Using_a_Locally_Adaptive_Blur_Model\figure_9.jpg
  Figure 9 caption: Temporally consistent optical flows over three frames.
  First author gender probability: 0.7
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.83
  Name of the first author: Tae Hyun Kim
  Name of the last author: Kyoung Mu Lee
  Number of Figures: 18
  Number of Tables: 4
  Number of authors: 3
  Paper title: Dynamic Video Deblurring Using a Locally Adaptive Blur Model
  Publication Date: 2017-10-10 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Deblurring Performance Evaluations in Terms of PSNR (SSIM)
      with Our Synthetic Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Deblurring Performance Evaluations in Terms of SSIM with MPI
      SINTEL Dataset [46]
  Table 3 caption:
    table_text: TABLE 3 Deblurring Performances for Removing Defocus Blur, Motion
      Blur, and Both Blurs, Respectively Are Compared
  Table 4 caption:
    table_text: TABLE 4 Optical Flow Evaluations in Terms of End Point Error (EPE)
      with MPI SINTEL Dataset [46]
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2761348
- Affiliation of the first author: openlab, the university of newcastle, newcastle
    upon tune, united kingdom
  Affiliation of the last author: inception institute of artificial intelligence,
    abu dhabi, uae
  Figure 1 Link: articels_figures_by_rev_year\2017\ZeroShot_Learning_Using_Synthesised_Unseen_Visual_Data_with_Diffusion_Regularisa\figure_1.jpg
  Figure 1 caption: Given a conceptual description, human can imagine the outline
    of the scene by combining previous seen visual elements.
  Figure 10 Link: articels_figures_by_rev_year\2017\ZeroShot_Learning_Using_Synthesised_Unseen_Visual_Data_with_Diffusion_Regularisa\figure_10.jpg
  Figure 10 caption: Success and Failure cases of nearest neighbour matching. The
    query visual feature is synthesised from its attribute description. We find top-5
    nearest neighbours of the query feature from the real instances. It is a match
    if the nearest instance and the test image have the same label.
  Figure 2 Link: articels_figures_by_rev_year\2017\ZeroShot_Learning_Using_Synthesised_Unseen_Visual_Data_with_Diffusion_Regularisa\figure_2.jpg
  Figure 2 caption: 'Comparison of supervised and zero-shot classifications and existing
    ZSL frameworks. (A) A typical supervised classification: the training samples
    and labels are in pairs; (B) a zero-shot learning problem: without training samples,
    the classes C and D cannot be predicted; (C) Direct-Attribute Prediction model
    uses attributes as intermediate clues to associate visual features to class labels;
    (D) label-embedding: the attributes are concatenated as a semantic embedding;
    (E) we use semantic embedding to synthesise unseen visual data.'
  Figure 3 Link: articels_figures_by_rev_year\2017\ZeroShot_Learning_Using_Synthesised_Unseen_Visual_Data_with_Diffusion_Regularisa\figure_3.jpg
  Figure 3 caption: An illustration of our framework of unseen data synthesis. Unseen
    classes are represented by semantic attributes as inputs. We train a model that
    maps the semantic space to the visual data space to synthesise training data for
    these unseen classes. The crosses in the visual spaces denote test feature points.
  Figure 4 Link: articels_figures_by_rev_year\2017\ZeroShot_Learning_Using_Synthesised_Unseen_Visual_Data_with_Diffusion_Regularisa\figure_4.jpg
  Figure 4 caption: Objective function convergence on the AwA dataset.
  Figure 5 Link: articels_figures_by_rev_year\2017\ZeroShot_Learning_Using_Synthesised_Unseen_Visual_Data_with_Diffusion_Regularisa\figure_5.jpg
  Figure 5 caption: Some random image and attribute examples of the 4 datasets.
  Figure 6 Link: articels_figures_by_rev_year\2017\ZeroShot_Learning_Using_Synthesised_Unseen_Visual_Data_with_Diffusion_Regularisa\figure_6.jpg
  Figure 6 caption: Normalised variances of the synthesised data w.r.t. dimensions.
    Variance of each dimension is sorted in descending order. We make a comparison
    between the synthesised data variances 'with' (green) and 'without' (red) diffusion
    regularisation. The variances of real data (blue) are computed from real unseen
    data as references.
  Figure 7 Link: articels_figures_by_rev_year\2017\ZeroShot_Learning_Using_Synthesised_Unseen_Visual_Data_with_Diffusion_Regularisa\figure_7.jpg
  Figure 7 caption: Framework of the compared GAN model.
  Figure 8 Link: articels_figures_by_rev_year\2017\ZeroShot_Learning_Using_Synthesised_Unseen_Visual_Data_with_Diffusion_Regularisa\figure_8.jpg
  Figure 8 caption: 'T-SNE of the real and synthesised visual features of unseen classes:
    (A) real visual features; (B) synthesised visual features; (C) since t-SNE of
    different data is not aligned, we also show the distribution of mixed real and
    synthesised visual features.'
  Figure 9 Link: articels_figures_by_rev_year\2017\ZeroShot_Learning_Using_Synthesised_Unseen_Visual_Data_with_Diffusion_Regularisa\figure_9.jpg
  Figure 9 caption: The performance with respect to the Graph regularisation and Diffusion
    regularisation. The results are under the scenario of CA and using NN classifier.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Yang Long
  Name of the last author: Ling Shao
  Number of Figures: 10
  Number of Tables: 6
  Number of authors: 4
  Paper title: Zero-Shot Learning Using Synthesised Unseen Visual Data with Diffusion
    Regularisation
  Publication Date: 2017-10-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Key Statistics of the Four Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison with State-of-the-Art Methods
  Table 3 caption:
    table_text: TABLE 3 Detailed Analysis of Key Aspects of the Proposed Method
  Table 4 caption:
    table_text: TABLE 4 Computation Time on Each Dataset
  Table 5 caption:
    table_text: TABLE 5 Comparison with Published Results on the ImageNet Dataset
  Table 6 caption:
    table_text: TABLE 6 Comparison with Published Results on GZSL
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2762295
- Affiliation of the first author: "institut pascal - cnrsuniversit\xE9 clermont auvergne,\
    \ clermont-ferrand, france"
  Affiliation of the last author: "institut pascal - cnrsuniversit\xE9 clermont auvergne,\
    \ clermont-ferrand, france"
  Figure 1 Link: articels_figures_by_rev_year\2017\Inextensible_NonRigid_StructurefromMotion_by_SecondOrder_Cone_Programming\figure_1.jpg
  Figure 1 caption: Example reconstructions with our method on the KINECT Paper [38]
    images. The top row shows the input images and the bottom row shows the groundtruth
    in green overlaid on top of the reconstruction in white. Our best method gives
    a 3D error of 4.62 mm while the best compared method [23] has an error of 7.63
    mm. This is remarkable if we note that even the best performing SfT method in
    [10] produces an error of 3.82 mm on the dataset.
  Figure 10 Link: articels_figures_by_rev_year\2017\Inextensible_NonRigid_StructurefromMotion_by_SecondOrder_Cone_Programming\figure_10.jpg
  Figure 10 caption: Mean 3D errors for all the images in the Table mat dataset. The
    left plot shows errors for tlmdh against the compared methods and the right plot
    shows tlmdh against all proposed methods.
  Figure 2 Link: articels_figures_by_rev_year\2017\Inextensible_NonRigid_StructurefromMotion_by_SecondOrder_Cone_Programming\figure_2.jpg
  Figure 2 caption: The NRSfM problem and its associated geometric terms. We use O
    to represent the camera center from which we draw the sight lines. We show only
    three points for clarity. In practice there can be virtually any number of points
    and each point can have many neighbours.
  Figure 3 Link: articels_figures_by_rev_year\2017\Inextensible_NonRigid_StructurefromMotion_by_SecondOrder_Cone_Programming\figure_3.jpg
  Figure 3 caption: Illustration of the bounds set by equation (4) for NRSfM using
    three points and one image. The depth values cannot increase to the shaded region
    on the right because this would violate equation (4).
  Figure 4 Link: articels_figures_by_rev_year\2017\Inextensible_NonRigid_StructurefromMotion_by_SecondOrder_Cone_Programming\figure_4.jpg
  Figure 4 caption: 3D error for the synthetic Flag dataset against the number of
    images and points (first row) and against the % of missing data and the amount
    of noise (second row). The legend is shown on the top.
  Figure 5 Link: articels_figures_by_rev_year\2017\Inextensible_NonRigid_StructurefromMotion_by_SecondOrder_Cone_Programming\figure_5.jpg
  Figure 5 caption: 3D errors for all images in the KINECT Paper dataset. The left
    plot shows 3D error for tlmdh against the compared methods and the right plot
    shows tlmdh against all other proposed methods.
  Figure 6 Link: articels_figures_by_rev_year\2017\Inextensible_NonRigid_StructurefromMotion_by_SecondOrder_Cone_Programming\figure_6.jpg
  Figure 6 caption: Example of images present in the Hulk dataset (top row) and the
    T-Shirt dataset (bottom row).
  Figure 7 Link: articels_figures_by_rev_year\2017\Inextensible_NonRigid_StructurefromMotion_by_SecondOrder_Cone_Programming\figure_7.jpg
  Figure 7 caption: Example images from the Cardboard dataset.
  Figure 8 Link: articels_figures_by_rev_year\2017\Inextensible_NonRigid_StructurefromMotion_by_SecondOrder_Cone_Programming\figure_8.jpg
  Figure 8 caption: Mean 3D errors for different number of images in the Cardboard
    dataset.
  Figure 9 Link: articels_figures_by_rev_year\2017\Inextensible_NonRigid_StructurefromMotion_by_SecondOrder_Cone_Programming\figure_9.jpg
  Figure 9 caption: "Example images for the Table mat (top, cropped to the size of\
    \ 592\xD7349 px) and the Rug (bottom, original images) datasets."
  First author gender probability: 0.91
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Ajad Chhatkuli
  Name of the last author: Adrien Bartoli
  Number of Figures: 20
  Number of Tables: 3
  Number of authors: 4
  Paper title: Inextensible Non-Rigid Structure-from-Motion by Second-Order Cone Programming
  Publication Date: 2017-10-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 NRSfM Methods and Their Characteristics
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Mean 3D Errors in Real Datasets
  Table 3 caption:
    table_text: TABLE 3 Mean % 3D Errors in Real Datasets
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2762669
- Affiliation of the first author: heidelberg collaboratory for image processing,
    heidelberg university, heidelberg, bw, germany
  Affiliation of the last author: computer vision center, autonomous university of
    barcelona, bellaterra (cerdanyola), barcelona, spain
  Figure 1 Link: articels_figures_by_rev_year\2017\ErrorCorrecting_Factorization\figure_1.jpg
  Figure 1 caption: Example of a classification problem of 4 different sports balls.
    Note how One versus All or Dense Random ECOC designs do not take into account
    the data distribution while the proposed Error-Correcting Factorization method
    finds an ECOC matrix X by factorizing a design matrix D . In addition, the codes
    (rows of X ) ECF assigns to similar categories are very dissimilar in order to
    benefit from Error-Correcting principles.
  Figure 10 Link: articels_figures_by_rev_year\2017\ErrorCorrecting_Factorization\figure_10.jpg
  Figure 10 caption: (a) Summary of performance of ECF-H method over all datasets
    using the number of SVs and the number of dichotomies as the measure of complexity,
    respectively for ECF-H (a)(d), ECF-E (b)(e) and OVA (c)(f).
  Figure 2 Link: articels_figures_by_rev_year\2017\ErrorCorrecting_Factorization\figure_2.jpg
  Figure 2 caption: (a) SVM RBF boundaries learned from Error-Correcting Factorization
    along with the ECOC coding matrix X in a Toy problem, 77.12 percent classification
    accuracy (12 classifiers are trained). (b) Boundaries learned by the Dense Random
    ECOC coding design, 66.45 percent classification accuracy (12 classifiers are
    trained). (c) SVM boundaries induced by the One versus All approach, 49.53 percent
    classification accuracy (14 classifiers are trained).
  Figure 3 Link: articels_figures_by_rev_year\2017\ErrorCorrecting_Factorization\figure_3.jpg
  Figure 3 caption: Example of global versus pair-wise correction capability. On the
    left side of the fig. the calculation of the global correction capability is shown.
    The right side of the image shows a sample of pair-wise correction calculation
    for codewords x 2 and x 8 .
  Figure 4 Link: articels_figures_by_rev_year\2017\ErrorCorrecting_Factorization\figure_4.jpg
  Figure 4 caption: mathbf D matrix for the Traffic (a) and ARFace (b) datasets. mathbf
    Xmathbf Xtop term obtained via ECF for Traffic (c) and ARFace (d) datasets. ECOC
    coding matrix mathbf X obtained with ECF for Traffic (e) and ARFace (f).
  Figure 5 Link: articels_figures_by_rev_year\2017\ErrorCorrecting_Factorization\figure_5.jpg
  Figure 5 caption: Mean Frobenius norm value with standard deviation as a function
    of the number of coordinate updates on 50 different trials. The blue shaded area
    corresponds to cyclic update while the red area denotes random coordinate updates
    for Vowel (a) and ARFAce (b) datasets.
  Figure 6 Link: articels_figures_by_rev_year\2017\ErrorCorrecting_Factorization\figure_6.jpg
  Figure 6 caption: Visual examples for the ARFace and Traffic datasets.
  Figure 7 Link: articels_figures_by_rev_year\2017\ErrorCorrecting_Factorization\figure_7.jpg
  Figure 7 caption: Multi-class classification accuracy (y axis) as a function of
    the relative computational complexity (x axis) for all datasets and both decoding
    measures, using SVM-RBF as the base classifier.
  Figure 8 Link: articels_figures_by_rev_year\2017\ErrorCorrecting_Factorization\figure_8.jpg
  Figure 8 caption: Multi-class classification accuracy (y axis) as a function of
    the number of dichotomies for all datasets and both decoding measures (x axis),
    using SVM-RBF as the base classifier.
  Figure 9 Link: articels_figures_by_rev_year\2017\ErrorCorrecting_Factorization\figure_9.jpg
  Figure 9 caption: Multi-class classification accuracy (y axis) as a function of
    the number of dichotomies for all datasets and both decoding measures (x axis),
    using Random Forests as base classifier.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "Miguel \xC1ngel Bautista Martin"
  Name of the last author: Sergio Escalera
  Number of Figures: 10
  Number of Tables: 2
  Number of authors: 4
  Paper title: Error-Correcting Factorization
  Publication Date: 2017-10-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Dataset Characteristics
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Percentage of Wins Over All Datasets of Each Method Using
      as a Complexity Measure the Number of SVs and the Number of Classifiers
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2763146
- Affiliation of the first author: northeastern university, somerville, ma, usa
  Affiliation of the last author: northeastern university, somerville, ma, usa
  Figure 1 Link: articels_figures_by_rev_year\2017\Partition_Level_Constrained_Clustering\figure_1.jpg
  Figure 1 caption: The comparison between pairwise constraints and partition level
    side information. In (a), we cannot decide a Must-Link or Cannot-link only based
    on two instances; compared (b) with (c), it is more natural to label the instances
    in well-organised way, such as partition level rather than pairwise constraint.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Partition_Level_Constrained_Clustering\figure_2.jpg
  Figure 2 caption: "Impact of \u03BB on satimage and pendigits."
  Figure 3 Link: articels_figures_by_rev_year\2017\Partition_Level_Constrained_Clustering\figure_3.jpg
  Figure 3 caption: Improvement of constrained clustering on glass and wine compared
    with K-means.
  Figure 4 Link: articels_figures_by_rev_year\2017\Partition_Level_Constrained_Clustering\figure_4.jpg
  Figure 4 caption: Impact of noisy side information on breast and pendigits.
  Figure 5 Link: articels_figures_by_rev_year\2017\Partition_Level_Constrained_Clustering\figure_5.jpg
  Figure 5 caption: Impact of the number of side information.
  Figure 6 Link: articels_figures_by_rev_year\2017\Partition_Level_Constrained_Clustering\figure_6.jpg
  Figure 6 caption: Performance with inconsistent cluster number on four large scale
    data sets.
  Figure 7 Link: articels_figures_by_rev_year\2017\Partition_Level_Constrained_Clustering\figure_7.jpg
  Figure 7 caption: Illustration of the proposed SG-PLCC model.
  Figure 8 Link: articels_figures_by_rev_year\2017\Partition_Level_Constrained_Clustering\figure_8.jpg
  Figure 8 caption: Cosegmentation results of SG-PLCC on six image groups.
  Figure 9 Link: articels_figures_by_rev_year\2017\Partition_Level_Constrained_Clustering\figure_9.jpg
  Figure 9 caption: Some challenging examples for our SG-PLCC model.
  First author gender probability: 0.7
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Hongfu Liu
  Name of the last author: Yun Fu
  Number of Figures: 9
  Number of Tables: 8
  Number of authors: 3
  Paper title: Partition Level Constrained Clustering
  Publication Date: 2017-10-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Contingency Matrix
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Notations
  Table 3 caption:
    table_text: TABLE 3 Experimental Data Sets
  Table 4 caption:
    table_text: TABLE 4 Clustering Performance on Seven Real Datasets by NMI
  Table 5 caption:
    table_text: TABLE 5 Clustering Performance on Seven Real Datasets by R n
  Table 6 caption:
    table_text: TABLE 6 Comparison of Execution Time (in Seconds)
  Table 7 caption:
    table_text: TABLE 7 Clustering Performance of Our Method and Different Priors
      on iCoseg Dataset
  Table 8 caption:
    table_text: TABLE 8 Comparison of Segmentation Accuracy on iCoseg Dataset
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2763945
- Affiliation of the first author: university of tokyo, bunkyo, japan
  Affiliation of the last author: university of tokyo, bunkyo, japan
  Figure 1 Link: articels_figures_by_rev_year\2017\Continuous_D_Label_Stereo_Matching_Using_Local_Expansion_Moves\figure_1.jpg
  Figure 1 caption: Evolution of our stereo matching estimates. From top to bottom,
    we show disparity maps, normal maps of disparity planes, and error maps with 0.5
    pixel threshold where ground truth is given. In our framework, we start with random
    disparities that are represented by per-pixel 3D planes, i.e., disparities (top)
    and normals (middle). We then iteratively apply our local expansion moves using
    GC (middles) to update and propagate local disparity planes. Finally, the resulting
    disparity map is further refined at a post-processing stage using left-right consistency
    check and weighted median filtering (rightmost).
  Figure 10 Link: articels_figures_by_rev_year\2017\Continuous_D_Label_Stereo_Matching_Using_Local_Expansion_Moves\figure_10.jpg
  Figure 10 caption: Efficiency and accuracy comparison with PMBP [3]. Our methods
    achieve much faster convergence, reaching lower energies and better accuracies
    at the convergence. Accuracies are evaluated for all-regions at each iteration.
    See also Fig. 11 for visual comparison.
  Figure 2 Link: articels_figures_by_rev_year\2017\Continuous_D_Label_Stereo_Matching_Using_Local_Expansion_Moves\figure_2.jpg
  Figure 2 caption: Illustration of the smoothness term proposed in [33]. (a) The
    smoothness term penalizes the deviations of neighboring disparity planes shown
    as red arrows. (b) When neighboring pixels are assigned the same disparity plane,
    it gives no penalty; thus, it enforces second order smoothness for the disparity
    maps.
  Figure 3 Link: articels_figures_by_rev_year\2017\Continuous_D_Label_Stereo_Matching_Using_Local_Expansion_Moves\figure_3.jpg
  Figure 3 caption: "Illustration of the proposed local expansion moves. The local\
    \ expansion moves consist of many small \u03B1 -expansions (or local \u03B1 -expansions),\
    \ which are defined using grid structures such shown in the left figure. These\
    \ local \u03B1 -expansions are defined at each grid-cell and applied for 3\xD7\
    3 neighborhood cells (or expansion regions). In the middle part, we illustrate\
    \ how each of local \u03B1 -expansions works. (1) The candidate label \u03B1 (i.e.,\
    \ \u03B1=(a,b,c) representing a disparity plane d=au+bv+c ) is produced by randomly\
    \ choosing and perturbing one of the currently assigned labels in its center cell.\
    \ (2) The current labels in the expansion region are updated by \u03B1 in an energy\
    \ minimization manner using GC. Consequently, a current label in the center cell\
    \ can be propagated for its surrounding cells. In the right part, local \u03B1\
    \ -expansions are visualized as small patches on stacked layers with three different\
    \ sizes of grid structures. As shown here, using local \u03B1 -expansions we can\
    \ localize the scopes of label searching by their locations. Each layer represents\
    \ a group of mutually-disjoint local \u03B1 -expansions, which are performed individually\
    \ in a parallel manner."
  Figure 4 Link: articels_figures_by_rev_year\2017\Continuous_D_Label_Stereo_Matching_Using_Local_Expansion_Moves\figure_4.jpg
  Figure 4 caption: "Group index for \u03B1 ij -expansions. We perform \u03B1 ij -expansions\
    \ in the same group in parallel."
  Figure 5 Link: articels_figures_by_rev_year\2017\Continuous_D_Label_Stereo_Matching_Using_Local_Expansion_Moves\figure_5.jpg
  Figure 5 caption: "Expansion regions of mutually disjoint \u03B1 ij -expansions\
    \ (group index k=0 ). We leave white gaps between neighbors."
  Figure 6 Link: articels_figures_by_rev_year\2017\Continuous_D_Label_Stereo_Matching_Using_Local_Expansion_Moves\figure_6.jpg
  Figure 6 caption: Filtering region M ij . The margin width r corresponds with the
    radius of the matching window W p .
  Figure 7 Link: articels_figures_by_rev_year\2017\Continuous_D_Label_Stereo_Matching_Using_Local_Expansion_Moves\figure_7.jpg
  Figure 7 caption: Effect of grid-cell sizes. We use LE-GF with different combinations
    of grid structures. The S, M, and L denote small, medium, and large grid-cells,
    respectively. The joint use of different sizes of grid-cells improves the performance.
    See also Fig. 8 for visual comparison.
  Figure 8 Link: articels_figures_by_rev_year\2017\Continuous_D_Label_Stereo_Matching_Using_Local_Expansion_Moves\figure_8.jpg
  Figure 8 caption: Visual effect of grid-cell sizes. The use of larger grid-cells
    leads to smoother solutions and effective for occluded regions. The proposed combination
    (S, M, L) well balances localization and spatial propagation and performs best.
    These are all raw results without post-processing.
  Figure 9 Link: articels_figures_by_rev_year\2017\Continuous_D_Label_Stereo_Matching_Using_Local_Expansion_Moves\figure_9.jpg
  Figure 9 caption: Efficiency evaluation in comparison to our previous algorithm
    (LSL) [39]. Accuracies are evaluated for all-regions at each iteration.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Tatsunori Taniai
  Name of the last author: Takeshi Naemura
  Number of Figures: 16
  Number of Tables: 4
  Number of authors: 4
  Paper title: Continuous 3D Label Stereo Matching Using Local Expansion Moves
  Publication Date: 2017-10-24 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Middlebury Benchmark V2 for 0.5-Pixel Accuracy
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Middlebury Benchmark V3 for the bad 2.0 nonocc Metric
  Table 3 caption:
    table_text: TABLE 3 Effect of Our Post-Processing (PP) and RANSAC Proposer (RP)
  Table 4 caption:
    table_text: TABLE 4 Accuracy Comparison with Olsson et al. [33]
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2766072
- Affiliation of the first author: department of computer science and technology,
    tsinghua university, beijing, china
  Affiliation of the last author: department of computer science and technology, tsinghua
    university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2017\MaxMargin_Deep_Generative_Models_for_SemiSupervised_Learning\figure_1.jpg
  Figure 1 caption: 'a) and b): Graphical models of mmDGMs when labels are given or
    missing. c) and d): Graphical models of mmDCGMs when labels are given or missing.
    The solid line and the dash dot line represent the generative model and recognition
    model respectively. The dot line stands for the max-margin classifier. Compared
    with mmDGMs, mmDCGMs disentangle the label information from the latent variables
    and separate the pathways of inferring labels and latent variables.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\MaxMargin_Deep_Generative_Models_for_SemiSupervised_Learning\figure_2.jpg
  Figure 2 caption: Network architecture of Conv-MMVA with conv-net in the recognition
    model and unconv-net in the generative model (best view in color).
  Figure 3 Link: articels_figures_by_rev_year\2017\MaxMargin_Deep_Generative_Models_for_SemiSupervised_Learning\figure_3.jpg
  Figure 3 caption: 'Generation on MNIST. (a-b): images randomly generated by VA and
    MMVA respectively; (c-d): images randomly generated by Conv-VA and Conv-MMVA respectively.
    Our mmDGMs retain similar ability as the baselines to generate images.'
  Figure 4 Link: articels_figures_by_rev_year\2017\MaxMargin_Deep_Generative_Models_for_SemiSupervised_Learning\figure_4.jpg
  Figure 4 caption: 'Generation on SVHN. (a): training data preprocessed by LCN; (b):
    samples randomly generated by Conv-VA; (c-d): samples randomly generated by Conv-MMVA
    when C= 10 3 and C= 10 4 respectively.'
  Figure 5 Link: articels_figures_by_rev_year\2017\MaxMargin_Deep_Generative_Models_for_SemiSupervised_Learning\figure_5.jpg
  Figure 5 caption: 'Imputation results of MMVA in two noising conditions: column
    1 shows the true data; column 2 shows the perturbed data; and the remaining columns
    show the imputations for 20 iterations.'
  Figure 6 Link: articels_figures_by_rev_year\2017\MaxMargin_Deep_Generative_Models_for_SemiSupervised_Learning\figure_6.jpg
  Figure 6 caption: Effect of size of labeled data on MNIST. The results are averaged
    over four runs given different random seeds. For each run, we enlarge the labeled
    data set from 100 to 1,000 gradually by adding 100 labels per time. Generally,
    the error rates decrease as the number of labels increase and the peaks may be
    caused by the poor quality of new added labeled data. Nevertheless, 800 labels
    are sufficient to achieve an error rate that is comparable to the supervised learning
    results of other DGMs.
  Figure 7 Link: articels_figures_by_rev_year\2017\MaxMargin_Deep_Generative_Models_for_SemiSupervised_Learning\figure_7.jpg
  Figure 7 caption: Class-conditional generation on MNIST (100 labels) and SVHN (1000
    labels) datasets. (a) and (c) present 100 labeled training data sorted by class
    on MNIST and SVHN datasets respectively. (b) and (d) show samples on corresponding
    datasets where each row shares same class y and each column shares same latent
    variables z .
  Figure 8 Link: articels_figures_by_rev_year\2017\MaxMargin_Deep_Generative_Models_for_SemiSupervised_Learning\figure_8.jpg
  Figure 8 caption: Class-conditional generation on NORB dataset (1000 labels). (a)
    and (b) are labeled training data and generated samples respectively.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.83
  Name of the first author: Chongxuan Li
  Name of the last author: Bo Zhang
  Number of Figures: 8
  Number of Tables: 7
  Number of authors: 3
  Paper title: Max-Margin Deep Generative Models for (Semi-)Supervised Learning
  Publication Date: 2017-10-24 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Error Rates (%) on the MNIST Dataset Given Full Labeled Data
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Error Rates (%) on the SVHN Dataset Given Full Labeled Data
  Table 3 caption:
    table_text: TABLE 3 Effects of C on the MNIST Dataset in Conv-MMVA
  Table 4 caption:
    table_text: TABLE 4 MSE on MNIST Data with Missing Values in the Testing Procedure
  Table 5 caption:
    table_text: TABLE 5 Error Rates(%) with Missing Values on MNIST
  Table 6 caption:
    table_text: TABLE 6 Error Rates (%) on (Partially) Labeled MNIST Dataset
  Table 7 caption:
    table_text: TABLE 7 Error Rates (%) on SVHN and NORB Datasets Given 1000 Labels
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2766142
- Affiliation of the first author: adobe research, san jose, ca
  Affiliation of the last author: department of electrical and computer engineering,
    northeastern university, boston, ma
  Figure 1 Link: articels_figures_by_rev_year\2017\Person_ReIdentification_by_CrossView_MultiLevel_Dictionary_Learning\figure_1.jpg
  Figure 1 caption: Framework of our CMDL approach. Solid boxes represent variables
    related to view 1, while dashed boxes represent variables related to view 2. CMDL
    learns three pairs of dictionaries at three representation levels, and finally
    fuses the matching results. The shaded areas indicate view-consistency constraints.
    In particular, two views share similar codings (e.g., P (1,1) X (1,1) , P (2,1)
    X (2,1) ) in the image-level and part-level, and share similar dictionary (i.e.,
    D (1,3) , D (2,3) ) in patch-level.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Person_ReIdentification_by_CrossView_MultiLevel_Dictionary_Learning\figure_2.jpg
  Figure 2 caption: 'Illustration of images in five benchmark datasets: (a) VIPeR;
    (b) CUHK01 Campus; (c) iLIDS; (d) GRID and (e) PRID450S.'
  Figure 3 Link: articels_figures_by_rev_year\2017\Person_ReIdentification_by_CrossView_MultiLevel_Dictionary_Learning\figure_3.jpg
  Figure 3 caption: CMC curves of average matching rates on VIPeR dataset. Rank-1
    matching rate is marked before the name of each approach.
  Figure 4 Link: articels_figures_by_rev_year\2017\Person_ReIdentification_by_CrossView_MultiLevel_Dictionary_Learning\figure_4.jpg
  Figure 4 caption: CMC curves of average matching rates on CUHK01 dataset. Rank-1
    matching rate is marked before the name of each approach.
  Figure 5 Link: articels_figures_by_rev_year\2017\Person_ReIdentification_by_CrossView_MultiLevel_Dictionary_Learning\figure_5.jpg
  Figure 5 caption: Experimental analysis on VIPeR dataset. (a) Rank-1 matching rates
    versus different values of parameters; (b) Matching rates of image-level model,
    patch-level model and the fusion model; (c) Rank-1 matching rates versus different
    dictionary sizes.
  Figure 6 Link: articels_figures_by_rev_year\2017\Person_ReIdentification_by_CrossView_MultiLevel_Dictionary_Learning\figure_6.jpg
  Figure 6 caption: Rank-1 and Rank-5 matching rates versus different values of gamma
    1 and gamma 2 for score fusion on VIPeR dataset.
  Figure 7 Link: articels_figures_by_rev_year\2017\Person_ReIdentification_by_CrossView_MultiLevel_Dictionary_Learning\figure_7.jpg
  Figure 7 caption: Ranking of the true gallery samples after matching on VIPeR dataset.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.76
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Sheng Li
  Name of the last author: Yun Fu
  Number of Figures: 7
  Number of Tables: 8
  Number of authors: 3
  Paper title: Person Re-Identification by Cross-View Multi-Level Dictionary Learning
  Publication Date: 2017-10-26 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Top Ranked Matching Rates in (%) with 316 Persons on VIPeR
      Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparisons with State-of-the-Art Ranking and Ensemble Methods
      on VIPeR Dataset
  Table 3 caption:
    table_text: TABLE 3 Top Ranked Matching Rates in (%) on CUHK01 Dataset
  Table 4 caption:
    table_text: TABLE 4 Top Ranked Matching Rates in (%) on iLIDS Dataset
  Table 5 caption:
    table_text: TABLE 5 Top Ranked Matching Rates in (%) with on GRID Dataset
  Table 6 caption:
    table_text: TABLE 6 Top Ranked Matching Rates in (%) on PRID450S Dataset
  Table 7 caption:
    table_text: TABLE 7 Top Ranked Matching Rates in (%) on PRID2011 Dataset
  Table 8 caption:
    table_text: TABLE 8 Top Ranked Matching Rates in (%) on iLIDS-VID Dataset
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2764893
- Affiliation of the first author: cnrs-aist joint robotics laboratory, umi3218rl
    tsukuba central 1, tsukuba, ibaraki, japan
  Affiliation of the last author: cnrs-aist joint robotics laboratory, umi3218rl tsukuba
    central 1, tsukuba, ibaraki, japan
  Figure 1 Link: articels_figures_by_rev_year\2017\HandObject_Contact_Force_Estimation_from_Markerless_Visual_Tracking\figure_1.jpg
  Figure 1 caption: Instrumented devices of adjustable physical and grasping properties
    (a-c), or based on everyday objects to allow intuitive interactions (d-f).
  Figure 10 Link: articels_figures_by_rev_year\2017\HandObject_Contact_Force_Estimation_from_Markerless_Visual_Tracking\figure_10.jpg
  Figure 10 caption: Forces predicted from vision (red) versus ground-truth (grey).
  Figure 2 Link: articels_figures_by_rev_year\2017\HandObject_Contact_Force_Estimation_from_Markerless_Visual_Tracking\figure_2.jpg
  Figure 2 caption: Force distributions computed only by physics-based optimization
    are guaranteed to result in the observed motion (net force and torque) but can
    significantly differ from the real distributions at the finger level.
  Figure 3 Link: articels_figures_by_rev_year\2017\HandObject_Contact_Force_Estimation_from_Markerless_Visual_Tracking\figure_3.jpg
  Figure 3 caption: By SOCP, we extract force distributions compatible with the observed
    motion in the vicinity of target forces (measured or predicted).
  Figure 4 Link: articels_figures_by_rev_year\2017\HandObject_Contact_Force_Estimation_from_Markerless_Visual_Tracking\figure_4.jpg
  Figure 4 caption: Current forces are predicted from current motion and past forces.
  Figure 5 Link: articels_figures_by_rev_year\2017\HandObject_Contact_Force_Estimation_from_Markerless_Visual_Tracking\figure_5.jpg
  Figure 5 caption: Open-loop, offline correction and closed-loop force prediction.
  Figure 6 Link: articels_figures_by_rev_year\2017\HandObject_Contact_Force_Estimation_from_Markerless_Visual_Tracking\figure_6.jpg
  Figure 6 caption: "Open-loop, offline-corrected and closed-loop force predictions\
    \ for KDN-VF- \u0394 . Open-loop forces drift away from physically plausible forces,\
    \ becoming negative. Compatibility with the observed motion is enforced through\
    \ offline correction or closed-loop control at each time step."
  Figure 7 Link: articels_figures_by_rev_year\2017\HandObject_Contact_Force_Estimation_from_Markerless_Visual_Tracking\figure_7.jpg
  Figure 7 caption: "Tracking the articulated hand pose together with the object rigid\
    \ motion produces (a) inaccurate object and contact positions that (b) propagate\
    \ to forces. Baseline errors (std. dev.): contact: 10 mm (pos.), kinematics: 2.9\
    \ m s \u22122 (trans. acc.), 30.9 rad s \u22121 (rot. vel.) ."
  Figure 8 Link: articels_figures_by_rev_year\2017\HandObject_Contact_Force_Estimation_from_Markerless_Visual_Tracking\figure_8.jpg
  Figure 8 caption: The hand and the object are tracked as a rigid compound.
  Figure 9 Link: articels_figures_by_rev_year\2017\HandObject_Contact_Force_Estimation_from_Markerless_Visual_Tracking\figure_9.jpg
  Figure 9 caption: Forces from closed-loop KDN-VF-F and random initialization.
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Tu-Hoa Pham
  Name of the last author: Abderrahmane Kheddar
  Number of Figures: 11
  Number of Tables: 6
  Number of authors: 4
  Paper title: Hand-Object Contact Force Estimation from Markerless Visual Tracking
  Publication Date: 2017-10-26 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Force Estimation Errors on Complete Sequences-Mean (Std. Dev.)
      [N]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Force Estimation Errors for Proposed Prediction-Correction
      Architectures over Increasing Sequence Durations-Mean (Std. Dev.) [N]
  Table 3 caption:
    table_text: TABLE 3 Force Estimation Errors for Ground-Truth versus Random Force
      Initialization Over Increasing Sequence Durations-Mean (Std. Dev.) [N]
  Table 4 caption:
    table_text: TABLE 4 Kinematics and Force from Vision-Mean (Std. Dev.) Errors From
      Central Difference, Gaussian Filtering and Algebraic Differentiation
  Table 5 caption:
    table_text: TABLE 5 Force Estimation Errors From AHRS versus Vision - Mean (Std.
      Dev.) [N]
  Table 6 caption:
    table_text: TABLE 6 Computation Time Decomposition by Process
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2759736
- Affiliation of the first author: beijing key laboratory of traffic data analysis
    and mining, beijing jiaotong university, beijing, china
  Affiliation of the last author: beijing key laboratory of traffic data analysis
    and mining, beijing jiaotong university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2017\Graph_Matching_with_Adaptive_and_Branching_Path_Following\figure_1.jpg
  Figure 1 caption: Illustration of (a) generic predictor corrector (GPC) and (b)
    the proposed adaptive path estimation (APE). The blue curve indicates the solution
    path of the parameterized nonlinear equation system.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Graph_Matching_with_Adaptive_and_Branching_Path_Following\figure_2.jpg
  Figure 2 caption: Evaluation on the synthetic dataset under different settings.
  Figure 3 Link: articels_figures_by_rev_year\2017\Graph_Matching_with_Adaptive_and_Branching_Path_Following\figure_3.jpg
  Figure 3 caption: Comparison of graph matching on 2D point sets.
  Figure 4 Link: articels_figures_by_rev_year\2017\Graph_Matching_with_Adaptive_and_Branching_Path_Following\figure_4.jpg
  Figure 4 caption: Comparison of graph matching on the CMU house dataset.
  Figure 5 Link: articels_figures_by_rev_year\2017\Graph_Matching_with_Adaptive_and_Branching_Path_Following\figure_5.jpg
  Figure 5 caption: An example of graph matching on the CMU house dataset. The algorithm,
    the number of true matches per ground truths for each subfigure are captioned.
    Graph edges are represented by yellow lines, true matches by green lines and false
    matches by red lines (best viewed in color, and the same style is also used for
    Figs. 6, 8 and 9).
  Figure 6 Link: articels_figures_by_rev_year\2017\Graph_Matching_with_Adaptive_and_Branching_Path_Following\figure_6.jpg
  Figure 6 caption: A matching example of motorbike images in the Pascal dataset.
  Figure 7 Link: articels_figures_by_rev_year\2017\Graph_Matching_with_Adaptive_and_Branching_Path_Following\figure_7.jpg
  Figure 7 caption: Evaluation on (a) the Pascal dataset and (b) the Willow dataset.
  Figure 8 Link: articels_figures_by_rev_year\2017\Graph_Matching_with_Adaptive_and_Branching_Path_Following\figure_8.jpg
  Figure 8 caption: A matching example of duck images on the Willow dataset.
  Figure 9 Link: articels_figures_by_rev_year\2017\Graph_Matching_with_Adaptive_and_Branching_Path_Following\figure_9.jpg
  Figure 9 caption: A matching example on the Caltech dataset.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Tao Wang
  Name of the last author: Songhe Feng
  Number of Figures: 9
  Number of Tables: 3
  Number of authors: 4
  Paper title: Graph Matching with Adaptive and Branching Path Following
  Publication Date: 2017-10-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Graph Matching on the Caltech Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison with Baselines in Matching Accuracy (%) and Computational
      Time (Seconds) on Four Image Datasets
  Table 3 caption:
    table_text: TABLE 3 Comparison in Computational Time (Seconds) on Four Image Datasets
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2767591
