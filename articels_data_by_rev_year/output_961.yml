- Affiliation of the first author: school of informatics, university of edinburgh,
    edinburgh, united kingdom
  Affiliation of the last author: vgg, university of oxford, oxford, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2017\Action_Recognition_with_Dynamic_Image_Networks\figure_1.jpg
  Figure 1 caption: Examples of dynamic images summarizing short video sequences as
    still images. They provide a simple, powerful, and efficient representation of
    videos for action recognition. Can you guess what actions are visualized?. 1 1.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Action_Recognition_with_Dynamic_Image_Networks\figure_2.jpg
  Figure 2 caption: 'Left column: Dynamic images. Right column: Motion blur. Although
    fundamentally different both methodologically, as well as in terms of applications,
    they both seem to capture time in a similar manner.'
  Figure 3 Link: articels_figures_by_rev_year\2017\Action_Recognition_with_Dynamic_Image_Networks\figure_3.jpg
  Figure 3 caption: "The graph compares the approximate rank pooling weighting functions\
    \ \u03B1 t (for T=5 , T=10 and T=20 samples) of Eq. (3) using time-averaged feature\
    \ frames V t to the variant Eq. (2) that ranks directly the feature frames \u03C8\
    \ t as is."
  Figure 4 Link: articels_figures_by_rev_year\2017\Action_Recognition_with_Dynamic_Image_Networks\figure_4.jpg
  Figure 4 caption: Illustration of various dynamic imagemap network architectures.
  Figure 5 Link: articels_figures_by_rev_year\2017\Action_Recognition_with_Dynamic_Image_Networks\figure_5.jpg
  Figure 5 caption: The illustration of four stream dynamic image architecture that
    combines RGB data, optical flow with dynamic images and dynamic optical flow.
  Figure 6 Link: articels_figures_by_rev_year\2017\Action_Recognition_with_Dynamic_Image_Networks\figure_6.jpg
  Figure 6 caption: Comparing Dynamic Images (DI) to Motion History Images (MHI) [3]
    .The top row shows representative frames from different videos, middle and bottom
    rows depict MHI and DI of corresponding videos respectively. While both methods
    can represent the evolution of pixels along time, our method produces more interpretable
    images which are more robust to long-range and background motion.
  Figure 7 Link: articels_figures_by_rev_year\2017\Action_Recognition_with_Dynamic_Image_Networks\figure_7.jpg
  Figure 7 caption: Comparison between four score profiles and pairwise ranking accuracies
    (%) of ranking functions for approximate rank pooling (ARP) and rank pooling (RP).
    Generally the approximate rank pooling follows the trend of rank pooling.
  Figure 8 Link: articels_figures_by_rev_year\2017\Action_Recognition_with_Dynamic_Image_Networks\figure_8.jpg
  Figure 8 caption: "Visual analysis of different window sizes ( \u03C4 ) on dynamic\
    \ images. The top, middle, and bottom rows depict dynamic images for \u03C4=10\
    \ , \u03C4=50 , and \u03C4= (whole video length), respectively. Best seen in color."
  Figure 9 Link: articels_figures_by_rev_year\2017\Action_Recognition_with_Dynamic_Image_Networks\figure_9.jpg
  Figure 9 caption: Visualizing static images (SI), dynamic images (DI), optical flow
    (OF), and dynamic optical flow (DOF) in each row, respectively. Best seen in color.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.82
  Name of the first author: Hakan Bilen
  Name of the last author: Andrea Vedaldi
  Number of Figures: 9
  Number of Tables: 9
  Number of authors: 4
  Paper title: Action Recognition with Dynamic Image Networks
  Publication Date: 2017-11-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparing the Performance of Various Single Image Video Representation
      Methods on Split-1 the UCF101 Dataset in Terms of Mean Class Accuracy (%)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Approximate Rank Pooling versus Rank Pooling in Terms of Speed,
      Ranking Accuracy (%), and Classification Accuracy (%)
  Table 3 caption:
    table_text: "TABLE 3 MDI: Effect of Window Size ( \u03C4 ) and Stride ( s ) in\
      \ Terms of Multi-Class Classification Accuracy in the First Split of the UCF101"
  Table 4 caption:
    table_text: TABLE 4 Classification Accuracy (%) for Dynamic Image and Map Networks
      at Different Depths on the UCF101 and HMDB51 Datasets
  Table 5 caption:
    table_text: TABLE 5 Classification Accuracy (%) for Approximate Rank Pooling and
      Parametric Pooling at Different Depths on the UCF101 and HMDB51 Datasets
  Table 6 caption:
    table_text: TABLE 6 Classification Accuracy (%) with Dynamic Images When Using
      CaffeNet [28] and Deeper Convolutional Network Architectures, Specifically ResNeXt-50
      [76]
  Table 7 caption:
    table_text: 'TABLE 7 Optical Flow and Dynamic Optical Flow Streams: A Two-Stream
      ResNeXt-50 Architecture for Action Classification in Terms of Mean Multi-Class
      Accuracy (%)'
  Table 8 caption:
    table_text: TABLE 8 Combinations of Various RGB and Optical Flow Streams with
      ResNeXt-50 in Terms of Mean Multi-Class Accuracy (%)
  Table 9 caption:
    table_text: TABLE 9 Comparison with the State-of-the-art in Terms of Mean Multi-Class
      Accuracy (%)
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2769085
- Affiliation of the first author: college of science, national university of defense
    technology, changsha, hunan, china
  Affiliation of the last author: national key laboratory for novel software technology,
    nanjing university, nanjing, china
  Figure 1 Link: articels_figures_by_rev_year\2017\OnePass_Learning_with_Incremental_and_Decremental_Features\figure_1.jpg
  Figure 1 caption: The illustration of environment monitoring.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\OnePass_Learning_with_Incremental_and_Decremental_Features\figure_2.jpg
  Figure 2 caption: 'OPID notations. The feature and instance involve simultaneously.
    With time elapsing, we have three types of features: vanished feature, survived
    feature and augmented feature, together with data in two stages, i.e., C-stage
    and E-stage.'
  Figure 3 Link: articels_figures_by_rev_year\2017\OnePass_Learning_with_Incremental_and_Decremental_Features\figure_3.jpg
  Figure 3 caption: Multi-shot OPID notations.
  Figure 4 Link: articels_figures_by_rev_year\2017\OnePass_Learning_with_Incremental_and_Decremental_Features\figure_4.jpg
  Figure 4 caption: Comparison to the benchmark. Pegasos, OPMV and TCA should assume
    that all the vanished features are known in E-stage and all the augmented features
    are known in C-stage. We name them as the benchmark and they are not applicable
    to our setting.
  Figure 5 Link: articels_figures_by_rev_year\2017\OnePass_Learning_with_Incremental_and_Decremental_Features\figure_5.jpg
  Figure 5 caption: Experimental results with compact and original representations.
    SVM trained on compact representation mathbfZT1+1mathrm (s) are named as Compact.
    If the input of OPID, i.e., mathbfZT1+1mathrm (s) is replaced by the original
    representation mathbfXT1+1mathrm (s) , the results are named as Original.
  Figure 6 Link: articels_figures_by_rev_year\2017\OnePass_Learning_with_Incremental_and_Decremental_Features\figure_6.jpg
  Figure 6 caption: Accuracy comparison of different methods with different percents
    of vanished features, survived features and augmented features.
  Figure 7 Link: articels_figures_by_rev_year\2017\OnePass_Learning_with_Incremental_and_Decremental_Features\figure_7.jpg
  Figure 7 caption: Parameter influence of OPID and OPIDe in one-shot scenario. The
    x -axis represents the variable lambda , y -axis represents gamma and z -axis
    is the accuracy. Since OPIDe does not has the parameter gamma , we draw its results
    in a x - z plane with y=0 .
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.75
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Chenping Hou
  Name of the last author: Zhi-Hua Zhou
  Number of Figures: 7
  Number of Tables: 7
  Number of authors: 2
  Paper title: One-Pass Learning with Incremental and Decremental Features
  Publication Date: 2017-11-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Details About Experimental Setting
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 The One-Shot Testing Accuracies (mean \xB1 std) of the Compared\
      \ Methods on 10 Data Sets with Different Number of Training and Testing Examples"
  Table 3 caption:
    table_text: "TABLE 3 The Multi-Shot Testing Accuracies (mean \xB1 std.) of the\
      \ Compared Methods on 6 Data Sets with Different Number of Training and Testing\
      \ Examples in Task 1"
  Table 4 caption:
    table_text: "TABLE 4 The Multi-Shot Testing Accuracies (mean \xB1 std.) of the\
      \ Compared Methods on 6 Data Sets with Different Number of Training and Testing\
      \ Examples in Task 2"
  Table 5 caption:
    table_text: "TABLE 5 Computational Time (mean \xB1 std) of Different Methods"
  Table 6 caption:
    table_text: TABLE 6 The Details About Experimental Setting on Real Data
  Table 7 caption:
    table_text: "TABLE 7 The Testing Accuracies (mean \xB1 std.) of the Compared Methods\
      \ on the Real Data Sets with Different Percent of Training Points"
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2769047
- Affiliation of the first author: hikvision research america, santa clara, ca
  Affiliation of the last author: university of california at merced, merced, ca
  Figure 1 Link: articels_figures_by_rev_year\2017\Deblurring_LowLight_Images_with_Light_Streaks\figure_1.jpg
  Figure 1 caption: Deblurring a low-light image. The amount of salient image features
    that can be extracted from the low-light image is limited. This leads to the failure
    of the state-of-the-art methods [4], [9] on the image. The proposed method (d)
    improves the results by detecting and making use of the light streaks (the detected
    light streaks are shown in the boxes of (a)).
  Figure 10 Link: articels_figures_by_rev_year\2017\Deblurring_LowLight_Images_with_Light_Streaks\figure_10.jpg
  Figure 10 caption: Non-uniform deblurring on a real image by the evaluated methods.
  Figure 2 Link: articels_figures_by_rev_year\2017\Deblurring_LowLight_Images_with_Light_Streaks\figure_2.jpg
  Figure 2 caption: Examples of light streak detection. The red box indicates the
    best light streak patch and the green boxes show additional light streak patches
    that are automatically identified by the proposed algorithm.
  Figure 3 Link: articels_figures_by_rev_year\2017\Deblurring_LowLight_Images_with_Light_Streaks\figure_3.jpg
  Figure 3 caption: "Distribution fitting to empirical image gradient statistics of\
    \ a typical low-light image (top) with Gaussian \u03B1=2 , Laplacian \u03B1=1\
    \ and hyper-Laplacian \u03B1=0.8 ."
  Figure 4 Link: articels_figures_by_rev_year\2017\Deblurring_LowLight_Images_with_Light_Streaks\figure_4.jpg
  Figure 4 caption: Example of light streak detection for non-uniform blurred images.
    (a) A spatial-variant blurry image with direct application of the light streak
    detection method to tiled regions. (b) A spatial-variant blurry image with the
    light streak detection method and cross validation. (c) imperfect light streaks
    that are detected in (a) but are removed by cross validation in (b). False positives
    can be removed by cross validation of detected light streaks.
  Figure 5 Link: articels_figures_by_rev_year\2017\Deblurring_LowLight_Images_with_Light_Streaks\figure_5.jpg
  Figure 5 caption: A comparison with the approach by Hua and Low [27]. The image
    region enclosed by the red box is a manually selected light streak, and the region
    enclosed by the green box is determined by the proposed algorithm. (a) Input image;
    (b) a cropped region from (a); (c) & (e) cropped results by Hua and Low [27],
    using the red and green light streak patches, respectively; (d) & (f) cropped
    results by our method, using the red and green light streak patches, respectively.
  Figure 6 Link: articels_figures_by_rev_year\2017\Deblurring_LowLight_Images_with_Light_Streaks\figure_6.jpg
  Figure 6 caption: Comparisons on synthetic examples of uniform blur. The images
    on the second and fourth rows are zoom-in views of that in the first and third
    rows.
  Figure 7 Link: articels_figures_by_rev_year\2017\Deblurring_LowLight_Images_with_Light_Streaks\figure_7.jpg
  Figure 7 caption: Comparisons with the state-of-the-art methods on a real example.
    The images in the second and fourth rows are zoom-in views of that in the first
    and third rows.
  Figure 8 Link: articels_figures_by_rev_year\2017\Deblurring_LowLight_Images_with_Light_Streaks\figure_8.jpg
  Figure 8 caption: Comparisons with the state-of-the-art methods on a real example.
    The images on the second and fourth rows are zoom-in views of that in the first
    and third rows.
  Figure 9 Link: articels_figures_by_rev_year\2017\Deblurring_LowLight_Images_with_Light_Streaks\figure_9.jpg
  Figure 9 caption: Success rate of reconstruction error ratio [5] on the synthetic
    dataset of uniform blur. There are 154 blurry images consisting of 11 low-light
    images and 14 blur kernels.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Zhe Hu
  Name of the last author: Ming-Hsuan Yang
  Number of Figures: 13
  Number of Tables: 1
  Number of authors: 4
  Paper title: Deblurring Low-Light Images with Light Streaks
  Publication Date: 2017-11-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Comparisons Using Kernel Similarity (KS)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2768365
- Affiliation of the first author: department of computing, imperial college london,
    london, united kingdom
  Affiliation of the last author: department of computing, imperial college london,
    london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2017\PDT_PersonSpecific_Detection_Deformable_Tracking\figure_1.jpg
  Figure 1 caption: Visual overview of PD2T. PD2T accepts a sequence of frames and
    a (not necessarily dense temporally) sequence of landmarks as input; it learns
    person-specific representations for the object of interest and it outputs an improved
    sequence of landmarks. There are three main steps in PD2T followed by an optional
    iterative procedure.
  Figure 10 Link: articels_figures_by_rev_year\2017\PDT_PersonSpecific_Detection_Deformable_Tracking\figure_10.jpg
  Figure 10 caption: (Preferably viewed in colour) Indicative tracked frames from
    the experiment with the animal tracking ( Section 3.4).
  Figure 2 Link: articels_figures_by_rev_year\2017\PDT_PersonSpecific_Detection_Deformable_Tracking\figure_2.jpg
  Figure 2 caption: (Preferably viewed in colour) Pictorial representation of the
    person-specific detection learning. From top to bottom, the steps of a) extracting
    the patches from the current landmarks' estimates, b) rejecting the erroneous
    fittings with the classifier, c) learning the appearance (unary) and spatial (pairwise)
    subspaces.
  Figure 3 Link: articels_figures_by_rev_year\2017\PDT_PersonSpecific_Detection_Deformable_Tracking\figure_3.jpg
  Figure 3 caption: Indicative shapes for each cluster. Even with the seven clusters
    visualised in the figure significant shape variance is included. This ensures
    that the shape model (as detailed in Section 2.4 ) contains considerable variance.
  Figure 4 Link: articels_figures_by_rev_year\2017\PDT_PersonSpecific_Detection_Deformable_Tracking\figure_4.jpg
  Figure 4 caption: (Preferably viewed in colour) CED plots for validation experiments
    (Section 3.2). In all the plots, the blue line indicates the baseline (generic
    fitting) as a measure for comparison. (a) The comparison of the two versions of
    the pipeline, i.e., the online (incremental denoted as P D 2 T INCREM ) and the
    offline, for two different generic fitting methods. (b) Contribution of the different
    pipeline steps (two different initialisation methods, i.e., a stronger one (SRDCF
    + CFSS) and a weaker one (MIL + CFSS)). The legend with P-S is an abbreviation
    of person-specific.
  Figure 5 Link: articels_figures_by_rev_year\2017\PDT_PersonSpecific_Detection_Deformable_Tracking\figure_5.jpg
  Figure 5 caption: '(Preferably viewed in colour) CED plots for Section 3.3.1 (deformable
    facial tracking). Depicted in (a) are the CED for Category 1, in (b) Category
    2, in (c) Category 3. The common labelling method for all is that the green line
    denoted the initial result, while the red one the outcome of PD2 T. Two state-of-the
    art methods are depicted on the left, while two methods that benefit substantially
    from PD 2T are on the right two columns. Namely, from left to right the plots
    correspond to the methods: LRST + CFSS, MTCNN + CFSS, PREV + CFSS, SPT + CFSS.
    The complete set of plots is deferred to supplementary material, available online,
    due to limited space.'
  Figure 6 Link: articels_figures_by_rev_year\2017\PDT_PersonSpecific_Detection_Deformable_Tracking\figure_6.jpg
  Figure 6 caption: (Preferably viewed in colour) CED plot for the comparison of iCCR
    against the best reported curves for PD2 T. In the left plot the generic methods
    are visualised against iCCR, while on the right the output of PD2T with each respective
    initialisation. Apparently, iCCR is on-par with the generic MTCNN + CFSS; the
    two others are weaker, while on the right the PD2T results in all three adaptive
    results surpassing iCCR.
  Figure 7 Link: articels_figures_by_rev_year\2017\PDT_PersonSpecific_Detection_Deformable_Tracking\figure_7.jpg
  Figure 7 caption: (Preferably viewed in colour) CED plot for the newly introduced
    FTOVM in Section 3.3.2.
  Figure 8 Link: articels_figures_by_rev_year\2017\PDT_PersonSpecific_Detection_Deformable_Tracking\figure_8.jpg
  Figure 8 caption: (Preferably viewed in colour) CED plots for the animal tracking
    experiment ( Section 3.4).
  Figure 9 Link: articels_figures_by_rev_year\2017\PDT_PersonSpecific_Detection_Deformable_Tracking\figure_9.jpg
  Figure 9 caption: '(Preferably viewed in colour) Indicative tracked frames from
    the experiment with the deformable facial tracking ( Section 3.3.1). The first
    row represents the generic fitting outcome (used as initialisation for PD2T),
    the second row the outcome of PD2T, and the third the ground-truth annotation.
    From left to right the initialisation methods were: IVT + CFSS, KCF + SDM, SIAM-OXF
    + SDM, SRDCF + CFSS, DLSSVM + CFSS, LRST + CFSS. Evidently, PD2T results in a
    substantial improvement in the final localisation.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Grigorios G. Chrysos
  Name of the last author: Stefanos Zafeiriou
  Number of Figures: 11
  Number of Tables: 6
  Number of authors: 2
  Paper title: 'PD2T: Person-Specific Detection, Deformable Tracking'
  Publication Date: 2017-11-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of Primary Symbols
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 (Preferably Viewed in Colour) Exemplar Results with Indicative
      Fitting Quality per Experiment
  Table 3 caption:
    table_text: TABLE 3 The Hyperparameters Used in the Pipeline Steps as Utilised
      in the Experiments, i.e., the Facial Landmark Tracking, Animal Deformable Tracking
  Table 4 caption:
    table_text: TABLE 4 Computational Cost of the Top Performing (Generic and Adaptive)
      Methods
  Table 5 caption:
    table_text: TABLE 5 Results for Experiment of Section 3.3.1
  Table 6 caption:
    table_text: TABLE 6 Results for Experiment with Facial Landmark Tracking (Section
      3.3.1 )
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2769654
- Affiliation of the first author: "department of informatics, eth z\xFCrich, z\xFC\
    rich, switzerland"
  Affiliation of the last author: "department of informatics, eth z\xFCrich, z\xFC\
    rich, switzerland"
  Figure 1 Link: articels_figures_by_rev_year\2017\EventBased_DOF_Camera_Tracking_from_Photometric_Depth_Maps\figure_1.jpg
  Figure 1 caption: 'Sample application: 6-DOF tracking in ARVR (Augmented or Virtual
    Reality) scenarios. The pose of the event camera (rigidly attached to a hand or
    head tracker) is tracked from a previously built photometric depth map (RGB-D)
    of the scene. Positive and negative events are represented in blue and red, respectively,
    on the image plane of the event camera.'
  Figure 10 Link: articels_figures_by_rev_year\2017\EventBased_DOF_Camera_Tracking_from_Photometric_Depth_Maps\figure_10.jpg
  Figure 10 caption: 'Indoor experiment with 6-DOF motion. Left: Image of the standard
    camera overlaid with events (during mild motion). Events are displayed in red
    and green, according to polarity. Estimated position (center) and orientation
    (right) from our event-based algorithm (solid line), a frame-based method (dash-dot
    line) and ground truth (black line) from a motion capture system.'
  Figure 2 Link: articels_figures_by_rev_year\2017\EventBased_DOF_Camera_Tracking_from_Photometric_Depth_Maps\figure_2.jpg
  Figure 2 caption: 'High-speed motion sequence. Top left: Image from a standard camera,
    suffering from blur due to high-speed motion. Top right: Set of asynchronous events
    from an event camera in an interval of 3 milliseconds, colored according to polarity.
    Bottom: estimated poses using our event-based (EB) approach, which provides low
    latency and high temporal resolution updates. Ground truth (GT) poses are also
    displayed.'
  Figure 3 Link: articels_figures_by_rev_year\2017\EventBased_DOF_Camera_Tracking_from_Photometric_Depth_Maps\figure_3.jpg
  Figure 3 caption: An event camera and its output.
  Figure 4 Link: articels_figures_by_rev_year\2017\EventBased_DOF_Camera_Tracking_from_Photometric_Depth_Maps\figure_4.jpg
  Figure 4 caption: "Computation of the contrast (measurement function) by transferring\
    \ events from the event camera to a reference image. For each event, the predicted\
    \ contrast (13), \u0394lnI , used in the measurement function (7) is computed\
    \ as the log-intensity difference (as in (1)) at two points on the reference image\
    \ I r : the points (12) corresponding to the same pixel u on the event camera,\
    \ at times of the event ( t k and t k \u2212\u0394t )."
  Figure 5 Link: articels_figures_by_rev_year\2017\EventBased_DOF_Camera_Tracking_from_Photometric_Depth_Maps\figure_5.jpg
  Figure 5 caption: Error plots in position (relative to a mean scene depth of 60
    cm) and in orientation (in degrees) for one of the test sequences with ground
    truth provided by a motion capture system with sub-millimeter accuracy.
  Figure 6 Link: articels_figures_by_rev_year\2017\EventBased_DOF_Camera_Tracking_from_Photometric_Depth_Maps\figure_6.jpg
  Figure 6 caption: Error in position (relative to a mean scene depth of 60 cm) and
    orientation (in degrees) of the trajectories recovered by our method for all rocks
    sequences (ground truth is given by a motion capture system). We provide box plots
    of the root-mean-square (RMS) errors, the mean errors and the standard deviation
    (Std) of the errors.
  Figure 7 Link: articels_figures_by_rev_year\2017\EventBased_DOF_Camera_Tracking_from_Photometric_Depth_Maps\figure_7.jpg
  Figure 7 caption: An event camera (DVS) and a standard camera mounted on a rig.
    The standard camera was only used for comparison.
  Figure 8 Link: articels_figures_by_rev_year\2017\EventBased_DOF_Camera_Tracking_from_Photometric_Depth_Maps\figure_8.jpg
  Figure 8 caption: 'Error plots in position (2nd column, relative to the mean scene
    depth) and in orientation (3rd column, in degrees) for three outdoor test sequences
    (1st column): ivy, graffiti, and building. The mean scene depths are 2.5m, 3m,
    and 30m, respectively.'
  Figure 9 Link: articels_figures_by_rev_year\2017\EventBased_DOF_Camera_Tracking_from_Photometric_Depth_Maps\figure_9.jpg
  Figure 9 caption: The algorithm is able to track the pose of the event camera in
    spite of the considerable amount of events generated by moving objects (e.g.,
    people) in the scene.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Guillermo Gallego
  Name of the last author: Davide Scaramuzza
  Number of Figures: 13
  Number of Tables: 2
  Number of authors: 6
  Paper title: Event-Based, 6-DOF Camera Tracking from Photometric Depth Maps
  Publication Date: 2017-11-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Error Measurements of Three Outdoor Sequences Translation
      Errors Are Relative (i.e., Scaled by the Mean Scene Depth)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Error Measurements of the Sequences in Fig. 12. Translation
      Errors Are Relative (i.e., Scaled by the Mean Scene Depth)
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2769655
- Affiliation of the first author: department of computing, the hong kong polytechnic
    university, hong kong
  Affiliation of the last author: department of computing, the hong kong polytechnic
    university, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2017\Tetrahedron_Based_Fast_D_Fingerprint_Identification_Using_Colored_LEDs_Illuminat\figure_1.jpg
  Figure 1 caption: The block diagram for fast 3D fingerprint identification using
    colored LED illumination.
  Figure 10 Link: articels_figures_by_rev_year\2017\Tetrahedron_Based_Fast_D_Fingerprint_Identification_Using_Colored_LEDs_Illuminat\figure_10.jpg
  Figure 10 caption: (a) CMC curve using 3D features (b) CMC curve using combination
    of 2D and 3D features.
  Figure 2 Link: articels_figures_by_rev_year\2017\Tetrahedron_Based_Fast_D_Fingerprint_Identification_Using_Colored_LEDs_Illuminat\figure_2.jpg
  Figure 2 caption: Positional differences during 3D fingerprint imaging withwithout
    motion. The images in first two rows are sample with motion. The images in last
    two rows are sample without motion.
  Figure 3 Link: articels_figures_by_rev_year\2017\Tetrahedron_Based_Fast_D_Fingerprint_Identification_Using_Colored_LEDs_Illuminat\figure_3.jpg
  Figure 3 caption: Reconstructed 3D fingerprint samples using colored illumination
    (a) from Frankot Chellappa [26], (b) from Poisson solver [27], (c) from Shapelets
    correlated [28], and (d) from Frankot Chellappa [26].
  Figure 4 Link: articels_figures_by_rev_year\2017\Tetrahedron_Based_Fast_D_Fingerprint_Identification_Using_Colored_LEDs_Illuminat\figure_4.jpg
  Figure 4 caption: (a) Synthetic model, (b) 3D reconstruction model using Frankot
    Chellappa method [26], (c) 3D reconstruction model using Poisson solver method
    [27], (d) 3D reconstruction model using Shapelets correlated method [28].
  Figure 5 Link: articels_figures_by_rev_year\2017\Tetrahedron_Based_Fast_D_Fingerprint_Identification_Using_Colored_LEDs_Illuminat\figure_5.jpg
  Figure 5 caption: "(a) 2D minutiae Delaunay triangulation and features. m i represents\
    \ 2D minutia. l i presents the side of minutiae triangle, \u03C6 presents the\
    \ largest angle of minutiae triangle. (b) 3D minutiae tetrahedron. (c) Tetrahedron\
    \ sample and features. m i represents 3D minutia, l max presents the largest side\
    \ in this tetrahedron, \u03C6 presents the largest angle of the tetrahedron's\
    \ face. (d) Minutiae sample of tetrahedron and its minutiae direction \u03B8 and\
    \ orientation \u03D5 (blue line). Red line illustrates its projection on x\u2212\
    y plane."
  Figure 6 Link: articels_figures_by_rev_year\2017\Tetrahedron_Based_Fast_D_Fingerprint_Identification_Using_Colored_LEDs_Illuminat\figure_6.jpg
  Figure 6 caption: (a) Tetrahedron with minutiae quality > 0.7. (b) Tetrahedron with
    minutiae quality > 0.5. (c) Tetrahedron with all minutiae.
  Figure 7 Link: articels_figures_by_rev_year\2017\Tetrahedron_Based_Fast_D_Fingerprint_Identification_Using_Colored_LEDs_Illuminat\figure_7.jpg
  Figure 7 caption: (a) ROC curve using 3D minutiae feature. (b) ROC curve using different
    3D features. (c) ROC curve using 3D2D features. (d) ROC using combination of 2D
    fingerprint and 3D fingerprint features.
  Figure 8 Link: articels_figures_by_rev_year\2017\Tetrahedron_Based_Fast_D_Fingerprint_Identification_Using_Colored_LEDs_Illuminat\figure_8.jpg
  Figure 8 caption: (a) CMC curve using 3D features. (b) CMC curve using combination
    of 2D and 3D features.
  Figure 9 Link: articels_figures_by_rev_year\2017\Tetrahedron_Based_Fast_D_Fingerprint_Identification_Using_Colored_LEDs_Illuminat\figure_9.jpg
  Figure 9 caption: (a) ROC curve using 3D minutiae feature. (b) ROC curve using different
    3D features. (c) ROC curve using 3D2D features. (d) ROC using combination of 2D
    fingerprint and 3D fingerprint features.
  First author gender probability: 0.64
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Chenhao Lin
  Name of the last author: Ajay Kumar
  Number of Figures: 10
  Number of Tables: 4
  Number of authors: 2
  Paper title: Tetrahedron Based Fast 3D Fingerprint Identification Using Colored
    LEDs Illumination
  Publication Date: 2017-11-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparative Summary of 3D Fingerprint Identification System
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparative MSE and Computational Complexity
  Table 3 caption:
    table_text: TABLE 3 Matching Performance Using Protocol A
  Table 4 caption:
    table_text: TABLE 4 Matching Performance Using Protocol B
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2771292
- Affiliation of the first author: school of electrical and electronic engineering,
    nanyang technological university, singapore
  Affiliation of the last author: alibaba group, hangzhou, china
  Figure 1 Link: articels_figures_by_rev_year\2017\SkeletonBased_Action_Recognition_Using_SpatioTemporal_LSTM_Network_with_Trust_Ga\figure_1.jpg
  Figure 1 caption: Illustration of the spatio-temporal LSTM network. In temporal
    dimension, the corresponding body joints are fed over the frames. In spatial dimension,
    the skeletal joints in each frame are fed as a sequence. Each unit receives the
    hidden representation of the previous joints and the same joint from previous
    frames.
  Figure 10 Link: articels_figures_by_rev_year\2017\SkeletonBased_Action_Recognition_Using_SpatioTemporal_LSTM_Network_with_Trust_Ga\figure_10.jpg
  Figure 10 caption: Experimental results of our method by early stopping the network
    evolution at different time steps.
  Figure 2 Link: articels_figures_by_rev_year\2017\SkeletonBased_Action_Recognition_Using_SpatioTemporal_LSTM_Network_with_Trust_Ga\figure_2.jpg
  Figure 2 caption: Illustration of the proposed ST-LSTM with one unit.
  Figure 3 Link: articels_figures_by_rev_year\2017\SkeletonBased_Action_Recognition_Using_SpatioTemporal_LSTM_Network_with_Trust_Ga\figure_3.jpg
  Figure 3 caption: (a) The skeleton of the human body. In the simple joint chain
    model, the joint visiting order is 1-2-3-...-16. (b) The skeleton is transformed
    to a tree structure. (c) The tree traversal scheme. The tree structure can be
    unfolded to a chain with the traversal scheme, and the joint visiting order is
    1-2-3-2-4-5-6-5-4-2-7-8-9-8-7-2-1-10-11-12-13-12-11-10-14-15-16-15-14-10-1.
  Figure 4 Link: articels_figures_by_rev_year\2017\SkeletonBased_Action_Recognition_Using_SpatioTemporal_LSTM_Network_with_Trust_Ga\figure_4.jpg
  Figure 4 caption: Illustration of the deep tree-structured ST-LSTM network. For
    clarity, some arrows are omitted in this figure. The hidden representation of
    the first ST-LSTM layer is fed to the second ST-LSTM layer as its input. The second
    ST-LSTM layer's hidden representation is fed to the softmax layer for classification.
  Figure 5 Link: articels_figures_by_rev_year\2017\SkeletonBased_Action_Recognition_Using_SpatioTemporal_LSTM_Network_with_Trust_Ga\figure_5.jpg
  Figure 5 caption: Illustration of the proposed ST-LSTM with trust gate.
  Figure 6 Link: articels_figures_by_rev_year\2017\SkeletonBased_Action_Recognition_Using_SpatioTemporal_LSTM_Network_with_Trust_Ga\figure_6.jpg
  Figure 6 caption: Illustration of the proposed structure for feature fusion inside
    the ST-LSTM unit.
  Figure 7 Link: articels_figures_by_rev_year\2017\SkeletonBased_Action_Recognition_Using_SpatioTemporal_LSTM_Network_with_Trust_Ga\figure_7.jpg
  Figure 7 caption: Recognition accuracy per class on the NTU RGB+D dataset.
  Figure 8 Link: articels_figures_by_rev_year\2017\SkeletonBased_Action_Recognition_Using_SpatioTemporal_LSTM_Network_with_Trust_Ga\figure_8.jpg
  Figure 8 caption: Examples of the noisy skeletons from the NTU RGB+D dataset.
  Figure 9 Link: articels_figures_by_rev_year\2017\SkeletonBased_Action_Recognition_Using_SpatioTemporal_LSTM_Network_with_Trust_Ga\figure_9.jpg
  Figure 9 caption: "(a) Performance comparison of our approach using different values\
    \ of neuron size ( d ) on the NTU RGB+D dataset (X-subject). (b) Performance comparison\
    \ of our method using different \u03BB values on the NTU RGB+D dataset (X-subject).\
    \ The blue line represents our results when different \u03BB values are used for\
    \ trust gate, while the red dashed line indicates the performance of our method\
    \ when trust gate is not added."
  First author gender probability: 0.9
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Jun Liu
  Name of the last author: Gang Wang
  Number of Figures: 11
  Number of Tables: 14
  Number of authors: 5
  Paper title: Skeleton-Based Action Recognition Using Spatio-Temporal LSTM Network
    with Trust Gates
  Publication Date: 2017-11-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Experimental Results on the NTU RGB+D Dataset
  Table 10 caption:
    table_text: TABLE 10 Experimental Results on the Berkeley MHAD Dataset
  Table 2 caption:
    table_text: TABLE 2 Evaluation of Different Feature Fusion Strategies on the NTU
      RGB+D Dataset
  Table 3 caption:
    table_text: TABLE 3 Experimental Results on the UT-Kinect Dataset (LOOCV Protocol
      [7])
  Table 4 caption:
    table_text: TABLE 4 Results on the UT-Kinect Dataset (Half-versus-Half Protocol
      [79])
  Table 5 caption:
    table_text: TABLE 5 Evaluation of Our Approach for Feature Fusion on the UT-Kinect
      Dataset (LOOCV Protocol [7])
  Table 6 caption:
    table_text: TABLE 6 Experimental Results on the SBU Interaction Dataset
  Table 7 caption:
    table_text: TABLE 7 Experimental Results on the SYSU-3D Dataset
  Table 8 caption:
    table_text: TABLE 8 Experimental Results on the ChaLearn Gesture Dataset
  Table 9 caption:
    table_text: TABLE 9 Experimental Results on the MSR Action3D Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2771306
- Affiliation of the first author: institute of industrial science, university of
    tokyo, tokyo, japan
  Affiliation of the last author: institute of industrial science, university of tokyo,
    tokyo, japan
  Figure 1 Link: articels_figures_by_rev_year\2017\EgoSurfing_Person_Localization_in_FirstPerson_Videos_Using_EgoMotion_Signatures\figure_1.jpg
  Figure 1 caption: Self-search results. Target instances (yellow arrows) are detected
    (unmasked regions ) despite motion blurred, non-frontal faces (second column)
    or extreme body poses ( third column) where face detection (green rectangles)
    and recognition (red rectangles ) fail.
  Figure 10 Link: articels_figures_by_rev_year\2017\EgoSurfing_Person_Localization_in_FirstPerson_Videos_Using_EgoMotion_Signatures\figure_10.jpg
  Figure 10 caption: Affinity matrices between first-person videos on our dataset
    and the CMU dataset.
  Figure 2 Link: articels_figures_by_rev_year\2017\EgoSurfing_Person_Localization_in_FirstPerson_Videos_Using_EgoMotion_Signatures\figure_2.jpg
  Figure 2 caption: Global motion of the target (A)'s points-of-view video and local
    motions in the observer (B)'s video. The local motion in a target region (red
    line plot) has higher correlation with the global motion ( black line plot) compared
    to that in non-target regions (yellow and blue plots).
  Figure 3 Link: articels_figures_by_rev_year\2017\EgoSurfing_Person_Localization_in_FirstPerson_Videos_Using_EgoMotion_Signatures\figure_3.jpg
  Figure 3 caption: Overview of the proposed target search. Given target candidates
    extracted from observer videos, we evaluate their (a) correlation-based targetness
    (Section 2.2) and (b) data-driven generic targetness (Section 2.4) and combine
    them to construct (c) a pixel-level targetness map (Section 2.5). Correlation-based
    targetness is evaluated efficiently with the two-step scheme presented in Section
    2.3 and also used to measure (d) the affinity between target and observer videos
    as shown in Section 2.6.
  Figure 4 Link: articels_figures_by_rev_year\2017\EgoSurfing_Person_Localization_in_FirstPerson_Videos_Using_EgoMotion_Signatures\figure_4.jpg
  Figure 4 caption: Representative frames of interaction scenes in our dataset and
    CMU dataset.
  Figure 5 Link: articels_figures_by_rev_year\2017\EgoSurfing_Person_Localization_in_FirstPerson_Videos_Using_EgoMotion_Signatures\figure_5.jpg
  Figure 5 caption: Target localization results on our dataset. The target candidate
    with the highest targetness score is specified by the green circle. Results for
    VJ, LBPH, FisherFace were respectively shown in green, red, and yellow rectangles
    in the second column.
  Figure 6 Link: articels_figures_by_rev_year\2017\EgoSurfing_Person_Localization_in_FirstPerson_Videos_Using_EgoMotion_Signatures\figure_6.jpg
  Figure 6 caption: Target localization results on the CMU dataset. The target candidate
    with the highest targetness score is specified by the green circle. Results for
    VJ, LBPH, FisherFace were respectively shown in green, red, and yellow rectangles
    in the second column.
  Figure 7 Link: articels_figures_by_rev_year\2017\EgoSurfing_Person_Localization_in_FirstPerson_Videos_Using_EgoMotion_Signatures\figure_7.jpg
  Figure 7 caption: 'Comparison of AUC and AP scores on different combinations of
    P : the percentages of candidates to evaluate actual correlation-based targetness
    and K : the numbers of pieces for the piecewise-constant approximation.'
  Figure 8 Link: articels_figures_by_rev_year\2017\EgoSurfing_Person_Localization_in_FirstPerson_Videos_Using_EgoMotion_Signatures\figure_8.jpg
  Figure 8 caption: 'Comparison of computation times on different percentages of candidates
    to evaluate actual correlation-based targetness. Left: computation times to evaluate
    per-candidate targetness. Right: computation times to evaluate per-pixel targetness
    maps.'
  Figure 9 Link: articels_figures_by_rev_year\2017\EgoSurfing_Person_Localization_in_FirstPerson_Videos_Using_EgoMotion_Signatures\figure_9.jpg
  Figure 9 caption: Target segmentation with per-pixel targetness maps to preserve
    the privacy of people who accidentally came into the view of a camera.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ryo Yonetani
  Name of the last author: Yoichi Sato
  Number of Figures: 10
  Number of Tables: 4
  Number of authors: 3
  Paper title: 'Ego-Surfing: Person Localization in First-Person Videos Using Ego-Motion
    Signatures'
  Publication Date: 2017-11-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 AUC and AP Scores Averaged Over Each Scene in Our Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 AUC and AP Scores Averaged Over Scenes in the CMU Dataset
  Table 3 caption:
    table_text: TABLE 3 Comparison of the Mean R-Precision on Target Video Retrieval
  Table 4 caption:
    table_text: TABLE 4 Performance Comparison on Social Group Clustering
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2771767
- Affiliation of the first author: imaging biomarkers and computer-aided diagnosis
    laboratory, national institutes of health (nih) clinical center, bethesda, md
  Affiliation of the last author: "liris cnrs umr 5205, \xE9cole centrale de lyon,\
    \ \xE9cully, france"
  Figure 1 Link: articels_figures_by_rev_year\2017\Visual_and_Semantic_Knowledge_Transfer_for_Large_Scale_SemiSupervised_Object_Det\figure_1.jpg
  Figure 1 caption: In this work, we consider a dataset containing image-level labels
    for all the categories, while object-level bounding box annotations are only available
    for some of the categories (i.e., weakly labeled categories). How can we transform
    a CNN classification network into a detection network to detect the weakly labeled
    categories (e.g., the cat class)?
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Visual_and_Semantic_Knowledge_Transfer_for_Large_Scale_SemiSupervised_Object_Det\figure_2.jpg
  Figure 2 caption: "An illustration of our similarity-based knowledge transfer model.\
    \ The question we investigate is whether knowledge about object similarities \u2013\
    \ visual and semantic \u2013 can be exploited to improve detectors trained in\
    \ a semi-supervised manner. More specifically, to adapt the image-level classifier\
    \ (up-left) of a \u201Cweakly labeled\u201D category (no bounding boxes) into\
    \ a detector (up-right), we transfer information about the classifier and detector\
    \ differences of \u201Cstrong\u201D categories (with image-level and bounding\
    \ box annotations, bottom of the figure) by favoring categories that are more\
    \ similar to the target category (e.g, transfer information from dog and tiger\
    \ rather than basketball or bookshelf to produce a cat detector)."
  Figure 3 Link: articels_figures_by_rev_year\2017\Visual_and_Semantic_Knowledge_Transfer_for_Large_Scale_SemiSupervised_Object_Det\figure_3.jpg
  Figure 3 caption: "Some example visualizations of (a) visual similarity (first row\
    \ in the figure), (b) semantic similarity (middle row) and (c) mixture similarity\
    \ (last row) between a target \u201Cweakly labeled\u201D category and its source\
    \ categories from which knowledge is to be transferred. For each target category,\
    \ the top-10 weighted nearest neighbor categories are shown. The magnitude of\
    \ each column bar shows the relative weight (degree of similarity s v , s s ,\
    \ s in Eq. (6), where \u03B1 is set to 0.6)."
  Figure 4 Link: articels_figures_by_rev_year\2017\Visual_and_Semantic_Knowledge_Transfer_for_Large_Scale_SemiSupervised_Object_Det\figure_4.jpg
  Figure 4 caption: "Examples of correct detections (true positives) of our mixture\
    \ knowledge transfer model on ILSVRC2013 images. For each image, only detections\
    \ for the \u201Cweakly labeled\u201D target category (text below image) are listed."
  Figure 5 Link: articels_figures_by_rev_year\2017\Visual_and_Semantic_Knowledge_Transfer_for_Large_Scale_SemiSupervised_Object_Det\figure_5.jpg
  Figure 5 caption: Examples of incorrect detections (confusion with other objects)
    of our mixture knowledge transfer model on ILSVRC2013 images. The detected object
    label is shown in the top-left of its bounding box.
  Figure 6 Link: articels_figures_by_rev_year\2017\Visual_and_Semantic_Knowledge_Transfer_for_Large_Scale_SemiSupervised_Object_Det\figure_6.jpg
  Figure 6 caption: "Sensitivity of parameter alpha versus mAP for detection of \u201C\
    weakly labeled\u201D categories on the validation (val1) dataset. alpha in [0,1]\
    \ is a parameter used to control the relative influence of the two similarity\
    \ measurements. alpha is set to 1 when only considering visual similarity transfer,\
    \ and 0 for semantic similarity transfer."
  Figure 7 Link: articels_figures_by_rev_year\2017\Visual_and_Semantic_Knowledge_Transfer_for_Large_Scale_SemiSupervised_Object_Det\figure_7.jpg
  Figure 7 caption: "Some example detections before and after bounding box regression\
    \ on the \u201Cweakly labeled\u201D categories. Boxes before (resp. after) bounding-box\
    \ regression are shown in dashed blue (resp. green)."
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.93
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.71
  Name of the first author: Yuxing Tang
  Name of the last author: Liming Chen
  Number of Figures: 7
  Number of Tables: 4
  Number of authors: 7
  Paper title: Visual and Semantic Knowledge Transfer for Large Scale Semi-Supervised
    Object Detection
  Publication Date: 2017-11-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Detection Mean Average Precision (mAP) on ILSVRC2013 val2
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Mean Average Precision (mAP) for Semantic Similarity
      MeasuresRepresentations, Using Weighted - 100
  Table 3 caption:
    table_text: "TABLE 3 Comparisons of Detection Mean Average Precision (mAP) on\
      \ the \u201CWeakly Labeled\u201D Categories of ILSVRC2013 val2, using \u201C\
      VGG-Nets\u201D, \u201CGoogLeNet \u201D and \u201CResNets\u201D"
  Table 4 caption:
    table_text: "TABLE 4 Detection Performance Using Fast R-CNN on the \u201CWeakly\
      \ Labeled\u201D Categories of ILSVRC2013 val2"
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2771779
- Affiliation of the first author: department of electrical and computer engineering,
    northeastern university, boston, ma
  Affiliation of the last author: department of electrical and computer engineering,
    northeastern university, boston, ma
  Figure 1 Link: articels_figures_by_rev_year\2017\Learning_Consensus_Representation_for_Weak_Style_Classification\figure_1.jpg
  Figure 1 caption: Illustration of weak style phenomenon in fashion and manga style
    images in (a) and (b) respectively. Images in red frames on the boundary are weak
    style images from two different classes, but they seem visually similar. We denote
    l 1 to l 3 as different style levels.
  Figure 10 Link: articels_figures_by_rev_year\2017\Learning_Consensus_Representation_for_Weak_Style_Classification\figure_10.jpg
  Figure 10 caption: 'Examples for 10 categories in the Architecture Style dataset:
    (a) American craftsman, (b) Baroque architecture, (c) Chicago school architecture,
    (d) Colonial architecture, (e) Georgian architecture, (f) Gothic architecture,
    (g) Greek Revival architecture, (h) Queen Anne architecture, (i) Romanesque architecture,
    (j) Russian Revival architecture.'
  Figure 2 Link: articels_figures_by_rev_year\2017\Learning_Consensus_Representation_for_Weak_Style_Classification\figure_2.jpg
  Figure 2 caption: "Data visualization of the \u201Cspread out\u201D phenomenon in\
    \ \u201Cshoji\u201D and \u201Cshonen\u201D classes of manga style. PCA is conducted\
    \ to reduce the dimension of feature descriptors of \u201Cline strength\u201D\
    \ and \u201Cincluded an angle between lines\u201D into 2D for visualization in\
    \ (a) and (b) respectively. In both (a) and (b), five colors, blue, green, red,\
    \ cyan, and magenta, are used to present the data points in different style levels\
    \ from the strongest to the weakest. We could see that strong style data points\
    \ are dense and weak style data points are diffuse."
  Figure 3 Link: articels_figures_by_rev_year\2017\Learning_Consensus_Representation_for_Weak_Style_Classification\figure_3.jpg
  Figure 3 caption: Illustration of the Stacked Style Centralizing Auto-Encoder (SSCAE).
    Example images of each level l are presented with colored frames. In the step
    k , samples in l k are replaced by l k+1 samples' (red) nearest neighbors found
    in l k . Samples in the higher level than l k are not changed (blue).
  Figure 4 Link: articels_figures_by_rev_year\2017\Learning_Consensus_Representation_for_Weak_Style_Classification\figure_4.jpg
  Figure 4 caption: "Manifold learning perspective of SCAE with mangas in \u201Cshonen\u201D\
    \ style."
  Figure 5 Link: articels_figures_by_rev_year\2017\Learning_Consensus_Representation_for_Weak_Style_Classification\figure_5.jpg
  Figure 5 caption: Illustration of non-linear consensus style centralizing auto-encoder
    (NCSCAE).
  Figure 6 Link: articels_figures_by_rev_year\2017\Learning_Consensus_Representation_for_Weak_Style_Classification\figure_6.jpg
  Figure 6 caption: Illustration of the Linear Consensus Style Centralizing Auto-Encoder
    (LCSCAE). The illustration of the low-rank group sparsity structure of low-level
    features is shown in (a). The illustration of the matrix factorization in the
    solution of the model is shown in (b).
  Figure 7 Link: articels_figures_by_rev_year\2017\Learning_Consensus_Representation_for_Weak_Style_Classification\figure_7.jpg
  Figure 7 caption: 'Examples for 5 categories in the Hipster Wars dataset: (a) bohemian,
    (b) hipster, (c) goth (d) pinup, (e) preppy.'
  Figure 8 Link: articels_figures_by_rev_year\2017\Learning_Consensus_Representation_for_Weak_Style_Classification\figure_8.jpg
  Figure 8 caption: 'Examples for 12 categories in the Online Shopping dataset: (a)
    avant-garde, (b) elegant, (c) folk, (d) leisurely, (e) modern, (f) neutral, (g)
    renascent, (h) romantic, (i) sexy, (j) splendid, (k) technology, (l) young.'
  Figure 9 Link: articels_figures_by_rev_year\2017\Learning_Consensus_Representation_for_Weak_Style_Classification\figure_9.jpg
  Figure 9 caption: Examples for shojo style and shonen style in the Manga dataset.
    The first row is shoji style and the second row is shonen style.
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Shuhui Jiang
  Name of the last author: Yun Fu
  Number of Figures: 13
  Number of Tables: 8
  Number of authors: 4
  Paper title: Learning Consensus Representation for Weak Style Classification
  Publication Date: 2017-11-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performances (%) of Fashion Style Classification on Hipster
      Wars Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance of Fashion Style Classification on Online Shopping
      dataset
  Table 3 caption:
    table_text: TABLE 3 Performance (%) of Manga Style Classification
  Table 4 caption:
    table_text: TABLE 4 Performance (%) of Architecture Style Classification
  Table 5 caption:
    table_text: TABLE 5 p Values of t-Test of on Four Weak Style Datasets (with the
      Same Setting as the Rightmost Column in Table 1 -4)
  Table 6 caption:
    table_text: TABLE 6 Performance (%) of OSCAE and SSCAE on Hipster Wars Dataset
  Table 7 caption:
    table_text: TABLE 7 Comparison of Training Time of NCSCAE and LCSCAE (Hour)
  Table 8 caption:
    table_text: TABLE 8 Performances (%) of Manga Style Classification of 4 Style
      Level Label Settings of LCSCAE
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2771766
