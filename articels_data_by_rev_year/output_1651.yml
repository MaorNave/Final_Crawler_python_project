- Affiliation of the first author: department of computer science, computer vision
    and robotics lab, universidade federal de minas gerais, belo horizonte, brazil
  Affiliation of the last author: department of computer science, computer vision
    and robotics lab, universidade federal de minas gerais, belo horizonte, brazil
  Figure 1 Link: articels_figures_by_rev_year\2020\A_Sparse_SamplingBased_Framework_for_Semantic_FastForward_of_FirstPerson_Videos\figure_1.jpg
  Figure 1 caption: Graphical illustration of the proposed sparse sampling based framework
    to fast-forward first-person videos.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\A_Sparse_SamplingBased_Framework_for_Semantic_FastForward_of_FirstPerson_Videos\figure_2.jpg
  Figure 2 caption: Main steps of our semantic fast-forward framework. For each segment
    created in the temporal semantic profile segmentation (a), weights based on the
    camera movement are computed (b), and the frames are described (c). Frames are
    sampled by minimizing local-constrained and reconstruction problem (d). The smoothing
    step is applied to tackle the abrupt transitions of the selected frames inside
    segments (e). Fill processing is applied to handle visual gaps between segments
    (f). Frames selected in previous steps are used to composite the final fast-forward
    video (g).
  Figure 3 Link: articels_figures_by_rev_year\2020\A_Sparse_SamplingBased_Framework_for_Semantic_FastForward_of_FirstPerson_Videos\figure_3.jpg
  Figure 3 caption: 'Evaluation of the proposed Sparse Sampling methodology against
    the competitors using the Annotated Semantic Dataset. Dashed and doted lines in
    (b) are related to the mean instability of the input video and the uniform sampling,
    respectively. Better values are: (a) higher, (b) and (c) lower.'
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Michel Silva
  Name of the last author: Erickson R. Nascimento
  Number of Figures: 3
  Number of Tables: 4
  Number of authors: 4
  Paper title: A Sparse Sampling-Based Framework for Semantic Fast-Forward of First-Person
    Videos
  Publication Date: 2020-03-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Details of the Proposed DoMSEV
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results w.r.t. Semantic Retained, Speed-Up, Visual Instability,
      and Processing Time of the Proposed Method Against the State-of-the-Art Methods
      (see the Complete Table in Supplemetary Material)
  Table 3 caption:
    table_text: TABLE 3 Evaluation of the Frame Sampling by Locality-Constrained Linear
      Coding (LLC), Lasso (SC), and Orthogonal Matching Pursuit (OMP)
  Table 4 caption:
    table_text: TABLE 4 Evaluation of the Frame Sampling Describing the Video Frames
      Using the Proposed Handcrafted Features Against Using Deep Features
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2983929
- Affiliation of the first author: department of computing, hong kong polytechnic
    university, hong kong
  Affiliation of the last author: school of eie, university of sydney, maze crescent,
    darlington nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2020\Learning_ContentWeighted_Deep_Image_Compression\figure_1.jpg
  Figure 1 caption: "Decoding images of deep compression approach Ball\xE9 et al.\
    \ [5] and our content-weighted image compression method."
  Figure 10 Link: articels_figures_by_rev_year\2020\Learning_ContentWeighted_Deep_Image_Compression\figure_10.jpg
  Figure 10 caption: 'Visualization of the importance maps at 6 kinds of bit rates.
    Left: ground-truth. Right: importance maps ranging from 0.151 to 0.814 bpp.'
  Figure 2 Link: articels_figures_by_rev_year\2020\Learning_ContentWeighted_Deep_Image_Compression\figure_2.jpg
  Figure 2 caption: Illustration of our content weighted image compression model.
    The whole framework involves an encoder, a learnable channel-wise multi-valued
    quantization, an importance map subnet, and a decoder. The encoder produces 32
    feature maps which are further quantized by the channel-wise multi-valued quantization
    function to generate quantized codes. The importance map subnet estimates the
    informative importance of local image content and generate an importance map with
    only 1 channel. With the quantized importance map, an importance mask is generated
    for guiding spatially varying code pruning. By multiplying quantized codes with
    importance masks in an element-wise manner, the trimmed quantized codes are produced
    as the input of the decoder to generate the decoding image.
  Figure 3 Link: articels_figures_by_rev_year\2020\Learning_ContentWeighted_Deep_Image_Compression\figure_3.jpg
  Figure 3 caption: Illustration of the importance map. The regions with sharp edge
    and rich texture generally have higher values and should be allocated with more
    bits.
  Figure 4 Link: articels_figures_by_rev_year\2020\Learning_ContentWeighted_Deep_Image_Compression\figure_4.jpg
  Figure 4 caption: Scan order, context and masks used in triuMCN for a 2D code map.
  Figure 5 Link: articels_figures_by_rev_year\2020\Learning_ContentWeighted_Deep_Image_Compression\figure_5.jpg
  Figure 5 caption: Rate-distortion curves of different compression algorithms w.r.t.
    (a) PSNR and (b) MS-SSIM on the Kodak PhotoCD image dataset.
  Figure 6 Link: articels_figures_by_rev_year\2020\Learning_ContentWeighted_Deep_Image_Compression\figure_6.jpg
  Figure 6 caption: Rate-distortion curves of different compression algorithms w.r.t.
    (a) PSNR and (b) MS-SSIM on the Tecnick dataset.
  Figure 7 Link: articels_figures_by_rev_year\2020\Learning_ContentWeighted_Deep_Image_Compression\figure_7.jpg
  Figure 7 caption: "Decoding images produced by different compression systems. From\
    \ the left to right: ground-truth, Li et al. [15], Ball\xE9 et al. [5], BPG and\
    \ Ours(MS-SSIM). In general, our model achieves the best visual quality, demonstrating\
    \ the superiority of our model in preserving both sharp edges and detailed textures.\
    \ (Best viewed on screen in color)"
  Figure 8 Link: articels_figures_by_rev_year\2020\Learning_ContentWeighted_Deep_Image_Compression\figure_8.jpg
  Figure 8 caption: Decoding images produced by our models optimized with MSE and
    MS-SSIM, respectively. Ours(MS-SSIM) exhibits better textures at lower bpp but
    may slightly obscure small sharp edges.
  Figure 9 Link: articels_figures_by_rev_year\2020\Learning_ContentWeighted_Deep_Image_Compression\figure_9.jpg
  Figure 9 caption: Rate-distortion curves for ablation studies on Kodak. (a) comparison
    of four quantization variants, i.e., LCMQ, LMQ, FMQ, and BIN. (b) comparison of
    the other variants of our method.
  First author gender probability: 0.67
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Mu Li
  Name of the last author: David Zhang
  Number of Figures: 11
  Number of Tables: 3
  Number of authors: 5
  Paper title: Learning Content-Weighted Deep Image Compression
  Publication Date: 2020-03-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The MOS Test Results on the Kodak and Tecnick Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantization Error of Four Quantization Functions, i.e., LCMQ,
      LMQ, FMQ, and BIN, on 7 Parameter Sets
  Table 3 caption:
    table_text: TABLE 3 Running Time ( s s) of Different Context Models on the Codes
      and Quantized Importance Maps Generated by CWIC Trained on Seven Different Parameter
      Sets
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2983926
- Affiliation of the first author: centre for artificial intelligence, university
    of technology sydney, ultimo, nsw, australia
  Affiliation of the last author: facebook reality labs, pittsburgh, pa, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Supervision_by_Registration_and_Triangulation_for_Landmark_Detection\figure_1.jpg
  Figure 1 caption: Annotations are inconsistent. We show annotations of nine annotators
    on two images of the mouth. Each color indicates a different landmark. Note the
    inconsistencies of annotations exist even on the more discriminative landmarks
    such as the corner of the mouth.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Supervision_by_Registration_and_Triangulation_for_Landmark_Detection\figure_2.jpg
  Figure 2 caption: The Supervision by Registration and Triangulation (SRT) framework
    takes labeled images and unlabeled synchronized and geometrically calibrated multi-view
    video as input to train an image-based landmark detector which is more precise
    on imagesvideo, more stable on video, and also more consistent in multi-view scenarios.
    OF and 3DT stands for Optical Flow and 3D Triangulation respectively.
  Figure 3 Link: articels_figures_by_rev_year\2020\Supervision_by_Registration_and_Triangulation_for_Landmark_Detection\figure_3.jpg
  Figure 3 caption: The training of SRT with three complementary losses. The key idea
    is that the supervision from registration and triangulation can directly back-propagate
    through the optical flow (OF) and 3D triangulation (3DT) modules respectively,
    thus enabling the detector before the OF and 3DT modules to receive gradients
    which encourage temporal and multi-view coherency across frames.
  Figure 4 Link: articels_figures_by_rev_year\2020\Supervision_by_Registration_and_Triangulation_for_Landmark_Detection\figure_4.jpg
  Figure 4 caption: Forward-backward communication scheme between the detector and
    the OF module during the training procedure. The green and pink lines indicate
    the forward and backward OF tracking routes. The bluegreenpink dots indicate the
    landmark predictions from the detectorforward-OFbackward-OF.
  Figure 5 Link: articels_figures_by_rev_year\2020\Supervision_by_Registration_and_Triangulation_for_Landmark_Detection\figure_5.jpg
  Figure 5 caption: The effect of data size and noise level for the heatmap-based
    model on the test set of Synthetic-Face. We randomly add Gaussian noise with std=0,5,10
    pixels to the ground truth labels of the training set.
  Figure 6 Link: articels_figures_by_rev_year\2020\Supervision_by_Registration_and_Triangulation_for_Landmark_Detection\figure_6.jpg
  Figure 6 caption: Visualization results of the regression-based models on 300-W,
    300-VW, AFLW, and Mugsy-V1, and the heatmap-based models on MPII. Green and blue
    points are prediction results and ground truth labels, respectively.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.53
  Name of the first author: Xuanyi Dong
  Name of the last author: Shoou-I Yu
  Number of Figures: 6
  Number of Tables: 11
  Number of authors: 6
  Paper title: Supervision by Registration and Triangulation for Landmark Detection
  Publication Date: 2020-03-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Explanation of Notations in This Manuscript
  Table 10 caption:
    table_text: TABLE 10 Results on Three 300-VW Test Sets
  Table 2 caption:
    table_text: TABLE 2 The Description of 11 Datasets Used in Our Experiments
  Table 3 caption:
    table_text: TABLE 3 We Show the Effect of Utilizing Various Unlabeled Data Sets
      to Enhance the Regression-Based Detector
  Table 4 caption:
    table_text: TABLE 4 Results of SBR and SBT to Enhance the Regression-Based Detector
      on Mugsy-V1
  Table 5 caption:
    table_text: TABLE 5 We Utilize Various Unlabeled Data Sets to Enhance the Regression-Based
      Detector on Different AFLW Test Sets
  Table 6 caption:
    table_text: TABLE 6 The Effect of Different Loss Functions for Detectors on Mugsy-V1
  Table 7 caption:
    table_text: TABLE 7 The Comparison of the Normalized Mean Error (NME) on AFLW
  Table 8 caption:
    table_text: TABLE 8 PCKh With a Threshold of 0.5 Evaluation Metric on the MPII
      Human Pose Validation Set
  Table 9 caption:
    table_text: TABLE 9 The Comparison of NME w.r.t. the Inter-Ocular Distance on
      300-W
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2983935
- Affiliation of the first author: department of computer science and software engineering,
    the university of western australia, crawley, western australia
  Affiliation of the last author: department of computer science and software engineering,
    the university of western australia, crawley, western australia
  Figure 1 Link: articels_figures_by_rev_year\2020\Spherical_Kernel_for_Efficient_Graph_Convolution_on_D_Point_Clouds\figure_1.jpg
  Figure 1 caption: "The proposed spherical convolutional kernel systematically splits\
    \ the space around a point x i into multiple volumetric bins. For the j th neighboring\
    \ point x j , it determines its relevant bin and uses the weight w \u03BA for\
    \ that bin to compute the activation. The kernel is employed with graph based\
    \ networks that directly process raw point clouds using a pyramid of graph representations.\
    \ This is a simplified U-Net-like [8] architecture for semantic segmentation that\
    \ coarsens the input graph G 0 into G 1 with pooling, and latter uses unpooling\
    \ for expansion. In the network, the location of a point identifies a graph vertex\
    \ and point neighbourhood decides the graph edges. Our network allows convolutional\
    \ blocks with consecutive applications of the proposed kernels for more effective\
    \ representation learning."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Spherical_Kernel_for_Efficient_Graph_Convolution_on_D_Point_Clouds\figure_2.jpg
  Figure 2 caption: "Illustration of the primitive CNN3D and the proposed SPH3D discrete\
    \ kernels: The point x i has seven neighboring points including itself (the self-loop).\
    \ To perform convolution at x i , discrete kernels systematically partition the\
    \ space around it into bins. With x i at the center, CNN3D divides a 3D cubic\
    \ space around the point into uniform voxel bins. Our SPH3D kernel partitions\
    \ a spherical space around x i into non-uniform volumetric bins. For both kernels,\
    \ the bins and their corresponding weights are indexed. The points falling in\
    \ the \u03BA th bin are propagated to x i with the weight w \u03BA . Multiple\
    \ points falling in the same bin, e.g., x s and x t , use the same weight for\
    \ computing the output feature at x i ."
  Figure 3 Link: articels_figures_by_rev_year\2020\Spherical_Kernel_for_Efficient_Graph_Convolution_on_D_Point_Clouds\figure_3.jpg
  Figure 3 caption: Illustration of Encoder-decoder graph neural network for a toy
    example. A graph G l of 12 vertices gets coarsened to G l+1 (8 vertices) and further
    to G l+2 (4 vertices), and expanded back to 12 vertices. The width variation of
    feature maps depicts different number of feature channels, whereas the number
    of cells indicates the total vertices in the corresponding graph. The poolingunpooling
    operations compute features of the coarsenedexpanded graphs. Consecutive convolutions
    are applied to form convolution blocks. The shown architecture for semantic segmentation
    uses skip connections for feature concatenation, similar to U-Net. For classification,
    the decoder and skip connections are removed and a global representation is fed
    to a classifier. We omit self loops in the shown graphs for clarity.
  Figure 4 Link: articels_figures_by_rev_year\2020\Spherical_Kernel_for_Efficient_Graph_Convolution_on_D_Point_Clouds\figure_4.jpg
  Figure 4 caption: 'Representative samples from datasets: ModelNet40 and ShapeNet
    provide point clouds of synthetic models. We also illustrate ground truth segmentation
    for ShapeNet. RueMonge2014 comprises point clouds for outdoor scenes, while ScanNet
    and S3DIS contain indoor scenes.'
  Figure 5 Link: articels_figures_by_rev_year\2020\Spherical_Kernel_for_Efficient_Graph_Convolution_on_D_Point_Clouds\figure_5.jpg
  Figure 5 caption: Prediction visualization for two representative scenes of Area
    5 in S3DIS dataset. We have removed the ceiling so that the details inside the
    offices are clearly visible. Despite the scene complexity, the proposed SPH3D-GCN
    generally segments the points accurately.
  Figure 6 Link: articels_figures_by_rev_year\2020\Spherical_Kernel_for_Efficient_Graph_Convolution_on_D_Point_Clouds\figure_6.jpg
  Figure 6 caption: '(Top) Graph coarsening with FPS: A chair is coarsened from left
    to right into point clouds of smaller resolutions. (Bottom) Kernel visualization:
    Each row shows five spherical kernels learned in an SPH3D layer of the network
    for S3DIS.'
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Huan Lei
  Name of the last author: Ajmal Mian
  Number of Figures: 6
  Number of Tables: 10
  Number of authors: 3
  Paper title: Spherical Kernel for Efficient Graph Convolution on 3D Point Clouds
  Publication Date: 2020-03-31 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Segmentation Results on Area 5 of S3DIS With Different CNN3D
      Kernels, Their Variants With Self-Convolution Weight w 0 w0, and Our SPH3D Kernel
  Table 10 caption:
    table_text: TABLE 10 Computational and Memory Requirements of the Proposed Technique
      and Comparison to PointCNN
  Table 2 caption:
    table_text: "TABLE 2 Network Configuration Details: NN( \u03C1 \u03C1) Denotes\
      \ a Range Search With Radius \u03C1 \u03C1"
  Table 3 caption:
    table_text: 'TABLE 3 ModelNet40 Classification: Average Class and Instance Accuracies
      are Reported Along With the Number of Input Points Per Sample (point), the Number
      of Network Parameters (params), and the TrainTest Time'
  Table 4 caption:
    table_text: TABLE 4 Part Segmentation Results on ShapeNet Dataset
  Table 5 caption:
    table_text: TABLE 5 Semantic Segmentation on RueMonge2014 Dataset
  Table 6 caption:
    table_text: 'TABLE 6 3D Semantic Labelling on Scannet: All the Techniques Use
      3D Coordinates and Color Values as Input Features for Network Training'
  Table 7 caption:
    table_text: 'TABLE 7 Performance on S3DIS Dataset: Area 5 (top), All 6 Folds (Bottom)'
  Table 8 caption:
    table_text: TABLE 8 Runtime and Memory Consumption Comparison Between Regular
      and Separable Kernel
  Table 9 caption:
    table_text: TABLE 9 Ablation Study (Using Area 5 of S3DIS) When Pooling is Changed
      to Average Pooling, Data Augmentation is Excluded and When Uniform or Weighted
      Interpolation is Performed
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2983410
- Affiliation of the first author: microsoft research, beijing, p.r. china
  Affiliation of the last author: microsoft, redmond, wa, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Deep_HighResolution_Representation_Learning_for_Visual_Recognition\figure_1.jpg
  Figure 1 caption: The structure of recovering high resolution from low resolution.
    (a) A low-resolution representation learning subnetwork (such as VGGNet [102]
    and ResNet [40]), which is formed by connecting high-to-low convolutions in series.
    (b) A high-resolution representation recovering subnetwork, which is formed by
    connecting low-to-high convolutions in series. Representative examples include
    SegNet [3], DeconvNet [87], U-Net [97], Hourglass [85], encoder-decoder [92],
    and SimpleBaseline [126].
  Figure 10 Link: articels_figures_by_rev_year\2020\Deep_HighResolution_Representation_Learning_for_Visual_Recognition\figure_10.jpg
  Figure 10 caption: Comparing HRNetV1 and HRNetV2. (a) Segmentation on Cityscapes
    val and PASCAL-Context for comparing HRNetV1, its variant HRNetV1h, and HRNetV2
    (single scale and no flipping). (b) Object detection on COCO val for comparing
    HRNetV1, its variant HRNetV1h, and HRNetV2p (LS = learning schedule). We can see
    that HRNetV2 is superior to HRNetV1 and HRNetV1h for both semantic segmentation
    and object detection.
  Figure 2 Link: articels_figures_by_rev_year\2020\Deep_HighResolution_Representation_Learning_for_Visual_Recognition\figure_2.jpg
  Figure 2 caption: "An example of a high-resolution network. Only the main body is\
    \ illustrated, and the stem (two stride-2 3\xD73 convolutions) is not included.\
    \ There are four stages. The 1st stage consists of high-resolution convolutions.\
    \ The 2nd (3rd, 4th) stage repeats two-resolution (three-resolution, four-resolution)\
    \ blocks. The detail is given in Section 3."
  Figure 3 Link: articels_figures_by_rev_year\2020\Deep_HighResolution_Representation_Learning_for_Visual_Recognition\figure_3.jpg
  Figure 3 caption: "Illustrating how the fusion module aggregates the information\
    \ for high, medium and low resolutions from left to right, respectively. Right\
    \ legend: strided 3\xD73 = stride-2 3\xD73 convolution, up samp. 1\xD71 = bilinear\
    \ upsampling followed by a 1\xD71 convolution."
  Figure 4 Link: articels_figures_by_rev_year\2020\Deep_HighResolution_Representation_Learning_for_Visual_Recognition\figure_4.jpg
  Figure 4 caption: "(a) HRNetV1: only output the representation from the high-resolution\
    \ convolution stream. (b) HRNetV2: concatenate the (upsampled) representations\
    \ that are from all the resolutions (the subsequent 1\xD71 convolution is not\
    \ shown for clarity). (c) HRNetV2p: form a feature pyramid from the representation\
    \ by HRNetV2. The four-resolution representations at the bottom in each sub-figure\
    \ are outputted from the network in Fig. 2, and the gray box indicates how the\
    \ output representation is obtained from the input four-resolution representations."
  Figure 5 Link: articels_figures_by_rev_year\2020\Deep_HighResolution_Representation_Learning_for_Visual_Recognition\figure_5.jpg
  Figure 5 caption: (a) Multi-resolution parallel convolution. (b) Multi-resolution
    fusion. (c) A normal convolution (left) is equivalent to fully-connected multi-branch
    convolutions (right).
  Figure 6 Link: articels_figures_by_rev_year\2020\Deep_HighResolution_Representation_Learning_for_Visual_Recognition\figure_6.jpg
  Figure 6 caption: Qualitative COCO human pose estimation results over representative
    images with various human size, different poses, or clutter background.
  Figure 7 Link: articels_figures_by_rev_year\2020\Deep_HighResolution_Representation_Learning_for_Visual_Recognition\figure_7.jpg
  Figure 7 caption: Qualitative segmentation examples from Cityscapes (left two),
    PASCAL-Context (middle two), and LIP (right two).
  Figure 8 Link: articels_figures_by_rev_year\2020\Deep_HighResolution_Representation_Learning_for_Visual_Recognition\figure_8.jpg
  Figure 8 caption: Qualitative examples for COCO object detection (left three) and
    instance segmentation (right three).
  Figure 9 Link: articels_figures_by_rev_year\2020\Deep_HighResolution_Representation_Learning_for_Visual_Recognition\figure_9.jpg
  Figure 9 caption: Ablation study about the representation resolutions for human
    pose estimation (COCO val). 1times , 2times , 4times correspond to the representations
    of the high, medium, low resolutions, respectively. The results imply that higher
    resolution improves the performance.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Jingdong Wang
  Name of the last author: Bin Xiao
  Number of Figures: 10
  Number of Tables: 13
  Number of authors: 12
  Paper title: Deep High-Resolution Representation Learning for Visual Recognition
  Publication Date: 2020-04-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Human Pose Estimation Results on COCO val
  Table 10 caption:
    table_text: TABLE 10 Object Detection Results on COCO val in the Mask R-CNN and
      its Extended Frameworks
  Table 2 caption:
    table_text: TABLE 2 Human Pose Estimation Results on COCO test-dev
  Table 3 caption:
    table_text: TABLE 3 Semantic Segmentation Results on Cityscapes val (Single Scale
      and no Flipping)
  Table 4 caption:
    table_text: TABLE 4 Semantic Segmentation Results on Cityscapes Test
  Table 5 caption:
    table_text: TABLE 5 Semantic Segmentation Results on PASCAL-Context
  Table 6 caption:
    table_text: TABLE 6 Semantic Segmentation Results on LIP
  Table 7 caption:
    table_text: TABLE 7 GFLOPS and parameters for COCO Object Detection
  Table 8 caption:
    table_text: TABLE 8 Object Detection Results on COCO val in the Faster R-CNN and
      Cascade R-CNN Frameworks
  Table 9 caption:
    table_text: TABLE 9 Object Detection Results on COCO val in the FCOS and CenterNet
      Frameworks
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2983686
- Affiliation of the first author: nanjing university, nanjing, jiangsu, china
  Affiliation of the last author: university of california at riverside, riverside,
    ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\A_Dual_Camera_System_for_High_Spatiotemporal_Resolution_Video_Acquisition\figure_1.jpg
  Figure 1 caption: Snapshots of high spatial resolution-low frame rate (HSR-LFR)
    and low spatial resolution-high frame rate (LSR-HFR) videos and synthesized high
    spatiotemporal resolution (HSTR) video. A woman is throwing a ping-pong ball in
    indoor space. (a) HSR-LFR video 4K30FPS frame with zoomed-in region showing motion
    blur; (b) LSR-HFR video 720p240FPS frame with zoomed-region showing spatial blur;
    (c) HSTR video 4K240FPS frame.
  Figure 10 Link: articels_figures_by_rev_year\2020\A_Dual_Camera_System_for_High_Spatiotemporal_Resolution_Video_Acquisition\figure_10.jpg
  Figure 10 caption: Camera Parallax. Image reconstruction for our dual camera system
    when placing cameras with baseline distance at 5cm and 25cm. Our LSR-HFR camera
    operates at 240FPS. These images are captured using dual Grasshopper3 GS3-U3-51S5C
    cameras. The frames in the first column are the captured HSR-LFR frames. The frames
    in the second column are the captured LSR-HFR frames. Look at the repeated patterns
    on the checkerboard snapshots, there are some ghosts on the results of CrossNet
    because its multi-scale warping in feature domain, but our method does not have
    this problem. And our method has strong robustness when parallax is large. Zoom
    in the pictures, you will see more details in the larger images. More parallax
    settings in supplemental material.
  Figure 2 Link: articels_figures_by_rev_year\2020\A_Dual_Camera_System_for_High_Spatiotemporal_Resolution_Video_Acquisition\figure_2.jpg
  Figure 2 caption: "A Dual Camera System for High Spatiotemporal Acquisition: (a)\
    \ dual camera setup with one LSR-HFR video capture (e.g., x LSR\u2212HFR (t) with\
    \ h\xD7w at mf FPS), the other HSR-LFR video shooting (e.g., X HSR\u2212LFR (t)\
    \ with nh\xD7nw at f FPS) and synthesized HSTR video (e.g., Y HSTR (t) with nh\xD7\
    nw at mf FPS); (b) Recurrent RefSR structure for Y synthesis using I LSR and I\
    \ ref at each time instant; (c) Proposed AWnet for dual camera input with cascaded\
    \ FlowNet and FusionNet to learn adaptive weights for final synthesis; (d) An\
    \ U-net style [39] FusionNet structure for dynamic filter and mask generation.\
    \ \u2A01 and \u2A02 are element-wise addition and multiplication."
  Figure 3 Link: articels_figures_by_rev_year\2020\A_Dual_Camera_System_for_High_Spatiotemporal_Resolution_Video_Acquisition\figure_3.jpg
  Figure 3 caption: Dual Camera Alignment. (a) HSR-LFR frame I ref ; (b) LSR-HFR frame
    I LSR ; (c) HSR-LFR frame I ref frame warped using optical flow only; (d) HSR-LFR
    frame I ref warped using mesh-based homography; (e) HSR-LFR frame warped using
    both mesh-based homography and optical flow. (a) and (b) are captured using dual
    iPhone 7 with different views.
  Figure 4 Link: articels_figures_by_rev_year\2020\A_Dual_Camera_System_for_High_Spatiotemporal_Resolution_Video_Acquisition\figure_4.jpg
  Figure 4 caption: "Synthesized Quality and Weighting Map W . (a) to (d) are exemplified\
    \ for Vimeo90K simulation data: (a) is the up-scaled I LSR\u2191 using bicubic\
    \ method from I LSR by 8\xD7 for both spatial dimensions; (b) is the warped reference\
    \ frame I w ref ; (c) is the output synthesized frame Y ; (d) is the adaptive\
    \ weighting map W on (b); (e) to (h) are the visualization for camera captured\
    \ real data with 3\xD7 resolution scaling from I LSR to I LSR\u2191 : (e) is the\
    \ up-scaled image I LSR\u2191 from the captured LSR-HFR frame; (f) is the warped\
    \ reference frame I w ref ; (g) is the output synthesized frame Y ; (h) is the\
    \ adaptive weighting map W on (f)."
  Figure 5 Link: articels_figures_by_rev_year\2020\A_Dual_Camera_System_for_High_Spatiotemporal_Resolution_Video_Acquisition\figure_5.jpg
  Figure 5 caption: "Noise Regularization. (a)-(c) Reconstructions of synthesized\
    \ Y with noise level \u03C3 2 at 0, 0.001 and 0.01; (d)-(f) Weighting map W with\
    \ noise level \u03C3 2 at 0, 0.001 and 0.01. Video frames are captured using dual\
    \ iPhone 7. Noise regularization shifts more weights to I w ref in general to\
    \ improve the image quality, especially for those background stationary areas.\
    \ But for those regions with occlusions (edge of the athletes), motion blurs (soccer\
    \ ball) and warping artifacts (grassland), reconstruction still prefers pixels\
    \ from I LSR\u2191 to minimize the training loss. Ghosting artifact appear around\
    \ the players head in (b, c), which occur if the AWnet selects blurry regions\
    \ from I LSR\u2191 for the pixels that are not correctly aligned in I w ref ."
  Figure 6 Link: articels_figures_by_rev_year\2020\A_Dual_Camera_System_for_High_Spatiotemporal_Resolution_Video_Acquisition\figure_6.jpg
  Figure 6 caption: "Super-Resolution: Indoor activity with medium light illumination.\
    \ I ref is the captured 4k frame from the HSR-LFR camera, and synchronized I LSR\u2191\
    \ is the up-scaled frame from the captured 720p frame of the LSR-HFR camera. Zoomed-regions\
    \ are visualized for (c) I ref , (d) I LSR\u2191 , and super-resolved reconstructions\
    \ using (e) EDSR, (f) PM, (g) CrossNet, (h) AWnet with \u03C3 2 =0 (no noise regularization),\
    \ and (i) AWnet with \u03C3 2 =0.01 ."
  Figure 7 Link: articels_figures_by_rev_year\2020\A_Dual_Camera_System_for_High_Spatiotemporal_Resolution_Video_Acquisition\figure_7.jpg
  Figure 7 caption: "Super-Resolution: Outdoor activity with low light illumination.\
    \ I ref is the captured 4k frame from the HSR-LFR camera, and synchronized I LSR\u2191\
    \ is the up-scaled frame from the captured 720p frame of the LSR-HFR camera. Zoomed-regions\
    \ are visualized for (c) I ref , (d) I LSR\u2191 , and super-resolved reconstructions\
    \ using (e) EDSR, (f) PM, (g) CrossNet, (h) AWnet with \u03C3 2 =0 (no noise regularization),\
    \ and (i) AWnet with \u03C3 2 =0.01 ."
  Figure 8 Link: articels_figures_by_rev_year\2020\A_Dual_Camera_System_for_High_Spatiotemporal_Resolution_Video_Acquisition\figure_8.jpg
  Figure 8 caption: Frame interpolation. Indoor activity with medium light illumination.
    The most left and the most right of first rows are the captured HSR-LFR frames.
    Seven frames in-between are interpolated using ToFlow-Intp [47]; The second rows
    are the synthesized HSTR frames using our AWnet, which is trained with noise regularization
    with the variance of 0.01. Zoom in the pictures, and you will see more image details.
    (More in supplemental material.)
  Figure 9 Link: articels_figures_by_rev_year\2020\A_Dual_Camera_System_for_High_Spatiotemporal_Resolution_Video_Acquisition\figure_9.jpg
  Figure 9 caption: Subjective Evaluation of Adaptive Weighting Fusion and Convolution-based
    Direction Prediction Using Synthetic Vimeo90K Test Dataset. (a) I ref ; (b) I
    LSR ; (c) Convolution-based Direct Prediction; (d) Adaptive Weighting Fusion;
    (e) Ground truth. The upper part is a raining scene with zoomed rain drop and
    label. The bottom part is a gym scene with zoomed face of a trainee.
  First author gender probability: 0.75
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Ming Cheng
  Name of the last author: Jun Sun
  Number of Figures: 13
  Number of Tables: 9
  Number of authors: 7
  Paper title: A Dual Camera System for High Spatiotemporal Resolution Video Acquisition
  Publication Date: 2020-04-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Notations and Abbreviations
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Objective Performance Comparison of Super-Resolution Methods
      on Vimeo90K Dataset [47]
  Table 3 caption:
    table_text: TABLE 3 Objective Performance Comparison of Frame Interpolation Using
      Vimeo90K [47] (Downscaled 4th Frame Used as Reference in AWnet)
  Table 4 caption:
    table_text: TABLE 4 Performance Impact of Different Upscaling Filter
  Table 5 caption:
    table_text: TABLE 5 Objective PSNR of Reconstructed Images for Single-Reference
      and Multi-Reference AWnet
  Table 6 caption:
    table_text: TABLE 6 Objective Comparison Between Adaptive Weighting Fusion and
      Convolution-Based Direct Prediction for Output Pixel Reconstruction
  Table 7 caption:
    table_text: "TABLE 7 Objective Performance Comparison of 4\xD7 and 8\xD7 Super-Resolution\
      \ Methods on Flower and LFVideo Datasets"
  Table 8 caption:
    table_text: "TABLE 8 Objective Performance (PSNR) Comparison of 8\xD7 Super-Resolution\
      \ Methods on Stanford Light Field Dataset"
  Table 9 caption:
    table_text: TABLE 9 Performance Evaluation Using KITTI Dataset for Super-Resolution
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2983371
- Affiliation of the first author: wangxuan institute of computer technology, peking
    university, beijing, china
  Affiliation of the last author: wangxuan institute of computer technology, peking
    university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2020\TEK_Artistic_Text_Benchmark_for_Text_Effect_Transfer\figure_1.jpg
  Figure 1 caption: Representative text effects in TE141K. Text styles are grouped
    into three subsets based on glyph type, including TE141K-E (English alphabet subset,
    67 styles), TE141K-C (Chinese character subset, 65 styles), and TE141K-S (symbol
    and other language subset, 20 styles).
  Figure 10 Link: articels_figures_by_rev_year\2020\TEK_Artistic_Text_Benchmark_for_Text_Effect_Transfer\figure_10.jpg
  Figure 10 caption: Comparison with other methods on one-reference supervised text
    effect transfer. (a) Input example text effects with the target text in the lower-left
    corner. (b) Analogy [21]. (c) Doodles [23]. (d) T-Effect [9]. (e) Pix2pix+ [10].
    (f) TET-GAN+.
  Figure 2 Link: articels_figures_by_rev_year\2020\TEK_Artistic_Text_Benchmark_for_Text_Effect_Transfer\figure_2.jpg
  Figure 2 caption: 'Our TET-GAN implements two functions: destylization for removing
    style features from the text and stylization for transferring the visual effects
    from highly stylized text onto other glyphs.'
  Figure 3 Link: articels_figures_by_rev_year\2020\TEK_Artistic_Text_Benchmark_for_Text_Effect_Transfer\figure_3.jpg
  Figure 3 caption: Distribution-aware data augmentation. (a) Raw text image. (b)
    Results of distribution-aware text image preprocessing (the contrast is enhanced
    for better visualization). (c)-(e) Results of distribution-aware text effect augmentation
    by tinting (b) using random colormaps.
  Figure 4 Link: articels_figures_by_rev_year\2020\TEK_Artistic_Text_Benchmark_for_Text_Effect_Transfer\figure_4.jpg
  Figure 4 caption: A comparison of results with and without our distribution-aware
    data preprocessing and augmentation.
  Figure 5 Link: articels_figures_by_rev_year\2020\TEK_Artistic_Text_Benchmark_for_Text_Effect_Transfer\figure_5.jpg
  Figure 5 caption: Statistics of TE141K. (a) Visualized text effect distribution
    in TE141K by t-SNE [19] based on their VGG features [20]. Perceptually similar
    text effects are placed close to each other. The even distribution indicates the
    diversity and richness of our dataset. (b) Distribution of different background,
    foreground, stroke, and stereo effects subclasses of our dataset. Representative
    images are shown at the top. The first row contains schematics where red is used
    to represent specific text effect subclasses. The second row contains representative
    samples from TE141K.
  Figure 6 Link: articels_figures_by_rev_year\2020\TEK_Artistic_Text_Benchmark_for_Text_Effect_Transfer\figure_6.jpg
  Figure 6 caption: 'The TET-GAN architecture. (a) An overview of the TET-GAN architecture.
    Our network is trained via three objectives: an autoencoder, destylization and
    stylization. (b) A glyph autoencoder to learn content features. (c) Destylization
    by disentangling content features from text effect images. (d) Stylization by
    combining content and style features.'
  Figure 7 Link: articels_figures_by_rev_year\2020\TEK_Artistic_Text_Benchmark_for_Text_Effect_Transfer\figure_7.jpg
  Figure 7 caption: 'One-reference text effects transfer. (a) New, user-specified
    text effects. (b) Random crop of the style image to generate image pairs for training.
    (c) Top row: Stylization result on an unseen style. Bottom row: Stylization result
    after one-reference finetuning. The model finetuned over (a) is able to transfer
    text effects onto other unseen characters.'
  Figure 8 Link: articels_figures_by_rev_year\2020\TEK_Artistic_Text_Benchmark_for_Text_Effect_Transfer\figure_8.jpg
  Figure 8 caption: Hybrid font style and text effect transfer framework. Two TET-GANs
    trained on the font dataset and text effects dataset constitute a uniform framework
    to transfer both font style and text effects.
  Figure 9 Link: articels_figures_by_rev_year\2020\TEK_Artistic_Text_Benchmark_for_Text_Effect_Transfer\figure_9.jpg
  Figure 9 caption: Comparison with state-of-the-art methods on general text effect
    transfer. (a) Input example text effects with the target text in the lower-left
    corner. (b) AdaIN [3]. (c) WCT [5]. (d) Doodles [23]. (e) T-Effect [9]. (f) Pix2pix
    [10]. (g) BicycleGAN [25]. (h) StarGAN [12]. (i) TET-GAN.
  First author gender probability: 0.85
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.94
  Name of the first author: Shuai Yang
  Name of the last author: Jiaying Liu
  Number of Figures: 20
  Number of Tables: 5
  Number of authors: 3
  Paper title: 'TE141K: Artistic Text Benchmark for Text Effect Transfer'
  Publication Date: 2020-04-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 A Summary of TE141K
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Summary of Benchmarking Representative Style Transfer Models
      and the Proposed TET-GAN, Showing the Model Type, Model Names, Number of Style
      Supported per Model ( Style), Support for Supervised One-Reference (SOR) or
      Unsupervised One-Reference (UOR) Style Transfer, Availability of Feed-Forward
      Fast Style Transfer, Target Type (General Image Style or Text Effect Transfer),
      Usage of Deep Models and the Style Representation
  Table 3 caption:
    table_text: TABLE 3 Performance Benchmarking on the Task of General Text Effect
      Transfer With PSNR, SSIM, Perceptual Loss, Style Loss, and the Average Score
      of the User Study
  Table 4 caption:
    table_text: TABLE 4 Performance Benchmarking on the Task of Supervised One-Reference
      Text Effect Transfer With PSNR, SSIM, Perceptual Loss, Style Loss, and the Average
      Score of the User Study
  Table 5 caption:
    table_text: TABLE 5 Performance Benchmarking on the Task of Unsupervised One-Reference
      Text Effect Transfer With PSNR, SSIM, Perceptual Loss, Style Loss, and the Average
      Score of the User Study
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2983697
- Affiliation of the first author: department of electrical and electronic engineering,
    imperial college london, london, united kingdom
  Affiliation of the last author: department of electrical and electronic engineering,
    imperial college london, london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2020\Deep_Convolutional_Neural_Network_for_MultiModal_Image_Restoration_and_Fusion\figure_1.jpg
  Figure 1 caption: Examples of different multi-modal image restoration and fusion
    tasks. The first row shows the MIR related applications, including RGB guided
    depth image SR, RGB guided multi-spectral image SR, and flash guided non-flash
    image denoising. The second row shows the MIF related applications, including
    multi-exposure image fusion, multi-focus image fusion, and medical image fusion.
  Figure 10 Link: articels_figures_by_rev_year\2020\Deep_Convolutional_Neural_Network_for_MultiModal_Image_Restoration_and_Fusion\figure_10.jpg
  Figure 10 caption: Visual comparisons of medical image fusion results. (a) T1-weighted
    MR image, (b) T2-weighted MR image, (c) CSR [3], (d) OHWF [80], (e) PA-PCNN [81],
    (f) our CU-Net. Better view in electronic version.
  Figure 2 Link: articels_figures_by_rev_year\2020\Deep_Convolutional_Neural_Network_for_MultiModal_Image_Restoration_and_Fusion\figure_2.jpg
  Figure 2 caption: Network Architecture of the proposed CU-Net. For the MIR related
    tasks, the final reconstruction (Point 4) is composed of the common reconstruction
    (Point 1) and the unique reconstruction (Point 2). For the MIF related tasks,
    the final reconstruction is composed of the common reconstruction (Point 1) and
    the two unique reconstructions (Point 2 and Point 3).
  Figure 3 Link: articels_figures_by_rev_year\2020\Deep_Convolutional_Neural_Network_for_MultiModal_Image_Restoration_and_Fusion\figure_3.jpg
  Figure 3 caption: The network architectures of UFEM and CFPM.
  Figure 4 Link: articels_figures_by_rev_year\2020\Deep_Convolutional_Neural_Network_for_MultiModal_Image_Restoration_and_Fusion\figure_4.jpg
  Figure 4 caption: The change of objective performance across different images in
    the dataset in terms of PSNR, RMSE and SSIM.
  Figure 5 Link: articels_figures_by_rev_year\2020\Deep_Convolutional_Neural_Network_for_MultiModal_Image_Restoration_and_Fusion\figure_5.jpg
  Figure 5 caption: "Visual comparisons of RGB guided depth SR results for 4\xD7 upscaling:\
    \ (a) Bicubic, (b) Lu et al. [56], (c) Xie et al. [53], (d) Gu et al.[5], (e)\
    \ SCN [57], (f) DGF [30], (g) DJFR [11], (h) RADAR [60], (i) our CU-Net, and (j)\
    \ ground truth."
  Figure 6 Link: articels_figures_by_rev_year\2020\Deep_Convolutional_Neural_Network_for_MultiModal_Image_Restoration_and_Fusion\figure_6.jpg
  Figure 6 caption: "Visual comparisons of RGB guided multi-spectral image SR results\
    \ for 4\xD7 upscaling. The multi-spectral images are of 640nm wavelength. The\
    \ first row shows the super-resolved images using different methods, and the second\
    \ row shows the error maps between the images in the first row and the ground-truth."
  Figure 7 Link: articels_figures_by_rev_year\2020\Deep_Convolutional_Neural_Network_for_MultiModal_Image_Restoration_and_Fusion\figure_7.jpg
  Figure 7 caption: 'Visual comparisons of flash guided non-flash image denoising
    results with sigma 2=75 : (a) Noisy non-flash input image, (b) MuGIF [28], (c)
    CBM3D [71], (d) DnCNN [64], and (e) our CU-Net.'
  Figure 8 Link: articels_figures_by_rev_year\2020\Deep_Convolutional_Neural_Network_for_MultiModal_Image_Restoration_and_Fusion\figure_8.jpg
  Figure 8 caption: 'Visual comparisons of multi-exposure image fusion results: (a)
    input under-exposed image, (b) input over-exposed image, (c) SPD-MEF [73], (d)
    MEF-OPT [74], and (e) our CU-Net. The last row shows the enlarged regions in the
    corresponding images. Better view in electronic version.'
  Figure 9 Link: articels_figures_by_rev_year\2020\Deep_Convolutional_Neural_Network_for_MultiModal_Image_Restoration_and_Fusion\figure_9.jpg
  Figure 9 caption: Visual comparisons of multi-focus image fusion results. (a) The
    near-focus image, (b) The far-focus image, (c) CSR [3], (d) Paul et al. [78],
    (e) Liu et al. [47], (f) our CU-Net. Better view in electronic version.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xin Deng
  Name of the last author: Pier Luigi Dragotti
  Number of Figures: 14
  Number of Tables: 9
  Number of authors: 2
  Paper title: Deep Convolutional Neural Network for Multi-Modal Image Restoration
    and Fusion
  Publication Date: 2020-04-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of Notations
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Results on the RGBDepth Dataset for 4\xD7 Upscaling, With\
      \ the Best Results in Bold and the Second Best Results Underlined"
  Table 3 caption:
    table_text: "TABLE 3 Results on the RGBMS Dataset for 4\xD7 Upscaling, With the\
      \ Best Results in Bold and the Second Best Results Underlined"
  Table 4 caption:
    table_text: TABLE 4 Results on the FlashNon-Flash Dataset for Image Denoising,
      With the Best Results in Bold and the Second Best Results Underlined
  Table 5 caption:
    table_text: TABLE 5 The Quantitative Comparison Results for MIF Tasks, With the
      Best Results in Bold and the Second Best Underlined
  Table 6 caption:
    table_text: TABLE 6 Importance of the Residual Module Architecture
  Table 7 caption:
    table_text: TABLE 7 Effects of the Filter Size
  Table 8 caption:
    table_text: TABLE 8 Effects of the Network Depth in Each Module
  Table 9 caption:
    table_text: TABLE 9 The Average Running Time (in Seconds) of Our Method on Various
      Tasks
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2984244
- Affiliation of the first author: school of computer science, fudan university, shanghai,
    china
  Affiliation of the last author: school of computer science, fudan university, shanghai,
    china
  Figure 1 Link: articels_figures_by_rev_year\2020\PixelMesh_D_Mesh_Model_Generation_via_Image_Guided_Deformation\figure_1.jpg
  Figure 1 caption: Given a single color image and an initial mesh, our method can
    produce a high-quality mesh that contains details from the example. Comparatively,
    previous work based on volume and point cloud cannot recover detail and is non-trivial
    to be converted into water-tight mesh models. Moreover, our model can predict
    per-vertex properties such as a texture map.
  Figure 10 Link: articels_figures_by_rev_year\2020\PixelMesh_D_Mesh_Model_Generation_via_Image_Guided_Deformation\figure_10.jpg
  Figure 10 caption: (a) The ground truth face points is subdivided into 4 regions,
    central region has larger weight. (b)The result with weighted loss lead to more
    vertices in central region and fine-grained details.
  Figure 2 Link: articels_figures_by_rev_year\2020\PixelMesh_D_Mesh_Model_Generation_via_Image_Guided_Deformation\figure_2.jpg
  Figure 2 caption: The cascaded mesh deformation network. Our full model contains
    three mesh deformation blocks in a row. Each block increases mesh resolution and
    estimates vertex locations, which are then used to extract perceptual image features
    from the 2D CNN for the next block.
  Figure 3 Link: articels_figures_by_rev_year\2020\PixelMesh_D_Mesh_Model_Generation_via_Image_Guided_Deformation\figure_3.jpg
  Figure 3 caption: "(a) The vertex locations C i are used to extract image features,\
    \ which are then combined with vertex features F i and fed into G-ResNet. \u2A01\
    \ means a concatenation of the features. (b) The 3D vertices are projected to\
    \ the image plane using camera intrinsics, and perceptual features are pooled\
    \ from the 2D-CNN layers using bilinear interpolation."
  Figure 4 Link: articels_figures_by_rev_year\2020\PixelMesh_D_Mesh_Model_Generation_via_Image_Guided_Deformation\figure_4.jpg
  Figure 4 caption: (a) Black vertices and dashed edges are added in the unpooling
    layer. (b) The face based unpooling leads to imbalanced vertex degrees, while
    the edge-based unpooling remains regular.
  Figure 5 Link: articels_figures_by_rev_year\2020\PixelMesh_D_Mesh_Model_Generation_via_Image_Guided_Deformation\figure_5.jpg
  Figure 5 caption: The additional branch in the last block to predict the vertex
    property.
  Figure 6 Link: articels_figures_by_rev_year\2020\PixelMesh_D_Mesh_Model_Generation_via_Image_Guided_Deformation\figure_6.jpg
  Figure 6 caption: Qualitative results. (a) Input image; (b) volume from 3D-R2N2
    [1], converted using Marching Cube [51]; (c) point cloud from PSG [2], converted
    using ball pivoting [53]; (d) N3MR [29]; (e) AtlasNet [52]; (f) ONet [28]; and
    (g) ours; (h) ground truth.
  Figure 7 Link: articels_figures_by_rev_year\2020\PixelMesh_D_Mesh_Model_Generation_via_Image_Guided_Deformation\figure_7.jpg
  Figure 7 caption: Qualitative results of real-world images from the Online Products
    dataset and Internet.
  Figure 8 Link: articels_figures_by_rev_year\2020\PixelMesh_D_Mesh_Model_Generation_via_Image_Guided_Deformation\figure_8.jpg
  Figure 8 caption: Ground truth meshes (top) and failure cases of Pixel2Mesh (bottom).
  Figure 9 Link: articels_figures_by_rev_year\2020\PixelMesh_D_Mesh_Model_Generation_via_Image_Guided_Deformation\figure_9.jpg
  Figure 9 caption: Visualization of predicted colored meshes.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Nanyang Wang
  Name of the last author: Yu-Gang Jiang
  Number of Figures: 13
  Number of Tables: 5
  Number of authors: 8
  Paper title: 'Pixel2Mesh: 3D Mesh Model Generation via Image Guided Deformation'
  Publication Date: 2020-04-02 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 F-Score (%) on the ShapeNet Test Set at Different Thresholds,\
      \ Where \u03C4= 10 \u22124 \u03C4=10-4"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Chamfer Distance and Earth Movers Distance on the ShapeNet
      Test Set at Different Thresholds
  Table 3 caption:
    table_text: TABLE 3 Comparsion to Tartarchenko et al.( 128 3 1283) on ShapeNet-Cars
  Table 4 caption:
    table_text: TABLE 4 Ablation Study that Evaluates the Contributions of Different
      Ideas to the Performance of the Presented Model
  Table 5 caption:
    table_text: TABLE 5 Comparsion to State-of-the-Art Face Reconstruction Methods
      Using Normalised Mean Error (NME) on the AFLW2000-3D Dataset
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2984232
- Affiliation of the first author: university of barcelona and computer vision center,
    barcelona, spain
  Affiliation of the last author: donders institute for brain, cognition and behaviour,
    radboud university, nijmegen, the netherlands
  Figure 1 Link: Not Available
  Figure 1 caption: Not Available
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Sergio Escalera
  Name of the last author: "Umut G\xFC\xE7l\xFC"
  Number of Figures: Not Available
  Number of Tables: 1
  Number of authors: 9
  Paper title: 'Guest Editorial: Image and Video Inpainting and Denoising'
  Publication Date: 2020-04-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Overview of Articles in the Special Issue on Imagen and Video
      Inpainting and Denosing
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2971291
