- Affiliation of the first author: department of computer science and engineering,
    michigan state university, east lansing, mi
  Affiliation of the last author: department of computer science and engineering,
    michigan state university, east lansing, mi
  Figure 1 Link: articels_figures_by_rev_year\2017\Longitudinal_Study_of_Automatic_Face_Recognition\figure_1.jpg
  Figure 1 caption: Face image pairs of four subjects from the PCSOLS mugshot database
    which are age-separated by eight to ten years. Similarity scores from a state-of-the-art
    face matcher (COTS-A) are shown in parentheses (score range is [0.0, 1.0]). The
    thresholds at 0.01% and 0.1% FAR are 0.533 and 0.454, respectively. Hence, all
    of these genuine pairs would be falsely rejected at 0.01% FAR, while the two female
    subjects, (a) and (b), would also be rejected at 0.1% FAR.
  Figure 10 Link: articels_figures_by_rev_year\2017\Longitudinal_Study_of_Automatic_Face_Recognition\figure_10.jpg
  Figure 10 caption: Example outlier subjects, i.e., subjects whose subject-specific
    trends, estimated by model BT, significantly deviate from the spread of the population
    in the PCSOLS database. All images were aligned using COTS-A eye locations.
  Figure 2 Link: articels_figures_by_rev_year\2017\Longitudinal_Study_of_Automatic_Face_Recognition\figure_2.jpg
  Figure 2 caption: Statistics of the two longitudinal face image databases (PCSOLS
    and LEOLS) used in this study. (a) and (e) Number of face images per subject,
    (b) and (f) the time span of each subject (i.e., the number of years between a
    subject's youngest and oldest face image acquisitions), (c) and (g) demographic
    distributions of sex (male, female) and race (white, black, Asian, Indian, unknown),
    and (d) and (h) the age of the youngest image of each subject (in years).
  Figure 3 Link: articels_figures_by_rev_year\2017\Longitudinal_Study_of_Automatic_Face_Recognition\figure_3.jpg
  Figure 3 caption: Three examples of labeling errors in the PCSOLS face database.
    All pairs show two different subjects who are labeled with the same subject ID
    number in the database.
  Figure 4 Link: articels_figures_by_rev_year\2017\Longitudinal_Study_of_Automatic_Face_Recognition\figure_4.jpg
  Figure 4 caption: Examples of facial occlusions (sunglasses, bandages, and bruises)
    in the PCSOLS face database.
  Figure 5 Link: articels_figures_by_rev_year\2017\Longitudinal_Study_of_Automatic_Face_Recognition\figure_5.jpg
  Figure 5 caption: Age distribution of a random sample of 200 subjects from the PCSOLS
    database. Each line denotes the age span of a subject (i.e., age of youngest image
    to age of the oldest image), separated along the y -axis by the elapsed time for
    each subject (i.e., the length of the age span).
  Figure 6 Link: articels_figures_by_rev_year\2017\Longitudinal_Study_of_Automatic_Face_Recognition\figure_6.jpg
  Figure 6 caption: "An example of cross-sectional versus longitudinal analysis. (a)\
    \ Face images of six example subjects from the PCSOLS database. The enrollment\
    \ face image (leftmost column) is the youngest image of each subject, and all\
    \ query images are in order of increasing age. In this study, genuine similarity\
    \ scores are computed by comparing the query images of each subject to hisher\
    \ enrollment image. In (b), a cross-sectional approach (ordinary least squares\
    \ (OLS) linear regression) is applied, which incorrectly assumes that all the\
    \ scores are independent. In (c), OLS is instead applied six times, separately\
    \ to each subject's set of scores. The slope estimated by cross-sectional analysis\
    \ (black dotted line) is much flatter than the slopes of subject-specific trends\
    \ in (solid colored lines in (c)). The longitudinal analysis in this paper utilizes\
    \ mixed-effects models, which provide \u201Cshrunken\u201D OLS estimates for each\
    \ subject, where the OLS trends shrink towards a population-mean trend [22] ,\
    \ [23], further accounting for the correlation that exists between scores from\
    \ the same subject."
  Figure 7 Link: articels_figures_by_rev_year\2017\Longitudinal_Study_of_Automatic_Face_Recognition\figure_7.jpg
  Figure 7 caption: 'Distributions of standardized genuine comparison scores from
    the two longitudinal face databases used in this study: (a) COTS-A on PCSOLS and
    (b) COTS-B on LEOLS. There are a total of 129,773 and 26,216 genuine scores in
    (a) and (b), respectively.'
  Figure 8 Link: articels_figures_by_rev_year\2017\Longitudinal_Study_of_Automatic_Face_Recognition\figure_8.jpg
  Figure 8 caption: Normal probability plots of ((a) and (d)) level-1 residuals, varepsilon
    ij , and level-2 random effects for ((b) and (e)) intercepts, b0i , and ((c) and
    (f)) slopes, b1i , from model BT on the PCSOLS and LEOLS databases (top and bottom
    rows, respectively). Departure from normality at the tails of the distributions
    is likely due to low quality face images or errors in subject IDs.
  Figure 9 Link: articels_figures_by_rev_year\2017\Longitudinal_Study_of_Automatic_Face_Recognition\figure_9.jpg
  Figure 9 caption: Results from model BT on COTS-A genuine scores from the PCSOLS
    database. The bootstrap-estimated population-mean trend is shown in black (bootstrap
    confidence intervals are too small to be visible). The blue and green bands plot
    regions of 95% and 99% confidence, respectively, for subject-specific variations
    around the population-mean trend. Grey dotted lines additionally add one standard
    deviation of estimated residual variation, sigma varepsilon . Hence, model BT
    estimates that 95% and 99% of the subject trends fall within the blue and green
    bands, but scores can vary around their trends, extending to the grey dotted lines.
    Thresholds at 0.01% and 0.1% FAR for COTS-A are shown as dashed red lines.
  First author gender probability: 0.97
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Lacey Best-Rowden
  Name of the last author: Anil K. Jain
  Number of Figures: 14
  Number of Tables: 8
  Number of authors: 2
  Paper title: Longitudinal Study of Automatic Face Recognition
  Publication Date: 2017-01-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Table of Related Work on the Effects of Facial Aging on Face
      Recognition Performance
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Facial Aging Databases
  Table 3 caption:
    table_text: TABLE 3 Overall True Accept Rates (TARs) at Fixed False Accept Rates
      (FARs) for Various Face Matchers on the PCSOLS and LEOLS Databases
  Table 4 caption:
    table_text: TABLE 4 Mixed-Effects Model Formulations
  Table 5 caption:
    table_text: TABLE 5 Mixed-Effects Models on the PCSOLS Database and COTS-A Genuine
      Scores
  Table 6 caption:
    table_text: TABLE 6 Bootstrap Results for Mixed-Effects Models with Elapsed Time
      and Face Quality Covariates for the PCSOLS Database and COTS-A Genuine Scores
  Table 7 caption:
    table_text: TABLE 7 Elapsed Times (in Years) for When Population-Mean Trends in
      Genuine Scores Drop Below the Decision Thresholds at 0.001% and 0.01% FAR for
      Different Measures Related to Face Quality (Frontalness and IPD) of the Enrollment
      Image Q ie and the Query Image Q ij
  Table 8 caption:
    table_text: TABLE 8 Mixed-Effects Model Results for the LEOLS Database and COTS-B
      Genuine Scores
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2652466
- Affiliation of the first author: the engineering research center for advanced computing
    engineering software of ministry of education, china
  Affiliation of the last author: department of computing, hong kong polytechnic university,
    kowloon, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2017\Active_SelfPaced_Learning_for_CostEffective_and_Progressive_Face_Identification\figure_1.jpg
  Figure 1 caption: Illustration of high- and low-confidence samples in the feature
    space. (a) Shows a few face instances of different individuals, and these instances
    have large appearance variations. (b) Illustrates how the samples distribute in
    the feature space, where samples of high classification confidence distribute
    compactly to form several clusters and low confidence samples are scattered and
    close to the classifier decision boundary.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Active_SelfPaced_Learning_for_CostEffective_and_Progressive_Face_Identification\figure_2.jpg
  Figure 2 caption: Illustration of our proposed cost-effective framework. The pipeline
    includes stages of CNN and model initialization; classifier updating; high-confidence
    sample labeling by the SPL, low-confidence sample annotating by AL and CNN fine-tuning,
    where the arrows represent the workflow. The images highlighted by blue in the
    left panel represent the initially selected samples.
  Figure 3 Link: articels_figures_by_rev_year\2017\Active_SelfPaced_Learning_for_CostEffective_and_Progressive_Face_Identification\figure_3.jpg
  Figure 3 caption: Results on (a) CACD and (b) CASIA-WebFace-Sub datasets. The vertical
    axes represent the recognition accuracy and the horizontal axes represent the
    percentage of annotated samples of the whole set.
  Figure 4 Link: articels_figures_by_rev_year\2017\Active_SelfPaced_Learning_for_CostEffective_and_Progressive_Face_Identification\figure_4.jpg
  Figure 4 caption: Accuracies with the increase of annotated samples of different
    variants of our framework, using CASIA-Webface-Sub dataset.
  Figure 5 Link: articels_figures_by_rev_year\2017\Active_SelfPaced_Learning_for_CostEffective_and_Progressive_Face_Identification\figure_5.jpg
  Figure 5 caption: The accuracy and standard deviation of ASPL and SPL on the CASIA-Webface-Sub
    dataset.
  Figure 6 Link: articels_figures_by_rev_year\2017\Active_SelfPaced_Learning_for_CostEffective_and_Progressive_Face_Identification\figure_6.jpg
  Figure 6 caption: The comparison of different number of initial samples and the
    further required annotation ported of the AL process on the CASIA-Webface-Sub
    dataset. For fair comparison, these methods share the same feature representation
    as initialization.
  Figure 7 Link: articels_figures_by_rev_year\2017\Active_SelfPaced_Learning_for_CostEffective_and_Progressive_Face_Identification\figure_7.jpg
  Figure 7 caption: The comparison of different number of initial samples and the
    further required annotation ported of the AL process on the CASIA-WebFace-Sub
    dataset.
  Figure 8 Link: articels_figures_by_rev_year\2017\Active_SelfPaced_Learning_for_CostEffective_and_Progressive_Face_Identification\figure_8.jpg
  Figure 8 caption: Robust analysis of ASPL under two types of noisy samples. (a)
    Using different number of noisy samples as the initial annotation. (b) Adding
    different number of noisy samples at the 10th step (denoted by the black spots).
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.55
  Name of the first author: Liang Lin
  Name of the last author: Lei Zhang
  Number of Figures: 8
  Number of Tables: 3
  Number of authors: 5
  Paper title: Active Self-Paced Learning for Cost-Effective and Progressive Face
    Identification
  Publication Date: 2017-01-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Summarization of Datasets We Used
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Performance Comparison of Whether Handling Unseen New
      Classes or Not on the CASIA-WebFace-Sub Dataset
  Table 3 caption:
    table_text: TABLE 3 The Error Rates of the Pseudo-Labels Assigned by SPL on High-Confidence
      Samples
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2652459
- Affiliation of the first author: advanced digital sciences center, singapore
  Affiliation of the last author: advanced digital sciences center, singapore
  Figure 1 Link: articels_figures_by_rev_year\2017\CODE_Coherence_Based_Decision_Boundaries_for_Feature_Correspondence\figure_1.jpg
  Figure 1 caption: Feature matching for an example image pair. A) A-SIFT [6], a highly
    regarded wide-baseline feature matcher. B) Relaxed acceptance thresholds result
    in many more true matches hidden in a pile of false matches. C) Our fusion of
    A-SIFT with the proposed technique termed CODE. CODE regression can model the
    motion adequately even under such noisy matches in B). Note that only the right
    side of the main arch structure is visible in the second image.
  Figure 10 Link: articels_figures_by_rev_year\2017\CODE_Coherence_Based_Decision_Boundaries_for_Feature_Correspondence\figure_10.jpg
  Figure 10 caption: Evaluating CODE against other feature matchers. A challenging
    image dataset and a set of metrics as described in Section 4.1 are used for this
    evaluation. The dataset contains full-resolution, representative images primarily
    from a variety of papers [13] , [24], [26], [29]. Set A has limited out-of-plane
    rotation while Set B has very large viewpoint changes. CODE with constant parameters
    excels in a wide variety of scenarios. This is shown by low Total Failures and
    high Precision.
  Figure 2 Link: articels_figures_by_rev_year\2017\CODE_Coherence_Based_Decision_Boundaries_for_Feature_Correspondence\figure_2.jpg
  Figure 2 caption: "Regression can be understood as finding a continuous surface\
    \ that explains scattered data points (denoted by \u201C+\u201D)."
  Figure 3 Link: articels_figures_by_rev_year\2017\CODE_Coherence_Based_Decision_Boundaries_for_Feature_Correspondence\figure_3.jpg
  Figure 3 caption: Coherence based separation of true and false matches. Motions
    are considered coherent if (a) many local points make similar motions or (b) there
    is broad spatial support for the motion. This is enforced via the likelihood function
    in Eqn. (21). In contrast, feature matches in (c) and (d) do not give coherent
    motions, as the matches are not consistent in (c), while there are insufficient
    smoothly moving points to justify a long-range motion coherence model in (d).
  Figure 4 Link: articels_figures_by_rev_year\2017\CODE_Coherence_Based_Decision_Boundaries_for_Feature_Correspondence\figure_4.jpg
  Figure 4 caption: 'Inset: A set of motion hypotheses with discontinuities over the
    one-dimensional X axis. Main figure: The same data (black dots) on the bilateral
    domain u and X gives rise to a motion likelihood model built from the proposed
    regression technique. Observe that estimating a regression function over the bilateral
    domain involves estimating a value for every possible motion at every spatial
    location. Thus, a cross section along the specified X -position yields a likelihood
    function over the admissible motion range that a query X -position can take from.
    Note also that a smooth likelihood function computed for a motion-augmented bilateral
    domain can explain motion data with discontinuities.'
  Figure 5 Link: articels_figures_by_rev_year\2017\CODE_Coherence_Based_Decision_Boundaries_for_Feature_Correspondence\figure_5.jpg
  Figure 5 caption: CODE applied to A-SIFT features [6]. A and B demonstrate CODE's
    ability to handle large motion discontinuities and occlusions, while C demonstrates
    its ability to handle large rotations. In alternative display, we color code feature
    matches according to their spatial locations on the left image. The matching transfers
    these color codes to the right image. To better illustrate the large rotation,
    C uses a vector display.
  Figure 6 Link: articels_figures_by_rev_year\2017\CODE_Coherence_Based_Decision_Boundaries_for_Feature_Correspondence\figure_6.jpg
  Figure 6 caption: "Workflow of the proposed CODE technique. It consists of two major\
    \ steps: building regression functions and applying cascaded filters, which take\
    \ noisy feature matches obtained with different threshold values \u03C4 as the\
    \ input. See the text for details."
  Figure 7 Link: articels_figures_by_rev_year\2017\CODE_Coherence_Based_Decision_Boundaries_for_Feature_Correspondence\figure_7.jpg
  Figure 7 caption: Comparison between the original A-SIFT matcher [6] and A-SIFT
    w CODE, applying the proposed CODE regression to A-SIFT. For this test, we constructed
    a challenging dataset of static scene images. It includes representative images
    from a variety of prior papers [11], [13], [24], [29]. Set A has limited out-of-plane
    rotation, while Set B features very large viewpoint changes. CODE recovers many
    true feature matches that A-SIFT discards while maintaining high precision. This
    gain is especially noticeable for wide-baseline image pairs contained in Set B.
  Figure 8 Link: articels_figures_by_rev_year\2017\CODE_Coherence_Based_Decision_Boundaries_for_Feature_Correspondence\figure_8.jpg
  Figure 8 caption: Structure-from-motion seeks to fuse 2D images into 3D models.
    CODE recovers large numbers of reliable feature matches, making this task easier
    and more robust.
  Figure 9 Link: articels_figures_by_rev_year\2017\CODE_Coherence_Based_Decision_Boundaries_for_Feature_Correspondence\figure_9.jpg
  Figure 9 caption: Evaluation on the co-recognition dataset [49]. We use a Recall
    (area) metric which favors dense correspondence over feature matchers. Despite
    this, CODE is surprisingly competitive at the co-segmentation task. This is a
    testament to CODE's number of correspondences and ability to handle multiple motion
    models. Example results are visualized in the top panel.
  First author gender probability: 0.64
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Wen-Yan Lin
  Name of the last author: Jiangbo Lu
  Number of Figures: 12
  Number of Tables: 2
  Number of authors: 7
  Paper title: 'CODE: Coherence Based Decision Boundaries for Feature Correspondence'
  Publication Date: 2017-01-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of Parameters and Variables in Our Multi-Function
      Regression in Eqns. (16), (17), and (18)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Breakdown of Average Timing of Set A in Fig. 10
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2652468
- Affiliation of the first author: department of signal processing, tampere university
    of technology, tampere, finland
  Affiliation of the last author: department of signal processing, tampere university
    of technology, tampere, finland
  Figure 1 Link: articels_figures_by_rev_year\2017\Light_Field_Reconstruction_Using_Shearlet_Transform\figure_1.jpg
  Figure 1 caption: Epipolar-plane image (EPI) formation and its frequency domain
    properties. (a) Capturing setup and EPI formation, a scene point is observed by
    two pinhole cameras positioned at t 1 , t 2 at image coordinates v 1 and v 2 respectively;
    (b) Stack of captured images; an epipolar plane is highlighted for fixed vertical
    image coordinate u ; (c) Example of EPI; red line represents a scene point in
    different cameras; (d) Frequency support of a densely sampled EPI; green area
    represents the baseband bounded by min and max depth; yellow line corresponds
    to a depth layer, the slope determines the depth value; (e) Frequency domain structure
    of an EPI being insufficiently sampled over t -axis, the overlapping regions represent
    aliasing; (f) Desirable frequency domain separation based on depth layering; (g)
    Frequency domain separation based on dyadic scaling; (h) Composite directional
    and scaling based frequency domain separation for EPI sparse representation.
  Figure 10 Link: articels_figures_by_rev_year\2017\Light_Field_Reconstruction_Using_Shearlet_Transform\figure_10.jpg
  Figure 10 caption: (a) Considered scenes with and without semi-transparent plane
    in front. Reconstruction results are presented using the proposed method ( ST(5)
    ) and ( SGBM ). (b) Example of EPI of the scene and corresponding reconstructions.
  Figure 2 Link: articels_figures_by_rev_year\2017\Light_Field_Reconstruction_Using_Shearlet_Transform\figure_2.jpg
  Figure 2 caption: "(a) Frequency plane tilting by shearlet transform. C \u03C8 ,\
    \ C \u03C8 ~ are cone-like regions and C \u03D5 is low-frequency region. (b) Desirable\
    \ frequency domain tilting by proposed reconstruction algorithm. Gray color region\
    \ includes transform elements used for reconstruction; other transform elements\
    \ are not associated with valid shear values (disparities) in EPI. (c) \u03A8\
    \ d corresponding to constructed shearlet transform for J=2 . (d) Frequency domain\
    \ support of shearlet transform elements used in reconstruction algorithm corresponding\
    \ to gray color region in (b). Green contour regions in (d) represent significant\
    \ parts of transform elements support in frequency domain."
  Figure 3 Link: articels_figures_by_rev_year\2017\Light_Field_Reconstruction_Using_Shearlet_Transform\figure_3.jpg
  Figure 3 caption: The given 4 views with maximal disparity 16px between consecutive
    views are interpreted as every 16th view in the target densely sampled LF. (a)
    EPI for coarsely sampled LF over t -axis; (b) corresponding partially defined
    densely sampled EPI; (c) ground truth densely sampled EPI. Three different points
    from given input images forming traces are highlighted in the coarsely (a) and
    densely (c) sampled EPIs. Only in (c) they are revealed as a straight lines.
  Figure 4 Link: articels_figures_by_rev_year\2017\Light_Field_Reconstruction_Using_Shearlet_Transform\figure_4.jpg
  Figure 4 caption: Diagram of the EPI reconstruction algorithm.
  Figure 5 Link: articels_figures_by_rev_year\2017\Light_Field_Reconstruction_Using_Shearlet_Transform\figure_5.jpg
  Figure 5 caption: Example of reconstruction performance dependence on choice of
    acceleration coefficients alpha n . For constant value for all iterations alpha
    n=alpha , increasing alpha brings accelerating convergence. After some value,
    reconstruction starts to diverge (alpha =20) .
  Figure 6 Link: articels_figures_by_rev_year\2017\Light_Field_Reconstruction_Using_Shearlet_Transform\figure_6.jpg
  Figure 6 caption: Array of 17 times 17 views considered for reconstruction using
    5 times 5 views highlighted with black color. (a) Direct reconstruction method.
    (b) Hierarchic order of reconstruction (HR).
  Figure 7 Link: articels_figures_by_rev_year\2017\Light_Field_Reconstruction_Using_Shearlet_Transform\figure_7.jpg
  Figure 7 caption: (a) Input for reconstructing densely sampled EPI where only every
    16th row is available. (d) Densely sampled ground truth EPI. Reconstruction results
    using different transform are shown as follows (b) Haar 18.83 dB, (c) Shearlab
    [36] 29.27 dB, (e) FFST [42] 37.27 dB, (f) Proposed modified shearlet 40.75 dB.
  Figure 8 Link: articels_figures_by_rev_year\2017\Light_Field_Reconstruction_Using_Shearlet_Transform\figure_8.jpg
  Figure 8 caption: Reconstruction results for different multiview datasets, error
    shown in PSNR for reconstructed views.
  Figure 9 Link: articels_figures_by_rev_year\2017\Light_Field_Reconstruction_Using_Shearlet_Transform\figure_9.jpg
  Figure 9 caption: Reconstruction quality for datasets (a) Teddy and (b) Cones .
    Evaluation has been performed for proposed methods ST(5), ST(6) , DERS+VSRS, SGBM.
    Average PSNR of all reconstructed views presented in legend of the figures.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Suren Vagharshakyan
  Name of the last author: Atanas Gotchev
  Number of Figures: 17
  Number of Tables: 2
  Number of authors: 3
  Paper title: Light Field Reconstruction Using Shearlet Transform
  Publication Date: 2017-01-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Multiview Data Sets Details
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Full Parallax LF Reconstruction Quality Is Presented by Average
      PSNR in dB and Speed Is Given in Seconds per View (in Parentheses)
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2653101
- Affiliation of the first author: "department of signals and systems, g\xF6teborg,\
    \ g\xF6teborg, sweden"
  Affiliation of the last author: department of signals and systems, lund university,
    lund, sweden
  Figure 1 Link: articels_figures_by_rev_year\2017\Multiresolution_Search_of_the_Rigid_Motion_Space_for_IntensityBased_Registration\figure_1.jpg
  Figure 1 caption: "Bounds on high resolution registration fitness based on low resolution\
    \ registration. (a) A starfish image (obtained from http:www.ck12.org) is chosen\
    \ as the reference image f i , as it exhibits multiple local maxima, (b) f i is\
    \ rotated by 45 degrees around its centre and then undergoes a slight affine warp\
    \ to produce g i . The only registration parameter is the rotation angle \u03B8\
    \ . Both images are decimated by a factor of m=16 to create f l i and g l i (tiny\
    \ images on the bottom-right of each image). (c) the lower and upper bounds Q\
    \ l (\u03B8)\xB1 E h fg (red dashed graphs) envelop the high resolution fitness\
    \ Q(\u03B8)=Q(R,t) (blue solid graph). The horizontal line shows the value Q l\
    \ ( R l\u2217 , t l\u2217 )\u2212 E h fg . The red thick parts of this line show\
    \ the reduced search space, where we have Q l (R,t)+ E h fg \u2265 Q l ( R l\u2217\
    \ , t l\u2217 )\u2212 E h fg or Q l (R,t)\u2265 Q l ( R l\u2217 , t l\u2217 )\u2212\
    2 E h fg . (d) to further restrict the search space the high resolution fitness\
    \ Q is computed at ( R l\u2217 , t l\u2217 )\u2261 \u03B8 l\u2217 . The horizontal\
    \ line shows the value Q( R l\u2217 , t l\u2217 ) . The red thick parts of this\
    \ line shows the reduced search space, namely where Q l (R,t)+ E h fg \u2265Q(\
    \ R l\u2217 , t l\u2217 ) . Notice that decimating by a factor of 16 reduces the\
    \ computations by a factor of 256 for 2D images. The amount of reduction in the\
    \ search space provided by such a low resolution registration is notable."
  Figure 10 Link: articels_figures_by_rev_year\2017\Multiresolution_Search_of_the_Rigid_Motion_Space_for_IntensityBased_Registration\figure_10.jpg
  Figure 10 caption: Elapsed times for the single resolution and multiresolution approaches,
    plus the multiresolution speed gain for 3D rotation search. Note that the results
    are plotted in logarithmic scale.
  Figure 2 Link: articels_figures_by_rev_year\2017\Multiresolution_Search_of_the_Rigid_Motion_Space_for_IntensityBased_Registration\figure_2.jpg
  Figure 2 caption: The bounds obtained for the nearest neighbour interpolation for
    the registration problem given in Fig. 1. Images are decimated by a factor of
    (a) 16, (b) 8 and (c) 4. The search space reduction approach is the same as that
    of Fig. 1d. The bounds are not as effective as that of the sinc interpolation,
    but still are useful. (d,e,f) the same figures, but this time discretized integration
    is used instead of exact integration to compute the target function, and (39)
    is used for the bounds. Notice that the discretized integration has resulted in
    fluctuations in the bounds which increase as the decimation rate m gets larger.
  Figure 3 Link: articels_figures_by_rev_year\2017\Multiresolution_Search_of_the_Rigid_Motion_Space_for_IntensityBased_Registration\figure_3.jpg
  Figure 3 caption: Bounds for low-resolution to high-resolution registration with
    nearest neighbour interpolation and discretized computation of the target integral
    for the registration problem of Fig. 1. In all cases, the image f is decimated
    by a factor of m=16 , and g has been upsampled by a factor of (a) p=1 , (b) p=2
    and (c) p=4 . Changing p from 2 to 4 has not made a noticeable difference.
  Figure 4 Link: articels_figures_by_rev_year\2017\Multiresolution_Search_of_the_Rigid_Motion_Space_for_IntensityBased_Registration\figure_4.jpg
  Figure 4 caption: The frequency domain outside Omega has been partitioned into nested
    radial frequency bands Omega 1, Omega 2, ldots, Omega P ( P=5 ).
  Figure 5 Link: articels_figures_by_rev_year\2017\Multiresolution_Search_of_the_Rigid_Motion_Space_for_IntensityBased_Registration\figure_5.jpg
  Figure 5 caption: Applying the Lipschitz optimization algorithm on the starfish
    image of Fig. 1 (first row) and the brain MRI image of Fig. 6 (second row). In
    each case, we depict the elapsed time in minutes for the single resolution and
    multiresolution approaches, plus the speed gain given by the multiresolution approach.
    Different colours represent different random experiments.
  Figure 6 Link: articels_figures_by_rev_year\2017\Multiresolution_Search_of_the_Rigid_Motion_Space_for_IntensityBased_Registration\figure_6.jpg
  Figure 6 caption: Registering two different slices of a brain MRI image. (a) the
    first slice, (b) the second slice rotated and translated, and (c) the first slice
    registered to the second slice. The data is obtained from [26].
  Figure 7 Link: articels_figures_by_rev_year\2017\Multiresolution_Search_of_the_Rigid_Motion_Space_for_IntensityBased_Registration\figure_7.jpg
  Figure 7 caption: Samples of the leaf dataset [27].
  Figure 8 Link: articels_figures_by_rev_year\2017\Multiresolution_Search_of_the_Rigid_Motion_Space_for_IntensityBased_Registration\figure_8.jpg
  Figure 8 caption: Histograms of single resolution and multiresolution elapsed time
    in seconds, and the speed gain achieved by the multiresolution approach. Note
    that the histograms are computed in logarithmic scale.
  Figure 9 Link: articels_figures_by_rev_year\2017\Multiresolution_Search_of_the_Rigid_Motion_Space_for_IntensityBased_Registration\figure_9.jpg
  Figure 9 caption: 3D registration of binary vertebra shapes, (left, middle) the
    registration image pair, (right) the first image registered to the second image.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Behrooz Nasihatkon
  Name of the last author: Fredrik Kahl
  Number of Figures: 14
  Number of Tables: 1
  Number of authors: 2
  Paper title: Multiresolution Search of the Rigid Motion Space for Intensity-Based
    Registration
  Publication Date: 2017-01-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Percentage of Grid Elements Ruled Out at Each Level of
      Resolution for a Search Grid of Rotation (Column 2) and Rotation Plus Translation
      (Column 3) Parameters
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2654245
- Affiliation of the first author: data61-csiro, canberra research laboratory, canberra,
    act, australia
  Affiliation of the last author: data61-csiro, canberra research laboratory, canberra,
    act, australia
  Figure 1 Link: articels_figures_by_rev_year\2017\Dimensionality_Reduction_on_SPD_Manifolds_The_Emergence_of_GeometryAware_Methods\figure_1.jpg
  Figure 1 caption: "Dimensionality reduction on SPD manifolds: Given data on a high-dimensional\
    \ SPD manifold, where each sample represents an n\xD7n SPD matrix, we learn a\
    \ mapping to a lower-dimensional SPD manifold. We consider both the supervised\
    \ scenario, illustrated here, where the resulting m\xD7m SPD matrices are clustered\
    \ according to class labels, and the unsupervised one, where the resulting matrices\
    \ have maximum variance."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Dimensionality_Reduction_on_SPD_Manifolds_The_Emergence_of_GeometryAware_Methods\figure_2.jpg
  Figure 2 caption: "Parallel transport of a tangent vector \u0394 from a point W\
    \ to another point V on the manifold."
  Figure 3 Link: articels_figures_by_rev_year\2017\Dimensionality_Reduction_on_SPD_Manifolds_The_Emergence_of_GeometryAware_Methods\figure_3.jpg
  Figure 3 caption: Convergence behavior of the proposed eigen-decomposition-based
    solver compared to that of conjugate gradient optimization on the Grassmann manifold.
  Figure 4 Link: articels_figures_by_rev_year\2017\Dimensionality_Reduction_on_SPD_Manifolds_The_Emergence_of_GeometryAware_Methods\figure_4.jpg
  Figure 4 caption: Samples from the UIUC material dataset [56].
  Figure 5 Link: articels_figures_by_rev_year\2017\Dimensionality_Reduction_on_SPD_Manifolds_The_Emergence_of_GeometryAware_Methods\figure_5.jpg
  Figure 5 caption: Kicking action from the HDM05 motion capture sequences database
    [58].
  Figure 6 Link: articels_figures_by_rev_year\2017\Dimensionality_Reduction_on_SPD_Manifolds_The_Emergence_of_GeometryAware_Methods\figure_6.jpg
  Figure 6 caption: Samples from YouTube celebrity [59].
  Figure 7 Link: articels_figures_by_rev_year\2017\Dimensionality_Reduction_on_SPD_Manifolds_The_Emergence_of_GeometryAware_Methods\figure_7.jpg
  Figure 7 caption: Sample images from the UMD Keck body-gesture dataset [66].
  Figure 8 Link: articels_figures_by_rev_year\2017\Dimensionality_Reduction_on_SPD_Manifolds_The_Emergence_of_GeometryAware_Methods\figure_8.jpg
  Figure 8 caption: Accuracy on the UIUC material dataset for varying values of nu
    b .
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Mehrtash Harandi
  Name of the last author: Richard Hartley
  Number of Figures: 8
  Number of Tables: 6
  Number of authors: 3
  Paper title: 'Dimensionality Reduction on SPD Manifolds: The Emergence of Geometry-Aware
    Methods'
  Publication Date: 2017-01-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Definition of the Jacobian for the Stein Divergence, Jeffrey
      Divergence and AIRM, Respectively
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Overall Running-Times for Computing the Gradients According
      to Table 1 over 1,000 Trials
  Table 3 caption:
    table_text: TABLE 3 Recognition Accuracies for the UIUC Material Dataset [56]
  Table 4 caption:
    table_text: TABLE 4 Recognition Accuracies for the HDM05-MOCAP Dataset [58]
  Table 5 caption:
    table_text: TABLE 5 Recognition Accuracies for the YTC Dataset [59]
  Table 6 caption:
    table_text: TABLE 6 Recognition Accuracies and Normalized Mutual Information Scores
      (Mean and Standard Deviations) for the Keck Dataset [66]
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2655048
- Affiliation of the first author: international research institute for multidisciplinary
    science, beijing, china
  Affiliation of the last author: institute of industrial science, university of tokyo,
    tokyo, japan
  Figure 1 Link: articels_figures_by_rev_year\2017\SymPS_BRDF_Symmetry_Guided_Photometric_Stereo_for_Shape_and_Light_Source_Estimat\figure_1.jpg
  Figure 1 caption: (a) Standard BRDF parameterization; (b) in a view-centered coordinate
    system, we examine the surface normals having the same azimuth angle as the light
    source. We observe the 1D constrained half-vector symmetry in the resulting 1D
    BRDF slice; (c) in a view-centered coordinate system, we examine surface normals
    centered by the half vector. We observe BRDF iso-contours and the 2D constrained
    half-vector symmetry.
  Figure 10 Link: articels_figures_by_rev_year\2017\SymPS_BRDF_Symmetry_Guided_Photometric_Stereo_for_Shape_and_Light_Source_Estimat\figure_10.jpg
  Figure 10 caption: Results for 100 BRDFs by using the 2D-sym method under non-uniformly
    distributed light sources. Examples of the 2D BRDF symmetry (folded for easy illustration
    as in Fig. 3) beforeafter optimization are shown.
  Figure 2 Link: articels_figures_by_rev_year\2017\SymPS_BRDF_Symmetry_Guided_Photometric_Stereo_for_Shape_and_Light_Source_Estimat\figure_2.jpg
  Figure 2 caption: "(a) For each \u03B1 j , we interpolate its symmetric partner\
    \ in the 1D slice; (b) smoothness level in Eq. (4)."
  Figure 3 Link: articels_figures_by_rev_year\2017\SymPS_BRDF_Symmetry_Guided_Photometric_Stereo_for_Shape_and_Light_Source_Estimat\figure_3.jpg
  Figure 3 caption: "(a) BRDF values show 2D symmetry regarding \u03B8 h ; (b) Relation\
    \ between BRDF values and \u03B8 h values. Note that inaccurate surface normal\
    \ will bias the captured data and cause error in \u03B8 h ."
  Figure 4 Link: articels_figures_by_rev_year\2017\SymPS_BRDF_Symmetry_Guided_Photometric_Stereo_for_Shape_and_Light_Source_Estimat\figure_4.jpg
  Figure 4 caption: 'Illustration of the proposed optimization terms: (a) 2D BRDF
    symmetry; (b) Half vector''s direction as a special case; (c) Local linear structure
    preservation which involves additionally added anchor normals.'
  Figure 5 Link: articels_figures_by_rev_year\2017\SymPS_BRDF_Symmetry_Guided_Photometric_Stereo_for_Shape_and_Light_Source_Estimat\figure_5.jpg
  Figure 5 caption: 'Examples of 3D light source recovery from images. We compare
    different methods in terms of their resulting data linearity and the recovered
    light sources. (a) [39]: corr. = 0.35, err. >20circ ; (b) [41]: corr. = 0.86,
    err. = 8.50circ ; (c) our method: corr. = 0.96, err. = 4.16circ .'
  Figure 6 Link: articels_figures_by_rev_year\2017\SymPS_BRDF_Symmetry_Guided_Photometric_Stereo_for_Shape_and_Light_Source_Estimat\figure_6.jpg
  Figure 6 caption: Light sources and 3D surfaces (Hemisphere, Bunny, Dragon, Rabbit,
    Buddha, Armdillo, Beethoven and Lion) used in the experiments.
  Figure 7 Link: articels_figures_by_rev_year\2017\SymPS_BRDF_Symmetry_Guided_Photometric_Stereo_for_Shape_and_Light_Source_Estimat\figure_7.jpg
  Figure 7 caption: Examples of automatic material type classification.
  Figure 8 Link: articels_figures_by_rev_year\2017\SymPS_BRDF_Symmetry_Guided_Photometric_Stereo_for_Shape_and_Light_Source_Estimat\figure_8.jpg
  Figure 8 caption: 'Results for 100 BRDFs by using the 1D-sym method under roughly
    uniformly distributed light sources. Elevation angle re-mapping curves (blue:
    estimated; red: ground truth) and recovered symmetry data are shown.'
  Figure 9 Link: articels_figures_by_rev_year\2017\SymPS_BRDF_Symmetry_Guided_Photometric_Stereo_for_Shape_and_Light_Source_Estimat\figure_9.jpg
  Figure 9 caption: 'Average surface normal error maps of 100 BRDFS under non-uniform
    lights by using: Initialization (left), 1D-sym method (middle) and 2D-sym method
    (right).'
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Feng Lu
  Name of the last author: Yoichi Sato
  Number of Figures: 14
  Number of Tables: 5
  Number of authors: 4
  Paper title: 'SymPS: BRDF Symmetry Guided Photometric Stereo for Shape and Light
    Source Estimation'
  Publication Date: 2017-01-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average Surface NormalLight Source Errors for 100 BRDFs Under
      Roughly Uniformly Distributed Light Sources
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average Surface NormalLight Source Errors for 100 BRDFs Under
      Non-Uniformly Distributed Light Sources
  Table 3 caption:
    table_text: TABLE 3 Comparison of Average Estimation Accuracy for Different Methods
      with 100 BRDFs and 8283 Light Sources
  Table 4 caption:
    table_text: TABLE 4 Results for 3D Models with 10 BRDFs Under 83 Lights
  Table 5 caption:
    table_text: TABLE 5 Estimation Accuracy on Real World Data
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2655525
- Affiliation of the first author: berkeley biomedical data science center, lawrence
    berkeley national laboratory, berkeley, ca
  Affiliation of the last author: berkeley biomedical data science center, lawrence
    berkeley national laboratory, berkeley, ca
  Figure 1 Link: articels_figures_by_rev_year\2017\Unsupervised_Transfer_Learning_via_MultiScale_Convolutional_Sparse_Coding_for_Bi\figure_1.jpg
  Figure 1 caption: "Comparison of multi-scale filters learnt via MSCSC and CSC from\
    \ (a) GBM dataset, where each tissue image is decomposed into two spectra (channels)\
    \ corresponding to nuclei and extracellular matrix (ECM) for filter learning;\
    \ and (b) A synthetic image, consisting of four distinct binarized shapes ( \u2605\
    \ , \u25A0 , \u2219 , \u25B2 ) at two different scales ( 13\xD713 and 27\xD727\
    \ ). It is clear that, through joint learning via MSCSC, the filters at smaller\
    \ scale (i.e., 13\xD713 ) mainly captures lower-level featuressmall objects (e.g.,\
    \ edges in GBM dataset and small shapes in synthetic image), while the filters\
    \ at larger scale (i.e., 27\xD727 ) are more responsible for higher-level featureslarge\
    \ objects (e.g., complex pattern in ECM of GBM dataset and large shapes in synthetic\
    \ image). However, filters learnt separately per scale via CSC do not have such\
    \ scale-specificity, and present a mixture of low-high-level features at both\
    \ scales, which might lead to feature redundancy across scales. It is also worth\
    \ to mention that, for GBM dataset, the difference in scale-specificity becomes\
    \ more distinct for filters learnt from ECM, since compared with nuclear chromatin,\
    \ ECM sees much more complex patterns, which CSC fails to capture."
  Figure 10 Link: articels_figures_by_rev_year\2017\Unsupervised_Transfer_Learning_via_MultiScale_Convolutional_Sparse_Coding_for_Bi\figure_10.jpg
  Figure 10 caption: 'Illustration of generality and specificity of featuresknowledge
    derived by MSCSC across domains. It is worth to mention that the filter banks
    at different scales were jointly learned in an unsupervised fashion with clear
    scale-specificity: The filters at smaller scale mainly captures lower-level features,
    while the filters at larger scale are more responsible for higher-level features.
    Such an scale-specificity not only help reduce the feature redundancy across scales,
    but also serves as the basis for transfer learning.'
  Figure 2 Link: articels_figures_by_rev_year\2017\Unsupervised_Transfer_Learning_via_MultiScale_Convolutional_Sparse_Coding_for_Bi\figure_2.jpg
  Figure 2 caption: 'The proposed multi-scale multi-spectral feature extraction framework.
    CoD: Color decomposition; Abs: Absolute value rectification; LCN : Local contrast
    normalization; MP: Max-pooling.'
  Figure 3 Link: articels_figures_by_rev_year\2017\Unsupervised_Transfer_Learning_via_MultiScale_Convolutional_Sparse_Coding_for_Bi\figure_3.jpg
  Figure 3 caption: Examples from GBM and KIRC datasets. Note that the phenotypic
    signatures are highly diverse in each column.
  Figure 4 Link: articels_figures_by_rev_year\2017\Unsupervised_Transfer_Learning_via_MultiScale_Convolutional_Sparse_Coding_for_Bi\figure_4.jpg
  Figure 4 caption: Experimental revisit on color decomposition, where, by default,
    MultiScale-CSCSPM operated on decomposed spectra corresponding to the nuclear
    chromatin and the extracellular matrix respectively.
  Figure 5 Link: articels_figures_by_rev_year\2017\Unsupervised_Transfer_Learning_via_MultiScale_Convolutional_Sparse_Coding_for_Bi\figure_5.jpg
  Figure 5 caption: Experimental revisit on max-pooling, where, by default, MultiScale-CSCSPM
    utilized the max-pooling strategy.
  Figure 6 Link: articels_figures_by_rev_year\2017\Unsupervised_Transfer_Learning_via_MultiScale_Convolutional_Sparse_Coding_for_Bi\figure_6.jpg
  Figure 6 caption: Experimental revisit on absolute value rectification, where, by
    default, MultiScale-CSCSPM employed absolute value rectification.
  Figure 7 Link: articels_figures_by_rev_year\2017\Unsupervised_Transfer_Learning_via_MultiScale_Convolutional_Sparse_Coding_for_Bi\figure_7.jpg
  Figure 7 caption: Comparison with other related multi-scaledeep learning methods,
    where the best performances of each methodstrategy on GBM and KIRC datasets were
    reported with 160 and 280 training images per category, respectively.
  Figure 8 Link: articels_figures_by_rev_year\2017\Unsupervised_Transfer_Learning_via_MultiScale_Convolutional_Sparse_Coding_for_Bi\figure_8.jpg
  Figure 8 caption: 'Examples: First column: DCIS model; Second column: ERBB2+; Third
    column: Triple negative.'
  Figure 9 Link: articels_figures_by_rev_year\2017\Unsupervised_Transfer_Learning_via_MultiScale_Convolutional_Sparse_Coding_for_Bi\figure_9.jpg
  Figure 9 caption: Performance of different methods for the classification of subtypes
    in breast cancer.
  First author gender probability: 0.96
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.77
  Name of the first author: Hang Chang
  Name of the last author: Jian-Hua Mao
  Number of Figures: 13
  Number of Tables: 3
  Number of authors: 5
  Paper title: Unsupervised Transfer Learning via Multi-Scale Convolutional Sparse
    Coding for Biomedical Applications
  Publication Date: 2017-01-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Detailed Description of Experimental Setup with Different
      Methods
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance of Different Methods on the GBM Dataset
  Table 3 caption:
    table_text: TABLE 3 Performance of Different Methods on the KIRC Dataset
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2656884
- Affiliation of the first author: systems analysis laboratory, school of science,
    aalto university, aalto, finland
  Affiliation of the last author: systems analysis laboratory, school of science,
    aalto university, aalto, finland
  Figure 1 Link: articels_figures_by_rev_year\2017\Direct_Least_Square_Fitting_of_Hyperellipsoids\figure_1.jpg
  Figure 1 caption: "Effect of the axis ratio on the offset error in (a) and (c) as\
    \ well as on the shape error in (b) and (d). Noise level \u03C3 = 1 percent. Data\
    \ covers the whole ellipsoid surface. (a) and (b) 3D with 18 data points. (c)\
    \ and (d) 10D with 130 data points."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Direct_Least_Square_Fitting_of_Hyperellipsoids\figure_2.jpg
  Figure 2 caption: "Effect of the number of data points on (a) the offset error and\
    \ (b) the shape error in 10D. Axis ratio r ax = 1.3 and noise level \u03C3 = 1\
    \ percent. Data covers half of the ellipsoid surface."
  Figure 3 Link: articels_figures_by_rev_year\2017\Direct_Least_Square_Fitting_of_Hyperellipsoids\figure_3.jpg
  Figure 3 caption: Effect of the noise level on the failure rate in 3D. Axis ratio
    r ax = 3.0. Scarce data containing 12 points covers the whole ellipsoid surface.
  Figure 4 Link: articels_figures_by_rev_year\2017\Direct_Least_Square_Fitting_of_Hyperellipsoids\figure_4.jpg
  Figure 4 caption: Effect of the noise level on (a) the offset error, (b) the shape
    error and (c) the axis ratio of estimated ellipsoids in 3D. Axis ratio r ax =1.7
    . Excessive data containing 1,000 points covers the whole ellipsoid surface.
  Figure 5 Link: articels_figures_by_rev_year\2017\Direct_Least_Square_Fitting_of_Hyperellipsoids\figure_5.jpg
  Figure 5 caption: "Effect of the regularization on (a) the offset error and (b)\
    \ the shape error as a function of the failure rate is presented with solid lines.\
    \ Axis ratio r ax = 3.0 and noise level \u03C3 = 3 percent. Scarce data containing\
    \ 12 points covers the whole 3D ellipsoid surface. Large markers refer to the\
    \ regularization parameter \u03B3 = 0.0 and smaller markers to \u03B3 = 0.002,\
    \ 0.008, 0.018, 0.033, and 0.050, respectively. Effect of the parameter \u03B7\
    \ in (17) on the errors as a function of the failure rate is presented with dotted\
    \ lines. In order of decreasing failure rate, asterisks refer to \u03B7\u2192\u221E\
    \ , \u03B7 = 5.7, 3.1, 1.9, 1.3, and 1.1."
  Figure 6 Link: articels_figures_by_rev_year\2017\Direct_Least_Square_Fitting_of_Hyperellipsoids\figure_6.jpg
  Figure 6 caption: Fitting results obtained with partial data in 4D. The traces of
    the estimated ellipsoids in the 2D planes are illustrated as colored lines, and
    the traces of the original ellipsoid as dotted lines. The projections of the data
    points to the 2D planes are visualized as circles.
  Figure 7 Link: articels_figures_by_rev_year\2017\Direct_Least_Square_Fitting_of_Hyperellipsoids\figure_7.jpg
  Figure 7 caption: Fitting results obtained with extreme data in 2D. The estimated
    ellipses with the regularization parameter rmgamma = 0.001 are illustrated in
    blue and with rmgamma = 0.1 in yellow. The underdetermined problems containing
    3, 4, and 5 data points are presented on the top row. The problems containing
    6, 7, and 8 data points are presented on the bottom row.
  Figure 8 Link: articels_figures_by_rev_year\2017\Direct_Least_Square_Fitting_of_Hyperellipsoids\figure_8.jpg
  Figure 8 caption: Fitting results in 3D obtained with data originating from the
    surfaces of the segmented humerus on the left and the femur on the right. The
    estimated ellipsoids produced by HES are illustrated in blue and those produced
    by SOD in red. The corresponding data points used are marked with dots of the
    same colors.
  Figure 9 Link: articels_figures_by_rev_year\2017\Direct_Least_Square_Fitting_of_Hyperellipsoids\figure_9.jpg
  Figure 9 caption: Failure rates, i.e., percentages of non-ellipsoidal solutions
    when fitting is conducted with varying number of 3D (a) and 10D (b) normally distributed
    random data points.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.93
  Name of the first author: "Martti Kes\xE4niemi"
  Name of the last author: Kai Virtanen
  Number of Figures: 9
  Number of Tables: 0
  Number of authors: 2
  Paper title: Direct Least Square Fitting of Hyperellipsoids
  Publication Date: 2017-01-25 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2658574
- Affiliation of the first author: beijing laboratory of intelligent information technology,
    beijing institute of technology, beijing, china
  Affiliation of the last author: research school of engineering, nicta, canberra,
    act, australia
  Figure 1 Link: articels_figures_by_rev_year\2017\SaliencyAware_Video_Object_Segmentation\figure_1.jpg
  Figure 1 caption: Overview of our video object segmentation framework. Input frame
    is over-segmented into superpixels and a spatiotemporal edge map is produced by
    the combination of static edge probability and optical flow gradient magnitude.
    For each superpixel, we compute its object-probability and the refined saliency
    estimate via intra-frame graph and inter-frame graph, respectively. An object
    skeleton abstraction method is further derived for obtaining final saliency estimates
    via biasing the central skeleton regions with higher saliency values. Finally,
    spatiotemporal saliency priors, global appearance models and dynamic location
    models are combined for producing correct video segmentation.
  Figure 10 Link: articels_figures_by_rev_year\2017\SaliencyAware_Video_Object_Segmentation\figure_10.jpg
  Figure 10 caption: 'Assessment of individual steps of our saliency estimation by
    (a) precision-recall curves, and (b) MAE scores. Step1 and Step2 refer to saliency
    via intra-frame and inter-frame graphs, respectively. Step3 is the skeleton abstraction.
    Top: evaluation results on the extended SegTrack [60]. Bottom: evaluation results
    on the FBMS [1].'
  Figure 2 Link: articels_figures_by_rev_year\2017\SaliencyAware_Video_Object_Segmentation\figure_2.jpg
  Figure 2 caption: Overview of geodesic distance based spatiotemporal saliency prior.
    (a) Input frame F k . (b) Oversegmentation of F k into superpixels Y k . (c) Spatial
    edge probability map E k c of F k . (d) Gradient magnitude E k o of optical flow
    of F k . (e) Spatiotemporal edge map E k via (1). (f) Object result P k via intra-frame
    graph. (g) Saliency result S k via inter-frame graph. (h) Final video saliency
    via the proposed skeleton abstraction method.
  Figure 3 Link: articels_figures_by_rev_year\2017\SaliencyAware_Video_Object_Segmentation\figure_3.jpg
  Figure 3 caption: "Illustration of inter-frame graph construction. (a) Frame F k\
    \ . (b) Optical flow flied V k . (c) When the optical flow estimation is not accurate,\
    \ object probabilities P k are degraded. (d) F k is decomposed into background\
    \ regions B k and object-like regions U k by self-adaptive threshold \u03C3 k\
    \ in (6). The black regions indicate B k , while the bright regions indicate U\
    \ k . (e) The decomposition of previous frame F k\u22121 . (f) The object-like\
    \ regions U k\u22121 of F k\u22121 are projected onto F k . (g) Spatiotemporal\
    \ saliency result S k for F k with consideration of (d) and (e). (h) Spatiotemporal\
    \ saliency result S k for F k with consideration of (e) and (f)."
  Figure 4 Link: articels_figures_by_rev_year\2017\SaliencyAware_Video_Object_Segmentation\figure_4.jpg
  Figure 4 caption: "Illustration of skeleton abstraction process. (a) Frame F k .\
    \ (b) Saliency results S k of (a) obtained via (8). (c) F k is decomposed into\
    \ background regions B \u2032k (black area) and object-like regions U \u2032k\
    \ (bright area) via (9). (d) The red region corresponds to the first selected\
    \ skeleton region by (10). (e) The yellow regions correspond to the subsequently\
    \ selected skeleton regions by (11). (f) We iteratively add skeleton regions until\
    \ the number of selected skeleton regions reaches 10 percent of object-like regions\
    \ U \u2032k . (g) The blue regions are the other skeleton regions that lie on\
    \ the shortest geodesic path between the base and the selected skeleton regions.\
    \ (h) The enhanced saliency values of the skeleton regions."
  Figure 5 Link: articels_figures_by_rev_year\2017\SaliencyAware_Video_Object_Segmentation\figure_5.jpg
  Figure 5 caption: Illustration of video segmentation. (a) Input frame Fk . (b) Video
    saliency map Sk . (c) The regions within the red boundaries have higher saliency
    values than adaptive threshold, which are used for establishing foreground histogram
    model. The regions between the green boundaries and red boundaries are for building
    background histogram model. (d) Global appearance models lbrace Hf,Hbrbrace estimated
    from (c). (e) Foreground probability computed via appearance model in (d). (f)
    Accumulated optical flow gradient magnitude widehatEk yields trajectory of the
    object within few subsequent frames. (g) Dynamic location prior Lk obtained via
    intra-frame graph described in Section 3.2. (h) Final segmentation results by
    (13), which consists of the saliency term (b), the appearance term (e), and the
    location term (g), and two pairwise terms.
  Figure 6 Link: articels_figures_by_rev_year\2017\SaliencyAware_Video_Object_Segmentation\figure_6.jpg
  Figure 6 caption: 'Comparison of saliency detection methods using SegTrack [59]
    (top ), extended SegTrack [60] (middle) and FBMS [1] (bottom) with pixel-level
    ground-truth: (a) average precision recall curve by segmenting saliency maps using
    fixed thresholds, (b) F-score, (c) average MAE.'
  Figure 7 Link: articels_figures_by_rev_year\2017\SaliencyAware_Video_Object_Segmentation\figure_7.jpg
  Figure 7 caption: Qualitative comparison against the state-of-the-art methods on
    the SegTrack benchmark [59], the extended SegTrack [60] and the famous FBMS dataset
    [1] with pixel-level ground-truth labels. Our saliency method yields continuous
    saliency maps that are most similar to the ground-truth.
  Figure 8 Link: articels_figures_by_rev_year\2017\SaliencyAware_Video_Object_Segmentation\figure_8.jpg
  Figure 8 caption: Our segmentation results on extended SegTrack dataset [60] (Monkey
    ), and FBMS [1] (Horse) with pixel-level ground-truth masks. The pixels within
    the green boundaries are segmented as foreground.
  Figure 9 Link: articels_figures_by_rev_year\2017\SaliencyAware_Video_Object_Segmentation\figure_9.jpg
  Figure 9 caption: Computational load of our method and the state-of-the-art for
    320 times 240 video. (a) Execution time of video saliency estimation stage compared
    against other video saliency methods [15], [38], [48], [49]. (b) Execution time
    of overall method compared against other video segmentation methods [7], [9],
    [22]. (c) Execution time of each intermediate steps. Step1 and Step2 are saliency
    estimations via intra-frame graph and inter-frame graph, respectively. Step3 is
    the final saliency step.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Wenguan Wang
  Name of the last author: Fatih Porikli
  Number of Figures: 10
  Number of Tables: 3
  Number of authors: 4
  Paper title: Saliency-Aware Video Object Segmentation
  Publication Date: 2017-01-31 00:00:00
  Table 1 caption:
    table_text: TABLE 1 APFPER& Results on SegTrack Dataset [59] Compared to the Ground-Truth
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 IoU Scores on SegTrack Dataset [59] and Extended SegTrack
      Dataset [60] Compared to the Ground-Truth
  Table 3 caption:
    table_text: TABLE 3 IoU Scores on a Representative Subset of the FBMS Dataset
      [1], and the Average Computed over the 59 Video Sequences
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2662005
