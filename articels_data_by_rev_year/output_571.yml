- Affiliation of the first author: school of computer science and software engineering,
    the university of western australia, 35 stirling highway, crawley, western australia,
    australia
  Affiliation of the last author: school of computer science and software engineering,
    the university of western australia, 35 stirling highway, crawley, western australia,
    australia
  Figure 1 Link: articels_figures_by_rev_year\2016\Histogram_of_Oriented_Principal_Components_for_CrossView_Action_Recognition\figure_1.jpg
  Figure 1 caption: 3D pointcloud sequences of a subject performing the holding head
    action. Notice how the depth values (color) change significantly with viewpoint.
  Figure 10 Link: articels_figures_by_rev_year\2016\Histogram_of_Oriented_Principal_Components_for_CrossView_Action_Recognition\figure_10.jpg
  Figure 10 caption: STKs (shown in red) extracted using our proposed detector. STKs
    are projected on dimensions of all points within a 3D pointcloud sequence corresponding
    to the action two hand waving. The top nk=100, 400, 700, 1,000 with the best quality
    are shown in (a)-(d), respectively. Note that the highest quality STKs are detected
    where significant movement is performed. Noisy points begin to appear as late
    as nk=1,000 .
  Figure 2 Link: articels_figures_by_rev_year\2016\Histogram_of_Oriented_Principal_Components_for_CrossView_Action_Recognition\figure_2.jpg
  Figure 2 caption: After orientation normalization, the HOPC descriptors are similar
    for the two views. However, the HON and HOG descriptors are still different.
  Figure 3 Link: articels_figures_by_rev_year\2016\Histogram_of_Oriented_Principal_Components_for_CrossView_Action_Recognition\figure_3.jpg
  Figure 3 caption: STK detection. (a) A 3D pointcloud sequence corresponding to the
    holding head action, (b) the spatial support volume of a particular point p ,
    (c) the spatio-temporal support volume of p , (d) the HOPC descriptors, (e) STK
    detection.
  Figure 4 Link: articels_figures_by_rev_year\2016\Histogram_of_Oriented_Principal_Components_for_CrossView_Action_Recognition\figure_4.jpg
  Figure 4 caption: STKs (shown in red) projected onto XYZ dimensions of all points
    of a 3D pointcloud sequence corresponding to the two hand waving action. Four
    different views are shown. Note that the distribution of STKs encodes the action
    globally as they are detected only where movement is performed.
  Figure 5 Link: articels_figures_by_rev_year\2016\Histogram_of_Oriented_Principal_Components_for_CrossView_Action_Recognition\figure_5.jpg
  Figure 5 caption: "The same action (hand waving) is shown at three different speeds:\
    \ (a) slow, (b) moderate, and (c) fast. The number of frames reduces as the action\
    \ speed increases. For the slow movement, the optimal temporal scale is found\
    \ to be \u03C4 \u2217 =3 , for moderate movement \u03C4 \u2217 =2 , and for fast\
    \ movement \u03C4 \u2217 =1 ."
  Figure 6 Link: articels_figures_by_rev_year\2016\Histogram_of_Oriented_Principal_Components_for_CrossView_Action_Recognition\figure_6.jpg
  Figure 6 caption: Sample pointclouds from the Northwestern-UCLA Multiview Action3D
    dataset [29] captured by three cameras.
  Figure 7 Link: articels_figures_by_rev_year\2016\Histogram_of_Oriented_Principal_Components_for_CrossView_Action_Recognition\figure_7.jpg
  Figure 7 caption: Confusion matrix of our algorithm on the Northwestern-UCLA Multiview
    Action3D dataset [29].
  Figure 8 Link: articels_figures_by_rev_year\2016\Histogram_of_Oriented_Principal_Components_for_CrossView_Action_Recognition\figure_8.jpg
  Figure 8 caption: Sample pointclouds from the UWA3D Multiview Activity II dataset
    captured by one camera from four different views.
  Figure 9 Link: articels_figures_by_rev_year\2016\Histogram_of_Oriented_Principal_Components_for_CrossView_Action_Recognition\figure_9.jpg
  Figure 9 caption: Confusion matrix of our algorithm on the UWA3D Multiview Activity
    II dataset when view V 1 and view V 2 are used for training and view V 3 is used
    for test.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Hossein Rahmani
  Name of the last author: Ajmal Mian
  Number of Figures: 11
  Number of Tables: 7
  Number of authors: 4
  Paper title: Histogram of Oriented Principal Components for Cross-View Action Recognition
  Publication Date: 2016-02-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The 600 Vertices of a 120-Cell Regular Polychoron Centered
      at the Origin Generated from All and Even Permutations of These Coordinates
      [57]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Parameters and Their Values: K: Number of Codewords, n k\
      \ : Number of STKs, \u03B8 STK , \u03B8 l , \u03B8 g : Eigenratio Thresholds,\
      \ n x \xD7 n y \xD7 n t : Spatio-Temporal Cells (Section 5), \u03C4 m : Maximum\
      \ Temporal Scale, m k : Iterative Refinement (Section 6.3)"
  Table 3 caption:
    table_text: TABLE 3 Comparison of Action Recognition Accuracy (Percent) on the
      Northwestern-UCLA Multiview Action3D Dataset Where the Samples from the First
      Two Cameras Are Used as Training Data, and the Samples from the Third Camera
      Are Used as Test Data
  Table 4 caption:
    table_text: TABLE 4 Comparison of Action Recognition Accuracy (Percent) on the
      UWA3D Multiview Activity II Dataset
  Table 5 caption:
    table_text: TABLE 5 Comparison of Average Action Recognition Accuracy (Percent)
      on the MSR Action3D [38], MSR Daily Activity3D [39], and MSR Gesture3D [40]
      Datasets
  Table 6 caption:
    table_text: TABLE 6 Average Recognition Accuracy of the Proposed Method in Three
      Different Settings on the Northwestern-UCLA Multiview Action3D[29] and the UWA3D
      Multiview Activity II Datasets
  Table 7 caption:
    table_text: TABLE 7 Average Recognition Accuracy of the Proposed Method in Two
      Different Settings on the Northwestern-UCLA Multiview Action3D [29] and the
      UWA3D Multiview Activity II Datasets
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2533389
- Affiliation of the first author: centre for speech technology research at the university
    of edinburgh, united kingdom
  Affiliation of the last author: centre for speech technology research at the university
    of edinburgh, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2016\Minimum_Entropy_Rate_Simplification_of_Stochastic_Processes\figure_1.jpg
  Figure 1 caption: Power spectra of original, disturbed, and minimum entropy rate
    simplified Gaussian processes.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Minimum_Entropy_Rate_Simplification_of_Stochastic_Processes\figure_2.jpg
  Figure 2 caption: Heat map of cloud-coverage transition probabilities a ~ ij .
  Figure 3 Link: articels_figures_by_rev_year\2016\Minimum_Entropy_Rate_Simplification_of_Stochastic_Processes\figure_3.jpg
  Figure 3 caption: Entropy-divergence trade-offs for the cloud-coverage model.
  Figure 4 Link: articels_figures_by_rev_year\2016\Minimum_Entropy_Rate_Simplification_of_Stochastic_Processes\figure_4.jpg
  Figure 4 caption: Entropy-divergence trade-offs for the speech grammar.
  Figure 5 Link: articels_figures_by_rev_year\2016\Minimum_Entropy_Rate_Simplification_of_Stochastic_Processes\figure_5.jpg
  Figure 5 caption: Denoising performance for the speech grammar. A circle marks the
    optimum.
  Figure 6 Link: articels_figures_by_rev_year\2016\Minimum_Entropy_Rate_Simplification_of_Stochastic_Processes\figure_6.jpg
  Figure 6 caption: Simplified transition matrix elements at different rates, sorted
    by magnitude.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Gustav Eje Henter
  Name of the last author: Gustav Eje Henter
  Number of Figures: 6
  Number of Tables: 2
  Number of authors: 1
  Paper title: Minimum Entropy Rate Simplification of Stochastic Processes
  Publication Date: 2016-02-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Random Text Sampled from Simplified Models over a Range of
      Entropy Rates
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Translated Excerpts from the Clean and Disturbed Data, Along\
      \ with Translations of Random Samples from the Fitted Models X \u22C6 and X\
      \ \u02DC , the Optimally Denoised Models from MERS and Thresholding, and from\
      \ Low Entropy-Rate MERS ( 100 Symbols Each)"
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2533382
- Affiliation of the first author: school of computer science and engineering, nanjing
    university of science and technology, nanjing, china
  Affiliation of the last author: shenzhen graduate school, harbin institute of technology,
    shenzhen, china
  Figure 1 Link: articels_figures_by_rev_year\2016\Nuclear_Norm_Based_Matrix_Regression_with_Applications_to_Face_Recognition_with_\figure_1.jpg
  Figure 1 caption: Comparison of convergence rate of the NNR (Algorithm 1) and its
    fast version (Algorithm 2).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Nuclear_Norm_Based_Matrix_Regression_with_Applications_to_Face_Recognition_with_\figure_2.jpg
  Figure 2 caption: Example images with occlusion and illumination changes and the
    corresponding distributions of the pixel-level errors and singular values of the
    error image.
  Figure 3 Link: articels_figures_by_rev_year\2016\Nuclear_Norm_Based_Matrix_Regression_with_Applications_to_Face_Recognition_with_\figure_3.jpg
  Figure 3 caption: (a) Recognition rates (percent) of LRC, CRC, SRC, SSRC, RLRC,
    CESR, RSC, SSEC, HQA, HQM, NMR and Fast NMR under different levels of occlusion;
    (b) DET curves of all methods when the occlusion level is 50 percent.
  Figure 4 Link: articels_figures_by_rev_year\2016\Nuclear_Norm_Based_Matrix_Regression_with_Applications_to_Face_Recognition_with_\figure_4.jpg
  Figure 4 caption: Sample images of one person with occlusions of different kinds
    of objects.
  Figure 5 Link: articels_figures_by_rev_year\2016\Nuclear_Norm_Based_Matrix_Regression_with_Applications_to_Face_Recognition_with_\figure_5.jpg
  Figure 5 caption: Recognition rates (percent) of LRC, CRC, SRC, SSRC, RLRC, CESR,
    RSC, RSC, SSEC, HQA, HQM, NMR and fast NMR under different, contiguous occlusions.
  Figure 6 Link: articels_figures_by_rev_year\2016\Nuclear_Norm_Based_Matrix_Regression_with_Applications_to_Face_Recognition_with_\figure_6.jpg
  Figure 6 caption: Recognition rates (percent) of LRC, CRC, SRC, SSRC, RLRC, CESR,
    RSC, SSEC, HQA, HQM, NMR and Fast NMR under the different occlusion levels. (a)
    the case that test images are with the occlusion of black block; (b) the case
    that test images are with the occlusion of random block.
  Figure 7 Link: articels_figures_by_rev_year\2016\Nuclear_Norm_Based_Matrix_Regression_with_Applications_to_Face_Recognition_with_\figure_7.jpg
  Figure 7 caption: Recognition rates (percent) of each classifier under different
    illumination conditions on the extended yale B database. (a) on the subset 4,
    (b) on the subset 5.
  Figure 8 Link: articels_figures_by_rev_year\2016\Nuclear_Norm_Based_Matrix_Regression_with_Applications_to_Face_Recognition_with_\figure_8.jpg
  Figure 8 caption: Illustration of the average running time (base-10 log of seconds)
    of recognizing one testing sample for each method on the extended yale B database.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.9
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Jian Yang
  Name of the last author: Yong Xu
  Number of Figures: 8
  Number of Tables: 6
  Number of authors: 6
  Paper title: Nuclear Norm Based Matrix Regression with Applications to Face Recognition
    with Occlusion and Illumination Changes
  Publication Date: 2016-02-26 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Equal Error Rates of LRC, CRC, SRC, SSRC, RLRC, CESR, RSC,
      SSEC, HQA, HQM, NMR and Fast NMR under Different Occlusion Cases on the Extended
      Yale B Database
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Recognition Rates (percent) of LRC, CRC, SRC, SSRC, RLRC,
      CESR, RSC, SSEC, HQA, HQM, NMR and Fast NMR on the AR Database
  Table 3 caption:
    table_text: TABLE 3 Recognition Rates (RRs) and Equal Error Rates of LRC, CRC,
      SRC, SSRC, RLRC, CESR, RSC, SSEC, HQA, HQM, NMR and Fast NMR on the EURECOM
      Kinect Database
  Table 4 caption:
    table_text: TABLE 4 Recognition Rates (percent) of LRC, CRC, SRC, RLRC, CESR,
      RSC, SSEC, HQA, HQM and NMR on the Multi-PIE Database under Different Illuminations
  Table 5 caption:
    table_text: TABLE 5 Recognition Rates (percent) of LRC, CRC, SRC, SSRC, RLRC,
      CESR, RSC, SSEC, HQA, HQM, NMR and Fast NMR on the FRGC Database
  Table 6 caption:
    table_text: TABLE 6 The Recognition Rates (percent) of Different Methods Versus
      Different Occlusion Levels and Illumination Variations on the Multi-PIE Database
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2535218
- Affiliation of the first author: department of computer engineering, bilkent university,
    ankara, turkey
  Affiliation of the last author: "lear team, inria grenoble rh\xF4ne-alpes, laboratoire\
    \ jean kuntzmann, cnrs, university grenoble alpes, france"
  Figure 1 Link: articels_figures_by_rev_year\2016\Weakly_Supervised_Object_Localization_with_MultiFold_Multiple_Instance_Learning\figure_1.jpg
  Figure 1 caption: Distribution of the window scores in the positive training images
    after the fifth iteration of standard MIL training on VOC 2007 for FVs (left)
    and CNNs (right). For each figure, the right-most curve corresponds to the windows
    chosen in the most recent re-localization step and used for training the detector.
    The curve in the middle corresponds to the other windows that overlap more than
    50 percent with the training windows. Similarly, the left-most curve corresponds
    to the windows that overlap less than 50 percent. Each curve is obtained by averaging
    all per-class score distributions. The surrounding regions show the standard deviation.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Weakly_Supervised_Object_Localization_with_MultiFold_Multiple_Instance_Learning\figure_2.jpg
  Figure 2 caption: "Distribution of inner products, scaled to the interval [ \u2212\
    \ 1 +1], of pairs of 25,000 windows sampled from 250 images using our high-dimensional\
    \ FV (top), a low-dimensional FV (middle), and CNN features (bottom). (a) uses\
    \ all window pairs and (b) uses only within-image pairs, which are more likely\
    \ to be similar."
  Figure 3 Link: articels_figures_by_rev_year\2016\Weakly_Supervised_Object_Localization_with_MultiFold_Multiple_Instance_Learning\figure_3.jpg
  Figure 3 caption: Illustration of our window refinement. Dashed boxes (pink) show
    the localization before refinement, and the solid boxes (yellow) show the result
    of the window refinement method. The images on the right show the edge maps that
    are used to compute the objectness measure.
  Figure 4 Link: articels_figures_by_rev_year\2016\Weakly_Supervised_Object_Localization_with_MultiFold_Multiple_Instance_Learning\figure_4.jpg
  Figure 4 caption: Re-localization using standard and multi-fold MIL for images of
    the classes bicycle, motorbike, and cat from initialization (left) to the final
    localization (right) and three intermediate iterations based on FV (F+C) descriptors.
    Correct localizations are shown in yellow, incorrect ones in pink.
  Figure 5 Link: articels_figures_by_rev_year\2016\Weakly_Supervised_Object_Localization_with_MultiFold_Multiple_Instance_Learning\figure_5.jpg
  Figure 5 caption: Correct localization (CorLoc) performance (in the training set)
    over the MIL iterations, averaged across VOC 2007 classes. We show results for
    the high and low dimensional FVs (left panel), and the CNN features (middle panel).
    In the right panel, we compare 10-fold training with standard MIL training using
    different values of the SVM cost parameter (C) for the high-dimensional FVs.
  Figure 6 Link: articels_figures_by_rev_year\2016\Weakly_Supervised_Object_Localization_with_MultiFold_Multiple_Instance_Learning\figure_6.jpg
  Figure 6 caption: Example localization results on the training images for standard
    MIL and multi-fold MIL algorithms with high-dimensional FV and CNN features. Correct
    localizations are shown in yellow, incorrect ones in pink.
  Figure 7 Link: articels_figures_by_rev_year\2016\Weakly_Supervised_Object_Localization_with_MultiFold_Multiple_Instance_Learning\figure_7.jpg
  Figure 7 caption: AP versus CorLoc for multi-fold MIL (left), and ratio of WSL over
    supervised AP as a function of CorLoc (right) using the FV (blue circles), CNN
    (red squares) and FV+CNN (black triangles) representations. CorLoc and AP are
    measured on the training and test images, respectively. The left plot shows the
    line with least squares error for the data points.
  Figure 8 Link: articels_figures_by_rev_year\2016\Weakly_Supervised_Object_Localization_with_MultiFold_Multiple_Instance_Learning\figure_8.jpg
  Figure 8 caption: Per-class frequency of error modes, and averaged across all classes
    using FV+CNN features with 10-fold MIL and standard MIL training.
  Figure 9 Link: articels_figures_by_rev_year\2016\Weakly_Supervised_Object_Localization_with_MultiFold_Multiple_Instance_Learning\figure_9.jpg
  Figure 9 caption: Object detection results for training with mixed supervision.
    Each curve shows the test set detection AP as a function of the percentage of
    fully supervised positive training images. The horizontal axes are in logarithmic
    scale. The first two plots show per-class curves for selected classes using only
    FVs. The last three plots show the detection AP values averaged over all classes
    for the FV, CNN and FV+CNN features, respectively. The solid curves correspond
    to mixed supervision. The dotted curves correspond to results obtained by using
    only the fully-supervised examples.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Ramazan Gokberk Cinbis
  Name of the last author: Cordelia Schmid
  Number of Figures: 9
  Number of Tables: 8
  Number of authors: 3
  Paper title: Weakly Supervised Object Localization with Multi-Fold Multiple Instance
    Learning
  Publication Date: 2016-02-26 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Weakly Supervised Learning Using FV and CNN Features, Measured
      in Terms of Correct Localization (CorLoc) Measure on VOC 2007 Training Set
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Weakly Supervised Learning Using FV and CNN Features, Measured
      in Terms of Average Precision (AP) Measure on VOC 2007 Test Set
  Table 3 caption:
    table_text: TABLE 3 Evaluation of Window Refinement on the VOC 2007 Dataset, in
      Terms of Training Set Localization Accuracy (CorLoc)
  Table 4 caption:
    table_text: TABLE 4 Evaluation of Window Refinement on the VOC 2007 Dataset, in
      Terms of Test-Set Average Precision (AP)
  Table 5 caption:
    table_text: TABLE 5 Comparison of WSL Detectors on PASCAL VOC 2007 in Terms of
      Test-Set Detection AP
  Table 6 caption:
    table_text: TABLE 6 Performance in Test-Set Detection mAP on VOC 2007 Using FV,
      CNN and FV+CNN Features, with Varying Degrees of Supervision
  Table 7 caption:
    table_text: TABLE 7 Comparison of Standard MIL Training versus Our 10-fold MIL
      on VOC 2010 in Terms of Training Set Localization Accuracy (CorLoc)
  Table 8 caption:
    table_text: TABLE 8 Comparison of Standard MIL Training versus Our 10-fold MIL
      on VOC 2010 in Terms of Test set AP Measure
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2535231
- Affiliation of the first author: school of computer science and engineering, chung-ang
    univercity, seoul, korea
  Affiliation of the last author: department of electrical and computer engineering,
    automation and systems research institute, seoul national university, 599 gwanak-ro,
    gwanak-gu, seoul, korea
  Figure 1 Link: articels_figures_by_rev_year\2016\Adaptive_Visual_Tracking_with_Minimum_Uncertainty_Gap_Estimation\figure_1.jpg
  Figure 1 caption: Why should we consider the likelihood bounds? Experimentally,
    we found that the likelihood bounds are inevitably formed on each state over time,
    such as in the skaing1 sequence (above). The figure at the bottom shows the maximum
    gap of the likelihood bounds in each sequence and suggests that these gaps should
    not be ignored during the tracking process. Nevertheless conventional tracking
    methods overlook likelihood bounds during visual tracking, thus making tracking
    the target susceptible to failure. We address this problem in this work.
  Figure 10 Link: articels_figures_by_rev_year\2016\Adaptive_Visual_Tracking_with_Minimum_Uncertainty_Gap_Estimation\figure_10.jpg
  Figure 10 caption: Tracking Result 3. Yellow and sky blue rectangles represent tracking
    results of MUG and MUG+, respectively.
  Figure 2 Link: articels_figures_by_rev_year\2016\Adaptive_Visual_Tracking_with_Minimum_Uncertainty_Gap_Estimation\figure_2.jpg
  Figure 2 caption: Why are likelihood bounds formed? The target model can be updated
    in many different ways during the tracking process, and different updating strategies
    induce different target models. With several target models, we obtain diverse
    likelihood scores even at one state, which facilitates the formation of the likelihood
    bounds at the state.
  Figure 3 Link: articels_figures_by_rev_year\2016\Adaptive_Visual_Tracking_with_Minimum_Uncertainty_Gap_Estimation\figure_3.jpg
  Figure 3 caption: Why should we minimize the likelihood bounds? A large gap between
    the upper and lower bounds indicates that the corresponding state provides very
    different answers (likelihoods) depending on the target models used (red and blue).
    Thus, the likelihood estimation over such state is uncertain and unreliable. As
    a result, our method identifies the state that has the minimum gap (uncertainty),
    which provides consistent answers (likelihoods), regardless of the target models.
    Moreover, by maximizing the average likelihood bound, the proposed method can
    determine the state that confidently maximizes the likelihood. The figure at the
    bottom shows the likelihood score at a state of a frame in the skaing1 sequence.
    At the true positive state, the likelihood score slightly changes when a different
    target model is employed. However, the likelihood score at the false positive
    state changes severely.
  Figure 4 Link: articels_figures_by_rev_year\2016\Adaptive_Visual_Tracking_with_Minimum_Uncertainty_Gap_Estimation\figure_4.jpg
  Figure 4 caption: Efficient inference using the IMCMC sampling method. (a) Conventional
    inference method uses a single chain to find the best state from the combined
    target distribution. Because this distribution is very rough, inferring the best
    state is difficult. (b) Our inference method efficiently finds the best state
    by decomposing the combined target distribution to two simple ones and by using
    two chains for each distribution.
  Figure 5 Link: articels_figures_by_rev_year\2016\Adaptive_Visual_Tracking_with_Minimum_Uncertainty_Gap_Estimation\figure_5.jpg
  Figure 5 caption: Tracking environments when the gaps between the likelihood bounds
    are maximized in the soccer sequence.
  Figure 6 Link: articels_figures_by_rev_year\2016\Adaptive_Visual_Tracking_with_Minimum_Uncertainty_Gap_Estimation\figure_6.jpg
  Figure 6 caption: States of the target, which produce the maximum lower bound (blue
    rectangle) and the minimum upper bound (red rectangle) of the likelihood at a
    frame.
  Figure 7 Link: articels_figures_by_rev_year\2016\Adaptive_Visual_Tracking_with_Minimum_Uncertainty_Gap_Estimation\figure_7.jpg
  Figure 7 caption: "Weighting parameter \u03B1 for the model update presented in\
    \ (27). The red line illustrates \u03B1 over time with the normal model update\
    \ approach (Normal). The blue and sky blue curves illustrate \u03B1 overtime with\
    \ adaptive model update approaches (Adaptive 1 and 2). To update the target model,\
    \ Normal uses a current observation with the fixed weight 0.5 Adaptive 1 in (28)\
    \ uses a current observation with the adaptive weight according to its likelihood\
    \ score. Adaptive 2 in (29) uses a current observation with the adaptive weight\
    \ according to its likelihood score and the confidence of the likelihood estimation."
  Figure 8 Link: articels_figures_by_rev_year\2016\Adaptive_Visual_Tracking_with_Minimum_Uncertainty_Gap_Estimation\figure_8.jpg
  Figure 8 caption: Tracking Result 1. Yellow, blue, white, purple, green and red
    rectangles represent tracking results of MUG, MTT, VTS, MIL, FRAGT, and MC, respectively.
    Yellow and red curves represent the lower and upper bounds of the likelihood over
    time in MUG, respectively. Green curve represents gap between the bounds over
    time in MUG.
  Figure 9 Link: articels_figures_by_rev_year\2016\Adaptive_Visual_Tracking_with_Minimum_Uncertainty_Gap_Estimation\figure_9.jpg
  Figure 9 caption: Tracking Result 2. Yellow, white, purple, green, and orange rectangles
    represent tracking results of MUG, VTS, MIL, STRUCK, and ASLA, respectively. Yellow
    and red curves represent lower and upper bounds of the likelihood over time in
    MUG, respectively. Green curve represents gap between the bounds over time in
    MUG.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.83
  Name of the first author: Junseok Kwon
  Name of the last author: Kyoung Mu Lee
  Number of Figures: 11
  Number of Tables: 5
  Number of authors: 2
  Paper title: Adaptive Visual Tracking with Minimum Uncertainty Gap Estimation
  Publication Date: 2016-03-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Tracking Methods Using MAP, ML, and MUG
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Tracking Methods Using MCMC and IMCMC
  Table 3 caption:
    table_text: TABLE 3 Comparison of Tracking Methods Using Normal Model Update (Fixed)
      and Adaptive Model Update (Adaptive 1 and 2)
  Table 4 caption:
    table_text: TABLE 4 Analysis of Our Tracker in Summary
  Table 5 caption:
    table_text: TABLE 5 Comparison of Tracking Results
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2537330
- Affiliation of the first author: "department of signal theory and communications,\
    \ universitat polit\xE8cnica de catalunya, barcelonatech, barcelona, spain"
  Affiliation of the last author: department of electrical engineering and computer
    science, university of california at berkeley, berkeley, ca
  Figure 1 Link: articels_figures_by_rev_year\2016\Multiscale_Combinatorial_Grouping_for_Image_Segmentation_and_Object_Proposal_Gen\figure_1.jpg
  Figure 1 caption: 'Top: original image, instance-level ground truth from COCO and
    our multiscale hierarchical segmentation. Bottom: our best object proposals among
    150.'
  Figure 10 Link: articels_figures_by_rev_year\2016\Multiscale_Combinatorial_Grouping_for_Image_Segmentation_and_Object_Proposal_Gen\figure_10.jpg
  Figure 10 caption: Region distribution learnt by the Pareto front optimization on
    SegVOC12.
  Figure 2 Link: articels_figures_by_rev_year\2016\Multiscale_Combinatorial_Grouping_for_Image_Segmentation_and_Object_Proposal_Gen\figure_2.jpg
  Figure 2 caption: Multiscale Combinatorial Grouping. Starting from a multiresolution
    image pyramid, we perform hierarchical segmentation at each scale independently.
    We align these multiple hierarchies and combine them into a single multiscale
    segmentation hierarchy. Our grouping component then produces a ranked list of
    object proposals by efficiently exploring the combinatorial space of these regions.
  Figure 3 Link: articels_figures_by_rev_year\2016\Multiscale_Combinatorial_Grouping_for_Image_Segmentation_and_Object_Proposal_Gen\figure_3.jpg
  Figure 3 caption: 'Duality between a UCM and a region tree: Schematic view of the
    dual representation of a segmentation hierarchy as a region dendrogram and as
    an ultrametric contour map.'
  Figure 4 Link: articels_figures_by_rev_year\2016\Multiscale_Combinatorial_Grouping_for_Image_Segmentation_and_Object_Proposal_Gen\figure_4.jpg
  Figure 4 caption: "Example of segmentation projection. In order to \u201Csnap\u201D\
    \ the boundaries of a segmentation R (left) to those of a segmentation S (middle),\
    \ since they do not align, we compute \u03C0(R,S) (right) by assigning to each\
    \ segment in S its mode among the labels of R ."
  Figure 5 Link: articels_figures_by_rev_year\2016\Multiscale_Combinatorial_Grouping_for_Image_Segmentation_and_Object_Proposal_Gen\figure_5.jpg
  Figure 5 caption: BSDS500 test set. Precision-Recall curves for boundaries [35]
    (left) and for objects and parts [36] (right). The marker on each curve is placed
    on the Optimal Dataset Scale (ODS), and its F measure is presented in brackets
    in the legend. The isolated red asterisks refer to the human performance assessed
    on the same image (one human partition against the rest of human annotations on
    the same image) and on a different image (one human partition against the human
    partitions of a different, randomly selected, image).
  Figure 6 Link: articels_figures_by_rev_year\2016\Multiscale_Combinatorial_Grouping_for_Image_Segmentation_and_Object_Proposal_Gen\figure_6.jpg
  Figure 6 caption: 'Object segmentation as combinatorial optimization: Examples of
    objects (b), (c), formed by selecting regions from a hierarchy (a).'
  Figure 7 Link: articels_figures_by_rev_year\2016\Multiscale_Combinatorial_Grouping_for_Image_Segmentation_and_Object_Proposal_Gen\figure_7.jpg
  Figure 7 caption: 'Pareto front learning: Training the combinatorial generation
    of proposals using the Pareto front.'
  Figure 8 Link: articels_figures_by_rev_year\2016\Multiscale_Combinatorial_Grouping_for_Image_Segmentation_and_Object_Proposal_Gen\figure_8.jpg
  Figure 8 caption: Pareto front evaluation. Achievable quality of our proposals for
    singletons, pairs, triplets, and 4-tuples; and the raw proposals from the hierarchies
    on PASCAL SegVOC12 training (left) and validation (right) sets.
  Figure 9 Link: articels_figures_by_rev_year\2016\Multiscale_Combinatorial_Grouping_for_Image_Segmentation_and_Object_Proposal_Gen\figure_9.jpg
  Figure 9 caption: 'Object Proposals: Jaccard index at instance level. Results on
    SegVOC12, SBD, and COCO.'
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Jordi Pont-Tuset
  Name of the last author: Jitendra Malik
  Number of Figures: 13
  Number of Tables: 4
  Number of authors: 5
  Paper title: Multiscale Combinatorial Grouping for Image Segmentation and Object
    Proposal Generation
  Publication Date: 2016-03-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 BSDS500 Val Set
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Sizes of the Databases
  Table 3 caption:
    table_text: TABLE 3 Time in Seconds per Image of MCG and SCG
  Table 4 caption:
    table_text: TABLE 4 Time Comparison for All Considered State-of-the-Art Techniques
      That Produce Segmented Proposals
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2537320
- Affiliation of the first author: max planck institute for informatics, saarbruecken,
    germany
  Affiliation of the last author: inria grenoble, lear, montbonnot, france
  Figure 1 Link: articels_figures_by_rev_year\2016\Expanded_Parts_Model_for_Semantic_Description_of_Humans_in_Still_Images\figure_1.jpg
  Figure 1 caption: Illustration of the proposed method. During training (left) discriminative
    templates are learnt from a large pool of randomly sampled part candidates. During
    testing (right), the most relevant parts are used to score the test image.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Expanded_Parts_Model_for_Semantic_Description_of_Humans_in_Still_Images\figure_2.jpg
  Figure 2 caption: Illustration of a two-component model versus the proposed Expanded
    Parts Model. In a component-based model (left) each training image contributes
    to the training of a single model and, thus, its parts only score similar images.
    In contrast, the proposed EPM automatically mines discriminative parts from all
    images and uses all parts during testing. Also, while for component-based models,
    only images with typical training variations can be scored reliably, in the proposed
    EPM sub-articulations can be combined and score untypical variations not seen
    during training.
  Figure 3 Link: articels_figures_by_rev_year\2016\Expanded_Parts_Model_for_Semantic_Description_of_Humans_in_Still_Images\figure_3.jpg
  Figure 3 caption: Illustrations of scoring for different images, for different attributes
    and actions. Note how the model s cores only the discriminative regions in the
    image while ignoring the non-discriminative or background regions (in black).
    Such spatial sparsity is particularly interesting when the discriminative information
    is expected to be localized in space like in the case of many human attributes
    and actions.
  Figure 4 Link: articels_figures_by_rev_year\2016\Expanded_Parts_Model_for_Semantic_Description_of_Humans_in_Still_Images\figure_4.jpg
  Figure 4 caption: Example patches illustrating pruning for the 'riding a bike' class.
    While discriminative patches (top) at multiple atomicities are retained by the
    system, redundant or non-discriminative patches (middle) and random background
    (bottom) patches are discarded. The patches have been resized and contrast adjusted,
    for better visualization.
  Figure 5 Link: articels_figures_by_rev_year\2016\Expanded_Parts_Model_for_Semantic_Description_of_Humans_in_Still_Images\figure_5.jpg
  Figure 5 caption: The evolution of the (left) objective value, (middle) number of
    model parts along with the (right) average precision versus number of iterations,
    for the validation set of 'interacting with a computer' class of the Willow Actions
    dataset, demonstrating the convergence of our algorithm.
  Figure 6 Link: articels_figures_by_rev_year\2016\Expanded_Parts_Model_for_Semantic_Description_of_Humans_in_Still_Images\figure_6.jpg
  Figure 6 caption: 'Distribution of the norm of the part templates (left top) and
    some example ''parts'' (rest three). Each row illustrates one of the parts: the
    first image is the patch used to initialize the part and the remaining images
    are its top scoring patches. We show, for each class, parts with different norms
    (color coded) of the corresponding mathbfwp vectors, higher (lower) norm part
    at top (bottom). (see Section 4.3 for a discussion, best viewed in color).'
  Figure 7 Link: articels_figures_by_rev_year\2016\Expanded_Parts_Model_for_Semantic_Description_of_Humans_in_Still_Images\figure_7.jpg
  Figure 7 caption: Experiments to evaluate the impact of the number of parts and
    the number of initial candidate parts on the performance of the proposed model
    on the validation set of the Willow Actions dataset (see Table 1 for the full
    class names). The first row shows the performances and number of model parts for
    different values of k , i.e., the maximum number of model parts used to score
    a test image, while the second row shows those for varying n , i.e., the number
    of initial part candidates sampled per training image.
  Figure 8 Link: articels_figures_by_rev_year\2016\Expanded_Parts_Model_for_Semantic_Description_of_Humans_in_Still_Images\figure_8.jpg
  Figure 8 caption: The average precision obtained by the models for (left) Stanford
    Actions, (middle) HAT dataset and (right) the number of training images (for HAT;
    the number of training images for Stanford Actions dataset is same for all classes)
    versus the number of parts in the final trained models of the different classes
    (see Section 4.3 for discussion).
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Gaurav Sharma
  Name of the last author: Cordelia Schmid
  Number of Figures: 8
  Number of Tables: 2
  Number of authors: 3
  Paper title: Expanded Parts Model for Semantic Description of Humans in Still Images
  Publication Date: 2016-03-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performances (mAP) on the Willow Actions Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performances (mAP) of EPM and Deep Convolutional Neural Networks
      on the Stanford 40 Actions and the Human Attributes Datasets
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2537325
- Affiliation of the first author: perception and activity understanding, idiap, martigny,
    valais, switzerland
  Affiliation of the last author: computer vision, idiap research institute, martigny,
    valais, switzerland
  Figure 1 Link: articels_figures_by_rev_year\2016\Deep_Dynamic_Neural_Networks_for_Multimodal_Gesture_Segmentation_and_Recognition\figure_1.jpg
  Figure 1 caption: 'Gesture recognition model: the temporal model is an HMM (left),
    whose emission probability p( X t | H t ) (right) is modeled by feedforward neural
    networks. Observations X t (skeletal features X s t , or RGB-D image features
    X r t ) are first passed through the appropriate deep neural nets (a DBN pretrained
    with Gaussian-Bernouilli Restricted Boltzmann Machines for the skeleton modality
    and a 3DCNN for the RGB-D modality) to extract high-level features ( V s and V
    r ) . These are subsequently combined to produce an estimate of p( X t | H t )
    .'
  Figure 10 Link: articels_figures_by_rev_year\2016\Deep_Dynamic_Neural_Networks_for_Multimodal_Gesture_Segmentation_and_Recognition\figure_10.jpg
  Figure 10 caption: 'HMM temporal contribution. First row: output emission probabilities
    for each gesture as given by the late fusion scheme (see text) for the test set
    703. The dashed line represents the probability of the RestingOther gesture state,
    while other color represent different gestures. Second row: resulting recognized
    gestures, without HMM modeling. Third row: HMM output. Fourth row: ground truth
    segmentation. Without temporal modeling, the decision boundary of a gesture will
    be more rugged and it is more difficult to make hard decisions of where the gesture
    starts or ends. Hence, in general, it causes miss-detection and miss-merging.
    Thanks to the HMM temporal modeling and Viterbi path decoding, gesture boundaries
    are usually cleaner defined from the Resting state to the gesture states, resembling
    the behavior of the manual annotators with better accuracy.'
  Figure 2 Link: articels_figures_by_rev_year\2016\Deep_Dynamic_Neural_Networks_for_Multimodal_Gesture_Segmentation_and_Recognition\figure_2.jpg
  Figure 2 caption: State diagram of the ES-HMM model for low-latency gesture segmentation
    and recognition. An ergodic state ( ES ) is used to model the resting position
    between gesture sequences. Each node represents a single state and each row represents
    a single gesture model. The arrows indicate possible transitions between states.
  Figure 3 Link: articels_figures_by_rev_year\2016\Deep_Dynamic_Neural_Networks_for_Multimodal_Gesture_Segmentation_and_Recognition\figure_3.jpg
  Figure 3 caption: 'Left: A point cloud projection of a depth image and the 3D positional
    features. Right: A DBN is trained to predict the emission probability p( X s t
    | H t ) from the skeleton input f t . The double arrows indicate that the intermediate
    weights are first trained in an unsupervised fashion using stacked RBMs.'
  Figure 4 Link: articels_figures_by_rev_year\2016\Deep_Dynamic_Neural_Networks_for_Multimodal_Gesture_Segmentation_and_Recognition\figure_4.jpg
  Figure 4 caption: "3DCNN architecture. The input is 2\xD7264\u221764\u22174 , meaning\
    \ 2 modalities (depth and RGB) for the hand and body regions, each being 4 consecutive\
    \ 64 by 64 frames stacked together. See text for further details."
  Figure 5 Link: articels_figures_by_rev_year\2016\Deep_Dynamic_Neural_Networks_for_Multimodal_Gesture_Segmentation_and_Recognition\figure_5.jpg
  Figure 5 caption: "Visualization of input frames, first convolutional layer 5\xD7\
    5 filters, and corresponding response maps. As depth images are smoother than\
    \ the grayscale ones, the corresponding filters are smoother as well."
  Figure 6 Link: articels_figures_by_rev_year\2016\Deep_Dynamic_Neural_Networks_for_Multimodal_Gesture_Segmentation_and_Recognition\figure_6.jpg
  Figure 6 caption: Multimodal dynamic networks with late fusion scheme (left) and
    intermediate fusion scheme (right). The late approach simply combines the emission
    probabilities from two modalities. In the intermediate fusion scheme, each modality
    (skeleton and RGB-D) is first pretrained separately, and their high-level representation
    V s and V r (the penultimate node layers of their neural networks) are concatenated
    to generate a shared representation. The two sub-modules in the resulting architecture
    are trained jointly.
  Figure 7 Link: articels_figures_by_rev_year\2016\Deep_Dynamic_Neural_Networks_for_Multimodal_Gesture_Segmentation_and_Recognition\figure_7.jpg
  Figure 7 caption: "Examples of gestures in the ChaLearn dataset. This dataset is\
    \ challenging because of the \u201Cuser independent\u201D setting (a)&(b), some\
    \ of gestures differ primarily in hand pose but not in the arm movement (d)&(e).\
    \ Some gestures require both hands to perform (g,h,i). Subtle hand movement (c)\
    \ and differences in execution speed and range (f) also make this recognition\
    \ task challenging."
  Figure 8 Link: articels_figures_by_rev_year\2016\Deep_Dynamic_Neural_Networks_for_Multimodal_Gesture_Segmentation_and_Recognition\figure_8.jpg
  Figure 8 caption: Viterbi decoding of sample sequence 700, using skeleton (top),
    RGB-D (middle) and late fusion system (bottom). The x-axis represents time and
    the y-axis represents the hidden states of all classes and of the ergodic state
    (state 101 ) constituting the finite set mathcalH . The cyan lines represent the
    Viterbi shortest path, while red lines denote the ground truth labels, and the
    yellow segments are the predicted labels. The fusion method exploits the complementary
    properties of individual modules, e.g., around frame 200 the skeleton help solving
    the missed detection from the 3DCNN module, while around frame 1,450, the 3DCNN
    module can help suppress the false positive prediction made by the skeleton module.
  Figure 9 Link: articels_figures_by_rev_year\2016\Deep_Dynamic_Neural_Networks_for_Multimodal_Gesture_Segmentation_and_Recognition\figure_9.jpg
  Figure 9 caption: Confusion Matrices (log-form) for the different modalities.
  First author gender probability: 0.56
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Di Wu
  Name of the last author: Jean-Marc Odobez
  Number of Figures: 11
  Number of Tables: 3
  Number of authors: 7
  Paper title: Deep Dynamic Neural Networks for Multimodal Gesture Segmentation and
    Recognition
  Publication Date: 2016-03-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Results in Terms of Jaccard Index JI for the Different Network
      Structures and Modalities Modeling the Emission Probabilities
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Gesture Classification Performance at the Event Level, as
      a Percentage of the Number of Gestures
  Table 3 caption:
    table_text: TABLE 3 Comparison of Results in Terms of the ChaLearn Jaccard Index
      with State-of-the-Art Related Works
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2537340
- Affiliation of the first author: stars team - inria sophia antipolis mediterranee,
    valbonne, france
  Affiliation of the last author: stars team - inria sophia antipolis mediterranee,
    valbonne, france
  Figure 1 Link: articels_figures_by_rev_year\2016\Semantic_Event_Fusion_of_Different_Visual_Modality_Concepts_for_Activity_Recogni\figure_1.jpg
  Figure 1 caption: 'Semantic event fusion framework: detector modules (A-C) process
    data from their respective sensors (S1-S3) and output concepts (objects and low-level
    events). Semantic Event Fusion uses the ontological representation to initialize
    concepts to event models and then infer complex, composite activities. Concept
    fusion is performed on millisecond temporal resolution to cope with instantaneous
    errors of concept recognition.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Semantic_Event_Fusion_of_Different_Visual_Modality_Concepts_for_Activity_Recogni\figure_2.jpg
  Figure 2 caption: Processing pipeline for saliency-based object recognition in first-person
    camera videos.
  Figure 3 Link: articels_figures_by_rev_year\2016\Semantic_Event_Fusion_of_Different_Visual_Modality_Concepts_for_Activity_Recogni\figure_3.jpg
  Figure 3 caption: Physical object sub-tree of the ontology language.
  Figure 4 Link: articels_figures_by_rev_year\2016\Semantic_Event_Fusion_of_Different_Visual_Modality_Concepts_for_Activity_Recogni\figure_4.jpg
  Figure 4 caption: "Composite relations between concepts and event models. Multimodal\
    \ Event \u201CPrepare drink\u201D is composed of conceptual events \u201Cprepare\
    \ drink\u201D from KER and AR detectors and conceptual objects \u201CTea bag\u201D\
    , \u201CKettle\u201D and \u201CGlass\u201D from OR detectors. For instance, the\
    \ hierarchically lower event \u201CPrepare drink\u201D from KER detector can be\
    \ further decomposed into two sub-events, while other detector concepts are atomic."
  Figure 5 Link: articels_figures_by_rev_year\2016\Semantic_Event_Fusion_of_Different_Visual_Modality_Concepts_for_Activity_Recogni\figure_5.jpg
  Figure 5 caption: Observation room where daily living activities are undertaken.
    Contextual zones are depicted as free-from closed polygons in red, and contextual
    objects as black ellipses.
  Figure 6 Link: articels_figures_by_rev_year\2016\Semantic_Event_Fusion_of_Different_Visual_Modality_Concepts_for_Activity_Recogni\figure_6.jpg
  Figure 6 caption: 'Semantic alignment between the concept stream of the action recognition
    detector (AR) and a concept stream (GT) generated from events manually annotated
    by domain experts using the time axis of the color-depth camera. X-axis denotes
    time in frames, and Y-axis denotes activity code (1-8), respectively, search bus
    line on the map, establish bank account balance, prepare pill box, prepare a drink,
    read, talk on the telephone, watch tv, and water the plant. From top to bottom,
    images denote: (A) original GT and AR streams, (B) GT and AR streams warped, AR
    stream warped and smoothed (in red), (C) original GT and AR stream warped and
    then backprojected onto GT temporal axis, (D) original GT and AR warped, backprojected,
    and then smoothed with median filtering.'
  Figure 7 Link: articels_figures_by_rev_year\2016\Semantic_Event_Fusion_of_Different_Visual_Modality_Concepts_for_Activity_Recogni\figure_7.jpg
  Figure 7 caption: Event recognition performance according to probability threshold.
    BT refers to the threshold with best performance for each event.
  Figure 8 Link: articels_figures_by_rev_year\2016\Semantic_Event_Fusion_of_Different_Visual_Modality_Concepts_for_Activity_Recogni\figure_8.jpg
  Figure 8 caption: Performance of OR concept detector per object class.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Carlos F. Crispim-Junior
  Name of the last author: Francois Bremond
  Number of Figures: 8
  Number of Tables: 5
  Number of authors: 8
  Paper title: Semantic Event Fusion of Different Visual Modality Concepts for Activity
    Recognition
  Publication Date: 2016-03-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Semantic Alignment versus Event Recognition
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Event Recognition Performance in the Validation Set
  Table 3 caption:
    table_text: TABLE 3 Event Recognition Performance in the Test Set
  Table 4 caption:
    table_text: TABLE 4 Event Recognition Performance versus Concept Detector Composition
  Table 5 caption:
    table_text: TABLE 5 Comparison to Baseline Methods in the Test Set
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2537323
- Affiliation of the first author: school of electronic information engineering, tianjin
    university, tianjin, china
  Affiliation of the last author: school of computing, national university of singapore
  Figure 1 Link: articels_figures_by_rev_year\2016\Hierarchical_Clustering_MultiTask_Learning_for_Joint_Human_Action_Grouping_and_R\figure_1.jpg
  Figure 1 caption: Comparison of systematic frameworks. (a) The flowchart of the
    STL methods; (b) The flowchart of the MTL methods; (c) The flowchart of the proposed
    method with the shaded parts highlighting differences with respect to the state
    of the art.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Hierarchical_Clustering_MultiTask_Learning_for_Joint_Human_Action_Grouping_and_R\figure_2.jpg
  Figure 2 caption: "HC-MTL. W \u2217 denotes the optimal models and each column vector\
    \ depicts the parameter of each model; W h ( W h = L h + S h ) denotes the obtained\
    \ models in the hth level; L h is the low-rank part; S h is the group-sparse part;\
    \ the elements of the blue vector in S h denote the near 0 value; each box denotes\
    \ individual group of actions, T t , and T \u2217 is the optimal grouping information;\
    \ \u2A01 denotes the column-wise vector addition in Eq. (3)."
  Figure 3 Link: articels_figures_by_rev_year\2016\Hierarchical_Clustering_MultiTask_Learning_for_Joint_Human_Action_Grouping_and_R\figure_3.jpg
  Figure 3 caption: Grouping results. The samples in the first & second lines are
    from HMDB51 and UCF101 respectively.
  Figure 4 Link: articels_figures_by_rev_year\2016\Hierarchical_Clustering_MultiTask_Learning_for_Joint_Human_Action_Grouping_and_R\figure_4.jpg
  Figure 4 caption: Comparison by varying the iteration number.
  Figure 5 Link: articels_figures_by_rev_year\2016\Hierarchical_Clustering_MultiTask_Learning_for_Joint_Human_Action_Grouping_and_R\figure_5.jpg
  Figure 5 caption: Comparison between HC-MTL & CMTL (%).
  Figure 6 Link: articels_figures_by_rev_year\2016\Hierarchical_Clustering_MultiTask_Learning_for_Joint_Human_Action_Grouping_and_R\figure_6.jpg
  Figure 6 caption: Performance comparison by varying alpha ;&; beta on Hollywood2(a-b),
    UCF Sports(c-d), YouTube(e-f), UCF50(g-h), HMDB51(i-j), UCF101(k-l) (%).
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: An-An Liu
  Name of the last author: Mohan Kankanhalli
  Number of Figures: 6
  Number of Tables: 7
  Number of authors: 4
  Paper title: Hierarchical Clustering Multi-Task Learning for Joint Human Action
    Grouping and Recognition
  Publication Date: 2016-03-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Notations and Definitions
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance Comparison
  Table 3 caption:
    table_text: 'TABLE 3 Performance Comparison on Six Realistic Datasets (OI:Optimal
      Iteration Number; No Reg: no Regularization; L Reg: Low Rank Structure; S Reg:
      Group Sparsity Structure; L S Reg: Both Regularizations)'
  Table 4 caption:
    table_text: TABLE 4 Accuracy (Acc) Comparison on Hollywood2, UCF Sports, YouTube,
      UCF50, KTH, IXMAS (%)
  Table 5 caption:
    table_text: TABLE 5 Comparison on HMDB51 & UCF101 (%)
  Table 6 caption:
    table_text: TABLE 6 Comparison on Cross-Domain Scenarios (%)
  Table 7 caption:
    table_text: TABLE 7 Comparison on Action Grouping
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2537337
