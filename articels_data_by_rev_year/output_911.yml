- Affiliation of the first author: department of computer science, johns hopkins university,
    baltimore, md
  Affiliation of the last author: department of computer science, johns hopkins university,
    baltimore, md
  Figure 1 Link: articels_figures_by_rev_year\2017\Scalable_Joint_Models_for_Reliable_UncertaintyAware_Event_Prediction\figure_1.jpg
  Figure 1 caption: "(a) Shows estimates from a joint-model over longitudinal and\
    \ time-to-event data. Data from the shaded red region are used to estimate the\
    \ probability of occurrence of the event. Further, within a given \u0394 , the\
    \ distribution of the event probability is shown in the top right. (b) Describes\
    \ the observed event data (green stars). The latent deterioration state shows\
    \ an example pattern that may lead to the observed events. Here, the patient gradually\
    \ transitions from being healthy to becoming sick and when they get worse enough,\
    \ the symptoms associated with the event\u2014in this case, septic shock\u2014\
    become visible. For the desired output, ideally the system should identify that\
    \ the patient is deteriorating as soon as it starts to occur. For the detector\
    \ output, a positive (or negative) prediction is shown as above (or below) the\
    \ axis. The color indicates whether the prediction is correct (green) or wrong\
    \ (red). At a given time, the detector may choose to not predict. This is shown\
    \ as intervals where neither a positive nor negative prediction is made. Here,\
    \ a detection much prior to the event time is considered a false detection."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Scalable_Joint_Models_for_Reliable_UncertaintyAware_Event_Prediction\figure_2.jpg
  Figure 2 caption: Robust prediction policy.
  Figure 3 Link: articels_figures_by_rev_year\2017\Scalable_Joint_Models_for_Reliable_UncertaintyAware_Event_Prediction\figure_3.jpg
  Figure 3 caption: "Three example decisions made using the policy described in Fig.\
    \ 2 with L 1 =1 and L 2 =0.4 . The shaded area is the confidence interval [ h\
    \ (1\u2212q) , h (q) ] for some choice of q for the three distributions, (a),\
    \ (b), and (c). The arrows at 0.4 and 0.6 are L 2 and 1\u2212 L 2 L 1 , respectively.\
    \ All cases satisfy c q \u2265 L 2 1+ L 1 L 1 \u22121 . The optimal decisions\
    \ are \u03C8 =1 for (a), \u03C8 =0 for (b), and \u03C8 =a for (c)."
  Figure 4 Link: articels_figures_by_rev_year\2017\Scalable_Joint_Models_for_Reliable_UncertaintyAware_Event_Prediction\figure_4.jpg
  Figure 4 caption: Data from 10 signals (dots) and longitudinal fit (solid line)
    along with their confidence intervals (shaded area) for two patients, (a) patient
    A with septic shock and (b) patient B with no observed shock. On the right, we
    show the estimated event probability for the following 40 hour period conditioned
    on the longitudinal data for each patient shown on the left. Septic shock for
    patient A occurs on day 5 of the stay. J-LTM observes the first 3.5 days of the
    longitudinal data from this patient and predicts the shock 36 hours before its
    onset.
  Figure 5 Link: articels_figures_by_rev_year\2017\Scalable_Joint_Models_for_Reliable_UncertaintyAware_Event_Prediction\figure_5.jpg
  Figure 5 caption: (a) ROC curves. (b) Maximum TPR obtained at each PPV level. (c)
    and (d) the best TPR achieved at any decision rate fixing PPV >0.4 and PPV >0.5
    , respectively.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.98
  Name of the first author: Hossein Soleimani
  Name of the last author: Suchi Saria
  Number of Figures: 5
  Number of Tables: 0
  Number of authors: 3
  Paper title: Scalable Joint Models for Reliable Uncertainty-Aware Event Prediction
  Publication Date: 2017-08-21 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2742504
- Affiliation of the first author: ku leuven, esat-psi, iminds, gent, belgium
  Affiliation of the last author: ku leuven, esat-psi, iminds, gent, belgium
  Figure 1 Link: articels_figures_by_rev_year\2017\Reflectance_and_Natural_Illumination_from_SingleMaterial_Specular_Objects_Using_\figure_1.jpg
  Figure 1 caption: Overview of our approach. From the input image, in a first step
    we estimate a reflectance map either directly from the input image itself or indirectly
    with additional supervision, and in a second step we decompose the reflectance
    map into reflectance parameters and an illumination map.
  Figure 10 Link: articels_figures_by_rev_year\2017\Reflectance_and_Natural_Illumination_from_SingleMaterial_Specular_Objects_Using_\figure_10.jpg
  Figure 10 caption: 'Results of different variants and steps for our reflectance
    map estimation ( Fig. 1, Step 1). From left to right: input image, ground-truth
    reflectance map (RM), RM result of the Direct approach, RM result of the Indirect
    approach, the intermediate sparse RM produced in the Indirect variant, normals
    produced by the Indirect variant as well. Each result is annotated to come from
    the synthetic, photographed or Internet part of our database. For the Internet-based
    part, no ground-truth RM is available. Please visit the project''s webpage [54]
    for exhaustive results in this format.'
  Figure 2 Link: articels_figures_by_rev_year\2017\Reflectance_and_Natural_Illumination_from_SingleMaterial_Specular_Objects_Using_\figure_2.jpg
  Figure 2 caption: Architecture of the Direct approach for the reflectance map estimation
    (see also Fig. 1, Step 1, Direct). The bottom numbers represent the spatial resolution
    and the ones on top the size of the feature channels for the corresponding convolutional
    layer. Finally, the yellow boxes in the middle indicate the filters' size.
  Figure 3 Link: articels_figures_by_rev_year\2017\Reflectance_and_Natural_Illumination_from_SingleMaterial_Specular_Objects_Using_\figure_3.jpg
  Figure 3 caption: Architecture of the normals estimation sub-step of our Indirect
    approach for estimating the reflectance map (see also Fig. 1, Step 1, Indirect).
    The notation is the same as in Fig. 2. Note that, the middle elements here correspond
    to fully convolutional filters.
  Figure 4 Link: articels_figures_by_rev_year\2017\Reflectance_and_Natural_Illumination_from_SingleMaterial_Specular_Objects_Using_\figure_4.jpg
  Figure 4 caption: Architecture of the last sub-step of our Indirect approach for
    the reflectance map estimation. The notation is the same as in Fig. 2.
  Figure 5 Link: articels_figures_by_rev_year\2017\Reflectance_and_Natural_Illumination_from_SingleMaterial_Specular_Objects_Using_\figure_5.jpg
  Figure 5 caption: The Material CNN for estimating Phong reflectance parameters.
    The notation is the same as in Fig. 2.
  Figure 6 Link: articels_figures_by_rev_year\2017\Reflectance_and_Natural_Illumination_from_SingleMaterial_Specular_Objects_Using_\figure_6.jpg
  Figure 6 caption: The Illumination CNN for estimating natural illumination. The
    notation is the same as in Fig. 2.
  Figure 7 Link: articels_figures_by_rev_year\2017\Reflectance_and_Natural_Illumination_from_SingleMaterial_Specular_Objects_Using_\figure_7.jpg
  Figure 7 caption: Our dataset for the reflectance map estimation consists of synthetic
    images with random view, 3D shape, material, illumination, and exposure.
  Figure 8 Link: articels_figures_by_rev_year\2017\Reflectance_and_Natural_Illumination_from_SingleMaterial_Specular_Objects_Using_\figure_8.jpg
  Figure 8 caption: Different methods to reconstruct reflectance maps.
  Figure 9 Link: articels_figures_by_rev_year\2017\Reflectance_and_Natural_Illumination_from_SingleMaterial_Specular_Objects_Using_\figure_9.jpg
  Figure 9 caption: 'Four examples of training data for decomposing reflectance maps
    into material parameters and natural illumination: Each triplet is a sample of
    the training set. From left to right: the input reflectance map Lmathrmo , the
    output illumination map Lmathrmi , and material fmathrmr .'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.96
  Name of the first author: Stamatios Georgoulis
  Name of the last author: Tinne Tuytelaars
  Number of Figures: 15
  Number of Tables: 4
  Number of authors: 7
  Paper title: Reflectance and Natural Illumination from Single-Material Specular
    Objects Using Deep Learning
  Publication Date: 2017-08-22 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Results for the Reflectance Map Estimation (cf.
      Fig. 1, Step 1) Using the Different Methods Defined in Section 6.1
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Normals Estimation of Indirect Approach on Synthetic Data
  Table 3 caption:
    table_text: TABLE 3 Synthetic Evaluation for Our Material and Illumination Estimation
      (Fig. 1 , Step 2)
  Table 4 caption:
    table_text: TABLE 4 Evaluation on Real Reflectance Maps for Our Material and Illumination
      Estimation ( Fig. 1, Step 2)
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2742999
- Affiliation of the first author: biomedical image analysis group, imperial college
    london, london, united kingdom
  Affiliation of the last author: biomedical image analysis group, imperial college
    london, london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2017\MultiAtlas_Segmentation_Using_Partially_Annotated_Data_Methods_and_Annotation_St\figure_1.jpg
  Figure 1 caption: "(a) Toy dataset with an unlabelled target image on the left,\
    \ atlas images and corresponding manual annotations (blue and red depict different\
    \ labels) on the right. (b) In MAS, each voxel x in target image i is labelled\
    \ by label propagation from atlases j\u22081,\u2026,R with fusion weights \u03B2\
    \ ij (x) . This can also be interpreted as an MRF optimisation problem, where\
    \ atlas voxels are connected to the terminal nodes with infinitely weighted edges\
    \ and inter-image edges \u03B2 ij (x) encode label fusion."
  Figure 10 Link: articels_figures_by_rev_year\2017\MultiAtlas_Segmentation_Using_Partially_Annotated_Data_Methods_and_Annotation_St\figure_10.jpg
  Figure 10 caption: An example segmentation for PA-SW-CONF2 is shown in red and the
    ground truth segmentation in yellow. The same subject is shown at different slice
    positions in (a) and (b). From left to right, the proportion of labelled atlas
    slices q was 1, 0.8, 0.6, 0.4, 0.2, 0.1 .
  Figure 2 Link: articels_figures_by_rev_year\2017\MultiAtlas_Segmentation_Using_Partially_Annotated_Data_Methods_and_Annotation_St\figure_2.jpg
  Figure 2 caption: "Graph configuration representing patch-based segmentation. \u03B2\
    \ ij (x,y) is determined by a patch similarity measure between a patch centred\
    \ around voxel x in image i and voxel y in image j . Not all connections are drawn\
    \ for better visibility and to reflect that in practice, dissimilar patches are\
    \ omitted in the label fusion [14]."
  Figure 3 Link: articels_figures_by_rev_year\2017\MultiAtlas_Segmentation_Using_Partially_Annotated_Data_Methods_and_Annotation_St\figure_3.jpg
  Figure 3 caption: Different graph configurations representing (a) MAS with spatial
    regularisation in the target image, (b) an additional data term in the target
    image, i.e., encoding intensity models for the data, (c) MAS with missing atlas
    labels. Missing labels are reflected in the graph structure by missing terminal
    connections.
  Figure 4 Link: articels_figures_by_rev_year\2017\MultiAtlas_Segmentation_Using_Partially_Annotated_Data_Methods_and_Annotation_St\figure_4.jpg
  Figure 4 caption: Graph configurations for employing partially annotated atlas data.
    Voxels with missing labels (white) are disconnected from terminal nodes. In contrast
    to Fig. 3c, spatial regularisation is enabled in all images. (a) Voxels at each
    location x in the target image are connected to voxels in atlases j . (b) Additionally,
    atlas voxels are connected to voxels in other atlases. (c) Shows a possible graph
    sparsification by removing obsolete edges between labelled atlas voxels.
  Figure 5 Link: articels_figures_by_rev_year\2017\MultiAtlas_Segmentation_Using_Partially_Annotated_Data_Methods_and_Annotation_St\figure_5.jpg
  Figure 5 caption: 'Illustration of partial annotation strategies: (a) a volumetric
    image with partial slice-by-slice annotation and (b) the same image with scribbles
    placed on each slice. Red and blue depict foreground and background, respectively,
    and voxels in grey remained unlabelled.'
  Figure 6 Link: articels_figures_by_rev_year\2017\MultiAtlas_Segmentation_Using_Partially_Annotated_Data_Methods_and_Annotation_St\figure_6.jpg
  Figure 6 caption: "Flow constraints \u03B2 ij (x), C s,t i (x), \u03B1 i (x) for\
    \ label propagation, data term and spatial regularisation, and corresponding inter-image\
    \ flows r ij (x) , source and sink flows p s,t i (x) and spatial flows p i (x)\
    \ at location x in image i ."
  Figure 7 Link: articels_figures_by_rev_year\2017\MultiAtlas_Segmentation_Using_Partially_Annotated_Data_Methods_and_Annotation_St\figure_7.jpg
  Figure 7 caption: Schematic showing graph configuration for multi-label CMF using
    the Potts Model. The graph (in this figure only one image i is shown) is replicated
    for each label l . The data term is encoded in the sink constraints for every
    label.
  Figure 8 Link: articels_figures_by_rev_year\2017\MultiAtlas_Segmentation_Using_Partially_Annotated_Data_Methods_and_Annotation_St\figure_8.jpg
  Figure 8 caption: Mean Dice coefficients for MAS-MV, MAS-LW and MASr-LW using R=lbrace
    5, 10, 15, 20rbrace atlases. The error bars depict the standard error.
  Figure 9 Link: articels_figures_by_rev_year\2017\MultiAtlas_Segmentation_Using_Partially_Annotated_Data_Methods_and_Annotation_St\figure_9.jpg
  Figure 9 caption: Mean Dice coefficients for slicewise partial annotation for different
    proportions q of labelled atlas slices. PA-SW-CONF1, PA-SW-CONF2, PA-SW-baseline
    describe graph configurations and the error bars depict the standard error.
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Lisa Margret Koch
  Name of the last author: Daniel Rueckert
  Number of Figures: 18
  Number of Tables: 0
  Number of authors: 8
  Paper title: 'Multi-Atlas Segmentation Using Partially Annotated Data: Methods and
    Annotation Strategies'
  Publication Date: 2017-08-22 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2711020
- Affiliation of the first author: department of computer science, university of north
    carolina at chapel hill, chapel hill, nc
  Affiliation of the last author: department of computer science, university of north
    carolina at chapel hill, chapel hill, nc
  Figure 1 Link: articels_figures_by_rev_year\2017\SelfExpressive_Dictionary_Learning_for_Dynamic_D_Reconstruction\figure_1.jpg
  Figure 1 caption: (Left) Example frame from the multiple videos capturing a performance
    serving as input to our method, with overlaid structure (points), and (right three)
    different views of the reconstructed 3D points. Note our method only estimates
    the 3D points but no topology. The skeleton lines are plotted for visualization
    purposes.
  Figure 10 Link: articels_figures_by_rev_year\2017\SelfExpressive_Dictionary_Learning_for_Dynamic_D_Reconstruction\figure_10.jpg
  Figure 10 caption: The reconstruction accuracy when the 2D observations are corrupted
    with Gaussian noise of different standard deviation ( sigma ).
  Figure 2 Link: articels_figures_by_rev_year\2017\SelfExpressive_Dictionary_Learning_for_Dynamic_D_Reconstruction\figure_2.jpg
  Figure 2 caption: 'Left: Multiple videos capture a performance. The corresponding
    set of independent image streams serves as input to our method. Right: Each input
    video has a different sampling of a 3D point''s trajectory.'
  Figure 3 Link: articels_figures_by_rev_year\2017\SelfExpressive_Dictionary_Learning_for_Dynamic_D_Reconstruction\figure_3.jpg
  Figure 3 caption: We illustrate the output of Eq. (12) on a real motion capture
    dataset. For easy visualization, the shortest motion capture dataset (45 frames)
    presented in [34] is used. Each elementcolumn in X corresponds to ground truth
    3D structure. The estimation of W through Eq. (12) approximates the correct ordering
    after enforcing all elements in the diagonal to be 0.
  Figure 4 Link: articels_figures_by_rev_year\2017\SelfExpressive_Dictionary_Learning_for_Dynamic_D_Reconstruction\figure_4.jpg
  Figure 4 caption: Illustration of the triplets influencing the weights for S f and
    S j leading to an asymmetric W . The values in the figure represent the distance
    between adjacent points.
  Figure 5 Link: articels_figures_by_rev_year\2017\SelfExpressive_Dictionary_Learning_for_Dynamic_D_Reconstruction\figure_5.jpg
  Figure 5 caption: Example of incorrect initialization. The dataset 'hopBothLegs3hops'
    in [34] has the motion of hopping forward three times. The black and blue shapes
    (almost overlapped) are the incorrect initialization of the real shapes (shown
    in green and red) of frames 16 and 89 due to the accidental ray intersections.
    This typically happens in the case of periodic motion such as walking or jogging.
    In the figure, only one set of nearly intersecting rays is plotted.
  Figure 6 Link: articels_figures_by_rev_year\2017\SelfExpressive_Dictionary_Learning_for_Dynamic_D_Reconstruction\figure_6.jpg
  Figure 6 caption: Simulated camera setups. The blue curve is a trajectory of a 3D
    point obtained from motion capture data. Figs. 6a and 6b depict the camera setups
    of one and four slow-moving handheld cameras. Fig. 6c depicts a scenario where
    each random camera only captures one image. Figs. 6b and 6c show the camera setups
    used in our method and [6], respectively. Coordinates are in millimeters (mm).
  Figure 7 Link: articels_figures_by_rev_year\2017\SelfExpressive_Dictionary_Learning_for_Dynamic_D_Reconstruction\figure_7.jpg
  Figure 7 caption: The reconstructability of the system is lower if the period of
    single-camera capture is longer.
  Figure 8 Link: articels_figures_by_rev_year\2017\SelfExpressive_Dictionary_Learning_for_Dynamic_D_Reconstruction\figure_8.jpg
  Figure 8 caption: Average residuals res at different camera frame rates. Results
    are attained from 130 motion capture datasets in [34].
  Figure 9 Link: articels_figures_by_rev_year\2017\SelfExpressive_Dictionary_Learning_for_Dynamic_D_Reconstruction\figure_9.jpg
  Figure 9 caption: The reconstruction accuracy given different camera frame rates.
    We also test the case that the captures of object motion are randomly assigned
    to any of the image sequences without any constraint. 30 Hz in the figure represents
    the unconstrained assignment.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Enliang Zheng
  Name of the last author: Jan-Michael Frahm
  Number of Figures: 16
  Number of Tables: 0
  Number of authors: 4
  Paper title: Self-Expressive Dictionary Learning for Dynamic 3D Reconstruction
  Publication Date: 2017-08-22 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2742950
- Affiliation of the first author: sri international, princeton, nj
  Affiliation of the last author: max planck institute for informatics, germany
  Figure 1 Link: articels_figures_by_rev_year\2017\Discriminatively_Trained_Latent_Ordinal_Model_for_Video_Classification\figure_1.jpg
  Figure 1 caption: The relative improvements for different action classes for Adaptive
    LOMo versus global temporal pooling, both with iDT features. Color codes indicate
    the type of action and the respective datasets are in brackets.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Discriminatively_Trained_Latent_Ordinal_Model_for_Video_Classification\figure_2.jpg
  Figure 2 caption: Detections made by LOMo trained ( M=3 ) for classifying 'happy'
    expression on two expression sequences from Oulu-CASIA VIS dataset (ground-truth
    for top is 'happy' and bottom is 'sad'). The number below the timeline shows the
    relative location (in percentile of total number of frames).
  Figure 3 Link: articels_figures_by_rev_year\2017\Discriminatively_Trained_Latent_Ordinal_Model_for_Video_Classification\figure_3.jpg
  Figure 3 caption: Detection of multiple discriminative sub-events, discovered by
    LOMo, on the three different human action analysis datasets. The number below
    the timeline shows the relative location (in percentile of total number of frames).
  Figure 4 Link: articels_figures_by_rev_year\2017\Discriminatively_Trained_Latent_Ordinal_Model_for_Video_Classification\figure_4.jpg
  Figure 4 caption: "(Left) The effect of varying the regularization parameter \u03BB\
    \ 1 and (right) the relative gain in performance by using ordinal cost in the\
    \ scoring function."
  Figure 5 Link: articels_figures_by_rev_year\2017\Discriminatively_Trained_Latent_Ordinal_Model_for_Video_Classification\figure_5.jpg
  Figure 5 caption: The effect of varying the number of sub-events M .
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.88
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Karan Sikka
  Name of the last author: Gaurav Sharma
  Number of Figures: 5
  Number of Tables: 4
  Number of authors: 2
  Paper title: Discriminatively Trained Latent Ordinal Model for Video Classification
  Publication Date: 2017-08-22 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of LOMo with Baselines on the Facial Analysis Datasets
      Using SIFT Based Features (See Section 4.3.1)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of LOMo with Baselines on Three Human Activity
      Datasets
  Table 3 caption:
    table_text: TABLE 3 Comparison of the Proposed Approach with Several State-of-the-Art
      Methods on Three Facial Analysis Datasets
  Table 4 caption:
    table_text: TABLE 4 Comparison of the Proposed Approach with Several State-of-the-Art
      Methods on Three Human Action Recognition Datasets
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2741482
- Affiliation of the first author: department of electrical and computer engineering,
    northeastern university, boston, ma
  Affiliation of the last author: department of electrical and computer engineering,
    northeastern university, boston, ma
  Figure 1 Link: articels_figures_by_rev_year\2017\A_Probabilistic_Active_Learning_Algorithm_Based_on_Fisher_Information_Ratio\figure_1.jpg
  Figure 1 caption: Resulting average accuracy of running different FIR-based querying
    algorithms over several UCI and MNIST data sets.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\A_Probabilistic_Active_Learning_Algorithm_Based_on_Fisher_Information_Ratio\figure_2.jpg
  Figure 2 caption: Percentage of rejection of the null hypthesis H 0 over different
    iterations, when using MNIST data set and batch size k=20 .
  Figure 3 Link: articels_figures_by_rev_year\2017\A_Probabilistic_Active_Learning_Algorithm_Based_on_Fisher_Information_Ratio\figure_3.jpg
  Figure 3 caption: Iteration-wise average p -values of active learning algorithm
    comparisons on MNIST data set for batch size k=20 .
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Jamshid Sourati
  Name of the last author: Jennifer G. Dy
  Number of Figures: 3
  Number of Tables: 2
  Number of authors: 5
  Paper title: A Probabilistic Active Learning Algorithm Based on Fisher Information
    Ratio
  Publication Date: 2017-08-24 00:00:00
  Table 1 caption:
    table_text: TABLE 1 AverageSTD of Accuracies of Running Different Active Learning
      Algorithms Over Synthetic Data Set
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Summary of UCI Data Sets
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2743707
- Affiliation of the first author: chinese university of hong kong and sensetime group
    limited, shatin, hong kong
  Affiliation of the last author: department of electronic engineering, chinese university
    of hong kong, shatin, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2017\Crafting_GBDNet_for_Object_Detection\figure_1.jpg
  Figure 1 caption: The necessity of passing messages among features from supporting
    regions of different resolutions, and controlling message passing according different
    image instances. Blue windows indicate the ground truth bounding boxes. Red windows
    are candidate boxes. It is hard to classify candidate boxes which cover parts
    of objects because of similar local visual cues in (a) and variation of occlusion
    in (b). Local details of rabbit ears are useful for recognizing the rabbit head
    in (c). The contextual human head helps to find that the rabbit ear worn on human
    head should not be used to validate the existence of the rabbit head in (d). Best
    viewed in color.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Crafting_GBDNet_for_Object_Detection\figure_2.jpg
  Figure 2 caption: Overview of our framework. The network takes an image as input
    and produces feature maps by CNN. The roi-pooling is done on feature maps to obtain
    features with different resolutions and support regions. Red arrows denote our
    gated bi-directional structure for passing messages among features. Gate functions
    G are defined for controlling the message passing rate. Then all features go through
    multiple CNN layers with shared parameters to obtain the final features that are
    used to predict the class and location refinement of bounding box. Using only
    h 3 2 would reduce the network to Fast-RCNN. Parameters on black arrows are shared
    across branches, while parameters on red arrows are not shared. Best viewed in
    color.
  Figure 3 Link: articels_figures_by_rev_year\2017\Crafting_GBDNet_for_Object_Detection\figure_3.jpg
  Figure 3 caption: Exemplar implementation of our model. The gated bi-directional
    network, dedicated as GBD-Net, is placed between Inception (4d) and Inception
    (4e). Inception (4e), (5a) and (5b) are shared among all branches.
  Figure 4 Link: articels_figures_by_rev_year\2017\Crafting_GBDNet_for_Object_Detection\figure_4.jpg
  Figure 4 caption: Illustration of using roi-pooling to obtain CNN features with
    different resolutions and support regions. The red rectangle in the left image
    is a candidate box. The right four image patches show the supporting regions for
    lbrace mathbf bprbrace . Best viewed in color.
  Figure 5 Link: articels_figures_by_rev_year\2017\Crafting_GBDNet_for_Object_Detection\figure_5.jpg
  Figure 5 caption: Details of our bi-directional structure. otimes denotes convolution.
    The input of this structure is the features lbrace mathbf h0irbrace of multiple
    resolutions and contextual regions. Then bi-directional connections among these
    features are used for passing messages across resolutionscontexts. The output
    mathbf h3i are updated features for different resolutionscontexts after message
    passing.
  Figure 6 Link: articels_figures_by_rev_year\2017\Crafting_GBDNet_for_Object_Detection\figure_6.jpg
  Figure 6 caption: Illustration of the bi-directional structure with gate functions.
    The otimes represents the convolution and the switch button represents the gate
    function. The left one corresponds to Eq. (5) and the right one corresponds to
    Eq. (6).
  Figure 7 Link: articels_figures_by_rev_year\2017\Crafting_GBDNet_for_Object_Detection\figure_7.jpg
  Figure 7 caption: Details of our modified bi-directional structure. Compared with
    the stucture in Fig. 5, an identity mapping layer is added from mathbf h0 to mathbf
    h3 . The convolution from [mathbf h1, mathbf h2] to mathbf h3 in Fig. 5 is changed
    into max-pooling.
  Figure 8 Link: articels_figures_by_rev_year\2017\Crafting_GBDNet_for_Object_Detection\figure_8.jpg
  Figure 8 caption: Architecture for the baseline ResNet-269. Building blocks are
    the identity mapping blocks used in [43], with the numbers of blocks stacked.
    Downsampling is performed by conv31, conv41, and conv51 with a stride of 2.
  Figure 9 Link: articels_figures_by_rev_year\2017\Crafting_GBDNet_for_Object_Detection\figure_9.jpg
  Figure 9 caption: Fraction of high-scored false positives on ImageNet Val2 that
    are due to poor localization (Loc), confusion with other objects (Other), or confusion
    with background or unlabeled objects (Bg).
  First author gender probability: 0.67
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Xingyu Zeng
  Name of the last author: Xiaogang Wang
  Number of Figures: 9
  Number of Tables: 11
  Number of authors: 12
  Paper title: Crafting GBD-Net for Object Detection
  Publication Date: 2017-08-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Object Detection mAP (%) on ImageNet val2 for State-of-the-Art
      Approaches with Single Model (sgl) and Averaged Model (avg)
  Table 10 caption:
    table_text: TABLE 10 Models Used in the Model Ensemble, Global Contextual Scores
      Are Not Used in the Results for These Models
  Table 2 caption:
    table_text: TABLE 2 Object Detection mAP (%) on ImageNet for the Approaches Attending
      the ImageNet Challenge with Single Model (sgl) and Averaged Model (avg) When
      Tested on the val2 Data and Test Data without Using External Data for Training
  Table 3 caption:
    table_text: TABLE 3 Detection mAP (%) for Features with Different Padding Values
      p for GBD-Net-v1 Using Inception-v2 as the Baseline
  Table 4 caption:
    table_text: TABLE 4 Object Detection mAP (%) on MS-COCO for State-of-the-Art Approaches
  Table 5 caption:
    table_text: TABLE 5 Recall Rate on ImageNet val2 with Different Components in
      Proposal Generation
  Table 6 caption:
    table_text: TABLE 6 Object Detection mAP (%) on ImageNet val2 for Inception-v2
      Using Different Pretraining Schemes
  Table 7 caption:
    table_text: TABLE 7 Object Detection mAP (%) on ImageNet val2 for Different Baseline
      Networks with the GBD-v2
  Table 8 caption:
    table_text: "TABLE 8 Object Detection mAP (%) on ImageNet val2 for Inception-v2\
      \ Using GBD Structure with Different Scale Factor \u03B2 in Controlling the\
      \ Magnitude of Message"
  Table 9 caption:
    table_text: TABLE 9 Object Detection mAP (%) on ImageNet val2 for Different Baseline
      Deep Models
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2745563
- Affiliation of the first author: department of electronic engineering and computer
    science, queen mary university of london, london, united kingdom
  Affiliation of the last author: department of electronic engineering and computer
    science, queen mary university of london, london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2017\Foreground_Segmentation_with_TreeStructured_Sparse_RPCA\figure_1.jpg
  Figure 1 caption: 'i2R[6] results: top row is the original image, second row is
    the ground truth, the third row is DBSS results, and the last row is DSPSS output.
    We used the same frames as [19], [20], [31], [32], [33], and [34], for qualitative
    comparison.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Foreground_Segmentation_with_TreeStructured_Sparse_RPCA\figure_2.jpg
  Figure 2 caption: "CDnet 2012 [7] results: identical layout to Fig. 1 with multiple\
    \ rows. The ground truth includes is marked with various shades of gray \u2013\
    \ dark gray to indicate shadows, mid gray for ignored regions for evaluation,\
    \ and light gray for areas ignored per frame, usually the outline of objects where\
    \ foregroundbackground assignment is ambiguous."
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.79
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.64
  Name of the first author: Salehe Erfanian Ebadi
  Name of the last author: Ebroul Izquierdo
  Number of Figures: 2
  Number of Tables: 4
  Number of authors: 2
  Paper title: Foreground Segmentation with Tree-Structured Sparse RPCA
  Publication Date: 2017-08-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Description of the Parameters for DBSS and DSPSS
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 CDNet 2012 [7] Dataset: F-Measure Results of All the Categories
      for the Most Competitive methods'
  Table 3 caption:
    table_text: 'TABLE 3 SABS [5] Dataset: F-Measure Results for Nine Challenges;
      Only the Most Competitive Algorithms Were Included'
  Table 4 caption:
    table_text: TABLE 4 i2R [6] and WallFlower [4] Dataset F-Measure Results. We Report
      DBSS and DSPSS without Parameter Tuning, Although the Dataset Allows This
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2745573
- Affiliation of the first author: school of computer science, university of nottingham,
    nottingham, united kingdom
  Affiliation of the last author: school of computer science, university of nottingham,
    nottingham, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2017\A_Functional_Regression_Approach_to_Facial_Landmark_Tracking\figure_1.jpg
  Figure 1 caption: "Left: Difference between sampling-based regression and continuous\
    \ regression. The continuous regression accounts for all the samples within a\
    \ neighbourhood, whereas sampling-based needs to sample the data from a given\
    \ distribution. Right: Our new approach to Continuous Regression can be seen as\
    \ the inverse of a Monte Carlo sampling estimation. We will see that the probability\
    \ density function defined by p(\u03B4s) defines the volume within which samples\
    \ are taken."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\A_Functional_Regression_Approach_to_Facial_Landmark_Tracking\figure_2.jpg
  Figure 2 caption: "Results obtained for the Taylor approximation. Red curves correspond\
    \ to the distances described in Eq. (22), when i=j , for a varying set of initial\
    \ perturbations ranging from l=3 to l=7 . Blue curves correspond to the distances\
    \ described in Eq. (22) when i\u2260j ."
  Figure 3 Link: articels_figures_by_rev_year\2017\A_Functional_Regression_Approach_to_Facial_Landmark_Tracking\figure_3.jpg
  Figure 3 caption: Results attained by a sampling-based linear regression (green
    curves) and a regressor trained using Continuous Regression (red curves) for the
    given initial perturbations (blue curves).
  Figure 4 Link: articels_figures_by_rev_year\2017\A_Functional_Regression_Approach_to_Facial_Landmark_Tracking\figure_4.jpg
  Figure 4 caption: Cumulative Error Distribution (CED) curve for both the uncor-CR
    (red) and cor-CR (blue). Results are shown for the 49 points configuration. The
    contribution of a full covariance matrix is clear.
  Figure 5 Link: articels_figures_by_rev_year\2017\A_Functional_Regression_Approach_to_Facial_Landmark_Tracking\figure_5.jpg
  Figure 5 caption: 'Difference between classical Functional Regression (Left) and
    CR in correlated variables ( Right). The green area represents the volume within
    which samples are taken. Left: a diagonal covariance matrix, with entries defined
    as fraca23 . Right: A full covariance matrix and a non-zero mean vector. The sampling
    space is translated to the centre, defined by boldsymbolmu , and rotated according
    to the eigenvectors of boldsymbolSigma .'
  Figure 6 Link: articels_figures_by_rev_year\2017\A_Functional_Regression_Approach_to_Facial_Landmark_Tracking\figure_6.jpg
  Figure 6 caption: Overview of our incremental cascaded continuous regression algorithm
    (iCCR). The originally model mathbf Rmathcal T learnt offline is updated with
    each new frame, thus sequentially adapting to the target face.
  Figure 7 Link: articels_figures_by_rev_year\2017\A_Functional_Regression_Approach_to_Facial_Landmark_Tracking\figure_7.jpg
  Figure 7 caption: CED's for a set of different forgetting factors, along with the
    corresponding AUCs (Category 3). Red curve shows the Oracle. Magenta shows the
    best factor found for that Category. Green curve shows the results attained by
    the forgetting factor found on the training set. Blue curves are results attained
    by other learning rates.
  Figure 8 Link: articels_figures_by_rev_year\2017\A_Functional_Regression_Approach_to_Facial_Landmark_Tracking\figure_8.jpg
  Figure 8 caption: CED's for the 49-points configuration. Better seen in colour.
  Figure 9 Link: articels_figures_by_rev_year\2017\A_Functional_Regression_Approach_to_Facial_Landmark_Tracking\figure_9.jpg
  Figure 9 caption: Visual examples from Category 3 on 300VW. Upper rows show the
    CCR fitting (red points), whereas the lower rows show the iCCR fitting (green
    points). The sequences on the left show a clear case where iCCR outperforms CCR
    thanks to its incremental learning capabilities (fast movement in the upper sequence,
    low resolution in the sequence at the bottom). The sequences on the right show
    a failure case where the learned features are not capable to outperform CCR. All
    qualitative videos and results can be found at https:continuousregression.wordpress.comcode.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: "Enrique S\xE1nchez-Lozano"
  Name of the last author: Michel Valstar
  Number of Figures: 9
  Number of Tables: 1
  Number of authors: 5
  Paper title: A Functional Regression Approach to Facial Landmark Tracking
  Publication Date: 2017-08-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 AUC for 49 Points Configuration for the Different Categories
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2745568
- Affiliation of the first author: "department of electrical engineering and computer\
    \ science, university of li\xE9ge, li\xE9ge, belgium"
  Affiliation of the last author: "department of electrical engineering and computer\
    \ science, university of li\xE9ge, li\xE9ge, belgium"
  Figure 1 Link: articels_figures_by_rev_year\2017\Probabilistic_Framework_for_the_Characterization_of_Surfaces_and_Edges_in_Range_\figure_1.jpg
  Figure 1 caption: (a) Schematic view of a surface seen by a range camera. P and
    Q are two points that belong to this surface; they show up at pixels p and q in
    a corresponding range image. (b) Plot of the probability density function (pdf)
    of the random variable Z q that represents the range of q , given the range z
    p of p and the assumption that P and Q belong to the same surface. This paper
    establishes that this pdf is a Voigt pdf and shows how to use this finding for
    edge detection.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Probabilistic_Framework_for_the_Characterization_of_Surfaces_and_Edges_in_Range_\figure_2.jpg
  Figure 2 caption: (a) Representation with false colors of a range image obtained
    with a Microsoft Kinect 1 camera. (b) Corresponding edges obtained by application
    of an edge detector based on the framework described in this paper.
  Figure 3 Link: articels_figures_by_rev_year\2017\Probabilistic_Framework_for_the_Characterization_of_Surfaces_and_Edges_in_Range_\figure_3.jpg
  Figure 3 caption: "Geometrical configuration showing a pair of pixels p , q in the\
    \ image plane and their corresponding points P \u20D7 , Q \u20D7 in the real world."
  Figure 4 Link: articels_figures_by_rev_year\2017\Probabilistic_Framework_for_the_Characterization_of_Surfaces_and_Edges_in_Range_\figure_4.jpg
  Figure 4 caption: "(a) Comparison between the probability density function of the\
    \ surface orientation f \u0398 (\u03B8) (shown as a continuous line) derived from\
    \ our assumption that all surface orientations are equally probable in the real\
    \ world, and the histogram of surface orientations estimated from real world range\
    \ images from the RGB-D SLAM Dataset [36]. (b) Corresponding comparison for the\
    \ pdf of range values f Z p ( z p ) ."
  Figure 5 Link: articels_figures_by_rev_year\2017\Probabilistic_Framework_for_the_Characterization_of_Surfaces_and_Edges_in_Range_\figure_5.jpg
  Figure 5 caption: Each image shows, in false colors, and for the indicated range
    camera, the value, at each pixel, of the parameter eta that appears in Eq. (47)
    of Appendix A.5 and indicates the weight of the Cauchy pdf when one represents
    the Voigt pdf of fZmathbf q(zmathbf qvert zmathbf p,mathcal S) as a linear combination
    of a Cauchy pdf and a Gaussian pdf. The value shown at a pixel mathbf q shows
    the value of eta with mathbf p being immediately to the right of mathbf q . The
    value of eta varies across each image according to the local noise level and the
    range at each pixel.
  Figure 6 Link: articels_figures_by_rev_year\2017\Probabilistic_Framework_for_the_Characterization_of_Surfaces_and_Edges_in_Range_\figure_6.jpg
  Figure 6 caption: Illustration of the procedure for combining the pdfs f(zmathbf
    p,zmathbf q,zmathbf rvert cdots) of horizontally aligned pixels to produce the
    probability map Pleft(mathcal Smathbf pmathbf qvert zmathbf p,zmathbf qright)
    based on two ranges zmathbf p and ztextmathbf q , and the probability map Pleft(mathcal
    Smathbf pmathbf qvert zmathbf p,zmathbf q,zmathbf rright) based on three ranges
    zmathbf p , zmathbf q , and zmathbf r . The figure shows that adding a third range
    leads to more edges being detected. This is the basis for our edge detector.
  Figure 7 Link: articels_figures_by_rev_year\2017\Probabilistic_Framework_for_the_Characterization_of_Surfaces_and_Edges_in_Range_\figure_7.jpg
  Figure 7 caption: Edge detection results on our JUMP dataset. Black, red, and green
    pixels represents the true positives, false positives, and false negatives, respectively.
  Figure 8 Link: articels_figures_by_rev_year\2017\Probabilistic_Framework_for_the_Characterization_of_Surfaces_and_Edges_in_Range_\figure_8.jpg
  Figure 8 caption: Influence of the parameters of the Probabilistic Edge Detector
    (PED) on the performance (ODS, recall, and precision) for the Kinect 1 and Kinect
    2 images from our custom JUMP dataset.
  Figure 9 Link: articels_figures_by_rev_year\2017\Probabilistic_Framework_for_the_Characterization_of_Surfaces_and_Edges_in_Range_\figure_9.jpg
  Figure 9 caption: Edge detection results on the NYU Depth Dataset. The shown edge
    images were generated for the parameters maximizing the ODS score. Black, red,
    and green pixels represent the true positives, false positives, and false negatives,
    respectively.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Antoine Lejeune
  Name of the last author: Marc Van Droogenbroeck
  Number of Figures: 9
  Number of Tables: 3
  Number of authors: 3
  Paper title: Probabilistic Framework for the Characterization of Surfaces and Edges
    in Range Images, with Application to Edge Detection
  Publication Date: 2017-08-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of a Selection of Edge Detectors for Range Images
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results of Our Edge Detector for Each Camera Used to Produce
      Our JUMP Dataset
  Table 3 caption:
    table_text: TABLE 3 Best Performance Indicators Obtained for Each of the 12 Edge
      Detectors on the NYU Depth Dataset
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2746618
