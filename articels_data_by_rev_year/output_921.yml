- Affiliation of the first author: computer science department, bina nusantara university,
    jakarta, indonesia
  Affiliation of the last author: department of electrical and computer engineering,
    seoul national university, seoul, korea
  Figure 1 Link: articels_figures_by_rev_year\2017\Robust_Light_Field_Depth_Estimation_Using_OcclusionNoise_Aware_Data_Costs\figure_1.jpg
  Figure 1 caption: Overview of the light field parameterization.
  Figure 10 Link: articels_figures_by_rev_year\2017\Robust_Light_Field_Depth_Estimation_Using_OcclusionNoise_Aware_Data_Costs\figure_10.jpg
  Figure 10 caption: Comparison of the non-optimized disparity maps (Mona dataset)
    using the individual data costs; (a) BE; (b) V; (c) BCM; (d) SSDB; (e) SSDP; (f)
    GRAD; (g) OV; (h) SPO; (i) AE; (j) CAE; (k) LO; (l) PRD; (m) OPRD; (n) FSS; (o)
    AD; (p) CAD.
  Figure 2 Link: articels_figures_by_rev_year\2017\Robust_Light_Field_Depth_Estimation_Using_OcclusionNoise_Aware_Data_Costs\figure_2.jpg
  Figure 2 caption: "Angular patch analysis. (a) The center pinhole image with a spatial\
    \ patch; (b) Angular patch and its histogram ( \u03B1=8 ); (c) Angular patch and\
    \ its histogram ( \u03B1=28 ); (d) Angular patch and its histogram ( \u03B1=48\
    \ ); (First column) Non-occluded pixel; (Second column) Multi-occluded pixel.\
    \ Ground truth \u03B1 is 8. The contrast of each patch is enhanced for better\
    \ visualization."
  Figure 3 Link: articels_figures_by_rev_year\2017\Robust_Light_Field_Depth_Estimation_Using_OcclusionNoise_Aware_Data_Costs\figure_3.jpg
  Figure 3 caption: Data cost curve analysis for angular patch in (a) Fig. 2 (first
    column); (b) Fig. 2 (second column); (c) Fig. 4. Red line is the ground truth
    depth label.
  Figure 4 Link: articels_figures_by_rev_year\2017\Robust_Light_Field_Depth_Estimation_Using_OcclusionNoise_Aware_Data_Costs\figure_4.jpg
  Figure 4 caption: "Constrained histogram analysis. (a) The center pinhole image\
    \ with a spatial patch; (b) Angular patch ( \u03B1=5 ); (c) Angular patch ( \u03B1\
    =35 ); (d, e) The ordinary histogram of (b, c); (f, g) The constrained histogram\
    \ of (b, c). Ground truth \u03B1 is 5. The contrast of each patch is enhanced\
    \ for better visualization."
  Figure 5 Link: articels_figures_by_rev_year\2017\Robust_Light_Field_Depth_Estimation_Using_OcclusionNoise_Aware_Data_Costs\figure_5.jpg
  Figure 5 caption: "Defocus cost analysis. (a) The center pinhole image with a spatial\
    \ patch; (b) Data cost curve comparison; (c) \u223C (f) Spatial patches from the\
    \ refocus images ( \u03B1=7,27,47,67 ); (g) \u223C (j) Difference maps of the\
    \ patches in (c) \u223C (f). We multiply the spatial patches and difference maps\
    \ with a scalar value for better visualization. Red box shows the minimum subpatch.\
    \ Ground truth \u03B1 is 27."
  Figure 6 Link: articels_figures_by_rev_year\2017\Robust_Light_Field_Depth_Estimation_Using_OcclusionNoise_Aware_Data_Costs\figure_6.jpg
  Figure 6 caption: Disparity maps of (a) Adaptive defocus cost; (b) Constrained adaptive
    defocus cost.
  Figure 7 Link: articels_figures_by_rev_year\2017\Robust_Light_Field_Depth_Estimation_Using_OcclusionNoise_Aware_Data_Costs\figure_7.jpg
  Figure 7 caption: MSE curve for noisy light field images with various noise level.
    (a) non-optimized data costs and (b) optimized data costs.
  Figure 8 Link: articels_figures_by_rev_year\2017\Robust_Light_Field_Depth_Estimation_Using_OcclusionNoise_Aware_Data_Costs\figure_8.jpg
  Figure 8 caption: "The MSE comparison for various light field datasets. (a) Non-optimized\
    \ results of the image (Local); (b) Non-optimized results of the occlusion regions\
    \ (Local); (c) Optimized results of the image (Local + EPF-GC); (d) Optimized\
    \ results of the occlusion regions (Local + EPF-GC). For better visualization,\
    \ we use the logarithm value of MSE\u22171000 ."
  Figure 9 Link: articels_figures_by_rev_year\2017\Robust_Light_Field_Depth_Estimation_Using_OcclusionNoise_Aware_Data_Costs\figure_9.jpg
  Figure 9 caption: The MSE comparison for various noise levels using each local data
    cost. (a) Correspondence costs; (b) Defocus costs; (c) Best of correspondence
    and defocus costs; (From left to right) Non-optimized results of the image (Local);
    Non-optimized results of the occlusion regions (Local); Optimized results of the
    image (Local + EPF-GC); Optimized results of the occlusion regions (Local + EPF-GC).
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.83
  Name of the first author: Williem
  Name of the last author: Kyoung Mu Lee
  Number of Figures: 15
  Number of Tables: 3
  Number of authors: 3
  Paper title: Robust Light Field Depth Estimation Using Occlusion-Noise Aware Data
    Costs
  Publication Date: 2017-08-31 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The MSE Across All Light Field Datasets and Noise Levels
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The BP (%) Across All Light Field Datasets
  Table 3 caption:
    table_text: TABLE 3 The BP (%) Across All Noise Levels (Variance =0.02,0.04,0.06,0.08,0.1
      )
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2746858
- Affiliation of the first author: ibm research, melbourne, vic, australia
  Affiliation of the last author: murdoch university, murdoch, wa, australia
  Figure 1 Link: articels_figures_by_rev_year\2017\A_MultiModal_Discriminative_and_Spatially_Invariant_CNN_for_RGBD_Object_Labeling\figure_1.jpg
  Figure 1 caption: An overview of our framework. Given an input RGB-D video (A),
    our framework first computes the pose of the camera (B) on a frame-by-frame basis.
    Next, it generates patches (C) and combines them into object proposals (D). Next,
    each object proposal is transformed into an image representation (E) and fed as
    input to our classification model (G), to compute object-class scores, which are
    integrated into our CRF-based inference algorithm (H) to produce a labeling of
    the scene (I). Note that for clarity, we show the image representation (E) for
    only one object proposal.
  Figure 10 Link: articels_figures_by_rev_year\2017\A_MultiModal_Discriminative_and_Spatially_Invariant_CNN_for_RGBD_Object_Labeling\figure_10.jpg
  Figure 10 caption: 'Left: Average precision scores for different values of alpha
    on the WRGBD Scene dataset [8]. Right: Qualitative comparison of per-frame labeling
    results produced for different values of the parameter alpha on the WRGBD Scene
    dataset [8].'
  Figure 2 Link: articels_figures_by_rev_year\2017\A_MultiModal_Discriminative_and_Spatially_Invariant_CNN_for_RGBD_Object_Labeling\figure_2.jpg
  Figure 2 caption: CNN responses from the first convolutional layer for the individual
    feature maps of our input image representation ( Section 4.1). Note that the CNN
    responses are considerably different for different feature maps and therefore
    provide complementary information.
  Figure 3 Link: articels_figures_by_rev_year\2017\A_MultiModal_Discriminative_and_Spatially_Invariant_CNN_for_RGBD_Object_Labeling\figure_3.jpg
  Figure 3 caption: An overview of our Multi-modal Discriminative Spatially Invariant
    CNN (Fig. 1G ) training process. It starts with the generation of a batch of augmented
    training samples for each input modality. Next, the augmented batch is used to
    train a CNN model (composed of a Spatial Transformation branch (Section 4.2.1),
    a CNN-I branch ( Section 4.2.2), a CNN-P branch (Section 4.2.3), and a Fisher
    fusion branch (Section 4.2.4)), for image-wise and pixel-wise classification.
    Note that for clarity, we show the training process for only two input modalities
    (i.e., RGB color and surface normal feature maps). In experiments, we evaluated
    our model using six input feature maps generated from RGB-D images (see Fig. 1E
    and Section 4.1).
  Figure 4 Link: articels_figures_by_rev_year\2017\A_MultiModal_Discriminative_and_Spatially_Invariant_CNN_for_RGBD_Object_Labeling\figure_4.jpg
  Figure 4 caption: 'Left: An illustration of the test process of our classification
    model for a test image of the WRGBD scene dataset [8]. The object proposals generated
    from the test image (A) are fed as input to our classification model, which compute
    pixel-level (B) and object-level (D) class scores. The CNN scores are then fed
    as unary potentials to our CRF-based inference algorithm (G) which generates the
    final labeling (H). Note that the outputs of our model are class scores. We show
    the labeling in (C) and (E) to show the differences of the two outputs. Right:
    Illustration of object cliques. The predicted label for each clique is listed
    on the bottom for clique identification.'
  Figure 5 Link: articels_figures_by_rev_year\2017\A_MultiModal_Discriminative_and_Spatially_Invariant_CNN_for_RGBD_Object_Labeling\figure_5.jpg
  Figure 5 caption: Qualitative comparison of the 3D labeling results of the FCN model
    in [17] (C), the CNN model in [16] (D), and our classification model (E). The
    ground truths for the input scenes (A) are shown in (B). Figure best viewed in
    color.
  Figure 6 Link: articels_figures_by_rev_year\2017\A_MultiModal_Discriminative_and_Spatially_Invariant_CNN_for_RGBD_Object_Labeling\figure_6.jpg
  Figure 6 caption: Qualitative comparison of 2D labeling results of our model (A),
    the CNN model in [16] (B), and the FCN model in [17] (C).
  Figure 7 Link: articels_figures_by_rev_year\2017\A_MultiModal_Discriminative_and_Spatially_Invariant_CNN_for_RGBD_Object_Labeling\figure_7.jpg
  Figure 7 caption: Qualitative visualization of labeling produced by our model with
    and without using the proposed Spatial transformation regularization (Section
    4.2.1). Note that the labeling produced with the Spatial transformation branch
    contains less false positives (top row) compared to the labeling produced without
    the spatial transformation branch (bottom row). For instance, instances of soda-can
    were classified as cap (B-E).
  Figure 8 Link: articels_figures_by_rev_year\2017\A_MultiModal_Discriminative_and_Spatially_Invariant_CNN_for_RGBD_Object_Labeling\figure_8.jpg
  Figure 8 caption: Confusion matrices of our model without the regularizations of
    the spatial invariance and Fisher encoding (left), with the regularization of
    only spatial invariance (middle), and with all the regularization (right), on
    the SUNRGND dataset [7]. These results show that there is a performance improvement
    for almost every class when using all the proposed regularizations suggesting
    their importance for indoor scene recognition.
  Figure 9 Link: articels_figures_by_rev_year\2017\A_MultiModal_Discriminative_and_Spatially_Invariant_CNN_for_RGBD_Object_Labeling\figure_9.jpg
  Figure 9 caption: "Qualitative comparison of the labelings produced using FCN [17]\
    \ (C), DCNN [16] (D), our clique potentials (E), and our all three potentials\
    \ (F) for the images shown in (A). The labels produced by FCN are estimated by\
    \ max-pooling over the scores maps shown in (B). Our CRF-potentials ensure a smooth\
    \ and accurate labeling for the cases of \u201Cconfusing classes\u201D compared\
    \ to the results produced by the FCN and DCNN models."
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Umar Asif
  Name of the last author: Ferdous A. Sohel
  Number of Figures: 11
  Number of Tables: 11
  Number of authors: 3
  Paper title: A Multi-Modal, Discriminative and Spatially Invariant CNN for RGB-D
    Object Labeling
  Publication Date: 2017-08-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average Precision and Recall Scores on the WRGBD Scene Dataset
      [8], Computed over the Five Selected Object Categories
  Table 10 caption:
    table_text: TABLE 10 3D Object Labeling Results on the Real-World Scenes Shown
      in Fig. 11
  Table 2 caption:
    table_text: TABLE 2 Comparisons with the State-of-the-Art Methods on the WRGBD
      Object Dataset [7] for Object Category and Instance Recognition
  Table 3 caption:
    table_text: TABLE 3 Scene Recognition Accuracies (%) on the SUNRGBD Dataset [9]
  Table 4 caption:
    table_text: TABLE 4 Scene Recognition Accuracies (%) on the NYU Dataset V2 [10]
  Table 5 caption:
    table_text: TABLE 5 Semantic Segmentation Results on the SUNRGBD Dataset [9]
  Table 6 caption:
    table_text: TABLE 6 Accuracy (%) of Our Model for the Individual Feature Maps
      of Our Image Representation ( Section 4.1) on WRGBD Object Dataset [7] and SUNRGBD
      Dataset [9]
  Table 7 caption:
    table_text: TABLE 7 Accuracy (%) of Our Model Trained with and without the Proposed
      Regularizations ( Section 4.2) on WRGBD Object Dataset [7] and SUNRGBD Dataset
      [9]
  Table 8 caption:
    table_text: TABLE 8 Accuracy (%) of Our Framework Trained with the Proposed Unary
      (U), Object Clique (OCP), and Pairwise (PW) Potentials
  Table 9 caption:
    table_text: TABLE 9 Comparison of Our Model to the CNN-I and the CNN-P Models
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2747134
- Affiliation of the first author: smart systems institute, national university of
    singapore, singapore
  Affiliation of the last author: department of electrical and computer engineering,
    national university of singapore, singapore
  Figure 1 Link: articels_figures_by_rev_year\2017\Image_Visual_Realism_From_Human_Perception_to_Machine_Computation\figure_1.jpg
  Figure 1 caption: Images of different realism levels from the Visual Realism Dataset.
    In each row, the two images on the left are computer generated, whereas the two
    on the right are photos. The number in parentheses represents the realism score
    (the proportion of participants who rated each image as a photo rather than as
    CG).
  Figure 10 Link: articels_figures_by_rev_year\2017\Image_Visual_Realism_From_Human_Perception_to_Machine_Computation\figure_10.jpg
  Figure 10 caption: Test images at different resolutions (a,b,d,e) in the Washington
    3D Scene Dataset. For each pair at a given resolution, the reference photo is
    on the left, and the rendered image is on the right. The 256-pixel resolution
    (c) is created by us for the feature generalizability experiment.
  Figure 2 Link: articels_figures_by_rev_year\2017\Image_Visual_Realism_From_Human_Perception_to_Machine_Computation\figure_2.jpg
  Figure 2 caption: An overview of our framework. First, we created a dataset of both
    real photos and computer graphics (a). Second, we performed psychophysics experiments
    and signal-detection-theory analysis to understand human perception of realism
    (b). We then performed factor analysis to empirically model human perception (c).
    Finally, we designed computer algorithms to predict visual realism (d).
  Figure 3 Link: articels_figures_by_rev_year\2017\Image_Visual_Realism_From_Human_Perception_to_Machine_Computation\figure_3.jpg
  Figure 3 caption: An overview of statistics regarding (a) realism, (b) lighting,
    and (c) scene category for the Visual Realism Dataset. In (a), high realism, medium
    realism, and low realism indicate realism scores between the ranges of (.67, 1]
    , (.33, .67] , [0, .33] , respectively.
  Figure 4 Link: articels_figures_by_rev_year\2017\Image_Visual_Realism_From_Human_Perception_to_Machine_Computation\figure_4.jpg
  Figure 4 caption: "(a) Distribution of human sensitivity on realism judgment ( d\
    \ \u2032 ) across all participants. Higher values of d \u2032 represent higher\
    \ sensitivity. d \u2032 values near zero indicate chance performance. (b) Participants\
    \ with CG experience (gamers, photographers, graphic designers) have higher sensitivity\
    \ than laypersons on realism judgment."
  Figure 5 Link: articels_figures_by_rev_year\2017\Image_Visual_Realism_From_Human_Perception_to_Machine_Computation\figure_5.jpg
  Figure 5 caption: "Results of exploratory and confirmatory factor analysis on Visual\
    \ Realism Dataset. The correlation coefficients between four latent factors (\u201C\
    naturalness\u201D, \u201Cattraction\u201D, \u201Coddness\u201D and \u201Cface\u201D\
    ) and \u201Cvisual realism\u201D are highlighted in bold."
  Figure 6 Link: articels_figures_by_rev_year\2017\Image_Visual_Realism_From_Human_Perception_to_Machine_Computation\figure_6.jpg
  Figure 6 caption: ROC curve of binary image classification of the whole dataset.
    Our computer models (EF-SVM, EF-MLP, VR-CNN) outperform other comparing methods,
    yet still fall short of human performance.
  Figure 7 Link: articels_figures_by_rev_year\2017\Image_Visual_Realism_From_Human_Perception_to_Machine_Computation\figure_7.jpg
  Figure 7 caption: "Evaluation results using sign tests on the Visual Realism Dataset.\
    \ The vertical axis denotes percentages of images whose regression scores are\
    \ within the range of human scores, which means that these are the percentage\
    \ of images for which we can not reject the null hypothesis of no difference between\
    \ humans and predictor. Figures (a)-(e) show results on images with realism score\
    \ between 0\u223C0.2 , 0.2\u223C0.4 , 0.4\u223C0.6 , 0.6\u223C0.8 , 0.8\u223C\
    1 , respectively. Figure (f) shows the results on all images."
  Figure 8 Link: articels_figures_by_rev_year\2017\Image_Visual_Realism_From_Human_Perception_to_Machine_Computation\figure_8.jpg
  Figure 8 caption: Example images predicted by our EF-SVM model. The green and red
    background colors represent correct and false predictions respectively. Our model
    overpredicts realism for images with unusual scenes (e.g., CG character on the
    upper left quadrant), whereas it under-predicts realism for images of common scenes
    but with unusual illumination (e.g., the glass reflectance in the lower right
    image).
  Figure 9 Link: articels_figures_by_rev_year\2017\Image_Visual_Realism_From_Human_Perception_to_Machine_Computation\figure_9.jpg
  Figure 9 caption: "Realism prediction performance ( +\u2212SD ) as a function of\
    \ feature dimension, increased in an incremental manner for the three components:\
    \ attraction (A), naturalness (N), oddness (O) and face (F). Baseline is obtained\
    \ by randomly selecting dimensions from our empirically-based features, CNN-learned\
    \ features, and geometry features."
  First author gender probability: 0.9
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Shaojing Fan
  Name of the last author: Qi Zhao
  Number of Figures: 12
  Number of Tables: 2
  Number of authors: 7
  Paper title: 'Image Visual Realism: From Human Perception to Machine Computation'
  Publication Date: 2017-08-30 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Image Attributes (Attr), Related Survey Items, Attribute\
      \ Categories, and Their Spearman's Rank Correlations ( \u03C1 ) with Ground\
      \ Truth Image Realism Scores (from Psychophysics Study I)"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Experimental Results of Realism Prediction (Regression) and
      Image Type Classification on Visual Realism Dataset
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2747150
- Affiliation of the first author: arlington and tencent ai lab, university of texas,
    arlington, tx
  Affiliation of the last author: arlington and tencent ai lab, university of texas,
    arlington, tx
  Figure 1 Link: articels_figures_by_rev_year\2017\Hierarchical_Sparse_Representation_for_Robust_Image_Registration\figure_1.jpg
  Figure 1 caption: Hierarchical sparse representation of the optimally registered
    images. First we sparsify the image tensor into the gradient tensor (1st stage).
    The sparse error tensor is then separated out in the 2nd stage. The gradient tensor
    with repetitive patterns are sparsified in the frequency domain. Finally we obtain
    an extremely sparse frequency tensor (composed of Fourier coefficients) in the
    3rd stage.
  Figure 10 Link: articels_figures_by_rev_year\2017\Hierarchical_Sparse_Representation_for_Robust_Image_Registration\figure_10.jpg
  Figure 10 caption: "Synthetic experiment with non-rigid transformation: (a) reference\
    \ image, (b) source image with intensity distortion, (c) registration result by\
    \ RC, (d) registration by our method, (e) transformation estimated by RC, and\
    \ (f) transformation estimated by our method. Best viewed in a \xD72 sized color\
    \ pdf file."
  Figure 2 Link: articels_figures_by_rev_year\2017\Hierarchical_Sparse_Representation_for_Robust_Image_Registration\figure_2.jpg
  Figure 2 caption: "A toy registration example with respect to horizontal translation\
    \ using different similarity measures (SSD [2], RC [14], SAD [2], CC [36], CD2\
    \ [9], MS [37], MI [10] and the proposed pair mode). (a) The Lena image ( 128\xD7\
    128 ). (b) A toy Lena image under a severe intensity distortion. Blue curves show\
    \ registration between (a) and (a); red curves show registration between (b) and\
    \ (a)."
  Figure 3 Link: articels_figures_by_rev_year\2017\Hierarchical_Sparse_Representation_for_Robust_Image_Registration\figure_3.jpg
  Figure 3 caption: "Registration results on the \u201CNUTS\u201D data set: (a) the\
    \ original \u201CNUTS\u201D images, (b) the average image of perturbed images,\
    \ (c) the average image by RASL, (d) the average image by t-GRASTA, and (e) the\
    \ average image by our method."
  Figure 4 Link: articels_figures_by_rev_year\2017\Hierarchical_Sparse_Representation_for_Robust_Image_Registration\figure_4.jpg
  Figure 4 caption: 'Batch image registration on the NUTS data sets: (a) low rank
    component by RASL, (b) sparse errors by RASL, (c) subspace representation by t-GRASTA,
    (d) sparse errors by t-GRASTA, (e) visualization of A by our method and (f) sparse
    error E by our method.'
  Figure 5 Link: articels_figures_by_rev_year\2017\Hierarchical_Sparse_Representation_for_Robust_Image_Registration\figure_5.jpg
  Figure 5 caption: (a) An example input of the Multi-PIE image database, (b) STD
    (in degrees) of rotations after registration, (c) STD (in pixels) of X-translation
    after registration, and (d) STD (in pixels) of Y-translation after registration.
  Figure 6 Link: articels_figures_by_rev_year\2017\Hierarchical_Sparse_Representation_for_Robust_Image_Registration\figure_6.jpg
  Figure 6 caption: 'Registration performance comparisons with random transformation
    perturbations and random intensity distortions: (a) intensity RMSE on the Lena
    image and (b) transformation (affine) RMSE on the Lena image.'
  Figure 7 Link: articels_figures_by_rev_year\2017\Hierarchical_Sparse_Representation_for_Robust_Image_Registration\figure_7.jpg
  Figure 7 caption: "Registration of a multispectral image and a panchromatic image:\
    \ (a) reference image, (b) source image, (c) difference in image before registration,\
    \ (d) difference in image by SSD, (e) difference in image by RC, and (f) difference\
    \ in image by our method. Visible misalignments are highlighted by the yellow\
    \ circles. Best viewed in \xD72 sized color pdf file."
  Figure 8 Link: articels_figures_by_rev_year\2017\Hierarchical_Sparse_Representation_for_Robust_Image_Registration\figure_8.jpg
  Figure 8 caption: "Registration of an aerial photograph and a digital orthophoto.\
    \ From left to right, the images are: the reference image, the source image, the\
    \ overlay by MATLAB, the overlay by RC and the overlay by our method. The second\
    \ row shows the zoomed-in areas of streets A and B. Best viewed in \xD72 sized\
    \ color pdf file."
  Figure 9 Link: articels_figures_by_rev_year\2017\Hierarchical_Sparse_Representation_for_Robust_Image_Registration\figure_9.jpg
  Figure 9 caption: 'Face alignment results on the LFW data set [52]. Left to right:
    the input images, the warped results by SSD, RC and HSR, the overlays by SSD,
    RC and HSR.'
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yeqing Li
  Name of the last author: Junzhou Huang
  Number of Figures: 13
  Number of Tables: 2
  Number of authors: 4
  Paper title: Hierarchical Sparse Representation for Robust Image Registration
  Publication Date: 2017-09-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 MeanMax Registration Errors in Pixels of RASL, t-GRASTA and
      Our Method on the Four Lighting Data Sets.
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Unconstrained Face Verification Accuracy on View 1 of LFW
      Using Images Produced by Different Alignment Algorithms
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2748125
- Affiliation of the first author: department of computer science and engineering,
    the chinese university of hong kong, hong kong
  Affiliation of the last author: cooperative medianet innovation center, shanghai
    jiao tong university, shanghai, p.r. china
  Figure 1 Link: articels_figures_by_rev_year\2017\Bilinear_Factor_Matrix_Norm_Minimization_for_Robust_PCA_Algorithms_and_Applicati\figure_1.jpg
  Figure 1 caption: "Sparsity priors. Left: A typical image of video sequences. Middle:\
    \ the sparse component recovered by [6]. Right: the empirical distribution of\
    \ the sparse component (blue solid line), along with a hyper-Laplacian fit with\
    \ \u03B1=12 (green dashdot line) and \u03B1=23 (red dotted line)."
  Figure 10 Link: articels_figures_by_rev_year\2017\Bilinear_Factor_Matrix_Norm_Minimization_for_Robust_PCA_Algorithms_and_Applicati\figure_10.jpg
  Figure 10 caption: Quantitative comparison for different methods in terms of F-measure
    (left) and running time (right, in seconds and in logarithmic scale) on five subsequences
    of surveillance videos.
  Figure 2 Link: articels_figures_by_rev_year\2017\Bilinear_Factor_Matrix_Norm_Minimization_for_Robust_PCA_Algorithms_and_Applicati\figure_2.jpg
  Figure 2 caption: The heavy-tailed empirical distributions (bottom) of the singular
    values of the three channels of these three images (top).
  Figure 3 Link: articels_figures_by_rev_year\2017\Bilinear_Factor_Matrix_Norm_Minimization_for_Robust_PCA_Algorithms_and_Applicati\figure_3.jpg
  Figure 3 caption: "Histograms with the percentage of estimated ranks for Gaussian\
    \ noise and outlier corrupted matrices of size 500\xD7500 (left) and 1,000\xD7\
    1,000 (right), whose true ranks are 10 and 20, respectively."
  Figure 4 Link: articels_figures_by_rev_year\2017\Bilinear_Factor_Matrix_Norm_Minimization_for_Robust_PCA_Algorithms_and_Applicati\figure_4.jpg
  Figure 4 caption: "Phase transition plots for different algorithms on corrupted\
    \ matrices of size 200\xD7200 (top) and 500\xD7500 (bottom). X -axis denotes the\
    \ matrix rank, and Y -axis indicates the corruption ratio. The color magnitude\
    \ indicates the success ratio [0,1] , and a larger red area means a better performance\
    \ of the algorithm (best viewed in colors)."
  Figure 5 Link: articels_figures_by_rev_year\2017\Bilinear_Factor_Matrix_Norm_Minimization_for_Robust_PCA_Algorithms_and_Applicati\figure_5.jpg
  Figure 5 caption: "Comparison of RSE results on corrupted matrices of size 500\xD7\
    500 (left) and 1,000\xD71,000 (right) with different noise factors."
  Figure 6 Link: articels_figures_by_rev_year\2017\Bilinear_Factor_Matrix_Norm_Minimization_for_Robust_PCA_Algorithms_and_Applicati\figure_6.jpg
  Figure 6 caption: Comparison of different methods on corrupted matrices under varying
    outlier and missing ratios in terms of RSE and F-measure.
  Figure 7 Link: articels_figures_by_rev_year\2017\Bilinear_Factor_Matrix_Norm_Minimization_for_Robust_PCA_Algorithms_and_Applicati\figure_7.jpg
  Figure 7 caption: "Comparison of RSE (left) and F-measure (right) of different methods\
    \ on corrupted matrices of size 1,000\xD71,000 versus running time."
  Figure 8 Link: articels_figures_by_rev_year\2017\Bilinear_Factor_Matrix_Norm_Minimization_for_Robust_PCA_Algorithms_and_Applicati\figure_8.jpg
  Figure 8 caption: 'Text removal results of different methods: detected text masks
    (top) and recovered background images (bottom). (a) Input image (top) and original
    image (bottom); (b) RPCA (F-M: 0.9543, RSE: 0.1051); (c) PSVT (F-M: 0.9560, RSE:
    0.1007); (d) WNNM (F-M: 0.9536, RSE: 0.0943); (e) Unifying (F-M: 0.9584, RSE:
    0.0976); (f) LpSq (F-M: 0.9665, RSE: 0.1097); (g) (S+L ) 12 (F-M: 0.9905, RSE:
    0.0396); and (h) (S+L ) 23 (F-M: 0.9872, RSE: 0.0463).'
  Figure 9 Link: articels_figures_by_rev_year\2017\Bilinear_Factor_Matrix_Norm_Minimization_for_Robust_PCA_Algorithms_and_Applicati\figure_9.jpg
  Figure 9 caption: Background and foreground separation results of different algorithms
    on the Bootstrap data set. The one frame with missing data of each sequence (top)
    and its manual segmentation (bottom) are shown in (a). The results of different
    algorithms are presented from (b) to (f), respectively. The top panel is the recovered
    background, and the bottom panel is the segmentation.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.64
  Name of the first author: Fanhua Shang
  Name of the last author: Zhouchen Lin
  Number of Figures: 13
  Number of Tables: 3
  Number of authors: 5
  Paper title: 'Bilinear Factor Matrix Norm Minimization for Robust PCA: Algorithms
    and Applications'
  Publication Date: 2017-09-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Norms of Sparse and Low-Rank Matrices
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Various RPCA Models and Their Properties
  Table 3 caption:
    table_text: TABLE 3 Comparison of Average RSE, F-M and Time (Seconds) on Corrupted
      Matrices
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2748590
- Affiliation of the first author: personal robotics lab, imperial college london,
    london, united kingdom
  Affiliation of the last author: personal robotics lab, imperial college london,
    london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2017\Highly_Articulated_Kinematic_Structure_Estimation_Combining_Motion_and_Skeleton_\figure_1.jpg
  Figure 1 caption: The proposed framework reliably learns the underlying kinematic
    structure of highly articulated objects without any object model required. From
    an image sequence, we extract dense 2D feature point trajectories. From the trajectories,
    we adaptively learn motion segments and the skeleton distance map in parallel.
    The rigid body parts are segmented out intentionally via an iterative fine-to-coarse
    motion segmentation because we assume that we have no prior knowledge about the
    number of segments. The skeleton distance map is generated based on an adaptively
    estimated object silhouette with a new optimal parameter selection in order to
    relate the object shape to the skeleton information. We finally estimate an accurate
    kinematic structure implying both kinematic correlation and skeletal topology
    followed by a structure refining step which merges the over-segmented parts guided
    by the skeleton distance map.
  Figure 10 Link: articels_figures_by_rev_year\2017\Highly_Articulated_Kinematic_Structure_Estimation_Combining_Motion_and_Skeleton_\figure_10.jpg
  Figure 10 caption: "Average values of metrics for all data sequences. \u2217 ( p-value<0.05\
    \ ) and \u2217\u2217 ( p-value<0.005 ) indicate statistically significant difference\
    \ with the proposed method. The proposed method achieves better accuracies than\
    \ the other methods."
  Figure 2 Link: articels_figures_by_rev_year\2017\Highly_Articulated_Kinematic_Structure_Estimation_Combining_Motion_and_Skeleton_\figure_2.jpg
  Figure 2 caption: "Object boundary generation results with various kernel parameters.\
    \ A small parameter value produces over-estimated results with separated boundary\
    \ regions, and a large value gives an under-estimated boundary result. The distributions\
    \ of the sample margin \u03B3 are shown in the middle with respect to each kernel\
    \ value. As we can see, the most appropriate boundary is generated by the kernel\
    \ value which results in the maximum entropy."
  Figure 3 Link: articels_figures_by_rev_year\2017\Highly_Articulated_Kinematic_Structure_Estimation_Combining_Motion_and_Skeleton_\figure_3.jpg
  Figure 3 caption: "Comparison between skeleton maps. (a) Puppet image (b) Generated\
    \ object boundary \u03B4 \u03A9 f from feature points X f (yellow dots) (c) The\
    \ skeleton distance map \u03A0 f . The arms and legs are not well-separated and\
    \ detailed shape information is lost. (d) The density map D f . (e) The density\
    \ weighted skeleton map \u03A8 f . By re-weighting the skeleton distance map according\
    \ to the density map, detailed shape characteristics as well as skeleton distance\
    \ information can be retained. Figure best viewed in colour."
  Figure 4 Link: articels_figures_by_rev_year\2017\Highly_Articulated_Kinematic_Structure_Estimation_Combining_Motion_and_Skeleton_\figure_4.jpg
  Figure 4 caption: "The white dotted line shows the geodesic distance \u03B6 between\
    \ two points, and the green solid line shows the euclidean distance. The black\
    \ solid line is the skeleton of the object. The geodesic distance represents the\
    \ minimum distance following the skeleton. Figure best viewed in colour."
  Figure 5 Link: articels_figures_by_rev_year\2017\Highly_Articulated_Kinematic_Structure_Estimation_Combining_Motion_and_Skeleton_\figure_5.jpg
  Figure 5 caption: Refining process for removing over-segmentation by iterative segments
    merging guided by skeleton information. Best viewed in colour.
  Figure 6 Link: articels_figures_by_rev_year\2017\Highly_Articulated_Kinematic_Structure_Estimation_Combining_Motion_and_Skeleton_\figure_6.jpg
  Figure 6 caption: The number of motion segments converges closely to the ground
    truth using the proposed iterative process.
  Figure 7 Link: articels_figures_by_rev_year\2017\Highly_Articulated_Kinematic_Structure_Estimation_Combining_Motion_and_Skeleton_\figure_7.jpg
  Figure 7 caption: "(a) The entropy value distribution over frames. The optimal kernel\
    \ values for the 'Baxter' sequence over frames are 182.3(\xB17.4) . The selected\
    \ kernel values are relatively consistent while the object moves. (b) Test data\
    \ and artificially generated outliers for the 'Baxter' sequence. (c) The loss\
    \ function result of 'Baxter' sequence. The kernel parameter with the minimum\
    \ loss function value (200.69) is the best selected values for each sequence by\
    \ [44]. Note that the false positive rate are calculated based on artificial outliers."
  Figure 8 Link: articels_figures_by_rev_year\2017\Highly_Articulated_Kinematic_Structure_Estimation_Combining_Motion_and_Skeleton_\figure_8.jpg
  Figure 8 caption: "Comparisons of the moving object boundary generation of various\
    \ approaches. (a) One frame of the 'iCub body' sequence. (b) Canny edge detection\
    \ results from a grey image. (c) Using only static RGB cues and the learning approach\
    \ based edge detector [39]. Internal edges and background object edges are detected\
    \ as well. (d) State-of-the-art motion boundary estimation [40] using RGB + optical\
    \ flow cues. (e) Clustering dense point trajectories [23]. (f) Proposed method's\
    \ boundary generation result. (g) The density weighted skeleton map \u03A8 f using\
    \ the generated boundary((e)) of the dense point trajectories [23]. (h) The density\
    \ weighted skeleton map \u03A8 f using the proposed boundary generation result\
    \ ((h))."
  Figure 9 Link: articels_figures_by_rev_year\2017\Highly_Articulated_Kinematic_Structure_Estimation_Combining_Motion_and_Skeleton_\figure_9.jpg
  Figure 9 caption: Comparison with other kinematic structure estimation methods.
    All values are based on one hundred trials except for probabilistic method [24]
    (ten trials instead of one hundred as each run takes too much time), and MoSeg
    Sparse [38] and energy based method [8] (results are consistent). The number in
    parentheses of MoSeg [38] indicates a sampling rate.
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Hyung Jin Chang
  Name of the last author: Yiannis Demiris
  Number of Figures: 12
  Number of Tables: 4
  Number of authors: 2
  Paper title: Highly Articulated Kinematic Structure Estimation Combining Motion
    and Skeleton Information
  Publication Date: 2017-09-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Properties of the Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Selected Kernel Parameter Values and Time Requirements
      for Various Methods
  Table 3 caption:
    table_text: 'TABLE 3 Acronyms are P: Average Precision, R: Average Recall, and
      F: F-Measure as Described in [38]'
  Table 4 caption:
    table_text: TABLE 4 Summary of the Kinematic Structure Estimation Algorithms
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2748579
- Affiliation of the first author: "inria d\xE9partement d'informatique de l'ens,\
    \ cnrs, psl research university, paris, france"
  Affiliation of the last author: "cs & or department, universit\xE9 de montr\xE9\
    al, montr\xE9al, quebec, canada"
  Figure 1 Link: articels_figures_by_rev_year\2017\Learning_from_Narrated_Instruction_Videos\figure_1.jpg
  Figure 1 caption: Given a set of narrated instruction videos demonstrating a particular
    task, we wish to automatically discover main steps to achieve the task and to
    associate each step with its corresponding narration and a temporal interval in
    each video. Here two videos of changing a car tire are illustrated by corresponding
    frames and excerpts of narrations. Steps of the same type are highlighted by the
    same color. Note the large variations in narrations and appearance of corresponding
    steps across videos.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Learning_from_Narrated_Instruction_Videos\figure_2.jpg
  Figure 2 caption: 'Illustration of our newly collected dataset of instructions videos.
    Examples of transcribed narrations together with still frames from the corresponding
    videos are shown for two (out of 5) tasks: Changing car tire and Making coffee.
    The dataset contains challenging real-world videos performed by many different
    people, captured in uncontrolled settings in a variety of outdoor and indoor environments.
    Note the large variability of verbal expressions and the terminology in the transcribed
    narrations as well as the large variability of visual appearance due to viewpoint,
    used objects, and actions performed in different manner. See our project webpage
    [2] for more examples.'
  Figure 3 Link: articels_figures_by_rev_year\2017\Learning_from_Narrated_Instruction_Videos\figure_3.jpg
  Figure 3 caption: "Clustering transcribed verbal instructions. Left: The input raw\
    \ text for each video is converted into a sequence of direct object relations.\
    \ Here, an illustration of four sequences from four different videos is shown.\
    \ Middle: Multiple sequence alignment is used to align all sequences together.\
    \ Note that different direct object relations are aligned together as long as\
    \ they have the same sense, e.g., \u201Cloosen nut\u201D and \u201Cundo bolt\u201D\
    . Right: The main instruction steps are extracted as the K=3 most common steps\
    \ in all the sequences."
  Figure 4 Link: articels_figures_by_rev_year\2017\Learning_from_Narrated_Instruction_Videos\figure_4.jpg
  Figure 4 caption: Illustration of the dynamic programming solution to the linear
    program (11). (a) shows an example cost matrix tildeC and the corresponding optimal
    path in red. The gray entries in the matrix tildeC correspond to the values from
    the original cost matrix C (see text). The white entries have minimal cost and
    are thus always preferred over any gray entry. The orange entries have maximal
    cost, e.g., infty , and correspond to text constraints (type I). These constraints
    reduce the number of possible paths. The obtained latent variable solution Z is
    displayed in (b). Red and white entries respectively correspond to the value 1
    and 0.
  Figure 5 Link: articels_figures_by_rev_year\2017\Learning_from_Narrated_Instruction_Videos\figure_5.jpg
  Figure 5 caption: Results for temporally localizing recovered steps in the input
    videos. We give in bold the number of ground truth steps.
  Figure 6 Link: articels_figures_by_rev_year\2017\Learning_from_Narrated_Instruction_Videos\figure_6.jpg
  Figure 6 caption: Examples of three recovered instruction steps for each of the
    five tasks in our dataset. For each step, we first show clustered direct object
    relations, followed by representative example frames localizing the step in the
    videos. Correct localizations are shown in green. Some steps are incorrectly localized
    in some videos (red), but often look visually very similar.
  Figure 7 Link: articels_figures_by_rev_year\2017\Learning_from_Narrated_Instruction_Videos\figure_7.jpg
  Figure 7 caption: Sensitivity of the performance (measured by the F1 score) to the
    different hyperparameters.
  Figure 8 Link: articels_figures_by_rev_year\2017\Learning_from_Narrated_Instruction_Videos\figure_8.jpg
  Figure 8 caption: Analysis of constraints extracted from transcribed narrations.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jean-Baptiste Alayrac
  Name of the last author: Simon Lacoste-Julien
  Number of Figures: 8
  Number of Tables: 3
  Number of authors: 6
  Paper title: Learning from Narrated Instruction Videos
  Publication Date: 2017-09-05 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Statistics of the Newly Collected Instruction Video Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Automatically Recovered Sequences of Steps for the Five Tasks
      Considered in This Work
  Table 3 caption:
    table_text: TABLE 3 Comparison of Different Optimization Approaches for Solving
      Problem (6)
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2749223
- Affiliation of the first author: school of electrical and electronic engineering,
    nanyang technological university, singapore, singapore
  Affiliation of the last author: school of electrical and electronic engineering,
    nanyang technological university, singapore, singapore
  Figure 1 Link: articels_figures_by_rev_year\2017\Sharable_and_Individual_MultiView_Metric_Learning\figure_1.jpg
  Figure 1 caption: "Illustration of the neural network architectures adopted by the\
    \ proposed MvDML method. For a sample x i , x 1,i and x 2,i are its inputs to\
    \ networks in two views, respectively. In the network of view \u03BA , x (m) \u03BA\
    ,i is the output of the layer m , and W (m) \u03BA and b (m) \u03BA are the weights\
    \ and biases to be learned, m=1,2 and \u03BA=1,2 . At the top layer of each network,\
    \ the outputs x (2) \u03BA,i is projected by W s \u03BA and W c \u03BA to preserve\
    \ the specific representation h s \u03BA,i of the view \u03BA and to share a common\
    \ representation h c i of two views, respectively. The resulting representation\
    \ of x i is stacked as h i =[ h s 1,i \u22A4 , h s 2,i \u22A4 , h c i \u22A4 ]\
    \ \u22A4 via these two networks."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Sharable_and_Individual_MultiView_Metric_Learning\figure_2.jpg
  Figure 2 caption: ROC curves of several methods on LFW dataset under image-restricted,
    label-free outside data setting.
  Figure 3 Link: articels_figures_by_rev_year\2017\Sharable_and_Individual_MultiView_Metric_Learning\figure_3.jpg
  Figure 3 caption: Convergence curves of our methods on LFW dataset.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.68
  Name of the first author: Junlin Hu
  Name of the last author: Yap-Peng Tan
  Number of Figures: 3
  Number of Tables: 7
  Number of authors: 3
  Paper title: Sharable and Individual Multi-View Metric Learning
  Publication Date: 2017-09-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Mean Verification Accuracy (%) and Standard Error (%) of Baseline
      and Proposed Methods on LFW Dataset Under Image-Restricted, Label-Free Outside
      Data Setting
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Image-Restricted, Label-Free Outside Data Results on LFW Dataset
      When HDLBP Is Added to MvML and MvDML
  Table 3 caption:
    table_text: TABLE 3 Comparison with State-of-the-Art Methods on LFW Dataset Under
      Image-Restricted, Label-Free Outside Data Setting
  Table 4 caption:
    table_text: TABLE 4 Mean Verification Accuracy (%) of Baseline and Proposed Methods
      on KinFaceW-II Dataset, Where Con. Means Concatenation of All Features
  Table 5 caption:
    table_text: TABLE 5 Comparison (%) of Several Multiple Metric Learning Methods
      on KinFaceW-II Dataset
  Table 6 caption:
    table_text: TABLE 6 Top Ranked Matching Rate (%) of Baseline and Proposed Methods
      on VIPeR Dataset with p=316 Testing Persons, Where Con. Means Concatenation
      of All Features
  Table 7 caption:
    table_text: TABLE 7 Comparison with State-of-the-Art Methods in Top Ranked Matching
      Rate (%) on VIPeR Dataset with p=316 Testing Persons
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2749576
- Affiliation of the first author: signal processing and speech communication laboratory,
    graz university of technology, graz, austria
  Affiliation of the last author: signal processing and speech communication laboratory,
    graz university of technology, graz, austria
  Figure 1 Link: articels_figures_by_rev_year\2017\Fixed_Points_of_Belief_PropagationAn_Analysis_via_Polynomial_Homotopy_Continuati\figure_1.jpg
  Figure 1 caption: (a) Polytope S 1 , (b) polytope S 2 , and (c) Minkowski sum S
    1 + S 2 .
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Fixed_Points_of_Belief_PropagationAn_Analysis_via_Polynomial_Homotopy_Continuati\figure_2.jpg
  Figure 2 caption: 'Structure of Ising graphs considered: (a) grid graph with 9 RVs;
    (b) fully connected graph.'
  Figure 3 Link: articels_figures_by_rev_year\2017\Fixed_Points_of_Belief_PropagationAn_Analysis_via_Polynomial_Homotopy_Continuati\figure_3.jpg
  Figure 3 caption: "Number of fixed points on the grid graph of size N=3\xD73 : (a)\
    \ number of positive real fixed points (yellow: unique fixed point, red: three\
    \ fixed points); (b) number of real fixed points. The increase in number of both\
    \ real solutions and positive real solutions, indicates a phase transition."
  Figure 4 Link: articels_figures_by_rev_year\2017\Fixed_Points_of_Belief_PropagationAn_Analysis_via_Polynomial_Homotopy_Continuati\figure_4.jpg
  Figure 4 caption: "Fully connected graph with N=2\xD72 . (a) Convergence of BP:\
    \ for the blue region BP did convergence\u2013for the red region it did not converge\
    \ after 4\u22C5 10 5 iterations. (b) Number of fixed points (yellow: unique fixed\
    \ point, red: three fixed points. (c) Number of real solutions\u2013note the sudden\
    \ increase at the onset of phase transitions."
  Figure 5 Link: articels_figures_by_rev_year\2017\Fixed_Points_of_Belief_PropagationAn_Analysis_via_Polynomial_Homotopy_Continuati\figure_5.jpg
  Figure 5 caption: 'Results for the grid graph of size N=3times 3 ; mean magnetization
    langle m rangle and tildelangle m rangle for J in [-2,2] and for: (a) theta =0
    , (c) theta =0.1 , and (d) theta =0.5 . The exact solution is illustrated in red.
    All fixed points obtained by NPHC are depicted by blue dots (stable) and by green
    dots (unstable). The fixed point maximizing the partition function is illustrated
    in black.'
  Figure 6 Link: articels_figures_by_rev_year\2017\Fixed_Points_of_Belief_PropagationAn_Analysis_via_Polynomial_Homotopy_Continuati\figure_6.jpg
  Figure 6 caption: 'Results for the fully connected graph of size N=2times 2 ; mean
    magnetization langle m rangle and tildelangle m rangle for J in [-2,2] and for:
    (a) theta =0 , (b) theta =0.1 , and (c) theta =0.5 . The exact solution is illustrated
    in red. All fixed points obtained by NPHC are depicted by blue dots (stable) and
    by green dots (unstable). The fixed point maximizing the partition function is
    illustrated in black.'
  Figure 7 Link: articels_figures_by_rev_year\2017\Fixed_Points_of_Belief_PropagationAn_Analysis_via_Polynomial_Homotopy_Continuati\figure_7.jpg
  Figure 7 caption: (a) Binary symmetric channel with error probability epsilon ;
    (b) Factor graph for the (7,4) Hamming code that corresponds to (31) where Y1
    is flipped.
  Figure 8 Link: articels_figures_by_rev_year\2017\Fixed_Points_of_Belief_PropagationAn_Analysis_via_Polynomial_Homotopy_Continuati\figure_8.jpg
  Figure 8 caption: 'Results for the (7,4) Hamming code. We compare the exact solution
    P(Xi = 0|mathbf Y=mathbf y) (red) to the approximate solution tildeP(Xi = 0|mathbf
    Y=mathbf y) of NPHC (blue) as epsilon increases for: (a) Y1 is flipped and (b)
    Y6 is flipped.'
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Christian Knoll
  Name of the last author: Franz Pernkopf
  Number of Figures: 8
  Number of Tables: 3
  Number of authors: 4
  Paper title: "Fixed Points of Belief Propagation\u2014An Analysis via Polynomial\
    \ Homotopy Continuation"
  Publication Date: 2017-09-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 MSE of Marginals and Combined Marginals Obtained by BP and
      NPHC for the Grid Graph with Uniform Factors
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 MSE of Marginals and Combined Marginals Obtained by BP and
      NPHC for the Fully Connected Graph With Uniform Factors
  Table 3 caption:
    table_text: TABLE 3 Runtime Comparison between BP and NPHC (in Seconds)
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2749575
- Affiliation of the first author: azm center-edst, lebanese university, tripoli,
    lebanon
  Affiliation of the last author: "universit\xE9 de rennes 1, rennes, france"
  Figure 1 Link: articels_figures_by_rev_year\2017\SimiNet_A_Novel_Method_for_Quantifying_Brain_Network_Similarity\figure_1.jpg
  Figure 1 caption: 'Illustration of SimiNet algorithm steps. A) Finding the similarity
    index between G 1 and G 2 . B) Step1-i: detection of nodes common to G 1 and G
    2 . B) Step1-ii: shifting the nearest neighbors located in the defined spatial
    neighborhood (disk with radius R = 1.5), shifting cost equals 1. C) Step 2: insertion
    of new nodes when no neighbor is found within the neighborhood (cost of insertion
    = R = 1.5). D) Step 3: deletion of remaining nodes of G 1 (deletion cost = insertion
    cost = 1.5). E) Step 4: computing the edge distance between G 1 and G 2 . F) Final
    state: G 1 matches G 2 and d( G 1 , G 2 ) = ND + ED = 14.5.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\SimiNet_A_Novel_Method_for_Quantifying_Brain_Network_Similarity\figure_2.jpg
  Figure 2 caption: Pseudo-code of the proposed SimiNet algorithm.
  Figure 3 Link: articels_figures_by_rev_year\2017\SimiNet_A_Novel_Method_for_Quantifying_Brain_Network_Similarity\figure_3.jpg
  Figure 3 caption: 'Variation of similarity indexes computed from the 5 algorithms
    under evaluation with respect to three types of graph alterations: A) Increase
    of edge weights. B) Insertion of nodes C) Shifts in the spatial location of nodes.
    The dark color represents the average value of the similarity measures while the
    shadowed area represents the standard deviation.'
  Figure 4 Link: articels_figures_by_rev_year\2017\SimiNet_A_Novel_Method_for_Quantifying_Brain_Network_Similarity\figure_4.jpg
  Figure 4 caption: 'Variation of four graph kernel algorithms under evaluation with
    respect to three types of graph alterations: A) Increase of edge weights. B) Insertion
    of nodes C) Shifts in the spatial location of nodes. The dark color represents
    the average value of the similarity measures while the shadowed area represents
    the standard deviation.'
  Figure 5 Link: articels_figures_by_rev_year\2017\SimiNet_A_Novel_Method_for_Quantifying_Brain_Network_Similarity\figure_5.jpg
  Figure 5 caption: 'A- Inter-subject variability of the similarity index ( si m SimiNet
    ( G 1 , G 2 ) ) on real brain networks identified from EEG where G 1 and G 2 represent
    respectively the connectivity graphs of the subjects during tools and animals
    picture naming. Left: value of connectivity graphs in the first period (1-119ms).
    Right: similarity values of connectivity graphs during the second period (151-190ms).
    B- The connectivity graphs: 3D representation for 2 different subjects. C- Boxplots
    show significant difference of similarity values between the two first periods
    of the cognitive process using SimiNet. Networks were obtained and visualized
    using EEGNET [44].'
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Ahmad Mheich
  Name of the last author: Fabrice Wendling
  Number of Figures: 5
  Number of Tables: 1
  Number of authors: 6
  Paper title: 'SimiNet: A Novel Method for Quantifying Brain Network Similarity'
  Publication Date: 2017-09-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Description of the Notations Used in This Paper
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2750160
