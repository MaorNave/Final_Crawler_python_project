- Affiliation of the first author: beijing national research center for information
    science and technology (bnrist) and the department of automation, tsinghua university,
    beijing, china
  Affiliation of the last author: beijing national research center for information
    science and technology (bnrist) and the department of automation, tsinghua university,
    beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\StructurePreserving_Image_SuperResolution\figure_1.jpg
  Figure 1 caption: SR results of different methods. RCAN represents PSNR-oriented
    methods, typically generating straight but blurry edges for the bricks. Perceptual-driven
    methods including SRGAN, ESRGAN, and NatSR commonly recover sharper but geometric-inconsistent
    textures. Our SPSR result is sharper than that of RCAN, and preserves finer geometric
    structures compared with perceptual-driven methods. Best viewed on screen.
  Figure 10 Link: articels_figures_by_rev_year\2021\StructurePreserving_Image_SuperResolution\figure_10.jpg
  Figure 10 caption: Visualization of gradient maps (im073 from General100). The HR
    gradient map has thin outlines while those in the LR gradient map are thick. Our
    gradient branch is able to recover HR gradient maps with pleasant structures.
  Figure 2 Link: articels_figures_by_rev_year\2021\StructurePreserving_Image_SuperResolution\figure_2.jpg
  Figure 2 caption: Comparison of SPSR with gradient guidance and SPSR with self-supervised
    neural structure extractor. The former one imposes constraints on the gradient
    maps (GM) of super-resolved images by hand-crafted operators. The latter one learns
    a powerful neural structure extractor by self-supervised structure learning methods.
    Additional supervision is provided by reducing the distances of the extracted
    structure features (SF) for SR results and the corresponding HR images.
  Figure 3 Link: articels_figures_by_rev_year\2021\StructurePreserving_Image_SuperResolution\figure_3.jpg
  Figure 3 caption: Overall framework of our SPSR-G method. Our architecture consists
    of two branches, the SR branch and the gradient branch. The gradient branch aims
    to super-resolve LR gradient maps to the HR counterparts. It incorporates multi-level
    representations from the SR branch to reduce parameters and outputs gradient information
    to guide the SR process by a fusion block in return. The final SR outputs are
    optimized by not only conventional image-space losses, but also the proposed gradient-space
    objectives.
  Figure 4 Link: articels_figures_by_rev_year\2021\StructurePreserving_Image_SuperResolution\figure_4.jpg
  Figure 4 caption: Illumination of a simple 1-D case. The first row shows the pixel
    sequences and the second row shows the corresponding gradient maps.
  Figure 5 Link: articels_figures_by_rev_year\2021\StructurePreserving_Image_SuperResolution\figure_5.jpg
  Figure 5 caption: Illustration of the self-supervised structure learning schemes
    for training neural structure extractors. The top figure shows the flowchart of
    learning by contrastive prediction while the bottom one is by solving jigsaw puzzles.
  Figure 6 Link: articels_figures_by_rev_year\2021\StructurePreserving_Image_SuperResolution\figure_6.jpg
  Figure 6 caption: Validation results of the obtained NSEs on the two self-supervised
    structure learning tasks.
  Figure 7 Link: articels_figures_by_rev_year\2021\StructurePreserving_Image_SuperResolution\figure_7.jpg
  Figure 7 caption: Visual comparison with state-of-the-art SR methods. The results
    show that our proposed SPSR-G and SPSR-P methods significantly outperform other
    methods in structure restoration while generating perceptual pleasant SR images.
    Best viewed on screen.
  Figure 8 Link: articels_figures_by_rev_year\2021\StructurePreserving_Image_SuperResolution\figure_8.jpg
  Figure 8 caption: Comparison of SR results and gradient maps with state-of-the-art
    SR methods. The proposed SPSR methods can better preserve gradients and structures.
    Best viewed on screen.
  Figure 9 Link: articels_figures_by_rev_year\2021\StructurePreserving_Image_SuperResolution\figure_9.jpg
  Figure 9 caption: User study results of different GAN-based SR methods. Our SPSR-P
    and SPSR-G methods outperform state-of-the-art SR methods in generating high-quality
    images.
  First author gender probability: 0.97
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.59
  Name of the first author: Cheng Ma
  Name of the last author: Jie Zhou
  Number of Figures: 11
  Number of Tables: 4
  Number of authors: 4
  Paper title: Structure-Preserving Image Super-Resolution
  Publication Date: 2021-09-22 00:00:00
  Table 1 caption: TABLE 1 Comparison With State-of-the-Art SR Methods on Benchmark
    Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison of SPSR-G Models With Different Components
  Table 3 caption: TABLE 3 Comparison of SPSR-P Models With Different Components
  Table 4 caption: TABLE 4 Experimental Results of Parameter Sensitivity for the SPSR-P
    Model
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3114428
- Affiliation of the first author: college of computer science and technology, zhejiang
    university, hangzhou, china
  Affiliation of the last author: college of computer science and technology and alibaba-zhejiang
    university joint research institute of frontier technologies, zhejiang university,
    hangzhou, china
  Figure 1 Link: articels_figures_by_rev_year\2021\SeekandHide_Adversarial_Steganography_via_Deep_Reinforcement_Learning\figure_1.jpg
  Figure 1 caption: Performance of the image-to-image steganography. C denotes the
    Container image, R denotes the Revealed image and RD denotes the Residual Difference
    of the container image and the cover image.
  Figure 10 Link: articels_figures_by_rev_year\2021\SeekandHide_Adversarial_Steganography_via_Deep_Reinforcement_Learning\figure_10.jpg
  Figure 10 caption: Comparisons of the residual results between [1] and our method
    for global steganography. Our steganography method based on adversarial examples
    consistently reveals less visually-interpretable cues.
  Figure 2 Link: articels_figures_by_rev_year\2021\SeekandHide_Adversarial_Steganography_via_Deep_Reinforcement_Learning\figure_2.jpg
  Figure 2 caption: The basic flow of traditional image steganography.
  Figure 3 Link: articels_figures_by_rev_year\2021\SeekandHide_Adversarial_Steganography_via_Deep_Reinforcement_Learning\figure_3.jpg
  Figure 3 caption: 'The framework of the proposed AdaSteg system. The model consists
    of three modules: the concealing-revealing module, the patch selection module
    and the detector.'
  Figure 4 Link: articels_figures_by_rev_year\2021\SeekandHide_Adversarial_Steganography_via_Deep_Reinforcement_Learning\figure_4.jpg
  Figure 4 caption: Comparative results of hiding secret images in different patches.
    The MSE of the container image is computed on a patch level. The red boxes in
    the first column indicate the positions of local patches.
  Figure 5 Link: articels_figures_by_rev_year\2021\SeekandHide_Adversarial_Steganography_via_Deep_Reinforcement_Learning\figure_5.jpg
  Figure 5 caption: Overview of the proposed adaptive patch selection algorithm based
    on deep reinforcement learning.
  Figure 6 Link: articels_figures_by_rev_year\2021\SeekandHide_Adversarial_Steganography_via_Deep_Reinforcement_Learning\figure_6.jpg
  Figure 6 caption: Network architecture of the proposed policy network and the steganalysis
    detector. The red box represents the selected local patch at the current time
    step, and the green box represents the context of the local patch.
  Figure 7 Link: articels_figures_by_rev_year\2021\SeekandHide_Adversarial_Steganography_via_Deep_Reinforcement_Learning\figure_7.jpg
  Figure 7 caption: An example of the intermediate DRL behavior. The orange frame
    represents the initial ROI, and the green frame represents the final ROI.
  Figure 8 Link: articels_figures_by_rev_year\2021\SeekandHide_Adversarial_Steganography_via_Deep_Reinforcement_Learning\figure_8.jpg
  Figure 8 caption: An overview of our network architecture for secret encryption.
    The concealing stage is denoted as orange arrows, and the revealing stage is denoted
    as blue arrows.
  Figure 9 Link: articels_figures_by_rev_year\2021\SeekandHide_Adversarial_Steganography_via_Deep_Reinforcement_Learning\figure_9.jpg
  Figure 9 caption: Comparisons of the residual results between using one single fixed
    network and the online one.
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.67
  Name of the first author: Wenwen Pan
  Name of the last author: Mingli Song
  Number of Figures: 15
  Number of Tables: 10
  Number of authors: 5
  Paper title: 'Seek-and-Hide: Adversarial Steganography via Deep Reinforcement Learning'
  Publication Date: 2021-09-22 00:00:00
  Table 1 caption: TABLE 1 Detailed Descriptions of the Proposed Transformation Actions
  Table 10 caption: "TABLE 10 Results of [1] and Our Approach With Different \u03B1\
    \ \u03B1 on ImageNet (S) & Open Images (C) (10 Random Seeds)"
  Table 2 caption: TABLE 2 Quantitative Results of Different Patch Selections for
    Local Steganography
  Table 3 caption: TABLE 3 The Detailed Hyperparameters for DQN
  Table 4 caption: "TABLE 4 Quantitative Results of Different Hyperparameters for\
    \ Discount Factor( \u03B3 \u03B3) in Local Steganography"
  Table 5 caption: TABLE 5 Quantitative Results of Different Quality Function(VOC)
    for Local Steganography
  Table 6 caption: TABLE 6 Different Initial Patches for Local Steganography
  Table 7 caption: TABLE 7 Quantitative Results of the Balujas and Our Steganography
    for Global Steganography
  Table 8 caption: "TABLE 8 Quantitative Results of the Different \u03B1 \u03B1 for\
    \ Global Steganography"
  Table 9 caption: TABLE 9 Quantitative Results of Different Normalization Methods
    for Global Steganography
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3114555
- Affiliation of the first author: department of electronic engineering, shenzhen
    international graduate school, tsinghua university, beijing, china
  Affiliation of the last author: department of electronic engineering, beijing national
    research center for information science and technology, institute for brain and
    cognitive science, tsinghua university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\GigaMVS_A_Benchmark_for_UltraLargeScale_GigapixelLevel_D_Reconstruction\figure_1.jpg
  Figure 1 caption: The ruins of the great fountain in the Old Summer Palace, which
    is around 7200 m 2 . The ground-truth 3D model with camera trajectories are provided,
    associated with 1326 million points and 800 gigapixel-level images. The red boxes
    indicate the camera positions in the space.
  Figure 10 Link: articels_figures_by_rev_year\2021\GigaMVS_A_Benchmark_for_UltraLargeScale_GigapixelLevel_D_Reconstruction\figure_10.jpg
  Figure 10 caption: 'Reconstructed point clouds using state-of-the-art methods: R-MVSNet,
    CasMVSNet, COLMAP. The reference images are captured by the camera directly.'
  Figure 2 Link: articels_figures_by_rev_year\2021\GigaMVS_A_Benchmark_for_UltraLargeScale_GigapixelLevel_D_Reconstruction\figure_2.jpg
  Figure 2 caption: Illustration of the high-resolution (gigapixel-level) property
    in our GigaMVS benchmark. The captured gigapixel-level images are with both wide-FoV
    and high-resolution, supporting highly multi-scale observations, such as the Palace-scale
    scenes and Relievo-scale local details.
  Figure 3 Link: articels_figures_by_rev_year\2021\GigaMVS_A_Benchmark_for_UltraLargeScale_GigapixelLevel_D_Reconstruction\figure_3.jpg
  Figure 3 caption: "Comparison of COLMAP reconstruction results using (a) \u201C\
    the captured standard images\u201D and (b) \u201Cthe stitched gigapixel images\u201D\
    \ as inputs."
  Figure 4 Link: articels_figures_by_rev_year\2021\GigaMVS_A_Benchmark_for_UltraLargeScale_GigapixelLevel_D_Reconstruction\figure_4.jpg
  Figure 4 caption: Illustration of the artifacts existed in the scanned point cloud
    data, including the points reflected by the mirrorglasses, dynamic objects such
    as walking people etc. These artifacts marked by red points have been removed
    by post-processing.
  Figure 5 Link: articels_figures_by_rev_year\2021\GigaMVS_A_Benchmark_for_UltraLargeScale_GigapixelLevel_D_Reconstruction\figure_5.jpg
  Figure 5 caption: (a) Illustration of the calibrated camera poses of all the viewpoints.
    The displacements among the multiple images used to generate the high-resolution
    texture are very small relative to the distant scene, which can be treated as
    monocentric. (b) The stitched gigapixel-level image.
  Figure 6 Link: articels_figures_by_rev_year\2021\GigaMVS_A_Benchmark_for_UltraLargeScale_GigapixelLevel_D_Reconstruction\figure_6.jpg
  Figure 6 caption: Comparison of stitched images via GigaPan solution and ours.
  Figure 7 Link: articels_figures_by_rev_year\2021\GigaMVS_A_Benchmark_for_UltraLargeScale_GigapixelLevel_D_Reconstruction\figure_7.jpg
  Figure 7 caption: Reconstructed point clouds from the stitched images using GigaPan
    solution and ours.
  Figure 8 Link: articels_figures_by_rev_year\2021\GigaMVS_A_Benchmark_for_UltraLargeScale_GigapixelLevel_D_Reconstruction\figure_8.jpg
  Figure 8 caption: Illustration of the registration between geometry and texture.
    The point clouds are overlaid on the corresponding images, where the sharp edges
    indicate the good matching results.
  Figure 9 Link: articels_figures_by_rev_year\2021\GigaMVS_A_Benchmark_for_UltraLargeScale_GigapixelLevel_D_Reconstruction\figure_9.jpg
  Figure 9 caption: Ground-truth models for the representative scenes in our benchmark.
    For better visualization, obstructing trees are rendered transparently.
  First author gender probability: 0.65
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Jianing Zhang
  Name of the last author: Lu Fang
  Number of Figures: 15
  Number of Tables: 4
  Number of authors: 10
  Paper title: 'GigaMVS: A Benchmark for Ultra-Large-Scale Gigapixel-Level 3D Reconstruction'
  Publication Date: 2021-09-24 00:00:00
  Table 1 caption: TABLE 1 Benchmark Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Quantitative Geometry Evaluation for Point-MVSNet [54],
    R-MVSNet [34], CasMVSNet [37], UCS-Net [38] and COLMAP [19]
  Table 3 caption: TABLE 3 Quantitative Texture Evaluation for Point-MVSNet [54],
    R-MVSNet [34], CasMVSNet [37], UCS-Net [38] and COLMAP [19]
  Table 4 caption: TABLE 4 GPU Memory Consumption for Learning-Based MVS Methods
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3115028
- Affiliation of the first author: hangzhou dianzi university, hangzhou, zhejiang,
    china
  Affiliation of the last author: queen mary university of london, london, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2021\Learning_Frequency_Domain_Priors_for_Image_Demoireing\figure_1.jpg
  Figure 1 caption: Moire patterns of different scales, frequencies, and colors.
  Figure 10 Link: articels_figures_by_rev_year\2021\Learning_Frequency_Domain_Priors_for_Image_Demoireing\figure_10.jpg
  Figure 10 caption: Qualitative comparison on TIP2018 dataset.
  Figure 2 Link: articels_figures_by_rev_year\2021\Learning_Frequency_Domain_Priors_for_Image_Demoireing\figure_2.jpg
  Figure 2 caption: The architecture of our MBCNN. The network is U-Net like and mainly
    consists of three branches. In each branch, the moire pattern removal block (MPRB),
    global tone mapping block (GTMB) and local tone mapping block (LTMB) are sequentially
    stacked and finally output a clean image at the corresponding scale. The additional
    GTMB and MPRB are introduced to branch I and II to reduce the texture and color
    errors caused by fusing the features of the current branch and the output of the
    coarser branch. The details of the three blocks are explained in Section 4.
  Figure 3 Link: articels_figures_by_rev_year\2021\Learning_Frequency_Domain_Priors_for_Image_Demoireing\figure_3.jpg
  Figure 3 caption: "The structure of moire pattern removal block (MPRB). The MPRB\
    \ is formulated by a dense block, the frequency domain transform (FDT) and a feature\
    \ scale layer (FSL). The FDT consists of two convolution layers C M1 and C M2\
    \ , and a frequency domain inverse transform (FDIT) layer T \u22121 with the learnable\
    \ passband (LP) \u03B8 ."
  Figure 4 Link: articels_figures_by_rev_year\2021\Learning_Frequency_Domain_Priors_for_Image_Demoireing\figure_4.jpg
  Figure 4 caption: The structure of moire pattern removal block constructed by MLBFs.
  Figure 5 Link: articels_figures_by_rev_year\2021\Learning_Frequency_Domain_Priors_for_Image_Demoireing\figure_5.jpg
  Figure 5 caption: The structure of global tone mapping block.
  Figure 6 Link: articels_figures_by_rev_year\2021\Learning_Frequency_Domain_Priors_for_Image_Demoireing\figure_6.jpg
  Figure 6 caption: The structure of local tone mapping block.
  Figure 7 Link: articels_figures_by_rev_year\2021\Learning_Frequency_Domain_Priors_for_Image_Demoireing\figure_7.jpg
  Figure 7 caption: Constructing the FDIT layer with IDCT, IDWT, IDFT, and LNT.
  Figure 8 Link: articels_figures_by_rev_year\2021\Learning_Frequency_Domain_Priors_for_Image_Demoireing\figure_8.jpg
  Figure 8 caption: Demoireing results produced by MBCNN with and without MPRB.
  Figure 9 Link: articels_figures_by_rev_year\2021\Learning_Frequency_Domain_Priors_for_Image_Demoireing\figure_9.jpg
  Figure 9 caption: Demoireing results on the validation set of LCDMoire produced
    by proposed methods and other prior methods.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Bolun Zheng
  Name of the last author: Gregory Slabaugh
  Number of Figures: 12
  Number of Tables: 9
  Number of authors: 9
  Paper title: Learning Frequency Domain Priors for Image Demoireing
  Publication Date: 2021-09-24 00:00:00
  Table 1 caption: TABLE 1 Attributions of Learnable Layers in GTMB
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison of MBCNNs Constructed by LBF With Different
    Frequency Domain Transforms
  Table 3 caption: TABLE 3 Ablation Investigation Between MPRB, GTMB, LTMB, and CA
  Table 4 caption: TABLE 4 Performance of MBCNN, MBCNN-nLP and MBCNN-nTDT on LCDMoire
    Validation Set
  Table 5 caption: TABLE 5 Performance Comparison of Different Loss Functions
  Table 6 caption: TABLE 6 Performance Comparison of MBCNN Models and Other Prior
    Work on the Validation Set of LCDMoire
  Table 7 caption: TABLE 7 Performance Comparison of MBCNN Models Using Different
    Number of Feature Channels on TIP2018 Dataset
  Table 8 caption: TABLE 8 Performance Comparison of MBCNN Models and Other Related
    Works on TIP2018 Dataset
  Table 9 caption: TABLE 9 Quantitative Comparison on Londons Buildings
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3115139
- Affiliation of the first author: department of multimedia engineering, graduate
    school of information science and technology, osaka university, suita, osaka,
    japan
  Affiliation of the last author: department of multimedia engineering, graduate school
    of information science and technology, osaka university, suita, osaka, japan
  Figure 1 Link: articels_figures_by_rev_year\2021\PatchBased_Uncalibrated_Photometric_Stereo_Under_Natural_Illumination\figure_1.jpg
  Figure 1 caption: Natural lighting versus directional lighting.
  Figure 10 Link: articels_figures_by_rev_year\2021\PatchBased_Uncalibrated_Photometric_Stereo_Under_Natural_Illumination\figure_10.jpg
  Figure 10 caption: Ablation study on the Bear case. The top row shows the error
    map of normal estimates at each stage of Ours (GPM). The bottom row gives the
    binary ambiguity estimation, where the blue and green pixels correspond to binary
    ambiguity lbrace 1, -1rbrace , respectively. The difference between (d) and (e)
    is shown in (h). About 6.2% patches have wrong binary ambiguity estimation.
  Figure 2 Link: articels_figures_by_rev_year\2021\PatchBased_Uncalibrated_Photometric_Stereo_Under_Natural_Illumination\figure_2.jpg
  Figure 2 caption: Complete pipeline of patch-based uncalibrated photometric stereo.
    Variables shown in red represent the unknowns in our method.
  Figure 3 Link: articels_figures_by_rev_year\2021\PatchBased_Uncalibrated_Photometric_Stereo_Under_Natural_Illumination\figure_3.jpg
  Figure 3 caption: Illustration of environment lighting approximation. Patches highlighted
    by concentric red circles contain varying radii r and mean angular difference
    of surface normal v . For each patch, we draw the equivalent directional lighting
    directions (dot on spheres) and intensities (red means strong while black means
    weak) for three different environment maps (figures courtesy of [15]).
  Figure 4 Link: articels_figures_by_rev_year\2021\PatchBased_Uncalibrated_Photometric_Stereo_Under_Natural_Illumination\figure_4.jpg
  Figure 4 caption: An example of non-uniform albedo causing large errors across patches.
    (a) Image observation of the Bunny object with non-uniform albedo. (b) Mean angular
    errors (degree) of the patch-wise pseudo surface normals w.r.t. the true surface
    normals. Each pixel value encodes the mean angular error of the estimated surface
    normals for the patch centered at that pixel location. (c) Confidence map of patch
    surface normal estimation.
  Figure 5 Link: articels_figures_by_rev_year\2021\PatchBased_Uncalibrated_Photometric_Stereo_Under_Natural_Illumination\figure_5.jpg
  Figure 5 caption: "Illustration of consistent surface normal clustering. (a) Consistent\
    \ surface normal pairs are extracted from overlapping patch region \u0398 and\
    \ scene point pair (q,s) satisfies consistent orthogonality condition. (b) The\
    \ relative orthogonal transformation between patch N k1 and N k2 is calculated\
    \ following the relationship between surface normals in the overlapping patch\
    \ region \u0398 (Eqs. (9) and (10)). (c) The relative orthogonal transformation\
    \ between patch N k2 and N k3 is extracted based on the consistent orthogonality\
    \ condition between scene points q and s (Eqs. (13) and (14))."
  Figure 6 Link: articels_figures_by_rev_year\2021\PatchBased_Uncalibrated_Photometric_Stereo_Under_Natural_Illumination\figure_6.jpg
  Figure 6 caption: Synthetic dataset. Environment maps (visualized as light probes)
    from sIBL Archive are shown in the top row. Below we show ground truth normals
    for three objects in the first column and examples of rendered images in other
    columns corresponding to the environment maps above.
  Figure 7 Link: articels_figures_by_rev_year\2021\PatchBased_Uncalibrated_Photometric_Stereo_Under_Natural_Illumination\figure_7.jpg
  Figure 7 caption: Comparisons between different environment lighting approximation
    model shown in Table 1. The top row shows the shading maps and the bottom row
    provides the absolute error maps and the mean absolute error value of approximated
    shadings.
  Figure 8 Link: articels_figures_by_rev_year\2021\PatchBased_Uncalibrated_Photometric_Stereo_Under_Natural_Illumination\figure_8.jpg
  Figure 8 caption: Evaluation of equivalent lighting model. Top row provides the
    mean angular difference of equivalent lighting directions vbarmathbf l within
    a 3 times 3 patch w.r.t. that of surface normals vmathbf n in the corresponding
    range shown in x -axis. The bottom row provides the mean angular error of patch
    normal estimation w.r.t. vmathbf n .
  Figure 9 Link: articels_figures_by_rev_year\2021\PatchBased_Uncalibrated_Photometric_Stereo_Under_Natural_Illumination\figure_9.jpg
  Figure 9 caption: Comparison between different patch merging methods (MPM & GPM)
    under varying numbers of lights (10, 15, and 20). The top two rows show the angular
    error distributions from Ours (MPM) and Ours (GPM) of the Sphere object. The table
    below provides the mean angular errors (in degree) w.r.t. to the ground truth
    shown in Fig. 6. Our newly proposed GPM outperforms our previous MPM [34] on all
    the three objects.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Heng Guo
  Name of the last author: Yasuyuki Matsushita
  Number of Figures: 15
  Number of Tables: 1
  Number of authors: 7
  Paper title: Patch-Based Uncalibrated Photometric Stereo Under Natural Illumination
  Publication Date: 2021-09-24 00:00:00
  Table 1 caption: TABLE 1 Summary of Uncalibrated Photometric Stereo Methods Under
    Natural Illumination, Where f f, o o, p p, and k k Represent the Numbers of Images,
    Spherical Harmonic (SH) Lighting Basis, Valid Pixels, and Extracted Patches, Respectively
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3115229
- Affiliation of the first author: department of information engineering, the chinese
    university of hong kong, hong kong, hong kong
  Affiliation of the last author: department of computer science, the university of
    hong kong, hong kong, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2021\Exploiting_Deep_Generative_Prior_for_Versatile_Image_Restoration_and_Manipulatio\figure_1.jpg
  Figure 1 caption: These image restoration (a)(b)(c)(d) and manipulation (e)(f)(g)
    effects are achieved by merely leveraging the rich generative prior of a GAN without
    task-specific modeling. The GAN does not see these images during training.
  Figure 10 Link: articels_figures_by_rev_year\2021\Exploiting_Deep_Generative_Prior_for_Versatile_Image_Restoration_and_Manipulatio\figure_10.jpg
  Figure 10 caption: (a) Colorizing an image under different class conditions. (b)
    Simultaneously conduct colorization, inpainting, and super-resolution ( times
    2 ).
  Figure 2 Link: articels_figures_by_rev_year\2021\Exploiting_Deep_Generative_Prior_for_Versatile_Image_Restoration_and_Manipulatio\figure_2.jpg
  Figure 2 caption: Comparison of various methods in reconstructing a gray image under
    the gray-scale observation space using a GAN. Conventional GAN inversion strategies
    like (b) [15] and (c) [18] produce imprecise reconstruction for the existing semantics.
    In this work, we relax the generator so that it can be fine-tuned on-the-fly,
    achieving more accurate reconstruction as in (d)(e)(f), of which optimization
    is based on (d) VGG perceptual loss, (e) discriminator feature matching loss,
    and (f) combined with progressive reconstruction, respectively. We highlight that
    discriminator is important to preserve the generative prior so as to achieve better
    restoration for the missing information (i.e., color). The proposed progressive
    strategy eliminates the information lingering artifacts as in the red box in (e).
  Figure 3 Link: articels_figures_by_rev_year\2021\Exploiting_Deep_Generative_Prior_for_Versatile_Image_Restoration_and_Manipulatio\figure_3.jpg
  Figure 3 caption: Image restoration with GAN as a prior. For a degraded target image,
    we view the GAN image manifold as the approximated natural image manifold, and
    search for the image that matches the target image in the observation space, which
    becomes the restored image.
  Figure 4 Link: articels_figures_by_rev_year\2021\Exploiting_Deep_Generative_Prior_for_Versatile_Image_Restoration_and_Manipulatio\figure_4.jpg
  Figure 4 caption: Comparison of different loss types when fine-tuning the generator
    to reconstruct the image.
  Figure 5 Link: articels_figures_by_rev_year\2021\Exploiting_Deep_Generative_Prior_for_Versatile_Image_Restoration_and_Manipulatio\figure_5.jpg
  Figure 5 caption: Progressive reconstruction of the generator can better preserves
    the consistency between missing and existing semantics in comparison to simultaneous
    fine-tuning on all the parameters at once. Here the list of images shown in the
    middle are the outputs of the generator in different fine-tuning stages.
  Figure 6 Link: articels_figures_by_rev_year\2021\Exploiting_Deep_Generative_Prior_for_Versatile_Image_Restoration_and_Manipulatio\figure_6.jpg
  Figure 6 caption: Image reconstruction. We compare our method with other GAN inversion
    methods including (a) optimizing latent code [16], [17], (b) learning an encoder
    [15], (c) a combination of (a)(b) [15], (d) adding small perturbations to early
    stages based on (c) [18], and (e) using multiple latent codes [27].
  Figure 7 Link: articels_figures_by_rev_year\2021\Exploiting_Deep_Generative_Prior_for_Versatile_Image_Restoration_and_Manipulatio\figure_7.jpg
  Figure 7 caption: Colorization. Qualitative comparison of Autocolorize [1], other
    GAN inversion methods [18], [32], mGANprior [27], and our DGP.
  Figure 8 Link: articels_figures_by_rev_year\2021\Exploiting_Deep_Generative_Prior_for_Versatile_Image_Restoration_and_Manipulatio\figure_8.jpg
  Figure 8 caption: Inpainting. Compared with DIP, [18], [32], and mGANprior, the
    proposed DGP could preserve the spatial coherence in image inpainting with large
    missing regions.
  Figure 9 Link: articels_figures_by_rev_year\2021\Exploiting_Deep_Generative_Prior_for_Versatile_Image_Restoration_and_Manipulatio\figure_9.jpg
  Figure 9 caption: Super-resolution ( times 4 ) on 64 times 64 size images. The comparisons
    of our method with DIP, SinGAN, [32], and mGANprior are shown, where DGP produces
    sharper super-resolution results.
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.5
  Name of the first author: Xingang Pan
  Name of the last author: Ping Luo
  Number of Figures: 24
  Number of Tables: 6
  Number of authors: 6
  Paper title: Exploiting Deep Generative Prior for Versatile Image Restoration and
    Manipulation
  Publication Date: 2021-09-24 00:00:00
  Table 1 caption: TABLE 1 Comparison With Other GAN Inversion Methods, Including
    (a) Optimizing Latent Code [16], [17], (b) Learning an Encoder [15], (c) A Combination
    of (a)(b) [15], (d) Adding Small Perturbations to Early Stages Based on (c) [18],
    and (e) Using Multiple Latent Codes [27]
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Inpainting Evaluation
  Table 3 caption: "TABLE 3 Super-Resolution ( \xD74 \xD74) Evaluation"
  Table 4 caption: TABLE 4 Comparison of Different Loss Type and Fine-Tuning Strategy
  Table 5 caption: TABLE 5 Comparison With Other Methods on StyleGAN and PGGAN for
    Image Inpainting
  Table 6 caption: TABLE 6 Adversarial Defense Evaluation
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3115428
- Affiliation of the first author: department of mathematics, paderborn university,
    paderborn, germany
  Affiliation of the last author: department of computer science, paderborn university,
    paderborn, germany
  Figure 1 Link: articels_figures_by_rev_year\2021\On_the_Treatment_of_Optimization_Problems_With_L_Penalty_Terms_via_Multiobjectiv\figure_1.jpg
  Figure 1 caption: "(a) Different points on the Pareto front by solving (MOP\u2212\
    \ \u2113 1 ) using the weighted-sum approach with varying \u03B1 . (b) For non-convex\
    \ problems, multiple solutions correspond to the same \u03B1 such that the solution\
    \ cannot be computed in the non-convex regions. (c) For strictly concave problems,\
    \ only the individual minima can be found."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\On_the_Treatment_of_Optimization_Problems_With_L_Penalty_Terms_via_Multiobjectiv\figure_2.jpg
  Figure 2 caption: "(a) P c for problem (7). (b) part of P c where the turning points\
    \ x 1 and x 2 occur. (c) Image of P c with the same coloring as in (b). (d) Derivative\
    \ of f in the direction of the predictor, i.e., \u2207f(x ) \u22A4 v 1 ."
  Figure 3 Link: articels_figures_by_rev_year\2021\On_the_Treatment_of_Optimization_Problems_With_L_Penalty_Terms_via_Multiobjectiv\figure_3.jpg
  Figure 3 caption: (a) P c for problem (14). (b) part of P c of problem (15).
  Figure 4 Link: articels_figures_by_rev_year\2021\On_the_Treatment_of_Optimization_Problems_With_L_Penalty_Terms_via_Multiobjectiv\figure_4.jpg
  Figure 4 caption: (a) P c for the toy example. (b) Image of P c . (c) and (d) comparison
    to the weighted sum method, with which the red parts cannot be computed.
  Figure 5 Link: articels_figures_by_rev_year\2021\On_the_Treatment_of_Optimization_Problems_With_L_Penalty_Terms_via_Multiobjectiv\figure_5.jpg
  Figure 5 caption: Pareto front for (textMOP-ell 1) with f according to (16).
  Figure 6 Link: articels_figures_by_rev_year\2021\On_the_Treatment_of_Optimization_Problems_With_L_Penalty_Terms_via_Multiobjectiv\figure_6.jpg
  Figure 6 caption: (a) Convex Pareto front for the SINDy approach applied to the
    Lorenz system. (b) Evolution of all Wi,j which become greater than 0.5 over the
    iterations. (c) Trajectory of the Lorenz system (black) and the resulting trajectories
    of the approximated solutions where the corresponding point on the Pareto front
    is marked in (a).
  Figure 7 Link: articels_figures_by_rev_year\2021\On_the_Treatment_of_Optimization_Problems_With_L_Penalty_Terms_via_Multiobjectiv\figure_7.jpg
  Figure 7 caption: Pareto front for the NN training on the Iris data set (black).
    The colored dots show solutions obtained via Adam with different weights for the
    ell 1 penalty term. These cluster in two places, cf. the zoomed in plots in the
    bottom.
  Figure 8 Link: articels_figures_by_rev_year\2021\On_the_Treatment_of_Optimization_Problems_With_L_Penalty_Terms_via_Multiobjectiv\figure_8.jpg
  Figure 8 caption: Test error (black) and training error (grey) against ell 1 -norm
    for Pareto critical points computed via continuation. The colored dots show the
    test error of the Adam solutions.
  Figure 9 Link: articels_figures_by_rev_year\2021\On_the_Treatment_of_Optimization_Problems_With_L_Penalty_Terms_via_Multiobjectiv\figure_9.jpg
  Figure 9 caption: (a) Pareto front for the NN training on the reduced MNIST data
    set (black). The colored dots show solutions obtained via Adam with different
    weights for the ell 1 -term. (b) Test error (black) and training error (grey)
    against the ell 1 -norm for Pareto critical points computed via continuation.
    The colored dots show the test error of the Adam solutions.
  First author gender probability: 0.99
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Katharina Bieker
  Name of the last author: Sebastian Peitz
  Number of Figures: 9
  Number of Tables: 0
  Number of authors: 3
  Paper title: On the Treatment of Optimization Problems With L1 Penalty Terms via
    Multiobjective Continuation
  Publication Date: 2021-09-24 00:00:00
  Table 1 caption: Not Available
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3114962
- Affiliation of the first author: "bifold \u2013 berlin institute for the foundations\
    \ of learning and data, berlin institute of technology (tu berlin), berlin, germany"
  Affiliation of the last author: "bifold \u2013 berlin institute for the foundations\
    \ of learning and data, berlin institute of technology (tu berlin), berlin, germany"
  Figure 1 Link: articels_figures_by_rev_year\2021\HigherOrder_Explanations_of_Graph_Neural_Networks_via_Relevant_Walks\figure_1.jpg
  Figure 1 caption: High-level illustration of GNN-LRP. The explanation procedure
    starts at the GNN output, and proceeds backwards to progressively uncover the
    walks that are relevant for the prediction.
  Figure 10 Link: articels_figures_by_rev_year\2021\HigherOrder_Explanations_of_Graph_Neural_Networks_via_Relevant_Walks\figure_10.jpg
  Figure 10 caption: 'Left: Paracetamol molecule, and the GNN-LRP explanation of its
    predicted energy. Red and blue indicate positive and negative contributions. Opacity
    indicates the magnitude of these contributions. Right: Average energy contribution
    per bond, depicted for each bond type separately, in arbitrary units.'
  Figure 2 Link: articels_figures_by_rev_year\2021\HigherOrder_Explanations_of_Graph_Neural_Networks_via_Relevant_Walks\figure_2.jpg
  Figure 2 caption: Illustration of a bag-of-edges B and the corresponding walks W
    for a simple input graph of three nodes. The two walks associated to the given
    bag-of-edges are shown with a solid and a dashed line, respectively.
  Figure 3 Link: articels_figures_by_rev_year\2021\HigherOrder_Explanations_of_Graph_Neural_Networks_via_Relevant_Walks\figure_3.jpg
  Figure 3 caption: Diagram of GNN-LRP annotated with variables that are used. (1)
    Sketch of an interaction block featuring an aggregation layer followed by a combine
    layer. The sketch shows nodes J and K , each of them represented by neurons. Two
    neurons of consecutive layers of the combine function are denoted by a and b .
    (2) GNN-LRP propagation flow, where we observe that only relevance scores that
    propagate along a particular walk in the input graph are retained.
  Figure 4 Link: articels_figures_by_rev_year\2021\HigherOrder_Explanations_of_Graph_Neural_Networks_via_Relevant_Walks\figure_4.jpg
  Figure 4 caption: 'Left: Examples from the two classes of the BA-growth dataset.
    Right: GNN-LRP explanation for the GIN prediction of a graph of class 1. Relevant
    (positively contributing) walks are shown in red and negatively contributing walks
    are in blue. Circles represent walks (or part of the walks) that are stationary.'
  Figure 5 Link: articels_figures_by_rev_year\2021\HigherOrder_Explanations_of_Graph_Neural_Networks_via_Relevant_Walks\figure_5.jpg
  Figure 5 caption: Comparison of different explanation techniques on the same graph
    as in Fig. 4. GNN-LRP produces more detailed explanations compared to [24] and
    [26], and brings more robustness compared to GNN-GI.
  Figure 6 Link: articels_figures_by_rev_year\2021\HigherOrder_Explanations_of_Graph_Neural_Networks_via_Relevant_Walks\figure_6.jpg
  Figure 6 caption: The effect of the parameter gamma of GNN-LRP on the AUAC score
    on the BA-growth dataset. Higher values of gamma give better performance.
  Figure 7 Link: articels_figures_by_rev_year\2021\HigherOrder_Explanations_of_Graph_Neural_Networks_via_Relevant_Walks\figure_7.jpg
  Figure 7 caption: Sentence predicted by the GNN and the BoW model, and explained
    by GNN-LRP (applied on the difference between the positive and negative sentiment
    logit, and with the LRP parameter gamma = 3 ). Contributions to positive sentiment
    are in red, and contributions to negative sentiment are in blue.
  Figure 8 Link: articels_figures_by_rev_year\2021\HigherOrder_Explanations_of_Graph_Neural_Networks_via_Relevant_Walks\figure_8.jpg
  Figure 8 caption: Two selected examples of dependency trees from the SST dataset,
    predicted by the GNN model to be positive, and for which GNN-LRP highlights a
    flaw in the prediction strategy.
  Figure 9 Link: articels_figures_by_rev_year\2021\HigherOrder_Explanations_of_Graph_Neural_Networks_via_Relevant_Walks\figure_9.jpg
  Figure 9 caption: Walks that contribute the most to positive and negative sentiment
    according to the GNN, split by the number of unique words they contain.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Thomas Schnake
  Name of the last author: "Gr\xE9goire Montavon"
  Number of Figures: 13
  Number of Tables: 4
  Number of authors: 7
  Paper title: Higher-Order Explanations of Graph Neural Networks via Relevant Walks
  Publication Date: 2021-09-24 00:00:00
  Table 1 caption: TABLE 1 Notation Used Throughout the Paper
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Practical GNN-LRP Propagation Rules for Different Types
    of GNNs
  Table 3 caption: TABLE 3 AUAC Scores of Each Explanation Method on Various Datasets
    and Models
  Table 4 caption: TABLE 4 AUROC of Selected Explanation Methods on BA-2motifs
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3115452
- Affiliation of the first author: key laboratory for ubiquitous network and service
    software of liaoning province, dut-ru international school of information science
    and engineering, dalian university of technology, dalian, liaoning, china
  Affiliation of the last author: key laboratory for ubiquitous network and service
    software of liaoning province, dut-ru international school of information science
    and engineering, dalian university of technology, dalian, liaoning, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Learning_Deformable_Image_Registration_From_Optimization_Perspective_Modules_Bil\figure_1.jpg
  Figure 1 caption: "The first row shows our optimization learning perspective, which\
    \ is to cascade three modules to propagate the optimization of registration fields\
    \ on feature space. In the second row, the feature extraction F , error-based\
    \ matching M , regularization R , constraint C modules form one iteration of our\
    \ framework. Different from conventional training, thanks to upper-level and lower-level\
    \ objectives, our bilevel training in the third row could learn model parameters\
    \ w and hyper-parameters \u03BB collaboratively. We also give its illustration.\
    \ Blue cures represent the lower-level objective and their minimal values are\
    \ shown as blue dots. The red cure denotes the upper-level objective, whose minimal\
    \ value is the red dot."
  Figure 10 Link: articels_figures_by_rev_year\2021\Learning_Deformable_Image_Registration_From_Optimization_Perspective_Modules_Bil\figure_10.jpg
  Figure 10 caption: Illustration of the importance of affine pre-processing for registration
    result. The source, target, warped image of our model trained without and with
    the affine network.
  Figure 2 Link: articels_figures_by_rev_year\2021\Learning_Deformable_Image_Registration_From_Optimization_Perspective_Modules_Bil\figure_2.jpg
  Figure 2 caption: Comparisons on deformation fields by warping the source image
    to target without and with the constraint module. Singularities emerge in the
    circled fields when applying no constraint.
  Figure 3 Link: articels_figures_by_rev_year\2021\Learning_Deformable_Image_Registration_From_Optimization_Perspective_Modules_Bil\figure_3.jpg
  Figure 3 caption: The evolution of deformation color maps and registered images
    with the propagation of registration fields in the first two scales.
  Figure 4 Link: articels_figures_by_rev_year\2021\Learning_Deformable_Image_Registration_From_Optimization_Perspective_Modules_Bil\figure_4.jpg
  Figure 4 caption: Example MR coronal slices of input target, source and warped image
    for our method with corresponding labels of ventricles, thalami, and hippocampi.
    The last column shows the RGB image of the registration field. Each row refers
    to an example registration case of brain MR data.
  Figure 5 Link: articels_figures_by_rev_year\2021\Learning_Deformable_Image_Registration_From_Optimization_Perspective_Modules_Bil\figure_5.jpg
  Figure 5 caption: Boxplot indicates the Dice scores for SyN, VM and our algorithm
    over sixteen anatomical structures including Cerebral White Matter (CblmWM), Cerebral
    Cortex (CblmC), Lateral Ventricle (LV), Inferior Lateral Ventricle (ILV), Cerebellum
    White Matter (CeblWM), Cerebellum Cortex (CereC), Thalamus (Tha), Caudate (Cau),
    Putamen (Pu), Pallidum (Pa), Hippocampus (Hi), Accumbens area (Am), Vessel, Third
    Ventricle (3V), Fourth Ventricle (4V), and Brain Stem (BS).
  Figure 6 Link: articels_figures_by_rev_year\2021\Learning_Deformable_Image_Registration_From_Optimization_Perspective_Modules_Bil\figure_6.jpg
  Figure 6 caption: Registered MR slices overlaid with atlas using different methods.
    The Dice scores are given in the bottom parentheses. Circles indicate several
    evident inconsistencies.
  Figure 7 Link: articels_figures_by_rev_year\2021\Learning_Deformable_Image_Registration_From_Optimization_Perspective_Modules_Bil\figure_7.jpg
  Figure 7 caption: Registered MR slices and segmented anatomical structures using
    VM, VM-diff, and our method. The Dice scores are parenthesized.
  Figure 8 Link: articels_figures_by_rev_year\2021\Learning_Deformable_Image_Registration_From_Optimization_Perspective_Modules_Bil\figure_8.jpg
  Figure 8 caption: The first row demonstrates cortex visualization and zoomed-in
    overlaid sulci registered using different methods. The second row gives the MR
    slices and zoomed-in warped patches. Parenthesized values in the bottom give the
    Dice score.
  Figure 9 Link: articels_figures_by_rev_year\2021\Learning_Deformable_Image_Registration_From_Optimization_Perspective_Modules_Bil\figure_9.jpg
  Figure 9 caption: Two example registration cases of liver CT data, including target,
    source, registered images, and the corresponding labels and flow fields.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Risheng Liu
  Name of the last author: Zhongxuan Luo
  Number of Figures: 13
  Number of Tables: 11
  Number of authors: 6
  Paper title: 'Learning Deformable Image Registration From Optimization: Perspective,
    Modules, Bilevel Training and Beyond'
  Publication Date: 2021-09-28 00:00:00
  Table 1 caption: TABLE 1 Ablation Analysis of the Feature Pyramid Extraction Module
    on Five Brain MRI Datasets in Terms of Dice Score and NCC
  Table 10 caption: TABLE 10 Qualitative Comparison Results for Multi-Modal Registration
    Tests
  Table 2 caption: TABLE 2 Ablation Analysis of Model Configurations on Five Brain
    MRI Datasets in Terms of Dice Score and NCC
  Table 3 caption: TABLE 3 Ablation Analysis of Model Components on Five Brain MRI
    Datasets in Terms of Dice Score and NCC
  Table 4 caption: TABLE 4 Ablation Analysis of the Bilevel Self-Tuned Training Strategy
    on Five Brain MRI Datasets and Two Liver CT Datasets in Terms of Dice Score and
    NCC
  Table 5 caption: TABLE 5 Qualitative Comparison Results on Brain MR Registration
    Tasks
  Table 6 caption: TABLE 6 Qualitative Comparison Results on Brain MR Registration
    Tasks
  Table 7 caption: TABLE 7 Qualitative Comparison Results on Liver CT Registration
    Tasks
  Table 8 caption: TABLE 8 Ablation Analysis of the Affine Network on Two Liver CT
    Datasets in Terms of Dice Score and NCC
  Table 9 caption: TABLE 9 Average Registration Time in Second for Brain MR and Liver
    CT Test Pairs
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3115825
- Affiliation of the first author: school of computer science, faculty of engineering,
    university of sydney, darlington, nsw, australia
  Affiliation of the last author: school of computer science, faculty of engineering,
    university of sydney, darlington, nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2021\Exposure_Trajectory_Recovery_From_Motion_Blur\figure_1.jpg
  Figure 1 caption: Illustration of our proposed motion offset estimation method.
    The figure on the left is our motion offset generation network. It takes blurry
    images as input and outputs the corresponding motion offsets. Afterwards, the
    blur creation module (on the right) takes a sharp image and the extracted motion
    offsets to reconstruct the input blurry image.
  Figure 10 Link: articels_figures_by_rev_year\2021\Exposure_Trajectory_Recovery_From_Motion_Blur\figure_10.jpg
  Figure 10 caption: Comparison of video extraction results. In the top-down order,
    we show ours, result of [19], result of [18].
  Figure 2 Link: articels_figures_by_rev_year\2021\Exposure_Trajectory_Recovery_From_Motion_Blur\figure_2.jpg
  Figure 2 caption: Examples of motion offsets with different constraints. Suppose
    the green curve is the ground truth exposure trajectory. (a)-(d) simulate the
    fitting results of motion offsets with no constraint, linear constraint, b-d linear
    constraint, and quadratic constraint, respectively. Red points are the offsets
    that output by the estimation network and blue points are calculated by the different
    constraints.
  Figure 3 Link: articels_figures_by_rev_year\2021\Exposure_Trajectory_Recovery_From_Motion_Blur\figure_3.jpg
  Figure 3 caption: The proposed motion-aware deblurring network. An encoder-decoder
    residual architecture for image deblurring is shown on the left, while the schematic
    of a motion-aware convolution in the motion-aware block is shown on the right.
  Figure 4 Link: articels_figures_by_rev_year\2021\Exposure_Trajectory_Recovery_From_Motion_Blur\figure_4.jpg
  Figure 4 caption: Examples of motion estimation on the synthetic dataset. The top
    row shows the blurry input, ground truth motion, and results of previous methods.
    The bottom row shows our estimated motion offsets under different constraints.
  Figure 5 Link: articels_figures_by_rev_year\2021\Exposure_Trajectory_Recovery_From_Motion_Blur\figure_5.jpg
  Figure 5 caption: Examples of motion estimation on the GoPro dataset. The top row
    shows the blurry input and results of previous methods. The bottom row shows our
    estimated motion offsets under different constraints.
  Figure 6 Link: articels_figures_by_rev_year\2021\Exposure_Trajectory_Recovery_From_Motion_Blur\figure_6.jpg
  Figure 6 caption: Visual comparison of motion estimation on GoPro test set. From
    left to right shows the groud-truth image patches, the results of Sun et al. [16],
    Gong et al. [15] and our quadratic model. For each method, we visualize the estimated
    motion field and the reblurred result.
  Figure 7 Link: articels_figures_by_rev_year\2021\Exposure_Trajectory_Recovery_From_Motion_Blur\figure_7.jpg
  Figure 7 caption: The effect of offset number on blur creation. Left to right show
    the ground truth blurry image, the result of the model with 5 offsets, the result
    of the model with 9 offsets, and the result of the model with 15 offsets. It is
    clear that increasing the number of offsets creates a smoother and more realistic
    blurry output.
  Figure 8 Link: articels_figures_by_rev_year\2021\Exposure_Trajectory_Recovery_From_Motion_Blur\figure_8.jpg
  Figure 8 caption: Visualize the magnitude and direction of motion offsets in a color
    coded map. Different colors represent different directions. The model with TV
    loss generates a much more smooth color map than the model wo TV loss (best view
    in high resolutions).
  Figure 9 Link: articels_figures_by_rev_year\2021\Exposure_Trajectory_Recovery_From_Motion_Blur\figure_9.jpg
  Figure 9 caption: Visual comparison with GoPro dataset. From left to right, we show
    input, deblurring result of DeblurGAN-V2 [56], Gao et al. [2], DMPHN [3], ours,
    and stack(4)-DMPHN [3] (best view in high resolutions).
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Youjian Zhang
  Name of the last author: Dacheng Tao
  Number of Figures: 14
  Number of Tables: 7
  Number of authors: 4
  Paper title: Exposure Trajectory Recovery From Motion Blur
  Publication Date: 2021-09-28 00:00:00
  Table 1 caption: TABLE 1 Detailed Architecture of the Motion Offset Estimation Network
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Quantitative Comparison of Motion Estimation on Both Synthetic
    and the GoPro [1] Dataset
  Table 3 caption: TABLE 3 Comparison for the Setting of Offset Numbers N N
  Table 4 caption: TABLE 4 Ablation Study for Loss Function
  Table 5 caption: TABLE 5 Quantitative Deblurring Results on GoPro Dataset
  Table 6 caption: TABLE 6 Ablation Study of Motion-Aware Convolution on GoPro Dataset
  Table 7 caption: TABLE 7 Quantitative Comparison of Video Extraction on GoPro Dataset
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3116135
