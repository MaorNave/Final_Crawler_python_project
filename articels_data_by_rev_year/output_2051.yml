- Affiliation of the first author: department of statistics, virginia tech, blacksburg,
    va, usa
  Affiliation of the last author: department of statistics, purdue university, west
    lafayette, in, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\Nonparametric_Testing_Under_Randomized_Sketching\figure_1.jpg
  Figure 1 caption: "Phase transition in (\u03BB,s) for signal detection. The horizontal\
    \ axis is the smoothing parameter \u03BB , and the vertical axis is the projection\
    \ dimension s . The shade indicates the values of SWDS: dark red corresponds to\
    \ greater values of SWDS than light blue. The vertical line labeled by \u201C\
    optimal\u201D indicates the choices of \u03BB that achieve the smallest SWDS."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Nonparametric_Testing_Under_Randomized_Sketching\figure_2.jpg
  Figure 2 caption: Trade-offs for achieving (a) optimal estimation rate; (b) optimal
    testing rate.
  Figure 3 Link: articels_figures_by_rev_year\2021\Nonparametric_Testing_Under_Randomized_Sketching\figure_3.jpg
  Figure 3 caption: Size for (a) DT and (b) AT with projection dimension varies. Signal
    strength c=0 .
  Figure 4 Link: articels_figures_by_rev_year\2021\Nonparametric_Testing_Under_Randomized_Sketching\figure_4.jpg
  Figure 4 caption: Power for DT and AT with varying projection dimension. Signal
    strength c=0.01 for (a) and (b) ; c=0.02 for (c) and (d) ; c=0.03 for (e) and
    (f) .
  Figure 5 Link: articels_figures_by_rev_year\2021\Nonparametric_Testing_Under_Randomized_Sketching\figure_5.jpg
  Figure 5 caption: Size and power for DT with varying projection dimensions. Signal
    strength c=0 for (a) ; c=0.05 for (b) ; c=0.1 for (c) ; c=0.15 for (d) .
  Figure 6 Link: articels_figures_by_rev_year\2021\Nonparametric_Testing_Under_Randomized_Sketching\figure_6.jpg
  Figure 6 caption: 'Computing time for DT with varying projection dimensions: (a)
    is polynomially decay kernels; (b) is exponentially decay kernels.'
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.92
  Name of the first author: Meimei Liu
  Name of the last author: Guang Cheng
  Number of Figures: 6
  Number of Tables: 1
  Number of authors: 4
  Paper title: Nonparametric Testing Under Randomized Sketching
  Publication Date: 2021-03-03 00:00:00
  Table 1 caption: "TABLE 1 Lower Bound of s s and Choice of \u03BB \u03BB for Optimal\
    \ Estimation or Testing in PDK and EDK"
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3063223
- Affiliation of the first author: s-lab, nanyang technological university (ntu),
    singapore, singapore
  Affiliation of the last author: s-lab, nanyang technological university (ntu), singapore,
    singapore
  Figure 1 Link: articels_figures_by_rev_year\2021\Learning_to_Enhance_LowLight_Image_via_ZeroReference_Deep_Curve_Estimation\figure_1.jpg
  Figure 1 caption: Visual comparisons on a typical low-light image comprising nonuniform
    illumination. The proposed Zero-DCE and Zero-DCE++ achieve visually pleasing results
    in terms of brightness, color, contrast, and naturalness, while existing methods
    either fail to cope with the extreme back light or generate color artifacts. In
    contrast to other deep learning-based methods, our approach is trained without
    any reference image.
  Figure 10 Link: articels_figures_by_rev_year\2021\Learning_to_Enhance_LowLight_Image_via_ZeroReference_Deep_Curve_Estimation\figure_10.jpg
  Figure 10 caption: Ablation study of the impact of training data. Zero-DCE Low represents
    that the Zero-DCE was trained on only 900 low-light images out of 2,422 images
    in the original training set. Zero-DCE LargeL represents that the Zero-DCE was
    trained on 9,000 unlabeled low-light images provided in the DARK FACE dataset
    [42]. Zero-DCE LargeLH represents that the Zero-DCE was trained on 4800 multi-exposure
    images from the data augmented combination of Part1 and Part2 subsets in the SICE
    dataset [38]. (b) suggests that Zero-DCE has a good balance between over-enhancement
    and under-enhancement.
  Figure 2 Link: articels_figures_by_rev_year\2021\Learning_to_Enhance_LowLight_Image_via_ZeroReference_Deep_Curve_Estimation\figure_2.jpg
  Figure 2 caption: "(a) The framework of Zero-DCE. A DCE-Net is devised to estimate\
    \ a set of best-fitting Light-Enhancement curves (LE-curves) that iteratively\
    \ enhance a given input image (i.e., takes the enhanced image as the input of\
    \ next iteration and the input is enhanced in a progressive manner). (b, c) LE-curves\
    \ with different adjustment parameters \u03B1 and numbers of iteration n . In\
    \ (c), \u03B1 1 , \u03B1 2 , and \u03B1 3 are equal to -1 while n is equal to\
    \ 4. In each subfigure, the horizontal axis represents the input pixel values\
    \ while the vertical axis represents the output pixel values."
  Figure 3 Link: articels_figures_by_rev_year\2021\Learning_to_Enhance_LowLight_Image_via_ZeroReference_Deep_Curve_Estimation\figure_3.jpg
  Figure 3 caption: An example of the pixel-wise curve parameter maps. For visualization,
    we average the curve parameter maps of all iterations ( n=8 ) and normalize the
    values to the range of [0,1]. A R n , A G n , and A B n represent the averaged
    best-fitting curve parameter maps of R, G, and B channels, respectively. The maps
    in (b), (c), and (d) are represented by heatmaps.
  Figure 4 Link: articels_figures_by_rev_year\2021\Learning_to_Enhance_LowLight_Image_via_ZeroReference_Deep_Curve_Estimation\figure_4.jpg
  Figure 4 caption: The architecture of DCE-Net.
  Figure 5 Link: articels_figures_by_rev_year\2021\Learning_to_Enhance_LowLight_Image_via_ZeroReference_Deep_Curve_Estimation\figure_5.jpg
  Figure 5 caption: An illustration of the spatial consistency loss.
  Figure 6 Link: articels_figures_by_rev_year\2021\Learning_to_Enhance_LowLight_Image_via_ZeroReference_Deep_Curve_Estimation\figure_6.jpg
  Figure 6 caption: The estimated curve parameter maps in different iteration stages.
    Subfigure (b), (c), and (d) depict the estimated curve parameter maps in iterations
    1, 2, and 8, respectively. Subfigure (e) and (f) show the difference maps between
    iteration 1 and iteration 2 as well as between iteration 1 and iteration 8, respectively.
    For visualization, we normalize the curve parameter maps and amplify the intensity
    of the difference maps by 30 times.
  Figure 7 Link: articels_figures_by_rev_year\2021\Learning_to_Enhance_LowLight_Image_via_ZeroReference_Deep_Curve_Estimation\figure_7.jpg
  Figure 7 caption: Ablation study of the contribution of each loss (spatial consistency
    loss Lspa , exposure control loss Lexp , color constancy loss Lcol , illumination
    smoothness loss Ltvmathcal A ). Red boxes indicate the obvious differences and
    amplified details.
  Figure 8 Link: articels_figures_by_rev_year\2021\Learning_to_Enhance_LowLight_Image_via_ZeroReference_Deep_Curve_Estimation\figure_8.jpg
  Figure 8 caption: Ablation study of the advantage of three channels adjustment (RGB,
    CIE Lab, and YCbCr color spaces).
  Figure 9 Link: articels_figures_by_rev_year\2021\Learning_to_Enhance_LowLight_Image_via_ZeroReference_Deep_Curve_Estimation\figure_9.jpg
  Figure 9 caption: Ablation study of the effect of parameter settings. l - f - n
    represents the proposed Zero-DCE with l convolutional layers, f feature maps of
    each layer (except the last layer), and n iterations.
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.64
  Name of the first author: Chongyi Li
  Name of the last author: Chen Change Loy
  Number of Figures: 16
  Number of Tables: 8
  Number of authors: 3
  Paper title: Learning to Enhance Low-Light Image via Zero-Reference Deep Curve Estimation
  Publication Date: 2021-03-03 00:00:00
  Table 1 caption: TABLE 1 Quantitative Comparisons in Terms of Peak Signal-to-Noise
    Ratio (PSNR, dB), Structural Similarity (SSIM) [43], and Mean Absolute Error (MAE)
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Quantitative Comparisons in Terms of PSNR, SSIM, and MAE
  Table 3 caption: TABLE 3 The Statistic Relations Between Enhancement Performance
    and Input Sizes Measured in PSNR and FLOPs
  Table 4 caption: TABLE 4 Ablation Study Between Zero-DCE and Zero-DCE++ in Terms
    of PSNR, Trainable Parameters (P), and FLOPs (in G)
  Table 5 caption: "TABLE 5 User Study (US) \u2191 \u2191Perceptual Index (PI) \u2193\
    \ \u2193 Scores on the Image Sets (NPE, LIME, MEF, DICM, VV)"
  Table 6 caption: TABLE 6 Quantitative Comparisons in Terms of PSNR, SSIM, and MAE
    on the Part2 Testing Set
  Table 7 caption: TABLE 7 Runtime (RT, in second), Trainable Parameters (P), and
    FLOPs (in G) Comparisons
  Table 8 caption: TABLE 8 The Average Precision (AP) for Face Detection in the Dark
    Under Different IoU Thresholds (0.5, 0.7, 0,9)
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3063604
- Affiliation of the first author: national key laboratory for novel software technology,
    nanjing university, nanjing, china
  Affiliation of the last author: national key laboratory for novel software technology,
    nanjing university, nanjing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Learning_Graph_Convolutional_Networks_for_MultiLabel_Recognition_and_Application\figure_1.jpg
  Figure 1 caption: "Illustration of label dependencies modeled in the proposed models.\
    \ In this figure, \u201C mathrm LabelA rightarrow LabelB \u201D means when mathrm\
    \ LabelA appears, mathrm LabelB is likely to appear, but the reverse may not be\
    \ true."
  Figure 10 Link: articels_figures_by_rev_year\2021\Learning_Graph_Convolutional_Networks_for_MultiLabel_Recognition_and_Application\figure_10.jpg
  Figure 10 caption: Top-3 retrieved images with the query image. The returned results
    on the left are based on our proposed C-GCN, the results in the middle are based
    on our proposed P-GCN, and the results on the right are based on vanilla ResNet.
    All results are sorted in the ascending order according to the distance from the
    query image.
  Figure 2 Link: articels_figures_by_rev_year\2021\Learning_Graph_Convolutional_Networks_for_MultiLabel_Recognition_and_Application\figure_2.jpg
  Figure 2 caption: Overall framework of our Classifier Learning GCN (C-GCN) model
    for multi-label image recognition. The object labels are represented by word embeddings
    boldsymbolZ in mathbb RC times d ( C is the number of categories and d is the
    dimensionality of word-embedding vector). A directed graph is built over these
    label representations, where each node denotes a label. Stacked GCNs are learned
    over the label graph to map these label representations into a set of inter-dependent
    object classifiers, i.e., boldsymbolWin mathbb RCtimes D , which are applied to
    the image representation extracted from the input image via a convolutional network
    for multi-label image recognition.
  Figure 3 Link: articels_figures_by_rev_year\2021\Learning_Graph_Convolutional_Networks_for_MultiLabel_Recognition_and_Application\figure_3.jpg
  Figure 3 caption: 'Overview of our proposed Prediction Learning GCN (P-GCN) for
    multi-label image recognition. The model is mainly composed of three modules:
    representation learning, label-aware modulation, and GCN based prediction score
    learning. The label-aware modulation module disentangles the representation extracted
    from the image into a set of label-relevant features and the stacked GCNs are
    adopted to map these features into inter-dependent prediction scores.'
  Figure 4 Link: articels_figures_by_rev_year\2021\Learning_Graph_Convolutional_Networks_for_MultiLabel_Recognition_and_Application\figure_4.jpg
  Figure 4 caption: "Illustration of conditional probability between two labels. As\
    \ usual, when \u201Csurfboard\u201D appears in the image, \u201Cperson\u201D will\
    \ also occur with a high probability. However, in the condition of \u201Cperson\u201D\
    \ appearing, \u201Csurfboard\u201D will not necessarily occur."
  Figure 5 Link: articels_figures_by_rev_year\2021\Learning_Graph_Convolutional_Networks_for_MultiLabel_Recognition_and_Application\figure_5.jpg
  Figure 5 caption: Illustration of different multi-label recognition tasks.
  Figure 6 Link: articels_figures_by_rev_year\2021\Learning_Graph_Convolutional_Networks_for_MultiLabel_Recognition_and_Application\figure_6.jpg
  Figure 6 caption: Performance evaluation on MS-COCO and VOC 2007 w.r.t different
    values of tau .
  Figure 7 Link: articels_figures_by_rev_year\2021\Learning_Graph_Convolutional_Networks_for_MultiLabel_Recognition_and_Application\figure_7.jpg
  Figure 7 caption: Performance evaluation on MS-COCO and VOC 2007 w.r.t different
    values of p . Note that, when p=1 , the C-GCN model does not converge.
  Figure 8 Link: articels_figures_by_rev_year\2021\Learning_Graph_Convolutional_Networks_for_MultiLabel_Recognition_and_Application\figure_8.jpg
  Figure 8 caption: Effect of different types of word embeddings on C-GCN performance.
  Figure 9 Link: articels_figures_by_rev_year\2021\Learning_Graph_Convolutional_Networks_for_MultiLabel_Recognition_and_Application\figure_9.jpg
  Figure 9 caption: Visualization of the learned inter-dependent classifiers by C-GCN
    and plain classifiers of vanilla ResNet on MS-COCO.
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.71
  Name of the first author: Zhao-Min Chen
  Name of the last author: Yanwen Guo
  Number of Figures: 10
  Number of Tables: 9
  Number of authors: 4
  Paper title: Learning Graph Convolutional Networks for Multi-Label Recognition and
    Applications
  Publication Date: 2021-03-03 00:00:00
  Table 1 caption: TABLE 1 Comparisons With State-of-the-Art Methods on the MS-COCO
    Dataset
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparisons of AP and mAP With State-of-the-Art Methods
    on the VOC 2007 Dataset
  Table 3 caption: TABLE 3 Comparisons With State-of-the-Art Methods on the NUS-WIDE
    Dataset
  Table 4 caption: TABLE 4 Effects of Different Predictions on P-GCN
  Table 5 caption: TABLE 5 Results for the Partial Label Problem on MS-COCO
  Table 6 caption: TABLE 6 Results for the Partial Label Problem on VOC 2007
  Table 7 caption: TABLE 7 Results for the Partial Label Problem on NUS-WIDE
  Table 8 caption: TABLE 8 Comparisons With Existing Methods for Multi-Label Face
    Attributes Recognition on the CelebA Dataset
  Table 9 caption: TABLE 9 Comparisons With Existing Methods for Multi-Label Human
    Attributes Recognition on the WIDER Attribute Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3063496
- Affiliation of the first author: school of computer science and technology, huazhong
    university of science and technology, wuhan, hubei, china
  Affiliation of the last author: reler, aaii, university of technology sydney, ultimo,
    nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2021\CategoryLevel_Adversarial_Adaptation_for_Semantic_Segmentation_Using_Purified_Fe\figure_1.jpg
  Figure 1 caption: "(Best viewed in color.) Illustration of the issues in traditional\
    \ method and our solution in the proposed CLAN. (a) Traditional adversarial learning\
    \ embeds various of task-independent factors into crude features, causing these\
    \ features prone to be wrongly aligned between two domains (improper entanglement).\
    \ (b) Traditional adversarial learning ignores the semantic consistency when pursuing\
    \ the marginal distribution alignment. Using an equal adversarial loss weight\
    \ for all features, the global movement might cause the already well-aligned features\
    \ (class \u25A0 ) to be mapped onto different joint distributions (negatively\
    \ transfer). (c) CLAN addresses the improper entanglement by employing an information\
    \ bottleneck before the adversarial feature adaptation. The information bottleneck\
    \ filters out the nuisance factors and maintains pure semantic information. (d)\
    \ Based on the pure semantic information from upstream, CLAN further boosts the\
    \ category-level feature alignment by employing orthogonal classifiers to predict\
    \ local alignment scores of features and reweight the adversarial loss. Our method\
    \ reduces the influence of the adversaries when discovers a high predictive consistency\
    \ from the orthogonal classifiers (indicating an already local alignment for that\
    \ feature), or otherwise increasing the weight for adversarial loss. The size\
    \ of the gray arrow represents the weight of the adversarial loss."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\CategoryLevel_Adversarial_Adaptation_for_Semantic_Segmentation_Using_Purified_Fe\figure_2.jpg
  Figure 2 caption: Pipeline of the proposed category-level adversarial network. It
    consists of a feature extractor F , orthogonal classifiers C 1 and C 2 , and two
    discriminator D f and D o . D f aims to supervise the global feature alignment
    in latent space while D o focuses on the category-level alignment in output space.
    A significance-aware information bottleneck is assigned to the end of F with the
    goal of filtering out the task-independent factors for both domains. C 1 and C
    2 are fed with the purified deep feature map extracted from F and predict semantic
    labels for each pixel from diverse views. In source flow, the latent features
    z S are fed to D f to yield a feature-space adversarial loss, while the ensemble
    of the two prediction maps p S is used to calculate a segmentation loss as well
    as an output-space adversarial loss from D o . Analogously, in target flow, the
    latent features z T are also fed to D f to yield a feature-space adversarial loss
    and the ensemble prediction p T is forwarded to D o to produce a raw adversarial
    loss map. Differently, we additionally adopt the discrepancy of the two prediction
    maps p (1) T and p (2) T to produce a local alignment score map. This map evaluates
    the category-level alignment degree of each feature and is used to adaptively
    weight the raw adversarial loss map mentioned above.
  Figure 3 Link: articels_figures_by_rev_year\2021\CategoryLevel_Adversarial_Adaptation_for_Semantic_Segmentation_Using_Purified_Fe\figure_3.jpg
  Figure 3 caption: Significance-aware information bottleneck (SIB). We use a significance-aware
    module to detect the channel-wise significance V sig. for each pixel-level feature,
    with which the original information constraint loss is adaptively weighted. The
    different sizes of red arrows indicate SIB attaches different compression to each
    channel according to their significance, while the standard IB compress each channel
    equally.
  Figure 4 Link: articels_figures_by_rev_year\2021\CategoryLevel_Adversarial_Adaptation_for_Semantic_Segmentation_Using_Purified_Fe\figure_4.jpg
  Figure 4 caption: 'A contrastive analysis of CLAN and traditional adversarial network
    (Baseline). (a): A target image, and we focus on the poles and traffic signs in
    orange boxes. (b): A non-adapted segmentation result. Although the global segmentation
    result is poor, the poles and traffic signs can be correctly segmented. It indicates
    that some classes are originally aligned between domains, even without any domain
    adaptation. (c): Adapted result of baseline, in which a decent segmentation map
    is produced but poles and traffic signs are poorly segmented. The reason is that
    the global alignment strategy tends to assign a conservative prediction to a feature
    and would lead some features to be predicted to other prevalent classes [45],
    [46], thus causing those infrequent features being negatively transferred. (d):
    Adapted result from CLAN. CLAN reduces the weight of adversarial loss for those
    aligned features. As a result, the original well-segmented class are well preserved.
    We then map the high-dimensional features of (b), (c) and (d) to a 2-D space with
    t-SNE [47] shown in (e), (f) and (g). The comparison of feature distributions
    further proves that CLAN can enforce category-level alignment during the trend
    of global alignment. (For a clear illustration, we only show 4 related classes,
    i.e., building in blue, traffic sign in orange, pole in red and vegetation in
    green.)'
  Figure 5 Link: articels_figures_by_rev_year\2021\CategoryLevel_Adversarial_Adaptation_for_Semantic_Segmentation_Using_Purified_Fe\figure_5.jpg
  Figure 5 caption: 'First two rows: the visualization results of the outputs ( p
    (1) T and p (2) T ) from the two classifiers ( C 1 and C 2 ). Last row: the visualization
    results of the local alignment score map M( p (1) T , p (2) T ) .'
  Figure 6 Link: articels_figures_by_rev_year\2021\CategoryLevel_Adversarial_Adaptation_for_Semantic_Segmentation_Using_Purified_Fe\figure_6.jpg
  Figure 6 caption: 'Left: Cluster center distance variation as training goes on.
    Right: Quantitative analysis of the feature joint distributions. For each class,
    we show the distance of the feature cluster centers (CCD) between source domain
    and target domain. These results are from 1) the model pre-trained on ImageNet
    [60] without any fine-tuning, 2) the model fine-tuned with source images only,
    3) the adapted model using baseline, 4) the adapted model using CLAN without the
    SIB and 5) the full CLAN, respectively.'
  Figure 7 Link: articels_figures_by_rev_year\2021\CategoryLevel_Adversarial_Adaptation_for_Semantic_Segmentation_Using_Purified_Fe\figure_7.jpg
  Figure 7 caption: (Better zoom in.) We confirm the effects of CLAN through a visualization
    of the learned representations z S & z T from two domains, using t-distributed
    stochastic neighbor embedding (t-SNE) [47]. Specifically, we show the results
    of Non-adapted model in (a)&(c) and CLAN in (b)&(d), respectively. In the first
    row, we label the t-SNE map by domains, where red denotes the source domain and
    blue denotes the target domain. In the second row, we label the t-SNE map by different
    classes. The colors are consistent with the annotation maps.
  Figure 8 Link: articels_figures_by_rev_year\2021\CategoryLevel_Adversarial_Adaptation_for_Semantic_Segmentation_Using_Purified_Fe\figure_8.jpg
  Figure 8 caption: "(a). Training stability and mIoU under different settings. (b).\
    \ A -distance between source and target domain. (c). Comparison of IB and SIB\
    \ under different information constraint. (d). KL -divergence curves over the\
    \ course of training under different information constraint. (e). The values of\
    \ adaptive \u03B2 S \u03B2 T over the course of training under different information\
    \ constraint."
  Figure 9 Link: articels_figures_by_rev_year\2021\CategoryLevel_Adversarial_Adaptation_for_Semantic_Segmentation_Using_Purified_Fe\figure_9.jpg
  Figure 9 caption: "Qualitative results of UDA segmentation for GTA5 \u2192 Cityscapes.\
    \ For each target image, we show the non-adapted (source only) result, adapted\
    \ result with MCD [9], AdaptSeg (our baseline) [10], CLAN (SIB-only), CLAN (CLA-only),\
    \ CLAN (Full) and the ground truth label map, respectively. In the baseline results,\
    \ we use yellow arrows to mark some task-independent factors embedded in latent\
    \ space that mislead the adaptation, e.g., similar texture aligns \u201Cbuilding\u201D\
    \ to \u201Cwall\u201D, similar shape aligns \u201Crider\u201D to \u201Cperson\u201D\
    , and the context wrongly couples \u201Cperson\u201D and \u201Csidewalk\u201D\
    . As we can observe, the introduction of SIB relieves this issue. Blue arrows\
    \ indicate some infrequent categories, e.g. sign and fence, \u201Cmissing\u201D\
    \ in the segmentation map dues to the negative transfer. The proposed CLA mechanism\
    \ aims for this problem. Finally, CLAN (Full) takes the advantage of both modules\
    \ to yibetter segmentation results."
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.66
  Name of the first author: Yawei Luo
  Name of the last author: Yi Yang
  Number of Figures: 9
  Number of Tables: 3
  Number of authors: 6
  Paper title: Category-Level Adversarial Adaptation for Semantic Segmentation Using
    Purified Features
  Publication Date: 2021-03-08 00:00:00
  Table 1 caption: TABLE 1 Adaptation From GTA5 [6] to Cityscapes [27]
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Adaptation From SYNTHIA [7] to Cityscapes [27]
  Table 3 caption: TABLE 3 Adaptation Across Seasons
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3064379
- Affiliation of the first author: school of remote sensing and information engineering,
    wuhan university, wuhan, china
  Affiliation of the last author: school of remote sensing and information engineering,
    wuhan university, wuhan, china
  Figure 1 Link: articels_figures_by_rev_year\2021\A_Practical_O_N___ON_Outlier_Removal_Method_for_CorrespondenceBased_Point_Cloud_\figure_1.jpg
  Figure 1 caption: Illustrating the framework of our proposed PCR method, which consists
    of three main stages. First, we use the ISS and FPFH to detect initial correspondence
    set mathcal H , which contains N != !15081 correspondences with an outlier rate
    of 99.49 percent. True inliers are represented by green lines while true outliers
    are represented by red lines. Then, our outlier removal method reduces mathcal
    H to a small set mathcal Hprime of size 98 with an outlier rate of 21.43 percent.
    Finally, an accurate rigid model is estimated for registration by using a SA-Cauchy
    estimate, which only costs 0.003 seconds. Note that directly performing RANSAC
    on mathcal H cannot find a good solution within 105 trials.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\A_Practical_O_N___ON_Outlier_Removal_Method_for_CorrespondenceBased_Point_Cloud_\figure_2.jpg
  Figure 2 caption: Illustration of line voting. In the figure, solid line correspondences
    have equal lengths, while dotted ones have unequal lengths. As shown, ( x i ,
    y i ) gets four votes (blue solid lines) while ( x g , y g ) only gets one vote
    (green solid line). Thus, ( x i , y i ) is more likely to be an inlier.
  Figure 3 Link: articels_figures_by_rev_year\2021\A_Practical_O_N___ON_Outlier_Removal_Method_for_CorrespondenceBased_Point_Cloud_\figure_3.jpg
  Figure 3 caption: "Illustration of the solution space of the ACM cost, where y t\
    \ i is the true correspondence of x i and y ~ i is the optimal correspondence\
    \ of x i . Generally, the space of a true inlier is a spherical region \u03A9\
    \ with radius \u03B5 . Differently, the solution space of the ACM cost is a plane\
    \ \u03C0 (the red line) and the space of inlier candidate is a region \u03A6 between\
    \ plane \u03C0 1 (blue line) and plane \u03C0 2 (green line)."
  Figure 4 Link: articels_figures_by_rev_year\2021\A_Practical_O_N___ON_Outlier_Removal_Method_for_CorrespondenceBased_Point_Cloud_\figure_4.jpg
  Figure 4 caption: The coarse-to-fine IRLS. SA-Cauchy function with a larger s has
    a smoother weight curve, so more correspondences can take part in the optimization.
    As s becomes smaller, the function curve becomes sharper and the estimation becomes
    more precise.
  Figure 5 Link: articels_figures_by_rev_year\2021\A_Practical_O_N___ON_Outlier_Removal_Method_for_CorrespondenceBased_Point_Cloud_\figure_5.jpg
  Figure 5 caption: Results on simulated data. Subfigures (a), (b), (c), and (d) report
    the rotation error, translation error, running time, and number of inliers preserved
    by each method, respectively.
  Figure 6 Link: articels_figures_by_rev_year\2021\A_Practical_O_N___ON_Outlier_Removal_Method_for_CorrespondenceBased_Point_Cloud_\figure_6.jpg
  Figure 6 caption: "Qualitative evaluation of the proposed method on the ETH dataset.\
    \ Left column: Initial correspondence set H , where green lines are inlier correspondences\
    \ and red lines are outliers. Middle column: Subset H \u2032 after outlier removal.\
    \ Right column: SA-Cauchy registration results. The first \u223C fifth rows are\
    \ scan pairs from the Arch, Courtyard, Facade, Office, and Trees, respectively."
  Figure 7 Link: articels_figures_by_rev_year\2021\A_Practical_O_N___ON_Outlier_Removal_Method_for_CorrespondenceBased_Point_Cloud_\figure_7.jpg
  Figure 7 caption: "Individual evaluations on the challenging ETH dataset. Subfigures\
    \ (a) \u223C (e) show the results on the Arch, Courtyard, Facade, Office, and\
    \ Trees, respectively. The results of rotation error E R are displayed in the\
    \ left and the results of translation error E t are in the right. For better visualization,\
    \ only partial results of the Courtyard, Facade, and Trees are displayed."
  Figure 8 Link: articels_figures_by_rev_year\2021\A_Practical_O_N___ON_Outlier_Removal_Method_for_CorrespondenceBased_Point_Cloud_\figure_8.jpg
  Figure 8 caption: Our scene reconstruction results. Left is the result of Hannover
    reconstructed by 468 laser scans and right is the result of Bremen with 100 LiDAR
    scans.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.72
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.72
  Name of the first author: Jiayuan Li
  Name of the last author: Jiayuan Li
  Number of Figures: 8
  Number of Tables: 4
  Number of authors: 1
  Paper title: A Practical O( N 2 ) O(N2) Outlier Removal Method for Correspondence-Based
    Point Cloud Registration
  Publication Date: 2021-03-09 00:00:00
  Table 1 caption: TABLE 1 Detailed Settings of the Compared Algorithms (MNI Represents
    Maximum Number of Iterations)
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Detailed Information of the ETH Dataset
  Table 3 caption: TABLE 3 Registration Results on the ETH Dataset
  Table 4 caption: TABLE 4 Success Rates on the ETH Dataset
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3065021
- Affiliation of the first author: tklndst, college of computer science, nankai university,
    tianjin, china
  Affiliation of the last author: infervision, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\SANet_A_SliceAware_Network_for_Pulmonary_Nodule_Detection\figure_1.jpg
  Figure 1 caption: Examples of the pulmonary nodules in the PN9. Each image belongs
    to a different class of nodules. SN, GGN, and PSN denote solid, ground-glass,
    and part-solid nodules, respectively. The size of each nodule is labeled in the
    parentheses. The first and third rows are the complete slices, while the rest
    two rows are zoomed-in images, respectively.
  Figure 10 Link: articels_figures_by_rev_year\2021\SANet_A_SliceAware_Network_for_Pulmonary_Nodule_Detection\figure_10.jpg
  Figure 10 caption: Visualization of some cases that our SANet fails. The first two
    columns are nodules of 0-3 SN, and the last two columns are nodules of 0-5 PSN.
  Figure 2 Link: articels_figures_by_rev_year\2021\SANet_A_SliceAware_Network_for_Pulmonary_Nodule_Detection\figure_2.jpg
  Figure 2 caption: Overall architecture of the proposed slice-aware network (SANet).
    The red dashed box represents the nodule candidate. And the CT images below are
    zoomed-in images of the ones above.
  Figure 3 Link: articels_figures_by_rev_year\2021\SANet_A_SliceAware_Network_for_Pulmonary_Nodule_Detection\figure_3.jpg
  Figure 3 caption: "The slice grouped non-local module (SGNL). After three 1\xD7\
    1\xD71 convolutional layers, the feature maps are divided into multiple groups\
    \ along the depth dimension. The depth dimension is grouped into D \u2032 =DG\
    \ , where G is the group number."
  Figure 4 Link: articels_figures_by_rev_year\2021\SANet_A_SliceAware_Network_for_Pulmonary_Nodule_Detection\figure_4.jpg
  Figure 4 caption: Statistics of the proposed PN9 dataset. (a) Slice thickness distribution
    of CT scans. (b) Distribution of nodule count in one patient. (c) Percentage of
    CT manufacturer.
  Figure 5 Link: articels_figures_by_rev_year\2021\SANet_A_SliceAware_Network_for_Pulmonary_Nodule_Detection\figure_5.jpg
  Figure 5 caption: Statistics of class in PN9. (a)Taxonomy of the PN9 dataset. It
    contains four super-classes and nine sub-classes. The percentage represents the
    proportion of a certain class of nodules to all nodules. (b) Mutual dependencies
    among super-classes. (c) Mutual dependencies among sub-classes.
  Figure 6 Link: articels_figures_by_rev_year\2021\SANet_A_SliceAware_Network_for_Pulmonary_Nodule_Detection\figure_6.jpg
  Figure 6 caption: FROC curves of compared methods and our SANet.
  Figure 7 Link: articels_figures_by_rev_year\2021\SANet_A_SliceAware_Network_for_Pulmonary_Nodule_Detection\figure_7.jpg
  Figure 7 caption: FROC IoU curves of compared methods and our SANet.
  Figure 8 Link: articels_figures_by_rev_year\2021\SANet_A_SliceAware_Network_for_Pulmonary_Nodule_Detection\figure_8.jpg
  Figure 8 caption: 'Qualitative comparison of central slices for our SANet and other
    methods. The first row to the fifth row show the comparison results with different
    nodule classes: 3-10 SN, 10-30 SN, > 5 GGN, >5 PSN, and CN, respectively. (a)
    Ground truth. (b)-(i) Detection results of Faster R-CNN [34], RetinaNet [38],
    SSD512 [12], Leaky Noisy-OR [7], 3D Faster R-CNN [23], DeepLung [23], NoduleNet
    (N 2 ) [71], I3DR-Net [43], DeepSEED [39], and our SANet.'
  Figure 9 Link: articels_figures_by_rev_year\2021\SANet_A_SliceAware_Network_for_Pulmonary_Nodule_Detection\figure_9.jpg
  Figure 9 caption: The precision-recall curve for the nodule detection on the small-scale
    pulmonary nodule testing dataset. The Doctor1 and Doctor2 denote the detection
    results of two experienced doctors.
  First author gender probability: 0.99
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Jie Mei
  Name of the last author: Huan Zhang
  Number of Figures: 10
  Number of Tables: 10
  Number of authors: 5
  Paper title: 'SANet: A Slice-Aware Network for Pulmonary Nodule Detection'
  Publication Date: 2021-03-09 00:00:00
  Table 1 caption: TABLE 1 Comparison With the Existing Datasets of the Pulmonary
    Nodule
  Table 10 caption: TABLE 10 Analysis of How Different CT Manufacturers Affect the
    Performance
  Table 2 caption: TABLE 2 Comparison of Our SANet and Other Methods in Terms of FROC
    on Dataset PN9
  Table 3 caption: TABLE 3 Comparison of Our SANet and Other Methods in Terms of FROC
    IoU IoU (%) on Dataset PN9
  Table 4 caption: TABLE 4 Comparison of Our SANet and Other Methods in Terms of FROC
    (%) on the Dataset LUNA16 [30]
  Table 5 caption: TABLE 5 Comparison of SANet and NoduleNet Based on AP
  Table 6 caption: TABLE 6 Ablation Study for the Proposed SGNL and FPR (%)
  Table 7 caption: TABLE 7 Comparison of Other Methods Equipped With the Proposed
    SGNL and FPR Modules (%)
  Table 8 caption: TABLE 8 Ablation Study for the Proposed SGNL Module With Different
    Configurations of the SGNL Block (%)
  Table 9 caption: TABLE 9 Ablation Study for the Proposed SGNL Module With Different
    Numbers of Groups G G (%)
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3065086
- Affiliation of the first author: school of cyber science and security, university
    of science and technology of china, hefei, anhui, china
  Affiliation of the last author: school of cyber science and security, university
    of science and technology of china, hefei, anhui, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Deep_Model_Intellectual_Property_Protection_via_Deep_Watermarking\figure_1.jpg
  Figure 1 caption: The simplest watermarking mechanism by adding unified visible
    watermarks onto output of the target model, which will sacrifice the visual quality
    and usability.
  Figure 10 Link: articels_figures_by_rev_year\2021\Deep_Model_Intellectual_Property_Protection_via_Deep_Watermarking\figure_10.jpg
  Figure 10 caption: "The relationship between the extracting ability and the surrogate\
    \ model performance. Left: the extracting success rates ( SRNC,SRC ) and performance\
    \ (PSNR) change for the surrogate model CNet equipped with different channel numbers\
    \ (from 8 to 256). Right: One visual example (the first row): the middle two column\
    \ represent the surrogate model output for CNet8 and CNet256 respectively while\
    \ the first and the last column represent the input image with \u201CBone\u201D\
    \ and the original target model output. The extracted watermarks are displayed\
    \ in the second row."
  Figure 2 Link: articels_figures_by_rev_year\2021\Deep_Model_Intellectual_Property_Protection_via_Deep_Watermarking\figure_2.jpg
  Figure 2 caption: The proposed deep watermarking framework by leveraging spatial
    invisible watermarking algorithms.
  Figure 3 Link: articels_figures_by_rev_year\2021\Deep_Model_Intellectual_Property_Protection_via_Deep_Watermarking\figure_3.jpg
  Figure 3 caption: The overall pipeline of the proposed deep invisible watermarking
    algorithm and two-stage training strategy. In the first training stage, a basic
    watermark embedding sub-network H and extractor sub-network R are trained. Then
    another surrogate network SM is leveraged as the adversarial competitor to further
    enhance the extracting ability of R .
  Figure 4 Link: articels_figures_by_rev_year\2021\Deep_Model_Intellectual_Property_Protection_via_Deep_Watermarking\figure_4.jpg
  Figure 4 caption: Illustration diagram to show the training difference between the
    original target model and self-watermarked target model.
  Figure 5 Link: articels_figures_by_rev_year\2021\Deep_Model_Intellectual_Property_Protection_via_Deep_Watermarking\figure_5.jpg
  Figure 5 caption: "Some visual examples to show the capability of the proposed deep\
    \ invisible watermarking algorithm: (A) watermark-free image b i from domain B\
    \ , (B) watermarked image b \u2032 i from domain B \u2032 (C) the residual between\
    \ b i and b \u2032 i (enhanced 10\xD7), (D) ground-truth watermark, (E) extracted\
    \ watermark from b \u2032 i . The first column is the results of traditional spatial\
    \ bit-based invisible watermarking algorithms (64-bit embedded), and the middle\
    \ and right parts are the results of our methods for the debone and deraining\
    \ tasks respectively."
  Figure 6 Link: articels_figures_by_rev_year\2021\Deep_Model_Intellectual_Property_Protection_via_Deep_Watermarking\figure_6.jpg
  Figure 6 caption: The gray histogram comparison between watermark-free image and
    watermarked image for the Derain-Flower case.
  Figure 7 Link: articels_figures_by_rev_year\2021\Deep_Model_Intellectual_Property_Protection_via_Deep_Watermarking\figure_7.jpg
  Figure 7 caption: "Example output image and corresponding extracted watermark from\
    \ different surrogate models. (A) input watermark-free image a i ; (B) watermarked\
    \ image b \u2032 i ; (C) \u223C (F) are cases for different network structures\
    \ with L 2 loss: CNet, Res9, Res16 and UNet in turn; (G) \u223C (K) are cases\
    \ for different loss combinations with UNet: L 2, L 2+ L adv , L 1, L 1+ L adv\
    \ and L perc + L adv in turn."
  Figure 8 Link: articels_figures_by_rev_year\2021\Deep_Model_Intellectual_Property_Protection_via_Deep_Watermarking\figure_8.jpg
  Figure 8 caption: "Visual results of traditional spatial invisible watermarking:\
    \ (a) watermark-free image b i , (b) watermarked image b \u2032 i , (c) and (d)\
    \ are the outputs of surrogate model (UNet and Res9 with L 2, respectively), (e)\
    \ ground-truth watermark, (f) \u223C (h) are the corresponding extracted watermark\
    \ from (b) \u223C (d). It is defined as successful extracting when the extracted\
    \ pattern matches mostly with ground-truth pattern like pattern in blue and orange\
    \ box, otherwise as failure like pattern in red box."
  Figure 9 Link: articels_figures_by_rev_year\2021\Deep_Model_Intellectual_Property_Protection_via_Deep_Watermarking\figure_9.jpg
  Figure 9 caption: "Some visual examples of the extractor against watermark overwriting.\
    \ Three different watermark images (a1 \u223C a3) are considered as attackers\
    \ watermark, and the corresponding second and third columns are the extracted\
    \ watermark by our extractor without and with the enhanced training strategy."
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.53
  Name of the first author: Jie Zhang
  Name of the last author: Nenghai Yu
  Number of Figures: 15
  Number of Tables: 11
  Number of authors: 7
  Paper title: Deep Model Intellectual Property Protection via Deep Watermarking
  Publication Date: 2021-03-09 00:00:00
  Table 1 caption: TABLE 1 Quantitative Results of the Proposed Deep Image Based Invisible
    Watermarking Algorithm
  Table 10 caption: TABLE 10 Quantitative Results of Applying the Proposed Framework
    to Data (DPED [67]) and Traditional Algorithms (RTV [68]) Protection
  Table 2 caption: TABLE 2 The Success Rate ( S R NC S R C SRNCSRC) of Our Method
    Resisting the Attack From Surrogate Models
  Table 3 caption: "TABLE 3 The Comparison of Embedding & Extracting Ability (Column\
    \ 2 \u223C \u223C 4) and Robustness to Surrogate Model Attack (Column 5 \u223C\
    \ \u223C 8) Among Our Method and Two Typical Methods in Debone Task"
  Table 4 caption: TABLE 4 Quantitative Results With Different Watermark Images for
    Watermark Overwriting
  Table 5 caption: TABLE 5 Quantitative Results of Our Method With Different Size
    Watermark Images
  Table 6 caption: TABLE 6 Quantitative Results of Our Method With Different Size
    Watermark Images
  Table 7 caption: "TABLE 7 Comparison of the Original Embedding and Extracting Ability\
    \ Between the Multiple-Watermark ( \u2217 ) and Per-Watermark-Per-Network Setting"
  Table 8 caption: "TABLE 8 Comparison of the Success Rate ( S R NC SRNC S R C SRC)\
    \ Against Surrogate Model Attack Between the Multiple-Watermark( \u2217 ) and\
    \ Per-Watermark-Per-Network Setting"
  Table 9 caption: "TABLE 9 The Performance Comparison Between the Original Target\
    \ Model (\u201Coriginal\u201D) and Self-Watermarked Target Model (\u201CFlower\u201D\
    )"
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3064850
- Affiliation of the first author: "ecovision lab - photogrammetry and remote sensing,\
    \ eth zurich, z\xFCrich, switzerland"
  Affiliation of the last author: "ecovision lab - photogrammetry and remote sensing,\
    \ eth zurich, z\xFCrich, switzerland"
  Figure 1 Link: articels_figures_by_rev_year\2021\Gating_Revisited_Deep_MultiLayer_RNNs_That_can_be_Trained\figure_1.jpg
  Figure 1 caption: (a) General structure of an unfolded deep RNN (b) Detail of the
    gradient backpropagation in the two dimensional lattice.
  Figure 10 Link: articels_figures_by_rev_year\2021\Gating_Revisited_Deep_MultiLayer_RNNs_That_can_be_Trained\figure_10.jpg
  Figure 10 caption: Test accuracy versus training time for the gesture recognition
    task (Jester), four layers networks.
  Figure 2 Link: articels_figures_by_rev_year\2021\Gating_Revisited_Deep_MultiLayer_RNNs_That_can_be_Trained\figure_2.jpg
  Figure 2 caption: "Mean value of gradient magnitude with respect to the parameters\
    \ for different RNN units. top row: loss L( h L T ) only on final prediction.\
    \ bottom row: loss L( h L 1 \u2026 h L T ) over all time steps. As the gradients\
    \ flow back through time and layers, for a network of vanilla RNN units they get\
    \ amplified; for LSTM units they get attenuated; whereas the proposed STAR unit\
    \ approximately preserves their magnitude. See the Appendix, available in the\
    \ online supplemental material, for the results with the real data."
  Figure 3 Link: articels_figures_by_rev_year\2021\Gating_Revisited_Deep_MultiLayer_RNNs_That_can_be_Trained\figure_3.jpg
  Figure 3 caption: 'RNN cell structures: STAR, GRU, and LSTM, respectively.'
  Figure 4 Link: articels_figures_by_rev_year\2021\Gating_Revisited_Deep_MultiLayer_RNNs_That_can_be_Trained\figure_4.jpg
  Figure 4 caption: Gradient magnitudes of pix-by-pix MNIST. (a) Mean gradient norm
    per layer at the start of training. (b) Evolution of gradient norm during first
    training epoch. (c) Loss during first epoch.
  Figure 5 Link: articels_figures_by_rev_year\2021\Gating_Revisited_Deep_MultiLayer_RNNs_That_can_be_Trained\figure_5.jpg
  Figure 5 caption: Accuracy results for pixel-by-pixel MNIST tasks.
  Figure 6 Link: articels_figures_by_rev_year\2021\Gating_Revisited_Deep_MultiLayer_RNNs_That_can_be_Trained\figure_6.jpg
  Figure 6 caption: Performance comparison for adding problem.
  Figure 7 Link: articels_figures_by_rev_year\2021\Gating_Revisited_Deep_MultiLayer_RNNs_That_can_be_Trained\figure_7.jpg
  Figure 7 caption: Performance comparison for copy memory task.
  Figure 8 Link: articels_figures_by_rev_year\2021\Gating_Revisited_Deep_MultiLayer_RNNs_That_can_be_Trained\figure_8.jpg
  Figure 8 caption: Time series classification. (a) Crop classes. (b) Hand gestures
    (convolutional RNNs).
  Figure 9 Link: articels_figures_by_rev_year\2021\Gating_Revisited_Deep_MultiLayer_RNNs_That_can_be_Trained\figure_9.jpg
  Figure 9 caption: Accuracy versus number of model parameters for the gesture recognition
    task (Jester).
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Mehmet Ozgur Turkoglu
  Name of the last author: Konrad Schindler
  Number of Figures: 12
  Number of Tables: 6
  Number of authors: 4
  Paper title: 'Gating Revisited: Deep Multi-Layer RNNs That can be Trained'
  Publication Date: 2021-03-09 00:00:00
  Table 1 caption: TABLE 1 Performance Comparison for Pixel-by-Pixel MNIST Tasks
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Performance Comparison for Time Series Crop Classification
  Table 3 caption: TABLE 3 Performance Comparison for Music Task
  Table 4 caption: TABLE 4 Performance Comparison for PennTreebank Character-Level
    Language Modeling
  Table 5 caption: TABLE 5 Performance Comparison for the Gesture Recognition Task
    (Jester)
  Table 6 caption: TABLE 6 Performance Comparison for TUM Pixel-Wise Image Classification
    Task
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3064878
- Affiliation of the first author: state key laboratory of virtual reality technology
    and systems, school of computer science and engineering, beihang university, beijing,
    china
  Affiliation of the last author: department of computer science, university of kentucky,
    lexington, ky, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\PartLevel_Car_Parsing_and_Reconstruction_in_Single_Street_View_Images\figure_1.jpg
  Figure 1 caption: An illustration of pose and shape estimation based on instance
    masks, landmarks, and semantic parts. Pose and shape results of image (a) and
    (b) are outputted from [15] and our network. Green and red arrows represent ground
    truth and predicted axes respectively. (First row) Image patches cropped for visualization.
    Red boxes enclose the target cars that are either low-resolution or occluded.
    (Second row) Instance masks and estimated car pose and shape. (Third row) 2D landmarks
    and estimated car pose and shape. (Fourth row) Semantic parts and estimated car
    pose and shape.
  Figure 10 Link: articels_figures_by_rev_year\2021\PartLevel_Car_Parsing_and_Reconstruction_in_Single_Street_View_Images\figure_10.jpg
  Figure 10 caption: Visualization of reconstructed results with varying dimensions
    K . An SUV and a minivan are selected to be reconstructed with different thresholds
    thr . As K increases, we can see that the mean vertex errors decrease and reconstructed
    shapes are more accurate. In our approach, we choose K=22 which can be used to
    reconstruct more than 97.86 percent of details of all models.
  Figure 2 Link: articels_figures_by_rev_year\2021\PartLevel_Car_Parsing_and_Reconstruction_in_Single_Street_View_Images\figure_2.jpg
  Figure 2 caption: An overview of our approach. It consists of two stages, part segmentation
    and pose and shape estimation enclosed by green and grey boxes respectively. We
    further show the details of part feature fusion in the zoomed yellow box. Following
    the disentangled components prediction on the 2D image plane, 3D per-vertex alignment
    is used to refine results.
  Figure 3 Link: articels_figures_by_rev_year\2021\PartLevel_Car_Parsing_and_Reconstruction_in_Single_Street_View_Images\figure_3.jpg
  Figure 3 caption: 'Illustration of procedure to build dense correspondences. The
    whole procedure include template selection(the red box) and dense correspondences(the
    blue box). The dense correspondences stage can be divided into three step: 1.
    Deforming models without annotations based on selected template; 2. Transferring
    annotations from template; 3. Mapping and tire assembly.'
  Figure 4 Link: articels_figures_by_rev_year\2021\PartLevel_Car_Parsing_and_Reconstruction_in_Single_Street_View_Images\figure_4.jpg
  Figure 4 caption: Synthesized examples (Syn.) by applying textures, background images
    and projective (Proj.) textures. (a) Car model and its part annotation; (b) Texture
    1 and synthesized car; (c) Texture 2 and synthesized car; (d) Background image
    is added; (e) Projective texture is applied to simulate more complex lighting
    conditions.
  Figure 5 Link: articels_figures_by_rev_year\2021\PartLevel_Car_Parsing_and_Reconstruction_in_Single_Street_View_Images\figure_5.jpg
  Figure 5 caption: (Left) Examples of 3D car models. (Top right) Examples of synthesized
    images. (Bottom right) Distributions of car orientations and distances in 2D synthesized
    dataset.
  Figure 6 Link: articels_figures_by_rev_year\2021\PartLevel_Car_Parsing_and_Reconstruction_in_Single_Street_View_Images\figure_6.jpg
  Figure 6 caption: The details of network for estimation of translation, rotation,
    shape, and semantic parts in 3D space. Each prediction branch consists of several
    convolutional layers.
  Figure 7 Link: articels_figures_by_rev_year\2021\PartLevel_Car_Parsing_and_Reconstruction_in_Single_Street_View_Images\figure_7.jpg
  Figure 7 caption: Illustration of the mean vertex error E w , which can be decomposed
    into E s , E r and E t . Cars in blue boxes represent the ground truth and cars
    in red boxes are the predicted results. We can find that, due to reconstruction
    ambiguities and varying loss magnitudes, it is necessary to impose supervision
    on the shape after each transformation from the model space.
  Figure 8 Link: articels_figures_by_rev_year\2021\PartLevel_Car_Parsing_and_Reconstruction_in_Single_Street_View_Images\figure_8.jpg
  Figure 8 caption: The examples of part parsing results. The first column contains
    the input images selected from the ApolloScape dataset (cropped for visualization).
    The second column contains parsing results without class-consistent loss. The
    third column contains results based on our class-consistent method. The fourth
    column contains the ground truth annotations.
  Figure 9 Link: articels_figures_by_rev_year\2021\PartLevel_Car_Parsing_and_Reconstruction_in_Single_Street_View_Images\figure_9.jpg
  Figure 9 caption: Performance comparison of the global alignment and local embedded
    deformation. Two different input shapes are selected to be transferred to the
    template shape. For each input shape, we show the alignment errors in the first
    row and transferred annotations in the second row. With the initial alignment
    (e.g., only applying translation between two car centroids), the alignment errors
    are large and transferred parts also cannot be used. When the global alignment
    is applied, errors are reduced except the regions that contain large non-linear
    deformations such as mirrors, tailgate, rear window, and lights. After the embedded
    deformation, we can achieve mean vertex errors less than 0.01m and accurate part
    annotations. Note that tires are not included in the transformation and they are
    assembled in the post-processing step.
  First author gender probability: 0.77
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Qichuan Geng
  Name of the last author: Ruigang Yang
  Number of Figures: 13
  Number of Tables: 5
  Number of authors: 7
  Paper title: Part-Level Car Parsing and Reconstruction in Single Street View Images
  Publication Date: 2021-03-09 00:00:00
  Table 1 caption: TABLE 1 The IoU Scores for Part-Level Parsing
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 A P 3D AP3D and A P BEV APBEV on KITTI Validation (IoU=0.7)
  Table 3 caption: TABLE 3 Ablation Results on the Validation Set
  Table 4 caption: "TABLE 4 Comparison With \u201CHuman\u201D Performance, 3D-RCNN\
    \ and DeepMANTA Algorithms on 1,041 Testing Images"
  Table 5 caption: TABLE 5 Evaluation of Car Orientation Using Average Precision (AP)
    for Detection, Average Orientation Similarity (AOS), and Orientation Score (OS)
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3064837
