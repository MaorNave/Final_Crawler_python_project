- Affiliation of the first author: school of computing science, simon fraser university,
    burnaby, b.c., canada
  Affiliation of the last author: school of computing science, simon fraser university,
    burnaby, b.c., canada
  Figure 1 Link: articels_figures_by_rev_year\2015\GaussianBased_Hue_Descriptors\figure_1.jpg
  Figure 1 caption: "The spectral reflectance of Munsell 5 YR 56 (dashed black) illuminated\
    \ by D65 and its metameric rectangular (dashed blue), subtractive Gaussian (dashed\
    \ green), and wraparound Gaussian (solid red) spectra. Results are for the CIE1931\
    \ x \xAF y \xAF z \xAF 2-degree standard observer."
  Figure 10 Link: articels_figures_by_rev_year\2015\GaussianBased_Hue_Descriptors\figure_10.jpg
  Figure 10 caption: "Hue classification using KSM \u03BC versus CIECAM02 hue of the\
    \ Flowers image from the Columbia University spectral database [28]. First and\
    \ second rows depict the classification results for illuminants D65 and A, respectively.\
    \ Left panel: Approximate sRGB rendering of the image. Middle: segmentation based\
    \ on \u03BC. Right: classification based on CIECAM02 hue. Each pixel is colored\
    \ to roughly represent the hue name assigned to it."
  Figure 2 Link: articels_figures_by_rev_year\2015\GaussianBased_Hue_Descriptors\figure_2.jpg
  Figure 2 caption: "Two metameric subtractive Gaussian functions, one of type G+\
    \ (black) and type G\u2212 (red). Under D65, both these reflectances have CIE\
    \ XYZ values (63.69, 64.97, 20.95). Parameters (\u03B1,\u03C3,\u03D5) defining\
    \ these G+ and G\u2212 spectra are (0.8670, 93.5557, 621.1427) and (\u20131.0000,\
    \ 118.4952, 380.0106), respectively."
  Figure 3 Link: articels_figures_by_rev_year\2015\GaussianBased_Hue_Descriptors\figure_3.jpg
  Figure 3 caption: "Three G\u2212 spectra having the same chromaticity but with their\
    \ XYZ differing by a scale factor. Red curve: G\u2212 spectrum having XYZ = (77.9114,\
    \ 79.2585, 20.8546). Dotted blue curve: 0.95 x XYZ. Dashed green curve: 0.9 x\
    \ XYZ. Parameters (\u03B1,\u03C3,\u03D5) defining these three G\u2212 spectra\
    \ are (\u20131.0000, 38.5005, 469.9995), (\u20130.8835, 56.2495, 455.2326) and\
    \ (\u20130.8922, 74.5022, 436.1658), respectively. Although the three curves appear\
    \ to intersect at a common point in the plot, they in fact do not."
  Figure 4 Link: articels_figures_by_rev_year\2015\GaussianBased_Hue_Descriptors\figure_4.jpg
  Figure 4 caption: "Plots of the 1600 papers from the Munsell glossy set as a function\
    \ of three different hue descriptors specified in degrees on the hue circle (see\
    \ text). Left, KSM hue \u03BC; middle, ADL hue \u03BB; right, CIECAM02 hue. Each\
    \ dot color only roughly approximates that of the corresponding Munsell paper\
    \ under illuminant C."
  Figure 5 Link: articels_figures_by_rev_year\2015\GaussianBased_Hue_Descriptors\figure_5.jpg
  Figure 5 caption: "Munsell hue versus hue descriptor specified in degrees. The triangle\
    \ interiors represent the approximate color under illuminant C of the Munsell\
    \ papers of different Munsell value, each at maximal chroma for the given value,\
    \ for the five hues 10B, 10G, 10Y, 10R, and 10PB. The triangle boundaries are\
    \ colored with the maximal chroma for the given Munsell hue. Left to right the\
    \ plots are of the KSM Gaussian peak wavelength \u03BC, the ADL rectangular central\
    \ wavelength \u03BB, and CIECAM02 hue. The vertical alignment in the left and\
    \ right panels shows that papers of the same Munsell hue but differing value are\
    \ all being assigned the same hue descriptor. In the central panel, there is some\
    \ mingling of the red with the yellow and of the blue with the purple hues."
  Figure 6 Link: articels_figures_by_rev_year\2015\GaussianBased_Hue_Descriptors\figure_6.jpg
  Figure 6 caption: "Hue misclassification rate for KSM \u03BC (grey) versus CIECAM\
    \ hue (black) over papers of the intermediate Munsell hues. The average misclassification\
    \ rate for all the hues combined is 31 percent for KSM \u03BC versus 41 percent\
    \ for CIECAM02 hue."
  Figure 7 Link: articels_figures_by_rev_year\2015\GaussianBased_Hue_Descriptors\figure_7.jpg
  Figure 7 caption: "NCS hue versus hue descriptor specified in degrees. The triangle\
    \ interiors represent the approximate color under illuminant C of the NCS papers\
    \ of hues R, Y50R, Y, G50Y, G, B50G, B, and R50B for chromaticness greater than\
    \ or equal to 40. The triangle boundaries are colored to indicate the given NCS\
    \ hue name. Left to right the plots are of the KSM Gaussian peak wavelength \u03BC\
    , the ADL rectangular central wavelength \u03BB, and CIECAM02 hue. The vertical\
    \ alignment in the left and right panels shows that papers of the same NCS hue\
    \ but differing chromaticness and blackness are all being correctly assigned the\
    \ same hue descriptor. In the central panel, there is some intermingling of the\
    \ red, orange and yellow."
  Figure 8 Link: articels_figures_by_rev_year\2015\GaussianBased_Hue_Descriptors\figure_8.jpg
  Figure 8 caption: "The color thesaurus samples from the 8 sets of color names (green,\
    \ red, blue, yellow, purple, brown, pink and orange) plotted in terms of their\
    \ KSM \u03BC (left) and CIECAM02 hue (right). A dot's color indicates the corresponding\
    \ hue set to which the sample belongs. The dashed vertical bars indicate the hue\
    \ boundaries minimizing the misclassification rate."
  Figure 9 Link: articels_figures_by_rev_year\2015\GaussianBased_Hue_Descriptors\figure_9.jpg
  Figure 9 caption: Spectra (peaking in left-to-right order) of the blue (B), neutral
    (N), green (G), yellow (Y), first red (R1) and second red (R2) illuminants used
    in Logvinenko and Tokunaga's experiments [27] . The plot colors indicate the corresponding
    spectrum's name, along with grey for N and dashed red for R2.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Hamidreza Mirzaei
  Name of the last author: Brian Funt
  Number of Figures: 16
  Number of Tables: 2
  Number of authors: 2
  Paper title: Gaussian-Based Hue Descriptors
  Publication Date: 2015-04-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 KSM, ADL and CIECAM02 Hue Shifts for D65 to A in Degrees
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 KSM, ADL and CIECAM02 Hue Shifts in Degrees for each Illuminant
      Pair
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2420560
- Affiliation of the first author: department of electrical and computer engineering,
    national university of singapore, 4 engineering drive 3, singapore
  Affiliation of the last author: department of electrical and computer engineering,
    national university of singapore, 4 engineering drive 3, singapore
  Figure 1 Link: articels_figures_by_rev_year\2015\Parsing_Based_on_Parselets_A_Unified_Deformable_Mixture_Model_for_Human_Parsing\figure_1.jpg
  Figure 1 caption: Parselets are image segments that can generally be obtained by
    low-level segmentation techniques and bear strong semantic meaning. With the Parselet
    representation, the human parsing can be converted into the structure learning
    problem upon Parselets. The instantiated Parselets, which are activated by our
    deformable mixture parsing model, directly provide accurate semantic labeling
    for human parsing.
  Figure 10 Link: articels_figures_by_rev_year\2015\Parsing_Based_on_Parselets_A_Unified_Deformable_Mixture_Model_for_Human_Parsing\figure_10.jpg
  Figure 10 caption: Comparison of parsing results between DMPM and enhanced DMPM.
    By generating more segments with the diversified parameter strategy and utilizing
    geometry information, the enhanced model can successfully remove the artifacts
    of the raw DMPM.
  Figure 2 Link: articels_figures_by_rev_year\2015\Parsing_Based_on_Parselets_A_Unified_Deformable_Mixture_Model_for_Human_Parsing\figure_2.jpg
  Figure 2 caption: The overview of the proposed human parsing pipeline. First, a
    pool of class-independent segments are generated by various bottom-up techniques.
    Then the early rejection based on hierarchal filtering is employed to prune the
    segments for each type of Patselets. Finally the DMPM is run on the filtered Parselet
    hypotheses. The instantiated Parselets directly provide pixel-level parsing results.
    (Best viewed in color.)
  Figure 3 Link: articels_figures_by_rev_year\2015\Parsing_Based_on_Parselets_A_Unified_Deformable_Mixture_Model_for_Human_Parsing\figure_3.jpg
  Figure 3 caption: Human decomposition based on different basic elements. The original
    image, Parselet based decomposition and joint-based decomposition are shown sequentially.
    It can be seen that the Parselet based decomposition is more suitable for human
    parsing.
  Figure 4 Link: articels_figures_by_rev_year\2015\Parsing_Based_on_Parselets_A_Unified_Deformable_Mixture_Model_for_Human_Parsing\figure_4.jpg
  Figure 4 caption: Over-segmentation results under different parameters. For the
    highlighted regions, super-pixels in the left image have consistent distribution
    and have large probability to form a suitable Pasrelet candidate. On the contrary,
    super-pixels in the right image have either pure black or pure white distribution
    and thus are hard to be merged in the early stage. For other regions, such as
    hair and face, the super-pixels for right images may lead to better Parselet candidates
    as they are more consistent with the boundary. Hence, the two over-segmentation
    results should be complementary to each other.
  Figure 5 Link: articels_figures_by_rev_year\2015\Parsing_Based_on_Parselets_A_Unified_Deformable_Mixture_Model_for_Human_Parsing\figure_5.jpg
  Figure 5 caption: Filtering based on hierarchical structure. First, a merging tree
    is constructed based on the order of hierarchical merging. Then we argue that
    besides having a large score, a node should be preserved only if it is a local
    maximal node. Here, the local maximal node means that the score of this node is
    larger than the scores of its ancestors and descendants for several layers. Thus,
    though the scores of nodes i,j,k are all above the threshold, only the node j
    is preserved as a hypothesis since it is a local maximal node.
  Figure 6 Link: articels_figures_by_rev_year\2015\Parsing_Based_on_Parselets_A_Unified_Deformable_Mixture_Model_for_Human_Parsing\figure_6.jpg
  Figure 6 caption: "The subgraph from our human \u201CAnd-Or\u201D graph. The diamonds,\
    \ rectangles, ellipses and ellipses with boundary represent \u201COr\u201D nodes,\
    \ \u201CAnd\u201D nodes, \u201CLeaf\u201D nodes and virtual \u201CLeaf\u201D nodes,\
    \ respectively."
  Figure 7 Link: articels_figures_by_rev_year\2015\Parsing_Based_on_Parselets_A_Unified_Deformable_Mixture_Model_for_Human_Parsing\figure_7.jpg
  Figure 7 caption: Trade-off between number of preserved hypotheses and maximal wIoU
    score for different filtering strategies.
  Figure 8 Link: articels_figures_by_rev_year\2015\Parsing_Based_on_Parselets_A_Unified_Deformable_Mixture_Model_for_Human_Parsing\figure_8.jpg
  Figure 8 caption: Trade-off between number of preserved hypotheses and maximal IoU
    score for different types of Parselets based on the proposed hierarchical filtering
    strategy.
  Figure 9 Link: articels_figures_by_rev_year\2015\Parsing_Based_on_Parselets_A_Unified_Deformable_Mixture_Model_for_Human_Parsing\figure_9.jpg
  Figure 9 caption: Comparison of parsing results. Original images, our results, Yamaguchi's
    results [8] and scene parsing based results [40] are shown sequentially.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jian Dong
  Name of the last author: Shuicheng Yan
  Number of Figures: 13
  Number of Tables: 4
  Number of authors: 5
  Paper title: 'Parsing Based on Parselets: A Unified Deformable Mixture Model for
    Human Parsing'
  Publication Date: 2015-04-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 18 Types of Parselets for Human
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Parselets versus Objects in Terms of the IoU
      Score on FS and DP Datasets
  Table 3 caption:
    table_text: TABLE 3 The IoU Scores for Each Type of Parselets on the FS and DP
      Datasets
  Table 4 caption:
    table_text: TABLE 4 Comparison of Human Parsing IoU Scores on FS and DP Datasets
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2420563
- Affiliation of the first author: national taiwan university, taipei, taiwan
  Affiliation of the last author: national taiwan university, taipei, taiwan
  Figure 1 Link: articels_figures_by_rev_year\2015\CoSegmentation_Guided_Hough_Transform_for_Robust_Feature_Matching\figure_1.jpg
  Figure 1 caption: "Feature matching by our approach. (a) Input images I P and I\
    \ Q together with all the detected feature points. (b) Initial co-segmentation\
    \ results by [36]. (c) Correspondences in the Homography space. (d) Hough voting\
    \ and its comparison with SIFT. 245 out of 275 correct correspondences in M are\
    \ identified via Hough voting. White lines denote the correct correspondences\
    \ detected by both approaches. Red and cyan lines are the correct correspondences\
    \ by only Hough voting and the nearest SIFT searching, respectively. (e) The refined\
    \ co-segmentation results by taking feature matchings into account. (f) Our approach\
    \ CoSeg-HV. It recommends 251 (=526\u2212275) correct candidates in M and leads\
    \ to additional 219 (=464\u2212245) correct correspondences (green lines) detected\
    \ by the successive Hough voting."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\CoSegmentation_Guided_Hough_Transform_for_Robust_Feature_Matching\figure_2.jpg
  Figure 2 caption: (a) The matching results of our approach in an image pair. The
    incorrect correspondences are drawn in black. Each correct correspondence is shown
    in a particular color according to the object that it belongs to. (b) The corresponding
    2 D homography space generated by using multi-dimensional scaling.
  Figure 3 Link: articels_figures_by_rev_year\2015\CoSegmentation_Guided_Hough_Transform_for_Robust_Feature_Matching\figure_3.jpg
  Figure 3 caption: The performances of various approaches on the six image pairs
    of the co-recognition dataset.
  Figure 4 Link: articels_figures_by_rev_year\2015\CoSegmentation_Guided_Hough_Transform_for_Robust_Feature_Matching\figure_4.jpg
  Figure 4 caption: "(a) Average endpoint errors of the top ranked correct correspondences\
    \ by CoSeg-HV. (b) \u223C (g) The feature correspondences detected by different\
    \ approaches on image pairs Books and Bulletins. In each figure, the adopted approach\
    \ as well as its performance (correct detectionscorrect candidates in M ) are\
    \ also shown. The correct correspondences are plotted in green, and the incorrect\
    \ ones are in black. Note that the performance shown here is not the recall defined\
    \ in Eq. (28). The denominator here is the number of correct correspondences in\
    \ M , while the denominator in recall is the number of correct correspondences\
    \ in set V P \xD7 V Q ."
  Figure 5 Link: articels_figures_by_rev_year\2015\CoSegmentation_Guided_Hough_Transform_for_Robust_Feature_Matching\figure_5.jpg
  Figure 5 caption: "The matching results by three different approaches on three image\
    \ pairs of the SYM-BENCH dataset [49], including image pair sanmarco2 in the first\
    \ column, image pair arch in the second column, and image pair riga in the third\
    \ column. (a) \u223C (i) The used approaches as well as their performances (correct\
    \ detectionscorrect candidates in M ) are attached below the figures. (j) \u223C\
    \ (l) The corresponding precision-recall curves on the three image pairs."
  Figure 6 Link: articels_figures_by_rev_year\2015\CoSegmentation_Guided_Hough_Transform_for_Robust_Feature_Matching\figure_6.jpg
  Figure 6 caption: Plug-in comparison with the LIOP and DAISY descriptors on six
    image pairs, including.
  Figure 7 Link: articels_figures_by_rev_year\2015\CoSegmentation_Guided_Hough_Transform_for_Robust_Feature_Matching\figure_7.jpg
  Figure 7 caption: (a) The performance of our approach with different numbers of
    co-segments. (b) The performance of our approach along the iterative procedure.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.64
  Name of the first author: Hsin-Yi Chen
  Name of the last author: Bing-Yu Chen
  Number of Figures: 7
  Number of Tables: 3
  Number of authors: 3
  Paper title: Co-Segmentation Guided Hough Transform for Robust Feature Matching
  Publication Date: 2015-04-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Performances in mAP of Our Approach and the Baselines
      on Three Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Effect of Using OCSVM on Our Approach
  Table 3 caption:
    table_text: TABLE 3 Step-Wise Running Time in Second
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2420556
- Affiliation of the first author: department of computer science and engineering,
    chinese university of hong kong. shatin, hong kong
  Affiliation of the last author: department of computer science and engineering,
    chinese university of hong kong. shatin, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2015\Multispectral_Joint_Image_Restoration_via_Optimizing_a_Scale_Map\figure_1.jpg
  Figure 1 caption: Appearance comparison of RGB and NIR images. (a) RGB image. (b)
    Corresponding NIR image. (c) Close-ups. The four columns are for channels R, G,
    B, and NIR.
  Figure 10 Link: articels_figures_by_rev_year\2015\Multispectral_Joint_Image_Restoration_via_Optimizing_a_Scale_Map\figure_10.jpg
  Figure 10 caption: Shadow detection in multispectral restoration. Our shadow detection
    results in (d) are more accurate than those of [22] (shown in (c)). (e) are results
    of our method after shadow detection.
  Figure 2 Link: articels_figures_by_rev_year\2015\Multispectral_Joint_Image_Restoration_via_Optimizing_a_Scale_Map\figure_2.jpg
  Figure 2 caption: Optimal scale map s computed from images in Fig. 1 according to
    Eq. (1) . Dark to bright pixels correspond to negative to positive values in different
    scales.
  Figure 3 Link: articels_figures_by_rev_year\2015\Multispectral_Joint_Image_Restoration_via_Optimizing_a_Scale_Map\figure_3.jpg
  Figure 3 caption: 1D illustration. (a) contains patches in the color image, NIR
    image and s map. Plot (b) contains gradients along the vertical line in the top
    two patches. (c) shows corresponding s values. Most of them are zeros; positive
    and negative values are also allowed.
  Figure 4 Link: articels_figures_by_rev_year\2015\Multispectral_Joint_Image_Restoration_via_Optimizing_a_Scale_Map\figure_4.jpg
  Figure 4 caption: Isotropic versus anisotropic smoothing of the s map. Result in
    (b) resulted from anisotropic smoothing contains clearer and higher contrast structure.
    The input images are shown in Figs. 7a and 7b.
  Figure 5 Link: articels_figures_by_rev_year\2015\Multispectral_Joint_Image_Restoration_via_Optimizing_a_Scale_Map\figure_5.jpg
  Figure 5 caption: A shadow handling example. (a) is a high-noise-level RGB image
    and (b) is the corresponding NIR image with shadow. (c) is the result restored
    without shadow handling while (d) is the result considering shadow.
  Figure 6 Link: articels_figures_by_rev_year\2015\Multispectral_Joint_Image_Restoration_via_Optimizing_a_Scale_Map\figure_6.jpg
  Figure 6 caption: Multispectral shadow detection. Given noisy RGB image (a) and
    NIR image (b), our algorithm detects shadow regions in (b). (c) is the denoising
    result of (a) with the help of (b). (d) is the shadow removed and smoothed result
    of (b). (e) is the rough shadow map while (f) is obtained by applying labeling
    to (e). (g) is the exacted small structures of I 0 and (h) shows the final shadow
    detection result.
  Figure 7 Link: articels_figures_by_rev_year\2015\Multispectral_Joint_Image_Restoration_via_Optimizing_a_Scale_Map\figure_7.jpg
  Figure 7 caption: s map estimation in iterations. Given image pairs in (a) and (b),
    our method can get the high-quality restoration result in (c). The s maps in different
    iterations are shown in (e)-(h).
  Figure 8 Link: articels_figures_by_rev_year\2015\Multispectral_Joint_Image_Restoration_via_Optimizing_a_Scale_Map\figure_8.jpg
  Figure 8 caption: "Comparison with guided image filter [12]. (c) is the guided image\
    \ filter result with r=6 and \u03F5= 0.03 2 . (d) is our result computed with\
    \ \u03BB=2.5 and \u03B2=0.5 ."
  Figure 9 Link: articels_figures_by_rev_year\2015\Multispectral_Joint_Image_Restoration_via_Optimizing_a_Scale_Map\figure_9.jpg
  Figure 9 caption: Restoration results by varying parameters. The inputs are shown
    in Figs. 6 a and 6b.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.65
  Name of the first author: Xiaoyong Shen
  Name of the last author: Jiaya Jia
  Number of Figures: 19
  Number of Tables: 0
  Number of authors: 5
  Paper title: Multispectral Joint Image Restoration via Optimizing a Scale Map
  Publication Date: 2015-04-08 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2417569
- Affiliation of the first author: department of computer science, university of toronto,
    ontario, canada
  Affiliation of the last author: adobe research, san francisco, california and the
    department of computer science, university of toronto, ontario, canada
  Figure 1 Link: articels_figures_by_rev_year\2015\Efficient_Optimization_for_Sparse_Gaussian_Process_Regression\figure_1.jpg
  Figure 1 caption: Exact vs approximate costs, based on the 1D example of Section
    4, with z=10 , n=200 .
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Efficient_Optimization_for_Sparse_Gaussian_Process_Regression\figure_2.jpg
  Figure 2 caption: Test performance on discrete datasets. (top row) BindingDB, values
    at each marker is the average of 150 runs (50-fold random traintest splits times
    three random initialization); (bottom row) HoG dataset, each marker is the average
    of 10 randomly initialized runs.
  Figure 3 Link: articels_figures_by_rev_year\2015\Efficient_Optimization_for_Sparse_Gaussian_Process_Regression\figure_3.jpg
  Figure 3 caption: Training time versus test performance on discrete datasets. (a)
    the average BindingDB training time; (b) the average BindingDB objective function
    value at convergence; (d) and (e) show test scores versus training time with m=32
    for a single run; (c) shows the trade-off between training time and testing SMSE
    on the HoG dataset with m=32 , for various methods including multiple variants
    of CholQR and CSI; (f) a zoomed-in version of (c) comparing the variants of CholQR.
  Figure 4 Link: articels_figures_by_rev_year\2015\Efficient_Optimization_for_Sparse_Gaussian_Process_Regression\figure_4.jpg
  Figure 4 caption: 'Snelson''s 1D example: prediction mean (red curves); one standard
    deviation in prediction uncertainty (green curves); inducing point initialization
    (black points at top of each figure); learned inducing point locations (the cyan
    points at the bottom, also overlaid on data for CholQR).'
  Figure 5 Link: articels_figures_by_rev_year\2015\Efficient_Optimization_for_Sparse_Gaussian_Process_Regression\figure_5.jpg
  Figure 5 caption: 'Test scores on KIN40K as function of number of inducing points:
    for each number of inducing points the value plotted is averaged over 10 runs
    from 10 different (shared) initializations.'
  Figure 6 Link: articels_figures_by_rev_year\2015\Efficient_Optimization_for_Sparse_Gaussian_Process_Regression\figure_6.jpg
  Figure 6 caption: 'Comparison of SMSE on the four benchmark datasets: (left column)
    SMSE as a function of hyperparameter learning time; (right column) SMSE as a function
    of testing time. Both axes are log-scaled in all figures.'
  Figure 7 Link: articels_figures_by_rev_year\2015\Efficient_Optimization_for_Sparse_Gaussian_Process_Regression\figure_7.jpg
  Figure 7 caption: 'Comparison of SNLP on the four benchmark datasets: (left column)
    SNLP as a function of hyperparameter learning time; (right column) SNLP as a function
    of testing time. Horizontal (time) axes are log-scaled in all figures.'
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.69
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yanshuai Cao
  Name of the last author: Aaron Hertzmann
  Number of Figures: 7
  Number of Tables: 0
  Number of authors: 4
  Paper title: Efficient Optimization for Sparse Gaussian Process Regression
  Publication Date: 2015-04-20 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2424873
- Affiliation of the first author: microsoft research, escience group, redmond, wa
  Affiliation of the last author: microsoft research, escience group, redmond, wa
  Figure 1 Link: articels_figures_by_rev_year\2015\Capturing_Spatial_Interdependence_in_Image_Features_The_Counting_Grid_an_Epitomi\figure_1.jpg
  Figure 1 caption: "Feature counts change slightly as the field of view moves. For\
    \ example, the abundance of the \u201Ccar\u201D features is reduced, but the counts\
    \ of the features found on building facades are increased. The counting grid model\
    \ accounts for such changes naturally, and it can also account for images of different\
    \ scenes."
  Figure 10 Link: articels_figures_by_rev_year\2015\Capturing_Spatial_Interdependence_in_Image_Features_The_Counting_Grid_an_Epitomi\figure_10.jpg
  Figure 10 caption: Results on Torralba Dataset. We reported the results of [17]
    (Torralba's approach) and [23] (Epitome) from the original papers. We followed
    the evaluation procedure of [17] and the error bars indicate variability in accuracy
    across different image sequences.
  Figure 2 Link: articels_figures_by_rev_year\2015\Capturing_Spatial_Interdependence_in_Image_Features_The_Counting_Grid_an_Epitomi\figure_2.jpg
  Figure 2 caption: Counting grid illustration. i) Images and their Bag of feature
    representation. ii) Images of a train station, taken as windows into the larger
    scene. iii) Hand labeled features. iv) Scene reconstructed (e.g., counting grid)
    starting from bags taken from 50 windows (see the Text for details).
  Figure 3 Link: articels_figures_by_rev_year\2015\Capturing_Spatial_Interdependence_in_Image_Features_The_Counting_Grid_an_Epitomi\figure_3.jpg
  Figure 3 caption: Illustration of counting grids using different data at different
    levels of abstraction. At the top of each CG we show few examples of the training
    images from which we extracted the bags. The bottom row of each panel, illustrates
    the features. i) A counting grid learned using patches extracted from the train
    station toy example of Fig. 2. In this case all the bags come from windows into
    the same image, which is reconstructed on the grid. ii) A counting grid estimated
    from images taken with a wearable camera [18]. In this case we learned a dictionary
    of features (illustrated by the textons on the bottom) clustering image patches,
    as in [19]. To illustrate the office scene (e.g., the computer screens - see images
    on the top), we overlap these textons by as much as the patches were overlapping
    during feature extraction process, and then average to create a clearer visual
    representation. iii) A counting grid estimated starting from LabelMe annotations
    (see the top row) [20].
  Figure 4 Link: articels_figures_by_rev_year\2015\Capturing_Spatial_Interdependence_in_Image_Features_The_Counting_Grid_an_Epitomi\figure_4.jpg
  Figure 4 caption: "i) Counting grid generative model, ii) Two-dimensional variable\
    \ \u2113=( k x , k y ) points to a window that encompasses W x \xD7 W y sources\
    \ out of the E x \xD7 E y grid sources \u03C0 j,z , j\u2208[1.. E x ]\xD7[1..\
    \ E y ] . Each source is a distribution over features z . The uniform mixture\
    \ of sources captured in the window produces the observed counts c z . iii) In\
    \ the tessellated model S sections of the window generate S separate sets of features,\
    \ one for each section of the image. Neither the window size W nor the tessellation\
    \ S have to match the image resolution. When they both do, however, the model\
    \ is equivalent to a feature epitome, making epitomes a special case of tessellated\
    \ counting grids."
  Figure 5 Link: articels_figures_by_rev_year\2015\Capturing_Spatial_Interdependence_in_Image_Features_The_Counting_Grid_an_Epitomi\figure_5.jpg
  Figure 5 caption: "Tessellation step illustration. Once the features are extracted\
    \ and quantized one can decide tessellation S= S x \xD7 S y of the image and compute\
    \ the feature counts separately in each section as illustrated by the second and\
    \ third column. In the extreme when S=W= N x \xD7 N y , each section has a single\
    \ feature with non-zero count, and the learned tessellated counting grid will\
    \ be equivalent to a discrete feature epitome."
  Figure 6 Link: articels_figures_by_rev_year\2015\Capturing_Spatial_Interdependence_in_Image_Features_The_Counting_Grid_an_Epitomi\figure_6.jpg
  Figure 6 caption: "The source of 50 image patches taken from random locations i),\
    \ and counting grids estimated by various versions of the algorithm. Most remarkably\
    \ ii) is the reconstruction obtained using only 50 histograms of image features,\
    \ and for reconstruction in iii) we used only 50 sets of 4 histograms (from 2\xD7\
    2 sections of the input images). iv) Result using 16 histograms, (from 4\xD74\
    \ sections of the input images) v) Result using epitomes. In all the cases, colors\
    \ were treated as unrelated 64 discrete features. SEE THE VIDEOS IN THE ADDITIONAL\
    \ MATERIAL, which can be found on the Computer Society Digital Library at http:doi.ieeecomputersociety.org10.1109TPAMI.2015.2424864\
    \ available online."
  Figure 7 Link: articels_figures_by_rev_year\2015\Capturing_Spatial_Interdependence_in_Image_Features_The_Counting_Grid_an_Epitomi\figure_7.jpg
  Figure 7 caption: 15-Scenes classification results. Using pink circles, we also
    reported the results of [22] which are computed using the hybrid CG-Epitome (see
    Table 4). Due to the presence of many training images, the method generalizes
    very well.
  Figure 8 Link: articels_figures_by_rev_year\2015\Capturing_Spatial_Interdependence_in_Image_Features_The_Counting_Grid_an_Epitomi\figure_8.jpg
  Figure 8 caption: Images from the SenseCam dataset.
  Figure 9 Link: articels_figures_by_rev_year\2015\Capturing_Spatial_Interdependence_in_Image_Features_The_Counting_Grid_an_Epitomi\figure_9.jpg
  Figure 9 caption: Results for SenseCam dataset. i) mathbf S= 1 times 1 . ii) Tessellated
    version mathbf S= 2 times 2 . iii) Tessellated version mathbf S= 4 times 4 and
    comparison with the reconfigurable bag of words model [16] and with latent Dirichlet
    allocation using the same tessellation.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Alessandro Perina
  Name of the last author: Alessandro Perina
  Number of Figures: 11
  Number of Tables: 7
  Number of authors: 1
  Paper title: 'Capturing Spatial Interdependence in Image Features: The Counting
    Grid, an Epitomic Representation for Bags of Features'
  Publication Date: 2015-04-20 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Generative Approaches to Scene Analysis
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Relationship between Counting Grids and Other Computer Vision
      Methods
  Table 3 caption:
    table_text: TABLE 3 15-Scenes Dataset Results
  Table 4 caption:
    table_text: TABLE 4 MIT 67 Indoor Scenes Dataset Results
  Table 5 caption:
    table_text: TABLE 5 SenseCam Dataset Results
  Table 6 caption:
    table_text: TABLE 6 Where Was I?
  Table 7 caption:
    table_text: TABLE 7 Unsupervised Place Clustering
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2424864
- Affiliation of the first author: state key laboratory for novel software technology,
    nanjing university, china
  Affiliation of the last author: department of brain and cognitive engineering, korea
    university, seoul, republic of korea
  Figure 1 Link: articels_figures_by_rev_year\2015\SemiAutomatic_Segmentation_of_Prostate_in_CT_Images_via_Coupled_Feature_Represen\figure_1.jpg
  Figure 1 caption: Challenges in prostate CT segmentation. (a, b) Low contrast in
    CT image (without and with manual segmentation); (c) Large prostate motion, (d)
    large shape change, relative to the bones, even after bone-based alignment for
    the two CT images.
  Figure 10 Link: articels_figures_by_rev_year\2015\SemiAutomatic_Segmentation_of_Prostate_in_CT_Images_via_Coupled_Feature_Represen\figure_10.jpg
  Figure 10 caption: Comparison of mean Dice Ratio among different feature selection
    methods per patient using OF and CF, respectively.
  Figure 2 Link: articels_figures_by_rev_year\2015\SemiAutomatic_Segmentation_of_Prostate_in_CT_Images_via_Coupled_Feature_Represen\figure_2.jpg
  Figure 2 caption: A typical example to illustrate that the three different local
    regions prefer choosing different features. We adopt Lasso as feature selection
    method, and the features include LBP, Haar wavelet, and HoG, which will be discussed
    in the following section.
  Figure 3 Link: articels_figures_by_rev_year\2015\SemiAutomatic_Segmentation_of_Prostate_in_CT_Images_via_Coupled_Feature_Represen\figure_3.jpg
  Figure 3 caption: The major differences between the previous learning-based methods
    and our proposed method. Our proposed method employs the coupled feature representation,
    and adopts a local strategy for feature selection and prostate-likelihood estimation.
  Figure 4 Link: articels_figures_by_rev_year\2015\SemiAutomatic_Segmentation_of_Prostate_in_CT_Images_via_Coupled_Feature_Represen\figure_4.jpg
  Figure 4 caption: The flowchart of the proposed prostate segmentation method.
  Figure 5 Link: articels_figures_by_rev_year\2015\SemiAutomatic_Segmentation_of_Prostate_in_CT_Images_via_Coupled_Feature_Represen\figure_5.jpg
  Figure 5 caption: Typical examples for illustrating the sampling of the training
    voxels, with the red points denoting the prostate voxels and the blue points denoting
    the background voxels.
  Figure 6 Link: articels_figures_by_rev_year\2015\SemiAutomatic_Segmentation_of_Prostate_in_CT_Images_via_Coupled_Feature_Represen\figure_6.jpg
  Figure 6 caption: An illustration (three features, E=2 , j=1 ) of calculating the
    intra- and inter- coupled interactions. Matrix expansion is firstly applied to
    calculate each feature up to its second-order power (light blue, yellow, and violet
    are first-order power, and dark colors are corresponding second-order power),
    then intra- and inter- coupled interaction matrixes are calculated, and finally
    we combine them for coupled feature representation. Here, we take one voxel as
    an example.
  Figure 7 Link: articels_figures_by_rev_year\2015\SemiAutomatic_Segmentation_of_Prostate_in_CT_Images_via_Coupled_Feature_Represen\figure_7.jpg
  Figure 7 caption: An illustration for convergence of SCOTO (PA1).
  Figure 8 Link: articels_figures_by_rev_year\2015\SemiAutomatic_Segmentation_of_Prostate_in_CT_Images_via_Coupled_Feature_Represen\figure_8.jpg
  Figure 8 caption: Comparison of Dice Ratio, TPF, ASD, CD on 24 patients between
    OF+SCOTO and CF+SCOTO by using different maximum power number E .
  Figure 9 Link: articels_figures_by_rev_year\2015\SemiAutomatic_Segmentation_of_Prostate_in_CT_Images_via_Coupled_Feature_Represen\figure_9.jpg
  Figure 9 caption: Three typical prostate-likelihood maps showing that OF+SCOTO cannot
    achieve good results, but CF+SCOTO can. Red curves indicate manual delineations.
  First author gender probability: 0.55
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Yinghuan Shi
  Name of the last author: Dinggang Shen
  Number of Figures: 18
  Number of Tables: 4
  Number of authors: 6
  Paper title: Semi-Automatic Segmentation of Prostate in CT Images via Coupled Feature
    Representation and Spatial-Constrained Transductive Lasso
  Publication Date: 2015-04-20 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Experimental Results among Different Feature
      Selection Methods, with the Best Results Marked by Bold Font
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 The p -Values (the Dice Ratio) of OF+SCOTO and CF+SCOTO Compared
      with Related Feature Selection Methods: Lasso-S, tLasso-S, mRMR, Lasso-B, tLasso-B
      and Fused Lasso by Using Both the Original and Coupled Features, Respectively'
  Table 3 caption:
    table_text: TABLE 3 Normalized Selection Percentages for Three Kinds of Features
  Table 4 caption:
    table_text: TABLE 4 Comparison of Different Performance Measurements (Mean Dice
      Ratio, Median TPF, and Mean ASD) with Other Related Methods, with the Best Performance
      Marked by Bold Font
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2424869
- Affiliation of the first author: international research institute for multidisciplinary
    science at beihang university, beijing, china
  Affiliation of the last author: national engineering laboratory for video technology,
    peking university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2015\Finding_the_Secret_of_Image_Saliency_in_the_Frequency_Domain\figure_1.jpg
  Figure 1 caption: 'Spectral phase contains the secret for locating the salient targets,
    and spectral amplitude helps to determine the saliency strength. A I and A G :
    amplitude spectra; P I and P G : phase spectra. IDFT: inverse DFT.'
  Figure 10 Link: articels_figures_by_rev_year\2015\Finding_the_Secret_of_Image_Saliency_in_the_Frequency_Domain\figure_10.jpg
  Figure 10 caption: Performance of 18 saliency models on the 100 testing images of
    MIT1003. Note that the models in each group are listed from left to right with
    increasing FS scores.
  Figure 2 Link: articels_figures_by_rev_year\2015\Finding_the_Secret_of_Image_Saliency_in_the_Frequency_Domain\figure_2.jpg
  Figure 2 caption: Candidate filters used in the quantitative study, including the
    idealGaussian low-pass, high-pass and band-pass filters.
  Figure 3 Link: articels_figures_by_rev_year\2015\Finding_the_Secret_of_Image_Saliency_in_the_Frequency_Domain\figure_3.jpg
  Figure 3 caption: Sampled instances for evaluation. (a) density maps for fixated
    and non-fixated pixels; (b) Sampled fixated pixels (green) and non-fixated pixels
    (yellow).
  Figure 4 Link: articels_figures_by_rev_year\2015\Finding_the_Secret_of_Image_Saliency_in_the_Frequency_Domain\figure_4.jpg
  Figure 4 caption: Each Fourier coefficient is computed by dividing an image into
    two pairs of regions and computing the weighted contrast between each pair of
    regions.
  Figure 5 Link: articels_figures_by_rev_year\2015\Finding_the_Secret_of_Image_Saliency_in_the_Frequency_Domain\figure_5.jpg
  Figure 5 caption: Fourier transform can be interpreted as dividing an image with
    each of the 2 N 2 template pairs for contrast computation. As a consequence, saliency
    detection can be described as highlighting the most discriminative templates so
    as to correctly assign the energy only to salient targets in the inverse Fourier
    transform.
  Figure 6 Link: articels_figures_by_rev_year\2015\Finding_the_Secret_of_Image_Saliency_in_the_Frequency_Domain\figure_6.jpg
  Figure 6 caption: Coefficient maps obtained by ICA have different visual characteristics
    from the original color channels.
  Figure 7 Link: articels_figures_by_rev_year\2015\Finding_the_Secret_of_Image_Saliency_in_the_Frequency_Domain\figure_7.jpg
  Figure 7 caption: "Band-pass filters used in pre-filtering and post-filtering. In\
    \ pre-filtering, the band-pass filter acts as a high-pass filter which also suppresses\
    \ the highest frequency ( \u03C3 c =0.8 , \u03C3 w =0.4 ). In post-filtering,\
    \ the band-pass filter acts as a low-pass filter that contains a notch at the\
    \ zero frequency ( \u03C3 c =0.1 , \u03C3 w =0.2 )."
  Figure 8 Link: articels_figures_by_rev_year\2015\Finding_the_Secret_of_Image_Saliency_in_the_Frequency_Domain\figure_8.jpg
  Figure 8 caption: "Frequency spectrum should be re-adjusted after applying any operation\
    \ so as to remove the probable imaginary parts generated in inverse DFT. (a)-(b)\
    \ input images and amplitude spectra. (c) amplitude spectra filtered with a Gaussian\
    \ band-pass filter ( \u03C3 c =0.8 , \u03C3 w =0.4 ). (d)-(e) maps obtained by\
    \ inverse DFT (modulus calculated) and the new amplitude spectra."
  Figure 9 Link: articels_figures_by_rev_year\2015\Finding_the_Secret_of_Image_Saliency_in_the_Frequency_Domain\figure_9.jpg
  Figure 9 caption: A feature-specific convolution template can be learned for modulating
    the phase spectrum from each of the C feature channels.
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.74
  Name of the first author: Jia  Li
  Name of the last author: Yonghong Tian
  Number of Figures: 12
  Number of Tables: 3
  Number of authors: 5
  Paper title: Finding the Secret of Image Saliency in the Frequency Domain
  Publication Date: 2015-04-20 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance of Saliency Maps on Toronto When Various Amplitude
      and Phase Spectra Are Combined at Three Resolutions
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance of Our Approach on MIT1003 When Various Features
      Are Used
  Table 3 caption:
    table_text: TABLE 3 Performance of Our Approach on MIT1003 When Center-Biased
      Gaussian Re-Weihting Is Applied
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2424870
- Affiliation of the first author: school of electronic information and communications,
    huazhong university of science and technology, 1037 luoyu road, wuhan, hubei,
    p.r. china
  Affiliation of the last author: department of computer and information sciences,
    temple university, 1925 n. 12th street, philadelphia, pa
  Figure 1 Link: articels_figures_by_rev_year\2015\D_Shape_Matching_via_Two_Layer_Coding\figure_1.jpg
  Figure 1 caption: The pipeline of the proposed coding framework.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\D_Shape_Matching_via_Two_Layer_Coding\figure_2.jpg
  Figure 2 caption: The difference in the discriminative power between a single view
    and a view pair. (a) Some examples of unrecognized views. (b) Adding a second
    view significantly increases the recognition likelihood. This motives our usage
    of view pairs in encoding 3D shapes.
  Figure 3 Link: articels_figures_by_rev_year\2015\D_Shape_Matching_via_Two_Layer_Coding\figure_3.jpg
  Figure 3 caption: "Two 3D models from the category of \u201CBack Doors\u201D and\
    \ \u201CRectangular Housings\u201D from the ESB dataset. As shown in the second\
    \ column, they have a similar view, but their view pairs that form the same angles\
    \ are quite different."
  Figure 4 Link: articels_figures_by_rev_year\2015\D_Shape_Matching_via_Two_Layer_Coding\figure_4.jpg
  Figure 4 caption: "The illustration of the projection. The two black dots represent\
    \ two view points. Each projected view is located by the angles \u03B8 el and\
    \ \u03B8 az . \u03B2 is the angle between the two projected views."
  Figure 5 Link: articels_figures_by_rev_year\2015\D_Shape_Matching_via_Two_Layer_Coding\figure_5.jpg
  Figure 5 caption: Some typical 3D models from the McGill dataset (the first column)
    and their corresponding depth-buffer images. The darker parts in the projections
    are associated with higher values in depth.
  Figure 6 Link: articels_figures_by_rev_year\2015\D_Shape_Matching_via_Two_Layer_Coding\figure_6.jpg
  Figure 6 caption: The Precision-Recall curves of TLC and other comparative approaches
    on five datasets.
  Figure 7 Link: articels_figures_by_rev_year\2015\D_Shape_Matching_via_Two_Layer_Coding\figure_7.jpg
  Figure 7 caption: The influence of the codebook size in each layer. (a) The influence
    of the codebook size in the first layer for the four evaluation metrics. The size
    of each sub-codebook in the second layer is set to 500. (b) The influence of codebook
    size in the second layer for the four evaluation metrics. The size of codebook
    in the first layer is set to 32.
  Figure 8 Link: articels_figures_by_rev_year\2015\D_Shape_Matching_via_Two_Layer_Coding\figure_8.jpg
  Figure 8 caption: The retrieval performance of TLC on PSB dataset is reported when
    adding Gaussian noise with standard deviation sigma increasing from 0 to 1 during
    view sampling procedure.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.91
  Name of the first author: Xiang Bai
  Name of the last author: Longin Jan Latecki
  Number of Figures: 8
  Number of Tables: 5
  Number of authors: 4
  Paper title: 3D Shape Matching via Two Layer Coding
  Publication Date: 2015-04-20 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Details of the Five Datasets Used in This Paper
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Performance of Different Algorithms on Five Standard Datasets
  Table 3 caption:
    table_text: TABLE 3 The Comparison with Baseline Methods
  Table 4 caption:
    table_text: TABLE 4 The Comparison on SHREC 2011 Non-Rigid Dataset
  Table 5 caption:
    table_text: TABLE 5 The Average Pairwise Matching Time of Different Algorithms
      on PSB Dataset
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2424863
- Affiliation of the first author: human language technology center of excellence,
    johns hopkins university, baltimore, md 21218
  Affiliation of the last author: duke university, durham, nc 27708
  Figure 1 Link: articels_figures_by_rev_year\2015\Graph_Matching_Relax_at_Your_Own_Risk\figure_1.jpg
  Figure 1 caption: "For \u03C1\u2208[0.1,1] , we plot \u2225 A \u2032 D \u2217 \u2212\
    \ D \u2217 B \u2225 2 F (red green) and \u2225 A \u2032 P c \u2212 P c B \u2225\
    \ 2 F (bluegray). Redblue dots correspond to simulations where P c \u2260 P \u2217\
    \ , and greygreen dots to P c = P \u2217 . Black dots correspond to \u2225 A \u2032\
    \ P \u2217 \u2212 P \u2217 B \u2225 2 F . For each \u03C1, we ran 100 MC replicates."
  Figure 10 Link: articels_figures_by_rev_year\2015\Graph_Matching_Relax_at_Your_Own_Risk\figure_10.jpg
  Figure 10 caption: "Running time for the nonconvex relaxation when starting from\
    \ D , for different number of seeds. A red \u201Cx\u201D indicates the algorithm\
    \ failed to recover P , and a black \u201Co\u201D indicates it succeeded. In each,\
    \ the algorithm was run to termination at discovery of a local min."
  Figure 2 Link: articels_figures_by_rev_year\2015\Graph_Matching_Relax_at_Your_Own_Risk\figure_2.jpg
  Figure 2 caption: "Distance from D \u2217 to P c (in blue), to P \u2217 (in red),\
    \ and to a random permutation (in black). For each value of \u03C1 , we ran 100\
    \ MC replicates."
  Figure 3 Link: articels_figures_by_rev_year\2015\Graph_Matching_Relax_at_Your_Own_Risk\figure_3.jpg
  Figure 3 caption: "Success rate in recovering P \u2217 . In gray, FAQ starting at,\
    \ from left to right, P \u2217 , D \u2217 , and J ; in black, P c ; in red, PATH;\
    \ in blue, GLAG. For each \u03C1, we ran 100 MC replicates."
  Figure 4 Link: articels_figures_by_rev_year\2015\Graph_Matching_Relax_at_Your_Own_Risk\figure_4.jpg
  Figure 4 caption: "Average run times. In solid gray, FAQ: D \u2217 (note that this\
    \ does not include the time to find D \u2217 ) and FAQ: J ; finding P c (first\
    \ finding D \u2217 ) in black; PATH in red; and GLAG in blue. In dashed gray,\
    \ the total time for FAQ: D \u2217 (including the time to find D \u2217 ). Note\
    \ that the runtime of PATH drops precipitously at \u03C1=0.6, which corresponds\
    \ to the performance increase in Fig. 3."
  Figure 5 Link: articels_figures_by_rev_year\2015\Graph_Matching_Relax_at_Your_Own_Risk\figure_5.jpg
  Figure 5 caption: "Value of \u2212\u27E8 A \u2032 D,DB\u27E9 for D= P \u2217 (black)\
    \ and for the output of FAQ: D \u2217 (redblue indicating failuresuccess in recovering\
    \ the true permutation). For each \u03C1, we ran 100 MC replicates."
  Figure 6 Link: articels_figures_by_rev_year\2015\Graph_Matching_Relax_at_Your_Own_Risk\figure_6.jpg
  Figure 6 caption: 'Success rate in recovering P for 150 vertex power law graphs
    with beta =2 for: In gray, from right to left, FAQ: P , FAQ: D , and FAQ: J ;
    in black, Pc ; in red, PATH; in blue, GLAG. For each value of the bit-flip parameter
    p, we ran 100 MC replicates.'
  Figure 7 Link: articels_figures_by_rev_year\2015\Graph_Matching_Relax_at_Your_Own_Risk\figure_7.jpg
  Figure 7 caption: 'Success rate in recovering P for bounded degree graphs (max degree
    4 ). In gray, from right to left, FAQ: P , FAQ: D , and FAQ: J ; in black, Pc
    ; in red, PATH; in blue, GLAG. For each probability we ran 100 MC replicates.'
  Figure 8 Link: articels_figures_by_rev_year\2015\Graph_Matching_Relax_at_Your_Own_Risk\figure_8.jpg
  Figure 8 caption: 'Success rate for directed graphs. We plot Pc (black), the GLAG
    method (blue), and the nonconvex relaxation starting from different points in
    green, from right to left: FAQ: J , FAQ: D , FAQ: P .'
  Figure 9 Link: articels_figures_by_rev_year\2015\Graph_Matching_Relax_at_Your_Own_Risk\figure_9.jpg
  Figure 9 caption: 'Success rate of different methods using seeds. We plot Pc (top
    left), FAQ: J (top right), FAQ: D (bottom left), and FAQ: P (bottom right). For
    each method, the number of seeds increases from right to left: 0 (black), 5 (green),
    10 (blue) and 15 (red) seeds. Note that more seeds increases the success rate
    across the board.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Vince Lyzinski
  Name of the last author: Guillermo Sapiro
  Number of Figures: 11
  Number of Tables: 3
  Number of authors: 6
  Paper title: 'Graph Matching: Relax at Your Own Risk'
  Publication Date: 2015-04-21 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Notation
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 \u2225 A \u2032 P\u2212PB \u2225 F for the P Given by Each\
      \ Algorithm Together with the Number of Vertices Correctly Matched ( n corr.\
      \ ) in Real Data Experiments"
  Table 3 caption:
    table_text: "TABLE 3 \u2225 A \u2032 P\u2212PB \u2225 2 F for the Different Tested\
      \ Algorithms on 16 Benchmark Examples of the QAPLIB Library"
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2424894
