- Affiliation of the first author: department of computer science, city university
    of hong kong, hong kong sar, china
  Affiliation of the last author: department of computer science, city university
    of hong kong, hong kong sar, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Neuron_CoverageGuided_Domain_Generalization\figure_1.jpg
  Figure 1 caption: Our proposed framework for domain generalization. Given the two
    different samples with similar semantics, we propose to maximize the neuron coverage
    of DNN with gradient similarity regularization between two samples with similar
    semantic information.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Neuron_CoverageGuided_Domain_Generalization\figure_2.jpg
  Figure 2 caption: "Parameter sensitivity analysis by varying (a) \u03BB , (b) \u03B2\
    \ , (c) t . Each curve denotes the performance by considering the domain shown\
    \ in legend as source domain. The performance is reported by averaging the results\
    \ on other three domains."
  Figure 3 Link: articels_figures_by_rev_year\2022\Neuron_CoverageGuided_Domain_Generalization\figure_3.jpg
  Figure 3 caption: Visualization results of network dissection. Each column indicates
    images from one target domain and each row shows the visualization result of unit
    170 by either DeepAll model or our proposed NCDG.
  Figure 4 Link: articels_figures_by_rev_year\2022\Neuron_CoverageGuided_Domain_Generalization\figure_4.jpg
  Figure 4 caption: Visualization results of network dissection. Each column indicates
    images from one target domain and each row shows the visualization result of ResNet-18
    block 2 unit 20 by either DeepAll model or our proposed NCDG.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.92
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Chris Xing Tian
  Name of the last author: Shiqi Wang
  Number of Figures: 4
  Number of Tables: 13
  Number of authors: 5
  Paper title: Neuron Coverage-Guided Domain Generalization
  Publication Date: 2022-03-08 00:00:00
  Table 1 caption: "TABLE 1 SSDG Classification Accuracy (%) on Digits. The Superscript\
    \ + + Denotes the Augmentation With Our Proposed Loss. The Superscript \u2217\u2217\
    \ Denotes the Baselines With Pixel Intensity Reversing. In Particular, the Vanilla\
    \ \u2217\u2217 Denotes the Vanilla Scheme (Only Cross-Entropy Loss) With the Pixel\
    \ Intensity Reversing. By Comparing the NCDG With the Four Schemes Augmented With\
    \ Pixel Intensity Reversing (Vanilla \u2217\u2217 , JiGen \u2217\u2217 , GUD \u2217\
    \u2217 , M-ADA \u2217\u2217 ), It is Apparent That the Proposed Loss Achieves\
    \ Better Performance Under the Same Augmentation Method. By Comparing the GUD\
    \ + + and GUD, as Well as the M-ADA + + With M-ADA, Significant Performance Improvement\
    \ Originating From Our Proposed Loss is Observed"
  Table 10 caption: "TABLE 10 Kullback\u2013leibler Divergences Between the Source\
    \ Domain: Sketch, With the Other Three Target Domains of DeepAll and NCDG Models"
  Table 2 caption: TABLE 2 SSDG Classification Accuracy (%) on PACS Dataset With Resnet-18
    as Backbone Model. Each Row Indicates the Result of Training on a Single Source
    Domain and Testing on the Other Three Domains
  Table 3 caption: "TABLE 3 Robustness Comparison on CIFAR-10-C. We Report the Classification\
    \ Accuracy (%) of 19 Corruptions Under the Severest Corruption Level \u201D5\u201D"
  Table 4 caption: TABLE 4 Performance Comparisons on Cross-Domain Semantic Image
    Segmentation
  Table 5 caption: TABLE 5 Performance Comparisons on Cross-Domain Semantic Image
    Segmentation on DeepLabv3+ With Resnet50 as Backbone
  Table 6 caption: TABLE 6 Component Analysis on PACS Dataset
  Table 7 caption: TABLE 7 Layer Analysis of Resnet-18 on PACS (Sketch as the Source
    Domain)
  Table 8 caption: TABLE 8 SSDG Comparison Between Different Activation Functions
    of Resnet-18 on PACS (Sketch as the Source Domain)
  Table 9 caption: TABLE 9 SSDG Comparison Between NCDG Proposed Loss and L1 Regularization
    Loss on PACS Dataset (Sketch Domain as the Source)
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3157441
- Affiliation of the first author: department of electronic engineering, shanghai
    jiao tong university, shanghai, china
  Affiliation of the last author: department of electrical & computer engineering,
    mcmaster university, hamilton, ontario, canada
  Figure 1 Link: articels_figures_by_rev_year\2022\MultiModality_Deep_Restoration_of_Extremely_Compressed_Face_Videos\figure_1.jpg
  Figure 1 caption: Visual comparisons of the proposed MDVD-Net and the stat-of-the-art
    method EDVR. MDVD-Net can produce more precise mouth shape, clearer teeth, sharper
    lips, and muscle contours.
  Figure 10 Link: articels_figures_by_rev_year\2022\MultiModality_Deep_Restoration_of_Extremely_Compressed_Face_Videos\figure_10.jpg
  Figure 10 caption: Rate-distortion curves of the competing methods on the VoxCeleb2
    dataset on H.265 video codec. The proposed MDVD-Net cleayly outperforms all existing
    methods by a large margin.
  Figure 2 Link: articels_figures_by_rev_year\2022\MultiModality_Deep_Restoration_of_Extremely_Compressed_Face_Videos\figure_2.jpg
  Figure 2 caption: The framework of the proposed Multi-modality Deep Video Decompression
    Network (MDVD-Net). It consists of four branches, for speech, video, landmarks,
    and codec information, respectively.
  Figure 3 Link: articels_figures_by_rev_year\2022\MultiModality_Deep_Restoration_of_Extremely_Compressed_Face_Videos\figure_3.jpg
  Figure 3 caption: Architectures of individual subnets in our model. (a) is the architecture
    of A-subnet used to produce 2-D feature maps from speech signal in preparation
    for being combined with other modalities; (b) is the architecture of V-subnet
    used to extract features of the aligned video frames; (c) is the architecture
    of L-subnet used to extract deep features from the corrected facial landmarks;
    (d) is the architecture of C-subnet used to extract features from the video codec
    information.
  Figure 4 Link: articels_figures_by_rev_year\2022\MultiModality_Deep_Restoration_of_Extremely_Compressed_Face_Videos\figure_4.jpg
  Figure 4 caption: MV-guided alignment with deformable convolutions.
  Figure 5 Link: articels_figures_by_rev_year\2022\MultiModality_Deep_Restoration_of_Extremely_Compressed_Face_Videos\figure_5.jpg
  Figure 5 caption: Facial landmarks before and after correction by speech.
  Figure 6 Link: articels_figures_by_rev_year\2022\MultiModality_Deep_Restoration_of_Extremely_Compressed_Face_Videos\figure_6.jpg
  Figure 6 caption: The architecture of the spatial attention fusion module.
  Figure 7 Link: articels_figures_by_rev_year\2022\MultiModality_Deep_Restoration_of_Extremely_Compressed_Face_Videos\figure_7.jpg
  Figure 7 caption: The architecture of the back projection module.
  Figure 8 Link: articels_figures_by_rev_year\2022\MultiModality_Deep_Restoration_of_Extremely_Compressed_Face_Videos\figure_8.jpg
  Figure 8 caption: 'The illustration of codec information of the H.264 video compression
    standard. From left to right are: original frame, transform unit (TU) partition,
    prediction frame in Y channel, and prediction residue in Y channel, respectively.'
  Figure 9 Link: articels_figures_by_rev_year\2022\MultiModality_Deep_Restoration_of_Extremely_Compressed_Face_Videos\figure_9.jpg
  Figure 9 caption: Rate-distortion curves of the competing methods on the Obama and
    VoxCeleb2 dataset on H.264 video codec. The proposed MDVD-Net cleayly outperforms
    all existing methods by a large margin.
  First author gender probability: 0.97
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.75
  Name of the first author: Xi Zhang
  Name of the last author: Xiaolin Wu
  Number of Figures: 14
  Number of Tables: 4
  Number of authors: 2
  Paper title: Multi-Modality Deep Restoration of Extremely Compressed Face Videos
  Publication Date: 2022-03-08 00:00:00
  Table 1 caption: 'TABLE 1 Quantitative Results (PSNR) of Ablation Studies on the
    Obama Dataset and VoxCeleb2 Dataset. VPB: Video Processing Branch; MV: Motion
    Vectors; SPB: Speech Processing Branch; FLWOC: Facial Landmark Without Correction;
    FLWC: Facial Landmark With Correction; SAF: Spatial Attention Fusion; VCI: Video
    Codec Informaion'
  Table 10 caption: Not Available
  Table 2 caption: 'TABLE 2 PSNR Gains of the Upper and the Lower Part of Face in
    the Ablation Study of Speech Signals. SPB: Speech Processing Branch; FLWC: Facial
    Landmark With Correction'
  Table 3 caption: "TABLE 3 Test Speed (Frame Per Second, FPS) on GPU for 224\xD7\
    224 224\xD7224 Video Sequences"
  Table 4 caption: TABLE 4 Performance of the Competing Methods Evaluated on the Videos
    Which are Beyond the Quality Range in Training
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3157388
- Affiliation of the first author: department of electrical and electronic engineering,
    imperial college london, london, u.k.
  Affiliation of the last author: department of electrical and electronic engineering,
    imperial college london, london, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2022\Reciprocal_GAN_Through_Characteristic_Functions_RCFGAN\figure_1.jpg
  Figure 1 caption: The overall structure of the proposed RCF-GAN. The generator serves
    to minimise the CF loss between the embedded real and fake distributions. The
    critic serves to minimise the CF loss between the embedded real and the input
    noise distributions, whilst maximising the CF loss between the embedded fake and
    the input noise distributions. Moreover, an MSE loss between the embedded fake
    and the input noise distributions is regularised as the auto-encoder loss, which
    has not been shown in the figure. An optional t -net can be employed to optimally
    sample the CF loss.
  Figure 10 Link: articels_figures_by_rev_year\2022\Reciprocal_GAN_Through_Characteristic_Functions_RCFGAN\figure_10.jpg
  Figure 10 caption: The AUC and AP scores of our RCF-GAN and standard VGAE, with
    m layer ranging from 256 to 2048. The solid lines show the AUC and AP of our RCF-GAN
    in testing, whereas the dashed lines designate for the VGAE. Since spectral clustering
    [79] is a typical baseline when not using GCNs, spectral clustering results from
    [30] are shown in dotted lines.
  Figure 2 Link: articels_figures_by_rev_year\2022\Reciprocal_GAN_Through_Characteristic_Functions_RCFGAN\figure_2.jpg
  Figure 2 caption: "Two experiments on the MNIST dataset which illustrate the physical\
    \ meaning of the phase and amplitude of the CF. (a) A multivariate Gaussian fit\
    \ to the images of digits 1 and 2, by naively assuming that each pixel is independent\
    \ from other pixels. The phase and amplitude information of the CFs between the\
    \ two multivariate distributions were then swapped, followed by random sampling\
    \ from the swapped distributions. (b)-(d) A generator was directly trained on\
    \ the given images of each digit. To avoid the impact from the critic, we DID\
    \ NOT employ the critic in this experiment but directly calculated the loss between\
    \ images after the generator with different \u03B1 . We performed training for\
    \ amplitude for \u03B1=0.999 in (b), phase only for \u03B1=0.001 in (c), and equally\
    \ training the amplitude and phase information for \u03B1=0.5 in (d)."
  Figure 3 Link: articels_figures_by_rev_year\2022\Reciprocal_GAN_Through_Characteristic_Functions_RCFGAN\figure_3.jpg
  Figure 3 caption: Convergence curves and the images generated by the proposed RCF-GAN
    from Gaussian noise, under the DCGAN [54] architecture. The curves represent an
    average over a moving window, with 500 iterations.
  Figure 4 Link: articels_figures_by_rev_year\2022\Reciprocal_GAN_Through_Characteristic_Functions_RCFGAN\figure_4.jpg
  Figure 4 caption: "The FID and KID scores for different \u03B1 , under the DCGAN\
    \ [54] structure. Observe the embedded space (by interpolated images) of the proposed\
    \ RCF-GAN, which was learnt with different \u03B1 on CelebA dataset. The details\
    \ of image interpolation are given in Section 5.4."
  Figure 5 Link: articels_figures_by_rev_year\2022\Reciprocal_GAN_Through_Characteristic_Functions_RCFGAN\figure_5.jpg
  Figure 5 caption: Image reconstruction (upper panel) and interpolation (lower panel)
    by the proposed RCF-GAN, AGE [36] and MMD-GAN [13] for the CelebA dataset, under
    the DCGAN [54] architecture. The upper panel shows the reconstructed images (in
    even columns) corresponding to the original images (in odd columns). The lower
    panel displays the linear interpolation in the embedded domain.
  Figure 6 Link: articels_figures_by_rev_year\2022\Reciprocal_GAN_Through_Characteristic_Functions_RCFGAN\figure_6.jpg
  Figure 6 caption: "Random generation, reconstruction and interpolation by the proposed\
    \ RCF-GAN, using BigGAN (wo att.) architecture and images of size 128\xD7128 pixels.\
    \ The upper panel shows images of the CelebA dataset and the lower panel images\
    \ from the LSUNB dataset."
  Figure 7 Link: articels_figures_by_rev_year\2022\Reciprocal_GAN_Through_Characteristic_Functions_RCFGAN\figure_7.jpg
  Figure 7 caption: Random samples on ImageNet generated from our RCF-GAN.
  Figure 8 Link: articels_figures_by_rev_year\2022\Reciprocal_GAN_Through_Characteristic_Functions_RCFGAN\figure_8.jpg
  Figure 8 caption: Successful trials in terms of FID thresholds for the 200 random
    architectures given in Table 5. The x-axis, representing the FID thresholds, ranges
    from 20 to 120 because, as shown on the right hand side, the generation with the
    FID larger than 120 basically cannot recognise any perceptual cues from images.
    The y-axis represents the successful trials, i.e., the number of trials with FIDs
    smaller than the corresponding threshold. Thus, large areas under the curves designate
    stable converging performances over different architectures.
  Figure 9 Link: articels_figures_by_rev_year\2022\Reciprocal_GAN_Through_Characteristic_Functions_RCFGAN\figure_9.jpg
  Figure 9 caption: "Convergence curves, in terms of FIDs, for the proposed RCF-GAN\
    \ and the unconditional BigGAN method. We evaluated both methods on the CIFAR-10,\
    \ CelebA (image sizes of 64\xD764 and 128\xD7128 ) and LSUNB (image sizes of 64\xD7\
    64 and 128\xD7128 ) datasets, and report the Pytorch version of FIDs. The curves\
    \ denoted by \u2217 designate a reduction in the learning rate by a half when\
    \ training BigGANs, because these did not converge at the beginning of training\
    \ under the standard learning rate employed in this paper."
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Shengxi Li
  Name of the last author: Danilo Mandic
  Number of Figures: 10
  Number of Tables: 6
  Number of authors: 4
  Paper title: Reciprocal GAN Through Characteristic Functions (RCF-GAN)
  Publication Date: 2022-03-08 00:00:00
  Table 1 caption: TABLE 1 The FID and KID Scores Obtained From the DCGAN [54] Structure.
    The Results of the DCGAN and W-GAN-GP are from [55] and [14]. The Corresponding
    Publicly Available Codes Were Run to Obtain the Results of the W-GAN [5], MMD-GAN
    [13], OCF-GAN and OCF-GAN-GP [31]. The Results of the AGE Were Tested From Its
    Pre-Trained models [36]
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 FID and KID Scores of Reconstructed Images Under DCGAN
    Architecture. The AGE Results Were Tested From Its Pre-Trained models [36]
  Table 3 caption: TABLE 3 The FID Scores for Unconditional Generation Obtained From
    the ResNet [9] Architectures. For Results Without References Nearby, We Ran the
    Available Codes of Different GANs by Their Corresponding Default Settings
  Table 4 caption: "TABLE 4 Unconditional Image Generation on ImageNet Dataset. The\
    \ Symbol Denotes the Result from [65], Whereas \u2020 \u2020 Designates That from\
    \ [62] ( Base Base and No\u03B5 No\u025B Rows in Table 1). Please Note That We\
    \ Do Not Compare With the Results in Table 3 of [62] Because They Used a Different\
    \ Data Augmentation method [62], Whereas We Kept Consistent With the Standard\
    \ Pre-Processing Method [60]"
  Table 5 caption: TABLE 5 Basic Building Blocks in the Employed Random Architectures
  Table 6 caption: "TABLE 6 Ablation Study on the CIFAR-10 Dataset Under the BigGAN\
    \ (Wo Att.) Architecture. The Proposed RCF-GAN Was Evaluated and Compared to the\
    \ One Without Reciprocal (Wo Recip.) Requirement ( \u03BB=0 \u03BB=0), and That\
    \ Without the Anchor (Wo Anc.) Design"
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3157444
- Affiliation of the first author: department of information technology and electrical
    engineering, eth zurich, zurich, switzerland
  Affiliation of the last author: department of information engineering and computer
    science (disi), university of trento, trento, tn, italy
  Figure 1 Link: articels_figures_by_rev_year\2022\Local_and_Global_GANs_With_SemanticAware_Upsampling_for_Image_Generation\figure_1.jpg
  Figure 1 caption: Examples of semantic image synthesis results on Cityscapes (top)
    and cross-view image translation results on Dayton (bottom) with different settings
    of our LGGAN.
  Figure 10 Link: articels_figures_by_rev_year\2022\Local_and_Global_GANs_With_SemanticAware_Upsampling_for_Image_Generation\figure_10.jpg
  Figure 10 caption: Examples of arbitrary cross-view image translation.
  Figure 2 Link: articels_figures_by_rev_year\2022\Local_and_Global_GANs_With_SemanticAware_Upsampling_for_Image_Generation\figure_2.jpg
  Figure 2 caption: "Overview of the proposed method, which contains a semantic-guided\
    \ generator G and discriminator D s . G consists of a parameter-sharing encoder\
    \ E , an image-level global generator G g , a class-level local generator G l\
    \ and a weight map generator G w . The global generator and local generator are\
    \ automatically combined by two learned weight maps from the weight map generator\
    \ to reconstruct the target image. D s tries to distinguish the generated images\
    \ from two modality spaces, i.e., the image space and semantic space. Moreover,\
    \ to learn a more discriminative class-specific feature representation, a novel\
    \ classification module is proposed. Lastly, to better preserve semantic information\
    \ when upsampling feature maps, a novel semantic-aware upsampling method is introduced.\
    \ All of these components are trained in an end-to-end fashion so that the local\
    \ generation and the global generation can benefit from each other. The symbols\
    \ \u2295 , \u2297 and \u25EFs denote element-wise addition, element-wise multiplication\
    \ and channel-wise Softmax, respectively."
  Figure 3 Link: articels_figures_by_rev_year\2022\Local_and_Global_GANs_With_SemanticAware_Upsampling_for_Image_Generation\figure_3.jpg
  Figure 3 caption: Comparison with different feature upsampling and enhancement methods
    on the semantic-guided image generation task. Given two locations (indicated by
    red and magenta squares) in the output feature map, our goal is to generate these
    locations by selectively upsampling several points (indicated by circles) in the
    input feature map.
  Figure 4 Link: articels_figures_by_rev_year\2022\Local_and_Global_GANs_With_SemanticAware_Upsampling_for_Image_Generation\figure_4.jpg
  Figure 4 caption: "Overview of the proposed SAU, which consists of two branches,\
    \ i.e., SAKG and SAFU. The SAKG branch aims to generate semantically adaptive\
    \ kernels according to the input layout. The SAFU branch aims to selectively upsample\
    \ the feature f\u2208C\xD7H\xD7W to the target one f \u2032 \u2208C\xD7Hs\xD7\
    Ws based on the kernels learned in SAKG, where s is the expected upsample scale."
  Figure 5 Link: articels_figures_by_rev_year\2022\Local_and_Global_GANs_With_SemanticAware_Upsampling_for_Image_Generation\figure_5.jpg
  Figure 5 caption: Visualization of semantically adaptive kernels learned on COCO-Stuff.
    In the second column, we show three representative locations in each generated
    image, with different colored squares. The other three columns show semantically
    adaptive kernels learned for those three locations, with corresponding color arrows
    summarizing the most-attended regions for upsampling the target location. The
    network learns to allocate attention according to regions with the same semantic
    information, rather than just spatial adjacency.
  Figure 6 Link: articels_figures_by_rev_year\2022\Local_and_Global_GANs_With_SemanticAware_Upsampling_for_Image_Generation\figure_6.jpg
  Figure 6 caption: "Overview of the proposed local class-specific generator G l ,\
    \ which consists of four steps, i.e., semantic class mask calculation, class-specific\
    \ feature map filtering, classification-based discriminative feature learning\
    \ and class-specific generation. A cross-entropy loss with void classes filtered\
    \ is applied to the feature representation of each class to learn more discriminative\
    \ class-specific representations. A semantic-mask guided pixel-wise L1 loss is\
    \ applied at the end for class-level reconstruction. The symbols \u2297 and \u25EF\
    c denote element-wise multiplication and channel-wise concatenation, respectively.\
    \ Note that we assume the size of f \u2032 is C\xD7H\xD7W for simplicity, which\
    \ is different from the one in Fig. 4 (i.e., C\xD7Hs\xD7Ws )."
  Figure 7 Link: articels_figures_by_rev_year\2022\Local_and_Global_GANs_With_SemanticAware_Upsampling_for_Image_Generation\figure_7.jpg
  Figure 7 caption: Qualitative comparison of cross-view image translation in a2g
    direction on SVA.
  Figure 8 Link: articels_figures_by_rev_year\2022\Local_and_Global_GANs_With_SemanticAware_Upsampling_for_Image_Generation\figure_8.jpg
  Figure 8 caption: Qualitative comparison of cross-view image translation in both
    a2g (top three rows) and g2a (bottom three rows) directions on Dayton.
  Figure 9 Link: articels_figures_by_rev_year\2022\Local_and_Global_GANs_With_SemanticAware_Upsampling_for_Image_Generation\figure_9.jpg
  Figure 9 caption: Qualitative comparison of cross-view image translation in a2g
    direction on CVUSA.
  First author gender probability: 0.93
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Hao Tang
  Name of the last author: Nicu Sebe
  Number of Figures: 18
  Number of Tables: 12
  Number of authors: 4
  Paper title: Local and Global GANs With Semantic-Aware Upsampling for Image Generation
  Publication Date: 2022-03-09 00:00:00
  Table 1 caption: TABLE 1 Quantitative Comparison of Cross-View Image Translation
    on SVA in the a2g Direction.
  Table 10 caption: TABLE 10 User Study
  Table 2 caption: TABLE 2 Quantitative Comparison of Cross-View Image Translation
    on Dayton in the a2g Direction
  Table 3 caption: TABLE 3 Quantitative Comparison of Cross-View Image Translation
    on CVUSA in a2g Direction
  Table 4 caption: TABLE 4 Comparison of the Number of Network Parameters (M)
  Table 5 caption: TABLE 5 Quantitative Comparison of Semantic Image Synthesis on
    Cityscapes, ADE20K, and COCO-Stuff
  Table 6 caption: TABLE 6 User Study I
  Table 7 caption: TABLE 7 User Study II
  Table 8 caption: TABLE 8 User Study III
  Table 9 caption: TABLE 9 Ablation Study of the Proposed LGGAN on Cityscapes
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3155989
- Affiliation of the first author: school of software, tsinghua university, beijing,
    china
  Affiliation of the last author: school of software, bnrist, tsinghua university,
    beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\PMPNet_Point_Cloud_Completion_by_TransformerEnhanced_MultiStep_Point_Moving_Path\figure_1.jpg
  Figure 1 caption: "Illustration of the differences between the generation based\
    \ methods and the deformation based methods, where the task is to complete a short\
    \ line AB to a long line A \u2032 B \u2032 (in (a) and (b)). The differences of\
    \ the effect between the path constraint and the widely used CDEMD is further\
    \ illustrated in (c)."
  Figure 10 Link: articels_figures_by_rev_year\2022\PMPNet_Point_Cloud_Completion_by_TransformerEnhanced_MultiStep_Point_Moving_Path\figure_10.jpg
  Figure 10 caption: Visualization of more completion results using our PMP-Net++
    on PCN dataset.
  Figure 2 Link: articels_figures_by_rev_year\2022\PMPNet_Point_Cloud_Completion_by_TransformerEnhanced_MultiStep_Point_Moving_Path\figure_2.jpg
  Figure 2 caption: "Illustration of path searching with multiple steps under the\
    \ coarse-to-fine searching radius. The PMP-Net++ moves point A to point A \u2032\
    \ by three steps, with each step reducing its searching radius, and looking back\
    \ to consider the moving history in order to decide the next place to move."
  Figure 3 Link: articels_figures_by_rev_year\2022\PMPNet_Point_Cloud_Completion_by_TransformerEnhanced_MultiStep_Point_Moving_Path\figure_3.jpg
  Figure 3 caption: 'Detailed structure of PMD-module at step k . It mainly consists
    of three parts: (1) point cloud encoder and (2) feature prorogation module (FP-module)
    to extract per-point features; (3) RPA module to recurrently learn and forget
    the path searching information from the previous steps.'
  Figure 4 Link: articels_figures_by_rev_year\2022\PMPNet_Point_Cloud_Completion_by_TransformerEnhanced_MultiStep_Point_Moving_Path\figure_4.jpg
  Figure 4 caption: Illustration of path searching with multiple steps under the coarse-to-fine
    searching radius. The PMP-Net++ moves point A to point A by three steps, with
    each step reducing its searching radius, and looking back to consider the moving
    history in order to decide the next place to move.
  Figure 5 Link: articels_figures_by_rev_year\2022\PMPNet_Point_Cloud_Completion_by_TransformerEnhanced_MultiStep_Point_Moving_Path\figure_5.jpg
  Figure 5 caption: The architecture of encoder used in PMD-module. Moreover, we also
    show the comparison with previous work and the detailed structure of transformer.
  Figure 6 Link: articels_figures_by_rev_year\2022\PMPNet_Point_Cloud_Completion_by_TransformerEnhanced_MultiStep_Point_Moving_Path\figure_6.jpg
  Figure 6 caption: Detailed structure of RPA module at step k , level l .
  Figure 7 Link: articels_figures_by_rev_year\2022\PMPNet_Point_Cloud_Completion_by_TransformerEnhanced_MultiStep_Point_Moving_Path\figure_7.jpg
  Figure 7 caption: Illustration of multiple solutions when deforming input point
    cloud (green) into target point cloud (red). The PMD-constraint guarantees the
    uniqueness of point level correspondence (a) between input and target point cloud,
    and filter out various redundant solutions for moving points (b).
  Figure 8 Link: articels_figures_by_rev_year\2022\PMPNet_Point_Cloud_Completion_by_TransformerEnhanced_MultiStep_Point_Moving_Path\figure_8.jpg
  Figure 8 caption: Illustration of the effectiveness of mathcal LPMD . By minimizing
    the point moving distance, the network is encouraged to learn more consistent
    paths from source to target, which will reduce redundant searching in each step
    and improve the efficiency.
  Figure 9 Link: articels_figures_by_rev_year\2022\PMPNet_Point_Cloud_Completion_by_TransformerEnhanced_MultiStep_Point_Moving_Path\figure_9.jpg
  Figure 9 caption: Visualization of point cloud completion comparison with previous
    methods on PCN dataset.
  First author gender probability: 0.6
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.57
  Name of the first author: Xin Wen
  Name of the last author: Yu-Shen Liu
  Number of Figures: 17
  Number of Tables: 13
  Number of authors: 7
  Paper title: 'PMP-Net++: Point Cloud Completion by Transformer-Enhanced Multi-Step
    Point Moving Paths'
  Publication Date: 2022-03-15 00:00:00
  Table 1 caption: TABLE 1 The Detailed Structure of Encoder
  Table 10 caption: "TABLE 10 The Effect of Searching Radius (Baseline Marked by \u201C\
    \u201D)"
  Table 2 caption: TABLE 2 The Detailed Architecture of Feature Propagation Module
  Table 3 caption: "TABLE 3 Point Cloud Completion on PCN Dataset in Terms of Per-Point\
    \ L1 Chamfer Distance \xD7 10 3 \xD7103 (Lower is Better)"
  Table 4 caption: "TABLE 4 Point Cloud Completion on Completion3D Dataset in Terms\
    \ of Per-Point L2 Chamfer Distance \xD7 10 4 \xD7104 (Lower is Better)"
  Table 5 caption: TABLE 5 Quantitative Evaluation of ScanNet Chairs
  Table 6 caption: TABLE 6 Quantitative Comparison on Point Cloud Up-Sampling Task
  Table 7 caption: "TABLE 7 Analysis of RPA and PMP Loss (Baseline Marked by \u201C\
    \u201D)"
  Table 8 caption: "TABLE 8 The Effect of Different Steps (Baseline Marked by \u201C\
    \u201D)"
  Table 9 caption: TABLE 9 Comparison With Deformation From Grid Points
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3159003
- Affiliation of the first author: national laboratory of pattern recognition, institute
    of automation, chinese academy of sciences, beijing, china
  Affiliation of the last author: national laboratory of pattern recognition, institute
    of automation, chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\OptimizationBased_PostTraining_Quantization_With_BitSplit_and_Stitching\figure_1.jpg
  Figure 1 caption: "An illustration of Bit-split and Stitching (Bit-split) optimization\
    \ procedure for 4-bit weight quantization. (a) Initial low-bit weights before\
    \ Bit-split optimization. (b) In the bit-split stage, each 4-bit value is split\
    \ into 3 ternary values. (c) The bit-optimization for the decomposed ternary vectors.\
    \ (d) The last stage to stitch optimized bits back into integers. Take the third\
    \ value for example, 2 0 \u22C51+ 2 1 \u22C5(\u22121)+ 2 2 \u22C5(\u22121)=\u2212\
    5=\u2212101 b."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\OptimizationBased_PostTraining_Quantization_With_BitSplit_and_Stitching\figure_2.jpg
  Figure 2 caption: (a) Illustration of per-channel activation quantization. Each
    input channel is quantized using a separate scale factor denoted by a specific
    color. (b) Illustration of transforming per-channel quantization into per-layer
    quantization by re-scaling. The scale factors of input activations are merged
    into the corresponding 2D kernels of the following floating-point filters. The
    bottom row shows how to conduct convolution (CONV) using matrix multiplication
    (MatMul) operations. Note that at this stage, the weights are not quantized yet.
    After merge, the floating-point filters can be further quantized into integers
    using the Optimization-based Weight Quantization proposed in Section 3.3, so that
    the convolutions can be conducted by integer-only accumulations. Best viewed in
    color.
  Figure 3 Link: articels_figures_by_rev_year\2022\OptimizationBased_PostTraining_Quantization_With_BitSplit_and_Stitching\figure_3.jpg
  Figure 3 caption: The ranges of activations for each channel (after ReLU) of the
    first and second convolutions of VGG-16-BN. It can be seen that the ranges for
    different channels differ severely.
  Figure 4 Link: articels_figures_by_rev_year\2022\OptimizationBased_PostTraining_Quantization_With_BitSplit_and_Stitching\figure_4.jpg
  Figure 4 caption: The accuracy comparison between uniform weight quantization and
    logarithmic weight quantization with various bit-widths.
  Figure 5 Link: articels_figures_by_rev_year\2022\OptimizationBased_PostTraining_Quantization_With_BitSplit_and_Stitching\figure_5.jpg
  Figure 5 caption: The correlation between the Hessian error term and the activation
    reconstruction error for a convolutional kernel of ResNet-18.
  Figure 6 Link: articels_figures_by_rev_year\2022\OptimizationBased_PostTraining_Quantization_With_BitSplit_and_Stitching\figure_6.jpg
  Figure 6 caption: The changes of quantization scales and the low-bit weights after
    W4 Bit-split optimization on ResNet-18. (a) The percentage of quantized weights
    that have changed the values after optimization. Overall, 15.45% weights have
    changed their values. (b) The distribution of the absolute value change among
    these weights. (c) The quantization scales before and after optimization for the
    first 50 channels of the 16th convolutional layer.
  Figure 7 Link: articels_figures_by_rev_year\2022\OptimizationBased_PostTraining_Quantization_With_BitSplit_and_Stitching\figure_7.jpg
  Figure 7 caption: The accuracy of post-training quantization on ResNet-18 with different
    numbers of images for calibration. A8W4 with uniform quantization is adopted for
    evaluation.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Peisong Wang
  Name of the last author: Jian Cheng
  Number of Figures: 7
  Number of Tables: 7
  Number of authors: 6
  Paper title: Optimization-Based Post-Training Quantization With Bit-Split and Stitching
  Publication Date: 2022-03-15 00:00:00
  Table 1 caption: TABLE 1 Comparison Results of the Top-1 and Top-5 Accuracy (%)
    for Uniform Weight Quantization With Various Bit-Widths
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison Results of Top-1 and Top-5 Accuracy (%) for
    Uniform Quantization of Both Weights and Activations With Various Bit-Widths
  Table 3 caption: TABLE 3 Comparison Results of the Top-1 Accuracy (%) for Post-Training
    Activation Quantization of ResNet-18
  Table 4 caption: TABLE 4 Comparison of Different Post-Training Quantization Approaches
    on ImageNet Classification Benchmark
  Table 5 caption: TABLE 5 Object Detection (Bounding Box AP) and Instance Segmentation
    (Mask AP) Results on COCO Minival Set
  Table 6 caption: TABLE 6 The Effect of Calibration Data Source of Post-Training
    Quantization on ResNet-18
  Table 7 caption: TABLE 7 Speed Comparison of ResNet-18 With Various Bit-Widths on
    FPGA
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3159369
- Affiliation of the first author: department of computer science, city university
    of hong kong, hong kong
  Affiliation of the last author: department of computer science, city university
    of hong kong, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2022\On_Distinctive_Image_Captioning_via_Comparing_and_Reweighting\figure_1.jpg
  Figure 1 caption: The human ground-truth captions of a target image and a semantically
    similar image contain both common words (highlighted in green) and distinctive
    words (highlighted in red for the target, and blue for the similar image). We
    underline the words that are irrelevant to the image. The baseline model, Transformer
    [1] trained with MLE and SCST, generates the same caption for both images, while
    our model generates distinctive captions with words unique to each image. The
    distinctiveness is measured using CIDErBtw, the average CIDEr metric between the
    target caption and the GT captions of the similar images set, where lower values
    mean more distinctive.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\On_Distinctive_Image_Captioning_via_Comparing_and_Reweighting\figure_2.jpg
  Figure 2 caption: "The framework of our proposed method, Distinctiveness via Comparing\
    \ and Reweighting (DCR). C \u2217 and C 0 indicate the generated caption and the\
    \ ground-truth captions. The similar images set is retrieved by the input image,\
    \ and CIDErBtw is computed for each ground-truth captions of the input image.\
    \ The blue shade in the ground-truth image captions indicates word frequency,\
    \ i.e., dark blue represents Long-Tail words and vice versa. The green box outlining\
    \ each sentence denotes the distinctiveness, i.e., dark green represents distinctive\
    \ caption, and vice versa."
  Figure 3 Link: articels_figures_by_rev_year\2022\On_Distinctive_Image_Captioning_via_Comparing_and_Reweighting\figure_3.jpg
  Figure 3 caption: Word frequency distribution (orange line) and the long-tailed
    weights (blue line) assigned to each word. The beginning and ending word frequency
    indices to be emphasized with the long-tailed weights are F b (5,000) and F e
    (9,487). We show several example words with different frequencies.
  Figure 4 Link: articels_figures_by_rev_year\2022\On_Distinctive_Image_Captioning_via_Comparing_and_Reweighting\figure_4.jpg
  Figure 4 caption: Statistics of word frequency in generated captions on the test
    split. The x -axis represents the word frequency f in log scale, and the y -axis
    is the number of words with frequency smaller than f . TFRL denotes Transformer+SCST.
    The total vocabulary sizes for the four models are 695, 640, 603, 593, which is
    indicated by the maximum height of each curve.
  Figure 5 Link: articels_figures_by_rev_year\2022\On_Distinctive_Image_Captioning_via_Comparing_and_Reweighting\figure_5.jpg
  Figure 5 caption: User study comparing captions generated from models trained with
    and without our DCR method. Here UDRL and TFRL denote UpDown+SCST and Transformer+SCST
    for short. Users selected our models trained with DCR more frequently when assessing
    accuracy and distinctiveness (Chi-Square test, p < 0.001 for each pair).
  Figure 6 Link: articels_figures_by_rev_year\2022\On_Distinctive_Image_Captioning_via_Comparing_and_Reweighting\figure_6.jpg
  Figure 6 caption: Example captions from the baseline model and our model. The distinctive
    words are highlighted. The number in parenthesis is the CIDErBtw score, with lower
    values meaning more distinctive.
  Figure 7 Link: articels_figures_by_rev_year\2022\On_Distinctive_Image_Captioning_via_Comparing_and_Reweighting\figure_7.jpg
  Figure 7 caption: Example captions for a set of similar images. The distinctive
    words are highlighted in red.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Jiuniu Wang
  Name of the last author: Antoni B. Chan
  Number of Figures: 7
  Number of Tables: 9
  Number of authors: 4
  Paper title: On Distinctive Image Captioning via Comparing and Reweighting
  Publication Date: 2022-03-16 00:00:00
  Table 1 caption: "TABLE 1 Comparison of Caption Accuracy and Distinctiveness on\
    \ MSCOCO Test Split: (Top) Baseline Models Trained With MLE Using Standard or\
    \ DCR Weighted XE Loss (Denoted as DCR); (Middle) Models Trained With SCST Using\
    \ Standard or DCR Weighted LossReward (Denoted as DCR); (Bottom) SOTA Methods\
    \ for Generating DistinctiveDiscriminative Captions. CIDEr, BLEU34, METEOR, ROUGE-L,\
    \ and SPICE Measure Caption Accuracy, While CIDErBtw and Rk Measure Distinctiveness.\
    \ \u2191 \u2191 or \u2193 \u2193 Show Whether Higher or Lower Scores are Better\
    \ for Each Metric. CIDErBtw Could not be Computed for Some Models Because the\
    \ Captions are not Publicly Available. Our Self-Retrieval Results (Rk) and Those\
    \ of [12], [16], [17], [42] use the pre-trained VSE++ Model and the Same Protocol.\
    \ \u2020 \u2020 Note that [9] Reports Self-Retrieval Results Using a Different\
    \ Retrieval ModelProtocol \u2013 They use Their Own Model for Retrieval \u2013\
    \ Which Makes it not Directly Comparable. We Re-Implement COS-CVAE [41] on the\
    \ Karpathy Split [66] with Publicly Available Code."
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Ablation Study of Transformer Baseline Trained With Different
    XE Loss Functions and Reinforcement Learning
  Table 3 caption: "TABLE 3 The Performance of Our Model Under Different CIDErBtwReweight\
    \ Parameter \u03B1 w \u03B1w"
  Table 4 caption: TABLE 4 Ablation Study of F b Fb on Model Transformer+DCR
  Table 5 caption: TABLE 5 The Evaluation Results for Different Similar Images Settings
  Table 6 caption: "TABLE 6 Top (Row 1 to 5): The Models Trained Under Different Similar\
    \ Image Set Size. Here CBV( K K) Represents the CIDErBtw Under K K Similar Images\
    \ Retrieved by VSE++ Retrieval Similarity. Middle (Row 6 to 7): CB-Image Indicates\
    \ Similar Images Set Constructed by Images Feature Similarity. CB-GT-Caption Indicates\
    \ Using the Target Image Itself to Reweight Each Caption. Bottom (Row 8 to 12):\
    \ the Models Trained with Similar Image Sets Constructed From Four Different Image-Text\
    \ Retrieval Methods, CIDEr Similarity (CBC), SCAN [63] (CB-SCAN), Vilbert [64]\
    \ (CB-Vilbert), CLIP [65] (CB-CLIP) and Random Sampling (CB-Random). The Number\
    \ of Images in the Similar Images set is K=5 K=5. Note That \u201CCBV(5)\u201D\
    \ After the Model Name Means Training Strategies, While in the Table Header it\
    \ Means Evaluation Metric. These Models are Transformer+SCST Trained with CIDErBtwReweight\
    \ Under Different Similar Image Sets."
  Table 7 caption: TABLE 7 User Study on Image Retrieval to Assess Caption Distinctiveness
  Table 8 caption: TABLE 8 User Study Results on Image Retrieval and Caption Rating
  Table 9 caption: TABLE 9 The Correlation Between Human Judgment and Automatic Metrics,
    i.e., VSE++ Recall (VR) and CIDErBtw Metric (CB)
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3159811
- Affiliation of the first author: department of engineering, university of cambridge,
    cambridge, u.k.
  Affiliation of the last author: department of engineering, university of cambridge,
    cambridge, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2022\SphereFace_Revived_Unifying_Hyperspherical_Face_Recognition\figure_1.jpg
  Figure 1 caption: "(a) \u0394(\u03B8) for current representative hyperspherical\
    \ FR methods. (b) \u0394(\u03B8) of SphereFace and SphereFace-R. We set m to 0.4,\
    \ 0.5, 1.4, 1.4 and 1.4 for CosFace, ArcFace, SphereFace, SphereFace-R v1 and\
    \ SphereFace-R v2, respectively."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\SphereFace_Revived_Unifying_Hyperspherical_Face_Recognition\figure_2.jpg
  Figure 2 caption: "(a) How the loss value changes as feature magnitude s increases\
    \ under different angular scenarios in Eq. (7). For this figure, we consider the\
    \ binary case where \u03B8 1 =\u03C03 ( y=1 ) and \u03B8 2 =\u03C02 . Both \u03B7\
    (\u22C5) and \u03C8(\u22C5) are cosine function. (b) How the loss curve of Eq.\
    \ (6) varies under different feature magnitude s . For this figure, we consider\
    \ the binary case where y=1 and \u03B8 2 =\u03C02 ."
  Figure 3 Link: articels_figures_by_rev_year\2022\SphereFace_Revived_Unifying_Hyperspherical_Face_Recognition\figure_3.jpg
  Figure 3 caption: An intuitive comparison among no angular margin (e.g., [9], [30]),
    additive angular margin (CosFace [3], [4] and ArcFace [5]) and multiplicative
    angular margin (SphereFace, SphereFace-R v1 and SphereFace-R v2).
  Figure 4 Link: articels_figures_by_rev_year\2022\SphereFace_Revived_Unifying_Hyperspherical_Face_Recognition\figure_4.jpg
  Figure 4 caption: An illustration of the backward activation of CGD for SphereFace
    ( m=1.4 ), SphereFace-R v1 ( m=1.4 ) and SphereFace-R v2 ( m=1.4 ). The green
    curves demonstrate the effect of CGD in the backward propagation, and the forward
    computation still follows the red curves (i.e., Delta (theta) ) without any approximation.
  Figure 5 Link: articels_figures_by_rev_year\2022\SphereFace_Revived_Unifying_Hyperspherical_Face_Recognition\figure_5.jpg
  Figure 5 caption: A comparison of loss characteristics among normalized softmax,
    SphereFace, SphereFace-R v1 and SphereFace-R v2.
  Figure 6 Link: articels_figures_by_rev_year\2022\SphereFace_Revived_Unifying_Hyperspherical_Face_Recognition\figure_6.jpg
  Figure 6 caption: Training objective of SphereFace-R v2 with NFN, HFN and SFN on
    (a) VGGFace2 and (b) MS-Celeb-1M. For SFN, we only plot the softmax-based loss
    and the feature norm regularization is not plotted.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Weiyang Liu
  Name of the last author: Adrian Weller
  Number of Figures: 6
  Number of Tables: 12
  Number of authors: 5
  Paper title: 'SphereFace Revived: Unifying Hyperspherical Face Recognition'
  Publication Date: 2022-03-16 00:00:00
  Table 1 caption: TABLE 1 Instantiations of the Unified Hyperspherical Face Recognition
    Framework
  Table 10 caption: TABLE 10 Evaluation on MegaFace, IJB-B and IJB-C
  Table 2 caption: TABLE 2 Statistics for the Used Datasets
  Table 3 caption: TABLE 3 Varying m m for No Feature Normalization on VGGFace2 (
    % %)
  Table 4 caption: TABLE 4 Grid Searching for m m and s s With Hard Feature Normalization
    on VGGFace2
  Table 5 caption: TABLE 5 Varying t t for Soft Feature Normalization on VGGFace2
    ( % %)
  Table 6 caption: TABLE 6 Ablation of CGD for Different FN Strategies on VGGFace2
    ( % %)
  Table 7 caption: TABLE 7 Varying m m With No Feature Normalization on MS-Celeb-1M
    ( % %)
  Table 8 caption: TABLE 8 Varying m m With Hard Feature Normalization on MS-Celeb-1M
    ( % %)
  Table 9 caption: TABLE 9 Varying t t With Soft Feature Normalization on MS-Celeb-1M
    ( % %)
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3159732
- Affiliation of the first author: department of computer science and engineering,
    the chinese university of hong kong, hong kong
  Affiliation of the last author: department of computer science and engineering,
    the chinese university of hong kong, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2022\Adaptive_Perspective_Distillation_for_Semantic_Segmentation\figure_1.jpg
  Figure 1 caption: Deep semantic segmentation framework is abstracted as the process
    that the final pixel-wise observation (prediction) is obtained from the perspective
    (classifier) based on encoded features produced by the deep neural networks.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Adaptive_Perspective_Distillation_for_Semantic_Segmentation\figure_2.jpg
  Figure 2 caption: Qualitative t-SNE [35] results of the difference between the fixed
    universal perspective (top) and adaptive perspective (bottom). Categories are
    represented by different colors. Top figures show that generally correct observations
    can be obtained by the fixed universal perspective, while the lack of specification
    to individual samples causes erroneous observationsknowledge for distillation.
    On the other hand, with our proposed APD, models learn to form adaptive perspectives
    that are clearer decision boundaries as demonstrated in the bottom figures where
    the adaptive perspective, conditioned on the content of each image, decently describes
    the feature distribution. Therefore, APD reveals additional detailed co-occurring
    semantic cues conditioned on individual training samples so as to better accomplish
    the knowledge distillation.
  Figure 3 Link: articels_figures_by_rev_year\2022\Adaptive_Perspective_Distillation_for_Semantic_Segmentation\figure_3.jpg
  Figure 3 caption: Training mIoU curves of the auxiliary prediction p a,s and the
    main prediction p s of the student model on PASCAL-Context. p a,s and p s are
    obtained from the adaptive perspective and fixed universal perspective respectively.
    The auxiliary prediction p a,s achieves much higher mIoU on the training set because
    p a,s is generated by the adaptive perspective A s that is with high specification
    to each image, mining more details for knowledge distillation and forming better
    decision boundaries as depicted by the bottom examples in Fig. 2. The comparison
    on the validation set is presented in Fig. 5.
  Figure 4 Link: articels_figures_by_rev_year\2022\Adaptive_Perspective_Distillation_for_Semantic_Segmentation\figure_4.jpg
  Figure 4 caption: Illustration of our method. The input image is first processed
    by teacher and student encoders ( mathcal Gt and mathcal Gs ) respectively to
    get the encoded feature maps boldsymbolft and boldsymbolfs . To accomplish normal
    KD, mathcal Lkd [12] is applied to the predictions obtained from the main classifiers
    boldsymbolCt and boldsymbolCs , offering a global perspective. boldsymbolft and
    boldsymbolfs are also transformed by projectors ( mathcal Pt and mathcal Ps )
    to form adaptive classifiers mathcal At and mathcal As , serving as local perspectives
    that reveal useful details by better describing the feature distributions as shown
    in Figs. 2 and 3. We note that the projected features boldsymbolfa,t and boldsymbolfa,s
    are l-2 normalized. Then, the distillation from the adaptive perspectives is accomplished
    by the proposed mathcal Lrec and mathcal Lob that rectifies adaptive classifiers
    and aligns auxiliary predictions ( boldsymbolpa,t and boldsymbolpa,s ) respectively.
    mathcal Lt only updates teachers projector mathcal Pt , and the gradients yielded
    by mathcal Lkd , mathcal Lrec and mathcal Lob will not be back-propagated to boldsymbolpt
    , mathcal At and boldsymbolpa,t . The normal cross entropy loss mathcal Lce applied
    to boldsymbolps is omitted in this figure for simplicity.
  Figure 5 Link: articels_figures_by_rev_year\2022\Adaptive_Perspective_Distillation_for_Semantic_Segmentation\figure_5.jpg
  Figure 5 caption: Validation mIoU curves on Cityscapes, ADE20K and PASCAL-Context.
    Our proposed APD (colored in red) consistently outperforms other methods throughout
    the training process. The teacher is PSPNet with ResNet-101 and the student is
    PSPNet with ResNet-18.
  Figure 6 Link: articels_figures_by_rev_year\2022\Adaptive_Perspective_Distillation_for_Semantic_Segmentation\figure_6.jpg
  Figure 6 caption: Visual comparison on Cityscapes, ADE20K and PASCAL-Context. White
    regions in GT are ignored during evaluation.
  Figure 7 Link: articels_figures_by_rev_year\2022\Adaptive_Perspective_Distillation_for_Semantic_Segmentation\figure_7.jpg
  Figure 7 caption: Visual comparison of object detection (first three rows) and instance
    segmentation (last three rows) on COCO2017 val set.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.91
  Name of the first author: Zhuotao Tian
  Name of the last author: Jiaya Jia
  Number of Figures: 7
  Number of Tables: 15
  Number of authors: 9
  Paper title: Adaptive Perspective Distillation for Semantic Segmentation
  Publication Date: 2022-03-16 00:00:00
  Table 1 caption: TABLE 1 Training Configurations on Different Datasets
  Table 10 caption: TABLE 10 Ablation Study of the Effects of Feature Selection (FS)
    Mechanisms on the Validation Sets of PASCAL-Context and Cityscapes
  Table 2 caption: TABLE 2 Performance Comparison With State-of-the-Art Methods on
    Cityscapes val With PSPNet [54] and DeepLab-V3 [3]
  Table 3 caption: TABLE 3 Efficiency Comparison on Cityscapes Test
  Table 4 caption: 'TABLE 4 Performance Comparison With State-of-the-Art Methods Using
    PSPNet on the Validation Sets of Three Popular Benchmarks: Cityscapes [6], ADE20K
    [56] and PASCAL-Context [22]'
  Table 5 caption: TABLE 5 Cross-Model Distillation Results on Cityscapes Val With
    PSPNet [54] and DeepLab-V3 [3]
  Table 6 caption: TABLE 6 Ablation Study on the Validation Sets Of PASCAL-Context
    and Cityscapes
  Table 7 caption: TABLE 7 Ablation Study of Different Methods for Yielding L rec
    Lrec on the Validation Sets of PASCAL-Context and Cityscapes
  Table 8 caption: TABLE 8 Ablation Study on the Validation Sets Of PASCAL-Context
    and Cityscapes With PSPNet
  Table 9 caption: TABLE 9 Ablation Study of Different Layer Numbers for Constructing
    the Projectors for Teacher and Student Models on the Validation Sets of PASCAL-Context
    and Cityscapes
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3159581
- Affiliation of the first author: school of software, tsinghua university, beijing,
    china
  Affiliation of the last author: school of software, tsinghua university, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2022\From_Big_to_Small_Adaptive_Learning_to_PartialSet_Domains\figure_1.jpg
  Figure 1 caption: The challenge and solution of partial domain adaptation (PDA).
    The solid shapes show the data in the source domain and the hollow shapes show
    the data in the target domain. Each shape represents a class. The main assumption
    is that the source class space subsumes the target class space. (a) The challenge
    is caused by the source-specific outlier class (hexagon) that is absent from the
    target domain, as well as the source-target distribution shift on the shared classes
    (circle, square, and triangle) shown by the different decision boundaries (solid
    lines) for the source and target domains. (b) We propose a bi-level selection
    strategy to eliminate the influence of source-specific outlier examples so as
    to promote discriminative learning and distribution alignment in the shared class
    space.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\From_Big_to_Small_Adaptive_Learning_to_PartialSet_Domains\figure_2.jpg
  Figure 2 caption: "(a) shows the data in the toy experiment. The source domain data\
    \ are shown by the marker \u22C5 and the target domain data are shown by the marker\
    \ \xD7. Different colors indicate the samples of different classes. There are\
    \ five classes in the source domain and three classes in the target domain, where\
    \ the source class space subsumes the target class space. The black dashed lines\
    \ show the optimal decision boundary for the source domain. (b) The change of\
    \ the value of each term in the bound during the training process, and the sum\
    \ of the right side in Eqn. (1)."
  Figure 3 Link: articels_figures_by_rev_year\2022\From_Big_to_Small_Adaptive_Learning_to_PartialSet_Domains\figure_3.jpg
  Figure 3 caption: The architecture of Selective Adversarial Network (SAN++) for
    partial domain adaptation. F is the feature extractor and f is the feature. G
    is the classifier and y is the predicted label. D k | | C s | k=1 is the multi-task
    discriminator with shared bottom layers and | C s | top heads, and d k | | C s
    | k=1 are the predicted domain labels. L sup and L self , L k adv | | C s | k=1
    are the source supervised training loss, the target self-training loss and the
    distribution alignment loss respectively. w denotes the class transferable probability.
    Solid arrows show data flow. Dashed arrows show instance selection and dotted
    arrows show class selection.
  Figure 4 Link: articels_figures_by_rev_year\2022\From_Big_to_Small_Adaptive_Learning_to_PartialSet_Domains\figure_4.jpg
  Figure 4 caption: The negative natural logarithm of p-value that SAN++ outperforms
    SAN. The red line is -log (0.05) indicating the bar of statistical significance.
  Figure 5 Link: articels_figures_by_rev_year\2022\From_Big_to_Small_Adaptive_Learning_to_PartialSet_Domains\figure_5.jpg
  Figure 5 caption: The negative natural logarithm of p-value that SAN++ outperforms
    ETN. The red line is -log (0.05) indicating the bar of statistical significance.
  Figure 6 Link: articels_figures_by_rev_year\2022\From_Big_to_Small_Adaptive_Learning_to_PartialSet_Domains\figure_6.jpg
  Figure 6 caption: (a) Accuracy by varying the size of target class space on A rightarrow
    W; (b) The estimated class transferable probability mathbf w for the task A rightarrow
    W.
  Figure 7 Link: articels_figures_by_rev_year\2022\From_Big_to_Small_Adaptive_Learning_to_PartialSet_Domains\figure_7.jpg
  Figure 7 caption: The class transferable probability mathbf w evolves and improves
    throughout training. The shared classes in mathcal C are upweighted while the
    outlier source classes in barmathcal Cs are filtered out progressively.
  Figure 8 Link: articels_figures_by_rev_year\2022\From_Big_to_Small_Adaptive_Learning_to_PartialSet_Domains\figure_8.jpg
  Figure 8 caption: Confusion matrix of prediction results of ResNet, RTN, IWAN and
    SAN++ on VisDA-2017 dataset.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.69
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Zhangjie Cao
  Name of the last author: Mingsheng Long
  Number of Figures: 8
  Number of Tables: 15
  Number of authors: 5
  Paper title: 'From Big to Small: Adaptive Learning to Partial-Set Domains'
  Publication Date: 2022-03-16 00:00:00
  Table 1 caption: TABLE 1 Comparison of Different Domain Adaptation Settings Based
    on Their Assumptions
  Table 10 caption: TABLE 10 Comparison With BA 3 3US on Office-31 (ResNet-50)
  Table 2 caption: TABLE 2 Details of the Datasets, the Tasks, and the Target Class
    Space for all PDA Experiments
  Table 3 caption: TABLE 3 Accuracy (%) of the Closed-set Domain Adaptation Tasks
    on Office-31 (ResNet-50)
  Table 4 caption: TABLE 4 Accuracy (%) of Partial Domain Adaptation Tasks on Office-Home
    (ResNet-50)
  Table 5 caption: TABLE 5 Accuracy (%) of Partial Domain Adaptation Tasks on Office-31
    (ResNet-50)
  Table 6 caption: TABLE 6 Accuracy (%) of Partial Domain Adaptation Tasks on VisDA-2017,
    ImageNet-Caltech and OpenMIC (ResNet-50)
  Table 7 caption: TABLE 7 Accuracy (%) of SAN++, SAN and Pixel-Level Domain Adaptation
    Method on Digits
  Table 8 caption: TABLE 8 Comparison With BA 3 3US on Office-Home (ResNet-50)
  Table 9 caption: TABLE 9 Comparison With BA 3 3US on VisDA-2017, ImageNet-Caltech
    and OpenMIC (ResNet-50)
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3159831
