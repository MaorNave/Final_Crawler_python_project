- Affiliation of the first author: school of computer science and technology, east
    china normal university, shanghai, china
  Affiliation of the last author: school of computer science and technology, east
    china normal university, shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2020\LCBM_A_MultiView_Probabilistic_Model_for_MultiLabel_Classification\figure_1.jpg
  Figure 1 caption: "Graphical illustration of the LCBM model. \u03B8 v V v=1 are\
    \ model parameters associated with each input view x v . \u03B1 and \u03B2 are\
    \ model parameters to be learned for label predictions."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\LCBM_A_MultiView_Probabilistic_Model_for_MultiLabel_Classification\figure_2.jpg
  Figure 2 caption: Variational view of the LCBM model.
  Figure 3 Link: articels_figures_by_rev_year\2020\LCBM_A_MultiView_Probabilistic_Model_for_MultiLabel_Classification\figure_3.jpg
  Figure 3 caption: Externalize the randomness in t by reparameterization.
  Figure 4 Link: articels_figures_by_rev_year\2020\LCBM_A_MultiView_Probabilistic_Model_for_MultiLabel_Classification\figure_4.jpg
  Figure 4 caption: Comparison of latent multi-view representation and the impact
    of varying number of components.
  Figure 5 Link: articels_figures_by_rev_year\2020\LCBM_A_MultiView_Probabilistic_Model_for_MultiLabel_Classification\figure_5.jpg
  Figure 5 caption: Convergence of LCBM on nine benchmark datasets.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.89
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Shiliang Sun
  Name of the last author: Daoming Zong
  Number of Figures: 5
  Number of Tables: 5
  Number of authors: 2
  Paper title: 'LCBM: A Multi-View Probabilistic Model for Multi-Label Classification'
  Publication Date: 2020-02-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of the Experimental Data Sets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Results (mean \xB1 \xB1std) on Nine Datasets With Respect\
      \ to the Subset Accuracy"
  Table 3 caption:
    table_text: TABLE 3 Wilcoxon Signed-Ranks Test for LCBM Against Other Single-View
      Competitors With Respect to Each Evaluation Metric
  Table 4 caption:
    table_text: TABLE 4 Wilcoxon Signed-Ranks Test for LCBM Against Other Multi-View
      Competitors With Respect to Each Evaluation Metric
  Table 5 caption:
    table_text: TABLE 5 Comparison of Multi-View Multi-Label Methods on Nine Datasets
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2974203
- Affiliation of the first author: department of computer science, university of texas
    at austin, austin, tx, usa
  Affiliation of the last author: department of computer science, university of texas
    at austin, austin, tx, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Learning_Compressible__Video_Isomers\figure_1.jpg
  Figure 1 caption: "Our approach learns to automatically rotate the 360 \u2218 video\
    \ axis before storing the video in cubemap format. While the 360 \u2218 videos\
    \ are equivalent under rotation (\u201Cisomers\u201D), the bitstreams are not\
    \ because of the video compression procedures. Our approach analyzes the videos\
    \ visual content to predict its most compressible isomer."
  Figure 10 Link: articels_figures_by_rev_year\2020\Learning_Compressible__Video_Isomers\figure_10.jpg
  Figure 10 caption: Absolute size reduction (MB) of each video. Each point represents
    the input video size versus size reduction relative to Center achieved by our
    model.
  Figure 2 Link: articels_figures_by_rev_year\2020\Learning_Compressible__Video_Isomers\figure_2.jpg
  Figure 2 caption: "Cubemap format transformation. The 360 \u2218 video is first\
    \ projected to a cube enclosing the unit sphere and then unwrapped into 6 faces.\
    \ The 6 faces are re-arranged to form a rectangular picture to fit video compression\
    \ standards ( 2\xD73 frame on the right)."
  Figure 3 Link: articels_figures_by_rev_year\2020\Learning_Compressible__Video_Isomers\figure_3.jpg
  Figure 3 caption: "(a) Relative clip size distribution w.r.t. \u03A9 . We cluster\
    \ the distribution into 16 clusters and show the cluster centers. (b) Size distribution\
    \ for one particular video. We show the cubemaps corresponding to \u03A9 max and\
    \ \u03A9 min , its worst (max) and best (min) orientations for compressibility."
  Figure 4 Link: articels_figures_by_rev_year\2020\Learning_Compressible__Video_Isomers\figure_4.jpg
  Figure 4 caption: "Explanations for why different \u03A9 have different compression\
    \ rates, shown for good ( \u03A9 min ) and bad ( \u03A9 max ) rotations. (a) From\
    \ a static picture perspective, some \u03A9 introduce content discontinuity and\
    \ reduce spatial redundancy. (b) From a dynamic picture perspective, some \u03A9\
    \ make the motion more disordered and break the temporal redundancy."
  Figure 5 Link: articels_figures_by_rev_year\2020\Learning_Compressible__Video_Isomers\figure_5.jpg
  Figure 5 caption: Real examples for the explanations in Fig. 4. (A) shows content
    discontinuity introduced by rotation. (B) shows motion discontinuity, where the
    encoder fails to find reference blocks and the number of intra-coded blocks increases.
  Figure 6 Link: articels_figures_by_rev_year\2020\Learning_Compressible__Video_Isomers\figure_6.jpg
  Figure 6 caption: Achievable video size reduction through rotation for (a) lossless
    compression in different formats and (b) lossy compression at different compression
    rates and GOP lengths for H264. Clip-level reductons are per GOP (fixed in length);
    video-level reductions are aggregated over all clips in a video (and hence vary
    in length). We can reduce the video size by up to 77 percent by optimally changing
    the cubemap orientation.
  Figure 7 Link: articels_figures_by_rev_year\2020\Learning_Compressible__Video_Isomers\figure_7.jpg
  Figure 7 caption: Our model takes a video clip GOP as input and predicts Omega min
    as output. (A) It first divides the video into 4 segments temporally and (B) extracts
    appearance and motion features from each segment. (C) It then concatenates the
    appearance and motion feature maps and feeds them into a CNN. (D) The model concatenates
    the outputs of each segment together and joins the output with the input feature
    map using skip connections to form the video feature. (F) It then learns a regression
    model that predicts the relative video size Sprime Omega for all Omega and takes
    the minimum one as the predicted optimally compressible isomer.
  Figure 8 Link: articels_figures_by_rev_year\2020\Learning_Compressible__Video_Isomers\figure_8.jpg
  Figure 8 caption: We propose a two-pass compression pipeline. The second pass optimizes
    the compression rate by taking the orientation predicted from the output bitstream
    of the first pass.
  Figure 9 Link: articels_figures_by_rev_year\2020\Learning_Compressible__Video_Isomers\figure_9.jpg
  Figure 9 caption: Example frames from JVET test sequences. The images are from ChairliftRide,
    Gaslamp, Harbor, KiteFlite, SkateboardInLot, and Trolley sequences, respectively,
    from top left to bottom right.
  First author gender probability: 0.79
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Yu-Chuan Su
  Name of the last author: Kristen Grauman
  Number of Figures: 12
  Number of Tables: 5
  Number of authors: 2
  Paper title: "Learning Compressible 360\n\u2218\n\u2218 Video Isomers"
  Publication Date: 2020-02-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Correlation of Relative Video Sizes for (a) Lossless Compression
      Across Video Formats and (b) Lossy Compression Across Different Compression
      Rates
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Size Reduction of Each Method for Lossless Compression
  Table 3 caption:
    table_text: TABLE 3 Video Size Reduction for Lossy Video Compression (Higher is
      Better)
  Table 4 caption:
    table_text: TABLE 4 Size Reduction (%) of Our Approach in the Transferability
      Experiment
  Table 5 caption:
    table_text: "TABLE 5 Bitrate Reduction on JVET Test Sequences and the YouTube\
      \ 360 \u2218 360\u2218 Video Dataset"
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2974472
- Affiliation of the first author: "department of energy engineering, s\xE3o paulo\
    \ state university (unesp), rosana, brazil"
  Affiliation of the last author: "icmc, university of s\xE3o paulo (usp), s\xE3o\
    \ carlos, brazil"
  Figure 1 Link: articels_figures_by_rev_year\2020\Laplacian_Coordinates_Theory_and_Methods_for_Seeded_Image_Segmentation\figure_1.jpg
  Figure 1 caption: "A summary of different capabilities of the Laplacian Coordinates\
    \ framework. From left to right, (a) Smoothness and seed propagability: Laplacian\
    \ Coordinates solution \xD7 the well-established Random Walker [21] for line graphs\
    \ as shown in the top row, with unitary weights and seeds in yellow and purple,\
    \ (b) Contour adherenceboundary fitting, (c) Multiple-region segmentation, (d)\
    \ Fast superpixel segmentation for large size images, (e) Invariance to seed placement."
  Figure 10 Link: articels_figures_by_rev_year\2020\Laplacian_Coordinates_Theory_and_Methods_for_Seeded_Image_Segmentation\figure_10.jpg
  Figure 10 caption: 'From top to bottom: sample images + scribbles from BSD dataset,
    and the outputs produced by each method.'
  Figure 2 Link: articels_figures_by_rev_year\2020\Laplacian_Coordinates_Theory_and_Methods_for_Seeded_Image_Segmentation\figure_2.jpg
  Figure 2 caption: "Solution x i (for i=7 ) in terms of its local neighborhood nodes.\
    \ The gray circle illustrates the points used to compute the differential coordinate\
    \ \u03B4 j at node j=11 ."
  Figure 3 Link: articels_figures_by_rev_year\2020\Laplacian_Coordinates_Theory_and_Methods_for_Seeded_Image_Segmentation\figure_3.jpg
  Figure 3 caption: 'The use of Eq. (21) for multiple segmentation. First to fourth
    rows: multiple seeds are sketched as colored strokes, from which Laplacian Coordinates
    produces the segmented regions. Bottom row: the five computed solutions x (j)
    that give rise to the multiple partitions y (j) merged in the womans segmentation.'
  Figure 4 Link: articels_figures_by_rev_year\2020\Laplacian_Coordinates_Theory_and_Methods_for_Seeded_Image_Segmentation\figure_4.jpg
  Figure 4 caption: 'Selecting different targets by exploiting the seeding flexibility
    of Laplacian Coordinates. Odd rows: multiple markings are given as input to the
    soft (first example) and hard (middle and bottom examples) LC modalities.'
  Figure 5 Link: articels_figures_by_rev_year\2020\Laplacian_Coordinates_Theory_and_Methods_for_Seeded_Image_Segmentation\figure_5.jpg
  Figure 5 caption: "Superpixel graph representation. (Left) A superpixel partition\
    \ of the image. (Right) Differential operator \u03B4 i evaluated at node i=5 ."
  Figure 6 Link: articels_figures_by_rev_year\2020\Laplacian_Coordinates_Theory_and_Methods_for_Seeded_Image_Segmentation\figure_6.jpg
  Figure 6 caption: Quantitative evaluation concerning the boundary-based metrics
    Recall, Precision and F-measure score over different neighborhood radius R [115]
    for the MSRC dataset with usual seeds.
  Figure 7 Link: articels_figures_by_rev_year\2020\Laplacian_Coordinates_Theory_and_Methods_for_Seeded_Image_Segmentation\figure_7.jpg
  Figure 7 caption: Quantitative evaluation concerning the boundary-based metrics
    Recall, Precision and F-measure score over different neighborhood radius R [115]
    for the MSRC with the S1 seed set.
  Figure 8 Link: articels_figures_by_rev_year\2020\Laplacian_Coordinates_Theory_and_Methods_for_Seeded_Image_Segmentation\figure_8.jpg
  Figure 8 caption: Quantitative evaluation concerning the boundary-based metrics
    Recall, Precision and F-measure score over different neighborhood radius R [115]
    for the MSRC with the S2 seed set.
  Figure 9 Link: articels_figures_by_rev_year\2020\Laplacian_Coordinates_Theory_and_Methods_for_Seeded_Image_Segmentation\figure_9.jpg
  Figure 9 caption: Quantitative evaluation concerning the boundary-based metrics
    Recall, Precision and F-score over different neighborhood radius R [115] for the
    BSD dataset.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Wallace Casaca
  Name of the last author: Luis Gustavo Nonato
  Number of Figures: 12
  Number of Tables: 5
  Number of authors: 5
  Paper title: 'Laplacian Coordinates: Theory and Methods for Seeded Image Segmentation'
  Publication Date: 2020-02-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 RI, VoI, BDE and Dice Scores Obtained from the Original MSRC
      Dataset for Each Segmentation Method
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 RI, VoI, BDE and Dice Scores Obtained from the MSRC With the
      S1 Seed Set for Each Segmentation Method
  Table 3 caption:
    table_text: TABLE 3 RI, VoI, BDE and Dice Scores Obtained from the MSRC With the
      S2 Seed Set for Each Segmentation Method
  Table 4 caption:
    table_text: TABLE 4 RI, VoI, BDE and Dice Scores Obtained from the BSD Dataset
      for Each Segmentation Method
  Table 5 caption:
    table_text: TABLE 5 Sensitivity of the Segmentation Methods w.r.t. Seed Quantity
      and Seed Placement When Measured by the Average of a 0 a0 Overlap Measure
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2974475
- Affiliation of the first author: department of computer science, cornell tech, cornell
    university, ithaca, ny, usa
  Affiliation of the last author: google research, mountain view, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\MannequinChallenge_Learning_the_Depths_of_Moving_People_by_Watching_Frozen_Peopl\figure_1.jpg
  Figure 1 caption: "Our model predicts dense depth when both an ordinary camera and\
    \ people in the scene are freely moving (right). We train our model on our new\
    \ MannequinChallenge dataset\u2014a collection of Internet videos of people imitating\
    \ mannequins, i.e., freezing in diverse, natural poses, while a camera tours the\
    \ scene (left). Because people are stationary, geometric constraints hold; this\
    \ allows us to use multi-view stereo to estimate depth which serves as supervision\
    \ during training. In all figures, we use inverse depth maps for visualization\
    \ purposes, and refer to them as depth maps."
  Figure 10 Link: articels_figures_by_rev_year\2020\MannequinChallenge_Learning_the_Depths_of_Moving_People_by_Watching_Frozen_Peopl\figure_10.jpg
  Figure 10 caption: Qualitative comparisons on the TUM RGBD dataset. (a) Reference
    images, (b) source images (used to compute our initial depth input), (c) ground
    truth sensor depth, (d) results of the single-view depth prediction method DORN
    [9], (e) result of the two-frame motion stereo method DeMoN [48], (f-g) depth
    predictions from our single view and two-frame models, respectively.
  Figure 2 Link: articels_figures_by_rev_year\2020\MannequinChallenge_Learning_the_Depths_of_Moving_People_by_Watching_Frozen_Peopl\figure_2.jpg
  Figure 2 caption: 'Traditional stereo versus our setup. Left: A person is observed
    at the same time instant from two different views. The 3D position of points can
    be computed using triangulation. Right: when both the camera and the objects in
    the scene are moving, triangulation is no longer possible since the epipolar constraint
    does not apply.'
  Figure 3 Link: articels_figures_by_rev_year\2020\MannequinChallenge_Learning_the_Depths_of_Moving_People_by_Watching_Frozen_Peopl\figure_3.jpg
  Figure 3 caption: Sample images from Mannequin Challenge videos. Each image is a
    frame from a video sequence in which the camera is moving but the humans are all
    static. The videos span a variety of natural scenes, poses, and configurations
    of people.
  Figure 4 Link: articels_figures_by_rev_year\2020\MannequinChallenge_Learning_the_Depths_of_Moving_People_by_Watching_Frozen_Peopl\figure_4.jpg
  Figure 4 caption: Effect of depth cleaning. (a-b) Raw MVS depth maps, D MVS , may
    contain errors and outliers, especially in untextured regions (see regions circled
    in yellow). (c) Our depth cleaning method effectively filters out such erroneous
    depth values.
  Figure 5 Link: articels_figures_by_rev_year\2020\MannequinChallenge_Learning_the_Depths_of_Moving_People_by_Watching_Frozen_Peopl\figure_5.jpg
  Figure 5 caption: Sample frames from clips removed during filtering. (a) Videos
    captured with fisheye cameras; (b) videos with synthetic backgrounds; (c) sequences
    with truly moving objects (pairs of frames shown in each column).
  Figure 6 Link: articels_figures_by_rev_year\2020\MannequinChallenge_Learning_the_Depths_of_Moving_People_by_Watching_Frozen_Peopl\figure_6.jpg
  Figure 6 caption: System overview. Our model takes as input the RGB frame, a human
    segmentation mask, masked depth from motion parallax (via optical flow and SfM
    pose), and associated confidence map. We ask the network to use these inputs to
    predict depths that match the ground truth MVS depth.
  Figure 7 Link: articels_figures_by_rev_year\2020\MannequinChallenge_Learning_the_Depths_of_Moving_People_by_Watching_Frozen_Peopl\figure_7.jpg
  Figure 7 caption: 'System inputs and training data. The input to our network consists
    of: (a) An RGB image, (b) a human mask, (c) a masked depth map computed from motion
    parallax w.r.t. a selected source image, and (d) a masked confidence map. Low
    confidence regions (dark circles) in the first two rows indicate the vicinity
    of the camera epipole, where depth from parallax is unreliable and removed. The
    network is trained to regress to MVS depth (e).'
  Figure 8 Link: articels_figures_by_rev_year\2020\MannequinChallenge_Learning_the_Depths_of_Moving_People_by_Watching_Frozen_Peopl\figure_8.jpg
  Figure 8 caption: Examples of keypoint images. The top row shows examples of input
    images and the bottom row shows corresponding detected human keypoint images,
    where different colors indicating different joints. We perform morphological dilation
    to the keypoint maps to make each keypoint location more visible.
  Figure 9 Link: articels_figures_by_rev_year\2020\MannequinChallenge_Learning_the_Depths_of_Moving_People_by_Watching_Frozen_Peopl\figure_9.jpg
  Figure 9 caption: 'Qualitative results on the MC test set. From top to bottom: Reference
    images and their corresponding MVS depth (pseudo ground truth); our depth predictions
    using: Our single view model (third row) and our two-frame model (forth row).
    The additional network inputs give improved performance in both human and non-human
    regions.'
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Zhengqi Li
  Name of the last author: William T. Freeman
  Number of Figures: 14
  Number of Tables: 2
  Number of authors: 7
  Paper title: 'MannequinChallenge: Learning the Depths of Moving People by Watching
    Frozen People'
  Publication Date: 2020-02-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Comparisons on the MC Test Set
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results on the TUM RGBD Dataset
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2974454
- Affiliation of the first author: department of computational and data sciences,
    video analytics lab, indian institute of science, bangalore, india
  Affiliation of the last author: department of computational and data sciences, video
    analytics lab, indian institute of science, bangalore, india
  Figure 1 Link: articels_figures_by_rev_year\2020\Locate_Size_and_Count_Accurately_Resolving_People_in_Dense_Crowds_via_Detection\figure_1.jpg
  Figure 1 caption: Face detection versus Crowd counting. Tiny Face detector [1],
    trained on face dataset [2] with box annotations, is able to capture 731 out of
    the 1151 people in the first image [3], losing mainly in highly dense regions.
    In contrast, despite being trained on crowd dataset [4] having only point head
    annotations, our LSC-CNN detects 999 persons (second image) consistently across
    density ranges and provides fairly accurate boxes.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Locate_Size_and_Count_Accurately_Resolving_People_in_Dense_Crowds_via_Detection\figure_2.jpg
  Figure 2 caption: The architecture of the proposed LSC-CNN is shown. LSC-CNN jointly
    processes multi-scale information from the feature extractor and provides predictions
    at multiple resolutions, which are combined to form the final detections. The
    model is optimized for per-pixel classification of pseudo ground truth boxes generated
    in the GWTA training phase (indicated with dotted lines).
  Figure 3 Link: articels_figures_by_rev_year\2020\Locate_Size_and_Count_Accurately_Resolving_People_in_Dense_Crowds_via_Detection\figure_3.jpg
  Figure 3 caption: The exact configuration of Feature Extractor, which is a modified
    version of VGG-16 [43] and outputs feature maps at multiple scales.
  Figure 4 Link: articels_figures_by_rev_year\2020\Locate_Size_and_Count_Accurately_Resolving_People_in_Dense_Crowds_via_Detection\figure_4.jpg
  Figure 4 caption: The implementation of the TFM module is depicted. TFM( s ) processes
    the features from scale s (terminal 1 ) along with s multi-scale inputs from higher
    branches (terminal 3 ) to output head detections (terminal 4 ) and the features
    (terminal 2 ) for the next scale branch.
  Figure 5 Link: articels_figures_by_rev_year\2020\Locate_Size_and_Count_Accurately_Resolving_People_in_Dense_Crowds_via_Detection\figure_5.jpg
  Figure 5 caption: Samples of generated pseudo box ground truth. Boxes with same
    color belong to one scale branch.
  Figure 6 Link: articels_figures_by_rev_year\2020\Locate_Size_and_Count_Accurately_Resolving_People_in_Dense_Crowds_via_Detection\figure_6.jpg
  Figure 6 caption: Illustration of the operations in GWTA training. GWTA only selects
    the highest loss making cell in every scale. The per-pixel cross-entropy loss
    is computed between the prediction and pseudo ground truth maps.
  Figure 7 Link: articels_figures_by_rev_year\2020\Locate_Size_and_Count_Accurately_Resolving_People_in_Dense_Crowds_via_Detection\figure_7.jpg
  Figure 7 caption: Predictions made by LSC-CNN on images from Shanghaitech, UCF-QNRF
    and UCF-CC-50 datasets. The results emphasize the ability of our approach to pinpoint
    people consistently across crowds of different types than the baseline density
    regression method.
  Figure 8 Link: articels_figures_by_rev_year\2020\Locate_Size_and_Count_Accurately_Resolving_People_in_Dense_Crowds_via_Detection\figure_8.jpg
  Figure 8 caption: Demonstrating the effectiveness of GWTA in proper training of
    high resolution scale branches (notice the highlighted region).
  Figure 9 Link: articels_figures_by_rev_year\2020\Locate_Size_and_Count_Accurately_Resolving_People_in_Dense_Crowds_via_Detection\figure_9.jpg
  Figure 9 caption: Comparison of predictions made by face detectors SSH [18] and
    TinyFaces [1] against LSC-CNN. Note that the Ground Truth shown for WIDERFACE
    dataset is the actual and not the pseudo box ground truth. Normal face detectors
    are seen to fail on dense crowds.
  First author gender probability: 0.96
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Deepak Babu Sam
  Name of the last author: R. Venkatesh Babu
  Number of Figures: 9
  Number of Tables: 10
  Number of authors: 5
  Paper title: 'Locate, Size, and Count: Accurately Resolving People in Dense Crowds
    via Detection'
  Publication Date: 2020-02-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of LSC-CNN on Localization Metrics Against the
      Baseline Regression Method
  Table 10 caption:
    table_text: TABLE 10 Efficiency of Detectors in Terms of Inference Speed and Model
      Size
  Table 2 caption:
    table_text: TABLE 2 Evaluation of LSC-CNN Box Prediction on WIDERFACE [2]
  Table 3 caption:
    table_text: TABLE 3 Counting Performance Comparison of LSC-CNN on UCF-QNRF [9]
  Table 4 caption:
    table_text: TABLE 4 Benchmarking LSC-CNN Counting Accuracy on Shanghaitech [4]
      and UCFCC50 [9] Datasets
  Table 5 caption:
    table_text: TABLE 5 LSC-CNN on WorldExpo10 [9] Beats Other Methods in Average
      MAE
  Table 6 caption:
    table_text: TABLE 6 Evaluation of LSC-CNN on TRANCOS [45] Vehicle Counting Dataset
  Table 7 caption:
    table_text: TABLE 7 MAE Obtained by LSC-CNN With Different Hyper-Parameter Settings
  Table 8 caption:
    table_text: TABLE 8 Validating Various Architectural Design Choices of LSC-CNN
  Table 9 caption:
    table_text: TABLE 9 LSC-CNN Compared With Existing Detectors Trained on Crowd
      Datasets
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2974830
- Affiliation of the first author: institute for chemical research, kyoto university,
    kyoto, japan
  Affiliation of the last author: institute for chemical research, kyoto university,
    kyoto, japan
  Figure 1 Link: articels_figures_by_rev_year\2020\Learning_on_Hypergraphs_With_Sparsity\figure_1.jpg
  Figure 1 caption: Hypergraph example with nodes, which are houses in a neighborhood,
    numbered from 1 to 7. Hyperedges (street 1, street 2, and street 3) contain houses
    on the same streets. For the problem of predicting house prices, this is a similarity
    hypergraph as houses on the same street are expected to have similar prices.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Learning_on_Hypergraphs_With_Sparsity\figure_2.jpg
  Figure 2 caption: Simulation with irrelevant hyperedges without noisy nodes. X axis
    was the number of irrelevant hyperedges and y axis was the RMSEs of different
    models on test data. Hyperedge selection method gave the lowest RMSEs consistently
    at different numbers of irrelevant hyperedges.
  Figure 3 Link: articels_figures_by_rev_year\2020\Learning_on_Hypergraphs_With_Sparsity\figure_3.jpg
  Figure 3 caption: Simulation with noisy nodes and without irrelevant hyperedges.
    X axis was the number of noisy nodes added to each hyperedge and y axis was the
    RMSEs of the models. Hyperedge selection model had the worst performances while
    the node selection and joint selection models performed best in terms of RMSEs.
  Figure 4 Link: articels_figures_by_rev_year\2020\Learning_on_Hypergraphs_With_Sparsity\figure_4.jpg
  Figure 4 caption: Simulation data with both irrelevant hyperedges and noisy nodes,
    generated with five irrelevant hyperedges. X axis was different numbers of noisy
    nodes added to each relevant hyperedge and y axis was the RMSEs of the models.
    Joint selection gave the lowest RMSEs, showing its effectiveness for both irrelevant
    hyperedges and noisy nodes.
  Figure 5 Link: articels_figures_by_rev_year\2020\Learning_on_Hypergraphs_With_Sparsity\figure_5.jpg
  Figure 5 caption: Simulation results on hypergraphs with five relevant hyperedges
    and five irrelevant hyperedges. X axis was the indexes of hyperedges. From 1 to
    5 were relevant hyperedges and from 6 to 10 were irrelevant ones. Y axis was the
    smoothness measures on each hyperedge with different lambda . We could observe
    that when lambda were small enough, large gaps occured between smoothness measures
    of relevant and irrelevant hyperedge groups.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.9
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Canh Hao Nguyen
  Name of the last author: Hiroshi Mamitsuka
  Number of Figures: 5
  Number of Tables: 3
  Number of authors: 2
  Paper title: Learning on Hypergraphs With Sparsity
  Publication Date: 2020-02-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Different Variants of (1) as Smoothness Measures for GraphsHypergraphs
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Smoothness Measures and Their Sparse-Inducing
      Property
  Table 3 caption:
    table_text: TABLE 3 RMSEs of the Four Models On Real Data
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2974746
- Affiliation of the first author: school of electronic information and communications,
    huazhong university of science and technology (hust), wuhan, hubei, china
  Affiliation of the last author: school of electronic information and communications,
    huazhong university of science and technology (hust), wuhan, hubei, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Gliding_Vertex_on_the_Horizontal_Bounding_Box_for_MultiOriented_Object_Detection\figure_1.jpg
  Figure 1 caption: Pipeline of the proposed method. An image is fed into a CNN, which
    outputs a classification score (blue value), a horizontal bounding box, four length
    ratios between each segment s i and corresponding side, and an obliquity factor
    (green value) for each detection. Based on obliquity factor, we select horizontal
    box (in purple) or oriented detection (in orange) as the final result. Best viewed
    in electronic version.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Gliding_Vertex_on_the_Horizontal_Bounding_Box_for_MultiOriented_Object_Detection\figure_2.jpg
  Figure 2 caption: "Illustration of proposed representation for an oriented object\
    \ O based on four intersecting points v i between O and its horizontal bounding\
    \ box B h =( v \u2032 1 , v \u2032 2 , v \u2032 3 , v \u2032 4 )=(x,y,w,h) . We\
    \ adopt (x,y,w,h, \u03B1 1 , \u03B1 2 , \u03B1 3 , \u03B1 4 ) to represent oriented\
    \ objects."
  Figure 3 Link: articels_figures_by_rev_year\2020\Gliding_Vertex_on_the_Horizontal_Bounding_Box_for_MultiOriented_Object_Detection\figure_3.jpg
  Figure 3 caption: 'Network architecture. We simply add five extra target variables
    (normalized to [0,1] using the sigmoid funciton) to the head of faster R-CNN [1].
    K : number of classes; k : a certain class.'
  Figure 4 Link: articels_figures_by_rev_year\2020\Gliding_Vertex_on_the_Horizontal_Bounding_Box_for_MultiOriented_Object_Detection\figure_4.jpg
  Figure 4 caption: Some detection results of the proposed method on DOTA [23]. The
    arbitrary-oriented objects are correctly detected.
  Figure 5 Link: articels_figures_by_rev_year\2020\Gliding_Vertex_on_the_Horizontal_Bounding_Box_for_MultiOriented_Object_Detection\figure_5.jpg
  Figure 5 caption: Mean absolute error (MAE) of obliquity factor r and gliding offset
    alpha regression with respect to different ranges of ground-truth obliquity factors
    for the proposed method on DOTA.
  Figure 6 Link: articels_figures_by_rev_year\2020\Gliding_Vertex_on_the_Horizontal_Bounding_Box_for_MultiOriented_Object_Detection\figure_6.jpg
  Figure 6 caption: Qualitative comparison with baseline methods in detecting objects
    of different orientations (by rotating an input image with different angles).
    The meaning of colors is the same as that in Fig. 4.
  Figure 7 Link: articels_figures_by_rev_year\2020\Gliding_Vertex_on_the_Horizontal_Bounding_Box_for_MultiOriented_Object_Detection\figure_7.jpg
  Figure 7 caption: Some detection results of the proposed method on HRSC2016 [36]
    in (a), MSRA-TD500 [37] in (b-c), and RCTW-17 [39] in (d-e).
  Figure 8 Link: articels_figures_by_rev_year\2020\Gliding_Vertex_on_the_Horizontal_Bounding_Box_for_MultiOriented_Object_Detection\figure_8.jpg
  Figure 8 caption: Qualitative illustrations of different methods on MW-18Mar [40].
  Figure 9 Link: articels_figures_by_rev_year\2020\Gliding_Vertex_on_the_Horizontal_Bounding_Box_for_MultiOriented_Object_Detection\figure_9.jpg
  Figure 9 caption: Evaluation on MW-18Mar [40]. The numbers are the LAMRs.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Yongchao Xu
  Name of the last author: Xiang Bai
  Number of Figures: 9
  Number of Tables: 5
  Number of authors: 7
  Paper title: Gliding Vertex on the Horizontal Bounding Box for Multi-Oriented Object
    Detection
  Publication Date: 2020-02-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Comparison With Other Methods on DOTA
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Comparison With Some State-of-the-Art Methods
      on HRSC2016
  Table 3 caption:
    table_text: TABLE 3 Ablation Study on Different Thresholds t r tr of Obliquity
      Factor r r
  Table 4 caption:
    table_text: TABLE 4 Quantitative Comparison With Other Methods on MSRA-TD500 [37]
  Table 5 caption:
    table_text: TABLE 5 Quantitative Comparison With Other Methods on RCTW-17 [39]
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2974745
- Affiliation of the first author: college of computer, national university of defense
    technology, changsha, hunan, china
  Affiliation of the last author: college of computer, national university of defense
    technology, changsha, hunan, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Efficient_and_Effective_Regularized_Incomplete_MultiView_Clustering\figure_1.jpg
  Figure 1 caption: "ACC and NMI comparison with the variation of missing ratios on\
    \ Flower17 and Flower102 datasets. For each given missing ratio, the \u201Cincomplete\
    \ patterns\u201D are randomly generated for 10 times and their averaged results\
    \ are reported. The Purity and Rand Index comparison are provided in the appendix\
    \ due to space limit."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Efficient_and_Effective_Regularized_Incomplete_MultiView_Clustering\figure_2.jpg
  Figure 2 caption: "ACC and NMI comparison with the variation of missing ratios on\
    \ UCI Digital dataset. For each given missing ratio, the \u201Cincomplete patterns\u201D\
    \ are randomly generated for 10 times and their averaged results are reported.\
    \ The Purity and Rand Index comparison are provided in the appendix due to space\
    \ limit."
  Figure 3 Link: articels_figures_by_rev_year\2020\Efficient_and_Effective_Regularized_Incomplete_MultiView_Clustering\figure_3.jpg
  Figure 3 caption: "ACC and NMI comparison with the variation of missing ratios on\
    \ CCV dataset. For each given missing ratio, the \u201Cincomplete patterns\u201D\
    \ are randomly generated for 10 times and their averaged results are reported.\
    \ The Purity and Rand Index comparison are provided in the appendix due to space\
    \ limit."
  Figure 4 Link: articels_figures_by_rev_year\2020\Efficient_and_Effective_Regularized_Incomplete_MultiView_Clustering\figure_4.jpg
  Figure 4 caption: "ACC and NMI comparison with the variation of missing ratios on\
    \ Caltech102-30 dataset. For each given missing ratio, the \u201Cincomplete patterns\u201D\
    \ are randomly generated for 10 times and their averaged results are reported.\
    \ The Purity and Rand Index comparison are provided in the appendix due to space\
    \ limit."
  Figure 5 Link: articels_figures_by_rev_year\2020\Efficient_and_Effective_Regularized_Incomplete_MultiView_Clustering\figure_5.jpg
  Figure 5 caption: "ACC and NMI comparison with the variation of missing ratios on\
    \ Protein Fold dataset. For each given missing ratio, the \u201Cincomplete patterns\u201D\
    \ are randomly generated for 10 times and their averaged results are reported.\
    \ The Purity and Rand Index comparison are provided in the appendix due to space\
    \ limit."
  Figure 6 Link: articels_figures_by_rev_year\2020\Efficient_and_Effective_Regularized_Incomplete_MultiView_Clustering\figure_6.jpg
  Figure 6 caption: The evolution of the learned consensus clustering matrix mathbf
    H by EE-IMVC and EE-R-IMVC with missing ratio 0.1 on all datasets. The curves
    with other missing ratios are similar and we omit them due to space limit.
  Figure 7 Link: articels_figures_by_rev_year\2020\Efficient_and_Effective_Regularized_Incomplete_MultiView_Clustering\figure_7.jpg
  Figure 7 caption: The sensitivity of EE-R-IMVC with the variation of lambda with
    missing ratio 0.1 on Flower17, Flower102, UCI-Digtal, CCV, Caltech102-30 and ProteinFold
    datasets. The results of EE-IMVC are also provided as a reference. The results
    in terms of ACC, Purity and Rand Index with other missing ratios are similar and
    omitted due to space limit.
  Figure 8 Link: articels_figures_by_rev_year\2020\Efficient_and_Effective_Regularized_Incomplete_MultiView_Clustering\figure_8.jpg
  Figure 8 caption: The objective values of EE-IMVC and EE-R-IMVC with iterations
    with missing ratio 0.1 on all datasets. The curves with other missing ratios are
    similar and we omit them due to space limit.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.92
  Name of the first author: Xinwang Liu
  Name of the last author: En Zhu
  Number of Figures: 8
  Number of Tables: 3
  Number of authors: 8
  Paper title: Efficient and Effective Regularized Incomplete Multi-View Clustering
  Publication Date: 2020-02-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Datasets Used in Our Experiments
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Aggregated ACC, NMI, Purity and Rand Index Comparison (mean\
      \ \xB1 \xB1std) of Different Clustering Algorithms on all Benchmark Datasets"
  Table 3 caption:
    table_text: "TABLE 3 ACC, NMI, Purity and Rand Index Comparison (Mean \xB1 \xB1\
      std) of Different Clustering Algorithms on all Benchmark Datasets (With Missing\
      \ Ratio=0.1)"
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2974828
- Affiliation of the first author: tianjin key lab of machine learning, college of
    intelligence and computing, tianjin university, tianjin, china
  Affiliation of the last author: school of information and communication engineering,
    dalian university of technology, liaoning, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Deep_CNNs_Meet_Global_Covariance_Pooling_Better_Representation_and_Generalizatio\figure_1.jpg
  Figure 1 caption: 'Overview of global Matrix Power Normalized COVariance (MPN-COV)
    Pooling networks. The MPN-COV block, inspired by vN-MLE [17] for robust covariance
    estimation and Power-Euclidean metric [18] for usage of geometry of covariance
    matrices, is inserted after the last convolution layer of a backbone model for
    summarizing the second-order statistics as inputs of classifier. We further refine
    MPN-COV from three aspects: first, in G 2 DeNet, we combine first-order statistics
    with covariance matrix by using a Gaussian, identified as matrix square root of
    an SPD matrix, for further performance improvement; then, an iterative matrix
    square root normalization (iSQRT-COV) method is developed for fast network training
    (Speed + +); finally, compact covariance representations are presented to reduce
    model complexity (Model Complexity - -).'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Deep_CNNs_Meet_Global_Covariance_Pooling_Better_Representation_and_Generalizatio\figure_2.jpg
  Figure 2 caption: Diagram of the proposed MPN-COV block.
  Figure 3 Link: articels_figures_by_rev_year\2020\Deep_CNNs_Meet_Global_Covariance_Pooling_Better_Representation_and_Generalizatio\figure_3.jpg
  Figure 3 caption: Diagram of the proposed global Gaussian embedding block.
  Figure 4 Link: articels_figures_by_rev_year\2020\Deep_CNNs_Meet_Global_Covariance_Pooling_Better_Representation_and_Generalizatio\figure_4.jpg
  Figure 4 caption: Diagram of the proposed iSQRT-COV block.
  Figure 5 Link: articels_figures_by_rev_year\2020\Deep_CNNs_Meet_Global_Covariance_Pooling_Better_Representation_and_Generalizatio\figure_5.jpg
  Figure 5 caption: Diagram of the proposed compact strategy. We propose to combine
    progressive 1times 1 convolutions and group convolution to compress covariance
    representations, as indicated by red dotted boxes.
  Figure 6 Link: articels_figures_by_rev_year\2020\Deep_CNNs_Meet_Global_Covariance_Pooling_Better_Representation_and_Generalizatio\figure_6.jpg
  Figure 6 caption: 'Ablation studies on ImageNet: (a) Effect of alpha on MPN-COV
    with AlextNet, where Top-1 errors (1-crop prediction) are reported and the bold
    line indicates the result of original AlexNet; (b) Impact of number N of Newton-Schulz
    iterations on iSQRT-COV with AlexNet, evaluated with single-crop Top-1 error;
    (c) Images per second (FP + BP) of network training with AlexNet.'
  Figure 7 Link: articels_figures_by_rev_year\2020\Deep_CNNs_Meet_Global_Covariance_Pooling_Better_Representation_and_Generalizatio\figure_7.jpg
  Figure 7 caption: Illustration of empirical distribution of eigenvalues (left) and
    normalization functions (right).
  Figure 8 Link: articels_figures_by_rev_year\2020\Deep_CNNs_Meet_Global_Covariance_Pooling_Better_Representation_and_Generalizatio\figure_8.jpg
  Figure 8 caption: Comparison of classification error with different CNN models.
    (a) Show MPN-COV-Net versus corresponding modified first-order networks evaluated
    with single-crop Top-1Top-5 errors. (b) and (c) show MPN-COV-Net versus original
    first-order networks and versus state-of-the-arts evaluated with ten-crop Top-1
    error and Top-5 error, respectively.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.66
  Name of the first author: Qilong Wang
  Name of the last author: Peihua Li
  Number of Figures: 8
  Number of Tables: 11
  Number of authors: 5
  Paper title: 'Deep CNNs Meet Global Covariance Pooling: Better Representation and
    Generalization'
  Publication Date: 2020-02-18 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Time (ms) Taken by Matrix Decomposition (Single Precision\
      \ Arithmetic) of a 256\xD7256 256\xD7256 Covariance Matrix"
  Table 10 caption:
    table_text: TABLE 10 Classification Accuracy ( % %) of B-CNN and iSQRT-COV With
      Different Fine-Tuning Strategies on Birds (B), Aircrafts (A), Cars (C), DTD
      (D) and Indoor67 (I) Datasets
  Table 2 caption:
    table_text: TABLE 2 Top-1 Error (%, 1-Crop Prediction) of GCP With Various Normalization
      Methods With AlexNet on ImageNet
  Table 3 caption:
    table_text: TABLE 3 Impact of Post-Compensation on iSQRT-COV With ResNet-50 on
      ImageNet, Evaluated With 1-Crop Prediction
  Table 4 caption:
    table_text: TABLE 4 Error Rate (%, 1-Crop Prediction) and Extra Network Parameters
      (Params.) of iSQRT-COV Using Various Compact Covariance Representations (Repr.)
      on ImageNet
  Table 5 caption:
    table_text: TABLE 5 Error rate (%, 1-Crop Prediction) and Time of FP+BP (ms) Per
      Image of Different Covariance Pooling Methods With AlexNet on ImageNet
  Table 6 caption:
    table_text: TABLE 6 Comparison of Classification Error (%, 1-Crop Prediction)
      With Local Second-Order Networks With ResNet as Backbone Models on ImageNet
  Table 7 caption:
    table_text: TABLE 7 Comparison of Classification Error (%, 10-Crop Prediction)
      on Places365 Dataset
  Table 8 caption:
    table_text: TABLE 8 Comparison of Classification Accuracy (%) With State-of-the-Art
      Methods on Fine-Grained Benchmarks
  Table 9 caption:
    table_text: TABLE 9 Comparison of Classification Accuracy ( % %) With State-of-the-Art
      Methods on DTD and Indoor67
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2974833
- Affiliation of the first author: robotics institute, carnegie mellon university,
    pittsburgh, pa, usa
  Affiliation of the last author: robotics institute, carnegie mellon university,
    pittsburgh, pa, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\SelfSupervised_MultiView_Person_Association_and_its_Applications\figure_1.jpg
  Figure 1 caption: The input to our CNN has 59-channels, consisting of the color
    image, the feature maps of the 18 anatomical keypoints and their affinity fields
    computed by CPM.
  Figure 10 Link: articels_figures_by_rev_year\2020\SelfSupervised_MultiView_Person_Association_and_its_Applications\figure_10.jpg
  Figure 10 caption: The 2D projection of the keypoints to all views corresponds well
    to the expected person anatomical keypoints and tracks people even through occlusions.
  Figure 2 Link: articels_figures_by_rev_year\2020\SelfSupervised_MultiView_Person_Association_and_its_Applications\figure_2.jpg
  Figure 2 caption: The t-SNE visualization of our descriptor for 30k images of 80
    people collected by the CMU Panoptic studio. Despite having many people with similar
    appearances, the images of the same person are clustered together.
  Figure 3 Link: articels_figures_by_rev_year\2020\SelfSupervised_MultiView_Person_Association_and_its_Applications\figure_3.jpg
  Figure 3 caption: 10-NN cross-view matching of the several people with confusing
    appearance and their cosine similarity score using the pretrained model and our
    multitask descriptor learning (MTL). Green denotes the query and red denotes incorrect
    matches. We label the query in green and wrong association in red. Our method
    retrieves more positive matches and provides easy-to-separate similarity score.
    All top three neighbors are of the same person.
  Figure 4 Link: articels_figures_by_rev_year\2020\SelfSupervised_MultiView_Person_Association_and_its_Applications\figure_4.jpg
  Figure 4 caption: t-SNE visualization of the person descriptor extracted using a
    pretrained model and our multitask learning (MTL) for sequence [C]. Except for
    images of the same tracklet within a single view, the pretrained descriptors are
    scatter. Our descriptor groups images of the same person from all views and time
    instances into cleanly separated clusters. See Fig. 7 for extra quantitative evidences.
  Figure 5 Link: articels_figures_by_rev_year\2020\SelfSupervised_MultiView_Person_Association_and_its_Applications\figure_5.jpg
  Figure 5 caption: t-SNE visualization of the person descriptor extracted using a
    pretrained model (left) and our multitask learning model (right) for sequence
    [H]. Despite being a very complex scene with high number of people, our proposed
    algorithm shows better discrimination and the same person is better grouped into
    a single cluster.
  Figure 6 Link: articels_figures_by_rev_year\2020\SelfSupervised_MultiView_Person_Association_and_its_Applications\figure_6.jpg
  Figure 6 caption: The CMC for the Chasing (left), Tagging (middle), and Halloween
    (right) scene at different stage of our algorithm. Our method outperforms the
    pretrained model at every stage.
  Figure 7 Link: articels_figures_by_rev_year\2020\SelfSupervised_MultiView_Person_Association_and_its_Applications\figure_7.jpg
  Figure 7 caption: 'The confusion matrix of the top-1 matches for the all sequences
    ([C] top left, [T] top right, [H] bottom) at different stages: pretrained model,
    multi-view bootstrapping (MB), and multitask learning (MTL). There are consistent
    improvements in accuracy as more sophisticated stage is executed.'
  Figure 8 Link: articels_figures_by_rev_year\2020\SelfSupervised_MultiView_Person_Association_and_its_Applications\figure_8.jpg
  Figure 8 caption: 1-NN matching accuracy analysis of the proposed method for different
    number of cameras, percentage of tracklet noise (two or more people grouped in
    1 tracklet), fraction of domain data required for generalization, and triplet
    mining iterations. P denotes the pretrained model. Please refer to the text for
    the details.
  Figure 9 Link: articels_figures_by_rev_year\2020\SelfSupervised_MultiView_Person_Association_and_its_Applications\figure_9.jpg
  Figure 9 caption: 3D tracking for [C] (top) and [T] (bottom) for the entire event.
    Owing to an accurate association, our method gives smooth and clean trajectories
    despite strong occlusion, similar people appearance, and complex motion pattern.
    Please refer to the supplementary material, available online, for visualization
    of the comparison with the baseline where obvious tracking artifacts occur.
  First author gender probability: 0.81
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Minh Vo
  Name of the last author: Srinivasa G. Narasimhan
  Number of Figures: 12
  Number of Tables: 8
  Number of authors: 6
  Paper title: Self-Supervised Multi-View Person Association and its Applications
  Publication Date: 2020-02-18 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Three New Group Activity Tracking Datasets Scenes: Chasing
      [C] (Left), Tagging [T] (Middle), and Halloween [H] (Right)'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Structure of Our CNN Model for Person ReID
  Table 3 caption:
    table_text: TABLE 3 Ablative Analysis of the Pose Heatmaps for the Top-1 Accuracy
  Table 4 caption:
    table_text: TABLE 4 Analysis of the Clustering Algorithms by the Number of Clusters
      C, ARI Measure and Clustering Accuracy
  Table 5 caption:
    table_text: TABLE 5 3D Human-Aware Tracking Cost Functions
  Table 6 caption:
    table_text: TABLE 6 Comparison Between Per-Frame 3D Skeleton Reconstruction Using
      Ground Truth Association and Human Aware Tracking
  Table 7 caption:
    table_text: TABLE 7 Analysis of Multi-View 3D Tracking
  Table 8 caption:
    table_text: TABLE 8 MOT-Accuracy Comparison for Different Threshold Radius r r
      on WILDTRACK [14]
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2974726
