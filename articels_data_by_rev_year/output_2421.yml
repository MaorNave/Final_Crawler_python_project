- Affiliation of the first author: computer science and a.i. department, university
    of granada, granada, spain
  Affiliation of the last author: computer science and a.i. department, university
    of granada, granada, spain
  Figure 1 Link: articels_figures_by_rev_year\2021\Reducing_Data_Complexity_Using_Autoencoders_With_ClassInformed_Loss_Functions\figure_1.jpg
  Figure 1 caption: Different situations relating to class complexity. The graph on
    the left shows separable classes, the middle one is an example where classes are
    not separable and the one on the right shows separable classes with complex boundaries
    (small disjuncts).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Reducing_Data_Complexity_Using_Autoencoders_With_ClassInformed_Loss_Functions\figure_2.jpg
  Figure 2 caption: "The essential structure of an AE implemented as a fully connected\
    \ feed-forward neural network, composed of an encoder f and a decoder g . The\
    \ training loss of this model is measured as the distance d between the input\
    \ x and its reconstruction x \u2032 =(g\u2218f)(x) ."
  Figure 3 Link: articels_figures_by_rev_year\2021\Reducing_Data_Complexity_Using_Autoencoders_With_ClassInformed_Loss_Functions\figure_3.jpg
  Figure 3 caption: Schematic illustration of the Scorer model. The average Fisher
    discriminant ratio of the encoded class distributions contributes to the training
    loss.
  Figure 4 Link: articels_figures_by_rev_year\2021\Reducing_Data_Complexity_Using_Autoencoders_With_ClassInformed_Loss_Functions\figure_4.jpg
  Figure 4 caption: Schematic illustration of the Skaler model. The KLD between the
    positive and the negative-class encodings contributes negatively to the training
    loss.
  Figure 5 Link: articels_figures_by_rev_year\2021\Reducing_Data_Complexity_Using_Autoencoders_With_ClassInformed_Loss_Functions\figure_5.jpg
  Figure 5 caption: Schematic illustration of the Slicer model.
  Figure 6 Link: articels_figures_by_rev_year\2021\Reducing_Data_Complexity_Using_Autoencoders_With_ClassInformed_Loss_Functions\figure_6.jpg
  Figure 6 caption: Critical distance plot for all results. Horizontal lines join
    methods where significant differences were not detected.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: David Charte
  Name of the last author: Francisco Herrera
  Number of Figures: 6
  Number of Tables: 4
  Number of authors: 3
  Paper title: Reducing Data Complexity Using Autoencoders With Class-Informed Loss
    Functions
  Publication Date: 2021-11-12 00:00:00
  Table 1 caption: TABLE 1 Brief Description of Each Method Available in the Experiments
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Brief Description of Each Evaluation Metric Used for the
    Experiments, classified According to Their Dependency on a Classifier
  Table 3 caption: TABLE 3 Enumeration of Parameters Used Throughout the Experiments
  Table 4 caption: TABLE 4 Average Ranking for Each Method in Each Evaluated Metric
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3127698
- Affiliation of the first author: john a. paulson school of engineering and applied
    sciences, harvard university, cambridge, ma, usa
  Affiliation of the last author: john a. paulson school of engineering and applied
    sciences, harvard university, cambridge, ma, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\Generalizing_Correspondence_Analysis_for_Applications_in_Machine_Learning\figure_1.jpg
  Figure 1 caption: CA on parts of the brand-personality dataset [36].
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Generalizing_Correspondence_Analysis_for_Applications_in_Machine_Learning\figure_2.jpg
  Figure 2 caption: The architecture of the PICE, consisting of two encoders F-Net
    and G-Net for X and Y , followed by a whitening process, to estimate the principal
    functions. The PIC loss is given by (12).
  Figure 3 Link: articels_figures_by_rev_year\2021\Generalizing_Correspondence_Analysis_for_Applications_in_Machine_Learning\figure_3.jpg
  Figure 3 caption: PICE recovers the Hermite polynomials, the principal functions
    in the Gaussian case.
  Figure 4 Link: articels_figures_by_rev_year\2021\Generalizing_Correspondence_Analysis_for_Applications_in_Machine_Learning\figure_4.jpg
  Figure 4 caption: 'The first factoring plane of CA on Kaggle Whats cooking dataset
    (Colored dots: recipe, dark blue: ingredient). The x -axis corresponds to f 1
    (x) and g 1 (y) , and the y -axis to f 2 (x) and g 2 (y) , where each x is a recipe
    and each y a cuisine label.'
  Figure 5 Link: articels_figures_by_rev_year\2021\Generalizing_Correspondence_Analysis_for_Applications_in_Machine_Learning\figure_5.jpg
  Figure 5 caption: The first and second factoring planes on UCI wine quality dataset.
  Figure 6 Link: articels_figures_by_rev_year\2021\Generalizing_Correspondence_Analysis_for_Applications_in_Machine_Learning\figure_6.jpg
  Figure 6 caption: The first factoring planes of samples from three classes in CIFAR-10
    dataset, including the decision boundaries between each classes and the confusing
    images. Note that the locations of the confusing images in the plane are slightly
    rearranged for visualization purposes.
  Figure 7 Link: articels_figures_by_rev_year\2021\Generalizing_Correspondence_Analysis_for_Applications_in_Machine_Learning\figure_7.jpg
  Figure 7 caption: The first 40 PICs of the two views of images.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.5
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Hsiang Hsu
  Name of the last author: Flavio P. Calmon
  Number of Figures: 7
  Number of Tables: 6
  Number of authors: 3
  Paper title: Generalizing Correspondence Analysis for Applications in Machine Learning
  Publication Date: 2021-11-12 00:00:00
  Table 1 caption: TABLE 1 PICE Reliably Approximates the Top Four PICs in the BSC
    and Gaussian Cases
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 PICE Outperforms Contingency Table-Based CA (SVD) and CCAKCCADCCA
    on Kaggle Whats Cooking Dataset to Explore Dependencies in Samples
  Table 3 caption: TABLE 3 Top Eight PICs Between Images and Captions From the PICE
    for the Flickr-30k Dataset
  Table 4 caption: TABLE 4 Image-to-Tag Task Using PICE on the Flickr-30k Dataset
  Table 5 caption: TABLE 5 Tag-to-Image Task Using PICE on the Flickr-30k Dataset
  Table 6 caption: TABLE 6 Tags-to-Image Retrieval Results on Flickr-30k Dataset With
    20 Extracted Embeddings (From DCCA) and Principal Functions (From PICE)
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3127870
- Affiliation of the first author: stony brook university, stony brook, ny, usa
  Affiliation of the last author: inria and universite cote dazur, valbonne, france
  Figure 1 Link: articels_figures_by_rev_year\2021\VPN_Rethinking_VideoPose_Embeddings_for_Understanding_Activities_of_Daily_Living\figure_1.jpg
  Figure 1 caption: 'Illustration of the challenges in Activities of Daily Living:
    fine-grained actions (top), actions with similar visual pattern (middle) and actions
    viewed from different cameras (below).'
  Figure 10 Link: articels_figures_by_rev_year\2021\VPN_Rethinking_VideoPose_Embeddings_for_Understanding_Activities_of_Daily_Living\figure_10.jpg
  Figure 10 caption: Qualitative visualization of class activation maps of RGB, VPN-F,
    VPN-A, and VPN++ using Grad-CAM [65]. The red bounding box refers to the precised
    Region of Interest relevant to classifying the action.
  Figure 2 Link: articels_figures_by_rev_year\2021\VPN_Rethinking_VideoPose_Embeddings_for_Understanding_Activities_of_Daily_Living\figure_2.jpg
  Figure 2 caption: 'Accuracy versus Time plot on Toyota Smarthome dataset for RGB
    and Pose modalities. 3D Poses are estimated using LCRNet++ [7] followed by Videopose3D
    [8]. Early fusion indicates concatenation of features at the last layer before
    prediction whereas Late fusion indicates averaging the prediction from both modalities.
    Our proposed models (marked with bounding box): VPN-F, VPN-A and VPN++ mimicking
    Pose stream, outperforms all other RGB and Pose combining strategies, while being
    significantly faster. Late fusion of the distilled models with Pose stream further
    boosts the classification accuracy, but at the price of the model efficiency.
    Note that the model with input modalities denoted by RGB (+Poses) have been trained
    with RGB and Poses but do not require Poses at inference time.'
  Figure 3 Link: articels_figures_by_rev_year\2021\VPN_Rethinking_VideoPose_Embeddings_for_Understanding_Activities_of_Daily_Living\figure_3.jpg
  Figure 3 caption: "VPN takes as input RGB images with their corresponding 3D poses.\
    \ The RGB images are processed by a visual backbone which generates a spatio-temporal\
    \ feature map ( f ). The proposed VPN takes as input the feature map ( f ) and\
    \ the 3D poses ( P ). VPN consists of two components: an attention network and\
    \ a spatial embedding (SE). The attention network further consists of a Pose Backbone\
    \ and STC (spatio-temporal Coupler). VPN computes a modulated feature map f \u2032\
    \ . This modulated feature map f \u2032 is then used for classification."
  Figure 4 Link: articels_figures_by_rev_year\2021\VPN_Rethinking_VideoPose_Embeddings_for_Understanding_Activities_of_Daily_Living\figure_4.jpg
  Figure 4 caption: "STC: Spatio-temporal Coupler to generate spatio-temporal attention\
    \ weights A from the latent pose based feature h \u2217 ."
  Figure 5 Link: articels_figures_by_rev_year\2021\VPN_Rethinking_VideoPose_Embeddings_for_Understanding_Activities_of_Daily_Living\figure_5.jpg
  Figure 5 caption: A schematic diagram of our models - VPN, VPN-F, VPN-A, and VPN++
    reflecting their variation for providing video-pose embeddings. The inputs to
    each model at training are the RGB and Poses. STC indicates the spatio-temporal
    coupler learning the attention weights. SE indicates the spatial embedding module.
  Figure 6 Link: articels_figures_by_rev_year\2021\VPN_Rethinking_VideoPose_Embeddings_for_Understanding_Activities_of_Daily_Living\figure_6.jpg
  Figure 6 caption: "(A) The positive & negative video-pose pairs (at the left) are\
    \ input to the teacher-student network. (B) VPN-F: VPN++ distillation model with\
    \ only feature-level distillation. Here, the Pose Teacher network is pre-trained\
    \ for action classification. Supervised Contrastive Distillation (SCD) is applied\
    \ between the RGB and Pose features. (C) VPN-A: VPN++ distillation model with\
    \ only attention-level distillation. Here, the teacher VPN is the video-pose network\
    \ [30] without the spatial embedding (SE). Also, A T and T A ( V i , P i ) can\
    \ be referred to as the attention weights ( A ) and modulated feature map ( f\
    \ \u2032 ) of VPN (see Fig. 3). Teacher network VPN is trained collaboratively\
    \ with the student RGB backbone."
  Figure 7 Link: articels_figures_by_rev_year\2021\VPN_Rethinking_VideoPose_Embeddings_for_Understanding_Activities_of_Daily_Living\figure_7.jpg
  Figure 7 caption: 'VPN++: The proposed distillation model when both VPN-F and VPN-A
    are integrated into a single model. The student network consists of a RGB backbone
    and a self-attention bock. At training, the model is trained in a contrastive
    manner for the feature-level distillation, and collaborative manner for the attention-level
    distillation. Note that Video-Pose attention model VPN does not have the spatial
    embedding module.'
  Figure 8 Link: articels_figures_by_rev_year\2021\VPN_Rethinking_VideoPose_Embeddings_for_Understanding_Activities_of_Daily_Living\figure_8.jpg
  Figure 8 caption: "Accuracy of VPN-F (on left) and VPN-A (on right) for different\
    \ values of \u03B1 & \u03B2 respectively on Smarthome (CS) and NTU-60 (CS) datasets."
  Figure 9 Link: articels_figures_by_rev_year\2021\VPN_Rethinking_VideoPose_Embeddings_for_Understanding_Activities_of_Daily_Living\figure_9.jpg
  Figure 9 caption: Action classification accuracy (in %) of top-10 actions (a) for
    which Pose stream outperforms RGB stream, and (b) which are mis-classified by
    both RGB and Poses.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Srijan Das
  Name of the last author: Francois Bremond
  Number of Figures: 10
  Number of Tables: 13
  Number of authors: 4
  Paper title: 'VPN++: Rethinking Video-Pose Embeddings for Understanding Activities
    of Daily Living'
  Publication Date: 2021-11-12 00:00:00
  Table 1 caption: TABLE 1 Ablation for Choice of Distillation Loss in VPN-F
  Table 10 caption: "TABLE 10 Results on N-UCLA Dataset With Cross-View V 3 1,2 V1,23\
    \ Settings (Accuracies in %); Pose \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF\
    \ \xAF \xAF Pose\xAF Indicate its Usage Only in the Training Phase"
  Table 2 caption: TABLE 2 Impact of Pose Driven Attention (VPN) Compared to RGB Based
    Non Local (NL) Attention Mechanism
  Table 3 caption: TABLE 3 Comparison of VPN-A With Other Strategies to Distill Pose
    Driven Attention
  Table 4 caption: TABLE 4 Comparison of Different Strategies to Combine VPN-F & VPN-A
  Table 5 caption: TABLE 5 Top-1 Accuracy of RGB, 3D Poses, VPN-F, VPN-A, and VPN++
    on 4 Datasets
  Table 6 caption: TABLE 6 Performance of Several Methods With Different Levels of
    Pose Quality
  Table 7 caption: "TABLE 7 Results on Smarthome Dataset With Cross-Subject (CS) and\
    \ Cross-View ( C V 2 CV2) Settings (Accuracies in %); Att Indicates Attention\
    \ Mechanism, \u2218 \u2218 Indicates That the Modality has been Used Only in Training"
  Table 8 caption: TABLE 8 Results on NTU-60 Dataset With Cross-Subject (CS) and Cross-View
    (CV) Settings (Accuracies in %)
  Table 9 caption: TABLE 9 Results on NTU-120 Dataset With Cross-Subject ( C S 1 CS1)
    and Cross-Setup ( C S 2 CS2) Settings (Accuracies in %); Att Indicates Attention
    Mechanism
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3127885
- Affiliation of the first author: adana alparslan turkes science and technology university,
    adana, turkey
  Affiliation of the last author: adana alparslan turkes science and technology university,
    adana, turkey
  Figure 1 Link: articels_figures_by_rev_year\2021\Pharmacological_NonPharmacological_Policies_and_Mutation_An_Artificial_Intellige\figure_1.jpg
  Figure 1 caption: The modified ScSpInHItIbRD-VN model architecture.
  Figure 10 Link: articels_figures_by_rev_year\2021\Pharmacological_NonPharmacological_Policies_and_Mutation_An_Artificial_Intellige\figure_10.jpg
  Figure 10 caption: Mutant virus with high infectious rate and the corresponding
    casualties (dotted red lines) and controlled casualties with the AI policies (black
    lines) under 50.000 daily vaccination. The dotted blue lines represent the casualties
    without mutation and the non-pharmacological policies.
  Figure 2 Link: articels_figures_by_rev_year\2021\Pharmacological_NonPharmacological_Policies_and_Mutation_An_Artificial_Intellige\figure_2.jpg
  Figure 2 caption: The background ScSpInHItIbRD-VN model architecture.
  Figure 3 Link: articels_figures_by_rev_year\2021\Pharmacological_NonPharmacological_Policies_and_Mutation_An_Artificial_Intellige\figure_3.jpg
  Figure 3 caption: The susceptible S c k sub-model.
  Figure 4 Link: articels_figures_by_rev_year\2021\Pharmacological_NonPharmacological_Policies_and_Mutation_An_Artificial_Intellige\figure_4.jpg
  Figure 4 caption: Priority and age specific vaccination policy for the infected
    VkIn and death VkD . Data collected from [24].
  Figure 5 Link: articels_figures_by_rev_year\2021\Pharmacological_NonPharmacological_Policies_and_Mutation_An_Artificial_Intellige\figure_5.jpg
  Figure 5 caption: The non-pharmacological policies with respect to the priority
    and age specific vaccination policy under the of 100.000 daily vaccination.
  Figure 6 Link: articels_figures_by_rev_year\2021\Pharmacological_NonPharmacological_Policies_and_Mutation_An_Artificial_Intellige\figure_6.jpg
  Figure 6 caption: Estimated future COVID-19 casualties. The blue line represents
    the 50.000 daily vaccination and dashed red line represents the 100.000 daily
    vaccination.
  Figure 7 Link: articels_figures_by_rev_year\2021\Pharmacological_NonPharmacological_Policies_and_Mutation_An_Artificial_Intellige\figure_7.jpg
  Figure 7 caption: Mean values of the casualties where 1, 2, 3, 4 represent the casualties
    when there are no curfews, there are curfews on the people with chronic diseases
    and age over 65, people under 20, and curfews on the weekends and holidays.
  Figure 8 Link: articels_figures_by_rev_year\2021\Pharmacological_NonPharmacological_Policies_and_Mutation_An_Artificial_Intellige\figure_8.jpg
  Figure 8 caption: AI learning results.
  Figure 9 Link: articels_figures_by_rev_year\2021\Pharmacological_NonPharmacological_Policies_and_Mutation_An_Artificial_Intellige\figure_9.jpg
  Figure 9 caption: Generated AI policies with the weighted death casualties (dashed
    blue) and the weighted suspicious casualties (solid red).
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Onder Tutsoy
  Name of the last author: Onder Tutsoy
  Number of Figures: 11
  Number of Tables: 4
  Number of authors: 1
  Paper title: 'Pharmacological, Non-Pharmacological Policies and Mutation: An Artificial
    Intelligence Based Multi-Dimensional Policy Making Algorithm for Controlling the
    Casualties of the Pandemic Diseases'
  Publication Date: 2021-11-12 00:00:00
  Table 1 caption: TABLE 1 Parameters of the Models
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Priority and Age Specific Vaccination Parameters
  Table 3 caption: TABLE 3 Weighting Parameters of the Non-Pharmacological Policies
  Table 4 caption: TABLE 4 Mean Error and Standard Deviation Ratios of the Estimates
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3127674
- Affiliation of the first author: department of computer science and technology,
    tnlist lab, institute for ai, center for bio-inspired computing research, tsinghua
    university, beijing, china
  Affiliation of the last author: department of computer science and technology, tnlist
    lab, institute for ai, center for bio-inspired computing research, tsinghua university,
    beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Triple_Generative_Adversarial_Networks\figure_1.jpg
  Figure 1 caption: "The left panel illustrates a representative prior work [24].\
    \ D is the initialized model in the hypothesis space H D . C \u2217 is the optimal\
    \ classifier and D \u2217 is the optimal discriminator when p g (x)=p(x) . D will\
    \ converge to a solution D ~ that depends on the trade-off between classification\
    \ and generation. The right panel illustrates our approach where C and D are optimized\
    \ in two separated hypothesis spaces H C and H D , respectively. C and D will\
    \ converge to the corresponding optimum under a nonparametric assumption [1] (See\
    \ Section 3.2 for the proof)."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Triple_Generative_Adversarial_Networks\figure_2.jpg
  Figure 2 caption: Comparison between the generated data and augmented data. The
    first row shows the data generated from Triple-GAN-V2. The second row shows the
    corresponding nearest neighbours in the training data of the same class. The last
    three rows show the randomly augmented data. The diversity of the generated data
    is higher than the augmented ones and the patterns of the two types of data are
    different.
  Figure 3 Link: articels_figures_by_rev_year\2021\Triple_Generative_Adversarial_Networks\figure_3.jpg
  Figure 3 caption: Samples of Triple-GAN-V2 in semi-supervised learning. In all sub-figures,
    each row shares the same label and each column shares the same latent variables.
  Figure 4 Link: articels_figures_by_rev_year\2021\Triple_Generative_Adversarial_Networks\figure_4.jpg
  Figure 4 caption: Conditional samples of Triple-GAN-V2 in the extremely low data
    regime. In the figures, Each row shares the same label.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.83
  Name of the first author: Chongxuan Li
  Name of the last author: Bo Zhang
  Number of Figures: 4
  Number of Tables: 5
  Number of authors: 5
  Paper title: Triple Generative Adversarial Networks
  Publication Date: 2021-11-12 00:00:00
  Table 1 caption: TABLE 1 Benchmark Results (Error Rates %) Without Data Augmentation
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Benchmark Results (Error Rates %) With Standard Data Augmentation
    Using a Similar 13-Layer CNN Classifier
  Table 3 caption: TABLE 3 Ablation Study of the Main Factors in Triple-GAN-V2
  Table 4 caption: TABLE 4 IS [26] and FID [46] on CIFAR10
  Table 5 caption: TABLE 5 Preliminary Results (Error Rates %) Using a Similar 13-layer
    CNN Classifier
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3127558
- Affiliation of the first author: department of computer science and technology,
    huaqiao university, xiamen, china
  Affiliation of the last author: concordia institute for information systems engineering
    (ciise), concordia university, montreal, qc, canada
  Figure 1 Link: articels_figures_by_rev_year\2021\Unsupervised_Grouped_Axial_Data_Modeling_via_Hierarchical_Bayesian_Nonparametric\figure_1.jpg
  Figure 1 caption: Graphical model of HPYP-Wat.
  Figure 10 Link: articels_figures_by_rev_year\2021\Unsupervised_Grouped_Axial_Data_Modeling_via_Hierarchical_Bayesian_Nonparametric\figure_10.jpg
  Figure 10 caption: Sample clustering results on the NYU-V2 data set. (a) rgb image;
    (b) depth image; (c) image normals; (d) WMM; (e) DP-Wat; (f) HDP-Wat; (g) HPYP-Wat.
    Different colors in (d), (e), (f) and (g) denote different clusters.
  Figure 2 Link: articels_figures_by_rev_year\2021\Unsupervised_Grouped_Axial_Data_Modeling_via_Hierarchical_Bayesian_Nonparametric\figure_2.jpg
  Figure 2 caption: Graphical model of HDP-Wat.
  Figure 3 Link: articels_figures_by_rev_year\2021\Unsupervised_Grouped_Axial_Data_Modeling_via_Hierarchical_Bayesian_Nonparametric\figure_3.jpg
  Figure 3 caption: The visualization of the first synthetic data set.
  Figure 4 Link: articels_figures_by_rev_year\2021\Unsupervised_Grouped_Axial_Data_Modeling_via_Hierarchical_Bayesian_Nonparametric\figure_4.jpg
  Figure 4 caption: Group information of the second synthetic data set.
  Figure 5 Link: articels_figures_by_rev_year\2021\Unsupervised_Grouped_Axial_Data_Modeling_via_Hierarchical_Bayesian_Nonparametric\figure_5.jpg
  Figure 5 caption: Performance of our models on the first synthetic data set in terms
    of clustering accuracy rate by varying the dimension from 10 to 70.
  Figure 6 Link: articels_figures_by_rev_year\2021\Unsupervised_Grouped_Axial_Data_Modeling_via_Hierarchical_Bayesian_Nonparametric\figure_6.jpg
  Figure 6 caption: Performance of our models on the second synthetic data set in
    terms of clustering accuracy rate by varying the dimension from 10 to 70.
  Figure 7 Link: articels_figures_by_rev_year\2021\Unsupervised_Grouped_Axial_Data_Modeling_via_Hierarchical_Bayesian_Nonparametric\figure_7.jpg
  Figure 7 caption: Performance of our models on the first synthetic data set in terms
    of clustering accuracy rate with different values of K and T .
  Figure 8 Link: articels_figures_by_rev_year\2021\Unsupervised_Grouped_Axial_Data_Modeling_via_Hierarchical_Bayesian_Nonparametric\figure_8.jpg
  Figure 8 caption: Performance of our models on the second synthetic data set in
    terms of clustering accuracy rate with different values of K and T .
  Figure 9 Link: articels_figures_by_rev_year\2021\Unsupervised_Grouped_Axial_Data_Modeling_via_Hierarchical_Bayesian_Nonparametric\figure_9.jpg
  Figure 9 caption: Clustering results obtained for the YMG and YS data sets by the
    proposed models. (a) The HDP-Wat model on the YMG data set; (b) The HDP-Wat model
    on the YS data set; (c) The HPYP-Wat model on the YMG data set; (d) The HPYP-Wat
    model on the YS data set. The x -axis denotes the dimension (i.e., timepoint label)
    and the y -axis indicates the log2 gene expression level.
  First author gender probability: 0.9
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Wentao Fan
  Name of the last author: Nizar Bouguila
  Number of Figures: 11
  Number of Tables: 9
  Number of authors: 3
  Paper title: Unsupervised Grouped Axial Data Modeling via Hierarchical Bayesian
    Nonparametric Models With Watson Distributions
  Publication Date: 2021-11-16 00:00:00
  Table 1 caption: TABLE 1 Initial Values of Hyperparameters of HPYP-Wat in Our Experiments
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Initial Values of Hyperparameters of HDP-Wat in Our Experiments
  Table 3 caption: TABLE 3 The Detailed Information of the First Synthetic Data
  Table 4 caption: TABLE 4 True and Estimated Parameters for the First Synthetic Data
    Set
  Table 5 caption: TABLE 5 True and Estimated Parameters for the Second Synthetic
    Data Set
  Table 6 caption: TABLE 6 Average Clustering Performance of Gene Expression Data
    by Different Methods
  Table 7 caption: "TABLE 7 Average Clustering Performance With Different Numbers\
    \ of Groups ( J\u2208[1,5] J\u2208[1,5]) Over 10 Runs by Different Hierarchical\
    \ Nonparametric Models"
  Table 8 caption: TABLE 8 Computational Run Time (in seconds) of Different Methods
    on Clustering Gene Expression Data
  Table 9 caption: TABLE 9 Clustering Results of the NYU-V2 Data Set by Different
    Methods
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3128271
- Affiliation of the first author: department of computer science, college of william
    and mary, williamsburg, va, usa
  Affiliation of the last author: department of computer science, university of illinois
    at urbana-champaign, urbana, il, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\ControlVAE_Tuning_Analytical_Properties_and_Performance_Analysis\figure_1.jpg
  Figure 1 caption: "Framework of ControlVAE. It combines a controller with the basic\
    \ VAE framework to stabilize the KL divergence to a specified value via automatically\
    \ tuning the weight \u03B2(t) in the objective."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\ControlVAE_Tuning_Analytical_Properties_and_Performance_Analysis\figure_2.jpg
  Figure 2 caption: "The block diagram of PI controller. It uses the output KL-divergence\
    \ at training step t as the feedback to the PI algorithm to compute \u03B2(t)\
    \ ."
  Figure 3 Link: articels_figures_by_rev_year\2021\ControlVAE_Tuning_Analytical_Properties_and_Performance_Analysis\figure_3.jpg
  Figure 3 caption: "(a) (b) shows the comparison of reconstruction error and \u03B2\
    (t) using 2D Shapes data over 5 random seeds. (c) shows an example about the disentangled\
    \ factors in the latent variable as the total KL-divergence increases from 0.5\
    \ to 18 for ControlVAE (KL=18). Each curve with positive KL-divergence (except\
    \ black one) represents one disentangled factor by ControlVAE."
  Figure 4 Link: articels_figures_by_rev_year\2021\ControlVAE_Tuning_Analytical_Properties_and_Performance_Analysis\figure_4.jpg
  Figure 4 caption: "Rows: latent traversals ordered by the value of KL-divergence\
    \ with the prior in a descending order. Following prior work [16], we initialize\
    \ the latent representation from a seed image, and then traverse a single latent\
    \ dimension in a range of [\u22123,3] , while keeping the remaining latent dimensions\
    \ fixed. ControlVAE and Control-FactorVAE can disentangle all the five generative\
    \ factors for 2D Shapes data, while \u03B2 -VAE entangles the scale and shape\
    \ (in 3rd row), and FactorVAE does not disentangle y position very well."
  Figure 5 Link: articels_figures_by_rev_year\2021\ControlVAE_Tuning_Analytical_Properties_and_Performance_Analysis\figure_5.jpg
  Figure 5 caption: Performance comparison for different methods on the CIFAR-10 averaged
    over 5 random seeds. Fig.(a)(b) shows that ControlVAE has a higher ELBO and lower
    reconstruction loss than the other methods given the desired KL-divergence 145.
    Fig.(c) illustrates that ControlVAE is able to stabilize the KL-divergence to
    the target value, 145, while Lagrange multiplier (LM) method has a bias so that
    it cannot precisely manipulate the output KL-divergence.
  Figure 6 Link: articels_figures_by_rev_year\2021\ControlVAE_Tuning_Analytical_Properties_and_Performance_Analysis\figure_6.jpg
  Figure 6 caption: Performance comparison for different methods on the PTB data.
    (a) shows that ControlVAE and Cyclical annealing ( 4,8 cycles) can avert KL vanishing,
    while Cost annealing still suffers from KL vanishing after 20K and 50K training
    steps. Moreover, ControlVAE can control the KL-divergence and has a lower reconstruction
    errors than the other methods in (b). (c) shows that ControlVAE has higher ELBO
    than the baselines.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Huajie Shao
  Name of the last author: Tarek Abdelzaher
  Number of Figures: 6
  Number of Tables: 6
  Number of authors: 9
  Paper title: 'ControlVAE: Tuning, Analytical Properties, and Performance Analysis'
  Publication Date: 2021-11-17 00:00:00
  Table 1 caption: TABLE 1 Comparison of RMIG and MIG for Different Methods Averaged
    Over 5 Random Seeds
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Performance Comparison for Different Methods Using the
    ELBO and FID on CIFAR-10 Data Set AveragedOver 5 Random Seeds
  Table 3 caption: TABLE 3 Performance Comparison for Different Methods Using ELBO
    and FID on CelebA Using 5 Random Seeds
  Table 4 caption: TABLE 4 Performance Comparison for Different Methods on Dialog-Generation
    Using SW Data Over 5 Random Seeds
  Table 5 caption: TABLE 5 RMIG Score for Different Target KL-Divergence Averaged
    Over 5 Random Seeds
  Table 6 caption: TABLE 6 RMIG Score for Different Step Values Averaged Over 5 Random
    Seeds
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3127323
- Affiliation of the first author: shenzhen key lab of computer vision and pattern
    recognition, siat-sensetime joint lab, shenzhen institute of advanced technology,
    chinese academy of sciences, shenzhen, china
  Affiliation of the last author: shenzhen key lab of computer vision and pattern
    recognition, siat-sensetime joint lab, shenzhen institute of advanced technology,
    chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Interactive_MultiDimension_Modulation_for_Image_Restoration\figure_1.jpg
  Figure 1 caption: "Two-dimension (2D) modulation for a corrupted image with blur\
    \ r2 + noise \u03C325 . When the blur level is fixed to r0 , we can only modulate\
    \ the denoising effect ( \u03C310\u2192\u03C350 ), which is a typical single dimension\
    \ (SD) modulation. In multi-dimension (MD) modulation, the users are allowed to\
    \ modulate both the deblurring and denoising levels."
  Figure 10 Link: articels_figures_by_rev_year\2021\Interactive_MultiDimension_Modulation_for_Image_Restoration\figure_10.jpg
  Figure 10 caption: "Three different conditional strategies: concatenating, AdaFM\
    \ and CResMD. In the left figure, we spatially expand the condition vector to\
    \ a condition map, and then concatenate the condition map with the input image\
    \ as the input of base network. The middle figure shows that the condition vector\
    \ is transformed by several fully-connected layers to generate the parameters\
    \ of AdaFM layers, which perform multiplication and addition with \u03B1 and \u03B2\
    \ on each feature map. The right figure illustrates the controlling strategy of\
    \ our proposed CResMD, while we only use one fully-connected layer to generate\
    \ the weight \u03B1 for each building block."
  Figure 2 Link: articels_figures_by_rev_year\2021\Interactive_MultiDimension_Modulation_for_Image_Restoration\figure_2.jpg
  Figure 2 caption: "The input image is with blur r2 +noise \u03C330 . In the first\
    \ row, we first remove the noise with a denoising \u03C330 model, and then use\
    \ a deblurring r2 model for further restoration. The final output is over-sharpened\
    \ compared with the joint restoration output. In the second row, we use two SD\
    \ models to gradually modulate the restoration output. The first SD model could\
    \ fairly remove the noise by modulation. However, the second SD model could hardly\
    \ change the blurring effect. While our proposed MD modulation framework is able\
    \ to achieve a satisfactory modulation output."
  Figure 3 Link: articels_figures_by_rev_year\2021\Interactive_MultiDimension_Modulation_for_Image_Restoration\figure_3.jpg
  Figure 3 caption: "Framework of CResMD, consisting of two branches: base network\
    \ and condition network. The base network deals with image restoration, while\
    \ the condition network generates the tunable weights \u03B1 i for the controllable\
    \ residual connections. The condition network contains several fully-connected\
    \ layers and accepts the normalized restoration information as input. The building\
    \ block (green) can be replaced by any existing block like residual attention\
    \ block [23] or dense block [24]."
  Figure 4 Link: articels_figures_by_rev_year\2021\Interactive_MultiDimension_Modulation_for_Image_Restoration\figure_4.jpg
  Figure 4 caption: "Different levels of restoration effects by setting different\
    \ \u03B1 on global residual connection. When \u03B1=1 , the network outputs the\
    \ restored image. To achieve identity mapping, we set \u03B1=0 to disable the\
    \ residual branch."
  Figure 5 Link: articels_figures_by_rev_year\2021\Interactive_MultiDimension_Modulation_for_Image_Restoration\figure_5.jpg
  Figure 5 caption: Beta distribution with different values of a and b . Our CResMD
    adopts a=0.5 and b=1.0 , which could sample more mild degradations than severe
    ones.
  Figure 6 Link: articels_figures_by_rev_year\2021\Interactive_MultiDimension_Modulation_for_Image_Restoration\figure_6.jpg
  Figure 6 caption: Qualitative results of 2D modulation. In each row, we only change
    one factor with other factors fixed. We arrive at the best choice in the yellow
    box. Better view in zoom and color.
  Figure 7 Link: articels_figures_by_rev_year\2021\Interactive_MultiDimension_Modulation_for_Image_Restoration\figure_7.jpg
  Figure 7 caption: Quantitative comparison with SD methods on CBSD68 data set. The
    PSNR distances are given.
  Figure 8 Link: articels_figures_by_rev_year\2021\Interactive_MultiDimension_Modulation_for_Image_Restoration\figure_8.jpg
  Figure 8 caption: Visual results achieved by utilizing estimation network. In the
    second row, the estimation network predicts the condition vector, and then use
    CResMD to generate the restoration results. The third row shows the restoration
    results based on ground truth condition vectors.
  Figure 9 Link: articels_figures_by_rev_year\2021\Interactive_MultiDimension_Modulation_for_Image_Restoration\figure_9.jpg
  Figure 9 caption: Visual results obtained from real-world images. We first modulate
    the denoising effects by changing the second element of the condition vector,
    then modulate the deblurring effects by changing the first element of the condition
    vector. The best modulation results obtained by each step in highlighted with
    read rectangle.
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.53
  Name of the first author: Jingwen He
  Name of the last author: Yu Qiao
  Number of Figures: 18
  Number of Tables: 10
  Number of authors: 4
  Paper title: Interactive Multi-Dimension Modulation for Image Restoration
  Publication Date: 2021-11-19 00:00:00
  Table 1 caption: TABLE 1 Comparison Between Stacked Two SD Models and CResMD on
    CBSD68 Dataset, PSNR(dB) is Given
  Table 10 caption: 'TABLE 10 Comparison of 3D, 4D, and 5D Experiments, Considering
    These Degradations: Anisotropic ( l 1 > l 2 l1>l2) & Isotropic ( l 1 = l 2 l1=l2)
    Blur Kernels, wo Gaussian Noise, wo JPEG Compression'
  Table 2 caption: TABLE 2 2D Experiments
  Table 3 caption: TABLE 3 3D Experiments
  Table 4 caption: TABLE 4 The Effectiveness of Estimation Network Evaluated on LIVE1
    Dataset, PSNR(dB) is Given
  Table 5 caption: TABLE 5 Comparison With State-of-the-Art Denoising Methods Evaluated
    on CBSD68 Dataset
  Table 6 caption: TABLE 6 The Effectiveness of Global Connection
  Table 7 caption: TABLE 7 2D Experiments Under Different Sampling Curves Evaluated
    on LIVE1 [28]
  Table 8 caption: TABLE 8 3D Experiments Under Different Sampling Curves Evaluated
    on LIVE1 [28]
  Table 9 caption: TABLE 9 3D Experiments on Degradations of Anisotropic ( l 1 > l
    2 l1>l2) & Isotropic ( l 1 = l 2 l1=l2) Gaussian Blur
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3129345
- Affiliation of the first author: school of information science and technology, shanghaitech
    university, shanghai, china
  Affiliation of the last author: shanghai engineering research center of intelligent
    vision and imaging, shanghaitech university, shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Future_Frame_Prediction_Network_for_Video_Anomaly_Detection\figure_1.jpg
  Figure 1 caption: An illustration of our idea. Here the normal pattern is to add
    2 while the abnormal pattern is erratic. Both self-reconstruction and prediction
    are based on U-Net-like models. The left sub-figure illustrates a phenomenon that
    the self-reconstruction model may reconstruct well for both normal and abnormal
    samples despite different motion patterns between them. The right sub-figure shows
    that the prediction model can avoid identity mapping because the network only
    predicts the normal motion pattern well, and cannot well predict the abnormal
    pattern, which is desirable for anomaly detection.
  Figure 10 Link: articels_figures_by_rev_year\2021\Future_Frame_Prediction_Network_for_Video_Anomaly_Detection\figure_10.jpg
  Figure 10 caption: We compute the average score for normal frames and that for abnormal
    frames in the testing set of the Ped1, Ped2, and CUHK Avenue datasets. Then, we
    compute the difference between these two scores to measure how abnormal data discriminates
    from normal data. A larger gap corresponds to a lower false-positive rate and
    a higher detection rate. The results show that our method consistently outperforms
    the Conv-AE in terms of the score gap between normal and abnormal events.
  Figure 2 Link: articels_figures_by_rev_year\2021\Future_Frame_Prediction_Network_for_Video_Anomaly_Detection\figure_2.jpg
  Figure 2 caption: The pipeline of our video frame prediction network. Here we adopt
    U-Net as a generator to predict the next frame. We also embed a visual relation
    module at the position with the coarsest resolution. To generate a high-quality
    image, we adopt the constraints in terms of appearance (intensity loss and gradient
    loss) and motion (optical flow loss). Here Flownet is a pre-trained network used
    to calculate optical flow. We also leverage adversarial training to discriminate
    whether the prediction is real or fake.
  Figure 3 Link: articels_figures_by_rev_year\2021\Future_Frame_Prediction_Network_for_Video_Anomaly_Detection\figure_3.jpg
  Figure 3 caption: The network architecture of our main prediction network (U-Net).
    The resolutions of input and output are the same.
  Figure 4 Link: articels_figures_by_rev_year\2021\Future_Frame_Prediction_Network_for_Video_Anomaly_Detection\figure_4.jpg
  Figure 4 caption: Some samples including normal and abnormal frames in the UCSD,
    CUHK Avenue and ShanghaiTech datasets are illustrated. Red boxes denote anomalies
    in abnormal frames.
  Figure 5 Link: articels_figures_by_rev_year\2021\Future_Frame_Prediction_Network_for_Video_Anomaly_Detection\figure_5.jpg
  Figure 5 caption: We show some sampled scenes in the JTA dataset. The first row
    is the vanilla style and the second row is made by SG-GAN [20] which transfers
    GTA V [21] style to city scapes style [22].
  Figure 6 Link: articels_figures_by_rev_year\2021\Future_Frame_Prediction_Network_for_Video_Anomaly_Detection\figure_6.jpg
  Figure 6 caption: Some predicted frames and their ground truth in normal and abnormal
    events. Here the region is a walking zone. When pedestrians walk in the area,
    the frames can be well predicted. While for some abnormal events (for example,
    a bicycle intrudes, or two men fight), the prediction is blurred and with color
    distortion. Best viewed in color.
  Figure 7 Link: articels_figures_by_rev_year\2021\Future_Frame_Prediction_Network_for_Video_Anomaly_Detection\figure_7.jpg
  Figure 7 caption: The predictions of an anomaly with fast motion on a toy dataset.
    In this dataset, normal events mean that all persons move slowly, and a sudden
    speed-up is regarded as an anomaly. MCNet [18] takes history frame difference
    as input for future prediction thus can still predict anomalies well, while ours
    only use optical flow as a regularizer, thus it will lead to a large reconstruction
    error. Besides, RC-GAN [19] introduces a retrospective cycle constraint which
    benefits to the prediction of both normal and abnormal events. Consequently, our
    method is more sensitive to anomalies.
  Figure 8 Link: articels_figures_by_rev_year\2021\Future_Frame_Prediction_Network_for_Video_Anomaly_Detection\figure_8.jpg
  Figure 8 caption: The visualization of optical flow and the predicted images on
    the Ped1 dataset. The red boxes highlight the optical flow predicted by the model
    withwithout the motion constraint. One can see that the optical flow predicted
    by the model with motion constraint is closer to its ground truth. Best viewed
    in color.
  Figure 9 Link: articels_figures_by_rev_year\2021\Future_Frame_Prediction_Network_for_Video_Anomaly_Detection\figure_9.jpg
  Figure 9 caption: The evaluation of different losses in our future frame prediction
    network in the Avenue dataset. Each column in the histogram corresponds to a method
    with different loss functions. We calculate the average scores of normal and abnormal
    events in the testing set. The gap is calculated by subtracting the abnormal score
    from the normal score.
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Weixin Luo
  Name of the last author: Shenghua Gao
  Number of Figures: 12
  Number of Tables: 5
  Number of authors: 4
  Paper title: Future Frame Prediction Network for Video Anomaly Detection
  Publication Date: 2021-11-19 00:00:00
  Table 1 caption: TABLE 1 AUC of Different Methods on the Avenue, Ped1, Ped2 and
    ShanghaiTech Datasets
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 The Gap ( \u0394 s \u0394s) and AUC of Different Prediction\
    \ Networks on the Ped1 and Ped2 Datasets"
  Table 3 caption: TABLE 3 The Ablation Study on the Position of the Visual Relation
    Module in the U-Net
  Table 4 caption: TABLE 4 The Ablation Study on the Number of Layers in the Visual
    Relation Module
  Table 5 caption: TABLE 5 AUC for Anomaly Detection of Networks WithWithout the Motion
    Constraint on the Ped1 and Ped2
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3129349
- Affiliation of the first author: department of intelligence science and technology,
    kyoto university, kyoto, japan
  Affiliation of the last author: department of intelligence science and technology,
    kyoto university, kyoto, japan
  Figure 1 Link: articels_figures_by_rev_year\2021\Invertible_Neural_BRDF_for_Object_Inverse_Rendering\figure_1.jpg
  Figure 1 caption: (a) Architecture of the invertible neural BRDF model. The base
    distribution ( p r in Eq. (5)) is represented with a normalizing flow [7], [47],
    which transforms the input 3D uniform distribution q(x) into a BRDF p r (y) through
    a cascade of bijective transformations. (b) We condition the parameters with a
    code z , which let us learn an embedding space of real-world BRDFs (shown in 2D
    with PCA, materials that are not part of the MERL dataset are marked with dashed
    rectangles). Similar materials are grouped together and arranged in an intuitive
    manner in this continuous latent space which can directly be used as a reflectance
    prior.
  Figure 10 Link: articels_figures_by_rev_year\2021\Invertible_Neural_BRDF_for_Object_Inverse_Rendering\figure_10.jpg
  Figure 10 caption: Results of object inverse rendering using iBRDF and its latent
    space (conditional iBRDF), and the deep illumination prior from (a) synthetic
    images and (b) real images. All results are in HDR shown with fixed exposure,
    which exaggerates subtle differences. The results show that the model successfully
    disentangles the complex interaction of reflectance and illumination and recovers
    details unattainable in past methods.
  Figure 2 Link: articels_figures_by_rev_year\2021\Invertible_Neural_BRDF_for_Object_Inverse_Rendering\figure_2.jpg
  Figure 2 caption: Network architecture of the deep illumination prior. Each downsampling
    block is composed of a convolutional layer with stride two to reduce the resolution
    by half, followed by another convolutional layer, which preserves the spatial
    resolution. To avoid the checkerboard artifacts caused by transposed convolutional
    layer, we use a convolutional layer followed by a bilinear upsampling layer for
    each upsampling step. To preserve finer details, skip connections are added.
  Figure 3 Link: articels_figures_by_rev_year\2021\Invertible_Neural_BRDF_for_Object_Inverse_Rendering\figure_3.jpg
  Figure 3 caption: Log RMSE of iBRDF and conditional iBRDF (i.e., iBRDF with learned
    latent space) for (a) 100 MERL materials and (b) 51 RGL materials. The iBRDF has
    higher accuracy than a nonparametric bivariate model. The conditional iBRDF models
    using 100%, 80%, and 60% of the leave-one-out training data achieves higher accuracy
    than other parametric models (i.e., DSBRDF and Cook-Torrance). These results show
    the expressive power and generalizability of the invertible neural BRDF model.
  Figure 4 Link: articels_figures_by_rev_year\2021\Invertible_Neural_BRDF_for_Object_Inverse_Rendering\figure_4.jpg
  Figure 4 caption: "Visualizations of ( \u03B8 h , \u03B8 d ) slices of several materials\
    \ from MERL and RGL datasets when \u03D5 d =\u03C02 . Compared to densely-sampled\
    \ MERL materials, materials from the RGL dataset are less smooth and exhibit drastically\
    \ different patterns."
  Figure 5 Link: articels_figures_by_rev_year\2021\Invertible_Neural_BRDF_for_Object_Inverse_Rendering\figure_5.jpg
  Figure 5 caption: "Interpolation in the learned latent space of conditional iBRDF.\
    \ Spheres are rendered with the \u201Cgrace\u201D environment map and the two\
    \ BRDFs on the ends are interpolated in the learned latent space with affine combinations\
    \ of their corresponding embedding codes."
  Figure 6 Link: articels_figures_by_rev_year\2021\Invertible_Neural_BRDF_for_Object_Inverse_Rendering\figure_6.jpg
  Figure 6 caption: (a) Log RMSE of estimated reflectance from single images of a
    sphere with different materials under different known natural illumination with
    iBRDF (solid curve with circle) and [58] (dashed curve). (b) Renderings of three
    samples of ground truth and estimated BRDFs. These results clearly demonstrate
    the high accuracy iBRDF can provide when estimating the full BRDF from partial
    angular observations under complex illumination.
  Figure 7 Link: articels_figures_by_rev_year\2021\Invertible_Neural_BRDF_for_Object_Inverse_Rendering\figure_7.jpg
  Figure 7 caption: Relative log-space RMSE errors of estimated illumination with
    (solid curve with circles) and without (dashed curve) deep illumination prior
    for 15 different natural illumination recovered from 100 different BRDFs. For
    most combinations of illumination and material, the RMSE when estimated with the
    deep illumination prior is lower, demonstrating the effectiveness of the prior.
  Figure 8 Link: articels_figures_by_rev_year\2021\Invertible_Neural_BRDF_for_Object_Inverse_Rendering\figure_8.jpg
  Figure 8 caption: Results of illumination estimation without and with the deep illumination
    prior, and with priors proposed by Lombardi and Nishino [2]. The illumination
    estimates using the deep illumination prior clearly shows the highest details
    that match those of the ground truth.
  Figure 9 Link: articels_figures_by_rev_year\2021\Invertible_Neural_BRDF_for_Object_Inverse_Rendering\figure_9.jpg
  Figure 9 caption: Log-space RMSE of jointly estimated reflectance and illumination
    using the conditional invertible neural BRDF and deep illumination prior for 1500
    different combinations of 100 MERL BRDFs and 15 environmentmaps. The blue curve
    in (a) is the BRDF fit of conditional iBRDF for reference. These extensive results
    demonstrate the effectiveness of our proposed models and method (see main text).
  First author gender probability: 0.83
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.8
  Name of the first author: Zhe Chen
  Name of the last author: Ko Nishino
  Number of Figures: 12
  Number of Tables: 5
  Number of authors: 3
  Paper title: Invertible Neural BRDF for Object Inverse Rendering
  Publication Date: 2021-11-22 00:00:00
  Table 1 caption: TABLE 1 Mean Log-Space RMSE of Estimated Reflectance Over 100 Materials
    Under Each Illumination
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Mean Relative Log-Space RMSE of Estimated Illumination
    Without and With the Deep Illumination Prior, and With Priors Proposed by Lombardi
    and Nishino [2]
  Table 3 caption: TABLE 3 Mean Log-Space RMSE Errors of Estimated Reflectance for
    Joint Estimation of Reflectance and Illumination Under 15 Different Illuminations
  Table 4 caption: TABLE 4 Mean Log-Space RMSE Errors of Estimated Illumination for
    Joint Estimation of Reflectance and Illumination Under 15 Different Illuminations
  Table 5 caption: TABLE 5 Mean Log RMSE and Mean DSSIM Errors of Illumination Estimates
    (Mirror) and Reflectance Estimates (nat. illum.)
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3129537
