- Affiliation of the first author: department of electrical engineering, tel-aviv
    university, tel aviv-yafo, israel
  Affiliation of the last author: department of electrical engineering, tel-aviv university,
    tel aviv-yafo, israel
  Figure 1 Link: articels_figures_by_rev_year\2017\BestBuddies_SimilarityRobust_Template_Matching_Using_Mutual_Nearest_Neighbors\figure_1.jpg
  Figure 1 caption: 'Best-Buddies Similarity (BBS) for template matching: (a) The
    template, marked in green, contains an object of interest against a background.
    (b) The object in the target image undergoes complex deformation (background clutter
    and large geometric deformation); the detection results using different similarity
    measures are marked on the image (see legend); our result is marked in blue. (c)
    The Best-Buddies Pairs (BBPs) between the template and the detected region are
    mostly found the object of interest and not on the background; each BBP is connected
    by a line and marked in a unique color.'
  Figure 10 Link: articels_figures_by_rev_year\2017\BestBuddies_SimilarityRobust_Template_Matching_Using_Mutual_Nearest_Neighbors\figure_10.jpg
  Figure 10 caption: Example results using deep features. Top, input images with annotated
    template marked in green. Middle, target images and detected bounding boxes (see
    legend); ground-truth (GT) marked in green (our results in blue). Bottom, BBS
    likelihood maps. BBS successfully match the template in all these examples.
  Figure 2 Link: articels_figures_by_rev_year\2017\BestBuddies_SimilarityRobust_Template_Matching_Using_Mutual_Nearest_Neighbors\figure_2.jpg
  Figure 2 caption: "Best-Buddies Pairs (BBPs) between 2D Gaussian signals: First\
    \ row, Signal P consists of \u201Cforeground\u201D points drawn from a normal\
    \ distribution, N( \u03BC 1 , \u03C3 1 ) , marked in blue; and \u201Cbackground\u201D\
    \ points drawn from N( \u03BC 2 , \u03C3 2 ) , marked in red. Similarly, the points\
    \ in the second signal Q are drawn from the same distribution N( \u03BC 1 , \u03C3\
    \ 1 ) , and a different background distribution N( \u03BC 3 , \u03C3 3 ) . The\
    \ color of points is for illustration only, i.e., BBS does not know which point\
    \ belongs to which distribution. Second row, only the BBPs between the two signals\
    \ which are mostly found between foreground points."
  Figure 3 Link: articels_figures_by_rev_year\2017\BestBuddies_SimilarityRobust_Template_Matching_Using_Mutual_Nearest_Neighbors\figure_3.jpg
  Figure 3 caption: 'BBS template matching results. Three toys examples are shown:
    (A) cluttered background, (B) occlusions, (C) nonrigid deformation. The template
    (first column) is detected in the target image (second column) using the BBS;
    the results using BBS are marked in a blue. The likelihood maps (third column)
    show well-localized distinct modes. The BBPs are shown in last column. See text
    for more details.'
  Figure 4 Link: articels_figures_by_rev_year\2017\BestBuddies_SimilarityRobust_Template_Matching_Using_Mutual_Nearest_Neighbors\figure_4.jpg
  Figure 4 caption: "The expectation of BBS in the 1D Gaussian case: Two point sets,\
    \ P and Q, are generated by sampling points from N(0,1) , and N(\u03BC,\u03C3\
    ) , respectively. (a) the approximated expectation of BBS(P,Q) as a function of\
    \ \u03C3 (x -axis), and \u03BC (y-axis). (b)-(c) the expectation of SSD(P,Q),\
    \ and SAD(P,Q), respectively. (d) the expectation of BBS as a function of \u03BC\
    \ plotted for different \u03C3 ."
  Figure 5 Link: articels_figures_by_rev_year\2017\BestBuddies_SimilarityRobust_Template_Matching_Using_Mutual_Nearest_Neighbors\figure_5.jpg
  Figure 5 caption: "Finding a Best-Buddy: We illustrate how the underlying density\
    \ functions affect the probability that a point p (bold red circle) has a best\
    \ buddy. (a) Points from set P (red circles) are dense but points from set Q (blue\
    \ cross) are sparse. Although q is the nearest neighbor of p in Q , p is not the\
    \ nearest neighbor of q in P ( p \u2032 is closer). (b) Points from set Q are\
    \ dense and points from set P are sparse. In this case, p and q are best buddies,\
    \ as p is the closest point to q ."
  Figure 6 Link: articels_figures_by_rev_year\2017\BestBuddies_SimilarityRobust_Template_Matching_Using_Mutual_Nearest_Neighbors\figure_6.jpg
  Figure 6 caption: 'Illustrating Lemma 1: Point sets P and Q are sampled iid from
    the two Gaussian mixtures shown in (a). The probability that a point in set P
    has a best buddy in set Q is empirically computed for different set sizes (b).
    When the size of the sets increase, the empirical probability converges to the
    analytical solution in Lemma 1 (dashed black line).'
  Figure 7 Link: articels_figures_by_rev_year\2017\BestBuddies_SimilarityRobust_Template_Matching_Using_Mutual_Nearest_Neighbors\figure_7.jpg
  Figure 7 caption: 'BBS results on real data: (a) the templates are marked in green
    over the input images. (b) the target images marked with the detection results
    of 6 different methods (see text for more details). BBS results are marked in
    blue. (c)-(e) the resulting likelihood maps using BBS, EMD and NCC, respectively;
    each map is marked with the detection result, i.e., its global maxima. BBS produces
    well localized modes with respect to other methods and is able to indicate the
    correct target location in all these examples.'
  Figure 8 Link: articels_figures_by_rev_year\2017\BestBuddies_SimilarityRobust_Template_Matching_Using_Mutual_Nearest_Neighbors\figure_8.jpg
  Figure 8 caption: 'Template matching accuracy: Evaluation of method performance
    using 270 template-image pairs with df=25 . BBS outperforms competing methods
    as can be seen in ROC curves showing fraction of examples with overlap greater
    than threshold values in [0,1]. Top: only best mode is considered. Bottom: best
    out of top 3 modes is taken. Left: Color features. Right: Deep features. Mean-average-precision
    (mAP) values taken as area-under-curve are shown in the legend. Best viewed in
    color.'
  Figure 9 Link: articels_figures_by_rev_year\2017\BestBuddies_SimilarityRobust_Template_Matching_Using_Mutual_Nearest_Neighbors\figure_9.jpg
  Figure 9 caption: Example results using color features. Top, input images with annotated
    template marked in green. Middle, target images and detected bounding boxes (see
    legend); ground-truth (GT) marked in green (our results in blue). Bottom, BBS
    likelihood maps. BBS successfully match the template in all these examples.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.72
  Name of the first author: Shaul Oron
  Name of the last author: Shai Avidan
  Number of Figures: 14
  Number of Tables: 0
  Number of authors: 5
  Paper title: "Best-Buddies Similarity\u2014Robust Template Matching Using Mutual\
    \ Nearest Neighbors"
  Publication Date: 2017-08-09 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2737424
- Affiliation of the first author: school of electrical engineering, korea advanced
    institute of science and technology, daejeon, korea
  Affiliation of the last author: school of electrical engineering, korea advanced
    institute of science and technology, daejeon, korea
  Figure 1 Link: articels_figures_by_rev_year\2017\ELDNet_An_Efficient_Deep_Learning_Architecture_for_Accurate_Saliency_Detection\figure_1.jpg
  Figure 1 caption: (a) Input images, (b) Ground truth masks, (c) Fuzzy saliency masks
    from GoogLeNet features (GoogLeNet-Conv-1-HF setting, described in Section 3.3),
    (d-f) Results of (d) MDF [16], (e) MCDL [17], and (f) our method.
  Figure 10 Link: articels_figures_by_rev_year\2017\ELDNet_An_Efficient_Deep_Learning_Architecture_for_Accurate_Saliency_Detection\figure_10.jpg
  Figure 10 caption: Comparisons of the discriminative power of different features
    and our ELD-map feature space. (a) Input images, the query superpixels are highlighted.
    (b)-(d) Sum of the three Chi-square distance maps of histograms of three channels
    in (b) RGB (c) LAB (d) HSV color space. (e) Sum of values from three channels
    of our Encoded Low level Distance map (ELD-map) (f) The coarse saliency map from
    the high level features (g) The final saliency map generated using both ELD-map
    and the high level features.
  Figure 2 Link: articels_figures_by_rev_year\2017\ELDNet_An_Efficient_Deep_Learning_Architecture_for_Accurate_Saliency_Detection\figure_2.jpg
  Figure 2 caption: Overall pipeline of our method. We compute the ELD-map from the
    initial feature distance map for each query region and concatenate the coarse
    saliency map generated from the high-level features of the Inception4eoutput layer
    of the GoogLeNet model.
  Figure 3 Link: articels_figures_by_rev_year\2017\ELDNet_An_Efficient_Deep_Learning_Architecture_for_Accurate_Saliency_Detection\figure_3.jpg
  Figure 3 caption: "Visualization of the construction of the initial low-level feature\
    \ distance map. (a) SLIC [38] segments and uniformly divided grid cells are generated.\
    \ (b) Initial feature descriptors are calculated as described in Table 1. (c)\
    \ Using the features descriptors, we construct an N 1 \xD7 N 2 \xD7K feature distance\
    \ map. The computed features and distances are summarized in Tables 1 and 2."
  Figure 4 Link: articels_figures_by_rev_year\2017\ELDNet_An_Efficient_Deep_Learning_Architecture_for_Accurate_Saliency_Detection\figure_4.jpg
  Figure 4 caption: Visualization of training and validation loss of models with different
    initial features and kernel sizes. The dashed lines represents the loss of training
    data and the solid lines the loss of validation data. The model utilizes low-level
    features only(ELD setting, described in Section 3.3), and MSRA-B [28] is used
    for both training and validation datasets.
  Figure 5 Link: articels_figures_by_rev_year\2017\ELDNet_An_Efficient_Deep_Learning_Architecture_for_Accurate_Saliency_Detection\figure_5.jpg
  Figure 5 caption: Visualization of the high-level feature map prior to concatenation
    with ELD-map. From top to bottom, row 1 shows the given images, row 2 the ground
    truth masks, and row 3 the normalized feature map of the high-level feature map.
    The map roughly models the saliency map of an entire image.
  Figure 6 Link: articels_figures_by_rev_year\2017\ELDNet_An_Efficient_Deep_Learning_Architecture_for_Accurate_Saliency_Detection\figure_6.jpg
  Figure 6 caption: Visual comparisons of the quality of high-level features. (a)
    Input images (b) Ground truth masks (c-d) high-level feature map, HF-map in Figs.
    7a and 7c , if (c)GoogLeNet-Conv-1 (d)GoogLeNet-FC-1. (e-f) Results of (e) GoogLeNet-Conv-1-HF
    (f) GoogLeNet-FC-1-HF. The details of each setting are in Table 3.
  Figure 7 Link: articels_figures_by_rev_year\2017\ELDNet_An_Efficient_Deep_Learning_Architecture_for_Accurate_Saliency_Detection\figure_7.jpg
  Figure 7 caption: Alterations in the model used in the controlled experiments in
    Table 3 . Each figure represents a setting description with a suffix (a) Conv-1,
    (b) Conv-100, (c) FC-1, (d) FC-100. Conv and FC describe a type of layer used
    after feature integration. The number in the suffix represents the number of the
    channel of the high-level feature map.
  Figure 8 Link: articels_figures_by_rev_year\2017\ELDNet_An_Efficient_Deep_Learning_Architecture_for_Accurate_Saliency_Detection\figure_8.jpg
  Figure 8 caption: An alteration in the model to widen its receptive filed. The figures
    show layers prior to the feature integration of (a) VGG-Conv-1, and (b) VGG-Conv-1-RF+.
  Figure 9 Link: articels_figures_by_rev_year\2017\ELDNet_An_Efficient_Deep_Learning_Architecture_for_Accurate_Saliency_Detection\figure_9.jpg
  Figure 9 caption: Precision-recall graphs of the controlled experiments listed in
    Table 3 .
  First author gender probability: 0.83
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.7
  Name of the first author: Gayoung Lee
  Name of the last author: Junmo Kim
  Number of Figures: 14
  Number of Tables: 7
  Number of authors: 3
  Paper title: 'ELD-Net: An Efficient Deep Learning Architecture for Accurate Saliency
    Detection'
  Publication Date: 2017-08-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The List of Extracted Features of a Query Superpixel f( r
      q ) and a Grid Cell f( c ij )
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The List of Feature Distances Used to Compute the Initial
      Low-Level Feature Distance Map
  Table 3 caption:
    table_text: TABLE 3 Detailed Settings of the Controlled Experiments
  Table 4 caption:
    table_text: TABLE 4 The F-Measure Scores of Salient Region Detection Algorithms
      on Five Popular Datasets
  Table 5 caption:
    table_text: TABLE 5 The Mean Absolute Error(MAE) of Salient Region Detection Algorithms
      on Five Popular Datasets
  Table 6 caption:
    table_text: TABLE 6 Comparison of the Run Time of All Methods
  Table 7 caption:
    table_text: TABLE 7 Analysis of Run Time of ELDHF [24] and Our Method
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2737631
- Affiliation of the first author: university of chinese academy of sciences, beijing,
    china
  Affiliation of the last author: university of chinese academy of sciences, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2017\Heterogeneous_Face_Attribute_Estimation_A_Deep_MultiTask_Learning_Approach\figure_1.jpg
  Figure 1 caption: Individual face attributes have both correlation and heterogeneity.
    While attribute correlation can be utilized to improve the robustness of attribute
    estimation, attribute heterogeneity should also be tackled by designing appropriate
    prediction models.
  Figure 10 Link: articels_figures_by_rev_year\2017\Heterogeneous_Face_Attribute_Estimation_A_Deep_MultiTask_Learning_Approach\figure_10.jpg
  Figure 10 caption: Examples of (a,b) good and (c) poor age estimations by the proposed
    approach on the LAPAge2015 database. 'mn' denotes the (estimated age)(ground-truth
    apparent age) respectively, for each face image.
  Figure 2 Link: articels_figures_by_rev_year\2017\Heterogeneous_Face_Attribute_Estimation_A_Deep_MultiTask_Learning_Approach\figure_2.jpg
  Figure 2 caption: Overview of the proposed deep multi-task learning (DMTL) network
    consisting of an early-stage shared feature learning for all the attributes, followed
    by category-specific feature learning for heterogeneous attribute categories.
    We use a modified AlexNet [11] with a batch normalization (BN) layer inserted
    after each Conv. layer for shared feature learning. The subnetworks are used to
    fine-tune the shared features towards the optimal estimation of individual heterogeneous
    attributes, e.g., nominal versus ordinal and holistic versus local.
  Figure 3 Link: articels_figures_by_rev_year\2017\Heterogeneous_Face_Attribute_Estimation_A_Deep_MultiTask_Learning_Approach\figure_3.jpg
  Figure 3 caption: 'Pair-wise co-occurrence matrix of the 40 face attributes (see
    Table 2 ) provided with the CelebA database. Examples of attributes with a strong
    positive correlation include: 1 (5 O''Clock Shadow) and attribute 21 (Male), and
    attribute 19 (Heavy Makeup) and 37 (Wear Lipstick).'
  Figure 4 Link: articels_figures_by_rev_year\2017\Heterogeneous_Face_Attribute_Estimation_A_Deep_MultiTask_Learning_Approach\figure_4.jpg
  Figure 4 caption: The benefit of using MTL is that individual attribute groups which
    are not well separable from each other in the original image space could become
    separable in the feature space learned by MTL, leading to improved multi-attribute
    estimation accuracy.
  Figure 5 Link: articels_figures_by_rev_year\2017\Heterogeneous_Face_Attribute_Estimation_A_Deep_MultiTask_Learning_Approach\figure_5.jpg
  Figure 5 caption: 'Revised network input for the label information, with each attribute
    taking two fields: one for the attribute value and the other for attribute category.
    Here, ''N'' and ''O'' represent the nominal and ordinal attributes, respectively.'
  Figure 6 Link: articels_figures_by_rev_year\2017\Heterogeneous_Face_Attribute_Estimation_A_Deep_MultiTask_Learning_Approach\figure_6.jpg
  Figure 6 caption: Examples of face images with nominal and ordinal attributes from
    (a) MORPH database (total of 78K face images) [36], and (b) LFW+ database (total
    of 15K face images); face images with 40 binary attributes from (c) CelebA database
    (total of 200K face images) [23], and (d) LFWA database (total of 13K images)
    [23]; and face images from (e) ChaLearn LAPAge2015 and FotW databases (total of
    4K and 30K face images) [51]. MF and BW in (a-b) denote the gender (male, female)
    and race (black, white) information, respectively.
  Figure 7 Link: articels_figures_by_rev_year\2017\Heterogeneous_Face_Attribute_Estimation_A_Deep_MultiTask_Learning_Approach\figure_7.jpg
  Figure 7 caption: Examples of (a,b) good and (c) poor estimates for age, gender,
    and race by the proposed approach on the MORPH II and LFW+ databases. 'mnl' denotes
    the ageracegender information of each image, with 'MF' denoting malefemale, and
    'WO' denoting whiteother, respectively.
  Figure 8 Link: articels_figures_by_rev_year\2017\Heterogeneous_Face_Attribute_Estimation_A_Deep_MultiTask_Learning_Approach\figure_8.jpg
  Figure 8 caption: Examples of (a,b) good and (c) poor estimates for the 40 binary
    face attributes by the proposed approach on the CelebA databases. 'mn' denotes
    (the number of correct estimates)(total number of attributes) for each face image.
  Figure 9 Link: articels_figures_by_rev_year\2017\Heterogeneous_Face_Attribute_Estimation_A_Deep_MultiTask_Learning_Approach\figure_9.jpg
  Figure 9 caption: Attribute estimation accuracies by the proposed DMTL approach
    and the baseline single-task learning (STL) method for eight common attributes
    from the CelebA database. On average, MTL works much better than STL using networks
    with a similar depth.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.64
  Name of the first author: Hu Han
  Name of the last author: Xilin Chen
  Number of Figures: 10
  Number of Tables: 7
  Number of authors: 5
  Paper title: 'Heterogeneous Face Attribute Estimation: A Deep Multi-Task Learning
    Approach'
  Publication Date: 2017-08-10 00:00:00
  Table 1 caption:
    table_text: TABLE 1 A Summary of Published Methods on Multi-Attribute Estimation
      from a Face Image
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Summary of the 40 Face Attributes Provided with the CelebA
      Database [23]
  Table 3 caption:
    table_text: TABLE 3 Estimation Accuracies of the Three Heterogeneous Attributes
      (Age, Gender, and Race) on the MORPH II and LFW+ Databases (in %)
  Table 4 caption:
    table_text: TABLE 4 Attribute Estimation Accuracies (in %) for the 40 Binary Attributes
      (See Table 2) on the CelebA and LFWA Databases by the Proposed Approach and
      the State-of-the-Art Methods [23], [27] , [33], [34], [57]
  Table 5 caption:
    table_text: TABLE 5 Accuracies (in %) of the Proposed Approach and the State-of-the-Art
      Methods (Reported in [44]) for (a) Accessory Classification, and (b) Smile and
      Gender Classification on the FotW Datasets
  Table 6 caption:
    table_text: TABLE 6 Cross-Database Testing Accuracies (in %) of the Proposed Approach
      Using MORPH II and LFW+, as Well as CelebA and LFWA
  Table 7 caption:
    table_text: TABLE 7 Computational Cost of Different Face Attribute Estimation
      Methods
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2738004
- Affiliation of the first author: school of electrical and information engineering,
    university of sydney, camperdown, nsw, australia
  Affiliation of the last author: department of electronic engineering, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2017\Jointly_Learning_Deep_Features_Deformable_Parts_Occlusion_and_Classification_for\figure_1.jpg
  Figure 1 caption: 'Motivation of this paper to jointly learn the four key components
    in pedestrian detection: feature extraction, deformation handling models, occlusion
    handling models, and classifiers.'
  Figure 10 Link: articels_figures_by_rev_year\2017\Jointly_Learning_Deep_Features_Deformable_Parts_Occlusion_and_Classification_for\figure_10.jpg
  Figure 10 caption: Results of various designs of the deep model on the Caltech-Test
    dataset.
  Figure 2 Link: articels_figures_by_rev_year\2017\Jointly_Learning_Deep_Features_Deformable_Parts_Occlusion_and_Classification_for\figure_2.jpg
  Figure 2 caption: "Overview of our basic deep model. Image data is convolved with\
    \ 64 9\xD79\xD73 filters and averagely pooled to obtain 64 feature maps. The feature\
    \ maps are then processed by the second convolutional layer and the deformation\
    \ layer to obtain 20 part scores. Finally the visibility reasoning model is used\
    \ to estimate the detection label y ."
  Figure 3 Link: articels_figures_by_rev_year\2017\Jointly_Learning_Deep_Features_Deformable_Parts_Occlusion_and_Classification_for\figure_3.jpg
  Figure 3 caption: "Preparation of three-channel input data, i.e., 84\xD728 Y-channel\
    \ image (left), concatenation of 42\xD714 YUV channels (right), and concatenation\
    \ of the edges for 42\xD714 YUV image."
  Figure 4 Link: articels_figures_by_rev_year\2017\Jointly_Learning_Deep_Features_Deformable_Parts_Occlusion_and_Classification_for\figure_4.jpg
  Figure 4 caption: The parts model (a) and the filters (b) learned at the second
    convolutional layer. We follow [19] and visualize the filter that optimizes the
    corresponding stimuli of the neurons, which is also used in [34].
  Figure 5 Link: articels_figures_by_rev_year\2017\Jointly_Learning_Deep_Features_Deformable_Parts_Occlusion_and_Classification_for\figure_5.jpg
  Figure 5 caption: The deformation layer when deformation map is defined in (6).
    Part detection map M p and deformation maps D n,p are summed up with weights c
    n,p for n=1,2,3,4 to obtain the summed map B p . Global max pooling is then performed
    on the summed map B p to obtain the score s p for the p th part.
  Figure 6 Link: articels_figures_by_rev_year\2017\Jointly_Learning_Deep_Features_Deformable_Parts_Occlusion_and_Classification_for\figure_6.jpg
  Figure 6 caption: The visibility reasoning and detection label estimation model.
    For the i th part at the l th level, sli is the detection score and hil is the
    visibility. For example, h11 indicates the visibility of the left-head-shoulder
    part. Best viewed in color.
  Figure 7 Link: articels_figures_by_rev_year\2017\Jointly_Learning_Deep_Features_Deformable_Parts_Occlusion_and_Classification_for\figure_7.jpg
  Figure 7 caption: Joint learning of deformation and visibility using the VGG as
    baseline network and the fast R-CNN for obtaining features.
  Figure 8 Link: articels_figures_by_rev_year\2017\Jointly_Learning_Deep_Features_Deformable_Parts_Occlusion_and_Classification_for\figure_8.jpg
  Figure 8 caption: Details on the fully connected layers (fc6, fc7, fc8), convolutional
    layers (conv6 i , conv7 i , conv8 i for i=1,2,3 ) and deformation layers (def8
    1 , def8 2 , def8 3 ) after the roi-pooling layer pl5. Kernel denotes the filter
    kernel size of convolution. Pad denotes the padding used for convolution. Out
    denotes the output map size. For example, 7times 128 denotes the 128 maps of size
    7 times 7 in width and height. There are 1 full-body score and 27 part scores
    obtained.
  Figure 9 Link: articels_figures_by_rev_year\2017\Jointly_Learning_Deep_Features_Deformable_Parts_Occlusion_and_Classification_for\figure_9.jpg
  Figure 9 caption: Overall results on the Caltech-Test dataset. The original annotation
    is used for training our UDN and UDN+ and evaluation on the test data. In the
    legend, 12 percent for our UDN+ model denotes the log average miss rate in [10-2,
    100] . Similarly for other approaches.
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Wanli Ouyang
  Name of the last author: Xiaogang Wang
  Number of Figures: 17
  Number of Tables: 0
  Number of authors: 6
  Paper title: Jointly Learning Deep Features, Deformable Parts, Occlusion and Classification
    for Pedestrian Detection
  Publication Date: 2017-08-11 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2738645
- Affiliation of the first author: department of information engineering, the chinese
    university of hong kong, xianggangdao, hong kong
  Affiliation of the last author: department of information engineering, the chinese
    university of hong kong, xianggangdao, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2017\FacenessNet_Face_Detection_through_Deep_Facial_Part_Responses\figure_1.jpg
  Figure 1 caption: (a) We propose a deep convolutional network for face detection,
    which achieves high recall of faces even under severe occlusions and head pose
    variations. The key to the success of our approach is the new mechanism for scoring
    face likeliness based on deep network responses on local facial parts. (b) The
    part-level response maps (we call it 'partness' map) generated by our deep network
    given a full image without prior face detection. All these occluded faces are
    difficult to handle by conventional approaches.
  Figure 10 Link: articels_figures_by_rev_year\2017\FacenessNet_Face_Detection_through_Deep_Facial_Part_Responses\figure_10.jpg
  Figure 10 caption: We compare the performance between Faceness-Net, Faceness-Net-SR,
    and various generic objectness measures on proposing face candidate windows.
  Figure 2 Link: articels_figures_by_rev_year\2017\FacenessNet_Face_Detection_through_Deep_Facial_Part_Responses\figure_2.jpg
  Figure 2 caption: The pipeline of the baseline Faceness-Net. The first stage of
    Faceness-Net applies attribute-aware networks to generate response maps of different
    facial parts. The maps are subsequently employ to produce face proposals. The
    second stage of Faceness-Net refines candidate window generated from first stage
    using a multi-task convolutional neural network (CNN), where face classification
    and bounding box regression are jointly optimized. (Best viewed in color).
  Figure 3 Link: articels_figures_by_rev_year\2017\FacenessNet_Face_Detection_through_Deep_Facial_Part_Responses\figure_3.jpg
  Figure 3 caption: (a) The pipeline for generating face proposals. (b) Bounding box
    re-ranking by face measure (Best viewed in color).
  Figure 4 Link: articels_figures_by_rev_year\2017\FacenessNet_Face_Detection_through_Deep_Facial_Part_Responses\figure_4.jpg
  Figure 4 caption: In the baseline Faceness-Net, we adopt different attribute-aware
    networks for different facial parts. (a) This figure shows the architecture of
    an attribute-aware deep network used for discovering the responses of 'hair' component.
    Other architectures are possible. See Section 3.1 for details. (b) The response
    map from conv7, which is generated by applying element-wise averaging along the
    channels for l2 normalized feature maps, indicates the location of hair component.
    The response map is upsampled through unpooling operation [40] to obtain the final
    partness map of the same size as the input image.
  Figure 5 Link: articels_figures_by_rev_year\2017\FacenessNet_Face_Detection_through_Deep_Facial_Part_Responses\figure_5.jpg
  Figure 5 caption: The first stage of Faceness-Net-SR, a variant of the baseline
    Faceness-Net. We share representations between the different attribute-aware networks
    and reduce filters leading to improved efficiency and performance. See Section
    3.1 for details.
  Figure 6 Link: articels_figures_by_rev_year\2017\FacenessNet_Face_Detection_through_Deep_Facial_Part_Responses\figure_6.jpg
  Figure 6 caption: The partness maps obtained by using different types of supervisions
    and fine-tuning strategies. The maps in (a-e) are generated using the baseline
    Faceness-Net depicted in Fig. 2. The maps in (f) is generated using Faceness-Net-SR
    with shared representation, as illustrated in Fig. 5.
  Figure 7 Link: articels_figures_by_rev_year\2017\FacenessNet_Face_Detection_through_Deep_Facial_Part_Responses\figure_7.jpg
  Figure 7 caption: Examples of template proposal and faceness measurement. The partness
    maps of hair and eyes are shown in (a) and (b), respectively. Delta w is the faceness
    score of a window, w . (Best viewed in color).
  Figure 8 Link: articels_figures_by_rev_year\2017\FacenessNet_Face_Detection_through_Deep_Facial_Part_Responses\figure_8.jpg
  Figure 8 caption: The figure shows examples of ground truth bounding boxes of facial
    parts. Hair ground truth bounding boxes are generated from superpixel maps [49].
    Eye, nose, and mouth bounding boxes are generated from 68 ground truth facial
    landmarks [50].
  Figure 9 Link: articels_figures_by_rev_year\2017\FacenessNet_Face_Detection_through_Deep_Facial_Part_Responses\figure_9.jpg
  Figure 9 caption: Examples of cropped and uncropped images.
  First author gender probability: 0.76
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.93
  Name of the first author: Shuo Yang
  Name of the last author: Xiaoou Tang
  Number of Figures: 22
  Number of Tables: 4
  Number of authors: 4
  Paper title: 'Faceness-Net: Face Detection through Deep Facial Part Responses'
  Publication Date: 2017-08-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Facial Attributes Grouping
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Evaluating the Robustness to Unconstrained Training Input
  Table 3 caption:
    table_text: TABLE 3 The Number of Proposals Needed for Different Detection Rate
  Table 4 caption:
    table_text: TABLE 4 A Comparison of Training Data and Annotations Adopted in State-of-the-Art
      Face Detection Methods
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2738644
- Affiliation of the first author: bioinformatics institute, astar, singapore
  Affiliation of the last author: bioinformatics institute, astar, singapore
  Figure 1 Link: articels_figures_by_rev_year\2017\Transduction_on_Directed_Graphs_via_Absorbing_Random_Walks\figure_1.jpg
  Figure 1 caption: "(a) An illustrative example of a weighted digraph-based transduction\
    \ setting: Two different class labels are to be propagated from the labeled nodes\
    \ (the green and the red nodes, each for one class) to other nodes following the\
    \ graph structure. Here only a subset of the graph nodes and edges are displayed.\
    \ Quantities such as W and P can be computed accordingly. (b) As some nodes might\
    \ have zero in-degree (i.e., source nodes), a new node is further added with directed\
    \ edges to every nodes including itself, which gives P ~ . According to the c\
    \ vector, the edges toward those previous source nodes are weighted by 1, and\
    \ other edges are weighted by 1\u2212\u03B1 . (c) Its transpose, Q ~ , corresponds\
    \ to the same graph but with edge directions being reversed. This facilitates\
    \ the evaluation of affinity scores flowing from unlabeled nodes (e.g., leaf nodes)\
    \ to the labeled nodes (e.g., source nodes)."
  Figure 10 Link: articels_figures_by_rev_year\2017\Transduction_on_Directed_Graphs_via_Absorbing_Random_Walks\figure_10.jpg
  Figure 10 caption: From skeleton to digraph. (A) A exemplar skeleton map. (B) Its
    digraph G . The highlighted zone of nodes are shown as an example where the corresponding
    directed subgraph is formed. The segments marked with red and blue dots at their
    tips are the root segments, with each being regarded as the labeled node for its
    class. In other words, each class (corresponds to a vessel tree) has exactly its
    root node labeled, which corresponds to a source node in graph.
  Figure 2 Link: articels_figures_by_rev_year\2017\Transduction_on_Directed_Graphs_via_Absorbing_Random_Walks\figure_2.jpg
  Figure 2 caption: Accuracy comparisons on three UCI datasets as well as the citation
    benchmarks including CoRA, CiteseerX, and US Patent. Here the micro-averaged accuracy
    (AC) is adopted as the evaluation metric. The first column presents results on
    CoRA, CiteseerX, and US Patent, while the second column shows results on UCI datasets
    COIL20, TDT2, and 20Newsgroups. In all plots, the horizontal axis denotes the
    label ratio (percentage of labeled nodes) varying from 10 to 90 percent with 10
    percent increment. See text for details.
  Figure 3 Link: articels_figures_by_rev_year\2017\Transduction_on_Directed_Graphs_via_Absorbing_Random_Walks\figure_3.jpg
  Figure 3 caption: Comparisons of F1-Score on the multi-label problem of Google+
    and Twitter datasets. In both plots, the horizontal axis denotes the label ratio
    (percentage of labeled nodes) varying from 10 to 90 percent with 10 percent increment.
    See text for details.
  Figure 4 Link: articels_figures_by_rev_year\2017\Transduction_on_Directed_Graphs_via_Absorbing_Random_Walks\figure_4.jpg
  Figure 4 caption: Empirical time-complexity of batch update (3) versus online updates
    (7) and (8) of the fundamental matrix E . (A) and (B) show the CPU-time (log-seconds)
    of batch update versus online update for changing one row using (7), and for inserting
    deleting a new node with (8), respectively. See text for details.
  Figure 5 Link: articels_figures_by_rev_year\2017\Transduction_on_Directed_Graphs_via_Absorbing_Random_Walks\figure_5.jpg
  Figure 5 caption: Averaged absolute difference between online and batch updates.
    The red and the blue curves show the average difference for changing one row using
    (7), and for insertingdeleting a new node with (8), respectively.
  Figure 6 Link: articels_figures_by_rev_year\2017\Transduction_on_Directed_Graphs_via_Absorbing_Random_Walks\figure_6.jpg
  Figure 6 caption: Comparison with state-of-the-art methods based on undirected graphs.
  Figure 7 Link: articels_figures_by_rev_year\2017\Transduction_on_Directed_Graphs_via_Absorbing_Random_Walks\figure_7.jpg
  Figure 7 caption: "Robustness of our system versus changing \u03B1 values between\
    \ .01 and .99. (A) For CoRA and CiteseerX, the performance or our system is rather\
    \ stable (with around .001 variation) when \u03B1 is within .01 and 0.9, and start\
    \ to decrease slightly (around .01 variation) when \u03B1 value goes beyond .9.\
    \ (B) The performance remains stable in vessel tracing problems."
  Figure 8 Link: articels_figures_by_rev_year\2017\Transduction_on_Directed_Graphs_via_Absorbing_Random_Walks\figure_8.jpg
  Figure 8 caption: "Robustness of our system with respect to varying label ratio.\
    \ The error bar of each labeling ratio ( r ) displays 5 \u2013 95 percentile of\
    \ accuracy when \u03B1 values are systematically sampled between .01 and .99 with\
    \ an .01 increment. The narrow deviations from median as shown in the error bar\
    \ (usually less than 2 percent) clearly suggest that our system is rather stable\
    \ against changes of \u03B1 values."
  Figure 9 Link: articels_figures_by_rev_year\2017\Transduction_on_Directed_Graphs_via_Absorbing_Random_Walks\figure_9.jpg
  Figure 9 caption: Preprocessing of retinal blood vessel tracing. (A) An input image
    from DRIVE. (B) Binary image after segmentation. (C) Image after skeleton extraction
    and optical disk removal. The red elliptical area in (A) and (B) is the optical
    disk. The red dots in (C) are tips of the root segments identified as those directly
    contacting the optical disk. Note that each root segment induces a distinct vessel
    tree from the graph with itself being the tree root, due to the nature of blood
    flow in vessels.
  First author gender probability: 0.66
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.66
  Name of the first author: Jaydeep De
  Name of the last author: Li Cheng
  Number of Figures: 12
  Number of Tables: 2
  Number of authors: 4
  Paper title: Transduction on Directed Graphs via Absorbing Random Walks
  Publication Date: 2017-08-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Averaged CPU Time (Seconds) for All Competing Methods
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average DIADEM Scores (DS) Are Reported for the Synthetic
      Dataset, as well as the Widely Used DRIVE and STARE Testbeds
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2730871
- Affiliation of the first author: department of electrical and computer engineering,
    national university of singapore, singapore
  Affiliation of the last author: department of mathematics, national university of
    singapore, singapore
  Figure 1 Link: articels_figures_by_rev_year\2017\Simultaneous_Clustering_and_Model_Selection_Algorithm_Theory_and_Applications\figure_1.jpg
  Figure 1 caption: 'Left: A contaminated affinity matrix A with 5 clusters. Right:
    The recovered G contains 5 almost perfect blocks. Further processing by the proposed
    Boolean matrixtensor factorization algorithm will obtain perfect blocks from this
    G .'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Simultaneous_Clustering_and_Model_Selection_Algorithm_Theory_and_Applications\figure_2.jpg
  Figure 2 caption: 'Comparison on line clustering. Left: F1-measure on clustering
    the lines perturbed with increasing noise levels. Right: F1-measure on clustering
    varying number of lines.'
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.73
  Name of the first author: Zhuwen Li
  Name of the last author: Kim-Chuan Toh
  Number of Figures: 2
  Number of Tables: 4
  Number of authors: 4
  Paper title: 'Simultaneous Clustering and Model Selection: Algorithm, Theory and
    Applications'
  Publication Date: 2017-08-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Motion Segmentation (F1-Measure) on Hopkins155 Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Multiple Homography Fitting ( mean\xB1variance ) on AdelaideRMF\
      \ Dataset"
  Table 3 caption:
    table_text: "TABLE 3 Multiple Fundamental Matrix Fitting ( mean\xB1variance )\
      \ on AdelaideRMF Dataset"
  Table 4 caption:
    table_text: TABLE 4 Image Segmentation (RI and VI) on Stanford Background Dataset
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2739147
- Affiliation of the first author: skolkovo institute of science and technology, moscow,
    russia
  Affiliation of the last author: skolkovo institute of science and technology, moscow,
    russia
  Figure 1 Link: articels_figures_by_rev_year\2017\Photorealistic_Monocular_Gaze_Redirection_Using_Machine_Learning\figure_1.jpg
  Figure 1 caption: "The setting for monocular gaze redirection. Left\u2013an input\
    \ frame with the gaze directed below the camera. Middle\u2013a \u201Cground truth\u201D\
    \ frame with the gaze directed 15 degrees higher than in the input. Given an input\
    \ image and the desired change in angle and direction (\u201C15 degrees higher\u201D\
    ) our method aims to produce an image that for human perception is as close to\
    \ ground truth as possible. The result of one of the proposed systems (Section\
    \ 3.4) is shown on the right. In this particular example, the computation time\
    \ of the method is 5 ms on a single laptop core (excluding feature point localization).\
    \ Such speed makes our system suitable for real-time use in videoconferencing."
  Figure 10 Link: articels_figures_by_rev_year\2017\Photorealistic_Monocular_Gaze_Redirection_Using_Machine_Learning\figure_10.jpg
  Figure 10 caption: Ordered errors for 15circ vertical gaze redirection (see text
    for more discussion of the error metric). The best performance is shown by the
    full coarse-to-fine architecture with the lightness correction module.
  Figure 2 Link: articels_figures_by_rev_year\2017\Photorealistic_Monocular_Gaze_Redirection_Using_Machine_Learning\figure_2.jpg
  Figure 2 caption: "Gaze redirection with our neural network-based system trained\
    \ for vertical gaze redirection. The model takes an input image (middle row) and\
    \ the desired redirection angle (here varying between \u221215 and +15 degrees)\
    \ and re-synthesize the new image with the new gaze direction. Note the preservation\
    \ of fine details including specular highlights in the re-synthesized images."
  Figure 3 Link: articels_figures_by_rev_year\2017\Photorealistic_Monocular_Gaze_Redirection_Using_Machine_Learning\figure_3.jpg
  Figure 3 caption: "Left\u2013dataset collection process. Right\u2013examples of\
    \ training pairs for 15 \u2218 vertical redirection."
  Figure 4 Link: articels_figures_by_rev_year\2017\Photorealistic_Monocular_Gaze_Redirection_Using_Machine_Learning\figure_4.jpg
  Figure 4 caption: Processing of a pixel (green square) at test time in an eye flow
    tree. The pixel is passed through an eye flow tree by applying a sequence of tests
    that compare the position of the pixels w.r.t., the feature points (red crosses)
    or compare the differences in intensity with adjacent pixels (bluish squares)
    with some threshold. Once a leaf is reached, this leaf defines a matching of an
    input pixel with other pixels in the training data. The leaf stores the map of
    the compatibilities between such pixels and eye flow vectors. The system then
    takes the optimal eye flow vector (yellow square minus green square) and uses
    it to copy-paste an appropriately-displaced pixel in place of the input pixel
    into the output image. Here, a one tree version is shown for clarity, our actual
    system would sum up the compatibility scores coming from several trees before
    making the decision about the eye flow vector to use.
  Figure 5 Link: articels_figures_by_rev_year\2017\Photorealistic_Monocular_Gaze_Redirection_Using_Machine_Learning\figure_5.jpg
  Figure 5 caption: The deep warp system takes an input eye region, feature points
    (anchors) as well as a correction angle alpha and sends them to the multi-scale
    neural network (see Section 3.3.1) predicting a flow field. The flow field is
    then applied to the input image to produce an image of a redirected eye. Finally,
    the output is enhanced by processing with the lightness correction neural network
    (see Section 3.3.3).
  Figure 6 Link: articels_figures_by_rev_year\2017\Photorealistic_Monocular_Gaze_Redirection_Using_Machine_Learning\figure_6.jpg
  Figure 6 caption: 'The architecture of the two warping modules: (process 0.5times
    -scale 6(a) and process 1times -scale 6(b)) predicting and applying pixel-flow
    to the input image; 6(c) represents a fully convolutional sequence of layers inside
    warping modules.'
  Figure 7 Link: articels_figures_by_rev_year\2017\Photorealistic_Monocular_Gaze_Redirection_Using_Machine_Learning\figure_7.jpg
  Figure 7 caption: Visualization of three challenging redirection cases where the
    Lightness Correction Module helps considerably compared to the system based solely
    on coarse-to-fine warping (CFW), which is having difficulties with expanding the
    area to the left of the iris. The 'Mask' column shows the soft mask corresponding
    to parts where lightness is increased. Lightness correction fixes problems with
    dis-occluded eye-white, and also emphasizes the specular highlight increasing
    the perceived realism of the result.
  Figure 8 Link: articels_figures_by_rev_year\2017\Photorealistic_Monocular_Gaze_Redirection_Using_Machine_Learning\figure_8.jpg
  Figure 8 caption: "8(a)\u2013The architecture of the Lightness Correction Module.\
    \ The output of the lightness correction module is a weighted sum of the image\
    \ created by the warping modules and the palette (which in this paper is taken\
    \ to be a single white colour). The mixing weights predicted by the network are\
    \ passed through the softmax activation and therefore sum to one at each pixel.\
    \ The module takes the features computed by the coarse and the fine warping modules\
    \ (from Fig. 8b) as input."
  Figure 9 Link: articels_figures_by_rev_year\2017\Photorealistic_Monocular_Gaze_Redirection_Using_Machine_Learning\figure_9.jpg
  Figure 9 caption: The output flow of warping modules (Section 3.3.1) on random samples
    from a training set. The coarse-to-fine model without lightness correction was
    trained on a task of 15circ redirection upwards. This data is used to train a
    neural network-supervised regression random forest. The right down figure is a
    color pattern, explaining how the direction of the flow vector is encoded with
    the color of the pixel. The more intense the pixel is, the longer the flow vector
    in this pixel is.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Daniil Kononenko
  Name of the last author: Victor Lempitsky
  Number of Figures: 16
  Number of Tables: 1
  Number of authors: 4
  Paper title: Photorealistic Monocular Gaze Redirection Using Machine Learning
  Publication Date: 2017-08-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 User Assessment for the Photorealism of the Results for the
      Four Methods
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2737423
- Affiliation of the first author: department of mathematics and computer science,
    university of basel, basel, switzerland
  Affiliation of the last author: department of mathematics and computer science,
    university of basel, basel, switzerland
  Figure 1 Link: articels_figures_by_rev_year\2017\Gaussian_Process_Morphable_Models\figure_1.jpg
  Figure 1 caption: The 3D face surface used to illustrate the effect of different
    models.
  Figure 10 Link: articels_figures_by_rev_year\2017\Gaussian_Process_Morphable_Models\figure_10.jpg
  Figure 10 caption: Accuracy of the active shape model fitting algorithm for three
    different models.
  Figure 2 Link: articels_figures_by_rev_year\2017\Gaussian_Process_Morphable_Models\figure_2.jpg
  Figure 2 caption: "Samples using a Gaussian kernel with scale factor s=10 and bandwidth\
    \ \u03C3=100mm ."
  Figure 3 Link: articels_figures_by_rev_year\2017\Gaussian_Process_Morphable_Models\figure_3.jpg
  Figure 3 caption: Samples using a sample covariance kernel, which is learned from
    200 training faces. All the random samples look like valid faces.
  Figure 4 Link: articels_figures_by_rev_year\2017\Gaussian_Process_Morphable_Models\figure_4.jpg
  Figure 4 caption: Samples using a kernel defined on multiple scales. The random
    sample show large deformations which change the overall face shape, as well as
    local, detailed shape variations.
  Figure 5 Link: articels_figures_by_rev_year\2017\Gaussian_Process_Morphable_Models\figure_5.jpg
  Figure 5 caption: Random samples form a localized point distribution model. Whereas
    the variations look anatomically valid locally, there are no global correlations
    anymore, which makes the model more flexible.
  Figure 6 Link: articels_figures_by_rev_year\2017\Gaussian_Process_Morphable_Models\figure_6.jpg
  Figure 6 caption: Random samples from a posterior model, which has been obtained
    by taking the Gaussian process model shown in Fig. 2, and applying Gaussian process
    regression to keep the points shown in red fixed.
  Figure 7 Link: articels_figures_by_rev_year\2017\Gaussian_Process_Morphable_Models\figure_7.jpg
  Figure 7 caption: A slice through a CT image of the forearm (left) and the extracted
    bone surface from a ground-truth segmentation.
  Figure 8 Link: articels_figures_by_rev_year\2017\Gaussian_Process_Morphable_Models\figure_8.jpg
  Figure 8 caption: The effect of varying the first two modes of variation for each
    model.
  Figure 9 Link: articels_figures_by_rev_year\2017\Gaussian_Process_Morphable_Models\figure_9.jpg
  Figure 9 caption: Generalization ability measured by fitting the different models
    to all ulna surfaces.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "Marcel L\xFCthi"
  Name of the last author: Thomas Vetter
  Number of Figures: 17
  Number of Tables: 1
  Number of authors: 4
  Paper title: Gaussian Process Morphable Models
  Publication Date: 2017-08-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Specificity and Compactness Values Computed for Each of
      the Three Models
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2739743
- Affiliation of the first author: "computer aided medical procedures, technische\
    \ universit\xE4t m\xFCnchen, m\xFCnchen, germany"
  Affiliation of the last author: siemens ag, munich, germany
  Figure 1 Link: articels_figures_by_rev_year\2017\TrackingbyDetection_of_D_Human_Shapes_From_Surfaces_to_Volumes\figure_1.jpg
  Figure 1 caption: Given a reference shape (a) and input data (b), our method discovers
    reliable data-model correspondences by random forests, color-coded in (c). This
    strategy detects user-specific shapes in a frame-wise manner, resulting in better
    sustainability. In (d) the reference model (a) is deformed with correspondences
    (c) to fit the input data (b).
  Figure 10 Link: articels_figures_by_rev_year\2017\TrackingbyDetection_of_D_Human_Shapes_From_Surfaces_to_Volumes\figure_10.jpg
  Figure 10 caption: Cumulative errors on FAUST [65].
  Figure 2 Link: articels_figures_by_rev_year\2017\TrackingbyDetection_of_D_Human_Shapes_From_Surfaces_to_Volumes\figure_2.jpg
  Figure 2 caption: Centroidal Voronoi tessellations yields volumetric cells of uniform
    shape and connectivity with controllable complexity. The cells of the observed
    shape are matched discriminatively to those of the template.
  Figure 3 Link: articels_figures_by_rev_year\2017\TrackingbyDetection_of_D_Human_Shapes_From_Surfaces_to_Volumes\figure_3.jpg
  Figure 3 caption: 'The pipeline of our tracking-by-detection framework. Data-model
    associations are visualized in the same color. Upper row: Surface-based associations
    (black means no correspondence found for that vertex); bottom row: Volumetric
    associations.'
  Figure 4 Link: articels_figures_by_rev_year\2017\TrackingbyDetection_of_D_Human_Shapes_From_Surfaces_to_Volumes\figure_4.jpg
  Figure 4 caption: "The intuition of adjusting offsets. (a) original offset pair\
    \ \u03C8 . (b) \u03B7=0 results in \u03C8 without re-orientation, i.e., R=I .\
    \ (c) \u03B7=1 . \u03C8 is orientated by a rotation matrix R=[ e 1 , e 2 , e 3\
    \ ] characterized by a LCF."
  Figure 5 Link: articels_figures_by_rev_year\2017\TrackingbyDetection_of_D_Human_Shapes_From_Surfaces_to_Volumes\figure_5.jpg
  Figure 5 caption: Our method leads to quasi pose-covariant LCFs.
  Figure 6 Link: articels_figures_by_rev_year\2017\TrackingbyDetection_of_D_Human_Shapes_From_Surfaces_to_Volumes\figure_6.jpg
  Figure 6 caption: 'CVT-based feature. Left: CVT cells mathcal S sample a distance
    field, where each cell stores the distance d(mathbf xs,partial Omega) . Blue to
    red colors means from close to far. Red dot: Cell s to be described. Right: A
    toy example of our feature mathbf f , where L=5 . Shadowed and transparent layers
    have coefficients cl =-1 and 1 respectively. See text for more explanations.'
  Figure 7 Link: articels_figures_by_rev_year\2017\TrackingbyDetection_of_D_Human_Shapes_From_Surfaces_to_Volumes\figure_7.jpg
  Figure 7 caption: 'The schematic flowchart of the multi-template learning framework.
    Red arrows: Mappings gmu that associate the indices from each subject-specific
    template mathcal S mu to the common one hatmathcal S . mathbf Xmu t are the temporal
    evolutions of each template. Blue: Training; green: Prediction.'
  Figure 8 Link: articels_figures_by_rev_year\2017\TrackingbyDetection_of_D_Human_Shapes_From_Surfaces_to_Volumes\figure_8.jpg
  Figure 8 caption: '(a): Illustration of our strategy adapting skinning weights to
    CVT cells. Distances to the surface d(mathbf xs,partial Omega) are reflected in
    the normalization constants ed . (b): Result of matching two templates.'
  Figure 9 Link: articels_figures_by_rev_year\2017\TrackingbyDetection_of_D_Human_Shapes_From_Surfaces_to_Volumes\figure_9.jpg
  Figure 9 caption: Qualitative results of surface matching on FAUST. Best viewed
    in Pdf.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Chun-Hao Paul Huang
  Name of the last author: Slobodan Ilic
  Number of Figures: 15
  Number of Tables: 5
  Number of authors: 7
  Paper title: 'Tracking-by-Detection of 3D Human Shapes: From Surfaces to Volumes'
  Publication Date: 2017-08-15 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average Silhouette Overlap Error in Pixels 4 Sequences at
      Low Frame Rate
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Silhouette Pixel Error on Sequence GoalkeeperUpJump
  Table 3 caption:
    table_text: TABLE 3 Silhouette Pixel Error on Sequence BalletSeq2
  Table 4 caption:
    table_text: TABLE 4 Silhouette Pixel Error on Sequence ThomasSeq2
  Table 5 caption:
    table_text: TABLE 5 Statistics of Surface Registration Error at Marker Locations,
      on the BalletSeq2 Sequence
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2740308
