- Affiliation of the first author: "faculty of sciences, lrit\u2013cnrst urac 29,\
    \ mohammed v university in rabat, rabat, morocco"
  Affiliation of the last author: department of information engineering, university
    of florence, florence, italy
  Figure 1 Link: articels_figures_by_rev_year\2020\Dynamic_Facial_Expression_Generation_on_Hilbert_Hypersphere_With_Conditional_Was\figure_1.jpg
  Figure 1 caption: Given a neutral image, our proposed model is able to generate
    sequences of facial landmarks for different facial expressions and transform them
    to videos.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Dynamic_Facial_Expression_Generation_on_Hilbert_Hypersphere_With_Conditional_Was\figure_2.jpg
  Figure 2 caption: Overview of the proposed approach. The proposed architecture consists
    of two GANs. First, MotionGAN synthesizes facial expression motion corresponding
    to the desired expression from noise. The resulting motion encoded by the Square-Root
    Velocity Function is then applied to neutral face landmark configurations to generate
    a sequence of landmarks. Finally, TextureGAN transforms the sequence of landmarks
    to a sequence of frames corresponding to the input identity.
  Figure 3 Link: articels_figures_by_rev_year\2020\Dynamic_Facial_Expression_Generation_on_Hilbert_Hypersphere_With_Conditional_Was\figure_3.jpg
  Figure 3 caption: Overview of MotionGAN, the Conditional Wasserstein GAN used for
    motion generation.
  Figure 4 Link: articels_figures_by_rev_year\2020\Dynamic_Facial_Expression_Generation_on_Hilbert_Hypersphere_With_Conditional_Was\figure_4.jpg
  Figure 4 caption: Visualization of some facial landmark sequences. The left block
    shows landmark sequences obtained with the generated SRVF using MotionGAN applied
    to neutral landmark configurations. The right block shows some landmark sequences
    from the Oulu-CASIA dataset used in the training of MotionGAN. Each row corresponds
    to one facial expression. The original sequences contain 32 frames from which
    7 frames were selected to visualize each sequence.
  Figure 5 Link: articels_figures_by_rev_year\2020\Dynamic_Facial_Expression_Generation_on_Hilbert_Hypersphere_With_Conditional_Was\figure_5.jpg
  Figure 5 caption: 2D visualization of the SRVF data using MDS based on the geodesic
    distance in mathbb S .
  Figure 6 Link: articels_figures_by_rev_year\2020\Dynamic_Facial_Expression_Generation_on_Hilbert_Hypersphere_With_Conditional_Was\figure_6.jpg
  Figure 6 caption: Generated videos by MotionGAN and TextureGAN. The image sequences
    were randomly selected and the reported images are taken every 5 frames. Some
    examples of animated videos are shown in this link.
  Figure 7 Link: articels_figures_by_rev_year\2020\Dynamic_Facial_Expression_Generation_on_Hilbert_Hypersphere_With_Conditional_Was\figure_7.jpg
  Figure 7 caption: Qualitative comparison with the state-of-the-art on the MUG Facial
    Expression database. The samples generated by our model are randomly selected,
    while those generated with state-of-art approaches are taken from [40].
  Figure 8 Link: articels_figures_by_rev_year\2020\Dynamic_Facial_Expression_Generation_on_Hilbert_Hypersphere_With_Conditional_Was\figure_8.jpg
  Figure 8 caption: 2D visualization of the identity features space of six subjects
    chosen randomely from the MUG (top) and Oulu-CASIA (bottom) datasets. The neutral
    face images in the plot show the identity of the subjects, while the colored dots
    indicate the generated expressive ones.
  Figure 9 Link: articels_figures_by_rev_year\2020\Dynamic_Facial_Expression_Generation_on_Hilbert_Hypersphere_With_Conditional_Was\figure_9.jpg
  Figure 9 caption: Facial expression transfer. Expressions from the left column taken
    from the Oulu-CASIA dataset are transferred to faces in the middle column taken
    from the MUG dataset. Each expression was transferred to two different identities
    and the results of the transfer are shown in the right column.
  First author gender probability: 0.99
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Naima Otberdout
  Name of the last author: Stefano Berretti
  Number of Figures: 9
  Number of Tables: 4
  Number of authors: 5
  Paper title: Dynamic Facial Expression Generation on Hilbert Hypersphere With Conditional
    Wasserstein Generative Adversarial Nets
  Publication Date: 2020-06-15 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison With State-of-the-Art for Using Inception Score
      (IS), PSNR, Structural Similarity (SSIM), Average Content Distance (ACD) and
      ACD-I
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison With State-of-the-Art in Expression Recognition
      Using an LSTM on the Generated Samples
  Table 3 caption:
    table_text: TABLE 3 Ablation Study on the Oulu-CASIA Dataset
  Table 4 caption:
    table_text: TABLE 4 Expression Recognition (%) Obtained by Training an LSTM on
      the CASIA Original Training Set (Baseline), and the Original Training Set Augmented
      With an Increasing Number of Synthesized Videos
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3002500
- Affiliation of the first author: school of mechanical engineering and center for
    optical imagery analysis and learning (optimal), northwestern polytechnical university,
    shaanxi, china
  Affiliation of the last author: school of computer science and center for optical
    imagery analysis and learning (optimal), northwestern polytechnical university,
    shaanxi, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Large_Graph_Clustering_With_Simultaneous_Spectral_Embedding_and_Discretization\figure_1.jpg
  Figure 1 caption: "Illustration of how LPA works on a graph. Nodes with blue color\
    \ are u unlabeled data, each of which can be denoted by a row vector of the label\
    \ matrix Y \xAF . Yellow nodes denote the m selected nodes, each one of which\
    \ has different labels. The labels of the m labeled nodes propagate to the u unlabeled\
    \ nodes according to the probability transition matrix T , until the label matrix\
    \ Y \xAF converges."
  Figure 10 Link: articels_figures_by_rev_year\2020\Large_Graph_Clustering_With_Simultaneous_Spectral_Embedding_and_Discretization\figure_10.jpg
  Figure 10 caption: The accuracy as a function of m (number the representative nodes)
    and the parameter gamma on (a) LFR and (b) SBM, respectively. m is set to be s
    times of k (number of total classes), where s ranges from 1 to 10.
  Figure 2 Link: articels_figures_by_rev_year\2020\Large_Graph_Clustering_With_Simultaneous_Spectral_Embedding_and_Discretization\figure_2.jpg
  Figure 2 caption: Illustration of GCSED on Karate Club dataset. Black nodes denote
    representative nodes selected according to node degree. The bipartite graph on
    the left denotes the representation matrix, where numbers are sequences of individuals,
    and line width denotes link weight. In the Karate Club graph on the right, blue
    and yellow nodes are assigned to different clusters according to the clustering
    result of GCSED. The numbers close to the nodes are the true labels of the corresponding
    nodes.
  Figure 3 Link: articels_figures_by_rev_year\2020\Large_Graph_Clustering_With_Simultaneous_Spectral_Embedding_and_Discretization\figure_3.jpg
  Figure 3 caption: "Convergence of F -step. These figures show that how many iterations\
    \ the F -step needs in each main loop of algorithm 3. (a) \u223C (d) are plotted\
    \ with the datasets D1, D3, D5, D7, D11, and D12, respectively."
  Figure 4 Link: articels_figures_by_rev_year\2020\Large_Graph_Clustering_With_Simultaneous_Spectral_Embedding_and_Discretization\figure_4.jpg
  Figure 4 caption: "The convergence of the overall objective function. These figures\
    \ illustrate that the value of the objective function decreases monotonically\
    \ until it converges. (a) \u223C (d) are plotted with the datasets D1, D3, D5,\
    \ D7, D11, and D12, respectively."
  Figure 5 Link: articels_figures_by_rev_year\2020\Large_Graph_Clustering_With_Simultaneous_Spectral_Embedding_and_Discretization\figure_5.jpg
  Figure 5 caption: Comparison of runtimes of different algorithms on real datasets.
  Figure 6 Link: articels_figures_by_rev_year\2020\Large_Graph_Clustering_With_Simultaneous_Spectral_Embedding_and_Discretization\figure_6.jpg
  Figure 6 caption: 'Performance of spectral clustering algorithms on SBM. (a), (c),
    and (e) are ACC, NMI and Purity measures on the algorithms with two-block SBM
    respectively, where the parameters are set as: n ranging from 1,000 to 20,000,
    k=2 , ain=120n, aout=80n . (b), (d) and (f) are ACC, NMI and Purity measures on
    the algorithms with k-block SBM respectively, where the parameters are set as:
    n=1,000 , k ranging from 2 to 50, ain=(10k+180)n, aout=80n .'
  Figure 7 Link: articels_figures_by_rev_year\2020\Large_Graph_Clustering_With_Simultaneous_Spectral_Embedding_and_Discretization\figure_7.jpg
  Figure 7 caption: Performance of spectral clustering algorithms on LFR. (a) Runtimes
    t of different algorithms as a function of the size n of LFR benchmark graphs,
    where n is set from 1,000 to 1,000,000. The dashed lines denote reference curves,
    which are t1=(n3000)3 and t2=n7000 , respectively. (b) ACC of different algorithms
    as a function of n , where n is the same as it in (a).
  Figure 8 Link: articels_figures_by_rev_year\2020\Large_Graph_Clustering_With_Simultaneous_Spectral_Embedding_and_Discretization\figure_8.jpg
  Figure 8 caption: The accuracy as a function of the parameter gamma on real datasets.
    (a) sim (h) are plotted with D1, D2, D3, D4, D9, D10, D11, and D12, respectively.
  Figure 9 Link: articels_figures_by_rev_year\2020\Large_Graph_Clustering_With_Simultaneous_Spectral_Embedding_and_Discretization\figure_9.jpg
  Figure 9 caption: The accuracy as a function of m (number the representative nodes)
    on real datasets. m is set to be s times of k (number of total classes), where
    s ranges from 1 to 7. (a) sim (h) are plotted with D1, D2, D3, D4, D9, D10, D11,
    and D12, respectively.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Zhen Wang
  Name of the last author: Feiping Nie
  Number of Figures: 10
  Number of Tables: 6
  Number of authors: 4
  Paper title: Large Graph Clustering With Simultaneous Spectral Embedding and Discretization
  Publication Date: 2020-06-15 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Characteristics of the Data Sets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 The Average ACC ( \xB1 \xB1Standard Deviation) of Several\
      \ Methods on Real Datasets (The Best Result of Each Data is Highlighted in Bold)"
  Table 3 caption:
    table_text: "TABLE 3 The Average NMI ( \xB1 \xB1Standard Deviation) of Several\
      \ Methods on Real Datasets (The Best Result of Each Data is Highlighted in Bold)"
  Table 4 caption:
    table_text: "TABLE 4 The Average Accuracy Purity ( \xB1 \xB1Standard Deviation)\
      \ of Several Methods on Real Datasets (The Best Result of Each Data is Highlighted\
      \ in Bold)"
  Table 5 caption:
    table_text: "TABLE 5 The Average ACC ( \xB1 \xB1Standard Deviation) of Several\
      \ Methods on Real Datasets (The Best Result of Each Data is Highlighted in Bold)"
  Table 6 caption:
    table_text: TABLE 6 The Runtime (s) of Several Methods on LFR Graphs (The Shortest
      Runtime on Each Data is Highlighted in Bold)
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3002587
- Affiliation of the first author: intelligent information media lab, toyota technological
    institute (tti), nagoya, japan
  Affiliation of the last author: intelligent information media lab, toyota technological
    institute (tti), nagoya, japan
  Figure 1 Link: articels_figures_by_rev_year\2020\Deep_BackProjectiNetworks_for_Single_Image_SuperResolution\figure_1.jpg
  Figure 1 caption: "Super-resolution result on 8\xD7 enlargement. PSNR: LapSRN [13]\
    \ (15.25 dB), EDSR [30] (15.33 dB), and Ours [31] (16.63 dB)."
  Figure 10 Link: articels_figures_by_rev_year\2020\Deep_BackProjectiNetworks_for_Single_Image_SuperResolution\figure_10.jpg
  Figure 10 caption: Performance versus number of parameters for 4times enlargement
    using Set5. The horizontal axis is log-scale.
  Figure 2 Link: articels_figures_by_rev_year\2020\Deep_BackProjectiNetworks_for_Single_Image_SuperResolution\figure_2.jpg
  Figure 2 caption: Comparisons of Deep Network SR. (a) Predefined upsampling (e.g.,
    SRCNN [17], VDSR [12], DRRN [14]) commonly uses the conventional interpolation,
    such as Bicubic, to upscale LR input images before entering the network. (b) Single
    upsampling (e.g., FSRCNN [18], ESPCN [19]) propagates the LR features, then construct
    the SR image at the last step. (c) Progressive upsampling uses a Laplacian pyramid
    network to gradually predict SR images [13]. (d) Iterative up- and down-sampling
    approach is proposed by our DBPN that exploit the mutually connected up- (blue
    box) and down-sampling (gold box) units to obtain numerous HR feature maps in
    different depths.
  Figure 3 Link: articels_figures_by_rev_year\2020\Deep_BackProjectiNetworks_for_Single_Image_SuperResolution\figure_3.jpg
  Figure 3 caption: Proposed up- and down-projection units in the DBPN. These units
    produce residual e between the initial features and the reconstructed features,
    then fuse it back by summing it to the initial features.
  Figure 4 Link: articels_figures_by_rev_year\2020\Deep_BackProjectiNetworks_for_Single_Image_SuperResolution\figure_4.jpg
  Figure 4 caption: An implementation of DBPN for super-resolution which exploits
    densely connected projection unit to encourage feature reuse.
  Figure 5 Link: articels_figures_by_rev_year\2020\Deep_BackProjectiNetworks_for_Single_Image_SuperResolution\figure_5.jpg
  Figure 5 caption: Proposed up- and down-projection unit in the Dense DBPN. The feature
    maps of all preceding units (i.e., [L1, ldots, Lt-1] and [H1, ldots, Ht] in up-
    and down-projections units, respectively) are concatenated and used as inputs,
    and its own feature maps are used as inputs into all subsequent units.
  Figure 6 Link: articels_figures_by_rev_year\2020\Deep_BackProjectiNetworks_for_Single_Image_SuperResolution\figure_6.jpg
  Figure 6 caption: Recurrent DBPN with shared parameter (DBPN-R).
  Figure 7 Link: articels_figures_by_rev_year\2020\Deep_BackProjectiNetworks_for_Single_Image_SuperResolution\figure_7.jpg
  Figure 7 caption: Recurrent DBPN with transition layer (DBPN-MR).
  Figure 8 Link: articels_figures_by_rev_year\2020\Deep_BackProjectiNetworks_for_Single_Image_SuperResolution\figure_8.jpg
  Figure 8 caption: "The depth analysis of DBPNs compare to other networks (VDSR [12],\
    \ DRCN [20], DRRN [14], LapSRN [13]) on Set5 dataset for 4\xD7 enlargement."
  Figure 9 Link: articels_figures_by_rev_year\2020\Deep_BackProjectiNetworks_for_Single_Image_SuperResolution\figure_9.jpg
  Figure 9 caption: "The depth analysis of DBPN on Set5 dataset for 8\xD7 enlargement.\
    \ DBPN-S ( T=2 ), DBPN-M ( T=4 ), and DBPN-L ( T=6 )."
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Muhammad Haris
  Name of the last author: Norimichi Ukita
  Number of Figures: 16
  Number of Tables: 10
  Number of authors: 3
  Paper title: Deep Back-ProjectiNetworks for Single Image Super-Resolution
  Publication Date: 2020-06-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Model Architecture of DBPN
  Table 10 caption:
    table_text: TABLE 10 PIRM2018 Challenge Results [33]
  Table 2 caption:
    table_text: "TABLE 2 Analysis of EF Using DBPN-S on 4\xD7 4\xD7 Enlargement"
  Table 3 caption:
    table_text: "TABLE 3 Analysis of EF Module on Same Model Size (D-DBPN) on 4\xD7\
      \ 4\xD7 Enlargement"
  Table 4 caption:
    table_text: "TABLE 4 Analysis of Filter Size in the Back-Projection Stages on\
      \ 4\xD7 4\xD7 Enlargement From D-DBPN"
  Table 5 caption:
    table_text: TABLE 5 Analysis of InputOutput Color Channel Using DBPN-L
  Table 6 caption:
    table_text: "TABLE 6 Comparison of the DBPN-L and D-DBPN-L on 4\xD7 and 8\xD7\
      \ Enlargement"
  Table 7 caption:
    table_text: "TABLE 7 Quantitative Evaluation of DBPNs Variants on 4\xD7"
  Table 8 caption:
    table_text: "TABLE 8 Quantitative Evaluation of State-of-the-Art SR Algorithms:\
      \ Average PSNRSSIM for Scale Factors 2\xD7, 4\xD7, and 8\xD7"
  Table 9 caption:
    table_text: "TABLE 9 Runtime Evaluation With Input Size 64\xD764"
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3002836
- Affiliation of the first author: school of computing science, university of glasgow,
    glasgow, u.k
  Affiliation of the last author: department of computer science, university of verona,
    verona, italy
  Figure 1 Link: articels_figures_by_rev_year\2020\Infinite_Feature_Selection_A_Graphbased_Feature_Filtering_Approach\figure_1.jpg
  Figure 1 caption: Classification results on the small-sample, high-dimensional challenge.
    On the left, the average performance curves for unsupervised approaches, and on
    the right, supervised methods are shown. In all of the cases, the performance
    is measured at different numbers of selected features (on the x -axis).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Infinite_Feature_Selection_A_Graphbased_Feature_Filtering_Approach\figure_2.jpg
  Figure 2 caption: Comparison between Inf-FS U and Inf-FS S . All the supervised
    approaches are reported by solid lines and the unsupervised ones by dotted lines.
    Results are expressed in terms of classification accuracy (%).
  Figure 3 Link: articels_figures_by_rev_year\2020\Infinite_Feature_Selection_A_Graphbased_Feature_Filtering_Approach\figure_3.jpg
  Figure 3 caption: Performance achieved for the image classification task reported
    in terms of mAP (VOC 2007) and classification accuracy (Caltech-101) while selecting
    the first 5, 10, 15, 20, and 25 percent features. Solid lines individuate supervised
    feature selection approaches, dotted lines indicate unsupervised approaches.
  Figure 4 Link: articels_figures_by_rev_year\2020\Infinite_Feature_Selection_A_Graphbased_Feature_Filtering_Approach\figure_4.jpg
  Figure 4 caption: Bubble plot showing the average ranking performance ( y -axis)
    overall the datasets while increasing the number of selected features for the
    unsupervised approaches (left) and supervised ones (right). The area of each circle
    is proportional to the variance of the ranking.
  Figure 5 Link: articels_figures_by_rev_year\2020\Infinite_Feature_Selection_A_Graphbased_Feature_Filtering_Approach\figure_5.jpg
  Figure 5 caption: Varying the cardinality of the selected features on VOC 2007.
    Mean average precision instead of classification accuracy is provided here.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Giorgio Roffo
  Name of the last author: Marco Cristani
  Number of Figures: 5
  Number of Tables: 3
  Number of authors: 5
  Paper title: 'Infinite Feature Selection: A Graph-based Feature Filtering Approach'
  Publication Date: 2020-06-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Feature Selection Approaches Considered in the Experiments
      of Section 4
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Datasets and the Challenges for the Feature Selection Scenario
  Table 3 caption:
    table_text: TABLE 3 The Feature Subset Selection Results Reported in Terms of
      Accuracy (%)
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3002843
- Affiliation of the first author: national lab of radar signal processing, collaborative
    innovation center of information sensing and understanding, xidian university,
    xian, shaanxi, china
  Affiliation of the last author: mccombs school of business, university of texas
    at austin, austin, tx, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Deep_Autoencoding_Topic_Model_With_Scalable_Hybrid_Bayesian_Inference\figure_1.jpg
  Figure 1 caption: '(a-b): Inference (or encoderrecognition) and generative (or decoder)
    models for (a) DATM and (b) AVITM; (c) the generative model and a sketch of the
    upward-downward Gibbs sampler of DLDA, where mathbf Z l are augmented latent counts
    that are upward sampled in each Gibbs sampling iteration. Circles are stochastic
    variables and squares are deterministic variables. The orange and blue arrows
    denote the upward and downward information propagation respectively, and the red
    ones denote the data generation; (d) the generative model of sDPATM.'
  Figure 10 Link: articels_figures_by_rev_year\2020\Deep_Autoencoding_Topic_Model_With_Scalable_Hybrid_Bayesian_Inference\figure_10.jpg
  Figure 10 caption: The test errors change with different sizes of training dataset
    on 20News.
  Figure 2 Link: articels_figures_by_rev_year\2020\Deep_Autoencoding_Topic_Model_With_Scalable_Hybrid_Bayesian_Inference\figure_2.jpg
  Figure 2 caption: The KL divergence from the inferred Weibull distribution to the
    target gamma one as (a) textGamma(0.05,1) , (b) textGamma(0.5,1) , and (c) textGamma(5,1)
    . Subplot (d) shows the KL divergence as a function of the gamma shape parameter,
    where the gamma scale parameter is fixed at 1.
  Figure 3 Link: articels_figures_by_rev_year\2020\Deep_Autoencoding_Topic_Model_With_Scalable_Hybrid_Bayesian_Inference\figure_3.jpg
  Figure 3 caption: Plot of per-heldout-word perplexity as a function of time for
    (a) 20News, (b) RCV1, and (c) Wiki. Except for AVITM that has a single hidden
    layer with 128 topics, all the other algorithms have the same network size of
    128-64-32 for their deep generative models.
  Figure 4 Link: articels_figures_by_rev_year\2020\Deep_Autoencoding_Topic_Model_With_Scalable_Hybrid_Bayesian_Inference\figure_4.jpg
  Figure 4 caption: Learned topics on MNIST digits with a three-hidden-layer DATM
    of size 128-64-32. Shown in (a)-(c) are example topics for layers 1, 2 and 3,
    respectively, learned with a deterministic-upward-stochastic-downward encoder
    (DATM-WHAI), and shown in (d)-(f) are the ones learned with a deterministic-upward
    encoder (DATM-IWHAI).
  Figure 5 Link: articels_figures_by_rev_year\2020\Deep_Autoencoding_Topic_Model_With_Scalable_Hybrid_Bayesian_Inference\figure_5.jpg
  Figure 5 caption: Example of hierarchical topics learned from Wiki by a three-hidden-layer
    DATM-WHAI of size 128-64-32.
  Figure 6 Link: articels_figures_by_rev_year\2020\Deep_Autoencoding_Topic_Model_With_Scalable_Hybrid_Bayesian_Inference\figure_6.jpg
  Figure 6 caption: Topic-layer-adaptive learning rates inferred with a three-layer
    DATM of size 128-64-32. (a) 20News. (b) RCV1. (c) Wiki.
  Figure 7 Link: articels_figures_by_rev_year\2020\Deep_Autoencoding_Topic_Model_With_Scalable_Hybrid_Bayesian_Inference\figure_7.jpg
  Figure 7 caption: Latent space interpolations on the MNIST test set. Left and right
    columns correspond to the images generated from boldsymbolz 1(3) and boldsymbolz
    2(3) , and the others are generated from the latent representations interpolated
    linearly from boldsymbolz 1(3) to boldsymbolz 2(3) .
  Figure 8 Link: articels_figures_by_rev_year\2020\Deep_Autoencoding_Topic_Model_With_Scalable_Hybrid_Bayesian_Inference\figure_8.jpg
  Figure 8 caption: The test errors change with different layers and width on (a)
    MNIST and (b) 20News by sDATM-N.
  Figure 9 Link: articels_figures_by_rev_year\2020\Deep_Autoencoding_Topic_Model_With_Scalable_Hybrid_Bayesian_Inference\figure_9.jpg
  Figure 9 caption: The top five first-layer topics learned by DATM (the first row)
    and those by sDATM (the second row).
  First author gender probability: 0.9
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Hao Zhang
  Name of the last author: Mingyuan Zhou
  Number of Figures: 12
  Number of Tables: 4
  Number of authors: 6
  Paper title: Deep Autoencoding Topic Model With Scalable Hybrid Bayesian Inference
  Publication Date: 2020-06-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparisons of Per-Heldout-Word Perplexity and Testing Time
      (Average Seconds Per Document, With 3000 Random Samples) on Three Different
      Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparisons of Per-Heldout-Word Perplexity by Layer-Wise Training
      Strategy to Infer the Network Structure (the Same Settings With Table 1) on
      Three Different Datasets
  Table 3 caption:
    table_text: TABLE 3 Error Rates ( % %) and Testing Time (Average Seconds Per Image)
      on MNIST Dataset
  Table 4 caption:
    table_text: TABLE 4 Test Error Rates ( % %) and Testing Time (Average Seconds
      Per Document) on 20News, RCV1, and IMDB Datasets
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3003660
- Affiliation of the first author: university of california, berkeley, berkeley, ca,
    usa
  Affiliation of the last author: university of florida, gainesville, fl, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\ManifoldNet_A_Deep_Neural_Network_for_ManifoldValued_Data_With_Applications\figure_1.jpg
  Figure 1 caption: "Left: Schematic diagram of a ManifoldNet; Right: (2\xD72) ManifoldNet\
    \ conv. example."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\ManifoldNet_A_Deep_Neural_Network_for_ManifoldValued_Data_With_Applications\figure_2.jpg
  Figure 2 caption: "Schematic of equivariance and invariance where X i \u2282M ,\
    \ M is the wFM and g\u2208G is the group element."
  Figure 3 Link: articels_figures_by_rev_year\2020\ManifoldNet_A_Deep_Neural_Network_for_ManifoldValued_Data_With_Applications\figure_3.jpg
  Figure 3 caption: M1 template.
  Figure 4 Link: articels_figures_by_rev_year\2020\ManifoldNet_A_Deep_Neural_Network_for_ManifoldValued_Data_With_Applications\figure_4.jpg
  Figure 4 caption: Schematic description of autoencoder+iFME network.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.92
  Name of the first author: Rudrasis Chakraborty
  Name of the last author: Baba C. Vemuri
  Number of Figures: 4
  Number of Tables: 2
  Number of authors: 4
  Paper title: 'ManifoldNet: A Deep Neural Network for Manifold-Valued Data With Applications'
  Publication Date: 2020-06-22 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison Results on Diffusion MRI Classification
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison Results on Moving MNIST
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3003846
- Affiliation of the first author: university of adelaide, adelaide, sa, australia
  Affiliation of the last author: university of adelaide, adelaide, sa, australia
  Figure 1 Link: articels_figures_by_rev_year\2020\Index_Networks\figure_1.jpg
  Figure 1 caption: Alpha mattes of four deep models for image matting. From left
    to right, Deeplabv3+ [6], RefineNet [5], Deep Matting [7] and IndexNet (Ours).
    Bilinear upsampling tends to fail to recover subtle details, while unpooling and
    our learned upsampling operator can produce much clear mattes with good local
    contrast.
  Figure 10 Link: articels_figures_by_rev_year\2020\Index_Networks\figure_10.jpg
  Figure 10 caption: The DnCNN architecture and our modified SegNet-like DnCNN.
  Figure 2 Link: articels_figures_by_rev_year\2020\Index_Networks\figure_2.jpg
  Figure 2 caption: The index-guided encoder-decoder framework. The proposed IndexNet
    dynamically predicts indices for individual local regions, conditioned on the
    input local feature map itself. The predicted indices are further used to guide
    the downsampling in the encoding stage and the upsampling in the corresponding
    decoding stage.
  Figure 3 Link: articels_figures_by_rev_year\2020\Index_Networks\figure_3.jpg
  Figure 3 caption: Conceptual differences between holistic and depthwise index.
  Figure 4 Link: articels_figures_by_rev_year\2020\Index_Networks\figure_4.jpg
  Figure 4 caption: A taxonomy of proposed index networks.
  Figure 5 Link: articels_figures_by_rev_year\2020\Index_Networks\figure_5.jpg
  Figure 5 caption: Modelwise IndexNet versus stagewise IndexNet.
  Figure 6 Link: articels_figures_by_rev_year\2020\Index_Networks\figure_6.jpg
  Figure 6 caption: Holistic index networks. (a) A linear index network. (b) A nonlinear
    index network.
  Figure 7 Link: articels_figures_by_rev_year\2020\Index_Networks\figure_7.jpg
  Figure 7 caption: Depthwise index networks. M=1,N=1 for Modelwise O2O DINs and Shared
    Stagewise O2O DINs; M=C,N=C for Unshared Stagewise O2O DINs; and M=C,N=1 for the
    M2O DINs. The masked modules are invisible to linear networks.
  Figure 8 Link: articels_figures_by_rev_year\2020\Index_Networks\figure_8.jpg
  Figure 8 caption: Qualitative results on the Composition-1k testing set. From left
    to right, the original image, trimap, ground-truth alpha matte, Closed-form Matting
    [30], DeepMatting [30], and ours (M2O DIN with Nonlinearity+Context).
  Figure 9 Link: articels_figures_by_rev_year\2020\Index_Networks\figure_9.jpg
  Figure 9 caption: Visualization of the randomly initialized index map (left) and
    the learned index map (right) of HINs (top) and DINs (bottom). Best viewed on
    screen.
  First author gender probability: 0.95
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.99
  Name of the first author: Hao Lu
  Name of the last author: Yutong Dai
  Number of Figures: 10
  Number of Tables: 10
  Number of authors: 2
  Paper title: Index Networks
  Publication Date: 2020-06-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 A Comparison of Model Complexity of Different Index Networks
  Table 10 caption:
    table_text: TABLE 10 Performance of Hu et al. [51] on the NYUDv2 Dataset
  Table 2 caption:
    table_text: TABLE 2 Performance of Image Reconstruction on the Fashion-MNIST Dataset
  Table 3 caption:
    table_text: TABLE 3 Ablation Study of Design Choices
  Table 4 caption:
    table_text: TABLE 4 Results on the Composition-1k Testing Set
  Table 5 caption:
    table_text: TABLE 5 Ablation Study of Different Normalization Choices on Index
      Maps
  Table 6 caption:
    table_text: TABLE 6 Average PSNR (dB) and SSIM Results of Various Noise Levels
      on the BSD68 and Set12 Image Denoising Benchmarks
  Table 7 caption:
    table_text: TABLE 7 Performance on the SUN RGB-D Dataset
  Table 8 caption:
    table_text: TABLE 8 Performance on the ADE-20K Dataset
  Table 9 caption:
    table_text: TABLE 9 Performance of FastDepth [50] on the NYUDv2 Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3004474
- Affiliation of the first author: tokyo institute of technology, tokyo, japan
  Affiliation of the last author: tokyo institute of technology, tokyo, japan
  Figure 1 Link: articels_figures_by_rev_year\2020\Scalable_and_Practical_Natural_Gradient_for_LargeScale_Deep_Learning\figure_1.jpg
  Figure 1 caption: Top-1 validation accuracy versus the number of steps to converge
    (left) and versus training time (right) of ResNet-50 on ImageNet (1000 class)
    classification by related work with SGD and this work with Scalable and Practical
    NGD (SP-NGD).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Scalable_and_Practical_Natural_Gradient_for_LargeScale_Deep_Learning\figure_2.jpg
  Figure 2 caption: Illustration of Fisher information matrix approximations for feed-forward
    deep neural networks used in this work.
  Figure 3 Link: articels_figures_by_rev_year\2020\Scalable_and_Practical_Natural_Gradient_for_LargeScale_Deep_Learning\figure_3.jpg
  Figure 3 caption: "Estimate the interval (steps) \u0394 until the next step t X\
    \ next to refresh the statistics X ."
  Figure 4 Link: articels_figures_by_rev_year\2020\Scalable_and_Practical_Natural_Gradient_for_LargeScale_Deep_Learning\figure_4.jpg
  Figure 4 caption: (Left) Overview of our distributed natural gradient descent (a
    single step of training). (Right) Illustrations of AllReduce, ReduceScatterV,
    and AllGatherV collective. Different colors correspond to data (and its communication)
    from different data sources.
  Figure 5 Link: articels_figures_by_rev_year\2020\Scalable_and_Practical_Natural_Gradient_for_LargeScale_Deep_Learning\figure_5.jpg
  Figure 5 caption: Time per step for trainig ResNet-50 (107 layers in total) on ImageNet
    with our scalable and practical NGD. Each GPU processes 32 images. 1mc and emp
    correspond to NGD with hatboldsymbolFell,mathrm 1mc and that with hatboldsymbolFell,mathrm
    emp , respectively. fullBN and unitBN correspond to NGD and unit-wise NGD on BatchNorm
    parameters, respectively. stale corresponds to NGD with the stale statistics.
  Figure 6 Link: articels_figures_by_rev_year\2020\Scalable_and_Practical_Natural_Gradient_for_LargeScale_Deep_Learning\figure_6.jpg
  Figure 6 caption: "The communication amount (bytes) for the statistics ( boldsymbolAell\
    \ -1,boldsymbolGell ,boldsymbolFell,mathrm unitBN ) in each step in training ResNet-50\
    \ on ImageNet with BS=4K,8K,16K,32K (stacked graph \u2014 the amount for GF is\
    \ stacked on the amount for A). A and GF correspond to the communication amount\
    \ for boldsymbolAell -1 and boldsymbolGell boldsymbolFell,mathrm unitBN , respectively.\
    \ The reduction rate (smaller is better) of the communication amount for all the\
    \ statistics throughout the training is shown with the percentage (%)."
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.86
  Name of the first author: Kazuki Osawa
  Name of the last author: Rio Yokota
  Number of Figures: 6
  Number of Tables: 2
  Number of authors: 6
  Paper title: Scalable and Practical Natural Gradient for Large-Scale Deep Learning
  Publication Date: 2020-06-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Training Time and Top-1 Single-Crop Validation Accuracy of
      ResNet-50 for ImageNet Reported by Related Work and This Work
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Hyperparameters of the Training With Large Mini-Batch
      Size (BS) Used for Our Schemes in Section 7.2 and Top-1 Single-Crop Validation
      Accuracy of ResNet-50 for ImageNet
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3004354
- Affiliation of the first author: center for future media, and the school of computer
    science and engineering, university of electronic science and technology of china,
    chengdu, china
  Affiliation of the last author: center for future media, and the school of computer
    science and engineering, university of electronic science and technology of china,
    chengdu, china
  Figure 1 Link: articels_figures_by_rev_year\2020\MRANet_Improving_VQA_Via_MultiModal_Relation_Attention_Network\figure_1.jpg
  Figure 1 caption: Illustration of VQA examples and different types of object relations.
    Bounding boxes in red are the question specific object regions (visual objects),
    lines in blue represent the visual relations, words in red are the textual objects
    and words in blue are the textual object relations.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\MRANet_Improving_VQA_Via_MultiModal_Relation_Attention_Network\figure_2.jpg
  Figure 2 caption: The overview of our proposed model.
  Figure 3 Link: articels_figures_by_rev_year\2020\MRANet_Improving_VQA_Via_MultiModal_Relation_Attention_Network\figure_3.jpg
  Figure 3 caption: Illustration of question embedding part.
  Figure 4 Link: articels_figures_by_rev_year\2020\MRANet_Improving_VQA_Via_MultiModal_Relation_Attention_Network\figure_4.jpg
  Figure 4 caption: The ratio of questions containing objects of different number
    on VQA-2.0 training set (left) and testing set (right). Best viewed in color.
  Figure 5 Link: articels_figures_by_rev_year\2020\MRANet_Improving_VQA_Via_MultiModal_Relation_Attention_Network\figure_5.jpg
  Figure 5 caption: Results on VQA-2.0 validation set. Best viewed in color.
  Figure 6 Link: articels_figures_by_rev_year\2020\MRANet_Improving_VQA_Via_MultiModal_Relation_Attention_Network\figure_6.jpg
  Figure 6 caption: The results of different loss functions.
  Figure 7 Link: articels_figures_by_rev_year\2020\MRANet_Improving_VQA_Via_MultiModal_Relation_Attention_Network\figure_7.jpg
  Figure 7 caption: The influence of kb and kt on the performance of our model.
  Figure 8 Link: articels_figures_by_rev_year\2020\MRANet_Improving_VQA_Via_MultiModal_Relation_Attention_Network\figure_8.jpg
  Figure 8 caption: The qualitative evaluation of our MRA-Net. The most interested
    object regions are marked by boxes. The most important words are in orange, the
    darker the color is, the more important the word is. The visual and question relations
    are marked by blue lines. In binary and trinary attention map, we only show the
    top-20 and -5 regions, respectively. Best viewed in color.
  Figure 9 Link: articels_figures_by_rev_year\2020\MRANet_Improving_VQA_Via_MultiModal_Relation_Attention_Network\figure_9.jpg
  Figure 9 caption: Failure case.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Liang Peng
  Name of the last author: Heng Tao Shen
  Number of Figures: 9
  Number of Tables: 9
  Number of authors: 5
  Paper title: 'MRA-Net: Improving VQA Via Multi-Modal Relation Attention Network'
  Publication Date: 2020-06-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison With the State-of-the-Art Approaches on test-dev
      and test-std of VQA-1.0
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison With the State-of-the-Art Approaches on test-dev
      and test-std of VQA-2.0
  Table 3 caption:
    table_text: TABLE 3 Comparison With the State-of-the-Art Approaches on the TDIUC
      Dataset
  Table 4 caption:
    table_text: TABLE 4 Comparison With the State-of-the-Art Approaches on the COCO-QA
      Dataset
  Table 5 caption:
    table_text: TABLE 5 Comparison With the State-of-the-Art Approaches on the VQA-CP
      v2
  Table 6 caption:
    table_text: TABLE 6 Effectiveness of the Relation Features on the VQA-2.0 Validation
      Set
  Table 7 caption:
    table_text: TABLE 7 Results on the VQA-2.0 Validation Set
  Table 8 caption:
    table_text: TABLE 8 Results for Different Operations in Relation Embedding on
      the VQA-2.0 Validation Set
  Table 9 caption:
    table_text: TABLE 9 Effectiveness of the Relation Attention on the VQA-2.0 Validation
      Set
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3004830
- Affiliation of the first author: department of computer science, university at albany,
    state university of new york, albany, ny, usa
  Affiliation of the last author: national laboratory of pattern recognition, casia,
    university of chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Average_Topk_Aggregate_Loss_for_Supervised_Learning\figure_1.jpg
  Figure 1 caption: "Comparison of different aggregate losses on 2D synthetic datasets\
    \ with 200 samples for binary classification on a balanced but multi-modal dataset\
    \ and with outliers (top) and an imbalanced dataset with outliers (bottom) with\
    \ logistic loss (left) and hinge loss (right). Outliers in data are shown as \xD7\
    \ and the optimal Bayes classifications are shown as shaded areas. The figures\
    \ in the second and fourth columns show the misclassification rate of AT k w.r.t.\
    \ k for each case."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Average_Topk_Aggregate_Loss_for_Supervised_Learning\figure_2.jpg
  Figure 2 caption: The AT k loss interpreted at the individual loss level. Shaded
    area corresponds to region of correct classification.
  Figure 3 Link: articels_figures_by_rev_year\2020\Average_Topk_Aggregate_Loss_for_Supervised_Learning\figure_3.jpg
  Figure 3 caption: Tendency curves of misclassification rate of MAT k w.r.t. k on
    each dataset. The y -axis for dataset Splice and Spambase are clipped for better
    visualization.
  Figure 4 Link: articels_figures_by_rev_year\2020\Average_Topk_Aggregate_Loss_for_Supervised_Learning\figure_4.jpg
  Figure 4 caption: Tendency curves of RMSE of different model versus k on each dataset.
    The y -axis for dataset Sinc, Housing and Abalone are clipped for better visualization.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.65
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Siwei Lyu
  Name of the last author: Bao-Gang Hu
  Number of Figures: 4
  Number of Tables: 7
  Number of authors: 4
  Paper title: Average Top-k Aggregate Loss for Supervised Learning
  Publication Date: 2020-06-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Statistics of Datasets for Binary Classification, Where c,n,d
      c,n,d are the Number of Classes, Samples and Features, Respectively
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average Misclassification Rate (%) and Standard Derivation
      of Different Aggregate Losses Combined With Individual Logistic Loss Over 8
      Datasets
  Table 3 caption:
    table_text: TABLE 3 Average Misclassification Rate (%) and Standard Derivation
      of Different Aggregate Losses Combined With Individual Hinge Loss Over 8 Datasets
  Table 4 caption:
    table_text: "TABLE 4 Misclassification Rates (%, Single-Crop, 224\xD7224 224\xD7\
      224) on ImageNet Validation Set"
  Table 5 caption:
    table_text: TABLE 5 Statistics of Datasets for Regression, Where n n and d d are
      the Number of Samples and Features, Respectively
  Table 6 caption:
    table_text: TABLE 6 Average RMSE and Standard Derivation of Different Aggregate
      Losses Combined With Individual Square Loss Over 4 Datasets
  Table 7 caption:
    table_text: TABLE 7 Average RMSE and Standard Derivation of Different Aggregate
      Losses Combined With Individual Absolute Loss Over 4 Datasets
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3005393
