- Affiliation of the first author: milsoft, ankara, turkey
  Affiliation of the last author: "lear team, inria grenoble rh\xF4ne-alpes, laboratoire\
    \ jean kuntzmann, cnrs, university grenoble alpes, france"
  Figure 1 Link: articels_figures_by_rev_year\2015\Approximate_Fisher_Kernels_of_Noniid_Image_Models_for_Image_Categorization\figure_1.jpg
  Figure 1 caption: 'Local image appearance is not iid: the visible regions are informative
    on the masked-out ones; one has the impression to have seen the complete image
    by looking at half of the pixels.'
  Figure 10 Link: articels_figures_by_rev_year\2015\Approximate_Fisher_Kernels_of_Noniid_Image_Models_for_Image_Categorization\figure_10.jpg
  Figure 10 caption: "Performance when varying the number of topics: PLSA (red), power-normalized\
    \ PLSA (green), and LDA (blue). BoWP\xF3lya model performance included as the\
    \ left-most data point on each curve. All experiments use SPM, and K=1,024 visual\
    \ words."
  Figure 2 Link: articels_figures_by_rev_year\2015\Approximate_Fisher_Kernels_of_Noniid_Image_Models_for_Image_Categorization\figure_2.jpg
  Figure 2 caption: 'The score of a linear ''cow'' classifier will increase similarly
    from images (a) through (d) due to the increasing number of cow patches. This
    is undesirable: the score should sharply increase from (a) to (b), and remain
    stable among (b), (c), and (d).'
  Figure 3 Link: articels_figures_by_rev_year\2015\Approximate_Fisher_Kernels_of_Noniid_Image_Models_for_Image_Categorization\figure_3.jpg
  Figure 3 caption: "Comparison of \u2113 2 , Hellinger, and chi-square distances\
    \ for values in the unit interval. Both the Hellinger and chi-square distance\
    \ discount the effect of small changes in large values, unlike the \u2113 2 distance."
  Figure 4 Link: articels_figures_by_rev_year\2015\Approximate_Fisher_Kernels_of_Noniid_Image_Models_for_Image_Categorization\figure_4.jpg
  Figure 4 caption: "Graphical representation of the models in Section 4.1: (a) multinomial\
    \ BoW model, (b) P\xF3lya model. The outer plate in (b) refer to images. The index\
    \ i runs over the visual word indices in an image. Nodes of observed variables\
    \ are shaded, and those of (hyper-)parameters are marked with a central dot in\
    \ the node."
  Figure 5 Link: articels_figures_by_rev_year\2015\Approximate_Fisher_Kernels_of_Noniid_Image_Models_for_Image_Categorization\figure_5.jpg
  Figure 5 caption: "Digamma functions \u03C8(\u03B1+n) for various \u03B1 , and n\
    \ \u2212 \u2212 \u221A as a function of n ; functions have been rescaled to the\
    \ range [0,1] ."
  Figure 6 Link: articels_figures_by_rev_year\2015\Approximate_Fisher_Kernels_of_Noniid_Image_Models_for_Image_Categorization\figure_6.jpg
  Figure 6 caption: Graphical representation of LDA. The outer plate refers to images.
    The index i runs over patches, and index t over topics.
  Figure 7 Link: articels_figures_by_rev_year\2015\Approximate_Fisher_Kernels_of_Noniid_Image_Models_for_Image_Categorization\figure_7.jpg
  Figure 7 caption: 'Graphical representation of the models in Section 4.3: (a) MoG
    model, (b) latent MoG model. The outer plate in (b) without indexing refer to
    images. The index i runs over the local descriptors, and index k over Gaussians
    in the mixture which represent the visual words.'
  Figure 8 Link: articels_figures_by_rev_year\2015\Approximate_Fisher_Kernels_of_Noniid_Image_Models_for_Image_Categorization\figure_8.jpg
  Figure 8 caption: Comparison of the discounting functions learned by the latent
    BoW model for 64 visual words (solid), and the square-root transformation (dashed).
    Transformed counts are rescaled to the range [0,1] .
  Figure 9 Link: articels_figures_by_rev_year\2015\Approximate_Fisher_Kernels_of_Noniid_Image_Models_for_Image_Categorization\figure_9.jpg
  Figure 9 caption: "Topic models ( T=2 , solid) compared with BoW models (dashed):\
    \ BoWPLSA (red), power-normalized BoWPLSA (green), and P\xF3lyaLDA (blue). SPM\
    \ grids are used in all experiments."
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Ramazan Gokberk Cinbis
  Name of the last author: Cordelia Schmid
  Number of Figures: 12
  Number of Tables: 6
  Number of authors: 3
  Paper title: Approximate Fisher Kernels of Non-iid Image Models for Image Categorization
  Publication Date: 2015-10-01 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Comparison of Representations with and without SPM: BoW,\
      \ Two Types of Power Normalized BoW, and P\xF3lya"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Comparison of MoG-Based FV Representations: Plain MoG, Two
      Types of Power Normalized MoG, and Latent MoG'
  Table 3 caption:
    table_text: 'TABLE 3 Comparison of mAP Scores on PASCAL VOC''07 Dataset: Plain
      MoG, Two Types of Power Normalized MoG and Latent MoG'
  Table 4 caption:
    table_text: TABLE 4 Comparison of the Power Normalized MoG and Latent MoG Representations
      Against Recent Results on the PASCAL VOC'07 Dataset
  Table 5 caption:
    table_text: 'TABLE 5 Comparison of Classification Accuracy on MIT Indoor: Plain
      MoG, Two Types of Power Normalized MoG and Latent MoG'
  Table 6 caption:
    table_text: TABLE 6 Comparison of the Power Normalized MoG and Latent MoG Representations
      Against Recent Results on the MIT Indoor Dataset
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2484342
- Affiliation of the first author: department of computer science and engineering,
    postech, korea
  Affiliation of the last author: university of maryland institute of advanced computer
    studies (umiacs), university of maryland, college park, md
  Figure 1 Link: articels_figures_by_rev_year\2015\Joint_Image_Clustering_and_Labeling_by_Matrix_Factorization\figure_1.jpg
  Figure 1 caption: Overview of our approach. First, query images are represented
    by noun-typed label features with the help of a reference database (Ref-DB). Based
    on the label feature representation, we perform the non-negative matrix factorization
    with sparsity and orthogonality constraints (SO-NMF), which enables us to produce
    discriminative clusters and label the query images jointly.
  Figure 10 Link: articels_figures_by_rev_year\2015\Joint_Image_Clustering_and_Labeling_by_Matrix_Factorization\figure_10.jpg
  Figure 10 caption: Performance evaluation by confusion tables in CIFAR-100 dataset.
    (a) SO-NMF+SVM (b) SVM-KNN (c) KNN. Our algorithm performs notably better in identifying
    correct class labels. Average precisions of entire 10 subsets of the three algorithms
    are 0.66, 0.36, and 0.32, respectively.
  Figure 2 Link: articels_figures_by_rev_year\2015\Joint_Image_Clustering_and_Labeling_by_Matrix_Factorization\figure_2.jpg
  Figure 2 caption: Examples of label feature representations of images in (left)
    CIFAR-100 and (right) ImageNet datasets. Labels for highly related classes are
    annotated in the histograms.
  Figure 3 Link: articels_figures_by_rev_year\2015\Joint_Image_Clustering_and_Labeling_by_Matrix_Factorization\figure_3.jpg
  Figure 3 caption: Clustering performance comparison under various situations in
    (top) CIFAR-100 and (bottom) ImageNet dataset. (a) Clustering accuracy in the
    presence of missing labels in the Ref-DB. (b) Clustering accuracy in the presence
    of incorrect labels in Ref-DB. (c) Clustering accuracy by varying the number of
    clusters. Note that SO-NMF based on label features outperforms all other methods.
    The performance with visual features is not illustrated in these graphs since
    the ARI for all algorithms is very low (almost below 0.1) as seen in Table 1 .
  Figure 4 Link: articels_figures_by_rev_year\2015\Joint_Image_Clustering_and_Labeling_by_Matrix_Factorization\figure_4.jpg
  Figure 4 caption: Comparison of clustering performance with increasing number of
    classes in query images in (top) CIFAR-100 and (bottom) ImageNet dataset.
  Figure 5 Link: articels_figures_by_rev_year\2015\Joint_Image_Clustering_and_Labeling_by_Matrix_Factorization\figure_5.jpg
  Figure 5 caption: Quality of per-cluster labels obtained by our approach. The semantic
    distance is defined as the sum of distances to the lowest common ancestor in the
    WordNet hierarchy of ground truth and identified labels. Blue and red bar graphs
    indicate the distributions of WordNet distances between ground truth and identified
    labels by SO-NMF. Gray bar graphs illustrate the distribution of WordNet distances
    between the labels of randomly chosen images in the Ref-DB. Note that our method
    tends to find more relevant labels, even if they are not correct.
  Figure 6 Link: articels_figures_by_rev_year\2015\Joint_Image_Clustering_and_Labeling_by_Matrix_Factorization\figure_6.jpg
  Figure 6 caption: Cumulative match characteristic curves for image labeling accuracy.
    SO-NMF is substantially better than other methods.
  Figure 7 Link: articels_figures_by_rev_year\2015\Joint_Image_Clustering_and_Labeling_by_Matrix_Factorization\figure_7.jpg
  Figure 7 caption: Joint clustering and labeling performance in (top) CIFAR-100 and
    (bottom) ImageNet datasets. In (a) and (b), 15 most confident and three least
    confident images are presented for each class (sorted by confidence).
  Figure 8 Link: articels_figures_by_rev_year\2015\Joint_Image_Clustering_and_Labeling_by_Matrix_Factorization\figure_8.jpg
  Figure 8 caption: Per-image annotation results in both datasets. Only relevant labels
    are shown, where true labels are underlined and marked in bold.
  Figure 9 Link: articels_figures_by_rev_year\2015\Joint_Image_Clustering_and_Labeling_by_Matrix_Factorization\figure_9.jpg
  Figure 9 caption: Performance of the extension to supervised image categorization.
    Our algorithm with SVM (SO-NMF+SVM) outperforms all other methods except SVM-GT,
    which represents an upper bound on performance of our algorithm.
  First author gender probability: 0.61
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Seunghoon Hong
  Name of the last author: Larry S. Davis
  Number of Figures: 11
  Number of Tables: 1
  Number of authors: 5
  Paper title: Joint Image Clustering and Labeling by Matrix Factorization
  Publication Date: 2015-10-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Image Clustering Performance by SO-NMF Compared to Other Methods
      with Visual and Label Features
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2487982
- Affiliation of the first author: biomedical signal and image processing laboratory
    (bisipl), department of electrical engineering, sharif university of technology,
    tehran, iran
  Affiliation of the last author: biomedical signal and image processing laboratory
    (bisipl), department of electrical engineering, sharif university of technology,
    tehran, iran
  Figure 1 Link: articels_figures_by_rev_year\2015\Nonlinear_Dimensionality_Reduction_via_PathBased_Isometric_Mapping\figure_1.jpg
  Figure 1 caption: Graphical illustration of the scheme that maps geodesic paths
    on the manifold to straight lines. Direction and starting point of each line are
    tuned in a way that global geometry of data points is preserved.
  Figure 10 Link: articels_figures_by_rev_year\2015\Nonlinear_Dimensionality_Reduction_via_PathBased_Isometric_Mapping\figure_10.jpg
  Figure 10 caption: The result of applying Path-Based Isomap on MNIST database. There
    are 5,000 images of handwritten ' 2 's images in the database. There is clearly
    a meaningful relation between place of data points and geometrical features of
    their corresponding images.
  Figure 2 Link: articels_figures_by_rev_year\2015\Nonlinear_Dimensionality_Reduction_via_PathBased_Isometric_Mapping\figure_2.jpg
  Figure 2 caption: SSPC sample run on a Swiss-Roll dataset consisting of 1,000 data
    points. The number of paths obtained by the method in this example is P=154 .
    (a) The high-dimensional data in R 3 . (b) Result of the SSPC in R 3 . (c) The
    results shown for the unfolded manifold.
  Figure 3 Link: articels_figures_by_rev_year\2015\Nonlinear_Dimensionality_Reduction_via_PathBased_Isometric_Mapping\figure_3.jpg
  Figure 3 caption: Number of optimization variables depicted as a function of number
    of data points in a log-log plot. Reduction in complexity is shown for three manifold
    dimensionalities. The original number of variables used in Isomap is also plotted
    for comparison.
  Figure 4 Link: articels_figures_by_rev_year\2015\Nonlinear_Dimensionality_Reduction_via_PathBased_Isometric_Mapping\figure_4.jpg
  Figure 4 caption: Number of the uncovered data samples shown during execution of
    SSPC algorithm.
  Figure 5 Link: articels_figures_by_rev_year\2015\Nonlinear_Dimensionality_Reduction_via_PathBased_Isometric_Mapping\figure_5.jpg
  Figure 5 caption: Applying Path-Based Isomap on (a) Swiss-Roll dataset with N=10,000
    and P=846 , and (b) Swiss-Hole dataset with N=10,000 and P=930 .
  Figure 6 Link: articels_figures_by_rev_year\2015\Nonlinear_Dimensionality_Reduction_via_PathBased_Isometric_Mapping\figure_6.jpg
  Figure 6 caption: Investigating the effect of noise on Path-Based Isomap via a noisy
    S-shape one-dimensional manifold in R 2 . Number of data samples is N=2,000 and
    the number of obtained paths is 185 .
  Figure 7 Link: articels_figures_by_rev_year\2015\Nonlinear_Dimensionality_Reduction_via_PathBased_Isometric_Mapping\figure_7.jpg
  Figure 7 caption: Comparison of performance for the proposed path-based method and
    four state-of-the-art algorithms on a Swiss-Hole dataset with N=2,000 . The result
    are obtained by Path-Based Isomap (A), Hessian LLE (B), LLE (C), Isomap (D) and
    LTSA (E). Hessian LLE, LTSA and Path-Based Isomap have comparable performances.
  Figure 8 Link: articels_figures_by_rev_year\2015\Nonlinear_Dimensionality_Reduction_via_PathBased_Isometric_Mapping\figure_8.jpg
  Figure 8 caption: Execution-time versus number of data samples for 10 manifold learning
    techniques including the proposed path-based approach. Experiments are done on
    a noisy S-shape manifold.
  Figure 9 Link: articels_figures_by_rev_year\2015\Nonlinear_Dimensionality_Reduction_via_PathBased_Isometric_Mapping\figure_9.jpg
  Figure 9 caption: "Applying Path-Based Isomap on 64\xD764 gray-scale images of the\
    \ statue-face database. Some images are shown to illustrate performance of algorithm."
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Amir Najafi
  Name of the last author: Emad Fatemizadeh
  Number of Figures: 13
  Number of Tables: 4
  Number of authors: 3
  Paper title: Nonlinear Dimensionality Reduction via Path-Based Isometric Mapping
  Publication Date: 2015-10-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Experimental Parameters
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Experimental Values for \u03BB"
  Table 3 caption:
    table_text: "TABLE 3 Normalized Error \u03F5 n of a Variety of Manifold Learning\
      \ Methods Including the Proposed Framework, Computed for a Number of Benchmark\
      \ Synthetic Datasets"
  Table 4 caption:
    table_text: TABLE 4 Fisher's Criterion of a Variety of Manifold Learning Methods
      Including the Proposed Framework, Computed for Two Real-World Datasets
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2487981
- Affiliation of the first author: department of mathematics, national university
    of singapore, singapore
  Affiliation of the last author: department of mathematics, national university of
    singapore, singapore
  Figure 1 Link: articels_figures_by_rev_year\2015\Dictionary_Learning_for_Sparse_Coding_Algorithms_and_Convergence_Analysis\figure_1.jpg
  Figure 1 caption: 'Convergence behavior: the increments of the coefficient sequence
    C k generated by K-SVD and by the proposed method in image denoising.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Dictionary_Learning_for_Sparse_Coding_Algorithms_and_Convergence_Analysis\figure_2.jpg
  Figure 2 caption: Objective function value versus iteration number in sparse coding
    based image de-noising.
  Figure 3 Link: articels_figures_by_rev_year\2015\Dictionary_Learning_for_Sparse_Coding_Algorithms_and_Convergence_Analysis\figure_3.jpg
  Figure 3 caption: Six test images for image denoising.
  Figure 4 Link: articels_figures_by_rev_year\2015\Dictionary_Learning_for_Sparse_Coding_Algorithms_and_Convergence_Analysis\figure_4.jpg
  Figure 4 caption: "The dictionaries learned from the image \u201CLena512\u201D with\
    \ noise level \u03C3=25 using the K-SVD method and algorithm 2."
  Figure 5 Link: articels_figures_by_rev_year\2015\Dictionary_Learning_for_Sparse_Coding_Algorithms_and_Convergence_Analysis\figure_5.jpg
  Figure 5 caption: Visual illustration of a noisy image and the denoised one by algorithm
    2.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Chenglong Bao
  Name of the last author: Zuowei Shen
  Number of Figures: 5
  Number of Tables: 4
  Number of authors: 4
  Paper title: 'Dictionary Learning for Sparse Coding: Algorithms and Convergence
    Analysis'
  Publication Date: 2015-10-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Running Time (Seconds) versus Dimension of Dictonary Atom
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 PSNR Values of the Denoised Results
  Table 3 caption:
    table_text: TABLE 3 Average PSNR Value of the Denoised Results Using Different
      Initializations
  Table 4 caption:
    table_text: TABLE 4 Classification Accuracies (Percent) on Four Datasets
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2487966
- Affiliation of the first author: google, 1600 amphitheatre parkway, mountain view,
    ca
  Affiliation of the last author: school of electrical engineering and computer science,
    queensland university of technology, gpo box 2434, brisbane, australia
  Figure 1 Link: articels_figures_by_rev_year\2015\General_Nested_and_Constrained_Wiberg_Minimization\figure_1.jpg
  Figure 1 caption: "L 1 -Wiberg bundle adjustment reconstructs the correct structure\
    \ and motion from the \u201Crover\u201D sequence, which includes about 700 images\
    \ and 10,000 points. Left: an example image from the sequence, with tracked points\
    \ shown as black dots. Right: an oblique view of the recovered camera positions\
    \ at the time of each image."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Dennis Strelow
  Name of the last author: Anders Eriksson
  Number of Figures: 1
  Number of Tables: 0
  Number of authors: 4
  Paper title: General, Nested, and Constrained Wiberg Minimization
  Publication Date: 2015-10-07 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2487987
- Affiliation of the first author: computer vision and multimodal computing group
    of the max-planck institute for informatics, saarbrucken, germany
  Affiliation of the last author: "lear group of inria grenoble rh\xF4ne-alpes, montbonnot,\
    \ france"
  Figure 1 Link: articels_figures_by_rev_year\2015\LabelEmbedding_for_Image_Classification\figure_1.jpg
  Figure 1 caption: "Much work in computer vision has been devoted to image embedding\
    \ (left): how to extract suitable features from an image. We focus on label embedding\
    \ (right): how to embed class labels in a euclidean space. We use side information\
    \ such as attributes for the label embedding and measure the \u201Ccompatibility\u201D\
    \ between the embedded inputs and outputs with a function F ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\LabelEmbedding_for_Image_Classification\figure_2.jpg
  Figure 2 caption: "Illustration of Hierarchical Label Embedding (HLE). In this example,\
    \ given 7 classes (including a \u201Croot\u201D class), class 6 is encoded in\
    \ a binary 7-dimensional space as \u03C6 H (6)=[1,0,1,0,0,1,0] ."
  Figure 3 Link: articels_figures_by_rev_year\2015\LabelEmbedding_for_Image_Classification\figure_3.jpg
  Figure 3 caption: Classification accuracy on AWA and CUB as a function of the label
    embedding dimensionality. We compare the baseline which uses all attributes, with
    an SVD dimensionality reduction and a sampling of attributes (we report the mean
    and standard deviation over 10 samplings).
  Figure 4 Link: articels_figures_by_rev_year\2015\LabelEmbedding_for_Image_Classification\figure_4.jpg
  Figure 4 caption: Sample attributes recognized with high ( > 90%) accuracy (top)
    and low (i.e., < 50%) accuracy (bottom) by ALE on AWA. For each attribute we show
    the images ranked highest. Note that a AUC < 50% means that the prediction is
    worse than random on average. The images whose attribute is predicted correctly
    are circled in green and those whose attribute is predicted incorrectly are circled
    in red.
  Figure 5 Link: articels_figures_by_rev_year\2015\LabelEmbedding_for_Image_Classification\figure_5.jpg
  Figure 5 caption: "Classification accuracy on AWA and CUB as a function of the number\
    \ of training samples per class. To train the classifiers, we use all the images\
    \ of the training \u201Cbackground\u201D classes (used in zero-shot learning),\
    \ and a small number of images randomly drawn from the relevant evaluation classes.\
    \ Reported results are 10-way in AWA and 50-way in CUB."
  Figure 6 Link: articels_figures_by_rev_year\2015\LabelEmbedding_for_Image_Classification\figure_6.jpg
  Figure 6 caption: 'Learning on AWA and CUB using 14, 12, 34 and all the training
    data. Compared output embeddings: OVR, GLE, WSABIE, ALE, HLE, AHLE early and AHLE
    late. Experiments repeated 10 times for different sampling of Gaussians. We use
    64K FVs.'
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Zeynep Akata
  Name of the last author: Cordelia Schmid
  Number of Figures: 6
  Number of Tables: 6
  Number of authors: 4
  Paper title: Label-Embedding for Image Classification
  Publication Date: 2015-10-07 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Comparison of the Continuous Embedding (cont), the Binary\
      \ 0,1 Embedding and the Binary +1,\u22121 Embedding"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Comparison of Different Learning Algorithms for ALE: Ridge-Regression
      (RR), Multi-Class SSVM (SSVM) and Ranking Based on WSABIE (RNK)'
  Table 3 caption:
    table_text: 'TABLE 3 Comparison of DAP [2] with ALE. Left: Object Classification
      Accuracy (top-1 %) on the 10 AWA and 50 CUB Evaluation Classes. Right: Attribute
      Prediction Accuracy (AUC %) on the 85 AWA and 312 CUB Attributes. We Use 64K
      FVs'
  Table 4 caption:
    table_text: TABLE 4 Comparison of Attributes (ALE), Hierarchies (HLE) and Word2Vec
      (WLE) for Label Embedding. We Consider the Combination of ALE and HLE by Simple
      Concatenation (AHLE Early) or by the Averaging of the Scores (AHLE Late). We
      Use 64K FVs
  Table 5 caption:
    table_text: "TABLE 5 Comparison of Different Output Encodings: Binary 0,1 Encoding,\
      \ Continuous Encoding, withwithout Mean-Centering ( \u03BC ) and withwithout\
      \ \u2113 2 -Normalization"
  Table 6 caption:
    table_text: TABLE 6 Comparison of Different Output Embedding Methods (OVR, GLE,
      WSABIE, ALE, HLE, AHLE Early and AHLE Late) on the Full AWA and CUB Datasets
      (resp. 50 and 200 Classes). We Use 64K FVs
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2487986
- Affiliation of the first author: image and pattern analysis group (ipa), heidelberg
    university, speyerer str. 6, heidelberg, germany
  Affiliation of the last author: heidelberg collaboratory for image processing (hci),
    heidelberg university, speyerer str. 6, heidelberg, germany
  Figure 1 Link: articels_figures_by_rev_year\2015\Partial_Optimality_by_Pruning_for_MAPInference_with_General_Graphical_Models\figure_1.jpg
  Figure 1 caption: An exemplary graph containing inside nodes (yellow with crosshatch
    pattern) and boundary nodes (green with diagonal pattern). The blue dashed line
    encloses the set A . Boundary edges are those crossed by the dashed line.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Partial_Optimality_by_Pruning_for_MAPInference_with_General_Graphical_Models\figure_2.jpg
  Figure 2 caption: "Illustration of a boundary potential \u03B8 y constructed in\
    \ (3.3). The second label comes from the test labeling y , therefore entries are\
    \ maximized for the second row and minimized otherwise."
  Figure 3 Link: articels_figures_by_rev_year\2015\Partial_Optimality_by_Pruning_for_MAPInference_with_General_Graphical_Models\figure_3.jpg
  Figure 3 caption: Illustration of one iteration of Algorithm 4.
  Figure 4 Link: articels_figures_by_rev_year\2015\Partial_Optimality_by_Pruning_for_MAPInference_with_General_Graphical_Models\figure_4.jpg
  Figure 4 caption: Iterations needed by TRWS [18] in Algorithm 4 for three instances
    from the Potts dataset.
  Figure 5 Link: articels_figures_by_rev_year\2015\Partial_Optimality_by_Pruning_for_MAPInference_with_General_Graphical_Models\figure_5.jpg
  Figure 5 caption: 'Comparison between Kovtun''s Method [20] and Our Method. The
    red area denotes pixels which could not be labelled persistently. Contrary to
    ours the Kovtun''s method allows to eliminate separate labels, which is denoted
    by different intensity of the red color: the more intensive is red, the less labels
    were eliminated.'
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Paul Swoboda
  Name of the last author: Bogdan Savchynskyy
  Number of Figures: 5
  Number of Tables: 3
  Number of authors: 5
  Paper title: Partial Optimality by Pruning for MAP-Inference with General Graphical
    Models
  Publication Date: 2015-10-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison between Partial Optimality Methods
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison between Our Method and CombiLP [25]
  Table 3 caption:
    table_text: TABLE 3 Percentage of Persistent Variables Obtained by Methods [17],
      [20],[12], [4],[10] and Our Methods with Boundary Potentials Computed as in
      (3.4) (Ours Original) and as in (6.3) (Ours Optimal)
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2484327
- Affiliation of the first author: department of computer science and digital technologies,
    northumbria university, newcastle upon tyne, uk
  Affiliation of the last author: department of computer science and digital technologies,
    northumbria university, newcastle upon tyne, uk
  Figure 1 Link: articels_figures_by_rev_year\2015\StructurePreserving_Binary_Representations_for_RGBD_Action_Recognition\figure_1.jpg
  Figure 1 caption: "Basic principle of the projection with angle-preserving in a\
    \ two-dimensional example. The distances of two negative pairs \u2225 x 1 \u2212\
    \ x 2 \u2225 and \u2225 x 1 \u2212 x 3 \u2225 are expected to be maximized after\
    \ the projection. The shape of ( x 1 , x 2 , x 3 ) has collapsed in the Hamming\
    \ space without angle-preserving, therefore, lost the discriminative ability."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\StructurePreserving_Binary_Representations_for_RGBD_Action_Recognition\figure_2.jpg
  Figure 2 caption: Illustration of the computation of local fluxes in the gradient
    field. The output LFF is regarded as a foundation for learning binary codes.
  Figure 3 Link: articels_figures_by_rev_year\2015\StructurePreserving_Binary_Representations_for_RGBD_Action_Recognition\figure_3.jpg
  Figure 3 caption: 'Example frames of the three RGB-D datasets we used in the experiments.
    From top to bottom: SKIG, MSRDailyActivity3D and CAD-60.'
  Figure 4 Link: articels_figures_by_rev_year\2015\StructurePreserving_Binary_Representations_for_RGBD_Action_Recognition\figure_4.jpg
  Figure 4 caption: Performance comparison with different training sizes in each category
    and different versions of LFFs on the SKIG dataset at 96-bit.
  Figure 5 Link: articels_figures_by_rev_year\2015\StructurePreserving_Binary_Representations_for_RGBD_Action_Recognition\figure_5.jpg
  Figure 5 caption: Performance comparison with different training sizes for each
    subject and different versions of LFFs on the MSRDailyActivity3D dataset at 96-bit.
  Figure 6 Link: articels_figures_by_rev_year\2015\StructurePreserving_Binary_Representations_for_RGBD_Action_Recognition\figure_6.jpg
  Figure 6 caption: Performance comparison with different training sizes in each action
    and different versions of LFFs on the CAD-60 dataset at 96-bit.
  Figure 7 Link: articels_figures_by_rev_year\2015\StructurePreserving_Binary_Representations_for_RGBD_Action_Recognition\figure_7.jpg
  Figure 7 caption: Performance comparison of NBNN with different point selection
    methods on three datasets.
  Figure 8 Link: articels_figures_by_rev_year\2015\StructurePreserving_Binary_Representations_for_RGBD_Action_Recognition\figure_8.jpg
  Figure 8 caption: Average runtime of one test sample of NBNN by using 96-bit binary
    codes after SPP and the original 882-dimensional LFF with different training sizes.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.53
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Mengyang Yu
  Name of the last author: Ling Shao
  Number of Figures: 8
  Number of Tables: 5
  Number of authors: 3
  Paper title: Structure-Preserving Binary Representations for RGB-D Action Recognition
  Publication Date: 2015-10-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance Comparison (%) of NBNN with the LFFs Computed
      on Detected Points with Different Radii
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance Comparison (%) of Different Variants of LFF+SPP
      to Prove the Effectiveness of the Improvement on RGB-D Fusion
  Table 3 caption:
    table_text: TABLE 3 Performance Comparison (%) of Our Algorithm and Other Coding
      Methods on Three Datasets
  Table 4 caption:
    table_text: TABLE 4 T-Test on Performance Improvements
  Table 5 caption:
    table_text: TABLE 5 Recognition Accuracy (%) of LFF and Dense Trajectory Features
      on the UCF YouTube and HMDB51 Datasets
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2491925
- Affiliation of the first author: department of ece, northeastern university, boston,
    ma
  Affiliation of the last author: department of ece and college of cis, northeastern
    university, boston, ma
  Figure 1 Link: articels_figures_by_rev_year\2015\MaxMargin_Action_Prediction_Machine\figure_1.jpg
  Figure 1 caption: Our method predicts action label given a partially observed video.
    Action dynamics are captured by both local templates (solid rectangles) and global
    templates (dashed rectangles).
  Figure 10 Link: articels_figures_by_rev_year\2015\MaxMargin_Action_Prediction_Machine\figure_10.jpg
  Figure 10 caption: Prediction results of each component in the full MMAPM with C
    parameter values equal to 0.08 and 10 .
  Figure 2 Link: articels_figures_by_rev_year\2015\MaxMargin_Action_Prediction_Machine\figure_2.jpg
  Figure 2 caption: Framework of our method. Given an unfinished action video, we
    first use bounding boxes or a detector to localize the action of interest. Interest
    points and dense trajectories are then extracted. Temporal segments and partial
    observations are both characterized using these two types of features to express
    both local and global information. The proposed action predictor that consists
    of a local progress model and a global progress model predicts the action label
    based on partially observed action features.
  Figure 3 Link: articels_figures_by_rev_year\2015\MaxMargin_Action_Prediction_Machine\figure_3.jpg
  Figure 3 caption: "Example of video segments x (m) , partial video x (1,m) , feature\
    \ representation g( x (1,m) ,l) of a segment ( l=1,\u2026,m ), and the representation\
    \ of the partial video g( x (1,m) ,1:m) ."
  Figure 4 Link: articels_figures_by_rev_year\2015\MaxMargin_Action_Prediction_Machine\figure_4.jpg
  Figure 4 caption: Graphical illustration of the temporal action evolution over time
    and the label consistency of segments. Blue solid rectangles are LPMs, and purple
    and red dashed rectangles are GPMs.
  Figure 5 Link: articels_figures_by_rev_year\2015\MaxMargin_Action_Prediction_Machine\figure_5.jpg
  Figure 5 caption: Illustration of the components in the composite kernel K(Psi +im,Psi
    -iprime mprime) . Red rectangles are partial observations and the blue ones are
    temporal segments.
  Figure 6 Link: articels_figures_by_rev_year\2015\MaxMargin_Action_Prediction_Machine\figure_6.jpg
  Figure 6 caption: Prediction results on the UTI 1, UTI 2, BIT, and UCF11 dataset.
    This figure is best viewed in color.
  Figure 7 Link: articels_figures_by_rev_year\2015\MaxMargin_Action_Prediction_Machine\figure_7.jpg
  Figure 7 caption: Contributions of the global progress model and the local progress
    model to the prediction task.
  Figure 8 Link: articels_figures_by_rev_year\2015\MaxMargin_Action_Prediction_Machine\figure_8.jpg
  Figure 8 caption: "Examples of visual similar segments ( m=6,8,10 ) in \u201Chandshake\u201D\
    \ and \u201Chug\u201D."
  Figure 9 Link: articels_figures_by_rev_year\2015\MaxMargin_Action_Prediction_Machine\figure_9.jpg
  Figure 9 caption: "Examples of visually similar segments in the \u201Cboxing\u201D\
    \ action (Top) and the \u201Cpushing\u201D action (Bottom) with segment index\
    \ min lbrace 2,4,6,8,10rbrace . Bounding boxes indicate the interest regions of\
    \ actions."
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Yu Kong
  Name of the last author: Yun Fu
  Number of Figures: 10
  Number of Tables: 7
  Number of authors: 2
  Paper title: Max-Margin Action Prediction Machine
  Publication Date: 2015-10-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Prediction Results on the UT1 1 Dataset with Different Observation
      Ratios
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Action Recognition Results on the UTI 1 Dataset Compared with
      Existing Methods Given Full Observations
  Table 3 caption:
    table_text: TABLE 3 Accuracy of MMAPM on Videos of Observation Ratios 0.3 , 0.5
      , and 0.8 with Different C Parameter Values
  Table 4 caption:
    table_text: TABLE 4 Prediction Results on the BIT Dataset with Different Observation
      Ratios
  Table 5 caption:
    table_text: TABLE 5 Accuracy of MMAPM on Videos of Observation Ratios 0.3 , 0.5
      , and 0.8 with Various C Parameter Values
  Table 6 caption:
    table_text: TABLE 6 Accuracy of Progress Level Inference Using X 2 -SVM with the
      Global Features (GF), the Local Segment Features (LSF), and the Combination
      of the Two Features (GF+LSF)
  Table 7 caption:
    table_text: TABLE 7 Prediction Results on the BIT Dataset with Different Progress
      Levels
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2491928
- Affiliation of the first author: university of california, riverside, ca
  Affiliation of the last author: department of electrical and computer engineering,
    university of california, riverside, ca
  Figure 1 Link: articels_figures_by_rev_year\2015\Network_Consistent_Data_Association\figure_1.jpg
  Figure 1 caption: 'Example of network inconsistency in data association. (a) Person
    re-identification: Among the three possible re-identification results, two are
    correct. Match of the target from camera 1 to camera 3 can be found in two ways.
    The first one is the direct pairwise re-identification result between cameras
    1 and 3 (shown as ''Path 1''), and the second one is the indirect re-identification
    result in camera 3 given via the matched person in camera 2 (shown as ''Path 2'').
    The two outcomes do not match and thus the overall associations of the target
    across three cameras is not consistent. (b) Network inconsistency in spatio-temporal
    cell tracking: In this schematic, association results between 2D projections of
    the same 3D cell on four spatio-temporal image planes are analyzed. The pairwise
    associations need to be consistent across the loop over the four image slices.
    This consistency can be used to obtain correspondences when there are no direct
    pairwise matches or to correct wrong ones. For example, the correspondence between
    the same cell in image slice 1 and slice 3 (broken arrow) is established via an
    indirect path (solid arrows) through slices 2 and 4.'
  Figure 10 Link: articels_figures_by_rev_year\2015\Network_Consistent_Data_Association\figure_10.jpg
  Figure 10 caption: Application of online NCDA in Online spatio-temporal cell tracking
    problem and comparison with Local graph based cell tracker [32]. Cell tracking
    accuracies with increasing amount of observations (expressed as percentage of
    total number of observations) are plotted for both [32] and the proposed online
    NCDA. The online NCDA outperforms [32] and the margin increases with more observations.
  Figure 2 Link: articels_figures_by_rev_year\2015\Network_Consistent_Data_Association\figure_2.jpg
  Figure 2 caption: An illustrative example showing the importance of the loop constraint
    in a data-association problem. It presents a simple person re-identification scenario
    in a camera network involving two persons (data points) in three cameras (groups).
  Figure 3 Link: articels_figures_by_rev_year\2015\Network_Consistent_Data_Association\figure_3.jpg
  Figure 3 caption: "A schematic showing time evolution of the online NCDA implementation.\
    \ (a) The green target appeared in both group 1 ( G 1 ) and group 2 ( G 2 ) until\
    \ time t and the black target only appeared in group 3 ( G 3 ). Data association\
    \ problem is solved until time t using NCDA and the labels are shown. (b) Two\
    \ new observations (from the blue and the green target) are observed simultaneously\
    \ at time point t+1 in groups 1 and 3 respectively. The past observations are\
    \ blurred out. (c) At time t+2 , no observation in G 1 or G 3 , but an observation\
    \ (from the black target) is obtained in G 2 . All past observations are blurred\
    \ including the ones from time t+1 . (d) NCDA is run to associate the three new\
    \ observations obtained in the time window [t,t+2] to the ones obtained until\
    \ t . Two dummy groups ( D G 1 and D G 2 ) are created for the new observations\u2014\
    \ D G 1 holds the observations from the blue and the green target as they have\
    \ time overlap and D G 2 holds the observation from the black target as it has\
    \ no spacetime overlap with the other two and hence can be legally associated\
    \ with either of them. Nodes from the dummy groups are connected via edges (dashed\
    \ lines) to one another and to the nodes corresponding to all past observations.\
    \ Online NCDA (as in Eqn. (18)) is run with past associations (solid lines) as\
    \ constraints and the new labels are estimated (only label 1 s are shown)."
  Figure 4 Link: articels_figures_by_rev_year\2015\Network_Consistent_Data_Association\figure_4.jpg
  Figure 4 caption: CMC curves for the WARD dataset. Results and comparisons in (a),
    (b) and (c) are shown for the camera pairs 1-2, 1-3, and 2-3 respectively.
  Figure 5 Link: articels_figures_by_rev_year\2015\Network_Consistent_Data_Association\figure_5.jpg
  Figure 5 caption: Two examples of correction of inconsistent re-identification from
    WARD dataset. The red dashed lines denote re-identifications performed on three
    camera pairs independently by FT. The green solid lines show the re-identification
    results on application of NCDA on FT. The NCDA algorithm exploits the consistency
    requirement and makes the resultant re-identification across three cameras correct.
  Figure 6 Link: articels_figures_by_rev_year\2015\Network_Consistent_Data_Association\figure_6.jpg
  Figure 6 caption: CMC curves for RAiD dataset. In (a), (b), (c), (d), (e), (f) comparisons
    are shown for the camera pairs 1-2 (both indoor), 1-3 (indoor-outdoor), 1-4 (indoor-outdoor),
    2-3 (indoor-outdoor), 2-4 (indoor-outdoor) and 3-4 (both outdoor) respectively.
  Figure 7 Link: articels_figures_by_rev_year\2015\Network_Consistent_Data_Association\figure_7.jpg
  Figure 7 caption: Performance of the NCDA algorithm after removing 40 percent of
    the people from both camera 3 and 4 in the RAiD dataset. In (a) re-identification
    accuracy on the training data is shown for camera pairs by varying k after removing
    40 percent of the training persons. (b) shows the re-identification accuracy on
    the test data for the chosen values of k=0.1 and 0.2 when 40 percent of the test
    people were not present.
  Figure 8 Link: articels_figures_by_rev_year\2015\Network_Consistent_Data_Association\figure_8.jpg
  Figure 8 caption: Application of online NCDA for the online re-id problem. Mean
    association accuracies with standard deviation are plotted for increasing number
    of observed tracklets in the online setup. The online NCDA maintains a high accuracy
    ( sim 65 percent) even as the number of observed tracklets increases.
  Figure 9 Link: articels_figures_by_rev_year\2015\Network_Consistent_Data_Association\figure_9.jpg
  Figure 9 caption: Effect of NCDA towards improvement of spatio-temporal tracking
    results. (a) The figure shows a spatio-temporal 2X2 block of confocal images.
    Pairwise assignments between cells in spatial or temporal pairs of images are
    obtained by performing MAP inference on graphs formed on every image slice. Infeasible
    4D assignments are observed when these pairwise associations are combined over
    the stack. The solid arrows represent correct associations between cells and the
    broken arrows depict no association which is incorrect and cause the infeasibility.
    Our proposed data association approach establishes consistency in association
    and corrects these errors. (b) Similar results are observed in a 2X3 confocal
    stack.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Anirban Chakraborty
  Name of the last author: Amit K. Roy-Chowdhury
  Number of Figures: 10
  Number of Tables: 0
  Number of authors: 3
  Paper title: Network Consistent Data Association
  Publication Date: 2015-10-16 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2491922
