- Affiliation of the first author: center for future media and school of computer
    science and engineering, university of electronic science and technology of china,
    chengdu, china
  Affiliation of the last author: center for future media and school of computer science
    and engineering, university of electronic science and technology of china, chengdu,
    china
  Figure 1 Link: articels_figures_by_rev_year\2020\Joint_Feature_Synthesis_and_Embedding_Adversarial_CrossModal_Retrieval_Revisited\figure_1.jpg
  Figure 1 caption: 'Illustration of the three different retrieval scenarios: (a)
    standard cross-modal retrieval, where the data in the source set and the target
    set share the same 10 classes; (b) zero-shot cross-modal retrieval, where the
    data in each set are from 5 disjoint classes; and (c) generalized zero-shot cross-modal
    retrieval, where the target set has additional 5 classes that are absent in the
    source set.'
  Figure 10 Link: articels_figures_by_rev_year\2020\Joint_Feature_Synthesis_and_Embedding_Adversarial_CrossModal_Retrieval_Revisited\figure_10.jpg
  Figure 10 caption: A sensitivity analysis of the hyper-parameters of our JFSE approach
    on Wikipedia dataset.
  Figure 2 Link: articels_figures_by_rev_year\2020\Joint_Feature_Synthesis_and_Embedding_Adversarial_CrossModal_Retrieval_Revisited\figure_2.jpg
  Figure 2 caption: The flowchart of our proposed JFSE method. It includes two coupled
    cWGANs that take the class embeddings as guidance to produce meaningful synthetic
    multimodal (e.g., image and text) features for robust training, as well as capture
    cross-modal correlation via distribution alignment under the common embedding
    space. The advanced cycle-consistency constraints further enhances the knowledge
    transfer between heterogeneous data with different classes.
  Figure 3 Link: articels_figures_by_rev_year\2020\Joint_Feature_Synthesis_and_Embedding_Adversarial_CrossModal_Retrieval_Revisited\figure_3.jpg
  Figure 3 caption: Comparison of the adversarial learning used in existing cross-modal
    GAN methods and our JFSE approach.
  Figure 4 Link: articels_figures_by_rev_year\2020\Joint_Feature_Synthesis_and_Embedding_Adversarial_CrossModal_Retrieval_Revisited\figure_4.jpg
  Figure 4 caption: The trainingtest data split of each dataset on the three retrieval
    scenarios, where the arrows with different colors represents the training and
    test procedures for each scenario.
  Figure 5 Link: articels_figures_by_rev_year\2020\Joint_Feature_Synthesis_and_Embedding_Adversarial_CrossModal_Retrieval_Revisited\figure_5.jpg
  Figure 5 caption: The precision-recall (PR) curves of standard retrieval results
    on the Wikipedia and PKU-XMediaNet datasets.
  Figure 6 Link: articels_figures_by_rev_year\2020\Joint_Feature_Synthesis_and_Embedding_Adversarial_CrossModal_Retrieval_Revisited\figure_6.jpg
  Figure 6 caption: Examples of the Img2Txt and Txt2Img retrieval results on PKU-XMediaNet
    dataset by our JFSE approach as well as compared methods DSCMR [38] and ACMR [13].
    In these examples the groundtruth class label for each query is presented for
    instruction. Besides, the true matches and the incorrect retrieval results are
    marked in green and red rectangles, respectively.
  Figure 7 Link: articels_figures_by_rev_year\2020\Joint_Feature_Synthesis_and_Embedding_Adversarial_CrossModal_Retrieval_Revisited\figure_7.jpg
  Figure 7 caption: The t-SNE results for the chosen data on four datasets. Clusters
    with different colors are from different classes.
  Figure 8 Link: articels_figures_by_rev_year\2020\Joint_Feature_Synthesis_and_Embedding_Adversarial_CrossModal_Retrieval_Revisited\figure_8.jpg
  Figure 8 caption: The t-SNE visualization for withwithout adversarial learning of
    JFSE on NUS-WIDE dataset.
  Figure 9 Link: articels_figures_by_rev_year\2020\Joint_Feature_Synthesis_and_Embedding_Adversarial_CrossModal_Retrieval_Revisited\figure_9.jpg
  Figure 9 caption: Experiments of our JFSE method with (a) different dimensions of
    the class embeddings and (b) different ratios of the seen and unseen classes on
    Wikipedia dataset.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Xing Xu
  Name of the last author: Heng Tao Shen
  Number of Figures: 12
  Number of Tables: 7
  Number of authors: 5
  Paper title: 'Joint Feature Synthesis and Embedding: Adversarial Cross-Modal Retrieval
    Revisited'
  Publication Date: 2020-12-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The General Statistics of the Four Datasets Under the Standard
      (top panel) and Zero-Shot Retrieval (bottom panel) Scenarios
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Different Distribution Alignment Schemes on
      All Datasets
  Table 3 caption:
    table_text: TABLE 3 Baseline Experiments for Standard Retrieval on All Datasets
  Table 4 caption:
    table_text: TABLE 4 The MAP Scores of Standard Retrieval for Our JFSE Approach
      and Other Compared Methods on All Datasets
  Table 5 caption:
    table_text: TABLE 5 The MAP Scores of Zero-Shot Retrieval on Seen Classes for
      Our JFSE Approach and Other Compared Methods on All Datasets
  Table 6 caption:
    table_text: TABLE 6 The MAP Scores of Zero-Shot Retrieval on Unseen Classes for
      Our JFSE Approach and Other Compared Methods on All Datasets
  Table 7 caption:
    table_text: TABLE 7 The MAP Scores of Generalized Zero-Shot Retrieval for Our
      JFSE Approach and Other Compared Methods on Wikipedia and Pascal Sentences Datasets
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3045530
- Affiliation of the first author: department of computer science, city university
    of hong kong, kowloon, hong kong
  Affiliation of the last author: flatiron institute of the simons foundation, and
    the center for neural science and the courant institute of mathematical sciences,
    new york university, new york, ny, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Image_Quality_Assessment_Unifying_Structure_and_Texture_Similarity\figure_1.jpg
  Figure 1 caption: Existing full-reference IQA models are overly sensitive to point-by-point
    deviations between images of the same texture. (a) A grass image and (b) the same
    image, distorted by JPEG compression. (c) Resampling of the same grass as in (a).
    Popular IQA measures, including PSNR, SSIM [3], FSIM [11], VIF [4], GMSD [12],
    DeepIQA [13], PieAPP [8], and LPIPS [7], predict that image (b) has a better perceived
    quality than image (c), which is in disagreement with human rating. In contrast,
    the proposed DISTS model makes the correct prediction. (Zoom in to improve visibility
    of details).
  Figure 10 Link: articels_figures_by_rev_year\2020\Image_Quality_Assessment_Unifying_Structure_and_Texture_Similarity\figure_10.jpg
  Figure 10 caption: "Five images of \u201Csoil\u201D, photographed under different\
    \ lighting and viewpoint conditions, from the ALOT dataset. We computed the DISTS\
    \ score for each of the images (b)-(e) with respect to the reference (a). Consistent\
    \ with the significantly higher values, (d) and (e) are visually distinct from\
    \ (a), although all of these images are drawn from the same category."
  Figure 2 Link: articels_figures_by_rev_year\2020\Image_Quality_Assessment_Unifying_Structure_and_Texture_Similarity\figure_2.jpg
  Figure 2 caption: "Recovery of a reference image by optimization of IQA measures.\
    \ Recovery is implemented by solving y \u22C6 = argmin y D(x,y) with gradient\
    \ descent, where D is an IQA distortion measure and x is a given reference image.\
    \ (a) Reference image. (b) Corrupted initial image y 0 , obtained by compressing\
    \ the reference image using JPEG at a low bitrate. (c)-(f) Images recovered from\
    \ (b) by optimizing different metrics (as indicated). (g) Corrupted initial image,\
    \ obtained by adding white Gaussian noise. (h)-(k) Images recovered from (g) by\
    \ optimizing indicated metrics. In all cases, the optimization converges, yielding\
    \ a distortion score substantially lower than that of the initial."
  Figure 3 Link: articels_figures_by_rev_year\2020\Image_Quality_Assessment_Unifying_Structure_and_Texture_Similarity\figure_3.jpg
  Figure 3 caption: Images synthesized to match the mean values of channels up to
    a given layer (top) or from individual layers (bottom) of the pre-trained VGG
    network. (a) Reference texture. (b) Up to conv12 . (c) Up to conv22 . (d) Up to
    conv33 . (e) Up to conv43 . (f) Up to conv53 . (g) Only conv12 . (h) Only conv22
    . (i) Only conv33 . (j) Only conv43 . (k) Only conv53 .
  Figure 4 Link: articels_figures_by_rev_year\2020\Image_Quality_Assessment_Unifying_Structure_and_Texture_Similarity\figure_4.jpg
  Figure 4 caption: Synthesis results for three example texture photographs. (a) Reference
    textures. (b) Images synthesized using the method of Portilla & Simoncelli [15].
    (c) Images synthesized using Gatys et al. [32]. (d) Images synthesized using our
    texture model (Eq. (4)).
  Figure 5 Link: articels_figures_by_rev_year\2020\Image_Quality_Assessment_Unifying_Structure_and_Texture_Similarity\figure_5.jpg
  Figure 5 caption: "Selected feature maps from the six layers of the VGG decomposition\
    \ of the \u201Cbuildings\u201D image. (a) Zeroth stage (original image). (b) First\
    \ stage. (c) Second stage. (d) Third stage. (e) Fourth stage. (f) Fifth stage.\
    \ The feature map intensities are re-scaled for better visibility."
  Figure 6 Link: articels_figures_by_rev_year\2020\Image_Quality_Assessment_Unifying_Structure_and_Texture_Similarity\figure_6.jpg
  Figure 6 caption: VGG-based perceptual representation for the proposed DISTS model.
    It contains a total of six stages (including the zeroth stage of raw pixels),
    and the numbers of feature maps at each stage are 3, 64, 128, 256, 512 and 512,
    respectively. Global texture and structure similarity measurements are made at
    each stage, and combined with a weighted summation, giving rise to the final model
    defined in Eq. (7).
  Figure 7 Link: articels_figures_by_rev_year\2020\Image_Quality_Assessment_Unifying_Structure_and_Texture_Similarity\figure_7.jpg
  Figure 7 caption: Comparison of human mean opinion scores (MOSs) against SSIM, FSIM
    c , VSI, and DISTS (ours) on the TID2013 database.
  Figure 8 Link: articels_figures_by_rev_year\2020\Image_Quality_Assessment_Unifying_Structure_and_Texture_Similarity\figure_8.jpg
  Figure 8 caption: One set of texture images from TQD, ordered according to their
    rankings by DISTS. (a) Reference image. (b)-(p) Corrupted images ranked by DISTS
    from high quality to low quality, respectively.
  Figure 9 Link: articels_figures_by_rev_year\2020\Image_Quality_Assessment_Unifying_Structure_and_Texture_Similarity\figure_9.jpg
  Figure 9 caption: Nine non-overlapping patches sampled from an example texture photograph
    in the Brodatz color texture dataset.
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Keyan Ding
  Name of the last author: Eero P. Simoncelli
  Number of Figures: 11
  Number of Tables: 6
  Number of authors: 4
  Paper title: 'Image Quality Assessment: Unifying Structure and Texture Similarity'
  Publication Date: 2020-12-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance Comparison on Three Standard IQA Databases
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance Comparison of Various IQA Methods on the BAPPS
      [7] Dataset Using the 2AFC Score, Which Quantifies the Agreement With Human
      Judgments
  Table 3 caption:
    table_text: TABLE 3 Performance Comparison on Two Texture Quality Databases
  Table 4 caption:
    table_text: TABLE 4 Classification and Retrieval Performance Comparison on the
      Brodatz Texture Dataset [75]
  Table 5 caption:
    table_text: TABLE 5 SRCC Comparison of IQA Models to Human Perception Using the
      LIVE Database Augmented With Geometric Transformations
  Table 6 caption:
    table_text: 'TABLE 6 Ablation Experiments: Proposed DISTS Model (Last Line) Compared
      to LPIPS (First Line), and Intermediate Variations'
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3045810
- Affiliation of the first author: "department of information technology and electrical\
    \ engineering, eth z\xFCrich, z\xFCrich, switzerland"
  Affiliation of the last author: "department of information technology and electrical\
    \ engineering, eth z\xFCrich, z\xFCrich, switzerland"
  Figure 1 Link: articels_figures_by_rev_year\2020\MapGuided_Curriculum_Domain_Adaptation_and_UncertaintyAware_Evaluation_for_Seman\figure_1.jpg
  Figure 1 caption: A general overview of our MGCDA method for adaptation to nighttime.
    FT stands for fine-tuning. Red arrows denote training of a model, while gray arrows
    denote generation of predictions. Inference with the final nighttime model (bottom
    right) does not require guided refinement.
  Figure 10 Link: articels_figures_by_rev_year\2020\MapGuided_Curriculum_Domain_Adaptation_and_UncertaintyAware_Evaluation_for_Seman\figure_10.jpg
  Figure 10 caption: Qualitative comparison on Dark Zurich-test of MGCDA without and
    with map guidance at test time. TG stands for test-time guidance.
  Figure 2 Link: articels_figures_by_rev_year\2020\MapGuided_Curriculum_Domain_Adaptation_and_UncertaintyAware_Evaluation_for_Seman\figure_2.jpg
  Figure 2 caption: Sample images from the training sets used in MGCDA.
  Figure 3 Link: articels_figures_by_rev_year\2020\MapGuided_Curriculum_Domain_Adaptation_and_UncertaintyAware_Evaluation_for_Seman\figure_3.jpg
  Figure 3 caption: Illustration of two components of MGCDA involving the identification
    of cross-time-of-day correspondences at image level and pixel level.
  Figure 4 Link: articels_figures_by_rev_year\2020\MapGuided_Curriculum_Domain_Adaptation_and_UncertaintyAware_Evaluation_for_Seman\figure_4.jpg
  Figure 4 caption: Comparison of guided segmentation refinement using prediction
    alignment with cross bilateral filter versus depth-based warping on an example
    pair of corresponding images from Dark Zurich. Best viewed on a screen with zoom.
  Figure 5 Link: articels_figures_by_rev_year\2020\MapGuided_Curriculum_Domain_Adaptation_and_UncertaintyAware_Evaluation_for_Seman\figure_5.jpg
  Figure 5 caption: Example input images from Dark Zurich-test and output annotations
    with our protocol. Valid pixels in J are marked green.
  Figure 6 Link: articels_figures_by_rev_year\2020\MapGuided_Curriculum_Domain_Adaptation_and_UncertaintyAware_Evaluation_for_Seman\figure_6.jpg
  Figure 6 caption: Number of annotated pixels per class in Dark Zurich.
  Figure 7 Link: articels_figures_by_rev_year\2020\MapGuided_Curriculum_Domain_Adaptation_and_UncertaintyAware_Evaluation_for_Seman\figure_7.jpg
  Figure 7 caption: "Qualitative semantic segmentation results on Dark Zurich-test.\
    \ \u201CAdaptSegNet\u201D adapts from Cityscapes to Dark Zurich-night."
  Figure 8 Link: articels_figures_by_rev_year\2020\MapGuided_Curriculum_Domain_Adaptation_and_UncertaintyAware_Evaluation_for_Seman\figure_8.jpg
  Figure 8 caption: "Qualitative semantic segmentation results on BDD100K-night. \u201C\
    BDL\u201D adapts from Cityscapes to Dark Zurich-night."
  Figure 9 Link: articels_figures_by_rev_year\2020\MapGuided_Curriculum_Domain_Adaptation_and_UncertaintyAware_Evaluation_for_Seman\figure_9.jpg
  Figure 9 caption: 'Comparison of CycleGAN configurations for generation of synthetic
    stylized data from Cityscapes and Dark Zurich. From left to right: input Cityscapes
    image, stylized nighttime image with training on 256times 256 crops, stylized
    nighttime image with training on full 360times 720 images.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Christos Sakaridis
  Name of the last author: Luc Van Gool
  Number of Figures: 12
  Number of Tables: 9
  Number of authors: 3
  Paper title: Map-Guided Curriculum Domain Adaptation and Uncertainty-Aware Evaluation
    for Semantic Nighttime Image Segmentation
  Publication Date: 2020-12-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Training Sets Used in MGCDA
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Dark Zurich Against Related Datasets With Nighttime
      Semantic Annotations
  Table 3 caption:
    table_text: TABLE 3 Performance Comparison of Our Method With State-of-the-Art
      Approaches and Daytime-Trained Baselines on Our Dark Zurich-Test Dataset
  Table 4 caption:
    table_text: TABLE 4 Performance Comparison of Our Method With State-of-the-Art
      Approaches and Daytime-Trained Baselines on Nighttime Driving [11]
  Table 5 caption:
    table_text: TABLE 5 Performance Comparison of Our Method With State-of-the-Art
      Approaches and Daytime-Trained Baselines on BDD100K-Night [75]
  Table 6 caption:
    table_text: TABLE 6 Comparison on Dark Zurich-Test of CycleGAN Configurations
      for Generation of Synthetic Stylized Data to Adapt to Nighttime
  Table 7 caption:
    table_text: TABLE 7 Ablation Study of the Components of MGCDA on Dark Zurich-Test,
      Reporting mIoU (%)
  Table 8 caption:
    table_text: TABLE 8 Comparison on Dark Zurich-Test Focusing on the Usage of Map
      Guidance at Test Time and Reporting mIoU (%)
  Table 9 caption:
    table_text: TABLE 9 Comparison on Dark Zurich-Test of Different Preprocessing
      Baselines, Using the Daytime RefineNet [4] Model for Predictions and Reporting
      mIoU (%)
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3045882
- Affiliation of the first author: "university c\xF4te dazur, route des lucioles,\
    \ france"
  Affiliation of the last author: "university c\xF4te dazur, route des lucioles, france"
  Figure 1 Link: articels_figures_by_rev_year\2020\Discrete_BoxConstrained_Minimax_Classifier_for_Uncertain_and_Imbalanced_Class_Pr\figure_1.jpg
  Figure 1 caption: "Comparison between the empirical Bayes classifier \u03B4 B \u03C0\
    \ , the minimax classifier \u03B4 B \u03C0 \xAF and the box-constrained minimax\
    \ classifier \u03B4 B \u03C0\u22C6 . These results come from a synthetic dataset\
    \ for which K=2 classes. The generation of this dataset is detailed in Appendix\
    \ A.1, available in the online supplemental material."
  Figure 10 Link: articels_figures_by_rev_year\2020\Discrete_BoxConstrained_Minimax_Classifier_for_Uncertain_and_Imbalanced_Class_Pr\figure_10.jpg
  Figure 10 caption: 'Framingham database: On the top, the boxplots illustrate the
    dispersion of the global risks of misclassification [hatr(pi (1),delta),ldots,hatr(pi
    (100),delta)] associated to each classifier delta in Delta E when prior probability
    shifts occur over mathbb U . On the bottom, the boxplots correspond to the dispersion
    of the conditional risks [hatRk(delta),ldots,hatRk(delta)] associated to each
    class lbrace textC1, C2rbrace of each classifier delta when dealing with these
    prior probability shifts.'
  Figure 2 Link: articels_figures_by_rev_year\2020\Discrete_BoxConstrained_Minimax_Classifier_for_Uncertain_and_Imbalanced_Class_Pr\figure_2.jpg
  Figure 2 caption: "Framingham database: choice of T from the training set in the\
    \ first iteration of the 4-fold cross-validation procedure, and when considering\
    \ \u03B5=0.01 . The dashed curves show the standard-deviation around the mean\
    \ of r ( \u03C0 , \u03B4 B \u03C0 ) ."
  Figure 3 Link: articels_figures_by_rev_year\2020\Discrete_BoxConstrained_Minimax_Classifier_for_Uncertain_and_Imbalanced_Class_Pr\figure_3.jpg
  Figure 3 caption: "Pie plots corresponding to the priors \u03C0 , \u03C0 \u22C6\
    \ and \u03C0 \xAF associated with each databases. These results correspond to\
    \ the average of the computed priors at each iteration of the 4-folds cross-validation\
    \ procedure."
  Figure 4 Link: articels_figures_by_rev_year\2020\Discrete_BoxConstrained_Minimax_Classifier_for_Uncertain_and_Imbalanced_Class_Pr\figure_4.jpg
  Figure 4 caption: Flowchart of the box-constrained minimax classifier delta pi star
    mathrmB which includes both the discretization and the training steps.
  Figure 5 Link: articels_figures_by_rev_year\2020\Discrete_BoxConstrained_Minimax_Classifier_for_Uncertain_and_Imbalanced_Class_Pr\figure_5.jpg
  Figure 5 caption: 'Synthetic database: Scatter plot of the instances from each class
    lbrace textC1, C3, C3rbrace . The database generation is described in Appendix
    A.2, available in the online supplemental material.'
  Figure 6 Link: articels_figures_by_rev_year\2020\Discrete_BoxConstrained_Minimax_Classifier_for_Uncertain_and_Imbalanced_Class_Pr\figure_6.jpg
  Figure 6 caption: 'Synthetic database: Impact of the priors on the class-label assignment
    to each discrete profile for the classifiers delta hatpi B , delta pi star B and
    delta barpi B .'
  Figure 7 Link: articels_figures_by_rev_year\2020\Discrete_BoxConstrained_Minimax_Classifier_for_Uncertain_and_Imbalanced_Class_Pr\figure_7.jpg
  Figure 7 caption: 'Synthetic database: Comparison of the risks of misclassification
    after the 4-fold cross-validation procedure for which the class proportions of
    the test set were similar to the training set. On the top, the boxplots (training
    versus test) illustrate the dispersion of the global risks of misclassification.
    On the bottom, the barplots correspond to the average conditional risks associated
    to each class lbrace textC1, C2, C3rbrace for each classifier.'
  Figure 8 Link: articels_figures_by_rev_year\2020\Discrete_BoxConstrained_Minimax_Classifier_for_Uncertain_and_Imbalanced_Class_Pr\figure_8.jpg
  Figure 8 caption: 'Synthetic database: On the top, the boxplots illustrate the dispersion
    of the global risks of misclassification [hatr(pi (1),delta),ldots,hatr(pi (100),delta)]
    associated to each classifier delta in Delta E when prior probability shifts occur
    over mathbb U . On the bottom, the boxplots correspond to the dispersion of the
    conditional risks [hatRk(delta),ldots,hatRk(delta)] associated to each class lbrace
    textC1, C2, C3rbrace of each classifier delta when dealing with these prior probability
    shifts.'
  Figure 9 Link: articels_figures_by_rev_year\2020\Discrete_BoxConstrained_Minimax_Classifier_for_Uncertain_and_Imbalanced_Class_Pr\figure_9.jpg
  Figure 9 caption: 'Framingham database: Impact of the box-constraint radius on delta
    pi star mathrmB when beta increases from 0 to 1 in (28), after a 4-fold cross-validation
    procedure. As beta increases, the box-constraint radius increases which changes
    the values of pi star , and therefore the values of V(pi star ) and psi (delta
    pi star mathrmB) . Results are presented as mathrmmeanpm mathrmstd .'
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Cyprien Gilet
  Name of the last author: Lionel Fillatre
  Number of Figures: 11
  Number of Tables: 2
  Number of authors: 3
  Paper title: Discrete Box-Constrained Minimax Classifier for Uncertain and Imbalanced
    Class Proportions
  Publication Date: 2020-12-22 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Overview on Each Database
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Results Associated With Each Classifier \u03B4\u2208 \u0394\
      \ E \u03B4\u2208\u0394E and Each Database After the Four-Fold Cross-Validation\
      \ Procedure"
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3046439
- Affiliation of the first author: "departamento de inteligencia artificial, universidad\
    \ polit\xE9cnica de madrid, boadilla del monte, spain"
  Affiliation of the last author: "departamento de inteligencia artificial, universidad\
    \ polit\xE9cnica de madrid, boadilla del monte, spain"
  Figure 1 Link: articels_figures_by_rev_year\2020\MultiTask_Head_Pose_Estimation_intheWild\figure_1.jpg
  Figure 1 caption: Simultaneous head pose estimation, facial landmark location and
    their visibility predictions when processing a video from 300VW [14]. Green and
    red points show visible and non-visible landmarks respectively. The co-ordinate
    system qualitatively represents head pose.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\MultiTask_Head_Pose_Estimation_intheWild\figure_2.jpg
  Figure 2 caption: Multi-task encoder-decoder for the estimation of head pose, L
    ED ( p ED ) , rigid and deformable facial landmarks location, L AE ( p AE ) and
    L CE (h) , and their visibilities, L CE (v) . We locate the head pose and rigid
    landmarks estimation tasks at the bottleneck layer, and the non-rigid face deformation
    and visibilities at the decoder end.
  Figure 3 Link: articels_figures_by_rev_year\2020\MultiTask_Head_Pose_Estimation_intheWild\figure_3.jpg
  Figure 3 caption: The OR is initialized with the 3D face model projected landmarks,
    x 0 , and their visibilities, v . It incrementally updates the landmark location
    discarding the predictions of those regression trees whose features are extracted
    around occluded landmarks, shown in red.
  Figure 4 Link: articels_figures_by_rev_year\2020\MultiTask_Head_Pose_Estimation_intheWild\figure_4.jpg
  Figure 4 caption: Blue, orange and green colored learning curves compare the overall
    validation loss, L , obtained with MNN by fine-tuning from landmarks, training
    from scratch, and locating the rigid pose losses at the end of the decoder respectively.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Roberto Valle
  Name of the last author: Luis Baumela
  Number of Figures: 4
  Number of Tables: 6
  Number of authors: 3
  Paper title: Multi-Task Head Pose Estimation in-the-Wild
  Publication Date: 2020-12-22 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Head Pose Mean MAEs for Different Training Strategies
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Head Pose MAEs for AFLW, AFLW2000-3D and Biwi
  Table 3 caption:
    table_text: TABLE 3 Recall of Landmarks Visibility Estimation Methods at 80% Precision
      Using COFW
  Table 4 caption:
    table_text: TABLE 4 Face Alignment NME Using COFW
  Table 5 caption:
    table_text: TABLE 5 Face Alignment NME using AFLW
  Table 6 caption:
    table_text: TABLE 6 Face Alignment NME Using AFLW2000-3D
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3046323
- Affiliation of the first author: school of data science, fudan university, shanghai,
    china
  Affiliation of the last author: school of data science, fudan university, shanghai,
    china
  Figure 1 Link: articels_figures_by_rev_year\2020\RankOne_Network_An_Effective_Framework_for_Image_Restoration\figure_1.jpg
  Figure 1 caption: Visualization of image decomposition obtained by our unsupervised
    RO decomposition. The top row shows that an noisy image could be decomposed into
    the sum of the almost noise-free low-rank part and the noisy residual error. We
    visualize the surfaces of their gray images at the bottom row, which shows that
    the low-rank component is more self-similar than the original image. One can refer
    to the text in Introduction section for details.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\RankOne_Network_An_Effective_Framework_for_Image_Restoration\figure_2.jpg
  Figure 2 caption: Pipelines of the proposed method. Here, RODec denotes the main
    body of our RO decomposition network, which is pre-trained by the loss L unsup
    Dec in (19), RecRos, RecRes, RecFus, and Upsampling (only for image super-resolution)
    consist of the main body of our RO reconstruction network, which is trained by
    the loss L Rec in (18). RecROs, RecRes, and RecFus are aimed at reconstructing
    RO components, restoring residual, and fusing the concatenation, respectively.
    They share the similar architecture but have different depths. ROP network is
    illustrated in Fig. 3.
  Figure 3 Link: articels_figures_by_rev_year\2020\RankOne_Network_An_Effective_Framework_for_Image_Restoration\figure_3.jpg
  Figure 3 caption: The architecture of the rank-one projection network (ROPNet),
    which is aimed at projecting the input to be an RO matrix.
  Figure 4 Link: articels_figures_by_rev_year\2020\RankOne_Network_An_Effective_Framework_for_Image_Restoration\figure_4.jpg
  Figure 4 caption: Visualization of RO decomposition by UROD-C, SROD-C, and SVD.
    Here, the noise level of X is 50.
  Figure 5 Link: articels_figures_by_rev_year\2020\RankOne_Network_An_Effective_Framework_for_Image_Restoration\figure_5.jpg
  Figure 5 caption: "Visualization of noise-free image SR: three typical examples\
    \ (cropped patches of size 80\xD780 ) from the four datasets. Please refer to\
    \ Supplementary Material, which can be found on the Computer Society Digital Library\
    \ at http:doi.ieeecomputersociety.org10.1109TPAMI.2020.3046476, for high-resolution\
    \ images."
  Figure 6 Link: articels_figures_by_rev_year\2020\RankOne_Network_An_Effective_Framework_for_Image_Restoration\figure_6.jpg
  Figure 6 caption: "Visualization of realistic image SR: four typical examples (cropped\
    \ patches of size 80\xD780 ). Please refer to Supplementary Material, available\
    \ online, for high-resolution images."
  Figure 7 Link: articels_figures_by_rev_year\2020\RankOne_Network_An_Effective_Framework_for_Image_Restoration\figure_7.jpg
  Figure 7 caption: "Visualization of gray-scale image denoising: three typical examples\
    \ (cropped patches of size 80\xD780 ) from Set12. Please refer to Supplementary\
    \ Material, available online, for high-resolution images."
  Figure 8 Link: articels_figures_by_rev_year\2020\RankOne_Network_An_Effective_Framework_for_Image_Restoration\figure_8.jpg
  Figure 8 caption: "Visualization of color image denoising: three typical examples\
    \ (cropped patches of size 80\xD780 ) from McMaster. Please refer to Supplementary\
    \ Material, available online, for high-resolution images."
  Figure 9 Link: articels_figures_by_rev_year\2020\RankOne_Network_An_Effective_Framework_for_Image_Restoration\figure_9.jpg
  Figure 9 caption: PSNR of the RO components for CBM3D, CDnCNN, FFDNet, and RONet-C
    when the noise level is 50.
  First author gender probability: 0.95
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Shangqi Gao
  Name of the last author: Xiahai Zhuang
  Number of Figures: 9
  Number of Tables: 7
  Number of authors: 2
  Paper title: 'Rank-One Network: An Effective Framework for Image Restoration'
  Publication Date: 2020-12-22 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summarization of the Notions and Notations Used in the Methodology
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Evaluation of RO Decomposition by the Unsupervised RO Decomposition
      (UROD), Supervised RO Decomposition (SROD) and SVD
  Table 3 caption:
    table_text: 'TABLE 3 Results of Ablation Studies: RONet for Realistic Image SR
      With Different Settings, Including Whether Train it in an End-to-End Fashion,
      Different RO Decompositions, the Number of RO Projections (ROP), the Number
      of Residual Blocks (ResBlocks), and Whether Use Batch Normalization (BN)'
  Table 4 caption:
    table_text: 'TABLE 4 Results of Noise-Free Image SR: Including SRCNN [7], EDSR
      [34], EnhanceNet [40], SRGAN [8], ESRGAN [35], OISR [60], and RONet'
  Table 5 caption:
    table_text: 'TABLE 5 Results of Realistic Image SR: Including EDSR [34], WDSR
      [42] and RONet-R'
  Table 6 caption:
    table_text: TABLE 6 The Quantitative Evaluation of BM3D [21], WNNW [23], TNRD
      [32], DnCNN [5], FFDNet [6], N 3 3Net [37], and RONet-G on the Task of Gray-Scale
      Image Denoising
  Table 7 caption:
    table_text: 'TABLE 7 Results of Color Image Denoising: CBM3D [21], CDnCNN [5],
      FFDNet [6], and RONet-C'
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3046476
- Affiliation of the first author: "department of industrial engineering, universit\xE0\
    \ degli studi di napoli \u201Cfederico ii\u201D, napoli, italy"
  Affiliation of the last author: "department of industrial engineering, universit\xE0\
    \ degli studi di napoli \u201Cfederico ii\u201D, napoli, italy"
  Figure 1 Link: articels_figures_by_rev_year\2020\Perspective_Camera_Model_With_Refraction_Correction_for_Optical_Velocimetry_Meas\figure_1.jpg
  Figure 1 caption: Perspective projection model with inclusion of a transparent body.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Perspective_Camera_Model_With_Refraction_Correction_for_Optical_Velocimetry_Meas\figure_2.jpg
  Figure 2 caption: Definition of the ray and image distortions.
  Figure 3 Link: articels_figures_by_rev_year\2020\Perspective_Camera_Model_With_Refraction_Correction_for_Optical_Velocimetry_Meas\figure_3.jpg
  Figure 3 caption: Detailed schematic of the refraction of the guess LOS C A k across
    the body wall for computation of the optical ray distortion of the object point
    P . Distortions are exaggerated for clarity.
  Figure 4 Link: articels_figures_by_rev_year\2020\Perspective_Camera_Model_With_Refraction_Correction_for_Optical_Velocimetry_Meas\figure_4.jpg
  Figure 4 caption: Occurrence of complex and shadow regions in imaging from air to
    water through a PMMA cylindrical wall.
  Figure 5 Link: articels_figures_by_rev_year\2020\Perspective_Camera_Model_With_Refraction_Correction_for_Optical_Velocimetry_Meas\figure_5.jpg
  Figure 5 caption: "Calibration results from numerical simulations of a PMMA cylinder\
    \ surrounded by water for different control point grids and camera bundles (single\
    \ camera versus 4-cameras bundle): (a) behavior of true modeling error \u03B5\
    \ true (solid lines) and calibration reprojection error \u03B5 calib (dashed lines)\
    \ as a function of the Gaussian noise; (b-f) estimated values of the cylinder\
    \ parameters, respectively: origin shift y 0 b , axis angle \u03B1 b , internal\
    \ radius r i , wall thickness \u0394r and refractive index ratio \u03F1 . The\
    \ green lines represent the exact parameter values."
  Figure 6 Link: articels_figures_by_rev_year\2020\Perspective_Camera_Model_With_Refraction_Correction_for_Optical_Velocimetry_Meas\figure_6.jpg
  Figure 6 caption: Schematic of the experimental apparatus showing the configuration
    of cameras and refractive bodies.
  Figure 7 Link: articels_figures_by_rev_year\2020\Perspective_Camera_Model_With_Refraction_Correction_for_Optical_Velocimetry_Meas\figure_7.jpg
  Figure 7 caption: Comparative tomographic reconstructions of the instantaneous velocity
    field obtained with (a) classical pinhole camera model and (b) refractive camera
    model. Isosurfaces of the x -velocity component. Velocity values are in mms, coordinates
    in mm.
  Figure 8 Link: articels_figures_by_rev_year\2020\Perspective_Camera_Model_With_Refraction_Correction_for_Optical_Velocimetry_Meas\figure_8.jpg
  Figure 8 caption: Comparative tomographic reconstructions obtained with (a) classical
    pinhole camera model and (b) refractive camera model. Slices of the time-averaged
    field of the z -velocity component fluctuation at the xy and xz planes. Velocity
    values are in mms, coordinates in mm.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Gerardo Paolillo
  Name of the last author: Tommaso Astarita
  Number of Figures: 8
  Number of Tables: 2
  Number of authors: 2
  Paper title: Perspective Camera Model With Refraction Correction for Optical Velocimetry
    Measurements in Complex Geometries
  Publication Date: 2020-12-22 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Convergence Rates of Iterative Methods for (15)
      and AFRT Procedure [8] in Terms of the Average Number of Computations of the
      Optical Paths Per Point
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Calibration Reprojection Errors and Estimates of Some Parameters
      of the Refractive Camera Model With the Tank Windows and the Investigated Cylinder
      Through the Different Steps of the Experimental Calibration Procedure
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3046467
- Affiliation of the first author: research school of electrical, energy and materials
    engineering, australian national university, canberra, act, australia
  Affiliation of the last author: research school of electrical, energy and materials
    engineering, australian national university, canberra, act, australia
  Figure 1 Link: articels_figures_by_rev_year\2020\Learning_Saliency_From_Single_Noisy_Labelling_A_Robust_Model_Fitting_Perspective\figure_1.jpg
  Figure 1 caption: 'First row: input image, its ground truth saliency map, noisy
    saliency map by RBD [66] and saliency map by MNL [61]. Second and third rows show
    the early, middle and final iterations masks (or weights) and saliency maps of
    our hard mask selection and soft mask reweigting methods, respectively.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Learning_Saliency_From_Single_Noisy_Labelling_A_Robust_Model_Fitting_Perspective\figure_2.jpg
  Figure 2 caption: Performance evaluation (Mean Absolute Error) of saliency prediction
    by training a same network with different levels of supervision across eight different
    datasets.
  Figure 3 Link: articels_figures_by_rev_year\2020\Learning_Saliency_From_Single_Noisy_Labelling_A_Robust_Model_Fitting_Perspective\figure_3.jpg
  Figure 3 caption: Conceptual illustration of our framework. We start with noisy
    pseudo label. By iteratively updating a dense mask for partial supervision with
    the noisy labeling, we can effectively identify the inlier labels inside the noisy
    labels.
  Figure 4 Link: articels_figures_by_rev_year\2020\Learning_Saliency_From_Single_Noisy_Labelling_A_Robust_Model_Fitting_Perspective\figure_4.jpg
  Figure 4 caption: F-measure and E-measure curves on four benchmark datasets (ECSSD,
    DUT, HKU-IS and THUR). Best Viewed on screen.
  Figure 5 Link: articels_figures_by_rev_year\2020\Learning_Saliency_From_Single_Noisy_Labelling_A_Robust_Model_Fitting_Perspective\figure_5.jpg
  Figure 5 caption: Comparison of saliency maps with competing methods.
  Figure 6 Link: articels_figures_by_rev_year\2020\Learning_Saliency_From_Single_Noisy_Labelling_A_Robust_Model_Fitting_Perspective\figure_6.jpg
  Figure 6 caption: Model performance with regard to different lambda in equation
    (7).
  Figure 7 Link: articels_figures_by_rev_year\2020\Learning_Saliency_From_Single_Noisy_Labelling_A_Robust_Model_Fitting_Perspective\figure_7.jpg
  Figure 7 caption: Model performance with regard to different k in equation (8).
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.76
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jing Zhang
  Name of the last author: Richard Hartley
  Number of Figures: 7
  Number of Tables: 2
  Number of authors: 6
  Paper title: 'Learning Saliency From Single Noisy Labelling: A Robust Model Fitting
    Perspective'
  Publication Date: 2020-12-22 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Benchmarking Results
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Assumption Validation and Ablation Study
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3046486
- Affiliation of the first author: department of computer science and engineering,
    shanghai jiao tong university, shanghai, china
  Affiliation of the last author: department of computer science and engineering,
    shanghai jiao tong university, shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2020\SGNet_Syntax_Guided_Transformer_for_Language_Representation\figure_1.jpg
  Figure 1 caption: (a) Example of syntax-guided span-based QA. The SDOI of each word
    consists of all its ancestor words. (b-c) The dependency parsing tree of the given
    passage sentence and question.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\SGNet_Syntax_Guided_Transformer_for_Language_Representation\figure_2.jpg
  Figure 2 caption: Example of dependency formats.
  Figure 3 Link: articels_figures_by_rev_year\2020\SGNet_Syntax_Guided_Transformer_for_Language_Representation\figure_3.jpg
  Figure 3 caption: Overview of the syntax-guided network.
  Figure 4 Link: articels_figures_by_rev_year\2020\SGNet_Syntax_Guided_Transformer_for_Language_Representation\figure_4.jpg
  Figure 4 caption: The input formulation for the span-based and multi-choice reading
    comprehension tasks.
  Figure 5 Link: articels_figures_by_rev_year\2020\SGNet_Syntax_Guided_Transformer_for_Language_Representation\figure_5.jpg
  Figure 5 caption: Accuracy for different question length. Each data point means
    the accuracy for the questions in the same length range (a) or of the same number
    (b) and the horizontal axis in (b) shows that most of questions are of length
    7-8 and 9-10.
  Figure 6 Link: articels_figures_by_rev_year\2020\SGNet_Syntax_Guided_Transformer_for_Language_Representation\figure_6.jpg
  Figure 6 caption: Translation qualities of different sentence lengths. Each data
    point means the BLEU score in (a) word-level or (b) subword-level.
  Figure 7 Link: articels_figures_by_rev_year\2020\SGNet_Syntax_Guided_Transformer_for_Language_Representation\figure_7.jpg
  Figure 7 caption: Visualization of the vanilla BERT attention (left) and syntax-guided
    self-attention (right). Weights of attention are selected from first head of the
    last attention layer. For the syntax-guided self-attention, the columns with weights
    represent the SDOI for each word in the row. For example, the SDOI of passed contains
    name, of, legislation, passed. Weights are normalized by SoftMax for each row.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Zhuosheng Zhang
  Name of the last author: Rui Wang
  Number of Figures: 7
  Number of Tables: 9
  Number of authors: 6
  Paper title: 'SG-Net: Syntax Guided Transformer for Language Representation'
  Publication Date: 2020-12-22 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Our Notations and Symbols
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Exact Match (EM) and F1 Scores (%) on SQuAD 2.0 Dataset for
      Single Models
  Table 3 caption:
    table_text: TABLE 3 Accuracy (%) on RACE Test Set for Single Models
  Table 4 caption:
    table_text: TABLE 4 Accuracy on the SNLI Test Set
  Table 5 caption:
    table_text: TABLE 5 BLEU Scores on EN-DE Dataset for the NMT Tasks
  Table 6 caption:
    table_text: TABLE 6 Ablation Study on Potential Components and Aggregation Methods
      on SQuAD 2.0 Dev Set
  Table 7 caption:
    table_text: TABLE 7 Eight Probing Tasks [82] to Study What Kind of Properties
      are Captured by the Encoders
  Table 8 caption:
    table_text: TABLE 8 Accuracy on Eight Probing Tasks of Evaluating Linguistics
      Embedded in the Encoder Outputs
  Table 9 caption:
    table_text: TABLE 9 The Comparison of Answers From Baseline and Our Model
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3046683
- Affiliation of the first author: brain and artificial intelligence laboratory, school
    of automation, northwestern polytechnical university, xian, china
  Affiliation of the last author: brain and artificial intelligence laboratory, school
    of automation, northwestern polytechnical university, xian, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Weakly_Supervised_Object_Detection_Using_Proposal_and_SemanticLevel_Relationship\figure_1.jpg
  Figure 1 caption: An example to illustrate the effect of using helpful context relationships
    for reasoning the object location and semantic existence. Bounding boxes containing
    different object categories are drawn in different colors. Multi-label categorization
    scores of each image are shown in the bottom, where the correct categorization
    scores are marked in red while the incorrect ones are marked in blue. The result
    shown on the left side is obtained by a conventional deep multiple instance learning
    (MIL) model [3] while the result shown on the right side is obtained by the deep
    multiple instance reasoning method which is presented in this paper. Due to the
    miss detection of person regions, the conventional deep MIL approach tends to
    obtain the wrong prediction score on the person category. On the contrary, by
    using the proposal- and semantic-level relationships to implement the reasoning
    process in MIL, our approach can improve both the detection results and categorization
    results.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Weakly_Supervised_Object_Detection_Using_Proposal_and_SemanticLevel_Relationship\figure_2.jpg
  Figure 2 caption: Illustration of the classic multiple instance learning framework
    widely used in the existing weakly supervised object detection methods (the left
    part) and the proposed multiple instance reasoning framework (the right part).
    As the feature of each individual object proposal region has been validated to
    work quite well for recognizing the proposal content, here we simply adopt the
    commonly-used fully connected layer in the classification branch by considering
    the computational complexity.
  Figure 3 Link: articels_figures_by_rev_year\2020\Weakly_Supervised_Object_Detection_Using_Proposal_and_SemanticLevel_Relationship\figure_3.jpg
  Figure 3 caption: Illustration of the proposed deep multiple instance reasoning
    framework. Besides the conventional network branches for feature extraction, classification
    mapping, and online instance classier refinement, it contains two novel branches
    (marked in blue) to leverage the proposal adjacent relationship and the semantic
    co-occurring relationship to identify the object location and semantic existence,
    respectively. Notice that the green arrows indicate the data inputs while the
    red arrows indicate the supervision information. In testing, the online instance
    classifier refinement branch outputs the detection results.
  Figure 4 Link: articels_figures_by_rev_year\2020\Weakly_Supervised_Object_Detection_Using_Proposal_and_SemanticLevel_Relationship\figure_4.jpg
  Figure 4 caption: Some examples of the detection results of the proposed approach
    on the PASCAL VOC 2007 test set, PASCAL VOC 2012 test set, and COCO 2014 validation
    set, where the yellow boxes are the ground-truth boxes while the green boxes are
    the predicted results. Notice that the ground-truth boxes are unacquirable for
    the examples from the PASCAL VOC 2012 test set.
  Figure 5 Link: articels_figures_by_rev_year\2020\Weakly_Supervised_Object_Detection_Using_Proposal_and_SemanticLevel_Relationship\figure_5.jpg
  Figure 5 caption: Some examples of the failure cases of MELM, PCL, and the proposed
    approach. Examples in the left three columns are the failure cases of MELM and
    PCL, while those in the right three columns are the failure cases of our approaches.
    In this figure, yellow boxes indicate the ground-truth object locations while
    the green boxes indicate the predicted object locations.
  Figure 6 Link: articels_figures_by_rev_year\2020\Weakly_Supervised_Object_Detection_Using_Proposal_and_SemanticLevel_Relationship\figure_6.jpg
  Figure 6 caption: Visual comparisons of the detection results obtained by different
    baseline methods and the proposed approach (Base+GCLR+GCMLR) on the VOC 2007 test
    set, where the yellow boxes are the ground-truth boxes while the green boxes are
    the predicted results.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.67
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: Dingwen Zhang
  Name of the last author: Junwei Han
  Number of Figures: 6
  Number of Tables: 6
  Number of authors: 4
  Paper title: Weakly Supervised Object Detection Using Proposal- and Semantic-Level
    Relationships
  Publication Date: 2020-12-22 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of the Detection Performance of the Proposed Approach
      and Other State-of-the-Art Methods on the VOC 2007 Test Set
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of the Localization Performance of the Proposed
      Approach and State-of-the-Art Methods on the VOC 2007 Trainval Set
  Table 3 caption:
    table_text: TABLE 3 Comparison of the Detection and Localization Performance of
      the Proposed Approach and Other State-of-the-Art Methods on the VOC 2012 Test
      Set and Trainval Set
  Table 4 caption:
    table_text: TABLE 4 Comparison of the Detection Performance of the Proposed Approach
      and Other State-of-the-Art Methods on the MS COCO Datasets
  Table 5 caption:
    table_text: TABLE 5 Ablation Study of the Detection Performance of the Proposed
      Approach on the VOC 2007 Test Set
  Table 6 caption:
    table_text: TABLE 6 Ablation Study of the Localization Performance of the Proposed
      Approach on the VOC 2007 Trainval Set
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3046647
