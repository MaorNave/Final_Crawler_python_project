- Affiliation of the first author: georgia tech, atlanta, ga
  Affiliation of the last author: georgia tech, atlanta, ga
  Figure 1 Link: articels_figures_by_rev_year\2018\Visual_Dialog\figure_1.jpg
  Figure 1 caption: "We introduce a new AI task\u2014Visual Dialog, where an AI agent\
    \ must hold a dialog with a human about visual content. We introduce a large-scale\
    \ dataset (VisDial), an evaluation protocol, and novel encoder-decoder models\
    \ for this task."
  Figure 10 Link: articels_figures_by_rev_year\2018\Visual_Dialog\figure_10.jpg
  Figure 10 caption: Answer lengths by question type and round. Average response length
    tends to be longest in the middle rounds.
  Figure 2 Link: articels_figures_by_rev_year\2018\Visual_Dialog\figure_2.jpg
  Figure 2 caption: Differences between image captioning, Visual Question Answering
    (VQA) and Visual Dialog. Two (partial) dialogs are shown from our VisDial dataset,
    which is curated from a live chat between two Amazon Mechanical Turk workers (Section
    3).
  Figure 3 Link: articels_figures_by_rev_year\2018\Visual_Dialog\figure_3.jpg
  Figure 3 caption: Collecting visually-grounded dialog data on Amazon Mechanical
    Turk via a live chat interface where one person is assigned the role of 'questioner'
    and the second person is the 'answerer'. We show the first two questions being
    collected via the interface as Turkers interact with each other in Figs. 3a and
    3b. Remaining questions are shown in Fig. 3c.
  Figure 4 Link: articels_figures_by_rev_year\2018\Visual_Dialog\figure_4.jpg
  Figure 4 caption: Detailed instructions for Amazon mechanical Turkers on our interface.
  Figure 5 Link: articels_figures_by_rev_year\2018\Visual_Dialog\figure_5.jpg
  Figure 5 caption: Examples from VisDial.
  Figure 6 Link: articels_figures_by_rev_year\2018\Visual_Dialog\figure_6.jpg
  Figure 6 caption: Distribution of lengths for questions and answers (a); and percent
    coverage of unique answers over all answers from the train dataset (b), compared
    to VQA. For a given coverage, VisDial has more unique answers indicating greater
    diversity.
  Figure 7 Link: articels_figures_by_rev_year\2018\Visual_Dialog\figure_7.jpg
  Figure 7 caption: Question lengths by type and round. Average length of question
    by type is fairly consistent across rounds. Questions starting with 'any' ('any
    people?', etc.) tend to be the shortest.
  Figure 8 Link: articels_figures_by_rev_year\2018\Visual_Dialog\figure_8.jpg
  Figure 8 caption: Percentage coverage of question types per round. As conversations
    progress, 'Is', 'What' and 'How' questions reduce while 'Can', 'Do', 'Does', 'Any'
    questions occur more often. Questions starting with 'is' are the most popular
    in the dataset.
  Figure 9 Link: articels_figures_by_rev_year\2018\Visual_Dialog\figure_9.jpg
  Figure 9 caption: Distribution of first n-grams for (left to right) VQA questions,
    VisDial questions and VisDial answers. Word ordering starts towards the center
    and radiates outwards, and arc length is proportional to number of questions containing
    the word.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Abhishek Das
  Name of the last author: Dhruv Batra
  Number of Figures: 15
  Number of Tables: 4
  Number of authors: 8
  Paper title: Visual Dialog
  Publication Date: 2018-04-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Existing Image Question Answering Datasets with
      VisDial
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Sequences in VisDial, VQA, and Cornell Movie-Dialogs
      Corpus in Their Original Ordering versus Permuted 'Shuffled' Ordering
  Table 3 caption:
    table_text: TABLE 3 Performance on VisDial v0.9, Measured by Mean Reciprocal Rank
      (MRR), Recall k k and Mean Rank
  Table 4 caption:
    table_text: TABLE 4 Human-Machine Performance Comparison on VisDial v0.5, Measured
      by Mean Reciprocal rank (MRR), Recall k k for k=1,5 k=1,5 and Mean Rank
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2828437
- Affiliation of the first author: key laboratory of intelligent perception and systems
    for high-dimensional information of ministry of education, school of computer
    science and engineering, nanjing university of science and technology, nanjing,
    china
  Affiliation of the last author: key laboratory of child development and learning
    science, ministry of education, school of biological sciences and medical engineering,
    southeast universit, nanjing, china
  Figure 1 Link: articels_figures_by_rev_year\2018\Recurrent_Shape_Regression\figure_1.jpg
  Figure 1 caption: "Illustration of the main idea. We convert the conventional cascaded\
    \ shape regression into the recurrent dynamic regression. The basic recurrent\
    \ unit is the shape-dependent dynamic regression. It consists of two processes:\
    \ the inference (solid lines) and the prediction (dash lines). In the inference\
    \ process, the parameterized dynamic regressor \u03D5( S t\u22121 , S t ,\u03B8\
    ) can produce a proper mapping from the current running states. In the prediction\
    \ process, this just produced mapping as well as the current state will be used\
    \ to estimate the next state S t+1 . The predicted state S t+1 can be fed (copied\
    \ in the figure) into the next stage of shape regression. So the mappings produced\
    \ by the dynamic regressor can be regarded as hidden states, and the interactions\
    \ between hidden states are implemented by the predicted states."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Recurrent_Shape_Regression\figure_2.jpg
  Figure 2 caption: "The proposed RSR framework. Each recurrent unit consists of two\
    \ procedures, the dynamic regressor learning (the orange lines) and the shape\
    \ prediction (the black dashed lines). The parameters \u0398 1 = \u03B8 1 1 ,\
    \ \u03B8 2 1 , \u03D6 1 1 , \u03D6 2 1 ,g and F are regarded as the recurrence\
    \ model in shape regression."
  Figure 3 Link: articels_figures_by_rev_year\2018\Recurrent_Shape_Regression\figure_3.jpg
  Figure 3 caption: Shape-indexed gradient-oriented feature learning.
  Figure 4 Link: articels_figures_by_rev_year\2018\Recurrent_Shape_Regression\figure_4.jpg
  Figure 4 caption: Comparison of cumulative errors distribution (CED) curves on 300-W
    (2013) (68 points). Best viewed in zooming sizes.
  Figure 5 Link: articels_figures_by_rev_year\2018\Recurrent_Shape_Regression\figure_5.jpg
  Figure 5 caption: Comparison of cumulative errors distribution (CED) curves on 300-W
    (2015). Best viewed in zooming sizes.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Zhen Cui
  Name of the last author: Wenming Zheng
  Number of Figures: 5
  Number of Tables: 4
  Number of authors: 5
  Paper title: Recurrent Shape Regression
  Publication Date: 2018-04-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Main Procedures of Second-Order RSR
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparisons of Mean RMS Errors on 300-W (2013)
  Table 3 caption:
    table_text: TABLE 3 Comparisons of Mean RMS Errors on LFPWHELEN
  Table 4 caption:
    table_text: TABLE 4 Comparisons of Mean RMS Errors on COFW
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2828424
- Affiliation of the first author: key laboratory of machine perception (moe), school
    of eecs, peking university, beijing, p.r.china
  Affiliation of the last author: johns hopkins university, baltimore, md
  Figure 1 Link: articels_figures_by_rev_year\2018\Robust_D_Human_Pose_Estimation_from_Single_Images_or_Video_Sequences\figure_1.jpg
  Figure 1 caption: Method overview. (1) On a test image, we first estimate the 2D
    joint locations and obtain an initial 3D pose by the mean pose in the training
    data. This initializes an alternating direction method which recursively alternates
    the two steps (i.e., steps 2 and 3). (2) Estimate the the camera parameters from
    the 2D pose and current estimate of the 3D pose. (3) Re-estimate the 3D pose using
    the 2D pose and the current estimates of the camera parameters. The algorithm
    converges when the difference of the estimates is small.
  Figure 10 Link: articels_figures_by_rev_year\2018\Robust_D_Human_Pose_Estimation_from_Single_Images_or_Video_Sequences\figure_10.jpg
  Figure 10 caption: 'Left figure: Error distribution of the estimated Camera rotation
    angles. The units are degrees. Right figure: 3D pose estimation errors when camera
    parameters are (1) set by ground truth, (2) estimated by initializing the 3D pose
    with mean pose, or (3) estimated by initializing the 3D pose with 30 cluster centers
    for parallel optimization (but only the best result is reported). The y -axis
    is the percentage of the cases whose estimation error is less than x. The units
    for x are mms. See Section 6.2.5.'
  Figure 2 Link: articels_figures_by_rev_year\2018\Robust_D_Human_Pose_Estimation_from_Single_Images_or_Video_Sequences\figure_2.jpg
  Figure 2 caption: (a) The estimated 2D joint locations where the right foot location
    is inaccurate. (b-c) are the estimated 3D poses using the L 1 -norm and L 2 -norm
    projection error, respectively. Using L 2 -norm biases the estimation to a completely
    wrong pose. In contrast, using L 1 -norm returns a reasonable pose which does
    not have obvious errors despite the right foot joint. See Section 3.1.2
  Figure 3 Link: articels_figures_by_rev_year\2018\Robust_D_Human_Pose_Estimation_from_Single_Images_or_Video_Sequences\figure_3.jpg
  Figure 3 caption: Top-six 3D pose estimations of a sample image. The plots in blue
    and red are the ground-truth and estimated 3D poses, respectively. The fifth estimation
    is the best among the candidates. See Section 3.3.
  Figure 4 Link: articels_figures_by_rev_year\2018\Robust_D_Human_Pose_Estimation_from_Single_Images_or_Video_Sequences\figure_4.jpg
  Figure 4 caption: 'All results are based on the HumanEva dataset for three subjects.
    Left figure: The rank distribution of the best poses among the eight candidates.
    Right figure: The error distribution when choosing the first candidate versus
    choosing the best candidate (by oracle). X -axis is the average joint error of
    the three subjects and the y -axis represents the percentage of cases whose errors
    are smaller than X. The estimation error units are millimetres (mms). See Section
    3.3.'
  Figure 5 Link: articels_figures_by_rev_year\2018\Robust_D_Human_Pose_Estimation_from_Single_Images_or_Video_Sequences\figure_5.jpg
  Figure 5 caption: Comparison of the three basis learning methods. (a) 3D pose reconstruction
    errors using different number of bases. In this experiment, the 3D poses are normalized
    so that the length of the right lower leg is one. (b) Distribution of the number
    of activated bases for representing a 3D pose. The y -axis is the percentage of
    the cases whose number of activated bases is less than x. See Section 5.2.
  Figure 6 Link: articels_figures_by_rev_year\2018\Robust_D_Human_Pose_Estimation_from_Single_Images_or_Video_Sequences\figure_6.jpg
  Figure 6 caption: 3D pose estimation errors of the baselines and our method (L1AS).
    The units for estimation errors are mms.
  Figure 7 Link: articels_figures_by_rev_year\2018\Robust_D_Human_Pose_Estimation_from_Single_Images_or_Video_Sequences\figure_7.jpg
  Figure 7 caption: Average limb length error of the L1AS and L1S.
  Figure 8 Link: articels_figures_by_rev_year\2018\Robust_D_Human_Pose_Estimation_from_Single_Images_or_Video_Sequences\figure_8.jpg
  Figure 8 caption: Results when different levels of noises are added to 2D poses.
    The x -axis is the estimation error and the y -axis is the percentage of cases
    where the estimation error is less than the corresponding x value.
  Figure 9 Link: articels_figures_by_rev_year\2018\Robust_D_Human_Pose_Estimation_from_Single_Images_or_Video_Sequences\figure_9.jpg
  Figure 9 caption: 3D pose estimation errors when the human-camera angle varies from
    0 to 180 degrees. We compare with Ramakrishna's method [13]. See Section 6.2.4.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Chunyu Wang
  Name of the last author: Alan L. Yuille
  Number of Figures: 11
  Number of Tables: 3
  Number of authors: 4
  Paper title: Robust 3D Human Pose Estimation from Single Images or Video Sequences
  Publication Date: 2018-04-19 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Real Experiment on the HumanEva Dataset: Comparison with
      the State-of-the-Art Methods [24], [50]'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Real Experiment on the H3.6M Dataset: Comparison with the
      State-of-the-Art Methods'
  Table 3 caption:
    table_text: TABLE 3 2D Pose Estimation Results
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2828427
- Affiliation of the first author: department of computer science, university of north
    carolina at charlotte, nc
  Affiliation of the last author: department of computer science, university of north
    carolina at charlotte, nc
  Figure 1 Link: articels_figures_by_rev_year\2018\Deep_Mixture_of_Diverse_Experts_for_LargeScale_Visual_Recognition\figure_1.jpg
  Figure 1 caption: The flowchart of our deep mixture of diverse experts algorithm,
    where certain degrees of inter-group task overlapping are allowed for task group
    generation.
  Figure 10 Link: articels_figures_by_rev_year\2018\Deep_Mixture_of_Diverse_Experts_for_LargeScale_Visual_Recognition\figure_10.jpg
  Figure 10 caption: The comparisons on the accuracy rates for 1,000 atomic object
    classes in Task Group 7.
  Figure 2 Link: articels_figures_by_rev_year\2018\Deep_Mixture_of_Diverse_Experts_for_LargeScale_Visual_Recognition\figure_2.jpg
  Figure 2 caption: Part of our two-layer ontology for indexing 7,756 atomic object
    classes in ImageNet10K image set.
  Figure 3 Link: articels_figures_by_rev_year\2018\Deep_Mixture_of_Diverse_Experts_for_LargeScale_Visual_Recognition\figure_3.jpg
  Figure 3 caption: The flowchart to illustrate our ontology-guided left-to-right
    task assignment process when 50 percent inter-group overlapping is used.
  Figure 4 Link: articels_figures_by_rev_year\2018\Deep_Mixture_of_Diverse_Experts_for_LargeScale_Visual_Recognition\figure_4.jpg
  Figure 4 caption: "The comparison on the accuracy rates for: (a) our deep mixture\
    \ of diverse experts algorithm when deep multi-task learning, inter-group overlapping\
    \ and special class of \u201Cnot-in-group\u201D are used; (b) AlexNet Extension;\
    \ (c) our deep mixture algorithm without multi-task learning; (d) our deep mixture\
    \ of diverse experts algorithm without using inter-group overlapping but having\
    \ special class of \u201Cnot-in-group\u201D; (e) our deep mixture of diverse experts\
    \ algorithm with only multi-task learning."
  Figure 5 Link: articels_figures_by_rev_year\2018\Deep_Mixture_of_Diverse_Experts_for_LargeScale_Visual_Recognition\figure_5.jpg
  Figure 5 caption: 'The comparison on the accuracy rates for our deep mixture of
    diverse experts algorithm: (a) ontology (tree)-guided task assignment is used;
    (b) random task assignment is used.'
  Figure 6 Link: articels_figures_by_rev_year\2018\Deep_Mixture_of_Diverse_Experts_for_LargeScale_Visual_Recognition\figure_6.jpg
  Figure 6 caption: 'The comparisons on the prediction scores: (a) test images; (b)
    predicted object classes and their scores from our deep mixture of diverse experts
    algorithm; (c) predicted object classes and their scores from the AlexNet Extension
    approach.'
  Figure 7 Link: articels_figures_by_rev_year\2018\Deep_Mixture_of_Diverse_Experts_for_LargeScale_Visual_Recognition\figure_7.jpg
  Figure 7 caption: 'The comparisons on the prediction scores: (a) test images; (b)
    predicted object classes and their scores from our deep mixture of diverse experts
    algorithm; (c) predicted object classes and their scores from the AlexNet Extension
    approach.'
  Figure 8 Link: articels_figures_by_rev_year\2018\Deep_Mixture_of_Diverse_Experts_for_LargeScale_Visual_Recognition\figure_8.jpg
  Figure 8 caption: The comparisons on the accuracy rates for 1,000 atomic object
    classes in Task Group 1.
  Figure 9 Link: articels_figures_by_rev_year\2018\Deep_Mixture_of_Diverse_Experts_for_LargeScale_Visual_Recognition\figure_9.jpg
  Figure 9 caption: The comparisons on the accuracy rates for 1,000 atomic object
    classes in Task Group 5.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.9
  Name of the first author: Tianyi Zhao
  Name of the last author: Jianping Fan
  Number of Figures: 18
  Number of Tables: 6
  Number of authors: 5
  Paper title: Deep Mixture of Diverse Experts for Large-Scale Visual Recognition
  Publication Date: 2018-04-20 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Comparisons on the Average Accuracy Rates
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 The Comparisons on the Accuracy Rates (Top 1) for 7 Task
      Groups: wo MT Means That Multi-Task Learning Is Not Used; w MT Means That Multi-Task
      Learning Is Used'
  Table 3 caption:
    table_text: "TABLE 3 The Performance Comparisons of Our Deep Mixture of Diverse\
      \ Experts Algorithm When Different Inter-Group Overlapping Percentages \u03BB\
      \ \u03BB Are Used"
  Table 4 caption:
    table_text: TABLE 4 The Comparisons on the Average Accuracy Rates
  Table 5 caption:
    table_text: TABLE 5 The Comparisons on the Average Accuracy Rates When Different
      Designs of Our Base Deep CNNs Are Used
  Table 6 caption:
    table_text: TABLE 6 The Comparisons on the Average Accuracy Rates When Different
      Types of Deep Networks Are Used to Configure the Base Deep CNNs
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2828821
- Affiliation of the first author: school of electronic information and communications,
    huazhong university of science and technology, wuhan, hubei, china
  Affiliation of the last author: department of computer and information sciences,
    temple university, philadelphia, pa
  Figure 1 Link: articels_figures_by_rev_year\2018\Regularized_Diffusion_Process_on_Bidirectional_Context_for_Object_Retrieval\figure_1.jpg
  Figure 1 caption: The retrieval results returned by the euclidean distance (a) and
    the proposed algorithm (b). The two crosses denote the query points. All other
    points are colored according to the larger similarity to one of the two query
    points.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Regularized_Diffusion_Process_on_Bidirectional_Context_for_Object_Retrieval\figure_2.jpg
  Figure 2 caption: The illustrations of the smoothness criterion of the proposed
    RDP (a), ARDP (b) and HRDP (c). W denotes the input similarity, and A denotes
    the output similarity.
  Figure 3 Link: articels_figures_by_rev_year\2018\Regularized_Diffusion_Process_on_Bidirectional_Context_for_Object_Retrieval\figure_3.jpg
  Figure 3 caption: The two crosses denote the query points. The retrieval results
    of MR (first row) are given when iteration number is 5 (a), 10 (b), 20 (c) and
    100 (d). The retrieval results of RDP (second row) are given when iteration number
    is 5 (e), 10 (f), 20 (g) and 100 (h). The gray points have zero similarities with
    both query points.
  Figure 4 Link: articels_figures_by_rev_year\2018\Regularized_Diffusion_Process_on_Bidirectional_Context_for_Object_Retrieval\figure_4.jpg
  Figure 4 caption: The qualitative comparison between the baseline and RDP on the
    Ukbench dataset. Query images are in blue boxes. False positives and true positives
    are in red and green boxes, respectively.
  Figure 5 Link: articels_figures_by_rev_year\2018\Regularized_Diffusion_Process_on_Bidirectional_Context_for_Object_Retrieval\figure_5.jpg
  Figure 5 caption: The performance comparison in FT (a) and ST (b) between hypergraph
    learning, HRDP and HRDP+RDP on the PSB dataset.
  Figure 6 Link: articels_figures_by_rev_year\2018\Regularized_Diffusion_Process_on_Bidirectional_Context_for_Object_Retrieval\figure_6.jpg
  Figure 6 caption: The objective value (1st row)and the retrieval performance (2nd
    row) of the proposed approaches as a function of iteration number on the YALE
    (a)(e), the MPEG-7 (b)(d), the Wikipedia (c)(g), and the PSB (d)(h) datasets.
  Figure 7 Link: articels_figures_by_rev_year\2018\Regularized_Diffusion_Process_on_Bidirectional_Context_for_Object_Retrieval\figure_7.jpg
  Figure 7 caption: The influence of the regularizer mu and the number of nearest
    neighbors k on the retrieval performance on the YALE dataset.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.67
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Song Bai
  Name of the last author: Longin Jan Latecki
  Number of Figures: 7
  Number of Tables: 6
  Number of authors: 4
  Paper title: Regularized Diffusion Process on Bidirectional Context for Object Retrieval
  Publication Date: 2018-04-20 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Performance Comparison with Other Variants of Diffusion
      Process on the ORL, the YALE and the MPEG-7 Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Performance Comparison on the Ukbench and Holidays Datasets
  Table 3 caption:
    table_text: TABLE 3 The Comparison in mAP with the State-of-the-Art on the Oxford5K
      and Oxford105K Datasets
  Table 4 caption:
    table_text: TABLE 4 The mAP Comparison with the Baselines on the Wikipedia Dataset
  Table 5 caption:
    table_text: TABLE 5 The Comparison with the State-of-the-Art on the Wikipedia
      Dataset
  Table 6 caption:
    table_text: TABLE 6 The Average Query Time on the 10 Datasets Used in This Work
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2828815
- Affiliation of the first author: school of computer science and technology, harbin
    institute of technology, harbin, heilongjiang, china
  Affiliation of the last author: school of engineering, university of california,
    merced, ca
  Figure 1 Link: articels_figures_by_rev_year\2018\Hedging_Deep_Features_for_Visual_Tracking\figure_1.jpg
  Figure 1 caption: Tracking results obtained by separately using CNN features extracted
    from six different convolutional layers of the VGGNet (with 19 layers) and by
    combining all features via the proposed adaptive hedge algorithm on representative
    frames of four sequences with different challenging factors. Red and green boxes
    denote the tracking results and ground truth, respectively. The tracking results
    by the proposed algorithm are more accurate than the ones obtained by using features
    only from one single layer.
  Figure 10 Link: articels_figures_by_rev_year\2018\Hedging_Deep_Features_for_Visual_Tracking\figure_10.jpg
  Figure 10 caption: One pass spatial robustness evaluation and temporal robustness
    evaluation results on the OTB100 dataset.
  Figure 2 Link: articels_figures_by_rev_year\2018\Hedging_Deep_Features_for_Visual_Tracking\figure_2.jpg
  Figure 2 caption: "Main steps of the proposed HDT \u2217 . When a new frame arrives,\
    \ we first extract CNN features of the region of interest from different convolutional\
    \ layers with the pre-trained VGGNet (19 layers), and then each weak tracker computes\
    \ correlation filter responses using features from one specific layer (Section\
    \ 3.2). Finally, these responses are combined together by the proposed adaptive\
    \ hedge algorithm and produce the ultimate target location (Sections 3.3 and 3.4)."
  Figure 3 Link: articels_figures_by_rev_year\2018\Hedging_Deep_Features_for_Visual_Tracking\figure_3.jpg
  Figure 3 caption: The proposed similarity Siamese network takes two images and computes
    appearance similarity. The convolutional layers and the FC4 layer of these two
    streams share the same weights, respectively.
  Figure 4 Link: articels_figures_by_rev_year\2018\Hedging_Deep_Features_for_Visual_Tracking\figure_4.jpg
  Figure 4 caption: "Shape of the hyperbolic tangent function (17) is controlled by\
    \ parameter \u03B3 . A smaller value of \u03B3 makes the function smoother. On\
    \ the other hand, it becomes a step function when a large value of \u03B3 is used."
  Figure 5 Link: articels_figures_by_rev_year\2018\Hedging_Deep_Features_for_Visual_Tracking\figure_5.jpg
  Figure 5 caption: "Precision and success plots using OPE for the ablation analysis\
    \ on the SSN appearance loss (denoted by A) and the Euclidean distance loss (denoted\
    \ by D). In this plot, HDT \u2217 D denotes HDT \u2217 without using the Euclidean\
    \ loss, and likewise HDT \u2217 A denotes HDT \u2217 without using the appearance\
    \ loss."
  Figure 6 Link: articels_figures_by_rev_year\2018\Hedging_Deep_Features_for_Visual_Tracking\figure_6.jpg
  Figure 6 caption: Precision and success plots using OPE for the ablation analysis
    on the adaptive cumulative regret (denoted by AR) and one-degree potential function
    (denoted by ODP). In this plot, HDT AR is obtained by replacing AR with the original
    one (12), and HDT ODP is obtained by replacing ODP with the original one (18).
  Figure 7 Link: articels_figures_by_rev_year\2018\Hedging_Deep_Features_for_Visual_Tracking\figure_7.jpg
  Figure 7 caption: Weight distributions on component trackers generated by the original
    potential function (18) and the proposed one (20), respectively, on the Deer sequence.
    The top images show that the target is challenged by scale variation, occlusion,
    and deformation. The middle figure shows the weights obtained using the original
    potential function (18). The bottom figure shows the weights obtained using the
    proposed potential function (20). The x-axis denotes frame indices, y-axis the
    names of component trackers, and z-axis the values of weights. Here, HDT 16 denotes
    the component tracker using features extracted only from the 16th convolutional
    layer, and likewise for the others.
  Figure 8 Link: articels_figures_by_rev_year\2018\Hedging_Deep_Features_for_Visual_Tracking\figure_8.jpg
  Figure 8 caption: Experimental results of HDT and its component trackers on the
    OTB50 dataset.
  Figure 9 Link: articels_figures_by_rev_year\2018\Hedging_Deep_Features_for_Visual_Tracking\figure_9.jpg
  Figure 9 caption: Experimental results of HDT and three ensemble methods as well
    as three baseline methods on the OTB100 dataset.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yuankai Qi
  Name of the last author: Ming-Hsuan Yang
  Number of Figures: 12
  Number of Tables: 7
  Number of authors: 7
  Paper title: Hedging Deep Features for Visual Tracking
  Publication Date: 2018-04-20 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Architecture of the Proposed Similarity Siamese Network
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 AUC Score and Precision at a Threshold of 20 Pixels for the
      Ablation Analysis on the SSN Appearance (Denoted by A) and Euclidean Distance
      (Denoted by D) Losses
  Table 3 caption:
    table_text: TABLE 3 AUC Score and Precision at a Threshold of 20 Pixels for the
      Ablation Analysis on the Adaptive Cumulative Regret (AR) and One-Degree Potential
      (ODP) Models
  Table 4 caption:
    table_text: "TABLE 4 Sensitivity Analysis of \u03B3 \u03B3 in Terms of AUC Score\
      \ and Precision at a Threshold of 20 Pixels on the OTB100 and VOT2014 Datasets"
  Table 5 caption:
    table_text: "TABLE 5 Sensitivity Analysis of the Proposed HDT \u2217 to Initial\
      \ Weights (0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667) of Six Component\
      \ Trackers (HDT \u2217 10 10 , HDT \u2217 11 11 , HDT \u2217 12 12 , HDT \u2217\
      \ 14 14 , HDT \u2217 15 15 , and HDT \u2217 16 16 ) on the OTB100 and VOT2014\
      \ Datasets in Terms of AUC"
  Table 6 caption:
    table_text: TABLE 6 Experimental Results in Terms of the Expected Average Overlap
      Metric on the VOT2016 Dataset
  Table 7 caption:
    table_text: TABLE 7 Run-Time of the Evaluated Tracking Methods in Terms of Frames
      per Second on the OTB100 Dataset
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2828817
- Affiliation of the first author: national key laboratory for novel software technology,
    nanjing university, nanjing, china
  Affiliation of the last author: national key laboratory for novel software technology,
    nanjing university, nanjing, china
  Figure 1 Link: articels_figures_by_rev_year\2018\What_Makes_Objects_Similar_A_Unified_MultiMetric_Learning_Approach\figure_1.jpg
  Figure 1 caption: Illustration of the difference between spatial and semantic learned
    multiple metrics M 1 , M 2 , M 3 . The left plot shows the multiple metrics with
    spatial locality, while the right plots presents the semantic multi-metric case.
    Different colors denote different classes. Dashed lines indicate the contours
    of learned metrics. Although anchored on different centers with multiple spatial
    metrics, there is actually only one responsible metric for a particular example
    in the first case, which has distinct differences from the multiple choices of
    metric candidates in the semantic multi-metric scenario.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\What_Makes_Objects_Similar_A_Unified_MultiMetric_Learning_Approach\figure_2.jpg
  Figure 2 caption: Illustration of Um 2 l ADS in a three-metric scenario. The leftmost
    plot shows a social network where green dot-dashed line and red dashed line represent
    friends (similar) and non-friends (dissimilar) linkages between users, respectively.
    We can think each metric in M K has a semantic similarity graph, where circles
    in the squares indicate friendship linkages between users, from a specific view.
    The rightmost plot corresponds to the overall similarity graph. To illustrate
    the overall graph, three sub-graphs explain from their own perspectives. They
    all follow the rule that two users linked in a certain semantic space results
    in their linkage in the overall graph, and overall dissimilar constraints restrict
    there are no possible linkages in any decomposed spaces.
  Figure 3 Link: articels_figures_by_rev_year\2018\What_Makes_Objects_Similar_A_Unified_MultiMetric_Learning_Approach\figure_3.jpg
  Figure 3 caption: Classification test error of Um 2 l with other Dml methods on
    8 datasets. Um 2 l based on ADS and RGS are placed at first. Followings are local
    and global based methods. Last is euclidean 3NN baseline. Four groups of methods,
    namely Um 2 l, local Dml, global Dml and baseline k NN with euclidean distance
    (denoted as Euclid), are separated by spaces.
  Figure 4 Link: articels_figures_by_rev_year\2018\What_Makes_Objects_Similar_A_Unified_MultiMetric_Learning_Approach\figure_4.jpg
  Figure 4 caption: The change of classification performance (test error rate) when
    the number of metrics learned in Um 2 l mathrmADS and Um 2 l mathrmRGS is changed.
    Bracket after the name of each dataset shows the number of classes ( C ).
  Figure 5 Link: articels_figures_by_rev_year\2018\What_Makes_Objects_Similar_A_Unified_MultiMetric_Learning_Approach\figure_5.jpg
  Figure 5 caption: "Subspaces discovered by MVTE, Sca, MmTSne and Um 2 l given instances\
    \ with two semantic components, i.e., color and shape. Blue dot-lines give the\
    \ possible decision boundary (best viewed in color). \u201CV\u201D means the discovered\
    \ view, and \u201CNV\u201D means the noise perturbed case. Right upper corner\
    \ of each plot shows the paired semantic NMI (PSNMI) value, a numerical measurement\
    \ of multi-view discovery task, for a single projection, the higher the better."
  Figure 6 Link: articels_figures_by_rev_year\2018\What_Makes_Objects_Similar_A_Unified_MultiMetric_Learning_Approach\figure_6.jpg
  Figure 6 caption: Word clouds generated from the results of compared Dml methods.
    The size of a word depends on the importance weight of each word (feature). The
    weight is calculated by decomposing each metric Mk=Lk Lktop , and calculate the
    ell 2 -norm of each row in Lk , where each row corresponds to a specific word.
    Each subplot gives a word cloud for a learned base metric.
  Figure 7 Link: articels_figures_by_rev_year\2018\What_Makes_Objects_Similar_A_Unified_MultiMetric_Learning_Approach\figure_7.jpg
  Figure 7 caption: Results of visual semantic discovery on images. The first annotation
    in the bracket is the provided weak label, and the second one is one of the latent
    semantic labels discovered by Um 2 l.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.54
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Han-Jia Ye
  Name of the last author: Zhi-Hua Zhou
  Number of Figures: 7
  Number of Tables: 5
  Number of authors: 4
  Paper title: 'What Makes Objects Similar: A Unified Multi-Metric Learning Approach'
  Publication Date: 2018-04-20 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Gradient Computation w.r.t. Metric M k Mk for Both Pairwise
      and Triplet Versions of Um 2 2L
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 BER (Lower Is Better) of the Linkage Discovering Comparisons
      on Facebook Datasets: Um 2 2L ADS ADS versus Others'
  Table 3 caption:
    table_text: "TABLE 3 Test Error Comparison Between Different \u03BA \u03BA Implementation\
      \ in the Um 2 2L Framework, for Pairwise and Triplet Variants"
  Table 4 caption:
    table_text: "TABLE 4 Comparisons of Classification Performance (Test Errors, mean\
      \ \xB1 \xB1 std.) Based on 3NN"
  Table 5 caption:
    table_text: 'TABLE 5 Clustering and Retrieval Performance Comparisons on CAR196
      Datasets: Um 2 2l versus Others'
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2829192
- Affiliation of the first author: sas institute inc., cary, nc
  Affiliation of the last author: department of electrical and computer engineering,
    university of wisconsin-madison, madison, wi
  Figure 1 Link: articels_figures_by_rev_year\2018\Online_Data_Thinning_via_MultiSubspace_Tracking\figure_1.jpg
  Figure 1 caption: Conceptual illustration of proposed objectives. An airborne platform
    collects wide-area motion imagery (WAMI), identifies task-specific salient patches,
    and transmits only those patches. The ground-based receiver can then perform more
    sophisticated processing, including registration, geolocation, and activity analysis.
  Figure 10 Link: articels_figures_by_rev_year\2018\Online_Data_Thinning_via_MultiSubspace_Tracking\figure_10.jpg
  Figure 10 caption: Data thinning result using Online Thinning and SUN algorithms
    on the surveillance video at frames 50 and 100. The first row shows the original
    video, and the second row shows the data thinning results. In the results, green
    patches are flagged by both methods, blue patches are only flagged by Online Thinning,
    and red patches are only flagged by SUN. Online Thinning outperforms the SUN algorithm
    by consistently flagging the people, which are sometimes missed by the SUN algorithm.
  Figure 2 Link: articels_figures_by_rev_year\2018\Online_Data_Thinning_via_MultiSubspace_Tracking\figure_2.jpg
  Figure 2 caption: Illustration of the union of subspaces idea. Fig. 2a shows a pedestrian
    walking on a road with trees on the sides [59]. The road and the plants occupy
    most of the pixels, and they can be considered living in a union of subspaces.
    The person on the road would be considered as an outlier. The ellipses in Fig.
    2b represent the low-rank mixture components in the GMM model.
  Figure 3 Link: articels_figures_by_rev_year\2018\Online_Data_Thinning_via_MultiSubspace_Tracking\figure_3.jpg
  Figure 3 caption: Flow chart of the main steps in the data thinning method.
  Figure 4 Link: articels_figures_by_rev_year\2018\Online_Data_Thinning_via_MultiSubspace_Tracking\figure_4.jpg
  Figure 4 caption: Multiscale representation of low-rank Gaussian mixture model.
    Consider a density with its mass concentrated along the black dashed curve. Each
    successive level in the multiscale representation has more Gaussian mixture components
    (depicted via contour plots) with covariance matrices corresponding to more compact
    ellipsoids, and hence yields a more accurate approximation of the underlying density.
    Given a particular binary tree representation of a GMM, the approximation error
    can be allowed to increase or decrease by pruning or growing the binary tree connecting
    the different scales. The ellipsoids are all very compact along some axes because
    they correspond to covariance matrices that are the sum of a low-rank matrix and
    a scaled identity matrix.
  Figure 5 Link: articels_figures_by_rev_year\2018\Online_Data_Thinning_via_MultiSubspace_Tracking\figure_5.jpg
  Figure 5 caption: "Detection error as a function of subsampling rate. The two curves\
    \ correspond to different subspace rotation speed ( \u03B4 ). A subsampling rate\
    \ at 55 percent still keeps the detection error less than 5 percent."
  Figure 6 Link: articels_figures_by_rev_year\2018\Online_Data_Thinning_via_MultiSubspace_Tracking\figure_6.jpg
  Figure 6 caption: Detection error as a function of mini-batch size Nt . The three
    curves correspond to different subspace changing speeds ( delta ). The detection
    error increases as Nt increases. For all three delta values, the change in detection
    error relative to Nt is less than 2 percent.
  Figure 7 Link: articels_figures_by_rev_year\2018\Online_Data_Thinning_via_MultiSubspace_Tracking\figure_7.jpg
  Figure 7 caption: Comparison between Online Thinning using a dynamic low-rank GMM,
    MMLE-MFA [62], full-rank GMM [60], [61], online full-rank GMM [25], and MPPCA
    [63], [64], [65]. Online thinning and online full-rank GMM are initialized using
    the first one thousand samples, then updated online; MPPCA and full-rank GMM are
    initialized with the first one thousand samples, then retrained every five hundred
    samples (with all data received at time of update); MMLE-MFA is trained once over
    the entire nine thousand samples.
  Figure 8 Link: articels_figures_by_rev_year\2018\Online_Data_Thinning_via_MultiSubspace_Tracking\figure_8.jpg
  Figure 8 caption: Online Thinning performance when the subspace rank is under-estimated
    ( hatr=6, 8 ), true rank ( hatr=r=10 ), or slightly over-estimated ( hatr=12 ).
    Online full-rank GMM performance is shown as a reference baseline. The subspaces
    change at a rate of 1times 10-2 . Under-estimation of subspace ranks decreases
    saliency detection accuracy, while slight over-estimation actually improves performance
    due to noise in the signal.
  Figure 9 Link: articels_figures_by_rev_year\2018\Online_Data_Thinning_via_MultiSubspace_Tracking\figure_9.jpg
  Figure 9 caption: Number of mixture components estimated by Online Thinning with
    different alpha values. For the first and third 104 samples, the non-anomalous
    data (95 percent of samples) come from four different rank-ten components, while
    in the second 104 samples, the non-anomalous data come from two different rank-ten
    components. Online Thinning quickly adapts to the actual number of mixture components.
    alpha (forgetting factor) does not affect the estimated number of mixture components
    in steady state, but impacts how quickly Online Thinning adapts when changes happen.
  First author gender probability: 0.6
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Xin J. Hunt
  Name of the last author: Rebecca Willett
  Number of Figures: 14
  Number of Tables: 0
  Number of authors: 2
  Paper title: Online Data Thinning via Multi-Subspace Tracking
  Publication Date: 2018-04-20 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2829189
- Affiliation of the first author: school of computer science and technology, harbin
    institute of technology, harbin, heilongjiang, china
  Affiliation of the last author: school of engineering, university of california,
    merced, ca
  Figure 1 Link: articels_figures_by_rev_year\2018\Learning_Support_Correlation_Filters_for_Visual_Tracking\figure_1.jpg
  Figure 1 caption: Illustration of the proposed SCF learning algorithm. The proposed
    algorithm iterates between updating e and updating SVM classifier w,b until convergence.
    In each iteration, only one DFT and one IDFT are required, which make the proposed
    algorithm computationally efficient. The black blocks in e denote support vectors,
    and our algorithm can adaptively find and exploit difficult samples (i.e., support
    vectors) to learn support correlation filters.
  Figure 10 Link: articels_figures_by_rev_year\2018\Learning_Support_Correlation_Filters_for_Visual_Tracking\figure_10.jpg
  Figure 10 caption: OPE plots of the KSCF, SKSCF and other state-of-the art trackers,
    including MEEM [42], TGPR [15], KCF [24], SCM [45], TLD [28], ASLA [27], L1APG
    [5], MIL [3] and CT [44].
  Figure 2 Link: articels_figures_by_rev_year\2018\Learning_Support_Correlation_Filters_for_Visual_Tracking\figure_2.jpg
  Figure 2 caption: Differences between the proposed SCF model and existing CF approaches
    [8], [23], [43]. (a) Existing CF-based models are designed to learn correlation
    filters that make the actual output being close to the predefined confidence maps.
    (b) The SCF model aims to learn a support correlation filter together with the
    bias b for distinguishing a target object from the background based on the max-margin
    principle. The peak value in the right response map of (b) locates the target
    object well.
  Figure 3 Link: articels_figures_by_rev_year\2018\Learning_Support_Correlation_Filters_for_Visual_Tracking\figure_3.jpg
  Figure 3 caption: OPE plots of the MSCF and DCF [24] with different feature representations.
    The AUC values are shown next to the legends.
  Figure 4 Link: articels_figures_by_rev_year\2018\Learning_Support_Correlation_Filters_for_Visual_Tracking\figure_4.jpg
  Figure 4 caption: OPE plots of the KSCF and KCF [24] methods with different feature
    representations.
  Figure 5 Link: articels_figures_by_rev_year\2018\Learning_Support_Correlation_Filters_for_Visual_Tracking\figure_5.jpg
  Figure 5 caption: OPE plots of the SCF methods (i.e., SCF, MSCF, KSCF, and SKSCF)
    and other CF-based trackers (i.e., MOSSE [8], CSK [23], DCF [24], KCF [24], STC
    [43], CN [13], DSST [12] and SAMF [31]).
  Figure 6 Link: articels_figures_by_rev_year\2018\Learning_Support_Correlation_Filters_for_Visual_Tracking\figure_6.jpg
  Figure 6 caption: Screenshots of tracking results on 8 challenging benchmark sequences.
    For the sake of clarity, we only show the results of six trackers, i.e., KSCF,
    KCF [24], MEEM [42], TGPR [15], Struck [21] and SCM [45].
  Figure 7 Link: articels_figures_by_rev_year\2018\Learning_Support_Correlation_Filters_for_Visual_Tracking\figure_7.jpg
  Figure 7 caption: OPE plots of the KSCF, SKSCF, DSST [12] and SAMF [31] methods
    on sequences with large scale variation.
  Figure 8 Link: articels_figures_by_rev_year\2018\Learning_Support_Correlation_Filters_for_Visual_Tracking\figure_8.jpg
  Figure 8 caption: OPE plots of the KSCF, SKSCF and other SVM-based trackers, including
    MEEM [42] and Struck [21].
  Figure 9 Link: articels_figures_by_rev_year\2018\Learning_Support_Correlation_Filters_for_Visual_Tracking\figure_9.jpg
  Figure 9 caption: Precision and success metrics of four top-performing trackers
    for the 11 attributes.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Wangmeng Zuo
  Name of the last author: Ming-Hsuan Yang
  Number of Figures: 10
  Number of Tables: 7
  Number of authors: 5
  Paper title: Learning Support Correlation Filters for Visual Tracking
  Publication Date: 2018-04-20 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Results of MSCF and DCF [24] (MSCFDCF) with Different Feature
      Representations
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results of KSCF and KCF [24] (KSCFKCF) with Different Feature
      Representations
  Table 3 caption:
    table_text: TABLE 3 Results of KSCF with Different Kernels
  Table 4 caption:
    table_text: 'TABLE 4 Performance of Tracking Methods Based on Correlation Filters:
      Top Three Results Are Shown in Red, Blue and Orange'
  Table 5 caption:
    table_text: TABLE 5 Comparison of KSCF, SKSCF, and the State-of-the-Art Trackers
      (Top Three Are Shown in Red, Blue and Orange)
  Table 6 caption:
    table_text: TABLE 6 Comparison of SVM-Based Trackers
  Table 7 caption:
    table_text: TABLE 7 Precision (Top) and Success Rate (Bottom) of the Evaluated
      Trackers (Top Three Are Shown in Red, Blue and Orange)
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2829180
- Affiliation of the first author: computer vision group, department of mathematics
    and computer science, friedrich schiller university jena, jena, germany
  Affiliation of the last author: computer vision group, department of mathematics
    and computer science, friedrich schiller university jena, jena, germany
  Figure 1 Link: articels_figures_by_rev_year\2018\Detecting_Regions_of_Maximal_Divergence_for_SpatioTemporal_Anomaly_Detection\figure_1.jpg
  Figure 1 caption: "Schematic illustration of the principle of the MDI algorithm:\
    \ The distribution of the data in the inner interval I is compared with the distribution\
    \ of the remaining time-series in the outer interval \u03A9 ."
  Figure 10 Link: articels_figures_by_rev_year\2018\Detecting_Regions_of_Maximal_Divergence_for_SpatioTemporal_Anomaly_Detection\figure_10.jpg
  Figure 10 caption: Performance of the original and the unbiased KL divergence on
    test cases with multiple or subtle anomalies.
  Figure 2 Link: articels_figures_by_rev_year\2018\Detecting_Regions_of_Maximal_Divergence_for_SpatioTemporal_Anomaly_Detection\figure_2.jpg
  Figure 2 caption: "Illustration of time-delay embedding with \u03BA=3,\u03C4=4 .\
    \ The attribute vector of each sample is augmented with the attributes of the\
    \ samples 4 and 8 time steps earlier."
  Figure 3 Link: articels_figures_by_rev_year\2018\Detecting_Regions_of_Maximal_Divergence_for_SpatioTemporal_Anomaly_Detection\figure_3.jpg
  Figure 3 caption: Exemplary illustration of spatial-neighbor embedding with different
    parameters. The attribute vector of the sample with a solid fill color is augmented
    with the attributes of the samples with a striped pattern.
  Figure 4 Link: articels_figures_by_rev_year\2018\Detecting_Regions_of_Maximal_Divergence_for_SpatioTemporal_Anomaly_Detection\figure_4.jpg
  Figure 4 caption: "Example for the bias of D KL ( p \u03A9 , p I ) detections towards\
    \ small intervals with low empirical variance on a synthetic time-series. The\
    \ intensity of the fill color of the detected intervals corresponds to the detection\
    \ scores. The ground-truth anomalous interval is indicated by a red box."
  Figure 5 Link: articels_figures_by_rev_year\2018\Detecting_Regions_of_Maximal_Divergence_for_SpatioTemporal_Anomaly_Detection\figure_5.jpg
  Figure 5 caption: (a) Top 10 detections obtained from the KL divergence on a real
    time-series and (b) top 3 detections obtained from the unbiased KL divergence
    on the same time-series. This example illustrates the phenomenon of several contiguous
    minimum-size detections when using the original KL divergence (note the thin lines
    between the single detections in the left plot). The MDI algorithm has been applied
    with a time-delay embedding of kappa =3, tau =1 and the size of the intervals
    to analyze has been limited to be between 25 and 250 samples.
  Figure 6 Link: articels_figures_by_rev_year\2018\Detecting_Regions_of_Maximal_Divergence_for_SpatioTemporal_Anomaly_Detection\figure_6.jpg
  Figure 6 caption: Two exemplary synthetic time-series along with the corresponding
    Hotelling's T2 scores and their gradients. The dashed black line indicates the
    mean of the scores and the dashed blue line marks a threshold that is 1.5 standard
    deviations above the mean. Time-delay embedding with kappa =3, tau =1 was applied
    before computing the scores.
  Figure 7 Link: articels_figures_by_rev_year\2018\Detecting_Regions_of_Maximal_Divergence_for_SpatioTemporal_Anomaly_Detection\figure_7.jpg
  Figure 7 caption: Examples from the synthetic test data set.
  Figure 8 Link: articels_figures_by_rev_year\2018\Detecting_Regions_of_Maximal_Divergence_for_SpatioTemporal_Anomaly_Detection\figure_8.jpg
  Figure 8 caption: Performance comparison of different variants of the MDI algorithm
    and the baselines on the synthetic data set.
  Figure 9 Link: articels_figures_by_rev_year\2018\Detecting_Regions_of_Maximal_Divergence_for_SpatioTemporal_Anomaly_Detection\figure_9.jpg
  Figure 9 caption: Effect of time-delay embedding with kappa =6, tau =2 on the performance
    of the MDI algorithm and the baselines on the synthetic data set.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "Bj\xF6rn Barz"
  Name of the last author: Joachim Denzler
  Number of Figures: 13
  Number of Tables: 0
  Number of authors: 4
  Paper title: Detecting Regions of Maximal Divergence for Spatio-Temporal Anomaly
    Detection
  Publication Date: 2018-04-30 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2823766
