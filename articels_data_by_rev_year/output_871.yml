- Affiliation of the first author: school of computer science, university of manchester,
    manchester, united kingdom
  Affiliation of the last author: school of computer science, university of manchester,
    manchester, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2017\Data_Visualization_with_Structural_Control_of_Global_Cohort_and_Local_Data_Neigh\figure_1.jpg
  Figure 1 caption: Illustration of COVA-E1 and t-SNE embeddings for the 2D synthetic
    Sinusoid data. The used cohort prototypes for COVA-E1 are marked in (a) as solid
    circles.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Data_Visualization_with_Structural_Control_of_Global_Cohort_and_Local_Data_Neigh\figure_2.jpg
  Figure 2 caption: Illustration of COVA-E2 and COVA-E3 embeddings computed for Cylinder2
    data. (a) The computed prototypes.
  Figure 3 Link: articels_figures_by_rev_year\2017\Data_Visualization_with_Structural_Control_of_Global_Cohort_and_Local_Data_Neigh\figure_3.jpg
  Figure 3 caption: "(a)-(d): Illustration of class neighbor preservation (two effective\
    \ neighbors are identified). For each method pair \u201CX vs. Y\u201D, edges in\
    \ solid, dotted and dashed lines indicate true positive, false positive and false\
    \ negative neighbor links of X compared to Y. Link preservation accuracies are\
    \ shown in percentages. (e): COVA-E2 output, where the prototypes are shown as\
    \ \u201C \u25A1 \u201D. (f): t-SNE output. Different cohorts are numbered and\
    \ correspond to different shadings."
  Figure 4 Link: articels_figures_by_rev_year\2017\Data_Visualization_with_Structural_Control_of_Global_Cohort_and_Local_Data_Neigh\figure_4.jpg
  Figure 4 caption: "(a)-(d): Performance change for COVA embedding models for varying\
    \ percentages of random prototypes using Places2 images. (e)-(g): Visualization\
    \ output of COVA-E1 generated by using 0, 33.3 and 100 percent random prototypes,\
    \ where the prototypes are shown as \u201C \u25A1 \u201D. Different cohorts are\
    \ numbered and correspond to different shadings."
  Figure 5 Link: articels_figures_by_rev_year\2017\Data_Visualization_with_Structural_Control_of_Global_Cohort_and_Local_Data_Neigh\figure_5.jpg
  Figure 5 caption: "(a): Performance change for COVA-E4 and S-t-SNE for varying percentages\
    \ of labeled samples using Places2 images. (b): COVA-E4 output, where the prototypes\
    \ are shown as \u201C square \u201D. (c): S-t-SNE output. Both (b) and (c) are\
    \ generated with 5 percent images labeled."
  Figure 6 Link: articels_figures_by_rev_year\2017\Data_Visualization_with_Structural_Control_of_Global_Cohort_and_Local_Data_Neigh\figure_6.jpg
  Figure 6 caption: '(a)-(d): Performance change for the COVA embedding models by
    varying alpha from 0.1 to 0.9 using Places2 images. (e)-(g): Visualization output
    of COVA-E3 with alpha = 0.1, 0.6, and 0.9.'
  Figure 7 Link: articels_figures_by_rev_year\2017\Data_Visualization_with_Structural_Control_of_Global_Cohort_and_Local_Data_Neigh\figure_7.jpg
  Figure 7 caption: '(a)-(c): Performance change for the COVA projection models by
    varying class number from 5 to 30 using Places2 images. Example output is illustrated
    for c = 10, 30, including (d),(g) displaying the used COVA cohort prototypes,
    (e),(h) COVA-P1 output, and (f),(i) NCA output.'
  Figure 8 Link: articels_figures_by_rev_year\2017\Data_Visualization_with_Structural_Control_of_Global_Cohort_and_Local_Data_Neigh\figure_8.jpg
  Figure 8 caption: "Improved cohort arrangement exemplified by P1 and eigen-COVA\
    \ for different class numbers. Cohort prototypes are shown as \u201C square \u201D\
    . Different cohorts are numbered and correspond to different shadings."
  Figure 9 Link: articels_figures_by_rev_year\2017\Data_Visualization_with_Structural_Control_of_Global_Cohort_and_Local_Data_Neigh\figure_9.jpg
  Figure 9 caption: COVA visualization for the Cora publications. Cohort locations
    are controlled by the citation information. Different cohorts correspond to different
    shadings. Centroids of the neighboring classes are connected with solid lines
    and connection mismatches are shown with dotted lines.
  First author gender probability: 0.97
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Tingting Mu
  Name of the last author: Sophia Ananiadou
  Number of Figures: 9
  Number of Tables: 4
  Number of authors: 3
  Paper title: Data Visualization with Structural Control of Global Cohort and Local
    Data Neighborhoods
  Publication Date: 2017-06-15 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Visualization of Cora Publications Using Different Techniques.\
      \ For the supervised visualization, different values of the shrinking factor\
      \ \u03BB are used, and the three nearest neighbor classes of the \u201Cgenetic\
      \ algorithms\u201D class are highlighted. Different classes correspond to different\
      \ shadings."
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Illustration of Twenty Cora Clusters
  Table 3 caption:
    table_text: TABLE 3 Visualization of 3D Data Points in 2D Space Using Unsupervised
      Techniques. Data partitions are highlighted using different shadings.
  Table 4 caption:
    table_text: TABLE 4 Visualization of 3D Data Points in 2D Space Using Supervised
      Techniques. Data partitions are highlighted using different shadings.
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2715806
- Affiliation of the first author: beijing key laboratory of traffic data analysis
    and mining, beijing jiaotong university, beijing, china
  Affiliation of the last author: meitu hiscene lab, hiscene information technologies,
    shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2017\Gracker_A_GraphBased_Planar_Object_Tracker\figure_1.jpg
  Figure 1 caption: The framework for the proposed Gracker algorithm.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Gracker_A_GraphBased_Planar_Object_Tracker\figure_2.jpg
  Figure 2 caption: Average tracking accuracy and computational time of the proposed
    Gracker algorithm with respect to the size of graphs.
  Figure 3 Link: articels_figures_by_rev_year\2017\Gracker_A_GraphBased_Planar_Object_Tracker\figure_3.jpg
  Figure 3 caption: The success curves (left) and precision curves (right) on (a)
    the UCSB dataset, (b) the TMT dataset, and (c) the Motion Blur dataset.
  Figure 4 Link: articels_figures_by_rev_year\2017\Gracker_A_GraphBased_Planar_Object_Tracker\figure_4.jpg
  Figure 4 caption: Examples of tracking results under various transformations. Boxes
    of different colors indicate tracking results of different algorithms, while absence
    of boxes of specific colors means the corresponding algorithms lose the object
    (best viewed in color). Only cropped regions around the object are shown for better
    illustration.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.95
  Name of the first author: Tao Wang
  Name of the last author: Haibin Ling
  Number of Figures: 4
  Number of Tables: 5
  Number of authors: 2
  Paper title: 'Gracker: A Graph-Based Planar Object Tracker'
  Publication Date: 2017-06-16 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Average Tracking Accuracy ( \xB1 Standard Deviation) on the\
      \ UCSB Dataset"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average Tracking Accuracy on the TMT Dataset
  Table 3 caption:
    table_text: "TABLE 3 Comparison on Accuracy Between Gracker and Gracker \u2212\
      \ ( N=100 )"
  Table 4 caption:
    table_text: TABLE 4 Average Tracking Accuracy (%) on the Motion Blur Dataset
  Table 5 caption:
    table_text: TABLE 5 Average Computational Time (Second) per Frame of the Algorithms
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2716350
- Affiliation of the first author: department of computer science, city university
    of hong kong, hong kong, china
  Affiliation of the last author: school of engineering, university of california,
    merced, ca
  Figure 1 Link: articels_figures_by_rev_year\2017\Spatiotemporal_GMM_for_Background_Subtraction_with_Superpixel_Hierarchy\figure_1.jpg
  Figure 1 caption: Spatiotemporal background subtraction with a superpixel hierarchy.
    (a) a video frame extracted from the SABS dataset [43]. (b)-(f) foreground masks
    obtained from GMM, the proposed spatially-consistent and spatiotemporally-consistent
    background model without and with a superpixel hierarchy respectively.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Spatiotemporal_GMM_for_Background_Subtraction_with_Superpixel_Hierarchy\figure_2.jpg
  Figure 2 caption: Precision-recall curves on the SABS dataset [43] with different
    challenging factors. The red and dark solid curves show the performance of the
    proposed spatiotemporal background subtraction algorithms with and without superpixel
    hierarchy. Overall, the proposed algorithms (especially the STSHBM) perform favorably
    against all the other methods.
  Figure 3 Link: articels_figures_by_rev_year\2017\Spatiotemporal_GMM_for_Background_Subtraction_with_Superpixel_Hierarchy\figure_3.jpg
  Figure 3 caption: 'Sample background subtraction results. From top to bottom: video
    frames extracted from the baseline , dynamic background, camera jitter, intermittent
    object motion, shadow and thermal categories. (a) sample video frames. (b) to
    (g) background subtraction results obtained from the GMM, state-of-the-art PAWCS,
    proposed SBM, STBM, SSHBM and STSHBM. (h) ground-Truth (GT) foreground masks are
    shown. The proposed algorithms perform favorably against the other methods on
    the ChangeDetection dataset.'
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.96
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Mingliang Chen
  Name of the last author: Ming-Hsuan Yang
  Number of Figures: 3
  Number of Tables: 3
  Number of authors: 6
  Paper title: Spatiotemporal GMM for Background Subtraction with Superpixel Hierarchy
  Publication Date: 2017-06-21 00:00:00
  Table 1 caption:
    table_text: TABLE 1 F-Measures on the SABS Dataset [43]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 F-Measures for the ChangeDetection 2012 Dataset
  Table 3 caption:
    table_text: TABLE 3 Computational Cost of the Proposed Background Subtraction
      Algorithms for QVGA Videos (MillisecondsFrame)
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2717828
- Affiliation of the first author: daqri, vienna, austria
  Affiliation of the last author: "inria grenoble rh\xF4ne-alpes, montbonnot saint-martin,\
    \ france"
  Figure 1 Link: articels_figures_by_rev_year\2017\Joint_Alignment_of_Multiple_Point_Sets_with_Batch_and_Incremental_ExpectationMax\figure_1.jpg
  Figure 1 caption: "The proposed joint registration method assumes that all points\
    \ from all sets, e.g., V 1 to V 4 are realizations of the same mixture (shown\
    \ in the center). An observed point, e.g., v 45 \u2208 V 4 , once rotated and\
    \ translated from the set-centered coordinate frame to the mixture-centered coordinate\
    \ frame ( R 4 and t 4 ) is assigned to the k th mixture component defined by \u03BC\
    \ k , \u03A3 k and p k . As shown on the figure, the estimated mixture is not\
    \ associated to any of the point sets, as is the case with pairwise registration\
    \ methods."
  Figure 10 Link: articels_figures_by_rev_year\2017\Joint_Alignment_of_Multiple_Point_Sets_with_Batch_and_Incremental_ExpectationMax\figure_10.jpg
  Figure 10 caption: Dense point-cloud reconstruction obtained with JRMPC-I for the
    fr1desk sequence.
  Figure 2 Link: articels_figures_by_rev_year\2017\Joint_Alignment_of_Multiple_Point_Sets_with_Batch_and_Incremental_ExpectationMax\figure_2.jpg
  Figure 2 caption: "Top: log -RMSE as a function of outlier percentage when SNR =\
    \ 10 dB. Bottom: The learning curve of algorithms for a range of 100 iterations\
    \ when the models are disturbed by SNR = 10 dB and 20 percent outliers. (a) \u201C\
    Lucy\u201D, (b) \u201CBunny\u201D (c) \u201CArmadillo\u201D."
  Figure 3 Link: articels_figures_by_rev_year\2017\Joint_Alignment_of_Multiple_Point_Sets_with_Batch_and_Incremental_ExpectationMax\figure_3.jpg
  Figure 3 caption: "RMSE as a function of the overlap (rotation angle) when two point\
    \ sets are registered (SNR = 20 dB, 30 percent outliers) (a), (b) \u201CLucy\u201D\
    \ (c), (d) \u201CArmadillo\u201D."
  Figure 4 Link: articels_figures_by_rev_year\2017\Joint_Alignment_of_Multiple_Point_Sets_with_Batch_and_Incremental_ExpectationMax\figure_4.jpg
  Figure 4 caption: "(a), (b) Two point sets (out of four) with outliers; (c) distribution\
    \ of estimated variances; instances of GMM means after (d) 5, (e) 15, and (f)\
    \ 30 iterations; (g) the splitting of model points into inliers and outliers;\
    \ joint-registration of four point sets (h) before and (i) after removing \u201C\
    bad\u201D points (best viewed on-screen)."
  Figure 5 Link: articels_figures_by_rev_year\2017\Joint_Alignment_of_Multiple_Point_Sets_with_Batch_and_Incremental_ExpectationMax\figure_5.jpg
  Figure 5 caption: Integrated models of Bunny (first row), Dragon (second row) and
    Happy Buddha (third row) based on four joint-wise registration methods (best viewed
    on-screen).
  Figure 6 Link: articels_figures_by_rev_year\2017\Joint_Alignment_of_Multiple_Point_Sets_with_Batch_and_Incremental_ExpectationMax\figure_6.jpg
  Figure 6 caption: Cross-section of Bunny (top), Dragon (middle) and Happy Buddha
    (bottom) obtained from several algorithms ( best viewed on-screen).
  Figure 7 Link: articels_figures_by_rev_year\2017\Joint_Alignment_of_Multiple_Point_Sets_with_Batch_and_Incremental_ExpectationMax\figure_7.jpg
  Figure 7 caption: "The GMM means obtained from JRMPC-based algorithms for (top)\
    \ \u201CBunny\u201D and (bottom) \u201CDragon\u201D. Unlike JRMPC-B, JRMPC-I leads\
    \ to non-uniformly distributed mixture components (biased towards the initial\
    \ sets) since \u201Cold\u201D means cannot be freely re-distributed (best viewed\
    \ on-screen)."
  Figure 8 Link: articels_figures_by_rev_year\2017\Joint_Alignment_of_Multiple_Point_Sets_with_Batch_and_Incremental_ExpectationMax\figure_8.jpg
  Figure 8 caption: 'Integrated point clouds from the joint registration of 10 TOF
    images that record a static scene (EXBI data-set). Top: Color images that roughly
    show the scene content of each range image (occlusions due to cameras baseline
    may cause texture artefacts). Bottom: Front-view and top-view of integrated sets
    after joint registration.'
  Figure 9 Link: articels_figures_by_rev_year\2017\Joint_Alignment_of_Multiple_Point_Sets_with_Batch_and_Incremental_ExpectationMax\figure_9.jpg
  Figure 9 caption: Camera trajectories obtained with JRMPC-I and with RGBD-SLAM [51].
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.95
  Name of the first author: Georgios Dimitrios Evangelidis
  Name of the last author: Radu Horaud
  Number of Figures: 10
  Number of Tables: 4
  Number of authors: 2
  Paper title: Joint Alignment of Multiple Point Sets with Batch and Incremental Expectation-Maximization
  Publication Date: 2017-06-21 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Registration Error of Indirect Mappings
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Multi-View Registration Methods without Adding
      Noise
  Table 3 caption:
    table_text: 'TABLE 3 Performance of Multi-View Registration Methods when Points
      Are Perturbed by Gaussian Noise (SNR: 25 dB)'
  Table 4 caption:
    table_text: TABLE 4 RMSE ( m ) of Translation for SLAM Methods and for the Proposed
      Method for the TUM Dataset
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2717829
- Affiliation of the first author: "institut de rob\xF2tica i inform\xE0tica industrial,\
    \ csic-upc, barcelona, spain"
  Affiliation of the last author: department of electrical and electronic engineering,
    imperial college london, london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2017\BreakingNews_Article_Annotation_by_Image_and_Text_Processing\figure_1.jpg
  Figure 1 caption: 'BreakingNews dataset. The dataset contains a variety of news-related
    information including: the text of the article, captions, related images, part-of-speech
    tagging, GPS coordinates, semantic topics list or results of sentiment analysis,
    for about 100K news articles. The figure shows two sample images. All this volume
    of heterogeneous data makes BreakingNews an appropriate benchmark for several
    tasks exploring the relation between text and images.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\BreakingNews_Article_Annotation_by_Image_and_Text_Processing\figure_2.jpg
  Figure 2 caption: Ground truth geolocations of the articles.
  Figure 3 Link: articels_figures_by_rev_year\2017\BreakingNews_Article_Annotation_by_Image_and_Text_Processing\figure_3.jpg
  Figure 3 caption: "CNN and LSTM architectures for article analysis and caption generation.\
    \ (a1-3) CNN for article analysis. Dashed boxes are the CNN inputs, and solid\
    \ boxes correspond to the layers. White boxes (enclosed within the \u201CShared\
    \ Textual CNN\u201D box) are shared by all tasks, and shaded boxes are task specific\
    \ layers. (b1, b2) Models for caption generation: (b1) LSTM with fixed features,\
    \ and (b2) end-to-end learning with CNNs+LSTM. Note that the \u201CShared Textual\
    \ CNN\u201D from (a1) is used in the textual branch."
  Figure 4 Link: articels_figures_by_rev_year\2017\BreakingNews_Article_Annotation_by_Image_and_Text_Processing\figure_4.jpg
  Figure 4 caption: 'Source detection: t-SNE embedding of shallow and deep features
    for the articles in the test set.'
  Figure 5 Link: articels_figures_by_rev_year\2017\BreakingNews_Article_Annotation_by_Image_and_Text_Processing\figure_5.jpg
  Figure 5 caption: Comparison of the distribution of errors between the Places CNN
    with GCD loss (row 11 of Table 5) and the combination of VGG19 and Places CNNs
    for the image data and the W2V matrix embedding for the text.
  Figure 6 Link: articels_figures_by_rev_year\2017\BreakingNews_Article_Annotation_by_Image_and_Text_Processing\figure_6.jpg
  Figure 6 caption: Random sample images with low geolocation error (top row) and
    high geolocation error (bottom row) when using only Places CNN features.
  Figure 7 Link: articels_figures_by_rev_year\2017\BreakingNews_Article_Annotation_by_Image_and_Text_Processing\figure_7.jpg
  Figure 7 caption: 'Example images, their corresponding captions (first row in each
    block, boldface), and generated captions (second row: With LSTM; last row: With
    CCA-based sentence selection). Note that the captions do not always explicitly
    describe the visual content of the images.'
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Arnau Ramisa
  Name of the last author: Krystian Mikolajczyk
  Number of Figures: 7
  Number of Tables: 6
  Number of authors: 4
  Paper title: 'BreakingNews: Article Annotation by Image and Text Processing'
  Publication Date: 2017-06-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Some Popular Tags Associated to News Articles in the Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 BreakingNews Dataset Statistics
  Table 3 caption:
    table_text: TABLE 3 Results of Source Detection
  Table 4 caption:
    table_text: TABLE 4 Article Illustration Experiments
  Table 5 caption:
    table_text: TABLE 5 Results of the Geolocation Prediction Task
  Table 6 caption:
    table_text: TABLE 6 Results of Caption Generation
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2721945
- Affiliation of the first author: college of information and computer sciences, university
    of massachusetts amherst, amherst, ma
  Affiliation of the last author: college of information and computer sciences, university
    of massachusetts amherst, amherst, ma
  Figure 1 Link: articels_figures_by_rev_year\2017\Bilinear_Convolutional_Neural_Networks_for_FineGrained_Visual_Recognition\figure_1.jpg
  Figure 1 caption: Image classification using a B-CNN. An image is passed through
    CNNs A and B, and their outputs at each location are combined using the matrix
    outer product and average pooled to obtain the bilinear feature representation.
    This is passed through a linear + softmax layer to obtain class predictions.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Bilinear_Convolutional_Neural_Networks_for_FineGrained_Visual_Recognition\figure_2.jpg
  Figure 2 caption: Flow of gradients in a B-CNN.
  Figure 3 Link: articels_figures_by_rev_year\2017\Bilinear_Convolutional_Neural_Networks_for_FineGrained_Visual_Recognition\figure_3.jpg
  Figure 3 caption: Feature functions in B-CNNs can (a) share no computations (e.g.,
    B-CNN model based on VGG-M and VGG-D), (b) share computations partially (e.g.,
    NetVLAD, B-CNN PCA model described in Section 5.1 ), and (c) share all computations
    (e.g., B-CNN model based on VGG-M).
  Figure 4 Link: articels_figures_by_rev_year\2017\Bilinear_Convolutional_Neural_Networks_for_FineGrained_Visual_Recognition\figure_4.jpg
  Figure 4 caption: Top six pairs of classes that are most confused with each other
    on the CUB dataset. In each row we show the images in the test set that were most
    confidently classified as the class in the other column.
  Figure 5 Link: articels_figures_by_rev_year\2017\Bilinear_Convolutional_Neural_Networks_for_FineGrained_Visual_Recognition\figure_5.jpg
  Figure 5 caption: Performance of NetVLAD and NetFV models encoding VGG-M relu5 features
    with different number of cluster centers on fine-grained datasets before (dashed
    lines) and after (solid lines) fine-tuning. Given the same number of cluster centers,
    the feature dimension of NetFV representation is twice as large as NetVLAD.
  Figure 6 Link: articels_figures_by_rev_year\2017\Bilinear_Convolutional_Neural_Networks_for_FineGrained_Visual_Recognition\figure_6.jpg
  Figure 6 caption: Performance of B-CNNs using VGG-M relu5 features as function of
    feature dimension before (dashed lines) and after (solid lines) fine-tuning. One
    of the 512 dimensional feature is projected using PCA to k dimensions leading
    to an outer product of size k times 512 (see Section 5.1 for details). The performance
    of compact bilinear pooling (CBP) [15] is shown in megenta, and the full 512 times
    512 -dimensional B-CNN model is shown in black.
  Figure 7 Link: articels_figures_by_rev_year\2017\Bilinear_Convolutional_Neural_Networks_for_FineGrained_Visual_Recognition\figure_7.jpg
  Figure 7 caption: Patches with the highest activations for several filters of the
    fine-tuned B-CNN (VGG-D + VGG-M) model on birds, aircrafts and cars classification.
  Figure 8 Link: articels_figures_by_rev_year\2017\Bilinear_Convolutional_Neural_Networks_for_FineGrained_Visual_Recognition\figure_8.jpg
  Figure 8 caption: Visualizing various categories by inverting the B-CNN based on
    VGG-D network trained on DTD [57], FMD [70], MIT Indoor dataset [72] (first three
    rows, two columns each from left to right), and the CUB dataset [1] (last two
    rows, all columns). Best viewed in color and with zoom.
  Figure 9 Link: articels_figures_by_rev_year\2017\Bilinear_Convolutional_Neural_Networks_for_FineGrained_Visual_Recognition\figure_9.jpg
  Figure 9 caption: Inverse images obtained from a multilayer B-CNN. From left to
    right different layers (shown on top) are added one by one.
  First author gender probability: 0.64
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Tsung-Yu Lin
  Name of the last author: Subhransu Maji
  Number of Figures: 9
  Number of Tables: 4
  Number of authors: 3
  Paper title: Bilinear Convolutional Neural Networks for Fine-Grained Visual Recognition
  Publication Date: 2017-07-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Texture Representations as Bilinear Models
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Per-Image Accuracy on the Birds [1], Aircrafts [3], and Cars
      [4] Dataset
  Table 3 caption:
    table_text: TABLE 3 Mean Per-Class Accuracy on DTD, FMD, KTH-T2b and MIT Indoor
      Datasets
  Table 4 caption:
    table_text: TABLE 4 Accuracy on the ILSVRC 2014 Validation Set with Different
      Data Jittering Schemes
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2723400
- Affiliation of the first author: department of electrical and computer engineering,
    university of maryland, college park, md
  Affiliation of the last author: department of electrical and computer engineering,
    university of maryland, college park, md
  Figure 1 Link: articels_figures_by_rev_year\2017\Learning_from_Ambiguously_Labeled_Face_Images\figure_1.jpg
  Figure 1 caption: The names in the captions are not explicitly associated with the
    face images in the news photo.
  Figure 10 Link: articels_figures_by_rev_year\2017\Learning_from_Ambiguously_Labeled_Face_Images\figure_10.jpg
  Figure 10 caption: Labeling error rates of MCar-based methods versus gamma in the
    Lost (16,8) dataset with lambda = lambda o .
  Figure 2 Link: articels_figures_by_rev_year\2017\Learning_from_Ambiguously_Labeled_Face_Images\figure_2.jpg
  Figure 2 caption: MCar reassigns labels for those ambiguously labeled instances
    such that instances of the same subjects cohesively form potentially-separable
    convex hulls. The vertices of each convex hull are the representatives of each
    class, forming D k . The interior and outline of the circles are color-coded to
    represent three different classes and various ambiguous labels, respectively.
  Figure 3 Link: articels_figures_by_rev_year\2017\Learning_from_Ambiguously_Labeled_Face_Images\figure_3.jpg
  Figure 3 caption: Ideal decomposition of the heterogeneous feature matrix using
    MCar. The underlying low-rank structure and the ambiguous labeling are recovered
    simultaneously.
  Figure 4 Link: articels_figures_by_rev_year\2017\Learning_from_Ambiguously_Labeled_Face_Images\figure_4.jpg
  Figure 4 caption: "Performance comparisons on the FIW(10b) dataset. (a) \u03B1\u2208\
    [0,0.95] , \u03B2=2 , inductive experiment. (b) \u03B1=1.0 , \u03B2=1 , \u03F5\
    \u2208[1(c\u22121),1] , inductive experiment. (c) \u03B1=1.0 , \u03B2\u2208[0,1,\u2026\
    ,9] , transductive experiment."
  Figure 5 Link: articels_figures_by_rev_year\2017\Learning_from_Ambiguously_Labeled_Face_Images\figure_5.jpg
  Figure 5 caption: "Performance comparisons on the CMU PIE dataset. (a) \u03B1\u2208\
    [0,0.95] , \u03B2=2 , transductive experiment. (b) \u03B1=1.0 , \u03B2\u2208[0,1,\u2026\
    ,9] , transductive experiment."
  Figure 6 Link: articels_figures_by_rev_year\2017\Learning_from_Ambiguously_Labeled_Face_Images\figure_6.jpg
  Figure 6 caption: 'Subsets of images from (a) FIW(10b) and (b) CMU PIE dataset demonstrate
    the low-rank decomposition of feature matrix in MCar: the original face images,
    histogram-equalized images X , low-rank component Z , and noisy component E X
    , from the first row to the forth row, respectively.'
  Figure 7 Link: articels_figures_by_rev_year\2017\Learning_from_Ambiguously_Labeled_Face_Images\figure_7.jpg
  Figure 7 caption: The confusion matrix of the ambiguous labeling in Lost (16,8)
    dataset. The upper number of each square accounts for the number of occurrences
    of a candidate label, whereas the lower number of each square is computed by accumulating
    the soft labeling score of each occurrence of a candidate label.
  Figure 8 Link: articels_figures_by_rev_year\2017\Learning_from_Ambiguously_Labeled_Face_Images\figure_8.jpg
  Figure 8 caption: The groundtruth label distribution of the Lost (16,8) dataset.
    'Groundtruth' denotes the number of instances per class counted from the groundtruth
    labels, and 'Estimated' denotes the estimate of the groundtruth label distribution
    from the ambiguous labels.
  Figure 9 Link: articels_figures_by_rev_year\2017\Learning_from_Ambiguously_Labeled_Face_Images\figure_9.jpg
  Figure 9 caption: Labeling error rates of WMCar evaluated with a set of parameters
    (lambda, gamma) in the Lost (16,8) dataset.
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.62
  Name of the first author: Ching-Hui Chen
  Name of the last author: Rama Chellappa
  Number of Figures: 11
  Number of Tables: 2
  Number of authors: 3
  Paper title: Learning from Ambiguously Labeled Face Images
  Publication Date: 2017-07-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Labeling Error Rates for the Lost (16,8) Dataset (Available
      at http:www.timotheecour.comtvdatatvdata.html)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average Testing Error Rates for the Labeled Yahoo! News Dataset
      (Available at http:lear.inrialpes.frdata )
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2723401
- Affiliation of the first author: computer science and artificial intelligence laboratory,
    massachusetts institute of technology, cambridge, ma
  Affiliation of the last author: computer science and artificial intelligence laboratory,
    massachusetts institute of technology, cambridge, ma
  Figure 1 Link: articels_figures_by_rev_year\2017\Places_A__Million_Image_Database_for_Scene_Recognition\figure_1.jpg
  Figure 1 caption: 'Image samples from various categories of the Places Database
    (two samples per category). The dataset contains three macro-classes: Indoor,
    Nature, and Urban.'
  Figure 10 Link: articels_figures_by_rev_year\2017\Places_A__Million_Image_Database_for_Scene_Recognition\figure_10.jpg
  Figure 10 caption: The predictions given by the Places365-VGG for the images from
    the validation set. The ground-truth label (GT) and the top 5 predictions are
    shown. The number beside each label indicates the prediction confidence.
  Figure 2 Link: articels_figures_by_rev_year\2017\Places_A__Million_Image_Database_for_Scene_Recognition\figure_2.jpg
  Figure 2 caption: Sorted distribution of image number per category in the Places
    Database. Places contains 10,624,928 images from 434 categories. Category names
    are shown for every six intervals.
  Figure 3 Link: articels_figures_by_rev_year\2017\Places_A__Million_Image_Database_for_Scene_Recognition\figure_3.jpg
  Figure 3 caption: Image samples from four scene categories grouped by queries to
    illustrate the diversity of the dataset. For each query we show nine annotated
    images.
  Figure 4 Link: articels_figures_by_rev_year\2017\Places_A__Million_Image_Database_for_Scene_Recognition\figure_4.jpg
  Figure 4 caption: Annotation interface in the Amazon Mechanical Turk for selecting
    the correct exemplars of the scene from the downloaded images. a) instruction
    given to the workers in which we define positive and negative examples. b) binary
    selection interface.
  Figure 5 Link: articels_figures_by_rev_year\2017\Places_A__Million_Image_Database_for_Scene_Recognition\figure_5.jpg
  Figure 5 caption: Boundaries between place categories can be blurry, as some images
    can be made of a mixture of different components. The images shown in this figure
    show a soft transition between a field and a forest. Although the extreme images
    can be easily classified as field and forest scenes, the middle images can be
    ambiguous.
  Figure 6 Link: articels_figures_by_rev_year\2017\Places_A__Million_Image_Database_for_Scene_Recognition\figure_6.jpg
  Figure 6 caption: Annotation interface in Amazon Mechanical Turk for differentiating
    images from two similar categories. a) instruction in which we give several typical
    examples in each category. b) the binary selection interface, in which the worker
    has to classify the shown image into one of the classes or none of them.
  Figure 7 Link: articels_figures_by_rev_year\2017\Places_A__Million_Image_Database_for_Scene_Recognition\figure_7.jpg
  Figure 7 caption: Comparison of the number of images per scene category for the
    common 88 scene categories in Places, ImageNet, and SUN datasets.
  Figure 8 Link: articels_figures_by_rev_year\2017\Places_A__Million_Image_Database_for_Scene_Recognition\figure_8.jpg
  Figure 8 caption: Examples of pairs for the diversity experiment for a) playground
    and b) bedroom. Which pair shows the most similar images? The bottom pairs were
    chosen in these examples. c) Histogram of relative diversity per each category
    (88 categories) and dataset. Places88 (in blue line) contains the most diverse
    set of images, then ImageNet88 (in red line) and the lowest diversity is in the
    SUN88 database (in yellow line) as most images are prototypical of their class.
  Figure 9 Link: articels_figures_by_rev_year\2017\Places_A__Million_Image_Database_for_Scene_Recognition\figure_9.jpg
  Figure 9 caption: 'Cross dataset generalization of training on the 88 common scenes
    between Places, SUN and ImageNet then testing on the 88 common scenes from: a)
    SUN, b) ImageNet and c) Places database.'
  First author gender probability: 0.69
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Bolei Zhou
  Name of the last author: Antonio Torralba
  Number of Figures: 17
  Number of Tables: 3
  Number of authors: 5
  Paper title: 'Places: A 10 Million Image Database for Scene Recognition'
  Publication Date: 2017-07-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Classification Accuracy on the Test Set of Places205 and the
      Test Set of SUN205
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Classification Accuracy on the Validation Set and Test Set
      of Places365
  Table 3 caption:
    table_text: TABLE 3 Classification AccuracyPrecision on Scene-Centric Databases
      (the First Four Datasets) and Object-Centric Databases (the Last Four Datasets)
      for the Deep Features of Various Places-CNNs and ImageNet-CNNs
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2723009
- Affiliation of the first author: school of electronic science and engineering, nanjing
    university, nanjing, china
  Affiliation of the last author: university of kentucky, lexington, ky
  Figure 1 Link: articels_figures_by_rev_year\2017\D_Reconstruction_in_the_Presence_of_Glass_and_Mirrors_by_Acoustic_and_Visual_Fus\figure_1.jpg
  Figure 1 caption: Our modified Kinect camera has an additional ultrasonic range
    sensor (e.g., a sonar), which can measure the distance to any object, including
    transparent surfaces.
  Figure 10 Link: articels_figures_by_rev_year\2017\D_Reconstruction_in_the_Presence_of_Glass_and_Mirrors_by_Acoustic_and_Visual_Fus\figure_10.jpg
  Figure 10 caption: Our result on the fish tank scene with all surrounding surfaces
    being transparent. We scanned the fish tank separately from four different sides.
    The first two rows represent the scene and input frames. The third row shows the
    labeling. The final row shows KinectFusion result combined with ultrasonic measurements
    and our final merged result.
  Figure 2 Link: articels_figures_by_rev_year\2017\D_Reconstruction_in_the_Presence_of_Glass_and_Mirrors_by_Acoustic_and_Visual_Fus\figure_2.jpg
  Figure 2 caption: 'Kinect is not affected by ambient reflection on the glass. (left)
    The color image; (middle): Enhanced IR image captured by a Kinect V1, showing
    the direct reflection of the IR illuminator and some glare. The reflection on
    the glass due to natural illumination is mostly gone. (right): The captured depth
    map.'
  Figure 3 Link: articels_figures_by_rev_year\2017\D_Reconstruction_in_the_Presence_of_Glass_and_Mirrors_by_Acoustic_and_Visual_Fus\figure_3.jpg
  Figure 3 caption: Data used for joint calibration. We scanned some common objects,
    such as a corner of an office desk, and a chair next to a wall. The green dots
    are the ultrasonic measurements.
  Figure 4 Link: articels_figures_by_rev_year\2017\D_Reconstruction_in_the_Presence_of_Glass_and_Mirrors_by_Acoustic_and_Visual_Fus\figure_4.jpg
  Figure 4 caption: Ultrasonic measurement profile. The black line represents the
    opaque surface, blue dash line represents the transparent surface, and the colored
    dots indicate the ultrasonic measurement. For the opaque surface, the ultrasonic
    measurement is consistent with the depth sensor data (green squares). For the
    boundary area, the ultrasonic measurement may represent the nearest surface. We
    consider these points as outliers (yellow triangle). For these transparent or
    mirrored surfaces, the ultrasonic measurement (red circles) is different with
    the depth sensor data.
  Figure 5 Link: articels_figures_by_rev_year\2017\D_Reconstruction_in_the_Presence_of_Glass_and_Mirrors_by_Acoustic_and_Visual_Fus\figure_5.jpg
  Figure 5 caption: An example showing transparent point detection. The image on the
    left shows all the ultrasonic measurements and the image on the right shows only
    transparent ones. The insets in the middle correspond to zoomed-in views of the
    highlighted areas. Measurements are on the opaque surfaces are not included in
    the transparent point set.
  Figure 6 Link: articels_figures_by_rev_year\2017\D_Reconstruction_in_the_Presence_of_Glass_and_Mirrors_by_Acoustic_and_Visual_Fus\figure_6.jpg
  Figure 6 caption: The graphical model shows the conditional dependence structure
    between random variables. The data term on the left side describes the sensor
    measurement process, while the smoothing term on the right side defines the constraints
    between neighboring labels. See Section 5.2 for more details.
  Figure 7 Link: articels_figures_by_rev_year\2017\D_Reconstruction_in_the_Presence_of_Glass_and_Mirrors_by_Acoustic_and_Visual_Fus\figure_7.jpg
  Figure 7 caption: An example demonstrating our prior model. In this example, pi
    1 is the surface fitted from all of the red points in (a) and pi 2 is the surface
    fitted from all of the green points in (b).
  Figure 8 Link: articels_figures_by_rev_year\2017\D_Reconstruction_in_the_Presence_of_Glass_and_Mirrors_by_Acoustic_and_Visual_Fus\figure_8.jpg
  Figure 8 caption: A basic mask for Poisson blending can be obtained from our labeling
    result in (a). In this case, the mask will contain the red and the green region.
    To reduce the noise in the object boundary caused by the depth sensor, we extend
    this mask by dilating the mask for 5 pixels. The boundary along empty space is
    also included, and the resulting mask as shown in (b), (c) and (d), demonstrates
    the necessity of adopting the adaptive differential operators during Poisson blending.
  Figure 9 Link: articels_figures_by_rev_year\2017\D_Reconstruction_in_the_Presence_of_Glass_and_Mirrors_by_Acoustic_and_Visual_Fus\figure_9.jpg
  Figure 9 caption: Our results on various real scenes. We scanned a single piece
    of glass window with an opaque object behind in case 1, multiple glass pane in
    a meeting room with an opaque object behind and go cross the boundary between
    two glass panes in case 2, and a two glass door partially occluded by an extra
    foreground object in case 3.
  First author gender probability: 0.53
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yu Zhang
  Name of the last author: Ruigang Yang
  Number of Figures: 19
  Number of Tables: 4
  Number of authors: 4
  Paper title: 3D Reconstruction in the Presence of Glass and Mirrors by Acoustic
    and Visual Fusion
  Publication Date: 2017-07-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Estimated R and T in our Joint Calibration Procedure from
      Fig. 3 (left)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Probabilistic Modeling of Ultrasonic Measurements, Namely
      P( S t i | L t i )
  Table 3 caption:
    table_text: TABLE 3 The Probabilistic Modeling of Depth Sensor Measurements, Namely
      P( Z t i | O t i , L t i )
  Table 4 caption:
    table_text: TABLE 4 The Parameters Used in our Experiments
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2723883
- Affiliation of the first author: cooperative medianet innovation center, beijing,
    haidian, china
  Affiliation of the last author: cooperative medianet innovation center, beijing,
    haidian, china
  Figure 1 Link: articels_figures_by_rev_year\2017\Joint_Semantic_and_Latent_Attribute_Modelling_for_CrossClass_Transfer_Learning\figure_1.jpg
  Figure 1 caption: 'Some examples of user-defined semantic attributes and latent
    attributes. The user-defined semantic attributes are shown in the first row: (a)
    black jacket and (b) jeans. While those in the second row are two discriminative
    latent attributes learned by the proposed method: (c) white shirt with an open
    darker jacket and (d) white logo on the chest of a top. We can see that these
    latent attributes are often semantically meaningful and interpretable, but in
    a more subtle way, and may have been ignored by human annotators. It is clear
    that person A and person B cannot be distinguished by only two user-defined semantic
    attributes. But when complemented by latent attributes, it becomes easier.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Joint_Semantic_and_Latent_Attribute_Modelling_for_CrossClass_Transfer_Learning\figure_2.jpg
  Figure 2 caption: Our framework for joint learning of user-defined-attribute-correlated
    (UDAC), discriminative latent attribute (D-LA), and background latent attribute
    (B-LA) dictionary subspace. Class labels are person identities in the Re-ID problem.
  Figure 3 Link: articels_figures_by_rev_year\2017\Joint_Semantic_and_Latent_Attribute_Modelling_for_CrossClass_Transfer_Learning\figure_3.jpg
  Figure 3 caption: A schematic illustration of the proposed multi-task dictionary
    learning model.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.61
  Name of the first author: Peixi Peng
  Name of the last author: Tiejun Huang
  Number of Figures: 3
  Number of Tables: 5
  Number of authors: 6
  Paper title: Joint Semantic and Latent Attribute Modelling for Cross-Class Transfer
    Learning
  Publication Date: 2017-07-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparative Results on Zero-Shot Learning (Recognition Accuracy
      in %)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparative Results on Predicting User-Defined Attributes
  Table 3 caption:
    table_text: TABLE 3 Evaluation on the Contributions of Different Components to
      ZSL and Attribute Prediction (APre)
  Table 4 caption:
    table_text: TABLE 4 Supervised Re-ID Results
  Table 5 caption:
    table_text: TABLE 5 Unsupervised Re-ID Results on VIPeR and PRID
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2723882
