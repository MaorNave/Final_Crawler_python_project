- Affiliation of the first author: department of electrical engineering, ku leuven,
    flanders, leuven, belgium
  Affiliation of the last author: department of electrical engineering, ku leuven,
    flanders, leuven, belgium
  Figure 1 Link: articels_figures_by_rev_year\2016\Novel_Views_of_Objects_from_a_Single_Image\figure_1.jpg
  Figure 1 caption: Given an input image (left), we synthesize novel views of the
    depicted object at interactive frames rates with view-dependent apearance (right).
  Figure 10 Link: articels_figures_by_rev_year\2016\Novel_Views_of_Objects_from_a_Single_Image\figure_10.jpg
  Figure 10 caption: "The improved novel-view synthesis (Section 5) segments the input\
    \ image into parts using the material IDs of a 3D mesh as a guide. To synthesize\
    \ a new image, a reflectance map is extracted for every material\u2014shown on\
    \ the left\u2014, allowing high speed and view-dependent appearance."
  Figure 2 Link: articels_figures_by_rev_year\2016\Novel_Views_of_Objects_from_a_Single_Image\figure_2.jpg
  Figure 2 caption: 'We compute a novel view image f 2 from the input image f 1 using
    3D information of an aligned 3D template (left to right: radiance, normals, reflectance,
    positions) as guidance, even if its renderings L 1 and L 2 have appearance largely
    different from f 1 . q is the sampling quality, indicating the visibility in the
    original view, and w is the flow between the views, which is optional and used
    to speed up the sampling.'
  Figure 3 Link: articels_figures_by_rev_year\2016\Novel_Views_of_Objects_from_a_Single_Image\figure_3.jpg
  Figure 3 caption: "Reconstruction weights c and distances \u0394 L,n,p,r of three\
    \ output locations x 2,3,4 in respect to all positions from the input image."
  Figure 4 Link: articels_figures_by_rev_year\2016\Novel_Views_of_Objects_from_a_Single_Image\figure_4.jpg
  Figure 4 caption: Contribution of distances to the reconstruction weights (see text).
  Figure 5 Link: articels_figures_by_rev_year\2016\Novel_Views_of_Objects_from_a_Single_Image\figure_5.jpg
  Figure 5 caption: "Reflectance Map for the input image f 1 seen in Fig. 4: a): Reconstructed\
    \ using nearest-neighbor. Note the increasing density towards frontal views. b):\
    \ Our reconstruction without position, material and radiance information (only\
    \ normal differences). c\u2013f): Our reconstruction with all information included\
    \ for different materials of the object (body, rim, tire and windshield)."
  Figure 6 Link: articels_figures_by_rev_year\2016\Novel_Views_of_Objects_from_a_Single_Image\figure_6.jpg
  Figure 6 caption: Overview of our approach comprising an off-line and an on-line
    phase. The off-line phase samples shapes from a database in many views. In the
    on-line phase, coarse, fine and per-part 2D-to-3D alignment is performed which
    is based on a coarse image segmentation derived from user input (scribbles). Finally,
    novel views are synthesized by rendering the 3D shape in the old and novel views
    and transferring view-dependent appearance between those images.
  Figure 7 Link: articels_figures_by_rev_year\2016\Novel_Views_of_Objects_from_a_Single_Image\figure_7.jpg
  Figure 7 caption: Sampling of the shape-view-space (a) and sampling of a view for
    a specific shape (b) .
  Figure 8 Link: articels_figures_by_rev_year\2016\Novel_Views_of_Objects_from_a_Single_Image\figure_8.jpg
  Figure 8 caption: 'Effect of alignment steps on appearance and geometry matching:
    The coarse match is the closest exemplar of examples and fails to align even in
    a rigid sense. Fine alignment finds a good rigid alignment, but still cannot produce
    a match as the 3D shape is different from the one in the database. Only the part-based
    alignment puts the arm rests and wheels in the right location from where appearance
    can be transferred.'
  Figure 9 Link: articels_figures_by_rev_year\2016\Novel_Views_of_Objects_from_a_Single_Image\figure_9.jpg
  Figure 9 caption: 'Novel views synthesis: For every material index (2nd col.), a
    Reflectance Map is constructed based on the appearance of the object in the image
    (1st col.) and the normals in view space. These appearance maps are then used
    to synthesize appearance for a novel view (3rd col.).'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.96
  Name of the first author: Konstantinos Rematas
  Name of the last author: Tinne Tuytelaars
  Number of Figures: 20
  Number of Tables: 3
  Number of authors: 5
  Paper title: Novel Views of Objects from a Single Image
  Publication Date: 2016-08-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance of the DPM Detector When Trained in Different
      Data and with Different Number of Components ( N )
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance of the DPM Detector on PASCAL VOC Dataset with
      Varying Training Set and Different Number of Components ( N )
  Table 3 caption:
    table_text: TABLE 3 Performance of the DPM Detector on UCLA Dataset with Varying
      Training Set (Images from VOC) and Different Number of Components ( N )
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2601093
- Affiliation of the first author: university of science and technology of china,
    230026, hefei, china
  Affiliation of the last author: visual computing group, microsoft research, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2016\Object_Detection_Networks_on_Convolutional_Feature_Maps\figure_1.jpg
  Figure 1 caption: Overview of NoC. The convolutional feature maps are generated
    by the shared convolutional layers. A feature map region is extracted and RoI-pooled
    into a fixed-resolution feature. A new network, called a NoC, is then designed
    and trained on these features. In this illustration, the NoC architecture consists
    of two convolutional layers and three fully-connected layers.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Object_Detection_Networks_on_Convolutional_Feature_Maps\figure_2.jpg
  Figure 2 caption: "A maxout NoC of \u201Cc256-mo-c256-f4096-f4096-f21\u201D. The\
    \ features are RoI-pooled from two feature maps computed at two scales. In this\
    \ figure, maxout is used after conv +1 ."
  Figure 3 Link: articels_figures_by_rev_year\2016\Object_Detection_Networks_on_Convolutional_Feature_Maps\figure_3.jpg
  Figure 3 caption: Distribution of top-ranked True Positives (TP) and False Positives
    (FP), generated by the published diagnosis code of [30]. The types of positive
    predictions are categorized [30] as Cor (correct), Loc (false due to poor localization),
    Sim (confusion with a similar category), Oth (confusion with a dissimilar category),
    BG (fired on background). The total number of samples in each disk is the same
    and equal to the total number of ground-truth labels [30]. More explanations are
    in the main text.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.95
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Shaoqing Ren
  Name of the last author: Jian Sun
  Number of Figures: 3
  Number of Tables: 9
  Number of authors: 5
  Paper title: Object Detection Networks on Convolutional Feature Maps
  Publication Date: 2016-08-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Detection mAP (%) of NoC as MLP for PASCAL VOC 07 Using a
      ZF Net
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Detection mAP (%) of NoC as ConvNet for PASCAL VOC 07 Using
      a ZF Net
  Table 3 caption:
    table_text: TABLE 3 Detection mAP (%) of Maxout NoC for PASCAL VOC 07 Using a
      ZF Net
  Table 4 caption:
    table_text: TABLE 4 Detection mAP (%) of NoC for PASCAL VOC 07 Using ZFVGG-16
      Nets with Different Initialization
  Table 5 caption:
    table_text: TABLE 5 Detection Results for PASCAL VOC 07 Using VGG Nets
  Table 6 caption:
    table_text: TABLE 6 Detection Results for the PASCAL VOC 2007 Test Set Using the
      VGG-16 Model [15]
  Table 7 caption:
    table_text: TABLE 7 Detection Results for the PASCAL VOC 2012 Test Set Using the
      VGG-16 Model [15]
  Table 8 caption:
    table_text: TABLE 8 Detection Results of Faster R-CNN on the MS COCO Val Set
  Table 9 caption:
    table_text: TABLE 9 Detection Results of Faster R-CNN + ResNet-101 on MS COCO
      Val (Trained on MS COCO train) and PASCAL VOC 2007 Test (Trained on 07+12),
      Based on Different NoC Structures
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2601099
- Affiliation of the first author: school of mathematical science and state key laboratory
    of cad & cg, zhejiang university, hangzhou, china
  Affiliation of the last author: school of mathematics and statistics, xi'an jiaotong
    university, xi'an, china
  Figure 1 Link: articels_figures_by_rev_year\2016\Uniform_Projection_for_MultiView_Learning\figure_1.jpg
  Figure 1 caption: 'An illustration of view distortion: Intact 3D samples with separated
    three classes (left) and the nonlinear 2D projections in three views, given in
    Section 6.1.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Uniform_Projection_for_MultiView_Learning\figure_2.jpg
  Figure 2 caption: "Comparison of MDS and UMDS on the synthetic data set: the NMI\
    \ curves of MDS and UMDS depending on the parameter s\u2208[0,0.8] with a fixed\
    \ \u03C3\u22480.084 or \u03C3\u22480.137 (left four plates) and the noise scale\
    \ \u03C3 with a fixed s=0 or s=0.8 (right four plates). The dimension of projection\
    \ is set as d=4 (top line) or d=6 (bottom line)."
  Figure 3 Link: articels_figures_by_rev_year\2016\Uniform_Projection_for_MultiView_Learning\figure_3.jpg
  Figure 3 caption: Sparsity plots of link matrices in the four sets Cornell, Texas,
    Washington, and Wisconsin of the WebKB data. The lines indicate the classes of
    samples and class sizes.
  Figure 4 Link: articels_figures_by_rev_year\2016\Uniform_Projection_for_MultiView_Learning\figure_4.jpg
  Figure 4 caption: The ACC curves (red-star solid lines) and NMI curves (blue-circle
    dash lines) of the iterative solutions of UMDS (top row) and UCA (bottom row)
    on partial examples.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.66
  Name of the first author: Zhenyue Zhang
  Name of the last author: Limin Li
  Number of Figures: 4
  Number of Tables: 5
  Number of authors: 3
  Paper title: Uniform Projection for Multi-View Learning
  Publication Date: 2016-08-19 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Scale of the Real-World Data Sates: Numbers of Samples, Features,
      Views, and Clusters'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Tested Parameter Values, Initial Settings, and Stopping Conditions
  Table 3 caption:
    table_text: TABLE 3 The Clustering Accuracy of the Algorithms on Real Data Sets
  Table 4 caption:
    table_text: TABLE 4 Strategy, Computed Combination Coefficients, and Accuracy
      of OKkC, MKkC, and NMFce.
  Table 5 caption:
    table_text: TABLE 5 CPU Time (in Seconds) of the Compared Algorithms
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2601608
- Affiliation of the first author: department of computing, the hong kong polytechnic
    university, hong kong
  Affiliation of the last author: school of engineering, university of california,
    merced, ca
  Figure 1 Link: articels_figures_by_rev_year\2016\An_Efficient_Globally_Optimal_Algorithm_for_Asymmetric_Point_Matching\figure_1.jpg
  Figure 1 caption: "The convex envelope \u2212(r+s)t+rs of the function \u2212 t\
    \ 2 taken over an interval [r,s] ."
  Figure 10 Link: articels_figures_by_rev_year\2016\An_Efficient_Globally_Optimal_Algorithm_for_Asymmetric_Point_Matching\figure_10.jpg
  Figure 10 caption: Examples of matching results by (from left to right column) APM
    using similarity and affine transformation, MSTT, RRWM, IPFP, MPM and TM on images
    (from top to bottom) of motorbike, car, eiffel and revolver. Here blue lines indicate
    correct matches and red lines indicate wrong matches.
  Figure 2 Link: articels_figures_by_rev_year\2016\An_Efficient_Globally_Optimal_Algorithm_for_Asymmetric_Point_Matching\figure_2.jpg
  Figure 2 caption: "If M ki \u2282 M k , we have E M ki (p)\u2265 E M k (p) for p\u2208\
    \ M ki because of the concavity of E ."
  Figure 3 Link: articels_figures_by_rev_year\2016\An_Efficient_Globally_Optimal_Algorithm_for_Asymmetric_Point_Matching\figure_3.jpg
  Figure 3 caption: Ellipse p| E M k (p)=E(p) is the circumscribed ellipse of rectangle
    M k and their centers also coincide.
  Figure 4 Link: articels_figures_by_rev_year\2016\An_Efficient_Globally_Optimal_Algorithm_for_Asymmetric_Point_Matching\figure_4.jpg
  Figure 4 caption: 'First 4 columns: model point sets (left column) and examples
    of scene point sets in the deformation, noise and outlier tests, respectively
    (column 2 to 4). Last 2 columns: examples of model (column 5) and scene (right
    column) point sets in the clutter test.'
  Figure 5 Link: articels_figures_by_rev_year\2016\An_Efficient_Globally_Optimal_Algorithm_for_Asymmetric_Point_Matching\figure_5.jpg
  Figure 5 caption: "Average matching errors by APM with \u03F5 d =0.1 and other methods\
    \ in the tests of deformation, noise, outliers and clutter."
  Figure 6 Link: articels_figures_by_rev_year\2016\An_Efficient_Globally_Optimal_Algorithm_for_Asymmetric_Point_Matching\figure_6.jpg
  Figure 6 caption: "Average matching error by APM with respect to different choices\
    \ of \u03F5 d in the tests of deformation, noise, outlier and clutter for the\
    \ fish shape."
  Figure 7 Link: articels_figures_by_rev_year\2016\An_Efficient_Globally_Optimal_Algorithm_for_Asymmetric_Point_Matching\figure_7.jpg
  Figure 7 caption: Average upper and lower bounds for the feasible region by the
    two bounding schemes (left column) and average differences of the lower bounds
    (right column) in each iteration of our algorithm.
  Figure 8 Link: articels_figures_by_rev_year\2016\An_Efficient_Globally_Optimal_Algorithm_for_Asymmetric_Point_Matching\figure_8.jpg
  Figure 8 caption: Average run time of APM with respect to different choices of n1
    in the outlier test for the fish shape.
  Figure 9 Link: articels_figures_by_rev_year\2016\An_Efficient_Globally_Optimal_Algorithm_for_Asymmetric_Point_Matching\figure_9.jpg
  Figure 9 caption: 'Left panel: images of motorbike, car, eiffel and revolver overlaid
    with manually extracted points. The first image in each category is used to extract
    model points and the remaining images are respectively used to extract scene points.
    Middle and right panels: matching accuracies and errors by APM with epsilon d=1
    and other methods.'
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Wei Lian
  Name of the last author: Ming-Hsuan Yang
  Number of Figures: 17
  Number of Tables: 3
  Number of authors: 3
  Paper title: An Efficient Globally Optimal Algorithm for Asymmetric Point Matching
  Publication Date: 2016-08-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average Run Time (in Seconds)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average Run Time (in Seconds)
  Table 3 caption:
    table_text: TABLE 3 Average Run Time (in Seconds)
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2603988
- Affiliation of the first author: department of computer science and engineering,
    the chinese university of hong kong, hong kong
  Affiliation of the last author: department of computer science and engineering,
    the chinese university of hong kong, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2016\Blind_Image_Denoising_via_Dependent_Dirichlet_Process_Tree\figure_1.jpg
  Figure 1 caption: 'Left to right: Noisy image, denoising results of [12] and ours.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Blind_Image_Denoising_via_Dependent_Dirichlet_Process_Tree\figure_2.jpg
  Figure 2 caption: "Three components of a tree-structured dependent mixture model.\
    \ Each layer denotes a mixture model consist of certain components represented\
    \ by nodes. In the bottom layer, each component is parameterized by \u03B8 1 ,\
    \ \u03B8 2 and \u03B8 3 . In the higher layer, each component is parameterized\
    \ by \u03B8 1 . The children of a node in the bottom layer shares the same \u03B8\
    \ 2 and \u03B8 3 , e.g., component 2 and 3."
  Figure 3 Link: articels_figures_by_rev_year\2016\Blind_Image_Denoising_via_Dependent_Dirichlet_Process_Tree\figure_3.jpg
  Figure 3 caption: (a) Dirichlet process stick-breaking procedure, with a linear
    partitioning. (b) Two-layer DDPT stick breaking process. A stick with unit length
    is partitioned into stick segments via a stick-breaking process. Each stick segment
    is set to be unit length and an extra stick-breaking process is performed.
  Figure 4 Link: articels_figures_by_rev_year\2016\Blind_Image_Denoising_via_Dependent_Dirichlet_Process_Tree\figure_4.jpg
  Figure 4 caption: "Performance of each algorithm on two images of BSDS500: 66053\
    \ and 295087. From left to right: clean image, noisy image with white Gaussian\
    \ noise ( \u03C3=50 ), result with K-SVD, SURE-guided GMM, BM3D, NL-Bayes and\
    \ our approach."
  Figure 5 Link: articels_figures_by_rev_year\2016\Blind_Image_Denoising_via_Dependent_Dirichlet_Process_Tree\figure_5.jpg
  Figure 5 caption: "Comparison of BM3D with noise estimation, [12] and ours on I23\
    \ in TID2008 contaminated by (a) homogeneous white Gaussian noise with \u03C3\
    =30 , PSNR: BM3D = 31.3907, multiscale = 28.9064, ours = 32.5856; (b) heterogeneous\
    \ noise with b=4 , PSNR: BM3D = 26.8859, multiscale = 25.2097, ours = 30.7666;\
    \ (c) Laplace noise with \u03C3=30 , PSNR: BM3D = 29.9387, multiscale = 28.3428,\
    \ ours = 32.4594; (d) uniform noise with a=30 , PSNR: BM3D = 33.2771, multiscale\
    \ = 32.1877, ours = 35.2331; (e) combined noise, PSNR: BM3D = 27.8518, multiscale\
    \ = 25.6925, ours = 30.8966; (f) complex noise, PSNR: BM3D = 30.1301, multiscale\
    \ = 24.7418, ours = 31.2972."
  Figure 6 Link: articels_figures_by_rev_year\2016\Blind_Image_Denoising_via_Dependent_Dirichlet_Process_Tree\figure_6.jpg
  Figure 6 caption: 'First line: From left to right, observed noisy images, the denoising
    results of adaptive BM3D and [12]. Second line: The results of NeatImage, Noise
    Ninja and ours.'
  Figure 7 Link: articels_figures_by_rev_year\2016\Blind_Image_Denoising_via_Dependent_Dirichlet_Process_Tree\figure_7.jpg
  Figure 7 caption: "An example of denoising real CCD images. First line: From left\
    \ to right, the ground truth, results of adaptive BM3D and the multiscale approach.\
    \ Second line: From left to right, the results of \u201CNoise Ninja\u201D, \u201C\
    Neat Image\u201D and ours."
  Figure 8 Link: articels_figures_by_rev_year\2016\Blind_Image_Denoising_via_Dependent_Dirichlet_Process_Tree\figure_8.jpg
  Figure 8 caption: 'From left to right: Old photos of young Winston Churchill and
    Old Tomas Morris, the denoising results of [12], NeatImage, Noise Ninja and ours.'
  Figure 9 Link: articels_figures_by_rev_year\2016\Blind_Image_Denoising_via_Dependent_Dirichlet_Process_Tree\figure_9.jpg
  Figure 9 caption: 'A crop of a screen-shot in the old movie ''Breakfast at Tiffany''s''.
    From left to right: Original image, denoising results of [12], NeatImage, Noise
    Ninja and ours. Zoom in for better visualization.'
  First author gender probability: 0.8
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Fengyuan Zhu
  Name of the last author: Pheng-Ann Heng
  Number of Figures: 9
  Number of Tables: 4
  Number of authors: 4
  Paper title: Blind Image Denoising via Dependent Dirichlet Process Tree
  Publication Date: 2016-08-31 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance on Synthesized Noisy Images with Homogeneous White
      Gaussian Noise with Different Deviation
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Averaged Execution Time ( s ) of Different Algorithms
  Table 3 caption:
    table_text: TABLE 3 Performance on Images Contaminated by Six Different Types
      of Noise
  Table 4 caption:
    table_text: TABLE 4 Performance on Real CCD Images
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2604816
- Affiliation of the first author: college of computer science and technology, jilin
    university, changchun, china
  Affiliation of the last author: national institute of parasitic diseases, chinese
    cdc, shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2016\Characterizing_and_Discovering_Spatiotemporal_Social_Contact_Patterns_for_Health\figure_1.jpg
  Figure 1 caption: The framework for inferring a dynamic social contact structure
    from heterogeneous data. The rich types of data explored comprise demographic
    data, surveillance data and epidemiological data gathered from socio-demographical
    census material available from governments, particular epidemic situations and
    medical reports available from CDCs and hospitals, respectively.
  Figure 10 Link: articels_figures_by_rev_year\2016\Characterizing_and_Discovering_Spatiotemporal_Social_Contact_Patterns_for_Health\figure_10.jpg
  Figure 10 caption: The phases of evolving social contact during the 2009 Hong Kong
    H1N1 outbreak. Panel A reveals two clear phases of social contact, corresponding
    to the summer holidays (July and August) and the autumn school term (October to
    December). Panels B and C show snapshots of the social contact of Hong Kong people
    during the summer holidays and the autumn school term, respectively.
  Figure 2 Link: articels_figures_by_rev_year\2016\Characterizing_and_Discovering_Spatiotemporal_Social_Contact_Patterns_for_Health\figure_2.jpg
  Figure 2 caption: Illustration of problem formulation and the framework for tensor
    deconvolution. The top part of the figure shows the epidemic dynamics monitored
    by public health institutions. The middle part shows the underlying dynamic social
    contact structure, M , which is segmented into multiple phases according to significant
    change-points in time. Social contact snapshots in the same phase have similar
    structures. The bottom part demonstrates a tensor deconvolution method for representing
    and inferring M based on a dictionary of social contact bases.
  Figure 3 Link: articels_figures_by_rev_year\2016\Characterizing_and_Discovering_Spatiotemporal_Social_Contact_Patterns_for_Health\figure_3.jpg
  Figure 3 caption: The spatial patterns of social contact in Hong Kong. Panels A
    to D represent the four basic bases of social contact, household, school, workplace
    and general community, respectively.
  Figure 4 Link: articels_figures_by_rev_year\2016\Characterizing_and_Discovering_Spatiotemporal_Social_Contact_Patterns_for_Health\figure_4.jpg
  Figure 4 caption: "The strength dynamics of the social contact bases \u03A6 . The\
    \ red dashed lines denote ground truth and the blue solid lines our estimations.\
    \ The time dimension is on the x -axis and the activity strength of the contact\
    \ bases on the y-axis."
  Figure 5 Link: articels_figures_by_rev_year\2016\Characterizing_and_Discovering_Spatiotemporal_Social_Contact_Patterns_for_Health\figure_5.jpg
  Figure 5 caption: The dynamics of two imaginary epidemics. The prior outbreak is
    the imaginary H1N1 influenza generated with the SIR model and the posterior outbreak
    is the imaginary mumps generated with the SEIR model. Red circles denote original
    dynamics, green diamonds denote synthetic surveillance data and blue solid lines
    represent predictions. The time dimension is on the x-axis and the number of cases
    of infection on the y-axis.
  Figure 6 Link: articels_figures_by_rev_year\2016\Characterizing_and_Discovering_Spatiotemporal_Social_Contact_Patterns_for_Health\figure_6.jpg
  Figure 6 caption: "Social contact phase segments. The variation strength of all\
    \ of the social contact bases in terms of \u2211 w \xAF \xAF \xAF \xAF is on the\
    \ y -axis. Long stems denote significant changes in social contact corresponding\
    \ to change-points. Three phases are segmented by such change-points."
  Figure 7 Link: articels_figures_by_rev_year\2016\Characterizing_and_Discovering_Spatiotemporal_Social_Contact_Patterns_for_Health\figure_7.jpg
  Figure 7 caption: Predicting incomplete epidemic dynamics. Red circles denote observed
    epidemic dynamics, green triangles denote unobserved dynamics and blue solid lines
    indicate predictions using our method.
  Figure 8 Link: articels_figures_by_rev_year\2016\Characterizing_and_Discovering_Spatiotemporal_Social_Contact_Patterns_for_Health\figure_8.jpg
  Figure 8 caption: The dynamics of social contact during the 2009 Hong Kong H1N1
    outbreak. The top panels show the estimated strengths of the household and school
    social contact bases, respectively, in terms of W (top panels) and overlineW (bottom
    panels). Two clear transitional phases are detected, corresponding to schools
    closing and autumn term starting.
  Figure 9 Link: articels_figures_by_rev_year\2016\Characterizing_and_Discovering_Spatiotemporal_Social_Contact_Patterns_for_Health\figure_9.jpg
  Figure 9 caption: 2009 Hong Kong H1N1 influenza dynamics with different social contact
    patterns. The solid line represents a 34-fold amplification of the real surveillance
    data. The solid line with triangles denotes the prediction using a dynamic social
    contact pattern. The solid line with circles denotes the prediction using a static
    social contact pattern. The dashed line and dashed line with circles denote the
    epidemic dynamics that would occur if the new autumn term were to be delayed by
    15 and 60 days, respectively.
  First author gender probability: 0.83
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Bo Yang
  Name of the last author: Shang Xia
  Number of Figures: 17
  Number of Tables: 1
  Number of authors: 5
  Paper title: Characterizing and Discovering Spatiotemporal Social Contact Patterns
    for Healthcare
  Publication Date: 2016-09-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Epidemiological Parameters in Experiments
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2605095
- Affiliation of the first author: computer and information science department and
    grasp laboratory, university of pennsylvania, philadelphia, pa
  Affiliation of the last author: computer and information science department and
    grasp laboratory, university of pennsylvania, philadelphia, pa
  Figure 1 Link: articels_figures_by_rev_year\2016\Sparse_Representation_for_D_Shape_Estimation_A_Convex_Relaxation_Approach\figure_1.jpg
  Figure 1 caption: Illustration of the problem studied in this paper. The unknown
    3D model is defined by a set of landmarks and assumed to be a linear combination
    of some predefined basis shapes with sparse coefficients. Given the 2D correspondences
    of the landmarks in a single image, the computational problem is to simultaneously
    estimate the coefficients of the sparse representation as well as the viewpoint
    of the camera.
  Figure 10 Link: articels_figures_by_rev_year\2016\Sparse_Representation_for_D_Shape_Estimation_A_Convex_Relaxation_Approach\figure_10.jpg
  Figure 10 caption: Qualitative results on the FG3DCar dataset with detected 2D landmarks.
    In each example, the detected landmarks superposed on the original image, 2D fitted
    model and 3D reconstruction are shown. The 2D landmarks are located by learned
    detectors based on HOG and SVM. The green and red dots correspond to the inliers
    and outliers, respectively. The outlier is defined as any detected landmark at
    least 30-pixel away from the manual annotation. Note that the inlieroutlier classification
    is only for the purpose of illustration and not for model fitting, i.e., the models
    are fitted with all landmarks despite of the correctness of detection. The last
    row shows two failed examples.
  Figure 2 Link: articels_figures_by_rev_year\2016\Sparse_Representation_for_D_Shape_Estimation_A_Convex_Relaxation_Approach\figure_2.jpg
  Figure 2 caption: Illustration of the proximal operator of the spectral norm.
  Figure 3 Link: articels_figures_by_rev_year\2016\Sparse_Representation_for_D_Shape_Estimation_A_Convex_Relaxation_Approach\figure_3.jpg
  Figure 3 caption: The frequency of exact recovery. The intensity indicates the frequency
    of success under a variety of problem settings.
  Figure 4 Link: articels_figures_by_rev_year\2016\Sparse_Representation_for_D_Shape_Estimation_A_Convex_Relaxation_Approach\figure_4.jpg
  Figure 4 caption: Representability of principal component analysis (PCA), sparse
    coding (SC) and nonnegative sparse coding (NNSC) for human poses.
  Figure 5 Link: articels_figures_by_rev_year\2016\Sparse_Representation_for_D_Shape_Estimation_A_Convex_Relaxation_Approach\figure_5.jpg
  Figure 5 caption: Quantitative results on the CMU motion capture dataset. (a) The
    mean 3D estimation errors for different motions. (b) The box plot of estimation
    errors. (c) The comparison between objective values achieved by the mean shape
    initialization and by the convex initialization. (d) The sensitivity to Gaussian
    noise. (e) The sensitivity to outliers. The length of error bar in (d) and (e)
    indicates half standard deviation.
  Figure 6 Link: articels_figures_by_rev_year\2016\Sparse_Representation_for_D_Shape_Estimation_A_Convex_Relaxation_Approach\figure_6.jpg
  Figure 6 caption: Qualitative results on the CMU motion capture dataset. The rows
    from top to bottom correspond to the input 2D poses, the ground-truth 3D poses
    (visualized in a novel view), and the reconstructions from the proposed method,
    the alternating minimization, and the PMP method [8], respectively. Red and green
    indicate left and right, respectively.
  Figure 7 Link: articels_figures_by_rev_year\2016\Sparse_Representation_for_D_Shape_Estimation_A_Convex_Relaxation_Approach\figure_7.jpg
  Figure 7 caption: Qualitative results on the PARSE dataset with 2D poses detected
    by an existing 2D pose detector [54]. In each example, the detected 2D pose superposed
    on the original image and the reconstruction in two different views are shown.
    The last row shows two failed examples.
  Figure 8 Link: articels_figures_by_rev_year\2016\Sparse_Representation_for_D_Shape_Estimation_A_Convex_Relaxation_Approach\figure_8.jpg
  Figure 8 caption: Qualitative results on the FG3DCar dataset given 2D correspondences.
    The columns correspond to the original image, the input 2D landmarks, and the
    2D3D models output by the proposed method and the nonlinear optimization [15],
    respectively. Only visible landmarks ( sim 40 per image) are used for model fitting.
    The 3D models are visualized in a novel view different from the original image.
    The car models are the BMW 5 Series 2011 (sedan), the Nissan Xterra 2005 (SUV)
    and the Dodge Ram 2003 (pick-up truck), respectively.
  Figure 9 Link: articels_figures_by_rev_year\2016\Sparse_Representation_for_D_Shape_Estimation_A_Convex_Relaxation_Approach\figure_9.jpg
  Figure 9 caption: "Quantitative comparison between the proposed method (\u201Cconvex+refine\u201D\
    ) and the alternating minimization (\u201Caltern\u201D) on the FG3DCar dataset\
    \ with synthesized outliers. (a) The 2D shape error. (b) The objective value."
  First author gender probability: 0.69
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Xiaowei Zhou
  Name of the last author: Kostas Daniilidis
  Number of Figures: 11
  Number of Tables: 0
  Number of authors: 4
  Paper title: 'Sparse Representation for 3D Shape Estimation: A Convex Relaxation
    Approach'
  Publication Date: 2016-09-01 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2605097
- Affiliation of the first author: key laboratory of symbolic computation and knowledge
    engineer, jilin university, changchun, jilin, china
  Affiliation of the last author: department of computing, the hong kong polytechnic
    university, hung hom, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2016\Social_Collaborative_Filtering_by_Trust\figure_1.jpg
  Figure 1 caption: 'Epinions: An example of online social networking services.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Social_Collaborative_Filtering_by_Trust\figure_2.jpg
  Figure 2 caption: '(a) Truster model B T V : How others affect user i ''s opinions.
    (b) Trustee model W T V : how others follow user i ''s opinions.'
  Figure 3 Link: articels_figures_by_rev_year\2016\Social_Collaborative_Filtering_by_Trust\figure_3.jpg
  Figure 3 caption: The graphical model of Truster-PMF.
  Figure 4 Link: articels_figures_by_rev_year\2016\Social_Collaborative_Filtering_by_Trust\figure_4.jpg
  Figure 4 caption: The graphical model of TrustPMF.
  Figure 5 Link: articels_figures_by_rev_year\2016\Social_Collaborative_Filtering_by_Trust\figure_5.jpg
  Figure 5 caption: Degree distribution of each group of users. The average number
    of ratings of each group is shown on the top of corresponding bar.
  Figure 6 Link: articels_figures_by_rev_year\2016\Social_Collaborative_Filtering_by_Trust\figure_6.jpg
  Figure 6 caption: Experimental results on testing different group of users.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.83
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.75
  Name of the first author: Bo Yang
  Name of the last author: Wenjie Li
  Number of Figures: 6
  Number of Tables: 8
  Number of authors: 4
  Paper title: Social Collaborative Filtering by Trust
  Publication Date: 2016-09-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Training Algorithm of TrustPMF
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Statistics of Datasets
  Table 3 caption:
    table_text: TABLE 3 Parameter Settings of Compared Methods
  Table 4 caption:
    table_text: TABLE 4 Experimental Results on Testing All Users
  Table 5 caption:
    table_text: TABLE 5 Experimental Results on Testing Cold Start Users
  Table 6 caption:
    table_text: TABLE 6 Performance of IR Metrics on Epinions Dataset
  Table 7 caption:
    table_text: TABLE 7 Performance of IR Metrics on Douban Dataset
  Table 8 caption:
    table_text: TABLE 8 Comparisons with TrustSVD, SPF and Popularity in Terms of
      Top- k Based Ranking Metrics
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2605085
- Affiliation of the first author: department of electrical engineering and computer
    science, university of california, berkeley, berkeley, ca
  Affiliation of the last author: international computer science institute, berkeley,
    ca
  Figure 1 Link: articels_figures_by_rev_year\2016\LongTerm_Recurrent_Convolutional_Networks_for_Visual_Recognition_and_Description\figure_1.jpg
  Figure 1 caption: We propose Long-term Recurrent Convolutional Networks (LRCNs),
    a class of architectures leveraging the strengths of rapid progress in CNNs for
    visual recognition problems, and the growing desire to apply such models to time-varying
    inputs and outputs. LRCN processes the (possibly) variable-length visual input
    (left) with a CNN (middle-left), whose outputs are fed into a stack of recurrent
    sequence models (LSTMs, middle-right), which finally produce a variable-length
    prediction (right). Both the CNN and LSTM weights are shared across time, resulting
    in a representation that scales to arbitrarily long sequences.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\LongTerm_Recurrent_Convolutional_Networks_for_Visual_Recognition_and_Description\figure_2.jpg
  Figure 2 caption: A diagram of a basic RNN cell (left) and an LSTM memory cell (right)
    used in this paper (from [13], a slight simplification of the architecture described
    in [14], which was derived from the LSTM initially proposed in [7]).
  Figure 3 Link: articels_figures_by_rev_year\2016\LongTerm_Recurrent_Convolutional_Networks_for_Visual_Recognition_and_Description\figure_3.jpg
  Figure 3 caption: Task-specific instantiations of our LRCN model for activity recognition,
    image description, and video description.
  Figure 4 Link: articels_figures_by_rev_year\2016\LongTerm_Recurrent_Convolutional_Networks_for_Visual_Recognition_and_Description\figure_4.jpg
  Figure 4 caption: "Three variants of the LRCN image captioning architecture that\
    \ we experimentally evaluate. We explore the effect of depth in the LSTM stack,\
    \ and the effect of the \u201Cfactorization\u201D of the modalities."
  Figure 5 Link: articels_figures_by_rev_year\2016\LongTerm_Recurrent_Convolutional_Networks_for_Visual_Recognition_and_Description\figure_5.jpg
  Figure 5 caption: Our approaches to video description. (a) LSTM encoder & decoder
    with CRF max, (b) LSTM decoder with CRF max, (c) LSTM decoder with CRF probabilities.
  Figure 6 Link: articels_figures_by_rev_year\2016\LongTerm_Recurrent_Convolutional_Networks_for_Visual_Recognition_and_Description\figure_6.jpg
  Figure 6 caption: 'Image description: Images with corresponding captions generated
    by our finetuned LRCN model. These are images 1-12 of our randomly chosen validation
    set from COCO 2014 [33]. We used beam search with a beam size of 5 to generate
    the sentences, and display the top (highest likelihood) result above.'
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jeff Donahue
  Name of the last author: Trevor Darrell
  Number of Figures: 6
  Number of Tables: 9
  Number of authors: 7
  Paper title: Long-Term Recurrent Convolutional Networks for Visual Recognition and
    Description
  Publication Date: 2016-09-01 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Activity Recognition: Comparing Single Frame Models to LRCN
      Networks for Activity Recognition on the UCF101 [25] Dataset, with RGB and Flow
      Inputs'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Activity Recognition: Comparison of Improvement \u0394 in\
      \ LRCN's Per-Class Recognition Accuracy versus the Single-Frame Baseline"
  Table 3 caption:
    table_text: 'TABLE 3 Activity Recognition: Comparison of Per-Class Recognition
      Accuracy Between the Flow and RGB LRCN Models'
  Table 4 caption:
    table_text: 'TABLE 4 Image Description: Retrieval Results for the Flickr30k [32]
      Datasets'
  Table 5 caption:
    table_text: TABLE 5 Retrieval Results (Image to Caption and Caption to Image)
      for a Randomly Chosen Subset (1,000 Images) of the COCO 2014 [33] Validation
      Set
  Table 6 caption:
    table_text: TABLE 6 Image Caption Generation Performance (Under the BLEU 1-4 [34]
      (B1-B4), CIDEr-D [35] (C), METEOR [36] (M), and ROUGE-L [37] (R) Metrics) Across
      Various Network Architectures and Generation Strategies
  Table 7 caption:
    table_text: TABLE 7 Image Caption Generation Results from Top-Performing Methods
      in the 2015 COCO Caption Challenge Competition, Sorted by Performance Under
      the CIDEr-D Metric
  Table 8 caption:
    table_text: 'TABLE 8 Image Description: Human Evaluator Rankings from 1-6 (Low
      Is Good) Averaged for Each Method and Criterion'
  Table 9 caption:
    table_text: 'TABLE 9 Video Description: Results on Detailed Description of TACoS
      Multilevel [48] , in Percent, See Section 6 for Details'
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2599174
- Affiliation of the first author: korea advanced institute of science and technology,
    daejeon, korea
  Affiliation of the last author: korea advanced institute of science and technology,
    daejeon, korea
  Figure 1 Link: articels_figures_by_rev_year\2016\Automatic_Trimap_Generation_and_Consistent_Matting_for_LightField_Images\figure_1.jpg
  Figure 1 caption: "(a) Parallel plane representation of a light-field image. (b)\
    \ The multiple sub-images of a light-field image after decoding. (c) After stacking\
    \ the images within the yellow line region in (b), we have an epipolar plane image\
    \ in x\u2212s plane with fixed y,t ."
  Figure 10 Link: articels_figures_by_rev_year\2016\Automatic_Trimap_Generation_and_Consistent_Matting_for_LightField_Images\figure_10.jpg
  Figure 10 caption: 'Qualitative comparisons on Lytro dataset with user inputs. We
    compare our estimated alpha mattes with results from previous methods. Top to
    bottom: Input images, results from closed form matting [9], non-local matting
    [12], KNN matting [13], comprehensive matting [7], video matting [45], our method,
    and ground truth.'
  Figure 2 Link: articels_figures_by_rev_year\2016\Automatic_Trimap_Generation_and_Consistent_Matting_for_LightField_Images\figure_2.jpg
  Figure 2 caption: (a) EPI of matting boundary. (b) Alpha matte of the EPI image.
    (c) Foreground and background color sample correspondences of a pixel in the matting
    area.
  Figure 3 Link: articels_figures_by_rev_year\2016\Automatic_Trimap_Generation_and_Consistent_Matting_for_LightField_Images\figure_3.jpg
  Figure 3 caption: (a) Estimated depth map from [39]. (b) Binary mask. (c) Guided
    matte from [40]. (d) Trimap.
  Figure 4 Link: articels_figures_by_rev_year\2016\Automatic_Trimap_Generation_and_Consistent_Matting_for_LightField_Images\figure_4.jpg
  Figure 4 caption: (a) An image of central view. (b) An estimated trimap from Eq.
    (10) . (c) Regions for clustering. Blue, green, and red areas are for foreground,
    unknown, background color samples. (d,e,f) show color samples for clustering.
    In this example, foreground and background color distributions are well separated,
    and unknown distribution has mixtures of foreground and background colors.
  Figure 5 Link: articels_figures_by_rev_year\2016\Automatic_Trimap_Generation_and_Consistent_Matting_for_LightField_Images\figure_5.jpg
  Figure 5 caption: The effects of window size, blue, green, and red areas are foreground,
    unknown, and background regions of a estimated trimap from Eq. (10) respectively.
    (a) Long hair structures with small window size. Note that background color (sky
    blue) are included in foreground regions. (b) Short hair structures with large
    window size. Note that unknown regions of a triamp are unnecessarily large. Window
    size of guided filter should be changed properly according to the object structures.
  Figure 6 Link: articels_figures_by_rev_year\2016\Automatic_Trimap_Generation_and_Consistent_Matting_for_LightField_Images\figure_6.jpg
  Figure 6 caption: (a) A center view image with EPI an image. (b,c) Foreground and
    background images masked using trimap with EPI images.
  Figure 7 Link: articels_figures_by_rev_year\2016\Automatic_Trimap_Generation_and_Consistent_Matting_for_LightField_Images\figure_7.jpg
  Figure 7 caption: "(a) Case 1: If B 1 \u2260 B 2 , we solve the alpha in a closed\
    \ form. If B 1 = B 2 , we solve the alpha using the comprehensive sample set.\
    \ (b) Case 2: Foreground and Background samples are solved individually along\
    \ its EPI, and the median F and B are selected to solve the alpha."
  Figure 8 Link: articels_figures_by_rev_year\2016\Automatic_Trimap_Generation_and_Consistent_Matting_for_LightField_Images\figure_8.jpg
  Figure 8 caption: Illustration of color sample selection via propagation along with
    EPI line. Red arrows show directions of updating. This process reduces the Case
    2 to the Case1.
  Figure 9 Link: articels_figures_by_rev_year\2016\Automatic_Trimap_Generation_and_Consistent_Matting_for_LightField_Images\figure_9.jpg
  Figure 9 caption: Our dataset for testing. From top to bottom rows show center view
    images, trimaps given by users, estimated trimaps using a proposed method in Section
    4 and ground truth alpha mattes respectively.
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.88
  Name of the first author: Donghyeon Cho
  Name of the last author: In So Kweon
  Number of Figures: 15
  Number of Tables: 1
  Number of authors: 4
  Paper title: Automatic Trimap Generation and Consistent Matting for Light-Field
    Images
  Publication Date: 2016-09-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Comparisons in Terms of RMSE and Consistency
      with Trimaps Given by User and Automatically Generated Trimaps
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2606397
