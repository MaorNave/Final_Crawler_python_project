- Affiliation of the first author: vistec, rayong, thailand
  Affiliation of the last author: vistec, rayong, thailand
  Figure 1 Link: articels_figures_by_rev_year\2022\Repurposing_GANs_for_OneShot_Semantic_Part_Segmentation\figure_1.jpg
  Figure 1 caption: One-shot segmentation results. In each task, our segmentation
    network is given only one example of part labels.
  Figure 10 Link: articels_figures_by_rev_year\2022\Repurposing_GANs_for_OneShot_Semantic_Part_Segmentation\figure_10.jpg
  Figure 10 caption: Results on one-shot part segmentation from BigGANs generated
    images. Each segmentation network is trained on 1 annotation from different classes
    and tested with other classes. The last row is the results from the segmentation
    network trained with 3 training images together.
  Figure 2 Link: articels_figures_by_rev_year\2022\Repurposing_GANs_for_OneShot_Semantic_Part_Segmentation\figure_2.jpg
  Figure 2 caption: "Representation extraction To extract a representation from an\
    \ image, we embed the image into the latent space of GAN by optimizing for the\
    \ latent z that reproduces the input image. z is then fed to the generator and\
    \ we collect multiple activation maps a 1 , a 2 ,\u2026, a n of dimensions ( h\
    \ 1 , w 1 , c 1 ),\u2026,( h n , w n , c n ) . Each of these maps is upsampled\
    \ to A i with dimension ( h n , w n , c i ) . The representation is a concatenation\
    \ of all A i along the channel dimension."
  Figure 3 Link: articels_figures_by_rev_year\2022\Repurposing_GANs_for_OneShot_Semantic_Part_Segmentation\figure_3.jpg
  Figure 3 caption: Few-shot segmentation pipeline For training, we use a trained
    GAN to generate a few images along with their representations by feeding random
    latent codes. Then, we manually annotate these images and train our few-shot segmenter
    to output segmentation maps that match our annotated masks. For inference, we
    extract a representation from a test image (Fig. 2) then input it to the few-shot
    segmenter to obtain a segmentation map.
  Figure 4 Link: articels_figures_by_rev_year\2022\Repurposing_GANs_for_OneShot_Semantic_Part_Segmentation\figure_4.jpg
  Figure 4 caption: Auto-shot segmentation pipeline during training, the auto-shot
    segmenter uses generated images from the trained GAN as input and segmentation
    masks predicted by the few-shot segmenter as ground truth.
  Figure 5 Link: articels_figures_by_rev_year\2022\Repurposing_GANs_for_OneShot_Semantic_Part_Segmentation\figure_5.jpg
  Figure 5 caption: Few-shot face segmentation results on CelebAMASK-HQ.
  Figure 6 Link: articels_figures_by_rev_year\2022\Repurposing_GANs_for_OneShot_Semantic_Part_Segmentation\figure_6.jpg
  Figure 6 caption: 10-class face segmentation results of supervised baseline and
    the number of segmentation labels used. Our few-shot segmentation results are
    shown in dot line for comparison. Supervised baseline consumes over 100 annotations
    to surpass our 1-shot segmenter, and around 500 annotation to reach same-level
    of IOU on our 10-shot segmenter.
  Figure 7 Link: articels_figures_by_rev_year\2022\Repurposing_GANs_for_OneShot_Semantic_Part_Segmentation\figure_7.jpg
  Figure 7 caption: One-shot car part segmentation results on GANs generated images.
    The segmentation network can segment car images from varied points of view even
    though it is trained on annotations of one car from one angle. However, there
    are some failure cases when the cars appear unusually big or small, or when GANs
    generate unrealistic cars.
  Figure 8 Link: articels_figures_by_rev_year\2022\Repurposing_GANs_for_OneShot_Semantic_Part_Segmentation\figure_8.jpg
  Figure 8 caption: Results on few-shot horse part segmentation from GANs generated
    images. Compared to cars and faces with good 1-shot results, horses need more
    labels. 1-shot horse segmentation often mistakes the rider as a part of horse.
  Figure 9 Link: articels_figures_by_rev_year\2022\Repurposing_GANs_for_OneShot_Semantic_Part_Segmentation\figure_9.jpg
  Figure 9 caption: Some examples of auto-shot segmentation trained with datasets
    generated by 10-shot segmenters on CelebAMask-HQ, PASCAL-Part Car and PASCAL-Part
    horse. The pretrained StyleGAN2 for each class was trained on FFHQ, LSUN-Car and
    LSUN-Horse.
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.86
  Name of the first author: Pitchaporn Rewatbowornwong
  Name of the last author: Supasorn Suwajanakorn
  Number of Figures: 16
  Number of Tables: 9
  Number of authors: 3
  Paper title: Repurposing GANs for One-Shot Semantic Part Segmentation
  Publication Date: 2022-08-24 00:00:00
  Table 1 caption: TABLE 1 IOU Scores of Our 10-Shot versus Auto-Shot Segmenters on
    10-Class Face Segmentation
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Weighted IOU Scores on Few-Shot Human Face Segmentation
  Table 3 caption: TABLE 3 Per-Class IOU Scores on 3-Class Human Face Segmentation
  Table 4 caption: TABLE 4 IOU Scores on PASCAL-Parts Car Segmentation
  Table 5 caption: TABLE 5 IOU Scores on PASCAL-Parts Horse Segmentation
  Table 6 caption: TABLE 6 Average IOU Scores on PASCAL-Parts Horse Segmentation
  Table 7 caption: TABLE 7 IOU Scores on 1-Shot Face Segmentation of Different Size
    of Few-Shot Segmenters
  Table 8 caption: "TABLE 8 The Mean IOU Scores (\xB1Standard Deviation n=5) of 5-Shot\
    \ Segmentation Network Trained on Datasets Selected by 4 Sources: Our Method,\
    \ Author, Novice, and Random Selection"
  Table 9 caption: TABLE 9 Normalized MSE Scores on Facial Landmark Detection
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3201285
- Affiliation of the first author: king abdullah university of science and technology,
    thuwal, saudi arabia
  Affiliation of the last author: king abdullah university of science and technology,
    thuwal, saudi arabia
  Figure 1 Link: articels_figures_by_rev_year\2022\On_the_Decision_Boundaries_of_Neural_Networks_A_Tropical_Geometry_Perspective\figure_1.jpg
  Figure 1 caption: "Tropical Hypersurfaces and their Corresponding Dual Subdivisions.\
    \ We show three tropical polynomials, where the solid red and black lines are\
    \ the tropical hypersurfaces T(f) and dual subdivisions \u03B4(f) to the corresponding\
    \ tropical polynomials, respectively. T(f) divides The domain of f into convex\
    \ regions where f is linear. Moreover, each region is in one-to-one correspondence\
    \ with each node of \u03B4(f) . Lastly, the tropical hypersurfaces are parallel\
    \ to the normals of the edges of \u03B4(f) shown by dashed red lines."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\On_the_Decision_Boundaries_of_Neural_Networks_A_Tropical_Geometry_Perspective\figure_2.jpg
  Figure 2 caption: "Decision Boundaries as Geometric Structures. The decision boundaries\
    \ B (in red) comprise two linear pieces separating classes C 1 and C 2 . As per\
    \ Theorem 2, the dual subdivision of this single hidden neural network is the\
    \ convex hull between the zonotopes Z G 1 and Z G 2 . The normals to the dual\
    \ subdivison \u03B4(R(x)) are in one-to-one correspondence to the tropical hypersurface\
    \ T(R(x)) , which is a superset to the decision boundaries B . Note that some\
    \ of the normals to \u03B4(R(x)) (in red) are parallel to the decision boundaries."
  Figure 3 Link: articels_figures_by_rev_year\2022\On_the_Decision_Boundaries_of_Neural_Networks_A_Tropical_Geometry_Perspective\figure_3.jpg
  Figure 3 caption: 'Effect of Different Initializations on the Decision Boundaries
    Polytope. From left to right: training dataset, decision boundaries polytope of
    original network (before pruning), followed by the decision boundaries polytope
    for networks pruned at different pruning percentages using different initializations.
    Note that in the original polytope there are many more vertices than just 4, but
    they are very close to each other forming many small edges that are not visible
    in the figure.'
  Figure 4 Link: articels_figures_by_rev_year\2022\On_the_Decision_Boundaries_of_Neural_Networks_A_Tropical_Geometry_Perspective\figure_4.jpg
  Figure 4 caption: Tropical Pruning Pipeline. Pruning the 4th node, or equivalently
    removing the two yellow vertices of zonotope Z G 2 does not affect the decision
    boundaries polytope, which will lead to no change in accuracy.
  Figure 5 Link: articels_figures_by_rev_year\2022\On_the_Decision_Boundaries_of_Neural_Networks_A_Tropical_Geometry_Perspective\figure_5.jpg
  Figure 5 caption: 'Pruning Ressults on Toy Networks. We apply tropical pruning on
    the toy network that is in the form of Affine followed by a ReLU followed by another
    Affine. From left to right: (a) dataset used for training (b) pruning networks
    with 100 hidden nodes (c) 200 hidden nodes (d) 300 hidden nodes.'
  Figure 6 Link: articels_figures_by_rev_year\2022\On_the_Decision_Boundaries_of_Neural_Networks_A_Tropical_Geometry_Perspective\figure_6.jpg
  Figure 6 caption: Results of Tropical Pruning. Pruning-accuracy plots for AlexNet
    (top) and VGG16 (bottom) trained on SVHN, CIFAR10, and CIFAR100, pruned with our
    tropical method and three other pruning methods.
  Figure 7 Link: articels_figures_by_rev_year\2022\On_the_Decision_Boundaries_of_Neural_Networks_A_Tropical_Geometry_Perspective\figure_7.jpg
  Figure 7 caption: 'Dual View of Tropical Adversarial Attacks. We show the effects
    of tropical adversarial attacks on a synthetic binary dataset at four different
    input points (black dots in black). From left to right: the decision regions of
    the original model, perturbed model, and the corresponding decision boundaries
    polytopes (green for original and blue for perturbed).'
  Figure 8 Link: articels_figures_by_rev_year\2022\On_the_Decision_Boundaries_of_Neural_Networks_A_Tropical_Geometry_Perspective\figure_8.jpg
  Figure 8 caption: 'Effect of Tropical Adversarial Attacks on MNIST Images. First
    row from the left: Clean image, perturbed images classified as [7,3,2,1,0] respectively.
    Second row from left: Clean image, perturbed images classified as [9,8,7,3,2]
    respectively. Third row from left: Clean image, perturbed images classified as
    [9,8,7,5,3] respectively. Fourth row from left: Clean image, perturbed images
    classified as [9,4,3,2,1] respectively. Fifth row from left: Clean image, perturbed
    images classified as [8,4,3,2,1] respectively.'
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Motasem Alfarra
  Name of the last author: Bernard Ghanem
  Number of Figures: 8
  Number of Tables: 1
  Number of authors: 5
  Paper title: 'On the Decision Boundaries of Neural Networks: A Tropical Geometry
    Perspective'
  Publication Date: 2022-08-24 00:00:00
  Table 1 caption: TABLE 1 Robust Accuracy on MNIST
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3201490
- Affiliation of the first author: beijing key laboratory of mobile computing and
    pervasive device, institute of computing technology, chinese academy of sciences,
    beijing, china
  Affiliation of the last author: beijing key laboratory of mobile computing and pervasive
    device, institute of computing technology, chinese academy of sciences, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2022\Human_Motion_Transfer_With_D_Constraints_and_Detail_Enhancement\figure_1.jpg
  Figure 1 caption: Given two monocular video clips, our method is able to transfer
    the motion of a source character (Top) to a target character (Middle), with realistic
    details (Bottom). We zoom in the enhanced details for better visualization.
  Figure 10 Link: articels_figures_by_rev_year\2022\Human_Motion_Transfer_With_D_Constraints_and_Detail_Enhancement\figure_10.jpg
  Figure 10 caption: Comparison s with the state-of-the-art methods. We show the generated
    results by vid2vid, Everybody Dance Now by Chan et al. Liquid Warping GAN (LW-GAN),
    and our method. Our method produces richer and more realistic details.
  Figure 2 Link: articels_figures_by_rev_year\2022\Human_Motion_Transfer_With_D_Constraints_and_Detail_Enhancement\figure_2.jpg
  Figure 2 caption: 'The architecture of our pipeline. Training: At Stage I, we train
    MT-Net(src) and MT-Net(tgt) separately within the corresponding domains. The MT-Net
    transfers the motion from temporally adjacent pose labels to the image domain
    represented by the appearance image. At Stage II, we send a source appearance
    image I S app and adjacent target pose labels I T pose to MT-Net(src) to obtain
    a coarse source image I S MT . After aligned to the target characters position
    and size, it is converted to the blending domain by blended with a corresponding
    real target frame. The DE-Net learns to translate the blended image to the target
    domain. Transfer: Our system obtains a coarse transfer result I T MT with target
    appearance and the source pose labels by MT-Net(tgt), and converts I T MT to the
    blending domain by blending it with the aligned real source frame. Finally the
    DE-Net translates the blended image I blend to the target domain with details
    enhanced.'
  Figure 3 Link: articels_figures_by_rev_year\2022\Human_Motion_Transfer_With_D_Constraints_and_Detail_Enhancement\figure_3.jpg
  Figure 3 caption: Illustration of the 3D constraints generation process. We reconstruct
    a 3D mesh of a source or target subject image I X real , associate the three smallest
    eigenvalues of its Laplace matrix as intrinsic features (visualized in RGB color)
    with the mesh vertices, and project the colored mesh to form a 3D constraint image,
    denoted as I X pose .
  Figure 4 Link: articels_figures_by_rev_year\2022\Human_Motion_Transfer_With_D_Constraints_and_Detail_Enhancement\figure_4.jpg
  Figure 4 caption: "Architecture of the generative network. Appearance image I X\
    \ app is randomly selected and sent to the Appearance Encoder (denoted as E app\
    \ ) to obtain appearance features. Adjacent pose label images including I X pose\
    \ (for the current frame) and I \u2032 X pose (for the previous frame) are sent\
    \ to Pose GAN (denoted as G pose ) together with the appearance features to generate\
    \ a reconstructed image I X MT ."
  Figure 5 Link: articels_figures_by_rev_year\2022\Human_Motion_Transfer_With_D_Constraints_and_Detail_Enhancement\figure_5.jpg
  Figure 5 caption: Comparison of a source image I S real and a source-to-target transfer
    result I T MT , as well as a target image I T real and a target-to-source transfer
    result I S MT . The upper circle s show the blending results in hands and face
    regions. The blending results are of a similar style for the two opposite blending
    modes.
  Figure 6 Link: articels_figures_by_rev_year\2022\Human_Motion_Transfer_With_D_Constraints_and_Detail_Enhancement\figure_6.jpg
  Figure 6 caption: "Architecture of Detail Enhancement Net (DE-Net). The main part\
    \ of our DE-Net is a U-net, which takes the linear blending of paired data ( I\
    \ \u02DC S MT , I T real ) or ( I \u02DC S real , I T MT ) as input, and synthesizes\
    \ a target image I T DE with details enhanced."
  Figure 7 Link: articels_figures_by_rev_year\2022\Human_Motion_Transfer_With_D_Constraints_and_Detail_Enhancement\figure_7.jpg
  Figure 7 caption: The illustration of post-processing facial enhancement. The cropped
    facial part f syn of the synthesized frame I T DE is embedded into the StyleGAN3
    latent space together with real faces F real = f i real . Through a retrieval-and-interpolation
    strategy, the latent embedding is refined to get an enhanced face, which is finally
    fused to the synthesized frame with a predicted facial mask.
  Figure 8 Link: articels_figures_by_rev_year\2022\Human_Motion_Transfer_With_D_Constraints_and_Detail_Enhancement\figure_8.jpg
  Figure 8 caption: Motion transfer results. We show the generated frames of several
    subjects with different genders, races, and builds. In each group, the top row
    shows the source subject and the bottom row shows the generated target subject.
    Please refer to the supplemental materials for synthesized videos.
  Figure 9 Link: articels_figures_by_rev_year\2022\Human_Motion_Transfer_With_D_Constraints_and_Detail_Enhancement\figure_9.jpg
  Figure 9 caption: The effect of the number of source frames on transfer results.
    We show the inception score of transfer results ( y -axis) with respect to the
    ratio of source frame number to target ( x -axis). For the inception score, the
    higher is the better.
  First author gender probability: 0.51
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.64
  Name of the first author: Yang-Tian Sun
  Name of the last author: Lin Gao
  Number of Figures: 16
  Number of Tables: 4
  Number of authors: 7
  Paper title: Human Motion Transfer With 3D Constraints and Detail Enhancement
  Publication Date: 2022-08-26 00:00:00
  Table 1 caption: TABLE 1 Quantitative Comparisons With the State-of-the-art Methods
    on the Dance Dataset
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Ablation Study
  Table 3 caption: TABLE 3 User Study Results
  Table 4 caption: TABLE 4 Ablation Study for Pose Labels
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3201904
- Affiliation of the first author: school of artificial intelligence, optics and electronics
    (iopen), northwestern polytechnical university, xian, china
  Affiliation of the last author: school of artificial intelligence, optics and electronics
    (iopen), northwestern polytechnical university, xian, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Unsupervised_Graph_Embedding_via_Adaptive_Graph_Learning\figure_1.jpg
  Figure 1 caption: The architecture of the proposed framework (See Algorithm 1 for
    details).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Unsupervised_Graph_Embedding_via_Adaptive_Graph_Learning\figure_2.jpg
  Figure 2 caption: Information flow of the adaptive learning process.
  Figure 3 Link: articels_figures_by_rev_year\2022\Unsupervised_Graph_Embedding_via_Adaptive_Graph_Learning\figure_3.jpg
  Figure 3 caption: The parameters study results on COIL dataset.
  Figure 4 Link: articels_figures_by_rev_year\2022\Unsupervised_Graph_Embedding_via_Adaptive_Graph_Learning\figure_4.jpg
  Figure 4 caption: The data visualization comparison on Cora.
  Figure 5 Link: articels_figures_by_rev_year\2022\Unsupervised_Graph_Embedding_via_Adaptive_Graph_Learning\figure_5.jpg
  Figure 5 caption: The data visualization comparison on Citeseer.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Rui Zhang
  Name of the last author: Chengjun Lu
  Number of Figures: 5
  Number of Tables: 9
  Number of authors: 3
  Paper title: Unsupervised Graph Embedding via Adaptive Graph Learning
  Publication Date: 2022-08-26 00:00:00
  Table 1 caption: TABLE 1 Information of the Two Graph Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Information of the Six General Datasets
  Table 3 caption: TABLE 3 The Node Clustering Results on the Cora Dataset
  Table 4 caption: TABLE 4 The Node Clustering Results on the Citeseer Dataset
  Table 5 caption: TABLE 5 The F1-Score (%) on the Node Classification Task
  Table 6 caption: "TABLE 6 The Parameters Study (%) for \u03BD \u03BD on COIL Dataset"
  Table 7 caption: TABLE 7 The Link Prediction Results on Graph Datasets (1)
  Table 8 caption: TABLE 8 The Link Prediction Results on Graph Datasets (2)
  Table 9 caption: TABLE 9 The Link Prediction Results on OBG-ddi
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3202158
- Affiliation of the first author: moe key lab of artificial intelligence, ai institute,
    shanghai jiao tong university, shanghai, china
  Affiliation of the last author: moe key lab of artificial intelligence, ai institute,
    shanghai jiao tong university, shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Fast_Quaternion_Product_Units_for_Learning_Disentangled_Representations_in_SO\figure_1.jpg
  Figure 1 caption: An illustration of our quaternion product unit. Each rotation
    in SO(3) can be represented via a unit quaternion in a hypercomplex space H ,
    whose real part and imaginary part indicates the rotation angle and the direction
    of the rotation axis, respectively. The QPU merges input rotations via a weighted
    chain of Hamilton products and derives a rotation as its output, whose real part
    and imaginary part yield to the rotation-invariance and the rotation-equivariance,
    respectively. Here the weighting operation imposed on each input quaternion is
    achieved by a shifting-scaling mechanism of the corresponding rotation angle.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Fast_Quaternion_Product_Units_for_Learning_Disentangled_Representations_in_SO\figure_2.jpg
  Figure 2 caption: An illustration of the rotation-invariance and the rotation-equivariance
    of QPU. For each skeleton, we represent the relative rotations between its joints
    as unit quaternions and pass them through a QPU. To visualize the rotation discrepancy,
    for a v o in (c) and the corresponding v o in (d), we connect each of them (normalized
    to a sphere) with the origin via a red line.
  Figure 3 Link: articels_figures_by_rev_year\2022\Fast_Quaternion_Product_Units_for_Learning_Disentangled_Representations_in_SO\figure_3.jpg
  Figure 3 caption: An illustration of the tree structure stored by a QPU when N=7
    . Given a backward way, its passing nodes are indicated by the red nodes at different
    depths, and at each depth, the leftright sibling node of the passing node is indicated
    by bluegreen nodes. Each node is associated with an index, which is also represented
    in a binary format. Given a target node (i.e., p 5 ), we can construct its backward
    way by indexing the corresponding passing nodes and sibling nodes. Accordingly,
    the gradient of the QPU to the target node can be computed by (25).
  Figure 4 Link: articels_figures_by_rev_year\2022\Fast_Quaternion_Product_Units_for_Learning_Disentangled_Representations_in_SO\figure_4.jpg
  Figure 4 caption: "(a) An illustration of the representation of skeleton data. The\
    \ \u03B8 1 associated with the node \u201C1\u201D indicates the rotation from\
    \ the edge 0\u21921 to the edge 1\u21927 . (b) An illustration of the neighborhood\
    \ of a centroid. The small gray points indicate the points out of the range. The\
    \ black point represents the centroid, and the color points are its neighbors.\
    \ The color sector areas on the rotation plane represents the angles used to sort\
    \ the neighbor points. The arrows associated with the sector areas indicate the\
    \ order of the neighbor points."
  Figure 5 Link: articels_figures_by_rev_year\2022\Fast_Quaternion_Product_Units_for_Learning_Disentangled_Representations_in_SO\figure_5.jpg
  Figure 5 caption: Typical quaternion neural network layers constructed by our fast
    QPUs.
  Figure 6 Link: articels_figures_by_rev_year\2022\Fast_Quaternion_Product_Units_for_Learning_Disentangled_Representations_in_SO\figure_6.jpg
  Figure 6 caption: 'Illustrations of the CubeEdge dataset. (a) The steps to generate
    the skeletons: i ) fix the two first edges; ii ) pick the next edge from two possible
    candidates (doubling the number of classes); iii ) apply random shear on xy -plane.
    The root vertex is marked with a triangle. (b) Samples from the testing set.'
  Figure 7 Link: articels_figures_by_rev_year\2022\Fast_Quaternion_Product_Units_for_Learning_Disentangled_Representations_in_SO\figure_7.jpg
  Figure 7 caption: The architectures of the models for synthetic data.
  Figure 8 Link: articels_figures_by_rev_year\2022\Fast_Quaternion_Product_Units_for_Learning_Disentangled_Representations_in_SO\figure_8.jpg
  Figure 8 caption: Comparisons on testing accuracy.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.67
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Shaofei Qin
  Name of the last author: Yi Xu
  Number of Figures: 8
  Number of Tables: 11
  Number of authors: 4
  Paper title: Fast Quaternion Product Units for Learning Disentangled Representations
    in SO(3)
  Publication Date: 2022-08-29 00:00:00
  Table 1 caption: TABLE 1 Comparison for the Quat-First Data Format and the Quat-Last
    Data Format on the Number of Memory Accesses
  Table 10 caption: TABLE 10 The Impact of Sorting Methods on Testing Accuracy (%)
  Table 2 caption: TABLE 2 Comparison Between the Quat-First Data Format and the Quat-Last
    Data Format on the Accumulated Operation Time of 1,000 Repeated Calculations
  Table 3 caption: TABLE 3 Comparisons for Various Implementations on Their Computational
    Efficiency
  Table 4 caption: TABLE 4 Comparisons on Testing Accuracy (%) on NTU and FPHA
  Table 5 caption: TABLE 5 Comparisons on Testing Accuracy (%) on ModelNet40 and ModelNet10
  Table 6 caption: TABLE 6 Comparison With Other Rotation-Invariant Networks on Testing
    Accuracy (%)
  Table 7 caption: TABLE 7 The Comparisons for the QMLP-LSTM R R Models Designed Based
    on Different Shifting-Scaling Mechanisms on Classifying the FPHA (AR) Data
  Table 8 caption: TABLE 8 The Impact of Angle-Axis Map on Accuracy (%) With FPHA
    as Dataset
  Table 9 caption: TABLE 9 The Impact of the QMLP and the AAM Function f f on the
    Testing Accuracy (%)
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3202217
- Affiliation of the first author: bosch center for artificial intelligence, renningen,
    germany
  Affiliation of the last author: intelligent autonomous systems group, technical
    university darmstadt, darmstadt, germany
  Figure 1 Link: articels_figures_by_rev_year\2022\A_Deterministic_Approximation_to_Neural_SDEs\figure_1.jpg
  Figure 1 caption: Our deterministic approximation provides well-calibrated uncertainty
    scores with a computational cost equal to 12 particles. Reaching a comparable
    level of calibration by Monte Carlo sampling demands at least 64 particles.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\A_Deterministic_Approximation_to_Neural_SDEs\figure_2.jpg
  Figure 2 caption: 'Nonlinear activations get statistically independent as the network
    width increases, supporting our assumption in Eq. (16). Imagine a matrix containing
    mutual information between all pairs of nonlinear activations h l k in a two-hidden-layer
    neural net. Group its entries into blocks as shown in panel (a): the diagonal
    (A) giving the entropy of an activation, the within-layer off-diagonal block (B)
    giving the dependence of sibling activations, the cross-layer off-diagonal (C)
    giving the dependence of activations in different layers. As seen in panel (b),
    the average mutual information in blocks (B) and (C) decreases sharply with increasing
    layer width. Solid lines and shaded area represent average mutual information
    and its standard deviation over 100 repetitions. We show an example of the matrix
    with all pairwise mutual information values for a hidden layer width of 16 neurons
    in panel (c).'
  Figure 3 Link: articels_figures_by_rev_year\2022\A_Deterministic_Approximation_to_Neural_SDEs\figure_3.jpg
  Figure 3 caption: Dropout reduces the correlation coefficient between different
    activations. We pass a multivariate normal distributed random vector through a
    neural net with three 50-neuron-wide hidden layers with ReLU activation. The off-diagonals
    of the covariance matrix of the activation map are suppressed when Dropout is
    used after each ReLU activation, as shown in Panel (a) for an intermediate layer
    and Panel (b) for the output layer. Decorrelation of a large number of co-variates
    makes a normal distribution an accurate approximation on their sum due to the
    central limit theorem.
  Figure 4 Link: articels_figures_by_rev_year\2022\A_Deterministic_Approximation_to_Neural_SDEs\figure_4.jpg
  Figure 4 caption: "Comparison of VMM versus cubature in terms of relative error\
    \ and computation time. We generate a randomly initialized neural net f \u03B8\
    \ (x) . The dimensionalities of x and f \u03B8 (x) are equal and vary in the horizontal\
    \ axis of all plots. The neural net f \u03B8 (x) has three fully-connected layers\
    \ of varying widths color-coded according to the heatmap on the right of the figures,\
    \ ReLU activation, and Dropout with rate 0.2. As cubature cannot handle a random\
    \ f \u03B8 (x) and discontinuous activations, we evaluate it with tanh activation\
    \ and without Dropout. We aim to approximate the intractable expectation r=\u222B\
    \ f \u03B8 (x)N(x|\u03BC,\u03A3)dx , where \u03BC\u223CN(0,I) and \u03A3\u223C\
    W(I,dim(I)) with Wishart distribution W . We repeat the experiment 512 times and\
    \ report the average relative error ||r\u2212 r | | 2 ||r| | 2 in Panel (a), where\
    \ r is represented by averaging over 10 million Monte Carlo simulations and r\
    \ is approximated via cubature and VMM, respectively. We calculate the computation\
    \ time of this experiment in all repetitions and report its average as a function\
    \ of input dimensionality in Panel (b). In Panel (c) we report the average relative\
    \ error of the true Jacobian E[ \u2207 x f \u03B8 (x)] , which is obtained by\
    \ the Monte Carlo simulation, and our approximate Jacobian obtained by backward\
    \ vertical moment matching."
  Figure 5 Link: articels_figures_by_rev_year\2022\A_Deterministic_Approximation_to_Neural_SDEs\figure_5.jpg
  Figure 5 caption: "Handling multimodality with a NSDE applied to double-well potential\
    \ dynamics: d x t =4 x t (1\u2212 x 2 t )dt+d w t . We train a separate model\
    \ on each mode and predict with BMM in Panel (a), and with NSDE-MC in Panel (b)."
  Figure 6 Link: articels_figures_by_rev_year\2022\A_Deterministic_Approximation_to_Neural_SDEs\figure_6.jpg
  Figure 6 caption: Cost-benefit analysis of calibration with different methods on
    the regression task for a flow time of 8 seconds. One particle is equal to one
    MC simulation along a trajectory. BMM is our proposed method, Cubature is our
    method that uses cubature for VMM, the orange line is training and prediction
    with MC sampling, and the blue line is training with our method and prediction
    with MC sampling. We show mean and standard deviation over 10 runs.
  Figure 7 Link: articels_figures_by_rev_year\2022\A_Deterministic_Approximation_to_Neural_SDEs\figure_7.jpg
  Figure 7 caption: Cost-benefit analysis of calibration with different methods. One
    particle is equal to one MC simulation along a trajectory. BMM is our proposed
    method, Cubature is our method that uses cubature for VMM, the orange line is
    training and prediction with MC sampling, and the blue line is training with our
    method and prediction with MC sampling. We show mean and standard deviation over
    10 runs.
  Figure 8 Link: articels_figures_by_rev_year\2022\A_Deterministic_Approximation_to_Neural_SDEs\figure_8.jpg
  Figure 8 caption: Comparison of BMM to MC in terms of ECPE (left) and relative error
    (right) as a function of the dimensionality of x . We calculate the relative error
    as frac1Ksum k=1K||rk-hatrk||2||rk||2 , where rk is the true mean at time step
    k and hatrk is the predicted mean. The dotted black line is our method BMM and
    the solid lines represent MC sampling. The color of the solid lines indicates
    the number of particles S as a function of the dimensionality D . The dashed line
    is the sampling strategy S=D , which has the same computational complexity as
    our method mathcal O(D2) . We show the mean over 20 runs.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.91
  Name of the first author: Andreas Look
  Name of the last author: Jan Peters
  Number of Figures: 8
  Number of Tables: 4
  Number of authors: 4
  Paper title: A Deterministic Approximation to Neural SDEs
  Publication Date: 2022-08-29 00:00:00
  Table 1 caption: TABLE 1 Negative Log Likelihood Values of 8 Benchmark Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 RMSE Values of 8 Benchmark Datasets
  Table 3 caption: TABLE 3 Results for Different Time Series Classification Tasks
  Table 4 caption: TABLE 4 Forecasting Results for Different Time Series Prediction
    Task
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3202237
- Affiliation of the first author: "faculty of mathematics and computer science, university\
    \ of m\xFCnster, m\xFCnster, germany"
  Affiliation of the last author: "faculty of mathematics and computer science, university\
    \ of m\xFCnster, m\xFCnster, germany"
  Figure 1 Link: articels_figures_by_rev_year\2022\KernelBased_Generalized_Median_Computation_for_Consensus_Learning\figure_1.jpg
  Figure 1 caption: "Computation of the ratio \u03B1 between objects for reconstruction."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\KernelBased_Generalized_Median_Computation_for_Consensus_Learning\figure_2.jpg
  Figure 2 caption: "Example for the weighted mean computation using strings. The\
    \ median object o is reconstructed from x \xAF using the two neighbors o a =AAAA\
    \ and o b =BBB and their respective embeddings. For visualization, possible reconstructed\
    \ objects using a weighted mean function are shown as points on the line between\
    \ \u03D5( o a ) and \u03D5( o b ) ."
  Figure 3 Link: articels_figures_by_rev_year\2022\KernelBased_Generalized_Median_Computation_for_Consensus_Learning\figure_3.jpg
  Figure 3 caption: "Histogram of the distance distortion constant c for the Darwin\
    \ dataset. The used parameters for kernel methods are \u03B2=2 for K nd \u03B4\
    \ , \u03B3=1 and p=1 for K pol \u03B4 , and | O \xAF |=3 for K comb \u03B4 . Explicit\
    \ embedding methods used 0.8\u22C5n dimensions where n is the number of objects."
  Figure 4 Link: articels_figures_by_rev_year\2022\KernelBased_Generalized_Median_Computation_for_Consensus_Learning\figure_4.jpg
  Figure 4 caption: Minimization of the SOD on the Darwin dataset using linear-recursive
    (first 6 iterations) and linear search (second 4 iterations). Values are normalized
    by the final SOD.
  Figure 5 Link: articels_figures_by_rev_year\2022\KernelBased_Generalized_Median_Computation_for_Consensus_Learning\figure_5.jpg
  Figure 5 caption: Relationship between embedding and median quality in the different
    datasets, measured in NCC and SOD respectively.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.71
  Name of the first author: "Andreas Nienk\xF6tter"
  Name of the last author: Xiaoyi Jiang
  Number of Figures: 5
  Number of Tables: 7
  Number of authors: 2
  Paper title: Kernel-Based Generalized Median Computation for Consensus Learning
  Publication Date: 2022-08-29 00:00:00
  Table 1 caption: TABLE 1 Complexity of the Kernel-Based Median Computation Framework
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Evaluated Datasets for Generalized Median Computation
  Table 3 caption: "TABLE 3 Convergence of the Weiszfeld Algorithm for 5 Indefinite\
    \ Kernel Functions K lin \u03B4 K\u03B4lin, K nd \u03B4 K\u03B4nd, K pol \u03B4\
    \ K\u03B4pol, K rbf \u03B4 K\u03B4rbf, K comb \u03B4 K\u03B4comb (First Value\
    \ in Each Column) and 3 Positive Definite Kernel Functions K ssk Kssk, K part\
    \ Kpart, K kend Kkend (Second Value in Each Column)."
  Table 4 caption: 'TABLE 4 Median Quality on Six Datasets: Comparison With SOD'
  Table 5 caption: TABLE 5 Values Used for the Normalization of the Results
  Table 6 caption: 'TABLE 6 Median Quality on Six Datasets: Comparison with Ranking'
  Table 7 caption: TABLE 7 Mean Computational Time on the Darwin Dataset (In Seconds)
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3202565
- Affiliation of the first author: the university of hong kong, hong kong
  Affiliation of the last author: shenzhen institute of advanced technology, chinese
    academy of sciences, shenzhen, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Blind_Image_SuperResolution_A_Survey_and_Beyond\figure_1.jpg
  Figure 1 caption: 'Left: an HR image and its different LR versions with assumed
    degradation types of four SISR methods. Right: Different HR images generated by
    each method from an LR input crop of Forrest Gump. Best viewed in color and zoom.'
  Figure 10 Link: articels_figures_by_rev_year\2022\Blind_Image_SuperResolution_A_Survey_and_Beyond\figure_10.jpg
  Figure 10 caption: Detailed frameworks of methods from Image-Specific Adaptation
    with Degradation Estimation. A conditional input is connected by a blue dotted
    line, which is used for feature adaptation at intermediate layers of SR network,
    in contrast to a normal input directly processed by the network. (a) IKC [7] and
    (b) DAN [20] with iterative scheme between kernel estimation (corrector network)
    and SR. (c) Cornillere et al. [40] optimize kernel estimation by minimizing the
    error map of SR output. (d) KMSR [36], RealSR [46] and Ren et al. [44] build generic
    training datasets by covering more realistic degradations.
  Figure 2 Link: articels_figures_by_rev_year\2022\Blind_Image_SuperResolution_A_Survey_and_Beyond\figure_2.jpg
  Figure 2 caption: Domain interpretation of differences between non-blind and blind
    SR. There exists a large domain gap between the SR result and desired high-quality
    HR, which is caused by applying a pre-trained non-blind model to LR input with
    degradation deviating from the assumed one (e.g., downsampling).
  Figure 3 Link: articels_figures_by_rev_year\2022\Blind_Image_SuperResolution_A_Survey_and_Beyond\figure_3.jpg
  Figure 3 caption: "Examples of degraded LR image. (a) Clean LR (top left, generated\
    \ with bicubic downsampling) and its blurry or noisy versions with different k\
    \ and n . The 2nd row is with isotropic Gaussian kernels while the 3rd row with\
    \ anisotropic Gaussian, and \u03C3 is variance of additive Gaussian noise. The\
    \ example image is from DIV8K [23]; (b) Image with typical patch recurrence, both\
    \ within and across different scales of the same image. The image is from Urban100\
    \ [24]; (c) Real-world smartphone images with complex unknown degradations. Images\
    \ are from DPED dataset [25]."
  Figure 4 Link: articels_figures_by_rev_year\2022\Blind_Image_SuperResolution_A_Survey_and_Beyond\figure_4.jpg
  Figure 4 caption: Real-world images (or video frames) captured with different devices.
    Images are from DPED [25], VIRAT [32], UCF-Crime [33] datasets. Best viewed in
    color and zoom.
  Figure 5 Link: articels_figures_by_rev_year\2022\Blind_Image_SuperResolution_A_Survey_and_Beyond\figure_5.jpg
  Figure 5 caption: Old photos with degradation rising from storage.
  Figure 6 Link: articels_figures_by_rev_year\2022\Blind_Image_SuperResolution_A_Survey_and_Beyond\figure_6.jpg
  Figure 6 caption: Our proposed taxonomy and the corresponding representative methods.
    Our taxonomy distinguishes the ways of degradation modelling and data used for
    solving SR models, which also naturally reveals the remaining research gap.
  Figure 7 Link: articels_figures_by_rev_year\2022\Blind_Image_SuperResolution_A_Survey_and_Beyond\figure_7.jpg
  Figure 7 caption: Overall frameworks for two types of methods with explicit modelling
    and external dataset. (a) Image-specific adaptation without degradation estimation.
    An external method could be applied to perform degradation estimation before SR
    process, thus adapting the framework to blind setting. (b) Image-specific adaptation
    with degradation estimation. An internal module is included for degradation estimation,
    and its output is usually an encoded representation. A dotted line indicates that
    the connection is optional for different methods.
  Figure 8 Link: articels_figures_by_rev_year\2022\Blind_Image_SuperResolution_A_Survey_and_Beyond\figure_8.jpg
  Figure 8 caption: "Frameworks of methods in Image-Specific Adaptation without Degradation\
    \ Estimation. \u201C(Estimated)\u201D indicates that the degradation information\
    \ could be estimated by an external algorithm. (a) SRMD [21] and UDVD [38], where\
    \ an LR image is concatenated with its degradation map as a unified input to SR\
    \ network. (b) DPSR [39], including two sub-processes: deblurring, SR plus denoising.\
    \ (c) USRNet [22], including two sub-processes: SR plus deblurring, denoising."
  Figure 9 Link: articels_figures_by_rev_year\2022\Blind_Image_SuperResolution_A_Survey_and_Beyond\figure_9.jpg
  Figure 9 caption: "Illustration on the limitations of Image-Specific Adaptation\
    \ without Degradation Estimation. Input image is blurred and downsampled with\
    \ Gaussian kernel k \u03C3 = 2. An incorrect kernel estimation input either leads\
    \ to blurry output or unnatural ringing artifacts with over-enhanced textures."
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Anran Liu
  Name of the last author: Chao Dong
  Number of Figures: 18
  Number of Tables: 6
  Number of authors: 5
  Paper title: 'Blind Image Super-Resolution: A Survey and Beyond'
  Publication Date: 2022-08-30 00:00:00
  Table 1 caption: TABLE 1 Classification of Methods With Explicit Modelling in Terms
    of Blur Kernel Types Used in Their Experiments
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Classification of All Methods in Terms of Learning Scheme,
    i.e., Whether Supervised or Unsupervised Learning is Used
  Table 3 caption: TABLE 3 Challenges on Blind Image SR
  Table 4 caption: TABLE 4 Quantitative Comparison (PSNR(dB)SSIM) of Image-Specific
    Adaptation Without Degradation Estimation (SRMD [21], ZSSR [26]) Combined With
    an External Blind Kernel Estimation Method (Michaeli & Irani [76], KernelGAN [6])
  Table 5 caption: TABLE 5 Quantitative Comparison (PSNR(dB)) of Image-Specific Adaptation
    With Degradation Estimation (IKC [7], DRL-DASR [41], KOALAnet [42])
  Table 6 caption: TABLE 6 Quantitative Comparison of Implicit Modelling With Data
    Distribution Learning (CinCGAN [8], FSSR [37], DASR [50])
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3203009
- Affiliation of the first author: school of information and communication engineering,
    dalian university of technology, dalian, liaoning, china
  Affiliation of the last author: school of information and communication engineering,
    dalian university of technology, dalian, liaoning, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Robust_Online_Tracking_With_MetaUpdater\figure_1.jpg
  Figure 1 caption: "Visualization and comparisons of representative tracking results\
    \ with and without our meta-updater on the LaSOT dataset. In the bar plot, \u201C\
    baseline\u201D denotes the baseline trackers without any meta-updater scheme,\
    \ \u201C MU \u2217 \u201D denotes trackers with the original meta-updater [8],\
    \ and \u201CMU\u201D represents trackers with our upgraded meta-updater."
  Figure 10 Link: articels_figures_by_rev_year\2022\Robust_Online_Tracking_With_MetaUpdater\figure_10.jpg
  Figure 10 caption: Qualitative comparison between our method StarkMU and other long-term
    trackers on the TLP dataset.
  Figure 2 Link: articels_figures_by_rev_year\2022\Robust_Online_Tracking_With_MetaUpdater\figure_2.jpg
  Figure 2 caption: Overview of the proposed meta-updater, including architectures
    of the subnetwork MetricNet and the cascaded LSTM.
  Figure 3 Link: articels_figures_by_rev_year\2022\Robust_Online_Tracking_With_MetaUpdater\figure_3.jpg
  Figure 3 caption: Intuitive explanations of some notions in this work.
  Figure 4 Link: articels_figures_by_rev_year\2022\Robust_Online_Tracking_With_MetaUpdater\figure_4.jpg
  Figure 4 caption: Illustration of the relation between the confidence scores and
    IoUs. We also visualize tracking results of some representative frames, where
    correct and wrong self-evaluation instances are marked with green and red boxes,
    respectively. Better viewed in color with zoom-in.
  Figure 5 Link: articels_figures_by_rev_year\2022\Robust_Online_Tracking_With_MetaUpdater\figure_5.jpg
  Figure 5 caption: Illustration of positive and negative samples for meta-updater
    training. The first two rows illustrate two positive examples, whereas the last
    two rows display the negative ones. In fact, there is no interval among frames,
    the interval 5 is merely for clear visualization.
  Figure 6 Link: articels_figures_by_rev_year\2022\Robust_Online_Tracking_With_MetaUpdater\figure_6.jpg
  Figure 6 caption: Overview of optimization strategies for the positive and negative
    sample sets of particle-based methods.
  Figure 7 Link: articels_figures_by_rev_year\2022\Robust_Online_Tracking_With_MetaUpdater\figure_7.jpg
  Figure 7 caption: Tracking performance of different trackers on the TLP dataset.
    Better viewed in color with zoom-in.
  Figure 8 Link: articels_figures_by_rev_year\2022\Robust_Online_Tracking_With_MetaUpdater\figure_8.jpg
  Figure 8 caption: Attribute analysis on the LaSOT dataset.
  Figure 9 Link: articels_figures_by_rev_year\2022\Robust_Online_Tracking_With_MetaUpdater\figure_9.jpg
  Figure 9 caption: Quantitative analysis of optimization strategies ability for positive
    and negative sample sets on the LaSOT dataset.
  First author gender probability: 0.59
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Jie Zhao
  Name of the last author: Huchuan Lu
  Number of Figures: 11
  Number of Tables: 10
  Number of authors: 5
  Paper title: Robust Online Tracking With Meta-Updater
  Publication Date: 2022-08-30 00:00:00
  Table 1 caption: TABLE 1 Input-Output Relations of Our Cascaded LSTM Model
  Table 10 caption: TABLE 10 Effectiveness of Our Meta-Updater for Different Trackers
  Table 2 caption: TABLE 2 Performance Evaluation of Our Tracker StarkMU and State-of-The-Art
    Short-Term Trackers on the LaSOT Dataset
  Table 3 caption: TABLE 3 Performance Evaluation of Our Tracker StarkMU and State-of-The-Art
    Long-Term Trackers on the VOT2020LT Dataset
  Table 4 caption: TABLE 4 Performance Evaluation of Our Tracker StarkMU and State-of-The-Art
    Long-Term Trackers on the OxUvALT Dataset
  Table 5 caption: TABLE 5 Tracking Results of Eight Various Types of Trackers With
    and Without Our Meta-Updater on Three Long-Term and Two Short-Term Tracking Benchmarks
  Table 6 caption: TABLE 6 Effects of Different Time Steps on Our Method (DiMPMU)
  Table 7 caption: TABLE 7 Effects of Different Input Information on Our Method (DiMPMU)
  Table 8 caption: TABLE 8 Effects of Different Update Mechanisms on the Tracker MDNet
  Table 9 caption: TABLE 9 Effects of Optimizing Different Types of Samples and Numbers
    of Frames Before the Time Step on Our Tracker (MDNetMU)
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3202785
- Affiliation of the first author: "bioimage analysis unit, institut pasteur, cnrs\
    \ umr3691, sorbonne universit\xE9 (upmc), paris, france"
  Affiliation of the last author: bioimage analysis unit, cnrs umr3691, institut pasteur,
    paris, france
  Figure 1 Link: articels_figures_by_rev_year\2022\Reformulating_Optical_Flow_to_Solve_ImageBased_Inverse_Problems_and_Quantify_Unc\figure_1.jpg
  Figure 1 caption: TFM as an OFIP. In TFM, u is extracted from the bead displacement
    in I 1,2 using motion estimation and then inverted for the traction forces f made
    by the cell on K . We will instead invert f directly from I 1,2 .
  Figure 10 Link: articels_figures_by_rev_year\2022\Reformulating_Optical_Flow_to_Solve_ImageBased_Inverse_Problems_and_Quantify_Unc\figure_10.jpg
  Figure 10 caption: 'Non-optical-flow: deformation and force reconstruction from
    noisy deformation data. Row 1) True simulated deformation mathbfu (resulting from
    mathbff ), deformation corrupted by eta mathbfu 20% noise, mathbfud , recovered
    deformation mathbfu , and error map |mathbfu-mathbfu|2 Vert mathbfuVert 2 (scale:
    0.00 to 0.053). Row 2) True imposed force mathbff , recovered force mathbff ,
    and error map |mathbff-mathbff|2 Vert mathbffVert 2 (0.00-0.34). Violet (low)
    to yellow (high).'
  Figure 2 Link: articels_figures_by_rev_year\2022\Reformulating_Optical_Flow_to_Solve_ImageBased_Inverse_Problems_and_Quantify_Unc\figure_2.jpg
  Figure 2 caption: "Schematic of the projection of 3D fluorescent compounds on a\
    \ 2D image. The arrows show the difference between the real 3D movement in \u03A9\
    \ and the visible movement on the image O ."
  Figure 3 Link: articels_figures_by_rev_year\2022\Reformulating_Optical_Flow_to_Solve_ImageBased_Inverse_Problems_and_Quantify_Unc\figure_3.jpg
  Figure 3 caption: "a) The importance of choosing the inner product. Number of CG\
    \ iterations to solve the inverse problem as a function of degrees of freedom\
    \ with P (blue) and without Id (Identity, red) preconditioning ( x -axis log-scaled).\
    \ b) Hessian spectrum. The dominant eigenvalues are independent of mesh dimension\
    \ (increasing degrees of freedom: red, blue, green, grey\u2014or bottom to top).\
    \ The horizontal line marks the 1-threshold. Eigenvalues are indexed in decreasing\
    \ order and correspond to the TFM application in the experiments section. c) Morozovs\
    \ principle. Image misfit (48) as a function of a regularisation parameter \u03B1\
    \ . Black line marks the noise level \u03B4 I . Morozovs principle suggests \u03B1\
    = 10 \u22129 ."
  Figure 4 Link: articels_figures_by_rev_year\2022\Reformulating_Optical_Flow_to_Solve_ImageBased_Inverse_Problems_and_Quantify_Unc\figure_4.jpg
  Figure 4 caption: "Hessian eigenvectors. Some eigenvectors f i for i\u22080,1,2,3,4,5,10,15\
    \ of the solution in Fig. 7 of an application to TFM, resolution lowered for clarity."
  Figure 5 Link: articels_figures_by_rev_year\2022\Reformulating_Optical_Flow_to_Solve_ImageBased_Inverse_Problems_and_Quantify_Unc\figure_5.jpg
  Figure 5 caption: "Non-constrained optical flow versus PDE-constrained optical flow.\
    \ Row 1) The deformed image I 2 (red) after using the displacement field u \u2217\
    \ to warp the reference image I 1 (cyan). Row 2) After adding noise (SNR 27.6)\
    \ to both images the force is recovered with a state-of-the-art multi-scale two-step\
    \ optical flow + TFM ( u o ) and the PDE-constrained method ( u \u2217 ); the\
    \ central image is the superposition (white) of both noisy images. Row 3) Same\
    \ as (Row 2) but with a SNR of 1.7. All meshes (and thus the resolution of the\
    \ fields) are pixel sized; the results shown in Fig. 10 are from the same simulated\
    \ system but coarser. Violet (low) to yellow (high), same scale among all subfigures.\
    \ These four maps correspond to (match by SNR) four error points in Fig. 6b."
  Figure 6 Link: articels_figures_by_rev_year\2022\Reformulating_Optical_Flow_to_Solve_ImageBased_Inverse_Problems_and_Quantify_Unc\figure_6.jpg
  Figure 6 caption: "a) Non-OF: error in direct inversions. Error( f \u2217 , f \u2217\
    \ ) of the inverted force field compared to the groundtruth as a function of percentage\
    \ of error added \u03B7 u ; Error( u \u2217 , u \u2217 ) of the displacement with\
    \ respect to that resulting from the force groundtruth; and Error( u \u2217 ,\
    \ u d ) of the displacement compared to the erroneous data, illustrating Morozovs\
    \ criteria. b) Non-constrained OF TFM versus PDE-constrained OF TFM. Error( u\
    \ \u2217 , u \u2217 ) of the displacement field as a function of noise. Image\
    \ noise is the coefficient of variation, with reciprocal SNRs: 55.2, 27.6, 13.8,\
    \ 6.9, 3.5, 1.7, 0.9. Noise axis is log-scaled. c) Error( f \u2217 , f \u2217\
    \ ) of the force field as a function of the multi-resolution scale for different\
    \ SNR. Scale axis is log-scaled."
  Figure 7 Link: articels_figures_by_rev_year\2022\Reformulating_Optical_Flow_to_Solve_ImageBased_Inverse_Problems_and_Quantify_Unc\figure_7.jpg
  Figure 7 caption: "PDE-reconstructed force. Recovered force fields f \u2217 (SNRs:\
    \ 1.7 left, 27.6 right) compared to true field f \u2217 (middle). Corresponding\
    \ velocity fields u \u2217 and u in Fig. 5. See corresponding error points in\
    \ Fig. 6."
  Figure 8 Link: articels_figures_by_rev_year\2022\Reformulating_Optical_Flow_to_Solve_ImageBased_Inverse_Problems_and_Quantify_Unc\figure_8.jpg
  Figure 8 caption: textTV versus L2 . Vector fields (left column) and magnitudes
    (right column) for the true imposed force (top row), the force recovered using
    TV regularisation (middle row) and the force recovered using L2 regularisation
    (bottom row). TV brings down the error from 30% to 20% and from 2% to 1% for the
    force and the displacement (respectively) as compared to L2 . Violet (low) to
    yellow (high). Here the subdomain K was chosen as a centered disc of diameter
    0.9 compared to the unitary length of the image and to the 0.6 diameter of the
    simulated force.
  Figure 9 Link: articels_figures_by_rev_year\2022\Reformulating_Optical_Flow_to_Solve_ImageBased_Inverse_Problems_and_Quantify_Unc\figure_9.jpg
  Figure 9 caption: Effect of BCs. Vector fields for the displacement (top row) and
    the force (bottom) of an example with non-zero BCs (a cut-out of one of the spots
    in Figs. 5 and 7 in fact). The first column corresponds to the true, imposed values
    ( subscript); the second to the reconstruction with the 0 boundary conditions
    assumed by the state-of-the-art ( o ; errors 0.1, 18.); and the third to our reconstruction
    ( superscript; errors 0.01, 0.12).
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Aleix Boquet-Pujadas
  Name of the last author: Jean-Christophe Olivo-Marin
  Number of Figures: 11
  Number of Tables: 0
  Number of authors: 2
  Paper title: Reformulating Optical Flow to Solve Image-Based Inverse Problems and
    Quantify Uncertainty
  Publication Date: 2022-08-30 00:00:00
  Table 1 caption: Not Available
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3202855
