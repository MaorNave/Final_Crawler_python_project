- Affiliation of the first author: school of electronic information and communications,
    huazhong university of science and technology, wuhan, china
  Affiliation of the last author: school of electronic information and communications,
    huazhong university of science and technology, wuhan, china
  Figure 1 Link: articels_figures_by_rev_year\2018\ASTER_An_Attentional_Scene_Text_Recognizer_with_Flexible_Rectification\figure_1.jpg
  Figure 1 caption: Examples of irregular text.
  Figure 10 Link: articels_figures_by_rev_year\2018\ASTER_An_Attentional_Scene_Text_Recognizer_with_Flexible_Rectification\figure_10.jpg
  Figure 10 caption: Word accuracies (left) and training losses (right) with different
    model initialization schemes.
  Figure 2 Link: articels_figures_by_rev_year\2018\ASTER_An_Attentional_Scene_Text_Recognizer_with_Flexible_Rectification\figure_2.jpg
  Figure 2 caption: Overview of the proposed model. Dashed lines show the flow of
    gradients.
  Figure 3 Link: articels_figures_by_rev_year\2018\ASTER_An_Attentional_Scene_Text_Recognizer_with_Flexible_Rectification\figure_3.jpg
  Figure 3 caption: The TPS transformation can rectify various types of irregular
    text, including but not limited to, loosely bounded a), oriented or perspectively
    distorted (b)(c), and curved text (d).
  Figure 4 Link: articels_figures_by_rev_year\2018\ASTER_An_Attentional_Scene_Text_Recognizer_with_Flexible_Rectification\figure_4.jpg
  Figure 4 caption: Structure of the rectification network.
  Figure 5 Link: articels_figures_by_rev_year\2018\ASTER_An_Attentional_Scene_Text_Recognizer_with_Flexible_Rectification\figure_5.jpg
  Figure 5 caption: "Text rectification with TPS transformation. Crosses are control\
    \ points. The yellow arrow represents the transformation T , connecting a point\
    \ p i and its corresponding point p \u2032 i ."
  Figure 6 Link: articels_figures_by_rev_year\2018\ASTER_An_Attentional_Scene_Text_Recognizer_with_Flexible_Rectification\figure_6.jpg
  Figure 6 caption: Construction of the grid generator. concat and matmul are respectively
    matrix concatenation and multiplication operators.
  Figure 7 Link: articels_figures_by_rev_year\2018\ASTER_An_Attentional_Scene_Text_Recognizer_with_Flexible_Rectification\figure_7.jpg
  Figure 7 caption: Structure of the basic text recognition network.
  Figure 8 Link: articels_figures_by_rev_year\2018\ASTER_An_Attentional_Scene_Text_Recognizer_with_Flexible_Rectification\figure_8.jpg
  Figure 8 caption: "Bidirectional decoder. \u201C0.5\u201D and \u201C0.8\u201D are\
    \ recognition scores."
  Figure 9 Link: articels_figures_by_rev_year\2018\ASTER_An_Attentional_Scene_Text_Recognizer_with_Flexible_Rectification\figure_9.jpg
  Figure 9 caption: Rectification improvement versus portion of irregular text.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Baoguang Shi
  Name of the last author: Xiang Bai
  Number of Figures: 14
  Number of Tables: 8
  Number of authors: 6
  Paper title: 'ASTER: An Attentional Scene Text Recognizer with Flexible Rectification'
  Publication Date: 2018-06-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Text Recognition Network Configurations
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Recognition Accuracies with and without Rectification
  Table 3 caption:
    table_text: TABLE 3 Rectified Images and Recognition Results by [55] and by ASTER
  Table 4 caption:
    table_text: TABLE 4 Selected Results on SVT-Perspective and CUTE80
  Table 5 caption:
    table_text: TABLE 5 Recognition Accuracies with Different Decoders
  Table 6 caption:
    table_text: TABLE 6 Recognition Results Comparison
  Table 7 caption:
    table_text: TABLE 7 Detection Accuracies by Different Systems
  Table 8 caption:
    table_text: TABLE 8 End-to-End Results Comparison
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2848939
- Affiliation of the first author: department of computer science and engineering,
    university of notre dame, notre dame, in, usa
  Affiliation of the last author: department of computer science and engineering,
    university of notre dame, notre dame, in, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\PsyPhy_A_Psychophysics_Driven_Evaluation_Framework_for_Visual_Recognition\figure_1.jpg
  Figure 1 caption: In this article, the concept of applying psychophysics [1], [2]
    on a recognition model is introduced. In this figure, A and B are two models being
    compared. (Top) In traditional dataset-based evaluation, summary statistics are
    generated over large sets of data, with little consideration given to specific
    conditions that lead to incorrect recognition instances. (Bottom) Psychophysics,
    a set of experimental concepts and procedures from psychology and neuroscience,
    helps us plot the exact relationships between perturbed test images and resulting
    model behavior to determine the precise conditions under which models fail. Instead
    of comparing summary statistics, we compare item-response curves representing
    performance (y-axis) versus the dimension of the image being manipulated (x-axis).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\PsyPhy_A_Psychophysics_Driven_Evaluation_Framework_for_Visual_Recognition\figure_2.jpg
  Figure 2 caption: (Left and Center) a selection of item-response curves for the
    2AFC task. These rendered scene experiments reflect the accuracy across 40 classes.
    Each experiment used five well-known CNNs [6], [17], [18], [19]. A perfect curve
    would be a flat line at the top of the plot. The images at the bottom of each
    curve show how the perturbations increase from right to left, starting with no
    perturbation (i.e., the original image) for all conditions. The red dot indicates
    mean human performance for a selected stimulus level. (Right) a summary plot of
    all VGG-16 2AFC item-response curves using AUIRC (Riemann sum) as a summary statistic,
    normalized by the total area above and below the curve. These plots (as well as
    the next sets in Figs. 3 & 4) are best viewed in color.
  Figure 3 Link: articels_figures_by_rev_year\2018\PsyPhy_A_Psychophysics_Driven_Evaluation_Framework_for_Visual_Recognition\figure_3.jpg
  Figure 3 caption: A selection of item-response curves for the MAFC task. (Top) natural
    scenes. (Bottom-Left and Center) rendered scenes. The top-left can be directly
    compared to the top-center as well as the bottom-left to its corresponding plot
    in Fig. 2. (Bottom-Right) MAFC summary plot for VGG-16.
  Figure 4 Link: articels_figures_by_rev_year\2018\PsyPhy_A_Psychophysics_Driven_Evaluation_Framework_for_Visual_Recognition\figure_4.jpg
  Figure 4 caption: Item-response curves for five different runs of an AlexNet model
    with dropout applied at test time [43] for a 3D rotation transformation. The black
    line indicates the mean of the five AlexNet curves. The maximum difference between
    points on any two curves in this plot is 12.2 percent.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Brandon RichardWebster
  Name of the last author: Walter J. Scheirer
  Number of Figures: 4
  Number of Tables: 0
  Number of authors: 3
  Paper title: 'PsyPhy: A Psychophysics Driven Evaluation Framework for Visual Recognition'
  Publication Date: 2018-06-25 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2849989
- Affiliation of the first author: institute of computer graphics and vision, graz
    university of technology, graz, austria
  Affiliation of the last author: institute of computer graphics and vision, graz
    university of technology, graz, austria
  Figure 1 Link: articels_figures_by_rev_year\2018\Deep_Metric_Learning_with_BIER_Boosting_Independent_Embeddings_Robustly\figure_1.jpg
  Figure 1 caption: BIER divides a large embedding into an ensemble of several smaller
    embeddings. During training we reweight the training set for successive learners
    in the ensemble with the negative gradient of the loss function. During test time
    we concatenate the individual embeddings of all learners into a single embedding
    vector.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Deep_Metric_Learning_with_BIER_Boosting_Independent_Embeddings_Robustly\figure_2.jpg
  Figure 2 caption: Illustration of triplet loss, contrastive loss (for negative samples)
    and binomial deviance loss (for negative samples) and their gradients. Triplet
    and contrastive loss have a non-continuous gradient, whereas binomial deviance
    has a continuous gradient.
  Figure 3 Link: articels_figures_by_rev_year\2018\Deep_Metric_Learning_with_BIER_Boosting_Independent_Embeddings_Robustly\figure_3.jpg
  Figure 3 caption: We divide the embedding (shown as dashed layer) of a metric CNN
    into several weak learners and cast training them as online gradient boosting
    problem. Each learner iteratively reweights samples according to the gradient
    of the loss function. Training a metric CNN this way encourages successive learners
    to focus on different samples than previous learners and consequently reduces
    correlation between learners and their feature representation.
  Figure 4 Link: articels_figures_by_rev_year\2018\Deep_Metric_Learning_with_BIER_Boosting_Independent_Embeddings_Robustly\figure_4.jpg
  Figure 4 caption: Illustration of our Activation Loss. Neurons (green) of different
    embeddings (red) suppress each other. We apply this loss during training time
    between all pairs of our learners.
  Figure 5 Link: articels_figures_by_rev_year\2018\Deep_Metric_Learning_with_BIER_Boosting_Independent_Embeddings_Robustly\figure_5.jpg
  Figure 5 caption: Illustration of our adversarial regressors (blue) between learner
    i and learner j of our embedding (red). We learn a regressor which maps the vector
    of learner j to learner i , maximizing the similarity of feature vectors. The
    gradient reversal layer flips the sign of the gradients which are backpropagated
    to our embeddings, therefore minimizing the similarity of feature vectors. We
    apply these regressors during training between all pairs of our learners.
  Figure 6 Link: articels_figures_by_rev_year\2018\Deep_Metric_Learning_with_BIER_Boosting_Independent_Embeddings_Robustly\figure_6.jpg
  Figure 6 caption: Evaluation of different embedding sizes and group sizes on the
    CUB-200-2011 [23] dataset.
  Figure 7 Link: articels_figures_by_rev_year\2018\Deep_Metric_Learning_with_BIER_Boosting_Independent_Embeddings_Robustly\figure_7.jpg
  Figure 7 caption: "Evaluation of \u03BB div on CUB-200-2011 [23]."
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Michael Opitz
  Name of the last author: Horst Bischof
  Number of Figures: 7
  Number of Tables: 12
  Number of authors: 4
  Paper title: 'Deep Metric Learning with BIER: Boosting Independent Embeddings Robustly'
  Publication Date: 2018-06-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Evaluation of Classifier (Clf.) and Feature Correlation on
      CUB-200-2011 [23]
  Table 10 caption:
    table_text: TABLE 10 Comparison with the State-of-the-Art on the Stanford Online
      Products [7] Dataset
  Table 2 caption:
    table_text: TABLE 2 Evaluation of Loss Functions on CUB-200-2011 [23]
  Table 3 caption:
    table_text: TABLE 3 Evaluation of Group Sizes on CUB-200-2011 [23]
  Table 4 caption:
    table_text: TABLE 4 Evaluation of Embedding Size on CUB-200-2011 [23]
  Table 5 caption:
    table_text: TABLE 5 Evaluation of Glorot, Orthogonal and our Activation Loss and
      Adversarial Loss Initialization Method on CUB-200-2011 [23]
  Table 6 caption:
    table_text: TABLE 6 Comparison of Auxiliary Loss Functions on CUB [23]
  Table 7 caption:
    table_text: TABLE 7 Impact of Auxiliary Loss Functions on Strength and Correlation
  Table 8 caption:
    table_text: TABLE 8 Comparison with the State-of-the-Art on the CUB-200-2011 [23]
      and Cars-196 [20] Dataset
  Table 9 caption:
    table_text: TABLE 9 Comparison with the State-of-the-Art on the Cropped Versions
      of the CUB-200-2011 [23] and Cars-196 [20] Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2848925
- Affiliation of the first author: visual geometry and modelling (vgm), lab of the
    istituto italiano di tecnologia (iit), genova, italy
  Affiliation of the last author: visual geometry and modelling (vgm), lab of the
    istituto italiano di tecnologia (iit), genova, italy
  Figure 1 Link: articels_figures_by_rev_year\2018\Revisiting_Projective_Structure_from_Motion_A_Robust_and_Efficient_Incremental_S\figure_1.jpg
  Figure 1 caption: The black rectangular boxes represent the matrix D containing
    the projective depths for 6 cameras (rows) and 12 points (columns). Dots represent
    single entries while small boxes are tiles that contain possibly more than one
    entry. (b) and (a) show examples of valid constraints. (c) is an invalid configuration
    of D .
  Figure 10 Link: articels_figures_by_rev_year\2018\Revisiting_Projective_Structure_from_Motion_A_Robust_and_Efficient_Incremental_S\figure_10.jpg
  Figure 10 caption: Reconstructions obtained with P2SfM for the medium scale sequences.
    All P2SfM reconstructions are convincing with respect to the scene observed. The
    colours gradient corresponds to the depth along the reconstruction principal axis.
  Figure 2 Link: articels_figures_by_rev_year\2018\Revisiting_Projective_Structure_from_Motion_A_Robust_and_Efficient_Incremental_S\figure_2.jpg
  Figure 2 caption: Example of the incremental procedure to reconstruct a scene with
    6 views and 12 points. The dots indicates visible projections, red ones are outliers.
    We start in (a) with a previously solved sub-problem of 4 views and 8 points in
    green, grey tiles indicates data not yet considered. Then at each step, green
    tiles are the current reconstruction and tiles currently added to expand it are
    in blue. For instance, in (b) we robustly add a view, automatically removing an
    outlier projection. In (c) we then robustly add three points. This is repeated
    until we reach a final outliers free reconstruction in (d).
  Figure 3 Link: articels_figures_by_rev_year\2018\Revisiting_Projective_Structure_from_Motion_A_Robust_and_Efficient_Incremental_S\figure_3.jpg
  Figure 3 caption: Practical projective SfM (P2SfM).
  Figure 4 Link: articels_figures_by_rev_year\2018\Revisiting_Projective_Structure_from_Motion_A_Robust_and_Efficient_Incremental_S\figure_4.jpg
  Figure 4 caption: "Behavior with outliers for the reprojection error (a), the 3D\
    \ structure error (b) and the number of unestimated points (c) at two different\
    \ standard deviations of the noise ( \u03C3=0.5 and \u03C3=1.5 pixels) and 60\
    \ percent of missing data. YDHL and VarPro are quickly and strongly afflicted\
    \ by outliers while P2SfM is almost unaffected thanks to the RANSAC based estimation."
  Figure 5 Link: articels_figures_by_rev_year\2018\Revisiting_Projective_Structure_from_Motion_A_Robust_and_Efficient_Incremental_S\figure_5.jpg
  Figure 5 caption: Running times of P2SfM compared to YDHL and VarPro on small scale
    sequences (a). On the larger scale experiments, timings are reported with increasing
    number of points (b) or views (c) for P2SfM and VarPro. If on small scale sequences
    VarPro is faster (a), P2SfM has a clear advantage on larger scale sequences (b)(c).
    YDHL is the slowest and cannot even handle medium scale sequences.
  Figure 6 Link: articels_figures_by_rev_year\2018\Revisiting_Projective_Structure_from_Motion_A_Robust_and_Efficient_Incremental_S\figure_6.jpg
  Figure 6 caption: Noise level effect on the 2D reprojection error (a) and behaviour
    of the error on 3D structure (b) with increasing missing data ratio. Both VarPro
    and P2SfM outperform YDHL.
  Figure 7 Link: articels_figures_by_rev_year\2018\Revisiting_Projective_Structure_from_Motion_A_Robust_and_Efficient_Incremental_S\figure_7.jpg
  Figure 7 caption: Comparison of the cross product (CP) and pseudo inversion (PINV)
    eliminations. (a) and (b) are respectively the 2D reprojection error and 3D structure
    error with increasing noise level at two ratio of missing data (55 and 70 percent).
    (c) and (d) are respectively the percentage of unestimated points or views when
    the missing data ratio is increasing at two level of noise ( sigma = 0.5 and sigma
    = 1.5 pixels).
  Figure 8 Link: articels_figures_by_rev_year\2018\Revisiting_Projective_Structure_from_Motion_A_Robust_and_Efficient_Incremental_S\figure_8.jpg
  Figure 8 caption: Improvement made by the non-linear refinement on the objective
    function (a) and 3D points error (b) compared to the output of the alternation
    based refinement. The gain in accuracy is minimal for both the small scale dataset
    and the larger one.
  Figure 9 Link: articels_figures_by_rev_year\2018\Revisiting_Projective_Structure_from_Motion_A_Robust_and_Efficient_Incremental_S\figure_9.jpg
  Figure 9 caption: The dinosaur sequence. (a) shows the 2D image trajectories after
    completion with a random colour for each one, making evident the rotational motion
    of the dino. (b) presents the 3D reconstruction after metric upgrade where the
    colours gradient corresponds to the depth along the reconstruction principal axis.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ludovic Magerand
  Name of the last author: Alessio Del Bue
  Number of Figures: 20
  Number of Tables: 1
  Number of authors: 2
  Paper title: 'Revisiting Projective Structure from Motion: A Robust and Efficient
    Incremental Solution'
  Publication Date: 2018-06-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Real Sequences Results
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2849973
- Affiliation of the first author: blavatnik school of computer science, tel aviv
    university, tel aviv, israel
  Affiliation of the last author: blavatnik school of computer science, tel aviv university,
    tel aviv, israel
  Figure 1 Link: articels_figures_by_rev_year\2018\Outlier_Detection_for_Robust_MultiDimensional_Scaling\figure_1.jpg
  Figure 1 caption: Two outlier distances (marked in dashed lines on the left) lead
    to a significant distortion in the embedding, as reflected by the large offsets
    between ground-truth and embedded positions, shown on the right.
  Figure 10 Link: articels_figures_by_rev_year\2018\Outlier_Detection_for_Robust_MultiDimensional_Scaling\figure_10.jpg
  Figure 10 caption: TMDS results using two approaches for thresholds selection. (a)
    The optimal threshold value corresponds to the known number of outliers. (b) The
    selected value using our heuristic. The blue dots represent the ground-truth embedding,
    while the red crossed is the embedding result. The dissimilarity matrix were generated
    for N=50 points uniformly sampled in a two-dimensional square. The dissimilarity
    matrix was contaminated with 100 outliers (8 percent). Note that the embedding
    results differ.
  Figure 2 Link: articels_figures_by_rev_year\2018\Outlier_Detection_for_Robust_MultiDimensional_Scaling\figure_2.jpg
  Figure 2 caption: The effect of distorting two distances, marked by red dashed lines
    (a). The green lines represent edges that violate the triangle inequality (b).
    A stronger distortion leads to a larger number of broken triangles (c).
  Figure 3 Link: articels_figures_by_rev_year\2018\Outlier_Detection_for_Robust_MultiDimensional_Scaling\figure_3.jpg
  Figure 3 caption: (a) The blue lines represent the enlarged distances. (b) Embedding
    with SMACOF. (c) Sammon embedding.
  Figure 4 Link: articels_figures_by_rev_year\2018\Outlier_Detection_for_Robust_MultiDimensional_Scaling\figure_4.jpg
  Figure 4 caption: (a) The red lines represent the shortened distances. (b) Embedding
    with SMACOF. (c) Sammon embedding.
  Figure 5 Link: articels_figures_by_rev_year\2018\Outlier_Detection_for_Robust_MultiDimensional_Scaling\figure_5.jpg
  Figure 5 caption: (a-c) Different lambda applied to the same dataset with the same
    initial guess, leads to different embedding qualities. (d-f) Same lambda applied
    to the same datasets with different initial guesses, yields different embedding
    qualities.
  Figure 6 Link: articels_figures_by_rev_year\2018\Outlier_Detection_for_Robust_MultiDimensional_Scaling\figure_6.jpg
  Figure 6 caption: This graph presents the number of non-zero elements in O (which
    represent outliers) as a function of lambda . The three plots were generated using
    different initial guesses that were uniformly sampled. This suggests that the
    FG12 method is overly sensitive to the initial guess. For this experiment we used
    N = 50 (1225 edges) and 100 outliers.
  Figure 7 Link: articels_figures_by_rev_year\2018\Outlier_Detection_for_Robust_MultiDimensional_Scaling\figure_7.jpg
  Figure 7 caption: Histogram H(b) counts the number of edges that break b triangles.
    It can be seen that most of the edges break only a few triangles. The tail of
    the histogram is associated with outliers. The y -axis is logarithmic to better
    perceive the variance.
  Figure 8 Link: articels_figures_by_rev_year\2018\Outlier_Detection_for_Robust_MultiDimensional_Scaling\figure_8.jpg
  Figure 8 caption: The diagrams (a-b) present two typical distributions of inlines
    and outliers. Each diagram consists of two overlaid histograms of broken-triangles
    ( x -axis) versus log2(textnumber of edges) ( y -axis). One histogram for inliers
    (in blue) and one for outliers (in red). The two datasets contains 40 data points
    (780 distances) and the number of outliers is 20.
  Figure 9 Link: articels_figures_by_rev_year\2018\Outlier_Detection_for_Robust_MultiDimensional_Scaling\figure_9.jpg
  Figure 9 caption: The number of outliers detected as a function of the number of
    subsampled per-edge triangles. The data consists of 15 outliers.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Leonid Blouvshtein
  Name of the last author: Daniel Cohen-Or
  Number of Figures: 20
  Number of Tables: 1
  Number of authors: 2
  Paper title: Outlier Detection for Robust Multi-Dimensional Scaling
  Publication Date: 2018-06-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 CPU Time in Seconds of Three Embedding Algorithms of Data
      Sampled Uniformly from a 2d Unit Hypercube with a Various Number ( N N) of Objects
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2851513
- Affiliation of the first author: school of computer science and engineering, nanjing
    university of science and technology, nanjing, china
  Affiliation of the last author: jd ai research building a, north-star century center,
    beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2018\Deep_Collaborative_Embedding_for_Social_Image_Understanding\figure_1.jpg
  Figure 1 caption: The framework of the proposed DCE method for multiple image understanding
    tasks.
  Figure 10 Link: articels_figures_by_rev_year\2018\Deep_Collaborative_Embedding_for_Social_Image_Understanding\figure_10.jpg
  Figure 10 caption: The average scores for tag expansion.
  Figure 2 Link: articels_figures_by_rev_year\2018\Deep_Collaborative_Embedding_for_Social_Image_Understanding\figure_2.jpg
  Figure 2 caption: The graphical model of the proposed collaborative factor analysis
    model.
  Figure 3 Link: articels_figures_by_rev_year\2018\Deep_Collaborative_Embedding_for_Social_Image_Understanding\figure_3.jpg
  Figure 3 caption: Comparison of different methods on MIRFlickr and NUS-WIDE for
    image tagging in terms of recall.
  Figure 4 Link: articels_figures_by_rev_year\2018\Deep_Collaborative_Embedding_for_Social_Image_Understanding\figure_4.jpg
  Figure 4 caption: Illustration of social image tagging results on the MIRFlickr
    and NUS-WIDE datasets. The irrelevant tags are marked in red. Best viewed in color.
  Figure 5 Link: articels_figures_by_rev_year\2018\Deep_Collaborative_Embedding_for_Social_Image_Understanding\figure_5.jpg
  Figure 5 caption: Experimental results for new tags on the MIRFlickr and NUS-WIDE
    datasets in terms of F1. Best viewed in color.
  Figure 6 Link: articels_figures_by_rev_year\2018\Deep_Collaborative_Embedding_for_Social_Image_Understanding\figure_6.jpg
  Figure 6 caption: Tag-based image retrieval results on the MIRFlickr and NUS-WIDE
    datasets in terms of MAP.
  Figure 7 Link: articels_figures_by_rev_year\2018\Deep_Collaborative_Embedding_for_Social_Image_Understanding\figure_7.jpg
  Figure 7 caption: Examples of tag-based image retrieval on the MIRFlickr dataset.
    The irrelevant images are marked with a red boundary. Best viewed in color.
  Figure 8 Link: articels_figures_by_rev_year\2018\Deep_Collaborative_Embedding_for_Social_Image_Understanding\figure_8.jpg
  Figure 8 caption: Examples of tag-based image retrieval on the NUS-WIDE dataset.
    The irrelevant images are marked with a red boundary. Best viewed in color.
  Figure 9 Link: articels_figures_by_rev_year\2018\Deep_Collaborative_Embedding_for_Social_Image_Understanding\figure_9.jpg
  Figure 9 caption: Illustration of content-based image retrieval results on the MIRFlickr
    and NUS-WIDE datasets. The irrelevant images are marked with a red boundary. Best
    viewed in color.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.99
  Name of the first author: Zechao Li
  Name of the last author: Tao Mei
  Number of Figures: 10
  Number of Tables: 6
  Number of authors: 3
  Paper title: Deep Collaborative Embedding for Social Image Understanding
  Publication Date: 2018-07-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Statistics of the Community-Contributed Datasets with Image
      and Tag Counts in the Format Mean Maximum
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Experimental Results (Mean Microauc \xB1 \xB1 Standard Deviation,\
      \ Mean Macroauc \xB1 \xB1 Standard Deviation and Mean F1 \xB1 \xB1 Standard\
      \ Deviation) for Social Image Tag Refinement on the MIRFlickr and NUS-WIDE Datasets"
  Table 3 caption:
    table_text: "TABLE 3 Experimental Results (Mean Microauc \xB1 \xB1 Standard Deviation,\
      \ Mean Macroauc \xB1 \xB1 Standard Deviation and Mean F1 \xB1 \xB1 Standard\
      \ Deviation) for Social Image Tag Assignment on the MIRFlickr and NUS-WIDE Datasets"
  Table 4 caption:
    table_text: TABLE 4 The Performance for Content-Based Image Retrieval in Terms
      of NDCG k k on MIRFlickr
  Table 5 caption:
    table_text: TABLE 5 The Performance of Content-Based Image Retrieval in Terms
      of NDCG k k on NUS-WIDE
  Table 6 caption:
    table_text: TABLE 6 The Left Part Shows the Average and Standard Deviation Values.
      The Right Part Shows the ANOVA Test Results
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2852750
- Affiliation of the first author: sungkyunkwan university, suwon, south korea
  Affiliation of the last author: korea advanced institute of science and technology
    (kaist), daejeon, south korea
  Figure 1 Link: articels_figures_by_rev_year\2018\Distance_Encoded_Product_Quantization_for_Approximate_KNearest_Neighbor_Search_i\figure_1.jpg
  Figure 1 caption: The left and right figures show toy examples of partitioned space
    and assigned compact codes for PQ and DPQ, respectively, when using 4 bit codes.
    Our method allocates first two bits for encoding a cluster index and another underlined
    two bits for quantized distances from cluster centers, while PQ utilizes all the
    bits for encoding a cluster index.
  Figure 10 Link: articels_figures_by_rev_year\2018\Distance_Encoded_Product_Quantization_for_Approximate_KNearest_Neighbor_Search_i\figure_10.jpg
  Figure 10 caption: "(a) shows empirical distributions of an angle between two vectors\
    \ x\u2212q(x) and y\u2212q(x) in three different benchmarks, where 100K (x,y)\
    \ pairs are randomly sampled and q(\u22C5) is a vector quantizer. (b) shows the\
    \ distribution in the subspaces constructed by OPQ with M=8, N K =256 ."
  Figure 2 Link: articels_figures_by_rev_year\2018\Distance_Encoded_Product_Quantization_for_Approximate_KNearest_Neighbor_Search_i\figure_2.jpg
  Figure 2 caption: This figure shows compact codes computed by Product Quantization
    (PQ) [5], Distance encoded Product Quantization (DPQ, Section 4), and Global Distance
    encoded Product Quantization (GDPQ, Section 5). In each subspace, DPQ encodes
    the cluster index ( C - i d i ) and quantized distance ( D - i d i ) between a
    data point and its corresponding cluster center, while PQ encodes only the cluster
    index. On the other hand, GDPQ encodes quantized global residual distances in
    the original space instead of subspaces.
  Figure 3 Link: articels_figures_by_rev_year\2018\Distance_Encoded_Product_Quantization_for_Approximate_KNearest_Neighbor_Search_i\figure_3.jpg
  Figure 3 caption: This figure shows the empirical quantization distortions as a
    function of the number of clusters (= N K ) in each subspace on (a) 960-dimensional
    GIST descriptors and (b) 4096-dimensional CNN features. M indicates the number
    of subspaces used. Details about the datasets are given in Section 6.
  Figure 4 Link: articels_figures_by_rev_year\2018\Distance_Encoded_Product_Quantization_for_Approximate_KNearest_Neighbor_Search_i\figure_4.jpg
  Figure 4 caption: (a) and (b) show the error of estimated symmetric distance (SD)
    and asymmetric distance (AD) with various number of subspaces, respectively. We
    randomly sample 100K pairs from the GIST-1M-960D dataset in the experiment. (c)
    and (d) similarly show mAP curves of 100-nearest neighbor search with SD and AD,
    respectively. M is the number of subspaces used.
  Figure 5 Link: articels_figures_by_rev_year\2018\Distance_Encoded_Product_Quantization_for_Approximate_KNearest_Neighbor_Search_i\figure_5.jpg
  Figure 5 caption: This figure visualizes errors of the symmetric distances (SD).
    We sample two random points, x and y , in a randomly selected subspace. The x-axis
    indicates the distance between x i and its corresponding cluster center q i (
    x i ) , and y-axis shows similar information for y . The vertical axis is the
    difference between the actual distance d( x i , y i ) and its estimated distance
    d( q i ( x i ), q i ( y i )) . The errors of estimated distances tend to be higher
    as the distance between data points and their corresponding cluster centers becomes
    larger. We use OPQ with M=8 and N K =256 to define subspaces and clusters on the
    GIST-960D dataset.
  Figure 6 Link: articels_figures_by_rev_year\2018\Distance_Encoded_Product_Quantization_for_Approximate_KNearest_Neighbor_Search_i\figure_6.jpg
  Figure 6 caption: The same protocol with Fig. 5 is used, however, the asymmetric
    distances are investigated on two different datasets. We treat that x is encoded
    but y is not. The y-axis indicates the error of asymmetric distance. Similar to
    the case of SD, the error of estimated distances tends to be higher when x is
    farther from its corresponding cluster center. The linear correlation coefficient
    of (a) and (b) are 0.68 and 0.70, respectively. In this experiment, we have used
    OPQ with M=8 and N K =256 to define subspaces and clusters.
  Figure 7 Link: articels_figures_by_rev_year\2018\Distance_Encoded_Product_Quantization_for_Approximate_KNearest_Neighbor_Search_i\figure_7.jpg
  Figure 7 caption: "This figure shows the distribution of differences from the ground-truth\
    \ distances to results estimated by the error corrected version of asymmetric\
    \ distance estimators (ECAD) on two different datasets. Specifically, PQ and OPQ\
    \ used Eq. (4)), and DPQ and DOPQ used Eq. (7). Our encoding scheme and distance\
    \ estimator provide lower bias and variance in distance estimation. The values\
    \ of bias and variance are available in Table 1. We draw 100K pairs of samples\
    \ randomly chosen from each dataset. All the tested methods utilized 64 bit compact\
    \ codes. PQ and OPQ used M=8 and N K =256 while DPQ and DOPQ used M=8 , N \u2032\
    \ K =128 , and N D =2 . (Best viewed in color)."
  Figure 8 Link: articels_figures_by_rev_year\2018\Distance_Encoded_Product_Quantization_for_Approximate_KNearest_Neighbor_Search_i\figure_8.jpg
  Figure 8 caption: The same protocol with Fig. 7 but different distance estimators
    are investigated. PQOPQ used the asymmetric distance estimator (Eq. (3)), and
    DPQDOPQ used our geometry based asymmetric distance estimator (Eq. (12)). PQ and
    OPQ with AD have significantly high biases and variances in distance estimation.
    In contrast, DPQ and DOPQ with GMAD provide a negligible bias with much smaller
    variances compuared to PQ or OPQ. Such bias and variance values are available
    in Table 1. (Best viewed in color).
  Figure 9 Link: articels_figures_by_rev_year\2018\Distance_Encoded_Product_Quantization_for_Approximate_KNearest_Neighbor_Search_i\figure_9.jpg
  Figure 9 caption: This figure shows two points x and y on the hyper-spheres centered
    at c x and c y respectively. r x and r y represent the radii of hyper-spheres.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.56
  Name of the first author: Jae-Pil Heo
  Name of the last author: Sung-Eui Yoon
  Number of Figures: 20
  Number of Tables: 4
  Number of authors: 3
  Paper title: Distance Encoded Product Quantization for Approximate K-Nearest Neighbor
    Search in High-Dimensional Space
  Publication Date: 2018-07-05 00:00:00
  Table 1 caption:
    table_text: TABLE 1 This Table Shows Empirically Measured Biases and Variances
      of Differences Between Exact Distances and Results Estimated by Various Distance
      Estimators
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 This Table Shows the mAPs of DPQ with Different Distance Estimators
  Table 3 caption:
    table_text: TABLE 3 Parameters Used in the Experiments
  Table 4 caption:
    table_text: TABLE 4 This Table Shows Computational Times for Encoding 1M (Million)
      Data, 1M Symmetric Distance Estimations, 1M Asymmetric Distance Estimations
      withwithout Utilizing Lookup Tables
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2853161
- Affiliation of the first author: google inc., mountain view, ca, usa
  Affiliation of the last author: google inc., mountain view, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\Visual_Dynamics_Stochastic_Future_Generation_via_Layered_Cross_Convolutional_Net\figure_1.jpg
  Figure 1 caption: Predicting the movement of an object from a single snapshot is
    often ambiguous. For instance, is the girls leg in (a) moving up or down? We propose
    a probabilistic, content-aware motion prediction model (b) that learns the conditional
    distribution of future frames, and produces a probable set of future frames (c).
    This schematic illustrates the idea behind our method, but does not show actual
    results produced by our model.
  Figure 10 Link: articels_figures_by_rev_year\2018\Visual_Dynamics_Stochastic_Future_Generation_via_Layered_Cross_Convolutional_Net\figure_10.jpg
  Figure 10 caption: Statistics of latent vectors z mean and z logvar extracted from
    1,000 image pairs from the Shapes, Sprites, and Exercise datasets, respectively.
    Each vertical line is for a single dimension in z . Although the z vector has
    3,200 dimensions, only a small number of those have values deviating from the
    prior (mean 0, log-variance 0). This shows the system is learning to encode motion
    information in a compact way. Also, as the motion in the Exercise dataset is more
    complex, the system needs more bits for encoding (c.f., Table 1).
  Figure 2 Link: articels_figures_by_rev_year\2018\Visual_Dynamics_Stochastic_Future_Generation_via_Layered_Cross_Convolutional_Net\figure_2.jpg
  Figure 2 caption: A toy example. Imagine a world composed of circles that move mostly
    vertically and squares that move mostly horizontally (a). We consider three different
    models (b-d) to learn the mapping from an image to a motion field. The first row
    shows graphical models, the second row shows corresponding network structures,
    and the third row shows estimated motion distributions. The deterministic motion
    prediction structure shown in (b) attempts to learn a one-to-one mapping from
    appearance to motion, but is unable to model multiple possible motions of an object,
    and tends to predict a mean motion for each object (the third row of (b)). The
    content-agnostic motion prior structure shown in (c) is able to capture a low-dimensional
    representation of motion, but is unable to leverage cues from image appearance
    for motion prediction. Therefore, it can only recovers the joint distribution
    of all objects (third row of (c)). The content-aware probabilistic motion predictor
    (d) brings together the advantages of models of (b) and (c) and uses appearance
    cues along with motion modeling to predict a motion field from a single input
    image. Therefore, the estimated motion distribution is very close to the ground
    truth (compare the last row and (a) and (d)).
  Figure 3 Link: articels_figures_by_rev_year\2018\Visual_Dynamics_Stochastic_Future_Generation_via_Layered_Cross_Convolutional_Net\figure_3.jpg
  Figure 3 caption: 'Our network consists of five components: (a) a motion encoder,
    (b) a kernel decoder, (c) an image encoder, (d) a cross convolution layer, and
    (e) a motion decoder. Our image encoder takes images at four scales as input.
    For simplicity, we only show two scales in this figure. See Section 4 for details
    of our model. The motion encoder (the grayed region) is only used in training.
    At testing time, the motion vector z is sampled from its empirical distribution
    (see text for more details).'
  Figure 4 Link: articels_figures_by_rev_year\2018\Visual_Dynamics_Stochastic_Future_Generation_via_Layered_Cross_Convolutional_Net\figure_4.jpg
  Figure 4 caption: Results on the Shapes dataset containing circles, squares, and
    triangles. For a given frame (a) our goal is to predict probable motion. In (b)
    we show the ground truth future frame. Notice how squares move horizontally, circles
    vertically, triangles diagonally, and the triangles motion is correlated with
    the circles. Our model is able to reconstruct the motion (c) after encoding and
    decoding with the ground truth image pairs. By sampling from the latent representation,
    we can also synthesize additional novel future frames with probable motion (d).
    In (e), we show zoomed-in regions for these samples. Note the significant variation
    among the sampled frames.
  Figure 5 Link: articels_figures_by_rev_year\2018\Visual_Dynamics_Stochastic_Future_Generation_via_Layered_Cross_Convolutional_Net\figure_5.jpg
  Figure 5 caption: 'The motion of the sampled data is consistent with the ground
    truth motion distributions. Left: for each object, comparison between its ground-truth
    motion distribution and the distribution predicted by our method. It shows the
    network learns to move circles vertically, squares, horizontally, and the motion
    of circles and triangles is correlated. Right: KL divergence between ground-truth
    distributions and distributions predicted by three different algorithms. Our network
    scores much better than a simple nearest-neighbor motion transfer algorithm.'
  Figure 6 Link: articels_figures_by_rev_year\2018\Visual_Dynamics_Stochastic_Future_Generation_via_Layered_Cross_Convolutional_Net\figure_6.jpg
  Figure 6 caption: "Left: Results on the Sprites dataset, where we show input images\
    \ (a), ground truth next frames (b), our reconstruction (c), two sampled future\
    \ frames (d), and corresponding zoomed-in views (e). Right: Percentages (%) of\
    \ synthesized results that were labeled as real by human subjects in two-way forced\
    \ choices on Amazon Mechanical Turk, at resolution 32\xD732 and 64\xD764. A perfect\
    \ algorithm would achieve a percentage around 50 percent."
  Figure 7 Link: articels_figures_by_rev_year\2018\Visual_Dynamics_Stochastic_Future_Generation_via_Layered_Cross_Convolutional_Net\figure_7.jpg
  Figure 7 caption: "Left: Results on Exercise dataset, where we show input images\
    \ (a), ground truth next frames (b), our reconstruction (c), three sampled future\
    \ frames (d), and corresponding zoomed-in views (e). Right: Percentages (%) of\
    \ synthesized results that were labeled as real by human subjects in two-way forced\
    \ choices on Amazon Mechanical Turk, at resolution 32\xD732 and 64\xD764. A perfect\
    \ algorithm would achieve a percentage around 50 percent."
  Figure 8 Link: articels_figures_by_rev_year\2018\Visual_Dynamics_Stochastic_Future_Generation_via_Layered_Cross_Convolutional_Net\figure_8.jpg
  Figure 8 caption: Learned layers on the Shapes dataset (left), the Sprites dataset
    (top right), and the Exercise dataset (bottom right). Our system is able to implicitly
    discover semantic structure from this self-supervised task. On the Shapes dataset,
    it learns to detect circles and triangles; it also detects vertical boundaries
    of squares, as squares always move horizontally. On the Exercise dataset, it learns
    to respond to only humans, not the carpet below them, as the carpet never moves.
  Figure 9 Link: articels_figures_by_rev_year\2018\Visual_Dynamics_Stochastic_Future_Generation_via_Layered_Cross_Convolutional_Net\figure_9.jpg
  Figure 9 caption: Sampling results on the PennAction dataset [43], where we show
    input images (a), ground truth next frames (b), our reconstruction (c), three
    sampled future frames (d), and corresponding zoomed-in views (e).
  First author gender probability: 0.72
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Tianfan Xue
  Name of the last author: William T. Freeman
  Number of Figures: 16
  Number of Tables: 2
  Number of authors: 4
  Paper title: 'Visual Dynamics: Stochastic Future Generation via Layered Cross Convolutional
    Networks'
  Publication Date: 2018-07-10 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Statistics of the 3,200-Dimensional Motion Vector z z
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Mean Squared Pixel Error on Test Analogies, by Animation
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2854726
- Affiliation of the first author: data61csiro, canberra, act, australia
  Affiliation of the last author: data61csiro, canberra, act, australia
  Figure 1 Link: articels_figures_by_rev_year\2018\Image_Deblurring_with_a_ClassSpecific_Prior\figure_1.jpg
  Figure 1 caption: Recovering spatial frequencies that have been suppressed by a
    blur kernel using band-pass frequency components from the training data.
  Figure 10 Link: articels_figures_by_rev_year\2018\Image_Deblurring_with_a_ClassSpecific_Prior\figure_10.jpg
  Figure 10 caption: Comparison of several methods on a sample image selected from
    the INRIA dataset [42]. Our method successfully recovers parts of the image with
    a significant resemblance to the ground-truth, including the pedestrians and the
    bus in the background. Our estimated kernel is also the most accurate among all
    the methods.
  Figure 2 Link: articels_figures_by_rev_year\2018\Image_Deblurring_with_a_ClassSpecific_Prior\figure_2.jpg
  Figure 2 caption: "A visual demonstration of the proposed prior. Top row (from left\
    \ to right): Original (ground-truth) image x \u2217 , input blurred image y ,\
    \ the image reconstructed by the weighted combination of 200 filtered training\
    \ images, and the absolute difference \u2225x\u2212 x \u2217 \u2225 . Second row\
    \ (from left to right): The four most important filtered training images sorted\
    \ by the descending order of their weights (shown in the inset). Third row: The\
    \ training images corresponding to those in the second row. Fourth row: The bandpass\
    \ filters (shown in the frequency domain) involved in the filtered training images\
    \ in the second row."
  Figure 3 Link: articels_figures_by_rev_year\2018\Image_Deblurring_with_a_ClassSpecific_Prior\figure_3.jpg
  Figure 3 caption: Latent images and kernels recovered by our method without (third
    column) and with the proposed prior (fourth column).
  Figure 4 Link: articels_figures_by_rev_year\2018\Image_Deblurring_with_a_ClassSpecific_Prior\figure_4.jpg
  Figure 4 caption: "Influence of the data fidelity term in the objective function\
    \ on the kernel estimate. A pair of PSNRSSIM error metrics is shown for each kernel\
    \ estimate in the sub-figures (d)\u2013 (f). (a) Ground-truth image, (b) blurred\
    \ image, (c) ground-truth kernel, (d) estimated kernel with the intensity term\
    \ only, (e) estimated kernel with the gradient term only, (f) estimated kernel\
    \ with both terms."
  Figure 5 Link: articels_figures_by_rev_year\2018\Image_Deblurring_with_a_ClassSpecific_Prior\figure_5.jpg
  Figure 5 caption: The relative reconstruction errors (averaged over 80 test images)
    for the INRIA person [42], the CMU-PIE [38] and the Yale-B [41] datasets.
  Figure 6 Link: articels_figures_by_rev_year\2018\Image_Deblurring_with_a_ClassSpecific_Prior\figure_6.jpg
  Figure 6 caption: The convergence of the iterative algorithm. The image and kernel
    similarity between the estimated and the ground-truth are measured in terms of
    the SSIM.
  Figure 7 Link: articels_figures_by_rev_year\2018\Image_Deblurring_with_a_ClassSpecific_Prior\figure_7.jpg
  Figure 7 caption: Estimated kernels for the sample image in Fig. 6 at different
    scales. As visible, the kernel becomes progressively more similar to the ground-truth
    at finer resolutions.
  Figure 8 Link: articels_figures_by_rev_year\2018\Image_Deblurring_with_a_ClassSpecific_Prior\figure_8.jpg
  Figure 8 caption: Deblurring results for real input images from [12], where the
    one in the first row contains noise and saturated pixels.
  Figure 9 Link: articels_figures_by_rev_year\2018\Image_Deblurring_with_a_ClassSpecific_Prior\figure_9.jpg
  Figure 9 caption: Results for a sample image from the Car dataset in [39]. The restored
    image from our method has more legible text on the license plate compared to the
    other methods.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Saeed Anwar
  Name of the last author: Fatih Porikli
  Number of Figures: 21
  Number of Tables: 9
  Number of authors: 3
  Paper title: Image Deblurring with a Class-Specific Prior
  Publication Date: 2018-07-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 A Comparison of the Accuracy Achieved by Our Deblurring Framework
      on All the Mentioned Datasets with and without the Proposed Prior
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Influence of Intensity and Gradient Fidelity Terms on the
      Deblurring Results
  Table 3 caption:
    table_text: TABLE 3 Deblurring Performance (in PSNR) on the CMU PIE Dataset for
      Different Numbers of Training Images
  Table 4 caption:
    table_text: TABLE 4 Deblurring Performance (in PSNR) on Each Class of Test Input
      (Blurred) Images When Using Various (External) Training Datasets
  Table 5 caption:
    table_text: "TABLE 5 The Average Image Accuracy (in PSNR) Achieved with a Constant\
      \ Prior Weight \u03B2 \u03B2 When Our Algorithm Is Evaluated on the Person Dataset\
      \ [42]"
  Table 6 caption:
    table_text: TABLE 6 The Average Accuracy of the Deblurred Image (in PNSR) for
      the Person Dataset [42], with Respect to Different Numbers of Bandpass Filters
      M M
  Table 7 caption:
    table_text: TABLE 7 A Comparison of the Image and Kernel Accuracy (in PSNR) Obtained
      Using Greyscale versus Colour Input Images
  Table 8 caption:
    table_text: TABLE 8 Accuracy of the Deblurred Images, Measured by SSIM and PSNR
  Table 9 caption:
    table_text: TABLE 9 The Accuracy of the Estimated Kernel, Measured by SSIM and
      Ratio of MSE
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2855177
- Affiliation of the first author: department of computer science, rutgers university,
    piscataway, nj, usa
  Affiliation of the last author: department of computer science, rutgers university,
    piscataway, nj, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\StackGAN_Realistic_Image_Synthesis_with_Stacked_Generative_Adversarial_Networks\figure_1.jpg
  Figure 1 caption: The architecture of the proposed StackGAN-v1. The Stage-I generator
    draws a low-resolution image by sketching rough shape and basic colors of the
    object from the given text and painting the background from a random noise vector.
    Conditioned on Stage-I results, the Stage-II generator corrects defects and adds
    compelling details into Stage-I results, yielding a more realistic high-resolution
    image.
  Figure 10 Link: articels_figures_by_rev_year\2018\StackGAN_Realistic_Image_Synthesis_with_Stacked_Generative_Adversarial_Networks\figure_10.jpg
  Figure 10 caption: Samples generated by our StackGAN-v1 from unseen texts in CUB
    test set. Each column lists the text description, images generated from the text
    by Stage-I and Stage-II of StackGAN-v1.
  Figure 2 Link: articels_figures_by_rev_year\2018\StackGAN_Realistic_Image_Synthesis_with_Stacked_Generative_Adversarial_Networks\figure_2.jpg
  Figure 2 caption: The overall framework of our proposed StackGAN-v2 for the conditional
    image synthesis task. c is the vector of conditioning variables which can be computed
    from the class label, the text description, etc. N g and N d are the numbers of
    channels of a tensor.
  Figure 3 Link: articels_figures_by_rev_year\2018\StackGAN_Realistic_Image_Synthesis_with_Stacked_Generative_Adversarial_Networks\figure_3.jpg
  Figure 3 caption: Example results by our StackGANs, GAWWN [33], and GAN-INT-CLS
    [35] conditioned on text descriptions from CUB test set.
  Figure 4 Link: articels_figures_by_rev_year\2018\StackGAN_Realistic_Image_Synthesis_with_Stacked_Generative_Adversarial_Networks\figure_4.jpg
  Figure 4 caption: Example results by our StackGANs and GAN-INT-CLS [35] conditioned
    on text descriptions from Oxford-102 test set (leftmost four columns) and COCO
    validation set (rightmost four columns).
  Figure 5 Link: articels_figures_by_rev_year\2018\StackGAN_Realistic_Image_Synthesis_with_Stacked_Generative_Adversarial_Networks\figure_5.jpg
  Figure 5 caption: Utilizing t-SNE to embed images generated by our StackGAN-v1 and
    StackGAN-v2 on the CUB test set.
  Figure 6 Link: articels_figures_by_rev_year\2018\StackGAN_Realistic_Image_Synthesis_with_Stacked_Generative_Adversarial_Networks\figure_6.jpg
  Figure 6 caption: Comparison of samples generated by models trained on LSUN bedroom
    dataset (Zoom in for better comparison).
  Figure 7 Link: articels_figures_by_rev_year\2018\StackGAN_Realistic_Image_Synthesis_with_Stacked_Generative_Adversarial_Networks\figure_7.jpg
  Figure 7 caption: Comparison of samples generated by models trained on ImageNet
    dog dataset.
  Figure 8 Link: articels_figures_by_rev_year\2018\StackGAN_Realistic_Image_Synthesis_with_Stacked_Generative_Adversarial_Networks\figure_8.jpg
  Figure 8 caption: "256 \xD7 256 samples generated by our StackGAN-v1 (top) and StackGAN-v2\
    \ (bottom) on ImageNet cat (left) and LSUN church (right)."
  Figure 9 Link: articels_figures_by_rev_year\2018\StackGAN_Realistic_Image_Synthesis_with_Stacked_Generative_Adversarial_Networks\figure_9.jpg
  Figure 9 caption: Examples of failure cases of StackGAN-v1 (top) and StackGAN-v2
    (bottom) on different datasets.
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Han Zhang
  Name of the last author: Dimitris N. Metaxas
  Number of Figures: 15
  Number of Tables: 5
  Number of authors: 7
  Paper title: 'StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial
    Networks'
  Publication Date: 2018-07-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Statistics of Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Inception Scores (IS), Fr\xE9chet Inception Distance (FID)\
      \ and Average Human Ranks (HR) of GAN-INT-CLS [35], GAWWN [33] and Our StackGAN-v1\
      \ on CUB, Oxford-102, and COCO"
  Table 3 caption:
    table_text: "TABLE 3 Comparison of StackGAN-v1 and StackGAN-v2 on Different Datasets\
      \ by Inception Scores (IS), Fr\xE9chet Inception Distance (FID) and Average\
      \ Human Ranks (HR)"
  Table 4 caption:
    table_text: TABLE 4 Inception Scores Calculated with 30,000 Samples Generated
      on CUB by Different Baseline Models of Our StackGAN-v1
  Table 5 caption:
    table_text: TABLE 5 Inception Scores by Our StackGAN-v2 and Its Baseline Models
      on CUB test set
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2856256
