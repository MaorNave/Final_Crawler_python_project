- Affiliation of the first author: department of computer science and engineering,
    michigan state university, east lansing, usa
  Affiliation of the last author: department of computer science and engineering,
    michigan state university, east lansing, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\Representation_Learning_by_Rotating_Your_Faces\figure_1.jpg
  Figure 1 caption: Given one or multiple in-the-wild face images as the input, DR-GAN
    can produce a unified identity representation, by virtually rotating the face
    to arbitrary poses. The learnt representation is both discriminative and generative,
    i.e., the representation is able to demonstrate superior PIFR performance, and
    synthesize identity-preserved faces at target poses specified by the pose code.
  Figure 10 Link: articels_figures_by_rev_year\2018\Representation_Learning_by_Rotating_Your_Faces\figure_10.jpg
  Figure 10 caption: Face rotation comparison on Multi-PIE. Given the input (in illumination
    07 and 75circ pose), we show synthetic images of L2 loss (top), adversarial loss
    (middle), and ground truth (bottom). Column 2-5 show the ability of DR-GAN in
    simultaneous face rotation and re-lighting.
  Figure 2 Link: articels_figures_by_rev_year\2018\Representation_Learning_by_Rotating_Your_Faces\figure_2.jpg
  Figure 2 caption: Comparison of previous GAN architectures and our proposed DR-GAN.
  Figure 3 Link: articels_figures_by_rev_year\2018\Representation_Learning_by_Rotating_Your_Faces\figure_3.jpg
  Figure 3 caption: Generator in multi-image DR-GAN. From an image set of a subject,
    we can fuse the features to a single representation via dynamically learnt coefficients
    and synthesize images in any pose.
  Figure 4 Link: articels_figures_by_rev_year\2018\Representation_Learning_by_Rotating_Your_Faces\figure_4.jpg
  Figure 4 caption: Recognition performance of G enc and D d when training DR-GAN
    with different D d on Multi-PIE dataset.
  Figure 5 Link: articels_figures_by_rev_year\2018\Representation_Learning_by_Rotating_Your_Faces\figure_5.jpg
  Figure 5 caption: The mean faces of 13 pose groups in CASIA-Webface. The blurriness
    shows the challenges of pose estimation for large poses.
  Figure 6 Link: articels_figures_by_rev_year\2018\Representation_Learning_by_Rotating_Your_Faces\figure_6.jpg
  Figure 6 caption: Generated faces of DR-GAN and its partial variants.
  Figure 7 Link: articels_figures_by_rev_year\2018\Representation_Learning_by_Rotating_Your_Faces\figure_7.jpg
  Figure 7 caption: 'Responses of two filters: filter with the highest responses to
    identity (left), and pose (right). Responses of each row are of the same subject,
    and each column are of the same pose. Note the within-row similarity on the left
    and within-column similarity on the right.'
  Figure 8 Link: articels_figures_by_rev_year\2018\Representation_Learning_by_Rotating_Your_Faces\figure_8.jpg
  Figure 8 caption: Coefficient distributions on IJB-A (a) and CFP (b). For IJB-A,
    we visualize images at four regions of the distribution. For CFP, we plot the
    distributions for frontal faces (blue) and profile faces (red) separately and
    show images at the heads and tails of each distribution.
  Figure 9 Link: articels_figures_by_rev_year\2018\Representation_Learning_by_Rotating_Your_Faces\figure_9.jpg
  Figure 9 caption: Coefficients and classification probabilities correlation.
  First author gender probability: 0.91
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.84
  Name of the first author: Luan Tran
  Name of the last author: Xiaoming Liu
  Number of Figures: 14
  Number of Tables: 11
  Number of authors: 3
  Paper title: Representation Learning by Rotating Your Faces
  Publication Date: 2018-09-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Structures of G enc Genc, G dec Gdec and D D Networks
      in Single-Image and Multi-Image DR-GAN
  Table 10 caption:
    table_text: TABLE 10 Identification Rate ( % %) Comparison on Multi-PIE Dataset
  Table 2 caption:
    table_text: TABLE 2 DR-GAN and its Partial Variants Performance Comparison
  Table 3 caption:
    table_text: TABLE 3 Comparison of Single versus Multi-Image DR-GAN on CFP
  Table 4 caption:
    table_text: TABLE 4 Performance of G enc Genc on Multi-PIE When Keep Switching
      to D d Dd. at Epoch 0, G enc Genc Is Trained with Only the Softmax Loss
  Table 5 caption:
    table_text: "TABLE 5 Performance of IJB-A When Removing Images by Threshold \u03C9\
      \ t \u03C9t"
  Table 6 caption:
    table_text: TABLE 6 Fusion Schemes Comparisons on IJB-A Dataset
  Table 7 caption:
    table_text: TABLE 7 Loss Function Comparisons
  Table 8 caption:
    table_text: TABLE 8 Performance Comparison on IJB-A Dataset
  Table 9 caption:
    table_text: TABLE 9 Performance (Accuracy) Comparison on CFP
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2868350
- Affiliation of the first author: university of canberra, bruce, act, australia
  Affiliation of the last author: school of computer science and software engineering,
    the university of western australia, crawley, wa, australia
  Figure 1 Link: Not Available
  Figure 1 caption: Not Available
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Munawar Hayat
  Name of the last author: Senjian An
  Number of Figures: Not Available
  Number of Tables: 0
  Number of authors: 3
  Paper title: "Response to \u201CGhost Numbers\u201D"
  Publication Date: 2018-09-02 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2789444
- Affiliation of the first author: state key laboratory for novel software technology,
    nanjing university, nanjing, china
  Affiliation of the last author: computer vision laboratory, eth zurich, zurich,
    switzerland
  Figure 1 Link: articels_figures_by_rev_year\2018\Temporal_Segment_Networks_for_Action_Recognition_in_Videos\figure_1.jpg
  Figure 1 caption: 'Temporal segment network: One input video is divided into K segments
    (here we show the K=3 case) and a short snippet is randomly selected from each
    segment. The snippets are represented by modalities such as RGB frames, optical
    flow (upper grayscale images), and RGB difference (lower grayscale images). The
    class scores of different snippets from the same modality are fused by a segmental
    consensus function to yield a video-level prediction. ConvNets on all snippets
    share the same parameters.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Temporal_Segment_Networks_for_Action_Recognition_in_Videos\figure_2.jpg
  Figure 2 caption: 'Examples of four types of input modalities: RGB images, RGB difference,
    optical flow field (x,y directions), and warped optical flow field (x,y directions).'
  Figure 3 Link: articels_figures_by_rev_year\2018\Temporal_Segment_Networks_for_Action_Recognition_in_Videos\figure_3.jpg
  Figure 3 caption: 'Visualization of ConvNet models for action recognition using
    DeepDraw [79]. It is worth noting that video frames are just representatives of
    the corresponding classes, but not used for these visualizations. All these images
    are generated from purely random pixels. We compare two settings: (1) without
    temporal segment network (No TSN); (2) with temporal segment network (TSN). For
    spatial ConvNets, we plot two generated visualization as color images. For temporal
    ConvNets, we plot the flow maps of x (left) and y (right) directions in gray scales.
    Left: classes in UCF101. Right: classes in ActivityNet v1.2.'
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Limin Wang
  Name of the last author: Luc Van Gool
  Number of Figures: 3
  Number of Tables: 9
  Number of authors: 7
  Paper title: Temporal Segment Networks for Action Recognition in Videos
  Publication Date: 2018-09-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Exploration of Different Training Strategies for Two-Stream
      ConvNets on the UCF101 Dataset (Split 1)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Exploration of Different Combination of Modalities with TSN
      on the UCF101 Dataset (Over Three Splits)
  Table 3 caption:
    table_text: TABLE 3 Comparison of Segment Based Sampling of TSN with the Regular
      Sampling on the UCF101 Dataset (Over Three Splits)
  Table 4 caption:
    table_text: TABLE 4 Exploration of Different Segment Numbers K K in TSN on the
      UCF101 Dataset (Over Three Splits) and the ActivityNet Dataset (Train on the
      Training Set and Test on the Validation Set)
  Table 5 caption:
    table_text: TABLE 5 Exploration of Different Segmental Consensus Functions for
      TSN on the UCF101 Dataset (Over Three Splits) and the ActivtyNet Dataset (Train
      on the Training Set and Test on the Validation Set)
  Table 6 caption:
    table_text: TABLE 6 Comparison of Different ConvNet Architectures on the UCF101
      Dataset (Over Three Splits)
  Table 7 caption:
    table_text: TABLE 7 Evaluation on the Validation Set of ActivityNet Challenge
      2016 Data (ActivityNet v1.3 Val.)
  Table 8 caption:
    table_text: TABLE 8 Winning Entries in the Untrimmed Video Classification Task
      of ActivityNet Challenge 2016 (ActivityNet v1.3 Test)
  Table 9 caption:
    table_text: TABLE 9 Comparison of Our Method Based on TSN with Other State-of-the-Art
      Approaches on the Datasets of HMDB51, UCF101, THUMOS14, ActivityNet v1.2, and
      Kinetics400
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2868668
- Affiliation of the first author: school of software, tsinghua university, beijing,
    china
  Affiliation of the last author: department of eecs, department of statistics, university
    of california, berkeley, berkeley, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\Transferable_Representation_Learning_with_Deep_Adaptation_Networks\figure_1.jpg
  Figure 1 caption: "The deep adaptation network (DAN) based on AlexNet [22]. Since\
    \ deep features transition from general to specific along the network: 1) the\
    \ features extracted by convolutional layers conv1 \u2013 conv5 are transferable,\
    \ hence these layers are learned via fine-tuning; 2) fully connected layers fc6\
    \ \u2013 fc8 are tailored to fit task-specific structures, hence they are not\
    \ safely transferable and should be adapted with MK-MMDME minimization. To mitigate\
    \ the misspecification of the source classifier to the target data, the low-density\
    \ separation of target data is achieved by entropy minimization."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Transferable_Representation_Learning_with_Deep_Adaptation_Networks\figure_2.jpg
  Figure 2 caption: "Visualization analysis: (a) t-SNE of source features by AlexNet;\
    \ (b)\u2013(e) t-SNE of target features by AlexNet, DDC, DAN-ent, and DAN."
  Figure 3 Link: articels_figures_by_rev_year\2018\Transferable_Representation_Learning_with_Deep_Adaptation_Networks\figure_3.jpg
  Figure 3 caption: Sample images where our method succeeds while the baseline method
    fails or vice versa.
  Figure 4 Link: articels_figures_by_rev_year\2018\Transferable_Representation_Learning_with_Deep_Adaptation_Networks\figure_4.jpg
  Figure 4 caption: "Empirical analysis: (a) Proxy- A -Distance of different features;\
    \ (b)\u2013(c) Sensitivity of \u03BB and \u03B3 (dashed lines show best baselines);\
    \ (d) Scalability."
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Mingsheng Long
  Name of the last author: Michael I. Jordan
  Number of Figures: 4
  Number of Tables: 5
  Number of authors: 5
  Paper title: Transferable Representation Learning with Deep Adaptation Networks
  Publication Date: 2018-09-05 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Accuracy on Office-31 Dataset Under Non-Sampling Protocol
      [5] for Unsupervised and Semi-Supervised Domain Adaptation (AlexNet)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Accuracy on Office-31 Dataset Under Down-Sampling [26] and
      Non-Sampling [5] Protocols for Unsupervised Domain Adaptation (GoogLeNet)
  Table 3 caption:
    table_text: TABLE 3 Accuracy on Office-31 Dataset Under Non-Sampling [5] Protocol
      for Unsupervised Domain Adaptation (ResNet)
  Table 4 caption:
    table_text: TABLE 4 Classification Accuracy on Office-Caltech Dataset Under Non-Sampling
      Protocol [5] for Unsupervised Domain Adaptation (AlexNet)
  Table 5 caption:
    table_text: TABLE 5 Classification Accuracy on ImageCLEF-DA Dataset Under Non-Sampling
      Protocol [5] for Unsupervised Domain Adaptation (GoogLeNet)
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2868685
- Affiliation of the first author: department of electrical and computer engineering,
    the ohio state university, columbus, usa
  Affiliation of the last author: department of electrical and computer engineering,
    the ohio state university, columbus, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\Discriminant_Functional_Learning_of_Color_Features_for_the_Recognition_of_Facial\figure_1.jpg
  Figure 1 caption: 'Top: A few frames of a video sequences showing a facial expression
    of happily surprised. Note we have demarked a local region on that individuals
    right cheek with red lines. You may notice that the average and standard devision
    of the color of the pixels in this local region change over time. The value changes
    of the red, green, and blue channels of the pixels in this local region are given
    in the bottom plot, with f R j (t) showing the functional change in the red channel,
    f G j (t) in the green, and f B j (t) in the blue. Our contribution is to derive
    a method that can learn to identify when a facial action unit is active by exclusively
    using these color changes.'
  Figure 10 Link: articels_figures_by_rev_year\2018\Discriminant_Functional_Learning_of_Color_Features_for_the_Recognition_of_Facial\figure_10.jpg
  Figure 10 caption: ROC curves for the AM-FED dataset.
  Figure 2 Link: articels_figures_by_rev_year\2018\Discriminant_Functional_Learning_of_Color_Features_for_the_Recognition_of_Facial\figure_2.jpg
  Figure 2 caption: "The local regions of the face (left image) used by the derived\
    \ algorithm. These local regions are obtained by Delaunay triangulation of the\
    \ automatically detected fiducial points shown on the right image. These fiducial\
    \ points, s ij ( j=1,\u2026,66 ), correspond to 15 anatomical landmarks (e.g.,\
    \ corners of the eyes, mouth and brows, tip of the nose, and chin) plus 51 pseudo-landmarks\
    \ defined about the edge of the eyelids, brows, nose, lips, and jaw line. The\
    \ number of pseudo-landmarks defining the contour of each facial component (e.g.,\
    \ the brows) is constant as is their inter-landmark distance. This guarantees\
    \ equivalency of landmark position across people. This triangulation yields 107\
    \ regions (patches)."
  Figure 3 Link: articels_figures_by_rev_year\2018\Discriminant_Functional_Learning_of_Color_Features_for_the_Recognition_of_Facial\figure_3.jpg
  Figure 3 caption: The top left plot shows the function f(.) of a video V . The bottom
    left plot is a template function f T (.) representing the color changes observed
    when people activate AU 1. Identifying this template f T (.) in f(.) requires
    us to test all possible locations about time. This matching process in computationally
    expensive. The Gabor transform solves this complexity issue by identifying the
    location where the template function matches the color function without resorting
    to a sliding-window approach (right-most plot).
  Figure 4 Link: articels_figures_by_rev_year\2018\Discriminant_Functional_Learning_of_Color_Features_for_the_Recognition_of_Facial\figure_4.jpg
  Figure 4 caption: 'Top: A schematic representation of positive samples (blue squares)
    and negative samples (orange triangles). Positive feature vectors correspond to
    videos with the activation of a specific AU. Negative sample videos do not have
    that AU present. Note that the sample videos need not be of the same length. Bottom:
    An example of a color functional space obtained with a SVM classifier for video
    sequences of facial expressions with AU 12 activeinactive.'
  Figure 5 Link: articels_figures_by_rev_year\2018\Discriminant_Functional_Learning_of_Color_Features_for_the_Recognition_of_Facial\figure_5.jpg
  Figure 5 caption: "Each video segment W ik (shown on the bottom left) yields a feature\
    \ representation f ijk (top left), j=1,\u2026,107 . We regress a function g(.)\
    \ to learn to map from the last image of W ik to f ijk , j=1,\u2026,107 (right\
    \ image)."
  Figure 6 Link: articels_figures_by_rev_year\2018\Discriminant_Functional_Learning_of_Color_Features_for_the_Recognition_of_Facial\figure_6.jpg
  Figure 6 caption: 'F1 scores for the proposed approach and a variety of published
    algorithms. Note that not all published methods provide results for all AUs. This
    is why you see empty columns in the plots above. Average (Avg) is computed using
    the results of the available AUs in each algorithm. First plot: BP4D. Second plot:
    DISFA. Third plot: SP. Fourth plot: AM-FED.'
  Figure 7 Link: articels_figures_by_rev_year\2018\Discriminant_Functional_Learning_of_Color_Features_for_the_Recognition_of_Facial\figure_7.jpg
  Figure 7 caption: ROC curves of the results on the BP4D dataset. The left image
    shows the ROC of all AUs combined. This shows the small variations between different
    AUs. The other plots show the ROCs of each AU. The area under the curve for each
    of these AUs are given in Table 3.
  Figure 8 Link: articels_figures_by_rev_year\2018\Discriminant_Functional_Learning_of_Color_Features_for_the_Recognition_of_Facial\figure_8.jpg
  Figure 8 caption: ROC curves of our results on the Shoulder Pain dataset.
  Figure 9 Link: articels_figures_by_rev_year\2018\Discriminant_Functional_Learning_of_Color_Features_for_the_Recognition_of_Facial\figure_9.jpg
  Figure 9 caption: ROC curves of our results on the DISFA dataset.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: C. Fabian Benitez-Quiroz
  Name of the last author: Aleix M. Martinez
  Number of Figures: 14
  Number of Tables: 3
  Number of authors: 3
  Paper title: Discriminant Functional Learning of Color Features for the Recognition
    of Facial Action Units and Their Intensities
  Publication Date: 2018-09-05 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Description of the Deep Network Architecture Used in This
      Paper
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average F1 Score for Different Classifiers
  Table 3 caption:
    table_text: TABLE 3 Area under the Curve of the ROC Curves Shown in Figs. 7, 8,
      9, and 10
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2868952
- Affiliation of the first author: department of computer science, university of york,
    york, united kingdom
  Affiliation of the last author: "research unit at department of mathematics, sapienza\
    \ - universit\xE0 di roma, rome, italy"
  Figure 1 Link: articels_figures_by_rev_year\2018\HeightfromPolarisation_with_Unknown_Lighting_or_Albedo\figure_1.jpg
  Figure 1 caption: 'Overview of method: from a single polarisation image in unknown
    (possibly outdoor) illumination, we estimate lighting and compute surface height
    directly (rightmost image shows result on real data, a piece of fruit).'
  Figure 10 Link: articels_figures_by_rev_year\2018\HeightfromPolarisation_with_Unknown_Lighting_or_Albedo\figure_10.jpg
  Figure 10 caption: Qualitative results indoors with point light source and uniform
    albedo.
  Figure 2 Link: articels_figures_by_rev_year\2018\HeightfromPolarisation_with_Unknown_Lighting_or_Albedo\figure_2.jpg
  Figure 2 caption: Polarimetric capture (a) and decomposition to polarisation image
    (b-d) from captured data of a piece of fruit.
  Figure 3 Link: articels_figures_by_rev_year\2018\HeightfromPolarisation_with_Unknown_Lighting_or_Albedo\figure_3.jpg
  Figure 3 caption: 'Visualisation of constraints on surface normal provided by polarisation
    image: phase angle (red), unpolarised intensity (green) and degree of polarisation
    (blue). In non-degenerate cases, the three constraints uniquely determine the
    surface normal direction and we show how to express these constraints directly
    in terms of surface height.'
  Figure 4 Link: articels_figures_by_rev_year\2018\HeightfromPolarisation_with_Unknown_Lighting_or_Albedo\figure_4.jpg
  Figure 4 caption: "(a) Relationship between degree of polarisation and zenith angle,\
    \ for specular and diffuse dielectric reflectance with \u03B7=1.5 . (b) Zenith\
    \ angle estimated from Fig. 2b. (c) Visualisation of the cosine of estimated zenith\
    \ angle."
  Figure 5 Link: articels_figures_by_rev_year\2018\HeightfromPolarisation_with_Unknown_Lighting_or_Albedo\figure_5.jpg
  Figure 5 caption: Illustration of ambiguity using a 1D surface viewed from above.
    Polarisation normals are locally ambiguous (green versus red), leading to 2 4
    possible disambiguations. With unknown lighting direction, the introduction of
    shading information reduces the ambiguity to a global, binary one. For the shading
    images at the bottom, the two possible disambiguations are black versus orange
    with the resulting local disambiguations shown in green.
  Figure 6 Link: articels_figures_by_rev_year\2018\HeightfromPolarisation_with_Unknown_Lighting_or_Albedo\figure_6.jpg
  Figure 6 caption: Typical surface normal estimates (c-e) from noisy synthetic data
    (a). The inset sphere in (b) shows how surface orientation is visualised as a
    colour. Results obtained by [3], [4] in (d) and [15] in (e) for comparison.
  Figure 7 Link: articels_figures_by_rev_year\2018\HeightfromPolarisation_with_Unknown_Lighting_or_Albedo\figure_7.jpg
  Figure 7 caption: From noisy synthetic data (a) we estimate a spatially varying
    albedo map (b). Ground truth is shown in (c). Surface normals (d) of height map
    estimated from (a) once estimated albedo has been divided out.
  Figure 8 Link: articels_figures_by_rev_year\2018\HeightfromPolarisation_with_Unknown_Lighting_or_Albedo\figure_8.jpg
  Figure 8 caption: Qualitative estimation results on a real teapot with varying albedo.
    Input (left), estimated albedo (middle), estimated surface normals (right).
  Figure 9 Link: articels_figures_by_rev_year\2018\HeightfromPolarisation_with_Unknown_Lighting_or_Albedo\figure_9.jpg
  Figure 9 caption: Qualitative comparison against [3], [4] and [15] on real world
    data. Light source direction = [2 0 7] . For our method we show estimated surface
    height, normals, relit surface and texture mapped surface. For the comparison
    methods we show normals and relit surface.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: William A. P. Smith
  Name of the last author: Silvia Tozza
  Number of Figures: 14
  Number of Tables: 1
  Number of authors: 3
  Paper title: Height-from-Polarisation with Unknown Lighting or Albedo
  Publication Date: 2018-09-06 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Quantitative Results on Synthetic Data ( \u03C3=0.5% \u03C3\
      =0.5%)"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2868065
- Affiliation of the first author: universidad del atlantico medio, las palmas, spain
  Affiliation of the last author: universidad de las palmas de gran canaria, las palmas,
    spain
  Figure 1 Link: articels_figures_by_rev_year\2018\Anthropomorphic_Features_for_OnLine_Signatures\figure_1.jpg
  Figure 1 caption: Anthropomorphic features for signature verification systems, extracted
    by a Virtual Skeletal Arm model.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Anthropomorphic_Features_for_OnLine_Signatures\figure_2.jpg
  Figure 2 caption: Architecture of the Virtual Skeletal Arm (VSA) model.
  Figure 3 Link: articels_figures_by_rev_year\2018\Anthropomorphic_Features_for_OnLine_Signatures\figure_3.jpg
  Figure 3 caption: Coordinate frames S 0 , S 6 and S s and their associated homogeneous
    transformation matrices 0 T i 6 , 0 T s and s T i 6 .
  Figure 4 Link: articels_figures_by_rev_year\2018\Anthropomorphic_Features_for_OnLine_Signatures\figure_4.jpg
  Figure 4 caption: Pen orientation lbrace S6rbrace with respect to the writing surface
    lbrace Ssrbrace .
  Figure 5 Link: articels_figures_by_rev_year\2018\Anthropomorphic_Features_for_OnLine_Signatures\figure_5.jpg
  Figure 5 caption: Configuration of VSA for computing qi2 and qi3 .
  Figure 6 Link: articels_figures_by_rev_year\2018\Anthropomorphic_Features_for_OnLine_Signatures\figure_6.jpg
  Figure 6 caption: Similarities in the real and robotic on-line signature trajectories.
  Figure 7 Link: articels_figures_by_rev_year\2018\Anthropomorphic_Features_for_OnLine_Signatures\figure_7.jpg
  Figure 7 caption: Example of the effects of different pen-tip angles on the orientation
    of the CF lbrace S6rbrace in the angle-based anthropomorphic features.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Moises Diaz
  Name of the last author: Jose J. Quintana
  Number of Figures: 7
  Number of Tables: 8
  Number of authors: 3
  Paper title: Anthropomorphic Features for On-Line Signatures
  Publication Date: 2018-09-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Notation Used in the Anthropomorphic Feature Extraction
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 DH Parameters, DH i k DHki
  Table 3 caption:
    table_text: TABLE 3 Influence of Pen-Tip Angles
  Table 4 caption:
    table_text: TABLE 4 Pen-Down & Pen-Ups Effect
  Table 5 caption:
    table_text: TABLE 5 Influence of the Initial Posture of the VSA Model
  Table 6 caption:
    table_text: TABLE 6 EER Results (in %) Using the Anthropomorphic Features Generated
      by the Virtual Skeletal Arm Model Against the Third Party Databases and Verifiers
      Used
  Table 7 caption:
    table_text: TABLE 7 EER Results (in %) Using the Anthropomorphic Features Generated
      by the Realistic Virtual Skeletal Arm (VSA r r) Model Against the Third Party
      Databases and Verifiers Used
  Table 8 caption:
    table_text: TABLE 8 On-Line Automatic Signature Verification Results in the State-of-the-Art
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2869163
- Affiliation of the first author: information systems, faculty of natural sciences,
    university of haifa, haifa, israel
  Affiliation of the last author: department of information systems, university of
    haifa, haifa, israel
  Figure 1 Link: articels_figures_by_rev_year\2018\Estimating_the_Number_of_Correct_Matches_Using_Only_Spatial_Order\figure_1.jpg
  Figure 1 caption: "Correctlyincorrectly matched features are marked as greenyellow\
    \ circles, respectively. The feature sequences are given by [N]=\u27E81,2,3,4,5,6,7,8,9,10\u27E9\
    \ and \u03C3=\u27E89,5,1,3,4,2,6,7,8\u27E9 . For example, \u03C3(1)=9 and \u03C3\
    (5)=4 . Note that the green lines (correct matches) do not intersect, while most\
    \ of the yellow lines (incorrect matches) do."
  Figure 10 Link: articels_figures_by_rev_year\2018\Estimating_the_Number_of_Correct_Matches_Using_Only_Spatial_Order\figure_10.jpg
  Figure 10 caption: USAC for inlier rate estimation without the halting condition
    ( y -axis) versus with the halting condition ( x -axis).
  Figure 2 Link: articels_figures_by_rev_year\2018\Estimating_the_Number_of_Correct_Matches_Using_Only_Spatial_Order\figure_2.jpg
  Figure 2 caption: "An example of an order inversion due to a \u201Cpole\u201D that\
    \ contradicts assumption A1. The blue feature in the upper image is to the left\
    \ of the green feature, and vice versa in the bottom image."
  Figure 3 Link: articels_figures_by_rev_year\2018\Estimating_the_Number_of_Correct_Matches_Using_Only_Spatial_Order\figure_3.jpg
  Figure 3 caption: Classification of the pairs of matches into (a) only correct matches,
    (b) only incorrect matches and (c) correct and incorrect matches. Note that, for
    clarity, some of the matched pairs are omitted from (c).
  Figure 4 Link: articels_figures_by_rev_year\2018\Estimating_the_Number_of_Correct_Matches_Using_Only_Spatial_Order\figure_4.jpg
  Figure 4 caption: "An example of a permutation with \u03B2 l 1 , \u03B2 l 2 , \u03B2\
    \ r 1 and \u03B2 r 2 indicated."
  Figure 5 Link: articels_figures_by_rev_year\2018\Estimating_the_Number_of_Correct_Matches_Using_Only_Spatial_Order\figure_5.jpg
  Figure 5 caption: A partial overlap between a pair of images from the ZuBuD dataset.
  Figure 6 Link: articels_figures_by_rev_year\2018\Estimating_the_Number_of_Correct_Matches_Using_Only_Spatial_Order\figure_6.jpg
  Figure 6 caption: "A graph of order inversions, H \u03C3 . The x -axis corresponds\
    \ to the rank of the feature in I 1 , and the y -axis corresponds to the percentage\
    \ of order inversions out of the maximal value, N\u22121 . The blue and red dots\
    \ correspond to the inliers and outliers, respectively (computed by USAC). The\
    \ green and cyan lines correspond to the range of inversions for the incorrect\
    \ matches."
  Figure 7 Link: articels_figures_by_rev_year\2018\Estimating_the_Number_of_Correct_Matches_Using_Only_Spatial_Order\figure_7.jpg
  Figure 7 caption: Scatter plots for synthetic experiment Test2; the top and bottom
    scatter plots are for the Kendall and Spearman distances, respectively.
  Figure 8 Link: articels_figures_by_rev_year\2018\Estimating_the_Number_of_Correct_Matches_Using_Only_Spatial_Order\figure_8.jpg
  Figure 8 caption: 'Scatter plots for the real datasets. From left to right: K ,
    K 2 and K 1 methods. The comparison is against BEEM. The axes correspond to N
    G N .'
  Figure 9 Link: articels_figures_by_rev_year\2018\Estimating_the_Number_of_Correct_Matches_Using_Only_Spatial_Order\figure_9.jpg
  Figure 9 caption: Examples for the estimation of the overlap between pairs of images.
    The columns correspond to pairs of images from the ZuBuD dataset.
  First author gender probability: 0.89
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.82
  Name of the first author: Lior Talker
  Name of the last author: Ilan Shimshoni
  Number of Figures: 12
  Number of Tables: 3
  Number of authors: 3
  Paper title: Estimating the Number of Correct Matches Using Only Spatial Order
  Publication Date: 2018-09-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Mean Normalized Absolute Error (Percentage) in the Estimation
      of N G NG in the Synthetic Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 The Mean Normalized Absolute Error (Percentage), \u03BC \u03BC\
      \ (Short for \u03BC( N G ) \u03BC(NG)), of the Kendall and Spearman Based Methods\
      \ in Comparison to the Ground Truth (Its Mean, N GT G NGGT) for the Left Four\
      \ Datasets (Columns) and to BEEM (Its Mean, | G BEEM | |GBEEM|) for the Right\
      \ Two Datasets (Columns)"
  Table 3 caption:
    table_text: TABLE 3 Real Values for K G KG, K G KG and K G KG, Obtained from the
      Real Datasets (See Section 6.1.2)
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2869560
- Affiliation of the first author: school of data science, fudan university, shanghai,
    china
  Affiliation of the last author: school of data science, fudan university, shanghai,
    china
  Figure 1 Link: articels_figures_by_rev_year\2018\Multivariate_Mixture_Model_for_Myocardial_Segmentation_Combining_MultiSource_Ima\figure_1.jpg
  Figure 1 caption: 'Illustration of multi-modality images and the common space of
    a subject. Three CMR images are acquired from the patient, i.e., the bSSFP, LGE,
    and T2 CMR, which form I=[ I bSSFP , I LGE , I T2 ] . The pink arrows indicate
    the same position at the common space and the three images which have different
    appearances and intensity values: The bSSFP CMR provides good boundary information
    between the myocardium and blood pools, but one could not see the scars from the
    bSSFP CMR; LGE CMR visualizes the scars as brighter texture, in contrast to the
    dark healthy myocardium, but the boundaries between the scars and the blood pools
    are indistinct; T2 CMR presents the myocardial edema in higher intensity values
    and can visualize the trabeculations and pupillary muscles in great detail, but
    the resolution in z-direction is low (generally 3 to 6 slices). Therefore, the
    author proposes to combine the complementary information from the three types
    of images within a unified framework for simultaneous registration and segmentation.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Multivariate_Mixture_Model_for_Myocardial_Segmentation_Combining_MultiSource_Ima\figure_2.jpg
  Figure 2 caption: Flowchart of the proposed myocardial segmentation method from
    the MS CMR.
  Figure 3 Link: articels_figures_by_rev_year\2018\Multivariate_Mixture_Model_for_Myocardial_Segmentation_Combining_MultiSource_Ima\figure_3.jpg
  Figure 3 caption: The graphical representation of the multivariate mixture model
    in three formulations. Readers are referred to the text for details.
  Figure 4 Link: articels_figures_by_rev_year\2018\Multivariate_Mixture_Model_for_Myocardial_Segmentation_Combining_MultiSource_Ima\figure_4.jpg
  Figure 4 caption: The three orthogonal views of the atlas intensity image (a), and
    the short-axis views of the four probabilistic atlases of myocardium (b), left
    ventricle blood pool (c), right ventricular blood pool (d) and background (e).
    The probability maps are superimposed onto the intensity image, and the color
    bars indicate the mapping between the probability values and displayed colors.
  Figure 5 Link: articels_figures_by_rev_year\2018\Multivariate_Mixture_Model_for_Myocardial_Segmentation_Combining_MultiSource_Ima\figure_5.jpg
  Figure 5 caption: Segmentation results of the three CMR sequences, (a) bSSFP, (b)
    T2-weight, (c) LGE; myocardial boundaries are highlighted in yellow color; the
    motion shifts are pointed out by the red arrows. The images in the green boxes
    of (a)-(c) are the shift corrected images.
  Figure 6 Link: articels_figures_by_rev_year\2018\Multivariate_Mixture_Model_for_Myocardial_Segmentation_Combining_MultiSource_Ima\figure_6.jpg
  Figure 6 caption: Illustration of multi-modality images with different coverage,
    where the log-likelihood LLmathtt HC!!=!!sum v=16 !!LLOmega v .
  Figure 7 Link: articels_figures_by_rev_year\2018\Multivariate_Mixture_Model_for_Myocardial_Segmentation_Combining_MultiSource_Ima\figure_7.jpg
  Figure 7 caption: The LGE CMR segmentation results using the atlas-based segmentation
    (a), GMM segmentation (b), and the Mvmm ominus without registration correction
    (c). The segmentation result using the proposed MvMM is presented in Fig. 5.
  Figure 8 Link: articels_figures_by_rev_year\2018\Multivariate_Mixture_Model_for_Myocardial_Segmentation_Combining_MultiSource_Ima\figure_8.jpg
  Figure 8 caption: The Dice scores of the white matter and gray matter segmentation
    combining multi-sequence brain MR. The UvMM-JA and UvMM-KVL are two well-developed
    algorithms for brain MR segmentation, respectively from [20] and [7].
  Figure 9 Link: articels_figures_by_rev_year\2018\Multivariate_Mixture_Model_for_Myocardial_Segmentation_Combining_MultiSource_Ima\figure_9.jpg
  Figure 9 caption: 'Illustration of the segmentation results from U-Net: (a) is a
    typical good result, where the myocardium has clear boundaries; (b) is a typical
    result where the scars induce misclassification and a resulting unrealistic myocardium
    shape; (c) is a typical result where the healthy myocardium is accurately delineated,
    but the scars and blood pools confuse the deep learning method; (d) shows a result
    visually assessed as a failure, where other organs are misclassified as ventricular
    labels.'
  First author gender probability: 0.8
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.8
  Name of the first author: Xiahai Zhuang
  Name of the last author: Xiahai Zhuang
  Number of Figures: 9
  Number of Tables: 4
  Number of authors: 1
  Paper title: Multivariate Mixture Model for Myocardial Segmentation Combining Multi-Source
    Images
  Publication Date: 2018-09-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Information of the CMR Sequences
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Dice, ACD and Hausdorff Distance of the Proposed MvMM
      from the Three CMR Sequences in Details; the Segmentation of T2 by GMM Based
      on the Results of MvMM Are Provided for Comparisons in T2(+GMM)
  Table 3 caption:
    table_text: TABLE 3 The Dice, ACD and Hausdorff Distance of the Inter-Observer
      (IOb) and Inter-Sequence (ISq) Variations
  Table 4 caption:
    table_text: TABLE 4 This Tabzle Provides the Dice Scores of the LGE CMR Segmentation
      by the Four Separate Segmentation Methods, Four Combined Segmentation Schemes,
      and the MvMM with Other Spatial Regularization (Spatial Regu) Approaches
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2869576
- Affiliation of the first author: graduate school of science and technology, nara
    institute of science and technology, ikoma, japan
  Affiliation of the last author: institute of scientific and industrial research,
    osaka university, suita, japan
  Figure 1 Link: articels_figures_by_rev_year\2018\Material_Classification_from_TimeofFlight_Distortions\figure_1.jpg
  Figure 1 caption: Depth distortion of a ToF camera. (a) A mayonnaise bottle is measured
    by a Microsoft Kinect. (b) Measured depth in a 3D view. There is a depth gap at
    the glass bottle and label boundary. We use this depth distortion for material
    classification. (c) Pixel-wise material classification. (d) Application of material
    classification to depth correction. Depths are corrected based on the segmentation
    result and the distortion database. Depth gaps among materials are corrected and
    a faithful 3D shape is recovered.
  Figure 10 Link: articels_figures_by_rev_year\2018\Material_Classification_from_TimeofFlight_Distortions\figure_10.jpg
  Figure 10 caption: Feature vector variation over surface orientation. We change
    the orientation of the target object, and plot the distance of features along
    with the orientation. The feature is stable under around 70 degrees, and shows
    large deviation at steep-slant orientation. Red line indicates the upper-bound
    distance for the correct classification.
  Figure 2 Link: articels_figures_by_rev_year\2018\Material_Classification_from_TimeofFlight_Distortions\figure_2.jpg
  Figure 2 caption: ToF cameras measure the attenuated amplitude and phase delay of
    the amplitude modulated light, and then calculate the depth of the object from
    the phase delay and the speed of light.
  Figure 3 Link: articels_figures_by_rev_year\2018\Material_Classification_from_TimeofFlight_Distortions\figure_3.jpg
  Figure 3 caption: Phasor representation of ToF observations. (a) Sinusoidal illumination,
    (b) Time domain PSF is expanded to the imaginary plane (orange). (c) When the
    object is placed at different depths, the observation gets rotated but phase distortion
    remains the same as (b). (d) Biased periodic illumination. This toy example adds
    20 percent harmonics to the sinusoid for biasing. (e) The unit ball of the phasor
    representation is distorted due to the biased illumination. (f) The object is
    placed at the same depth as (c). The distortion of the phase becomes different
    than (e) and (c).
  Figure 4 Link: articels_figures_by_rev_year\2018\Material_Classification_from_TimeofFlight_Distortions\figure_4.jpg
  Figure 4 caption: Measurement setup. The target object is measured by changing the
    modulation frequency and its distance to the camera.
  Figure 5 Link: articels_figures_by_rev_year\2018\Material_Classification_from_TimeofFlight_Distortions\figure_5.jpg
  Figure 5 caption: Experimental setup. We use Kinect as a ToF camera, and the target
    object is placed on a linear translation stage.
  Figure 6 Link: articels_figures_by_rev_year\2018\Material_Classification_from_TimeofFlight_Distortions\figure_6.jpg
  Figure 6 caption: All materials of our database. All images are captured by the
    same camera parameters, e.g., ISO, f-number, shutter speed, and focal length.
    The center part of each material is measured.
  Figure 7 Link: articels_figures_by_rev_year\2018\Material_Classification_from_TimeofFlight_Distortions\figure_7.jpg
  Figure 7 caption: Measured depth distortions using Kinect for three objects. The
    ground truth depth is obtained via a linear translation stage. The top row shows
    photographs of the target objects. Measurements shown in the second and third
    rows are different in terms of the surface orientation (5 and 25 degrees). Depth
    distortion of each frequency varies along with the actual depth and material.
    Depth distortion is similar for the same material regardless of the surface orientation,
    but largely distinct in different materials. This frequency- and depth-dependent
    depth distortion is our key observation for material classification.
  Figure 8 Link: articels_figures_by_rev_year\2018\Material_Classification_from_TimeofFlight_Distortions\figure_8.jpg
  Figure 8 caption: Statistics of measured depth distortions. Mean and standard deviation
    within each class are visualized as a box plot. Plus marks and blue points are
    outliers and the shortest distance to other class mean, respectively.
  Figure 9 Link: articels_figures_by_rev_year\2018\Material_Classification_from_TimeofFlight_Distortions\figure_9.jpg
  Figure 9 caption: 'Distance matrix and t-SNE visualization of measured depth distortion
    features. Left: The matrix represents the distance among classes, where the value
    increases from black to white. Materials with similar optical properties, e.g.,
    optically thin, moderately translucent, and diffusive objects, show their similarity
    as seen in the dark block diagonal components. Right: t-SNE visualization. It
    shows well-defined groups, which indicates the discriminative power of the features.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Kenichiro Tanaka
  Name of the last author: Yasushi Yagi
  Number of Figures: 17
  Number of Tables: 2
  Number of authors: 6
  Paper title: Material Classification from Time-of-Flight Distortions
  Publication Date: 2018-09-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison with Su et al. [17]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison Among Different Settings
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2869885
