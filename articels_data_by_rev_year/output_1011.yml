- Affiliation of the first author: center for future media, university of electronic
    science and technology of china, chengdu, sichuan, china
  Affiliation of the last author: center for future media, university of electronic
    science and technology of china, chengdu, sichuan, china
  Figure 1 Link: articels_figures_by_rev_year\2018\Unsupervised_Deep_Hashing_with_SimilarityAdaptive_and_Discrete_Optimization\figure_1.jpg
  Figure 1 caption: An overview of the proposed Similarity-Adaptive Deep Hashing.
    The deep hash model (in red dash box) is trained to fit the learned binary codes.
    Part of the trained model is taken to generate feature representations for the
    images, which help update the similarity graph among data. With the updated similarity
    graph, the binary codes are optimized by ADMM over the intersection (red dots)
    of two continuous areas (in purple). Best viewed in color.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Unsupervised_Deep_Hashing_with_SimilarityAdaptive_and_Discrete_Optimization\figure_2.jpg
  Figure 2 caption: Convergence of our binary code optimization algorithm on CIFAR-10
    and MNIST with 32 bits.
  Figure 3 Link: articels_figures_by_rev_year\2018\Unsupervised_Deep_Hashing_with_SimilarityAdaptive_and_Discrete_Optimization\figure_3.jpg
  Figure 3 caption: Traditional two-step method versus the proposed method with similarity
    graph updating.
  Figure 4 Link: articels_figures_by_rev_year\2018\Unsupervised_Deep_Hashing_with_SimilarityAdaptive_and_Discrete_Optimization\figure_4.jpg
  Figure 4 caption: Comparison of the shallow hashing models with the graph hashing
    loss (SH, AGH, DGH and SADH-L) on CIFAR-10 with GIST features. Results with the
    compared methods with 64 bits in precision-recall curve and precision curve with
    top 20,000 returned samples are reported.
  Figure 5 Link: articels_figures_by_rev_year\2018\Unsupervised_Deep_Hashing_with_SimilarityAdaptive_and_Discrete_Optimization\figure_5.jpg
  Figure 5 caption: Parameter sensitivity analysis of the proposed SADH.
  Figure 6 Link: articels_figures_by_rev_year\2018\Unsupervised_Deep_Hashing_with_SimilarityAdaptive_and_Discrete_Optimization\figure_6.jpg
  Figure 6 caption: Precision-recall curves (top) and Precision curves (bottom) with
    top 20,000 retrieved images for different unsupervised hashing methods on CIFAR-10
    dataset with 16, 32 and 64 bits, respectively.
  Figure 7 Link: articels_figures_by_rev_year\2018\Unsupervised_Deep_Hashing_with_SimilarityAdaptive_and_Discrete_Optimization\figure_7.jpg
  Figure 7 caption: Precision-recall curves (top) and Precision curves (bottom) with
    top 20,000 retrieved images for different unsupervised hashing methods on NUS-WIDE
    dataset with 16, 32 and 64 bits, respectively.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Fumin Shen
  Name of the last author: Heng Tao Shen
  Number of Figures: 7
  Number of Tables: 8
  Number of authors: 6
  Paper title: Unsupervised Deep Hashing with Similarity-Adaptive and Discrete Optimization
  Publication Date: 2018-01-05 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Time Cost of Our Method for the Three Training Steps and Encoding
      a Query on CIFAR-10 Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Our Discrete Algorithm versus Relaxation Method
  Table 3 caption:
    table_text: TABLE 3 Evaluation of Our Linear Model withwithout the Constraint
      of Balance or Uncorrelation
  Table 4 caption:
    table_text: TABLE 4 Comparison in Terms of MAP (%) and Precision5,000 (%) on CIFAR-10
      with 16, 32 and 64 Bits
  Table 5 caption:
    table_text: TABLE 5 Results by Hamming Ranking in Terms of MAP (%) and Precision5,000
      (%) on MNIST
  Table 6 caption:
    table_text: TABLE 6 Results by Hamming Ranking in Terms of MAP (%) and Precision5,000
      (%) on NUS-WIDE
  Table 7 caption:
    table_text: TABLE 7 Evaluation Using Top 2 Percent Euclidean Nearest Neighbors
      as Ground Truth on CIFAR-10
  Table 8 caption:
    table_text: TABLE 8 Evaluation Using top 2 Percent Euclidean Nearest Neighbors
      as Ground Truth on GIST1M
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2789887
- Affiliation of the first author: department of computer science, university of california,
    los angeles, ca
  Affiliation of the last author: department of computer science, university of california,
    los angeles, ca
  Figure 1 Link: articels_figures_by_rev_year\2018\Information_Dropout_Learning_Optimal_Representations_Through_Noisy_Computation\figure_1.jpg
  Figure 1 caption: 'Comparison of the empirical distribution p(z) of the post-noise
    activations with our proposed prior when using: (a) ReLU activations, for which
    we propose a log-uniform prior, and (b) Softplus activations, for which we propose
    a log-normal prior. In both cases, the empirical distribution approximately follows
    the proposed prior. Both histograms where obtained from the last dropout layer
    of the All-CNN-32 network described in Table 2, trained on CIFAR-10.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Information_Dropout_Learning_Optimal_Representations_Through_Noisy_Computation\figure_2.jpg
  Figure 2 caption: "Plot of the total KL-divergence at each spatial location in the\
    \ first three Information Dropout layers (of sizes 48\xD748, 24\xD724 and 12\xD7\
    12 respectively) of All-CNN-96 (see Table 2) trained on Cluttered MNIST with different\
    \ values of \u03B2 . This measures how much information from each part of the\
    \ image the Information Dropout layer is transmitting to the next layer. For small\
    \ \u03B2 information about the nuisances is transmitted to the next layers, while\
    \ for higher values of \u03B2 the dropout layers drop the information as soon\
    \ as the receptive field is big enough to recognize it as a nuisance. The resulting\
    \ representation is thus more robust to nuisances, improving generalization. Notice\
    \ that the noise added by Information Dropout is tailored to the specific sample,\
    \ to the point that the digit can be localized from the noise mask."
  Figure 3 Link: articels_figures_by_rev_year\2018\Information_Dropout_Learning_Optimal_Representations_Through_Noisy_Computation\figure_3.jpg
  Figure 3 caption: (a) Average classification error on MNIST over 3 runs of several
    dropout methods applied to a fully connected network with three hidden layers
    and ReLU activations. Information Dropout outperforms binary dropout, especially
    on smaller networks, possibly because dropout severely reduces the already limited
    capacity of the network, while Information Dropout can adapt the amount of noise
    to the data and the size of the network. Information dropout also outperforms
    a dropout layer that uses constant log-normal noise with the same variance, confirming
    the benefits of adaptive noise. (b) Classification error on CIFAR-10 for several
    dropout methods applied to the All-CNN-32 network (see Table 2) using Softplus
    activations.
  Figure 4 Link: articels_figures_by_rev_year\2018\Information_Dropout_Learning_Optimal_Representations_Through_Noisy_Computation\figure_4.jpg
  Figure 4 caption: "A few samples from our Occluded CIFAR dataset and the plot of\
    \ the testing error on the main task (classifying the CIFAR image) and on the\
    \ nuisance task (classifying the occluding MNIST digit) as \u03B2 varies. For\
    \ both tasks, we use the same representation of the data trained for the main\
    \ task using Information Dropout. For larger values of \u03B2 the representation\
    \ is increasingly more invariant to nuisances, making the nuisance classification\
    \ task harder, but improving the performance on the main task by preventing overfitting.\
    \ For the nuisance task, we test using the learned noisy representation of the\
    \ data, since we are interested specifically in the effects of the noise. For\
    \ the main task, we show the result both using the noisy representation (N), and\
    \ the deterministic representation (D) obtained by disabling the noise at testing\
    \ time."
  Figure 5 Link: articels_figures_by_rev_year\2018\Information_Dropout_Learning_Optimal_Representations_Through_Noisy_Computation\figure_5.jpg
  Figure 5 caption: "For different values of \u03B2 , plot of the test error and total\
    \ correlation of the final layer of the All-CNN-32 network with Softplus activations\
    \ trained on CIFAR-10 with 25 percent of the filters. Increasing \u03B2 the test\
    \ error decreases (we prevent overfitting) and the representation becomes increasingly\
    \ disentangled. When \u03B2 is too large, it prevents information from passing\
    \ through, jeopardizing sufficiency and causingia drastic increase in error."
  Figure 6 Link: articels_figures_by_rev_year\2018\Information_Dropout_Learning_Optimal_Representations_Through_Noisy_Computation\figure_6.jpg
  Figure 6 caption: Plots of (a) the total information transmitted through the two
    dropout layers of a All-CNN-32 network with Softplus activations trained on CIFAR
    and (b) the average quantity of information transmitted through each unit in the
    two layers. From (a) we see that the total quantity of information transmitted
    does not vary much with the number of filters and that, as expected, the second
    layer transmits less information than the first layer, since prior to it more
    nuisances have been disentangled and discarded. In (b) we see that when we decrease
    the number of filters, we force each single unit to let more information flow
    (i.e., we apply less noise), and that the units in the top dropout layer contain
    on average more information relevant to the task than the units in the bottom
    dropout layer.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Alessandro Achille
  Name of the last author: Stefano Soatto
  Number of Figures: 6
  Number of Tables: 2
  Number of authors: 2
  Paper title: 'Information Dropout: Learning Optimal Representations Through Noisy
    Computation'
  Publication Date: 2018-01-10 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Average Variational Lower-Bound L on the Testing Dataset\
      \ for a Simple VAE, Where the Size of the Latent Variable z Is 256\u22C5k and\
      \ the EncoderDecoder Each Contain 512\u22C5k Hidden Units"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Structure of the Networks Used in the Experiments
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2784440
- Affiliation of the first author: department of electrical engineering, grove school
    of engineering, cuny city college, new york, ny
  Affiliation of the last author: department of computer science, suny binghamton
    university, binghamton, ny
  Figure 1 Link: articels_figures_by_rev_year\2018\EACNet_Deep_Nets_with_Enhancing_and_Cropping_for_Facial_Action_Unit_Detection\figure_1.jpg
  Figure 1 caption: Action unit images for AU 4, 6, 7, 17.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\EACNet_Deep_Nets_with_Enhancing_and_Cropping_for_Facial_Action_Unit_Detection\figure_2.jpg
  Figure 2 caption: The structures of FVGG, E-Net and EAC-Net for AU detection.
  Figure 3 Link: articels_figures_by_rev_year\2018\EACNet_Deep_Nets_with_Enhancing_and_Cropping_for_Facial_Action_Unit_Detection\figure_3.jpg
  Figure 3 caption: Skipping layer connection in Residual Net (left) and E-Net (right).
  Figure 4 Link: articels_figures_by_rev_year\2018\EACNet_Deep_Nets_with_Enhancing_and_Cropping_for_Facial_Action_Unit_Detection\figure_4.jpg
  Figure 4 caption: 'Attention map generation. Left: Landmarks(blue) and AU centers
    (green) on a face; Right: Attention map of the face.'
  Figure 5 Link: articels_figures_by_rev_year\2018\EACNet_Deep_Nets_with_Enhancing_and_Cropping_for_Facial_Action_Unit_Detection\figure_5.jpg
  Figure 5 caption: Visualization of selected feature maps from Group 4, with FVGG
    only (left) and with E-Net (right).
  Figure 6 Link: articels_figures_by_rev_year\2018\EACNet_Deep_Nets_with_Enhancing_and_Cropping_for_Facial_Action_Unit_Detection\figure_6.jpg
  Figure 6 caption: Occluded faces for AU detection.
  Figure 7 Link: articels_figures_by_rev_year\2018\EACNet_Deep_Nets_with_Enhancing_and_Cropping_for_Facial_Action_Unit_Detection\figure_7.jpg
  Figure 7 caption: Nine head poses of a face in the BP4D AU dataset with large head
    poses from FERA 2017 data [34].
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.96
  Name of the first author: Wei Li
  Name of the last author: Lijun Yin
  Number of Figures: 7
  Number of Tables: 15
  Number of authors: 4
  Paper title: 'EAC-Net: Deep Nets with Enhancing and Cropping for Facial Action Unit
    Detection'
  Publication Date: 2018-01-10 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Rules for Defining AU Centers
  Table 10 caption:
    table_text: TABLE 10 F1 Score Comparison of AU Detection on Partially Occluded
      Faces of BP4D Dataset
  Table 2 caption:
    table_text: TABLE 2 BP4D Samples Balancing for AU Occurrences in Training
  Table 3 caption:
    table_text: TABLE 3 F1 Score Comparison of AU Detection on BP4D Dataset
  Table 4 caption:
    table_text: TABLE 4 Accuracy Comparison of AU Detection on BP4D Dataset
  Table 5 caption:
    table_text: TABLE 5 F1 Score Comparison of AU Detection on BP4D Dataset with FERA
      2015 Approaches
  Table 6 caption:
    table_text: TABLE 6 F1 Score Comparison of AU Detection on DISFA Dataset
  Table 7 caption:
    table_text: TABLE 7 Accuracy Comparison of AU Detection on DISFA Dataset
  Table 8 caption:
    table_text: TABLE 8 AU Occurrence Rates in DISFA Dataset
  Table 9 caption:
    table_text: TABLE 9 MSE and PCC Evaluation of AU Intensity Estimation
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2791608
- Affiliation of the first author: institute for robotics and intelligent systems,
    university of southern california, los angeles, ca
  Affiliation of the last author: institute for robotics and intelligent systems,
    university of southern california, los angeles, ca
  Figure 1 Link: articels_figures_by_rev_year\2018\Learning_PoseAware_Models_for_PoseInvariant_Face_Recognition_in_the_Wild\figure_1.jpg
  Figure 1 caption: Distributions of head yaw angles in LFW [6] and CASIA WebFaces
    [7] (the latter set used to train our system), compared to the new IJB-A benchmark
    on which our system is tested [8]. These demonstrate the far wider range of poses
    and the increased numbers of near-profile views in IJB-A.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Learning_PoseAware_Models_for_PoseInvariant_Face_Recognition_in_the_Wild\figure_2.jpg
  Figure 2 caption: Faces selected from the range of available yaw angles in the LFW
    and IJB-A benchmarks. Faces displayed in increasing yaw angles from left to right.
    Each face shows its frontalized view, and in the case of IJB-A, also a near-profile
    aligned view. (a) In LFW images, frontalization typically suffices as a means
    of compensating for pose variations. (b) Frontalization of IJB-A faces in extreme
    poses introduces serious artifacts, if performed following [9], [12]. These are
    addressed by aligning the images to near-profile views instead.
  Figure 3 Link: articels_figures_by_rev_year\2018\Learning_PoseAware_Models_for_PoseInvariant_Face_Recognition_in_the_Wild\figure_3.jpg
  Figure 3 caption: Given two sets of face images and videos to match, pose classification
    is used to select a corresponding Pose-Aware CNN Model for processing. Given multiple
    images, each model extracts features and matches them at the set level, independently.
    Finally, the contribution of each model is pooled into a single, final score.
    Note how our approach uses 3D rendering to adjust the pose to frontal (0 degree),
    half-profile (40 degree) and full-profile views (75 degree).
  Figure 4 Link: articels_figures_by_rev_year\2018\Learning_PoseAware_Models_for_PoseInvariant_Face_Recognition_in_the_Wild\figure_4.jpg
  Figure 4 caption: '(a) The directed graph used to map each mode of the CASIA yaw
    distribution to a desired target mode. (b) The process helps to properly render
    a face image: if the face is frontal we render it to both frontal and half-profile
    views; If it is far from frontal, we avoid frontalizing it and render only to
    profile views.'
  Figure 5 Link: articels_figures_by_rev_year\2018\Learning_PoseAware_Models_for_PoseInvariant_Face_Recognition_in_the_Wild\figure_5.jpg
  Figure 5 caption: Steep increase in validation accuracy as a function of the iterations
    in the fine-tuning process. Each curve represents a step in the co-training process.
    Iterations are shown in log scale.
  Figure 6 Link: articels_figures_by_rev_year\2018\Learning_PoseAware_Models_for_PoseInvariant_Face_Recognition_in_the_Wild\figure_6.jpg
  Figure 6 caption: Alignment is guided by the pose. (Left) Images are in-plane aligned
    separately for frontal or profile views. (Right) The approach renders faces only
    to near poses. We render an image to frontal and half-profile if the face is frontal;
    otherwise to profile and half-profile if the face is near-profile.
  Figure 7 Link: articels_figures_by_rev_year\2018\Learning_PoseAware_Models_for_PoseInvariant_Face_Recognition_in_the_Wild\figure_7.jpg
  Figure 7 caption: ROC (a) and CMC (b) improvements comparing a single CNN and the
    proposed PAMs on the IJB-A challenge. Horizontal axes are shown in log scale.
    Each curve also shows the standard deviation.
  Figure 8 Link: articels_figures_by_rev_year\2018\Learning_PoseAware_Models_for_PoseInvariant_Face_Recognition_in_the_Wild\figure_8.jpg
  Figure 8 caption: 'Image-to-Image TAR across poses: PAMs (b) show better pose invariance
    than the baseline (a). The relative difference between PAMs and the baseline is
    visible in (c).'
  Figure 9 Link: articels_figures_by_rev_year\2018\Learning_PoseAware_Models_for_PoseInvariant_Face_Recognition_in_the_Wild\figure_9.jpg
  Figure 9 caption: 'IJB-A performance by varying (a) input resolution, (b) noise
    added to landmark points. Two main metrics are reported: mathrm TARFAR=1% and
    recognition rate at Rank-1.'
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Iacopo Masi
  Name of the last author: Ram Nevatia
  Number of Figures: 9
  Number of Tables: 6
  Number of authors: 15
  Paper title: Learning Pose-Aware Models for Pose-Invariant Face Recognition in the
    Wild
  Publication Date: 2018-01-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Landmark Influence on Face Recognition in IJB-A
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 IJB-A Recognition Results with Varying Deep Feature Combinations
  Table 3 caption:
    table_text: TABLE 3 IJB-A Recognition Results with Varying Representations
  Table 4 caption:
    table_text: TABLE 4 IJB-A Recognition Results with Varying PAM Components
  Table 5 caption:
    table_text: TABLE 5 Improvement for Each Component on the IJB-A Dataset
  Table 6 caption:
    table_text: TABLE 6 Comparative Performance Analysis on IJB-A Benchmark for Verification
      (ROC) and Identification (CMC)
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2792452
- Affiliation of the first author: department of electrical and computer engineering,
    national university of singapore, singapore, singapore
  Affiliation of the last author: department of electrical and computer engineering,
    national university of singapore, singapore, singapore
  Figure 1 Link: articels_figures_by_rev_year\2018\Subspace_Clustering_by_Block_Diagonal_Representation\figure_1.jpg
  Figure 1 caption: 'Illustrations of three interesting structures of matrix: sparse,
    low-rank and block diagonal matrices. The first two are extensively studied before.
    This work focuses on the pursuit of block diagonal matrix.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Subspace_Clustering_by_Block_Diagonal_Representation\figure_2.jpg
  Figure 2 caption: "Plots of the shape interaction matrix V V \u22A4 , Z and B from\
    \ BDR and their binarized versions respectively for Example 1."
  Figure 3 Link: articels_figures_by_rev_year\2018\Subspace_Clustering_by_Block_Diagonal_Representation\figure_3.jpg
  Figure 3 caption: 'Percentage of sequences for which the clustering error is less
    than or equal to a given percentage of misclassification. Left: 2F -dimensional
    data. Right: 4n -dimensional data.'
  Figure 4 Link: articels_figures_by_rev_year\2018\Subspace_Clustering_by_Block_Diagonal_Representation\figure_4.jpg
  Figure 4 caption: Average computational time (sec.) of the algorithms on the Extended
    Yale B database as a function of the number of subjects.
  Figure 5 Link: articels_figures_by_rev_year\2018\Subspace_Clustering_by_Block_Diagonal_Representation\figure_5.jpg
  Figure 5 caption: "Clustering error (%) of BDR-Z as a function of \u03BB when fixing\
    \ \u03B3=1 (left) and \u03B3 when fixing \u03BB=50 (right) for the 10 subjects\
    \ problems from the Extended Yale B database."
  Figure 6 Link: articels_figures_by_rev_year\2018\Subspace_Clustering_by_Block_Diagonal_Representation\figure_6.jpg
  Figure 6 caption: Results on the MNIST database. (a) Plots of clustering errors
    versus the number of subjects; (b) Plots of average computational time (sec.)
    versus the number of subjects; (c) An example of the affinity matrix mathbfB obtained
    by our BDR model.
  Figure 7 Link: articels_figures_by_rev_year\2018\Subspace_Clustering_by_Block_Diagonal_Representation\figure_7.jpg
  Figure 7 caption: Plots of clustering errors versus the parameter k in model (13)
    on the subsets with 2, 4, 6 and 8 subjects from the MNISTdatabase.
  Figure 8 Link: articels_figures_by_rev_year\2018\Subspace_Clustering_by_Block_Diagonal_Representation\figure_8.jpg
  Figure 8 caption: Plots of the objective function value of (14) versus iterations
    on a 5 subjects subset.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Canyi Lu
  Name of the last author: Shuicheng Yan
  Number of Figures: 8
  Number of Tables: 5
  Number of authors: 5
  Paper title: Subspace Clustering by Block Diagonal Representation
  Publication Date: 2018-01-16 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 A Summary of Existing Spectral-Type Subspace Clustering Methods\
      \ Based on Different Choices of f and \u03A9"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Clustering Errors (%) of Different Algorithms on the Hopkins
      155 Database with the 2F -Dimensional Data Points
  Table 3 caption:
    table_text: TABLE 3 Clustering Errors (%) of Different Algorithms on the Hopkins
      155 Database with the 4k -Dimensional Data Points by Applying PCA
  Table 4 caption:
    table_text: TABLE 4 The Mean Clustering Errors ( % ) of 155 Sequences on Hopkins
      155 Dataset by State-of-the-Art Methods
  Table 5 caption:
    table_text: TABLE 5 Clustering Error ( % ) of Different Algorithms on the Extended
      Yale B Database
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2794348
- Affiliation of the first author: department of radiology, university of north carolina
    at chapel hill, chapel hill, nc
  Affiliation of the last author: department of radiology, university of north carolina
    at chapel hill, chapel hill, nc
  Figure 1 Link: articels_figures_by_rev_year\2018\SemiSupervised_Discriminative_Classification_Robust_to_SampleOutliers_and_Featur\figure_1.jpg
  Figure 1 caption: Overview of the proposed semi-supervised learning framework, robust
    to both sample-outliers and feature-noises.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\SemiSupervised_Discriminative_Classification_Robust_to_SampleOutliers_and_Featur\figure_2.jpg
  Figure 2 caption: Top selected regions for each experiment. Selected regions are
    shown with different colors for clarity.
  Figure 3 Link: articels_figures_by_rev_year\2018\SemiSupervised_Discriminative_Classification_Robust_to_SampleOutliers_and_Featur\figure_3.jpg
  Figure 3 caption: 't-SNE projection of AD versus NC samples (better viewed in color).
    Top: Samples detected as outliers by our method. Bottom: Samples detected as outliers
    using RANSAC [50].'
  Figure 4 Link: articels_figures_by_rev_year\2018\SemiSupervised_Discriminative_Classification_Robust_to_SampleOutliers_and_Featur\figure_4.jpg
  Figure 4 caption: "Area under the ROC curve (AUC) as a function of the RFS-LDA hyperparameter\
    \ \u039B 1 , related to \u03BB 1 in (3)."
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ehsan Adeli
  Name of the last author: Dinggang Shen
  Number of Figures: 4
  Number of Tables: 2
  Number of authors: 7
  Paper title: Semi-Supervised Discriminative Classification Robust to Sample-Outliers
    and Feature-Noises
  Publication Date: 2018-01-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Diagnosis Accuracy of the Proposed Method (RFS-LDA) and the
      Baseline Methods on Both PPMI and ADNI Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparisons of the Proposed Method with State-of-the-Art Methods
      for Diagnosis of AD and MCI
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2794470
- Affiliation of the first author: university of electronic science and technology
    of china, chengdu, cn
  Affiliation of the last author: ibm research, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2018\Dynamic_Structure_Embedded_Online_MultipleOutput_Regression_for_Streaming_Data\figure_1.jpg
  Figure 1 caption: "Convergence study of the learned P t on the synthetic dataset\
    \ and the stock dataset, where t represents the total number of samples collected\
    \ currently. Note that P \u2217 real = P real , when t is less than or equal to\
    \ 500, and P \u2217 real = P \u2032 real , when t is bigger than 500."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Dynamic_Structure_Embedded_Online_MultipleOutput_Regression_for_Streaming_Data\figure_2.jpg
  Figure 2 caption: MAEs of different approaches as a function of the number of samples
    t on different companies.
  Figure 3 Link: articels_figures_by_rev_year\2018\Dynamic_Structure_Embedded_Online_MultipleOutput_Regression_for_Streaming_Data\figure_3.jpg
  Figure 3 caption: "Effectiveness verification of the components and \u03BC in MORES."
  Figure 4 Link: articels_figures_by_rev_year\2018\Dynamic_Structure_Embedded_Online_MultipleOutput_Regression_for_Streaming_Data\figure_4.jpg
  Figure 4 caption: The study of parameter sensitivity on the weather dataset.
  Figure 5 Link: articels_figures_by_rev_year\2018\Dynamic_Structure_Embedded_Online_MultipleOutput_Regression_for_Streaming_Data\figure_5.jpg
  Figure 5 caption: Comparisons of update speeds between MORES and ELLA.
  Figure 6 Link: articels_figures_by_rev_year\2018\Dynamic_Structure_Embedded_Online_MultipleOutput_Regression_for_Streaming_Data\figure_6.jpg
  Figure 6 caption: Comparisons of MORES and MORES Appro.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.63
  Name of the first author: Changsheng Li
  Name of the last author: Xin Zhang
  Number of Figures: 6
  Number of Tables: 6
  Number of authors: 6
  Paper title: Dynamic Structure Embedded Online Multiple-Output Regression for Streaming
    Data
  Publication Date: 2018-01-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Learned Correlation Coefficient Matrices at Different
      t
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 MAEs of Different Methods on the Stock Dataset
  Table 3 caption:
    table_text: TABLE 3 MAEs of Different Methods on the Barrett WAM Dataset
  Table 4 caption:
    table_text: TABLE 4 Average MAEs of MORES and OMTL with Different Levels of Noise
  Table 5 caption:
    table_text: TABLE 5 MAEs of Different Methods on the Weather Dataset
  Table 6 caption:
    table_text: TABLE 6 MAEs of Different Methods on the Weather Dataset with Various
      Values N
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2794446
- Affiliation of the first author: school of computer science and software engineering,
    east china normal university, shanghai, china
  Affiliation of the last author: school of computer science and engineering, university
    of new south wales, sydney, nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2018\Metric_Learning_for_MultiOutput_Tasks\figure_1.jpg
  Figure 1 caption: The training time.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Metric_Learning_for_MultiOutput_Tasks\figure_2.jpg
  Figure 2 caption: The testing time.
  Figure 3 Link: articels_figures_by_rev_year\2018\Metric_Learning_for_MultiOutput_Tasks\figure_3.jpg
  Figure 3 caption: Improvement in the testing time of LMMO- kNN using FLANN.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.75
  Name of the first author: Weiwei Liu
  Name of the last author: Wenjie Zhang
  Number of Figures: 3
  Number of Tables: 12
  Number of authors: 4
  Paper title: Metric Learning for Multi-Output Tasks
  Publication Date: 2018-01-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Time Complexity
  Table 10 caption:
    table_text: "TABLE 10 The Results of MAP of Retrieving top 50 Documents on the\
      \ Various Data Sets (Mean \xB1 Standard Deviation)"
  Table 2 caption:
    table_text: "TABLE 2 The Results of Hamming Loss on the Various Data Sets (Mean\
      \ \xB1 Standard Deviation)"
  Table 3 caption:
    table_text: "TABLE 3 The Results of Micro-F1 on the Various Data Sets (Mean \xB1\
      \ Standard Deviation)"
  Table 4 caption:
    table_text: "TABLE 4 The Results of Example-F1 on the Various Data Sets (Mean\
      \ \xB1 Standard Deviation)"
  Table 5 caption:
    table_text: "TABLE 5 Comparison between LMNN and LMMO- kNN in Terms of Micro-F1\
      \ and Example-F1 (Mean \xB1 Standard Deviation)"
  Table 6 caption:
    table_text: "TABLE 6 The Results of Hamming Loss on the Various Data Sets of Different\
      \ LMMO- kNN Variants (Mean \xB1 Standard Deviation)"
  Table 7 caption:
    table_text: "TABLE 7 The Results of Micro-F1 on the Various Data Sets of Different\
      \ LMMO- kNN Variants (Mean \xB1 Standard Deviation)"
  Table 8 caption:
    table_text: "TABLE 8 The Results of Example-F1 on the Various Data Sets of Different\
      \ LMMO- kNN Variants (Mean \xB1 Standard Deviation)"
  Table 9 caption:
    table_text: "TABLE 9 The Results of RMSE on the Various Data Sets (Mean \xB1 Standard\
      \ Deviation)"
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2794976
- Affiliation of the first author: school of electrical engineering, kaist, daejeon,
    korea
  Affiliation of the last author: school of electrical engineering, kaist, daejeon,
    korea
  Figure 1 Link: articels_figures_by_rev_year\2018\Depth_from_a_Light_Field_Image_with_LearningBased_Matching_Costs\figure_1.jpg
  Figure 1 caption: A sample light field image captured by the Lytro camera.
  Figure 10 Link: articels_figures_by_rev_year\2018\Depth_from_a_Light_Field_Image_with_LearningBased_Matching_Costs\figure_10.jpg
  Figure 10 caption: Disparity maps corresponding to four matching costs (vertical
    axis) and four matching groups (horizontal axis). Matching group 4 corresponds
    to the largest angular deviation. Red color boxes highlight the best quality disparity
    map for each matching group. It varies depending on the matching group. Detailed
    discussions are in Section 4.3.3.
  Figure 2 Link: articels_figures_by_rev_year\2018\Depth_from_a_Light_Field_Image_with_LearningBased_Matching_Costs\figure_2.jpg
  Figure 2 caption: Standard deviation versus normalized pixel intensity.
  Figure 3 Link: articels_figures_by_rev_year\2018\Depth_from_a_Light_Field_Image_with_LearningBased_Matching_Costs\figure_3.jpg
  Figure 3 caption: An white plane image captured from the Lytro camera and its vignetting
    estimation.
  Figure 4 Link: articels_figures_by_rev_year\2018\Depth_from_a_Light_Field_Image_with_LearningBased_Matching_Costs\figure_4.jpg
  Figure 4 caption: Epipolar plane image (EPI) of checker board pattern. Because image
    plane and calibration pattern are in parallel, EPI slopes should be consistent
    in entire area. (a) and (b) EPI before and after distortion correction. (a) Shows
    varying EPI slopes and (b) shows consistent EPI slopes for all area. (c) Our compensation
    process for a pixel. (d) Zoom-in image of (a) and (b) that displays slope difference
    between before and after distortion compensation.
  Figure 5 Link: articels_figures_by_rev_year\2018\Depth_from_a_Light_Field_Image_with_LearningBased_Matching_Costs\figure_5.jpg
  Figure 5 caption: An estimated distortion map G from Fig. 4a. Green dots of depth
    (slope of EPI) are overlaid on the distortion map.
  Figure 6 Link: articels_figures_by_rev_year\2018\Depth_from_a_Light_Field_Image_with_LearningBased_Matching_Costs\figure_6.jpg
  Figure 6 caption: Disparity map before and after distortion correction (Section
    3.3). Real-world planar scene is captured and the depth map is computed using
    our approach (Section 4).
  Figure 7 Link: articels_figures_by_rev_year\2018\Depth_from_a_Light_Field_Image_with_LearningBased_Matching_Costs\figure_7.jpg
  Figure 7 caption: An original sub-aperture image is shifted with bilinear, bicubic
    and phase shift theorem.
  Figure 8 Link: articels_figures_by_rev_year\2018\Depth_from_a_Light_Field_Image_with_LearningBased_Matching_Costs\figure_8.jpg
  Figure 8 caption: "Overview of random forest-based depth label prediction. (a) Sub-aperture\
    \ images on the left show four matching groups in orange boxes. The reference\
    \ image is highlighted in yellow box. As described in Section 4.3, we make 31\
    \ depth images per matching group from mixture of various matching costs. For\
    \ each pixel, 124 dimensional feature vector mathbfq is made from the 31 depth\
    \ values \xD7 4 matching groups. The true disparity is highlighted in red. (b)\
    \ Feature mathbfq passes through random forest and the top 20 reactive depth values\
    \ (gets high importance value from the forest) in mathbfq are collected to build\
    \ a compact feature vector hatmathbfq . (c) hatmathbfq passes regression forest\
    \ to predict a final depth label. Compared to conventional matching cost (SAD+GRAD)\
    \ [11], the depth from our pipeline is less prone to the light field image degradation."
  Figure 9 Link: articels_figures_by_rev_year\2018\Depth_from_a_Light_Field_Image_with_LearningBased_Matching_Costs\figure_9.jpg
  Figure 9 caption: Quantitative evaluation according to compact feature dimension
    N used for regression forest.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.88
  Name of the first author: Hae-Gon Jeon
  Name of the last author: In So Kweon
  Number of Figures: 20
  Number of Tables: 1
  Number of authors: 7
  Paper title: Depth from a Light Field Image with Learning-Based Matching Costs
  Publication Date: 2018-01-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison on Light Field Benchmark [15]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2794979
- Affiliation of the first author: university of chinese academy of sciences, beijing,
    china
  Affiliation of the last author: school of engineering, university of california,
    merced, ca
  Figure 1 Link: articels_figures_by_rev_year\2018\Learning_MultiTask_Correlation_Particle_Filters_for_Visual_Tracking\figure_1.jpg
  Figure 1 caption: Comparisons of the proposed MCPF tracking algorithm with the state-of-the-art
    correlation filter trackers (DSST [1], KCF [2], CF2 [3], and HDT [4]) on the lemming,
    car4, and kitesurf sequences [5]. These methods perform differently as various
    features, occlusion handling, and scale adaption strategies are used. The proposed
    MCPF tracker performs favorably against these trackers.
  Figure 10 Link: articels_figures_by_rev_year\2018\Learning_MultiTask_Correlation_Particle_Filters_for_Visual_Tracking\figure_10.jpg
  Figure 10 caption: Precision and success plots over the 128 sequences using OPE
    on the Temple Color dataset. The legend contains the AUC and PS scores for each
    tracker. The proposed MCPF method performs favorably against the state-of-the-art
    trackers.
  Figure 2 Link: articels_figures_by_rev_year\2018\Learning_MultiTask_Correlation_Particle_Filters_for_Visual_Tracking\figure_2.jpg
  Figure 2 caption: A multi-task correlation filter is used to shepherd the sampled
    particles toward the modes of the target state distribution. The numbers in (b)
    are the scores of the correlation filter for the particles. Different colored
    boxes indicate the respective locations and scores.
  Figure 3 Link: articels_figures_by_rev_year\2018\Learning_MultiTask_Correlation_Particle_Filters_for_Visual_Tracking\figure_3.jpg
  Figure 3 caption: "An illustrative example of the learned multi-task correlation\
    \ filter via the APG method. Here, P=2 and K=3 . (a) A target object is represented\
    \ by two parts denoted in red and blue bounding boxes. (b) The objective function\
    \ value versus number of iteration. The problem (3) can be solved efficiently\
    \ via the APG method and converge quickly in less than 25 iterations. (c) The\
    \ learned correlation filters Z\u2208 R 768\xD76 where M=128 , N=96 , and P\xD7\
    K=6 . Notice that the columns of Z are similar and jointly sparse. It is clear\
    \ that the learned correlation filters for all parts among multiple features can\
    \ make similar circular shifts and have the consistent motion to preserve target\
    \ object structure. (d) Three examples of the learned correlation filters including\
    \ z 11 , z 12 , and z 21 , which are quite similar to each other."
  Figure 4 Link: articels_figures_by_rev_year\2018\Learning_MultiTask_Correlation_Particle_Filters_for_Visual_Tracking\figure_4.jpg
  Figure 4 caption: The MCPF can cover object state space well with a few particles.
    Each particle corresponds to an image region enclosed by a bounding box. (a) The
    MCPF can cover object state space well by using a few particles with the search
    region where each particle covers the state subspace corresponding to all shifted
    region of the target object. (b) The MCPF can shepherd the sampled particles toward
    the modes of the target state distribution, which correspond to the target locations
    in the image.
  Figure 5 Link: articels_figures_by_rev_year\2018\Learning_MultiTask_Correlation_Particle_Filters_for_Visual_Tracking\figure_5.jpg
  Figure 5 caption: Precision and success plots over all the 50 sequences using one-pass
    evaluation on the OTB2013 dataset. The legend contains the area-under-the-curve
    score and the average distance precision score at 20 pixels for each tracker.
    The MCPF method performs favorably against the state-of-the-art trackers.
  Figure 6 Link: articels_figures_by_rev_year\2018\Learning_MultiTask_Correlation_Particle_Filters_for_Visual_Tracking\figure_6.jpg
  Figure 6 caption: Tracking performance based on attributes of image sequences on
    the OTB2013 dataset. Success plots on 11 tracking challenges of scale variation,
    out of view, out-of-plane rotation, low resolution, in-plane rotation, illumination,
    motion blur, background clutter, occlusion, deformation, and fast motion. The
    legend contains the AUC scores for each tracker. Our MCPF method performs favorably
    against the state-of-the-art trackers.
  Figure 7 Link: articels_figures_by_rev_year\2018\Learning_MultiTask_Correlation_Particle_Filters_for_Visual_Tracking\figure_7.jpg
  Figure 7 caption: Tracking results of the 10 state-of-the-art trackers (denoted
    in different colors and lines) on 16 challenging sequences from the OTB2013 dataset
    (from left to right and top to down are car4, soccer, deer, skating1, shaking,
    singer1, singer2, couple, jogging-1, walking2, jumping, skiing, lemming, motorRolling,
    basketball, and tiger1).
  Figure 8 Link: articels_figures_by_rev_year\2018\Learning_MultiTask_Correlation_Particle_Filters_for_Visual_Tracking\figure_8.jpg
  Figure 8 caption: Precision and success plots over all the 100 sequences using one-pass
    evaluation on the OTB2015 dataset. The legend contains the area-under-the-curve
    score and the average distance precision score at 20 pixels for each tracker.
    Our MCPF method performs favorably against the state-of-the-art trackers.
  Figure 9 Link: articels_figures_by_rev_year\2018\Learning_MultiTask_Correlation_Particle_Filters_for_Visual_Tracking\figure_9.jpg
  Figure 9 caption: Tracking performance based on attributes of image sequences on
    the OTB2015 dataset. Success plots on 11 tracking challenges of scale variation,
    out of view, out-of-plane rotation, low resolution, in-plane rotation, illumination,
    motion blur, background clutter, occlusion, deformation, and fast motion. The
    legend contains the AUC scores for each tracker. Our MCPF method performs favorably
    against the state-of-the-art trackers.
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Tianzhu Zhang
  Name of the last author: Ming-Hsuan Yang
  Number of Figures: 10
  Number of Tables: 4
  Number of authors: 3
  Paper title: Learning Multi-Task Correlation Particle Filters for Visual Tracking
  Publication Date: 2018-01-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison with the State-of-the-Art Tracking Methods on the
      VOT2015 Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Effect of Particle Numbers on Visual Tracking Performance
  Table 3 caption:
    table_text: TABLE 3 Effect of Particle Scale ( s ) on Visual Tracking in Terms
      of AUC and PS Corresponding to the OPE
  Table 4 caption:
    table_text: TABLE 4 Model Analysis by Comparing MCPF, MCF, CPF, CF2, and CF2S
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2797062
