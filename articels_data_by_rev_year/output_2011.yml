- Affiliation of the first author: university of liverpool, liverpool, u.k.
  Affiliation of the last author: department of computer science, university of liverpool,
    liverpool, u.k
  Figure 1 Link: articels_figures_by_rev_year\2021\Discriminative_Triad_Matching_and_Reconstruction_for_Weakly_Referring_Expression\figure_1.jpg
  Figure 1 caption: Illustration of the difference between (a) the traditional WREG
    method, and (b) the proposed triad-level method. The blue and the yellow parts
    indicate the proposal features and the linguistic feature, while the red represents
    the individual loss function. In (a), although the meaning of the reconstructed
    sentence is the same as the label, the loss is large, as it is calculated word-by-word,
    making the network hard to converge. In (b) however, the reconstruction is conducted
    in the triad-level, and a 300-D MSE loss function replaces the 10,000-D CE loss
    function which dramatically facilitates the weakly-supervised training.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Discriminative_Triad_Matching_and_Reconstruction_for_Weakly_Referring_Expression\figure_2.jpg
  Figure 2 caption: The proposed framework mainly consists of the generation of discriminative
    triads, the triad matching module and the triad reconstruction module (rightmost
    three modules in the bottom row). Other modules are also illustrated, including
    the analysis of the sentence structure (leftmost module in the bottom row) and
    the organization of proposal pairs (modules in the top row). The parts in solid
    red, green and blue boxes represent the features of a proposal pair used to match
    the linguistics feature of the target unit, the discriminative unit and the reference
    unit in a discriminative triad, respectively. Note that the parts in dashed boxes,
    whose specific routes are omitted, work in the same way as the parts in solid
    boxes.
  Figure 3 Link: articels_figures_by_rev_year\2021\Discriminative_Triad_Matching_and_Reconstruction_for_Weakly_Referring_Expression\figure_3.jpg
  Figure 3 caption: Visualization results on MSCOCO datasets, where the red, green,
    blue parts represent the target unit, discriminative unit, reference unit and
    their corresponding proposals, respectively.
  Figure 4 Link: articels_figures_by_rev_year\2021\Discriminative_Triad_Matching_and_Reconstruction_for_Weakly_Referring_Expression\figure_4.jpg
  Figure 4 caption: The qualitative comparison with other WREG methods. The red box
    indicates the bounding box result of the target.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.51
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Mingjie Sun
  Name of the last author: John Y. Goulermas
  Number of Figures: 4
  Number of Tables: 3
  Number of authors: 5
  Paper title: Discriminative Triad Matching and Reconstruction for Weakly Referring
    Expression Grounding
  Publication Date: 2021-02-11 00:00:00
  Table 1 caption: TABLE 1 Common Query Sentences and Their Corresponding Discriminative
    Triads
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Effectiveness Comparison With Other WREG Methods
  Table 3 caption: TABLE 3 Ablation Studies Through Cross-Evaluation
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3058684
- Affiliation of the first author: department of electrical engineering, ecole polytechnique
    federale de lausanne (epfl), lausanne, switzerland
  Affiliation of the last author: department of computing, imperial college london,
    london, u.k
  Figure 1 Link: articels_figures_by_rev_year\2021\Deep_Polynomial_Neural_Networks\figure_1.jpg
  Figure 1 caption: "In this paper we introduce a class of networks called \u03A0\u2212\
    \ nets, where the output is a polynomial of the input. The input in this case,\
    \ z , can be either the latent space of Generative Adversarial Network for a generative\
    \ task or an image in the case of a discriminative task. Our polynomial networks\
    \ can be easily implemented."
  Figure 10 Link: articels_figures_by_rev_year\2021\Deep_Polynomial_Neural_Networks\figure_10.jpg
  Figure 10 caption: Top-1 and Top-5 error curves on the ImageNet dataset.
  Figure 2 Link: articels_figures_by_rev_year\2021\Deep_Polynomial_Neural_Networks\figure_2.jpg
  Figure 2 caption: "Schematic illustration of the CCP (for third order approximation).\
    \ Symbol \u2217 refers to the Hadamard product."
  Figure 3 Link: articels_figures_by_rev_year\2021\Deep_Polynomial_Neural_Networks\figure_3.jpg
  Figure 3 caption: "Schematic illustration of the NCP (for third order approximation).\
    \ Symbol \u2217 refers to the Hadamard product."
  Figure 4 Link: articels_figures_by_rev_year\2021\Deep_Polynomial_Neural_Networks\figure_4.jpg
  Figure 4 caption: Schematic illustration of the NCP-Skip (for third order approximation).
    The difference from Fig. 3 is the skip connections added in this model.
  Figure 5 Link: articels_figures_by_rev_year\2021\Deep_Polynomial_Neural_Networks\figure_5.jpg
  Figure 5 caption: Comparison of the proposed models in fashion image [54] generation
    without activation functions.
  Figure 6 Link: articels_figures_by_rev_year\2021\Deep_Polynomial_Neural_Networks\figure_6.jpg
  Figure 6 caption: Comparison of the proposed models in facial image [55] generation
    without activation functions.
  Figure 7 Link: articels_figures_by_rev_year\2021\Deep_Polynomial_Neural_Networks\figure_7.jpg
  Figure 7 caption: Abstract illustration of the ProdPoly. The input variable boldsymbolz
    on the left is the input to a 2nd order expansion; the output of this is used
    as the input for the next polynomial (also with a 2nd order expansion) and so
    on. If we use N such polynomials, the final output G(boldsymbolz) expresses a
    2N order expansion. In addition to the high order of approximation, the benefit
    of using the product of polynomials is that the model is flexible, in the sense
    that each polynomial can be implemented as a different decomposition of Section
    3.1.
  Figure 8 Link: articels_figures_by_rev_year\2021\Deep_Polynomial_Neural_Networks\figure_8.jpg
  Figure 8 caption: Samples synthesized from ProdPoly (trained on FFHQ).
  Figure 9 Link: articels_figures_by_rev_year\2021\Deep_Polynomial_Neural_Networks\figure_9.jpg
  Figure 9 caption: The test accuracy of (a) ResNet18 and (b) the respective Prodpoly-ResNet
    are plotted (CIFAR10 training). The two models perform similarly throughout the
    training, while ours has 46 percent less parameters. The width of the highlighted
    region denotes the standard deviation of each model.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Grigorios G. Chrysos
  Name of the last author: Stefanos Zafeiriou
  Number of Figures: 14
  Number of Tables: 12
  Number of authors: 6
  Paper title: Deep Polynomial Neural Networks
  Publication Date: 2021-02-11 00:00:00
  Table 1 caption: TABLE 1 Nomenclature
  Table 10 caption: TABLE 10 1:1 Verification TAR on the IJB-B and IJB-C Datasets
  Table 2 caption: TABLE 2 Single Polynomial Models (Section 3.1)
  Table 3 caption: TABLE 3 ISFID Scores on CIFAR10 [56] Generation
  Table 4 caption: TABLE 4 Speech Classification With ResNet
  Table 5 caption: TABLE 5 Image Classification on CIFAR10 With ResNet
  Table 6 caption: TABLE 6 CIFAR100 Classification With ResNet
  Table 7 caption: TABLE 7 ImageNet Classification Results of ResNet50 and the Proposed
    Prodpoly-ResNet50
  Table 8 caption: TABLE 8 Face Datasets for Training and Testing
  Table 9 caption: TABLE 9 Verification Performance ( % %) of ResNet50 and the Proposed
    Prodpoly-ResNet50 on LFW, CFP-FF, CFP-FP, CPLFW, AgeDB-30, CALFW and RFW (Caucasian,
    Indian, Asian, and African)
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3058891
- Affiliation of the first author: "department of mathematics and operational research,\
    \ facult\xE9 polytechnique, universit\xE9 de mons, mons, belgium"
  Affiliation of the last author: department of electrical and computer engineering,
    department of mathematics, national university of singapore, singapore, singapore
  Figure 1 Link: articels_figures_by_rev_year\2021\Distributionally_Robust_and_MultiObjective_Nonnegative_Matrix_Factorization\figure_1.jpg
  Figure 1 caption: "Comparison of Algorithm 3 with its variant where Step 5 is replaced\
    \ by a standard subgradient step (i.e., Step 4 of Algorithm 2). The curves display\
    \ the evolution of the scaled \u03B2 -divergences (referred to as \u201Cerror\u201D\
    \ in the legend)."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Distributionally_Robust_and_MultiObjective_Nonnegative_Matrix_Factorization\figure_2.jpg
  Figure 2 caption: "Pareto-optimal solutions for \u03BB=(\u2113,1\u2212\u2113) for\
    \ \u2113=0,0.1,\u2026,1 and \u03A9=0,1,0,2,1,2 (top to bottom), and solution computed\
    \ by DR-NMF (Algorithm 3)."
  Figure 3 Link: articels_figures_by_rev_year\2021\Distributionally_Robust_and_MultiObjective_Nonnegative_Matrix_Factorization\figure_3.jpg
  Figure 3 caption: "Musical score of \u201CMary had a little lamb\u201D. The notes\
    \ activate as follows: E 4 , D 4 , C 4 , D 4 , E 4 , E 4 , E 4 ."
  Figure 4 Link: articels_figures_by_rev_year\2021\Distributionally_Robust_and_MultiObjective_Nonnegative_Matrix_Factorization\figure_4.jpg
  Figure 4 caption: "Comparative study of NMF with IS- and KL-divergences, and DR-NMF\
    \ with \u03A9=0,1 applied to the amplitude spectrogram of \u201CMary had a little\
    \ lamb\u201D with r=4 . The figure shows the evolution of the scaled \u03B2 -divergences\
    \ fo the different NMF models."
  Figure 5 Link: articels_figures_by_rev_year\2021\Distributionally_Robust_and_MultiObjective_Nonnegative_Matrix_Factorization\figure_5.jpg
  Figure 5 caption: "Comparative study of NMF with IS- and KL-divergences, and DR-NMF\
    \ with Omega = lbrace 0,1rbrace applied to \u201CMary had a little lamb\u201D\
    \ amplitude spectrogram with r=4 and Poisson noise. The figure shows the activations\
    \ (that is, the rows of H ) of the recovered sources over time, and indicates\
    \ which source it corresponds to (the notes C4 , D4 , E4 , and the hammer noise\
    \ HN )."
  Figure 6 Link: articels_figures_by_rev_year\2021\Distributionally_Robust_and_MultiObjective_Nonnegative_Matrix_Factorization\figure_6.jpg
  Figure 6 caption: "Comparative study of NMF with IS- and KL-divergences, and DR-NMF\
    \ with Omega = lbrace 0,1rbrace applied to \u201CMary had a little lamb\u201D\
    \ amplitude spectrogram with r =5 and Gamma noise. The figure shows the activations\
    \ (that is, the rows of H ) of the recovered sources over time. Note that the\
    \ hammer noise is also extracted with C4 and D4 , although with a small intensity."
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Nicolas Gillis
  Name of the last author: Vincent Y. F. Tan
  Number of Figures: 6
  Number of Tables: 2
  Number of authors: 4
  Paper title: Distributionally Robust and Multi-Objective Nonnegative Matrix Factorization
  Publication Date: 2021-02-11 00:00:00
  Table 1 caption: "TABLE 1 Comparison of NMF With KL-Divergence and Frobenius Norm,\
    \ and DR-NMF With \u03A9=1,2 \u03A9=1,2 on Text Mining Data Sets From [43]"
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 Comparison of NMF With the IS- and KL-Divergences, and\
    \ DR-NMF With \u03A9=0,1 \u03A9=0,1 on Audio Data Sets With m=149 m=149 and r=10\
    \ r=10"
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3058693
- Affiliation of the first author: fbk, trento, italy
  Affiliation of the last author: fbk, trento, italy
  Figure 1 Link: articels_figures_by_rev_year\2021\Learning_to_Recognize_Actions_on_Objects_in_Egocentric_Video_With_Attention_Dict\figure_1.jpg
  Figure 1 caption: "Overview of the egocentric action prediction framework EgoACO\
    \ \u2013 Egocentric Action-Context-Object Net."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Learning_to_Recognize_Actions_on_Objects_in_Egocentric_Video_With_Attention_Dict\figure_2.jpg
  Figure 2 caption: "Long Short-Term Attention (LSTA) extends LSTM (black part) with\
    \ two novel components: recurrent attention and fine-grained output gating. The\
    \ first [red part, rca-attn in equation (16)] tracks a weight map to focus on\
    \ relevant feature regions, while the second (green part, equation (22)) introduces\
    \ a high-capacity output gate. At the core of both is a spatial self-attention\
    \ \u03C2(\u22C5,A) that pools parameters from attention dictionary A ."
  Figure 3 Link: articels_figures_by_rev_year\2021\Learning_to_Recognize_Actions_on_Objects_in_Egocentric_Video_With_Attention_Dict\figure_3.jpg
  Figure 3 caption: Multi-head prediction in EgoACO. To account for relationships
    between the tasks, we use the action logits to control the bias of verb and noun
    prediction, through linear maps (dashed arrows).
  Figure 4 Link: articels_figures_by_rev_year\2021\Learning_to_Recognize_Actions_on_Objects_in_Egocentric_Video_With_Attention_Dict\figure_4.jpg
  Figure 4 caption: Attention maps generated by mathbf dobj (first row), mathbf dctx
    (second) and LSTA (third) for three samples from the validation split of EPIC-KITCHENS.
    We show ten frames uniformly sampled from the 20 frames applied as input to the
    network.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Swathikiran Sudhakaran
  Name of the last author: Oswald Lanz
  Number of Figures: 4
  Number of Tables: 6
  Number of authors: 3
  Paper title: Learning to Recognize Actions on Objects in Egocentric Video With Attention
    Dictionaries
  Publication Date: 2021-02-11 00:00:00
  Table 1 caption: TABLE 1 Ablation Analysis on the Validation Split of EPIC-KITCHENS
    Dataset to Study the Effectiveness of Recurrent Attention and Fine-Grained Output
    Gating
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Sensitivity Analysis Done on the Validation Split of EPIC-KITCHENS
    Dataset to Study the Effect of Hyper-Parameter D D, the Size of the Memory Attention
    Dictionary A mem Amem Used in LSTA for Output Pooling, Eq. (21)
  Table 3 caption: TABLE 3 Ablation Analysis on the Validation Split of EPIC-KITCHENS
    Dataset to Study the Effectiveness of Active Object and Scene Context Pooling
  Table 4 caption: TABLE 4 Analysis on the Validation Split of EPIC-KITCHENS Dataset
    to Validate the Effectiveness of Training EgoACO in a Multi-Task Fashion
  Table 5 caption: TABLE 5 Comparison of Recognition Accuracies With State-of-the-Art
    on EPIC-KITCHENS Dataset
  Table 6 caption: TABLE 6 Comparison of Recognition Accuracies With State-of-the-Art
    on EGTEA Gaze+ Dataset
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3058649
- Affiliation of the first author: department of computer science and engineering,
    shanghai jiao tong university, shanghai, china
  Affiliation of the last author: national institute of information and communications
    technology (nict), kyoto, japan
  Figure 1 Link: articels_figures_by_rev_year\2021\Text_CompressionAided_Transformer_Encoding\figure_1.jpg
  Figure 1 caption: An overview of our text compression settings (left) and overall
    architectures of ETC and ITC (right). In the right sub-figures, the solid arrows
    represent differentiable operations and the dashed represent non-differentiable
    operations. ETC Pipeline, ETC Joint, and ITC Joint are three manners explored
    for text compression-aided Transformer encoding in this paper. In ETC Pipeline
    and ETC Joint manners, text is compressed using an independent model, denoted
    as ETC Model. In the ITC Joint manner, text compression is handled in the full
    model in a module we denote ITC Module.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Text_CompressionAided_Transformer_Encoding\figure_2.jpg
  Figure 2 caption: Comparison between autoregressive decoding in ETC Joint and non-autoregressive
    decoding in ITC Joint . The solid line represents differentiable operations, and
    the dashed line represents non-differentiable ones. The gray background represents
    the compressed text features. s is used for tokens in the source side, t for predicted
    tokens in the target side, and BOS for tokens to start the prediction.
  Figure 3 Link: articels_figures_by_rev_year\2021\Text_CompressionAided_Transformer_Encoding\figure_3.jpg
  Figure 3 caption: The architecture of the ITC module.
  Figure 4 Link: articels_figures_by_rev_year\2021\Text_CompressionAided_Transformer_Encoding\figure_4.jpg
  Figure 4 caption: The architecture of our NMT model with ETC Pipeline BEF.
  Figure 5 Link: articels_figures_by_rev_year\2021\Text_CompressionAided_Transformer_Encoding\figure_5.jpg
  Figure 5 caption: The architecture of our NMT model with ETC Pipeline BDF.
  Figure 6 Link: articels_figures_by_rev_year\2021\Text_CompressionAided_Transformer_Encoding\figure_6.jpg
  Figure 6 caption: Performances on EN-DE newstest2014 with different text compression
    ratios.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Zuchao Li
  Name of the last author: Eiichiro Sumita
  Number of Figures: 6
  Number of Tables: 13
  Number of authors: 7
  Paper title: Text Compression-Aided Transformer Encoding
  Publication Date: 2021-02-12 00:00:00
  Table 1 caption: TABLE 1 Examples of Text and Backbone Information
  Table 10 caption: TABLE 10 The Transfer Learning Results of ETC-Aided Models
  Table 2 caption: TABLE 2 ROUGE Performance of the Sentence-Level Text Compression
    Models Trained on the Gigaword Dataset
  Table 3 caption: TABLE 3 ROUGE Performance of the Paragraph-Level Text Compression
    Models on the CNNDM Dataset
  Table 4 caption: TABLE 4 Comparison With Existing NMT Systems on the WMT14 EN-DE
    and EN-FR Translation Tasks
  Table 5 caption: TABLE 5 Exact Match (EM) and F1 Scores on the SQuAD 2.0 Dev and
    Test Sets for Single Models
  Table 6 caption: TABLE 6 Accuracy on the RACE Test Set for Single Models
  Table 7 caption: TABLE 7 The Effect of Levels of Text Compression Quality on Translation
    Baseline
  Table 8 caption: TABLE 8 The Effect of Encoder Parameters
  Table 9 caption: TABLE 9 Comparison of ETC Pipeline, ETC Joint, ITC Joint, and DAE
    Pre-Training Approaches
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3058341
- Affiliation of the first author: computer science department, university of maryland,
    college park, md, usa
  Affiliation of the last author: computer science department, university of maryland,
    college park, md, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\Scale_Normalized_Image_Pyramids_With_AutoFocus_for_Object_Detection\figure_1.jpg
  Figure 1 caption: Fraction of RoIs in the dataset versus scale of RoIs relative
    to the image.
  Figure 10 Link: articels_figures_by_rev_year\2021\Scale_Normalized_Image_Pyramids_With_AutoFocus_for_Object_Detection\figure_10.jpg
  Figure 10 caption: Pruning detections while FocusStacking. (a) Original Image (b)
    The predicted FocusPixels and the generated FocusChip (c) Detection output by
    the network (d) Final detections for the FocusChip after pruning.
  Figure 2 Link: articels_figures_by_rev_year\2021\Scale_Normalized_Image_Pyramids_With_AutoFocus_for_Object_Detection\figure_2.jpg
  Figure 2 caption: Dull Gret by Pieter Bruegel the Elder. In the center one can see
    Dull Gret, a brave lady who is taking an army of women with her (to her right)
    and she is invading hell (the entire picture is about hell). We can see devils
    at different places, for example on the left, top right or top center. Notice
    how difficult it is to focus on all objects in a complex scene at once. It is
    essential to stop and focus on different regions while skimming through the image
    to find all the objects.
  Figure 3 Link: articels_figures_by_rev_year\2021\Scale_Normalized_Image_Pyramids_With_AutoFocus_for_Object_Detection\figure_3.jpg
  Figure 3 caption: The same layer convolutional features at different scales of the
    image are different and map to different semantic regions in the image at different
    scales.
  Figure 4 Link: articels_figures_by_rev_year\2021\Scale_Normalized_Image_Pyramids_With_AutoFocus_for_Object_Detection\figure_4.jpg
  Figure 4 caption: Different approaches for providing input for training the classifier
    of a proposal based detector.
  Figure 5 Link: articels_figures_by_rev_year\2021\Scale_Normalized_Image_Pyramids_With_AutoFocus_for_Object_Detection\figure_5.jpg
  Figure 5 caption: SNIP training and inference is shown. Invalid RoIs which fall
    outside the specified range at each scale are shown in purple. These are discarded
    during training and inference. Each batch during training consists of images sampled
    from a particular scale. Invalid GT boxes are used to invalidate anchors in RPN.
    Detections from each scale are rescaled and combined using NMS.
  Figure 6 Link: articels_figures_by_rev_year\2021\Scale_Normalized_Image_Pyramids_With_AutoFocus_for_Object_Detection\figure_6.jpg
  Figure 6 caption: SNIPER selectively trains the detector only on parts of the input
    pyramid (chips) while respecting the SNIP ranges. The light blue rectangle show
    the selected chips in each pyramid scale. The valid objects in each chip is also
    shown with colored boxes.
  Figure 7 Link: articels_figures_by_rev_year\2021\Scale_Normalized_Image_Pyramids_With_AutoFocus_for_Object_Detection\figure_7.jpg
  Figure 7 caption: 'SNIPER negative chip selection. First row: the image and the
    ground-truth boxes. Bottom row: negative proposals not covered in positive chips
    (represented by red circles located at the center of each proposal for the clarity)
    and the generated negative chips based on the proposals (represented by orange
    rectangles).'
  Figure 8 Link: articels_figures_by_rev_year\2021\Scale_Normalized_Image_Pyramids_With_AutoFocus_for_Object_Detection\figure_8.jpg
  Figure 8 caption: Area of objects of different sizes and the background in the COCO
    validation set. Objects are divided based on their area (in pixels) into small,
    medium, and large.
  Figure 9 Link: articels_figures_by_rev_year\2021\Scale_Normalized_Image_Pyramids_With_AutoFocus_for_Object_Detection\figure_9.jpg
  Figure 9 caption: The figure illustrates how FocusPixels are assigned at multiple
    scales of an image. At scale 1 (b), the smallest two elephants generate FocusPixels,
    the largest one is marked as background and the one on the left is ignored during
    training to avoid penalizing the network for borderline cases (see Section 3.4.1
    for assignment details). The labelling changes at scales 2 and 3 as the objects
    occupy more pixels. For example, only the smallest elephant would generate FocusPixels
    at scale 2 and the largest two elephants would generate negative labels.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Bharat Singh
  Name of the last author: Larry S. Davis
  Number of Figures: 15
  Number of Tables: 11
  Number of authors: 4
  Paper title: Scale Normalized Image Pyramids With AutoFocus for Object Detection
  Publication Date: 2021-02-12 00:00:00
  Table 1 caption: TABLE 1 mAP on Small Objects (smaller than 32x32 pixels) Under
    Different Training Protocols
  Table 10 caption: TABLE 10 Comparison on the COCO Test-Dev
  Table 2 caption: TABLE 2 MS Denotes Multi-Scale
  Table 3 caption: TABLE 3 For Individual Ranges (like 0-25 etc.) Recall at 50 percent
    Overlap is Reported Because Minor Localization Errors can be Fixed in the Second
    Stage
  Table 4 caption: TABLE 4 The Effect of SNIP on RCN and RPN
  Table 5 caption: TABLE 5 We Plot the Recall for SNIPER With and Without Negatives
  Table 6 caption: TABLE 6 The Effect Training on 2 Scales (1.667 and max size of
    512)
  Table 7 caption: TABLE 7 We Observe That SNIPER Matches the Performance Even After
    Reducing the Pixels Processed by 3.5 Times
  Table 8 caption: TABLE 8 We Highlight the Importance of Image Pyramids Even With
    Lightweight Backbones, Where We See a 12 percent Gain in Performance
  Table 9 caption: TABLE 9 Comparison Between the Efficient Multi-Scale Testing in
    AutoFocus and Testing on Full Image Pyramids in SNIPER on COCO Test-Dev
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3058945
- Affiliation of the first author: department of biomedical engineering, mathematical
    institute for data science, johns hopkins university, baltimore, md, usa
  Affiliation of the last author: department of biomedical engineering, mathematical
    institute for data science, johns hopkins university, baltimore, md, usa
  Figure 1 Link: "articels_figures_by_rev_year\\2021\\The_Fastest_\u2113__\u2113_Prox_in_the_West\\\
    figure_1.jpg"
  Figure 1 caption: "Illustration of the effect of the proximal operator. The left\
    \ plot corresponds to the original matrix while the right plot corresponds to\
    \ its thresholded counterpart. The bottom color plots represent the \u2113 1 norm\
    \ of each column. Warmer colors mean larger entries. The proximal operator projects\
    \ the columns of the input matrix onto the \u2113 1 ball of radius t \u22C6 ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: "articels_figures_by_rev_year\\2021\\The_Fastest_\u2113__\u2113_Prox_in_the_West\\\
    figure_2.jpg"
  Figure 2 caption: "Average execution time for the projection onto the \u2113 \u221E\
    ,1 ball for randomly generated matrices of n=100 rows and for different number\
    \ of columns. The data is projected onto a ball whose radius is a fraction \u03B1\
    =0.1 of the original matrix. The results correspond to an average over 100 realizations."
  Figure 3 Link: "articels_figures_by_rev_year\\2021\\The_Fastest_\u2113__\u2113_Prox_in_the_West\\\
    figure_3.jpg"
  Figure 3 caption: "Average execution time for the projection onto the \u2113 \u221E\
    ,1 ball for randomly generated matrices with m=100 columns and for different number\
    \ of rows. The data is projected onto a ball whose radius is a fraction \u03B1\
    =0.1 of the original matrix. The results correspond to an average over 100 realizations."
  Figure 4 Link: "articels_figures_by_rev_year\\2021\\The_Fastest_\u2113__\u2113_Prox_in_the_West\\\
    figure_4.jpg"
  Figure 4 caption: "Average execution time for the projection onto the \u2113 \u221E\
    ,1 ball for randomly generated matrices of size data 100\xD7100 . The data is\
    \ projected onto a ball whose radius is a fraction \u03B1 of the original matrix.\
    \ The results correspond to an average over 100 realizations."
  Figure 5 Link: "articels_figures_by_rev_year\\2021\\The_Fastest_\u2113__\u2113_Prox_in_the_West\\\
    figure_5.jpg"
  Figure 5 caption: "Average execution time for the projection onto the ell infty,1\
    \ ball for the multi-task learning problem using the Carcinom dataset as compared\
    \ to the computation time for the gradient step. We report the results for different\
    \ values of the ell infty,1 ball radius tau . The results correspond to an average\
    \ over 5 random data splits where 80 percent of the data is used for training.\
    \ Our methods are indicated by an asterisk where CS, AS, and BI denote \u201C\
    Column Sort\u201D, \u201CActive Set\u201D and \u201CBisection,\u201D respectively."
  Figure 6 Link: "articels_figures_by_rev_year\\2021\\The_Fastest_\u2113__\u2113_Prox_in_the_West\\\
    figure_6.jpg"
  Figure 6 caption: Average classification results using an SVM classifier as a function
    of the number of features selected.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: "Benjam\xEDn B\xE9jar"
  Name of the last author: "Ren\xE9 Vidal"
  Number of Figures: 6
  Number of Tables: 3
  Number of authors: 2
  Paper title: "The Fastest \u2113 1,\u221E \u21131,\u221E Prox in the West"
  Publication Date: 2021-02-15 00:00:00
  Table 1 caption: "TABLE 1 Average Execution Time and Average Number of Iterations\
    \ (in Brackets) of the Different Methods in Computing the Projection Onto the\
    \ \u2113 \u221E,1 \u2113\u221E,1 Ball"
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Characterization Summary of the Used Datasets, see [20]
  Table 3 caption: TABLE 3 Average Classification Accuracy Using Criterion (50)
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3059301
- Affiliation of the first author: miit key laboratory of pattern analysis and machine
    intelligence, collaborative innovation center of novel software technology and
    industrialization, nanjing university of aeronautics and astronautics, nanjing,
    china
  Affiliation of the last author: miit key laboratory of pattern analysis and machine
    intelligence, collaborative innovation center of novel software technology and
    industrialization, nanjing university of aeronautics and astronautics, nanjing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2021\Partial_MultiLabel_Learning_With_Noisy_Label_Identification\figure_1.jpg
  Figure 1 caption: An example of partial multi-label learning. The image is partially
    labeled by noisy annotators in crowdsourcing. Among the candidate labels, house,
    tree, car, light and cloud are ground-truth labels while flower, cat and people
    are noisy labels.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Partial_MultiLabel_Learning_With_Noisy_Label_Identification\figure_2.jpg
  Figure 2 caption: Comparison of PML-NI (control algorithm) against five comparing
    algorithms with the Bonferroni-Dunn test. Algorithms not connected with PML-NI
    in the CD diagram are considered to have a significantly different performance
    from the control algorithm (CD = 1.3623 at 0.05 significance level).
  Figure 3 Link: articels_figures_by_rev_year\2021\Partial_MultiLabel_Learning_With_Noisy_Label_Identification\figure_3.jpg
  Figure 3 caption: Results of PML-NI with varying value of trade-off parameters on
    musicemotion.
  Figure 4 Link: articels_figures_by_rev_year\2021\Partial_MultiLabel_Learning_With_Noisy_Label_Identification\figure_4.jpg
  Figure 4 caption: Comparison of MIPML-NI (control algorithm) against five comparing
    algorithms with the Bonferroni-Dunn test. Algorithms not connected with MIPML-NI
    in the CD diagram are considered to have a significantly different performance
    from the control algorithm (CD = 1.7587 at 0.05 significance level).
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ming-Kun Xie
  Name of the last author: Sheng-Jun Huang
  Number of Figures: 4
  Number of Tables: 6
  Number of authors: 2
  Paper title: Partial Multi-Label Learning With Noisy Label Identification
  Publication Date: 2021-02-15 00:00:00
  Table 1 caption: "TABLE 1 Experimental Results of Each Comparing Approach in Terms\
    \ of Ranking Loss, Where \u2022 \u2218 \u2218 Indicates Whether PML-NI is SuperiorInferior\
    \ to the Other Method"
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 Experimental Results of Each Comparing Approach in Terms\
    \ of Average Precision, Where \u2022 \u2218 \u2218 Indicates Whether PML-NI is\
    \ SuperiorInferior to the Other Method"
  Table 3 caption: TABLE 3 Friedman Statistics F F FF in Terms of Each Evaluation
    Metric and the Critical Value at 0.05 Significance Level ( comparing algorithms
    k=7 k=7, data sets N=35 N=35)
  Table 4 caption: "TABLE 4 Experimental Results of Each Comparing Approach in Terms\
    \ of Ranking Loss, Where \u2022 \u2218 \u2218 Indicates Whether MIPML-NI is SuperiorInferior\
    \ to the Other Method"
  Table 5 caption: "TABLE 5 Experimental Results of Each Comparing Approach in Terms\
    \ of Average Precision, Where \u2022 \u2218 \u2218 Indicates Whether MIPML-NI\
    \ is Superiorinferior to the Other Method"
  Table 6 caption: TABLE 6 Friedman Statistics F F FF in Terms of Each Evaluation
    Metric and the Critical Value at 0.05 Significance Level ( comparing algorithms
    k=7 k=7, data sets N=21 N=21)
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3059290
- Affiliation of the first author: college of computer and information engineering,
    zhejiang gongshang university, hangzhou, china
  Affiliation of the last author: school of computer science and information engineering,
    hefei university of technology, hefei, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Dual_Encoding_for_Video_Retrieval_by_Text\figure_1.jpg
  Figure 1 caption: "Showcase of video retrieval by text with and without the proposed\
    \ encoding. The \xD7 symbol indicates encoding by mean pooling. Numbers in the\
    \ third column are the rank of the relevant video returned by retrieval models\
    \ subject to specific queryvideo encoding strategies. The retrieval model with\
    \ dual encoding successfully answers this complex query."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Dual_Encoding_for_Video_Retrieval_by_Text\figure_2.jpg
  Figure 2 caption: "A conceptual diagram of the proposed dual encoding network for\
    \ video retrieval by text. Given a video v and a sentence s , the network performs\
    \ in parallel multi-level encodings, i.e., mean pooling, biGRU and biGRU-CNN,\
    \ eventually representing the two input by two combined vectors \u03D5(v) and\
    \ \u03D5(s) , respectively. The vectors are later projected into a hybrid common\
    \ space which consists of a latent space and a concept space. Once the network\
    \ is trained, encoding at each side is performed independently, meaning we can\
    \ process large-scale videos offline and answer ad-hoc queries on the fly."
  Figure 3 Link: articels_figures_by_rev_year\2021\Dual_Encoding_for_Video_Retrieval_by_Text\figure_3.jpg
  Figure 3 caption: An illustration of extracting concept-level annotations from sentence
    descriptions of training videos. Instead of binary labels, we extract frequency-based
    soft labels to better reflect the importance of a specific concept for a given
    video. E.g., the concept dance appears in all the five sentences of the example
    video, so its y value is 1, whilst the concept wedding occurring twice has a y
    value of 0.4.
  Figure 4 Link: articels_figures_by_rev_year\2021\Dual_Encoding_for_Video_Retrieval_by_Text\figure_4.jpg
  Figure 4 caption: A toy example showing potential weakness of a latent space and
    a concept space in two dimensions. Markers with the same color are relevant, while
    negative samples are with gray background. Red and light-red are semantically
    closer than the blue. Given a specific triplet, only their relative positions
    matter in the latent space. So even with zero triplet ranking loss, the red triplet
    can be projected into a region that is closer to the blue triplet rather than
    the light-red triplet. In the concept space, wherein each dimension corresponds
    to a distinct concept, the absolute positions of the triplets now matter. Samples
    positive w.r.t a specific concept are to be projected to one end, while negatives
    are to be placed near the origin. As the region around the origin naturally lacks
    discriminability, other dimensions have to be considered. However, as the concepts
    are pre-specified, they are not necessarily optimal to describe the video content.
    Best viewed in color.
  Figure 5 Link: articels_figures_by_rev_year\2021\Dual_Encoding_for_Video_Retrieval_by_Text\figure_5.jpg
  Figure 5 caption: Selected examples of movie retrieval by text on MPII-MD. The top
    retrieved shots, though not being ground truth, appear to be correct.
  Figure 6 Link: articels_figures_by_rev_year\2021\Dual_Encoding_for_Video_Retrieval_by_Text\figure_6.jpg
  Figure 6 caption: Selected examples of text-to-video retrieval by our model on MSR-VTT.
    For each query, the top 3 ranked videos and the ground-truth video (marked with
    red ticks) are shown. In case the ground-truth video is among the top three, the
    fourth video will be included as well. By definition, each query has only one
    ground-truth video. Number on the left hand side of each video indicates the videos
    rank in the retrieval result. Below a specific query are its predicted concepts,
    visualized in the form of a tag cloud, bigger font meaning larger predicted scores.
    Next to the videos are their predicted concepts. Putting these tag clouds together
    helps us better understand the video retrieval results.
  Figure 7 Link: articels_figures_by_rev_year\2021\Dual_Encoding_for_Video_Retrieval_by_Text\figure_7.jpg
  Figure 7 caption: Integrating pre-trained visual textual Transformers into the Dual
    Encoding network. The outputs of the two transformers are respectively concatenated
    with the video-side and text-side multi-level encodings in advance to hybrid space
    learning.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Jianfeng Dong
  Name of the last author: Meng Wang
  Number of Figures: 7
  Number of Tables: 9
  Number of authors: 7
  Paper title: Dual Encoding for Video Retrieval by Text
  Publication Date: 2021-02-15 00:00:00
  Table 1 caption: TABLE 1 State-of-the-Art on MSR-VTT
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 State-of-the-Art on the TRECVID AVS 2016 2017 2018
  Table 3 caption: TABLE 3 State-of-the-Art on VATEX
  Table 4 caption: TABLE 4 State-of-the-Art on MPII-MD
  Table 5 caption: TABLE 5 Effectiveness of Dual Encoding
  Table 6 caption: TABLE 6 Performance of Dual Encoding With Distinct Common Spaces
  Table 7 caption: TABLE 7 Comparison With Transformer-Based Multi-Modal Methods for
    Video Retrieval by Text
  Table 8 caption: TABLE 8 Comparison With MMT in Terms of Model Size and Computation
    Overhead at the Inference Stage
  Table 9 caption: TABLE 9 Performance of Image-Text Retrieval on Flickr30k and MSCOCO
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3059295
- Affiliation of the first author: school of computer science and technology, harbin
    university of science and technology, harbin, china
  Affiliation of the last author: department of automation, brnist, thuicbs, tsinghua
    university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Heterogeneous_Hypergraph_Variational_Autoencoder_for_Link_Prediction\figure_1.jpg
  Figure 1 caption: A conventional heterogeneous graph (e.g., bibliographic network)
    and its corresponding heterogeneous hypergraph.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Heterogeneous_Hypergraph_Variational_Autoencoder_for_Link_Prediction\figure_2.jpg
  Figure 2 caption: The framework of the proposed heterogeneous hypergraph variational
    autoencoder.
  Figure 3 Link: articels_figures_by_rev_year\2021\Heterogeneous_Hypergraph_Variational_Autoencoder_for_Link_Prediction\figure_3.jpg
  Figure 3 caption: Hyperedge attention module.
  Figure 4 Link: articels_figures_by_rev_year\2021\Heterogeneous_Hypergraph_Variational_Autoencoder_for_Link_Prediction\figure_4.jpg
  Figure 4 caption: Link prediction performance withwithout hyperedge attention module.
  Figure 5 Link: articels_figures_by_rev_year\2021\Heterogeneous_Hypergraph_Variational_Autoencoder_for_Link_Prediction\figure_5.jpg
  Figure 5 caption: Box plots of attention weights learned by HeteHG-VAE for different
    types of node.
  Figure 6 Link: articels_figures_by_rev_year\2021\Heterogeneous_Hypergraph_Variational_Autoencoder_for_Link_Prediction\figure_6.jpg
  Figure 6 caption: 2D t-SNE visualization of the latent embeddings on the Yelp dataset.
    (Orange points and blue points indicate the neighbor nodes and the randomly sampled
    non-neighbor nodes of the red triangle shaped target node, respectively).
  Figure 7 Link: articels_figures_by_rev_year\2021\Heterogeneous_Hypergraph_Variational_Autoencoder_for_Link_Prediction\figure_7.jpg
  Figure 7 caption: Impact of different embedding dimensions.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.9
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.76
  Name of the first author: Haoyi Fan
  Name of the last author: Qionghai Dai
  Number of Figures: 7
  Number of Tables: 5
  Number of authors: 7
  Paper title: Heterogeneous Hypergraph Variational Autoencoder for Link Prediction
  Publication Date: 2021-02-15 00:00:00
  Table 1 caption: TABLE 1 Notations
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Statistics of Datasets
  Table 3 caption: TABLE 3 Link Prediction Performance on DBLP and Douban
  Table 4 caption: TABLE 4 Link Prediction Performance on IMDB and Yelp
  Table 5 caption: TABLE 5 Time Efficiency Performance (CPU Time of Model Learning
    in Seconds for One Training Epoch)
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3059313
