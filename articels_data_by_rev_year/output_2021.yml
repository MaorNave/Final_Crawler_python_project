- Affiliation of the first author: school of mathematics and statistics and hubei
    key laboratory of mathematical sciences, central china normal university, wuhan,
    china
  Affiliation of the last author: department of mathematics, university of hong kong,
    pokfulam, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2021\Low_Rank_Tensor_Completion_With_Poisson_Observations\figure_1.jpg
  Figure 1 caption: MSE of synthetic data with different SRs and dimensions of tensors.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Low_Rank_Tensor_Completion_With_Poisson_Observations\figure_2.jpg
  Figure 2 caption: "Recovered images and zoomed regions by different methods for\
    \ the tenth channel of Scene 7, where SR=30% and \u03B2=100,c=5 ."
  Figure 3 Link: articels_figures_by_rev_year\2021\Low_Rank_Tensor_Completion_With_Poisson_Observations\figure_3.jpg
  Figure 3 caption: "SNR values versus SR of different methods for the hyperspectral\
    \ images datasets with \u03B2=50 and c=1 . (a) Samson. (b) Scene 7. (c) Washington\
    \ DC Mall."
  Figure 4 Link: articels_figures_by_rev_year\2021\Low_Rank_Tensor_Completion_With_Poisson_Observations\figure_4.jpg
  Figure 4 caption: 'Recovered images of different methods for the Copymachine video
    (from top to bottom: the 30th, 80th, 130th frames), where beta =50, c=1 , and
    textSR = 15% .'
  Figure 5 Link: articels_figures_by_rev_year\2021\Low_Rank_Tensor_Completion_With_Poisson_Observations\figure_5.jpg
  Figure 5 caption: SNR values versus SR of different methods for the video datasets
    with beta =50 and c=1 . (a) Hall. (b) Pets2006. (c) Copymachine.
  Figure 6 Link: articels_figures_by_rev_year\2021\Low_Rank_Tensor_Completion_With_Poisson_Observations\figure_6.jpg
  Figure 6 caption: Recovered images and zoomed regions by different methods for the
    tenth frame of fluidHighway video, where textSR = 50% .
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xiongjun Zhang
  Name of the last author: Michael K. Ng
  Number of Figures: 6
  Number of Tables: 3
  Number of authors: 2
  Paper title: Low Rank Tensor Completion With Poisson Observations
  Publication Date: 2021-02-15 00:00:00
  Table 1 caption: TABLE 1 CPU Time (in Seconds) of Different methods for the Hyperspectral
    Images Datasets With 40 percent Sampling Ratio
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 CPU Time (in Seconds) of Different Methods for the Video
    Datasets With 30 percent Sampling Ratio
  Table 3 caption: TABLE 3 The Q Q and BIQA Values of Different Methods for the fluidHighway
    Video With Different Sampling Ratios
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3059299
- Affiliation of the first author: department of electronic engineering, shanghai
    jiao tong university, shanghai, china
  Affiliation of the last author: huawei inc., shenzhen, guangdong, china
  Figure 1 Link: articels_figures_by_rev_year\2021\PartiallyConnected_Neural_Architecture_Search_for_Reduced_Computational_Redundan\figure_1.jpg
  Figure 1 caption: "Illustration of the proposed approach (best viewed in color),\
    \ partially-connected DARTS (PC-DARTS). We take node 3 ( j=3 ) as an example to\
    \ explain the propagation of information. PC-DARTS leverages two sets of architectural\
    \ parameters, i.e., \u03B1 o i,j for operations and \u03B2 i,j for edges with\
    \ 0\u2A7Di<j and o\u2208O , to determine the searched architectures. \u03B1 o\
    \ i,j is learned based on a subset sampled from the super-network. In the channel\
    \ level, a proportion of 1K channels are kept to the next stage, which means that\
    \ the memory consumption is reduced by K times, as shown in Section 3.2.1. Side\
    \ operation is adopted in Section 3.2.2 to process the remaining 1\u22121K channels\
    \ to guarantee the performance under extremely low sampling rates. In the spatial\
    \ level, a down sampling filter DW(\u22C5) is applied on input features to further\
    \ reduce the memory cost, as elaborated in Section 3.2.3. To minimize the uncertainty\
    \ led by sampling, we introduce \u03B2 i,j as extra edge parameters in Section\
    \ 3.3."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\PartiallyConnected_Neural_Architecture_Search_for_Reduced_Computational_Redundan\figure_2.jpg
  Figure 2 caption: Cells found on CIFAR-10 and ImageNet. The normal cell searched
    on ImageNet are more complex (deeper), although the reduction cell is very similar
    to that found on CIFAR-10.
  Figure 3 Link: articels_figures_by_rev_year\2021\PartiallyConnected_Neural_Architecture_Search_for_Reduced_Computational_Redundan\figure_3.jpg
  Figure 3 caption: Search cost (GPU-hours) and top-1 test error rates (%) for BPC-DARTS
    under the sampling rates 11, 12, 14, and 18, among which 14 makes a good trade-off
    between accuracy and efficiency.
  Figure 4 Link: articels_figures_by_rev_year\2021\PartiallyConnected_Neural_Architecture_Search_for_Reduced_Computational_Redundan\figure_4.jpg
  Figure 4 caption: "Normal cells found on the CIFAR-10 dataset by EPC-DARTS with\
    \ side operations: 3\xD73 dilated convolution, 5\xD75 dilated convolution, 3\xD7\
    3 separable convolution and 5\xD75 separable convolution."
  Figure 5 Link: articels_figures_by_rev_year\2021\PartiallyConnected_Neural_Architecture_Search_for_Reduced_Computational_Redundan\figure_5.jpg
  Figure 5 caption: "Cells found on ImageNet by EPC-DARTS with the side operation\
    \ 3\xD73 dilated convolution."
  Figure 6 Link: articels_figures_by_rev_year\2021\PartiallyConnected_Neural_Architecture_Search_for_Reduced_Computational_Redundan\figure_6.jpg
  Figure 6 caption: Cells found on ImageNet by RPC-DARTS with the sampling rate 12
    and mixed operations with stride 2 for the down-sampling filter.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Yuhui Xu
  Name of the last author: Qi Tian
  Number of Figures: 6
  Number of Tables: 12
  Number of authors: 8
  Paper title: Partially-Connected Neural Architecture Search for Reduced Computational
    Redundancy
  Publication Date: 2021-02-16 00:00:00
  Table 1 caption: TABLE 1 Comparison With State-of-the-Art Network Architectures
    on CIFAR-10
  Table 10 caption: TABLE 10 Comparisons of Memory Cost (GBytesGPU) for BPC-DARTS,
    EPC-DARTS, RPC-DARTS, and ER-PC-DARTS on CIFAR-10 and ImageNet
  Table 2 caption: TABLE 2 Comparison With State-of-the-Art Architectures on ImageNet
    (Mobile Setting)
  Table 3 caption: TABLE 3 Evaluations on the Stability of DARTS, BPC-DARTS, EPC-DARTS,
    and RPC-DARTS
  Table 4 caption: TABLE 4 Ablation Studies on CIFAR-10 and ImageNet
  Table 5 caption: "TABLE 5 Top-1 Test Error Rates (%) Obtained by EPC-DARTS With\
    \ Different Choices of o x (\u22C5) ox\xB7 on the CIFAR-10 Dataset"
  Table 6 caption: "TABLE 6 Top-1 Test Error Rates (%) Obtained by EPC-DARTS With\
    \ Different Choices of o x (\u22C5) ox\xB7 on the ImageNet Dataset"
  Table 7 caption: TABLE 7 Top-1 Test Error Rates (%) Obtained on the CIFAR-10 Dataset
    by RPC-DARTS With Different Configurations
  Table 8 caption: TABLE 8 Top-1 Test Error Rates (%) Obtained on the ImageNet Dataset
    by RPC-DARTS With Different Configurations
  Table 9 caption: TABLE 9 Top-1 Test Error Rates (%) Obtained on the CIFAR-10 Dataset
    by BPC-DARTS, EPC-DARTS and RPC-DARTS With the Same Search Cost on the Same GPU
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3059510
- Affiliation of the first author: school of information and communication engineering,
    dalian university of technology, dalian, liaoning, china
  Affiliation of the last author: school of information and communication engineering,
    dalian university of technology, dalian, liaoning, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Learning_to_Detect_Salient_Object_With_MultiSource_Weak_Supervision\figure_1.jpg
  Figure 1 caption: (a) annotations. (b) images. (c) saliency maps of the models trained
    with single weak supervision source shown in the first column of the corresponding
    row. (d) saliency maps of the models trained with multi-source image-level supervision.
    (e) saliency maps of the models trained with paired images and ground-truth leveraging
    Web image.
  Figure 10 Link: articels_figures_by_rev_year\2021\Learning_to_Detect_Salient_Object_With_MultiSource_Weak_Supervision\figure_10.jpg
  Figure 10 caption: Visual comparison.
  Figure 2 Link: articels_figures_by_rev_year\2021\Learning_to_Detect_Salient_Object_With_MultiSource_Weak_Supervision\figure_2.jpg
  Figure 2 caption: The schematic diagram of synthetic image dataset.
  Figure 3 Link: articels_figures_by_rev_year\2021\Learning_to_Detect_Salient_Object_With_MultiSource_Weak_Supervision\figure_3.jpg
  Figure 3 caption: An overview of the proposed multi-source weak supervision framework.
    (1, 2, 3, 4) images annotated with category labels, caption annotations, unlabelled
    images, and web data. (c1, c2, c3) saliency maps of images (1, 2, 3) generated
    by the classification network (CNet). (p1, p2, p3) saliency maps of images (1,
    2, 3) generated by the caption generation network (PNet). (s3, s4) saliency maps
    of images (3, 4) generated by saliency prediction network (SNet).
  Figure 4 Link: articels_figures_by_rev_year\2021\Learning_to_Detect_Salient_Object_With_MultiSource_Weak_Supervision\figure_4.jpg
  Figure 4 caption: The details of the attention module.
  Figure 5 Link: articels_figures_by_rev_year\2021\Learning_to_Detect_Salient_Object_With_MultiSource_Weak_Supervision\figure_5.jpg
  Figure 5 caption: "One iteration of the label and model updating (LMU). X sal and\
    \ X bkg indicate the collected simple images with salient objects and background\
    \ images. X syn and X ul indicate images in the synthetic image dataset D s =(\
    \ X i syn , Y i syn ),i=1,2,\u2026, N syn created with Web images and the natural\
    \ image dataset D n =( X i ul , Y i ul ),i=1,2,\u2026, N ul . Y syn and Y ul are\
    \ the pseudo labels corresponding to X syn and X ul . In each iteration, SNet\
    \ is updated under the supervision of D n and D s . The two datasets D n and D\
    \ s are reconstructed by the prediction of SNet."
  Figure 6 Link: articels_figures_by_rev_year\2021\Learning_to_Detect_Salient_Object_With_MultiSource_Weak_Supervision\figure_6.jpg
  Figure 6 caption: Data condition in each iteration of the recurrent optimization.
  Figure 7 Link: articels_figures_by_rev_year\2021\Learning_to_Detect_Salient_Object_With_MultiSource_Weak_Supervision\figure_7.jpg
  Figure 7 caption: Saliency maps predicted by SNet t .
  Figure 8 Link: articels_figures_by_rev_year\2021\Learning_to_Detect_Salient_Object_With_MultiSource_Weak_Supervision\figure_8.jpg
  Figure 8 caption: Precision-Recall curves. Our method outperforms unsupervised methods,
    weakly supervised method and some supervised methods.
  Figure 9 Link: articels_figures_by_rev_year\2021\Learning_to_Detect_Salient_Object_With_MultiSource_Weak_Supervision\figure_9.jpg
  Figure 9 caption: 'Visual effect of each component. Image: input image. GT: ground
    truth. Cls: the result of CNet trained with category localization loss L c . Cap:
    the result of PNet trained with caption localization loss L p . Avg: the average
    result of Cls and Cap. AT: Jointly training the two networks using the attention
    transfer loss L at . AC: Jointly training the two networks and using unlabelled
    data for regularization. Loss on the unlabelled data is the attention coherence
    loss L ac . D n : Training SNet on the natural images with the pseudo labels generated
    by CNet and PNet. D s : Training SNet with the synthetic images and the natural
    images. RO: Training SNet with recurrent optimization.'
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Hongshuang Zhang
  Name of the last author: Jinqing Qi
  Number of Figures: 10
  Number of Tables: 8
  Number of authors: 6
  Paper title: Learning to Detect Salient Object With Multi-Source Weak Supervision
  Publication Date: 2021-02-16 00:00:00
  Table 1 caption: TABLE 1 Summary of Our Training Data
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Effect of Each Component in Terms of Maximum F-Measure
    on ECSSD Dataset
  Table 3 caption: TABLE 3 Comparison of Model Performance When Mixing Ratio of Synthetic
    Samples to Natural Images Changes
  Table 4 caption: TABLE 4 Comparison of the Different Iterations of Recurrent Optimization
    in Terms of Maximum F-Measure (the Larger the Better) and MAE (the Smaller the
    Better)
  Table 5 caption: TABLE 5 Effect of Each Component of the Simplified Model in Terms
    of Maximum F-Measure on ECSSD Dataset
  Table 6 caption: TABLE 6 Comparison of the Different Iterations of Recurrent Optimization
    in Terms of Maximum F-Measure (the Larger the Better) and MAE (the Smaller the
    Better) When the CNet and PNet Share the Feature Extractor
  Table 7 caption: "TABLE 7 Comparison With Weakly Supervised (Marked With \u2217\
    \ ) and Unsupervised Methods in Terms of Maximum F-Measure (the Larger the Better)\
    \ and MAE (the Smaller the Better)"
  Table 8 caption: TABLE 8 Comparison With Fully Supervised Methods in Terms of Maximum
    F-Measure (the Larger the Better) and MAE (the Smaller the Better)
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3059783
- Affiliation of the first author: department of machine learning, mohamed bin zayed
    university of artificial intelligence, abu dhabi, uae
  Affiliation of the last author: department of electrical & computer engineering,
    university of pittsburgh, pittsburgh, pa, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\Scaling_Up_Generalized_Kernel_Methods\figure_1.jpg
  Figure 1 caption: Comparisons of AsyDSSKL(orth), AsyDSSKL(rand) and other state-of-the-art
    kernel methods. (a-f) Accuracy versus training time. (g-l) MSE versus training
    time.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Scaling_Up_Generalized_Kernel_Methods\figure_2.jpg
  Figure 2 caption: Testing time versus testing samples of AsyDSSKL(orth), AsyDSSKL(rand)
    and other state-of-the-art kernel methods.
  Figure 3 Link: articels_figures_by_rev_year\2021\Scaling_Up_Generalized_Kernel_Methods\figure_3.jpg
  Figure 3 caption: Multi-class comparisons of AsyDSSKL(orth), AsyDSSKL(rand) and
    other state-of-the-art kernel methods, Accuracy versus training time.
  Figure 4 Link: articels_figures_by_rev_year\2021\Scaling_Up_Generalized_Kernel_Methods\figure_4.jpg
  Figure 4 caption: Convergence of AsyDSSKL(rand) and AsyDSSKL(orth) on 1, 2, 4, 8
    and 16 cores.
  Figure 5 Link: articels_figures_by_rev_year\2021\Scaling_Up_Generalized_Kernel_Methods\figure_5.jpg
  Figure 5 caption: Speedup results of AsyDSSKL(rand) and AsyDSSKL(orth).
  Figure 6 Link: articels_figures_by_rev_year\2021\Scaling_Up_Generalized_Kernel_Methods\figure_6.jpg
  Figure 6 caption: Convergence results of AsyDSSKL(rand) and AsyDSSKL(orth) with
    16 threads under different delays.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Bin Gu
  Name of the last author: Heng Huang
  Number of Figures: 6
  Number of Tables: 3
  Number of authors: 5
  Paper title: Scaling Up Generalized Kernel Methods
  Publication Date: 2021-02-16 00:00:00
  Table 1 caption: TABLE 1 Commonly Used Smooth Loss Functions for Binary Classification
    (BC), Multiclass Classification (MC) and Regression (R)
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The State-of-the-Art Kernel Methods Compared in Our Experiments
  Table 3 caption: TABLE 3 The Large-Scale Dasetsets Used in the Experiments, Where
    the Multi-Class Classification Datasets are Treated as Regression Problem
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3059702
- Affiliation of the first author: graduate institute of communication engineering,
    national taiwan university, taipei, taiwan
  Affiliation of the last author: graduate institute of communication engineering,
    national taiwan university, taipei, taiwan
  Figure 1 Link: articels_figures_by_rev_year\2021\Learning_of_D_Graph_Convolution_Networks_for_Point_Cloud_Analysis\figure_1.jpg
  Figure 1 caption: Convolution in (a) 2D images and (b) 3D graphs. Note that standard
    2D CNN cannot be easily applied to handle 3D point cloud data, since the kernels
    in 3D graph convolution networks need to exhibit additional deformation in shape
    due to unstructured inputs.
  Figure 10 Link: articels_figures_by_rev_year\2021\Learning_of_D_Graph_Convolution_Networks_for_Point_Cloud_Analysis\figure_10.jpg
  Figure 10 caption: Example kernel responses in different layers (segmentation on
    ShapeNetPart). Note that points with larger responses are colored in darker red.
    As expected, the dominant responses are shifted from point (low) to part (high)
    levels in 3D-GCN.
  Figure 2 Link: articels_figures_by_rev_year\2021\Learning_of_D_Graph_Convolution_Networks_for_Point_Cloud_Analysis\figure_2.jpg
  Figure 2 caption: Illustration of receptive field R M n and kernel K S . We have
    R M n indicates the M neighboring points for the n th point p n , and kernel K
    S composes of S supports with center at k C =(0,0,0) . Note that directional vector
    d m,n and k s are used to measure the similarity in (4).
  Figure 3 Link: articels_figures_by_rev_year\2021\Learning_of_D_Graph_Convolution_Networks_for_Point_Cloud_Analysis\figure_3.jpg
  Figure 3 caption: "3D Graph Convolution. As in (4), sim( p m , k s ) calculates\
    \ the inner product between f( p m ) and w( k s ) based on the cosine similarity\
    \ between d m,n and k s . For each support k s , the largest sim output among\
    \ all neighbors p m is obtained. Summing up with \u27E8f( p n ),w( k C )\u27E9\
    \ produces the final convolution output [i.e., (5)]."
  Figure 4 Link: articels_figures_by_rev_year\2021\Learning_of_D_Graph_Convolution_Networks_for_Point_Cloud_Analysis\figure_4.jpg
  Figure 4 caption: Illustration of the lack of invariance property. Recent models
    like PointNet [9] require techniques like zero-mean normalization for 3D point
    cloud representation, which might be sensitive to noisy 3D input points (as verified
    in Section 5).
  Figure 5 Link: articels_figures_by_rev_year\2021\Learning_of_D_Graph_Convolution_Networks_for_Point_Cloud_Analysis\figure_5.jpg
  Figure 5 caption: Illustration of rotation effects on 3D-GCN Directional vector
    d of a 3D point and the kernel k (with support number as 1), where (a) and (b)
    consider the learned kernels in along and away from z -axis, respectively.
  Figure 6 Link: articels_figures_by_rev_year\2021\Learning_of_D_Graph_Convolution_Networks_for_Point_Cloud_Analysis\figure_6.jpg
  Figure 6 caption: "Graph Max-Pooling. This pooling process performs channel-wise\
    \ max-pooling from the features in the receptive field of each p n \u2208 P in\
    \ , followed by randomly sampling a subset from P in with a sampling rate r ."
  Figure 7 Link: articels_figures_by_rev_year\2021\Learning_of_D_Graph_Convolution_Networks_for_Point_Cloud_Analysis\figure_7.jpg
  Figure 7 caption: Architecture of 3D-GCN for (a) classification and (b) part segmentation.
    Note that grey and yellow blocks denote point and feature inputs, respectively.
    Green arrows denote 3D Graph Convolution Layers, while green triangles denote
    the Graph Max-Pooling layer. We have MLP and outputs denoted in brown and blue,
    respectively. For part segmentation in (b), blocks in pink denote the up-sampled
    feature maps from the consecutive layer, which are concatenated with those at
    the layer of interest (shown in yellow) as the feature map for performing 3D graph
    convolution.
  Figure 8 Link: articels_figures_by_rev_year\2021\Learning_of_D_Graph_Convolution_Networks_for_Point_Cloud_Analysis\figure_8.jpg
  Figure 8 caption: 'Evaluation of invariance properties on ModelNet40. (a) Shift:
    Objects randomly shifted within a distance along all directions (with unshifted
    version denoted as 0), (b) Scale: Objects scaled to different sizes (with the
    original size denoted as 1), (c) Rotation: Objects rotated along the upward direction
    (degree is denoted in this figure). Note that DGCNN in [15] was pre-trained on
    objects with scale variants (i.e., scale within [0.5, 1.5]), but it cannot handle
    unseen scale variants as shown in (b).'
  Figure 9 Link: articels_figures_by_rev_year\2021\Learning_of_D_Graph_Convolution_Networks_for_Point_Cloud_Analysis\figure_9.jpg
  Figure 9 caption: Effects on the presence of outlier points for the ModelNet40 dataset.
    Outlier points of different ratio numbers are added to the 3D point cloud input.
    Take a point cloud input with 1,000 points for example, 10 percent indicates additional
    100 outliers introduced. Note that all the outlier points are sampled from a fixed
    Gaussian distribution.
  First author gender probability: 0.64
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Zhi-Hao Lin
  Name of the last author: Yu-Chiang Frank Wang
  Number of Figures: 10
  Number of Tables: 12
  Number of authors: 3
  Paper title: Learning of 3D Graph Convolution Networks for Point Cloud Analysis
  Publication Date: 2021-02-16 00:00:00
  Table 1 caption: "TABLE 1 Architecture Comparison. g(),s( p j ) g(),s(pj) are Components\
    \ of Equation (11), and w \u03B8 , k \u03B8 w\u03B8,k\u03B8 are Trainable Parameters\
    \ in Models"
  Table 10 caption: TABLE 10 Performances of Shape Classification on ModelNet10 and
    ModelNet40 With Varying Support Number S S
  Table 2 caption: TABLE 2 Shape Classification Results on ModelNet40
  Table 3 caption: TABLE 3 Part Segmentation Results on ShapeNetPart
  Table 4 caption: TABLE 4 Visualization of Part Segmentation on ShapeNetPart
  Table 5 caption: TABLE 5 Part Segmentation in Terms of Class mIoU With Shift, Scale
    and Rotation Variations
  Table 6 caption: TABLE 6 Scene Segmentation on S3DIS
  Table 7 caption: TABLE 7 Visualization of Scene Segmentation
  Table 8 caption: TABLE 8 Number of Parameters in Different Models for Various Tasks
  Table 9 caption: TABLE 9 Effects on Shape Classification on ModelNet40 With Varying
    Neighboring Number
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3059758
- Affiliation of the first author: australian centre for robotic vision, research
    school of computer science, australian national university, canberra, act, australia
  Affiliation of the last author: australian centre for robotic vision, research school
    of computer science, australian national university, canberra, act, australia
  Figure 1 Link: articels_figures_by_rev_year\2021\Deep_Declarative_Networks\figure_1.jpg
  Figure 1 caption: Parametrized data processing nodes in an end-to-end learnable
    model with global objective function J . During the forward evaluation pass of
    an imperative node (top) the input x is transformed into output y based on some
    explicit parametrized function tildef(cdot ; theta) . During the forward evaluation
    pass of a declarative node (bottom) the output y is computed as the minimizer
    of some parametrized objective function f(x, cdot ; theta) . During the backward
    parameter update pass for either node type, the gradient of the global objective
    function with respect to the output textDJ(y) is propagated backwards via the
    chain rule to produce gradients with respect to the input textDJ(x) and parameters
    textDJ(theta) .
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Deep_Declarative_Networks\figure_2.jpg
  Figure 2 caption: "Bi-level optimization problem showing back-propagation of gradients\
    \ through a deep declarative node. The quantity (\u2217) is D Y J(x,y) D y(x)\
    \ which when added to D X J(x,y) gives D J(x,y(x)) . The bypass connections (topmost\
    \ and bottommost paths) do not exist when the upper-level objective J only depends\
    \ on x through y . Moreover, if f appears in J as the only term involving y then\
    \ D Y J(x,y) is zero and the backward edge (\u2217) is not required. That is,\
    \ D J(x)= D X J(x,y) ."
  Figure 3 Link: articels_figures_by_rev_year\2021\Deep_Declarative_Networks\figure_3.jpg
  Figure 3 caption: Illustration of different scenarios for the solution to inequality
    constrained deep declarative nodes. In the first scenario ( y 1 ) the solution
    is a local minimum strictly satisfying the constraints. In the second scenario
    ( y 2 ) the solution is on the boundary of the constraint set with the negative
    gradient of the objective pointing outside of the set. In the third scenario (
    y 3 ) the solution is on the boundary of the constraint set and is also a local
    minimum.
  Figure 4 Link: articels_figures_by_rev_year\2021\Deep_Declarative_Networks\figure_4.jpg
  Figure 4 caption: Geometry of the gradient for an equality constrained optimization
    problem. The unconstrained gradient D y unc (x) is corrected to ensure that the
    solution remains on the constraint surface after gradient descent with an infinitesimal
    step size.
  Figure 5 Link: articels_figures_by_rev_year\2021\Deep_Declarative_Networks\figure_5.jpg
  Figure 5 caption: 'The effect of robust pooling layers on point cloud classification
    results for the ModelNet40 dataset [55], with varying outlier rates (O). Left:
    the same rate of outliers is seen during training and testing, corresponding to
    results in Table 3. Right: no outliers are seen during training, corresponding
    to results in Table 4. Outliers points are uniformly sampled from the unit ball.
    PointNet [46] is compared to our variants that replace max pooling with robust
    pooling: quadratic (Q), pseudo-Huber (PH), Huber (H), Welsch (W), and truncated
    quadratic (TQ), all trained from scratch. Top-1 accuracy (top) and mean average
    precision (bottom) are reported.'
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Stephen Gould
  Name of the last author: Dylan Campbell
  Number of Figures: 5
  Number of Tables: 6
  Number of authors: 3
  Paper title: Deep Declarative Networks
  Publication Date: 2021-02-16 00:00:00
  Table 1 caption: "TABLE 1 The Gradient of the Estimate for a Robust Mean Over a\
    \ Vector of Values for Various Penalty Functions \u03D5(z;\u03B1) \u03D5(z;\u03B1\
    ) When it Exists"
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The Gradient of the Euclidean Projection Onto Various L
    p Lp-Spheres With Constraint Functions h h, When it Exists
  Table 3 caption: TABLE 3 The Effect of Robust Pooling Layers on Point Cloud Classification
    Results for the ModelNet40 Dataset [55], With Varying Outlier Percentages (O)
    and the Same Rate of Outliers Seen During Training and Testing
  Table 4 caption: TABLE 4 The Effect of Robust Pooling Layers on Point Cloud Classification
    Results for the ModelNet40 Dataset [55], With Varying Outlier Percentages (O)
    and No Outliers Seen During Training
  Table 5 caption: TABLE 5 Training Runtime (in ms Point Cloud) of the Point Cloud
    Classification Network, Divided into Data Loading, Forward Pass, and Backward
    Pass, for the ModelNet40 Dataset [55], With Varying Outlier Fractions (O) and
    the Same Rate of Outliers Seen During Training and Testing
  Table 6 caption: TABLE 6 The Effect of Projection Layers on Image Classification
    Results for the ImageNet 2012 Dataset [19]
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3059462
- Affiliation of the first author: school of computer science and technology, university
    of chinese academy of sciences, beijing, china
  Affiliation of the last author: cloud bu, huawei technologies, shenzhen, china
  Figure 1 Link: articels_figures_by_rev_year\2021\SelfRegulated_Learning_for_Egocentric_Video_Activity_Anticipation\figure_1.jpg
  Figure 1 caption: Illustration of the problem and the major workflow of SRL.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\SelfRegulated_Learning_for_Egocentric_Video_Activity_Anticipation\figure_2.jpg
  Figure 2 caption: The proposed SRL framework. SRL consists of three main steps for
    future activity anticipation. In the observed information encoding step, given
    the observed video clip I , a feature extractor phi and an aggregation function
    Phi are employed to obtain feature representations boldsymbolF at each observed
    time-step and hidden representation ho at the last observed time-step. At the
    recursive sequence prediction step, a GRU layer is utilized to obtain the initial
    feature representation h1t , then the Rev. loss is employed to rectify it. After
    that, the revised representation and the observed representation boldsymbolF are
    fed into the Rea. module to obtain representation f1t that relates to current
    video content. At last, f1t and h1t are fused by another GRU layer to get the
    final representation at current time-step. In the target anticipation step, a
    multi-task learning framework is utilized to enhance the final representation
    by exploiting the semantic context information (actions pvts and objects pnts
    ) related to the target activity, and the predicted activity probability distribution
    pats is obtained.
  Figure 3 Link: articels_figures_by_rev_year\2021\SelfRegulated_Learning_for_Egocentric_Video_Activity_Anticipation\figure_3.jpg
  Figure 3 caption: The anticipation result visualization. In each example, the observed
    video clips are shown on the left. The target activity frame and its ground-truth
    activity category (marked in red) and the predicted activity, action, object category
    are shown on the right. GT means ground-truth, PT means activity prediction, AP
    means action prediction and OP means object prediction.
  Figure 4 Link: articels_figures_by_rev_year\2021\SelfRegulated_Learning_for_Egocentric_Video_Activity_Anticipation\figure_4.jpg
  Figure 4 caption: The failure case visualization. In each example, the observed
    video frames are shown on the left. The target activity frame and its ground-truth
    activity category (mark in red) and the predicted activity, action and object
    category are shown on the right. GT means ground-truth, PT means activity prediction,
    AP means action prediction and OP means object prediction.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Zhaobo Qi
  Name of the last author: Qi Tian
  Number of Figures: 4
  Number of Tables: 9
  Number of authors: 6
  Paper title: Self-Regulated Learning for Egocentric Video Activity Anticipation
  Publication Date: 2021-02-17 00:00:00
  Table 1 caption: TABLE 1 Ablation Studies on EPIC-Kitchens
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Ablation Studies on the Sampling Methods and the Number
    of Samples N N on EPIC-Kitchens
  Table 3 caption: TABLE 3 Egocentric Activity Anticipation Results on the EPIC-Kitchens
    With Different Modality Features
  Table 4 caption: TABLE 4 Results on the EPIC-Kitchens in Terms of Top-5 Accuracy
    at Different Anticipation Time-Steps
  Table 5 caption: TABLE 5 Ablation Studies About Aggregation Function on EPIC-Kitchens
  Table 6 caption: TABLE 6 Results on the EPIC-Kitchens Test Set With Seen (S1) and
    Unseen (S2) Kitchens
  Table 7 caption: TABLE 7 Egocentric Activity Anticipation Results on EGTEA Gaze+
  Table 8 caption: TABLE 8 Third-Person Activity Anticipation Results on 50 Salads
    and Breakfast
  Table 9 caption: TABLE 9 Ablation Studies on 50 Salads
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3059923
- Affiliation of the first author: snapchat machine learning research, venice, ca,
    usa
  Affiliation of the last author: university of california, los angeles, los angeles,
    ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\Image_Segmentation_Using_Deep_Learning_A_Survey\figure_1.jpg
  Figure 1 caption: Segmentation results of DeepLabV3 [13] on sample images.
  Figure 10 Link: articels_figures_by_rev_year\2021\Image_Segmentation_Using_Deep_Learning_A_Survey\figure_10.jpg
  Figure 10 caption: A CNN + CRF model. From [36].
  Figure 2 Link: articels_figures_by_rev_year\2021\Image_Segmentation_Using_Deep_Learning_A_Survey\figure_2.jpg
  Figure 2 caption: Architecture of CNNs. From [15].
  Figure 3 Link: articels_figures_by_rev_year\2021\Image_Segmentation_Using_Deep_Learning_A_Survey\figure_3.jpg
  Figure 3 caption: Architecture of a simple RNN. Courtesy of Christopher Olah [21].
  Figure 4 Link: articels_figures_by_rev_year\2021\Image_Segmentation_Using_Deep_Learning_A_Survey\figure_4.jpg
  Figure 4 caption: Architecture of a standard LSTM module. Courtesy of Olah [21].
  Figure 5 Link: articels_figures_by_rev_year\2021\Image_Segmentation_Using_Deep_Learning_A_Survey\figure_5.jpg
  Figure 5 caption: Architecture of a simple encoder-decoder model.
  Figure 6 Link: articels_figures_by_rev_year\2021\Image_Segmentation_Using_Deep_Learning_A_Survey\figure_6.jpg
  Figure 6 caption: Architecture of a GAN. Courtesy of Ian Goodfellow.
  Figure 7 Link: articels_figures_by_rev_year\2021\Image_Segmentation_Using_Deep_Learning_A_Survey\figure_7.jpg
  Figure 7 caption: The FCN learns to make pixel-accurate predictions. From [30].
  Figure 8 Link: articels_figures_by_rev_year\2021\Image_Segmentation_Using_Deep_Learning_A_Survey\figure_8.jpg
  Figure 8 caption: Skip connections combine coarse and fine information. From [30].
  Figure 9 Link: articels_figures_by_rev_year\2021\Image_Segmentation_Using_Deep_Learning_A_Survey\figure_9.jpg
  Figure 9 caption: The ParseNet (e) uses extra global context to produce a segmentation
    (d) smoother than that of an FCN (c). From [35].
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Shervin Minaee
  Name of the last author: Demetri Terzopoulos
  Number of Figures: 37
  Number of Tables: 7
  Number of authors: 6
  Paper title: 'Image Segmentation Using Deep Learning: A Survey'
  Publication Date: 2021-02-17 00:00:00
  Table 1 caption: TABLE 1 Accuracies of Segmentation Models on the PASCAL VOC Test
    Set
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Accuracies of Segmentation Models on the Cityscapes Dataset
  Table 3 caption: TABLE 3 Accuracies of Segmentation Models on the MS COCO Stuff
    Dataset
  Table 4 caption: TABLE 4 Accuracies of Segmentation Models on the ADE20k Validation
    Dataset
  Table 5 caption: TABLE 5 Instance Segmentation Model Performance on COCO Test-Dev
    2017
  Table 6 caption: TABLE 6 Panoptic Segmentation Model Performance on MS-COCO Val
  Table 7 caption: TABLE 7 Segmentation Model Performance on the NYUD-v2 and SUN-RGBD
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3059968
- Affiliation of the first author: beijing laboratory of intelligent information technology,
    school of computer science and technology, beijing institute of technology, beijing,
    china
  Affiliation of the last author: beijing laboratory of intelligent information technology,
    school of computer science and technology, beijing institute of technology, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2021\Coded_Hyperspectral_Image_Reconstruction_Using_Deep_External_and_Internal_Learni\figure_1.jpg
  Figure 1 caption: Overview of our CNN-based coded HSI reconstruction method. The
    reconstruction network is first learned from an external dataset, and then customized
    in term of the internal information of the input coded image for each target scene.
  Figure 10 Link: articels_figures_by_rev_year\2021\Coded_Hyperspectral_Image_Reconstruction_Using_Deep_External_and_Internal_Learni\figure_10.jpg
  Figure 10 caption: The absolute error between ground truth and recovered results
    of scenes in Figs. 5, 6, 7, 8 and 9 along spectra for all methods.
  Figure 2 Link: articels_figures_by_rev_year\2021\Coded_Hyperspectral_Image_Reconstruction_Using_Deep_External_and_Internal_Learni\figure_2.jpg
  Figure 2 caption: Illustration of representative imaging systems.
  Figure 3 Link: articels_figures_by_rev_year\2021\Coded_Hyperspectral_Image_Reconstruction_Using_Deep_External_and_Internal_Learni\figure_3.jpg
  Figure 3 caption: The architecture of coded HSI reconstruction network.
  Figure 4 Link: articels_figures_by_rev_year\2021\Coded_Hyperspectral_Image_Reconstruction_Using_Deep_External_and_Internal_Learni\figure_4.jpg
  Figure 4 caption: The convergence of our method on SD-CASSI with and without total
    variation regularization.
  Figure 5 Link: articels_figures_by_rev_year\2021\Coded_Hyperspectral_Image_Reconstruction_Using_Deep_External_and_Internal_Learni\figure_5.jpg
  Figure 5 caption: Visual quality comparison on three typical HSIs for SD-CASSI.
    The error maps for TVNSRLRMAHSCNN lambda -NetAutoencoderDeepUnrollingour reconstructed
    results on SD-CASSI are shown from left to right, and the corresponding scenes
    are provided in Fig. 6.
  Figure 6 Link: articels_figures_by_rev_year\2021\Coded_Hyperspectral_Image_Reconstruction_Using_Deep_External_and_Internal_Learni\figure_6.jpg
  Figure 6 caption: Visual quality comparison on three typical HSIs for DCD. The error
    maps for TVNSRLRMAHSCNNAutoencoderour reconstructed results on DCD are shown from
    left to right.
  Figure 7 Link: articels_figures_by_rev_year\2021\Coded_Hyperspectral_Image_Reconstruction_Using_Deep_External_and_Internal_Learni\figure_7.jpg
  Figure 7 caption: "Visual quality comparison on three typical HSIs for DD-CASSI.\
    \ The error maps for TVNSRLRMAHSCNN \u03BB -NetAutoencoderDeepUnrollingour reconstructed\
    \ results on DD-CASSI are shown from left to right, and the corresponding scenes\
    \ are provided in Fig. 6."
  Figure 8 Link: articels_figures_by_rev_year\2021\Coded_Hyperspectral_Image_Reconstruction_Using_Deep_External_and_Internal_Learni\figure_8.jpg
  Figure 8 caption: "Visual quality comparison on three typical HSIs for SS-CASSI.\
    \ The error maps for TVNSRLRMAHSCNN \u03BB -NetAutoencoderDeepUnrollingour reconstructed\
    \ results on SS-CASSI are shown from left to right, and the corresponding scenes\
    \ are provided in Fig. 6."
  Figure 9 Link: articels_figures_by_rev_year\2021\Coded_Hyperspectral_Image_Reconstruction_Using_Deep_External_and_Internal_Learni\figure_9.jpg
  Figure 9 caption: "Visual quality comparison on three typical HSIs for MS-CASSI.\
    \ The error maps for TVNSRLRMAHSCNN \u03BB -NetAutoencoderDeepUnrollingour reconstructed\
    \ results on MS-CASSI are shown from left to right, and the corresponding scenes\
    \ are provided in Fig. 6."
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.61
  Name of the first author: Ying Fu
  Name of the last author: Hua Huang
  Number of Figures: 15
  Number of Tables: 9
  Number of authors: 4
  Paper title: Coded Hyperspectral Image Reconstruction Using Deep External and Internal
    Learning
  Publication Date: 2021-02-17 00:00:00
  Table 1 caption: TABLE 1 SD-CASSI Reconstructed Results of Different Methods on
    Three HSI Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 DCD Reconstructed Results of Different Methods on Three
    HSI Datasets
  Table 3 caption: TABLE 3 DD-CASSI Reconstructed Results of Different Methods on
    Three HSI Datasets
  Table 4 caption: TABLE 4 SS-CASSI Reconstructed Results of Different Methods on
    Three HSI Datasets
  Table 5 caption: TABLE 5 MS-CASSI Reconstructed Results of Different Methods on
    Three HSI Datasets
  Table 6 caption: TABLE 6 Evaluate the Effect From Distribution Variation Between
    Training and Testing Data
  Table 7 caption: TABLE 7 Comparison on SD-CASSI Reconstructed Results With Adversarial
    Loss in External Learning andor Total Variation Regularization in Internal Learning
    on ICVL Dataset
  Table 8 caption: TABLE 8 SD-CASSI Reconstructed Results Under Different Noise Levels
    on ICVL Dataset
  Table 9 caption: TABLE 9 Comparison Results for Fusion-Based HSI Super-Resolution
    on ICVL Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3059911
- Affiliation of the first author: college of computer science, nankai university,
    tianjin, china
  Affiliation of the last author: state key laboratory of internet of things for smart
    city, department of computer and information science, university of macau, macau,
    china
  Figure 1 Link: articels_figures_by_rev_year\2021\ReThinking_CoSalient_Object_Detection\figure_1.jpg
  Figure 1 caption: Different salient object detection (SOD) tasks. (a) Traditional
    SOD [30]. (b) Within-image co-salient object detection (CoSOD) [31], where common
    salient objects are detected from a single image. (c) Existing CoSOD, where salient
    objects are detected across a pair [32] or a group [33] of images with similar
    appearances. (d) The proposed CoSOD in the wild, which requires a large amount
    of semantic context, making it more challenging than existing CoSOD.
  Figure 10 Link: articels_figures_by_rev_year\2021\ReThinking_CoSalient_Object_Detection\figure_10.jpg
  Figure 10 caption: "Visualization of the common activation maps (second and third\
    \ row), using largest and second eigenvalue, and their corresponding post-processed\
    \ (i.e., manifold ranking and DenseCRF) co-attention map A n (fourth row) selected\
    \ from the \u201Cbanana\u201D group of CoSal2015 [40]."
  Figure 2 Link: articels_figures_by_rev_year\2021\ReThinking_CoSalient_Object_Detection\figure_2.jpg
  Figure 2 caption: Sample images from our CoSOD3k dataset. It has rich annotations,
    i.e., image-level categories (top), bounding boxes, object-level masks, and instance-level
    masks. Our CoSOD3k will provide a solid foundation for the CoSOD task and can
    benefit a wide range of related fields, e.g., co-segmentation, weakly supervised
    localization.
  Figure 3 Link: articels_figures_by_rev_year\2021\ReThinking_CoSalient_Object_Detection\figure_3.jpg
  Figure 3 caption: Number of images in the 160 sub-classes of our dataset. Best viewed
    on screen and zoomed-in for details.
  Figure 4 Link: articels_figures_by_rev_year\2021\ReThinking_CoSalient_Object_Detection\figure_4.jpg
  Figure 4 caption: Taxonomic structure of our dataset, which contains 13 super-classes
    with 160 sub-classes.
  Figure 5 Link: articels_figures_by_rev_year\2021\ReThinking_CoSalient_Object_Detection\figure_5.jpg
  Figure 5 caption: Some passed and rejected cases (e.g., occlusion, precision) in
    our CoSOD3k.
  Figure 6 Link: articels_figures_by_rev_year\2021\ReThinking_CoSalient_Object_Detection\figure_6.jpg
  Figure 6 caption: Illustration of our co-attention projection operation. Given the
    original feature representation which covers common objects (circle), noisy foregrounds
    (triangle) and background clutter (square), the co-attention projection identifies
    the principle components of common objects, helping to preserve the common objects
    while removing interference. By adopting our co-attention projection operation,
    we finally project the principle component and obtain the new feature representation.
    Please refer to Section 4.2 for more details.
  Figure 7 Link: articels_figures_by_rev_year\2021\ReThinking_CoSalient_Object_Detection\figure_7.jpg
  Figure 7 caption: Visualization of overlap masks for mixture-specific category and
    overall dataset masks of our CoSOD3k.
  Figure 8 Link: articels_figures_by_rev_year\2021\ReThinking_CoSalient_Object_Detection\figure_8.jpg
  Figure 8 caption: The number of images for the MSRC, iCoSeg, image pair, CoSal2015,
    WICOS, and our CoSOD3k dataset in terms of the number of instances (a) and the
    instanceobject size (b).
  Figure 9 Link: articels_figures_by_rev_year\2021\ReThinking_CoSalient_Object_Detection\figure_9.jpg
  Figure 9 caption: "Pipeline of the proposed architecture which contains two separate\
    \ branches. For a group of images I n N n=1 as inputs, in the top branch, the\
    \ extracted high-level image features are fed into the co-attention projection\
    \ module to produce a co-attention map A n for each input image I n . In the bottom\
    \ branch, each image I n is sent into the edge-guided saliency detection network\
    \ (EGNet) [118] to generate the saliency prior map S n . Finally, A n and S n\
    \ are simply integrated using element-wise multiply to produce the optimized outputs\
    \ A n \u2297 S n . See Section 4 for details."
  First author gender probability: 0.9
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Deng-Ping Fan
  Name of the last author: Jianbing Shen
  Number of Figures: 14
  Number of Tables: 7
  Number of authors: 8
  Paper title: Re-Thinking Co-Salient Object Detection
  Publication Date: 2021-02-18 00:00:00
  Table 1 caption: TABLE 1 Statistics of Existing CoSOD Datasets and the Proposed
    CoSOD3k, Showing that CoSOD3k Provides Higher-Quality and Much Richer Annotations
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Summary of 40 Classic and Cutting-Edge CoSOD Approaches
  Table 3 caption: TABLE 3 Table of Symbols, Their Dimensions, Indices, and Meaning
  Table 4 caption: TABLE 4 Benchmarking Results of 18 Leading CoSOD Approaches on
    Two Classical [40], [41], and Our CoSOD3k
  Table 5 caption: "TABLE 5 Per Super-Class Average E-Measure Performance E \u03D5\
    \ E\u03D5 on Our CoSOD3k"
  Table 6 caption: TABLE 6 Ablative Studies of Our Model on Three Benchmark Datasets,
    Where Ours-A, Ours-P, Ours-E Represent the Co-Salient Results of Amulet, PiCANet,
    EGNet on Our Baseline, Respectively
  Table 7 caption: TABLE 7 Average Running Time of Ten SOTA Models
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3060412
