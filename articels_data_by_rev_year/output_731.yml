- Affiliation of the first author: department of computer science, forman christian
    college, pakistan
  Affiliation of the last author: department of computer science, university of central
    florida, orlando, fl
  Figure 1 Link: articels_figures_by_rev_year\2016\NELasso_GroupSparse_Modeling_for_Characterizing_Relations_Among_Named_Entities_i\figure_1.jpg
  Figure 1 caption: Distribution of named entities over time; each bar represents
    frequency of a named entity during one month.
  Figure 10 Link: articels_figures_by_rev_year\2016\NELasso_GroupSparse_Modeling_for_Characterizing_Relations_Among_Named_Entities_i\figure_10.jpg
  Figure 10 caption: 'Human evaluation of NELasso; height of each bar represents mean
    of human-assigned strength to relations discovered by NELasso; Blue: gamma = gamma
    1 , Red: gamma = gamma 2 such that gamma 1 < gamma 2 .'
  Figure 2 Link: articels_figures_by_rev_year\2016\NELasso_GroupSparse_Modeling_for_Characterizing_Relations_Among_Named_Entities_i\figure_2.jpg
  Figure 2 caption: 'Overview of NELasso: Stage 1 = news articles collection, Stage
    2 = named entities extraction (Section 4.1.2), word group formation (Section 4.2),
    Stage 3 = learn sparse group model associating named entities and word groups
    (Section 3.2.2), Stage 4 = quantify semantic relations between named entities
    and build network (Section 3.2.3).'
  Figure 3 Link: articels_figures_by_rev_year\2016\NELasso_GroupSparse_Modeling_for_Characterizing_Relations_Among_Named_Entities_i\figure_3.jpg
  Figure 3 caption: Semantic network of named entities for Nov-Dec 2012 (TIME dataset).
  Figure 4 Link: articels_figures_by_rev_year\2016\NELasso_GroupSparse_Modeling_for_Characterizing_Relations_Among_Named_Entities_i\figure_4.jpg
  Figure 4 caption: Semantic network of named entities for Jun-Jul 2013 (TIME dataset).
  Figure 5 Link: articels_figures_by_rev_year\2016\NELasso_GroupSparse_Modeling_for_Characterizing_Relations_Among_Named_Entities_i\figure_5.jpg
  Figure 5 caption: 'Variation of average strength and WLM of relations over time
    (TIME dataset). Relations among the following named entities exist in each time
    period: Syria, Bashar Asad, Cairo, Damascus, Jerusalem, Hamas, Gaza, Israel Egypt,
    Benghazi, Hillary Clinton; x-axis: Time period (months). y-axis: Mean WLM and
    relation strength.'
  Figure 6 Link: articels_figures_by_rev_year\2016\NELasso_GroupSparse_Modeling_for_Characterizing_Relations_Among_Named_Entities_i\figure_6.jpg
  Figure 6 caption: "Effect of threshold \u03B3 on evaluation measures on TIME dataset;\
    \ x-axis: Time period (month), y-axis: Evaluation measure (connectivity, mean\
    \ WLM, mean retrieval score, mean strength); Solid line: \u03B3= \u03B3 1 , Dotted\
    \ line: \u03B3= \u03B3 2 where \u03B3 1 < \u03B3 2 ; Mean of each curve given\
    \ in legends."
  Figure 7 Link: articels_figures_by_rev_year\2016\NELasso_GroupSparse_Modeling_for_Characterizing_Relations_Among_Named_Entities_i\figure_7.jpg
  Figure 7 caption: "Summary statistics (boxplots) for connectivity, mean WLM, mean\
    \ retrieval score, and mean strength at \u03B3 2 minus that at \u03B3 1 over all\
    \ time periods (TIME dataset), \u03B3 2 > \u03B3 1 ."
  Figure 8 Link: articels_figures_by_rev_year\2016\NELasso_GroupSparse_Modeling_for_Characterizing_Relations_Among_Named_Entities_i\figure_8.jpg
  Figure 8 caption: "Effect of threshold \u03B3 on evaluation measures on BBC dataset;\
    \ x-axis: Dataset sample, y-axis: Evaluation measure (connectivity, mean WLM,\
    \ mean retrieval score, or mean strength); Solid line: \u03B3= \u03B3 1 , Dotted\
    \ line: \u03B3= \u03B3 2 where \u03B3 1 < \u03B3 2 ; Mean of each curve given\
    \ in legends."
  Figure 9 Link: articels_figures_by_rev_year\2016\NELasso_GroupSparse_Modeling_for_Characterizing_Relations_Among_Named_Entities_i\figure_9.jpg
  Figure 9 caption: "Summary statistics (boxplots) for connectivity, mean WLM, mean\
    \ retrieval score, and mean strength at \u03B3 2 minus that at \u03B3 1 over all\
    \ samples (BBC dataset), where \u03B3 2 > \u03B3 1 ."
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Amara Tariq
  Name of the last author: Hassan Foroosh
  Number of Figures: 16
  Number of Tables: 3
  Number of authors: 3
  Paper title: 'NELasso: Group-Sparse Modeling for Characterizing Relations Among
    Named Entities in News Articles'
  Publication Date: 2016-11-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Example Cliques Discovered by Our System (TIME Dataset); Each
      Clique Corresponds to a Distinct News Event Indicated by the Type of the Relation
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Example of a Named Entity Involved in Different Relations
      over Time (TIME Dataset)
  Table 3 caption:
    table_text: TABLE 3 Examples of Relations with More Than One Relation Type in
      One Time Period (TIME Dataset)
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2632117
- Affiliation of the first author: school of computer science, university of adelaide,
    adelaide, sa, australia
  Affiliation of the last author: school of computer science, university of adelaide,
    adelaide, sa, australia
  Figure 1 Link: articels_figures_by_rev_year\2016\Efficient_Globally_Optimal_Consensus_Maximisation_with_Tree_Search\figure_1.jpg
  Figure 1 caption: "With pseudoconvex residuals, maximum consensus amounts to finding\
    \ a \u03B8 with maximum depth (e.g., the red cross above), where the depth of\
    \ \u03B8 is the number of convex regions R i (8) intersected by \u03B8 ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Efficient_Globally_Optimal_Consensus_Maximisation_with_Tree_Search\figure_2.jpg
  Figure 2 caption: "Illustrating the concept of basis and combinatorial dimension\
    \ on linear regression (Example 1) with \u03B8\u2208 R 2 ( p=2 ). Since r i (\u03B8\
    ) is convex in this problem, the combinatorial dimension is exactly 3. Panels\
    \ (a) and (b) show two particular bases (of levels 0 and 9) for the same input\
    \ data X , as well as the coverage and violation set of the bases. Note that points\
    \ in the bases B are equally far away from \u03B8(B) . In (a), since the basis\
    \ is also a support set of X (i.e., the basis has an empty violation set), solving\
    \ (2) on the basis equates to solving (2) on X ."
  Figure 3 Link: articels_figures_by_rev_year\2016\Efficient_Globally_Optimal_Consensus_Maximisation_with_Tree_Search\figure_3.jpg
  Figure 3 caption: "Tree traversal for solving (17) (panel a) and (19) (panels b\
    \ and c) on a problem with p=1 . The ordinate is f(B) while the abscissa is \u03B8\
    (B) . Nodes in red are bases expanded in the respective algorithms. In Matou\u0161\
    ek's method, all level- k bases (here, k=4 ) must be tested, since there are multiple\
    \ local minima. In panels b and c, the two different globally optimal solutions\
    \ found are equally good w.r.t. (19)."
  Figure 4 Link: articels_figures_by_rev_year\2016\Efficient_Globally_Optimal_Consensus_Maximisation_with_Tree_Search\figure_4.jpg
  Figure 4 caption: If x b is a true outlier of C(B) , then the shortest path from
    the root through B must go through the branch that removes x b . The other branches
    of B can thus be safely ignored without affecting optimality.
  Figure 5 Link: articels_figures_by_rev_year\2016\Efficient_Globally_Optimal_Consensus_Maximisation_with_Tree_Search\figure_5.jpg
  Figure 5 caption: Illustrating problem (39) on a linear regression problem (4) with
    p=2 . (a) Solution of the original minmax problem (2) on mathcal C(mathcal B)
    . By construction, mathcal B is the support set of mathcal C(mathcal B) . (b)
    One of the datum in the support set in (a) is chosen as mathbf xb to solve the
    modified minmax problem (39). Here, f(mathcal B|mathbf xb) > epsilon and mathbf
    xb is not in the support set.
  Figure 6 Link: articels_figures_by_rev_year\2016\Efficient_Globally_Optimal_Consensus_Maximisation_with_Tree_Search\figure_6.jpg
  Figure 6 caption: Line fitting results.
  Figure 7 Link: articels_figures_by_rev_year\2016\Efficient_Globally_Optimal_Consensus_Maximisation_with_Tree_Search\figure_7.jpg
  Figure 7 caption: "Image pairs and SIFT correspondences used in the experiments.\
    \ Green and red lines respectively indicate inliers and outliers w.r.t. the maximum\
    \ consensus set for homography estimation (a\u2013e), affine registration (f\u2013\
    j) and linearised fundamental matrix estimation (k-l)."
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.94
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Tat-Jun Chin
  Name of the last author: David Suter
  Number of Figures: 7
  Number of Tables: 5
  Number of authors: 4
  Paper title: Efficient Globally Optimal Consensus Maximisation with Tree Search
  Publication Date: 2016-11-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Results for Homography Estimation
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results of Tree Search Variants with Different Algorithmic
      Components on Homography Estimation
  Table 3 caption:
    table_text: TABLE 3 Results for Affine Registration
  Table 4 caption:
    table_text: TABLE 4 Results for Linearised Fundamental Matrix Estimation
  Table 5 caption:
    table_text: TABLE 5 Results for Triangulation
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2631531
- Affiliation of the first author: institute for language, cognition and computation,
    edinburgh, united kingdom
  Affiliation of the last author: institute for language, cognition and computation,
    edinburgh, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2016\Visually_Grounded_Meaning_Representations\figure_1.jpg
  Figure 1 caption: Stacked autoencoder trained with semi-supervised objective. Input
    to the model are attribute-based vector representations of single words obtained
    from text and images (see Table 1 ).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Visually_Grounded_Meaning_Representations\figure_2.jpg
  Figure 2 caption: Attribute categories and examples of attribute instances and images.
    Parentheses denote the number of attributes per category.
  Figure 3 Link: articels_figures_by_rev_year\2016\Visually_Grounded_Meaning_Representations\figure_3.jpg
  Figure 3 caption: Attribute predictions for concepts seen during training (top;
    house, cheetah) and unseen concepts (bottom; boathouse, cicada).
  Figure 4 Link: articels_figures_by_rev_year\2016\Visually_Grounded_Meaning_Representations\figure_4.jpg
  Figure 4 caption: "Attribute classifier performance for thresholds \u03B4 (on the\
    \ test set)."
  Figure 5 Link: articels_figures_by_rev_year\2016\Visually_Grounded_Meaning_Representations\figure_5.jpg
  Figure 5 caption: Visual representation for concept chick. Attribute classifiers
    predict attributes for example images depicting chicks. These prediction scores
    are then converted into vectors (first arrow). To compute a single visual attribute
    vector for a concept, all vectors are aggregated into p chick .
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Carina Silberer
  Name of the last author: Mirella Lapata
  Number of Figures: 5
  Number of Tables: 7
  Number of authors: 3
  Paper title: Visually Grounded Meaning Representations
  Publication Date: 2016-12-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Examples of Attribute-Based Representations Provided as Input
      to Our Autoencoders
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Nearest Neighbors Represented by Visual, Textual and Bimodal
      (SAE) Vectors
  Table 3 caption:
    table_text: TABLE 3 Mean Semantic and Visual Similarity Ratings Using a Scale
      of 1 (Highly Dissimilar) to 5 (Highly Similar); Averaged Across AMT Participants
  Table 4 caption:
    table_text: TABLE 4 Word Pairs with Highest Semantic and Visual Similarity According
      to SAE Model
  Table 5 caption:
    table_text: "TABLE 5 Correlation of Model Predictions Against Similarity Ratings\
      \ for [14] 's Noun Pairs (Using Spearman's \u03C1 )"
  Table 6 caption:
    table_text: TABLE 6 F-Score Results on Concept Categorization
  Table 7 caption:
    table_text: TABLE 7 Examples of Clusters Produced by CW Using the Semantic Representations
      Obtained from the Bimodal SAE Model
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2635138
- Affiliation of the first author: technical university of denmark, lyngby, denmark
  Affiliation of the last author: massachusetts institute of technology, cambridge,
    ma
  Figure 1 Link: articels_figures_by_rev_year\2016\A_Bayesian_Additive_Model_for_Understanding_Public_Transport_Usage_in_Special_Ev\figure_1.jpg
  Figure 1 caption: Breakdown of the observed time-series of subway arrivals (black
    solid line) into the routine commuting (area in red) and the contributions of
    events (orange, yellow and green areas). The dotted line represents the median
    arrivals over all the days in the observed data that correspond the same weekday.
    Events start times are shown in parentheses.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\A_Bayesian_Additive_Model_for_Understanding_Public_Transport_Usage_in_Special_Ev\figure_2.jpg
  Figure 2 caption: Factor graph of the proposed Bayesian additive model with Gaussian
    process components. The blue arrows represent the message-passing algorithm for
    performing approximate Bayesian inference. The second flow of messages starting
    from the GP factor for the events component that goes in the opposite direction
    is not shown.
  Figure 3 Link: articels_figures_by_rev_year\2016\A_Bayesian_Additive_Model_for_Understanding_Public_Transport_Usage_in_Special_Ev\figure_3.jpg
  Figure 3 caption: Visualization of the topic proportions for a sample of the events
    data using multidimensional scaling (MDS).
  Figure 4 Link: articels_figures_by_rev_year\2016\A_Bayesian_Additive_Model_for_Understanding_Public_Transport_Usage_in_Special_Ev\figure_4.jpg
  Figure 4 caption: Comparison of the predictions of BAM-GP (with truncated Gaussians)
    with the true observed arrivals (black solid line) and the predictions of the
    GP models for four example days.
  Figure 5 Link: articels_figures_by_rev_year\2016\A_Bayesian_Additive_Model_for_Understanding_Public_Transport_Usage_in_Special_Ev\figure_5.jpg
  Figure 5 caption: Results obtained by three different approaches (columns) on two
    examples days in two different areas (rows) for disaggregating the total observed
    arrivals (black solid line) into the contributions of the routine component and
    the various nearby events. The dotted line represents the median arrivals over
    all the days in the observed data that correspond the same weekday. Events start
    times are shown in parentheses.
  Figure 6 Link: articels_figures_by_rev_year\2016\A_Bayesian_Additive_Model_for_Understanding_Public_Transport_Usage_in_Special_Ev\figure_6.jpg
  Figure 6 caption: Results obtained by BAM-GP for disaggregating the total observed
    arrivals (black solid line) in six example days into the contributions of the
    routine component and the various nearby events. The dotted line represents the
    median arrivals over all the days in the observed data that correspond to the
    same weekday. Events start times are shown in parentheses.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Filipe Rodrigues
  Name of the last author: Francisco C. Pereira
  Number of Figures: 6
  Number of Tables: 5
  Number of authors: 4
  Paper title: A Bayesian Additive Model for Understanding Public Transport Usage
    in Special Events
  Publication Date: 2016-12-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Prediction Results for Toy Problem
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Decomposition Results for Toy Problem
  Table 3 caption:
    table_text: TABLE 3 Descriptive Statistics for the Two Study Areas
  Table 4 caption:
    table_text: TABLE 4 Results (with Standard Errors) for Estimating the Total Arrivals
      in the Stadium Area Using 10-Fold Cross-Validation
  Table 5 caption:
    table_text: TABLE 5 Top-10 More Relevant Features According to ARD
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2635136
- Affiliation of the first author: Not Available
  Affiliation of the last author: Not Available
  Figure 1 Link: Not Available
  Figure 1 caption: Not Available
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Sven Dickinson
  Name of the last author: Sven Dickinson
  Number of Figures: Not Available
  Number of Tables: 0
  Number of authors: 1
  Paper title: Incoming EIC Editorial
  Publication Date: 2016-12-02 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2622418
- Affiliation of the first author: here company, chicago, il
  Affiliation of the last author: department of microbiology and immunology, the state
    university of new york at buffalo, buffalo, ny
  Figure 1 Link: articels_figures_by_rev_year\2016\Principal_Graph_and_Structure_Learning_Based_on_Reversed_Graph_Embedding\figure_1.jpg
  Figure 1 caption: Real world problems exhibiting a variety of graph structures.
    (a) 360 degree rotation of teapot images forming a circle. (b) Optical character
    templates for different digits containing loops, bifurcations, and multiple disconnected
    components. (c) Branching architecture of cancer evolution (modified from [40]
    ). Selective pressures allow some tumor clones to expand while others become extinct
    or remain dormant.
  Figure 10 Link: articels_figures_by_rev_year\2016\Principal_Graph_and_Structure_Learning_Based_on_Reversed_Graph_Embedding\figure_10.jpg
  Figure 10 caption: Experiments on two large-scale datasets. (a)-(b) Adjacency matrices
    of 1,000 centroids after reshuffle according to labels from 0 to 9 for PenDigits
    and MNIST, respectively. (c) The learned tree structure of PenDigits over centroids
    with each label colored (best viewed in color) is visualized in the first three
    principal components.
  Figure 2 Link: articels_figures_by_rev_year\2016\Principal_Graph_and_Structure_Learning_Based_on_Reversed_Graph_Embedding\figure_2.jpg
  Figure 2 caption: A cartoon illustrating the proposed formulation on the teapot
    images. Each circle marker represents one teapot image. Our assumption of the
    data generation process is that a graph G exists in the latent space (e.g., a
    rotation circle in the left subplot), and each image y i is then mapped to point
    h G ( y i ) in the input space by maintaining the graph structure through the
    reversed graph embedding, and finally image x i is observed conditioned on h G
    ( y i ) according to certain noise model.
  Figure 3 Link: articels_figures_by_rev_year\2016\Principal_Graph_and_Structure_Learning_Based_on_Reversed_Graph_Embedding\figure_3.jpg
  Figure 3 caption: Convergence analyses and intermediate results of proposed methods
    on the Tree dataset by learning two different graph structures.
  Figure 4 Link: articels_figures_by_rev_year\2016\Principal_Graph_and_Structure_Learning_Based_on_Reversed_Graph_Embedding\figure_4.jpg
  Figure 4 caption: "Results of parameter sensitivity analysis of Algorithm 1 by learning\
    \ \u2113 1 graph performed on the distorted S-shape dataset by varying different\
    \ parameters: (a) varying \u03B3 ; (b) varying \u03C3 ; (c)-(f) varying \u03BB\
    \ ."
  Figure 5 Link: articels_figures_by_rev_year\2016\Principal_Graph_and_Structure_Learning_Based_on_Reversed_Graph_Embedding\figure_5.jpg
  Figure 5 caption: 'Results of five compared methods performed on six synthetic datasets
    containing various situations including curves, loops, self-intersections, and
    multiple disconnected components. The first and second rows show the results of
    our proposed methods for learning either an ell 1 graph or a spanning tree. The
    last three columns report the results generated by the polygonal line method,
    SCMS, and Mapper, respectively. The results of Mapper can be interpreted as: the
    size of a node indicates the number of points in the set represented by the node;
    the color of a node indicates the value of filter function (red being high and
    blue being low) by a suitable average taken over the corresponding set, and the
    parameters used to generate figures are shown in the bottom-left side of each
    figure.'
  Figure 6 Link: articels_figures_by_rev_year\2016\Principal_Graph_and_Structure_Learning_Based_on_Reversed_Graph_Embedding\figure_6.jpg
  Figure 6 caption: Experimental results of the proposed methods applied to Teapot
    images. (a) Principal circle generated by the proposed method for learning a ell
    1 graph. Each dot represents one teapot image. Images following the principal
    curve are plotted at intervals of 30 for visualization. (b) The adjacency matrix
    of the circle follows the ordering of the 400 consecutive teapot images with 360
    degree rotation. (c)-(d) The principle curve and the adjacency matrix are obtained
    by the proposed method for learning a spanning tree.
  Figure 7 Link: articels_figures_by_rev_year\2016\Principal_Graph_and_Structure_Learning_Based_on_Reversed_Graph_Embedding\figure_7.jpg
  Figure 7 caption: Experimental results of our proposed method for learning a spanning
    tree performed on facial expression images. (a) The generated hierarchical tree.
    Each dot represents one face image. Images of three types of facial expressions
    from three subjects are plotted for visualization. The black circle can be considered
    as the root of the hierarchical structure for achieving two layers hierarchy over
    nine subjects; (b) the adjacency matrix of the tree on nine blocks indicates that
    each block corresponds to one facial expression of one subject.
  Figure 8 Link: articels_figures_by_rev_year\2016\Principal_Graph_and_Structure_Learning_Based_on_Reversed_Graph_Embedding\figure_8.jpg
  Figure 8 caption: Progression path of breast cancer data. (a) The tree structure
    learned by the proposed method for learning a spanning tree; (b) the topological
    structure obtained by mapper.
  Figure 9 Link: articels_figures_by_rev_year\2016\Principal_Graph_and_Structure_Learning_Based_on_Reversed_Graph_Embedding\figure_9.jpg
  Figure 9 caption: Sampled results of the proposed RPG- ell 1 in finding the smoothing
    skeletons of optical characters.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.51
  Name of the first author: Qi Mao
  Name of the last author: Yijun Sun
  Number of Figures: 10
  Number of Tables: 1
  Number of authors: 4
  Paper title: Principal Graph and Structure Learning Based on Reversed Graph Embedding
  Publication Date: 2016-12-05 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Parameters Used in the Proposed Algorithm for Synthetic Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2635657
- Affiliation of the first author: department of cybernetics, czech technical university
    in prague, prague, czech republic
  Affiliation of the last author: computer vision laboratory, epfl, lausanne, switzerland
  Figure 1 Link: articels_figures_by_rev_year\2016\Geometric_Graph_Matching_Using_Monte_Carlo_Tree_Search\figure_1.jpg
  Figure 1 caption: Localizing a small satellite image within a map. Yellow lines
    link matched nodes of the road networks extracted from a satellite image (red)
    and from a map (blue). Transformed version of the map network (blue) after matching
    is shown overlaid on the satellite image.
  Figure 10 Link: articels_figures_by_rev_year\2016\Geometric_Graph_Matching_Using_Monte_Carlo_Tree_Search\figure_10.jpg
  Figure 10 caption: 'Top left: Brain tissue acquired using two-photon light microscopy
    from live brain tissue at a 1,mu textm resolution and bottom left: A smaller area
    of the same tissue imaged using electron microscopy, at a 20, textnm resolution.
    Right: The alignment of the two structures after matching using the proposed method.'
  Figure 2 Link: articels_figures_by_rev_year\2016\Geometric_Graph_Matching_Using_Monte_Carlo_Tree_Search\figure_2.jpg
  Figure 2 caption: Example of a geometrical graph (top) and its superedges of length
    one (bottom left) and two (bottom left).
  Figure 3 Link: articels_figures_by_rev_year\2016\Geometric_Graph_Matching_Using_Monte_Carlo_Tree_Search\figure_3.jpg
  Figure 3 caption: "Example for a possible next move (top\u2014initial state, bottom\u2014\
    next state) for a simplified case of superedges with length one. The superedges\
    \ reachable in each state are connected with already matched superedge pairs."
  Figure 4 Link: articels_figures_by_rev_year\2016\Geometric_Graph_Matching_Using_Monte_Carlo_Tree_Search\figure_4.jpg
  Figure 4 caption: Example of the GMMC algorithm exploring the search tree for a
    simple pair of graphs. A node corresponding to a superedge pair is selected and
    expanded and then extended greedily in the simulation step. For each pair of graphs
    in blue and green, M V is represented by yellow lines connecting vertices, matched
    superedges from M S are shown in yellow and the newest match is shown in red.
    Below each simulation, we show the alignment such matching would produce. The
    optimal solution (highlighted on the tree) was found after 6 milliseconds in 44
    iterations.
  Figure 5 Link: articels_figures_by_rev_year\2016\Geometric_Graph_Matching_Using_Monte_Carlo_Tree_Search\figure_5.jpg
  Figure 5 caption: "Simple example of the sampling for calculating the path descriptor\
    \ h \u03C9 ( s k ) for a superedge s k ."
  Figure 6 Link: articels_figures_by_rev_year\2016\Geometric_Graph_Matching_Using_Monte_Carlo_Tree_Search\figure_6.jpg
  Figure 6 caption: Results for synthetic datasets in 3D showing the performance of
    the tested methods. The median correct correspondence percentage of 20 realizations
    for each parameter value is shown. Under each graph, we show 2D examples of the
    effects.
  Figure 7 Link: articels_figures_by_rev_year\2016\Geometric_Graph_Matching_Using_Monte_Carlo_Tree_Search\figure_7.jpg
  Figure 7 caption: The processing time, precision and recall as a function of varepsilon
    h , where varepsilon T = frac13 varepsilon h . For different values of varepsilon
    h , we calculate the processing time taken to obtain the solution, the precision
    (positive predictive value) and the recall (true positive rate). The values shown
    are a median over 100 synthetically generated pairs of graphs. Nmathrmmatch was
    set to the size of the true match.
  Figure 8 Link: articels_figures_by_rev_year\2016\Geometric_Graph_Matching_Using_Monte_Carlo_Tree_Search\figure_8.jpg
  Figure 8 caption: Precision and recall as a function of the allowed processing time
    and varepsilon h , where varepsilon T = frac13 varepsilon h . For this experiment,
    the maximum amount of time for the algorithm to stop was the only termination
    criterion used. The median true varepsilon h is shown on the horizontal axes.
  Figure 9 Link: articels_figures_by_rev_year\2016\Geometric_Graph_Matching_Using_Monte_Carlo_Tree_Search\figure_9.jpg
  Figure 9 caption: Experiment evaluating the complexity of methods ATS [4], RANSAC
    [12], IPFP [29], PATH [31], CPD [20] and the proposed method (GMMC). Both axes
    use logarithmic scale.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: "Miguel Am\xE1vel Pinheiro"
  Name of the last author: Pascal Fua
  Number of Figures: 14
  Number of Tables: 2
  Number of authors: 3
  Paper title: Geometric Graph Matching Using Monte Carlo Tree Search
  Publication Date: 2016-12-06 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Results for Road Datasets: Alignment Error (Graphs Were Normalized\
      \ s.t. V A , V B \u2208[\u22121,1 ] D ), Percentage of Correct Matches in the\
      \ Solution (Precision), Percentage of Ground Truth Matches Retrieved (Recall)\
      \ and Processing Time in Seconds"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Results for Medical Images: Average Distance between True\
      \ Matches of Aligned Graphs (Graphs were Normalized s.t. V A , V B \u2208[\u2212\
      1,1 ] D ), Percentage of Correct Matches in the Solution (Precision), Percentage\
      \ of Ground Truth Matches Retrieved (Recall) and Processing Time in Seconds"
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2636200
- Affiliation of the first author: institute of information science, beijing jiaotong
    university, beijing, china
  Affiliation of the last author: department of electrical and computer engineering,
    national university of singapore, singapore
  Figure 1 Link: articels_figures_by_rev_year\2016\STC_A_Simple_to_Complex_Framework_for_WeaklySupervised_Semantic_Segmentation\figure_1.jpg
  Figure 1 caption: An illustration of the proposed simple to complex (STC) framework.
    (a) High quality saliency maps of simple images are first generated by DRFI [22]
    as the supervised foregroundbackground masks to train the Initial-DCNN using the
    proposed loss function. (b) Then, a better Enhanced-DCNN is learned, supervised
    with the segmentation masks predicted by Initial-DCNN. (c) Finally, more masks
    of complex images are predicted to train a more powerful network, called Powerful-DCNN.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\STC_A_Simple_to_Complex_Framework_for_WeaklySupervised_Semantic_Segmentation\figure_2.jpg
  Figure 2 caption: Examples of simple images and the corresponding saliency maps
    generated by DRFI on the 20 classes of PASCAL VOC.
  Figure 3 Link: articels_figures_by_rev_year\2016\STC_A_Simple_to_Complex_Framework_for_WeaklySupervised_Semantic_Segmentation\figure_3.jpg
  Figure 3 caption: Examples of segmentation results generated by I-DCNN, E-DCNN and
    P-DCNN on the PASCAL VOC 2012 val set, respectively.
  Figure 4 Link: articels_figures_by_rev_year\2016\STC_A_Simple_to_Complex_Framework_for_WeaklySupervised_Semantic_Segmentation\figure_4.jpg
  Figure 4 caption: Qualitative segmentation results on PASCAL VOC 2012 val set. Some
    failure cases are shown in the last row.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yunchao Wei
  Name of the last author: Shuicheng Yan
  Number of Figures: 4
  Number of Tables: 5
  Number of authors: 8
  Paper title: 'STC: A Simple to Complex Framework for Weakly-Supervised Semantic
    Segmentation'
  Publication Date: 2016-12-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of I-DCNN Models Trained with Different Saliency
      Maps on VOC 2012 Val Set (mIoU in Percent)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of I-DCNN Models Trained on Different Numbers of
      Images on VOC 2012 Val Set (mIoU in Percent)
  Table 3 caption:
    table_text: TABLE 3 Comparison of Different Segmentation DCNNs on VOC 2012 Val
      Set
  Table 4 caption:
    table_text: TABLE 4 Comparison of Weakly-Supervised Semantic Segmentation Methods
      on VOC 2012 Val Set
  Table 5 caption:
    table_text: TABLE 5 Comparison of Fully- and Weakly- Supervised Semantic Segmentation
      Methods on VOC 2012 Test Set
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2636150
- Affiliation of the first author: computer engineering and information technology
    department, amirkabir university of technology, tehran, iran
  Affiliation of the last author: institute for computing and information sciences,
    radboud university, nijmegen, the netherlands
  Figure 1 Link: articels_figures_by_rev_year\2016\Exploiting_Experts_Knowledge_for_Structure_Learning_of_Bayesian_Networks\figure_1.jpg
  Figure 1 caption: Simple Bayesian network structure.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Exploiting_Experts_Knowledge_for_Structure_Learning_of_Bayesian_Networks\figure_2.jpg
  Figure 2 caption: "Graphical model of the factors affecting the structure, data,\
    \ and experts' opinions ( G : Graph structure; D : Data; O : Experts' opinions;\
    \ p : Prior distribution over edge types; K : Information that determines G ;\
    \ \u03B3 : Accuracy of experts; \u03F5 : Noise). See text for further explanation."
  Figure 3 Link: articels_figures_by_rev_year\2016\Exploiting_Experts_Knowledge_for_Structure_Learning_of_Bayesian_Networks\figure_3.jpg
  Figure 3 caption: Graphical models of the experts' opinions.
  Figure 4 Link: articels_figures_by_rev_year\2016\Exploiting_Experts_Knowledge_for_Structure_Learning_of_Bayesian_Networks\figure_4.jpg
  Figure 4 caption: Graphical model of the roles of different accuracy parameters.
  Figure 5 Link: articels_figures_by_rev_year\2016\Exploiting_Experts_Knowledge_for_Structure_Learning_of_Bayesian_Networks\figure_5.jpg
  Figure 5 caption: "Decision tree for computing P( O j i \u2223 g i , \u03B3 j )\
    \ for O j i \u2260\u2205 ."
  Figure 6 Link: articels_figures_by_rev_year\2016\Exploiting_Experts_Knowledge_for_Structure_Learning_of_Bayesian_Networks\figure_6.jpg
  Figure 6 caption: The structure of the Bayesian network for breast cancer diagnosis.
  Figure 7 Link: articels_figures_by_rev_year\2016\Exploiting_Experts_Knowledge_for_Structure_Learning_of_Bayesian_Networks\figure_7.jpg
  Figure 7 caption: The mean and one standard deviation error bars for the structural
    Hamming distances obtained from Mean and Marg scores as functions of the available
    error in the input average accuracy vector for the breast cancer network with
    real experts.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Hossein Amirkhani
  Name of the last author: Arjen Hommersom
  Number of Figures: 7
  Number of Tables: 12
  Number of authors: 4
  Paper title: "Exploiting Experts\u2019 Knowledge for Structure Learning of Bayesian\
    \ Networks"
  Publication Date: 2016-12-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Some Independence Statements Derived from Models of Figs.
      2, 3, and 4
  Table 10 caption:
    table_text: TABLE 10 Table that the Radiologists had to Fill in as Part of the
      Experiment
  Table 2 caption:
    table_text: TABLE 2 Description of the Networks Used in the Simulation Experiments
  Table 3 caption:
    table_text: TABLE 3 The Accuracy Parameters Assigned to Experts in Simulated Populations
  Table 4 caption:
    table_text: TABLE 4 The True Probability Distributions Over Edge Types for the
      Bayesian Networks Used in the Simulation Experiments
  Table 5 caption:
    table_text: TABLE 5 The Obtained Structural Hamming Distances for the Asia Network
  Table 6 caption:
    table_text: TABLE 6 The Obtained Structural Hamming Distances for the Insurance
      Network
  Table 7 caption:
    table_text: TABLE 7 The Obtained Structural Hamming Distances for the Alarm Network
  Table 8 caption:
    table_text: TABLE 8 The Obtained Structural Hamming Distances for the Hailfinder
      Network
  Table 9 caption:
    table_text: TABLE 9 The Obtained Structural Hamming Distances Using the Marginalization-Based
      Score with Different Values of Coefficient c for Asia and Alarm Bayesian Networks
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2636828
