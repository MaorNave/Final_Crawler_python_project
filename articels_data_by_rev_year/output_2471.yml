- Affiliation of the first author: department of computer and information science,
    university of macau, macao, china
  Affiliation of the last author: department of computer and information science,
    university of macau, macao, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Improved_Normalized_Cut_for_MultiView_Clustering\figure_1.jpg
  Figure 1 caption: Visualization of the clustering results of BBC dataset with t-SNE
    in different views.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Improved_Normalized_Cut_for_MultiView_Clustering\figure_2.jpg
  Figure 2 caption: Visualization of the clustering results of UCI digit dataset with
    t-SNE in different views.
  Figure 3 Link: articels_figures_by_rev_year\2021\Improved_Normalized_Cut_for_MultiView_Clustering\figure_3.jpg
  Figure 3 caption: Convergence curve of our proposed algorithm on BBC, MSRC, and
    UCI digit datasets.
  Figure 4 Link: articels_figures_by_rev_year\2021\Improved_Normalized_Cut_for_MultiView_Clustering\figure_4.jpg
  Figure 4 caption: "ACC versus parameter \u03B1 on BBC, MSRC, UCI digit, and ORL\
    \ datasets."
  Figure 5 Link: articels_figures_by_rev_year\2021\Improved_Normalized_Cut_for_MultiView_Clustering\figure_5.jpg
  Figure 5 caption: "ACC versus parameter \u03B2 on BBC and MSRC datasets."
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Guo Zhong
  Name of the last author: Chi-Man Pun
  Number of Figures: 5
  Number of Tables: 8
  Number of authors: 2
  Paper title: Improved Normalized Cut for Multi-View Clustering
  Publication Date: 2021-12-21 00:00:00
  Table 1 caption: TABLE 1 Notations and Abbreviations
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison of the Time Complexities
  Table 3 caption: TABLE 3 Description of the Data Sets Used in the Experiments
  Table 4 caption: TABLE 4 Clustering Performance Comparison for Different Methods
    Over BBC Dataset
  Table 5 caption: TABLE 5 Clustering Performance Comparison for Different Methods
    Over MSRC Dataset
  Table 6 caption: TABLE 6 Clustering Performance Comparison for Different Methods
    Over UCI Digit Dataset
  Table 7 caption: TABLE 7 Clustering Performance Comparison for Different Methods
    Over Hdigit Dataset
  Table 8 caption: TABLE 8 Clustering Performance Comparison With Deep Models
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3136965
- Affiliation of the first author: department of computer science and technology,
    shanghai jiao tong university, shanghai, china
  Affiliation of the last author: sensetime research, shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2021\MonoEF_Extrinsic_Parameter_Free_Monocular_D_Object_Detection\figure_1.jpg
  Figure 1 caption: The effect of extrinsic parameter perturbations on 3D detection
    task. When the vehicle undergoes a slight pose change on an uneven road, the 3D
    detection results are less accurate (second row). This happens often in realistic
    applications and the detection offset can be viewed more evidently in the bird-eyes
    view.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\MonoEF_Extrinsic_Parameter_Free_Monocular_D_Object_Detection\figure_2.jpg
  Figure 2 caption: System overview. The Extrinsic Regression module (blue block)
    predicts the ground plane as well as vanishing point. The pose information is
    thereby obtained and then fed into the Feature Transfer module (yellow block)
    as guidance for feature enhancement. By doing so, the original features (in gray
    color) after the backbone are transferred to a rectified set of features (in yellow
    color), immune to the extrinsic parameter perturbation. The Monocular 3D Detection
    module and coordinate alignment unit follow standard procedures [30].
  Figure 3 Link: articels_figures_by_rev_year\2021\MonoEF_Extrinsic_Parameter_Free_Monocular_D_Object_Detection\figure_3.jpg
  Figure 3 caption: Ego-vehicle pose changes (in degrees) between every two frames
    in the 00-10 sequences of the KITTI odometry dataset, with the pitch angle recorded
    in the upper panel and the roll angle recorded in the lower panel.
  Figure 4 Link: articels_figures_by_rev_year\2021\MonoEF_Extrinsic_Parameter_Free_Monocular_D_Object_Detection\figure_4.jpg
  Figure 4 caption: Visualization of the 3D detection results and localization depth
    when there is an angular perturbation of 2 degrees (both pitch and roll angles).
    The one above shows the original detection results and the one below shows the
    detection results after disturbance. For the farthest target (circled by yellow
    dotted line), 2 degrees of angle change resulte in a localization deviation of
    about 4 meters.
  Figure 5 Link: articels_figures_by_rev_year\2021\MonoEF_Extrinsic_Parameter_Free_Monocular_D_Object_Detection\figure_5.jpg
  Figure 5 caption: Visualization of the extrinsic perturbation. The pose of the ego
    vehicle varies due to the unevenness of road surfaces, which is quite common in
    realistic scenarios. It causes the cameras viewport i to be inconsistent with
    ground viewport j . Therefore, the position of keypoints found from the heat map
    and depth map are shifted from ( u i , v i ) to ( u j , v j ) by extrinsic perturbation,
    leading to a confusion for the 3D prediction and thereby inaccurate results.
  Figure 6 Link: articels_figures_by_rev_year\2021\MonoEF_Extrinsic_Parameter_Free_Monocular_D_Object_Detection\figure_6.jpg
  Figure 6 caption: "The training process of the transfer network f t . The feature\
    \ target is derived from a feature obtained by the backbone after a direct extrinsic\
    \ parameter correction of the image. The pre-trained loss network \u03A6 has two\
    \ branches, one with the style target for high-dimensional losses computed on\
    \ three layers and the other with the content target for low-dimensional losses\
    \ computed only on the last layer."
  Figure 7 Link: articels_figures_by_rev_year\2021\MonoEF_Extrinsic_Parameter_Free_Monocular_D_Object_Detection\figure_7.jpg
  Figure 7 caption: Qualitative results on KITTI odometry dataset. The prediction
    3D bounding boxes of SMOKE (the one above) and our model (the one below) are shown
    under camera extrinsic perturbation in the images. Green boxes and orange boxes
    in bird view mean ground truth and predictions of cars. A more pronounced difference
    in the predictions appears where the dashed line is circled. It can be seen from
    the figure that our model is effective against the perturbation of the external
    participants, especially for depth prediction.
  Figure 8 Link: articels_figures_by_rev_year\2021\MonoEF_Extrinsic_Parameter_Free_Monocular_D_Object_Detection\figure_8.jpg
  Figure 8 caption: The qualitative accuracy of the pose detection on KITTI odometry
    dataset. The solid yellow line and the red dot indicate the detected ground plane
    and the vanishing point, respectively.
  Figure 9 Link: articels_figures_by_rev_year\2021\MonoEF_Extrinsic_Parameter_Free_Monocular_D_Object_Detection\figure_9.jpg
  Figure 9 caption: Results on subsequent scenes on KITTI odometry dataset. First
    column in the figure above shows the initial frame of Baseline and MonoEF, where
    results are comparable. However, subsequent frames (second, third columns) demonstrate
    our results are more robust (orange dashed circles) in real-world scenarios, e.g.,
    turn-around (top) and bumper (bottom) cases.
  First author gender probability: 0.69
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.71
  Name of the first author: Yunsong Zhou
  Name of the last author: Qinhong Jiang
  Number of Figures: 9
  Number of Tables: 12
  Number of authors: 6
  Paper title: 'MonoEF: Extrinsic Parameter Free Monocular 3D Object Detection'
  Publication Date: 2021-12-21 00:00:00
  Table 1 caption: TABLE 1 A P 40 AP40 Scores(%) and Runtime(s) on KITTI3D Test Set
    for Car at 0.7 IoU Threshold Referred From the KITTI Benchmark Website
  Table 10 caption: TABLE 10 A P 40 AP40 Scores(%) Evaluated on KITTI3D Test Set for
    Car at 0.7 IoU in Different Data Environments
  Table 2 caption: TABLE 2 A P 40 AP40 Scores(%) on KITTI3D Validation Set for Car
    at 0.7 IoU and 0.5 IoU Thresholds
  Table 3 caption: TABLE 3 The Specific Network Structure of the Transfer Network
  Table 4 caption: TABLE 4 A P 40 AP40 Scores(%) on KITTI3D Validation Set for Car
    at 0.5 IoU Threshold Before and After Camera Extrinsic Disturbance
  Table 5 caption: TABLE 5 Evaluation Errors on the nuScenes Test Dataset
  Table 6 caption: TABLE 6 Results on Accumulating the Proposed Modules on the KITTI
    Odometry Sequence 00
  Table 7 caption: TABLE 7 Angular Error(degm) on KITTI Odometry Validation Sequence
    08
  Table 8 caption: TABLE 8 A P 40 AP40 Scores(%) Evaluated on KITTI Odometry Sequecnce
    00 (Trained on Single Sequence 00) and Sequecnce 08 (Trained on Sequence 00-07
    & 09-10) for Car
  Table 9 caption: TABLE 9 Representative Monocular 3D Detection Methods and the Extra
    Data Used by These Methods During Training or Inference Procedure
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3136899
- Affiliation of the first author: center for machine vision and signal analysis,
    university of oulu, oulu, finland
  Affiliation of the last author: center for machine vision and signal analysis, university
    of oulu, oulu, finland
  Figure 1 Link: articels_figures_by_rev_year\2021\Hyperbolic_Deep_Neural_Networks_A_Survey\figure_1.jpg
  Figure 1 caption: "Illustration of Klein model (left two) and Poincar\xE9 model\
    \ (Right two) in the hyperbolic space. Leftmost: the relationships between Lorentz\
    \ model and Klein model. We provide the examples of straight line in Klein model\
    \ (Second from the left). Rightmost: the Poincar\xE9 model and the examples of\
    \ straight line in it. Its relationship with Lorentz model is provided in the\
    \ second from the right."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Hyperbolic_Deep_Neural_Networks_A_Survey\figure_2.jpg
  Figure 2 caption: "Illustration of Poincar\xE9 Disk (2D Poincar\xE9 model), Left:\
    \ the distances comparison between euclidean space (in black) and hyperbolic space\
    \ (in blue). Right: an example of modeling a tree using hyperbolic model."
  Figure 3 Link: articels_figures_by_rev_year\2021\Hyperbolic_Deep_Neural_Networks_A_Survey\figure_3.jpg
  Figure 3 caption: Illustration of data with different structures. Leftmost shows
    a data with cycle. Middle shows a tree-structured data and the rightmost is a
    combination.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.67
  Name of the first author: Wei Peng
  Name of the last author: Guoying Zhao
  Number of Figures: 3
  Number of Tables: 1
  Number of authors: 5
  Paper title: 'Hyperbolic Deep Neural Networks: A Survey'
  Publication Date: 2021-12-21 00:00:00
  Table 1 caption: TABLE 1 Summary of the Advanced Machine Learning Methods in the
    Hyperbolic Space
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3136921
- Affiliation of the first author: data61, commonwealth scientific and industrial
    research organisation (csiro), black mountain, act, australia
  Affiliation of the last author: school of computing, australian national university,
    canberra, act, australia
  Figure 1 Link: articels_figures_by_rev_year\2021\PnPD_A_PlugandPlay_for_D_Point_Clouds\figure_1.jpg
  Figure 1 caption: Examples of point cloud semantic segmentation on S3DIS [18] dataset.
    The 3rd column shows the results of RandLA-Net [12] (baseline), while the 4th
    column presents the results of plugging our PnP-3D module in the baseline. The
    improved areas (i.e., the points that are correctly classified by using our module
    but misclassified by the baseline) are highlighted with red color in the last
    column.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\PnPD_A_PlugandPlay_for_D_Point_Clouds\figure_2.jpg
  Figure 2 caption: Detailed structure of the PnP-3D module. The local context fusion
    block (Section 3.2) aims to fuse local geometric and feature context based on
    the geometric relations between points in 3D space. Further, the global bilinear
    regularization block (Section 3.3) can regularize the feature map by aggregating
    global perceptions based on both point-wise and channel-wise information in feature
    space. More implementation details are in Section 4.
  Figure 3 Link: articels_figures_by_rev_year\2021\PnPD_A_PlugandPlay_for_D_Point_Clouds\figure_3.jpg
  Figure 3 caption: Visualization of the feature maps in ModelNet40 [46] classification.
    The first row shows the features learned from DGCNN [9], while the second row
    is the refined output from the PnP-3D module. We normalize the channels for a
    heat-map view. It can be clearly observed that our module better illustrates the
    outlines of point cloud objects.
  Figure 4 Link: articels_figures_by_rev_year\2021\PnPD_A_PlugandPlay_for_D_Point_Clouds\figure_4.jpg
  Figure 4 caption: Examples of the generated votes in SUN RGB-D [56] object detection.
    The bounding boxes (ground-truth) are in red frames, where the red points are
    the centroids. In the last column, we can find that the votes generated by the
    PnP-3D (blue points) better approach the object centroids than the baselines outputs
    (VoteNet [16], yellow points).
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.55
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Shi Qiu
  Name of the last author: Nick Barnes
  Number of Figures: 4
  Number of Tables: 7
  Number of authors: 3
  Paper title: 'PnP-3D: A Plug-and-Play for 3D Point Clouds'
  Publication Date: 2021-12-23 00:00:00
  Table 1 caption: TABLE 1 Overall Classification Results (%) on ModelNet40 [46] and
    ScanObjectNN [47]
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Detailed Semantic Segmentation Results (Intersection-Over-Union,
    %) on the Area 5 of S3DIS [18] Dataset
  Table 3 caption: TABLE 3 Detailed Object Detection Results (Average Precision, %)
    on the Validation Set of SUN RGB-D V1 [56] Dataset
  Table 4 caption: TABLE 4 Selecting the Pooling and Regularization Strategy, Tested
    on RandLA-Net [12] and Area 5 of S3DIS [18] Dataset
  Table 5 caption: "TABLE 5 Generating the Global Bilinear Response ( \u03B7 ij \u03B7\
    ij) With Point-Wise Response ( \u03BB i \u03BBi) and Channel-Wise Response ( \u03BC\
    \ j \u03BCj), Tested on RandLA-Net [12] and Area 5 of S3DIS [18] Dataset"
  Table 6 caption: TABLE 6 Comparisons With the Attention Modules Using VoteNet [16]
    on SUN RGB-D V1 [56] Dataset
  Table 7 caption: TABLE 7 Comparisons With the Task-Specific Modules Using Different
    Baselines, Tested on ModelNet40 [46] (Classification), S3DIS [18] (Semantic Segmentation)
    and SUN RGB-D V1 [56] (Object Detection) Datasets
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3137794
- Affiliation of the first author: reler lab, aaii, faculty of engineering and information
    technology, university of technology sydney, ultimo, nsw, australia
  Affiliation of the last author: school of computer science, carnegie mellon university,
    pittsburgh, pa, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\A_Comprehensive_Survey_of_Scene_Graphs_Generation_and_Application\figure_1.jpg
  Figure 1 caption: A classification of the methods and applications of SGG.
  Figure 10 Link: articels_figures_by_rev_year\2021\A_Comprehensive_Survey_of_Scene_Graphs_Generation_and_Application\figure_10.jpg
  Figure 10 caption: Schematic diagram of object detection.
  Figure 2 Link: articels_figures_by_rev_year\2021\A_Comprehensive_Survey_of_Scene_Graphs_Generation_and_Application\figure_2.jpg
  Figure 2 caption: 'An example of scene graph construction. Upper right: The ground-truth
    scene graph of a given image. Bottom: An example of a complete scene graph.'
  Figure 3 Link: articels_figures_by_rev_year\2021\A_Comprehensive_Survey_of_Scene_Graphs_Generation_and_Application\figure_3.jpg
  Figure 3 caption: 'The causal graph of SGG. Left: An abstract representation of
    a general SGG process. Right: An example of a general SGG process.'
  Figure 4 Link: articels_figures_by_rev_year\2021\A_Comprehensive_Survey_of_Scene_Graphs_Generation_and_Application\figure_4.jpg
  Figure 4 caption: 'The basic structure of CRF-based SGG models has two parts: object
    detection ( s and o ) and relationship prediction ( r ).'
  Figure 5 Link: articels_figures_by_rev_year\2021\A_Comprehensive_Survey_of_Scene_Graphs_Generation_and_Application\figure_5.jpg
  Figure 5 caption: Examples of the sparsity and variability of visual relationships.
  Figure 6 Link: articels_figures_by_rev_year\2021\A_Comprehensive_Survey_of_Scene_Graphs_Generation_and_Application\figure_6.jpg
  Figure 6 caption: Relevant TransE-based SGG models.
  Figure 7 Link: articels_figures_by_rev_year\2021\A_Comprehensive_Survey_of_Scene_Graphs_Generation_and_Application\figure_7.jpg
  Figure 7 caption: The overall structure of UVTransEs [50] visual detection model.
  Figure 8 Link: articels_figures_by_rev_year\2021\A_Comprehensive_Survey_of_Scene_Graphs_Generation_and_Application\figure_8.jpg
  Figure 8 caption: The mainstream CNN-based SGG models.
  Figure 9 Link: articels_figures_by_rev_year\2021\A_Comprehensive_Survey_of_Scene_Graphs_Generation_and_Application\figure_9.jpg
  Figure 9 caption: Comparison of the brief schematic diagrams of three CNN-based
    SGG methods.
  First author gender probability: 0.85
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: Xiaojun Chang
  Name of the last author: Alex Hauptmann
  Number of Figures: 19
  Number of Tables: 2
  Number of authors: 6
  Paper title: 'A Comprehensive Survey of Scene Graphs: Generation and Application'
  Publication Date: 2021-12-23 00:00:00
  Table 1 caption: TABLE 1 Statistics of the Scene Graph Dataset
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Performance Comparison of the SGG Method With Graph Constraints
    on VG150. [31]
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3137605
- Affiliation of the first author: department of radiation oncology and department
    of epidemiology and biostatistics, university of california, san francisco, ca,
    usa
  Affiliation of the last author: department of radiation oncology, stanford university,
    stanford, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\Representational_Gradient_Boosting_Backpropagation_in_the_Space_of_Functions\figure_1.jpg
  Figure 1 caption: Vanilla Representational Gradient Boosting (one hidden layer).
    p features, k base functions and u updates.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Representational_Gradient_Boosting_Backpropagation_in_the_Space_of_Functions\figure_2.jpg
  Figure 2 caption: (a) RGB without feature indexing. All bases receive all input
    variables. (b) RGB with feature indexing. Input variables are grouped to provide
    prior knowledge. In both cases only one base learner function per group is represented
    (instead of 5) for clarity of the graph.
  Figure 3 Link: articels_figures_by_rev_year\2021\Representational_Gradient_Boosting_Backpropagation_in_the_Space_of_Functions\figure_3.jpg
  Figure 3 caption: Architecture of RGB combining Alexnet, a linear channel and a
    Gradient Boosting channel. The output layer is a linear combination of the concatenated
    vector of the three different algorithms.
  Figure 4 Link: articels_figures_by_rev_year\2021\Representational_Gradient_Boosting_Backpropagation_in_the_Space_of_Functions\figure_4.jpg
  Figure 4 caption: (a) Architecture of RGB when there was no indexing according to
    the lag or time stamp (b) when variables are grouped according to their lag.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Gilmer Valdes
  Name of the last author: Efstathios D. Gennatas
  Number of Figures: 4
  Number of Tables: 5
  Number of authors: 4
  Paper title: 'Representational Gradient Boosting: Backpropagation in the Space of
    Functions'
  Publication Date: 2021-12-23 00:00:00
  Table 1 caption: TABLE 1 Comparison of Stacking versus sRGB Without Noise (First
    Two Columns) and With 10% Noise (Last Two Columns)
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison of GB and nRGB Without (wo) or With(w) Grouping
    (Indexing) the Right Variables for the Different Channels
  Table 3 caption: TABLE 3 Modeling F 1 (x)+ F 2 (x)+ F 3 (x) F1(x)+F2(x)+F3(x) With
    the Same Settings as Those Used in Table 2
  Table 4 caption: TABLE 4 R2 for Each Algorithm Evaluated on the Test Set
  Table 5 caption: TABLE 5 RRMSE for Each Algorithm
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3137715
- Affiliation of the first author: department of information engineering and mathematics,
    university of siena, siena, italy
  Affiliation of the last author: department of informatics, bioengineering, robotics,
    and systems engineering, university of genova, genoa, italy
  Figure 1 Link: articels_figures_by_rev_year\2021\Domain_Knowledge_Alleviates_Adversarial_Attacks_in_MultiLabel_Classifiers\figure_1.jpg
  Figure 1 caption: Leveraging domain knowledge to improve robustness of multi-label
    classifiers. At training time, domain knowledge is used to enforce constraints
    on the learning process using unlabeled or partially-labeled data. At evaluation
    time, domain-knowledge constraints are used to detect and reject samples outside
    of the training data distribution.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Domain_Knowledge_Alleviates_Adversarial_Attacks_in_MultiLabel_Classifiers\figure_2.jpg
  Figure 2 caption: "Toy example using the domain knowledge of Eqs. (4)\u2013(7) on\
    \ 4 classes: cat (yellow), animal (blue), motorbike (green), vehicle (red). Labeledunlabeled\
    \ training data are depicted with rounded dotsgray triangles. (a,b) The decision\
    \ regions for each class are shown in two sample outcomes of the training procedure:\
    \ (a) openloose decision boundaries; (b) tightclosed decision boundaries. The\
    \ white area is associated with no predictions. Some adversarial examples (purple\
    \ arrowsdots) are detected as they end up in regions that violate the constraints.\
    \ Moreover, in (c,d) The feasibleunfeasible regions (bluegray) that fulfillviolate\
    \ the constraints for (a,b) are shown. Decision boundaries of the classes in (a,b)\
    \ are also depicted in (c,d)."
  Figure 3 Link: articels_figures_by_rev_year\2021\Domain_Knowledge_Alleviates_Adversarial_Attacks_in_MultiLabel_Classifiers\figure_3.jpg
  Figure 3 caption: Single-label classifier on a set of mutually exclusive classes
    (main classes), computing the class activations by f v and exposing them to the
    user (red path). It internally computes by f h additional predictions over auxiliary
    classes that are involved in the domain knowledge (together with the main classes).
    Training considers all the classes, Fig. 1.
  Figure 4 Link: articels_figures_by_rev_year\2021\Domain_Knowledge_Alleviates_Adversarial_Attacks_in_MultiLabel_Classifiers\figure_4.jpg
  Figure 4 caption: "Black-box attacks. Classification quality of vanilla and knowledge-constrained\
    \ models in function of \u03F5 . Dotted plots include rejection (Rej) of inputs\
    \ that are detected to be adversarial."
  Figure 5 Link: articels_figures_by_rev_year\2021\Domain_Knowledge_Alleviates_Adversarial_Attacks_in_MultiLabel_Classifiers\figure_5.jpg
  Figure 5 caption: "White-box attacks in the case of the FT classifiers. Classification\
    \ quality of vanilla and knowledge-constrained models in function of \u03F5 .\
    \ Dotted plots include rejection (Rej) of inputs that are detected to be adversarial."
  Figure 6 Link: articels_figures_by_rev_year\2021\Domain_Knowledge_Alleviates_Adversarial_Attacks_in_MultiLabel_Classifiers\figure_6.jpg
  Figure 6 caption: "Further analysis of the proposed approach in the ANIMALS dataset\
    \ ( \u03F5=0.5 ). The first two plots are about the black-box setting, and the\
    \ last two ones are about the white-box case; (a,c\u2014legend reported only on\
    \ the latter, for better readability): increasing amounts of domain knowledge\
    \ K 1 ,\u2026, K 4 ; (b,d): different values of the rejection threshold \u03C4\
    \ (from larger to smaller values, left-to-right)."
  Figure 7 Link: articels_figures_by_rev_year\2021\Domain_Knowledge_Alleviates_Adversarial_Attacks_in_MultiLabel_Classifiers\figure_7.jpg
  Figure 7 caption: Noisy domain knowledge. Analysis of the proposed approach in the
    same setup of Fig. 6, when exploiting different noisy knowledge bases K ~ a ,
    K ~ b , K ~ c (see the paper text for details). K is the original noise-free knowledge.
  Figure 8 Link: articels_figures_by_rev_year\2021\Domain_Knowledge_Alleviates_Adversarial_Attacks_in_MultiLabel_Classifiers\figure_8.jpg
  Figure 8 caption: "Adversarial data generated ( \u03F5=0.5 ) by different attacks\
    \ \u2013 ANIMALS, TL+C(Rej), black-box. Examples that are rejectednot-rejected\
    \ by the proposed knowledge-based criterion are depicted with crossescircles (\u201C\
    Clean\u201D indicates unaltered examples from the test set; the vertical line\
    \ is the reject threshold)."
  Figure 9 Link: articels_figures_by_rev_year\2021\Domain_Knowledge_Alleviates_Adversarial_Attacks_in_MultiLabel_Classifiers\figure_9.jpg
  Figure 9 caption: "Adversarial data generated ( \u03F5=0.03 ) by different attacks\
    \ \u2013 CIFAR-100, TL+C(Rej), black-box. See Fig. 8."
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Stefano Melacci
  Name of the last author: Fabio Roli
  Number of Figures: 9
  Number of Tables: 7
  Number of authors: 7
  Paper title: Domain Knowledge Alleviates Adversarial Attacks in Multi-Label Classifiers
  Publication Date: 2021-12-23 00:00:00
  Table 1 caption: TABLE 1 List of the Main Symbols and Notations
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Datasets and Details on the Experimental Setting
  Table 3 caption: "TABLE 3 Values of the Hyperparameter \u03BB \u03BB Selected via\
    \ Cross-Validation in Our Experiments"
  Table 4 caption: "TABLE 4 Values of the Constraint Loss \u03C6 \u03C6 on the Test\
    \ Data T T"
  Table 5 caption: TABLE 5 Multi-Label Classification Results in T T, for Different
    Models, Averaged Across Different Repetitions
  Table 6 caption: TABLE 6 Vulnerability Analysis (ANIMALS Dataset)
  Table 7 caption: TABLE 7 Vulnerability Analysis (CIFAR-100 Dataset)
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3137564
- Affiliation of the first author: gaoling school of artificial intelligence, and
    beijing key laboratory of big data management and analysis methods, renmin university
    of china, beijing, china
  Affiliation of the last author: gaoling school of artificial intelligence, and beijing
    key laboratory of big data management and analysis methods, renmin university
    of china, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\ClassAware_Sounding_Objects_Localization_via_Audiovisual_Correspondence\figure_1.jpg
  Figure 1 caption: 'An example of cocktail-party scenario. It contains sounding guitar,
    sounding cello, and silent saxophone. We aim to discriminatively localize the
    sounding instruments and filter out the silent ones. Video URL: https:www.youtube.comwatch?v=ebugBtNiDMI.'
  Figure 10 Link: articels_figures_by_rev_year\2021\ClassAware_Sounding_Objects_Localization_via_Audiovisual_Correspondence\figure_10.jpg
  Figure 10 caption: Influence of alternative localization-classification learning
    on the VGGSound-Synthetic, Realistic DailyLife, MUSIC-Synthetic and MUSIC-Duet
    dataset.
  Figure 2 Link: articels_figures_by_rev_year\2021\ClassAware_Sounding_Objects_Localization_via_Audiovisual_Correspondence\figure_2.jpg
  Figure 2 caption: An overview of the proposed framework. First, we use audiovisual
    correspondence as the supervision to localize sounding area and learn object representation
    (left). Then, we employ a built object dictionary to generate class-aware localization
    maps and refer to the inferred sounding area to eliminate silent objects. In that
    way, we reduce the localization task into a distribution matching problem, where
    we use KL divergence to minimize the audiovisual distribution difference (right).
  Figure 3 Link: articels_figures_by_rev_year\2021\ClassAware_Sounding_Objects_Localization_via_Audiovisual_Correspondence\figure_3.jpg
  Figure 3 caption: Details of alternative localization-classification learning scheme.
    We alternate between audiovisual localization and feature classification, where
    the localization loss L loc and classification loss L cls are back propagated,
    and there is no gradient w.r.t. the clustering objective L clu .
  Figure 4 Link: articels_figures_by_rev_year\2021\ClassAware_Sounding_Objects_Localization_via_Audiovisual_Correspondence\figure_4.jpg
  Figure 4 caption: "Details of inner-product between object representation key and\
    \ visual feature. We take the cocktail-party music scene and object representation\
    \ key of cello as an example. With image feature map f( v c i )\u2208 R C\xD7\
    H\xD7W and the cello representation key d k \u2208 R C , we first spatially replicate\
    \ the vector d k for H\xD7W times, then perform inner-product between visual feature\
    \ and replicated representation key on each spatial grid to obtain the cello localization\
    \ map m k i ."
  Figure 5 Link: articels_figures_by_rev_year\2021\ClassAware_Sounding_Objects_Localization_via_Audiovisual_Correspondence\figure_5.jpg
  Figure 5 caption: Visual feature distribution visualized by t-SNE on MUSIC-solo.
    These figures from left to right are global image features without alternative
    learning, image features with alternative learning, and masked object features
    with alternative learning. The categories are indicated in different colours.
  Figure 6 Link: articels_figures_by_rev_year\2021\ClassAware_Sounding_Objects_Localization_via_Audiovisual_Correspondence\figure_6.jpg
  Figure 6 caption: Failure case of out-of-screen sounding objects. From left to right
    are the activation of three categories in object dictionary (cello, trombone,
    and violin) to a picture that does not contain the visual information of the object
    (but with corresponding audio information).
  Figure 7 Link: articels_figures_by_rev_year\2021\ClassAware_Sounding_Objects_Localization_via_Audiovisual_Correspondence\figure_7.jpg
  Figure 7 caption: Sounding object localization results in music scenes. We visualize
    some localization results of different methods on realistic and synthetic cocktail-party
    videos. The class-aware localization maps are expected to localize objects of
    different classes and filter out silent ones. The first 3 pictures and the last
    4 pictures in each row respectively depict the location of instruments at the
    same moment in a realistic video and a synthetic video. The green box indicates
    target sounding object area, and the red box means this class of object is silent,
    and its activation value should be low.
  Figure 8 Link: articels_figures_by_rev_year\2021\ClassAware_Sounding_Objects_Localization_via_Audiovisual_Correspondence\figure_8.jpg
  Figure 8 caption: Sounding object localization results on DailyLife, Realistic MUSIC
    and VGGSound dataset. We visualize our localization results in realistic daily
    life and music scenarios. The green box indicates target sounding object area.
    Our model can distinguish different categories and respectively assign them to
    corresponding objects.
  Figure 9 Link: articels_figures_by_rev_year\2021\ClassAware_Sounding_Objects_Localization_via_Audiovisual_Correspondence\figure_9.jpg
  Figure 9 caption: Failure cases on DailyLife dataset. We visualize some failure
    cases in realistic daily life scenarios. The first row shows that our method fails
    to localize sounding object but focus on some representative background areas.
    The second row indicates that our model cannot identify the specific sounding
    instance in some complex scenes.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.53
  Name of the first author: Di Hu
  Name of the last author: Ji-Rong Wen
  Number of Figures: 13
  Number of Tables: 9
  Number of authors: 6
  Paper title: Class-Aware Sounding Objects Localization via Audiovisual Correspondence
  Publication Date: 2021-12-23 00:00:00
  Table 1 caption: TABLE 1 Localization Results on MUSIC-Synthetic, MUSIC-Duet, Realistic
    MUSIC and AudioSet-Instrument-Multi
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Localization Results on MUSIC-Solo, MUSIC 21 and AudioSet-Instrument-Solo
  Table 3 caption: TABLE 3 Single Sounding Object Localization Results on VGGSound
  Table 4 caption: TABLE 4 Multiple Sounding Object Localization Results on VGGSound-Synthetic
    and Realistic DailyLife
  Table 5 caption: TABLE 5 Comparison Between Ours and Oracle Settings
  Table 6 caption: TABLE 6 Ablation Study for the Second Stage on the VGGSound-Synthetic,
    Realistic DailyLife, MUSIC-Synthetic and MUSIC-Duet Dataset
  Table 7 caption: TABLE 7 Ablation Study on the Number of Clusters in General Cases
  Table 8 caption: TABLE 8 Influence of the Number of Missing Categories and Noise
    Rate on the MUSIC-Solo, MUSIC-Synthetic and MUSIC-Duet Dataset
  Table 9 caption: TABLE 9 Unsupervised Object Results on the Subset of ImageNet Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3137988
- Affiliation of the first author: faculty of computer and information science, university
    of ljubljana, ljubljana, slovenia
  Affiliation of the last author: faculty of computer and information science, university
    of ljubljana, ljubljana, slovenia
  Figure 1 Link: articels_figures_by_rev_year\2021\A_Discriminative_SingleShot_Segmentation_Network_for_Visual_Object_Tracking\figure_1.jpg
  Figure 1 caption: The D3S 2 tracker represents the target by two models with complementary
    geometric properties, one invariant to a broad range of transformations, including
    non-rigid deformations (GIM - geometrically invariant model), the other assuming
    a rigid object with motion well approximated by an euclidean transformation (GEM
    - geometrically constrained euclidean model). The D3S 2 , exploiting the complementary
    strengths of GIM and GEM, provides both state-of-the-art localisation and accurate
    segmentation, even in the presence of substantial deformation.
  Figure 10 Link: articels_figures_by_rev_year\2021\A_Discriminative_SingleShot_Segmentation_Network_for_Visual_Object_Tracking\figure_10.jpg
  Figure 10 caption: Success plots of the state-of-the-art trackers on OTB100 [24]
    are shown on the top-left side. Qualitative examples on three sequences from OTB100
    are shown top-right and at the bottom. Yellow bounding boxes and segmentation
    masks denote predictions made by D3S 2 , while red bounding boxes represent ground-truth.
    Note the inconsistencies and large ambiguity in ground-truth annotations.
  Figure 2 Link: articels_figures_by_rev_year\2021\A_Discriminative_SingleShot_Segmentation_Network_for_Visual_Object_Tracking\figure_2.jpg
  Figure 2 caption: The D3S 2 segmentation architecture. The backbone features are
    processed by the GEM and GIM pathways, producing the target location ( L ), foreground
    similarity ( F ) and target posterior ( P ) channels. The three channels are concatenated
    and refined into a detailed segmentation mask, which is used in SEM for robust
    target size estimation.
  Figure 3 Link: articels_figures_by_rev_year\2021\A_Discriminative_SingleShot_Segmentation_Network_for_Visual_Object_Tracking\figure_3.jpg
  Figure 3 caption: Target and background feature vectors in GIM are extracted in
    the first frame at pixel locations corresponding to the target ( X F ) and from
    the immediate neighborhood for the background ( X B ).
  Figure 4 Link: articels_figures_by_rev_year\2021\A_Discriminative_SingleShot_Segmentation_Network_for_Visual_Object_Tracking\figure_4.jpg
  Figure 4 caption: "GIM \u2013 the geometrically invariant model \u2013 features\
    \ are matched to the features in the foreground-background model X F , X B to\
    \ obtain the target ( F ) and background ( B ) similarity channels. The posterior\
    \ channel ( P ) is the softmax of F and B ."
  Figure 5 Link: articels_figures_by_rev_year\2021\A_Discriminative_SingleShot_Segmentation_Network_for_Visual_Object_Tracking\figure_5.jpg
  Figure 5 caption: "GEM \u2013 the geometrically constrained euclidean model \u2013\
    \ reduces the backbone features dimensionality and correlates them with a DCF.\
    \ The target localisation channel ( L ) is the distance transform to the maximum\
    \ correlation response, representing the per-pixel confidence of target presence."
  Figure 6 Link: articels_figures_by_rev_year\2021\A_Discriminative_SingleShot_Segmentation_Network_for_Visual_Object_Tracking\figure_6.jpg
  Figure 6 caption: "The refinement pathway combines the GIM and GEM channels and\
    \ gradually upscales them by using adjusted features from the backbone. The AP\
    \ is an average pooling for channel attention in the feature combination process.\
    \ The \u2295 is summation of two tensors and \u2297 represents multiplication\
    \ of a tensor channels with the elements from a vector. The UP \u2217 is a modified\
    \ UP layer (see the text)."
  Figure 7 Link: articels_figures_by_rev_year\2021\A_Discriminative_SingleShot_Segmentation_Network_for_Visual_Object_Tracking\figure_7.jpg
  Figure 7 caption: The target scale estimation network (SEM) adjusts the mask by
    an adjustment module and combines it with the template and search region features
    to predict per-pixel likelihood of target inherent position along with the size
    using the target classification head and target region head.
  Figure 8 Link: articels_figures_by_rev_year\2021\A_Discriminative_SingleShot_Segmentation_Network_for_Visual_Object_Tracking\figure_8.jpg
  Figure 8 caption: The mask adjustment module extracts mask features adjusted for
    scale prediction.
  Figure 9 Link: articels_figures_by_rev_year\2021\A_Discriminative_SingleShot_Segmentation_Network_for_Visual_Object_Tracking\figure_9.jpg
  Figure 9 caption: The same architecture is used in the target classification head
    and the target region head with N OUT =2 and N OUT =4 output channels, respectively.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "Alan Luke\u017Ei\u010D"
  Name of the last author: Matej Kristan
  Number of Figures: 13
  Number of Tables: 8
  Number of authors: 3
  Paper title: A Discriminative Single-Shot Segmentation Network for Visual Object
    Tracking
  Publication Date: 2021-12-23 00:00:00
  Table 1 caption: TABLE 1 Comparison With the Top-Ten Trackers on the VOT2020 Challenge
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 VOT2020 \u2013 Comparison With the Published State-of-the-Art\
    \ Trackers"
  Table 3 caption: "TABLE 3 GOT-10k Test Set \u2013 Comparison With State-of-the-Art\
    \ Trackers"
  Table 4 caption: "TABLE 4 TrackingNet Test Set \u2013 Comparison With State-of-the-Art\
    \ Trackers"
  Table 5 caption: TABLE 5 Comparison With State-of-the-Art Trackers on the LaSoT
    and OTB100 Datasets
  Table 6 caption: "TABLE 6 VOT2020 \u2013 Ablation Study"
  Table 7 caption: "TABLE 7 Comparison of the D3S 2 2 and its Variant Without the\
    \ SEM Module ( SEM \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF SEM\
    \ \xAF) on the LaSoT Dataset wrt. the 14 Visual Attributes: Aspect Ratio Change\
    \ (AR), Low Resolution (LR), Out-of-View (OV), Fast Motion (FM), Full Occlusion\
    \ (FO), Scale Variation (SV), Viewpoint Change (VC), Background Clutter (BC),\
    \ Rotation (RO), Camera Motion (CM), Motion Blur (MB), Deformation (DE), Partial\
    \ Occlusion (PO) and Illumination Variation (IL)"
  Table 8 caption: TABLE 8 State-of-the-Art Comparison on the DAVIS16 and DAVIS17
    Segmentation Datasets
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3137933
- Affiliation of the first author: school of computer science and engineering, nanjing
    university of science and technology, nanjing, china
  Affiliation of the last author: "department of computer vision and machine learning,\
    \ max planck institute for informatics, saarbr\xFCcken, germany"
  Figure 1 Link: articels_figures_by_rev_year\2021\DWDN_Deep_Wiener_Deconvolution_Network_for_NonBlind_Image_Deblurring\figure_1.jpg
  Figure 1 caption: Deblurring results on a real blurry image from [10]. A recent
    blind image deblurring method [13], based on end-to-end trainable networks, does
    not effectively estimate a clear image. With an estimated blur kernel from [14],
    non-blind image deblurring methods [11], [14], [15] generate better results (c,d,e)
    than the blind method (b). Yet, our DWDN+ method recovers still visibly clearer
    results (f).
  Figure 10 Link: articels_figures_by_rev_year\2021\DWDN_Deep_Wiener_Deconvolution_Network_for_NonBlind_Image_Deblurring\figure_10.jpg
  Figure 10 caption: "Illustration of learned deep features. (a) Blurry input. (b)\
    \ Ground truth. (c)\u2013(e) are the visualization of one channel of the blurry\
    \ input, the image gradient along the vertical direction, and the image gradient\
    \ along the horizontal direction, respectively. (f)\u2013(h) are the deconvolved\
    \ results corresponding to (c)\u2013(e). (i) and (j) visualize some features F\
    \ i y learned by DWDNs feature extraction network and the corresponding deconvolved\
    \ results F i x , respectively."
  Figure 2 Link: articels_figures_by_rev_year\2021\DWDN_Deep_Wiener_Deconvolution_Network_for_NonBlind_Image_Deblurring\figure_2.jpg
  Figure 2 caption: Deep Wiener deconvolution network. While previous work mostly
    relies on a deconvolution in the image space, our neural network first extracts
    useful feature information from the blurry input image and then conducts an explicit
    Wiener deconvolution in the (deep) feature space through Eqs. (3) and (9). A multi-scale
    cascaded encoder-decoder network progressively restores clear images, with fewer
    artifacts and finer detail. The whole network is trained in an end-to-end manner.
  Figure 3 Link: articels_figures_by_rev_year\2021\DWDN_Deep_Wiener_Deconvolution_Network_for_NonBlind_Image_Deblurring\figure_3.jpg
  Figure 3 caption: (a) Blurry image and blur kernel. (b) and (c) show the results
    of methods that perform the deconvolution in the standard image space and a deep
    feature space, respectively. (d) Ground truth. The deblurred result (c) from the
    proposed approach contains fewer artifacts and much more detail in the yellow
    and red boxes than those in (b).
  Figure 4 Link: articels_figures_by_rev_year\2021\DWDN_Deep_Wiener_Deconvolution_Network_for_NonBlind_Image_Deblurring\figure_4.jpg
  Figure 4 caption: Multi-scale feature refinement in our conference version [24].
  Figure 5 Link: articels_figures_by_rev_year\2021\DWDN_Deep_Wiener_Deconvolution_Network_for_NonBlind_Image_Deblurring\figure_5.jpg
  Figure 5 caption: "Example with simulated blur ( 1% noise level) from the dataset\
    \ of [68]. The result obtained by [16] has severe artifacts in (d). For other\
    \ methods, small-scale structures and detail are over-smoothed as shown in the\
    \ red and blue boxes of (c) and (e)\u2013(i). Compared to existing methods, our\
    \ DWDN approach can effectively preserve finer detail as shown in (j). Our improved\
    \ multi-scale cascaded feature refinement (DWDN+) can make small-scale structures\
    \ even clearer and sharper as shown in (k)."
  Figure 6 Link: articels_figures_by_rev_year\2021\DWDN_Deep_Wiener_Deconvolution_Network_for_NonBlind_Image_Deblurring\figure_6.jpg
  Figure 6 caption: "Example with simulated blur and saturated pixels from [59]. The\
    \ methods [3], [6], [9], [11], [15], [17], [50] do not generate clear images,\
    \ which contain significant artifacts in (b)\u2013(g) and (j). The methods [57],\
    \ [58] are able to handle saturated pixels, but are not effective at recovering\
    \ fine detail as shown in the green boxes of (h) and (i). In contrast, our approach\
    \ can preserve small-scale structures and detail as shown in (k)."
  Figure 7 Link: articels_figures_by_rev_year\2021\DWDN_Deep_Wiener_Deconvolution_Network_for_NonBlind_Image_Deblurring\figure_7.jpg
  Figure 7 caption: "Example with simulated blur and JPEG compression ( q=50 ). The\
    \ images deblurred by the methods [3], [6], [15], [17], [18], [45], [50], [57]\
    \ contain severe artifacts as shown in (b) and (d)\u2013(j). The result obtained\
    \ by the method [9] is over-smoothed in (e). Compared to competing methods, our\
    \ approach can generate a visibly clearer image with sharper small-scale structures\
    \ and finer detail as shown in (k)."
  Figure 8 Link: articels_figures_by_rev_year\2021\DWDN_Deep_Wiener_Deconvolution_Network_for_NonBlind_Image_Deblurring\figure_8.jpg
  Figure 8 caption: "Example with real camera shake from [14]. Compared with the results\
    \ in (b)\u2013(k), the deblurred image (f) generated by our approach is notably\
    \ clearer with finer detail and fewer artifacts."
  Figure 9 Link: articels_figures_by_rev_year\2021\DWDN_Deep_Wiener_Deconvolution_Network_for_NonBlind_Image_Deblurring\figure_9.jpg
  Figure 9 caption: Learned features F i y for blurry images with noise of different
    levels. From (a) to (d), the noise level in the blurry input becomes higher and
    the extracted features contain more noise.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jiangxin Dong
  Name of the last author: Bernt Schiele
  Number of Figures: 13
  Number of Tables: 13
  Number of authors: 3
  Paper title: 'DWDN: Deep Wiener Deconvolution Network for Non-Blind Image Deblurring'
  Publication Date: 2021-12-28 00:00:00
  Table 1 caption: TABLE 1 Quantitative Comparison to State-of-the-Art Methods on
    the Dataset of Levin et al. [66] With Gaussian Noise of 1%,3% 1%,3%, and 5% 5%
  Table 10 caption: TABLE 10 Robustness of the Proposed Deep Wiener Devoncolution
    Network to Various Noise Levels, Evaluated on the Dataset of Levin et al. [66]
    (PSNR in dBSSIM)
  Table 2 caption: TABLE 2 Quantitative Comparison to State-of-the-Art Methods on
    the Dataset of [67] With Gaussian Noise of 1%,3% 1%,3%, and 5% 5%
  Table 3 caption: TABLE 3 Quantitative Comparison to State-of-the-Art Methods on
    the Dataset of [68] With Gaussian Noise of 1%,3% 1%,3%, and 5% 5%
  Table 4 caption: TABLE 4 Quantitative Comparison to State-of-the-Art Methods on
    a Dataset With Saturated Pixels (See Text for Details)
  Table 5 caption: TABLE 5 Quantitative Comparison to State-of-the-Art Methods on
    the Datasets of [67] With Gaussian Noise and JPEG Compression
  Table 6 caption: TABLE 6 Effectiveness of the Feature-Based Wiener Deconvolution
  Table 7 caption: TABLE 7 Effectiveness of the Deep Features Learned by Our Piece-Wise
    Linear Feature Extraction Network, Evaluated on the Datasets of [66], [67], and
    [68] (PSNR in dBSSIM)
  Table 8 caption: TABLE 8 Effect of the Multi-Scale Configuration (PSNR in dBSSIM)
  Table 9 caption: TABLE 9 Effectiveness of the Cascade Architecture
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3138787
