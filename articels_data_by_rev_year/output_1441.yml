- Affiliation of the first author: department of electrical engineering and computer
    science, peking university, beijing, china
  Affiliation of the last author: department of electrical engineering and computer
    science, peking university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2019\PoseGuided_Representation_Learning_for_Person_ReIdentification\figure_1.jpg
  Figure 1 caption: 'Illustration of the two fundamental challenging issues of person
    ReID: (a) the large pose variations and (b) the misalignment errors in detected
    bounding boxes. In (a), each body part exhibits substantial movements between
    two images. In (b), direct matching image regions at the same location is not
    reasonable for person ReID.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\PoseGuided_Representation_Learning_for_Person_ReIdentification\figure_2.jpg
  Figure 2 caption: CNN architecture for Pose Invariant Feature (PIF) learning. PIF
    is extracted from the global image. It is trained by the softmax loss and the
    mutual loss computed with a visual feature learned from the pose-normalized image.
    GAP denotes Global Average Pooling [17].
  Figure 3 Link: articels_figures_by_rev_year\2019\PoseGuided_Representation_Learning_for_Person_ReIdentification\figure_3.jpg
  Figure 3 caption: CNN architecture for Local Descriptive Feature (LDF) learning.
    PIF is extracted from the global image. It is trained with the softmax loss. The
    CNN for LDF extraction is co-trained in the regional feature learning.
  Figure 4 Link: articels_figures_by_rev_year\2019\PoseGuided_Representation_Learning_for_Person_ReIdentification\figure_4.jpg
  Figure 4 caption: Illustration of pose normalization and body parts extraction.
    (a) is the original input image. (b) shows 14 body joints estimated by pose estimation.
    (c) shows 6 body part regions. (d) shows rotated and resized part regions. (e)
    shows pose-normalized part regions. (f) and (g) show 3 cropped body regions.
  Figure 5 Link: articels_figures_by_rev_year\2019\PoseGuided_Representation_Learning_for_Person_ReIdentification\figure_5.jpg
  Figure 5 caption: Statistics of the positive-negative distance ratio computed on
    10,000 randomly sampled image triplets from CUHK03 [1]. Each triplet contains
    two positive images from a same person with different poses and one negative image
    from another person. The results of PIF and baseline GoogLeNet [49] feature are
    compared in (a) and (b), respectively.
  Figure 6 Link: articels_figures_by_rev_year\2019\PoseGuided_Representation_Learning_for_Person_ReIdentification\figure_6.jpg
  Figure 6 caption: Saliency maps generated by GoogLeNet and multiple variants of
    LDF network. (a) is the original image. (b) shows the saliency maps generated
    by GoogLeNet. (c)-(e) co-trains GoogLeNet with one additional regional branch
    on head, upper body, and lower body, respectively. (f) denotes our LDF learning
    network in Fig 3. Dark red denotes higher salience. Compared with the competitors,
    LDF network can better depict the person foregrounds.
  Figure 7 Link: articels_figures_by_rev_year\2019\PoseGuided_Representation_Learning_for_Person_ReIdentification\figure_7.jpg
  Figure 7 caption: Illustration of person ReID results on Market1501 and MSMT17 datasets.
    Each example shows the top-5 retrieved images by the baseline GoogLeNet [49],
    PDC [15] and PGR in each row. True positive is annotated by the green bounding
    box. False positive is annotated by the red bounding box.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Jianing Li
  Name of the last author: Wen Gao
  Number of Figures: 7
  Number of Tables: 11
  Number of authors: 5
  Paper title: Pose-Guided Representation Learning for Person Re-Identification
  Publication Date: 2019-07-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Correspondence between Body Parts and Detected Joints
  Table 10 caption:
    table_text: TABLE 10 Comparison on MSMT17 Dataset
  Table 2 caption:
    table_text: TABLE 2 Evaluation of Different Features and Pose Estimation Methods
      on Market1501
  Table 3 caption:
    table_text: TABLE 3 Evaluation of Several Variants of PIF Learning Strategy on
      Market1501
  Table 4 caption:
    table_text: TABLE 4 Evaluation of Different Weight Share Strategies in LDF Learning
      on Market1501
  Table 5 caption:
    table_text: TABLE 5 Person ReID Performance of PIF, LDF, and PGR on Five Datasets
  Table 6 caption:
    table_text: TABLE 6 Comparisons on CUHK03 Labeled and Detected Dataset
  Table 7 caption:
    table_text: TABLE 7 Comparison on Market1501 Dataset
  Table 8 caption:
    table_text: TABLE 8 Comparison on VIPeR Dataset
  Table 9 caption:
    table_text: TABLE 9 Comparison on DukeMTMC-reID Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2929036
- Affiliation of the first author: pattern analysis & computer vision, istituto italiano
    di tecnologia, genova, italy
  Affiliation of the last author: pattern analysis & computer vision, istituto italiano
    di tecnologia, genova, italy
  Figure 1 Link: articels_figures_by_rev_year\2019\Learning_with_Privileged_Information_via_Adversarial_Discriminative_Modality_Dis\figure_1.jpg
  Figure 1 caption: What is the best way of using all data available at training time,
    considering a missing (or noisy) modality at test time?
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Learning_with_Privileged_Information_via_Adversarial_Discriminative_Modality_Dis\figure_2.jpg
  Figure 2 caption: 'Architecture and training steps (solid lines - module is trained;
    dashed lines - module is frozen). Step 1: Separate pretraining of RGB and Depth
    networks (Resnet-50 backbone with temporal convolutions). The bottleneck described
    in Section 3.2 is highlighted as a separate component. At test time the raw predictions
    (logits) of the two separate streams are simply averaged. The complementary information
    carried by the two streams bring a significant boost in the recognition performance.
    Step 2: The depth stream is frozen. The hallucination stream H is initialized
    with the depth streams weights and adversarially trained against a discriminator.
    The discriminator is fed with the concatenation of the bottleneck feature vector
    and the temporal frame ordering label y t , as detailed in Section 3.1. The discriminator
    also features an additional classification task, i.e., not only it is trained
    to discriminate between hallucinated and depth features, but also to assign samples
    to the correct class (Eq. (2)). The hallucination stream thus learns monocular
    depth features from the depth stream while maintaining discriminative power. At
    test time, predictions from the RGB and the hallucination streams are fused.'
  Figure 3 Link: articels_figures_by_rev_year\2019\Learning_with_Privileged_Information_via_Adversarial_Discriminative_Modality_Dis\figure_3.jpg
  Figure 3 caption: Detail of the ResNet residual unit with temporal convolutions
    (blue block).
  Figure 4 Link: articels_figures_by_rev_year\2019\Learning_with_Privileged_Information_via_Adversarial_Discriminative_Modality_Dis\figure_4.jpg
  Figure 4 caption: 'Architectures for the discriminators used for the two different
    tasks. Left: D1 for object recognition. Right: D2 for action recognition.'
  Figure 5 Link: articels_figures_by_rev_year\2019\Learning_with_Privileged_Information_via_Adversarial_Discriminative_Modality_Dis\figure_5.jpg
  Figure 5 caption: Examples of RGB and depth frames from the NYUD (RGB-D) dataset.
  Figure 6 Link: articels_figures_by_rev_year\2019\Learning_with_Privileged_Information_via_Adversarial_Discriminative_Modality_Dis\figure_6.jpg
  Figure 6 caption: Discriminator confidence at predicting fake label as a function
    of noise in the depth frames. The more corrupted the frame, the more confident
    D , and the lower the accuracy of the Two-stream model (NYUD dataset).
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Nuno C. Garcia
  Name of the last author: Vittorio Murino
  Number of Figures: 6
  Number of Tables: 6
  Number of authors: 3
  Paper title: Learning with Privileged Information via Adversarial Discriminative
    Modality Distillation
  Publication Date: 2019-07-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Ablation Study - Bottleneck Size
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Ablation Study - Investigating Different Bottleneck Implementations
  Table 3 caption:
    table_text: TABLE 3 Ablation Study - Investigating Different Inputs and Tasks
      for the Discriminator
  Table 4 caption:
    table_text: TABLE 4 Classification Accuracies and Comparisons with the State of
      the Art for Video Action Recognition
  Table 5 caption:
    table_text: TABLE 5 Object Recognition
  Table 6 caption:
    table_text: TABLE 6 Accuracy Values for the Two-Stream Model Trained on RGB and
      Depth, and Tested with RGB and Noisy Depth Data
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2929038
- Affiliation of the first author: department of electrical and computer engineering,
    national university of singapore, singapore
  Affiliation of the last author: ponyai, fremont, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\D_Rigid_Motion_Segmentation_with_Mixed_and_Unknown_Number_of_Models\figure_1.jpg
  Figure 1 caption: Illustration of slicing effect of homography. (a-b) Red dots indicate
    inlier points of a hypothesis. All points lie on a virtual plane (a slice of the
    cube) highlighted in yellow. (c) Virtual planes are highlighted as triangles with
    points in the same color as inliers.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\D_Rigid_Motion_Segmentation_with_Mixed_and_Unknown_Number_of_Models\figure_2.jpg
  Figure 2 caption: Hypothesized F from background erroneously captures foreground
    as inliers.
  Figure 3 Link: articels_figures_by_rev_year\2019\D_Rigid_Motion_Segmentation_with_Mixed_and_Unknown_Number_of_Models\figure_3.jpg
  Figure 3 caption: Example frames of KT3DMoSeg dataset with trajectories overlapped.
    The color indicates different motion and outliers.
  Figure 4 Link: articels_figures_by_rev_year\2019\D_Rigid_Motion_Segmentation_with_Mixed_and_Unknown_Number_of_Models\figure_4.jpg
  Figure 4 caption: (a) The classification error on individual sequence of KT3DMoSeg
    dataset. (b-c) The impact of regularization parameters on co-regularization and
    subset constrained clustering performances. (d) The distribution of point number
    for each sequence.
  Figure 5 Link: articels_figures_by_rev_year\2019\D_Rigid_Motion_Segmentation_with_Mixed_and_Unknown_Number_of_Models\figure_5.jpg
  Figure 5 caption: Examples of motion segmentation on KT3DMoSeg sequences.
  Figure 6 Link: articels_figures_by_rev_year\2019\D_Rigid_Motion_Segmentation_with_Mixed_and_Unknown_Number_of_Models\figure_6.jpg
  Figure 6 caption: Original affinity matrix versus reconstructed affinity and residuals
    of model selection criteria.
  Figure 7 Link: articels_figures_by_rev_year\2019\D_Rigid_Motion_Segmentation_with_Mixed_and_Unknown_Number_of_Models\figure_7.jpg
  Figure 7 caption: Qualitative examples of 6 sequences selected from FBMS59.
  Figure 8 Link: articels_figures_by_rev_year\2019\D_Rigid_Motion_Segmentation_with_Mixed_and_Unknown_Number_of_Models\figure_8.jpg
  Figure 8 caption: Qualitative examples of all 5 sequences of the ComplexBackground
    dataset.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.66
  Name of the first author: Xun Xu
  Name of the last author: Zhuwen Li
  Number of Figures: 8
  Number of Tables: 6
  Number of authors: 3
  Paper title: 3D Rigid Motion Segmentation with Mixed and Unknown Number of Models
  Publication Date: 2019-07-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Motion Segmentation Results on Hopkins155, Hopkins12, MTPV62
      and KT3DMoSeg Datasets Evaluated as Classification Error ( % %)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Model Selection Results on Hopkins 155 Dataset
  Table 3 caption:
    table_text: TABLE 3 Model Selection Performance on MTPV62, KT3DMoSeg and Hopkins12
  Table 4 caption:
    table_text: TABLE 4 Comparisons on the First 10-Frame Subset of FBMS59 and the
      ComplexBackground Dataset
  Table 5 caption:
    table_text: TABLE 5 Evaluation of Runtime on Each Component of Our Pipeline
  Table 6 caption:
    table_text: TABLE 6 The Impact of Different Combinations of Multi-Model Fusion
      Schemes on Model Selection Task
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2929146
- Affiliation of the first author: berkeley artificial intelligence research lab (bair),
    university of california, berkeley, ca, usa
  Affiliation of the last author: robotics institute, carnegie mellon university,
    pittsburgh, pa, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\OpenPose_Realtime_MultiPerson_D_Pose_Estimation_Using_Part_Affinity_Fields\figure_1.jpg
  Figure 1 caption: 'Top: Multi-person pose estimation. Body parts belonging to the
    same person are linked, including foot keypoints (big toes, small toes, and heels).
    Bottom left: Part Affinity Fields (PAFs) corresponding to the limb connecting
    right elbow and wrist. The color encodes orientation. Bottom right: A 2D vector
    in each pixel of every PAF encodes the position and orientation of the limbs.'
  Figure 10 Link: articels_figures_by_rev_year\2019\OpenPose_Realtime_MultiPerson_D_Pose_Estimation_Using_Part_Affinity_Fields\figure_10.jpg
  Figure 10 caption: Keypoint annotation configuration for the 3 datasets.
  Figure 2 Link: articels_figures_by_rev_year\2019\OpenPose_Realtime_MultiPerson_D_Pose_Estimation_Using_Part_Affinity_Fields\figure_2.jpg
  Figure 2 caption: Overall pipeline. (a) Our method takes the entire image as the
    input for a CNN to jointly predict (b) confidence maps for body part detection
    and (c) PAFs for part association. (d) The parsing step performs a set of bipartite
    matchings to associate body part candidates. (e) We finally assemble them into
    full body poses for all people in the image.
  Figure 3 Link: articels_figures_by_rev_year\2019\OpenPose_Realtime_MultiPerson_D_Pose_Estimation_Using_Part_Affinity_Fields\figure_3.jpg
  Figure 3 caption: Architecture of the multi-stage CNN. The first set of stages predicts
    PAFs L t , while the last set predicts confidence maps S t . The predictions of
    each stage and their corresponding image features are concatenated for each subsequent
    stage. Convolutions of kernel size 7 from the original approach [3] are replaced
    with 3 layers of convolutions of kernel 3 which are concatenated at their end.
  Figure 4 Link: articels_figures_by_rev_year\2019\OpenPose_Realtime_MultiPerson_D_Pose_Estimation_Using_Part_Affinity_Fields\figure_4.jpg
  Figure 4 caption: PAFs of right forearm across stages. Although there is confusion
    between left and right body parts and limbs in early stages, the estimates are
    increasingly refined through global inference in later stages.
  Figure 5 Link: articels_figures_by_rev_year\2019\OpenPose_Realtime_MultiPerson_D_Pose_Estimation_Using_Part_Affinity_Fields\figure_5.jpg
  Figure 5 caption: 'Part association strategies. (a) The body part detection candidates
    (red and blue dots) for two body part types and all connection candidates (grey
    lines). (b) The connection results using the midpoint (yellow dots) representation:
    correct connections (black lines) and incorrect connections (green lines) that
    also satisfy the incidence constraint. (c) The results using PAFs (yellow arrows).
    By encoding position and orientation over the support of the limb, PAFs eliminate
    false associations.'
  Figure 6 Link: articels_figures_by_rev_year\2019\OpenPose_Realtime_MultiPerson_D_Pose_Estimation_Using_Part_Affinity_Fields\figure_6.jpg
  Figure 6 caption: Graph matching. (a) Original image with part detections. (b) K
    -partite graph. (c) Tree structure. (d) A set of bipartite graphs.
  Figure 7 Link: articels_figures_by_rev_year\2019\OpenPose_Realtime_MultiPerson_D_Pose_Estimation_Using_Part_Affinity_Fields\figure_7.jpg
  Figure 7 caption: Importance of redundant PAF connections. (a) Two different people
    are wrongly merged due to a wrong neck-nose connection. (b) The higher confidence
    of the right ear-shoulder connection avoids the wrong nose-neck link.
  Figure 8 Link: articels_figures_by_rev_year\2019\OpenPose_Realtime_MultiPerson_D_Pose_Estimation_Using_Part_Affinity_Fields\figure_8.jpg
  Figure 8 caption: Output of OpenPose, detecting body, foot, hand, and facial keypoints
    in real-time. OpenPose is robust against occlusions including during human-object
    interaction.
  Figure 9 Link: articels_figures_by_rev_year\2019\OpenPose_Realtime_MultiPerson_D_Pose_Estimation_Using_Part_Affinity_Fields\figure_9.jpg
  Figure 9 caption: Foot keypoint analysis. (a) Foot keypoint annotations, consisting
    of big toes, small toes, and heels. (b) Body-only model example at which right
    ankle is not properly estimated. (c) Analogous body+foot model example, the foot
    information helps predict the right ankle location.
  First author gender probability: 0.83
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Zhe Cao
  Name of the last author: Yaser Sheikh
  Number of Figures: 17
  Number of Tables: 9
  Number of authors: 5
  Paper title: 'OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity
    Fields'
  Publication Date: 2019-07-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Results on the MPII Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Different Structures on Our Custom Validation
      Set
  Table 3 caption:
    table_text: "TABLE 3 COCO Test-Dev Leaderboard [73], \u201C\u201D Indicates That\
      \ No Citation Was Provided"
  Table 4 caption:
    table_text: TABLE 4 Self-Comparison Experiments on the COCO Validation Set
  Table 5 caption:
    table_text: TABLE 5 Self-Comparison Experiments on the COCO Validation Set
  Table 6 caption:
    table_text: TABLE 6 Runtime Difference between the 3 Models Released in OpenPose
      with CUDA and CPU-Only Versions, Running in a NVIDIA GeForce GTX-1080 Ti GPU
      and a i7-6850K CPU
  Table 7 caption:
    table_text: TABLE 7 Foot Keypoint Analysis on the Foot Validation Set
  Table 8 caption:
    table_text: TABLE 8 Self-Comparison Experiments for Body on the COCO Validation
      Set
  Table 9 caption:
    table_text: TABLE 9 Vehicle Keypoint Validation Set
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2929257
- Affiliation of the first author: school of computer science, wuhan university, wuhan,
    china
  Affiliation of the last author: college of computer science, nanjing university
    of science and technology, nanjing, china
  Figure 1 Link: articels_figures_by_rev_year\2019\Multiset_Feature_Learning_for_Highly_Imbalanced_Data_Classification\figure_1.jpg
  Figure 1 caption: G-mean of highly imbalanced learning methods on Abalone19 dataset
    with increasing IR from 1:1 to 129:1.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Multiset_Feature_Learning_for_Highly_Imbalanced_Data_Classification\figure_2.jpg
  Figure 2 caption: 'The distribution of feature space on PC1 and Yeast7 datasets:
    (a) Results on PC1, (b) Results on Yeast7.'
  Figure 3 Link: articels_figures_by_rev_year\2019\Multiset_Feature_Learning_for_Highly_Imbalanced_Data_Classification\figure_3.jpg
  Figure 3 caption: 'The results of classification accuracy and similarity for each
    subset on PC1, Pageblock, Glass5 and Yeast7 datasets( X -axis represents the index
    of the subset): (a) Results on PC1, (b) Results on Pageblock, (c) Results on Glass5,
    (d) Results on Yeast7.'
  Figure 4 Link: articels_figures_by_rev_year\2019\Multiset_Feature_Learning_for_Highly_Imbalanced_Data_Classification\figure_4.jpg
  Figure 4 caption: Illustration of multiple sets construction strategy.
  Figure 5 Link: articels_figures_by_rev_year\2019\Multiset_Feature_Learning_for_Highly_Imbalanced_Data_Classification\figure_5.jpg
  Figure 5 caption: 'The results of similarity for each subset on PC1, Pageblock,
    Glass5 and Yeast7 datasets( X -axis represents the index of the subset, Y -axis
    represents the value of JSD, R-MP represents randomly multiset partition, G-MP
    represents GAN-based multiset construction strategy): (a) Results on PC1, (b)
    Results on Pageblock, (c) Results on Glass5, (d) Results on Yeast7.'
  Figure 6 Link: articels_figures_by_rev_year\2019\Multiset_Feature_Learning_for_Highly_Imbalanced_Data_Classification\figure_6.jpg
  Figure 6 caption: The flowchart of DM-UCML.
  Figure 7 Link: articels_figures_by_rev_year\2019\Multiset_Feature_Learning_for_Highly_Imbalanced_Data_Classification\figure_7.jpg
  Figure 7 caption: G-mean of highly imbalanced learning methods on Abalone19 dataset
    with increasing IR from 1:1 to 129:1.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.76
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.66
  Name of the first author: Xiao-Yuan Jing
  Name of the last author: Jing-Yu Yang
  Number of Figures: 7
  Number of Tables: 13
  Number of authors: 8
  Paper title: Multiset Feature Learning for Highly Imbalanced Data Classification
  Publication Date: 2019-07-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Cost Matrix for UCML
  Table 10 caption:
    table_text: TABLE 10 G-mean Values of UCML, UCML+g, DM-UCML and DM-UCML-g
  Table 2 caption:
    table_text: TABLE 2 Data Descriptions Used in the Experiment
  Table 3 caption:
    table_text: TABLE 3 Four Kinds of Classification Results
  Table 4 caption:
    table_text: TABLE 4 Experimental Results on the PC1, LC, Pageblock, and Kddcup
      Datasets
  Table 5 caption:
    table_text: TABLE 5 Experimental Results on the Glass5, Shuttle0vs4, Yeast7, Abalone19
      Datasets
  Table 6 caption:
    table_text: TABLE 6 CAcc Results of Each of the 40 Face Attributes on CelebA Dataset
  Table 7 caption:
    table_text: TABLE 7 G-mean Results of Different Twelve Partitions on X-Domain
      Dataset
  Table 8 caption:
    table_text: TABLE 8 Experimental Results of DM-UCML and UCML
  Table 9 caption:
    table_text: TABLE 9 Experimental Results of DM-UCML and the Related Deep Metric
      Learning Based Methods on Traditional Datasets
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2929166
- Affiliation of the first author: "department of electrical engineering, link\xF6\
    ping university, link\xF6ping, sweden"
  Affiliation of the last author: "department of electrical engineering, link\xF6\
    ping university, link\xF6ping, sweden"
  Figure 1 Link: articels_figures_by_rev_year\2019\Confidence_Propagation_through_CNNs_for_Guided_Sparse_Depth_Regression\figure_1.jpg
  Figure 1 caption: Our scene depth completion pipeline on an example image from the
    KITTI-Depth dataset [1]. The input to the pipeline is a very sparse projected
    LiDAR point cloud, an input confidence map which has zeros at missing pixels and
    ones otherwise, and an RGB image. The sparse point cloud input and the input confidence
    are fed to a multi-scale unguided network that acts as a generic estimator for
    the data. Afterwards, the continuous output confidence map is concatenated with
    the RGB image and fed to a feature extraction network. The output from the unguided
    network and the RGB feature extraction networks are concatenated and fed to a
    fusion network which produces the final dense depth map. [Images were dilated
    for visual clarity ]
  Figure 10 Link: articels_figures_by_rev_year\2019\Confidence_Propagation_through_CNNs_for_Guided_Sparse_Depth_Regression\figure_10.jpg
  Figure 10 caption: The effect of varying the degree of sparsity in the NYU-Depth-v2
    dataset [10] on our proposed method. EncDec-Net[EF] performs very well with different
    degrees of sparsity, while EncDec-Net[EF] performs slightly worse with high degrees
    of sparsity.
  Figure 2 Link: articels_figures_by_rev_year\2019\Confidence_Propagation_through_CNNs_for_Guided_Sparse_Depth_Regression\figure_2.jpg
  Figure 2 caption: (a) Examples for differentiable functions with non-negative co-domain,
    (b) Applying the SoftPlus function to a 2D surface preserves the surface trend.
  Figure 3 Link: articels_figures_by_rev_year\2019\Confidence_Propagation_through_CNNs_for_Guided_Sparse_Depth_Regression\figure_3.jpg
  Figure 3 caption: "An illustration of the Normalized Convolution layer that takes\
    \ in two inputs: data and confidence. The Normalized Convolution layer outputs\
    \ a data term and a confidence term. Convolution is denoted as \u2217 , the Hadamard\
    \ product (point-wise) as \u2299 , summation as \u03A3 , and point-wise inverse\
    \ as 1x ."
  Figure 4 Link: articels_figures_by_rev_year\2019\Confidence_Propagation_through_CNNs_for_Guided_Sparse_Depth_Regression\figure_4.jpg
  Figure 4 caption: "Our proposed multi-scale architecture for the task of unguided\
    \ scene depth completion that utilizes normalized convolution layers. Downsampling\
    \ is performed using max pooling on confidence maps and the indices of the pooled\
    \ pixels are used to select the pixels with highest confidences from the feature\
    \ maps. Different scales are fused by upsampling the coarser scale and concatenating\
    \ it with the finer scale. A normalized convolution layer is then used to fuse\
    \ the feature maps based on the confidence information. Finally, a 1 \xD7 1 normalized\
    \ convolution layer is used to merge different channels into one channel and produce\
    \ a dense output and an output confidence map."
  Figure 5 Link: articels_figures_by_rev_year\2019\Confidence_Propagation_through_CNNs_for_Guided_Sparse_Depth_Regression\figure_5.jpg
  Figure 5 caption: An example for the spatial error distribution of the output from
    our unguided normalized convolution network on the task of depth completion. (d)
    shows that the error is distributed around edges.
  Figure 6 Link: articels_figures_by_rev_year\2019\Confidence_Propagation_through_CNNs_for_Guided_Sparse_Depth_Regression\figure_6.jpg
  Figure 6 caption: 'An example of the output confidence from our unguided normalized
    convolution network on the task of depth completion. Images on the left are from
    top-to-bottom: sparse input, the dense output from the unguided normalized convolution
    network and the output confidence. The plot on the right shows the corresponding
    values for row 217. The red crosses are the sample points from the sparse input,
    the blue curve is the dense prediction and the orange curve is the output confidence
    (smoothed). It is shown that regions with high density of sample points tend to
    have a higher confidence. Note that all values are normalized to [0;1].'
  Figure 7 Link: articels_figures_by_rev_year\2019\Confidence_Propagation_through_CNNs_for_Guided_Sparse_Depth_Regression\figure_7.jpg
  Figure 7 caption: (a) A multi-stream architecture that contains a stream for depth
    and another stream for RGB+Output Confidence feature extraction. Afterwards, a
    fusion network combines both streams to produce the final dense output. (d) A
    multi-scale encoder-decoder architecture where depth is fed to the unguided network
    followed by an encoder and output confidence and RGB image are concatenated then
    fed to a similar encoder. Both streams have skip-connection to the decoder between
    the corresponding scales. (c) is similar to (a), but with early fusion and (d)
    is similar to (b) but with early fusion.
  Figure 8 Link: articels_figures_by_rev_year\2019\Confidence_Propagation_through_CNNs_for_Guided_Sparse_Depth_Regression\figure_8.jpg
  Figure 8 caption: Some qualitative examples for the top three performing methods
    from the KITTI-Depth dataset [1] on the task of scene depth completion. (a) RGB
    input, (b) Our method MS-Net[LF]-L2 (gd), (c) Sparse-to-Dense (gd) [8] and (d)
    HMS-Net (gd) [6]. For each method, the top image is the prediction and the lower
    image is the error. Our method MS-Net[LF]-L2 (gd) performs slightly better in
    handling outliers as highlighted with the yellow boxes, while Sparse-to-Dense
    produces smoother edges due to the use of a smoothness loss. Note that this figure
    is best viewed on screens.
  Figure 9 Link: articels_figures_by_rev_year\2019\Confidence_Propagation_through_CNNs_for_Guided_Sparse_Depth_Regression\figure_9.jpg
  Figure 9 caption: Some qualitative results on the NYU-Depth-v2 dataset with 200
    randomly sampled depth samples as the input. (a) RGB input, (b) The groundtruth
    depth, (c) our EncDec-Net[EF] results, (d) our MS-Net[LF] results, and (e) Sparse-to-Dense
    [25] results.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Abdelrahman Eldesokey
  Name of the last author: Fahad Shahbaz Khan
  Number of Figures: 14
  Number of Tables: 7
  Number of authors: 3
  Paper title: Confidence Propagation through CNNs for Guided Sparse Depth Regression
  Publication Date: 2019-07-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 A Categorization of the State-of-the-Art Methods Depending
      on the Architecture and the Fusion Scheme
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 A Quantitative Comparison between Different Fusion Schemes
      (Described in Section 5) on the Selected validation set of the KITTI-Depth Dataset
      [1]
  Table 3 caption:
    table_text: TABLE 3 Quantitative Results for Methods in Comparison on the KITTI-Depth
      Benchmark [1]
  Table 4 caption:
    table_text: TABLE 4 Quantitative Results for Methods in Comparison on the NYU-Depth-v2
      Dataset [10]
  Table 5 caption:
    table_text: TABLE 5 The Impact of Enforcing Non-Negativity on Normalized Convolution
      Layers
  Table 6 caption:
    table_text: TABLE 6 DS Refers to the Used Dataset, K Is the KITTI-Depth, N Is
      the NYU-Depth-v2
  Table 7 caption:
    table_text: TABLE 7 Number of Parameters and Runtime for Some Methods in Comparison
      (Lower Is Better)
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2929170
- Affiliation of the first author: national key laboratory of science and technology
    on blind signal processing, chengdu, sichuan, p. r. china
  Affiliation of the last author: college of computer, national university of defense
    technology, changsha, hunan, p. r. china
  Figure 1 Link: articels_figures_by_rev_year\2019\Stereo_Matching_Using_MultiLevel_Cost_Volume_and_MultiScale_Feature_Constancy\figure_1.jpg
  Figure 1 caption: "An illustration of the process to reconstruct the left image\
    \ from the right image using the estimated disparity. I L (i,j) and I R (i+ d\
    \ ij ,j) are corresponding pixels with an groundtruth disparity of d ij . A correctly\
    \ estimated disparity d ij (i.e., d ij = d ij ) can be used to reconstruct the\
    \ pixel in the left image using its corresponding pixel in the right image. In\
    \ contrast, an incorrect disparity (i.e., d ij \u2260 d ij ) will produce wrong\
    \ reconstruction at location (i,j) ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Stereo_Matching_Using_MultiLevel_Cost_Volume_and_MultiScale_Feature_Constancy\figure_2.jpg
  Figure 2 caption: "The architecture of our proposed network. A siamese network with\
    \ a shallow encoder-decoder architecture is used to extract shared multi-scale\
    \ features from left and right images. Part of these features are used to calculate\
    \ the cost volume (i.e., correlation, shown in green) for both DES-net and DRS-net.\
    \ Features of the first layer are further compressed to produce cconv1a and cconv1b\
    \ using 1\xD71 convolution. These shared features are also used to calculate reconstruction\
    \ errors for DRS-net. Skip connections are used between all encoder-decoders but\
    \ are omitted for better visualization."
  Figure 3 Link: articels_figures_by_rev_year\2019\Stereo_Matching_Using_MultiLevel_Cost_Volume_and_MultiScale_Feature_Constancy\figure_3.jpg
  Figure 3 caption: "The implementation details of cost volume calculation at multiple\
    \ levels and other alternative multi-scale schemes. \u201CConv2a\u201D represents\
    \ the second convolution layer of the shared features extraction module."
  Figure 4 Link: articels_figures_by_rev_year\2019\Stereo_Matching_Using_MultiLevel_Cost_Volume_and_MultiScale_Feature_Constancy\figure_4.jpg
  Figure 4 caption: Disparity histograms of the training sets of the four benchmarks.
    We plot the disparity histogram of half-resolution images for Middlebury 2014,
    and the disparity histograms of full-resolution images for other datasets.
  Figure 5 Link: articels_figures_by_rev_year\2019\Stereo_Matching_Using_MultiLevel_Cost_Volume_and_MultiScale_Feature_Constancy\figure_5.jpg
  Figure 5 caption: "An illustration of the disparities produced by different methods.\
    \ The input image and the groundtruth disparity are shown in (a) and (f), respectively.\
    \ Initial disparities estimated by 4P [27], ASPP [39], image pyramid [38], and\
    \ our feature-level cost volume schemes are shown in (b)-(e), respectively. The\
    \ final disparities refined by DES - net + DRS - net - nc , DES - net + DRS -\
    \ net , DES - net + DRS - net\xD72 and DES - net + DRS - net\xD73 are shown in\
    \ (g)-(j), respectively. The colorbar at the bottom visualizes the values in error\
    \ maps."
  Figure 6 Link: articels_figures_by_rev_year\2019\Stereo_Matching_Using_MultiLevel_Cost_Volume_and_MultiScale_Feature_Constancy\figure_6.jpg
  Figure 6 caption: Results achieved on the Middlebury 2014 test set. The image pair
    show above is taken from the Nkuba data. Our method generates much smoother results
    and performs better in occluded regions than other methods, especially for the
    region denoted with a white box.
  Figure 7 Link: articels_figures_by_rev_year\2019\Stereo_Matching_Using_MultiLevel_Cost_Volume_and_MultiScale_Feature_Constancy\figure_7.jpg
  Figure 7 caption: Results achieved on the KITTI 2015 dataset.
  Figure 8 Link: articels_figures_by_rev_year\2019\Stereo_Matching_Using_MultiLevel_Cost_Volume_and_MultiScale_Feature_Constancy\figure_8.jpg
  Figure 8 caption: Results achieved on the ETH3D 2017 test set. The storageroom22l
    image pair is used for test. The disparity map generated by our method is much
    smoother than other methods, especially on the ping-pong table.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.57
  Name of the first author: Zhengfa Liang
  Name of the last author: Hengzhu Liu
  Number of Figures: 8
  Number of Tables: 12
  Number of authors: 8
  Paper title: Stereo Matching Using Multi-Level Cost Volume and Multi-Scale Feature
    Constancy
  Publication Date: 2019-07-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Results Achieved on the SceneFlow Dataset by Different Disparity
      Refinement Schemes
  Table 10 caption:
    table_text: TABLE 10 Comparative Results on the KITTI 2015 Benchmark Achieved
      by Our Network Trained with the Two-Stage Finetuning Scheme and the Dataset-Specific
      Finetuning Scheme
  Table 2 caption:
    table_text: TABLE 2 Results Achieved on the SceneFlow Dataset by Different Kinds
      of Cost Volumes
  Table 3 caption:
    table_text: TABLE 3 Comparative Results Achieved by the State-of-the-Art Methods
      on the Middlebury Benchmark
  Table 4 caption:
    table_text: TABLE 4 Comparative Results Achieved by Top Performing Methods on
      the KITTI 2015 Benchmark
  Table 5 caption:
    table_text: TABLE 5 Results Achieved by Top Performing Methods on the ETH3D 2017
      Benchmark
  Table 6 caption:
    table_text: TABLE 6 Results Achieved on the Middlebury 2014 Benchmark for Robust
      Vision Challenge 2018
  Table 7 caption:
    table_text: TABLE 7 Results on the KITTI 2015 Benchmark for Robust Vision Challenge
      2018
  Table 8 caption:
    table_text: TABLE 8 Results on the ETH3D 2017 Dataset for Robust Vision Challenge
      2018
  Table 9 caption:
    table_text: TABLE 9 Comparative Results on the Middlebury 2014 Benchmark Achieved
      by Our Network Trained with the Two-Stage Finetuning Scheme and the Dataset-Specific
      Finetuning Scheme
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2928550
- Affiliation of the first author: mathematics and algorithmic sciences lab., paris
    research center, huawei technologies france, boulogne-billancourt, france
  Affiliation of the last author: "universit\xE9 paris-est, champs-sur-marne, france"
  Figure 1 Link: articels_figures_by_rev_year\2019\Distributed_Variational_Representation_Learning\figure_1.jpg
  Figure 1 caption: A model for distributed learning, e.g., multi-view learning.
  Figure 10 Link: articels_figures_by_rev_year\2019\Distributed_Variational_Representation_Learning\figure_10.jpg
  Figure 10 caption: Evolution of the relevance-complexity pairs during training with
    D-VIB for different values of the regularization parameter s over the relevance-complexity
    plane.
  Figure 2 Link: articels_figures_by_rev_year\2019\Distributed_Variational_Representation_Learning\figure_2.jpg
  Figure 2 caption: Distributed learning architecture for the minimization of the
    variational DIB cost in (25) for K=2 using multivariate Gaussian distributions
    to parameterize the encoders, decoders and prior distributions. The decoders Q
    Y| U 1 and Q Y| U 2 influence in the regularization.
  Figure 3 Link: articels_figures_by_rev_year\2019\Distributed_Variational_Representation_Learning\figure_3.jpg
  Figure 3 caption: Relevance versus sum-complexity tradeoff for vector Gaussian data
    model with K=2 encoders, n y =1 , n 1 = n 2 =3 , and achievable pairs with the
    BA-DIB and D-VIB algorithms for n=30.000 .
  Figure 4 Link: articels_figures_by_rev_year\2019\Distributed_Variational_Representation_Learning\figure_4.jpg
  Figure 4 caption: Mean square error versus sum-complexity tradeoff for vector Gaussian
    data model with K=2 encoders, ny=1 , n1 = n2 = 3 , and achievable pairs with the
    BA-DIB and D-VIB algorithms for n=30.000 .
  Figure 5 Link: articels_figures_by_rev_year\2019\Distributed_Variational_Representation_Learning\figure_5.jpg
  Figure 5 caption: Relevance versus sum-complexity tradeoff for vector Gaussian data
    model with K=2 encoders, ny=2 , n1 = n2 = 3 , and achievable pairs with D-VIB
    for training dataset n=lbrace 5.000, 10.000, 50.000rbrace .
  Figure 6 Link: articels_figures_by_rev_year\2019\Distributed_Variational_Representation_Learning\figure_6.jpg
  Figure 6 caption: Two-view handwritten MNIST dataset.
  Figure 7 Link: articels_figures_by_rev_year\2019\Distributed_Variational_Representation_Learning\figure_7.jpg
  Figure 7 caption: Relevance versus sum-complexity tradeoff for the two-view MNIST
    dataset with K=2 encoders, with the D-VIB algorithm for training dataset n=50.000
    and sin [10-10,1] .
  Figure 8 Link: articels_figures_by_rev_year\2019\Distributed_Variational_Representation_Learning\figure_8.jpg
  Figure 8 caption: Train and test accuracy for the two-view MNIST dataset with K=2
    encoders, from the estimators resulting with the D-VIB algorithm and the D-VIB
    avg. for n=50.000 and sin [10-10,1] .
  Figure 9 Link: articels_figures_by_rev_year\2019\Distributed_Variational_Representation_Learning\figure_9.jpg
  Figure 9 caption: Train and test accuracy for the two-view MNIST dataset with K=2
    encoders, from the estimators using both (U1,U2) , QY|U1,U2 , and only U1 or U2
    , i.e., QY|U1 and QY|U2 , from the application of the D-VIB algorithm for n=50.000
    and sin [10-10,1] .
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: "I\xF1aki Estella Aguerri"
  Name of the last author: Abdellatif Zaidi
  Number of Figures: 10
  Number of Tables: 3
  Number of authors: 2
  Paper title: Distributed Variational Representation Learning
  Publication Date: 2019-07-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 DNN Architecture for Figs. 3 and 5
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 DNN Architecture for Figs. 7 and 8
  Table 3 caption:
    table_text: TABLE 3 Accuracy for Different Algorithms with CNN Architectures
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2928806
- Affiliation of the first author: school of information engineering, changan university,
    xian, shaanxi, china
  Affiliation of the last author: department of computer science, center for research
    in computer vision (crcv), university of central florida, orlando, fl, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Deep_Affinity_Network_for_Multiple_Object_Tracking\figure_1.jpg
  Figure 1 caption: "Schematics of Deep Affinity Network (DAN): A pair of video frames\
    \ I t and I t\u2212n , which are n time stamps apart are input to the network\
    \ along with the sets of centers C t , C t\u2212n of pre-detected objects in those\
    \ frames. The frame pair is processed by two extended VGG-like networks with shared\
    \ parameters. The number of feature maps in nine selective layers of these networks\
    \ are reduced using 1\xD71 convolutional kernels. Compact features extracted from\
    \ those maps are concatenated to form 520-dimensional feature vectors. Exhaustive\
    \ permutations of those vectors in the feature matrices F t and F t\u2212n are\
    \ encoded in a tensor \u03A8 t\u2212n,t \u2208 R 1040\xD7 N m \xD7 N m , where\
    \ N m is the number of objects in each frame. The tensor \u03A8 t\u2212n,t is\
    \ mapped to a matrix M\u2208 R N m \xD7 N m using five convolution layers. To\
    \ account for multiple identities leaving and entering between the frames, M 1\
    \ and M 2 are formed by appending an extra column and an extra row to M . Row-\
    \ and column-wise softmax is performed over M 1 and M 2 respectively. The resulting\
    \ matrices A 1 , A 2 and their column- and row-trimmed variants A 1 \u02C6 , A\
    \ 2 \u02C6 are employed in network loss computation using the ground truth data\
    \ association matrix L t\u2212n,t . Affinity matrix for a pair of frames is predicted\
    \ using the matrices A 1 and A 2 predicted by DAN."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Deep_Affinity_Network_for_Multiple_Object_Tracking\figure_2.jpg
  Figure 2 caption: Illustration of data association matrix between frame 1 and 30,
    with N m =5 . Frame 1 and 30 in (a) and (b) jointly contain 5 detected objects.
    (c) Creation of an intermediate matrix that considers dummy objects (rows and
    columns with zeros) to achieve N m =5 per frame. (d) Augmentation with extra column
    and row to include Un-Identified (UI) targets (objects leaving and entering respectively)
    between the two frames.
  Figure 3 Link: articels_figures_by_rev_year\2019\Deep_Affinity_Network_for_Multiple_Object_Tracking\figure_3.jpg
  Figure 3 caption: "Deep tracking with DAN deployment: For the t frame I t , the\
    \ object centers C t provided by the detectors are used to compute the feature\
    \ matrix F t with one stream Feature Extractor of DAN. The F t is paired with\
    \ each of the last t feature matrices F 0:t\u22121 , and each pair is processed\
    \ by the Affinity Estimator to compute the same number of affinity matrices A\
    \ 0:t\u22121,t . The F t is also stored for computing affinity matrices in the\
    \ future. The trajectory set T t , is updated by associating the current frame\
    \ with t previous frames using the computed affinity matrices."
  Figure 4 Link: articels_figures_by_rev_year\2019\Deep_Affinity_Network_for_Multiple_Object_Tracking\figure_4.jpg
  Figure 4 caption: Tracking example of the proposed method from MOT17 (taken from
    the host server). The predicted tracks are identified by the color of bounding
    boxes. The mentioned identity numbers are for reference in the text only. In both
    scenes, our approach successfully tracks identity-1 despite inter-frame occlusions.
    Frame 141 of Scene 07 causes a temporary mis-identification, however, our approach
    is able to recover well due to deep track association.
  Figure 5 Link: articels_figures_by_rev_year\2019\Deep_Affinity_Network_for_Multiple_Object_Tracking\figure_5.jpg
  Figure 5 caption: Tracking examples from the UA-DETRAC challenge. Predicted tracks
    are identified by the bounding box colors and the identity numbers are for reference
    only. The proposed tracker is able to assign identities to their correct track
    (in both cases) despite missed detection in several frames due to limited performance
    of the detector for occluded objects.
  Figure 6 Link: articels_figures_by_rev_year\2019\Deep_Affinity_Network_for_Multiple_Object_Tracking\figure_6.jpg
  Figure 6 caption: "Illustration of cross-frame associations based on DAN outputs.\
    \ The frame pairs are randomly chosen n\u223C[1,30] time-stamps apart. The association\
    \ remains robust to illumination conditions, partial occlusions and existence\
    \ of multiple similar looking objects in the video frames."
  Figure 7 Link: articels_figures_by_rev_year\2019\Deep_Affinity_Network_for_Multiple_Object_Tracking\figure_7.jpg
  Figure 7 caption: Mean absolute error of predicted data association on MOT17. All
    possible frame pairs are used for each n - the time stamp difference between the
    two frames in a pair.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.51
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: ShiJie Sun
  Name of the last author: Mubarak Shah
  Number of Figures: 7
  Number of Tables: 11
  Number of authors: 5
  Paper title: Deep Affinity Network for Multiple Object Tracking
  Publication Date: 2019-07-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Input Data for Training DAN
  Table 10 caption:
    table_text: TABLE 10 Loss Values of DAN Variants at 50-120 Epochs on MOT17 While
      Training
  Table 2 caption:
    table_text: 'TABLE 2 Architectural Details of the Extension & Compression Networks
      in Fig. 1: I.C Denotes the Number of input Channels for a Layer, O.C Is the
      Number of Output Channels, S Is the Stride Size, B.N (YN) Indicates If the Batch
      Normalization Is Applied; and ReLU (YN) Indicates If the ReLU Activation Is
      Used'
  Table 3 caption:
    table_text: 'TABLE 3 Details of Feature Dimension Reduction Layers in Fig. 1:
      3 Layers Are Selected from VGG Network'
  Table 4 caption:
    table_text: TABLE 4 Attributes of MOT17 Training Data [29]
  Table 5 caption:
    table_text: TABLE 5 Metrics Used for Benchmarking
  Table 6 caption:
    table_text: "TABLE 6 MOT17 Challenge Results from the Server: The Symbol \u2191\
      \ \u2191 Indicates that Higher Values Are Better, and \u2193 \u2193 Implies\
      \ Lower Values Are Favored"
  Table 7 caption:
    table_text: "TABLE 7 MOT15 Challenge Results: The Symbols \u2191 \u2191 and \u2193\
      \ \u2193 Respectively Indicate that Higher and Lower Values Are Preferred"
  Table 8 caption:
    table_text: TABLE 8 Attributes of UA-DETRAC Dataset [30], [31]
  Table 9 caption:
    table_text: "TABLE 9 UA-DETRAC Challenge Results: The Symbol \u2191 \u2191 Indicates\
      \ that Higher Values Are Better, and \u2193 \u2193 Implies Lower Values Are\
      \ Favored"
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2929520
- Affiliation of the first author: department of statistics and actuarial science,
    stellenbosch university, stellenbosch, south africa
  Affiliation of the last author: department of statistics and actuarial science,
    stellenbosch university, stellenbosch, south africa
  Figure 1 Link: articels_figures_by_rev_year\2019\Fast_Exact_Evaluation_of_Univariate_Kernel_Sums\figure_1.jpg
  Figure 1 caption: "Relative efficiency of smooth poly(|x|) e \u2212|x| kernels of\
    \ varying polynomial degree ( \u03B1 ). Relative efficiency for density estimation\
    \ (left) and density derivative estimation (right). Relative efficiency of Gaussian\
    \ kernel (- - - -)."
  Figure 10 Link: articels_figures_by_rev_year\2019\Fast_Exact_Evaluation_of_Univariate_Kernel_Sums\figure_10.jpg
  Figure 10 caption: "Reconstruction of 512\xD7512 image Lena with missing pixel values.\
    \ Inputs (top) and outputs (bottom) from interpolation based on kernel regression\
    \ using the proposed fast approach. Running time was less than 0.2 seconds per\
    \ image including selection of the bandwidth value."
  Figure 2 Link: articels_figures_by_rev_year\2019\Fast_Exact_Evaluation_of_Univariate_Kernel_Sums\figure_2.jpg
  Figure 2 caption: "Plots of degree 1 (- - - -) and degree 4 (\u2013 \u22C5 \u2013\
    \ \u22C5 \u2013) smooth poly(|x|) e \u2212|x| kernels, as well as the Gaussian\
    \ kernel ( \u22EF\u22EF )."
  Figure 3 Link: articels_figures_by_rev_year\2019\Fast_Exact_Evaluation_of_Univariate_Kernel_Sums\figure_3.jpg
  Figure 3 caption: "Degree 3 (left) and degree 4 (right) poly(|x|) e \u2212|x| kernels\
    \ approximating the Gaussian kernel."
  Figure 4 Link: articels_figures_by_rev_year\2019\Fast_Exact_Evaluation_of_Univariate_Kernel_Sums\figure_4.jpg
  Figure 4 caption: Collection of densities used in experiments for kernel density
    and density derivative estimation.
  Figure 5 Link: articels_figures_by_rev_year\2019\Fast_Exact_Evaluation_of_Univariate_Kernel_Sums\figure_5.jpg
  Figure 5 caption: Computation times for density (c).
  Figure 6 Link: articels_figures_by_rev_year\2019\Fast_Exact_Evaluation_of_Univariate_Kernel_Sums\figure_6.jpg
  Figure 6 caption: Image deconvolution of a pair of images taken from the Berkeley
    image database.
  Figure 7 Link: articels_figures_by_rev_year\2019\Fast_Exact_Evaluation_of_Univariate_Kernel_Sums\figure_7.jpg
  Figure 7 caption: Separation of reflection from painting photographed behind glass.
    Running times for the proposed method and fast ICA were 19.64s and 0.24s respectively.
  Figure 8 Link: articels_figures_by_rev_year\2019\Fast_Exact_Evaluation_of_Univariate_Kernel_Sums\figure_8.jpg
  Figure 8 caption: Separation of window reflection from background. Running times
    for the proposed method and fast ICA were 5.65s and 0.11s respectively.
  Figure 9 Link: articels_figures_by_rev_year\2019\Fast_Exact_Evaluation_of_Univariate_Kernel_Sums\figure_9.jpg
  Figure 9 caption: "Smoothing of 512\xD7512 image Lena with additive Gaussian noise.\
    \ Inputs (top) and outputs (bottom) from smoothing based on kernel regression\
    \ using the proposed fast approach. Running time was 0.1 seconds per image including\
    \ selection of the bandwidth value."
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: David P. Hofmeyr
  Name of the last author: David P. Hofmeyr
  Number of Figures: 10
  Number of Tables: 3
  Number of authors: 1
  Paper title: Fast Exact Evaluation of Univariate Kernel Sums
  Publication Date: 2019-07-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Estimated Mean Integrated Squared Error of Density Estimates
      from 30 Replications
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Estimated Mean Integrated Squared Error of First Derivative
      Estimates from 30 Replications
  Table 3 caption:
    table_text: TABLE 3 Average Running Time of Estimation of First Derivative from
      30 Replications
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2930501
