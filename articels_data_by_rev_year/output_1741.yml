- Affiliation of the first author: school of electronics and communication engineering,
    sun yat-sen university, guangzhou, china
  Affiliation of the last author: department of computer science and software engineering,
    university of western australia, crawley, wa, australia
  Figure 1 Link: articels_figures_by_rev_year\2020\Deep_Learning_for_D_Point_Clouds_A_Survey\figure_1.jpg
  Figure 1 caption: A taxonomy of deep learning methods for 3D point clouds.
  Figure 10 Link: articels_figures_by_rev_year\2020\Deep_Learning_for_D_Point_Clouds_A_Survey\figure_10.jpg
  Figure 10 caption: Chronological overview of the most relevant deep learning-based
    3D semantic segmentation methods.
  Figure 2 Link: articels_figures_by_rev_year\2020\Deep_Learning_for_D_Point_Clouds_A_Survey\figure_2.jpg
  Figure 2 caption: Chronological overview of the most relevant deep learning-based
    3D shape classification methods.
  Figure 3 Link: articels_figures_by_rev_year\2020\Deep_Learning_for_D_Point_Clouds_A_Survey\figure_3.jpg
  Figure 3 caption: A lightweight architecture of PointNet. n denotes the number of
    input points, M denotes the dimension of the learned features for each point.
  Figure 4 Link: articels_figures_by_rev_year\2020\Deep_Learning_for_D_Point_Clouds_A_Survey\figure_4.jpg
  Figure 4 caption: An illustration of a continuous and discrete convolution for local
    neighbors of a point. (a) represents a local neighborhood q i centered at point
    p ; (b) and (c) represent 3D continuous and discrete convolution, respectively.
  Figure 5 Link: articels_figures_by_rev_year\2020\Deep_Learning_for_D_Point_Clouds_A_Survey\figure_5.jpg
  Figure 5 caption: An illustration of a graph-based network.
  Figure 6 Link: articels_figures_by_rev_year\2020\Deep_Learning_for_D_Point_Clouds_A_Survey\figure_6.jpg
  Figure 6 caption: An illustration of 3D object detection. (a) and (b) are originally
    shown in [124] and [125], respectively.
  Figure 7 Link: articels_figures_by_rev_year\2020\Deep_Learning_for_D_Point_Clouds_A_Survey\figure_7.jpg
  Figure 7 caption: Chronological overview of the most relevant deep learning-based
    3D object detection methods.
  Figure 8 Link: articels_figures_by_rev_year\2020\Deep_Learning_for_D_Point_Clouds_A_Survey\figure_8.jpg
  Figure 8 caption: 'Typical networks for three categories of region proposal-based
    3D object detection methods. From top to bottom: (a) multi-view based, (b) segmentation-based
    and (c) frustum-based methods.'
  Figure 9 Link: articels_figures_by_rev_year\2020\Deep_Learning_for_D_Point_Clouds_A_Survey\figure_9.jpg
  Figure 9 caption: A 3D scene flow between two KITTI point clouds, originally shown
    in [175]. Point clouds X , Y and the translated point cloud of X are highlighted
    in red, green, and blue, respectively.
  First author gender probability: 0.91
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yulan Guo
  Name of the last author: Mohammed Bennamoun
  Number of Figures: 13
  Number of Tables: 5
  Number of authors: 6
  Paper title: 'Deep Learning for 3D Point Clouds: A Survey'
  Publication Date: 2020-06-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 A Summary of Existing Datasets for 3D Shape Classification,
      3D Object Detection and Tracking, and 3D Point Cloud Segmentation
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparative 3D Shape Classification Results on the ModelNet1040
      Benchmarks
  Table 3 caption:
    table_text: TABLE 3 Comparative 3D Object Detection Results on the KITTI Test
      3D Detection Benchmark
  Table 4 caption:
    table_text: TABLE 4 Comparative 3D Object Detection Results on the KITTI Test
      BEV Detection Benchmark
  Table 5 caption:
    table_text: TABLE 5 Comparative Semantic Segmentation Results on the S3DIS (Including
      Both Area5 and 6-fold Cross Validation) [10], Semantic3D (Including Both semantic-8
      and reduced-8 Subsets) [12], ScanNet [11], and SemanticKITTI [15] Datasets
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3005434
- Affiliation of the first author: pca lab, key lab of intelligent perception and
    systems for high-dimensional information of ministry of education, nanjing, china
  Affiliation of the last author: pca lab, key lab of intelligent perception and systems
    for high-dimensional information of ministry of education, nanjing, china
  Figure 1 Link: articels_figures_by_rev_year\2020\HomographyBased_MinimalCase_Relative_Pose_Estimation_With_Known_Gravity_Directio\figure_1.jpg
  Figure 1 caption: We can calculate the roll, pitch angles of the camera coordinate
    with respect to the world coordinate (gravity) using the IMU data, and align the
    y-axes with the gravity.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\HomographyBased_MinimalCase_Relative_Pose_Estimation_With_Known_Gravity_Directio\figure_2.jpg
  Figure 2 caption: log 10 relative focal length errors under sideways motion. (a)
    comparison between the original and the normalized action matrix solvers for the
    equal and unknown focal length problem. (b)-(d) comparisons of the TEigen solvers
    and the action matrix solvers for three different camera configurations.
  Figure 3 Link: articels_figures_by_rev_year\2020\HomographyBased_MinimalCase_Relative_Pose_Estimation_With_Known_Gravity_Directio\figure_3.jpg
  Figure 3 caption: 'log 10 relative focal length errors for some critical motions.
    Left: Pure rotation. Middle: Planar motions when the optical axes lie in the plane
    for the equal and unknown focal length problem. Right: Pure translation for the
    one unknown focal length problem.'
  Figure 4 Link: articels_figures_by_rev_year\2020\HomographyBased_MinimalCase_Relative_Pose_Estimation_With_Known_Gravity_Directio\figure_4.jpg
  Figure 4 caption: 'Focal length errors for the equal and unknown focal length problem.
    From top to bottom: increased image noise, increased Pitch noise with constant
    0.5 pixel standard deviation image noise and increased Roll noise with constant
    0.5 pixel standard deviation image noise. Left column: Sideways motion. Right
    column: Forward motion.'
  Figure 5 Link: articels_figures_by_rev_year\2020\HomographyBased_MinimalCase_Relative_Pose_Estimation_With_Known_Gravity_Directio\figure_5.jpg
  Figure 5 caption: '(a) Focal length errors for the one unknown focal length problem.
    Left column: Sideways motion. Right column: Forward motion. (b) Focal length errors
    for the different and unknown focal lengths problem. From top to bottom: Increased
    image noise, increased Pitch noise with constant 0.5 pixel standard deviation
    image noise and increased Roll noise with constant 0.5 pixel standard deviation
    image noise.'
  Figure 6 Link: articels_figures_by_rev_year\2020\HomographyBased_MinimalCase_Relative_Pose_Estimation_With_Known_Gravity_Directio\figure_6.jpg
  Figure 6 caption: 'Example images of the 8 sequences. Top row: The first four sequences
    with a total of 3102 image pairs (scenes with dominant planar structures); Bottom
    row: The last four sequences with a total of 1200 image pairs (almost pure planar
    scenes).'
  Figure 7 Link: articels_figures_by_rev_year\2020\HomographyBased_MinimalCase_Relative_Pose_Estimation_With_Known_Gravity_Directio\figure_7.jpg
  Figure 7 caption: 'The cumulative distribution function (CDF) of the errors for
    different solvers. The suffixes -equal and -one indicate the equal and unknown
    focal length case and one unknown focal length case, respectively. Top row: The
    first four sequences with a total of 3102 image pairs; Bottom row: the last four
    sequences with a total of 1200 image pairs. From left to right: The cumulative
    distribution function of rotation error, translation error, and focal length error,
    respectively.'
  Figure 8 Link: articels_figures_by_rev_year\2020\HomographyBased_MinimalCase_Relative_Pose_Estimation_With_Known_Gravity_Directio\figure_8.jpg
  Figure 8 caption: Overview of our solvers for real applications.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.87
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.96
  Name of the first author: Yaqing Ding
  Name of the last author: Hui Kong
  Number of Figures: 8
  Number of Tables: 0
  Number of authors: 4
  Paper title: Homography-Based Minimal-Case Relative Pose Estimation With Known Gravity
    Direction
  Publication Date: 2020-06-29 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3005373
- Affiliation of the first author: university of hong kong, hong kong, china
  Affiliation of the last author: university of hong kong, hong kong, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Deep_Photometric_Stereo_for_NonLambertian_Surfaces\figure_1.jpg
  Figure 1 caption: Overview of the proposed method. Values above the layers indicate
    the number of feature channels.
  Figure 10 Link: articels_figures_by_rev_year\2020\Deep_Photometric_Stereo_for_NonLambertian_Surfaces\figure_10.jpg
  Figure 10 caption: Illustration of how max-pooling fusion layer handles surface
    regions with cast shadow using Goblet from the DiLiGenT benchmark. (Note that
    the provided object mask and ground-truth normal map do not include the concave
    interior of Goblet.)
  Figure 2 Link: articels_figures_by_rev_year\2020\Deep_Photometric_Stereo_for_NonLambertian_Surfaces\figure_2.jpg
  Figure 2 caption: Comparison between PS-FCN and PS-FCN +N on an object with spatially-varying
    BRDFs. Numbers in the parentheses denote mean angular error (MAE) in degree.
  Figure 3 Link: articels_figures_by_rev_year\2020\Deep_Photometric_Stereo_for_NonLambertian_Surfaces\figure_3.jpg
  Figure 3 caption: Illustration of the introduced data normalization operation on
    cat and ball in the DiLiGenT benchmark. The first and third rows show the original
    images, while the second and last rows show the normalized images. Only 5 out
    of 96 images for each object are shown.
  Figure 4 Link: articels_figures_by_rev_year\2020\Deep_Photometric_Stereo_for_NonLambertian_Surfaces\figure_4.jpg
  Figure 4 caption: "(a) Illustration of the coordinate system ( z axis is the viewing\
    \ direction). \u03D5\u2208[ 0 \u2218 , 180 \u2218 ] and \u03B8\u2208[\u2212 90\
    \ \u2218 , 90 \u2218 ] are the azimuth and elevation of the light direction, respectively.\
    \ (b) Example discretization of the light direction space when K d =18 ."
  Figure 5 Link: articels_figures_by_rev_year\2020\Deep_Photometric_Stereo_for_NonLambertian_Surfaces\figure_5.jpg
  Figure 5 caption: Examples of the synthetic training data (images are adjusted with
    gamma correction for visualization purpose).
  Figure 6 Link: articels_figures_by_rev_year\2020\Deep_Photometric_Stereo_for_NonLambertian_Surfaces\figure_6.jpg
  Figure 6 caption: (a) Lighting distribution of SynTest MERL dataset. The light direction
    is visualized by mapping a 3-d vector [x,y,z] to a point [x,y] . (b) Ground-truth
    normals of Sphere, Bunny, Dragon, and Armadillo. (c) Visualization of the selected
    material maps (Ramp, Checker, Irregular) and examples in Dragon SVBRDF dataset.
  Figure 7 Link: articels_figures_by_rev_year\2020\Deep_Photometric_Stereo_for_NonLambertian_Surfaces\figure_7.jpg
  Figure 7 caption: Lighting distributions of the real testing datasets. The color
    of the point indicates the light intensity (value is divided by the highest intensity
    to normalize to [0,1]).
  Figure 8 Link: articels_figures_by_rev_year\2020\Deep_Photometric_Stereo_for_NonLambertian_Surfaces\figure_8.jpg
  Figure 8 caption: Visualization of the learned feature map after fusion (the features
    were normalized to [0,1] ). The first two columns show the objects and ground-truth
    normals. The subsequent columns (a-j) visualize 10 out of 128 channels of the
    fused feature map. Note that different regions with similar normal directions
    are fired in different channels. Each channel can therefore be interpreted as
    the probability of the normal belonging to a certain direction (or alternatively
    as the object shading rendered under a certain light direction).
  Figure 9 Link: articels_figures_by_rev_year\2020\Deep_Photometric_Stereo_for_NonLambertian_Surfaces\figure_9.jpg
  Figure 9 caption: (a) Results of PS-FCN trained and tested with different numbers
    of input images on Sphere. (b) Results of PS-FCN trained with a fixed number of
    32 input images and tested with different numbers of input images.
  First author gender probability: 0.8
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Guanying Chen
  Name of the last author: Kwan-Yee K. Wong
  Number of Figures: 17
  Number of Tables: 8
  Number of authors: 5
  Paper title: Deep Photometric Stereo for Non-Lambertian Surfaces
  Publication Date: 2020-06-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Normal Estimation Results on SynTest MERL MERL Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results of PS-FCN on Bunny Rendered Using Three Different
      Lighting Distributions
  Table 3 caption:
    table_text: TABLE 3 Quantitative Comparison of Calibrated Photometric Stereo on
      the DiLiGenT Benchmark
  Table 4 caption:
    table_text: TABLE 4 Lighting Estimation Results (MAE in Degree for Light Direction
      and Relative Error for Intensity) on SynTest MERL MERL Dataset
  Table 5 caption:
    table_text: TABLE 5 Results of LCNet and LCNet reg reg on Sphere and Bunny Rendered
      Under Different Lighting Distributions
  Table 6 caption:
    table_text: TABLE 6 Lighting Estimation Results of LCNet on Dragon SVBRDF SVBRDF
      Dataset
  Table 7 caption:
    table_text: TABLE 7 Normal Estimation Results on SynTest MERL MERL Dataset
  Table 8 caption:
    table_text: TABLE 8 Quantitative Results on the DiLiGenT Benchmark
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3005397
- Affiliation of the first author: graduate school of information science and technology,
    osaka university, osaka, japan
  Affiliation of the last author: graduate school of information science and technology,
    osaka university, osaka, japan
  Figure 1 Link: articels_figures_by_rev_year\2020\Deep_Photometric_Stereo_Networks_for_Determining_Surface_Normal_and_Reflectances\figure_1.jpg
  Figure 1 caption: "Overview of the proposed deep photometric stereo network (DPSN).\
    \ It consists of three components: shared, normal estimation, and BRDF estimation\
    \ blocks. The shared block is formed by a shadow layer and dense layers. The shared\
    \ block takes as input the per-pixel measurement vector m= [ m 1 ,\u2026, m f\
    \ ] \u22A4 . The normal estimation and BRDF estimation blocks consist of dense\
    \ layers, which yield prediction of a surface normal vector n \u2217 and reflectance\
    \ parameters c \u2217 , respectively. From the reflectance parameters c \u2217\
    \ , a full BRDF table is reconstructed based on Eq. 2."
  Figure 10 Link: articels_figures_by_rev_year\2020\Deep_Photometric_Stereo_Networks_for_Determining_Surface_Normal_and_Reflectances\figure_10.jpg
  Figure 10 caption: Qualitative evaluation of re-rendering under novel light directions.
    We show the three images for each object rendered with the new light directions
    that are not included in the training data. Leftmost plot illustrates the light
    directions. Gray points represent the 96 light directions used in the training
    data, and red, green, and blue points are the new light directions that corresponds
    to images (a), (b), and (c), respectively.
  Figure 2 Link: articels_figures_by_rev_year\2020\Deep_Photometric_Stereo_Networks_for_Determining_Surface_Normal_and_Reflectances\figure_2.jpg
  Figure 2 caption: Overview of shadow layer. Shadow layer randomly drops some of
    the measurement vector elements to simulate the cast shadow effect. In this illustration,
    m 1 and m 3 are dropped and corresponding values of the input vector are set to
    0.
  Figure 3 Link: articels_figures_by_rev_year\2020\Deep_Photometric_Stereo_Networks_for_Determining_Surface_Normal_and_Reflectances\figure_3.jpg
  Figure 3 caption: "Examples of rendered images and the corresponding normal map\
    \ that are used for training. \u201Cwhite-paint\u201D, \u201Csilver-metallic-paint\u201D\
    , and \u201Cred-plastic\u201D are the material names in the MERL BRDF dataset\
    \ [8]. Here, three are shown out of 100 different materials. As seen in the figures,\
    \ the rendered images contain specularity and attached shadows."
  Figure 4 Link: articels_figures_by_rev_year\2020\Deep_Photometric_Stereo_Networks_for_Determining_Surface_Normal_and_Reflectances\figure_4.jpg
  Figure 4 caption: "Experimental result of synthetic scenes. In each row, a normal\
    \ map is shown on top of the corresponding error map. The numbers represent Mean\
    \ Angular Error (MAngE) in degree. In the top row, GT means the ground truth,\
    \ the images below the normal maps are examples of observation images. On the\
    \ top, material and shape names are shown. MERL1 and MERL2 correspond to \u201C\
    black-oxidized-steel\u201D and \u201Cwhite-fabric2\u201D, respectively. Material\
    \ A to D are the synthetic BRDFs that are created by linearly combining a pair\
    \ of BRDFs in MERL BRDF dataset: (\u201Cblue-fabric\u201D, \u201Csilver-metallic-paint\u201D\
    ), (\u201Ccherry-235\u201D, \u201Cnatural-209\u201D), (\u201Cbeige-fabric\u201D\
    , \u201Cyellow-paint\u201D), and (\u201Cblack-soft-plastic\u201D, \u201Cblue-metallic-paint\u201D\
    ), respectively."
  Figure 5 Link: articels_figures_by_rev_year\2020\Deep_Photometric_Stereo_Networks_for_Determining_Surface_Normal_and_Reflectances\figure_5.jpg
  Figure 5 caption: "Rendered results with the estimated BRDFs and the ground truth\
    \ normal maps for synthetic scenes. We show the images rendered under eight different\
    \ light conditions. \u201CTrain light 1\u201D to \u201CTrain light 4\u201D are\
    \ selected from the 96 light conditions that are used for training, while \u201C\
    Test light 1\u201D to \u201CTest light 4\u201D are from the test light conditions\
    \ that are not included in the training data. We sampled 50 test light directions\
    \ uniformly on a hemisphere. In \u201CMultiple (blob08)\u201D, we apply Gamma\
    \ correction with \u03B3=0.8 for visualization. \u201CGT\u201D and \u201CEst.\u201D\
    \ mean the ground truth and estimated respectively and are rendered in the same\
    \ manner to the generation of training data. MAE is the mean absolute error calculated\
    \ in the grayscale space with the intensities scaled in the range of [0.0,1.0]\
    \ . MAE for train light conditions is the mean of f ( =96 ) images, and for test\
    \ light conditions is the mean of 50 images."
  Figure 6 Link: articels_figures_by_rev_year\2020\Deep_Photometric_Stereo_Networks_for_Determining_Surface_Normal_and_Reflectances\figure_6.jpg
  Figure 6 caption: "Plots of estimated BRDFs. The plots show the reflectance \u03C1\
    (n,l,v) varying with the lighting direction l . We use the polar coordinate system\
    \ for the lighting direction l and viewing direction v as [ l \u03B8 , l \u03D5\
    \ ] and [ v \u03B8 , v \u03D5 ] , where \u03B8 and \u03D5 represent the polar\
    \ and azimuthal angles, respectively. We fix the normal direction to n=[0,0,1\
    \ ] \u22A4 , v \u03D5 = 0 \u2218 , and l \u03D5 = 180 \u2218 and vary l \u03B8\
    \ from 0 \u2218 to 90 \u2218 . In each row, from left to right, it shows a scene\
    \ image, and reflectance plots with three viewing directions v \u03B8 = 0 \u2218\
    \ , 30 \u2218 , 60 \u2218 . The dashed lines are the ground truth, and red and\
    \ blue lines are the estimated BRDFs at pixels marked by red and blue stars in\
    \ the leftmost figure. We show the grayscale reflectance by taking the average\
    \ of three color channels. Note that we cannot predict the absolute values of\
    \ reflectances from images, and there is a scaling ambiguity between the ground\
    \ truth and estimated BRDFs. We align them by normalization based on the median\
    \ of each BRDF table."
  Figure 7 Link: articels_figures_by_rev_year\2020\Deep_Photometric_Stereo_Networks_for_Determining_Surface_Normal_and_Reflectances\figure_7.jpg
  Figure 7 caption: Rendering of a sphere scene with estimated BRDFs under the natural
    lighting environment. The right-top figure shows the environment map. We also
    show the lighting condition by showing a rendered mirror sphere in the right-bottom.
    GT means the rendering with the ground truth BRDFs.
  Figure 8 Link: articels_figures_by_rev_year\2020\Deep_Photometric_Stereo_Networks_for_Determining_Surface_Normal_and_Reflectances\figure_8.jpg
  Figure 8 caption: Surface normal estimation result for real-world scenes from DiLiGenT
    [51]. In each row, a normal map is shown on top of an error map. The numbers represent
    Mean Angular Error (MAngE) in degree. GT means the ground truth, and figures under
    GT are examples of observed images. The contrast of observation images is adjusted
    for better visualization.
  Figure 9 Link: articels_figures_by_rev_year\2020\Deep_Photometric_Stereo_Networks_for_Determining_Surface_Normal_and_Reflectances\figure_9.jpg
  Figure 9 caption: Rendered results with the estimated BRDFs and the ground truth
    normal maps for DiLiGenT. buddha is the object with non-specular materials and
    goblet is made of metal materials. We apply Gamma correction with gamma =0.8 to
    the ground truth and rendered images for visualization. The light conditions Light
    1 to Light 4 are from 96 DiLiGenT light conditions.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Hiroaki Santo
  Name of the last author: Yasuyuki Matsushita
  Number of Figures: 15
  Number of Tables: 2
  Number of authors: 5
  Paper title: Deep Photometric Stereo Networks for Determining Surface Normal and
    Reflectances
  Publication Date: 2020-06-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The DPSN Structure
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Evaluation Results Using the DiLiGenT Benchmark [51]
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3005219
- Affiliation of the first author: department of computer science and engineering,
    and moe key lab of artificial intelligence, ai institute, shanghai jiao tong university,
    shanghai, p. r. china
  Affiliation of the last author: moe key lab of artificial intelligence, ai institute,
    shanghai jiao tong university, shanghai, p.r. china
  Figure 1 Link: articels_figures_by_rev_year\2020\Combinatorial_Learning_of_Robust_Deep_Graph_Matching_An_Embedding_Based_Approach\figure_1.jpg
  Figure 1 caption: Overview of our (a) permutation loss based intra-graph affinity
    (PIA-GM), cross-graph affinity (PCA-GM) and (b) iterative cross-graph affinity
    (IPCA-GM) approaches for deep combinatorial learning of graph matching. The CNN
    features are extracted from image pairs followed by node embedding and Sinkhorn
    operation for matching. The CNN model, embedding model and affinity metric are
    all learnable in an end-to-end fashion.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Combinatorial_Learning_of_Robust_Deep_Graph_Matching_An_Embedding_Based_Approach\figure_2.jpg
  Figure 2 caption: 'A failure case of offset loss: source image with keypoint (left)
    and target image with matching candidates (right), where numbers denote the probability
    of predicted matching. Ground truth matching nodes are colored in rose (only receives
    0.05 probability by this poor prediction, where the model mistakenly match the
    right ear to the left ear). Offset loss is computed by a weighted sum among all
    candidates, resulting in a misleading low loss 0.070. In this example offset loss
    fails to distinguish between leftright ears and such information will not be learned
    under its supervision. Our permutation loss, on the contrary, issues a reasonably
    high loss 5.139. The underlying rationale is that the problem at hand is fundamentally
    a combinatorial problem rather than a regression task.'
  Figure 3 Link: articels_figures_by_rev_year\2020\Combinatorial_Learning_of_Robust_Deep_Graph_Matching_An_Embedding_Based_Approach\figure_3.jpg
  Figure 3 caption: "Performance comparison of different methods on synthetic dataset\
    \ with noise on feature vectors, inlier numbers and outlier numbers, as well as\
    \ using different affinity models and losses. Default setting: K in =20, K out\
    \ =0, \u03C3 feat =1.5, \u03C3 coo =10 ."
  Figure 4 Link: articels_figures_by_rev_year\2020\Combinatorial_Learning_of_Robust_Deep_Graph_Matching_An_Embedding_Based_Approach\figure_4.jpg
  Figure 4 caption: Illustration of transfer learning experiments for our methods.
  Figure 5 Link: articels_figures_by_rev_year\2020\Combinatorial_Learning_of_Robust_Deep_Graph_Matching_An_Embedding_Based_Approach\figure_5.jpg
  Figure 5 caption: Transfer learning among eight categories from Pascal VOC Keypoint
    represented by confusion matrix. Models are trained on categories on the y -axis,
    and testing results are reported on categories on the x -axis. Note that accuracy
    does not degenerate much for our embedding models between similar categories (such
    as cat and dog). Numbers in cells are the corresponding accuracy. Here we plot
    two lines of confusion matrices in parallel for better illustration. For blue
    matrices, the color map stands for accuracy normalized by the highest accuracy
    on this column, and they do not denote the absolute value of accuracy among different
    categories and matrices. For the orange matrices, we plot the ranking of accuracy
    on this current cell among all 5 confusion matrices. Darker color corresponds
    to higher ranking in accuracy. We also report accuracy for elements in diagonal
    and overall for each confusion matrix, as shown in brackets on the top of blue
    matrices (best viewed in color and zoom in for better view).
  Figure 6 Link: articels_figures_by_rev_year\2020\Combinatorial_Learning_of_Robust_Deep_Graph_Matching_An_Embedding_Based_Approach\figure_6.jpg
  Figure 6 caption: Visualization on (a) Pascal VOC Keypoint and (b) Willow ObjectClass.
    On each column, from top to the bottom, are source image, GMN result, PCA-GM result
    and IPCA-GM result, respectively. Keypoints with the same color represent the
    predicted matching pairs. White outer circles represent correct matching, and
    red ones denote the wrong matching (best viewed in color and zoom in for better
    view).
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.89
  Name of the first author: Runzhong Wang
  Name of the last author: Xiaokang Yang
  Number of Figures: 6
  Number of Tables: 9
  Number of authors: 3
  Paper title: 'Combinatorial Learning of Robust Deep Graph Matching: An Embedding
    Based Approach'
  Publication Date: 2020-06-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Matching Accuracy (%) on Pascal VOC Keypoint, Without Outliers
      (White) and With Outliers (Gray)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Matching Accuracy (%) for Transfer Learning Test in the Presence
      of Outliers on Pascal VOC Keypoint
  Table 3 caption:
    table_text: TABLE 3 Matching Accuracy (%) for Transfer Learning Test Across Datasets
      on Willow ObjectClass
  Table 4 caption:
    table_text: TABLE 4 Matching accuracy (%) on Pascal VOC Keypoint With Different
      Module Configurations
  Table 5 caption:
    table_text: TABLE 5 Matching Accuracy (%) of PIA-GM and PCA-GM by Number of Embedding
      Layers on Pascal VOC Keypoint With No Outliers
  Table 6 caption:
    table_text: "TABLE 6 Matching Accuracy (%) of PIA-GM, PCA-GM and PCA-GM by Different\
      \ \u03C4 \u03C4 on Pascal VOC Keypoint With No Outliers"
  Table 7 caption:
    table_text: TABLE 7 Ablation Study on Proposed Components Using the Pascal VOC
      Keypoint as Benchmark
  Table 8 caption:
    table_text: TABLE 8 Matching Accuracy (%) by Number of Iterations for Iterative
      Cross-Graph Affinity Component Design in the IPCA-GM Method on Pascal VOC Keypoint
  Table 9 caption:
    table_text: TABLE 9 Matching Accuracy (%) on Pascal VOC Keypoint Without Outliers
      Tested With Varying Numbers of Iterations During Testing
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3005590
- Affiliation of the first author: bytedance ai lab, beijing, china
  Affiliation of the last author: state key lab of intelligent technologies and systems,
    beijing national research center for information science and technology (bnrist),
    department of automation, institute for artificial intelligence, tsinghua university
    (thuai), tsinghua university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2020\On_Connections_Between_Regularizations_for_Improving_DNN_Robustness\figure_1.jpg
  Figure 1 caption: "Illustration of how \u2225 r \u2217 \u2225 2 varies with the\
    \ prediction probability p(x ) y in binary scenarios."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\On_Connections_Between_Regularizations_for_Improving_DNN_Robustness\figure_2.jpg
  Figure 2 caption: "The adversarial robustness of obtained binary classification\
    \ models evaluated with FGSM, PGD, DeepFool, and C&Ws attacks: (a)-(d) for LeNet-300-100\
    \ and (e)-(h) for LeNet-5. Ten runs from different initializations were performed\
    \ and the average results are illustrated for fair comparisons. The y axis of\
    \ the four subfigures on the left are normalized to the same numerical scale,\
    \ and so as the four on the right. It can be seen that penalizing \u03BD 2 2 and\
    \ \u03BC 2 perform similarly."
  Figure 3 Link: articels_figures_by_rev_year\2020\On_Connections_Between_Regularizations_for_Improving_DNN_Robustness\figure_3.jpg
  Figure 3 caption: Different regularizers focus on samples with different prediction
    confidence.
  Figure 4 Link: articels_figures_by_rev_year\2020\On_Connections_Between_Regularizations_for_Improving_DNN_Robustness\figure_4.jpg
  Figure 4 caption: 'The robustness of all obtained binary classification models evaluated
    with FGSM, PGD, DeepFool, and the C&Ws attacks: (a)-(d) for LeNet-300-100 and
    (e)-(h) for LeNet-5. Ten runs from different initializations were performed and
    the average results are reported. The y axis of the four subfigures on the left
    are normalized to the same numerical scale, and so as the four on the right. It
    can be seen that the curvature regularization shows the most promising performance.'
  Figure 5 Link: articels_figures_by_rev_year\2020\On_Connections_Between_Regularizations_for_Improving_DNN_Robustness\figure_5.jpg
  Figure 5 caption: 'The robustness of obtained multi-class classification models
    evaluated with FGSM, PGD, DeepFool, and the C&Ws attacks: (a)-(d) for LeNet-300-100
    and (e)-(h) for LeNet-5. Ten runs from different initializations were performed
    and the average results over the multiple runs are reported. The curvature regularization
    is not compared as approximations seem inevitable in its multi-class implementation.'
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.63
  Name of the first author: Yiwen Guo
  Name of the last author: Changshui Zhang
  Number of Figures: 5
  Number of Tables: 0
  Number of authors: 4
  Paper title: On Connections Between Regularizations for Improving DNN Robustness
  Publication Date: 2020-07-03 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3006917
- Affiliation of the first author: school of electronic information and communications,
    huazhong university of science and technology, wuhan, china
  Affiliation of the last author: university of illinois at urbana-champaign, champaign,
    il, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\CCNet_CrissCross_Attention_for_Semantic_Segmentation\figure_1.jpg
  Figure 1 caption: "Diagrams of two attention-based context aggregation methods.\
    \ (a) For each position (e.g., blue), the Non-local module [5] generates a dense\
    \ attention map which has N weights (in green). (b) For each position (e.g., blue),\
    \ the criss-cross attention module generates a sparse attention map which only\
    \ has about 2 N \u2212 \u2212 \u221A weights. After the recurrent operation, each\
    \ position (e.g., red) in the final output feature maps can collect information\
    \ from all pixels. For clear display, residual connections are ignored."
  Figure 10 Link: articels_figures_by_rev_year\2020\CCNet_CrissCross_Attention_for_Semantic_Segmentation\figure_10.jpg
  Figure 10 caption: Visualized examples for instance segmentation result on COCO
    val set.
  Figure 2 Link: articels_figures_by_rev_year\2020\CCNet_CrissCross_Attention_for_Semantic_Segmentation\figure_2.jpg
  Figure 2 caption: Overview of the proposed CCNet for semantic segmentation.
  Figure 3 Link: articels_figures_by_rev_year\2020\CCNet_CrissCross_Attention_for_Semantic_Segmentation\figure_3.jpg
  Figure 3 caption: The details of criss-cross attention module.
  Figure 4 Link: articels_figures_by_rev_year\2020\CCNet_CrissCross_Attention_for_Semantic_Segmentation\figure_4.jpg
  Figure 4 caption: An example of information propagation when the loop number is
    2.
  Figure 5 Link: articels_figures_by_rev_year\2020\CCNet_CrissCross_Attention_for_Semantic_Segmentation\figure_5.jpg
  Figure 5 caption: The details of 3D criss-cross attention module.
  Figure 6 Link: articels_figures_by_rev_year\2020\CCNet_CrissCross_Attention_for_Semantic_Segmentation\figure_6.jpg
  Figure 6 caption: Visualization results of RCCA with different loops on Cityscapes
    validation set.
  Figure 7 Link: articels_figures_by_rev_year\2020\CCNet_CrissCross_Attention_for_Semantic_Segmentation\figure_7.jpg
  Figure 7 caption: Visualization of attention module on Cityscapes validation set.
    The left column is the input images, the 2 and 3 columns are pixel-wise attention
    maps when R=1 and R=2 in RCCA.
  Figure 8 Link: articels_figures_by_rev_year\2020\CCNet_CrissCross_Attention_for_Semantic_Segmentation\figure_8.jpg
  Figure 8 caption: Visualized examples on ADE20K val set withwithout category consistent
    loss (CCL).
  Figure 9 Link: articels_figures_by_rev_year\2020\CCNet_CrissCross_Attention_for_Semantic_Segmentation\figure_9.jpg
  Figure 9 caption: Visualized examples for human parsing result on LIP val set.
  First author gender probability: 0.61
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Zilong Huang
  Name of the last author: Thomas S. Huang
  Number of Figures: 10
  Number of Tables: 9
  Number of authors: 7
  Paper title: 'CCNet: Criss-Cross Attention for Semantic Segmentation'
  Publication Date: 2020-07-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison With State-of-the-Arts on Cityscapes (Test)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance on Cityscapes (val) for Different Number of Loops
      in RCCA
  Table 3 caption:
    table_text: TABLE 3 Performance on Cityscapes (val) for Different Kinds of Category
      Consistent Loss
  Table 4 caption:
    table_text: TABLE 4 Comparison of Context Aggregation Approaches on Cityscapes
      (val)
  Table 5 caption:
    table_text: TABLE 5 Comparison of Non-Local Module and RCCA
  Table 6 caption:
    table_text: TABLE 6 Comparison With State-of-the-Arts on ADE20K (val)
  Table 7 caption:
    table_text: TABLE 7 Comparison With State-of-the-Arts on LIP (val)
  Table 8 caption:
    table_text: TABLE 8 Comparison on COCO (val)
  Table 9 caption:
    table_text: TABLE 9 Results on the CamVid Test Set
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3007032
- Affiliation of the first author: department of computer science, school of eecs,
    peking university, beijing, china
  Affiliation of the last author: department of computer science, school of eecs,
    peking university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2020\BDCN_BiDirectional_Cascade_Network_for_Perceptual_Edge_Detection\figure_1.jpg
  Figure 1 caption: Sample images and their ground truth edge maps in BSDS500 dataset.
    The scale of edges in one image varies considerably, like the boundaries of two
    persons annotated with yellow and red bounding boxes, respectively.
  Figure 10 Link: articels_figures_by_rev_year\2020\BDCN_BiDirectional_Cascade_Network_for_Perceptual_Edge_Detection\figure_10.jpg
  Figure 10 caption: Sample edge detection results obtained by BDCN and several recent
    approaches on NYUDv2.
  Figure 2 Link: articels_figures_by_rev_year\2020\BDCN_BiDirectional_Cascade_Network_for_Perceptual_Edge_Detection\figure_2.jpg
  Figure 2 caption: The overall architecture of BDCN. ID Block denotes the Incremental
    Detection Block, which is the basic component of BDCN. Each ID Block is trained
    by a layer-specific supervision inferred by the bi-directional cascade structure.
    This structure trains each ID Block to spot edges at a proper scale. The predictions
    of ID Blocks are fused as the final result.
  Figure 3 Link: articels_figures_by_rev_year\2020\BDCN_BiDirectional_Cascade_Network_for_Perceptual_Edge_Detection\figure_3.jpg
  Figure 3 caption: The detailed architecture of BDCN and SEM. The illustrated BDCN
    is modified based on VGG16 [49]. The number of ID Blocks in BDCN can be flexibly
    set from 2 to 5 (see Fig. 6) to achieve a better trade-off between efficiency
    and accuracy.
  Figure 4 Link: articels_figures_by_rev_year\2020\BDCN_BiDirectional_Cascade_Network_for_Perceptual_Edge_Detection\figure_4.jpg
  Figure 4 caption: "Examples of edges detected by different ID Blocks (IDB for short).\
    \ Each ID Block generates two edge predictions, P s2d and P d2s , respectively.\u201C\
    fused\u201D denotes the final prediction and \u201CGT\u201D denotes the groundtruth\
    \ edge map."
  Figure 5 Link: articels_figures_by_rev_year\2020\BDCN_BiDirectional_Cascade_Network_for_Perceptual_Edge_Detection\figure_5.jpg
  Figure 5 caption: Comparison of edge detection accuracy as we decrease the number
    of ID Blocks from 5 to 2. HED learned with VGG16 is denoted as the solid line
    for comparison.
  Figure 6 Link: articels_figures_by_rev_year\2020\BDCN_BiDirectional_Cascade_Network_for_Perceptual_Edge_Detection\figure_6.jpg
  Figure 6 caption: "Comparison of parameters and performance with other methods.\
    \ The number behind \u201CBDCN\u201C indicates the number of ID Block. \u2021\
    means the multiscale results."
  Figure 7 Link: articels_figures_by_rev_year\2020\BDCN_BiDirectional_Cascade_Network_for_Perceptual_Edge_Detection\figure_7.jpg
  Figure 7 caption: Comparison of edge detection results on BSDS500 test set. GT denotes
    the groundtruth edge map. All the results except for the last row are raw edge
    maps computed with a single scale input before Non-Maximum Suppression.
  Figure 8 Link: articels_figures_by_rev_year\2020\BDCN_BiDirectional_Cascade_Network_for_Perceptual_Edge_Detection\figure_8.jpg
  Figure 8 caption: The precision-recall curves of our method and other works on BSDS500
    test set.
  Figure 9 Link: articels_figures_by_rev_year\2020\BDCN_BiDirectional_Cascade_Network_for_Perceptual_Edge_Detection\figure_9.jpg
  Figure 9 caption: The precision-recall curves of our method and other works on the
    NYUDv2 dataset.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jianzhong He
  Name of the last author: Tiejun Huang
  Number of Figures: 14
  Number of Tables: 9
  Number of authors: 5
  Paper title: 'BDCN: Bi-Directional Cascade Network for Perceptual Edge Detection'
  Publication Date: 2020-07-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Impact of SEM Parameters to the Edge Detection Performance
      on BSDS500 Validation Set
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Validity of Components in BDCN on BSDS500 Validation Set With
      the VGG16 Backbone
  Table 3 caption:
    table_text: TABLE 3 Validity of Components in BDCN on BSDS500 Validation Set With
      the ResNet50 Backbone
  Table 4 caption:
    table_text: TABLE 4 The Performance (ODS) of Each Layer in BDCN, RCF [10], and
      HED [16] on BSDS500 Test Set
  Table 5 caption:
    table_text: TABLE 5 Comparison With Other Methods on BSDS500 Test Set
  Table 6 caption:
    table_text: TABLE 6 Comparison With Recent Works on NYUDv2 Dataset
  Table 7 caption:
    table_text: TABLE 7 Comparison With Recent Works on Multicue
  Table 8 caption:
    table_text: TABLE 8 Optimal Flow Estimation Performance Achieved With HED, CED,
      and BDCN by the Epicflow [5] on Sintel Dataset
  Table 9 caption:
    table_text: TABLE 9 The Comparison of Results for Segmentation on the Pascal context
      Validation Set
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3007074
- Affiliation of the first author: department of computer science, university of southern
    california, los angeles, ca, usa
  Affiliation of the last author: pinscreen, los angeles, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\A_General_Differentiable_Mesh_Renderer_for_ImageBased_D_Reasoning\figure_1.jpg
  Figure 1 caption: "We propose Soft Rasterizer mathcal R (upper), a natually differentiable\
    \ renderer, which formulates rendering as a differentiable aggregating process\
    \ mathcal A(cdot) that fuses per-triangle contributions lbrace mathcal Dirbrace\
    \ in a \u201Csoft\u201D probabilistic manner. Our approach attacks the core problem\
    \ of differentiating the standard rasterizer, which cannot propagate gradients\
    \ from pixels to geometry due to the discrete sampling operation (below)."
  Figure 10 Link: articels_figures_by_rev_year\2020\A_General_Differentiable_Mesh_Renderer_for_ImageBased_D_Reasoning\figure_10.jpg
  Figure 10 caption: Single-view reconstruction results on real images.
  Figure 2 Link: articels_figures_by_rev_year\2020\A_General_Differentiable_Mesh_Renderer_for_ImageBased_D_Reasoning\figure_2.jpg
  Figure 2 caption: 'Forward rendering (left): various rendering effects generated
    by SoftRas by tuning the degree of transparency and blurriness. Applications based
    on the backward gradients provided by SoftRas: (1) 3D unsupervised mesh reconstruction
    from a single input image (middle) and (2) 3D pose fitting to the target image
    by flowing gradient to the occluded triangles (right).'
  Figure 3 Link: articels_figures_by_rev_year\2020\A_General_Differentiable_Mesh_Renderer_for_ImageBased_D_Reasoning\figure_3.jpg
  Figure 3 caption: Comparisons between the standard rendering pipeline (upper branch)
    and our rendering framework (lower branch).
  Figure 4 Link: articels_figures_by_rev_year\2020\A_General_Differentiable_Mesh_Renderer_for_ImageBased_D_Reasoning\figure_4.jpg
  Figure 4 caption: Probability maps of a triangle under euclidean (upper) and barycentric
    (lower) metric. (a) definition of pixel-to-triangle distance; (b)-(d) probability
    maps generated with different sigma .
  Figure 5 Link: articels_figures_by_rev_year\2020\A_General_Differentiable_Mesh_Renderer_for_ImageBased_D_Reasoning\figure_5.jpg
  Figure 5 caption: Comparisons with prior differentiable renderers in terms of gradient
    flow.
  Figure 6 Link: articels_figures_by_rev_year\2020\A_General_Differentiable_Mesh_Renderer_for_ImageBased_D_Reasoning\figure_6.jpg
  Figure 6 caption: The proposed framework for single-view mesh reconstruction.
  Figure 7 Link: articels_figures_by_rev_year\2020\A_General_Differentiable_Mesh_Renderer_for_ImageBased_D_Reasoning\figure_7.jpg
  Figure 7 caption: Network structure for color reconstruction.
  Figure 8 Link: articels_figures_by_rev_year\2020\A_General_Differentiable_Mesh_Renderer_for_ImageBased_D_Reasoning\figure_8.jpg
  Figure 8 caption: "3D mesh reconstruction from a single image. From left to right,\
    \ we show input image, ground truth, the results of our method (SoftRas), Neural\
    \ Mesh Renderer [2] and Pixel2mesh [37] \u2013 all visualized from 2 different\
    \ views. Along with the results, we also visualize mesh-to-scan distances measured\
    \ from reconstructed mesh to ground truth."
  Figure 9 Link: articels_figures_by_rev_year\2020\A_General_Differentiable_Mesh_Renderer_for_ImageBased_D_Reasoning\figure_9.jpg
  Figure 9 caption: Results of colorized mesh reconstruction. The learned principal
    colors and their usage histogram are visualize on the right.
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Shichen Liu
  Name of the last author: Hao Li
  Number of Figures: 14
  Number of Tables: 3
  Number of authors: 4
  Paper title: A General Differentiable Mesh Renderer for Image-Based 3D Reasoning
  Publication Date: 2020-07-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Mean IoU With Other 3D Unsupervised Reconstruction
      Methods on 13 Categories of ShapeNet Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Ablation Study of the Regularizer and Various Forms of Distance
      and Aggregate Functions
  Table 3 caption:
    table_text: TABLE 3 Comparison of Cube Rotation Estimation Error With NMR, Measured
      in Mean Relative Angular Error
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3007759
- Affiliation of the first author: school of computer science and the center for optical
    imagery analysis and learning (optimal), northwestern polytechnical university,
    xian, shaanxi, china
  Affiliation of the last author: school of computer science and the center for optical
    imagery analysis and learning (optimal), northwestern polytechnical university,
    xian, shaanxi, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Robust_BiStochastic_Graph_Regularized_Matrix_Factorization_for_Data_Clustering\figure_1.jpg
  Figure 1 caption: Curves of the loss functions of Hx(x) , L 2 (x) and L 1 (x) .
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Robust_BiStochastic_Graph_Regularized_Matrix_Factorization_for_Data_Clustering\figure_2.jpg
  Figure 2 caption: Comparison on synthetic data of three NMF methods.
  Figure 3 Link: articels_figures_by_rev_year\2020\Robust_BiStochastic_Graph_Regularized_Matrix_Factorization_for_Data_Clustering\figure_3.jpg
  Figure 3 caption: "Some noisy images of ORL dataset. The upper images have 4\xD7\
    4 block noises, and the lower images have 8\xD78 block noises."
  Figure 4 Link: articels_figures_by_rev_year\2020\Robust_BiStochastic_Graph_Regularized_Matrix_Factorization_for_Data_Clustering\figure_4.jpg
  Figure 4 caption: "Convergence of the proposed Hx-NMF, NMF and \u2113 2,1 -NMF."
  Figure 5 Link: articels_figures_by_rev_year\2020\Robust_BiStochastic_Graph_Regularized_Matrix_Factorization_for_Data_Clustering\figure_5.jpg
  Figure 5 caption: "Clustering ACC of the proposed RBSMF with respect to the parameters\
    \ \u03B1 and \u03B2 in eight datasets."
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Qi Wang
  Name of the last author: Xu Jiang
  Number of Figures: 5
  Number of Tables: 7
  Number of authors: 3
  Paper title: Robust Bi-Stochastic Graph Regularized Matrix Factorization for Data
    Clustering
  Publication Date: 2020-07-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Descriptions of all Notations
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Weights of 6 Outliers for all Methods
  Table 3 caption:
    table_text: TABLE 3 Experimental Results of ORL Dataset With Some Noises
  Table 4 caption:
    table_text: TABLE 4 Descriptions of Eight Datasets
  Table 5 caption:
    table_text: TABLE 5 ACC of Ten Clustering Algorithms on Eight Datasets
  Table 6 caption:
    table_text: TABLE 6 NMI of Ten Clustering Algorithms on Eight Datasets
  Table 7 caption:
    table_text: TABLE 7 PUR of Ten Clustering Algorithms on Eight Datasets
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3007673
