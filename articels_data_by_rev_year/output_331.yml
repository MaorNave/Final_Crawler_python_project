- Affiliation of the first author: department of computer science and engineering
    and the center for evolutionary medicine and informatics of the biodesign institute,
    arizona state university, tempe, usa
  Affiliation of the last author: department of computer science and engineering and
    the center for evolutionary medicine and informatics of the biodesign institute,
    arizona state university, tempe, usa
  Figure 1 Link: articels_figures_by_rev_year\2015\Fused_Lasso_Screening_Rules_via_the_Monotonicity_of_Subdifferentials\figure_1.jpg
  Figure 1 caption: Comparison of EDPP and FLAMS on synthetic datasets with different
    number of segments in the ground truth. The first and third rows show the rejection
    ratios. The second and fourth rows show the speedup gained by EDPP and FLAMS,
    respectively.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Fused_Lasso_Screening_Rules_via_the_Monotonicity_of_Subdifferentials\figure_2.jpg
  Figure 2 caption: The speedup gained by EDPP and FLAMS on three real datasets.
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.54
  Name of the first author: Jie Wang
  Name of the last author: Jieping Ye
  Number of Figures: 2
  Number of Tables: 2
  Number of authors: 3
  Paper title: Fused Lasso Screening Rules via the Monotonicity of Subdifferentials
  Publication Date: 2015-01-06 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Running Time (in Seconds) for Solving the Fused Lasso Problems\
      \ along a Sequence of 200 Tuning Parameter Values Equally Spaced on the Logarithmic\
      \ Scale of \u03BB \u03BB max from 1.0 to 0.05 by (a): the Solver [9] (Reported\
      \ in the 3rd Column) without Screening; (b): the Solver Combined with Different\
      \ Screening Methods (See the 4th and 5th Columns). The Last Two Columns Report\
      \ the Total Running Time (in Seconds) of the Screening Methods"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Running Time (in Seconds) for Solving the Fused Lasso Problems\
      \ along a Sequence of 200 Tuning Parameter Values Equally Spaced on the Logarithmic\
      \ Scale of \u03BB \u03BB max from 1.0 to 0.05 by (a): the Solver [9] (Reported\
      \ in the 2nd Column) without Screening; (b): the Solver Combined with Different\
      \ Screening Methods (See the 3rd and 4th Columns). The Last Two Columns Report\
      \ the Total Running Time (in Seconds) for the Screening Methods"
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2388203
- Affiliation of the first author: engineering and applied sciences, harvard university,
    33 oxford st., cambridge, ma
  Affiliation of the last author: department of statistics, university of oxford,
    1 south parks road, oxford, ox1 3tg, united kingdom
  Figure 1 Link: Not Available
  Figure 1 caption: Not Available
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.67
  Name of the first author: Ryan P. Adams
  Name of the last author: Yee Whye Teh
  Number of Figures: Not Available
  Number of Tables: 0
  Number of authors: 4
  Paper title: "Guest Editors\u2019 Introduction to the Special Issue on Bayesian\
    \ Nonparametrics"
  Publication Date: 2015-01-07 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2380478
- Affiliation of the first author: visual computing group, microsoft research, beijing,
    china
  Affiliation of the last author: visual computing group, microsoft research, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2015\Spatial_Pyramid_Pooling_in_Deep_Convolutional_Networks_for_Visual_Recognition\figure_1.jpg
  Figure 1 caption: 'Top: Cropping or warping to fit a fixed size. Middle: A conventional
    CNN. Bottom: our spatial pyramid pooling network structure.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Spatial_Pyramid_Pooling_in_Deep_Convolutional_Networks_for_Visual_Recognition\figure_2.jpg
  Figure 2 caption: Visualization of the feature maps. (a) Two images in Pascal VOC
    2007. (b) The feature maps of some conv 5 filters. The arrows indicate the strongest
    responses and their corresponding positions in the images. (c) The ImageNet images
    that have the strongest responses of the corresponding filters. The green rectangles
    mark the receptive fields of the strongest responses.
  Figure 3 Link: articels_figures_by_rev_year\2015\Spatial_Pyramid_Pooling_in_Deep_Convolutional_Networks_for_Visual_Recognition\figure_3.jpg
  Figure 3 caption: A network structure with a spatial pyramid pooling layer. Here
    256 is the filter number of the conv 5 layer, and conv 5 is the last convolutional
    layer.
  Figure 4 Link: articels_figures_by_rev_year\2015\Spatial_Pyramid_Pooling_in_Deep_Convolutional_Networks_for_Visual_Recognition\figure_4.jpg
  Figure 4 caption: An example three-level pyramid pooling in the cuda-convnet style
    [3]. Here sizeX is the size of the pooling window. This configuration is for a
    network whose feature map size of conv 5 is 13 times 13, so the pool 3times 3
    , pool 2times 2 , and pool 1times 1 layers will have 3 times 3, 2 times 2, and
    1 times 1 bins respectively.
  Figure 5 Link: articels_figures_by_rev_year\2015\Spatial_Pyramid_Pooling_in_Deep_Convolutional_Networks_for_Visual_Recognition\figure_5.jpg
  Figure 5 caption: Pooling features from arbitrary windows on feature maps. The feature
    maps are computed from the entire image. The pooling is performed in candidate
    windows.
  Figure 6 Link: articels_figures_by_rev_year\2015\Spatial_Pyramid_Pooling_in_Deep_Convolutional_Networks_for_Visual_Recognition\figure_6.jpg
  Figure 6 caption: "Example detection results of \u201CSPP-net ftfc 7 bb\u201D on\
    \ the Pascal VOC 2007 testing set (59.2 percent mAP). All windows with scores\
    \ > 0 are shown. The predicted categoryscore are marked. The window color is associated\
    \ with the predicted category. These images are manually selected because we find\
    \ them impressive. Visit our project website to see all 4,952 detection results\
    \ in the testing set."
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Kaiming He
  Name of the last author: Jian Sun
  Number of Figures: 6
  Number of Tables: 13
  Number of authors: 4
  Paper title: Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition
  Publication Date: 2015-01-09 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Network Architectures: Filter Number \xD7 Filter Size (e.g.,\
      \ 96\xD7 7 2 ), Filter Stride (e.g., str 2 ), Pooling Window Size (e.g., Pool\
      \ 3 2 ), and the Output Feature Map Size (e.g., map size 55\xD755 )"
  Table 10 caption:
    table_text: TABLE 10 Detection Results (mAP) on Pascal VOC 2007, Using the Same
      Pre-Trained Model of SPP (ZF-5)
  Table 2 caption:
    table_text: TABLE 2 Error Rates in the Validation Set of ImageNet 2012
  Table 3 caption:
    table_text: TABLE 3 Error Rates in the Validation Set of ImageNet 2012 Using a
      Single View
  Table 4 caption:
    table_text: TABLE 4 Error Rates in ImageNet 2012
  Table 5 caption:
    table_text: TABLE 5 The Competition Results of ILSVRC 2014 Classification [26]
  Table 6 caption:
    table_text: TABLE 6 Classification mAP in Pascal VOC 2007
  Table 7 caption:
    table_text: TABLE 7 Classification Accuracy in Caltech101
  Table 8 caption:
    table_text: TABLE 8 Classification Results for Pascal VOC 2007 (mAP) and Caltech101
      (Accuracy)
  Table 9 caption:
    table_text: TABLE 9 Detection Results (mAP) on Pascal VOC 2007
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2389824
- Affiliation of the first author: google inc., 1600 amphitheatre pkwy, mountain view,
    ca
  Affiliation of the last author: "max-planck-institut f\xFCr informatik, campus e1-4,,\
    \ saarbr\xFCcken, germany"
  Figure 1 Link: articels_figures_by_rev_year\2015\Efficient_Learning_of_Image_SuperResolution_and_Compression_Artifact_Removal_wit\figure_1.jpg
  Figure 1 caption: "Approximation accuracies of sparse GPs and semi-local GPs wrt.\
    \ full GP for the super-resolution experiment data points. Top: The Kullback-Leibler\
    \ divergences from full GPs of the predictive distributions of approximate GPs,\
    \ plotted against m \u2032 sparse GP inducing inputs. To compare, we use only\
    \ 20,000 data points to train all models, sampled from a large training set of\
    \ 200,000 . For each m \u2032 , a semi-local GP was trained, with number of inducing\
    \ inputs m and local training data points n such that the time complexity of test\
    \ point prediction roughly matches ( ( m \u2032 ) 2 \u2248 m 2 n ). Experiments\
    \ were repeated 10 times with randomly selected training data sets. Error bar\
    \ lengths are 2\xD7 std. dev. Bottom: Average PSNR increase from bicubic resampling,\
    \ measured from final super-resolution results. For comparison, we replace our\
    \ regression by linear regression and NN regression. The inducing inputs were\
    \ optimized by maximizing the marginal likelihood p( f \u2217 |u) [8]."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Efficient_Learning_of_Image_SuperResolution_and_Compression_Artifact_Removal_wit\figure_2.jpg
  Figure 2 caption: 'Variation of P2 as a function P1 in super-resolution experiments
    (see text for P1 and P2 descriptions): For each test input, we select 10 nearest
    neighbors and calculate the corresponding P1 and P2 values.'
  Figure 3 Link: articels_figures_by_rev_year\2015\Efficient_Learning_of_Image_SuperResolution_and_Compression_Artifact_Removal_wit\figure_3.jpg
  Figure 3 caption: 'An example of a major edge: (a) an image showing a strong edge
    (bicubic-resampled image is shown) and (b) cross-sections of the super-resolution
    results at the locations marked with a white horizontal bar in (a). In the result
    of our algorithm that uses the homogeneous noise model (denoted as ''uniform'')
    a fluctuation occurred at the vicinity of the edge (pixel indices 4 and 9). This
    ringing is suppressed with the adaptive noise model.'
  Figure 4 Link: articels_figures_by_rev_year\2015\Efficient_Learning_of_Image_SuperResolution_and_Compression_Artifact_Removal_wit\figure_4.jpg
  Figure 4 caption: "Examples of image super-resolution\u2014please zoom into the\
    \ electronic version! (a) Bicubic resampling, (b) Chang et al. [11], (c) Freeman\
    \ et al. [14], (d) He and Siu [13] (code courtesy of He), (e) Kim and Kwon [6],\
    \ and (f) our method. Increases of PSNRs (in dB) and SSIMs with respect to the\
    \ input bicubic resampled images (displayed below each column) were calculated\
    \ based on the complete images. For the input images (a), the original PSNR and\
    \ SSIM values are shown. The best results are marked with bold letters. See supplemental\
    \ material Fig. 17 for uncropped originals, available online. Note noise in (c),\
    \ blur in (b), smoothed texture details (woman; fourth column) and over-sharpen\
    \ edges (astronauts; third column) in (d). (e) and (f) are similar, but (f) shows\
    \ fewer ringing artifacts (lighthouse; second column)."
  Figure 5 Link: articels_figures_by_rev_year\2015\Efficient_Learning_of_Image_SuperResolution_and_Compression_Artifact_Removal_wit\figure_5.jpg
  Figure 5 caption: "Top: Bicubic resampling. Middle: Kim and Kwon [6] . Bottom: Our\
    \ method with dataset-specific DBs. Left: Face-specific super-resolution: [6]\
    \ \u0394 PSNR: 1.18 dB, \u0394 SSIM: 0.032 ; our method \u0394 PSNR: 2.10 dB,\
    \ \u0394 SSIM: 0.054 ; see supplemental material for experimental setup, available\
    \ online. Right: Document-specific super-resolution; 2\xD7 and 3\xD7 magnification.\
    \ Note our results (bottom) are more detailed (faces), and sharper (documents)\
    \ than competing results (middle, [6])."
  Figure 6 Link: articels_figures_by_rev_year\2015\Efficient_Learning_of_Image_SuperResolution_and_Compression_Artifact_Removal_wit\figure_6.jpg
  Figure 6 caption: "Examples of JPEG artifact suppression\u2014please zoom into the\
    \ electronic version! (a) Input JPEG images, (b) re-application of JPEG [15],\
    \ (c) SADCT [17] , (d) Edge-perts [1], (e) and (f) FOE [3] applied to (a) and\
    \ (b), respectively, and (g) our method. See supplemental material Fig. 18 for\
    \ uncropped originals, available online. Note blur in (b), (d), and (f) and remaining\
    \ block artifacts (biker; first column) in (e). Our approach (g) is slightly more\
    \ detailed (woman; third column) and has fewer ringing artifacts (tiger; fourth\
    \ column) than (c)."
  Figure 7 Link: articels_figures_by_rev_year\2015\Efficient_Learning_of_Image_SuperResolution_and_Compression_Artifact_Removal_wit\figure_7.jpg
  Figure 7 caption: "Plot of \u03BA(r) as a function of distance r(\u22C5)=\u2225\
    \ x \u2217 \u2212\u22C5 \u2225 2 for the data points used in super-resolution\
    \ experiments: 20,000 training data points are used in calculating K f,f while\
    \ a distinct set of 20,000 data points are used as the evaluation points x \u2217\
    \ . The gray area corresponds to twice the standard deviations."
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.94
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Younghee Kwon
  Name of the last author: Christian Theobalt
  Number of Figures: 7
  Number of Tables: 2
  Number of authors: 5
  Paper title: Efficient Learning of Image Super-Resolution and Compression Artifact
    Removal with Semi-Local Gaussian Processes
  Publication Date: 2015-01-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Parameters Used in the Experiments
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Performance of Different Image Enhancement Algorithms for\
      \ Two Example Applications: Increases of PSNRs (dB) and SSIMs ( \xD7 10 \u2212\
      2 ); Mean and Standard Deviation"
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2389797
- Affiliation of the first author: department of media analytics, nec laboratories
    america, cupertino, ca, 95014
  Affiliation of the last author: department of media analytics, nec laboratories
    america, cupertino, ca, 95014
  Figure 1 Link: articels_figures_by_rev_year\2015\Regionlets_for_Generic_Object_Detection\figure_1.jpg
  Figure 1 caption: Illustration of the regionlet representation. Regionlet representation
    can be applied to candidate bounding boxes that have different sizes and aspect
    ratios. A regionlet-based model is composed of a number of regions (denoted by
    blue rectangles), and then each region is represented by a group of competing
    regionlets (denoted by the small orange rectangles inside each region).
  Figure 10 Link: articels_figures_by_rev_year\2015\Regionlets_for_Generic_Object_Detection\figure_10.jpg
  Figure 10 caption: Statistics of number of regionlets used for each class. Deformable
    objects generally prefer more regionlets.
  Figure 2 Link: articels_figures_by_rev_year\2015\Regionlets_for_Generic_Object_Detection\figure_2.jpg
  Figure 2 caption: Illustration of the relationship among a detection bounding box,
    a feature extraction region and regionlets. A feature extraction region R , shown
    as a light blue rectangle, is cropped from a fixed position from three samples
    of a person. Inside R , several small subregions denoted as r 1 , r 2 and r 3
    (in orange small rectangles) are the regionlets to capture the possible locations
    of the hand for person detection.
  Figure 3 Link: articels_figures_by_rev_year\2015\Regionlets_for_Generic_Object_Detection\figure_3.jpg
  Figure 3 caption: Relative regions normalized by a candidate window adapt to scale
    and aspect ratio changes. Feature extraction region for a regionletregion is jointly
    determined by the relative coordinates of the regionletregion and the target detection
    window.
  Figure 4 Link: articels_figures_by_rev_year\2015\Regionlets_for_Generic_Object_Detection\figure_4.jpg
  Figure 4 caption: Example of regionlet-based feature extraction.
  Figure 5 Link: articels_figures_by_rev_year\2015\Regionlets_for_Generic_Object_Detection\figure_5.jpg
  Figure 5 caption: Illustration of an integral image. I(x,y) is the summation of
    feature values of all pixels which are located to the top-left of (x,y) , including
    the pixel itself. The sum of features in a rectangular region is computed by three
    operations on the integral image.
  Figure 6 Link: articels_figures_by_rev_year\2015\Regionlets_for_Generic_Object_Detection\figure_6.jpg
  Figure 6 caption: "Inductive integral image computation. The integral value at current\
    \ point (x,y) equals to the summation of its top immediate integral value at (x,y\u2212\
    1) and the sum of all the pixel values in the current row till (x,y) ."
  Figure 7 Link: articels_figures_by_rev_year\2015\Regionlets_for_Generic_Object_Detection\figure_7.jpg
  Figure 7 caption: 'Illustration of a support pixel integral image: (a) Two-dimensional
    features at four pixel locations, (b) The grid specified by these four locations,
    (c) The integral vectors on the support pixels, and (d) The dense hashing map.'
  Figure 8 Link: articels_figures_by_rev_year\2015\Regionlets_for_Generic_Object_Detection\figure_8.jpg
  Figure 8 caption: The Deep CNN architecture used to extract features.
  Figure 9 Link: articels_figures_by_rev_year\2015\Regionlets_for_Generic_Object_Detection\figure_9.jpg
  Figure 9 caption: The regionlet-based detector learns a single classification model
    but automatically adapts to objects with arbitrary sizes and aspect ratios, without
    building image pyramids.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.64
  Name of the first author: Xiaoyu Wang
  Name of the last author: Yuanqing Lin
  Number of Figures: 11
  Number of Tables: 8
  Number of authors: 4
  Paper title: Regionlets for Generic Object Detection
  Publication Date: 2015-01-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance Comparison with the Baselines on the PASCAL VOC
      2007 Dataset (Average Precision Percentage)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance Comparison with the Baselines on the PASCAL VOC
      2010 Dataset (Average Precision Percentage)
  Table 3 caption:
    table_text: TABLE 3 Performance of Different Features on the PASCAL VOC 2007 Dataset
  Table 4 caption:
    table_text: TABLE 4 Comparison with State of the Arts Using mAP Over 20 Classes
  Table 5 caption:
    table_text: TABLE 5 Performance of Regionlets Using DCNN Responses on the PASCAL
      VOC 2007 Dataset (Average Precision Percentage)
  Table 6 caption:
    table_text: TABLE 6 Performance of Regionlets Using DCNN Responses on the PASCAL
      VOC 2010 Dataset (Average Precision Percentage)
  Table 7 caption:
    table_text: TABLE 7 Comparison with the DPM on the ImageNet dataset, Trained on
      the Training Data and Tested on the Validation Data
  Table 8 caption:
    table_text: TABLE 8 Object Detection Performance in the ImageNet 2013 Challenge,
      Trained on the Training + Validation Data and Tested on the Test Data
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2389830
- Affiliation of the first author: electrical and computer engineering department,
    carnegie mellon university, pittsburgh, pa
  Affiliation of the last author: computer science and engineering department, arizona
    state university, tempe, az
  Figure 1 Link: articels_figures_by_rev_year\2015\Active_Batch_Selection_via_Convex_Relaxations_with_Guaranteed_Solution_Bounds\figure_1.jpg
  Figure 1 caption: Batch mode active learning on the UCI datasets. (Figures best
    viewed in color.)
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Active_Batch_Selection_via_Convex_Relaxations_with_Guaranteed_Solution_Bounds\figure_2.jpg
  Figure 2 caption: Batch mode active learning face recognition and facial expression
    recognition datasets. (Figures best viewed in color.)
  Figure 3 Link: articels_figures_by_rev_year\2015\Active_Batch_Selection_via_Convex_Relaxations_with_Guaranteed_Solution_Bounds\figure_3.jpg
  Figure 3 caption: Multi-label batch mode active learning on the scene and yeast
    datasets. (Figures best viewed in color.)
  Figure 4 Link: articels_figures_by_rev_year\2015\Active_Batch_Selection_via_Convex_Relaxations_with_Guaranteed_Solution_Bounds\figure_4.jpg
  Figure 4 caption: Validation of solution bounds of batchrank and batchrand. (Figures
    best viewed in color.)
  Figure 5 Link: articels_figures_by_rev_year\2015\Active_Batch_Selection_via_Convex_Relaxations_with_Guaranteed_Solution_Bounds\figure_5.jpg
  Figure 5 caption: Noise sensitivity of batchrank and batchrand on the VidTIMIT dataset.
    (Figures best viewed in color.)
  Figure 6 Link: articels_figures_by_rev_year\2015\Active_Batch_Selection_via_Convex_Relaxations_with_Guaranteed_Solution_Bounds\figure_6.jpg
  Figure 6 caption: 'Class imbalance: VidTIMIT dataset. (Figures best viewed in color.)'
  Figure 7 Link: articels_figures_by_rev_year\2015\Active_Batch_Selection_via_Convex_Relaxations_with_Guaranteed_Solution_Bounds\figure_7.jpg
  Figure 7 caption: 'Class imbalance: Confusion matrices for random selection, batchrank,
    and batchrand (Max trace = 4,500): VidTIMIT dataset.'
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Shayok Chakraborty
  Name of the last author: Jieping Ye
  Number of Figures: 7
  Number of Tables: 3
  Number of authors: 5
  Paper title: Active Batch Selection via Convex Relaxations with Guaranteed Solution
    Bounds
  Publication Date: 2015-01-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Dataset Details
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Time Taken (in Seconds) to Query a Batch of Samples from an
      Unlabeled Set
  Table 3 caption:
    table_text: TABLE 3 Time Taken (in Seconds) to Query a Batch of Samples from an
      Unlabeled Set
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2389848
- Affiliation of the first author: institute of industrial science, the university
    of tokyo, tokyo, japan
  Affiliation of the last author: institute of industrial science, the university
    of tokyo, tokyo, japan
  Figure 1 Link: articels_figures_by_rev_year\2015\From_Intensity_Profile_to_Surface_Normal_Photometric_Stereo_for_Unknown_Light_So\figure_1.jpg
  Figure 1 caption: Illustration of intensity profiles. Surface points A, B, and C
    have the same reflectance, but D is different. A, C, and D have the same surface
    normal, while B has a different normal.
  Figure 10 Link: articels_figures_by_rev_year\2015\From_Intensity_Profile_to_Surface_Normal_Photometric_Stereo_for_Unknown_Light_So\figure_10.jpg
  Figure 10 caption: Average surface normal error w.r.t. normal range. Spherical plots
    show the average error maps.
  Figure 2 Link: articels_figures_by_rev_year\2015\From_Intensity_Profile_to_Surface_Normal_Photometric_Stereo_for_Unknown_Light_So\figure_2.jpg
  Figure 2 caption: Using geodesic distance (right) preserves a linear relationship
    over a greater range of angular differences in comparison with using Euclidean
    distance (left).
  Figure 3 Link: articels_figures_by_rev_year\2015\From_Intensity_Profile_to_Surface_Normal_Photometric_Stereo_for_Unknown_Light_So\figure_3.jpg
  Figure 3 caption: "Examples of the linear relationship. Top: four typical distributions\
    \ and their partial linear fittings (solid lines) are shown under uniform lightings.\
    \ Dotted lines show non-uniform light cases. Bottom: setting different angular\
    \ threshold \u03B4 varies the estimation errors."
  Figure 4 Link: articels_figures_by_rev_year\2015\From_Intensity_Profile_to_Surface_Normal_Photometric_Stereo_for_Unknown_Light_So\figure_4.jpg
  Figure 4 caption: Intensity profiles w.r.t. material and surface normal. The top
    row shows intensity profiles of a specular surface recorded at two distinct surface
    normals. The bottom row shows those of a diffuse surface. The intensity distributions
    are plotted on their right-hand side.
  Figure 5 Link: articels_figures_by_rev_year\2015\From_Intensity_Profile_to_Surface_Normal_Photometric_Stereo_for_Unknown_Light_So\figure_5.jpg
  Figure 5 caption: 'Relationship between skewness values and alpha -1m values of
    100 materials. The correlation coefficients are 0.99 (Topbottom: 162 42 light
    directions).'
  Figure 6 Link: articels_figures_by_rev_year\2015\From_Intensity_Profile_to_Surface_Normal_Photometric_Stereo_for_Unknown_Light_So\figure_6.jpg
  Figure 6 caption: 'Matching-based photometric stereo: determining surface normals,
    light directions, and material of the test scene by matching intensity values
    between test data and reference data.'
  Figure 7 Link: articels_figures_by_rev_year\2015\From_Intensity_Profile_to_Surface_Normal_Photometric_Stereo_for_Unknown_Light_So\figure_7.jpg
  Figure 7 caption: For light source estimation, we find light source boldsymbolmathrm
    st and surface normal boldsymbolmathrm nt parallel to the viewing direction, also
    a set of normals lbrace boldsymbolmathrm nterbrace with similar elevation angles.
  Figure 8 Link: articels_figures_by_rev_year\2015\From_Intensity_Profile_to_Surface_Normal_Photometric_Stereo_for_Unknown_Light_So\figure_8.jpg
  Figure 8 caption: The estimated azimuth angles of lbrace boldsymbolmathrm nterbrace
    for two materials are shown in two rows. The spots' positions in the plots are
    the dimensionality reduction results of lbrace boldsymbolmathrm Iterbrace . They
    correctly convey the relative azimuth angles of lbrace boldsymbolmathrm nterbrace
    and the color shows the ground truth azimuth angles. Note the 2D rotation and
    flip ambiguity.
  Figure 9 Link: articels_figures_by_rev_year\2015\From_Intensity_Profile_to_Surface_Normal_Photometric_Stereo_for_Unknown_Light_So\figure_9.jpg
  Figure 9 caption: 'Synthetic surfaces used in the experiments: hemisphere, Bunny,
    Dragon, Rabbit, Beethoven, Lion, and Happy Buddha.'
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Feng Lu
  Name of the last author: Yoichi Sato
  Number of Figures: 17
  Number of Tables: 5
  Number of authors: 5
  Paper title: 'From Intensity Profile to Surface Normal: Photometric Stereo for Unknown
    Light Sources and Isotropic Reflectances'
  Publication Date: 2015-01-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average Normal Light Errors of 100 Materials
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Average Angular Errors of Surface Normal Estimates
      for 100 Materials and 16242 Light Directions
  Table 3 caption:
    table_text: TABLE 3 Results withwithout Exact Reference Material
  Table 4 caption:
    table_text: TABLE 4 Normal Error Increments Due to Non-Uniformity of Lights
  Table 5 caption:
    table_text: TABLE 5 Results for 3D Models with Six Non-Lambertian Materials
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2389841
- Affiliation of the first author: singapore institute for neurotechnology (sinapse),
    national university of singapore, singapore
  Affiliation of the last author: institut de la vision, universite pierre et marie
    curie, paris, france
  Figure 1 Link: articels_figures_by_rev_year\2015\HFirst_A_Temporal_Approach_to_Object_Recognition\figure_1.jpg
  Figure 1 caption: Event-based vision sensor acquisition principle. (a) typical signal
    showing the log of luminance of a pixel located at [u,v ] T . Dotted lines show
    how the thresholds for detecting increases and decreases in intensity change as
    outputs are generated. (b) asynchronous temporal contrast events generated by
    this pixel in response to the light variation shown in (a).
  Figure 10 Link: articels_figures_by_rev_year\2015\HFirst_A_Temporal_Approach_to_Object_Recognition\figure_10.jpg
  Figure 10 caption: The effect of timing noise on recognition accuracy for the character
    recognition task. Adding Gaussian noise to the stabilized training data (a) has
    little effect on accuracy because even when delayed, spikes occur in the correct
    location relative to the character center. Accuracy drops off significantly only
    when the timing jitter is large enough to cause the training data spikes to be
    too spread in time. Adding even a small degree of Gaussian noise to the moving
    characters used for testing (b) causes accuracy to drop off significantly because
    by the time the delayed (jittered) spikes arrive at the S1 inputs, the character
    has already moved on to a new location.
  Figure 2 Link: articels_figures_by_rev_year\2015\HFirst_A_Temporal_Approach_to_Object_Recognition\figure_2.jpg
  Figure 2 caption: Operation of an Integrate-and-Fire neurons used, showing how synaptic
    weights and time affect the neuron membrane potential, as well as the operation
    of lateral reset connections (lateral meaning connecting to other neurons in the
    same layer) and refractory period.
  Figure 3 Link: articels_figures_by_rev_year\2015\HFirst_A_Temporal_Approach_to_Object_Recognition\figure_3.jpg
  Figure 3 caption: Competition between neurons tuned to different orientations when
    presented with a visual edge oriented at 90 degrees. The neuron tuned to 90 degrees
    is strongest stimulated causing it to cross spiking threshold first and reset
    all other orientations.
  Figure 4 Link: articels_figures_by_rev_year\2015\HFirst_A_Temporal_Approach_to_Object_Recognition\figure_4.jpg
  Figure 4 caption: "The HFirst model architecture, consisting of four layers (S1,\
    \ C1, S2, C2). Only a 32 \xD7 32 pixel cropped region of real data extracted from\
    \ the model is shown to ease visibility while demonstrating recognition of the\
    \ character 'R'. Black dots represent data from the model. The character 'R' has\
    \ been superimposed on top of the S1 and C1 data to aid explanation. The size\
    \ of the (cropped) data is shown at the left of each layer (Table 1 shows the\
    \ sizes for the full model). The S1 layer performs orientation extraction at a\
    \ fine scale, followed by a pooling operation in C1. Note that due to lateral\
    \ reset in C1, some S1 responses are blocked (for example, the last three orientations\
    \ on the bottom row). The S2 layer combines responses from different orientations,\
    \ but maintains spatial information. The C2 layer pools across all S2 spatial\
    \ locations, providing only a single output neuron for each character."
  Figure 5 Link: articels_figures_by_rev_year\2015\HFirst_A_Temporal_Approach_to_Object_Recognition\figure_5.jpg
  Figure 5 caption: "The receptive field of an S2 neuron trained to recognize the\
    \ character 'G'. The neuron receives inputs from an 8 \xD7 8 (x \xD7 y) C1 region\
    \ and from all 12 orientations (orientations are indicated by the oriented blacked\
    \ bars). Dark regions indicate strong excitatory weights and can be seen to fall\
    \ along edges of the character wherever edge orientation matches the C1 neuron\
    \ orientation. Weaker response between 135 and 165 degrees (bottom right) are\
    \ due to the direction of motion of the character during training (roughly 150\
    \ degrees). Motion perpendicular to the direction of motion is required to elicit\
    \ temporal contrast, as shown in (1) and discussed in Section 2. After normalization\
    \ the weights in this example range from \u2212 1 to 33 mV, indicated by the bar\
    \ on the right."
  Figure 6 Link: articels_figures_by_rev_year\2015\HFirst_A_Temporal_Approach_to_Object_Recognition\figure_6.jpg
  Figure 6 caption: The test setup used to acquire the character dataset, consisting
    of a motorised rotating barrel covered with printed letters viewed by a DVS [21].
  Figure 7 Link: articels_figures_by_rev_year\2015\HFirst_A_Temporal_Approach_to_Object_Recognition\figure_7.jpg
  Figure 7 caption: "Examples of the stabilised characters and cards pip views used\
    \ for training. Each example measures 32 \xD7 32 pixels and shows 1.7 ms of data."
  Figure 8 Link: articels_figures_by_rev_year\2015\HFirst_A_Temporal_Approach_to_Object_Recognition\figure_8.jpg
  Figure 8 caption: HFirst S2 layer spikes (indicated by markers) over a 150 ms time
    period in response to the character data. This figure shows the ability of HFirst
    to detect multiple characters in the scene simultaneously. Both location of the
    objects and their class are indicated by S2 spikes. The 'X', 'F', 'Y', and 'G'
    characters are correctly detected, but the character 'H' is misclassified, being
    mistaken for an 'I' or 'F' at different times.
  Figure 9 Link: articels_figures_by_rev_year\2015\HFirst_A_Temporal_Approach_to_Object_Recognition\figure_9.jpg
  Figure 9 caption: Detection of characters for a single rotation of the barrel. Only
    every second character is labelled on the vertical axis to reduce clutter. Red
    lines indicate when each character is present in the visual field, while blue
    crosses mark detections made by HFirst. Note that up to four characters are present
    in the scene at any one time.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Garrick Orchard
  Name of the last author: Ryad Benosman
  Number of Figures: 10
  Number of Tables: 3
  Number of authors: 6
  Paper title: 'HFirst: A Temporal Approach to Object Recognition'
  Publication Date: 2015-01-15 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Neuron Parameters
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Required Computation and Resources
  Table 3 caption:
    table_text: TABLE 3 Detection Accuracy and Required Computation
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2392947
- Affiliation of the first author: "isit-alcov, clermont universit\xE9, france"
  Affiliation of the last author: "isit-alcov, clermont universit\xE9, france"
  Figure 1 Link: articels_figures_by_rev_year\2015\ShapefromTemplate\figure_1.jpg
  Figure 1 caption: Geometric modeling of SfT. We propose to use a deformed embedding
    to model SfT. This function maps a 2D template parameterization obtained from
    the 3D template to the unknown 3D deformed surface. In our algorithms, any C 1
    2D parameterization can be used.
  Figure 10 Link: articels_figures_by_rev_year\2015\ShapefromTemplate\figure_10.jpg
  Figure 10 caption: "Experimental results using real isometric deformations\u2014\
    the cushion dataset, construction of the template. The left part shows some of\
    \ the 20 images we use to reconstruct the 3D template shown on the right part\
    \ with SfM."
  Figure 2 Link: articels_figures_by_rev_year\2015\ShapefromTemplate\figure_2.jpg
  Figure 2 caption: "Finding connected components in ConPSfT. (a) shows a case where\
    \ the 2D parameterization is theoretically divided up in three connected components,\
    \ leading to eight solutions. (b) shows an example of result on this case, where\
    \ our practical criterion leads to the detection of four connected components,\
    \ and thus to 16 solutions. Our algorithm based on this criterion guarantees that\
    \ the three true connected components are, up to noise, included into the four\
    \ detected ones. (c) uses a 1D slice of the 2D parameterization to illustrate\
    \ the theoretical and practical criteria. We observe that the theoretical criterion\
    \ \u03C4 =0 , which would apply well to a noise-free estimate of \u03C4 (in dashed\
    \ red), cannot be used on a noise-contaminated estimate of \u03C4 (in full blue).\
    \ It would not indeed find any of the connected components. Numerical tests also\
    \ showed that one cannot find a constant threshold on the value of \u03C4 to cope\
    \ with noise. Our practical criterion \u2207 \u03C4 =0 and \u03BB ( \u2202 2 \u03C4\
    \ \u2202 p 2 )\u22650 is a relaxation of the theoretical criterion, which does\
    \ not involve setting a threshold value, and still applies well to noise-contaminated\
    \ data. Our experimental results show the number of detected connected components\
    \ is always small, being typically b=3 ."
  Figure 3 Link: articels_figures_by_rev_year\2015\ShapefromTemplate\figure_3.jpg
  Figure 3 caption: "Combining the connected components in ConPSfT. The left part\
    \ shows an example of 4 estimated connected components. The right part illustrates,\
    \ on component C 1 , that after integration, the restriction \u03C6 \xAF 1 of\
    \ the embedding function \u03C6 to C 1 , is recovered up to a two-fold ambiguity\
    \ and up to scale. The 16 up to scale solutions for the embedding \u03C6 are obtained\
    \ by forming all possible configurations of two-fold component-wise ambiguities,\
    \ and computing the best scale for each component so that the recovered embeddings\
    \ agree as best possible along the components' boundaries, here the blue curves\
    \ between C 1 C 2 , C 2 C 3 , and C 3 C 4 ."
  Figure 4 Link: articels_figures_by_rev_year\2015\ShapefromTemplate\figure_4.jpg
  Figure 4 caption: Examples of simulated surfaces. The four isometric deformation
    examples are all different, while the conformal ones illustrate the effect of
    the parameter w controlling the amount of conformity, with s chosen, from left
    to right, as 0, 0.25, 0.5 and 0.75.
  Figure 5 Link: articels_figures_by_rev_year\2015\ShapefromTemplate\figure_5.jpg
  Figure 5 caption: "Experimental results using simulated isometric deformations\u2014\
    noise and number of point correspondences. The 3D error in mm is shown as a function\
    \ of noise on the point correspondences and of the number of point correspondences\
    \ for simulated isometric deformations and isometric SfT methods."
  Figure 6 Link: articels_figures_by_rev_year\2015\ShapefromTemplate\figure_6.jpg
  Figure 6 caption: "Experimental results using simulated conformal deformations\u2014\
    noise and number of point correspondences. The 3D error in mm is shown as a function\
    \ of noise on the point correspondences and of the number of point correspondences\
    \ for simulated conformal deformations and isometric and conformal SfT methods."
  Figure 7 Link: articels_figures_by_rev_year\2015\ShapefromTemplate\figure_7.jpg
  Figure 7 caption: "Experimental results using simulated conformal deformations\u2014\
    amount of conformity. (left) 3D error in mm is shown as a function of the amount\
    \ of conformity for simulated conformal surfaces and isometric and conformal SfT\
    \ methods. (right) Non-conformity due to surface discretization is shown as the\
    \ average angle variation (in degrees) of the simulated surfaces as a function\
    \ of conformity."
  Figure 8 Link: articels_figures_by_rev_year\2015\ShapefromTemplate\figure_8.jpg
  Figure 8 caption: "Experimental results using simulated isometric and conformal\
    \ deformations\u2014regularization weight. 3D error in mm is shown as a function\
    \ of the regularization weight for simulated isometric and conformal deformations.\
    \ The vertical bars represent the standard deviation."
  Figure 9 Link: articels_figures_by_rev_year\2015\ShapefromTemplate\figure_9.jpg
  Figure 9 caption: "Experimental results using real isometric deformations\u2014\
    CVLab's paper dataset. (left) 3D error in mm with respect to groundtruth is shown\
    \ for all frames for isometric SfT methods. Note that the error is off the y axis\
    \ for PeIso. (right) The original images and the estimated warp."
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Adrien Bartoli
  Name of the last author: Daniel Pizarro
  Number of Figures: 18
  Number of Tables: 5
  Number of authors: 5
  Paper title: Shape-from-Template
  Publication Date: 2015-01-15 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Algorithm Implementing Our Analytical Solution to IsoPSfT
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Algorithm Implementing Our Analytical Solution to ConPSfT
  Table 3 caption:
    table_text: TABLE 3 Computation Time
  Table 4 caption:
    table_text: ''
  Table 5 caption:
    table_text: ''
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2392759
- Affiliation of the first author: laboratory for advanced brain signal processing,
    riken brain science institute, japan and the department of computer science and
    engineering, shanghai jiao tong university, china
  Affiliation of the last author: laboratory for advanced brain signal processing,
    riken brain science institute, japan and the systems research institute at the
    polish academy of science, warsaw, poland
  Figure 1 Link: articels_figures_by_rev_year\2015\Bayesian_CP_Factorization_of_Incomplete_Tensors_with_Automatic_Rank_Determinatio\figure_1.jpg
  Figure 1 caption: Probabilistic graphical model of Bayesian CP factorization of
    an N th-order tensor.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Bayesian_CP_Factorization_of_Incomplete_Tensors_with_Automatic_Rank_Determinatio\figure_2.jpg
  Figure 2 caption: "The top row shows Hinton diagram of factor matrices, where the\
    \ color and size of each square represent the sign and magnitude of the value,\
    \ respectively. The bottom row shows the posterior of \u03BB , the lower bound\
    \ of model evidence, and the posterior of \u03C4 from left to right."
  Figure 3 Link: articels_figures_by_rev_year\2015\Bayesian_CP_Factorization_of_Incomplete_Tensors_with_Automatic_Rank_Determinatio\figure_3.jpg
  Figure 3 caption: Determination of tensor rank under varying conditions. Each vertical
    bar shows the mean and standard deviation of estimations from 50 MC runs, while
    the accuracy of detections is shown on the top of the corresponding bar. The red
    and blue horizontal dash dotted lines indicate the true tensor rank.
  Figure 4 Link: articels_figures_by_rev_year\2015\Bayesian_CP_Factorization_of_Incomplete_Tensors_with_Automatic_Rank_Determinatio\figure_4.jpg
  Figure 4 caption: Predictive performance when SNR = 30 dB.
  Figure 5 Link: articels_figures_by_rev_year\2015\Bayesian_CP_Factorization_of_Incomplete_Tensors_with_Automatic_Rank_Determinatio\figure_5.jpg
  Figure 5 caption: Ground-truth of eight benchmark images.
  Figure 6 Link: articels_figures_by_rev_year\2015\Bayesian_CP_Factorization_of_Incomplete_Tensors_with_Automatic_Rank_Determinatio\figure_6.jpg
  Figure 6 caption: Visual effects of image inpainting. Seven examples shown from
    top to bottom are (1) facade image with 95 percent missing; (2) facade image with
    95 percent missing and an additive noise; (3) Lena image with 90 percent missing;
    (4) Lena image with 90 percent missing and an additive noise; (5) Lena image with
    superimposed text; (6) scribbled Lena image; (7) an image of ocean with an object.
  Figure 7 Link: articels_figures_by_rev_year\2015\Bayesian_CP_Factorization_of_Incomplete_Tensors_with_Automatic_Rank_Determinatio\figure_7.jpg
  Figure 7 caption: Tensor completion for Baboon image under missing rates of 70,
    80, 90 and 95 percent are shown from top to bottom. The left column shows observed
    images with randomly missing pixels, while the recovered images by nine different
    methods are shown from left to right.
  Figure 8 Link: articels_figures_by_rev_year\2015\Bayesian_CP_Factorization_of_Incomplete_Tensors_with_Automatic_Rank_Determinatio\figure_8.jpg
  Figure 8 caption: Facial images under multiple conditions including people, poses
    and illuminations. Some of the images chosen randomly are fully missing.
  Figure 9 Link: articels_figures_by_rev_year\2015\Bayesian_CP_Factorization_of_Incomplete_Tensors_with_Automatic_Rank_Determinatio\figure_9.jpg
  Figure 9 caption: The ground-truth of 49 missing facial images and the synthetic
    images obtained by different methods.
  First author gender probability: 0.67
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Qibin Zhao
  Name of the last author: Andrzej Cichocki
  Number of Figures: 9
  Number of Tables: 3
  Number of authors: 3
  Paper title: Bayesian CP Factorization of Incomplete Tensors with Automatic Rank
    Determination
  Publication Date: 2015-01-15 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance (RSEs) Evaluated on Missing Pixels
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Averaged Recovery Performance (RSE, PSNR, SSIM) and Runtime
      (Seconds) on Eight Images with Missing Rates of 70, 80, 90 and 95 Percent
  Table 3 caption:
    table_text: TABLE 3 RSEs on Observed Images (O) and Missing Images (M)
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2392756
