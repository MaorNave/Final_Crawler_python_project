- Affiliation of the first author: tsinghua-berkeley shenzhen institute, tsinghua
    university, beijing, china
  Affiliation of the last author: department of computer science and technology, tsinghua
    university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Semantic_Conditioned_Dynamic_Modulation_for_Temporal_Sentence_Grounding_in_Video\figure_1.jpg
  Figure 1 caption: The temporal sentence grounding in videos (TSG) task. Our proposed
    SCDM relies on the sentence to modulate the temporal convolution operations, which
    can thereby establish the sentence-video semantic interaction in a light-weight
    manner, and temporally correlate and compose the various sentence-relevant activities
    (highlighted in red and green) to get the grounding results. Additionally, the
    SCDM-coupled temporal convolutional network can also predict the actionness scores
    over the video sequence, and help adjust the boundaries of the segment for more
    accurate grounding results.
  Figure 10 Link: articels_figures_by_rev_year\2020\Semantic_Conditioned_Dynamic_Modulation_for_Temporal_Sentence_Grounding_in_Video\figure_10.jpg
  Figure 10 caption: Qualitative prediction examples of our proposed model with the
    clip-level actionness prediction. The rows with green background show the groundtruths
    for the given sentence queries, and the rows with brown and pink background show
    the predicted segments before and after the temporal boundary adjustment and refinement,
    respectively. The bottom three heatmaps show the predicted starting, middle and
    ending probability sequences, respectively. The deeper the color, the higher the
    probability score.
  Figure 2 Link: articels_figures_by_rev_year\2020\Semantic_Conditioned_Dynamic_Modulation_for_Temporal_Sentence_Grounding_in_Video\figure_2.jpg
  Figure 2 caption: An overview of our proposed model for the TSG task. Specifically,
    as shown in subfigure (a), the multimodal fusion fuses the entire sentence and
    each video clip in a fine-grained manner. Based on the fused representation, the
    semantic modulated temporal convolution correlates sentence-relevant video contents
    in the temporal convolution procedure, with the proposed SCDM dynamically modulating
    temporal feature maps with respect to the sentence. Afterwards, multiple segment-level
    proposals are predicted based on the modulated feature maps of different temporal
    layers. There is another branch of temporal convolution which is imposed on the
    bottom layer of the convolutional architecture. The clip-level actionness scores
    are predicted by this branch in a fine-grained level, which are further leveraged
    to adjust the temporal boundaries of the segment-level proposals, and get the
    final temporal grounding results. The detailed demonstration of the proposed SCDM
    mechanism is provided in subfigure (b). Best viewed in color.
  Figure 3 Link: articels_figures_by_rev_year\2020\Semantic_Conditioned_Dynamic_Modulation_for_Temporal_Sentence_Grounding_in_Video\figure_3.jpg
  Figure 3 caption: The comparison between conditional normalization and our proposed
    semantic conditioned dynamic modulation.
  Figure 4 Link: articels_figures_by_rev_year\2020\Semantic_Conditioned_Dynamic_Modulation_for_Temporal_Sentence_Grounding_in_Video\figure_4.jpg
  Figure 4 caption: The illustration of temporal scale ratios and offsets.
  Figure 5 Link: articels_figures_by_rev_year\2020\Semantic_Conditioned_Dynamic_Modulation_for_Temporal_Sentence_Grounding_in_Video\figure_5.jpg
  Figure 5 caption: Temporal sentence grounding results when SCDM is performed on
    different layers of the temporal convolutional architecture. Layer-x means that
    we only add the SCDM mechanism on the x th layer of the temporal feature map,
    and Layer-1 means adding SCDM on the lowestfirst layer.
  Figure 6 Link: articels_figures_by_rev_year\2020\Semantic_Conditioned_Dynamic_Modulation_for_Temporal_Sentence_Grounding_in_Video\figure_6.jpg
  Figure 6 caption: Comparison of (a) training (optimization) and (b) testing (generalization)
    performances on the Charades-STA dataset. (a) shows the training loss curves of
    4 different models PlainBN, FC, MUL and SCDM. (b) and (c) illustrate the corresponding
    testing accuracies R1,IoU0.5, R1,IoU0.7 of these models after each training epoch,
    respectively.
  Figure 7 Link: articels_figures_by_rev_year\2020\Semantic_Conditioned_Dynamic_Modulation_for_Temporal_Sentence_Grounding_in_Video\figure_7.jpg
  Figure 7 caption: t -SNE projections of temporal feature maps yielded by the models
    PlainBN and SCDM. Each temporal feature unit within these feature maps is represented
    by its corresponding video clip in the original video. Video clips marked with
    red color are within ground-truth video segments.
  Figure 8 Link: articels_figures_by_rev_year\2020\Semantic_Conditioned_Dynamic_Modulation_for_Temporal_Sentence_Grounding_in_Video\figure_8.jpg
  Figure 8 caption: Temporal grounding accuracies with different varepsilon,nu,delta
    values on the Charades-STA, TACoS, and ActivityNet Caption datasets.
  Figure 9 Link: articels_figures_by_rev_year\2020\Semantic_Conditioned_Dynamic_Modulation_for_Temporal_Sentence_Grounding_in_Video\figure_9.jpg
  Figure 9 caption: Qualitative prediction examples of our proposed model for temporal
    sentence grounding in videos. The rows with green background show the ground-truths
    for the given sentence queries, and the rows with pink background show the final
    location prediction results. The gray histograms show the word attention weights
    produced by SCDM at different temporal regions.
  First author gender probability: 0.61
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.61
  Name of the first author: Yitian Yuan
  Name of the last author: Wenwu Zhu
  Number of Figures: 10
  Number of Tables: 5
  Number of authors: 5
  Paper title: Semantic Conditioned Dynamic Modulation for Temporal Sentence Grounding
    in Videos
  Publication Date: 2020-11-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Notations Used in the Inference Section
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance Comparisons on the TACoS and Charades-STA Datasets
      (%)
  Table 3 caption:
    table_text: TABLE 3 Performance Comparisons on the ActivityNet Captions Dataset
      (%)
  Table 4 caption:
    table_text: TABLE 4 Ablation Studies on the TACoS and Charades-STA Datasets (%)
  Table 5 caption:
    table_text: TABLE 5 Comparison of Model Running Efficiency, Model Size, and Memory
      Footprint
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3038993
- Affiliation of the first author: school of electrical and computer engineering,
    purdue university, west lafayette, in, usa
  Affiliation of the last author: school of electrical and computer engineering, purdue
    university, west lafayette, in, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\The_Perils_and_Pitfalls_of_Block_Design_for_EEG_Classification_Experiments\figure_1.jpg
  Figure 1 caption: Our 1D CNN used to process EEG data.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\The_Perils_and_Pitfalls_of_Block_Design_for_EEG_Classification_Experiments\figure_2.jpg
  Figure 2 caption: (left) Fig. 6 from Tirupattur et al. [7] illustrating sample images
    purportedly generated by a GAN model from EEG encodings (except for the right
    column in red that illustrates a random image of the given class from the training
    data). (right) Corresponding identical ImageNet images for almost all of the generated
    images. Note that some, but not all, of the purportedly synthesized images on
    the left are horizontal mirrors of ImageNet images on the right. Also note that
    all of the purportedly synthesized images contain the same precise fine-grained
    detail as the corresponding ImageNet images. In particular, each image not only
    depicts the corresponding class but also depicts the exact non-class-specific
    background as the ImageNet counterpart.
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.95
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ren Li
  Name of the last author: Jeffrey Mark Siskind
  Number of Figures: 2
  Number of Tables: 10
  Number of authors: 7
  Paper title: The Perils and Pitfalls of Block Design for EEG Classification Experiments
  Publication Date: 2020-11-19 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Classification Accuracy Averaged Across Validation Sets,
      Test Sets, and All Six Splits Used by OP1 on Their Released Data With Their
      Software (an LSTM Combined With A Fully Connected Layer and a ReLU Layer) and
      Four New Classifiers: A Nearest Neighbor Classifier ( k k-NN), an SVM, an MLP,
      and a 1D CNN'
  Table 10 caption:
    table_text: TABLE 10 Reanalysis of the Rapid-Event Run for Subject 6 on (left)
      Image and (right) Video Stimuli With Incorrect Block-Level Labels, Where the
      Data has not been Preprocessed With Bandpass Filtering
  Table 2 caption:
    table_text: TABLE 2 Classification Accuracy for Varying Trial Window Lengths With
      Random Temporal Offset From the Stimulus Onset, Averaged Across Validation Sets,
      Test Sets, and All Six Splits Used by OP1 on Their Data With All Five Classifiers
  Table 3 caption:
    table_text: TABLE 3 Classification Accuracy for Varying Numbers of Channels, Averaged
      Across Validation Sets, Test Sets, and All Six Splits Used by OP1 on Their Data
      With All Five Classifiers and Varying Trial Window Lengths With Random Temporal
      Offset From the Stimulus Onset
  Table 4 caption:
    table_text: TABLE 4 Application of the Analysis From Table 3 to the First Block
      Run of Subject 6 on (left) Image and (right) Video Stimuli, Where the Data has
      been Preprocessed With Bandpass Filtering
  Table 5 caption:
    table_text: TABLE 5 Application of the Analysis From Table 3 to the Rapid-Event
      Run of Subject 6 on (left) Image and (right) Video Stimuli, Where the Data has
      been Preprocessed With Bandpass Filtering
  Table 6 caption:
    table_text: TABLE 6 Application of the Analysis From Table 3 to the First Block
      Run of Subject 6 on (left) Image and (right) Video Stimuli, Where the Data has
      not been Preprocessed With Bandpass Filtering
  Table 7 caption:
    table_text: TABLE 7 Application of the Analysis From Table 3 to the Rapid-Event
      Run of Subject 6 on (left) Image and (right) Video Stimuli, Where the Data has
      not been Preprocessed With Bandpass Filtering
  Table 8 caption:
    table_text: TABLE 8 Reanalysis of the Data Released by OP1 With Classification
      Accuracy Averaged Over Leave-One-Subject-Out Round-Robin Cross Validation Instead
      of the Provided Splits
  Table 9 caption:
    table_text: TABLE 9 Reanalysis of the Rapid-Event Run for Subject 6 on (left)
      Image and (right) Video Stimuli With Incorrect Block-Level Labels, Where the
      Data has Been Preprocessed With Bandpass Filtering
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2973153
- Affiliation of the first author: bnrist, kliss, school of software, and thuibcs,
    tsinghua university, beijing, china
  Affiliation of the last author: sun yat-sen university, guangzhou, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Hypergraph_Learning_Methods_and_Practices\figure_1.jpg
  Figure 1 caption: Illustration of a graph and hypergraph.
  Figure 10 Link: articels_figures_by_rev_year\2020\Hypergraph_Learning_Methods_and_Practices\figure_10.jpg
  Figure 10 caption: 'Left: performances of t-DHL by varying alpha and mu on the three
    datasets; lambda is set to 100. Right: performances of t-DHL by varying lambda
    on the three datasets; alpha and mu are set as 0.1 and 5, respectively.'
  Figure 2 Link: articels_figures_by_rev_year\2020\Hypergraph_Learning_Methods_and_Practices\figure_2.jpg
  Figure 2 caption: Distance-based hyperedge generation methods include (a) nearest-neighbor
    based methods and (b) clustering-based methods. (c) Distance-based hypergraph
    generation methods construct hyperedges using spatial information [32]. (d) Attribute-based
    hypergraph generation methods construct a hypergraph by linking the vertices sharing
    the same attributes with a hyperedge. (e) Network-based hypergraph generation
    methods could construct a hypergraph using not only first-order information, but
    also higher-order (e.g., second-order and third-order) information.
  Figure 3 Link: articels_figures_by_rev_year\2020\Hypergraph_Learning_Methods_and_Practices\figure_3.jpg
  Figure 3 caption: (a) Hypergraph structure. (b) Traditional incidence matrix representation
    of a hypergraph structure. (c) Proposed tensor representation of a hypergraph
    structure.
  Figure 4 Link: articels_figures_by_rev_year\2020\Hypergraph_Learning_Methods_and_Practices\figure_4.jpg
  Figure 4 caption: (a) A 3-order tensor. (b) Tensor representation of a hypergraph
    with 3 vertices.
  Figure 5 Link: articels_figures_by_rev_year\2020\Hypergraph_Learning_Methods_and_Practices\figure_5.jpg
  Figure 5 caption: (a) 2-D projection of the optimization space of DHSL model with
    respect to mathbf H . The optimization model of DHSL is non-convex. (b) 2-D projection
    of the optimization space of t-DHL model with respect to mathcal T . The optimization
    model of t-DHL is bi-convex.
  Figure 6 Link: articels_figures_by_rev_year\2020\Hypergraph_Learning_Methods_and_Practices\figure_6.jpg
  Figure 6 caption: Running time comparison of hypergraph generation methods.
  Figure 7 Link: articels_figures_by_rev_year\2020\Hypergraph_Learning_Methods_and_Practices\figure_7.jpg
  Figure 7 caption: Running time comparison of hypergraph learning methods.
  Figure 8 Link: articels_figures_by_rev_year\2020\Hypergraph_Learning_Methods_and_Practices\figure_8.jpg
  Figure 8 caption: Running time comparison on multi-modality datasets.
  Figure 9 Link: articels_figures_by_rev_year\2020\Hypergraph_Learning_Methods_and_Practices\figure_9.jpg
  Figure 9 caption: (a) The growth curve of running time with the size of dataset
    on the Facebook Page-to-Page dataset. (b) The convergence curve of DHSL and t-DHL
    on the MSRGesture3D dataset when the step size is set to 0.01, respectively.
  First author gender probability: 0.95
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.94
  Name of the first author: Yue Gao
  Name of the last author: Changqing Zou
  Number of Figures: 11
  Number of Tables: 9
  Number of authors: 6
  Paper title: 'Hypergraph Learning: Methods and Practices'
  Publication Date: 2020-11-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Notations and Definitions
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Hyperedge Categories
  Table 3 caption:
    table_text: TABLE 3 The Time Complexity of Hypergraph Learning Methods
  Table 4 caption:
    table_text: TABLE 4 Datasets Used for the Classification Task
  Table 5 caption:
    table_text: TABLE 5 Prediction Accuracy Comparison on the Microblog Dataset
  Table 6 caption:
    table_text: TABLE 6 Classification Accuracy and F 1 F1 Scores on the Facebook
      Page-Page Dataset
  Table 7 caption:
    table_text: TABLE 7 Classification Accuracy Comparison on the MSRGesture3D Dataset
  Table 8 caption:
    table_text: TABLE 8 Classification Accuracy Comparison on Multi-Modality Datasets
  Table 9 caption:
    table_text: TABLE 9 Clustering Results on the MNIST and Extended Yale B Datasets
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3039374
- Affiliation of the first author: "universit\xE9 paris-saclay, inria, cea, palaiseau,\
    \ france"
  Affiliation of the last author: "universit\xE9 paris-saclay, inria, cea, palaiseau,\
    \ france"
  Figure 1 Link: articels_figures_by_rev_year\2020\DiCoDiLe_Distributed_Convolutional_Dictionary_Learning\figure_1.jpg
  Figure 1 caption: "Decomposition of a noiseless univariate signal X (blue) as the\
    \ convolution Z\u2217D between a temporal pattern D (orange) and a sparse activation\
    \ signal Z (green)."
  Figure 10 Link: articels_figures_by_rev_year\2020\DiCoDiLe_Distributed_Convolutional_Dictionary_Learning\figure_10.jpg
  Figure 10 caption: Effect of the image size (in Megapixels) on letter recovery for
    two noise levels. The quality of the recovered patterns is evaluated with best
    assignment rho . One observes that increasing image size improves recovery of
    the patterns which are here letters, as the number of characters present in the
    image increases.
  Figure 2 Link: articels_figures_by_rev_year\2020\DiCoDiLe_Distributed_Convolutional_Dictionary_Learning\figure_2.jpg
  Figure 2 caption: "Communication process in DiCoDiLe Z with d=2 and 9 workers centered\
    \ around worker w . The update in \u03C9 0 is independent of the other workers.\
    \ The update in \u03C9 1 is performed if no better coordinate update is possible\
    \ in the soft-lock area V \u0398 ( \u03C9 1 )\u2229 S w+1 (red hatched area).\
    \ If accepted, the worker w+1 needs to be notified. The update in \u03C9 2 changes\
    \ the value of the optimal updates in the \u0398 -extension of the other workers\
    \ sub-domains. Thus it needs to notify all the neighboring workers."
  Figure 3 Link: articels_figures_by_rev_year\2020\DiCoDiLe_Distributed_Convolutional_Dictionary_Learning\figure_3.jpg
  Figure 3 caption: "Average running time of CD with three coordinate selection schemes\u2013\
    (green) Cyclic, (orange) Locally Greedy and (blue) Greedy\u2013for three signal\
    \ lengths and \u03BB=0.5 \u03BB max . LGCD consistently outperforms the two other\
    \ strategies."
  Figure 4 Link: articels_figures_by_rev_year\2020\DiCoDiLe_Distributed_Convolutional_Dictionary_Learning\figure_4.jpg
  Figure 4 caption: Reconstruction of the image Mandrill using DiCoDiLe Z (left) without
    and (right) with soft-locks for a grid of 7times 7 workers. The dashed lines show
    the partitions of the domain Omega . The algorithm diverges at the edges of some
    of the sub-domains due to interfering updates between more than two workers.
  Figure 5 Link: articels_figures_by_rev_year\2020\DiCoDiLe_Distributed_Convolutional_Dictionary_Learning\figure_5.jpg
  Figure 5 caption: 'Scaling on 2D images of DiCoDiLe Z with the number of workers
    for two partitioning strategies of Omega : (blue) Omega is split only along one
    direction, as in DICOD [21], (orange) Omega is split along both directions, on
    a grid. The running times are similar for low number of workers but the performances
    with the linear splits stop improving once W reaches the scaling limit T14L1 (green).
    With the grid of workers adapted to images, the performance improves further.
    The linear split stops when W reached T1L1 because no more coordinate in mathcal
    Sw are independent from other neighbors.'
  Figure 6 Link: articels_figures_by_rev_year\2020\DiCoDiLe_Distributed_Convolutional_Dictionary_Learning\figure_6.jpg
  Figure 6 caption: Scaling of DICOD (orange) and DiCoDiLe Z (blue) with the number
    of workers W . DiCoDiLe Z scales sub-linearly while DICOD scales super-linearly.
    However, DiCoDiLe Z is more efficient than DICOD in a regime with a low number
    of workers W , leading to improved performances in all regimes. The green line
    denotes the number of cores where DiCoDiLe Z and DICOD become the same as DiCoDiLe
    Z only has one sub-domain in each worker. The results are consistent for both
    small (top, T=150L ) and large (bottom T=750L ) signals sizes.
  Figure 7 Link: articels_figures_by_rev_year\2020\DiCoDiLe_Distributed_Convolutional_Dictionary_Learning\figure_7.jpg
  Figure 7 caption: 'Scaling of DiCoDiLe Z with the number of workers W for different
    values of lambda and for two strategies of coordinate selection: Greedy and Locally
    Greedy. The convergence is faster when lambda is large because the activation
    becomes sparser and less coordinates need to be updated. Also, the locally greedy
    coordinate selection outperforms the greedy selection up to a certain point where
    the performances become similar as the sub-domain mathcal Sw becomes too small
    to be further partitioned with sub-domains of size Theta .'
  Figure 8 Link: articels_figures_by_rev_year\2020\DiCoDiLe_Distributed_Convolutional_Dictionary_Learning\figure_8.jpg
  Figure 8 caption: Comparison of DiCoDiLe algorithm with Consensus ADMM dictionary
    learning proposed by Skau and Wohlberg [28]. Algorithms were run five times with
    36 workers on a random-patch of size 512times 512 taken from the Hubble Space
    Telescope GOODS South image. The solid lines denote the median curves obtained
    through interpolation.
  Figure 9 Link: articels_figures_by_rev_year\2020\DiCoDiLe_Distributed_Convolutional_Dictionary_Learning\figure_9.jpg
  Figure 9 caption: Letter recovery experiment using a noisy image of a text with
    5000 characters drawn uniformly from PAMI (top). For each row, the atoms that
    are displayed have been selected among K=10 with the best assignment rho with
    the original patterns (dashed red). Random patches drawn uniformly from the original
    image (dashed green) are used as initial estimates D0 . We use DiCoDiLe (orange)
    and Online DL (blue, [20]) to learn K=10 atoms. While the atoms recovered with
    Online DL are blurred, CDL is able to correcly recover the 4 original letters.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Thomas Moreau
  Name of the last author: Alexandre Gramfort
  Number of Figures: 12
  Number of Tables: 0
  Number of authors: 2
  Paper title: 'DiCoDiLe: Distributed Convolutional Dictionary Learning'
  Publication Date: 2020-11-19 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3039215
- Affiliation of the first author: "esiee paris, \xE9cole des ponts, ligm, univ gustave\
    \ eiffel, cnrs, marne-la-vall\xE9e, france"
  Affiliation of the last author: "perceiving systems department, max planck institute\
    \ for intelligent systems, t\xFCbingen, germany"
  Figure 1 Link: articels_figures_by_rev_year\2020\Occlusion_Boundary_A_Formal_Definition__Its_Detection_via_Deep_Exploration_of_Co\figure_1.jpg
  Figure 1 caption: Illustration of the ubiquity of occlusions and the variety of
    local occlusion patterns (source image from [17]).
  Figure 10 Link: articels_figures_by_rev_year\2020\Occlusion_Boundary_A_Formal_Definition__Its_Detection_via_Deep_Exploration_of_Co\figure_10.jpg
  Figure 10 caption: Precision-recall curves. The left and right subfigures show the
    precision-recall curves on the CMU and VSB100 benchmarks, respectively, which
    are drawn following [62], [69].
  Figure 2 Link: articels_figures_by_rev_year\2020\Occlusion_Boundary_A_Formal_Definition__Its_Detection_via_Deep_Exploration_of_Co\figure_2.jpg
  Figure 2 caption: "Illustration of the CNN architecture and the output of several\
    \ layers. For a 27\xD727 patch of the given input sequence, we first extract N\
    \ s static and N m motion feature maps, which serve as the input of the CNN (\
    \ N s =2 and N m =3 in the current implementation). The output of Maxp3 layer\
    \ corresponds to deep features that aggregate the high-level contextual information\
    \ (referred to as deep contextual features). fc3 layer outputs a probabilistic\
    \ labeling map on a 7\xD77 patch."
  Figure 3 Link: articels_figures_by_rev_year\2020\Occlusion_Boundary_A_Formal_Definition__Its_Detection_via_Deep_Exploration_of_Co\figure_3.jpg
  Figure 3 caption: Position sensitive box. The green box can provide more discriminative
    features than the blue box, if we use either of them to predict the category-level
    label of the red pixel.
  Figure 4 Link: articels_figures_by_rev_year\2020\Occlusion_Boundary_A_Formal_Definition__Its_Detection_via_Deep_Exploration_of_Co\figure_4.jpg
  Figure 4 caption: 'Illustration of the connection between PSSM (left) and L2S (right).
    Gray grid: image. Larger box: receptive field (or input patch). Smaller box: label
    patch. Let us take the red pixel as an example. In PSSM, the red pixel receives
    M 2 scores from M 2 neighboring larger boxes. In L2S, these M 2 neighboring larger
    boxes are mapped to M 2 neighboring label patches, all of which contain the red
    pixel. In other words, they provide M 2 scores to the red pixel. PSSM and L2S
    are equivalent in this sense.'
  Figure 5 Link: articels_figures_by_rev_year\2020\Occlusion_Boundary_A_Formal_Definition__Its_Detection_via_Deep_Exploration_of_Co\figure_5.jpg
  Figure 5 caption: Illustration of flow inconsistencies.
  Figure 6 Link: articels_figures_by_rev_year\2020\Occlusion_Boundary_A_Formal_Definition__Its_Detection_via_Deep_Exploration_of_Co\figure_6.jpg
  Figure 6 caption: L2S-FCN. The illustration of the L2S-FCN approach (see text).
  Figure 7 Link: articels_figures_by_rev_year\2020\Occlusion_Boundary_A_Formal_Definition__Its_Detection_via_Deep_Exploration_of_Co\figure_7.jpg
  Figure 7 caption: 'Fully-connected layer to convolutional layer. Top: a patch-level
    fully-connected layer (in L2S- C 2 ). Bottom: the corresponding image-level convolutional
    layer (in L2S-FCN).'
  Figure 8 Link: articels_figures_by_rev_year\2020\Occlusion_Boundary_A_Formal_Definition__Its_Detection_via_Deep_Exploration_of_Co\figure_8.jpg
  Figure 8 caption: 'Representative qualitative results on the CMU benchmark [10].
    Each row corresponds to one test sequence and consists of (from left to right):
    a reference frame, the occlusion boundary ground truth, the occlusion boundary
    maps obtained by gPb-owt-ucm [62], RCF [67], our L2S- textC2 and L2S-FCN approaches,
    successively.'
  Figure 9 Link: articels_figures_by_rev_year\2020\Occlusion_Boundary_A_Formal_Definition__Its_Detection_via_Deep_Exploration_of_Co\figure_9.jpg
  Figure 9 caption: 'Representative qualitative results on the VSB100 benchmark [13].
    The whole figure consists of the upper and lower sub-figures. Each column of each
    sub-figure corresponds to one test sequence and consists of (from top to bottom):
    a reference frame, the occlusion boundary ground truth, and the occlusion boundary
    maps obtained by our L2S- textC2 and L2S-FCN approaches.'
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Chaohui Wang
  Name of the last author: Michael J. Black
  Number of Figures: 10
  Number of Tables: 7
  Number of authors: 4
  Paper title: 'Occlusion Boundary: A Formal Definition & Its Detection via Deep Exploration
    of Context'
  Publication Date: 2020-11-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Comparison on the CMU Benchmark
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Comparison on the VSB100 Benchmark
  Table 3 caption:
    table_text: TABLE 3 Comparison Between Different Temporal Cues and Optical Flow
      Computation Algorithms
  Table 4 caption:
    table_text: TABLE 4 Comparison of Different Mapping Methods
  Table 5 caption:
    table_text: TABLE 5 Comparison of L2S-FCN With Different Label Patch Sizes
  Table 6 caption:
    table_text: TABLE 6 L2S-FCN of Different Depths
  Table 7 caption:
    table_text: TABLE 7 Quantitate Evaluation on the MPI Sintel Dataset
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3039478
- Affiliation of the first author: department of industrial and systems engineering,
    texas a&m university, college station, tx, usa
  Affiliation of the last author: department of industrial and systems engineering,
    texas a&m university, college station, tx, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Consistent_Estimation_of_the_MaxFlow_Problem_Towards_Unsupervised_Image_Segmenta\figure_1.jpg
  Figure 1 caption: "The variability in morphology and location of (a) tumorous cells\
    \ in MR scans and (b) defects (pores and balling effect\u2013see Section 6.3 for\
    \ additional details) in an AM component, adapted from [13]."
  Figure 10 Link: articels_figures_by_rev_year\2020\Consistent_Estimation_of_the_MaxFlow_Problem_Towards_Unsupervised_Image_Segmenta\figure_10.jpg
  Figure 10 caption: Comparative results of different algorithms tested for the segmentation
    of defects. (a) Original image for sample B (b) k-means with 2 clusters (c) GMM
    (d) SCGMM with k-means initialization (e) mean shift and (f) UMF.
  Figure 2 Link: articels_figures_by_rev_year\2020\Consistent_Estimation_of_the_MaxFlow_Problem_Towards_Unsupervised_Image_Segmenta\figure_2.jpg
  Figure 2 caption: "(a) Graph representation of an image \u03C9 in a continuous domain.\
    \ (b) A representative example of segmentation of the image \u03C9 into subdomains\
    \ \u03C9 1 ,\u2026, \u03C9 4 via graph cut."
  Figure 3 Link: articels_figures_by_rev_year\2020\Consistent_Estimation_of_the_MaxFlow_Problem_Towards_Unsupervised_Image_Segmenta\figure_3.jpg
  Figure 3 caption: Representation of the continuous image domain where each of the
    sites are associated with source, sink and spatial flows represented by p s (x),
    p t (x),p(x) , respectively.
  Figure 4 Link: articels_figures_by_rev_year\2020\Consistent_Estimation_of_the_MaxFlow_Problem_Towards_Unsupervised_Image_Segmenta\figure_4.jpg
  Figure 4 caption: "Visualization of the flow fields: (a) when the flow through x\
    \ is limited by the sink flow capacity such that the sink flow contributes to\
    \ the total energy (b) when the flow through x is limited by the source flow capacity\
    \ such that the source flow contributes to the total energy and (c) shows two\
    \ cases of divergence of the spatial flow. The spatial flow where divp(x)\u2260\
    0 contributes to the total energy. We omit the edges connecting non-adjacent sites\
    \ for the sake of clarity."
  Figure 5 Link: articels_figures_by_rev_year\2020\Consistent_Estimation_of_the_MaxFlow_Problem_Towards_Unsupervised_Image_Segmenta\figure_5.jpg
  Figure 5 caption: Representative example to illustrate the effect of variations
    in the source flow capacity on the segmentation result. (a) Input image showing
    the defects. Segmentation using the continuous max-flow approach with t(x) and
    spatial flow capacities fixed to 0.3 and 0.1 respectively, but with varying values
    of s(x) . (b) s(x) = 0.2 , (c) s(x) = 0.28 and (d) s(x) = 0.35 .
  Figure 6 Link: articels_figures_by_rev_year\2020\Consistent_Estimation_of_the_MaxFlow_Problem_Towards_Unsupervised_Image_Segmenta\figure_6.jpg
  Figure 6 caption: 'Segmentation results of different unsupervised approaches for
    the segmentation of brain tumor on HG (top row) and LG (bottom row) glioma: (a)
    UMF (b) mean shift (c) normalized cuts (d) blobworld (e) hierarchical image segmentation
    (f) GMM and (g) SCGMM.'
  Figure 7 Link: articels_figures_by_rev_year\2020\Consistent_Estimation_of_the_MaxFlow_Problem_Towards_Unsupervised_Image_Segmenta\figure_7.jpg
  Figure 7 caption: Comparison of the computational cost among the unsupervised algorithms
    that converged.
  Figure 8 Link: articels_figures_by_rev_year\2020\Consistent_Estimation_of_the_MaxFlow_Problem_Towards_Unsupervised_Image_Segmenta\figure_8.jpg
  Figure 8 caption: Comparative results of different algorithms tested for the segmentation
    of brain tumor in the BRATS 2013 dataset. Box plot adapted from [40].
  Figure 9 Link: articels_figures_by_rev_year\2020\Consistent_Estimation_of_the_MaxFlow_Problem_Towards_Unsupervised_Image_Segmenta\figure_9.jpg
  Figure 9 caption: Comparative results of different algorithms tested for the segmentation
    of defects. (a) Original image for sample A (b) k-means with 2 clusters (c) GMM
    (d) SCGMM with k-means initialization (e) mean shift and (f) UMF.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ashif Sikandar Iquebal
  Name of the last author: Satish Bukkapatnam
  Number of Figures: 11
  Number of Tables: 2
  Number of authors: 2
  Paper title: 'Consistent Estimation of the Max-Flow Problem: Towards Unsupervised
    Image Segmentation'
  Publication Date: 2020-11-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Dice Score Comparison for the BRATS 2015 Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average Dice Score and Hausdorff Distance of Various Unsupervised
      Approaches Implemented in Section 6.3.
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3039745
- Affiliation of the first author: department of computer science, university of siegen,
    siegen, germany
  Affiliation of the last author: department of computer science, university of siegen,
    siegen, germany
  Figure 1 Link: articels_figures_by_rev_year\2020\A_Generative_Model_for_Generic_Light_Field_Reconstruction\figure_1.jpg
  Figure 1 caption: "(a) A full 5\xD75 LF, with central view marked in red. (b) Central\
    \ view (CV) extracted from (a), with a small patch of this central view marked\
    \ in blue. (c) This patch passes through a convolutional feature extractor to\
    \ output central view features (CVF). (d) The encoder E 1 of the CVAE maps an\
    \ LF patch to a latent variable z , while generator G 1 of the CVAE maps z back\
    \ to the LF patch using CVF as an additional input."
  Figure 10 Link: articels_figures_by_rev_year\2020\A_Generative_Model_for_Generic_Light_Field_Reconstruction\figure_10.jpg
  Figure 10 caption: Effect of minor alterations to the coded mask on reconstruction.
    Shown is the top left view. (a) Our reconstruction with 3 swaps in the mask. (b)
    Reconstruction using [6] with 1 swap. (c) Reconstruction using [6] with 3 swaps.
  Figure 2 Link: articels_figures_by_rev_year\2020\A_Generative_Model_for_Generic_Light_Field_Reconstruction\figure_2.jpg
  Figure 2 caption: (a) Schematic of CVAE. (b) Central view feature (CVF) extraction.
    (c) Architecture of feature extractor, CVF= CVF 1 , CVF 2 . (d) Schematic of encoder
    E 1 of CVAE. (e) Schematic of generator G 1 of CVAE
  Figure 3 Link: articels_figures_by_rev_year\2020\A_Generative_Model_for_Generic_Light_Field_Reconstruction\figure_3.jpg
  Figure 3 caption: Sample reconstruction from CVAE. The first two rows are input
    LF patches and corresponding reconstructions from CVAE. The third row shows the
    CVAE mapping of an arbitrary central patch to an LF patch with disparity similar
    to input LF patch, using the latent code corresponding to the second row. Reported
    numbers are normalized RMSE (NMSE) values of the reconstructions with respect
    to the corresponding input patches.
  Figure 4 Link: articels_figures_by_rev_year\2020\A_Generative_Model_for_Generic_Light_Field_Reconstruction\figure_4.jpg
  Figure 4 caption: "Result of 7\xD77 view synthesis for the LF Cars. Shown is the\
    \ novel view at angular location (6,6), depicted as gray location in the inset.\
    \ The mask for selecting 5 input views is shown in the inset of ground truth view.\
    \ Figures in the first row a) \u2212 c) depict ground truth view, and the results\
    \ of our approach using 5 input views with and without overlapping patches in\
    \ that order. Figures d) \u2212 f) in the second row provide visual comparison\
    \ of novel views generated using approach of Wu et al. [28], and our approach\
    \ using 3\xD73 angular views. Error maps and zoomed in patches are depicted along\
    \ with corresponding novel views, with error magnified by a factor of 10. Results\
    \ best viewed when zoomed in."
  Figure 5 Link: articels_figures_by_rev_year\2020\A_Generative_Model_for_Generic_Light_Field_Reconstruction\figure_5.jpg
  Figure 5 caption: "Visual comparison of our synthesized views (depicted by gray\
    \ locations in the inset) for the task of 3\xD73\u21927\xD77 view synthesis for\
    \ the LF Reflective13. Columns 1\u22123 depict a) the ground truth views, the\
    \ result using b) Wu et al. [28] and c) our approach using overlapping patches\
    \ respectively. Columns 4\u22126 , the patches of columns 1\u22123 . Columns 7\u2212\
    8 depict the error maps corresponding to columns 2\u22123 with error magnified\
    \ by a factor of 10. The brightness of the zoomed in patches is increased for\
    \ better illustration. Average PSNR in dB of 40 novel views is shown."
  Figure 6 Link: articels_figures_by_rev_year\2020\A_Generative_Model_for_Generic_Light_Field_Reconstruction\figure_6.jpg
  Figure 6 caption: "Novel view at angular location (6,6) for the task 3\xD73\u2192\
    7\xD77 view synthesis. Columns 1\u22123 depict the result using Wu et al. [28]\
    \ and our approach using non-overlapping patches and overlapping patches respectively.\
    \ Shown are the zoomed in patches of the reconstructed views and error maps with\
    \ error magnified by a factor of 10. Among the 3\xD73 input views, central view\
    \ is clean. For the the remaining 8 views, we consider the following corruptions\
    \ (rows i \u2212 iv) i) additive Gaussian noise \u03C3=0.05 . ii) additive Gaussian\
    \ noise \u03C3=0.1 iii) salt and pepper noise with a probability of occurrence\
    \ of 0.05. iv) 50 percent pixels randomly dropped from views. Results best viewed\
    \ by zooming in."
  Figure 7 Link: articels_figures_by_rev_year\2020\A_Generative_Model_for_Generic_Light_Field_Reconstruction\figure_7.jpg
  Figure 7 caption: "Result of view synthesis of the LF Dino. Masks M 1 and M 2 are\
    \ provided as inset of the ground truth views. The columns 1\u22125 show the views\
    \ depicted by gray location in the inset corresponding to i) the ground truth,\
    \ and synthesized novel views using ii) the method of [60] iii) \u2212 iv) our\
    \ approach with overlapping patches and without overlapping patches and v) the\
    \ method of [2], respectively. Columns 6\u22129 illustrate the error maps corresponding\
    \ to the reconstructed views in columns 2\u22125 , with errors magnified by a\
    \ factor of 10. Shown are the PSNR values in dB of the reconstructed LFs. (Results\
    \ best viewed zoomed in)."
  Figure 8 Link: articels_figures_by_rev_year\2020\A_Generative_Model_for_Generic_Light_Field_Reconstruction\figure_8.jpg
  Figure 8 caption: "Result of spatial angular super-resolution of the LF Kitchen.\
    \ Masks M 1 and M 2 are provided as inset of the ground truth views. Central view\
    \ in full resolution is depicted in white. Measurements at the locations in red\
    \ are spatially down-sampled by a factor of 3. The columns 1\u22124 from left\
    \ to right show the views depicted by gray location in the inset corresponding\
    \ to the i) ground truth, and synthesized views using ii) \u2212 iii) our approach\
    \ with overlapping patches and without overlapping patches, and iv) approach of\
    \ [2]. Columns 5\u22127 illustrate the error maps corresponding to the reconstructed\
    \ views in columns 2\u22124 , with error magnified by a factor of 10.(Results\
    \ best viewed zoomed in)"
  Figure 9 Link: articels_figures_by_rev_year\2020\A_Generative_Model_for_Generic_Light_Field_Reconstruction\figure_9.jpg
  Figure 9 caption: "Coded aperture reconstruction using the coded mask M 1 of [6].\
    \ The column 1 depicts the the bottom right ground truth LF view. Columns 2\u2212\
    4 depict the reconstructed views using [6], our approach and [2] respectively.\
    \ The error maps corresponding to the views in columns 2\u22124 are illustrated\
    \ in the columns 5\u22127 , with errors magnified by a factor of 10. PSNR values\
    \ of recovered LFs are shown. (Results best viewed zoomed in)."
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Paramanand Chandramouli
  Name of the last author: Michael Moeller
  Number of Figures: 11
  Number of Tables: 5
  Number of authors: 5
  Paper title: A Generative Model for Generic Light Field Reconstruction
  Publication Date: 2020-11-23 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Average PSNR of Novel Views in dB for 7\xD77 7\xD77 View\
      \ Synthesis"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 3\xD73\u21927\xD77 3\xD73\u21927\xD77 View Synthesis Result\
      \ on the LF Cars, When Input Views Other Than Central View are Corrupted"
  Table 3 caption:
    table_text: "TABLE 3 5\xD75 5\xD75 View Synthesis: PSNR values in dB"
  Table 4 caption:
    table_text: 'TABLE 4 Spatial-Angular Super-Resolution: PSNR values in dB'
  Table 5 caption:
    table_text: 'TABLE 5 Coded Aperture Reconstruction: PSNR Values in dB'
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3039841
- Affiliation of the first author: school of software, shangdong university, jinan,
    china
  Affiliation of the last author: department of computer scienece, university of rochester,
    rochester, ny, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\ZeroShot_Video_Object_Segmentation_With_CoAttention_Siamese_Networks\figure_1.jpg
  Figure 1 caption: Illustration of the intuition behind this research. Given an input
    frame (b), our method leverages information from multiple reference frames (d)
    to better determine the foreground object (a), through a co-attention mechanism.
    (c) An inferior result without co-attention. The thickness of the red line represents
    the co-attention confidence.
  Figure 10 Link: articels_figures_by_rev_year\2020\ZeroShot_Video_Object_Segmentation_With_CoAttention_Siamese_Networks\figure_10.jpg
  Figure 10 caption: 'Qualitative results on three videos from the FBMS 59 dataset
    [12] (from top to bottom: car10, horse5 and giraffes01). It can be observed that
    COSNet is capable of handling the primary objects segmentation even contains multiple
    instances.'
  Figure 2 Link: articels_figures_by_rev_year\2020\ZeroShot_Video_Object_Segmentation_With_CoAttention_Siamese_Networks\figure_2.jpg
  Figure 2 caption: Overview of the pair-wise co-attention based COSNet in the training
    phase. A pair of frames F a , F b is fed into a feature embedding module to obtain
    the features V a , V b . Then, the pair-wise co-attention module computes the
    attention summaries that encode the correlations between V a and V b . Finally,
    Z and V are concatenated and handed over to a segmentation module to produce segmentation
    predictions.
  Figure 3 Link: articels_figures_by_rev_year\2020\ZeroShot_Video_Object_Segmentation_With_CoAttention_Siamese_Networks\figure_3.jpg
  Figure 3 caption: Illustration of our pair-wise co-attention operation. The feature
    embeddings V a and V b are flattened into matrices and used to compute the affinity
    matrix S via Eq. (3). Next, S is normalized column-wise and row-wise via Eq. (6)
    as co-attention weights (i.e, S c and S r ). Finally, co-attention summaries (i.e,
    Z a and Z b ) are computed by applying pair-wise co-attention weights to the corresponding
    features [Eq. (7)].
  Figure 4 Link: articels_figures_by_rev_year\2020\ZeroShot_Video_Object_Segmentation_With_CoAttention_Siamese_Networks\figure_4.jpg
  Figure 4 caption: Schematic illustration of training (a) and test (b) pipelines
    of pair-wise co-attention based COSNet. During training, COSNet takes pair-wise
    images F a and F b as input and computes the feature embeddings V a and V b ,
    as well as subsequent co-attention summaries Z a and Z b . Then, both the original
    feature embeddings and the co-attention summaries are concatenated and fed into
    the segmentation module for predicting masks Y a and Y b , respectively. Finally,
    we use Eq. (15) to compute the loss between the predictions and labels. During
    testing, COSNet takes one inference frame and N reference frames as input to consider
    more co-attention summaries Z b n N n=1 (Eq. (17)). Then, both the original feature
    embedding V a and averaged co-attention summary Z a are fed into the segmentation
    module to generate the final prediction. See Section 3.3 for details.
  Figure 5 Link: articels_figures_by_rev_year\2020\ZeroShot_Video_Object_Segmentation_With_CoAttention_Siamese_Networks\figure_5.jpg
  Figure 5 caption: Schematic illustration of training (a) and test (b) pipelines
    of group co-attention based COSNet. During training, a group of video frames lbrace
    boldsymbolFnrbrace n=1N+1 are sampled from a same video. For each frame boldsymbolFn
    , a corresponding group co-attention is computed over its feature boldsymbolVn
    and the embeddings [boldsymbolV1, !ldots !, !boldsymbolV!n-1, !boldsymbolV!n+1,
    !ldots !, !boldsymbolV!N+1] of the rest frames. Then, group co-attention summary
    boldsymbolZn is computed by Eq. (13). The group co-attention improved feature
    embedding boldsymbolXn of boldsymbolFn (Eq. (14)) is fed into the segmentation
    module and the loss (Eq. (15)) is computed between the segmentation output boldsymbolYn
    and ground-truth mask boldsymbolOn . During testing, with the co-attention module,
    COSNet is able to leverage and fuse correlation information from multiple reference
    frames to segment the query frame. See Section 3.3 for details.
  Figure 6 Link: articels_figures_by_rev_year\2020\ZeroShot_Video_Object_Segmentation_With_CoAttention_Siamese_Networks\figure_6.jpg
  Figure 6 caption: Visualization of gating weights of pair-wise co-attention on blackswan
    (left) and bmx-tree (right) video sequences. See Section 4.2 for details.
  Figure 7 Link: articels_figures_by_rev_year\2020\ZeroShot_Video_Object_Segmentation_With_CoAttention_Siamese_Networks\figure_7.jpg
  Figure 7 caption: 'Performance improvement with an increasing number of reference
    frames for pair-wise co-attention based COSNet (Section 4.2). (a) Testing frames
    with ground-truths overlaid. (b)-(e) Primary object predictions with considering
    different number of reference frames ( N!=!0,1,2, and 5). (f) Binary segmentations
    after applying CRF to (e). We can see that without co-attention, COSNet degrades
    to a frame-by-frame segmentation model ((b): N!=!0 ). Once co-attention is added
    [(c): N!=!1 ), similar foreground distraction can be suppressed efficiently. Furthermore,
    more inference frames contribute to better segmentation performance ((c)-(e)].'
  Figure 8 Link: articels_figures_by_rev_year\2020\ZeroShot_Video_Object_Segmentation_With_CoAttention_Siamese_Networks\figure_8.jpg
  Figure 8 caption: Attribute-based aggregate performance on the test set of DAVIS
    16 [11] with the region similarity (mean mathcal J ). See Section 4.2 for details.
  Figure 9 Link: articels_figures_by_rev_year\2020\ZeroShot_Video_Object_Segmentation_With_CoAttention_Siamese_Networks\figure_9.jpg
  Figure 9 caption: 'Qualitative results on three videos from the DAVIS 16 dataset
    [11] (from top to bottom: dancing-twirl, car-roundabout and parkour). It can be
    observed that COSNet is applicable to the primary target with shape deformation,
    similar target distraction and fast motion scenarios.'
  First author gender probability: 0.95
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.75
  Name of the first author: Xiankai Lu
  Name of the last author: Jiebo Luo
  Number of Figures: 12
  Number of Tables: 9
  Number of authors: 5
  Paper title: Zero-Shot Video Object Segmentation With Co-Attention Siamese Networks
  Publication Date: 2020-11-24 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Ablation Study of COSNet on DAVIS 16 16 [11], FBMS 59 59 [12]
      and Youtube-Objects [13] Datasets With Different Variant of Pair-Wise and Group
      Co-attention Mechanisms
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Ablation Study of Pair-Wise Co-attention Based COSNet (Section
      4.2) on DAVIS 16 16 [11], FBMS 59 59 [12] and Youtube-Objects [13] Datasets
      With Fusion Strategies, Sampling Strategies and CRF Post-Processing
  Table 3 caption:
    table_text: TABLE 3 Comparisons With Different Numbers of Reference Frames ( N
      N) During the Testing Stage on DAVIS 16 16 [11], FBMS 59 59 [12] and Youtube-Objects
      [13] Datasets
  Table 4 caption:
    table_text: TABLE 4 Quantitative Results on DAVIS 16 16 [11] Test Set, Using the
      Region Similarity J J, Boundary Accuracy F F, and Time Stability T T
  Table 5 caption:
    table_text: TABLE 5 Quantitative Results on the Test Set of FBMS 59 59 [12], Using
      Region Similarity J J and F Measure F F
  Table 6 caption:
    table_text: TABLE 6 Quantitative Performance of Each Category on Youtube-Objects
      [13] With the Region Similarity (mean J J)
  Table 7 caption:
    table_text: TABLE 7 Training Data Used by State-of-the-Art ZVOS Methods (see Section
      4.4.1).
  Table 8 caption:
    table_text: TABLE 8 Mean Value and Variance of Segmentation Results After Repeating
      10 Times on Three Evaluation Datasets
  Table 9 caption:
    table_text: TABLE 9 Runtime Comparison on DAVIS 16 16 Test Set
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3040258
- Affiliation of the first author: school of computer science and engineering, south
    china university of technology, guangdong, china
  Affiliation of the last author: school of computer science and engineering, south
    china university of technology, guangdong, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Integrating_Tensor_Similarity_to_Enhance_Clustering_Performance\figure_1.jpg
  Figure 1 caption: The framework of the IPS2 algorithm. Given a dataset as input,
    IPS2 first learns both pairwise relationships and information among pairs. Then
    it proposes to encode pair-to-pair relationships into a tensor similarity for
    exploring the spatial structure of data. Based on the tensor similarity, IPS2
    extracts a high order similarity, which can provide information ignored by the
    pairwise similarity. Afterwards, IPS2 combines the pairwise and the high order
    relationships to generate the fused similarity for achieving accurate and robust
    clustering.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Integrating_Tensor_Similarity_to_Enhance_Clustering_Performance\figure_2.jpg
  Figure 2 caption: Theorems 3.1 and 3.2 illustrate the connection between the pairwise
    similarity and the unfolded indecomposable tensor similarity.
  Figure 3 Link: articels_figures_by_rev_year\2020\Integrating_Tensor_Similarity_to_Enhance_Clustering_Performance\figure_3.jpg
  Figure 3 caption: The heatmaps of unfolded indecomposable tensor similarity over
    a synthetic dataset. The block structures in the heatmap on unfolded similarity
    are evident. This indicates that the indecomposable tensor similarity captures
    evident structural information than decomposable one does.
  Figure 4 Link: articels_figures_by_rev_year\2020\Integrating_Tensor_Similarity_to_Enhance_Clustering_Performance\figure_4.jpg
  Figure 4 caption: Synthetic example for validating the complementarity of the high
    order similarity. This first row shows the heatmaps of the pairwise similarity,
    the high order similarities and their summation. The second row shows the corresponding
    frequency distributions of the three similarities, respectively. This salient
    block structure and two peaks in the frequency distribution of the summation suggests
    that the high order similarity indeed captured information which is beneficial
    to the pairwise similarity.
  Figure 5 Link: articels_figures_by_rev_year\2020\Integrating_Tensor_Similarity_to_Enhance_Clustering_Performance\figure_5.jpg
  Figure 5 caption: The similarity heatmaps of SC, PPC and IPS2 over BBCSports and
    Yale datasets. To illustrate, SSC-BBCSports represents the heatmap of the pairwise
    similarity of SC over BBCSport, and the others are nominated in a similar way.
    These heatmaps demonstrate the learned high order similarity by IPS2 could relieve
    the influence of irrelevant features, and thereby enhance the robustness of the
    algorithm when facing high-dimensional datasets.
  Figure 6 Link: articels_figures_by_rev_year\2020\Integrating_Tensor_Similarity_to_Enhance_Clustering_Performance\figure_6.jpg
  Figure 6 caption: 'Example for validating the robustness of IPS2 over under-sampled
    datasets. (a): accuracy of SC, PPC, and IPS2 on the USdata1 with the varying feature
    dimension from 260 to 1860; (b): accuracy of SC, PPC, IPS2 over the USdata2 with
    the varying sample size from 75 to 275. The lines denote the accuracy gain of
    IPS2 over SC. The superior performance of IPS2 indicates that incorporating the
    high order similarity is beneficial to improve the accuracy and robustness of
    the pairwise similarity even on the datasets with serious imbalance.'
  Figure 7 Link: articels_figures_by_rev_year\2020\Integrating_Tensor_Similarity_to_Enhance_Clustering_Performance\figure_7.jpg
  Figure 7 caption: The performance of SC, PPC and IPS2 over synthetic dataset with
    different types of noise indicates the noise immunity of IPS2.
  Figure 8 Link: articels_figures_by_rev_year\2020\Integrating_Tensor_Similarity_to_Enhance_Clustering_Performance\figure_8.jpg
  Figure 8 caption: The performance of SC, PPC and IPS2 on synthetic datasets with
    different levels of Gaussian noise demonstrates that the neighborhood information
    captured by the high order similarity is robust to the noise contamination, even
    when it becomes highly destructive.
  Figure 9 Link: articels_figures_by_rev_year\2020\Integrating_Tensor_Similarity_to_Enhance_Clustering_Performance\figure_9.jpg
  Figure 9 caption: The time complexity and accuracy based on different k in K-nearest-neighbors-search
    suggests that choosing k in a suitable range could balance the complexity and
    accuracy by IPS2.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.66
  Name of the first author: Hong Peng
  Name of the last author: Hongmin Cai
  Number of Figures: 9
  Number of Tables: 3
  Number of authors: 6
  Paper title: Integrating Tensor Similarity to Enhance Clustering Performance
  Publication Date: 2020-11-24 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Statistics on Four Real-World Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Clustering Performance (Mean \xB1 \xB1 Standard Deviation)\
      \ of All Six Methods"
  Table 3 caption:
    table_text: TABLE 3 Computational Cost by Six Methods (Seconds)
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3040306
- Affiliation of the first author: department of computer science, north carolina
    state university, raleigh, nc, usa
  Affiliation of the last author: department of computer science, behavioral reinforcement
    learning lab, lirio, north carolina state university, raleigh, nc, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\A_Survey_of_SingleScene_Video_Anomaly_Detection\figure_1.jpg
  Figure 1 caption: Overview of single-scene video anomaly detection. Typical algorithms
    include a model building phase in which a model of normal activity is learned
    from one or more videos of a scene followed by a detection phase in which anomalies
    are detected in video from the same scene.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\A_Survey_of_SingleScene_Video_Anomaly_Detection\figure_2.jpg
  Figure 2 caption: One normal frame and one frame with an anomaly from each of the
    recommended datasets for single-scene video anomaly detection.
  Figure 3 Link: articels_figures_by_rev_year\2020\A_Survey_of_SingleScene_Video_Anomaly_Detection\figure_3.jpg
  Figure 3 caption: An overview of the 3 basic approaches past work has taken to video
    anomaly detection.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Bharathkumar Ramachandra
  Name of the last author: Ranga Raju Vatsavai
  Number of Figures: 3
  Number of Tables: 6
  Number of authors: 3
  Paper title: A Survey of Single-Scene Video Anomaly Detection
  Publication Date: 2020-11-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Characteristics of Video Anomaly Detection Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Thematic Grouping by Representation and Modeling Strategies
      Taken
  Table 3 caption:
    table_text: TABLE 3 Traditional Frame-Level and Pixel-Level Evaluation Criteria
      on the UCSD Ped1, UCSD Ped2 and CUHK Avenue Benchmark Datasets From Related
      Literature, Ordered Chronologically, Complied From This Same List
  Table 4 caption:
    table_text: TABLE 4 Track and Region-Based Area Under the ROC Curve for False
      Positive Rates up to 1.0 on UCSD Ped1, UCSD Ped2 and CUHK Avenue
  Table 5 caption:
    table_text: TABLE 5 Track-Based, Region-Based, Pixel-Level, and Frame-Level Area
      Under the ROC Curve on Street Scene
  Table 6 caption:
    table_text: TABLE 6 Running Times of Methods From Literature, Compiled From This
      Same List
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3040591
