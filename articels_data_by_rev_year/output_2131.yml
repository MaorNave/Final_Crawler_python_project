- Affiliation of the first author: department of electrical and computer engineering,
    rice university, houston, tx, usa
  Affiliation of the last author: department of electrical and computer engineering,
    rice university, houston, tx, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\High_Resolution_Deep_Imaging_Using_Confocal_TimeofFlight_Diffuse_Optical_Tomogra\figure_1.jpg
  Figure 1 caption: "Imaging depth and spatial resolution of DOT techniques. (a) Approximate\
    \ imaging depth of optical imaging techniques. Ballistic imaging techniques such\
    \ as OCT, confocal microscopy, and 2P microscopy cannot image past \u223C 15 mean\
    \ free paths (MFPs). DOT approaches can achieve 10s-100s of MFPs (b) Approximate\
    \ spatial resolution of different DOT techniques. Our technique is the only method\
    \ to demonstrate 1 mm spatial resolution."
  Figure 10 Link: articels_figures_by_rev_year\2021\High_Resolution_Deep_Imaging_Using_Confocal_TimeofFlight_Diffuse_Optical_Tomogra\figure_10.jpg
  Figure 10 caption: Simulated and Experimental Multiplexing Results. Multiplexing
    allows comparable performance with reduced integration time compared with single
    point scanning for CToF-DOT imaging. (Left) Plots shows PSNR versus integration
    time for simulated and experimental results. The images correspond to the image
    reconstructions performed at different integration times withwithout multiplexing.
    With multiplexing, the image reconstruction is more robust to noise at lower integration
    times. Measurements were captured through a 5 mm phantom with mu s=9 mm -1 ( sim
    45 MFPs).
  Figure 2 Link: articels_figures_by_rev_year\2021\High_Resolution_Deep_Imaging_Using_Confocal_TimeofFlight_Diffuse_Optical_Tomogra\figure_2.jpg
  Figure 2 caption: "ToF-DOT concept. (a) Photon trajectories for 2 source-detector\
    \ pairs. A and B are sources, \u03B1 and \u03B2 are detectors, and P, Q, R, and\
    \ S are voxels of interest. Source-detector pair A- \u03B1 is more sensitive to\
    \ P and Q and source-detector pair B- \u03B2 is more sensitive to R and S. (b)\
    \ Photon arrival times passing through specific voxels associated with source-detector\
    \ pair A- \u03B1 and B- \u03B2 . Photons passing through voxels closer to the\
    \ surface (P and R) tend to arrive earlier than photons passing through voxels\
    \ deeper inside (Q and S)."
  Figure 3 Link: articels_figures_by_rev_year\2021\High_Resolution_Deep_Imaging_Using_Confocal_TimeofFlight_Diffuse_Optical_Tomogra\figure_3.jpg
  Figure 3 caption: "Overview of DOT forward model. In the linear forward model, the\
    \ target scene ( \u03BC ) is mapped to a set of measurements ( m ) by the Jacobian\
    \ matrix ( J )."
  Figure 4 Link: articels_figures_by_rev_year\2021\High_Resolution_Deep_Imaging_Using_Confocal_TimeofFlight_Diffuse_Optical_Tomogra\figure_4.jpg
  Figure 4 caption: 'Validity of convolutional approximation. Visualization of (a)
    rows of Jacobian for different source-detector pair locations, and their corresponding
    (b) 1D profiles along X and Y directions (colored lines). Note: 1D profiles have
    been aligned for visualization.'
  Figure 5 Link: articels_figures_by_rev_year\2021\High_Resolution_Deep_Imaging_Using_Confocal_TimeofFlight_Diffuse_Optical_Tomogra\figure_5.jpg
  Figure 5 caption: Experimental setup to test CToF-DOT. (a) Rendering of our experimental
    setup showing a scanning laser beam and single pixel detector. (b) An image of
    the physical setup, with the SPAD (white), galvo mirrors (red), E-ink display
    (orange) and tissue phantom (blue).
  Figure 6 Link: articels_figures_by_rev_year\2021\High_Resolution_Deep_Imaging_Using_Confocal_TimeofFlight_Diffuse_Optical_Tomogra\figure_6.jpg
  Figure 6 caption: Jacobian matrix conditioning. The singular values of the Jacobian
    matrix are plotted to determine the matrix conditioning. We compare traditional
    DOT (blue), ToF-DOT (red), and our CToF-DOT (yellow). We see that the introduction
    of time binning (ToF-DOT) and confocal geometry (CToF-DOT) provides improvements
    to our matrix conditioning.
  Figure 7 Link: articels_figures_by_rev_year\2021\High_Resolution_Deep_Imaging_Using_Confocal_TimeofFlight_Diffuse_Optical_Tomogra\figure_7.jpg
  Figure 7 caption: Algorithm runtime characterization. The algorithm runtime was
    characterized as a function of source-detector array size (a), and the voxel grid
    size (b). We see almost two orders of magnitude decrease in runtime using our
    methods as compared to traditional DOT [8] and ToF-DOT [38].
  Figure 8 Link: articels_figures_by_rev_year\2021\High_Resolution_Deep_Imaging_Using_Confocal_TimeofFlight_Diffuse_Optical_Tomogra\figure_8.jpg
  Figure 8 caption: Simulated spatial resolution of CTOF-DoT. Our technique is able
    to resolve two 0.5 mm thick lines separated by 0.5 mm (shown in ground truth image
    on top right).
  Figure 9 Link: articels_figures_by_rev_year\2021\High_Resolution_Deep_Imaging_Using_Confocal_TimeofFlight_Diffuse_Optical_Tomogra\figure_9.jpg
  Figure 9 caption: Resolution test with experimental data. (Left) Eink target covered
    by the skull phantom. Inset image shows the matching calibration between Monte
    Carlo and experimental data, thus verifying the scattering coefficient. (Right)
    1D image reconstruction showing our system can resolve two lines separated by
    0.5 mm spacing.
  First author gender probability: 0.67
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yongyi Zhao
  Name of the last author: Ashok Veeraraghavan
  Number of Figures: 14
  Number of Tables: 0
  Number of authors: 6
  Paper title: High Resolution, Deep Imaging Using Confocal Time-of-Flight Diffuse
    Optical Tomography
  Publication Date: 2021-04-23 00:00:00
  Table 1 caption: Not Available
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3075366
- Affiliation of the first author: department of electrical and electronic engineering,
    imperial college london, london, u.k.
  Affiliation of the last author: department of electrical and electronic engineering,
    imperial college london, london, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2021\Sparse_SVM_for_Sufficient_Data_Reduction\figure_1.jpg
  Figure 1 caption: "Decreasing of \u2225F( z k , T k )\u2225 ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Sparse_SVM_for_Sufficient_Data_Reduction\figure_2.jpg
  Figure 2 caption: "Effect of \u03B7 ."
  Figure 3 Link: articels_figures_by_rev_year\2021\Sparse_SVM_for_Sufficient_Data_Reduction\figure_3.jpg
  Figure 3 caption: Effect of C and c .
  Figure 4 Link: articels_figures_by_rev_year\2021\Sparse_SVM_for_Sufficient_Data_Reduction\figure_4.jpg
  Figure 4 caption: Effect of s .
  Figure 5 Link: articels_figures_by_rev_year\2021\Sparse_SVM_for_Sufficient_Data_Reduction\figure_5.jpg
  Figure 5 caption: "Robustness to the initial points \u03B1 0 ."
  Figure 6 Link: articels_figures_by_rev_year\2021\Sparse_SVM_for_Sufficient_Data_Reduction\figure_6.jpg
  Figure 6 caption: Algorithm 1 versus Algorithm 2.
  Figure 7 Link: articels_figures_by_rev_year\2021\Sparse_SVM_for_Sufficient_Data_Reduction\figure_7.jpg
  Figure 7 caption: Classifiers by five solvers for Example 4.1.
  Figure 8 Link: articels_figures_by_rev_year\2021\Sparse_SVM_for_Sufficient_Data_Reduction\figure_8.jpg
  Figure 8 caption: Average results of six solvers for Example 4.1.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Shenglong Zhou
  Name of the last author: Shenglong Zhou
  Number of Figures: 8
  Number of Tables: 8
  Number of authors: 1
  Paper title: Sparse SVM for Sufficient Data Reduction
  Publication Date: 2021-04-23 00:00:00
  Table 1 caption: TABLE 1 List of Notations
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Complexity of Different Algorithms
  Table 3 caption: TABLE 3 Descriptions of Real Datasets
  Table 4 caption: TABLE 4 Selection of Parameters
  Table 5 caption: TABLE 5 Benchmark Methods
  Table 6 caption: TABLE 6 Results of Six Solvers Solving Example 4.2 With Datasets
    With Small Sizes
  Table 7 caption: TABLE 7 Results of Four Solvers Solving Example 4.1 With Large-Scale
    Datasets
  Table 8 caption: TABLE 8 Results of Four Solvers Solving Example 4.2 With Datasets
    With Large Sizes
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3075339
- Affiliation of the first author: ece department, rice university, houston, tx, usa
  Affiliation of the last author: ece department, carnegie mellon university, pittsburgh,
    pa, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\SASSI__SuperPixelated_Adaptive_SpatioSpectral_Imaging\figure_1.jpg
  Figure 1 caption: High resolution video rate hyperspectral imaging. We propose a
    novel hyperspectral camera that is capable of capturing high spatial and spectral
    resolution images at video rate. We achieve this by a sparse, scene-adaptive spectral
    sampling, and then fusing it with an auxiliary RGB image. Full video can be accessed
    from the supplementary material, which can be found on the Computer Society Digital
    Library at http:doi.ieeecomputersociety.org10.1109TPAMI.2021.3075228.
  Figure 10 Link: articels_figures_by_rev_year\2021\SASSI__SuperPixelated_Adaptive_SpatioSpectral_Imaging\figure_10.jpg
  Figure 10 caption: Lab prototype. The image above shows a photograph of our lab
    setup with key components marked. We also showed an overlay of ray tracing for
    easy understanding.
  Figure 2 Link: articels_figures_by_rev_year\2021\SASSI__SuperPixelated_Adaptive_SpatioSpectral_Imaging\figure_2.jpg
  Figure 2 caption: Various sampling schemes. CASSI-type cameras sense with a dense
    spatial pattern requiring spectral demultiplexing. On the other hand, pushbroom
    cameras do not require demultiplexing but lead to severe loss in spatial resolution.
    SASSI provides a unique tradeoff where the sampling pattern is sparse enough to
    avoid any spectral multiplexing, while requiring only a single scene-adaptive
    spectral measurement.
  Figure 3 Link: articels_figures_by_rev_year\2021\SASSI__SuperPixelated_Adaptive_SpatioSpectral_Imaging\figure_3.jpg
  Figure 3 caption: Homogeneity of super-pixels. We hypothesize that super-pixels
    represent homogeneous regions of spectra. We estimated similarity between spectral
    profile at one location within a super-pixel and all its other members for commonly
    available datasets. We observed that spectral profiles inside a super-pixel are
    highly correlated, evident from the small Spectral Angular Mapping (SAM) value.
  Figure 4 Link: articels_figures_by_rev_year\2021\SASSI__SuperPixelated_Adaptive_SpatioSpectral_Imaging\figure_4.jpg
  Figure 4 caption: Schematic of the proposed setup. Our optical setup consists of
    an RGB camera that guides the sampling system, and a spatio-spectral imager that
    consists of a spatial light modulator, a prism and a grayscale sensor. The guide
    image is utilized to generate a spatial mask that generates non-overlapping spectral
    profiles on the grayscale sensor. The guide image is then fused with the sparse
    spectral measurements to obtain a high spatial and spectral resolution HSI.
  Figure 5 Link: articels_figures_by_rev_year\2021\SASSI__SuperPixelated_Adaptive_SpatioSpectral_Imaging\figure_5.jpg
  Figure 5 caption: Super-pixel sampling strategy. We follow a computationally simple,
    three step sampling strategy for estimating the mask. Given a super-pixel segmentation,
    we first create a mask with centroids of super-pixels as sampling locations. Then,
    we enforce minimum separation along horizontal direction by movingremoving sampling
    locations. Next, we re-estimate the super-pixels with the new sampling locations
    as centroids. Finally, we increase light throughput by creating sampling locations
    everywhere with minimum separation between neighbors. Such a sampling strategy
    requires less than 2 ms on a modern computer, ensuring real time mask generation.
  Figure 6 Link: articels_figures_by_rev_year\2021\SASSI__SuperPixelated_Adaptive_SpatioSpectral_Imaging\figure_6.jpg
  Figure 6 caption: Visualization of guided filtering layer. We rely on a simple modification
    to traditional guided filtering. We represent the measurements as an affine scaling
    of the grayscale guide image. Then we solve a weighted least squares problem to
    estimate the coefficients, which is then used to reconstruct image at each wavelength
    band. This is followed by a two-layer refinement stage to get the final output.
    The learnable weights are trained along the two-layer neural network with a composite
    loss consisting of SSIM and MSE loss functions.
  Figure 7 Link: articels_figures_by_rev_year\2021\SASSI__SuperPixelated_Adaptive_SpatioSpectral_Imaging\figure_7.jpg
  Figure 7 caption: System pipeline. Our method avoids temporal registration as it
    relies only on the information captured at time instance t . Specifically, the
    guide image at t-1 is used to create a mask, which is used to capture another
    guide image and the spatio-spectral image at time t . Instead of fusing guide
    image from t-1 , we fuse images only from time instance t which ensures that there
    are no motion artifacts.
  Figure 8 Link: articels_figures_by_rev_year\2021\SASSI__SuperPixelated_Adaptive_SpatioSpectral_Imaging\figure_8.jpg
  Figure 8 caption: Performance with number of superpixels and noise. (a) As noise
    increases, a small number of superpixels is advantageous, as the spectra within
    a superpixel get averaged. (b) Our rank-1 reconstruction performs well against
    prior work under average to high light intensity. (c) At lower light levels, our
    guided filtering approach outperforms the rank-1 approach.
  Figure 9 Link: articels_figures_by_rev_year\2021\SASSI__SuperPixelated_Adaptive_SpatioSpectral_Imaging\figure_9.jpg
  Figure 9 caption: Comparisons with CASSI techniques. We compared reconstruction
    approaches which do not use a guide image [22], [44], and ones that use guide
    image [28]. SASSI outperforms other techniques across the board.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Vishwanath Saragadam
  Name of the last author: Aswin C. Sankaranarayanan
  Number of Figures: 19
  Number of Tables: 1
  Number of authors: 5
  Paper title: "SASSI \u2014 Super-Pixelated Adaptive Spatio-Spectral Imaging"
  Publication Date: 2021-04-23 00:00:00
  Table 1 caption: TABLE 1 Timing Per Frame
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3075228
- Affiliation of the first author: department of intelligence science and technology,
    graduate school of informatics, kyoto university, kyoto, japan
  Affiliation of the last author: department of intelligence science and technology,
    graduate school of informatics, kyoto university, kyoto, japan
  Figure 1 Link: articels_figures_by_rev_year\2021\NonRigid_Shape_From_Water\figure_1.jpg
  Figure 1 caption: We introduce a novel underwater 3D imaging method that can recover
    the holistic shape of a dynamic, non-rigid object. The shape instances of an underwater
    object at different time instances are reconstructed, accumulated and refined
    as it deforms and moves in water to form a dense and fuller reconstruction of
    the target as time progresses (bottom row left to right).
  Figure 10 Link: articels_figures_by_rev_year\2021\NonRigid_Shape_From_Water\figure_10.jpg
  Figure 10 caption: Estimation errors of the reconstructed 3D shape using correspondences
    over different numbers of frames. The error bars show standard deviations. The
    reconstruction is refined quickly with a handful of observations across different
    frames.
  Figure 2 Link: articels_figures_by_rev_year\2021\NonRigid_Shape_From_Water\figure_2.jpg
  Figure 2 caption: Light absorption in water and the proposed imaging system. (a)
    We model near-infrared light attenuation as an exponential function of the depth
    based on the Beer-Lambert law and use the difference of absorption rates (b) at
    different wavelengths. (c) We observe the object in water illuminated by a point
    light source through a flat refractive medium (i.e., a glass housing) with a perspective
    camera. Note that the 2D figure (c) illustrates the light paths from the light
    to the object and the object to the camera on a single plane-of-refraction (POR)
    for simplicity. In general, the POR from the light to the object and that from
    the object to the camera are not coplanar. The absorption rates (b) are computed
    using the Beer-Lambert law at different distances using the attenuation coefficients
    by Hale and Querry [22]. The plot shows that for our target sub-meter range, we
    can choose two wavelengths in the 700-1000nm range for bispectral shape-from-water.
  Figure 3 Link: articels_figures_by_rev_year\2021\NonRigid_Shape_From_Water\figure_3.jpg
  Figure 3 caption: Conical frustum constraint A reference point in water p w c defines
    a conical frustum in the air on which the light source can exist. We can therefore
    find the light source position as the intersection of three or more frusta defined
    by distinct reference points. Notice that, similar to Fig. 2, the plane-of-refraction
    for each light-to-object and object-to-camera path are not coplanar with each
    other in general.
  Figure 4 Link: articels_figures_by_rev_year\2021\NonRigid_Shape_From_Water\figure_4.jpg
  Figure 4 caption: Real-world water can scatter light. The light captured by the
    camera becomes the sum of (a) direct transmission, (b) backward scattering, and
    (c) forward scattering.
  Figure 5 Link: articels_figures_by_rev_year\2021\NonRigid_Shape_From_Water\figure_5.jpg
  Figure 5 caption: Overall framework of non-rigid shape from water. Given (a) input
    RGB and near-infrared images, we compute (b) per-pixel depth and 3D scene flow
    between consecutive frames (Section 5.1). We then define (c) a deformation graph
    in a canonical frame and estimate (d) the non-rigid transformation from other
    frames to integrate the depth estimates into (e) a single 3D point cloud.
  Figure 6 Link: articels_figures_by_rev_year\2021\NonRigid_Shape_From_Water\figure_6.jpg
  Figure 6 caption: Our imaging system consists of a point source, three texture light
    sources each placed with a RGB bandpass filter, and a custom-built multi-wavelength
    camera. Note that only the point light source emits NIR light and the texture
    light sources are optional.
  Figure 7 Link: articels_figures_by_rev_year\2021\NonRigid_Shape_From_Water\figure_7.jpg
  Figure 7 caption: Reconstruction results of the skull in water of different levels
    of turbidity. From left to right, the number on the top row show the increasing
    amount of milk added to the water tank. The skull was approximately 10cm square
    in size. From top to bottom, for each turbidity level, each row shows the captured
    RGB image, one of the NIR images (905nm), estimated 3D depth, and error map in
    comparison with ground truth. Ed is the mean absolute error from the ground truth
    3D shape in millimeter. Near-infrared imaging is robust to scattering and our
    method, by also accounting for scattering in its model, realizes accurate 3D reconstruction
    of underwater objects even for fairly murky water (20ml).
  Figure 8 Link: articels_figures_by_rev_year\2021\NonRigid_Shape_From_Water\figure_8.jpg
  Figure 8 caption: Quantitative evaluation with rigid objects in water. The objects
    are approximately 10cm square in size. (a) Room-light appearance. (b) Per-frame
    Reconstruction. (c)(d) Ground truth and estimated 3D shape integrated in the canonical
    frame and (e) error maps computed after aligning 3D reconstruction results with
    the ground truth. Ed is the mean absolute error from the ground truth 3D shape
    in millimeters. The results show that our method can accurately recover the complete
    surface geometry of objects in water.
  Figure 9 Link: articels_figures_by_rev_year\2021\NonRigid_Shape_From_Water\figure_9.jpg
  Figure 9 caption: Quantitative evaluation with non-rigid, deformable objects in
    water. The objects were approximately 6cm in size. (a) Room-light appearance.
    (b) Per-frame 3D reconstruction. (c)(d) Ground truth and estimated 3D shape integrated
    in the canonical frame, and (e) error maps obtained after aligning 3D reconstruction
    results with the ground truth. Ed is the mean absolute error from the ground truth
    3D shape in millimeter. Our method accurately reconstructs the rest shape geometry
    of the deforming object.
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.8
  Name of the first author: Meng-Yu Jennifer Kuo
  Name of the last author: Ko Nishino
  Number of Figures: 12
  Number of Tables: 1
  Number of authors: 4
  Paper title: Non-Rigid Shape From Water
  Publication Date: 2021-04-26 00:00:00
  Table 1 caption: "TABLE 1 Estimated Attenuation Coefficients \u03B1 c (\u03BB) \u03B1\
    c(\u03BB) (mm \u22121 -1)"
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3075450
- Affiliation of the first author: "institute of mathematics and computer sciences,\
    \ university of sao paulo, s\xE3o carlos - sp, brazil"
  Affiliation of the last author: college of computing, georgia institute of technology,
    atlanta, ga, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\Pay_Attention_to_Evolution_Time_Series_Forecasting_With_Deep_GraphEvolution_Lear\figure_1.jpg
  Figure 1 caption: A multiple multivariate time-series forecasting problem, where
    each multivariate time-series (i.e., sample) shares the same domain, timestream,
    and variables. When stacking the time-series together, we assemble a tridimensional
    tensor with the axes describing samples, timestamps, and variables. The multiple
    samples have equal variables recorded during the same timestamps, meaning that
    samples are unique but all observed in the same way. By tackling the problem altogether,
    we leverage inner and outer variables besides intra- and inter-temporal relationships
    to improve forecasting.
  Figure 10 Link: articels_figures_by_rev_year\2021\Pay_Attention_to_Evolution_Time_Series_Forecasting_With_Deep_GraphEvolution_Lear\figure_10.jpg
  Figure 10 caption: "Evolution Weights extracted from ReGENN after training on the\
    \ PhysioNet Computing in Cardiology Challenge 2012 dataset, in which we use the\
    \ cosine similarity to compare the relationship between pairs of variables. We\
    \ use \u201DABP\u201D as a shortening for Arterial Blood Pressure, \u201DNI\u201D\
    \ as Non-Invasive, \u201DDias\u201D as Diastolic, and \u201DSys\u201D as Systolic."
  Figure 2 Link: articels_figures_by_rev_year\2021\Pay_Attention_to_Evolution_Time_Series_Forecasting_With_Deep_GraphEvolution_Lear\figure_2.jpg
  Figure 2 caption: Graph Soft Evolution representation-learning, in which the set
    of multiple multivariate time-series is mapped into adjacency matrices of co-occurring
    variables. The matrices are element-wise summed to generate a shared graph among
    samples, which, after a linear transformation, goes through a similarity activation
    function and is scaled by an element-wise multiplication to produce an intermediate
    hidden adjacency-matrix with similarity properties inherent to the shared graph.
  Figure 3 Link: articels_figures_by_rev_year\2021\Pay_Attention_to_Evolution_Time_Series_Forecasting_With_Deep_GraphEvolution_Lear\figure_3.jpg
  Figure 3 caption: Graph Soft Evolution layers assembled for evolution-based learning.
    In such a case, the first GSE layers output (i.e., source) will feed further layers
    of the neural network, whose result goes through the second GSE layer (i.e., target).
    The GSE, as the last layer, does not use regularizers nor linear transformations
    before the output. Contrarily, it outputs the result from the scalar product between
    the learned representation and the data propagated throughout the network.
  Figure 4 Link: articels_figures_by_rev_year\2021\Pay_Attention_to_Evolution_Time_Series_Forecasting_With_Deep_GraphEvolution_Lear\figure_4.jpg
  Figure 4 caption: Data diagram of the Recurrent Graph Evolution Neural Network (ReGENN),
    which has a linear component parallel to a non-linear one. The linear component
    has a feed-forward layer, and the non-linear one has an auto-encoder and two GSE
    layers. Although equal to the first, the last GSE layer yields an early output
    as it is not stacked with another layer.
  Figure 5 Link: articels_figures_by_rev_year\2021\Pay_Attention_to_Evolution_Time_Series_Forecasting_With_Deep_GraphEvolution_Lear\figure_5.jpg
  Figure 5 caption: "Baseline results for the SARS-CoV-2 dataset over the Mean Absolute\
    \ Error (MAE), Root Mean Square Error (RMSE), and Mean Squared Logarithmic Error\
    \ (MSLE). The results are presented in descending order of MAE (i.e., worst performance\
    \ at the top). The results confirmed ReGENNs superior performance as it is the\
    \ algorithm with the lowest error and standard deviation \u2013 the improvement\
    \ in the experiment was no lower than 64.87 percent. In the image, the algorithms\
    \ are symbol-encoded based on their type and number of estimators; we use gray\
    \ arrows to report the standard deviation of the results. The negative deviation,\
    \ which is equal to the positive one, was suppressed for improved readability."
  Figure 6 Link: articels_figures_by_rev_year\2021\Pay_Attention_to_Evolution_Time_Series_Forecasting_With_Deep_GraphEvolution_Lear\figure_6.jpg
  Figure 6 caption: Set of Evolution Weights, i.e., cosine-similarity activated hidden
    weights extracted from ReGENN at the end of the network training. The images compare
    the cosine similarity between the set of variables within the SARS-CoV-2 dataset.
  Figure 7 Link: articels_figures_by_rev_year\2021\Pay_Attention_to_Evolution_Time_Series_Forecasting_With_Deep_GraphEvolution_Lear\figure_7.jpg
  Figure 7 caption: Baseline results for the Brazilian Weather dataset presented in
    descending order of MAE. In this experiment, ReGENN once more outperformed all
    the competing algorithms, demonstrating versatility by performing well even on
    a highly-seasonal dataset with improvement no lower than 11.95 percent. In the
    face of seasonality, the Elman RNN surpassed the Exponential Smoothing, the previously
    second-best algorithm. In the image, the algorithms are symbol-encoded based on
    their type and number of estimators; we use gray arrows to report the results
    standard deviation. The negative deviation, which is equal to the positive one,
    was suppressed for improved readability.
  Figure 8 Link: articels_figures_by_rev_year\2021\Pay_Attention_to_Evolution_Time_Series_Forecasting_With_Deep_GraphEvolution_Lear\figure_8.jpg
  Figure 8 caption: "The Evolution Weights from ReGENN for the Brazilian Weather dataset,\
    \ in which we use the cosine similarity activation function on the networks hidden\
    \ weights to compare the relationship between pairs of variables. The image uses\
    \ \u201CSol. Rad.\u201D as a shortening for Solar Radiation, \u201CTemp\u201D\
    \ for Temperature, \u201CMax\u201D for Maximum, and \u201CMin\u201D for Minimum."
  Figure 9 Link: articels_figures_by_rev_year\2021\Pay_Attention_to_Evolution_Time_Series_Forecasting_With_Deep_GraphEvolution_Lear\figure_9.jpg
  Figure 9 caption: Baseline results for the PhysioNet Computing in Cardiology Challenge
    2012 dataset presented in descending order of MAE, in which ReGENN was the algorithm
    with the best performance followed by the Linear SVR. The comparative improvement
    was no lower than 7.33 percent, but, in this case, ReGENN yielded an RMSE compatible
    with the Linear SVR. In the image, the algorithms are symbol-encoded based on
    their type and number of estimators; we use gray arrows to report standard deviation.
    The negative deviation, which is equal to the positive one, was suppressed for
    improved readability.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.51
  Name of the first author: Gabriel Spadon
  Name of the last author: Jimeng Sun
  Number of Figures: 10
  Number of Tables: 4
  Number of authors: 6
  Paper title: 'Pay Attention to Evolution: Time Series Forecasting With Deep Graph-Evolution
    Learning'
  Publication Date: 2021-04-27 00:00:00
  Table 1 caption: TABLE 1 Summary of Context-Specific Notations
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Ablation Results for the SARS-CoV-2 Dataset Using ReGENNs
    Data-Flow but no GSE Layer
  Table 3 caption: TABLE 3 Ablation Results From Experimenting Over the Brazilian
    Weather Dataset That was Conducted
  Table 4 caption: TABLE 4 Ablation Results Over the PhysioNet Computing in Cardiology
    Challenge 2012 Dataset
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3076155
- Affiliation of the first author: institute of information theory and automation,
    czech academy of sciences, prague, czech republic
  Affiliation of the last author: institute of information theory and automation,
    czech academy of sciences, prague, czech republic
  Figure 1 Link: articels_figures_by_rev_year\2021\Texture_Segmentation_Benchmark\figure_1.jpg
  Figure 1 caption: Sample mosaics with BTF (left) and dynamic (right) textures.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Texture_Segmentation_Benchmark\figure_2.jpg
  Figure 2 caption: Correlation between 36 segmentation criteria (mean correlation
    over 17 methods and 180 segmentation results each). A rectangles color and size
    correspond to the correlation value and standard deviation, respectively.
  Figure 3 Link: articels_figures_by_rev_year\2021\Texture_Segmentation_Benchmark\figure_3.jpg
  Figure 3 caption: Stability (dotted line averaged 20 test mosaics, solid line increasing
    test size from 20 to 180 mosaics) graphs for seven segmenters and the over-segmentation
    criterion.
  Figure 4 Link: articels_figures_by_rev_year\2021\Texture_Segmentation_Benchmark\figure_4.jpg
  Figure 4 caption: Criteria comparison for EDISON [9] and FSEG [80] methods on benchmark
    contest (PTSD&B) [30] and Berkeley data sets (BSDS) for increasing test size from
    20 to 180 images or mosaics. Arrow direction suggests the required criterion direction.
  Figure 5 Link: articels_figures_by_rev_year\2021\Texture_Segmentation_Benchmark\figure_5.jpg
  Figure 5 caption: "Performance curves and the corresponding performance integrals\
    \ for EWT-FCNT, FCNT, \u2020 FCNT, A3M, PCA-MS, GRPNMF, GRPNMF, Cooperative Mum-Shah\
    \ (CMS), Local Global Graph Cut (LGG), Improved GMRF (IGMRF), \u2020 RS methods\
    \ averaged over 20 or 80 mosaics and increasing threshold (t\u2208\u27E80.525;0.975\u27E9\
    ) ."
  Figure 6 Link: articels_figures_by_rev_year\2021\Texture_Segmentation_Benchmark\figure_6.jpg
  Figure 6 caption: "Three selected texture mosaics from the benchmark with the corresponding\
    \ ground-truth, and segmentation results for EWT-FCNT, FCNT, \u2020 FCNT, A3M,\
    \ PCA-MS, GRPNMF, Cooperative Mum-Shah (CMS), Local Global Graph Cut (LGG), Improved\
    \ GMRF (IGMRF), \u2020 RS, respectively."
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: "Stanislav Mike\u0161"
  Name of the last author: Michal Haindl
  Number of Figures: 6
  Number of Tables: 1
  Number of authors: 2
  Paper title: Texture Segmentation Benchmark
  Publication Date: 2021-04-27 00:00:00
  Table 1 caption: "TABLE 1 Color Benchmark Results for EWT-FCNT, FCNT, \u2020 \u2020\
    FCNT, A3M, PCA-MS, GRPNMF, CMS, LGG, IGMRF, \u2020 \u2020RS"
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3075916
- Affiliation of the first author: department of electrical and computer engineering,
    carnegie mellon university, pittsburgh, pa, usa
  Affiliation of the last author: department of electrical and computer engineering,
    carnegie mellon university, pittsburgh, pa, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\Designing_Display_Pixel_Layouts_for_UnderPanel_Cameras\figure_1.jpg
  Figure 1 caption: Improvements gained by redesigning the layout of the display pixels.
    Shown above are deblurred images from three lab prototypes of under-panel cameras
    corresponding to (from left to right) TOLED, POLED and proposed display layouts.
    The insets beside each result shows the corresponding input captured photographs
    as well as zoomed in regions. All results emulate displays with a resolution of
    300 dots per inch. The reader is encouraged to use the zoom tool to explore all
    three photographs.
  Figure 10 Link: articels_figures_by_rev_year\2021\Designing_Display_Pixel_Layouts_for_UnderPanel_Cameras\figure_10.jpg
  Figure 10 caption: Under-panel camera lab prototype. (a) Shows twelve photolighography
    masks that emulate different display designs. (b) Shows our overall prototype
    where we place a cell-phone camera tightly against the printed mask and capture
    images by accessing the touch screen.
  Figure 2 Link: articels_figures_by_rev_year\2021\Designing_Display_Pixel_Layouts_for_UnderPanel_Cameras\figure_2.jpg
  Figure 2 caption: Layout of the under-panel camera. The overall aperture consists
    of a collocated OLED display panel and a finite lens aperture.
  Figure 3 Link: articels_figures_by_rev_year\2021\Designing_Display_Pixel_Layouts_for_UnderPanel_Cameras\figure_3.jpg
  Figure 3 caption: Modeling the effective aperture of an under-panel camera.
  Figure 4 Link: articels_figures_by_rev_year\2021\Designing_Display_Pixel_Layouts_for_UnderPanel_Cameras\figure_4.jpg
  Figure 4 caption: Two commonly used OLED patterns, (a) T-OLED and (b) P-OLED, and
    the blur induced by them. For each LED type, we show (left) the display opening
    pattern, (center-left) the three-color tonemapped PSF, as well as (center-right)
    the PSF, in log-scale, corresponding to the green channel. The PSF for each color
    was computed by averaging across multiple wavelengths and weighted by camera spectral
    response. (right) The Fourier transform of the blur PSF, which is also the scaled
    auto-correlation of the aperture pattern.
  Figure 5 Link: articels_figures_by_rev_year\2021\Designing_Display_Pixel_Layouts_for_UnderPanel_Cameras\figure_5.jpg
  Figure 5 caption: We propose to optimize pixel layout by random tiling pixels and
    optimizing individual pixel openings. The figure above shows how the blur PSF
    changes when we introduce random tiling without changing the per-pixel pattern
    (top row), and when we optimize for per-pixel patterns under different criteria
    (bottom row), both with and without random tiling.
  Figure 6 Link: articels_figures_by_rev_year\2021\Designing_Display_Pixel_Layouts_for_UnderPanel_Cameras\figure_6.jpg
  Figure 6 caption: Effect of random tiling and pixel shape optimization. In the left
    four columns, we show the effect of random tiling to two common displays. In the
    right four columns, we show our optimized pixel opening shapes from two losses,
    top-10 L2 and top-10L2+invertible loss, and each with two tiling strategies during
    optimization.
  Figure 7 Link: articels_figures_by_rev_year\2021\Designing_Display_Pixel_Layouts_for_UnderPanel_Cameras\figure_7.jpg
  Figure 7 caption: Comparison of MTF plots. We compare radially min MTFs of different
    patterns and the table summarizes the area under the MTF curve (AUC) and light
    transmittance rate (LTR) for each mask. Larger AUC is better.
  Figure 8 Link: articels_figures_by_rev_year\2021\Designing_Display_Pixel_Layouts_for_UnderPanel_Cameras\figure_8.jpg
  Figure 8 caption: Performance of random tiling and pixel shape optimization. We
    compare six display layouts on the simulated dataset and evaluate PSNR and SSIM
    under varying noise levels.
  Figure 9 Link: articels_figures_by_rev_year\2021\Designing_Display_Pixel_Layouts_for_UnderPanel_Cameras\figure_9.jpg
  Figure 9 caption: Effect of display pixel density. We compare four pixel openingslayouts
    under varying display densities. The horizontal axes are display Dot-Per-Inch(DPI),
    and the vertical axes are PSNRs and SSIMs of the reconstructed images under a
    fixed noise level.
  First author gender probability: 0.66
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Anqi Yang
  Name of the last author: Aswin C. Sankaranarayanan
  Number of Figures: 21
  Number of Tables: 0
  Number of authors: 2
  Paper title: Designing Display Pixel Layouts for Under-Panel Cameras
  Publication Date: 2021-04-27 00:00:00
  Table 1 caption: Not Available
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3075978
- Affiliation of the first author: beihang university, beijing, china
  Affiliation of the last author: sea ai lab (sail), singapore, singapore
  Figure 1 Link: articels_figures_by_rev_year\2021\HumanCentric_Relation_Segmentation_Dataset_and_Solution\figure_1.jpg
  Figure 1 caption: Comparison of the proposed HRS task with VRD and HOI-det.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\HumanCentric_Relation_Segmentation_Dataset_and_Solution\figure_2.jpg
  Figure 2 caption: An example of the PIC dataset. For an image (a), both the entity
    segmentation (b) and human parsing (c) are labelled. We also label geometric and
    action relations. In the geometric relation graph (d), the nodes are entity instances
    of (b) while the edges are spatial relations. In the action relation graph, there
    are two kinds of nodes. The non-human nodes are also entity instances of (b) while
    the human nodes contain the interacted human parts in (c) which make the action.
  Figure 3 Link: articels_figures_by_rev_year\2021\HumanCentric_Relation_Segmentation_Dataset_and_Solution\figure_3.jpg
  Figure 3 caption: Distribution of action and geometric relations.
  Figure 4 Link: articels_figures_by_rev_year\2021\HumanCentric_Relation_Segmentation_Dataset_and_Solution\figure_4.jpg
  Figure 4 caption: (a) Distribution of action triplets and (b) distribution of geometric
    triplets.
  Figure 5 Link: articels_figures_by_rev_year\2021\HumanCentric_Relation_Segmentation_Dataset_and_Solution\figure_5.jpg
  Figure 5 caption: Distribution of things and stuff.
  Figure 6 Link: articels_figures_by_rev_year\2021\HumanCentric_Relation_Segmentation_Dataset_and_Solution\figure_6.jpg
  Figure 6 caption: Framework of the proposed SMS. It is composed of a feature extractor
    and three parallel branches. The entity segmentation branch detects entities centers
    and describes them by masks. The subject object matching branch pairs the corresponding
    subjects and objects and identifies their relations. The human parsing branch
    segments the semantic human parts. All these intermediate results are fused to
    generate the HRS results.
  Figure 7 Link: articels_figures_by_rev_year\2021\HumanCentric_Relation_Segmentation_Dataset_and_Solution\figure_7.jpg
  Figure 7 caption: Visualization of HRS results on PIC test set. For each image,
    only 25 most confident relation triplets are shown. In each result set, the upper
    row shows the original image, entity segmentation masks and interacted human part
    segments. The lower row are the geometric and action relation graph. In both graphs,
    green and red arrows stand for true positives and false negatives. Nodes with
    red border are ground truth masks not matched by the entities in the top 25 relations.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.72
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Si Liu
  Name of the last author: Shuicheng Yan
  Number of Figures: 7
  Number of Tables: 11
  Number of authors: 8
  Paper title: 'Human-Centric Relation Segmentation: Dataset and Solution'
  Publication Date: 2021-04-27 00:00:00
  Table 1 caption: TABLE 1 Comparison of VRD, HOI-det and HRS
  Table 10 caption: TABLE 10 Comparisons of Different Displacement Estimation Strategies
  Table 2 caption: TABLE 2 Human Parsing and Pose Labels
  Table 3 caption: TABLE 3 Comparison With Existing Datasets
  Table 4 caption: TABLE 4 Comparisons on PIC Test Set
  Table 5 caption: TABLE 5 Comparison on VCOCO Test Set
  Table 6 caption: TABLE 6 Performance on Geometric (Geo) and Action (Act) Relations
    With ResNet-50 Backbone
  Table 7 caption: TABLE 7 Performance on Rare and Non-Rare Relations With ResNet-50
    Backbone
  Table 8 caption: TABLE 8 Comparisons of Different Feature Extractors
  Table 9 caption: TABLE 9 Comparisons of Different Mask Generation Methods
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3075846
- Affiliation of the first author: department of computer science, city university
    of hong kong, hong kong
  Affiliation of the last author: department of computer science, city university
    of hong kong, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2021\ObjectLevel_Scene_Context_Prediction\figure_1.jpg
  Figure 1 caption: Inferring scene contexts from a standalone object. A standalone
    object provides rich information for predicting its scene context (i.e., other
    objects that co-occur with it and their spatial relations). While the pose and
    position of the person in the image suggest that the scene may be related to sports
    activities, the presence and position of the person provide hints as to what and
    where other objects can appear (e.g., the sky in the upper part of the image,
    and the sea in the lower part of the image).
  Figure 10 Link: articels_figures_by_rev_year\2021\ObjectLevel_Scene_Context_Prediction\figure_10.jpg
  Figure 10 caption: 'Hidden unit visualization of the encoder (first row) and layout
    discriminator (second row). Top: for a specific hidden unit of the encoder, we
    show four object layouts and the regions of them that maximally activate it. Bottom:
    for a specific hidden unit of the layout discriminator, we show four scene layouts
    and the regions of them that maximally activate it.'
  Figure 2 Link: articels_figures_by_rev_year\2021\ObjectLevel_Scene_Context_Prediction\figure_2.jpg
  Figure 2 caption: Overview of our network architecture. Our model takes as input
    an object layout encoding the properties of input objects and generates a scene
    layout representing the scene context. We use the category classifier to pre-train
    the encoder to obtain the object representations. The object representations and
    a noise vector are concatenated and passed to the shape generator and the region
    generator. The shape generator generates the shapes of all C object categories,
    while the region generator generates the parameters and confidence values of B
    bounding boxes to represent the potential region proposals for each category.
    The boundary thickness of each bounding box indicates the confidence score of
    the box. The bounding boxes are then used to warp their corresponding shapes to
    a coarse scene layout, which is then refined by a compositor to output a final
    scene layout. Finally, a shape discriminator and a layout discriminator are introduced
    to classify the generated object shapes and scene layouts, respectively, as real
    or fake.
  Figure 3 Link: articels_figures_by_rev_year\2021\ObjectLevel_Scene_Context_Prediction\figure_3.jpg
  Figure 3 caption: Details of various modules used, including encoder, shaperegion
    generators, compositor, and shapelayout discriminators.
  Figure 4 Link: articels_figures_by_rev_year\2021\ObjectLevel_Scene_Context_Prediction\figure_4.jpg
  Figure 4 caption: Qualitative results from our model and the baselines. Given the
    input object layouts (first row), which contain one or two standalone objects,
    we generate output scene layouts using our model (second row) and the baselines
    (third to fifth rows).
  Figure 5 Link: articels_figures_by_rev_year\2021\ObjectLevel_Scene_Context_Prediction\figure_5.jpg
  Figure 5 caption: Diversity evaluation of our method, compared with SPADE [45] and
    BicycleGAN [46]. Given an input object layout (left), we show two different scene
    layouts generated by our method, SPADE, and BicycleGAN, respectively.
  Figure 6 Link: articels_figures_by_rev_year\2021\ObjectLevel_Scene_Context_Prediction\figure_6.jpg
  Figure 6 caption: Qualitative results of our model by varying the categories, shapes
    and spatial relation of the input objects.
  Figure 7 Link: articels_figures_by_rev_year\2021\ObjectLevel_Scene_Context_Prediction\figure_7.jpg
  Figure 7 caption: Performance versus input object size. We use NLL to evaluate the
    performance on three object categories (i.e., person, car and airplane). The lower
    the score, the better the performance. The input object size is normalized with
    respect to the image size to range between 0 and 1).
  Figure 8 Link: articels_figures_by_rev_year\2021\ObjectLevel_Scene_Context_Prediction\figure_8.jpg
  Figure 8 caption: Qualitative results of our model by changing the category of an
    input object to an unusual one while keeping the shape fixed.
  Figure 9 Link: articels_figures_by_rev_year\2021\ObjectLevel_Scene_Context_Prediction\figure_9.jpg
  Figure 9 caption: Given a partial scene layout or a sketch as input, our method
    is able to generate a complete scene layout and further synthesize a realistic
    full scene image.
  First author gender probability: 0.62
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.76
  Name of the first author: Xiaotian Qiao
  Name of the last author: Rynson W.H. Lau
  Number of Figures: 10
  Number of Tables: 5
  Number of authors: 4
  Paper title: Object-Level Scene Context Prediction
  Publication Date: 2021-04-27 00:00:00
  Table 1 caption: TABLE 1 Quantitative Evaluation of the Baselines (i.e., pix2pix,
    pix2pixHD, SPADE, and BicycleGAN) and Our Model (Ablated Versions and Full Model)
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Plausibility Scores for the Baseline (Baseline), Our Method
    (Ours), and the Ground Truth (GT)
  Table 3 caption: TABLE 3 Fitness Preferences for the Baseline (Baseline), Our Method
    (Ours), and the Ground Truth (GT)
  Table 4 caption: TABLE 4 Accuracy of Outdoor Scene Recognition on the SUN Dataset
    [57]
  Table 5 caption: TABLE 5 Accuracy of Fake Scene Detection
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3075676
- Affiliation of the first author: school of information science and technology, shanghaitech
    university, shanghai, china
  Affiliation of the last author: shanghai engineering research center of intelligent
    vision and imaging, shanghaitech university, shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2021\NonlineofSight_Imaging_via_Neural_Transient_Fields\figure_1.jpg
  Figure 1 caption: A typical confocal NLOS imaging system aims a laser towards a
    diffuse wall that serves as a virtual reflector. The hidden scene is indirectly
    illuminated as spherical waves that intersect with the scene and are reflected
    back onto the wall. A SPAD sensor measures at different spots on the wall to form
    transient images.
  Figure 10 Link: articels_figures_by_rev_year\2021\NonlineofSight_Imaging_via_Neural_Transient_Fields\figure_10.jpg
  Figure 10 caption: Comparisons between NeTF and DLCT on the Bunny scene. Both methods
    manage to acquire the overall geometry yet DLCT misses one ear whereas NeTF captures
    both. In its own implementation [16], DLCT further uses the mask (silhouettes)
    of the bunny to further improve reconstruction to obtain the final mesh.
  Figure 2 Link: articels_figures_by_rev_year\2021\NonlineofSight_Imaging_via_Neural_Transient_Fields\figure_2.jpg
  Figure 2 caption: Our neural transient field (NeTF) reconstruction pipeline. We
    parameterize every point on a spherical wavefront in terms of the origin on the
    wall, and the direction and radius of its corresponding spherical coordinates.
    We set out to recover the transient field under this parameterization via a multi-layer
    perception under spherical volume rendering.
  Figure 3 Link: articels_figures_by_rev_year\2021\NonlineofSight_Imaging_via_Neural_Transient_Fields\figure_3.jpg
  Figure 3 caption: NeRF versus NeTF. In NeRF, volume density is accumulated along
    every line (ray) whereas in NeTF it is accumulated on a spherical wavefront.
  Figure 4 Link: articels_figures_by_rev_year\2021\NonlineofSight_Imaging_via_Neural_Transient_Fields\figure_4.jpg
  Figure 4 caption: The volume rendering model derived under the spherical coordinate
    system, suitable for processing in NeTF.
  Figure 5 Link: articels_figures_by_rev_year\2021\NonlineofSight_Imaging_via_Neural_Transient_Fields\figure_5.jpg
  Figure 5 caption: We use the loss induced from the first stage in the training process
    to guide resampling. At each spot on the relay wall (a), we measure the loss (b)
    and apply our second stage for resampling. (c) shows the final loss after applying
    resampling.
  Figure 6 Link: articels_figures_by_rev_year\2021\NonlineofSight_Imaging_via_Neural_Transient_Fields\figure_6.jpg
  Figure 6 caption: 'From left to right: recovered results using our one-stage training,
    two-stage training without hierarchical sampling, and two-stage with hierarchical
    sampling. One-stage training, same as most prior art (Fig. 9), fails to recover
    the second ear of the bunny. Our two-stage schemes manage to recover both ears
    of the bunny. With hierarchical sampling, NeTF further improves reconstruction
    with more complete shape and more accurate silhouettes.'
  Figure 7 Link: articels_figures_by_rev_year\2021\NonlineofSight_Imaging_via_Neural_Transient_Fields\figure_7.jpg
  Figure 7 caption: 'NeTF network architecture: we adopt an MLP structure analogous
    to the one used in NeRF. The key differences are (1) NeTF uses ReLU versus NeRF
    uses sigmoid and (2) the last four layers in NeRF are simplified to one layer.'
  Figure 8 Link: articels_figures_by_rev_year\2021\NonlineofSight_Imaging_via_Neural_Transient_Fields\figure_8.jpg
  Figure 8 caption: 'From left to right: the ground truth, the recovered volume density,
    reflectance, albedo, and 3D mesh reconstruction using NeTF. Top shows the results
    on the Lucy model and bottom on the Statue.'
  Figure 9 Link: articels_figures_by_rev_year\2021\NonlineofSight_Imaging_via_Neural_Transient_Fields\figure_9.jpg
  Figure 9 caption: Comparisons on simulated NLOS data. NeTF achieves comparable reconstructions
    as SOTA and further manages to recover challenging geometry such as the ear of
    Bunny, the wing of Lucy and the crown of Indonesian.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.99
  Name of the first author: Siyuan Shen
  Name of the last author: Jingyi Yu
  Number of Figures: 14
  Number of Tables: 2
  Number of authors: 8
  Paper title: Non-line-of-Sight Imaging via Neural Transient Fields
  Publication Date: 2021-04-27 00:00:00
  Table 1 caption: TABLE 1 Reconstruction Error Using NeTF versus SOTA on Three Confocal
    NLOS Datasets Measured by MAE
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Reconstruction Error Using NeTF versus SOTA on Two Non-Confocal
    NLOS Datasets Measured by MAE
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3076062
