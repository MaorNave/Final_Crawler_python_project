- Affiliation of the first author: school of computer science and engineering and
    the key lab of computer network and information integration (ministry of education),
    southeast university, nanjing, china
  Affiliation of the last author: school of computer science and engineering and the
    key lab of computer network and information integration (ministry of education),
    southeast university, nanjing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Multilabel_Ranking_With_Inconsistent_Rankers\figure_1.jpg
  Figure 1 caption: The relevant label ranking results from different human rankers
    for one image in (a) the natural scene image dataset and (b) the facial expression
    image dataset, respectively.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Multilabel_Ranking_With_Inconsistent_Rankers\figure_2.jpg
  Figure 2 caption: The training process of (a) Instance-oriented Preference Distribution
    Learning (IPDL) and (b) Ranker-oriented Preference Distribution Learning (RPDL).
  Figure 3 Link: articels_figures_by_rev_year\2021\Multilabel_Ranking_With_Inconsistent_Rankers\figure_3.jpg
  Figure 3 caption: Typical examples of preference distribution transformation. In
    each subfigure, the upper-left is the original image. The lower-left is the preference
    distribution generated from the inconsistent rankings shown in the right table.
    The threshold between the relevant and irrelevant labels determined by the virtual
    labels is represented by red.
  Figure 4 Link: articels_figures_by_rev_year\2021\Multilabel_Ranking_With_Inconsistent_Rankers\figure_4.jpg
  Figure 4 caption: The results of different measures with different number of rankers.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.6
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.53
  Name of the first author: Xin Geng
  Name of the last author: Yu Zhang
  Number of Figures: 4
  Number of Tables: 3
  Number of authors: 4
  Paper title: Multilabel Ranking With Inconsistent Rankers
  Publication Date: 2021-04-02 00:00:00
  Table 1 caption: TABLE 1 Evaluation Measures for Multilabel Ranking
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 Comparison Results (mean \xB1 \xB1std (t-test)) of the\
    \ Six Algorithms on Ten Evaluation Measures on the Natural Scene Image Dataset"
  Table 3 caption: "TABLE 3 Comparison Results (mean \xB1 \xB1std (t-test)) of the\
    \ Six Algorithms on Ten Evaluation Measures on the BU3DFE Dataset"
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3070709
- Affiliation of the first author: institute for advanced study in mathematics, harbin
    institute of technology, harbin, china
  Affiliation of the last author: inception institute of artificial intelligence,
    abu dhabi, uae
  Figure 1 Link: articels_figures_by_rev_year\2021\A_Generalized_Method_for_Binary_Optimization_Convergence_Analysis_and_Applicatio\figure_1.jpg
  Figure 1 caption: MAP comparison on the three datasets with code lengths of 16 to
    128. The two figures on the left show the results of supervised methods with objective
    function (14) and the two figures on the right show the results of unsupervised
    methods with objective function (21).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\A_Generalized_Method_for_Binary_Optimization_Convergence_Analysis_and_Applicatio\figure_2.jpg
  Figure 2 caption: Comparison of PR curves for the supervised and unsupervised methods
    on CIFAR-10 and SIFT1M, respectively, with code lengths of 32 and 64.
  Figure 3 Link: articels_figures_by_rev_year\2021\A_Generalized_Method_for_Binary_Optimization_Convergence_Analysis_and_Applicatio\figure_3.jpg
  Figure 3 caption: Convergence curve for ABMO-U on the CIFAR-10 dataset with code
    lengths of 32 and 64.
  Figure 4 Link: articels_figures_by_rev_year\2021\A_Generalized_Method_for_Binary_Optimization_Convergence_Analysis_and_Applicatio\figure_4.jpg
  Figure 4 caption: Comparison of ABMO with other binary optimization methods for
    the dense subgraph discovery task with kin lbrace 200,400,800,1600,3200,6400 rbrace
    on eight different graph datasets. Larger metric mathbf xtop mathbf Wmathbf xk
    represents the better performance.
  Figure 5 Link: articels_figures_by_rev_year\2021\A_Generalized_Method_for_Binary_Optimization_Convergence_Analysis_and_Applicatio\figure_5.jpg
  Figure 5 caption: Constrained image segmentation results on five images from the
    Berkeley dataset. The first row shows the original images (a) sim (e). Red and
    blue points represent prior given foreground and background pixels, respectively.
    The second rows to the seventh rows show the segmented images obtained by the
    proposed ABMO and other five methods IGC, MPEC-ADM, MPEC-EPM, LP, and L2box-ADMM,
    respectively.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Huan Xiong
  Name of the last author: Ling Shao
  Number of Figures: 5
  Number of Tables: 7
  Number of authors: 7
  Paper title: 'A Generalized Method for Binary Optimization: Convergence Analysis
    and Applications'
  Publication Date: 2021-04-02 00:00:00
  Table 1 caption: TABLE 1 Evaluation of the Proposed ABMO With or Without Bits Uncorrelation
    or Balance Constraints on the Image Retrieval Task
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Evaluation of the Proposed ABMO With or Without Bits Uncorrelation
    or Balance Constraints on NUS-WIDE
  Table 3 caption: TABLE 3 Comparison of the Proposed ABMO Method, Another General
    Binary Optimization Algorithm MPEC-EPM [9], and the Original Methods Based on
    the Same Objective Functions
  Table 4 caption: TABLE 4 Statistics for the Graph Datasets Used in the Graph Bisection
    and Dense Subgraph Discovery Experiments
  Table 5 caption: TABLE 5 The CPU Time (in Seconds) of Each Method for the Graph
    Bisection Task on the Nine Datasets
  Table 6 caption: TABLE 6 Numerical Results of Proposed ABMO Compared With Other
    Binary Optimization Methods for Graph Bisection
  Table 7 caption: TABLE 7 Numerical Results of Proposed ABMO Compared With Five Other
    Methods for Constrained Image Segmentation
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3070753
- Affiliation of the first author: university of bologna, bologna, italy
  Affiliation of the last author: university of bologna, bologna, italy
  Figure 1 Link: articels_figures_by_rev_year\2021\On_the_Confidence_of_Stereo_Matching_in_a_DeepLearning_Era_A_Quantitative_Evalua\figure_1.jpg
  Figure 1 caption: Confidence estimation example. From left to right, reference image,
    disparity map, and estimated confidence map (pixels from black to white encode
    confidence from lower to higher).
  Figure 10 Link: articels_figures_by_rev_year\2021\On_the_Confidence_of_Stereo_Matching_in_a_DeepLearning_Era_A_Quantitative_Evalua\figure_10.jpg
  Figure 10 caption: 'Qualitative results concerning MCCNN-SGM algorithm. Results
    on KITTI 2015 and Middlebury showing a variety of confidence measures. From top
    left to bottom right: reference image, disparity map and confidence maps by APKR
    7 , WMN, DA 31 , UCC, SAMM and LAF.'
  Figure 2 Link: articels_figures_by_rev_year\2021\On_the_Confidence_of_Stereo_Matching_in_a_DeepLearning_Era_A_Quantitative_Evalua\figure_2.jpg
  Figure 2 caption: 'Example of cost curves for a pixel p : on x axis, disparity hypotheses
    i , on y axis, matching cost c i . We show an ambiguous curve in black, for which
    d 1 and d 2 (respectively equal to 46 and 48) compete for the role of minimum
    and other local minima exist (at disparities 22 and 60, the former corresponding
    to d 2m ). We also show an ideal cost curve in blue with a clear winner. Best
    viewed in color.'
  Figure 3 Link: articels_figures_by_rev_year\2021\On_the_Confidence_of_Stereo_Matching_in_a_DeepLearning_Era_A_Quantitative_Evalua\figure_3.jpg
  Figure 3 caption: Impact of N(p) size, Census-CBCA algorithm.
  Figure 4 Link: articels_figures_by_rev_year\2021\On_the_Confidence_of_Stereo_Matching_in_a_DeepLearning_Era_A_Quantitative_Evalua\figure_4.jpg
  Figure 4 caption: 'Qualitative results concerning Census-CBCA algorithm. Results
    on KITTI 2015 and Middlebury showing a variety of confidence measures. From top
    left to bottom right: reference image, disparity map and confidence maps by APKR
    7 , WMN, DA 31 , UCC, SAMM and LAF.'
  Figure 5 Link: articels_figures_by_rev_year\2021\On_the_Confidence_of_Stereo_Matching_in_a_DeepLearning_Era_A_Quantitative_Evalua\figure_5.jpg
  Figure 5 caption: Impact of N(p) size, MCCNN-CBCA algorithm.
  Figure 6 Link: articels_figures_by_rev_year\2021\On_the_Confidence_of_Stereo_Matching_in_a_DeepLearning_Era_A_Quantitative_Evalua\figure_6.jpg
  Figure 6 caption: 'Qualitative results concerning MCCNN-CBCA algorithm. Results
    on KITTI 2015 and Middlebury showing a variety of confidence measures. From top
    left to bottom right: reference image, disparity map and confidence maps by APKR
    7 , WMN, DA 31 , UCC, SAMM and LAF.'
  Figure 7 Link: articels_figures_by_rev_year\2021\On_the_Confidence_of_Stereo_Matching_in_a_DeepLearning_Era_A_Quantitative_Evalua\figure_7.jpg
  Figure 7 caption: Impact of N(p) size, Census-SGM algorithm.
  Figure 8 Link: articels_figures_by_rev_year\2021\On_the_Confidence_of_Stereo_Matching_in_a_DeepLearning_Era_A_Quantitative_Evalua\figure_8.jpg
  Figure 8 caption: 'Qualitative results concerning Census-SGM algorithm. Results
    on KITTI 2015 and Middlebury showing a variety of confidence measures. From left
    to right: reference image, disparity map and confidence maps by APKR 7 , WMN,
    DA 31 , UCC, SAMM and LAF.'
  Figure 9 Link: articels_figures_by_rev_year\2021\On_the_Confidence_of_Stereo_Matching_in_a_DeepLearning_Era_A_Quantitative_Evalua\figure_9.jpg
  Figure 9 caption: Impact of N(p) size, MCCNN-SGM algorithm.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Matteo Poggi
  Name of the last author: Stefano Mattoccia
  Number of Figures: 12
  Number of Tables: 12
  Number of authors: 8
  Paper title: 'On the Confidence of Stereo Matching in a Deep-Learning Era: A Quantitative
    Evaluation'
  Publication Date: 2021-04-02 00:00:00
  Table 1 caption: TABLE 1 Taxonomy of Confidence Measures
  Table 10 caption: TABLE 10 Results With MCCNN-SGM Algorithm, Learned Measures
  Table 2 caption: TABLE 2 GANet Disparity Map Accuracy, With Different Selection
    Strategies
  Table 3 caption: TABLE 3 Results With Census-CBCA Algorithm, Hand-Crafted Measures
  Table 4 caption: TABLE 4 Results With Census-CBCA Algorithm, Learned Measures
  Table 5 caption: TABLE 5 Results With MCCNN-CBCA Algorithm, Hand-Crafted Measures
  Table 6 caption: TABLE 6 Results With MCCNN-CBCA Algorithm, Learned Measures
  Table 7 caption: TABLE 7 Results With Census-SGM Algorithm, Hand-Crafted Measures
  Table 8 caption: TABLE 8 Results With Census-SGM Algorithm, Learned Measures
  Table 9 caption: TABLE 9 Results With MCCNN-SGM Algorithm, Hand-Crafted Measures
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3069706
- Affiliation of the first author: politecnico di torino, torino, italy
  Affiliation of the last author: politecnico di torino, torino, italy
  Figure 1 Link: articels_figures_by_rev_year\2021\SelfSupervised_Learning_Across_Domains\figure_1.jpg
  Figure 1 caption: Recognizing objects across visual domains is a challenging task
    that requires high generalization abilities. Self-supervisory image signals allow
    to capture natural invariances and regularities that can help to bridge across
    large style gaps. With our multi-task approach we learn jointly to classify objects
    and solve jigsaw puzzles or recognize image orientation, showing that this supports
    generalization to new domains.
  Figure 10 Link: articels_figures_by_rev_year\2021\SelfSupervised_Learning_Across_Domains\figure_10.jpg
  Figure 10 caption: Histogram showing the elements of the gamma vector, corresponding
    to the class weight learned by PADA, SSPDA- gamma and SSPDA-PADA for the A rightarrow
    W experiment.
  Figure 2 Link: articels_figures_by_rev_year\2021\SelfSupervised_Learning_Across_Domains\figure_2.jpg
  Figure 2 caption: "Illustration of the proposed multi-task approach when using jigsaw\
    \ puzzle as self-supervised task. We start from images of multiple domains and\
    \ use a 3\xD73 grid to decompose them in 9 patches which are then randomly shuffled\
    \ and recomposed into images of the same dimension of the original ones. Through\
    \ the maximal Hamming distance algorithm in [58] we define a set of P patch permutations\
    \ and assign an index to each of them. Both the original ordered and the shuffled\
    \ images are fed to a convolutional network that is optimized to satisfy two objectives:\
    \ object classification on the ordered images and jigsaw classification (i.e.,\
    \ permutation index recognition) on the shuffled images. An analogous scheme holds\
    \ when using rotation recognition as self-supervision. The names assigned to each\
    \ network part refer to the notation adopted in Section 3."
  Figure 3 Link: articels_figures_by_rev_year\2021\SelfSupervised_Learning_Across_Domains\figure_3.jpg
  Figure 3 caption: "Our PDA approach with jigsaw puzzle self-supervision. The main\
    \ blocks of the network are in gray. The solid line arrows indicate the contribution\
    \ of each group of training samples to the corresponding final tasks. The related\
    \ optimization goals appear at the end of the blackgreenocher arrows. The red\
    \ blocks illustrate the domain adversarial classifier and source sample weighting\
    \ procedure (weight \u03B3 ). An analogous scheme holds with self-supervised rotation\
    \ recognition."
  Figure 4 Link: articels_figures_by_rev_year\2021\SelfSupervised_Learning_Across_Domains\figure_4.jpg
  Figure 4 caption: "Single Source DG experiments. We analyze the performance of our\
    \ multi-task Jigsaw (top row) and Rotation (bottom row) approaches in comparison\
    \ with Adv.DA [75]. The shaded background area covers the overall range of results\
    \ of Adv.DA obtained when changing the hyper-parameters of the method. The reference\
    \ result of Adv.DA ( \u03B3=1 , K=2 ) together with its standard deviation is\
    \ indicated here by the horizontal red line. The blue histogram bars show the\
    \ performance of Jigsaw and Rotation when changing the self-supervised task weight\
    \ \u03B1 and data bias \u03B2 ."
  Figure 5 Link: articels_figures_by_rev_year\2021\SelfSupervised_Learning_Across_Domains\figure_5.jpg
  Figure 5 caption: Ablation results and hyper-parameter analysis on the Alexnet-PACS
    DG setting when using Jigsaw. The reported accuracy is the global average over
    all the target domains with three repetitions for each run. The red line represents
    our DeepAll average from Table 2.
  Figure 6 Link: articels_figures_by_rev_year\2021\SelfSupervised_Learning_Across_Domains\figure_6.jpg
  Figure 6 caption: Ablation results on the Alexnet-PACS DG setting when using Rotation.
    We report the average accuracy over all target domains with three repetitions
    for each run. The red line is our DeepAll from Table 2.
  Figure 7 Link: articels_figures_by_rev_year\2021\SelfSupervised_Learning_Across_Domains\figure_7.jpg
  Figure 7 caption: Analysis of the Jigsaw classifier on Alexnet-PACS DG setting.
    In the left plot each axes refers to the color matching curve in the graph.
  Figure 8 Link: articels_figures_by_rev_year\2021\SelfSupervised_Learning_Across_Domains\figure_8.jpg
  Figure 8 caption: 'CAM activation maps: yellow corresponds to high values, while
    dark blue corresponds to low values. The jigsaw puzzle task is able to localize
    the most informative part of the image, useful for object class prediction regardless
    of the visual domain. Rotation recognition has a similar effect but tend to be
    less precise in localization especially for sketches, cartoon and paintings.'
  Figure 9 Link: articels_figures_by_rev_year\2021\SelfSupervised_Learning_Across_Domains\figure_9.jpg
  Figure 9 caption: Scheme of the Predictive DA setting. The goal is to recognize
    the four types of car, while the view point and the year are the meta-data.
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Silvia Bucci
  Name of the last author: Tatiana Tommasi
  Number of Figures: 10
  Number of Tables: 9
  Number of authors: 6
  Paper title: Self-Supervised Learning Across Domains
  Publication Date: 2021-04-02 00:00:00
  Table 1 caption: 'TABLE 1 Test on Different Tasks and Architectures: DG Classification
    Accuracy'
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison With DG-Sota Methods on PACS
  Table 3 caption: TABLE 3 Comparison With DG-Sota Methods on VLCS
  Table 4 caption: TABLE 4 Comparison With DG-Sota Methods on Office-Home
  Table 5 caption: TABLE 5 Predictive DA Results
  Table 6 caption: TABLE 6 Accuracy on Office-Home Under Single-Source DA Setting
  Table 7 caption: TABLE 7 Multi-Source Domain Adaptation Results on PACS
  Table 8 caption: 'TABLE 8 Classification Accuracy in the PDA Setting on Office-31
    (Source: 31 Classes, Target: 10 Classes)'
  Table 9 caption: 'TABLE 9 Classification Accuracy in the PDA Setting on VisDA2017
    (Source: 12 Classes, Target: 6 Classes)'
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3070791
- Affiliation of the first author: anhui provincial key laboratory of multimodal cognitive
    computation, school of computer science and technology, anhui university, hefei,
    china
  Affiliation of the last author: anhui provincial key laboratory of multimodal cognitive
    computation, school of computer science and technology, anhui university, hefei,
    china
  Figure 1 Link: articels_figures_by_rev_year\2021\GeCNs_Graph_Elastic_Convolutional_Networks_for_Data_Representation\figure_1.jpg
  Figure 1 caption: 2D t-SNE [34] visualizations of features output by the first convolutional
    layer of GCN [2], GAT [3] and GeCN respectively on Cora dataset. Different classes
    are labeled by different colors.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\GeCNs_Graph_Elastic_Convolutional_Networks_for_Data_Representation\figure_2.jpg
  Figure 2 caption: 2D t-SNE [34] visualizations of features output by the first convolutional
    layer of GCN [2], GAT [3] and GeCN respectively on Amazon Computers dataset. Different
    classes are labeled by different colors.
  Figure 3 Link: articels_figures_by_rev_year\2021\GeCNs_Graph_Elastic_Convolutional_Networks_for_Data_Representation\figure_3.jpg
  Figure 3 caption: Demonstration of cross-entropy loss values across different epochs
    on Cora dataset.
  Figure 4 Link: articels_figures_by_rev_year\2021\GeCNs_Graph_Elastic_Convolutional_Networks_for_Data_Representation\figure_4.jpg
  Figure 4 caption: Comparison results of GeCN and other related methods under different
    label rate on four datasets.
  Figure 5 Link: articels_figures_by_rev_year\2021\GeCNs_Graph_Elastic_Convolutional_Networks_for_Data_Representation\figure_5.jpg
  Figure 5 caption: Comparison results of GeCN, GAT-dot and GAT-topK under different
    label rate on three datasets.
  Figure 6 Link: articels_figures_by_rev_year\2021\GeCNs_Graph_Elastic_Convolutional_Networks_for_Data_Representation\figure_6.jpg
  Figure 6 caption: Comparison results of GCN and GeCN across different perturbation
    noise levels on Citeseer, Cora and Cora-ML datasets, respectively, which demonstrates
    the robustness of the proposed GeCN w.r.t graph edge noises.
  Figure 7 Link: articels_figures_by_rev_year\2021\GeCNs_Graph_Elastic_Convolutional_Networks_for_Data_Representation\figure_7.jpg
  Figure 7 caption: Comparison results of GCN and GeCN across different number of
    hidden layers on Cora dataset.
  Figure 8 Link: articels_figures_by_rev_year\2021\GeCNs_Graph_Elastic_Convolutional_Networks_for_Data_Representation\figure_8.jpg
  Figure 8 caption: Results of GeCN across different parameters alpha, gamma values
    on Cora and Citeseer datasets. They demonstrate the insensitivity of the proposed
    GeCN w.r.t its different parameters.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.83
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Bo Jiang
  Name of the last author: Bin Luo
  Number of Figures: 8
  Number of Tables: 10
  Number of authors: 4
  Paper title: 'GeCNs: Graph Elastic Convolutional Networks for Data Representation'
  Publication Date: 2021-04-02 00:00:00
  Table 1 caption: TABLE 1 Summary of all Datasets Used in Experiments
  Table 10 caption: TABLE 10 Running Time (s) on Simulated Data
  Table 2 caption: TABLE 2 Comparison Results of Different Methods on Cora and Citeseer
    Datasets
  Table 3 caption: TABLE 3 Comparison Results of Methods on Amazon Computers and Amazon
    Photo Datasets
  Table 4 caption: TABLE 4 Comparison Results of Different Methods on Pubmed and Coauthor
    CS Datasets
  Table 5 caption: TABLE 5 Comparison Results of Different Methods on Cora-ML Dataset
  Table 6 caption: TABLE 6 Inductive Learning Results on PPI Dataset
  Table 7 caption: TABLE 7 Comparison Results on Graph Classification Task
  Table 8 caption: TABLE 8 Comparison Results of Different K,L,T K,L,T on Cora-ML,
    Cora and Citeseer Datasets
  Table 9 caption: TABLE 9 Running Time (s) of Inductive Learning on PPI Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3070599
- Affiliation of the first author: stanford vision and learning laboratory, stanford
    university, stanford, ca, usa
  Affiliation of the last author: stanford vision and learning laboratory, stanford
    university, stanford, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\JRDB_A_Dataset_and_Benchmark_of_Egocentric_Robot_Visual_Perception_of_Humans_in_\figure_1.jpg
  Figure 1 caption: JackRabbot, our social robot and data collection platform, is
    equipped with four LiDAR sensors, three cameras, motion encoders, an IMU sensor,
    and a microphone.
  Figure 10 Link: articels_figures_by_rev_year\2021\JRDB_A_Dataset_and_Benchmark_of_Egocentric_Robot_Visual_Perception_of_Humans_in_\figure_10.jpg
  Figure 10 caption: Sensitivity to training set size of methods in the JRDB 2D detection
    benchmark. We include average precision (AP) and OSPA IoU at 3 different levels.
    Method performance marginally decreases with less data.
  Figure 2 Link: articels_figures_by_rev_year\2021\JRDB_A_Dataset_and_Benchmark_of_Egocentric_Robot_Visual_Perception_of_Humans_in_\figure_2.jpg
  Figure 2 caption: "Sample visualization of the dataset (best seen in color). For\
    \ each subfigure, top: 2D stitched 360 \u2218 panoarama with human-annotated 2D\
    \ bounding boxes, bottom: 3D Velodyne point clouds with human-annotated 3D oriented\
    \ bounding boxes. In order to demonstrate the accuracy of camera registration,\
    \ we visualize 3D point clouds with color extracted from the projected 2D RGB\
    \ stitched 360 panorama image. Our dataset captures a variety of pedestrian density,\
    \ indooroutdoor scenes, and movingstationary robot. Additional visualizations\
    \ for all scenes in Fig. A.3 in the Appendix, which can be found on the Computer\
    \ Society Digital Library at http:doi.ieeecomputersociety.org10.1109TPAMI.2021.3070543."
  Figure 3 Link: articels_figures_by_rev_year\2021\JRDB_A_Dataset_and_Benchmark_of_Egocentric_Robot_Visual_Perception_of_Humans_in_\figure_3.jpg
  Figure 3 caption: Histogram of areas of annotated 2D bounding boxes along with the
    Kernel Density Estimate. Train-validation and test sets show similar distributions
    (comparison between train and validation in Appendix, Fig. A.2, available in the
    online supplemental material). Our dataset contains a large number of frames in
    indoor scenes where people are closer to the robot, resulting in boxes with areas
    over 10000 pixel 2 . However, most boxes present an area of less than 10000 pixel
    2 as most annotated pedestrians are at 5m or further.
  Figure 4 Link: articels_figures_by_rev_year\2021\JRDB_A_Dataset_and_Benchmark_of_Egocentric_Robot_Visual_Perception_of_Humans_in_\figure_4.jpg
  Figure 4 caption: Histogram of distances of 3D bounding boxes to the sensor location.
    The distribution remains relatively unchanged between train-validation and test
    sets (comparison between train and validation in Appendix, Fig. A.2, available
    in the online supplemental material). Our dataset contains a large number of frames
    of indoor scenes where people tend to be closer to the robot, yielding a distribution
    with a mode at approximately 5m, which is quite different to existing datasets
    for autonomous cars. The long-tailed nature of the distribution is due to outdoor
    scenes, with some annotations as far as 80m from JackRabbot.
  Figure 5 Link: articels_figures_by_rev_year\2021\JRDB_A_Dataset_and_Benchmark_of_Egocentric_Robot_Visual_Perception_of_Humans_in_\figure_5.jpg
  Figure 5 caption: Kernel Density Estimates (KDEs) of the spatial distribution of
    people around the robot, across the train-validation and test splits of the dataset
    (comparison between train and validation in Appendix, Fig. A.2, available in the
    online supplemental material). In close proximity of JackRabbot, people are evenly
    spread across all directions, but tend to be along the X-axis (front and back
    of the robot with respect to the base frame) as the distance increases. This is
    due to the robot being driven along straight walkways when outdoors, and along
    corridors when indoors. Distances are in meters.
  Figure 6 Link: articels_figures_by_rev_year\2021\JRDB_A_Dataset_and_Benchmark_of_Egocentric_Robot_Visual_Perception_of_Humans_in_\figure_6.jpg
  Figure 6 caption: 'Distribution of occlusion levels (or lack thereof) in train-validation
    and test sets (comparison between train and validation in Appendix, Fig. A.2,
    available in the online supplemental material). JRDB considers four levels of
    occlusion: fully occluded, severely occluded, mostly occluded and fully visible.
    Approximately two thirds of the total number of boxes in both the train and test
    sets are more than 50 percent visible.'
  Figure 7 Link: articels_figures_by_rev_year\2021\JRDB_A_Dataset_and_Benchmark_of_Egocentric_Robot_Visual_Perception_of_Humans_in_\figure_7.jpg
  Figure 7 caption: 'Comparison between JRDB and similar existing datasets and benchmarks
    (MOT17, MOT20, KITTI, Waymo). Top row: 2D bounding box area divided by image area
    in training dataset. JRDB contains images where the pedestrians occupy a larger
    portion due to its closer proximity to people, while keeping a greater variance
    in sizes than other datasets. Bottom row: Amount of continuous time tracks are
    fully-occluded in training datasets. JRDB has longer and more diverse lengths
    in occluded track periods indicating more complex occlusion patterns caused by
    the robots point of view and the higher density of the crowd. Note that long tails
    past end of graph are not shown.'
  Figure 8 Link: articels_figures_by_rev_year\2021\JRDB_A_Dataset_and_Benchmark_of_Egocentric_Robot_Visual_Perception_of_Humans_in_\figure_8.jpg
  Figure 8 caption: Birds-eye view of LiDAR point clouds and 3D bounding box annotations
    of eight different JRDB sequences, including indoors and outdoors. The green boxes
    indicate pedestrians. JRDB includes an extremely large number of pedestrians at
    different distances and orientation to the sensors, including very close proximity
    ( approx 1m) or very distant ( > 50m), with social groups and clusters related
    to human walking patterns and activities, and navigation patterns strongly affected
    by the indoor and outdoor physical constraints. Other natural complexities such
    as cluttered indoor objects and stairs add to its challenges.
  Figure 9 Link: articels_figures_by_rev_year\2021\JRDB_A_Dataset_and_Benchmark_of_Egocentric_Robot_Visual_Perception_of_Humans_in_\figure_9.jpg
  Figure 9 caption: Sensitivity to occlusion of methods in the JRDB 2D detection benchmark.
    We include average precision (AP) and OSPA IoU at 3 different occlusion (Occ.)
    levels. All methods get significantly worse with occlusion, but YOLOv3 is the
    most strongly affected.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "Roberto Mart\xEDn-Mart\xEDn"
  Name of the last author: Silvio Savarese
  Number of Figures: 14
  Number of Tables: 8
  Number of authors: 8
  Paper title: 'JRDB: A Dataset and Benchmark of Egocentric Robot Visual Perception
    of Humans in Built Environments'
  Publication Date: 2021-04-02 00:00:00
  Table 1 caption: TABLE 1 Statistical Analysis of the Data in JRDB
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparing Statistics Between JRDB and Similar Existing
    Datasets
  Table 3 caption: TABLE 3 2D Detection Results in the JRDB Benchmark Using Average
    Precision (AP) and OSPA IoU I o U
  Table 4 caption: TABLE 4 2D Tracking Results in the JRDB Benchmark Using Clear-MOT
    Metrics and OSPA (2) IoU I o U ( 2 )
  Table 5 caption: TABLE 5 Number of ID Switches in 2D Trackers WithWithout Re-Identification
    After Long Occlusions (RALO)
  Table 6 caption: TABLE 6 3D Detection Results in the JRDB Zbenchmark Using Average
    Precision (AP) and OSPA IoU I o U
  Table 7 caption: TABLE 7 Results in the JRDB 3D Tracking Benchmark
  Table 8 caption: TABLE 8 Number of ID Switches in 3D Trackers WithWithout Re-Identification
    After Long Occlusions (RALO)
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3070543
- Affiliation of the first author: department of electrical engineering and computer
    science, lassonde school of engineering, york university, toronto, on, canada
  Affiliation of the last author: department of electrical engineering and computer
    science, lassonde school of engineering, york university, toronto, on, canada
  Figure 1 Link: articels_figures_by_rev_year\2021\CIE_XYZ_Net_Unprocessing_Images_for_LowLevel_Computer_Vision_Tasks\figure_1.jpg
  Figure 1 caption: 'We propose a cycle framework that can unprocess sRGB images back
    to the linear CIE XYZ color space and re-render the CIE XYZ images into the nonlinear
    sRGB color space. (A) The input camera-rendered sRGB image. (B) Our image decomposition
    (left: residual photo-finishing layer, right: scene-referred CIE XYZ reconstruction).
    (C) The ground-truth scene-referred CIE XYZ image. (D) Our re-rendering result
    from the reconstructed CIE XYZ image. To aid visualization, CIE XYZ images are
    scaled by a factor of two. Input image is taken from the MIT-Adobe FiveK dataset
    [2].'
  Figure 10 Link: articels_figures_by_rev_year\2021\CIE_XYZ_Net_Unprocessing_Images_for_LowLevel_Computer_Vision_Tasks\figure_10.jpg
  Figure 10 caption: Sensor raw-RGB image reconstruction. (A) An sRGB image rendered
    by Canon 5D from Gehler-Shi [38]. (B) Our reconstructed CIE XYZ image. (C) Our
    reconstructed raw image in the raw-RGB space of the Canon EOS-1Ds Mark III. (D)
    Two generated raw-RGB images with different illuminant responses in the Canon
    EOS-1Ds Mark IIIs sensor space. (E) A real raw-RGB image captured by the Canon
    EOS-1Ds Mark III taken from the eight-camera NUS dataset [39]. To aid visualization,
    the shown images are scaled by a factor of two.
  Figure 2 Link: articels_figures_by_rev_year\2021\CIE_XYZ_Net_Unprocessing_Images_for_LowLevel_Computer_Vision_Tasks\figure_2.jpg
  Figure 2 caption: A simplified depiction of a camera imaging pipeline, adapted from
    [1], [14], [15], [16]. Our method allows mapping back and forth between the common
    nonlinear sRGB image and to the colorimetric, linear, color-balanced CIE XYZ image
    state for computer vision tasks.
  Figure 3 Link: articels_figures_by_rev_year\2021\CIE_XYZ_Net_Unprocessing_Images_for_LowLevel_Computer_Vision_Tasks\figure_3.jpg
  Figure 3 caption: An illustration of using our inverse and forward image processing
    pipelines in an sRGB image restorationenhancement framework.
  Figure 4 Link: articels_figures_by_rev_year\2021\CIE_XYZ_Net_Unprocessing_Images_for_LowLevel_Computer_Vision_Tasks\figure_4.jpg
  Figure 4 caption: Our CIE XYZ image pipeline. The upper part is the inverse pipeline
    that unprocesses an sRGB image into a CIE XYZ image. The lower part is the forward
    pipeline that processes a CIE XYZ image into its equivalent sRGB image. The full
    framework is trainable end-to-end. The CIE XYZ images are scaled 2x to aid visualization.
  Figure 5 Link: articels_figures_by_rev_year\2021\CIE_XYZ_Net_Unprocessing_Images_for_LowLevel_Computer_Vision_Tasks\figure_5.jpg
  Figure 5 caption: Our inverse pipeline decomposites a given camera-rendered sRGB
    image into a local processed layer and the corresponding CIE XYZ image, while
    our forward pipeline maps the reconstructed CIE XYZ image to the sRGB color space
    in an inverse way of our decomposition. The shown image is taken from our testing
    set. To aid visualization, CIE XYZ images are scaled by a factor of two.
  Figure 6 Link: articels_figures_by_rev_year\2021\CIE_XYZ_Net_Unprocessing_Images_for_LowLevel_Computer_Vision_Tasks\figure_6.jpg
  Figure 6 caption: Qualitative comparisons for CIE XYZ reconstruction and rendering.
    (A) The input sRGB rendered image. (B) Standard display-referred CIE XYZ reconstruction
    [22], [23]. (C) Our reconstruction. (D) The ground-truth scene-referred CIE XYZ
    image. (E) Our re-rendering result from the reconstructed CIE XYZ image. To aid
    visualization, CIE XYZ images are scaled by a factor of two. Input images are
    taken from the MIT-Adobe FiveK dataset [2].
  Figure 7 Link: articels_figures_by_rev_year\2021\CIE_XYZ_Net_Unprocessing_Images_for_LowLevel_Computer_Vision_Tasks\figure_7.jpg
  Figure 7 caption: Qualitative results for motion deblurring application. (A) The
    blurred input image and the corresponding motion blur kernel. (B-D) Deblurring
    results in sRGB, standard XYZ, and our proposed CIE XYZ color space, respectively.
    The shown deblurring results are obtained by the non-blind image deblurring algorithm
    of Krishnan and Fergus [30]. (E) The ground-truth sharp image. Images are taken
    from the MIT-Adobe FiveK dataset [2], and the motion blur kernels are the four
    kernels used in the benchmark motion blur dataset of Lai et al. [31].
  Figure 8 Link: articels_figures_by_rev_year\2021\CIE_XYZ_Net_Unprocessing_Images_for_LowLevel_Computer_Vision_Tasks\figure_8.jpg
  Figure 8 caption: 'The precision-recall (PR) comparison of training light UNet on
    data from three different color spaces: sRGB, standard linearized CIE XYZ [22],
    [23], and our linearized CIE XYZ. The average accuracy for each model is shown
    in the plots legend. Our linear space achieves the best PR curve and the highest
    accuracy.'
  Figure 9 Link: articels_figures_by_rev_year\2021\CIE_XYZ_Net_Unprocessing_Images_for_LowLevel_Computer_Vision_Tasks\figure_9.jpg
  Figure 9 caption: Qualitative results of three light UNets trained on three different
    color spaces for the task of defocus map estimation. Training the light UNet using
    our linearized CIE XYZ images gives better visual results as shown in the third
    column. The CIE XYZ input images are gamma corrected for better visualization.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Mahmoud Afifi
  Name of the last author: Michael S. Brown
  Number of Figures: 15
  Number of Tables: 5
  Number of authors: 5
  Paper title: 'CIE XYZ Net: Unprocessing Images for Low-Level Computer Vision Tasks'
  Publication Date: 2021-04-02 00:00:00
  Table 1 caption: "TABLE 1 Results (in Terms of PSNR) of Camera-Rendered sRGB \u2194\
    \ \u2194 CIE XYZ Mapping"
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Effect of Different Polynomial Terms on Global Mapping
    Results (in Terms of PSNR) for Mapping Camera-Rendered sRGB to CIE XYZ Mapping;
    and Mapping From Both Reconstructed (Rec.) CIE XYZ Images and Ground Truth (GT)
    CIE XYZ Images to the Corresponding Camera-Rendered sRGB Images
  Table 3 caption: TABLE 3 Results of Image Denoising Performed on the SIDD-Validation
    Set [13] After Unprocessing the Images Using Our Method Compared to Standard Linearization
    [22], [23]
  Table 4 caption: TABLE 4 Angular Error of Illuminant Estimating Using the Image
    Set Captured by the Canon EOS-1Ds Mark III in the NUS Dataset [39]
  Table 5 caption: TABLE 5 Quantitative Results of the Photo-Finishing Enhancement
    Application Using 500 Under-Exposure Images Provided in [46]
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3070580
- Affiliation of the first author: school of computer science and technology, college
    of intelligence and computing, tianjin university, tianjin, china
  Affiliation of the last author: department of computer science and engineering,
    university of south carolina, columbia, sc, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\Multiple_Human_Association_and_Tracking_From_Egocentric_and_Complementary_Top_Vi\figure_1.jpg
  Figure 1 caption: 'An illustration of the proposed complementary-view subject association
    and collaborative tracking. (a) Mobile-camera network that produces both top-
    and horizontal-view videos. Here the top-view video is taken by a camera mounted
    to a drone from a high altitude with view direction roughly vertical to the ground
    and the horizontal-view videos are taken by GoPro worn by wearers among the subjects
    on the ground. (b) and (c) illustrate a sample cross-view subject association:
    identical-color boxes indicate the same subject across the top view (b) and the
    horizontal view (c). The location of horizontal-view camera that produces (c)
    is indicated by a blue box in (b). (d) and (e) illustrate the trajectories of
    two subjects in top and horizontal views, respectively.'
  Figure 10 Link: articels_figures_by_rev_year\2021\Multiple_Human_Association_and_Tracking_From_Egocentric_and_Complementary_Top_Vi\figure_10.jpg
  Figure 10 caption: Case analysis of long-term occlusion (top) and out-of-view (bottom)
    scenarios.
  Figure 2 Link: articels_figures_by_rev_year\2021\Multiple_Human_Association_and_Tracking_From_Egocentric_and_Complementary_Top_Vi\figure_2.jpg
  Figure 2 caption: An illustration of subject associations between consecutive clips
    and across the two views. Solid triangle in each cluster represents its dummy
    node.
  Figure 3 Link: articels_figures_by_rev_year\2021\Multiple_Human_Association_and_Tracking_From_Egocentric_and_Complementary_Top_Vi\figure_3.jpg
  Figure 3 caption: An illustration of the vector representation of subjects in (a)
    top view and (b) horizontal view.
  Figure 4 Link: articels_figures_by_rev_year\2021\Multiple_Human_Association_and_Tracking_From_Egocentric_and_Complementary_Top_Vi\figure_4.jpg
  Figure 4 caption: An illustration of mutual occlusion in the horizontal view (b)
    and its corresponding top view (a). (c) Damping function used for reducing searching
    space.
  Figure 5 Link: articels_figures_by_rev_year\2021\Multiple_Human_Association_and_Tracking_From_Egocentric_and_Complementary_Top_Vi\figure_5.jpg
  Figure 5 caption: An illustration of the proposed temporal synchronization strategy.
  Figure 6 Link: articels_figures_by_rev_year\2021\Multiple_Human_Association_and_Tracking_From_Egocentric_and_Complementary_Top_Vi\figure_6.jpg
  Figure 6 caption: Siamese neural network structure for measuring the cross-view
    appearance similarity.
  Figure 7 Link: articels_figures_by_rev_year\2021\Multiple_Human_Association_and_Tracking_From_Egocentric_and_Complementary_Top_Vi\figure_7.jpg
  Figure 7 caption: 'Illustration of association results. Row 1:Two sample results
    with large number of unshared subjects between two views. Row 2: Two sample results
    on image pairs with occlusions. Vector sets mathbf Vmathop mathrm topnolimits
    and mathbf Vmathop mathrm hornolimits are shown in the top-right corner of every
    image. The subjects with black 0 label are visible in the top view but invisible
    in the horizontal view, and with white 0 are visible in the horizontal view but
    invisible in the top view.'
  Figure 8 Link: articels_figures_by_rev_year\2021\Multiple_Human_Association_and_Tracking_From_Egocentric_and_Complementary_Top_Vi\figure_8.jpg
  Figure 8 caption: Cross-view subject association results of CVIDF 1 score (a) and
    CVMA score (b) over time.
  Figure 9 Link: articels_figures_by_rev_year\2021\Multiple_Human_Association_and_Tracking_From_Egocentric_and_Complementary_Top_Vi\figure_9.jpg
  Figure 9 caption: ID consistency accuracy in top (a) and horizontal view (b).
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Ruize Han
  Name of the last author: Song Wang
  Number of Figures: 12
  Number of Tables: 14
  Number of authors: 5
  Paper title: Multiple Human Association and Tracking From Egocentric and Complementary
    Top Views
  Publication Date: 2021-04-02 00:00:00
  Table 1 caption: TABLE 1 A Comparison of Cross-View Association Results From Different
    Methods
  Table 10 caption: TABLE 10 Tracklet Extraction Results Using Different Features
    (%)
  Table 2 caption: TABLE 2 A Comparison of Cross-View Association Results Using Different
    Representations (%)
  Table 3 caption: TABLE 3 Complexity and Speed Analysis of the Cross-View Association
    Algorithm Using Different Searching Strategies
  Table 4 caption: TABLE 4 Cross-View Association Results Under Different Temporal
    Offsets (%)
  Table 5 caption: TABLE 5 Temporal Synchronization Results Under Different Initial
    Offsets (Frames)
  Table 6 caption: TABLE 6 A Comparison of Cross-View Association and Tracking Results
    (%) of Different Methods
  Table 7 caption: TABLE 7 A Comparison of Tracking Results (%) From Different Methods
    on the Subsets of Top-View Videos and Horizontal-View Videos, Respectively
  Table 8 caption: "TABLE 8 A Comparison of Association and Tracking Results (%) by\
    \ Varying Values of c 0 c0, w 1 w1, w 2 w2, \u03C3 \u03C3, \u03BB \u03BB, and\
    \ \u03C4 \u03C4"
  Table 9 caption: TABLE 9 A Comparison of Association and Tracking Results by Using
    Different Features
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3070562
- Affiliation of the first author: damtp, university of cambridge, cambridge, u.k.
  Affiliation of the last author: damtp, university of cambridge, cambridge, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2021\Unsupervised_Image_Restoration_Using_Partially_Linear_Denoisers\figure_1.jpg
  Figure 1 caption: Partially linear denoiser. The clean image x (top middle), the
    noise n (top right), and the noisy image y:=x+n (top left) are modeled as random
    variables. A denoiser R is decomposed as R(y)=g(x)+Ln+e with a function g(x) (can
    be nonlinear), a linear mapping L (can depend on x ), and a residual term e .
    If the random variable e is of small variance, then R is called a partially linear
    denoiser. Such denoisers can be learned from only noisy images.
  Figure 10 Link: articels_figures_by_rev_year\2021\Unsupervised_Image_Restoration_Using_Partially_Linear_Denoisers\figure_10.jpg
  Figure 10 caption: Denoising results of three real microscopy datasets (on 3 columns).
  Figure 2 Link: articels_figures_by_rev_year\2021\Unsupervised_Image_Restoration_Using_Partially_Linear_Denoisers\figure_2.jpg
  Figure 2 caption: "Reconstructions for constant image patches corrupted by Poisson\
    \ noise with parameter \u03BB=1 , 2 and 4 (i.e., the column (a)-(c) respectively).\
    \ Top row: samples of corrupted patches (the ground truth is x=0.5\u03BB ). Second\
    \ and third rows: the optimal reconstructions R 0 ( y ) plotted against g(x)+L\
    \ n (represented by the yellow dots, each of which is generated with a realization\
    \ of noise n ). The blue line represents e=0 , i.e., (g(x)+L n ,g(x)+L n ) ."
  Figure 3 Link: articels_figures_by_rev_year\2021\Unsupervised_Image_Restoration_Using_Partially_Linear_Denoisers\figure_3.jpg
  Figure 3 caption: "The partial linearity of three denoisers. Top row: denoised images\
    \ by TV method, BM3D, and DnCNN respectively. Bottom row: the values of [R(y)\u2212\
    g(x) ] i plotted against [L n ] i where i is the pixel indicated by the red dot\
    \ on the top row. Each plot contains 8000 dots associated with different realizations\
    \ of n ."
  Figure 4 Link: articels_figures_by_rev_year\2021\Unsupervised_Image_Restoration_Using_Partially_Linear_Denoisers\figure_4.jpg
  Figure 4 caption: "Denoised results for the image Boat (with Gaussian noise \u03C3\
    =25 ). The last two rows are enlarged views of the indicated regions."
  Figure 5 Link: articels_figures_by_rev_year\2021\Unsupervised_Image_Restoration_Using_Partially_Linear_Denoisers\figure_5.jpg
  Figure 5 caption: "The influence of inaccurate noise variance during training. Top\
    \ row: PSNR on the test set BSD68 [26]. The dotted horizontal lines indicate results\
    \ of Noise2Self [2]. Bottom row: the mean of \u27E8z,Lz\u27E9 of the learned denoisers."
  Figure 6 Link: articels_figures_by_rev_year\2021\Unsupervised_Image_Restoration_Using_Partially_Linear_Denoisers\figure_6.jpg
  Figure 6 caption: Quality comparison for different methods for Poisson noise ( lambda
    !=!30 ). The last two rows are enlarged views of the indicated regions.
  Figure 7 Link: articels_figures_by_rev_year\2021\Unsupervised_Image_Restoration_Using_Partially_Linear_Denoisers\figure_7.jpg
  Figure 7 caption: Residual terms of DnCNN, Noise2Self and our method. The error
    images displayed on the first row are computed by subtracting the ground truth
    from the denoised images. The number on the top right corner of the images is
    the mean square of pixel values.
  Figure 8 Link: articels_figures_by_rev_year\2021\Unsupervised_Image_Restoration_Using_Partially_Linear_Denoisers\figure_8.jpg
  Figure 8 caption: PSNR (in blue) and SSIM (in red) plotted against the partially
    linear constraint parameter gamma (better viewed in color). The horizontal lines
    represent results of the DnCNN and Noise2Self for comparison.
  Figure 9 Link: articels_figures_by_rev_year\2021\Unsupervised_Image_Restoration_Using_Partially_Linear_Denoisers\figure_9.jpg
  Figure 9 caption: The Robustness of the denoisers with respect to the noise levels.
  First author gender probability: 0.96
  Gender of the first author: male
  Gender of the last author: Not Available
  Last author gender probability: 0
  Name of the first author: Rihuan Ke
  Name of the last author: "Carola-Bibiane Sch\xF6nlieb"
  Number of Figures: 11
  Number of Tables: 6
  Number of authors: 2
  Paper title: Unsupervised Image Restoration Using Partially Linear Denoisers
  Publication Date: 2021-04-05 00:00:00
  Table 1 caption: TABLE 1 The PSNR for R R (first row), the Variance of e e Averaged
    Over All Pixels (second row), and the PSNR for Modified Denoisers R :=g(x)+L n
    R:=g(x)+Ln (third row) on the Image Parrot
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Denoising Quality, Measured by PSNR (dB) and SSIM, for
    BSD68 [26] Corrupted by Gaussian Noise
  Table 3 caption: TABLE 3 Denoising Quality (in dB) for the 12 Wildly Used Image
    [6] and Gaussian Noise
  Table 4 caption: TABLE 4 Denoising Quality, Measured by PSNR (dB) and SSIM, for
    BSD68 [26] Corrupted by Poisson Noise
  Table 5 caption: TABLE 5 Denoising Quality (in dB) for the 12 Wildly Used Images
    [6] and Poisson Noise
  Table 6 caption: TABLE 6 Deblurring Results
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3070382
- Affiliation of the first author: department of computer science and engineering,
    university of bologna, bologna, italy
  Affiliation of the last author: department of computer science and engineering,
    university of bologna, bologna, italy
  Figure 1 Link: articels_figures_by_rev_year\2021\On_the_Synergies_Between_Machine_Learning_and_Binocular_Stereo_for_Depth_Estimat\figure_1.jpg
  Figure 1 caption: Years of progress in the field of stereo vision and machine learning
    enable the estimation of depth maps of unprecedented quality from a) stereo or
    b) monocular images.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\On_the_Synergies_Between_Machine_Learning_and_Binocular_Stereo_for_Depth_Estimat\figure_2.jpg
  Figure 2 caption: Overview of the most popular stereo datasets in literature, with
    examples of reference images and associated ground truth disparity. a) KITTI 2015
    [9], b) Middlebury 2014 [8], c) ETH3D [10], d) Freiburg SceneFlow [11].
  Figure 3 Link: articels_figures_by_rev_year\2021\On_the_Synergies_Between_Machine_Learning_and_Binocular_Stereo_for_Depth_Estimat\figure_3.jpg
  Figure 3 caption: Evolution of stereo algorithms. From left, reference image from
    KITTI 2015, disparity maps by SGM [26], MC-CNN-acrt [27] and DispNetC [11]. Learned
    matching costs outperform traditional pipelines, while end-to-end models perform
    even better in challenging regions (e.g., cars).
  Figure 4 Link: articels_figures_by_rev_year\2021\On_the_Synergies_Between_Machine_Learning_and_Binocular_Stereo_for_Depth_Estimat\figure_4.jpg
  Figure 4 caption: Example of confidence estimation. From left to right, reference
    image from KITTI 2012 dataset, disparity map by MC-CNN-fst [27] raw algorithm
    and confidence estimation inferred by LGC-Net [95].
  Figure 5 Link: articels_figures_by_rev_year\2021\On_the_Synergies_Between_Machine_Learning_and_Binocular_Stereo_for_Depth_Estimat\figure_5.jpg
  Figure 5 caption: Effects of domain-shift. On a KITTI 2015 stereo pair (top), a
    GWC-Net [89] instance trained on synthetic images [11] produces poor results (middle)
    on the road and in reflective surfaces. A short fine-tuning on KITTI 2012 dramatically
    improves the results (bottom).
  Figure 6 Link: articels_figures_by_rev_year\2021\On_the_Synergies_Between_Machine_Learning_and_Binocular_Stereo_for_Depth_Estimat\figure_6.jpg
  Figure 6 caption: Evolution of stereo-supervised monocular depth estimation, showing
    results achieved through 2017 [94], 2018 [130] and 2019 [113].
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Matteo Poggi
  Name of the last author: Stefano Mattoccia
  Number of Figures: 6
  Number of Tables: 6
  Number of authors: 5
  Paper title: 'On the Synergies Between Machine Learning and Binocular Stereo for
    Depth Estimation From Images: A Survey'
  Publication Date: 2021-04-05 00:00:00
  Table 1 caption: TABLE 1 KITTI 2015 Leaderboard [9], Showing Methods Learning Stages
    of the Pipeline
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Middlebury 2014 [8] and ETH3D [10] Leaderboards, Showing
    Methods Learning Stages of the Pipeline
  Table 3 caption: TABLE 3 KITTI 2015 Leaderboard [9], Showing End-to-End Methods
  Table 4 caption: TABLE 4 Middlebury 2014 [8] and ETH3D [10] Leaderboards, Showing
    End-to-End Methods
  Table 5 caption: TABLE 5 Experimental Comparison of Confidence Estimators
  Table 6 caption: TABLE 6 Quantitative Evaluation on the KITTI Dataset [12], Eigen
    Test Split [131]
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3070917
