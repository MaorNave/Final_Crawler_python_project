- Affiliation of the first author: institute of artificial intelligence and robotics,
    xi'an jiaotong university, xi'an, shaanxi, china
  Affiliation of the last author: wormpex ai research, bellevue, wa, usa
  Figure 1 Link: articels_figures_by_rev_year\2023\ContextLoc_A_Unified_Context_Model_for_Temporal_Action_Localization\figure_1.jpg
  Figure 1 caption: Illustration of the local and global contexts. (a) Snippet-level
    local context, i.e., a number of video snippets, composes each proposal consists.
    A snippet is a small number of consecutive frames and serves as the basic unit
    of feature extraction. It is the snippets capturing the start and end times of
    the action that play an important role in action localization. (b) Video-level
    global context is important as it involves background and high-level activity
    information that can be critical to distinguish action categories of similar appearance
    and motion patterns.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\ContextLoc_A_Unified_Context_Model_for_Temporal_Action_Localization\figure_2.jpg
  Figure 2 caption: Multi-scale context. Short-term and long-term proposal features
    are obtained from features of short-term snippets and long-term snippets, respectively.
    Multi-scale proposal features denote the combination of short-term and long-term
    proposal features.
  Figure 3 Link: articels_figures_by_rev_year\2023\ContextLoc_A_Unified_Context_Model_for_Temporal_Action_Localization\figure_3.jpg
  Figure 3 caption: The pipeline of our ContextLoc++. The input is an untrimmed video
    consisting of non-overlapping snippets. Multi-level features are extracted from
    snippets, proposals, and the video. The proposal unit which is shown in the bottom
    right part of Fig. 4 enhances the representation of each proposal via the local,
    global, and multi-scale contexts. Finally, we perform action classification, completeness
    judgment, and temporal boundary refinement for each proposal through MLPs and
    fully-connected layers. ContextLoc++ processes both original proposals and extended
    proposals, and fuses their predictions.
  Figure 4 Link: articels_figures_by_rev_year\2023\ContextLoc_A_Unified_Context_Model_for_Temporal_Action_Localization\figure_4.jpg
  Figure 4 caption: 'Top: L-Net first performs spatio-temporal fusion of the snippet-level
    features. Then these snippet-level features enhance proposals as the local context.
    Bottom left: G-Net adapts to the global context. Bottom right: The proposal unit
    consists of L-Net, G-Net, and M-Net.'
  Figure 5 Link: articels_figures_by_rev_year\2023\ContextLoc_A_Unified_Context_Model_for_Temporal_Action_Localization\figure_5.jpg
  Figure 5 caption: 'Different methods of processing the extended proposals. To better
    highlight the original and extended proposals, we do not include other entities
    (e.g., snippets and the video) in this figure. Top: prior approaches treat an
    extended proposal as a single proposal and process it in a separate branch. Bottom:
    our new method treats the extended proposal as three proposals in a proposal unit.'
  Figure 6 Link: articels_figures_by_rev_year\2023\ContextLoc_A_Unified_Context_Model_for_Temporal_Action_Localization\figure_6.jpg
  Figure 6 caption: Ablation study on classification and localization. The action
    classification accuracy (Class%) and tIoU w.r.t. ground truth (tIoU%) after regression
    are reported on the THUMOS14 test set.
  Figure 7 Link: articels_figures_by_rev_year\2023\ContextLoc_A_Unified_Context_Model_for_Temporal_Action_Localization\figure_7.jpg
  Figure 7 caption: Attention between a proposal and snippets within it. We visualize
    the attention between proposal features and features of snippets within the corresponding
    proposal. The attention focuses on the action foreground and action boundaries
    rather than the action background so as to be beneficial to localization.
  Figure 8 Link: articels_figures_by_rev_year\2023\ContextLoc_A_Unified_Context_Model_for_Temporal_Action_Localization\figure_8.jpg
  Figure 8 caption: Qualitative results from the ground truth, ContextLoc++, and ContextLoc++
    without L-Net or G-Net are respectively illustrated using green, red, and orange
    bars.
  Figure 9 Link: articels_figures_by_rev_year\2023\ContextLoc_A_Unified_Context_Model_for_Temporal_Action_Localization\figure_9.jpg
  Figure 9 caption: "Failure cases of our method on the ActivityNet v1.3 validation\
    \ set. \u201CInstance\u201D denotes the actual action instances in the video which\
    \ are annotated as one action instance."
  First author gender probability: 0.61
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.95
  Name of the first author: Zixin Zhu
  Name of the last author: Gang Hua
  Number of Figures: 9
  Number of Tables: 16
  Number of authors: 5
  Paper title: 'ContextLoc++: A Unified Context Model for Temporal Action Localization'
  Publication Date: 2023-01-17 00:00:00
  Table 1 caption:
    table_text: TABLE I Ablation Study on the Inference Time Comparison (For Each
      Proposal) of P-Net (PGCN) and MLP
  Table 10 caption:
    table_text: TABLE X Ablation Study on the Temporal Multi-Scale Relations on the
      THUMOS14 Test Set
  Table 2 caption:
    table_text: TABLE II Ablation Study on the Effectiveness of L-Net, G-Net, and
      M-Net
  Table 3 caption:
    table_text: TABLE III Ablation Study on the Early Fusion for Different Gating
      Functions in L-Net
  Table 4 caption:
    table_text: TABLE IV Ablation Study on the Effectiveness of L-Net
  Table 5 caption:
    table_text: TABLE V Comparison Between Attention Mechanism and RoI-Pooling Based
      on MUSES [23]
  Table 6 caption:
    table_text: "TABLE VI Ablation Study on the Effectiveness of G-Net and M-Net on\
      \ the THUMOS14 Test Set. Due to the Features Dimension of RoI-Pooling, We Cannot\
      \ Add the single G-Net to MUSES [23]. The Results of G-Net are Reported With\
      \ the ( L-Net \u2020 L-Net\u2020)"
  Table 7 caption:
    table_text: "TABLE VII Ablation Study on the Individual and Combinatorial Effects\
      \ of Global Context Adaptation in G-Net (Denoted as \u201Cadapt.\u201D)"
  Table 8 caption:
    table_text: TABLE VIII Ablation Study on Different Methods of Processing Extended
      Proposals on the THUMOS14 Test Set (As Illustrated in Fig. 5)
  Table 9 caption:
    table_text: TABLE IX Ablation Study on the Multi-Scale Step Size on the THUMOS14
      Test Set
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3237597
- Affiliation of the first author: beijing key laboratory of mobile computing and
    pervasive device, institute of computing technology, chinese academy of sciences,
    beijing, china
  Affiliation of the last author: beijing key laboratory of mobile computing and pervasive
    device, institute of computing technology, chinese academy of sciences, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2023\SceneHGN_Hierarchical_Graph_Networks_for_D_Indoor_Scene_Generation_With_FineGrai\figure_1.jpg
  Figure 1 caption: 'Our deep generative model SceneHGN encodes the indoor scene across
    multiple conceptual levels: the room, functional regions, furniture objects, and
    even fine-grained object part geometry. We utilize edges, including our proposed
    hyper-edges to strengthen the relations between objects during decoding. This
    enables some interesting applications, such as room editing with part-level geometry
    and scene interpolation. Our approach allows the entire 3D room to be represented
    and synthesized. Based on this, we can achieve part geometry editing (at different
    scales) in the scene, such as rigid transformation in a functional region and
    non-rigid deformation at the part level. Meanwhile, our network is capable of
    capturing the smooth latent space near similar scenes for plausible scene interpolation.'
  Figure 10 Link: articels_figures_by_rev_year\2023\SceneHGN_Hierarchical_Graph_Networks_for_D_Indoor_Scene_Generation_With_FineGrai\figure_10.jpg
  Figure 10 caption: 'Comparison on Room Generation for correlation metric o4 : We
    show the comparison of generation results on correlation metric o4 of our method,
    Deep Priors (DP) [40], GRAINS [4], ATISS [43], and GT (training data). We display
    some selected histograms on Bed-Cabinet, Bed-NS (Nightstands), Table-Chair and
    Sofa-Table. From the results we can see that our generated results can captures
    distribution of objects related to other objects. .'
  Figure 2 Link: articels_figures_by_rev_year\2023\SceneHGN_Hierarchical_Graph_Networks_for_D_Indoor_Scene_Generation_With_FineGrai\figure_2.jpg
  Figure 2 caption: 'Hierarchical Scene Representation: Our scene hierarchy has four
    conceptual levels: the room root node, functional regions, objects, and object
    parts. To train the recursive autoencoder, we use an encoder network to summarize
    the features in a bottom-up fashion and a decoder network that reconstructs the
    scene hierarchy from the room root node to regions to objects and finally to object
    parts in a top-down manner. We also model the rich edge relationships at different
    levels in this process to enforce the validity of the generated scene structures.'
  Figure 3 Link: articels_figures_by_rev_year\2023\SceneHGN_Hierarchical_Graph_Networks_for_D_Indoor_Scene_Generation_With_FineGrai\figure_3.jpg
  Figure 3 caption: 'Functional Region Visualization: In the figure, a whole scene
    is divided into three functional regions including a Cabinet Region, a Dining
    Region, and a Living Region, which are highlighted in different colors. The separation
    is conducted by a spatial clustering algorithm DBSCAN [58], which is a density-based
    and non-parametric clustering algorithm, where the number of clusters is self-adaptive.
    We can see that an indoor scene can be divided reasonably.'
  Figure 4 Link: articels_figures_by_rev_year\2023\SceneHGN_Hierarchical_Graph_Networks_for_D_Indoor_Scene_Generation_With_FineGrai\figure_4.jpg
  Figure 4 caption: 'Two Types of Binary Edges between Objects: We illustrate the
    two types of binary edges at the object level of our hierarchy. In (a), we show
    a binary edge example of the first kind which is defined between the room wall
    and an object. It encourages the object to locate within the boundary of the room
    and align with the room boundary. In (b), another type of binary edge describes
    the spatial relationship between two objects. For example, any pair of the four
    chairs have rich symmetry relationships of different kinds.'
  Figure 5 Link: articels_figures_by_rev_year\2023\SceneHGN_Hierarchical_Graph_Networks_for_D_Indoor_Scene_Generation_With_FineGrai\figure_5.jpg
  Figure 5 caption: 'Illustration of Hyper-edges: We define two types of hyper-edges
    that exist across multiple objects: rotation and parallel. A rotation hyper-edge
    indicates that objects are rotated around a center, and a parallel hyper-edge
    indicates objects are placed collinearly.'
  Figure 6 Link: articels_figures_by_rev_year\2023\SceneHGN_Hierarchical_Graph_Networks_for_D_Indoor_Scene_Generation_With_FineGrai\figure_6.jpg
  Figure 6 caption: 'Floor VAE: We train a separate Variational AutoEncoder for encoding
    floor boundaries. Specifically, we register a 2D ring of vertices onto the input
    floor boundary map, and then calculate the ACAP feature [60] on the registered
    ring structure. Finally, the VAE maps the ACAP feature into a latent vector which
    will serve as a condition for scene generation.'
  Figure 7 Link: articels_figures_by_rev_year\2023\SceneHGN_Hierarchical_Graph_Networks_for_D_Indoor_Scene_Generation_With_FineGrai\figure_7.jpg
  Figure 7 caption: 'The gallery of shape generation results given the room boundary
    and top-retrieved rooms in the training set: To demonstrate the novelty of room
    generation, we show the top-5 nearest neighbors in the training set by shape retrieval
    according to the CD (Chamfer Distance) on the sampled point clouds (with 100,000
    points). Given the room boundary, we can see that our generated shapes are different
    from the top-5 retrieved rooms on the object layout and geometric details, which
    demonstrates the novelty of generated rooms. .'
  Figure 8 Link: articels_figures_by_rev_year\2023\SceneHGN_Hierarchical_Graph_Networks_for_D_Indoor_Scene_Generation_With_FineGrai\figure_8.jpg
  Figure 8 caption: 'Comparison on Room Generation: We show the comparison of generation
    results of our method, Deep Priors [40], GRAINS [4], and ATISS [43]. From the
    results we can see that our method captures the functional regions (or local gathering
    of furniture) better. For example, our method can successfully predict four same
    chairs surrounding a dining table, while the three baseline methods cannot. .'
  Figure 9 Link: articels_figures_by_rev_year\2023\SceneHGN_Hierarchical_Graph_Networks_for_D_Indoor_Scene_Generation_With_FineGrai\figure_9.jpg
  Figure 9 caption: 'Room Generation results: Given the room boundary, we can utilize
    our trained decoder to generate new rooms. Our network is able to take arbitrary
    room boundaries as input to generate object layouts and geometric details in a
    recursive manner. The figure shows 12 generated rooms (4 living rooms, 4 bedrooms,
    and 4 libraries). From the results, our network learns the continuous latent space
    successfully, which can capture the plausible part geometries and reasonable object
    layout that fits the room boundary simultaneously. .'
  First author gender probability: 0.64
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.66
  Name of the first author: Lin Gao
  Name of the last author: Jie Yang
  Number of Figures: 17
  Number of Tables: 3
  Number of authors: 6
  Paper title: 'SceneHGN: Hierarchical Graph Networks for 3D Indoor Scene Generation
    With Fine-Grained Geometry'
  Publication Date: 2023-01-17 00:00:00
  Table 1 caption:
    table_text: TABLE I Generation Comparison Metrics Between Methods
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Perceptual Study Results on 3D Scene Generation
  Table 3 caption:
    table_text: TABLE III Quantitative Scene Reconstruction Performance of the Ablation
      Studies
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3237577
- Affiliation of the first author: department of computer science, brandeis university,
    waltham, ma, usa
  Affiliation of the last author: department of electrical and computer engineering,
    northeastern university, boston, ma, usa
  Figure 1 Link: Not Available
  Figure 1 caption: Not Available
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.7
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Hongfu Liu
  Name of the last author: Yun Fu
  Number of Figures: Not Available
  Number of Tables: 7
  Number of authors: 4
  Paper title: Transforming Complex Problems Into K-Means Solutions
  Publication Date: 2023-01-17 00:00:00
  Table 1 caption:
    table_text: TABLE I Instances of Bregman Divergence and Point-to-Centroid Distance
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Six Applications With K-Means Solutions
  Table 3 caption:
    table_text: TABLE III The Contingency Matrix
  Table 4 caption:
    table_text: TABLE IV Sample KCC Utility Functions
  Table 5 caption:
    table_text: TABLE V Statistics of Data Sets
  Table 6 caption:
    table_text: TABLE VI Experimental Results of the K-Means Solutions and Other Competitive
      Methods in Different Tasks
  Table 7 caption:
    table_text: TABLE VII Execution Time of the K-Means Solutions and Other Competitive
      Methods in Seconds
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3237667
- Affiliation of the first author: school of computer science and technology, beijing
    institute of technology, beijing, china
  Affiliation of the last author: school of computer science and technology, beijing
    institute of technology, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2023\SePiCo_SemanticGuided_Pixel_Contrast_for_Domain_Adaptive_Semantic_Segmentation\figure_1.jpg
  Figure 1 caption: Results preview on two popular synthetic-to-real semantic segmentation
    tasks. Our method is shown in bold.
  Figure 10 Link: articels_figures_by_rev_year\2023\SePiCo_SemanticGuided_Pixel_Contrast_for_Domain_Adaptive_Semantic_Segmentation\figure_10.jpg
  Figure 10 caption: Comparison results of model transferability for different methods
    trained on GTAV to Cityscapes.
  Figure 2 Link: articels_figures_by_rev_year\2023\SePiCo_SemanticGuided_Pixel_Contrast_for_Domain_Adaptive_Semantic_Segmentation\figure_2.jpg
  Figure 2 caption: 'Illustration of the main idea. By contrastively matching a pixel
    query q to distinct semantics, features with the same semantic concepts are drawn
    closer while those with different ones are pushed apart across domains. We first
    explore (a) Centroid-aware pixel contrast including (a.1) global category prototypes
    simply computed on the entire source domain, which render the overall appearance
    of each category and (a.2) local category centroids of each class in a single
    source image, which are stored into a memory bank. Further, we develop (b) Distribution-aware
    pixel contrast: the distributions of each category on source features are depicted
    as class-specific holistic concepts to guide the semantic alignment.'
  Figure 3 Link: articels_figures_by_rev_year\2023\SePiCo_SemanticGuided_Pixel_Contrast_for_Domain_Adaptive_Semantic_Segmentation\figure_3.jpg
  Figure 3 caption: Framework overview. First, our basic framework is based on a teacher-student
    architecture and the teacher model provides source feature map barFs and target
    pseudo labels barYt . Second, we propose class-balanced cropping to frequently
    crop image patches with under-represented objects that balance performance across
    classes. And third, except for self training losses, mathcal Lce and mathcal Lssl
    , we contrastively enforce the pixel representations Fs, Ft towards centroid-aware
    or distribution-aware semantics using mathcal Lcl and mathcal Lreg . After training
    is completed, we throw away projection head Theta p and use encoder Theta e and
    head Theta c for segmentation task.
  Figure 4 Link: articels_figures_by_rev_year\2023\SePiCo_SemanticGuided_Pixel_Contrast_for_Domain_Adaptive_Semantic_Segmentation\figure_4.jpg
  Figure 4 caption: 'Qualitative results on Cityscapes (val). From left to right:
    target image, ground truth, the maps predicted by Source Only, DACS, ProDA and
    Ours (DistCL) are shown one by one. Our method shows a clear visual improvement.'
  Figure 5 Link: articels_figures_by_rev_year\2023\SePiCo_SemanticGuided_Pixel_Contrast_for_Domain_Adaptive_Semantic_Segmentation\figure_5.jpg
  Figure 5 caption: 'Qualitative results on Dark Zurich (val). From left to right:
    target image, Ground Truth, the maps predicted by Ours (ProtoCL), Ours (BankCL)
    and Ours (DistCL).'
  Figure 6 Link: articels_figures_by_rev_year\2023\SePiCo_SemanticGuided_Pixel_Contrast_for_Domain_Adaptive_Semantic_Segmentation\figure_6.jpg
  Figure 6 caption: Parameter sensitivity analysis for CBC.
  Figure 7 Link: articels_figures_by_rev_year\2023\SePiCo_SemanticGuided_Pixel_Contrast_for_Domain_Adaptive_Semantic_Segmentation\figure_7.jpg
  Figure 7 caption: Comparison results of per-class IoU for CBC and RCS.
  Figure 8 Link: articels_figures_by_rev_year\2023\SePiCo_SemanticGuided_Pixel_Contrast_for_Domain_Adaptive_Semantic_Segmentation\figure_8.jpg
  Figure 8 caption: Quantitative analysis of the discrimination of features. For each
    class, we show the values of pixel-wise discrimination distance (PDD) as defined
    in (18) on Cityscapes validation set. These comparison results are from 1) category
    adversarial learning methods, i.e, CLAN and FADA; 2) category centroid-based alignment
    methods i.e., SIM and CAG-UDA; 3) pixel contrast methods, i.e., Ours (ProtoCLBankCLDistCL),
    respectively. A high PDD suggests the pixel-wise representations of same category
    are clustered densely while the distance between different categories is relatively
    large.
  Figure 9 Link: articels_figures_by_rev_year\2023\SePiCo_SemanticGuided_Pixel_Contrast_for_Domain_Adaptive_Semantic_Segmentation\figure_9.jpg
  Figure 9 caption: t-SNE analysis of existing comparable alignment methods and our
    SePiCo. As seen, the proposed pixel contrast objectives (ProtoCLBankCLDistCL)
    beget well-structured embedding spaces. Please zoom in for details.
  First author gender probability: 0.8
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Binhui Xie
  Name of the last author: Guoren Wang
  Number of Figures: 10
  Number of Tables: 11
  Number of authors: 6
  Paper title: 'SePiCo: Semantic-Guided Pixel Contrast for Domain Adaptive Semantic
    Segmentation'
  Publication Date: 2023-01-17 00:00:00
  Table 1 caption:
    table_text: "TABLE I Comparison Results of GTAV \u2192 \u2192 Cityscapes"
  Table 10 caption:
    table_text: "TABLE X Parameter sensitivity on GTAV \u2192 \u2192 Cityscapes (G\
      \ \u2192 \u2192 C) and SYNTHIA \u2192 \u2192 Cityscapes (S \u2192 \u2192 C)\
      \ tasks"
  Table 2 caption:
    table_text: "TABLE II Comparison results of SYNTHIA \u2192 \u2192 Cityscapes"
  Table 3 caption:
    table_text: "TABLE III Comparison Results of Cityscapes \u2192 \u2192 Dark Zurich"
  Table 4 caption:
    table_text: TABLE IV Comparison Results Using Swin-B ViT [82] and SegF
  Table 5 caption:
    table_text: "TABLE V Comparison Results of Cityscapes \u2192 \u2192 Dark Zurich\
      \ Trained Models for Generalization on Two Unseen Target Domains: Nighttime\
      \ Driving and BDD100k-Night Test Sets"
  Table 6 caption:
    table_text: "TABLE VI Experiments Over Weather DA Object Detection: Cityscapes\
      \ \u2192 \u2192 Foggy Cityscapes Based on Faster R-CNN"
  Table 7 caption:
    table_text: "TABLE VII Ablation Study on GTAV \u2192 \u2192 Cityscapes. All Models\
      \ are Trained End-to-End in a Total of 40k Iterations"
  Table 8 caption:
    table_text: "TABLE VIII Effect of the Teacher Network and the Bank Size on GTAV\
      \ \u2192 \u2192 Cityscapes"
  Table 9 caption:
    table_text: "TABLE IX Ablation of Feature Selection in SePiCo Variants for GTAV\
      \ \u2192 \u2192 Cityscapes Based on DeepLab-V2"
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3237740
- Affiliation of the first author: department of computer science and technology,
    institute for artificial intelligence, state key laboratory of intelligent technology
    and systems, bnrist, tsinghua university, beijing, china
  Affiliation of the last author: department of computer science and technology, institute
    for artificial intelligence, state key laboratory of intelligent technology and
    systems, bnrist, tsinghua university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2023\Recognizing_Object_by_Components_With_Human_Prior_Knowledge_Enhances_Adversarial\figure_1.jpg
  Figure 1 caption: Comparison between ROCK and ROW models. Green arrows denote the
    recognition process of benign images, and red arrows denote the generation and
    recognition process of adversarial images. Given an image, no matter benign or
    adversarial, ROCK predicts all predefined parts represented by segmentation results,
    and chooses a category as the output using a judgment block.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Recognizing_Object_by_Components_With_Human_Prior_Knowledge_Enhances_Adversarial\figure_2.jpg
  Figure 2 caption: The pipeline of ROCK. Given an input benign or adversarial image,
    we first perform part segmentation for each category and then use a judgment block
    to evaluate each segmentation result. The category with the highest score (bird
    in this image) is chosen as the output.
  Figure 3 Link: articels_figures_by_rev_year\2023\Recognizing_Object_by_Components_With_Human_Prior_Knowledge_Enhances_Adversarial\figure_3.jpg
  Figure 3 caption: Illustration of knowledge matching. (a) A toy example of knowledge
    matching. The number in each lattice of T c indicates the part label at the corresponding
    location. First, the MCC is calculated for each part, and then these MCCs are
    judged by linkage rules. (b) Examples from Face-Body (upper), Pascal-Part-C (middle),
    and PartImageNet-C (lower). To the right of the images, colored blocks denote
    annotated object parts and lines between them denote predefined linkage rules
    abstracted from commonsense knowledge.
  Figure 4 Link: articels_figures_by_rev_year\2023\Recognizing_Object_by_Components_With_Human_Prior_Knowledge_Enhances_Adversarial\figure_4.jpg
  Figure 4 caption: "ROCK's part segmentation results on examples which ROCK can still\
    \ successfully identify after being attacked. The examples are chosen from Pascal-Part-C\
    \ and the attack method was PGD-40 ( \u03F5=1 )."
  Figure 5 Link: articels_figures_by_rev_year\2023\Recognizing_Object_by_Components_With_Human_Prior_Knowledge_Enhances_Adversarial\figure_5.jpg
  Figure 5 caption: Examples of pseudo part labels kept (left) and filtered out (right).
  Figure 6 Link: articels_figures_by_rev_year\2023\Recognizing_Object_by_Components_With_Human_Prior_Knowledge_Enhances_Adversarial\figure_6.jpg
  Figure 6 caption: Texture and shape cue conflict experiment. (a) Two cue conflict
    images (the third column) and their segmentation results by ROCK (the fourth column).
    The first two columns show the images that were used to create the cue conflict
    images, e.g., we stylize bus with horse's texture in the first row. (b) Classification
    results of the ROCK (orange) and ROW (blue) models on Pascal-Part-C. The top icons
    are the original categories (content images), while the bottom icons are the transferred
    categories (texture images). Compared with ROW models, ROCK biased more towards
    shape categories.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Xiao Li
  Name of the last author: Xiaolin Hu
  Number of Figures: 6
  Number of Tables: 10
  Number of authors: 5
  Paper title: Recognizing Object by Components With Human Prior Knowledge Enhances
    Adversarial Robustness of Deep Neural Networks
  Publication Date: 2023-01-18 00:00:00
  Table 1 caption:
    table_text: TABLE I Classification Accuracies (%) Under Transfer-Based Attacks.
      Adversarial Examples Were Generated on Source Models and Evaluated on Target
      Models
  Table 10 caption:
    table_text: TABLE X Classification Accuracies (%) of ROCK Pseudo Pseudo and Some
      Comparison Models Under PGD-40 Attack When the Models Were Trained on 45 Selected
      Categories on PartImageNet-C
  Table 2 caption:
    table_text: "TABLE II ROCK's Accuracies (%) Under Various Adaptive Attacks. All\
      \ Attacks Iterated 40 Steps Using PGD With Step Size \u03F58 \u03B58. We Highlight\
      \ the Lowest Accuracies of ROCK Under the Five Adaptive Attacks"
  Table 3 caption:
    table_text: TABLE III Classification Accuracies (%) of ROCK and ROW Models Under
      the White-Box Attacks and AutoAttack. The Lowest Accuracies of ROCK Under Five
      Adaptive Attacks are Reported Here
  Table 4 caption:
    table_text: "TABLE IV Classification Accuracies (%) Under the Query-Based Black-Box\
      \ Attack. We Set \u03C3 \u03C3 Used in SPSA and NES to be 0.001 and the Number\
      \ of Random Samples in Each Iteration Step to be 128. Both SPSA and NES Attacks\
      \ Iterated 40 Steps. Performing Them Needed to Query a Model 5,120 Times in\
      \ Total. RayS Used 5,000 Search Steps"
  Table 5 caption:
    table_text: TABLE V Classification Accuracies (%) of ROW Models and ROCK Combined
      With Four AT Methods. The Lowest Accuracies of ROCK Under Five Adaptive Attacks
      are Reported Here
  Table 6 caption:
    table_text: "TABLE VI Classification Accuracies (%) on 15 Types of Image Corruptions.\
      \ \u201CmCA\u201D Denotes the Mean Accuracy Averaged Over Different Types of\
      \ Corruptions. The First Block (Row 2-5) Lists the Results of Models With Standard\
      \ Training, and the Second Block (Row 6-10) Lists the Results of Models With\
      \ TRADES-AWP-EMA Training"
  Table 7 caption:
    table_text: "TABLE VII Classification Accuracies (%) of ROCK and its Incomplete\
      \ Counterparts Under PGD-40 Attack. \u201CP\u201D, \u201CK\u201D, and \u201C\
      W\u201D Denote \u201CPart\u201D, \u201CKnowledge\u201D, and \u201CWeight\u201D\
      , Respectively"
  Table 8 caption:
    table_text: TABLE VIII Classification Accuracies (%) of DeiT-Ti and ROCK With
      DeiT-Ti as Backbone (ROCK T T) Under PGD-40 Attack. DeiT-Ti was Pretrained on
      ImageNet-1K and Then Fine-Tuned on the Two Datasets for 20 Epochs
  Table 9 caption:
    table_text: TABLE IX Classification Accuracies (%) of ROCK Few Few and Comparison
      Models Under PGD-40 Attack on the Randomly Selected 50 Categories on PartImageNet-C.
      ROCK Full Full Denotes the Model Trained by all Part Labels
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3237935
- Affiliation of the first author: key laboratory of intelligent information processing,
    institute of computing technology, chinese academy of sciences, beijing, china
  Affiliation of the last author: key laboratory of intelligent information processing,
    institute of computing technology, chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2023\Large_Scale_Visual_Food_Recognition\figure_1.jpg
  Figure 1 caption: (a) some categories from Food2K and (b) an example for collecting
    images via labels and image based retrieval.
  Figure 10 Link: articels_figures_by_rev_year\2023\Large_Scale_Visual_Food_Recognition\figure_10.jpg
  Figure 10 caption: Visualization results of proposed progressive learning on some
    samples from Food2K. The original method means we use feature maps from last three
    layers of the network without PL for visualization.
  Figure 2 Link: articels_figures_by_rev_year\2023\Large_Scale_Visual_Food_Recognition\figure_2.jpg
  Figure 2 caption: Example annotations for Food2K. For each column, the blue dashed
    box denotes the qualified image for this category, while the red one is unqualified.
    Different types of unqualified ones are showed, e.g., the painting food, the occlusion
    of the main part, missed important ingredients, and more categories in one image.
  Figure 3 Link: articels_figures_by_rev_year\2023\Large_Scale_Visual_Food_Recognition\figure_3.jpg
  Figure 3 caption: The ontology of Food2K.
  Figure 4 Link: articels_figures_by_rev_year\2023\Large_Scale_Visual_Food_Recognition\figure_4.jpg
  Figure 4 caption: The distributions over each category in the Food2K.
  Figure 5 Link: articels_figures_by_rev_year\2023\Large_Scale_Visual_Food_Recognition\figure_5.jpg
  Figure 5 caption: (a) Datasets distributed across number of images and categories
    and (b) distributions of categories on Food2K and typical datasets.
  Figure 6 Link: articels_figures_by_rev_year\2023\Large_Scale_Visual_Food_Recognition\figure_6.jpg
  Figure 6 caption: (a) Various visual appearances for one category and (b) one example
    with more fine-grained annotation in Food2K.
  Figure 7 Link: articels_figures_by_rev_year\2023\Large_Scale_Visual_Food_Recognition\figure_7.jpg
  Figure 7 caption: The framework of PRENet. (a) Global Feature Learning branch, which
    learns the global superclass features. (b) Progressive Local Feature Learning
    branch, which capture complementary multi-scale local features through progressive
    training strategy. (c) Region Feature Enhancement branch, which incorporate contexts
    into local features through self-attention. Note that the predicted scores by
    classifier A in (a) and B in (b) are combined for the final prediction.
  Figure 8 Link: articels_figures_by_rev_year\2023\Large_Scale_Visual_Food_Recognition\figure_8.jpg
  Figure 8 caption: Some classification results on Food2K. GT means the ground truth.
    The dishes with red color are not correctly classified in Top-1 results.
  Figure 9 Link: articels_figures_by_rev_year\2023\Large_Scale_Visual_Food_Recognition\figure_9.jpg
  Figure 9 caption: "Ablation study of PRENet on Food2K: (a) Different components.\
    \ (b) Different number of learning stages K . (c) Each learning stage. (d) Different\
    \ balance parameters ( \u03B1,\u03B2 )."
  First author gender probability: 0.58
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Weiqing Min
  Name of the last author: Shuqiang Jiang
  Number of Figures: 12
  Number of Tables: 8
  Number of authors: 8
  Paper title: Large Scale Visual Food Recognition
  Publication Date: 2023-01-18 00:00:00
  Table 1 caption:
    table_text: TABLE I Comparison of Current Food Recognition Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Comparison of Our Approach (PRENet) to Baselines on Food2K
      (%)
  Table 3 caption:
    table_text: TABLE III Performance Comparison on ETH Food-101 (%)
  Table 4 caption:
    table_text: TABLE IV Results of Transferring Visual Representations Learned on
      Food2K to Three Datasets (%)
  Table 5 caption:
    table_text: TABLE V Evaluating Visual Representation Learned From ETH Food101
      and Food2K for Retrieval on Three Datasets (%)
  Table 6 caption:
    table_text: TABLE VI Evaluating Visual Representation Learned From ETH Food-101
      and Food2K for Cross-Modal Recipe Retrieval on the Recipe1M Dataset (%)
  Table 7 caption:
    table_text: TABLE VII Evaluating Visual Representation Learned From ETH Food-101
      and Food2K for Food Detection on UNIMIB2016 and Oktoberfest(%)
  Table 8 caption:
    table_text: TABLE VIII Evaluating Visual Representation Learned From ETH Food-101
      and Food2K for Food Segmentation on the UEC-FoodPix Complete Dataset (%)
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3237871
- Affiliation of the first author: state key laboratory of information engineering
    in surveying, mapping and remote sensing, and institute of artificial intelligence,
    wuhan university, wuhan, hubei, china
  Affiliation of the last author: state key laboratory of information engineering
    in surveying, mapping and remote sensing, wuhan university, wuhan, hubei, china
  Figure 1 Link: articels_figures_by_rev_year\2023\Fully_Convolutional_Change_Detection_Framework_With_Generative_Adversarial_Netwo\figure_1.jpg
  Figure 1 caption: The framework of fully convolutional change detection with generative
    adversarial network.
  Figure 10 Link: articels_figures_by_rev_year\2023\Fully_Convolutional_Change_Detection_Framework_With_Generative_Adversarial_Netwo\figure_10.jpg
  Figure 10 caption: Change detection result of (a) SiamCRNNLSTM and (b) FCD-GN, where
    white indicates true detection, red indicates false alarms, and blue indicates
    omission error, as well as (c) the change probability output of FCD-GAN for HY
    dataset.
  Figure 2 Link: articels_figures_by_rev_year\2023\Fully_Convolutional_Change_Detection_Framework_With_Generative_Adversarial_Netwo\figure_2.jpg
  Figure 2 caption: The network structures of (a) segmentor, (b) generator, and (c)
    discriminator.
  Figure 3 Link: articels_figures_by_rev_year\2023\Fully_Convolutional_Change_Detection_Framework_With_Generative_Adversarial_Netwo\figure_3.jpg
  Figure 3 caption: The adversarial process of weakly supervised change detection.
  Figure 4 Link: articels_figures_by_rev_year\2023\Fully_Convolutional_Change_Detection_Framework_With_Generative_Adversarial_Netwo\figure_4.jpg
  Figure 4 caption: The adversarial process of regional supervised change detection.
  Figure 5 Link: articels_figures_by_rev_year\2023\Fully_Convolutional_Change_Detection_Framework_With_Generative_Adversarial_Netwo\figure_5.jpg
  Figure 5 caption: Wuhan multi-temporal GF-2 image dataset, (a) time-1 image, (b)
    time-2 image, and (c) reference, where red indicates changed pixels and green
    indicates unchanged pixels.
  Figure 6 Link: articels_figures_by_rev_year\2023\Fully_Convolutional_Change_Detection_Framework_With_Generative_Adversarial_Netwo\figure_6.jpg
  Figure 6 caption: Hanyang multi-temporal GF-2 image dataset, (a) time-1 image, (b)
    time-2 image, and (c) reference, where red indicates changed pixels and green
    indicates unchanged pixels.
  Figure 7 Link: articels_figures_by_rev_year\2023\Fully_Convolutional_Change_Detection_Framework_With_Generative_Adversarial_Netwo\figure_7.jpg
  Figure 7 caption: Multi-temporal ZY-3 image dataset, (a) time-1 image, (b) time-2
    image, and (c) reference, where white indicates changed pixels and black indicates
    unchanged pixels.
  Figure 8 Link: articels_figures_by_rev_year\2023\Fully_Convolutional_Change_Detection_Framework_With_Generative_Adversarial_Netwo\figure_8.jpg
  Figure 8 caption: Samples for weakly supervised change detection, including a changed
    pair (a) before change, (b) after change, (c) reference, and an unchanged pair
    (c) before change, (b) after change, (c) reference.
  Figure 9 Link: articels_figures_by_rev_year\2023\Fully_Convolutional_Change_Detection_Framework_With_Generative_Adversarial_Netwo\figure_9.jpg
  Figure 9 caption: A multi-temporal image pair covering Rio which are acquired on
    (a) April 24, 2016, (b) October 11, 2017, (c) pixel-level reference, and (d) region-level
    reference.
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.63
  Name of the first author: Chen Wu
  Name of the last author: Liangpei Zhang
  Number of Figures: 22
  Number of Tables: 7
  Number of authors: 3
  Paper title: Fully Convolutional Change Detection Framework With Generative Adversarial
    Network for Unsupervised, Weakly Supervised and Regional Supervised Change Detection
  Publication Date: 2023-01-18 00:00:00
  Table 1 caption:
    table_text: TABLE I Accuracy Assessment on WH Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Accuracy Assessment on HY Dataset
  Table 3 caption:
    table_text: TABLE III Accuracy Assessment on ZY Dataset
  Table 4 caption:
    table_text: TABLE IV Accuracy Assessment on BCD Dataset
  Table 5 caption:
    table_text: TABLE V Accuracy Assessment on OSCD Dataset
  Table 6 caption:
    table_text: TABLE VI Accuracy Assessment of l 1 l1 Weight
  Table 7 caption:
    table_text: TABLE VII Accuracy Assessment of Content Weight
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3237896
- Affiliation of the first author: national engineering laboratory for video technology,
    school of computer science, peking university, beijing, china
  Affiliation of the last author: department of computer science and engineering,
    university of south carolina, columbia, sc, usa
  Figure 1 Link: articels_figures_by_rev_year\2023\Capture_the_Moment_HighSpeed_Imaging_With_Spiking_Cameras_Through_ShortTerm_Plas\figure_1.jpg
  Figure 1 caption: Illustration of working mechanism for traditional cameras, event
    cameras, and spiking cameras. Traditional cameras acquire images according to
    the constant frame rate, event cameras generate asynchronous events for all pixels
    when brightness changes exceed a certain threshold, and spiking cameras continuously
    capture photons and generate asynchronous spike for all pixels when the accumulated
    intensity reaches a predefined threshold.
  Figure 10 Link: articels_figures_by_rev_year\2023\Capture_the_Moment_HighSpeed_Imaging_With_Spiking_Cameras_Through_ShortTerm_Plas\figure_10.jpg
  Figure 10 caption: "Illustration of dynamics of the STP models under different settings.\
    \ (a)\u2013(b) The number of spikes required for the convergence of R and u against\
    \ the change of the scale factor A . (c)\u2013(d) Relationship between the noise\
    \ after R and u convergent and the scale factor A . The light gray shaded area\
    \ represents those when inter-spike interval changes from \u0394 t 1 =20T to \u0394\
    \ t 2 =5T , while the dark shaded areas denote the cases from \u0394 t 2 =5T to\
    \ \u0394 t 1 =20T ."
  Figure 2 Link: articels_figures_by_rev_year\2023\Capture_the_Moment_HighSpeed_Imaging_With_Spiking_Cameras_Through_ShortTerm_Plas\figure_2.jpg
  Figure 2 caption: 'The postsynaptic potential (PSP) and postsynaptic current (PSC)
    generated by STP with received spike streams from a presynaptic neuron. Left:
    The short-term depression dominated model, the parameters are tau D=text750;textms,;
    tau F=text50;textms, ;U=0.45, ;C=0.3 . Right: The short-term facilitation dominate
    model, the parameters are tau D=text50;textms, ;tau F=text750;textms, ;U=0.15,
    ;C=0.15 .'
  Figure 3 Link: articels_figures_by_rev_year\2023\Capture_the_Moment_HighSpeed_Imaging_With_Spiking_Cameras_Through_ShortTerm_Plas\figure_3.jpg
  Figure 3 caption: Example results of the TFP [24] with different length of time
    window. Images in the blue dotted box are recovered with w=8 , and images in the
    red dotted are recovered with w=32 .
  Figure 4 Link: articels_figures_by_rev_year\2023\Capture_the_Moment_HighSpeed_Imaging_With_Spiking_Cameras_Through_ShortTerm_Plas\figure_4.jpg
  Figure 4 caption: (a) The dynamic of PSP regulated by STP. The dotted lines with
    different colors refer to spike trains with different frequencies, from 5Hz to
    30Hz . The short-term facilitation and depression model has a mixture of properties
    of both short-term depression and short-term facilitation. (b) The steady value
    of PSP, the number of neurotransmitters R , and the release probability u with
    respect to different input frequencies. The results are obtained with a short-term
    facilitation and depression model.
  Figure 5 Link: articels_figures_by_rev_year\2023\Capture_the_Moment_HighSpeed_Imaging_With_Spiking_Cameras_Through_ShortTerm_Plas\figure_5.jpg
  Figure 5 caption: Difference between STP dynamics of dark and bright area. (a) Spike
    raster and the corresponding STP dynamics of dark area (green circle in (b)).
    (b) Scenes reconstruction via Algorithm 1. (c) Spike raster and STP dynamics of
    bright area (red circle in (b)).
  Figure 6 Link: articels_figures_by_rev_year\2023\Capture_the_Moment_HighSpeed_Imaging_With_Spiking_Cameras_Through_ShortTerm_Plas\figure_6.jpg
  Figure 6 caption: Illustration of spike interval correction. In this example, as
    the true interval, 2.6T , is not a multiple of T , the spike intervals from the
    raw spike data will switch between 2T and 3T , which differ from the true spike
    interval. If we replace each interval with the average of five neighboring intervals,
    all intervals will be corrected to 2.6T .
  Figure 7 Link: articels_figures_by_rev_year\2023\Capture_the_Moment_HighSpeed_Imaging_With_Spiking_Cameras_Through_ShortTerm_Plas\figure_7.jpg
  Figure 7 caption: "The reconstruction images of the \u201Cviaduct-bridge\u201D dataset.\
    \ (a) reconstructed through TFI, without spike interval correction. (b) reconstructed\
    \ through TFI, with spike interval correction. (c) reconstructed through TFSTP,\
    \ without spike interval correction. (d) reconstructed through TFSTP, with spike\
    \ interval correction. To the right of each reconstructed image is the closeup."
  Figure 8 Link: articels_figures_by_rev_year\2023\Capture_the_Moment_HighSpeed_Imaging_With_Spiking_Cameras_Through_ShortTerm_Plas\figure_8.jpg
  Figure 8 caption: "The steady value of R (left) and u (right). Note that the steady\
    \ value of R is related to both \u03C4 D and \u03C4 F , so different STP type\
    \ leads to different steady value of R even if \u03C1 D is kept unchanged, which\
    \ contributes to the five different curves in the left subfigure. Nevertheless,\
    \ the steady value of u has no relation to \u03C4 D , hence the right subfigure\
    \ has only one curve."
  Figure 9 Link: articels_figures_by_rev_year\2023\Capture_the_Moment_HighSpeed_Imaging_With_Spiking_Cameras_Through_ShortTerm_Plas\figure_9.jpg
  Figure 9 caption: "A demonstration of our simulated data. In this figure, the spike\
    \ interval change from \u0394 t 1 =20T to \u0394 t 2 =5T at T 0 =0 , and change\
    \ from \u0394 t 2 =20T to \u0394 t 2 =5T at T 1 =400T . The red, blue, and green\
    \ lines represent the actual firing rate and those estimated from R and u of the\
    \ STP model, respectively."
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Yajing Zheng
  Name of the last author: Song Wang
  Number of Figures: 18
  Number of Tables: 3
  Number of authors: 5
  Paper title: 'Capture the Moment: High-Speed Imaging With Spiking Cameras Through
    Short-Term Plasticity'
  Publication Date: 2023-01-18 00:00:00
  Table 1 caption:
    table_text: "TABLE I Parameter Settings of the Short-Term Plasticity Models. T=25\u03BC\
      s T=25\u03BCs is the Temporal Resolution of Spiking Camera"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE II Comparison Among Different Reconstruction Methods. Downward\
      \ Arrow Denotes \u201Cthe Lower the Better,\u201D and Upward Arrow Denotes \u201C\
      the Higher the Better\u201D"
  Table 3 caption:
    table_text: TABLE III Referenced Quantitative Evaluation Using Simulated Data
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3237856
- Affiliation of the first author: institute of theoretical computer science, graz
    university of technology, graz, austria
  Affiliation of the last author: institute of theoretical computer science, graz
    university of technology, graz, austria
  Figure 1 Link: articels_figures_by_rev_year\2023\Restoring_Vision_in_Adverse_Weather_Conditions_With_PatchBased_Denoising_Diffusi\figure_1.jpg
  Figure 1 caption: An overview of the forward diffusion (dashed line) and reverse
    denoising (solid line) processes for a conditional diffusion model.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Restoring_Vision_in_Adverse_Weather_Conditions_With_PatchBased_Denoising_Diffusi\figure_2.jpg
  Figure 2 caption: (a) Illustration of the patch-based diffusive image restoration
    pipeline detailed in Algorithm 2. (b) Illustrating mean estimated noise guided
    sampling updates for overlapping pixels across patches. We demonstrate a simplified
    example where r=p2 , and there are only four overlapping patches sharing the grid
    cell marked with the white border and gratings. In this case, we would perform
    sampling updates for the pixels in this region based on the mean estimated noise
    over the four overlapping patches, at each denoising time step t .
  Figure 3 Link: articels_figures_by_rev_year\2023\Restoring_Vision_in_Adverse_Weather_Conditions_With_PatchBased_Denoising_Diffusi\figure_3.jpg
  Figure 3 caption: Quantitative comparisons in terms of PSNR and SSIM (higher is
    better) with state-of-the-art image desnowing and deraining methods. Above half
    of the tables show comparisons of our weather-specific SnowDiff p , RainHazeDiff
    p and RainDropDiff p models individually evaluated for each task. Bottom half
    of the tables show evaluations of our unified multi-weather model WeatherDiff
    p on all three test sets with respect to All-in-One [9] and TransWeather [7] multi-weather
    restoration methods. Best and second best values are indicated with bold text
    and underlined text respectively.
  Figure 4 Link: articels_figures_by_rev_year\2023\Restoring_Vision_in_Adverse_Weather_Conditions_With_PatchBased_Denoising_Diffusi\figure_4.jpg
  Figure 4 caption: Qualitative reconstruction comparisons of our best model on SnowTest100K
    test samples with DesnowNet [3] and DDMSNet [59].
  Figure 5 Link: articels_figures_by_rev_year\2023\Restoring_Vision_in_Adverse_Weather_Conditions_With_PatchBased_Denoising_Diffusi\figure_5.jpg
  Figure 5 caption: Qualitative reconstruction comparisons of our best model on Outdoor-Rain
    test samples with HRGAN [14] and MPRNet [3].
  Figure 6 Link: articels_figures_by_rev_year\2023\Restoring_Vision_in_Adverse_Weather_Conditions_With_PatchBased_Denoising_Diffusi\figure_6.jpg
  Figure 6 caption: Qualitative reconstruction comparisons of our best model on Raindrop
    test samples with RaindropAttn [55] and AttentiveGAN [12].
  Figure 7 Link: articels_figures_by_rev_year\2023\Restoring_Vision_in_Adverse_Weather_Conditions_With_PatchBased_Denoising_Diffusi\figure_7.jpg
  Figure 7 caption: Comparison of real-world snowy image restoration examples using
    TransWeather [7] and WeatherDiff 64 . In the above example TransWeather mistakenly
    removes the side view mirrors from both cars, however yields cleaner restorations
    than our method around the black car. In the below example our method obtains
    better removal of tiny snowflakes from images when viewed in detail.
  Figure 8 Link: articels_figures_by_rev_year\2023\Restoring_Vision_in_Adverse_Weather_Conditions_With_PatchBased_Denoising_Diffusi\figure_8.jpg
  Figure 8 caption: Comparison of real-world raindrop image restoration examples using
    TransWeather [7] and WeatherDiff 64 . Our method generates creative reconstructions
    in the above example with stones on the grass and sharper leaves on branches,
    whereas TransWeather smoothes out many details. In the below example, very bright
    raindrop artifacts could not be restored by TransWeather while our model recovers
    these.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "Ozan \xD6zdenizci"
  Name of the last author: Robert Legenstein
  Number of Figures: 8
  Number of Tables: 1
  Number of authors: 2
  Paper title: Restoring Vision in Adverse Weather Conditions With Patch-Based Denoising
    Diffusion Models
  Publication Date: 2023-01-19 00:00:00
  Table 1 caption:
    table_text: TABLE I Quantitative NIQE and IL-NIQE Score Comparisons on Real-World
      Image Datasets With Multi-Weather Restoration Models
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3238179
- Affiliation of the first author: college of science, national university of defense
    technology, changsha, china
  Affiliation of the last author: college of intelligence science and technology,
    national university of defense technology, changsha, china
  Figure 1 Link: articels_figures_by_rev_year\2023\Adaptive_Feature_Selection_With_Augmented_Attributes\figure_1.jpg
  Figure 1 caption: Illustration of incremental features in a dynamic environment.
    In the neuroimaging-based diagnosis of neuropsychiatric disorders, the current
    cortical thickness features (new type features) will accumulate to the previous
    sulcal depth features (old type features).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Adaptive_Feature_Selection_With_Augmented_Attributes\figure_2.jpg
  Figure 2 caption: Muti-shot AFS illustrations.
  Figure 3 Link: articels_figures_by_rev_year\2023\Adaptive_Feature_Selection_With_Augmented_Attributes\figure_3.jpg
  Figure 3 caption: 'Toy example. (a) and (f): The original three-dimensional samples
    in two aspects; (b) and (g): Projections on X-Y plane; (c) and (h): Projections
    on X-Z plane; (d) and (i): Projections on Y-Z plane; (e) and (j): Selected features
    of AFS.'
  Figure 4 Link: articels_figures_by_rev_year\2023\Adaptive_Feature_Selection_With_Augmented_Attributes\figure_4.jpg
  Figure 4 caption: ACC and Recall comparison with completion case.
  Figure 5 Link: articels_figures_by_rev_year\2023\Adaptive_Feature_Selection_With_Augmented_Attributes\figure_5.jpg
  Figure 5 caption: Influence of the ratios of previous features.
  Figure 6 Link: articels_figures_by_rev_year\2023\Adaptive_Feature_Selection_With_Augmented_Attributes\figure_6.jpg
  Figure 6 caption: "Sensitivity analysis with different \u03B1 and \u03B2 ."
  Figure 7 Link: articels_figures_by_rev_year\2023\Adaptive_Feature_Selection_With_Augmented_Attributes\figure_7.jpg
  Figure 7 caption: Convergence behavior.
  Figure 8 Link: articels_figures_by_rev_year\2023\Adaptive_Feature_Selection_With_Augmented_Attributes\figure_8.jpg
  Figure 8 caption: A figure illustration of three types of features in diagnostic
    clustering of schizophrenia.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.75
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Chenping Hou
  Name of the last author: Dewen Hu
  Number of Figures: 8
  Number of Tables: 6
  Number of authors: 4
  Paper title: Adaptive Feature Selection With Augmented Attributes
  Publication Date: 2023-01-19 00:00:00
  Table 1 caption:
    table_text: TABLE I A Brief Feature Description of Nine Real Data Sets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Acc Comparison With Different RAs
  Table 3 caption:
    table_text: TABLE III Recall Comparison With Different RAs
  Table 4 caption:
    table_text: TABLE IV Acc and Recall Comparison With Different RAs in Multi-Shot
      Clustering
  Table 5 caption:
    table_text: TABLE V Average Runtime Comparison (Seconds) on Two Representative
      Datasets
  Table 6 caption:
    table_text: TABLE VI Acc Comparison With Different RAs in Real Datasets
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3238011
- Affiliation of the first author: state key laboratory of virtual reality technology
    and systems, beihang university, beijing, china
  Affiliation of the last author: department of computer science, stony brook university,
    stony brook, ny, usa
  Figure 1 Link: articels_figures_by_rev_year\2023\Graph_Diffusion_Convolutional_Network_for_Skeleton_Based_Semantic_Recognition_of\figure_1.jpg
  Figure 1 caption: Illustration of our pipeline. The input of our network is two-person
    skeleton sequence graph. Graph diffusion is employed to create new connections
    and suppress useless edges according to different features and feature-specific
    similarity metrics, and carries on message propagation to obtain a new graph.
    Dynamic TCN is a dynamic temporal convolution module to calculate the importance
    of different frames. The first three layers of the network follow ST-GCN [4],
    the fourth and fifth layers guide the network learning through diffusion-guided
    GCN and dynamic TCN, the last five layers still follow ST-GCN [4], and finally
    the extracted features are sent to the standard classifier for classification.
  Figure 10 Link: articels_figures_by_rev_year\2023\Graph_Diffusion_Convolutional_Network_for_Skeleton_Based_Semantic_Recognition_of\figure_10.jpg
  Figure 10 caption: Since the joints of human body move according to the components,
    the joints are divided into body parts, and the representative joints are selected
    to establish a connection for the two-person skeleton structure involved in diffusion.
    Grey circles indicate the divisions of body parts, and red dots represent the
    joints for each part.
  Figure 2 Link: articels_figures_by_rev_year\2023\Graph_Diffusion_Convolutional_Network_for_Skeleton_Based_Semantic_Recognition_of\figure_2.jpg
  Figure 2 caption: Illustration of the two-person interactive action graph used in
    our graph diffusion convolutional network.
  Figure 3 Link: articels_figures_by_rev_year\2023\Graph_Diffusion_Convolutional_Network_for_Skeleton_Based_Semantic_Recognition_of\figure_3.jpg
  Figure 3 caption: Illustration of the decomposition process of two-person interactive
    skeleton model, the joint and bone are extracted separately.
  Figure 4 Link: articels_figures_by_rev_year\2023\Graph_Diffusion_Convolutional_Network_for_Skeleton_Based_Semantic_Recognition_of\figure_4.jpg
  Figure 4 caption: Illustration of spatial configuration partitioning. The nodes
    are classified according to their distances to the skeleton gravity center (blue
    cross) compared with that of the root node (green). Centripetal nodes have shorter
    distances (blue), while centrifugal nodes have longer distances (yellow) than
    the root node.
  Figure 5 Link: articels_figures_by_rev_year\2023\Graph_Diffusion_Convolutional_Network_for_Skeleton_Based_Semantic_Recognition_of\figure_5.jpg
  Figure 5 caption: Illustration of the diffusion results. This figure shows the relational
    matrix obtained by computing the similarity and diffusion on whole skeleton which
    creates new connections beyond direct neighbors between different bodies or remote
    joint.
  Figure 6 Link: articels_figures_by_rev_year\2023\Graph_Diffusion_Convolutional_Network_for_Skeleton_Based_Semantic_Recognition_of\figure_6.jpg
  Figure 6 caption: The skeleton on the left shows the skeleton structure and joint
    label of Kinetics skeleton dataset, the middle one shows the skeleton structure
    and joint label of NTU-RGB+D dataset, and the right shows that of SBU-Interaction
    dataset.
  Figure 7 Link: articels_figures_by_rev_year\2023\Graph_Diffusion_Convolutional_Network_for_Skeleton_Based_Semantic_Recognition_of\figure_7.jpg
  Figure 7 caption: The statistical results of node activity on NTU-26 dataset. The
    darker the color is, the more active the node is. Joint 1 and joint 26 are the
    least active node. It's worth noted that our statistics are for individuals, because
    the initiator of the action could be Volunteer No. 1 or Volunteer No. 2.
  Figure 8 Link: articels_figures_by_rev_year\2023\Graph_Diffusion_Convolutional_Network_for_Skeleton_Based_Semantic_Recognition_of\figure_8.jpg
  Figure 8 caption: 'First row: three levels of the graph structure adopted by ST-GCN;
    second row: three levels of the graph structure of our method. The difference
    is that, the number of joints is double and the position of gravity center is
    changed according to statistics.'
  Figure 9 Link: articels_figures_by_rev_year\2023\Graph_Diffusion_Convolutional_Network_for_Skeleton_Based_Semantic_Recognition_of\figure_9.jpg
  Figure 9 caption: This figure illustrates two kinds of graph structures adopted
    in our method, one is the prior skeleton graph derived from human body's natural
    connectivity, and the other is the graph structure dynamically learned through
    graph diffusion.
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.61
  Name of the first author: Shuai Li
  Name of the last author: Hong Qin
  Number of Figures: 19
  Number of Tables: 11
  Number of authors: 5
  Paper title: Graph Diffusion Convolutional Network for Skeleton Based Semantic Recognition
    of Two-Person Actions
  Publication Date: 2023-01-20 00:00:00
  Table 1 caption:
    table_text: TABLE I Accuracy Comparison Between the Baseline and Our Improvement
      on NTU-25
  Table 10 caption:
    table_text: TABLE X Classification Performance on NTU-60, NTU-120 and Kinetics-400
  Table 2 caption:
    table_text: TABLE II Accuracy Comparison Among Three Diffusion Paths on NTU-25
      X-Set
  Table 3 caption:
    table_text: TABLE III Accuracy Comparison of Temporal Dynamic Calculation Module
      on NTU-25 X-Set
  Table 4 caption:
    table_text: "TABLE IV Key Events Description of \u201CExchange Object\u201D Sample"
  Table 5 caption:
    table_text: "TABLE V Classification Results of \u201CExchange Object\u201D Sample\
      \ on Four Trained Models"
  Table 6 caption:
    table_text: TABLE VI Accuracy Comparison of Different Similarity Metrics on NTU-26
      X-Set
  Table 7 caption:
    table_text: TABLE VII Accuracy Comparison of Different Network Structure on NTU-26
      X-Set
  Table 8 caption:
    table_text: TABLE VIII Classification Performance on NTU-26
  Table 9 caption:
    table_text: TABLE IX Classification Performance on Kinetics-39
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3238411
- Affiliation of the first author: school of computer science and technology, nanjing
    university of aeronautics and astronautics, nanjing, jiangsu, china
  Affiliation of the last author: hong kong polytechnic university, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2023\AGConv_Adaptive_Graph_Convolution_on_D_Point_Clouds\figure_1.jpg
  Figure 1 caption: Illustration of adaptive kernels and fixed kernels in the convolution.
    (a) The standard graph convolution applies a fixedisotropic kernel (black arrow)
    to compute features for each point indistinguishably. (b) Based on these features
    in (a), several attentional weights ai are assigned to determine their importance.
    (c) Differently, AGConv generates an adaptive kernel hatei that is unique to the
    learned features of each point.
  Figure 10 Link: articels_figures_by_rev_year\2023\AGConv_Adaptive_Graph_Convolution_on_D_Point_Clouds\figure_10.jpg
  Figure 10 caption: Visualize the euclidean distances between the target point (green
    point in (a)) and other points in the feature space. Red color denotes a closer
    point and yellow one is far from the target. The feature distances in several
    layers provide a clear insight that our network can distinguish points belonging
    to different semantic parts. It also captures non-local similar structures (see
    the handles in the second row).
  Figure 2 Link: articels_figures_by_rev_year\2023\AGConv_Adaptive_Graph_Convolution_on_D_Point_Clouds\figure_2.jpg
  Figure 2 caption: The illustration of AGConv processed in the neighborhood of a
    target point xi . An adaptive kernel hateijm is generated from the feature input
    Delta fij of a pair of points on the edge, which is then convolved with the corresponding
    spatial input Delta xij . Concatenating hijm of all dimensions yields the edge
    feature hij . Finally, the output feature fiprime of the central point is obtained
    through a pooling function. AGConv differs from the other graph convolutions in
    that the convolution kernel is unique for each pair of points.
  Figure 3 Link: articels_figures_by_rev_year\2023\AGConv_Adaptive_Graph_Convolution_on_D_Point_Clouds\figure_3.jpg
  Figure 3 caption: AGConv for classification and segmentation. GraphConv denotes
    our standard convolution without an adaptive kernel. The segmentation model uses
    pooling and interpolating to build a hierarchical graph structure, while the classification
    model applies a dynamic structure [10]. The subsampled features are resized to
    the same resolution as the input points by interpolation.
  Figure 4 Link: articels_figures_by_rev_year\2023\AGConv_Adaptive_Graph_Convolution_on_D_Point_Clouds\figure_4.jpg
  Figure 4 caption: "Kernel function in our adaptive convolution. We apply a two-layer\
    \ MLP for the adaptive weight matrix. The output edge feature is obtained by matrix\
    \ multiplication between \u0394 x ij and the weight matrix. Optional ResNet block:\
    \ shortcut 1\xD71 convolution and batch normalization layer."
  Figure 5 Link: articels_figures_by_rev_year\2023\AGConv_Adaptive_Graph_Convolution_on_D_Point_Clouds\figure_5.jpg
  Figure 5 caption: Visualization of semantic segmentation results on S3DIS. We show
    the input point cloud, and labelled points mapped to RGB colors.
  Figure 6 Link: articels_figures_by_rev_year\2023\AGConv_Adaptive_Graph_Convolution_on_D_Point_Clouds\figure_6.jpg
  Figure 6 caption: Visualization of semantic segmentation results on NPM3D.
  Figure 7 Link: articels_figures_by_rev_year\2023\AGConv_Adaptive_Graph_Convolution_on_D_Point_Clouds\figure_7.jpg
  Figure 7 caption: Robustness test on ModelNet40 for classification. GraphConv indicates
    the standard graph convolution network. Attention Point and Attention Channel
    indicate the ablations where we replace the AGConv layers with graph attention
    layers (point- and channel-wise), respectively. Our model is more robust to point
    density and noise perturbation.
  Figure 8 Link: articels_figures_by_rev_year\2023\AGConv_Adaptive_Graph_Convolution_on_D_Point_Clouds\figure_8.jpg
  Figure 8 caption: Segmentation results on ShapeNet. The labelled points are visualized
    in different colors. We compare our adaptive graph convolution with DGCNN [10]
    (standard graph convolution) and attentional convolution network (Attention Point).
    Our method produces better results especially for points close to the object boundaries
    and edges.
  Figure 9 Link: articels_figures_by_rev_year\2023\AGConv_Adaptive_Graph_Convolution_on_D_Point_Clouds\figure_9.jpg
  Figure 9 caption: 'Visualize the euclidean distances between two points (blue and
    green stars) and other points in the feature space (red: near, yellow: far).'
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.76
  Name of the first author: Mingqiang Wei
  Name of the last author: Jing Qin
  Number of Figures: 20
  Number of Tables: 15
  Number of authors: 12
  Paper title: 'AGConv: Adaptive Graph Convolution on 3D Point Clouds'
  Publication Date: 2023-01-20 00:00:00
  Table 1 caption:
    table_text: TABLE I Segmentation Results on the ShapeNetPart Dataset. The Model
      Using STN Achieves Better Results
  Table 10 caption:
    table_text: TABLE X Number of Parameters, Forward Pass Time (Per Batch) and Overall
      Accuracy for Different Models Using AGConv
  Table 2 caption:
    table_text: TABLE II Classification Results on ModelNet40. Our Network Achieves
      the Best Results According to Both mAcc and OA
  Table 3 caption:
    table_text: TABLE III Part Segmentation Results on ShapeNetPart Evaluated by the
      Mean Class IoU (mcIoU) and Mean Instance IoU (mIoU)
  Table 4 caption:
    table_text: TABLE IV Ablation Studies on ShapeNetPart for Part Segmentation
  Table 5 caption:
    table_text: TABLE V Results of Ablation Networks on ModelNet40
  Table 6 caption:
    table_text: TABLE VI Semantic Segmentation Results on S3DIS Evaluated on Area
      5. We Report the Mean Classwise IoU (mIoU), Mean Classwise Accuracy (mAcc) and
      Overall Accuracy (OA). IoU of Each Class is Also Provided
  Table 7 caption:
    table_text: TABLE VII Semantic Segmentation Results on NPM3D. We Report the Mean
      Classwise IoU (mIoU) and IoU of Each Class
  Table 8 caption:
    table_text: TABLE VIII Our Classification Network With Different k k of the Nearest
      Neighbors
  Table 9 caption:
    table_text: TABLE IX The Number of Parameters, Floating Point Operations and Overall
      Accuracy of Different Models
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3238516
- Affiliation of the first author: school of computer science and engineering, sun
    yat-sen university, guangzhou, guangdong, china
  Affiliation of the last author: king abdullah university of science and technology,
    thuwal, saudi arabia
  Figure 1 Link: articels_figures_by_rev_year\2023\KnowledgeAware_Global_Reasoning_for_Situation_Recognition\figure_1.jpg
  Figure 1 caption: "(a) The example for situation recognition task, which aims to\
    \ predict the salient action (e.g., spraying) and associated nouns (e.g., crop\
    \ and farm) at the same time. (b) The previous methods [1] model local correlation\
    \ of nouns in a single image without any global supervision. (c) Humans can still\
    \ recognize the tiny object \u201Csalt\u201D and location \u201Ckitchen\u201D\
    \ in the \u201CPouring\u201D scene. This is because (1) this object looks similar\
    \ to the \u201Csalt\u201D as we saw before in the \u201CCooking\u201D situation;\
    \ (2) the \u201Csalt\u201D often appears in the \u201Ckitchen\u201D in the \u201C\
    Cooking\u201D situation. Thus, it inspires our design of Global Knowledge Pool.\
    \ Such rich human experience can be represented in a knowledge graph and guide\
    \ our recognition pipeline. Our proposed global reasoning is supervised by the\
    \ global knowledge pool which is constructed by noun-wise correlation statistic\
    \ on the dataset."
  Figure 10 Link: articels_figures_by_rev_year\2023\KnowledgeAware_Global_Reasoning_for_Situation_Recognition\figure_10.jpg
  Figure 10 caption: Visualization of the learned graph between the baseline (blue
    matrix) and our method (orange matrix). We compare different graphs to show the
    superior performance of our action-guided pairwise graph generated by our ELM.
    Our method can predict more precise noun values (e.g., Man versus Teacher, Man
    versus Groom, Man versus Chief, Man versus Ballplayer).
  Figure 2 Link: articels_figures_by_rev_year\2023\KnowledgeAware_Global_Reasoning_for_Situation_Recognition\figure_2.jpg
  Figure 2 caption: Overview of our edge learning module (ELM) to solve the ambiguous
    noun by querying knowledge from the global knowledge pool. Although ambiguous
    noun feature tends to predict wrong result (e.g., sand), our ELM can correct it
    by using related relationship (salt, bowl and kitchen).
  Figure 3 Link: articels_figures_by_rev_year\2023\KnowledgeAware_Global_Reasoning_for_Situation_Recognition\figure_3.jpg
  Figure 3 caption: "(a) Overview of our knowledge-aware global reasoning (KGR) framework.\
    \ (b) The processing progress of the normal pairwise knowledge and our action-guided\
    \ pairwise knowledge. Our KGR framework consists of a local encoder and a global\
    \ encoder. We adopt the GGNN [1] as our local encoder to extract the noun features.\
    \ The global encoder aims to enhance the noun features via global reasoning, which\
    \ is composed of an edge learning module (ELM) and a node evolution module. The\
    \ ELM is optimized by an objective function to calculate the loss of the noun\
    \ relations between the global knowledge pool and the generated soft edges from\
    \ the ELM. To construct the global knowledge pool from the dataset, we design\
    \ an action-guided pairwise knowledge, which can link every two nouns based on\
    \ their common actions. For instance, the nouns called \u201Cplayer,\u201D \u201C\
    man\u201D and \u201Cbaseball\u201D can be connected together in our action-guided\
    \ pairwise knowledge thanks to their common verb named \u201Cthrowing\u201D."
  Figure 4 Link: articels_figures_by_rev_year\2023\KnowledgeAware_Global_Reasoning_for_Situation_Recognition\figure_4.jpg
  Figure 4 caption: The 2D visualization of the noun-level feature by the t-SNE method
    [25]. The categories shared with the similar correlation are closed to each other.
    This verifies that our KGR can learn the corresponding knowledge.
  Figure 5 Link: articels_figures_by_rev_year\2023\KnowledgeAware_Global_Reasoning_for_Situation_Recognition\figure_5.jpg
  Figure 5 caption: Qualitative results between baseline and our KGR on Action40-105.
  Figure 6 Link: articels_figures_by_rev_year\2023\KnowledgeAware_Global_Reasoning_for_Situation_Recognition\figure_6.jpg
  Figure 6 caption: (a) The results of comparison between our KGR and the other approaches
    on the top 10 most frequent noun classes and the rest less frequent categories.
    (b) Comparison between our model and our baseline on the data of the extracted
    low-frequent noun classes. (c) Comparison between our KGR and the baseline on
    the data of high-frequent noun classes. Please zoom in the colored PDF version
    of this paper for more details.
  Figure 7 Link: articels_figures_by_rev_year\2023\KnowledgeAware_Global_Reasoning_for_Situation_Recognition\figure_7.jpg
  Figure 7 caption: Visualization comparison in long-tailed categories between baseline
    (GGNN) and our KGR.
  Figure 8 Link: articels_figures_by_rev_year\2023\KnowledgeAware_Global_Reasoning_for_Situation_Recognition\figure_8.jpg
  Figure 8 caption: (a) The distribution of the top 100 frequent noun classes on the
    imSitu dataset [2]. This distribution suffers from severe long-tailed problem
    due to a huge different number between the highest frequent classes and lowest
    frequent classes such as above 47,000 quantities. The long-tailed problem leads
    the model trained by a data-driven way with pure class-level supervision to wrongly
    predict rare classes as high-frequency classes. (b) The distribution of the top
    100 frequent noun relationships of action-aware graph on the imSitu dataset [2].
    Different from (a), the different frequency between each pair-wise relationship
    is very small (roughly 0.17). Hence, the rare classes can be benefited from more
    frequent ones by bridging structured pair-wise relation knowledge between classes.
  Figure 9 Link: articels_figures_by_rev_year\2023\KnowledgeAware_Global_Reasoning_for_Situation_Recognition\figure_9.jpg
  Figure 9 caption: The 2D visualization of noun-level feature by t-SNE method [25].
    (a) and (b) are the two examples for the output features of the node evolution
    module. The right regions are enlarged in left panels. The categories shared with
    the similar knowledge are closed to each other. This verifies that our knowledge
    variants learn the corresponding knowledge. Please zoom in the colored PDF version
    of this paper for more details.
  First author gender probability: 0.53
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Weijiang Yu
  Name of the last author: Bernard Ghanem
  Number of Figures: 10
  Number of Tables: 6
  Number of authors: 5
  Paper title: Knowledge-Aware Global Reasoning for Situation Recognition
  Publication Date: 2023-01-23 00:00:00
  Table 1 caption:
    table_text: "TABLE I Quantitative Comparison in Three Metrics on the Development\
      \ and Test Sets Against the State-of-The-Art Dataset. Each Model Was Run on\
      \ the Test Set Only Once. \u2020 \u2020 Denotes the Results of Our Implementation\
      \ Using Official Code Strictly Following the Settings From the Papers. We Highlight\
      \ the Best Results in Bold. Note That the Results of Different Methods on Verb\
      \ are the Same Since the Same Verb Predictor are Used"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE II Ablation Study on the Development and Test Set. the KGR\
      \ wo L(f, W K ,K) L(f,WK,K) Means Training the KGR Without Global Knowledge\
      \ Supervision. The ELM Means the Edge Learning Module. We Replace the Learned\
      \ Edges With the Ground-Truth Edges, Which is Denoted as KGR w GT. KGR \u2020\
      \ \u2020 Means the Global Knowledge Pool of KGR Only Built From Training Data.\
      \ KGR Indicates Replacing ELM With Sub-Graph That is Queried From Knowledge\
      \ Graph Pool Built by Pure Train Data."
  Table 3 caption:
    table_text: TABLE III Ablation Study on Development and Test Set. The Upper Right
      Number Means the Improvement Benefited From the Graph. K A KA Means the Action-Guided
      Knowledge Graph
  Table 4 caption:
    table_text: TABLE IV Generalization of Our Knowledge Pool. We Test Our Model on
      Another Dataset Named Action40-105 While Keeping the Current Knowledge Pool
      Built From the Imsitu Dataset, Which Aims to Explore Transfer Ability of Our
      Knowledge Pool
  Table 5 caption:
    table_text: TABLE V Quantitative Comparison W.r.t top-1 Accuracy Metric of the
      Noun Classes With Different Frequencies. the top-10 Means the Samples of Top
      10 Most Frequent Categories of Nouns. The Rest Indicates the Samples of the
      Rest Less Frequent Categories. Similar Meanings for top-20 and top-50. The Average
      Classes Means the top-1 Accuracy for Each Noun Class
  Table 6 caption:
    table_text: TABLE VI Analysis of the Average Precision (P), Recall (R), Parameters
      and Flops. the rare-10 Means the Top 10 Rarest Categories of Nouns. Similar
      Meaning for rare-20, rare-50 and rare-100. The All Means the Results Over All
      Nouns. The GFLOPs is Computed by Using the Fvcore Tool
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3238699
- Affiliation of the first author: department of computer science and engineering,
    shanghai jiao tong university, shanghai, china
  Affiliation of the last author: department of computer science and engineering,
    shanghai jiao tong university, shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2023\SelfAdversarial_Disentangling_for_Specific_Domain_Adaptation\figure_1.jpg
  Figure 1 caption: (a) Previous UDA methods do not leverage explicit prior knowledge
    about the domain shifts on a demand-specific dimension to perform domain adaptation,
    and (b) they cannot generalize well to a target domain with different unseen domainness
    ( beta ). (c) Our key idea is to learn domainness-invariant representations for
    narrowing the intra-domain gap induced by different domainness. Different domainness
    indicate different numerical magnitudes of domain shifts in a specific domain
    dimension, e.g., fog thickness.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\SelfAdversarial_Disentangling_for_Specific_Domain_Adaptation\figure_2.jpg
  Figure 2 caption: "Overview of the proposed Self-Adversarial Disentangling (SAD)\
    \ framework for specific domain adaptation (SDA). Our Domainness Creator (DC)\
    \ not only generates a diversified source image with random domainness, but also\
    \ provides additional supervisory signals d \u2032 gt for guiding the feature\
    \ disentangling. The encoder E spf and E inv are to extract the domainness-specific\
    \ representations z spf and the domainness-invariant representations z inv , respectively.\
    \ With the guidance of the generated domainness, E spf , E inv and SAR (Self-Adversarial\
    \ Regularizer) work in an adversarial manner, i.e., two opposite loss functions,\
    \ to disentangle the latent representations into z spf and z inv (Best viewed\
    \ in color)."
  Figure 3 Link: articels_figures_by_rev_year\2023\SelfAdversarial_Disentangling_for_Specific_Domain_Adaptation\figure_3.jpg
  Figure 3 caption: "Domainness creation in Fo V x dimension. O is the optical center\
    \ of the camera and F is the focal point. OF is the focal length, MN and PQ represent\
    \ the original width and the new width before and after the transformation, respectively.\
    \ Fo V x is reduced from \u2220MFN to \u2220PFQ after the process."
  Figure 4 Link: articels_figures_by_rev_year\2023\SelfAdversarial_Disentangling_for_Specific_Domain_Adaptation\figure_4.jpg
  Figure 4 caption: Visualization of diversified source images produced by Domainness
    Creator with different domainness values on style, fog, rain, and FoV dimensions,
    respectively. With the variations of domainness, our model can learn the domainness-invariant
    features (Best viewed in color).
  Figure 5 Link: articels_figures_by_rev_year\2023\SelfAdversarial_Disentangling_for_Specific_Domain_Adaptation\figure_5.jpg
  Figure 5 caption: "Hyper-parameter analysis: (a) effect of domainness number ( N\
    \ ) of DC with SCL [24] and SWDA [18] on VKIITI [61] to CKITTI [56], [62]. (b)\
    \ Effect of \u03BB spf on the performance with GPA [21] on Cityscapes [56] to\
    \ Foggy Cityscapes [57] (Best viewed in color)."
  Figure 6 Link: articels_figures_by_rev_year\2023\SelfAdversarial_Disentangling_for_Specific_Domain_Adaptation\figure_6.jpg
  Figure 6 caption: "Qualitative results of cross-domain object detection on Cityscapes\
    \ [56] \u2192 Foggy Cityscapes [57] and Cityscapes [56] \u2192 RTTS [58] set-up.\
    \ The first and the third rows plot the baseline predictions of GPA [21] and SWDA\
    \ [18], respectively. And the second and the fourth rows plot the predictions\
    \ of our models (with GPA [21] and SWDA [18]), respectively. Compared to these\
    \ baselines, our method can detect more objects in the images accurately."
  Figure 7 Link: articels_figures_by_rev_year\2023\SelfAdversarial_Disentangling_for_Specific_Domain_Adaptation\figure_7.jpg
  Figure 7 caption: "Qualitative results of cross-domain semantic segmentation on\
    \ Virtual KITTI [61] \u2192 CKITTI [56], [62] set-up. The four columns plot (a)\
    \ input RGB images, (b) ground truth, (c) the predictions of AdaptsegNet [34]\
    \ baseline, and (d) Ours (with AdaptsegNet [34]). Our method performs segmentation\
    \ more precisely than prior work [34]. (Best viewed in color)."
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.69
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.97
  Name of the first author: Qianyu Zhou
  Name of the last author: Lizhuang Ma
  Number of Figures: 7
  Number of Tables: 10
  Number of authors: 5
  Paper title: Self-Adversarial Disentangling for Specific Domain Adaptation
  Publication Date: 2023-01-23 00:00:00
  Table 1 caption:
    table_text: TABLE I Comparisons of Several Related Settings of UDA, Prior DA,
      Cross-FOV DA, DG, and Ours (SDA)
  Table 10 caption:
    table_text: TABLE X Ablations on the Domainness Number ( N N) of DC
  Table 2 caption:
    table_text: TABLE II Comparison Results for Object Detection in (a,b) Cross-Fog,
      (c) cross-FoV and (d) Cross-Rain Scenarios
  Table 3 caption:
    table_text: TABLE III Cross-FoV Adaptation of Semantic Segmentation From Virtual
      KITTI to CKITTI (multi-Domainness)
  Table 4 caption:
    table_text: TABLE IV Synthetic-to-Real Adaptation of Object Detection From SIM10
      K to Cityscapes (Default Dimension is Style)
  Table 5 caption:
    table_text: TABLE V Cross-Fog Adaptation of Semantic Segmentation From Cityscapes
      to Foggy Zurich++ (multi-Domainness)
  Table 6 caption:
    table_text: TABLE VI Ablation Study of Each Component on Two Tasks
  Table 7 caption:
    table_text: TABLE VII Comparison to Data Augmentations With Labels
  Table 8 caption:
    table_text: TABLE VIII Ablation Study of the Losses L inv Linv and L spf Lspf
  Table 9 caption:
    table_text: TABLE IX Comparison to Related Works on the Baseline [34]
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3238727
- Affiliation of the first author: college of electrical and information, xi'an polytechnic
    university, xi'an, china
  Affiliation of the last author: institute for integrated and intelligent systems,
    griffith university, nathan, qld, australia
  Figure 1 Link: articels_figures_by_rev_year\2023\Image_Intensity_Variation_Information_for_Interest_Point_Detection\figure_1.jpg
  Figure 1 caption: Examples of interest point models, (a) An example of an elementary
    corner model in the Cartesian coordinate system, (b) An example of a blob model
    in the Cartesian coordinate system.
  Figure 10 Link: articels_figures_by_rev_year\2023\Image_Intensity_Variation_Information_for_Interest_Point_Detection\figure_10.jpg
  Figure 10 caption: Comparison results of the mean matching accuracy for the fifteen
    methods on the HPatches dataset.
  Figure 2 Link: articels_figures_by_rev_year\2023\Image_Intensity_Variation_Information_for_Interest_Point_Detection\figure_2.jpg
  Figure 2 caption: Examples of FOGDDs (in the second column) and SOGDDs (in the third
    column) of a step edge, an L-type corner, and a Y-type corner models (as shown
    in the first column, with grey values T 1 =50 , T 2 =100 , and T 3 =120 ).
  Figure 3 Link: articels_figures_by_rev_year\2023\Image_Intensity_Variation_Information_for_Interest_Point_Detection\figure_3.jpg
  Figure 3 caption: "An anisotropic-type blob ( a=1 , b=1.3 , and T=1 ), the anisotropic-type\
    \ blob undergoing an IAT ( a=1 , b=1.5 , and T=1 ), and an isotropic-type blob\
    \ model ( a=1 and T=1 ) are illustrated in (a), (d), and (g) in the first column.\
    \ Their corresponding FOGDD and SOGDD ( \u03C3 2 =1 ) are illustrated in the second\
    \ column and the third column respectively."
  Figure 4 Link: articels_figures_by_rev_year\2023\Image_Intensity_Variation_Information_for_Interest_Point_Detection\figure_4.jpg
  Figure 4 caption: The positions of an edge and a corner and their corresponding
    positions under IATs are illustrated in (a) and (d). The FOAGDDs of the edge point
    and the corner are illustrated in the second column and the third column.
  Figure 5 Link: articels_figures_by_rev_year\2023\Image_Intensity_Variation_Information_for_Interest_Point_Detection\figure_5.jpg
  Figure 5 caption: Illustration of different corner detections on the 'Block' image,
    (a) 'Block' and its GT corner positions, (b) Harris, (c) FAST, (d) FOAGDD, (e)
    SOGGDD, (f) Proposed detector.
  Figure 6 Link: articels_figures_by_rev_year\2023\Image_Intensity_Variation_Information_for_Interest_Point_Detection\figure_6.jpg
  Figure 6 caption: Illustration of different corner detections on the 'Lab' image,
    (a) 'Lab' and its GT corner positions, (b) Harris, (c) FAST, (d) FOAGDD, (e) SOGGDD,
    (f) Proposed detector.
  Figure 7 Link: articels_figures_by_rev_year\2023\Image_Intensity_Variation_Information_for_Interest_Point_Detection\figure_7.jpg
  Figure 7 caption: Illustration of different blob detections on the 'Peanut leaf'
    image, (a) 'Peanut leaf' and its GT blob positions, (b) Harris-Laplace, (c) SIFT,
    (d) DT-CovNet, (e) LIFT, (f) Proposed detector.
  Figure 8 Link: articels_figures_by_rev_year\2023\Image_Intensity_Variation_Information_for_Interest_Point_Detection\figure_8.jpg
  Figure 8 caption: Illustration of different blob detections on the 'Citrus leaf'
    image, (a) 'Citrus leaf' and its GT blob positions, (b) Harris-Laplace, (c) SIFT,
    (d) DT-CovNet, (e) LIFT, (f) Proposed detector.
  Figure 9 Link: articels_figures_by_rev_year\2023\Image_Intensity_Variation_Information_for_Interest_Point_Detection\figure_9.jpg
  Figure 9 caption: Average repeatability metrics for the fifteen methods under (a)
    rotation, (b) isotropic scaling, (c) anisotropic scaling, (d) shearing, (e) JPEG
    compression, and (f) Gaussian noise.
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Weichuan Zhang
  Name of the last author: Yongsheng Gao
  Number of Figures: 10
  Number of Tables: 4
  Number of authors: 3
  Paper title: Image Intensity Variation Information for Interest Point Detection
  Publication Date: 2023-01-26 00:00:00
  Table 1 caption:
    table_text: TABLE I Comparison Results of Five Corner Detection Methods on Two
      Test Images ('Block' and 'Lab') With GTs
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Comparison Results of Ten Blob Detection Methods on Two Test
      Images With GTs
  Table 3 caption:
    table_text: "TABLE III Comparison Results for SfM ( \u2020 \u2020 Denotes That\
      \ the Result is Provided by [46])"
  Table 4 caption:
    table_text: TABLE IV Comparisons on Execution Time and Memory Usage
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3240129
- Affiliation of the first author: school of computer science and technology, university
    of chinese academy of sciences, beijing, china
  Affiliation of the last author: huawei cloud & ai, shenzhen, guangdong province,
    china
  Figure 1 Link: articels_figures_by_rev_year\2023\General_Greedy_DeBias_Learning\figure_1.jpg
  Figure 1 caption: Examples of dataset biases in different tasks. Models tend to
    capture spurious correlations between the inputs and the labels instead of the
    task of interest. From top to bottom, we illustrate the color bias in Biased-MNIST,
    background bias in image classification, and compositional language bias in VQA.
  Figure 10 Link: articels_figures_by_rev_year\2023\General_Greedy_DeBias_Learning\figure_10.jpg
  Figure 10 caption: "The original Labels and Pseudo Labels from GGD gs . The left\
    \ figure is the label changes in the Biased MNIST training set, while the right\
    \ figure is that of \u201Cis this\u201D question type in VQA-CP v2."
  Figure 2 Link: articels_figures_by_rev_year\2023\General_Greedy_DeBias_Learning\figure_2.jpg
  Figure 2 caption: Comparison between GGD gs and GGD cr . GGD gs (GGE) uses the gradient
    from the biased model as the pseudo label while GGD cr enlarges the prediction
    discrepancy between the base model and the biased model with curriculum learning.
    GGD gs is a special case of GGD when lambda t=1 under CE loss.
  Figure 3 Link: articels_figures_by_rev_year\2023\General_Greedy_DeBias_Learning\figure_3.jpg
  Figure 3 caption: Per-class Accuracies on Biased MNIST. All methods are trained
    with rho texttrain=0.999 and tested on rho texttest=0.1 . The upper row is the
    confusion matrix between the predictions and the ground-truth labels; the lower
    row shows the confusion matrix between the predicted labels and the background
    color labels.
  Figure 4 Link: articels_figures_by_rev_year\2023\General_Greedy_DeBias_Learning\figure_4.jpg
  Figure 4 caption: An example from Adversarial SQuAD. Blue sentence is the expected
    evidence, while red sentence is the distracting sentence to fool the models.
  Figure 5 Link: articels_figures_by_rev_year\2023\General_Greedy_DeBias_Learning\figure_5.jpg
  Figure 5 caption: Predicted distribution for three variants of GGD gs .
  Figure 6 Link: articels_figures_by_rev_year\2023\General_Greedy_DeBias_Learning\figure_6.jpg
  Figure 6 caption: Qualitative Evaluation for GGD gsdq . We provide a comparison
    between UpDn and GGD gsdq on the visualization of the most sensitive regions and
    confidence of the top-5 answers. Red answers denote the ground-truth.
  Figure 7 Link: articels_figures_by_rev_year\2023\General_Greedy_DeBias_Learning\figure_7.jpg
  Figure 7 caption: Comparison between GGD gsdq and GGD crdq . The major improvements
    are reflected on counting problems and questions that rarely appear in the training
    data.
  Figure 8 Link: articels_figures_by_rev_year\2023\General_Greedy_DeBias_Learning\figure_8.jpg
  Figure 8 caption: 'Failure cases for GGD crdq : From top to bottom, the failure
    cases are 1) counting problems; 2) Synonym answers; 3) incorrect visual evidences.'
  Figure 9 Link: articels_figures_by_rev_year\2023\General_Greedy_DeBias_Learning\figure_9.jpg
  Figure 9 caption: The loss ratio of the hard examples from baseline model and GGD
    cr base model.
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Xinzhe Han
  Name of the last author: Qi Tian
  Number of Figures: 10
  Number of Tables: 6
  Number of authors: 5
  Paper title: General Greedy De-Bias Learning
  Publication Date: 2023-01-27 00:00:00
  Table 1 caption:
    table_text: "TABLE I Comparison on Biased-MNIST. \u03C1 train \u03C1train and\
      \ \u03C1 test \u03C1test Denote the Level of Texture Bias During Training and\
      \ Testing, Respectively"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE II Ablations for \u03BB t \u03BBt"
  Table 3 caption:
    table_text: TABLE III Experimental Results on Adversarial QA
  Table 4 caption:
    table_text: TABLE IV Experimental Results on VQA-CP v2 Test Set and VQA v2 Val
      Set of State-of-The-Art Methods
  Table 5 caption:
    table_text: TABLE V Ablation Study for Different Versions GGD on VQA-CP v2 Test
      Set and VQA v2 Val Set
  Table 6 caption:
    table_text: TABLE VI Experimental results on GQA-OOD test-dev
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3240337
- Affiliation of the first author: school of computer and information technology,
    key laboratory of computational intelligence and chinese information processing
    of ministry of education, shanxi university, taiyuan, shanxi, china
  Affiliation of the last author: school of computer and information technology, key
    laboratory of computational intelligence and chinese information processing of
    ministry of education, shanxi university, taiyuan, shanxi, china
  Figure 1 Link: articels_figures_by_rev_year\2023\Evaluating_Classification_Model_Against_Bayes_Error_Rate\figure_1.jpg
  Figure 1 caption: Comparison of BER and BER bounds in model evaluation. The Chan
    data set has two classes and 5,621 instances. The Mixed Gaussian data set has
    four classes and 4,000 instances. The parameters of the classifiers are default
    in scikit-learn [18].
  Figure 10 Link: articels_figures_by_rev_year\2023\Evaluating_Classification_Model_Against_Bayes_Error_Rate\figure_10.jpg
  Figure 10 caption: "Comparison of different BER estimators on Mixed Gaussian data\
    \ sets with different hyper-parameter \u03BC . The another hyper-parameter for\
    \ generating these data sets is \u03C3 2 =0.3 ."
  Figure 2 Link: articels_figures_by_rev_year\2023\Evaluating_Classification_Model_Against_Bayes_Error_Rate\figure_2.jpg
  Figure 2 caption: Illustration of the Denoise - based strategy. The employed synthetic
    data set consists of two class samples. The samples of Class 1 are composed of
    80 samples with red circles that are generated by probability density function
    p1f1(x) , which is a univariate normal function of mean is 1 and variance is 0.9.
    The samples of Class 2 are composed of 80 samples with black triangles that are
    generated by a univariate normal distribution function p2f2(x) where the mean
    is 4.5 and variance is 1.3. The dotted line represents the Bayesian optimal decision
    boundary.
  Figure 3 Link: articels_figures_by_rev_year\2023\Evaluating_Classification_Model_Against_Bayes_Error_Rate\figure_3.jpg
  Figure 3 caption: The relationship between approximation error and sample size.
  Figure 4 Link: articels_figures_by_rev_year\2023\Evaluating_Classification_Model_Against_Bayes_Error_Rate\figure_4.jpg
  Figure 4 caption: Illustration of the basic concept of percolation theory.
  Figure 5 Link: articels_figures_by_rev_year\2023\Evaluating_Classification_Model_Against_Bayes_Error_Rate\figure_5.jpg
  Figure 5 caption: The schematic framework of BN-BER. The data points connected by
    the red arrows represent two homogeneous clusters, and the dotted line represents
    the Bayes decision boundary. The data points marked with a hollow red circle represent
    Bayes noisy samples.
  Figure 6 Link: articels_figures_by_rev_year\2023\Evaluating_Classification_Model_Against_Bayes_Error_Rate\figure_6.jpg
  Figure 6 caption: Illustration of homogeneous clusters construction. Panel (a) illustrates
    the concept of meta cluster of data point i . Panel (b) illustrates the concept
    of forward propagation of the data point i . Panel (c) illustrates the concept
    of backward propagation of the data point i . Panel (d) illustrates the concept
    of local belief cluster of the data point i . Panel (e) illustrates the constructing
    process of two homogeneous clusters. The constructing process is based on several
    anchor points (marked with the blue edge) and the meta clusters formed by these
    anchor points. The data connected by the red arrows are two homogeneous clusters
    of the data set.
  Figure 7 Link: articels_figures_by_rev_year\2023\Evaluating_Classification_Model_Against_Bayes_Error_Rate\figure_7.jpg
  Figure 7 caption: Eight synthetic data sets with different data distributions. In
    each subfigure, the different color represent the different class, and the numbers
    in subheading brackets represents the amount of samples in the data set.
  Figure 8 Link: articels_figures_by_rev_year\2023\Evaluating_Classification_Model_Against_Bayes_Error_Rate\figure_8.jpg
  Figure 8 caption: "Two 10-class Mixed Gaussian data sets. The mean parameter of\
    \ the data set in (a) and (b) are \u03BC=0.7 and \u03BC=2 , respectively. In the\
    \ both cases, \u03C3 2 =0.1 ."
  Figure 9 Link: articels_figures_by_rev_year\2023\Evaluating_Classification_Model_Against_Bayes_Error_Rate\figure_9.jpg
  Figure 9 caption: "Comparison of convergence rate of BN-BER and BG-BER. The hyper-parameters\
    \ that used to generate the Mixed Gaussian data sets are \u03BC=1.2 , \u03C3 2\
    \ =0.3 ."
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Qingqiang Chen
  Name of the last author: Jiye Liang
  Number of Figures: 11
  Number of Tables: 9
  Number of authors: 4
  Paper title: Evaluating Classification Model Against Bayes Error Rate
  Publication Date: 2023-01-27 00:00:00
  Table 1 caption:
    table_text: TABLE I List of Mathematical Notations
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Summary of the Benchmark Data Sets Used in the Experiments
  Table 3 caption:
    table_text: TABLE III Summary of the Image Data Sets Used in the Experiments
  Table 4 caption:
    table_text: TABLE IV Summary of the Classifiers Used in the Experiments. The Parameters
      of Each Classifier are the Default in Scikit-Learn
  Table 5 caption:
    table_text: "TABLE V Performance Comparison Among the Six BER Estimators on the\
      \ Synthetic Data Sets. The Lowest Classification Error Among the Nine Classifiers\
      \ is Marked With \u2022"
  Table 6 caption:
    table_text: TABLE VI Performance Comparison Among the Six BER Estimators on the
      Image Data Sets
  Table 7 caption:
    table_text: TABLE VII Performance Comparison Among the Six BER Estimators on the
      Benchmark Data Sets
  Table 8 caption:
    table_text: TABLE VIII The Comparison of the Running Time (seconds) of Computing
      BN-BER, BG-BER and GHP Bounds
  Table 9 caption:
    table_text: TABLE IX The Comparison of the Running Time (seconds) of Computing
      BN-BER, BG-BER and GHP Bounds on the Top 10 Image Data Sets. The NA Represents
      That the Running Time of GHP Bounds Estimator Exceeds 72 Hours
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3240194
- Affiliation of the first author: meta ai, menlo park, ca, usa
  Affiliation of the last author: meta ai, menlo park, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2023\TextStyleBrush_Transfer_of_Text_Aesthetics_From_a_Single_Example\figure_1.jpg
  Figure 1 caption: Text generationediting using TextStyleBrush. (Top) Detected words
    highlighted in blue boxes edited by changing their content and shown in the target
    image. (Bottom) Given the source handwritten word image (left), we generate a
    target sentence image mimicking the same handwritten style (right).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\TextStyleBrush_Transfer_of_Text_Aesthetics_From_a_Single_Example\figure_2.jpg
  Figure 2 caption: Overview of our proposed approach, at training. Content and style
    encoders, in green, are detailed in Section III-A. The mapping network and generator
    are explained in Section III-B. Our loss functions, in yellow, are explained in
    Section IV.
  Figure 3 Link: articels_figures_by_rev_year\2023\TextStyleBrush_Transfer_of_Text_Aesthetics_From_a_Single_Example\figure_3.jpg
  Figure 3 caption: Imgur5K word images. (Cols 1-4) Sample word images from the Imgur5K
    dataset. (Col 5) A single sample word image from each of the datasets (top to
    bottom) GW, CVL, Bentham, IAM respectively.
  Figure 4 Link: articels_figures_by_rev_year\2023\TextStyleBrush_Transfer_of_Text_Aesthetics_From_a_Single_Example\figure_4.jpg
  Figure 4 caption: Word-level style transfer results. Each image pair shows input
    source style on the left and output with novel content (string) on the right.
    All examples are real photos (no synthetic data) taken from ICDAR13 [57], TextVQA
    [59], IAM handwriting [53], and the Imgur5K set collected for this work. Note
    that the handwritten results are of variable widths, but resized to fixed dimension
    in this figure for better visualization. See supplemental for more results., available
    online
  Figure 5 Link: articels_figures_by_rev_year\2023\TextStyleBrush_Transfer_of_Text_Aesthetics_From_a_Single_Example\figure_5.jpg
  Figure 5 caption: Scene text editing results. On left we show the original scene
    image with word bounding boxes shown in blue rectangles, and on right we present
    the edited image with text content replaced and blended back using simple Poisson
    blending [64] to the scene image. These examples are taken from ICDAR13 [57] and
    TextVQA [59] dataset. See supplemental for more results., available online
  Figure 6 Link: articels_figures_by_rev_year\2023\TextStyleBrush_Transfer_of_Text_Aesthetics_From_a_Single_Example\figure_6.jpg
  Figure 6 caption: One style, multiple contents. Top row are source style images.
    All other rows show TSB results of applying source styles to multiple contents.
  Figure 7 Link: articels_figures_by_rev_year\2023\TextStyleBrush_Transfer_of_Text_Aesthetics_From_a_Single_Example\figure_7.jpg
  Figure 7 caption: Qualitative analysis. SRNet [8] versus TSB .
  Figure 8 Link: articels_figures_by_rev_year\2023\TextStyleBrush_Transfer_of_Text_Aesthetics_From_a_Single_Example\figure_8.jpg
  Figure 8 caption: Qualitative analysis. Davis et al. [14] versus TSB.
  Figure 9 Link: articels_figures_by_rev_year\2023\TextStyleBrush_Transfer_of_Text_Aesthetics_From_a_Single_Example\figure_9.jpg
  Figure 9 caption: Limitations of our approach. Qualitative example scene text (top)
    and handwritten (bottom) failures. These failures are due to local style variations
    (e.g., colors varying among characters), metallic colors which were not well represented
    in the training, and uniquely complex calligraphy for handwriting.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.74
  Name of the first author: Praveen Krishnan
  Name of the last author: Tal Hassner
  Number of Figures: 9
  Number of Tables: 5
  Number of authors: 5
  Paper title: 'TextStyleBrush: Transfer of Text Aesthetics From a Single Example'
  Publication Date: 2023-01-27 00:00:00
  Table 1 caption:
    table_text: TABLE I Comparison of Popular Datasets of Handwritten English Words
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Ablation Study Comparing the Influence of Different Loss
      Functions on Our TSB Results
  Table 3 caption:
    table_text: TABLE III Text Recognition Accuracy on Images From Three Datasets
  Table 4 caption:
    table_text: TABLE IV Quantitative Handwritten Results
  Table 5 caption:
    table_text: TABLE V User Study. The User Study Shows Clear Margin in Favor of
      Our Results, Across Scene Text and Handwriting Domains
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3239736
- Affiliation of the first author: university of bologna, bologna, bo, italy
  Affiliation of the last author: university of bologna, bologna, bo, italy
  Figure 1 Link: articels_figures_by_rev_year\2023\Learning_Good_Features_to_Transfer_Across_Tasks_and_Domains\figure_1.jpg
  Figure 1 caption: Our framework transfers knowledge across tasks and domains. Given
    two tasks (1 and 2) and two domains (A and B), with supervision for both tasks
    in A but only for one task in B, we learn the dependency between the tasks in
    A and exploit this in B in order to solve task 2 without the need of supervision.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Learning_Good_Features_to_Transfer_Across_Tasks_and_Domains\figure_2.jpg
  Figure 2 caption: "ATDT framework: here N 1 and N 2 are trained separately to solve\
    \ tasks T 1 and T 2 . While N 2 is trained only on images from domain A , N 1\
    \ is trained jointly on both domain A and domain B , to enable the extraction\
    \ of domain invariant features. Then, encoders from the two networks are frozen\
    \ and used to learn the transfer function G 1\u21922 , which aims at transforming\
    \ features extracted for T 1 in features that are good for T 2 . This step is\
    \ performed only on domain A , since we have no supervision for T 2 on domain\
    \ B . Finally, at inference time, features are extracted from E 1 starting from\
    \ images of domain B , transformed with the G 1\u21922 and fed to D 2 to produce\
    \ the final predictions."
  Figure 3 Link: articels_figures_by_rev_year\2023\Learning_Good_Features_to_Transfer_Across_Tasks_and_Domains\figure_3.jpg
  Figure 3 caption: Features alignment strategies across tasks and domains. We train
    jointly the networks N 1 , N 2 and a shared auxiliary decoder D aux . We train
    N 1 to solve T 1 on images from domains A and B using a supervised loss L T 1
    for T 1 alongside a novel feature Norm Discrepancy Alignment loss L NDA which
    helps better aligning the features computed by N 1 across the two domains. We
    train N 2 using a supervised loss L T 2 for T 2 on images from B . D aux is trained
    to solve an auxiliary task T aux using the loss L aux and based on the features
    computed by E 1 on images from A and B as well as by E 2 on images from B .
  Figure 4 Link: articels_figures_by_rev_year\2023\Learning_Good_Features_to_Transfer_Across_Tasks_and_Domains\figure_4.jpg
  Figure 4 caption: 'Two task transfer scenarios: depth-to-semantic on the left, the
    opposite on the right. First row: ground-truth depth and semantic segmentation
    maps; second row: corresponding edge maps. Red circles highlight information needed
    in the target task but missing in the source one.'
  Figure 5 Link: articels_figures_by_rev_year\2023\Learning_Good_Features_to_Transfer_Across_Tasks_and_Domains\figure_5.jpg
  Figure 5 caption: Spatial Priors Similarities Across Domains. Considered the semantic
    segmentation task, we compute the number of occurrences of each class at each
    pixel location for both domains. Domain A is CARLA, B is Cityscapes. We visualize
    the occurrence maps with a viridis colormap.
  Figure 6 Link: articels_figures_by_rev_year\2023\Learning_Good_Features_to_Transfer_Across_Tasks_and_Domains\figure_6.jpg
  Figure 6 caption: 'From left to right: RGB input image of domain A , depth prediction
    from N 1 , edges from f 1 , semantic segmentation from N 2 and edges from f 2
    . Task features f 1 and f 2 encode richer details than strictly needed to solve
    either tasks as we can recover all edges from both of them by D aux .'
  Figure 7 Link: articels_figures_by_rev_year\2023\Learning_Good_Features_to_Transfer_Across_Tasks_and_Domains\figure_7.jpg
  Figure 7 caption: "Qualitative results of the Dep\u2192Sem scenario. From left to\
    \ right: RGB image, ground-truth, baseline trained only on domain A , ours."
  Figure 8 Link: articels_figures_by_rev_year\2023\Learning_Good_Features_to_Transfer_Across_Tasks_and_Domains\figure_8.jpg
  Figure 8 caption: "Qualitative result of the Sem\u2192Dep scenario. From left to\
    \ right: RGB image, ground-truth, baseline network trained only on domain A ,\
    \ ours."
  Figure 9 Link: articels_figures_by_rev_year\2023\Learning_Good_Features_to_Transfer_Across_Tasks_and_Domains\figure_9.jpg
  Figure 9 caption: "Zoomed results in a Dep\u2192Sem scenario. From left to right:\
    \ base ATDT without edge and NDA, our proposed method, ground-truth. We notice\
    \ how, unlike base ATDT, our method is able to recover the fine-grained details\
    \ of the scene."
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Pierluigi Zama Ramirez
  Name of the last author: Luigi Di Stefano
  Number of Figures: 9
  Number of Tables: 10
  Number of authors: 6
  Paper title: Learning Good Features to Transfer Across Tasks and Domains
  Publication Date: 2023-01-27 00:00:00
  Table 1 caption:
    table_text: "TABLE I Experimental Results of Dep\u2192Sem Dep\u2192Sem Scenario.\
      \ Baseline Stands for N 2 N2 Trained on A A and Tested on B B, Transfer Oracle\
      \ Represents G 1\u21922 G1\u21922 Trained Only on B B, Oracle Refers to N 2\
      \ N2 Trained and Tested on B B"
  Table 10 caption:
    table_text: "TABLE X Results of Aligning Input andor Output Space of G 1\u2192\
      2 G1\u21922 in a Dep\u2192Sem Dep\u2192Sem Scenario"
  Table 2 caption:
    table_text: "TABLE II Experimental Results of Sem\u2192Dep Sem\u2192Dep Scenario.\
      \ Baseline Stands for N 2 N2 Trained on A A and Tested on B B, Transfer Oracle\
      \ Represents G 1\u21922 G1\u21922 Trained Only on B B, Oracle Refers to N 2\
      \ N2 Trained and Tested on B B"
  Table 3 caption:
    table_text: "TABLE III Ablation Study in the Dep\u2192Sem Dep\u2192Sem Scenario"
  Table 4 caption:
    table_text: "TABLE IV Comparison Between Autoencoder and Edge Detection as Auxiliary\
      \ Tasks in the Dep\u2192Sem Dep\u2192Sem Scenario"
  Table 5 caption:
    table_text: "TABLE V Auxiliary Tasks as Source Tasks in the Dep\u2192Sem Dep\u2192\
      Sem Scenario"
  Table 6 caption:
    table_text: TABLE VI Ablation Study on the Importance of Simultaneous Training
      of the T 1 T1, T 2 T2, and the Auxiliary Task
  Table 7 caption:
    table_text: TABLE VII Comparison Between NDA Loss and Other Strategies to Align
      E 1 E1 Features
  Table 8 caption:
    table_text: "TABLE VIII Results of Aligning Output Space of E 2 E2 in a Dep\u2192\
      Sem Dep\u2192Sem Scenario"
  Table 9 caption:
    table_text: "TABLE IX Results of Aligning Output Space of D 2 D2 in a Dep\u2192\
      Sem Dep\u2192Sem Scenario"
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3240316
- Affiliation of the first author: school of electronic information, wuhan university,
    wuhan, china
  Affiliation of the last author: school of computer science, wuhan university, wuhan,
    china
  Figure 1 Link: articels_figures_by_rev_year\2023\Learning_to_SuperResolve_Blurry_Images_With_Events\figure_1.jpg
  Figure 1 caption: Our eSL-Net++ reconstructs high-resolution, sharp, and clear intensity
    images for event cameras by Active Pixel Sensor (APS) frames and the corresponding
    event sequences. The eSL-Net++ performs much better than EDI [20] and LEDVDI [21]
    that are followed by an SR network RCAN [22].
  Figure 10 Link: articels_figures_by_rev_year\2023\Learning_to_SuperResolve_Blurry_Images_With_Events\figure_10.jpg
  Figure 10 caption: Qualitative results of single frame reconstructions (1st row)
    and their corresponding sequence reconstructions (2nd-5th rows) on the RWS dataset,
    where Baseline indicates eSL-Net without DSL and RESM.
  Figure 2 Link: articels_figures_by_rev_year\2023\Learning_to_SuperResolve_Blurry_Images_With_Events\figure_2.jpg
  Figure 2 caption: The Dual Sparse Learning (DSL) scheme, where (14) is optimized
    through the deep neural network by unfolding (15). Note that quadratic connection
    means that the input is squared and then fed into the block to which the arrow
    connects.
  Figure 3 Link: articels_figures_by_rev_year\2023\Learning_to_SuperResolve_Blurry_Images_With_Events\figure_3.jpg
  Figure 3 caption: Event-enhanced Sparse Learning Network for super-resolving blurry
    images with events, i.e., eSL-Net++. Coefficients mathbf beta and event reconstruction
    barboldsymbolE are omitted here.
  Figure 4 Link: articels_figures_by_rev_year\2023\Learning_to_SuperResolve_Blurry_Images_With_Events\figure_4.jpg
  Figure 4 caption: 'The Rigorous Event Shuffle-and-Merge (RESM) scheme: (a) the input
    event tensors when reconstructing the image of time f=0 (from top to bottom are
    positive event stream, negative event stream, and recovered latent image of time
    f=0 ); (b) the shuffle of events to input tensors when reconstructing the image
    of time f > 0 .'
  Figure 5 Link: articels_figures_by_rev_year\2023\Learning_to_SuperResolve_Blurry_Images_With_Events\figure_5.jpg
  Figure 5 caption: Quantitative and qualitative results on the GoPro dataset (top
    two rows) and the HQF dataset (bottom two rows), where our proposed eSL-Net and
    eSL-Net++ are compared to GFN, DASR, CDVD+RCAN, RED-Net+RCAN, LEDVDI+RCAN, and
    EFNet+DASR.
  Figure 6 Link: articels_figures_by_rev_year\2023\Learning_to_SuperResolve_Blurry_Images_With_Events\figure_6.jpg
  Figure 6 caption: Qualitative results on the RWS dataset, where our proposed eSL-Net
    and eSL-Net++ are compared with GFN and LEDVDI+RCAN.
  Figure 7 Link: articels_figures_by_rev_year\2023\Learning_to_SuperResolve_Blurry_Images_With_Events\figure_7.jpg
  Figure 7 caption: Qualitative results of single frame reconstructions (1st row)
    and their corresponding sequence reconstructions (2nd-6th rows) on the RWS dataset,
    where our proposed eSL-Net (5th row) and eSL-Net++ (6th row) are compared to EDI+RCAN
    (2nd row), LEDVDI+RCAN (3 rd row) and RED-Net+RCAN (4th row).
  Figure 8 Link: articels_figures_by_rev_year\2023\Learning_to_SuperResolve_Blurry_Images_With_Events\figure_8.jpg
  Figure 8 caption: Qualitative comparisons with Deblur-then-VSR methods on the GoPro
    (top row) and HQF (bottom row) datasets, where the VSR algorithm employed in this
    experiment is RealBasicVSR [29].
  Figure 9 Link: articels_figures_by_rev_year\2023\Learning_to_SuperResolve_Blurry_Images_With_Events\figure_9.jpg
  Figure 9 caption: Qualitative ablation study on the DSL module. (b) and (f) are
    respectively the output of LDI, i.e., E and the final SR result of eSL-Net [35]
    when inputting noisy synthesized events (a) and the corresponding blurry image
    (e). Accordingly, (c) and (g) are the results of eSL-Net when inputting noiseless
    events, while (d) and (h) are the results of eSL-Net++ when inputting noisy events.
  First author gender probability: 0.55
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.97
  Name of the first author: Lei Yu
  Name of the last author: Gui-Song Xia
  Number of Figures: 12
  Number of Tables: 2
  Number of authors: 7
  Paper title: Learning to Super-Resolve Blurry Images With Events
  Publication Date: 2023-01-30 00:00:00
  Table 1 caption:
    table_text: TABLE I Quantitative Comparison of Our Proposed eSL-Net and eSL-Net++
      With the State-of-The-Art SR Methods With Inputs of Pure Images, Pure Events,
      and Fusion of Images and Events
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Ablation Studies on DSL and RESM Over the GoPro Dataset With
      Synthetic Events and the HQF Dataset With Real Events
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3240397
- Affiliation of the first author: "computer vision lab, eth zurich, z\xFCrich, switzerland"
  Affiliation of the last author: reler, aaii, university of technology sydney, ultimo,
    nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2023\Differentiable_MultiGranularity_Human_Parsing\figure_1.jpg
  Figure 1 caption: Conceptual illustration of our new bottom-up regime for instance-aware
    human semantic parsing. By learning 1) category-level human semantic parsing,
    2) body-to-joint projection, and 3) bottom-up keypoint detection and association
    in a joint and end-to-end manner, our model tackles the task in a differentiable,
    multi-granularity human representation learning framework.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Differentiable_MultiGranularity_Human_Parsing\figure_2.jpg
  Figure 2 caption: Trade-off between performance versus efficiency on MHP-v2 [14]
    val. The x -axis and y -axis denote FPS and AP p 50 , respectively. The circle
    size indicates Multi-Adds (G). Top-down and bottom-up models are labeled in black
    and red, respectively. As seen, our approach shows promising performance with
    high efficiency.
  Figure 3 Link: articels_figures_by_rev_year\2023\Differentiable_MultiGranularity_Human_Parsing\figure_3.jpg
  Figure 3 caption: Our multi-granularity human representation learning framework
    for instance-aware human semantic parsing (Section III). Given an input image
    I , we extract a feature pyramid X l L l=1 from the backbone network. Then, we
    designate three branches for instance-agnostic human part parsing (Section III-A),
    multi-step DSPF estimation (Section III-B) and human keypoint detection and association
    (Section III-C), respectively. Through DSPF, our model can easily associate dense
    human semantics with sparse joints, eventually delivering instance-aware body
    parsing results.
  Figure 4 Link: articels_figures_by_rev_year\2023\Differentiable_MultiGranularity_Human_Parsing\figure_4.jpg
  Figure 4 caption: "Left: conceptual illustration of our bottom-up multi-person pose\
    \ estimator (Section III-C). Given an input image, our model performs joint detection\
    \ (Section III-C1) and limb scoring (Section III-C2) separately to obtain a set\
    \ of keypoint candidates and their association scores. Based on them, the algorithm\
    \ assembles the detected keypoints into human poses (Section III-C3) by solving\
    \ a set of bipartite matching problems, each for one limb. Taking the \u201Cleft\
    \ lower arm\u201D as an example. Here p k 1 , p k 2 and p k \u2032 1 , p k \u2032\
    \ 2 refer to keypoint candidates for category k (i.e., \u201Cleft wrist\u201D\
    ) and k \u2032 (i.e., \u201Cleft elbow\u201D), A denotes an affinity matrix measuring\
    \ pair-wise association score. Our algorithm then groups keypoints by inferring\
    \ an assignment matrix Y in (14)-(15). Right: K -dimensional graph matching based\
    \ on the tree structure or a set of bipartite graphs."
  Figure 5 Link: articels_figures_by_rev_year\2023\Differentiable_MultiGranularity_Human_Parsing\figure_5.jpg
  Figure 5 caption: Runtime analysis. Our model is fully convolutional and allows
    for efficient inference, irrespective of the number of people in the image. In
    contrast, the runtimes of top-down approaches (i.e., M-CE2P [20], P-RCNN [21],
    RP-RCNN [18]) grow linearly with the number of people.
  Figure 6 Link: articels_figures_by_rev_year\2023\Differentiable_MultiGranularity_Human_Parsing\figure_6.jpg
  Figure 6 caption: Instance-level body part parsing results (Section IV-D) on MHP-v2
    [14] val, DensePose-COCO [29] test and PASCAL-Person-Part [6] test.
  Figure 7 Link: articels_figures_by_rev_year\2023\Differentiable_MultiGranularity_Human_Parsing\figure_7.jpg
  Figure 7 caption: Body part parsing and pose estimation results (Section IV-D) in
    challenging scenarios with crowded, occlusions, pose variations, etc.
  Figure 8 Link: articels_figures_by_rev_year\2023\Differentiable_MultiGranularity_Human_Parsing\figure_8.jpg
  Figure 8 caption: Typical failure cases on DensePose-COCO [29] test.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.69
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Tianfei Zhou
  Name of the last author: Wenguan Wang
  Number of Figures: 8
  Number of Tables: 8
  Number of authors: 3
  Paper title: Differentiable Multi-Granularity Human Parsing
  Publication Date: 2023-01-30 00:00:00
  Table 1 caption:
    table_text: "TABLE I Quantitative Comparison on MHP-v2 [14] val, With mIoU, AP\
      \ p p and PCP. \u2020 \u2020"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE II Quantitative Comparison on DensePose-COCO [29] Test, With\
      \ mIoU, AP p p and PCP. \u2020 \u2020"
  Table 3 caption:
    table_text: "TABLE III Quantitative Comparison on PASCAL-Person-Part [6] Test,\
      \ With AP r r. \u2020 \u2020"
  Table 4 caption:
    table_text: TABLE IV Quantitative Comparison of Bottom-Up Multi-Person Pose Estimators
      on COCO 2017 [30] val. See Section IV-B
  Table 5 caption:
    table_text: TABLE V Runtime Comparison on MHP-v2 [14] val With Six Persons Per
      Image on Average
  Table 6 caption:
    table_text: TABLE VI Ablation Study on DSPF Estimation (Section III-B) on MHP-v2
      [14] val
  Table 7 caption:
    table_text: TABLE VII Study of Keypoint Association Algorithms (Section III-C)
      on MHP-v2 [14] val
  Table 8 caption:
    table_text: "TABLE VIII Hyper-Parameter Analysis of \u03B7 \u03B7, \u03C4 \u03C4\
      \ and M M in Algorithm 2"
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3239194
- Affiliation of the first author: school of automation science and engineering, xi'an
    jiaotong university, xi'an, shaanxi, china
  Affiliation of the last author: institute of automation, chinese academy of sciences,
    beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2023\Attention_Spiking_Neural_Networks\figure_1.jpg
  Figure 1 caption: The Conv-based SNN layer and the overview of MA-SNN. (a) Conv-based
    LIF-SNN layer. (b) Multi-dimensional attention SNN.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Attention_Spiking_Neural_Networks\figure_2.jpg
  Figure 2 caption: Illustration of different attention dimensions. (a) Temporal-wise
    attention. (b) Channel-wise attention. (c) Spatial-wise attention.
  Figure 3 Link: articels_figures_by_rev_year\2023\Attention_Spiking_Neural_Networks\figure_3.jpg
  Figure 3 caption: 'Attention residual block contains three parts: basic Res-SNN
    block (MS-Res-SNN [12], details in Fig. 5), shortcut, and CSA module. We exploit
    MA on the basic Res-SNN block outputs (i.e., membrane potential of spiking neurons)
    in each block. We recommend Att-Res-SNN-1 as the scheme for attention residual
    learning.'
  Figure 4 Link: articels_figures_by_rev_year\2023\Attention_Spiking_Neural_Networks\figure_4.jpg
  Figure 4 caption: 'Attention Locations in Plain SNN. From left to right: baseline
    CNN, processing spatial information. Attention CNN, performing attention module
    after activation [19], [25]. Vanilla SNN, processing spatio-temporal information.
    Conv-PRE Attention SNN, inserting attention modules before the Conv operation.
    Conv-POST, acting attention modules after the Conv operation while before the
    spatio-temporal integration. Activate-PRE, executing attention modules on the
    integrated membrane potential. MA-SNN, our recommended attention locations of
    three dimensions in plain SNN.'
  Figure 5 Link: articels_figures_by_rev_year\2023\Attention_Spiking_Neural_Networks\figure_5.jpg
  Figure 5 caption: 'Attention Residual Learning for SNNs. From left to right: Res-CNN
    [2]. Classic Att-Res-CNN [19], [25]. Vanilla Res-SNN [13], executing the same
    shortcut and residual block as Res-CNN. SEW-Res-SNN [14], mainly building shortcuts
    between spikes from different layers. MS-Res-SNN [12], constructing a shortcut
    among membrane potential of spiking neurons in different layers. Att-Res-SNN-1
    (our recommended method) and Att-Res-SNN-2, both select MS-Res-SNN as the backbone.
    The former performs attention between the residual block and shortcut connection,
    which is the same as classic Att-Res-CNN. The latter executes attention after
    the shortcut connection.'
  Figure 6 Link: articels_figures_by_rev_year\2023\Attention_Spiking_Neural_Networks\figure_6.jpg
  Figure 6 caption: Case study on DVS128 Gesture. We can observe that attention drives
    SNNs to focus on the target while the vanilla model shows more decentralized spiking
    activations. More case studies are given in the Supplementary Materials, available
    online.
  Figure 7 Link: articels_figures_by_rev_year\2023\Attention_Spiking_Neural_Networks\figure_7.jpg
  Figure 7 caption: "Visualization of overall spiking response on Gait. From top to\
    \ bottom: input event frames on Gait. Visualization of spiking response features\
    \ in vanilla SNN and TCA-SNN, where t=1,25,49 and n=1 . Each pixel on the feature\
    \ represents the spiking activity rate of one neuron over the whole validation\
    \ set. In each sample, we randomly select an event stream with a time window of\
    \ 900 ms as the input to the network. The starting point of each time window is\
    \ random. For a single channel, the redder the pixel, the higher spiking activity\
    \ rate; the bluer the pixel, the closer the spiking activity rate is to 0. The\
    \ dimension of S \xAF \xAF \xAF \xAF t,1 (first layer) is (32,32,64) at each time\
    \ step, and we depict all 64 channels. We can clearly observe that attention drives\
    \ the network to focus on the target and suppress the redundant background channels.\
    \ Suppressing the background channel will significantly reduce the SNN energy\
    \ cost, since the background contains a high spiking activity rate."
  Figure 8 Link: articels_figures_by_rev_year\2023\Attention_Spiking_Neural_Networks\figure_8.jpg
  Figure 8 caption: Visualization of overall spiking response on ImageNet-1 K. (a)
    Earlier stage. In contrast to the vanilla model, some dark blue (all neurons in
    these channels are not spiking at all) channels appear in the stage 1 of CSA-Res-SNN,
    which induces great energy efficiency. There are 17 dark blue channels in layer
    2. (b) Last stage. With the assistance of attention, features (red regions) of
    the last Conv layer (layer 33) become more diverse. (a) Earlier stage of overall
    spiking response. (b) Last stage of overall spiking response.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.96
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.66
  Name of the first author: Man Yao
  Name of the last author: Guoqi Li
  Number of Figures: 8
  Number of Tables: 7
  Number of authors: 8
  Paper title: Attention Spiking Neural Networks
  Publication Date: 2023-01-31 00:00:00
  Table 1 caption:
    table_text: "TABLE I Comparison With Previous Works on Gesture and Gait. The Numbers\
      \ or Percentages in Brackets Denote the Performance Improvement or Reduction\
      \ Proportions of Spiking Activity Over the Re-Implementation baseline [40] and\
      \ [52]. The Format of \u201CA \xB1 a\u201D Represents the Mean and Variance\
      \ of the Accuracy of Repeat Ten Independent Experiments"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE II Comparison With Previous Works on ImageNet-1 K. the Input\
      \ Crops are Enlarged to 288 \xD7 288 in Inference. The Default Inference Input\
      \ Resolution for Other Models is 224 \xD7 224. In ResNet-104, We Set T=4 T=4,\
      \ Which is Inconsistent With the Baseline Model ( T=5 T=5), Because We Found\
      \ That Continuing to Increase T T in Our Model Does Not Increase Performance.\
      \ In Addition to the Residual Architecture, MA Also Works Well in the VGG-Based\
      \ SNN Architecture. Due to Space Constraints, We Put This Part of the Experimental\
      \ Results in the Section S3.2 of Supplementary Materials, available online"
  Table 3 caption:
    table_text: "TABLE III Comparison With Baselines on ImageNet-1 K (Inference Spatial\
      \ Resolution is 224\xD7224 224\xD7224). Based on the MS-Res-SNN [12], We Re-Implement\
      \ Various Baseline Structures and Their Attention Counterpart With T=1 T=1 and\
      \ Report Corresponding Performance and Energy Shift. The Case of T=1 T=1 Can\
      \ Be Regarded as Pre-Training of Multi-Time Step SNNs [58]. Note That, Compared\
      \ With Attention CNNs, the Accuracy Improvement of the Attention on MS-Res-SNN\
      \ is Very Significant, E.g., Using Identical CSA for Res-CNN-34 [25] and Res-SNN-34\
      \ Can Improve the Accuracy by +0.7 and +5.0 Percent, Respectively"
  Table 4 caption:
    table_text: TABLE IV Effect of Attention Locations in Three-Layer SNN [40] at
      Different Dimensions on Gesture With dt=15,T=60 dt=15,T=60
  Table 5 caption:
    table_text: TABLE V Effect of Different Attention Modules in Three-Layer SNN [40]
      on Gesture With dt=15,T=60 dt=15,T=60. Note, Here is Only a Rough Estimate Without
      Additional Operations Such as Max-Pooling and Avg-Pooling
  Table 6 caption:
    table_text: TABLE VI Effect of Attention Dimensions in Three-Layer Conv-Based
      SNN [40] on Gesture With dt=15,T=60 dt=15,T=60
  Table 7 caption:
    table_text: "TABLE VII \u03D5( JJ T ) \u03C6(JJT) and \u03C6( JJ T ) \u03D5(JJT)\
      \ of ReLU, Conv, Orthogonal, and Sigmoid Operations in Neural Networks. Results\
      \ are Collected From Lemma S3 and Lemma S4 in Section S4"
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3241201
- Affiliation of the first author: college of intelligence science and technology,
    national university of defense technology, changsha, china
  Affiliation of the last author: department of radiology, third xiangya hospital,
    central south university, changsha, china
  Figure 1 Link: articels_figures_by_rev_year\2023\SSTBN_A_SemiSupervised_TriBranch_Network_for_COVID_Screening_and_Lesion_Segmenta\figure_1.jpg
  Figure 1 caption: 'The overall framework of the proposed SS-TBN. (a) The overall
    network consists of three branches: a pixel-level segmentation branch based on
    U-Net, a slice-level classification based on ResNet-50, and an individual-level
    diagnosis branch based on a four-layer fully connected network. The A symbols
    with a circle correspond to the LA modules. (b) Internal structure of the LA modules,
    which are introduced before each residual block to combine classification features
    with segmentation features of corresponding scales using an attention mechanism.
    In the LA modules, we first use 1x1 Conv ( Wcls , and Wseg ) to reduce the Icls
    and Iseg to the same dimension (64, 128, 256, and 512, respectively), and then
    use f1 , Wint , and f2 to extract spatial attention maps, the values of which
    are within the range of [0, 1]. If a pixel is more likely located within a lesion
    area, the corresponding value will be greater. Finally, each attention map is
    pixel-wise multiplied by the corresponding Icls to make the output features focus
    more on lesion areas.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\SSTBN_A_SemiSupervised_TriBranch_Network_for_COVID_Screening_and_Lesion_Segmenta\figure_2.jpg
  Figure 2 caption: "Manual and AI-based segmentation of lung lesions in CT images.\
    \ The first row shows the input lung images, the second row shows manual lesion\
    \ annotations, and the third, fourth, and last rows show the segmentation results\
    \ of basic TBN (U-Net), SS-TBN \u2212 (single-threshold pseudo labeling), and\
    \ SS-TBN (double-threshold pseudo labeling), respectively. The red and yellow\
    \ circles represent lesions that are missed and incorrectly detected by other\
    \ methods, respectively."
  Figure 3 Link: articels_figures_by_rev_year\2023\SSTBN_A_SemiSupervised_TriBranch_Network_for_COVID_Screening_and_Lesion_Segmenta\figure_3.jpg
  Figure 3 caption: The performance of different methods in the segmentation of lung
    lesions with different sizes. It shows the potential of our proposed method in
    segmentation of minor lesions.
  Figure 4 Link: articels_figures_by_rev_year\2023\SSTBN_A_SemiSupervised_TriBranch_Network_for_COVID_Screening_and_Lesion_Segmenta\figure_4.jpg
  Figure 4 caption: ROC curve of the proposed SS-TBN and radiologists in pixel-level
    lesion segmentation. It shows that our proposed method performs better than one
    radiologist, and is comparable with two other radiologists.
  Figure 5 Link: articels_figures_by_rev_year\2023\SSTBN_A_SemiSupervised_TriBranch_Network_for_COVID_Screening_and_Lesion_Segmenta\figure_5.jpg
  Figure 5 caption: ROC curves of COVID-19 classification. (a) Slice-level infection
    classification on the internal dataset. (b) Individual-level classification on
    the internal dataset. (c) Individual-level classification on the HNC external
    dataset. (d) Individual-level classification on the entire iCTCF external database.
  Figure 6 Link: articels_figures_by_rev_year\2023\SSTBN_A_SemiSupervised_TriBranch_Network_for_COVID_Screening_and_Lesion_Segmenta\figure_6.jpg
  Figure 6 caption: Slice-level infection classification performance of different
    methods based on CT image slices with different lesion sizes. It shows the potential
    of our proposed method in the COVID-19 screening based on CT image slices with
    minor lesions.
  Figure 7 Link: articels_figures_by_rev_year\2023\SSTBN_A_SemiSupervised_TriBranch_Network_for_COVID_Screening_and_Lesion_Segmenta\figure_7.jpg
  Figure 7 caption: Attention maps from lesion attention modules on the internal dataset.
    The attention maps promote interpretability for the classification.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.67
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Ling-Li Zeng
  Name of the last author: Wei Wang
  Number of Figures: 7
  Number of Tables: 8
  Number of authors: 7
  Paper title: 'SS-TBN: A Semi-Supervised Tri-Branch Network for COVID-19 Screening
    and Lesion Segmentation'
  Publication Date: 2023-01-31 00:00:00
  Table 1 caption:
    table_text: TABLE I CT Image Data Composition and Sources
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Comparison in Run-Time, Parameters, and MACs Between Our
      Method and Others
  Table 3 caption:
    table_text: TABLE III The Performance of Different Methods in the Segmentation
      of Lung Lesions on the Internal Dataset
  Table 4 caption:
    table_text: TABLE IV Classification Results of Different Methods on the Internal
      Dataset
  Table 5 caption:
    table_text: TABLE V Individual-Level Classification Results of Different Methods
      With Different Labeled Training Sample Sizes on the Internal Dataset
  Table 6 caption:
    table_text: TABLE VI Individual-Level Classification Results of Different Methods
      on the HNC External Dataset
  Table 7 caption:
    table_text: TABLE VII Individual-Level Classification Results of Different Methods
      on the iCTCF External Datasets
  Table 8 caption:
    table_text: TABLE VIII Individual-Level Classification Results WithWithout Domain
      Adaptation (DA) on the Internal Dataset
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3240886
- Affiliation of the first author: division of life sciences and medicine, university
    of science and technology of china, hefei, china
  Affiliation of the last author: school of physical sciences, university of science
    and technology of china, hefei, china
  Figure 1 Link: articels_figures_by_rev_year\2023\DeepEIT_Deep_Image_Prior_Enabled_Electrical_Impedance_Tomography\figure_1.jpg
  Figure 1 caption: Flowchart of the DIP based EIT reconstruction method.
  Figure 10 Link: articels_figures_by_rev_year\2023\DeepEIT_Deep_Image_Prior_Enabled_Electrical_Impedance_Tomography\figure_10.jpg
  Figure 10 caption: Comparison of the proposed DeepEIT framework with and without
    explicit TV regularization.
  Figure 2 Link: articels_figures_by_rev_year\2023\DeepEIT_Deep_Image_Prior_Enabled_Electrical_Impedance_Tomography\figure_2.jpg
  Figure 2 caption: Neural network architecture. The architecture is based on the
    popular U-net with skip connections ns=[0,0,0,4,4] between the down-sampling layers
    and up-sampling layers.
  Figure 3 Link: articels_figures_by_rev_year\2023\DeepEIT_Deep_Image_Prior_Enabled_Electrical_Impedance_Tomography\figure_3.jpg
  Figure 3 caption: Examples of target photographs and masks that are used to generate
    the inclusion conductivity distribution for computing the performance metrics.
  Figure 4 Link: articels_figures_by_rev_year\2023\DeepEIT_Deep_Image_Prior_Enabled_Electrical_Impedance_Tomography\figure_4.jpg
  Figure 4 caption: 'Case 1: results of the simulated heart-and-lungs phantom. Initial
    input: random noise image z ; Output: fhattheta (z) ; sigma DIP : DeepEIT based
    reconstruction, i.e., sigma DIP=P(fhattheta (z)) . sigma NOSER : NOSER based reconstruction;
    sigma L2 and sigma TV : reference estimations using smoothness prior and total
    variation, respectively.'
  Figure 5 Link: articels_figures_by_rev_year\2023\DeepEIT_Deep_Image_Prior_Enabled_Electrical_Impedance_Tomography\figure_5.jpg
  Figure 5 caption: "High resolution boundary extraction by mapping the output f \u03B8\
    \ (z) into meshes M i , i\u22651 is the mesh density level, the larger the level\
    \ the finer the mesh."
  Figure 6 Link: articels_figures_by_rev_year\2023\DeepEIT_Deep_Image_Prior_Enabled_Electrical_Impedance_Tomography\figure_6.jpg
  Figure 6 caption: 'Case 2: experimental study with a rectangle target. Otherwise
    as in Fig. 4.'
  Figure 7 Link: articels_figures_by_rev_year\2023\DeepEIT_Deep_Image_Prior_Enabled_Electrical_Impedance_Tomography\figure_7.jpg
  Figure 7 caption: 'Case 3: experimental study with one rectangular target and one
    triangular target. Otherwise as in Fig. 4.'
  Figure 8 Link: articels_figures_by_rev_year\2023\DeepEIT_Deep_Image_Prior_Enabled_Electrical_Impedance_Tomography\figure_8.jpg
  Figure 8 caption: 'Case 4: experimental study with one rectangular target and one
    cylindrical bar target. Otherwise as in Fig. 4.'
  Figure 9 Link: articels_figures_by_rev_year\2023\DeepEIT_Deep_Image_Prior_Enabled_Electrical_Impedance_Tomography\figure_9.jpg
  Figure 9 caption: 'Case 5: experimental study with three targets. Otherwise as in
    Fig. 4.'
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: Dong Liu
  Name of the last author: Jiangfeng Du
  Number of Figures: 14
  Number of Tables: 1
  Number of authors: 6
  Paper title: 'DeepEIT: Deep Image Prior Enabled Electrical Impedance Tomography'
  Publication Date: 2023-02-01 00:00:00
  Table 1 caption:
    table_text: TABLE I Performance Metrics of Simulation and Experimental Studies
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3240565
- Affiliation of the first author: department of electrical and computer engineering,
    university of california san diego, la jolla, ca, usa
  Affiliation of the last author: department of electrical and computer engineering,
    university of california san diego, la jolla, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2023\A_Generalized_Explanation_Framework_for_Visualization_of_Deep_Learning_Model_Pre\figure_1.jpg
  Figure 1 caption: An ideal explainable deep learning system should produce various
    explanations to satisfy different user requirements. GALORE addresses this problem
    by unifying attributive (center), deliberative (left), and counterfactual (right)
    explanations. Attributive explanations highlight the pixels responsible for the
    prediction of the label 'Cardinal' for the image shown. Deliberative explanations
    address the why question, producing a set of insecurities, which are image regions
    deemed ambiguous, together with the classes that define the ambiguity. Counterfactual
    explanations address the why not question, visualizing the input changes needed
    to elicit the prediction of a user-provided counter class ('Summer Tanager').
  Figure 10 Link: articels_figures_by_rev_year\2023\A_Generalized_Explanation_Framework_for_Visualization_of_Deep_Learning_Model_Pre\figure_10.jpg
  Figure 10 caption: Robustness of GALORE to image shifts on CUB200.
  Figure 2 Link: articels_figures_by_rev_year\2023\A_Generalized_Explanation_Framework_for_Visualization_of_Deep_Learning_Model_Pre\figure_2.jpg
  Figure 2 caption: 'Left: Illustration of the deliberations made by a human to categorize
    an ambiguous image. Insecurities are ambiguous regions. Right: Deliberative explanations
    expose this deliberative process. Unlike attributions, which simply attribute
    the prediction to image regions (left), they expose the insecurities experienced
    by the classifier while reaching that prediction (center). Each insecurity consists
    of an image region and an ambiguity, expressed as a pair of classes to which the
    region appears to belong to. Examples from the confusing classes are shown in
    the right.'
  Figure 3 Link: articels_figures_by_rev_year\2023\A_Generalized_Explanation_Framework_for_Visualization_of_Deep_Learning_Model_Pre\figure_3.jpg
  Figure 3 caption: A counterfactual explanation is derived from a pair of discriminant
    explanations. Given a query image (Cardinal) and a counterfactual class (Summer
    Tanager), discriminant explanations are obtained by combining attribution maps
    for each of the two classes and the confidence score. In this way, they bridge
    the gap between attributive and counterfactual explanations, enabling fast optimization-free
    computation of the latter.
  Figure 4 Link: articels_figures_by_rev_year\2023\A_Generalized_Explanation_Framework_for_Visualization_of_Deep_Learning_Model_Pre\figure_4.jpg
  Figure 4 caption: 'Multiclass deliberative explanation of dimension V=3 . Left (top
    and green): image x and candidate class set C= ''Philadelphia Vireo'' (PV), ''Worm
    Eating Warbler'' (WEW), ''Red Eyed Vireo'' (BEV), ''Common Yellow Throat'' (CYT)),
    ''Tropical Kingbird (TK)''. Center: attributions to each of the classes in C .
    Right: ambiguity map computed for each 3-tuples ambiguous classes in the candidate
    class ambiguity set A with (12) and resulting segmentation. For brevity, only
    six of the ten 3-tuples are randomly selected for display in the figure.'
  Figure 5 Link: articels_figures_by_rev_year\2023\A_Generalized_Explanation_Framework_for_Visualization_of_Deep_Learning_Model_Pre\figure_5.jpg
  Figure 5 caption: "Multi-class counterfactual explanation of dimension V=2 . Left:\
    \ query image of a 'Scarlet Tanager' (upper left) and two images randomly selected\
    \ from the counterfactual class set C= 'Red Headed Woodpecker,' 'Summer Tanager'\
    \ . Middle-left: attributive maps are computed for the query and each of counter\
    \ images, with respect to class predictions h y \u2217 and h y v , y v \u2208\
    C and confidence predictor s . Middle-right: attributive maps are combined with\
    \ (14) to generate discriminant explanations. Right: discriminant explanations\
    \ are thresholded to generate a multi-class counterfactual explanation."
  Figure 6 Link: articels_figures_by_rev_year\2023\A_Generalized_Explanation_Framework_for_Visualization_of_Deep_Learning_Model_Pre\figure_6.jpg
  Figure 6 caption: "GALORE explanation architecture ( x : Cardinal, x c : Summer\
    \ Tanager.). Feature activations F h and F s are computed for pre-determined layers\
    \ of the classifier (upper branch) and confidence predictor (lower branch), respectively.\
    \ Attributions for prediction h a , ambiguous or counter class h b , and confidence\
    \ score s are computed by attribution functions q(.,.) according to (18), (19),\
    \ (20). a , b are a class pair of candidate class ambiguities set for deliberative\
    \ explanations, and y \u2217 , y c for counterfactual explanations. These attributions\
    \ are combined with (5) or (11) to obtain the final map, which is thresholded\
    \ to produce explanations. Multiple pairs (a,b) are shown for deliberative explanations,\
    \ where a is Cardinal (Ca), and b Pine Grosbeak (PG), Purple Finch (PF) or Summer\
    \ Tanager (ST). Counterfactual explanations are obtained by additionally reversing\
    \ the roles of x and x c and thresholding the discriminant heat maps."
  Figure 7 Link: articels_figures_by_rev_year\2023\A_Generalized_Explanation_Framework_for_Visualization_of_Deep_Learning_Model_Pre\figure_7.jpg
  Figure 7 caption: 'Effect of confidence scores on precision-recall curves and IoU
    of different GALORE explanations. Top: on CUB200. Bottom: ADE20 K.'
  Figure 8 Link: articels_figures_by_rev_year\2023\A_Generalized_Explanation_Framework_for_Visualization_of_Deep_Learning_Model_Pre\figure_8.jpg
  Figure 8 caption: 'Impact of attribution function (left) and network architecture
    (right) on GALORE explanation performance. Top: precision-recall on CUB200. Bottom:
    IoU on ADE20 K.'
  Figure 9 Link: articels_figures_by_rev_year\2023\A_Generalized_Explanation_Framework_for_Visualization_of_Deep_Learning_Model_Pre\figure_9.jpg
  Figure 9 caption: Precision-recall of multi-class explanations on CUB200.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Pei Wang
  Name of the last author: Nuno Vasconcelos
  Number of Figures: 18
  Number of Tables: 3
  Number of authors: 2
  Paper title: A Generalized Explanation Framework for Visualization of Deep Learning
    Model Predictions
  Publication Date: 2023-02-01 00:00:00
  Table 1 caption:
    table_text: TABLE I Implementation of Different Explanation Strategies Under the
      GALORE Framework
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE II Pearson Correlation Coefficient ( \u03C1 \u03C1) and p-Value\
      \ Between Segment Strength and Quality of the Explanation on CUB200"
  Table 3 caption:
    table_text: 'TABLE III Comparison to the State of the Art in Counterfactual Explanations.
      (IPS: Images per Second, Implemented on NVIDIA TITAN Xp. Results are Omitted
      for the CounteRGAN [63] Due to the Very Long Training Times it Requires.) Results
      are Shown as Mean(stddev)'
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3241106
- Affiliation of the first author: university of science and technology of china,
    hefei, china
  Affiliation of the last author: university of science and technology of china, hefei,
    china
  Figure 1 Link: articels_figures_by_rev_year\2023\Continual_Image_Deraining_With_Hypergraph_Convolutional_Networks\figure_1.jpg
  Figure 1 caption: The overall architecture of our deraining network. The network
    consists of nine dilated residual dense (DRD) units, in which multiscale dilated
    convolutions are adopted to capture the local spatial information. Symmetrical
    skip-layer connections with hypergraph convolutional modules are deployed to propagate
    the shallow features and nonlocal content information to the deep layers.
  Figure 10 Link: articels_figures_by_rev_year\2023\Continual_Image_Deraining_With_Hypergraph_Convolutional_Networks\figure_10.jpg
  Figure 10 caption: Visual effect of our hypergraph convolutional modules.
  Figure 2 Link: articels_figures_by_rev_year\2023\Continual_Image_Deraining_With_Hypergraph_Convolutional_Networks\figure_2.jpg
  Figure 2 caption: Architectures of the dilated residual dense (DRD) unit, which
    contains three dense connected convolutions with different dilation factors. oplus
    denotes elementwise addition.
  Figure 3 Link: articels_figures_by_rev_year\2023\Continual_Image_Deraining_With_Hypergraph_Convolutional_Networks\figure_3.jpg
  Figure 3 caption: (a) The detailed illustration of the patch embedding operation.
    (b) The architecture of our proposed hypergraph convolutional module with annotations.
    First, the input feature map is embedded into small patches. Second, these patches
    are gathered to obtain the incidence matrix mathbfH . Third, the hypergraph convolution
    is calculated using mathbfH to update the embedded patches. Finally, the attention
    map is obtained by unembedding the updated patches. oplus denotes element-wise
    addition, otimes denotes matrix multiplication.
  Figure 4 Link: articels_figures_by_rev_year\2023\Continual_Image_Deraining_With_Hypergraph_Convolutional_Networks\figure_4.jpg
  Figure 4 caption: Pipeline of our continual learning algorithm. The training session
    returns the current student model as the teacher model to supervise the future
    student model. Then, on the new dataset, the student model is dynamically expanded
    according to Inc(n) and supervised by the teacher model. Only some of the increased
    parameters are updated. When the data sequence is traversed, the final trained
    model, i.e., the Student Model, is evaluated on all the seen datasets and unseen
    data.
  Figure 5 Link: articels_figures_by_rev_year\2023\Continual_Image_Deraining_With_Hypergraph_Convolutional_Networks\figure_5.jpg
  Figure 5 caption: The network expands dynamically when new data arrive.
  Figure 6 Link: articels_figures_by_rev_year\2023\Continual_Image_Deraining_With_Hypergraph_Convolutional_Networks\figure_6.jpg
  Figure 6 caption: Visual comparison of a synthetic rainy image from the Rain200H
    dataset [30]. Please zoom in for better visualization of the heavy rain removal
    and face recovery. CL and IL represent continual learning and independent learning,
    respectively.
  Figure 7 Link: articels_figures_by_rev_year\2023\Continual_Image_Deraining_With_Hypergraph_Convolutional_Networks\figure_7.jpg
  Figure 7 caption: Visual comparison on a synthetic rainy image from the DDN-Data
    dataset [51]. Please zoom in for better visualization of the rain removal and
    details preservation. CL and IL represent continual learning and independent learning,
    respectively.
  Figure 8 Link: articels_figures_by_rev_year\2023\Continual_Image_Deraining_With_Hypergraph_Convolutional_Networks\figure_8.jpg
  Figure 8 caption: Visual comparison on a real image from the SPA-Data dataset [57].
    Please zoom in for better visualization on the rain removal.
  Figure 9 Link: articels_figures_by_rev_year\2023\Continual_Image_Deraining_With_Hypergraph_Convolutional_Networks\figure_9.jpg
  Figure 9 caption: Visual comparison on a real image from the internet. Please zoom
    in for better visualization of the structure preservation.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.62
  Name of the first author: Xueyang Fu
  Name of the last author: Zheng-Jun Zha
  Number of Figures: 17
  Number of Tables: 12
  Number of authors: 6
  Paper title: Continual Image Deraining With Hypergraph Convolutional Networks
  Publication Date: 2023-02-02 00:00:00
  Table 1 caption:
    table_text: "TABLE I Complexity Analysis of Our Hypergraph Convolutional Module.\
      \ B B: Batch Size, H\xD7W\xD7C H\xD7W\xD7C: Feature Resolution, p\xD7p p\xD7\
      p: Patch Size"
  Table 10 caption:
    table_text: TABLE X Quantitative Results of Performing Our Continual Learning
      on Different Deraining Networks
  Table 2 caption:
    table_text: TABLE II Comparison of the Average PSNR | | SSIM Values on Five Synthetic
      Benchmark Datasets. The Best and the Second Best Results are Boldfaced and underlined
  Table 3 caption:
    table_text: TABLE III Quantitative Comparisons on the Real-World SPA-Data Dataset.
      The Best and the Second Best Results are Boldfaced and underlined
  Table 4 caption:
    table_text: TABLE IV Ablation Study on the Hypergraph Convolution
  Table 5 caption:
    table_text: TABLE V Comparisons With NLEDN [67] and IPT [97] on the Rain100 L
      Dataset
  Table 6 caption:
    table_text: TABLE VI PSNR | | SSIM Results of the Ultimate Model
  Table 7 caption:
    table_text: TABLE VII Comparison of Quantitative Results in Terms of PSNR and
      SSIM. The Network is Trained Sequentially on the Different Orders of Data Sequences
      and the Mixed Dataset, Respectively
  Table 8 caption:
    table_text: TABLE VIII Ablation Study on Different Learning Strategy
  Table 9 caption:
    table_text: TABLE IX Quantitative Results on Different Degradations
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3241756
- Affiliation of the first author: graduate school of information science and technology,
    university of tokyo, tokyo, japan
  Affiliation of the last author: national engineering research center of visual technology,
    school of computer science, peking university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2023\Hybrid_High_Dynamic_Range_Imaging_fusing_Neuromorphic_and_Conventional_Images\figure_1.jpg
  Figure 1 caption: "The \u201CNeurImg\u201D hybrid images fusion framework merges\
    \ a low-resolution, grayscale intensity map captured by a neuromorphic camera\
    \ with a high-resolution, colorful LDR image from a conventional camera. Previous\
    \ NeurImg-HDR [14] only supports HDR images at the resolution of 512times 512\
    \ . The improved NeurImg-HDR+ can produce HDR videos with higher spatial resolution\
    \ up to 3200times 2000 ."
  Figure 10 Link: articels_figures_by_rev_year\2023\Hybrid_High_Dynamic_Range_Imaging_fusing_Neuromorphic_and_Conventional_Images\figure_10.jpg
  Figure 10 caption: Real data results reconstructed by NeurImg-HDR+. The LDR images
    are captured by conventional cameras and the intensity maps are acquired by DAVIS
    (the left three cases) and spike camera (the right three cases), respectively.
  Figure 2 Link: articels_figures_by_rev_year\2023\Hybrid_High_Dynamic_Range_Imaging_fusing_Neuromorphic_and_Conventional_Images\figure_2.jpg
  Figure 2 caption: 'The conceptual pipeline of NeurImg fusion process, which consists
    of four steps: color space conversion of the LDR image, spatial upsampling of
    the intensity map, luminance fusion to produce HDR image in luminance domain,
    and chrominance compensation that refills the color information to get a colorful
    HDR result.'
  Figure 3 Link: articels_figures_by_rev_year\2023\Hybrid_High_Dynamic_Range_Imaging_fusing_Neuromorphic_and_Conventional_Images\figure_3.jpg
  Figure 3 caption: A real example of fusing an intensity map and an LDR image using
    a linear ramp as the weighting function. Such a straightforward fusion strategy
    results in various unpleasant artifacts, such as color distortion in the insets.
  Figure 4 Link: articels_figures_by_rev_year\2023\Hybrid_High_Dynamic_Range_Imaging_fusing_Neuromorphic_and_Conventional_Images\figure_4.jpg
  Figure 4 caption: Overview of NeurImg-HDR+ network architecture. It contains upsampling
    network, luminance fusion network, and chrominance compensation network.
  Figure 5 Link: articels_figures_by_rev_year\2023\Hybrid_High_Dynamic_Range_Imaging_fusing_Neuromorphic_and_Conventional_Images\figure_5.jpg
  Figure 5 caption: Examples of attention mask calculated from self-attention module,
    which assigns different weights for each pixel and reserves useful information
    for fusion.
  Figure 6 Link: articels_figures_by_rev_year\2023\Hybrid_High_Dynamic_Range_Imaging_fusing_Neuromorphic_and_Conventional_Images\figure_6.jpg
  Figure 6 caption: The architecture of recurrent block in chrominance compensation
    network.
  Figure 7 Link: articels_figures_by_rev_year\2023\Hybrid_High_Dynamic_Range_Imaging_fusing_Neuromorphic_and_Conventional_Images\figure_7.jpg
  Figure 7 caption: The prototype of our hybrid camera, which is composed of a conventional
    RGB camera and a neuromorphic camera. Radiance information is recorded simultaneously
    by two sensors.
  Figure 8 Link: articels_figures_by_rev_year\2023\Hybrid_High_Dynamic_Range_Imaging_fusing_Neuromorphic_and_Conventional_Images\figure_8.jpg
  Figure 8 caption: "Comparison between the proposed method and state-of-the-art deep\
    \ learning based inverse tone mapping methods: Liu et al. [27] and Santos et al.\
    \ [42]. We also compare with NeurImg-HDR [14] and a state-of-the-art approach\
    \ [5] of merging two LDR images, denoted as LDR \xD72 . The Q-Scores of HDR-VDP\
    \ [33] are displayed in each image. Please zoom-in on the electronic versions\
    \ for better details."
  Figure 9 Link: articels_figures_by_rev_year\2023\Hybrid_High_Dynamic_Range_Imaging_fusing_Neuromorphic_and_Conventional_Images\figure_9.jpg
  Figure 9 caption: Comparisons on quality maps calculated from HDR-VDP evaluation
    metrics [33]. Visual differences increase from blue to red in the quality maps.
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.55
  Name of the first author: Jin Han
  Name of the last author: Boxin Shi
  Number of Figures: 11
  Number of Tables: 4
  Number of authors: 9
  Paper title: Hybrid High Dynamic Range Imaging fusing Neuromorphic and Conventional
    Images
  Publication Date: 2023-02-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Detailed Specifications on Spatial Resolution, Frame Rate
      (FR) and Dynamic Range (DR) of Our Hybrid Camera
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Evaluations of the Proposed NeuImg-HDR+ and Comparing
      Methods
  Table 3 caption:
    table_text: TABLE 3 Quantitative Comparison on Real-World Data
  Table 4 caption:
    table_text: TABLE 4 Quantitative Comparison of Different Variants of the Proposed
      Method
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3231334
- Affiliation of the first author: national laboratory of pattern recognition, institute
    of automation, chinese academy of sciences, beijing, china
  Affiliation of the last author: department of engineering science, university of
    oxford, oxford, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2023\SiamMask_A_Framework_for_Fast_Online_Object_Tracking_and_Segmentation\figure_1.jpg
  Figure 1 caption: Our method addresses both tasks of visual tracking and video object
    segmentation to achieve high practical convenience. Like conventional object trackers
    such as [22] (red), it relies on a simple bounding box initialization (blue) and
    operates online. However, SiamMask (green) is able to produce binary segmentation
    masks (out of which we can infer rotated bounding-boxes) that much more accurately
    describe the target object.
  Figure 10 Link: articels_figures_by_rev_year\2023\SiamMask_A_Framework_for_Fast_Online_Object_Tracking_and_Segmentation\figure_10.jpg
  Figure 10 caption: Comparison in terms of mean IOU and speed (fps) between SiamMask
    and popular fast video object segmentation algorithms on the DAVIS-2016 dataset.
  Figure 2 Link: articels_figures_by_rev_year\2023\SiamMask_A_Framework_for_Fast_Online_Object_Tracking_and_Segmentation\figure_2.jpg
  Figure 2 caption: "Schematic illustration of the two-branch variant of SiamMask,\
    \ based on [23]. \u22C6 d denotes depth-wise cross correlation."
  Figure 3 Link: articels_figures_by_rev_year\2023\SiamMask_A_Framework_for_Fast_Online_Object_Tracking_and_Segmentation\figure_3.jpg
  Figure 3 caption: Schematic illustration of the three-branch variant of SiamMask,
    based on [24].
  Figure 4 Link: articels_figures_by_rev_year\2023\SiamMask_A_Framework_for_Fast_Online_Object_Tracking_and_Segmentation\figure_4.jpg
  Figure 4 caption: Schematic illustration of the stacked refinement modules. For
    a more detailed version of refinement modules, see Fig. 5.
  Figure 5 Link: articels_figures_by_rev_year\2023\SiamMask_A_Framework_for_Fast_Online_Object_Tracking_and_Segmentation\figure_5.jpg
  Figure 5 caption: Schematic illustration of one of the three refinement modules
    ( U 3 ).
  Figure 6 Link: articels_figures_by_rev_year\2023\SiamMask_A_Framework_for_Fast_Online_Object_Tracking_and_Segmentation\figure_6.jpg
  Figure 6 caption: 'In order to generate a bounding box from a binary mask (in yellow),
    we experiment with three different methods. Min-max: the axis-aligned rectangle
    containing the object (red); MBR: the minimum bounding rectangle (green); Opt:
    the rectangle obtained via the optimisation strategy proposed in VOT-2016 [2],
    [67] (blue).'
  Figure 7 Link: articels_figures_by_rev_year\2023\SiamMask_A_Framework_for_Fast_Online_Object_Tracking_and_Segmentation\figure_7.jpg
  Figure 7 caption: The two-stage version of SiamMask used for each object in the
    multiple object tracking and segmentation problem.
  Figure 8 Link: articels_figures_by_rev_year\2023\SiamMask_A_Framework_for_Fast_Online_Object_Tracking_and_Segmentation\figure_8.jpg
  Figure 8 caption: The EAO plot for the proposed SiamMask and the top ten real time
    competing trackers on the VOT2018 challenge.
  Figure 9 Link: articels_figures_by_rev_year\2023\SiamMask_A_Framework_for_Fast_Online_Object_Tracking_and_Segmentation\figure_9.jpg
  Figure 9 caption: Comparison between SiamMask and state-of-the-art trackers with
    respect to different visual scene attributes on VOT2016 and VOT2018.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Weiming Hu
  Name of the last author: Philip H.S. Torr
  Number of Figures: 16
  Number of Tables: 16
  Number of authors: 5
  Paper title: 'SiamMask: A Framework for Fast Online Object Tracking and Segmentation'
  Publication Date: 2023-02-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Backbone Architecture
  Table 10 caption:
    table_text: TABLE 10 Comparison With Unsupervised Learning-Based Methods on the
      TrackingNet Dataset
  Table 2 caption:
    table_text: TABLE 2 The Architecture of the Two-Branch Variant
  Table 3 caption:
    table_text: TABLE 3 The Architecture of the Three-Branch Variant
  Table 4 caption:
    table_text: TABLE 4 Accuracies for Different Bounding Box Representation Strategies
      on VOT-2016
  Table 5 caption:
    table_text: TABLE 5 Results of SiamMask on the VOT-2016 and VOT-2018 Benchmarks
  Table 6 caption:
    table_text: TABLE 6 Comparison With the State-of-the-Art on the VOT-2018 Benchmark
  Table 7 caption:
    table_text: TABLE 7 Comparison With Unsupervised Learning-Based Methods on the
      VOT-2018 Benchmark
  Table 8 caption:
    table_text: TABLE 8 Comparison on GOT-10k Benchmark
  Table 9 caption:
    table_text: TABLE 9 Comparison on TrackingNet Benchmark
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3172932
- Affiliation of the first author: school of mathematics and physics, university of
    science and technology beijing, beijing, china
  Affiliation of the last author: department of mathematics, city university of hong
    kong, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2023\Properties_of_Standard_and_Sketched_Kernel_Fisher_Discriminant\figure_1.jpg
  Figure 1 caption: MSE of tildef for standard KFD and sketched KFD. Different solid
    lines in red, green and black indicate the MSEs corresponding to n=2048,4096 and
    8192. circ , + , diamond , vartriangle and Box indicate the MSEs for Gaussian,
    ROS, JLE, OSNAP1 and OSNAP4 sketch methods. The dashed horizontal lines indicate
    the errors of the standard KFD.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Properties_of_Standard_and_Sketched_Kernel_Fisher_Discriminant\figure_2.jpg
  Figure 2 caption: Runtime for standard KFD and sketched KFD for 2-class problem
    with n=2048 .
  Figure 3 Link: articels_figures_by_rev_year\2023\Properties_of_Standard_and_Sketched_Kernel_Fisher_Discriminant\figure_3.jpg
  Figure 3 caption: "MSE of f ~ and classification error rate for standard KFD and\
    \ sketched KFD for 2-class problem with various tuning parameter \u03F5 ."
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Jiamin Liu
  Name of the last author: Heng Lian
  Number of Figures: 3
  Number of Tables: 3
  Number of authors: 4
  Paper title: Properties of Standard and Sketched Kernel Fisher Discriminant
  Publication Date: 2023-02-06 00:00:00
  Table 1 caption:
    table_text: "TABLE I Comparison of 2-Class Classification Error Rates ( \xD7100%\
      \ \xD7100%) on the Testing Samples With Different m m"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE II Comparison of 3-Class Classification Error Rates ( \xD7\
      100% \xD7100%) on the Testing Samples With Different m m"
  Table 3 caption:
    table_text: "TABLE III Comparison of Classification Error Rates ( \xD7100% \xD7\
      100%) Between Standard KFD and Sketched KFD With Different m m in Splice, Ringform,\
      \ Waveform and Occupancy Datasets"
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3242681
- Affiliation of the first author: department of computer science, city university
    of hong kong, hong kong
  Affiliation of the last author: department of automation, tsinghua university, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2023\ContentAware_Warping_for_View_Synthesis\figure_1.jpg
  Figure 1 caption: Comparison of the traditional image warping operation and the
    proposed content-aware warping. In contrast to the content-independent weights
    employed in the warping operation (taking bilinear interpolation weights as an
    example), we propose to learn geometry-aware and content-adaptive interpolation
    weights from carefully constructed embeddings.
  Figure 10 Link: articels_figures_by_rev_year\2023\ContentAware_Warping_for_View_Synthesis\figure_10.jpg
  Figure 10 caption: Visual comparisons of synthesized views from different methods
    on the RealEstate10K dataset [11]. (a) Ground Truth, (b) IBRNet [13], (c) SVNVS
    [17], (d) Guo et al. [18] and (e) Ours.
  Figure 2 Link: articels_figures_by_rev_year\2023\ContentAware_Warping_for_View_Synthesis\figure_2.jpg
  Figure 2 caption: 'Flowchart of the proposed framework for view synthesis from N
    input source views. It consists of three modules: content-aware warping, confidence-based
    blending, and feature-assistant spatial refinement. We refer the readers to Fig.
    4 for more details of the feature-assistant spatial refinement module.'
  Figure 3 Link: articels_figures_by_rev_year\2023\ContentAware_Warping_for_View_Synthesis\figure_3.jpg
  Figure 3 caption: Visual illustration of the effect of the confidence-based blending.
  Figure 4 Link: articels_figures_by_rev_year\2023\ContentAware_Warping_for_View_Synthesis\figure_4.jpg
  Figure 4 caption: Flowchart of the proposed feature-assistant spatial refinement
    module.
  Figure 5 Link: articels_figures_by_rev_year\2023\ContentAware_Warping_for_View_Synthesis\figure_5.jpg
  Figure 5 caption: Quantitative comparisons (PSNRSSIM) of different methods under
    various disparity ranges (pixels) between input SAIs on the MPI LF dataset [61].
    (a) Baseline (Warp), (b) Baseline (Disparity), (c) Kalantari et al. [33], (d)
    Wu et al. [34], (e) Wu et al. [56], (f) Jin et al. [36], (g) Guo et al. [18],
    (h) Bao et al. [57], (i) Ours. The two subfigures share the same legend.
  Figure 6 Link: articels_figures_by_rev_year\2023\ContentAware_Warping_for_View_Synthesis\figure_6.jpg
  Figure 6 caption: Visual comparisons of reconstructed SAIs from different methods
    on the Inria Sparse LF dataset [55]. (a) Ground Truth, (b) Baseline (Warp), (c)
    Baseline (Disparity), (d) Kalantari et al. [33], (e) Wu et al. [34], (f) Wu et
    al. [56], (g) Jin et al. [36], (h) Guo et al. [18], (i) Bao et al. [57], and (j)
    Ours. The disparity range between input SAIs of each LF is shown on the left.
  Figure 7 Link: articels_figures_by_rev_year\2023\ContentAware_Warping_for_View_Synthesis\figure_7.jpg
  Figure 7 caption: Visual comparisons of reconstructed SAIs from different methods
    on the MPI LF dataset [61]. (a) Ground Truth, (b) Baseline (Warp), (c) Baseline
    (Disparity), (d) Kalantari et al. [33], (e) Wu et al. [34], (f) Wu et al. [56],
    (g) Jin et al. [36], (h) Guo et al. [18], (i) Bao et al. [57], (j) Ours. The disparity
    range between input SAIs reaches 80 pixels for each reconstructed LF.
  Figure 8 Link: articels_figures_by_rev_year\2023\ContentAware_Warping_for_View_Synthesis\figure_8.jpg
  Figure 8 caption: Visual comparisons of estimated depth maps from ground-truth LFs
    and reconstructed LF by different methods on the Inria Sparse LF dataset [55].
    (a) Ground Truth, (b) Baseline (Warp), (c) Baseline (Disparity), (d) Kalantari
    et al. [33], (e) Wu et al. [34], (f) Wu et al. [56], (g) Jin et al. [36], (h)
    Guo et al. [18], (i) Bao et al. [57], (j) Ours. The disparity range between input
    SAIs of each LF is shown on the left.
  Figure 9 Link: articels_figures_by_rev_year\2023\ContentAware_Warping_for_View_Synthesis\figure_9.jpg
  Figure 9 caption: Visual comparisons of synthesized views from different methods
    on the DTU dataset [62]. (a) Ground Truth, (b) FVS [16], (c) pixelNeRF [44], (d)
    IBRNet [13], (e) SVNVS [17], (f) Guo et al. [18], and (g) Ours.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Mantang Guo
  Name of the last author: Jiwen Lu
  Number of Figures: 13
  Number of Tables: 8
  Number of authors: 6
  Paper title: Content-Aware Warping for View Synthesis
  Publication Date: 2023-02-06 00:00:00
  Table 1 caption:
    table_text: TABLE I Quantitative Comparisons (PSNRSSIM) of Different Methods on
      the Inria Sparse LF Dataset [55]. The Best and Second Best Results are Highlighted
      in Red and Blue, Respectively
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Quantitative Comparisons of the Depth Maps Estimated From
      the Ground-Truth LFs and the Reconstructed LFs by Different Methods on the Inria
      Sparse LF Dataset [55]. The Best and Second Best Results are Highlighted in
      Red and Blue, Respectively
  Table 3 caption:
    table_text: TABLE III Comparisons of Running Time (in Seconds per View) and Model
      Parameter Size (M) of Different Methods on the Inria Sparse LF Dataset [55]
  Table 4 caption:
    table_text: TABLE IV Quantitative Comparisons (PSNRSSIM) of Different Methods
      on the DTU Dataset [62]. The Best and Second Best Results are Highlighted in
      Red and Blue, Respectively
  Table 5 caption:
    table_text: TABLE V Quantitative Comparisons (PSNRSSIM) of Different Methods on
      the RealEstate10K Dataset [11]. The Best and Second Best Results are Highlighted
      in Red and Blue, Respectively
  Table 6 caption:
    table_text: TABLE VI Comparisons of Running Time (in Seconds per View) and Model
      Parameter Size (M) of Different Methods on the DTU Dataset [62]
  Table 7 caption:
    table_text: TABLE VII Quantitative Performance (PSNRSSIM) of Our Method Fed With
      Various Numbers of Source Views Over the DTU Dataset [62]
  Table 8 caption:
    table_text: TABLE VIII Quantitative Results of the Ablation Studies on the Five
      Key Components of Our Method
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3242709
- Affiliation of the first author: school of computer science and engineering, nanjing
    university of science and technology, nanjing, china
  Affiliation of the last author: google, university of california merced, merced,
    ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2023\Cascaded_Deep_Video_Deblurring_Using_Temporal_Sharpness_Prior_and_NonLocal_Spati\figure_1.jpg
  Figure 1 caption: Deblurred result on a real challenging video. Our method is motivated
    by the success of deblurring approaches based on variational models. It explores
    sharp pixels from adjacent frames by a temporal sharpness prior and non-local
    spatial-temporal similarity contexts to constrain deep convolutional neural networks
    (CNNs) and restores sharp videos by a cascaded inference process. We show that
    enforcing the temporal sharpness prior and non-local spatial-temporal similarity
    contexts and learning the deep CNNs by a cascaded inference manner can make the
    deep CNN more compact and generate better-deblurred results (i.e., (f) and (g))
    than both the CNN-based methods [2], [3], [4] and variational model-based method
    [1].
  Figure 10 Link: articels_figures_by_rev_year\2023\Cascaded_Deep_Video_Deblurring_Using_Temporal_Sharpness_Prior_and_NonLocal_Spati\figure_10.jpg
  Figure 10 caption: Effect of the number of the input frames on video deblurring.
    The results are generated on the dataset by Su et al. [10].
  Figure 2 Link: articels_figures_by_rev_year\2023\Cascaded_Deep_Video_Deblurring_Using_Temporal_Sharpness_Prior_and_NonLocal_Spati\figure_2.jpg
  Figure 2 caption: Overview of the self-attention method that is used to explore
    the non-local similarity from both each frame itself and long-range frames.
  Figure 3 Link: articels_figures_by_rev_year\2023\Cascaded_Deep_Video_Deblurring_Using_Temporal_Sharpness_Prior_and_NonLocal_Spati\figure_3.jpg
  Figure 3 caption: Deblurred results on the test dataset [10]. The deblurred results
    in (c)-(i) still contain significant blur effects. The proposed algorithm generates
    much clearer frames.
  Figure 4 Link: articels_figures_by_rev_year\2023\Cascaded_Deep_Video_Deblurring_Using_Temporal_Sharpness_Prior_and_NonLocal_Spati\figure_4.jpg
  Figure 4 caption: Deblurred results on the test dataset [50]. The proposed method
    generates much better deblurred frames, where the license numbers are recognizable.
  Figure 5 Link: articels_figures_by_rev_year\2023\Cascaded_Deep_Video_Deblurring_Using_Temporal_Sharpness_Prior_and_NonLocal_Spati\figure_5.jpg
  Figure 5 caption: Deblurred results on a real video from [6]. The proposed algorithm
    recovers a high-quality frame with clearer details.
  Figure 6 Link: articels_figures_by_rev_year\2023\Cascaded_Deep_Video_Deblurring_Using_Temporal_Sharpness_Prior_and_NonLocal_Spati\figure_6.jpg
  Figure 6 caption: Comparisons of the results by different proposed models on real
    videos from [6]. The proposed CDVDTSPNL method recovers high-quality frames with
    clearer characters.
  Figure 7 Link: articels_figures_by_rev_year\2023\Cascaded_Deep_Video_Deblurring_Using_Temporal_Sharpness_Prior_and_NonLocal_Spati\figure_7.jpg
  Figure 7 caption: Effectiveness of the cascaded training algorithm in the CDVDTSP
    method for video deblurring. (b) denotes the deblurred result by the proposed
    method without using cascaded training. (c)-(e) denote the results from stage
    1, 2, and 3, respectively.
  Figure 8 Link: articels_figures_by_rev_year\2023\Cascaded_Deep_Video_Deblurring_Using_Temporal_Sharpness_Prior_and_NonLocal_Spati\figure_8.jpg
  Figure 8 caption: Effect of optical flow in the CDVDTSP method for video deblurring.
    The optical flow by the proposed method contains sharp boundaries well (see (f)),
    which facilitates the latent frame restoration.
  Figure 9 Link: articels_figures_by_rev_year\2023\Cascaded_Deep_Video_Deblurring_Using_Temporal_Sharpness_Prior_and_NonLocal_Spati\figure_9.jpg
  Figure 9 caption: Effectiveness of the TSP. (a) Blurred input. (b) Visualizations
    of the intermediate TSP. (c)-(d) denote the results without and with the TSP,
    respectively.
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.66
  Name of the first author: Jinshan Pan
  Name of the last author: Ming-Hsuan Yang
  Number of Figures: 12
  Number of Tables: 11
  Number of authors: 5
  Paper title: Cascaded Deep Video Deblurring Using Temporal Sharpness Prior and Non-Local
    Spatial-Temporal Similarity
  Publication Date: 2023-02-07 00:00:00
  Table 1 caption:
    table_text: TABLE I Quantitative Evaluations on the Video Deblurring Dataset [10]
      in Terms of PSNR and SSIM. All the Comparison Results are Generated Using the
      Publicly Available Code. All the Restored Frames Instead of Randomly Selected
      30 Frames From Each Test Set [10] are Used for Evaluations
  Table 10 caption:
    table_text: TABLE X Running Time Performance, Where the Running Time is Evaluated
      on a Machine With an Intel Core i9-10900 K CPU3.70 GHz and NVIDIA RTX A6000
      GPU
  Table 2 caption:
    table_text: "TABLE II Quantitative Evaluations on the GoPro Dataset [50] Where\
      \ \u2217 Denotes the Reported Results From [51]"
  Table 3 caption:
    table_text: TABLE III Quantitative Evaluations on the BSD Video Deblurring Dataset
      [4] in Terms of PSNR and SSIM
  Table 4 caption:
    table_text: "TABLE IV Effectiveness of the Cascaded Training Algorithm in the\
      \ CDVDTSP Method for Video Deblurring, Where \u201CCT\u201D is the Abbreviation\
      \ of Cascaded Training"
  Table 5 caption:
    table_text: TABLE V Effectiveness of the TSP in the CDVDTSP Method for Video Deblurring
  Table 6 caption:
    table_text: TABLE VI Effectiveness of the Optical Flow Estimation Module in the
      CDVDTSP Method for Video Deblurring
  Table 7 caption:
    table_text: TABLE VII Effectiveness of the Non-Local Similarity Mining in the
      CDVDTSPNL Method for Video Deblurring
  Table 8 caption:
    table_text: TABLE VIII Effectiveness of the Global Feature Propagation in the
      CDVDTSPNL Method for Video Deblurring
  Table 9 caption:
    table_text: TABLE IX Comparisons of Model Parameters Against State-of-The-Art
      Methods and Baselines
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3243059
- Affiliation of the first author: state key laboratory for novel software technology,
    nanjing university, nanjing, china
  Affiliation of the last author: state key laboratory for novel software technology,
    nanjing university, nanjing, china
  Figure 1 Link: articels_figures_by_rev_year\2023\Salvage_of_Supervision_in_Weakly_Supervised_Object_Detection_and_Segmentation\figure_1.jpg
  Figure 1 caption: "The overall SoS pipeline for weakly supervised vision tasks.\
    \ The key idea of SoS is to effectively harness every potentially useful supervisory\
    \ signal. The first stage of SoS is Basic Training, which trains a model with\
    \ any weakly supervised method. The second stage, Pseudo Training, generates high\
    \ quality pseudo labels and can utilize almost all modern technologies and backbones.\
    \ Then, Semi-Training, the third stage, splits the dataset with pseudo-labels\
    \ into \u201Cclean\u201D labeled and noisy unlabeled parts. Semi-supervised learning\
    \ is used to further squeeze supervisory signals with higher quality. This figure\
    \ is best viewed in color and zoomed in."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Salvage_of_Supervision_in_Weakly_Supervised_Object_Detection_and_Segmentation\figure_2.jpg
  Figure 2 caption: "The SoS-WSOD pipeline. Stage 1 trains a detector with only image-level\
    \ labels. We design PGF to filter its detection results and to generate high-quality\
    \ pseudo box-level annotations in stage 2, which enable us to train a fully supervised\
    \ detector. Stage 3 splits the training set into \u201Cclean\u201D and unlabeled\
    \ \u201Cnoisy\u201D parts, and trains a detector in a semi-supervised manner.\
    \ This figure is best viewed in color and zoomed in."
  Figure 3 Link: articels_figures_by_rev_year\2023\Salvage_of_Supervision_in_Weakly_Supervised_Object_Detection_and_Segmentation\figure_3.jpg
  Figure 3 caption: Per-class accuracy of pseudo groundtruth bounding boxes of labeled
    and unlabeled subsets on VOC2007 when K=2000 .
  Figure 4 Link: articels_figures_by_rev_year\2023\Salvage_of_Supervision_in_Weakly_Supervised_Object_Detection_and_Segmentation\figure_4.jpg
  Figure 4 caption: The SoS-WSSS and SoS-WSIS pipeline. As stage 1 and stage 2 are
    both contained in state-of-the-art WSSS and WSIS methods, the proposed SoS only
    introduces one additional stage, in which SoS performs dataset split first and
    then perform semi-supervised segmentation.
  Figure 5 Link: articels_figures_by_rev_year\2023\Salvage_of_Supervision_in_Weakly_Supervised_Object_Detection_and_Segmentation\figure_5.jpg
  Figure 5 caption: Inference speed (seconds per image) versus accuracy ( mAP ) on
    VOC2007. Note that the x -axis is in logarithm scale.
  Figure 6 Link: articels_figures_by_rev_year\2023\Salvage_of_Supervision_in_Weakly_Supervised_Object_Detection_and_Segmentation\figure_6.jpg
  Figure 6 caption: Effect of t con and t keep in PGF
  Figure 7 Link: articels_figures_by_rev_year\2023\Salvage_of_Supervision_in_Weakly_Supervised_Object_Detection_and_Segmentation\figure_7.jpg
  Figure 7 caption: 'Visualization result of SoS-WSOD results on MS-COCO. Top row:
    groundtruth annotations. 2nd to 4th rows: detection results from stages 1, 2,
    3, respectively. Last column: a failure case.'
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.64
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.95
  Name of the first author: Lin Sui
  Name of the last author: Jianxin Wu
  Number of Figures: 7
  Number of Tables: 16
  Number of authors: 3
  Paper title: Salvage of Supervision in Weakly Supervised Object Detection and Segmentation
  Publication Date: 2023-02-07 00:00:00
  Table 1 caption:
    table_text: TABLE I Effectiveness of Various SoS-WSOD Stages on VOC2007, VOC2012
      and MS-COCO
  Table 10 caption:
    table_text: TABLE X Comparison With State-of-the-Art Methods on MS-COCO
  Table 2 caption:
    table_text: TABLE II VOC2007 Results on Enabling Vanilla ResNet in Initial WSOD
      Learning (Stage 1)
  Table 3 caption:
    table_text: TABLE III VOC2007 Results on Training WSOD Detectors Without ImageNet
      Pretraining
  Table 4 caption:
    table_text: TABLE IV Detailed Accuracy Gains in Stage 3
  Table 5 caption:
    table_text: TABLE V Using Different WSOD Methods in Stage 1
  Table 6 caption:
    table_text: TABLE VI Results for Different Groundtruth Mining Algorithms
  Table 7 caption:
    table_text: TABLE VII Comparison of Our Splitting Rule and Random Splitting
  Table 8 caption:
    table_text: TABLE VIII Effect of K K in Stage 3 on VOC2007
  Table 9 caption:
    table_text: TABLE IX Comparison With State-of-the-Art Methods on VOC2007
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3243054
- Affiliation of the first author: department of computer science, rutgers university,
    piscataway, nj, usa
  Affiliation of the last author: department of computer science, rutgers university,
    piscataway, nj, usa
  Figure 1 Link: articels_figures_by_rev_year\2023\SequenceMorph_A_Unified_Unsupervised_Learning_Framework_for_Motion_Tracking_on_C\figure_1.jpg
  Figure 1 caption: '(a) Different cardiac imaging modalities: ultrasound (US) imaging
    (4-chamber view), conventional untagged cine magnetic resonance (cMR) imaging
    and tagged MR (tMR) imaging (short-axis view). Number under the figure means percentage
    of one cardiac cycle. (b) Standard scan long-axis views (2-, 3-, 4-chamber views)
    of tMR images at end-diastole (ED) phase. Red and green contours depict the epi-
    and endo-cardial borders of left ventricle (LV) myocardium (MYO) wall, respectively.
    Blue contour depicts the right ventricle (RV). LA: left atrium. RA: right atrium.
    ES: end-systole.'
  Figure 10 Link: articels_figures_by_rev_year\2023\SequenceMorph_A_Unified_Unsupervised_Learning_Framework_for_Motion_Tracking_on_C\figure_10.jpg
  Figure 10 caption: "Motion tracking results on a cMR image sequence (best viewed\
    \ zoomed in). First row shows the images and second row shows the segmentation\
    \ masks. Between ED and ES, we show the warped results by the estimated motion\
    \ fields from the ED phase to the ES phase. Red contour shows the ground truth\
    \ edge of LV, MYO and RV on the ES frame. \u201CwoR\u201D means \u201Cwithout\
    \ Refinement\u201D."
  Figure 2 Link: articels_figures_by_rev_year\2023\SequenceMorph_A_Unified_Unsupervised_Learning_Framework_for_Motion_Tracking_on_C\figure_2.jpg
  Figure 2 caption: 'Motion decomposition and recomposition: For the trajectory Xt
    of a material point, we define the inter-frame (INF) motion boldsymbolphi as the
    motion between any time interval. We also define the Lagrangian motion boldsymbolPhi
    of this material point as the motion between the start position and any other
    position later in time. A Lagrangian motion vector is composed of a sum of INF
    motion vectors. boldsymbolPhi prime shows the drifted Lagrangian motion tracking
    result compared to the ground truth boldsymbolPhi .'
  Figure 3 Link: articels_figures_by_rev_year\2023\SequenceMorph_A_Unified_Unsupervised_Learning_Framework_for_Motion_Tracking_on_C\figure_3.jpg
  Figure 3 caption: "An overview of our scheme for cardiac motion tracking (illustrated\
    \ on tMR frames). The first frame is set as the reference frame. We first estimate\
    \ the inter-frame (INF) motion field ( \u03D5 ) between consecutive image pairs.\
    \ Then we recompose the Lagrangian motion field ( \u03A6 ) between the first frame\
    \ and any other later frame with the associated INF motion fields. Motion tracking\
    \ is achieved by predicting the position X n\u22121 on an arbitrary frame moved\
    \ from the position X 0 on the first frame with the estimated Lagrangian motion\
    \ field: X n\u22121 = \u03A6 0(n\u22121) ( X 0 ) . The colored points highlight\
    \ corresponding pairs of points between frames; the underlying motion field is\
    \ defined for all points."
  Figure 4 Link: articels_figures_by_rev_year\2023\SequenceMorph_A_Unified_Unsupervised_Learning_Framework_for_Motion_Tracking_on_C\figure_4.jpg
  Figure 4 caption: "An overview of our proposed bi-directional forward-backward generative\
    \ diffeomorphic registration network. For each consecutive image pair x and y\
    \ , we first utilize a FCN to learn the posterior probability parameters \u03BC\
    \ z|x;y and \u03A3 z|x;y which represent the stationary velocity field (SVF) mean\
    \ and variance. Then we sample the forward and backward velocities and transform\
    \ them to the diffeomorphic INF motion field \u03D5 and its inverse motion field\
    \ \u03D5 \u22121 by using differentiable squaring and scaling integration layers\
    \ (SS). Finally, we warp both images towards each other with the forward and backward\
    \ INF motion fields."
  Figure 5 Link: articels_figures_by_rev_year\2023\SequenceMorph_A_Unified_Unsupervised_Learning_Framework_for_Motion_Tracking_on_C\figure_5.jpg
  Figure 5 caption: "A composition layer C that transforms INF motion field \u03D5\
    \ to Lagrangian motion field \u03A6 . With a current INF motion field \u03D5 (n\u2212\
    2)(n\u22121) and a previous Lagrangian motion field \u03A6 0(n\u22122) , we compose\
    \ them into a new net Lagrangian motion field \u03A6 0(n\u22121) by which we can\
    \ warp the first image frame I 0 to the current frame: I 0 \u2218 \u03A6 0(n\u2212\
    1) . We measure the similarity between I n\u22121 and I 0 \u2218 \u03A6 0(n\u2212\
    1) to pose a global Lagrangian motion constraint on the estimated INF motion field.\
    \ \u201CW\u201D means \u201Cwarp\u201D."
  Figure 6 Link: articels_figures_by_rev_year\2023\SequenceMorph_A_Unified_Unsupervised_Learning_Framework_for_Motion_Tracking_on_C\figure_6.jpg
  Figure 6 caption: "(a) The differentiable composition layer C . (b) INF displacement\
    \ field \u03D5 u =\u03D5\u2212Id interpolation at the new tracked position p \u2032\
    \ ."
  Figure 7 Link: articels_figures_by_rev_year\2023\SequenceMorph_A_Unified_Unsupervised_Learning_Framework_for_Motion_Tracking_on_C\figure_7.jpg
  Figure 7 caption: "An illustration of Lagrangian motion estimation refinement. (a)\
    \ INF motion ground truth \u03D5 and drifted estimation \u03D5 \u2032 ; (b) Lagrangian\
    \ motion estimation through INF motion recomposition; (c) The recomposed Lagrangian\
    \ motion estimation \u03A6 \u2032 and ground truth \u03A6 ; (d) Residual Lagrangian\
    \ motion R \u2217 estimated by another registration network; (e) Lagrangian motion\
    \ estimation refinement by recomposing \u03A6 \u2032 and R \u2217 ; (f) The final\
    \ refined Lagrangian motion \u03A6 \u2217 ."
  Figure 8 Link: articels_figures_by_rev_year\2023\SequenceMorph_A_Unified_Unsupervised_Learning_Framework_for_Motion_Tracking_on_C\figure_8.jpg
  Figure 8 caption: "Mean and standard deviation of the RMS errors across a cardiac\
    \ cycle for baseline methods and ours on the tMR dataset. \u201CwoR\u201D means\
    \ \u201Cwithout Refinement\u201D."
  Figure 9 Link: articels_figures_by_rev_year\2023\SequenceMorph_A_Unified_Unsupervised_Learning_Framework_for_Motion_Tracking_on_C\figure_9.jpg
  Figure 9 caption: "Motion tracking results on a tMR image sequence of 19 frames\
    \ (best viewed zoomed in). Red is ground truth; green is prediction. \u201CF\u201D\
    \ means \u201Cframe\u201D. \u201CwoR\u201D means \u201Cwithout Refinement\u201D\
    . Blue arrows show the missing landmarks from the tracking results using HARP."
  First author gender probability: 0.54
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Meng Ye
  Name of the last author: Dimitris N. Metaxas
  Number of Figures: 15
  Number of Tables: 7
  Number of authors: 6
  Paper title: 'SequenceMorph: A Unified Unsupervised Learning Framework for Motion
    Tracking on Cardiac Image Sequences'
  Publication Date: 2023-02-07 00:00:00
  Table 1 caption:
    table_text: "TABLE I Average RMS Error, Percentage of Pixels With Non-Positive\
      \ Jacobian Determinant and Running Time for tMR Motion Tracking; number of Learning\
      \ Parameters for Learning-Based Models. \u201Cm\u201D Means \u201Cmillion\u201D\
      . \u201Cwor\u201D Means \u201Cwithout Refinement\u201D"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE II Average Dice, Hausdorff Distances (HD), Percentage of Pixels\
      \ With Non-Positive Jacobian Determinant and Running Time for cMR Motion Tracking.\
      \ \u201Cwor\u201D Means \u201Cwithout Refinement\u201D"
  Table 3 caption:
    table_text: "TABLE III Average Dice, Hausdorff Distances (HD), Percentage of Pixels\
      \ With Non-Positive Jacobian Determinant and Running Time for US Motion Tracking.\
      \ \u201Cwor\u201D Means \u201Cwithout Refinement\u201D"
  Table 4 caption:
    table_text: 'TABLE IV Ablation Study Results on the tMR Dataset. A1: Forward Lagrangian
      Tracking; A2: Forward INF Tracking; A3: A2 + Backward INF Tracking; A4: A3 +
      INF Motion Field Smoothing; A5: A4 + Every 4 Frames Lagrangian Constraint; A6:
      A4 + Full Sequence Lagrangian Constraint; A7: A6 + Lagrangian Motion Field Smoothing'
  Table 5 caption:
    table_text: 'TABLE V Ablation of Dynamic Motion Smoothness Regularization Study
      Results on the tMR Dataset. A7A8A9: Explicit Motion Smoothness Regularization
      in the wholeearlier Half of thelater Half of the Cardiac Cycle, Respectively;
      A10A11A12: A7 + Applying the Regularization Mask M M to the wholeearlier Half
      of thelater Half of the Cardiac Cycle, Respectively'
  Table 6 caption:
    table_text: 'TABLE VI Ablation of Lagrangian Motion Refinement Study Results on
      the cMR Dataset. A11: Without Refinement; B1: Using an Identical Registration
      Network for Both INF Motion Tracking and Lagrangian Motion Refinement; B2: Joint
      Training the Two Different Networks for INF Motion Tracking and Lagrangian Motion
      Refinement; Ours: Sequential Training the Two Different Networks'
  Table 7 caption:
    table_text: "TABLE VII Ablation of Each Component in Our Framework by Using the\
      \ \u201Cleave-One-Out\u201D Approach on the tMR Dataset. B: Bi-Directional Registration;\
      \ G: Global Lagrangian Motion Constraints; D: Dynamic Smoothing Regularization;\
      \ R: Lagrangian Motion Estimation Refinement"
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3243040
- Affiliation of the first author: school of computer science and technology, harbin
    institute of technology, harbin, china
  Affiliation of the last author: department of automation, tsinghua university, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2023\Asymmetric_Loss_Functions_for_NoiseTolerant_Learning_Theory_and_Applications\figure_1.jpg
  Figure 1 caption: Illustration of label noise model under clean-labels-domination
    and -non-domination settings.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Asymmetric_Loss_Functions_for_NoiseTolerant_Learning_Theory_and_Applications\figure_2.jpg
  Figure 2 caption: Verification of classification calibration. Solid and dashed lines
    denote the curve of Hell (eta) and Hell -(eta) , respectively. As can be observed,
    the curve of Hell - is always above that of Hell , i.e., the loss functions are
    classification-calibrated.
  Figure 3 Link: articels_figures_by_rev_year\2023\Asymmetric_Loss_Functions_for_NoiseTolerant_Learning_Theory_and_Applications\figure_3.jpg
  Figure 3 caption: "The validation for necessary and sufficient conditions of AGCE,\
    \ AUL and AEL, where m=arg max i w i , n=arg max i\u2260m w i , and a \u2217 is\
    \ the value such that w m w n \u22C5r(\u2113)=1 for different loss functions."
  Figure 4 Link: articels_figures_by_rev_year\2023\Asymmetric_Loss_Functions_for_NoiseTolerant_Learning_Theory_and_Applications\figure_4.jpg
  Figure 4 caption: Test accuracies of AGCE, AUL and AEL with different parameters
    on CIFAR-10 under 0.8 symmetric noise.
  Figure 5 Link: articels_figures_by_rev_year\2023\Asymmetric_Loss_Functions_for_NoiseTolerant_Learning_Theory_and_Applications\figure_5.jpg
  Figure 5 caption: Illustration of asymmetric losses (AGCEs, AULs, and AELs).
  Figure 6 Link: articels_figures_by_rev_year\2023\Asymmetric_Loss_Functions_for_NoiseTolerant_Learning_Theory_and_Applications\figure_6.jpg
  Figure 6 caption: "Illustration of the Negative Heat Kernel loss \u2113 heat (x;a)=\u2212\
    \ e \u2212 x 2 a 2 \u03C0 \u221A a , the Negative Poisson Kernel loss \u2113 poisson\
    \ (x;a)=\u2212 a \u03C0( a 2 + x 2 ) , and p -norm loss \u2113 p (x)=\u2225x \u2225\
    \ p p ."
  Figure 7 Link: articels_figures_by_rev_year\2023\Asymmetric_Loss_Functions_for_NoiseTolerant_Learning_Theory_and_Applications\figure_7.jpg
  Figure 7 caption: "Visualization for GCE (top) and AGCE (bottom) on MNIST with different\
    \ symmetric noise ( \u03B7\u2208[0.0,0.2,0.4,0.6,0.8] ) by t-SNE [64] 2D embeddings\
    \ of deep features. AGCE learns better presentations with more separated and clearly\
    \ bounded clusters than GCE."
  Figure 8 Link: articels_figures_by_rev_year\2023\Asymmetric_Loss_Functions_for_NoiseTolerant_Learning_Theory_and_Applications\figure_8.jpg
  Figure 8 caption: "Example results of various loss functions for training Gaussian,\
    \ salt-pepper, Bernoulli, and impulse denoisers with Noise2Clean [4], Noise2Noise\
    \ [31], and Noise2Self [33]. We also report the PSNR and SSIM for each denoised\
    \ image. As can be seen, \u2113 2 loss performs better than \u2113 heat and \u2113\
    \ poisson in most cases under Noise2Clean, while \u2113 heat and \u2113 poisson\
    \ obtain significant improvements for salt-pepper, Bernoulli and impulse denoising\
    \ under Noise2Noise and Noise2Self."
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.86
  Name of the first author: Xiong Zhou
  Name of the last author: Xiangyang Ji
  Number of Figures: 8
  Number of Tables: 4
  Number of authors: 5
  Paper title: 'Asymmetric Loss Functions for Noise-Tolerant Learning: Theory and
    Applications'
  Publication Date: 2023-02-07 00:00:00
  Table 1 caption:
    table_text: "TABLE I Validation Accuracies (%) of Different Methods on Benchmarks\
      \ With Symmetric Label Noise ( \u03B7\u2208[0.0,0.2,0.4,0.6,0.8] \u03B7\u2208\
      [0.0,0.2,0.4,0.6,0.8]) and Asymmetric Label Noise ( \u03B7\u2208[0.1,0.2,0.3,0.4]\
      \ \u03B7\u2208[0.1,0.2,0.3,0.4])"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Validation Results of Different Losses for Gaussian, Salt-Pepper,
      Bernoulli, and Impulse Noise Removal. The Results are Reported by the Average
      PSNR (dB) and the Average SSIM on the Validation Set
  Table 3 caption:
    table_text: TABLE III Accuracy (%) of Different Methods on CIFAR10 N [1]
  Table 4 caption:
    table_text: TABLE IV Top-1 Validation Accuracies (%) on WebVision Validation Set
      Using Different Loss Functions
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3236459
- Affiliation of the first author: school of mathematics and statistics, beijing jiaotong
    university, beijing, china
  Affiliation of the last author: itp lab, department of electrical and electronic
    engineering, imperial college london, london, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2023\Federated_Learning_Via_Inexact_ADMM\figure_1.jpg
  Figure 1 caption: Objective versus CR for Example V.1.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Federated_Learning_Via_Inexact_ADMM\figure_2.jpg
  Figure 2 caption: Effect of k 0 .
  Figure 3 Link: articels_figures_by_rev_year\2023\Federated_Learning_Via_Inexact_ADMM\figure_3.jpg
  Figure 3 caption: "Effect of \u03C1 ."
  Figure 4 Link: articels_figures_by_rev_year\2023\Federated_Learning_Via_Inexact_ADMM\figure_4.jpg
  Figure 4 caption: Effect of m .
  Figure 5 Link: articels_figures_by_rev_year\2023\Federated_Learning_Via_Inexact_ADMM\figure_5.jpg
  Figure 5 caption: Effect of larger sizes.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.69
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Shenglong Zhou
  Name of the last author: Geoffrey Ye Li
  Number of Figures: 5
  Number of Tables: 1
  Number of authors: 2
  Paper title: Federated Learning Via Inexact ADMM
  Publication Date: 2023-02-07 00:00:00
  Table 1 caption:
    table_text: TABLE I Descriptions of Two Real Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3243080
- Affiliation of the first author: school of electronic, electrical and communication
    engineering, university of chinese academy of sciences (ucas), beijing, china
  Affiliation of the last author: school of electronic, electrical and communication
    engineering, university of chinese academy of sciences (ucas), beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2023\Conformer_Local_Features_Coupling_Global_Representations_for_Recognition_and_Det\figure_1.jpg
  Figure 1 caption: Comparison of feature maps of CNN (ResNet-101) [4], vision transformer
    (DeiT-S) [10], and the proposed Conformer. The patch tokens in transformer are
    reshaped to feature maps for visualization. While CNN activates discriminative
    local regions (e.g., the peacock's head in (a) and tail in (e)), the CNN branch
    of Conformer takes advantage of global cues from the vision transformer and thereby
    activates complete object (e.g., full extent of the peacock in (b) and (f)). Compared
    with CNN, local feature details of the vision transformer are deteriorated (e.g.,
    (c) and (g)). In contrast, the transformer branch of Conformer retains the local
    feature details from CNN while depressing the background (e.g., the peacock contours
    in (d) and (h) are more complete than those in (c) and (g)). This figure is best
    viewed in color.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Conformer_Local_Features_Coupling_Global_Representations_for_Recognition_and_Det\figure_2.jpg
  Figure 2 caption: Conformer network architecture. (a) Up-sampling and down-sampling
    modules for spatial alignment of feature maps and patch tokens in the Feature
    Coupling Unit (FCU). (b) Implementation details of the CNN block, the transformer
    block, and the FCU. (c) Thumbnail of the Conformer network with the dual architecture.
  Figure 3 Link: articels_figures_by_rev_year\2023\Conformer_Local_Features_Coupling_Global_Representations_for_Recognition_and_Det\figure_3.jpg
  Figure 3 caption: Network structure analysis. Cn and Tr respectively denote a bottleneck
    and a transformer block. (a) The dual structure can be considered as a special
    serial case of the residual structure. (b) The CNN (e.g., ResNet); (c) A special
    hybrid structure where the transformer block is embedded to bottlenecks. (d) The
    vision transformers (e.g., ViT); (e) A special case where the bottlenecks are
    embedded to the transformer blocks.
  Figure 4 Link: articels_figures_by_rev_year\2023\Conformer_Local_Features_Coupling_Global_Representations_for_Recognition_and_Det\figure_4.jpg
  Figure 4 caption: Visualization of feature activation by CNN (ResNet-101) [4], vision
    transformer (DeiT-S) [10], and our proposed Conformer. (a) Class activation maps
    in ResNet-101 and the CNN branch of Conformer-S by using the CAM method [61].
    (b) Attention maps in DeiT-S and the transformer branch of Conformer-S by using
    the Attention Rollout method [62]. This figure is best viewed in color.
  Figure 5 Link: articels_figures_by_rev_year\2023\Conformer_Local_Features_Coupling_Global_Representations_for_Recognition_and_Det\figure_5.jpg
  Figure 5 caption: ConformerDet flowchart. (a) Augmented cross-attention unit (ACU).
    (b) ConformerDet head. (c) The Conformer backbone. Feature maps and proposal tokens
    from Conformer are fed to the detector head (ConformerDet), which leverages augmented
    cross-attention units (ACUs) to couple token representations with local CNN features
    to enhance the discriminability and localization accuracy.
  Figure 6 Link: articels_figures_by_rev_year\2023\Conformer_Local_Features_Coupling_Global_Representations_for_Recognition_and_Det\figure_6.jpg
  Figure 6 caption: "Generalization capability. (a) Comparison of rotation invariance.\
    \ The compared models are trained under the same data augmentation settings and\
    \ directly evaluated on rotated images without model fintuning. (b) Comparison\
    \ of scale invariance. The models are trained on images with the resolution of\
    \ 224\xD7224, and tested on different image resolutions without model finetuning."
  Figure 7 Link: articels_figures_by_rev_year\2023\Conformer_Local_Features_Coupling_Global_Representations_for_Recognition_and_Det\figure_7.jpg
  Figure 7 caption: Evaluation of hyper-parameters. (a) Augmented token number ( I
    ). (b) Resolution ( S ) of local features. (c) Stage number ( L ). (d) Token number
    ( N ). The numbers on the curves denote GFLOPs and model parameters.
  Figure 8 Link: articels_figures_by_rev_year\2023\Conformer_Local_Features_Coupling_Global_Representations_for_Recognition_and_Det\figure_8.jpg
  Figure 8 caption: Object detection responses (in red boxes). Compared with the transformer-based
    detector (DETR), ConformerDet activates more accurate regions due to the fine-detailed
    feature maps. Compared with the CNN-based detector (Faster R-CNN), ConformerDet
    can leverage the long-range semantic dependency to better activate objects from
    the same categories. This figure is best viewed in color.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.54
  Name of the first author: Zhiliang Peng
  Name of the last author: Qixiang Ye
  Number of Figures: 8
  Number of Tables: 14
  Number of authors: 8
  Paper title: 'Conformer: Local Features Coupling Global Representations for Recognition
    and Detection'
  Publication Date: 2023-02-07 00:00:00
  Table 1 caption:
    table_text: TABLE I Architecture Configuration of Conformer Variants
  Table 10 caption:
    table_text: TABLE X Performance of Weakly Supervised Object Localization on CUB-200-2011
      Test Set
  Table 2 caption:
    table_text: TABLE II Top-1 Accuracy for Image Classification on the ImageNet Validation
      Set. Latency and Peak GPU Memory (Mems) are Measured With Batch Size 16 Using
      the Timm Codebase [65]
  Table 3 caption:
    table_text: TABLE III Performance Under Parameter Proportions
  Table 4 caption:
    table_text: TABLE IV Ablation Study of Step-by-Step Construction of Conform From
      ViT
  Table 5 caption:
    table_text: "TABLE V Comparison of Hybrid Structures. DeiT-S32 Means the Model\
      \ With Patch Size 32\xD732 [10]"
  Table 6 caption:
    table_text: TABLE VI Comparison of Positional Embeddings Strategies
  Table 7 caption:
    table_text: TABLE VII Comparison of DownUpsampling Strategies
  Table 8 caption:
    table_text: TABLE VIII Performance Comparison of Ensemble Models
  Table 9 caption:
    table_text: TABLE IX Performance of Conformer Sub-Structures Fig. 3
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3243048
- Affiliation of the first author: centre for vision, speech and signal processing,
    university of surrey, guildford, u.k.
  Affiliation of the last author: centre for vision, speech and signal processing,
    university of surrey, guildford, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2023\Neural_Belief_Propagation_for_Scene_Graph_Generation\figure_1.jpg
  Figure 1 caption: Comparison between the traditional MPNN-based MFVB paradigm and
    the proposed NBP paradigm. In SGG, given an input image I , a factor graph representing
    the factorization of a scoring function is constructed, in which x and f represent
    the object and factor (relationship) vertices, respectively. A variational distribution
    q(x) is often used to approximate the computationally intractable model posterior
    p(x|I) derived from the relevant scoring function. Unlike MPNN-based MFVB, NBP
    includes certain higher-order dependencies ( fijk ) into the scoring function
    aiming to find a better bias-variance trade-off, and incorporates relevant pairwise
    dependencies ( qij and qjk ) into the variational distribution seeking to estimate
    the model posterior better.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Neural_Belief_Propagation_for_Scene_Graph_Generation\figure_2.jpg
  Figure 2 caption: 'Overview of the proposed neural belief propagation (NBP) method,
    in which the green dash line represents the proposed visual context reasoning
    module. Given an input image, visual perception module detects a set of region
    proposals. A feature vector factor graph G=(X,F,E) is constructed based on the
    above region proposals, in which the vertex variable set X , the factor set F
    and the related edge set E are all associated with the corresponding feature vectors.
    With such a feature vector factor graph, one can infer the resulting marginals
    via stacking various NBP layers plus a final MLP. Each of the NBP layers consists
    of two message passing neural network (MPNN) models: Factor-to-Variable MPNN and
    Variable-to-Factor MPNN, which essentially correspond to the two types of messages
    within the classical belief propagation method. Given the resulting marginals
    of the factor graph G , one can easily compute the optimum interpretations (for
    related vertex variables and pairwise factors, which corresponds to instances
    and predicates in a scene graph) via simple argmax operations. A cross-entropy
    loss is applied to train the proposed NBP method.'
  Figure 3 Link: articels_figures_by_rev_year\2023\Neural_Belief_Propagation_for_Scene_Graph_Generation\figure_3.jpg
  Figure 3 caption: An example factor graph. f 1 corresponds to 1-vertex (subset with
    only one vertex variable x 1 ) clique, while f 2 and f 3 have 2-vertex (subsets
    with two vertex variables x 1 , x 3 and x 2 , x 3 , respectively) cliques.
  Figure 4 Link: articels_figures_by_rev_year\2023\Neural_Belief_Propagation_for_Scene_Graph_Generation\figure_4.jpg
  Figure 4 caption: 'Three disjoint category groups in Visual Genome training split
    (follows a long-tail data distribution as demonstrated above): head (red bars),
    body (green bars) and tail (blue bars). y axis represents the number of samples.'
  Figure 5 Link: articels_figures_by_rev_year\2023\Neural_Belief_Propagation_for_Scene_Graph_Generation\figure_5.jpg
  Figure 5 caption: Comparison of the mR100 performance (represented as black dots)
    for each predicate category with the proposed NBP method and the resulting NBP+BA
    algorithm. Here, y axis denotes the min-max normalized frequency. Compared with
    the NBP method, the resulting NBP+BA algorithm has more balanced training samples,
    in which the informative tail (blue bars) and body (green bars) predicate categories
    have comparable training samples as the common head (red bars) predicate categories.
  Figure 6 Link: articels_figures_by_rev_year\2023\Neural_Belief_Propagation_for_Scene_Graph_Generation\figure_6.jpg
  Figure 6 caption: Visualization of the qualitative results of the ground-truth (GT),
    the baseline model BGNN, the ablated variant NBP without higher-order dependencies,
    the proposed NBP method and the derived NBP+BA algorithm in the SGDet task. The
    black, orange and green arrows represent the triplets with head predicate categories,
    the triplets with body or tail predicate categories and the reasonable triplets
    detected by models which are not included in GT, respectively. Compared with the
    baseline model BGNN, the scene graphs generated by the proposed NBP method and
    the derived NBP+BA algorithm are much closer to the ground-truth scene graph GT.
    Moreover, by adding higher-order dependencies, the proposed NBP method could potentially
    detect more triplet structures.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Daqi Liu
  Name of the last author: Josef Kittler
  Number of Figures: 6
  Number of Tables: 6
  Number of authors: 3
  Paper title: Neural Belief Propagation for Scene Graph Generation
  Publication Date: 2023-02-08 00:00:00
  Table 1 caption:
    table_text: TABLE I A Performance Comparison on Visual Genome Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Performance Comparison on the Visual Genome Dataset With
      a Balance Adjustment Strategy
  Table 3 caption:
    table_text: TABLE III A Performance Comparison on the Open Images V6 Dataset
  Table 4 caption:
    table_text: TABLE IV An Ablation Study of Different Types of Aggregators
  Table 5 caption:
    table_text: TABLE V An Ablation Study of Different Types of Scoring Functions
  Table 6 caption:
    table_text: TABLE VI An Ablation Study of the Bias-Variance Trade-Off
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3243306
- Affiliation of the first author: national key laboratory for multimedia information
    processing, school of computer science, peking university, beijing, china
  Affiliation of the last author: national key laboratory for multimedia information
    processing, school of computer science, peking university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2023\Contextual_Instance_Decoupling_for_InstanceLevel_Human_Analysis\figure_1.jpg
  Figure 1 caption: Illustration of several Instance-Level Human Analysis tasks. (a)
    Multi-person pose estimation estimates predefined keypoint locations for each
    person. (b) Multi-person segmentation densely predicts mask for each person. (c)
    Multi-person part segmentation predicts body part mask for each person. It is
    challenging to decouple overlapped persons, especially for crowded scenes.
  Figure 10 Link: articels_figures_by_rev_year\2023\Contextual_Instance_Decoupling_for_InstanceLevel_Human_Analysis\figure_10.jpg
  Figure 10 caption: Illustration of sampled mutli-person part segmentation results
    of CID on CIHP [23] and MHP v2.0 [20] datasets.
  Figure 2 Link: articels_figures_by_rev_year\2023\Contextual_Instance_Decoupling_for_InstanceLevel_Human_Analysis\figure_2.jpg
  Figure 2 caption: Illustration of different pipelines for differentiating persons
    in multi-person pose estimation. (a) Top-down methods use bounding box to crop
    person; (b) Bottom-up methods first detect all keypoints then group them into
    different persons; (c) Single-stage methods directly regress keypoint coordinates
    based on sampled feature vector. (d) The proposed Contextual Instance Decoupling
    (CID) first generates instance-aware feature maps, then infers heatmaps from each
    person. It has potential to enjoy better robustness to detection errors in (a),
    keypoint localization errors in (b), and alleviate the difficulty of long distance
    regression in (c).
  Figure 3 Link: articels_figures_by_rev_year\2023\Contextual_Instance_Decoupling_for_InstanceLevel_Human_Analysis\figure_3.jpg
  Figure 3 caption: Pipeline of the proposed Contextual Instance Decoupling (CID).
    CID uses a CNN to extract the feature map. Instance Information Abstraction (IIA)
    extracts the location and feature to represent each person. Global Feature Decoupling
    (GFD) modulates the original feature map to produce instance-aware feature maps,
    each of which is fed to task specific head to estimate heatmap or mask for a person,
    respectively.
  Figure 4 Link: articels_figures_by_rev_year\2023\Contextual_Instance_Decoupling_for_InstanceLevel_Human_Analysis\figure_4.jpg
  Figure 4 caption: Visualization of center heatmap C . IIA can identify each person
    and estimate their corresponding center location.
  Figure 5 Link: articels_figures_by_rev_year\2023\Contextual_Instance_Decoupling_for_InstanceLevel_Human_Analysis\figure_5.jpg
  Figure 5 caption: "Visualization of instance-aware feature maps and keypoint heatmaps\
    \ of persons highlighted by bounding boxes. \u2113( f (i) ) boosts the discriminative\
    \ power of person feature, making CID more robust to occlusions and distractions\
    \ from neighboring persons with similar appearance. With \u2113( f (i) ) , the\
    \ instance-aware feature map can better focus on each person foreground, and ensures\
    \ the generation of reliable keypoint heatmaps."
  Figure 6 Link: articels_figures_by_rev_year\2023\Contextual_Instance_Decoupling_for_InstanceLevel_Human_Analysis\figure_6.jpg
  Figure 6 caption: "Detailed architecture of CID components, e.g., backbone with\
    \ HRNet-W32 [4], IIA module, GFD module and final task head. 'Rec.' is the abbreviation\
    \ of recalibration. n denotes the number of classes in different task. A rectangular\
    \ box with (1\xD71,c) denotes a convolutional layer with kernel size 1 and output\
    \ channel dimension c . The symbol c\u2212d denotes a tensor with channel dimension\
    \ c ."
  Figure 7 Link: articels_figures_by_rev_year\2023\Contextual_Instance_Decoupling_for_InstanceLevel_Human_Analysis\figure_7.jpg
  Figure 7 caption: Efficiency analysis of CID. (a) shows inference time of each component
    w.r.t. different numbers of persons. (b) compares efficiency of CID with two variants
    of RoIAlign [1].
  Figure 8 Link: articels_figures_by_rev_year\2023\Contextual_Instance_Decoupling_for_InstanceLevel_Human_Analysis\figure_8.jpg
  Figure 8 caption: Illustration of sampled pose estimation results of CID from (a)
    COCO [18], (b) CrowdPose [19], and (c) OCHuman [20].
  Figure 9 Link: articels_figures_by_rev_year\2023\Contextual_Instance_Decoupling_for_InstanceLevel_Human_Analysis\figure_9.jpg
  Figure 9 caption: Illustration of sampled multi-person segmentation results of CID
    on COCO [18] and OCHuman [20] datasets.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.63
  Name of the first author: Dongkai Wang
  Name of the last author: Shiliang Zhang
  Number of Figures: 10
  Number of Tables: 10
  Number of authors: 2
  Paper title: Contextual Instance Decoupling for Instance-Level Human Analysis
  Publication Date: 2023-02-08 00:00:00
  Table 1 caption:
    table_text: TABLE I Validity of Contrastive Loss, Spatial and Channel Recalibration
      in CID on COCO val Set
  Table 10 caption:
    table_text: TABLE X Comparison With Recent Works of Multi-Person Part Segmentation
      on MHP v2.0 val Set
  Table 2 caption:
    table_text: TABLE II Performance With Different Embedding Dimensions on COCO val
      Set
  Table 3 caption:
    table_text: TABLE III Efficiency Comparison Between CID and Other Methods. 'mem'
      Refers to Memory Consumption During Inference, Which is Tested on COCO val Set
      With Batchsize 1 and a Single RTX 3090. Compared Methods are Tested With Codes
      Provided by Authors
  Table 4 caption:
    table_text: "TABLE IV Comparison With Recent Works of Multi-Person Pose Estimation\
      \ on COCO test-Dev Set. \u2217 Denotes Using Refinement and + + Denotes Using\
      \ Two Models. FPS is Measured on a Single RTX 2080Ti"
  Table 5 caption:
    table_text: "TABLE V Comparison With Recent Works of Multi-Person Pose Estimation\
      \ on CrowdPose test Set. \u2217 Denotes Using Refinement"
  Table 6 caption:
    table_text: TABLE VI Comparison With Recent Works of Multi-Person Pose Estimation
      on OCHuman val and test Set Under Two Evaluation Settings. The Second Column
      Denotes the First Evaluation Protocol in [41] and the Third and Fourth Columns
      Denote the Second Protocol Used in [11]
  Table 7 caption:
    table_text: "TABLE VII Comparison With Recent Works of Multi-Person Segmentation\
      \ on COCO val Set. \u2020 \u2020 Denotes Our Implementation Based on the Authors'\
      \ Code"
  Table 8 caption:
    table_text: "TABLE VIII Comparison With Recent Works of Multi-Person Segmentation\
      \ on OCHuman val and test Set. \u2020 \u2020 Denotes Our Implementation Based\
      \ on the Authors' Code"
  Table 9 caption:
    table_text: TABLE IX Comparison With Recent Works of Multi-Person Part Segmentation
      on CIHP val Set
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3243223
- Affiliation of the first author: national engineering research center of visual
    technology, school of computer science, peking university, beijing, china
  Affiliation of the last author: national engineering research center of visual technology,
    school of computer science, peking university, beijing, china
  Figure 1 Link: "articels_figures_by_rev_year\\2023\\Coarsetofine_Disentangling_Demoir\xE9\
    ing_Framework_for_Recaptured_Screen_Images\\figure_1.jpg"
  Figure 1 caption: "A typical scenario of capturing the content on screens with smartphones.\
    \ The blue dashed boxes emphasize moir\xE9 pattern's spatially-varying structure.\
    \ With the proposed demoir\xE9ing framework, the moir\xE9 pattern in the recaptured\
    \ screen image can be eliminated as shown on the right."
  Figure 10 Link: "articels_figures_by_rev_year\\2023\\Coarsetofine_Disentangling_Demoir\xE9\
    ing_Framework_for_Recaptured_Screen_Images\\figure_10.jpg"
  Figure 10 caption: 'Top: Non-local block helps the weak edge (red point) obtain
    a stronger response by strengthening its correlations with other edges. Bottom:
    Visualization of the predicted edge attention extracted from channel-wise input
    edge maps, supervised by normalized clean edges.'
  Figure 2 Link: "articels_figures_by_rev_year\\2023\\Coarsetofine_Disentangling_Demoir\xE9\
    ing_Framework_for_Recaptured_Screen_Images\\figure_2.jpg"
  Figure 2 caption: "Illustration of the proposed coarse-to-fine disentangling demoir\xE9\
    ing framework, consisting of two stages: The coarse layer disentanglement stage\
    \ that predicts a clean content layer and a moir\xE9 pattern layer by multi-scale\
    \ feature aggregation, and the refinement stage featuring dual-domain residue\
    \ removal and edge attention."
  Figure 3 Link: "articels_figures_by_rev_year\\2023\\Coarsetofine_Disentangling_Demoir\xE9\
    ing_Framework_for_Recaptured_Screen_Images\\figure_3.jpg"
  Figure 3 caption: "The formation process of moir\xE9 images alongside the imaging\
    \ pipeline. The lower left corner illustrates the false pattern in signal aliasing\
    \ (1). The dashed arrows reveal the relationship between moir\xE9 image formation\
    \ and the demoir\xE9ing solution."
  Figure 4 Link: "articels_figures_by_rev_year\\2023\\Coarsetofine_Disentangling_Demoir\xE9\
    ing_Framework_for_Recaptured_Screen_Images\\figure_4.jpg"
  Figure 4 caption: The higher energy ratio of Top-10 singular values stands for higher
    image self-similarity, and when the ratio is above 0.7, the low-rank approximation
    can well describe the fine structures [24].
  Figure 5 Link: "articels_figures_by_rev_year\\2023\\Coarsetofine_Disentangling_Demoir\xE9\
    ing_Framework_for_Recaptured_Screen_Images\\figure_5.jpg"
  Figure 5 caption: "Left: Comparing the spectra, we can see that the moir\xE9 pattern's\
    \ energy is more centralized in non-DC components than the natural image. Right:\
    \ Visualization of the 8times 8 patch-wise coefficients of different DCT subbands,\
    \ where the moir\xE9 pattern only shows evident response on band-3."
  Figure 6 Link: "articels_figures_by_rev_year\\2023\\Coarsetofine_Disentangling_Demoir\xE9\
    ing_Framework_for_Recaptured_Screen_Images\\figure_6.jpg"
  Figure 6 caption: The Bayer CFA and the edge maps on different sRGB color channels
    (better viewed when zoomed in).
  Figure 7 Link: "articels_figures_by_rev_year\\2023\\Coarsetofine_Disentangling_Demoir\xE9\
    ing_Framework_for_Recaptured_Screen_Images\\figure_7.jpg"
  Figure 7 caption: "Visualization of the multi-scale feature aggregation in the moir\xE9\
    \ pattern layer disentanglement branch."
  Figure 8 Link: "articels_figures_by_rev_year\\2023\\Coarsetofine_Disentangling_Demoir\xE9\
    ing_Framework_for_Recaptured_Screen_Images\\figure_8.jpg"
  Figure 8 caption: "An example of disentanglement results. Red boxes mark the regions\
    \ containing moir\xE9 pattern residues."
  Figure 9 Link: "articels_figures_by_rev_year\\2023\\Coarsetofine_Disentangling_Demoir\xE9\
    ing_Framework_for_Recaptured_Screen_Images\\figure_9.jpg"
  Figure 9 caption: 'Top: Illustration of the convolutional IDCT layer. Bottom: Visualization
    of different DCT domain feature channels before the last convolutional IDCT layer.'
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Ce Wang
  Name of the last author: Ling-Yu Duan
  Number of Figures: 16
  Number of Tables: 5
  Number of authors: 6
  Paper title: "Coarse-to-fine Disentangling Demoir\xE9ing Framework for Recaptured\
    \ Screen Images"
  Publication Date: 2023-02-08 00:00:00
  Table 1 caption:
    table_text: TABLE I Quantitative Comparison on TIP-2018 Dataset [22] Adopting
      the Training Setup in [4]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Qualitative Model Performance Comparisons When DMCNN [22]
      (0.3M Parameters), MBCNN [36] (14.9M Parameters), and the Proposed Model (15.4M
      Parameters) are Trained With Data of Different Quantities and Tested on the
      Same Testing set
  Table 3 caption:
    table_text: TABLE III Quantitative Comparison and Model Efficiency Analysis on
      FHDMi Dataset [3]
  Table 4 caption:
    table_text: TABLE IV Model Generalization Analysis on Different Data Sources (Measured
      by PSNR on [29], and PSNRSSIM on [31]
  Table 5 caption:
    table_text: "TABLE V Quantitative Results of Different Model Variants Trained\
      \ on TIP-2018- 10% 10% Subset [22]. DE, FR, EA, SRC, and LRC Stand for the Proposed\
      \ Disentanglement for the Moir\xE9 Pattern Layer, the Frequency Domain Based\
      \ Moir\xE9 Residue Removal, the Edge Attention, the Self-Reconstruction Constraint,\
      \ and the Low-Rank Constraint, Respectively"
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3243310
- Affiliation of the first author: school of computer science and engineering, southeast
    university, nanjing, jiangsu, china
  Affiliation of the last author: school of computer science and engineering, southeast
    university, nanjing, jiangsu, china
  Figure 1 Link: articels_figures_by_rev_year\2023\From_Instance_to_Metric_Calibration_A_Unified_Framework_for_OpenWorld_FewShot_Le\figure_1.jpg
  Figure 1 caption: An example of open-world few-shot learning.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\From_Instance_to_Metric_Calibration_A_Unified_Framework_for_OpenWorld_FewShot_Le\figure_2.jpg
  Figure 2 caption: Overview of the proposed IDEAL (a) The generation of intra-class
    weight and inter-class weight. (b) A brief illustration of the dual-networks structure
    for IDEAL. (c) The architecture of ProtoMod strategy.
  Figure 3 Link: articels_figures_by_rev_year\2023\From_Instance_to_Metric_Calibration_A_Unified_Framework_for_OpenWorld_FewShot_Le\figure_3.jpg
  Figure 3 caption: Ideal illustration of ensemble with consistency principle.
  Figure 4 Link: articels_figures_by_rev_year\2023\From_Instance_to_Metric_Calibration_A_Unified_Framework_for_OpenWorld_FewShot_Le\figure_4.jpg
  Figure 4 caption: Visualization of computation of ensemble with consistency similarity
    score.
  Figure 5 Link: articels_figures_by_rev_year\2023\From_Instance_to_Metric_Calibration_A_Unified_Framework_for_OpenWorld_FewShot_Le\figure_5.jpg
  Figure 5 caption: Intra-class weight and maximum inter-class weight of each sample
    in the studied OFSL task with type I ID noise. For each support sample, the class
    with the maximum inter-class weight and its corresponding value are given. The
    last column denotes the type I ID noise.
  Figure 6 Link: articels_figures_by_rev_year\2023\From_Instance_to_Metric_Calibration_A_Unified_Framework_for_OpenWorld_FewShot_Le\figure_6.jpg
  Figure 6 caption: Intra-class weight and maximum inter-class weight of each sample
    in the studied OFSL task with type II ID noise. For each support sample, the class
    with the maximum inter-class weight and its corresponding value are given. The
    last column denotes the type II ID noise.
  Figure 7 Link: articels_figures_by_rev_year\2023\From_Instance_to_Metric_Calibration_A_Unified_Framework_for_OpenWorld_FewShot_Le\figure_7.jpg
  Figure 7 caption: Intra-class weight and maximum inter-class weight of each sample
    in the studied OFSL task with OOD noise. For each support sample, the class with
    the maximum inter-class weight and its corresponding value are given. The last
    column denotes the OOD noise from CIFAR-FS dataset.
  Figure 8 Link: articels_figures_by_rev_year\2023\From_Instance_to_Metric_Calibration_A_Unified_Framework_for_OpenWorld_FewShot_Le\figure_8.jpg
  Figure 8 caption: Comparisons between different values of eta in different cases.
  Figure 9 Link: articels_figures_by_rev_year\2023\From_Instance_to_Metric_Calibration_A_Unified_Framework_for_OpenWorld_FewShot_Le\figure_9.jpg
  Figure 9 caption: Comparisons between different values of gamma in different cases.
  First author gender probability: 0.6
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Yuexuan An
  Name of the last author: Jing Wang
  Number of Figures: 9
  Number of Tables: 6
  Number of authors: 4
  Paper title: 'From Instance to Metric Calibration: A Unified Framework for Open-World
    Few-Shot Learning'
  Publication Date: 2023-02-10 00:00:00
  Table 1 caption:
    table_text: TABLE I Comparisons on FSL Tasks (5-Way 5-Shot, Acc.%)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Comparisons on OFSL Tasks with Varying Noise Rates of Type
      I ID Noise (5-Way 5-Shot, Acc.%)
  Table 3 caption:
    table_text: TABLE III Comparisons on OFSL Tasks with Varying Noise Rates of Type
      II ID Noise (5-Way 5-Shot, Acc.%)
  Table 4 caption:
    table_text: TABLE IV Comparisons on OFSL Tasks with Varying Noise Rates of OOD
      Noise (5-Way 5-Shot, Acc.%)
  Table 5 caption:
    table_text: TABLE V Ablation Results on OFSL Tasks under Different Cases on Mini
      ImageNet Dataset (5-Way 5-Shot, Acc.%)
  Table 6 caption:
    table_text: "TABLE VI Comparisons on OFSL Tasks (5-Way 5-Shot, Acc.%, Test on\
      \ C base \u22C3 C novel Cbase\u22C3Cnovel)"
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3244023
- Affiliation of the first author: german research center for artificial intelligence
    (dfki), kaiserslautern, germany
  Affiliation of the last author: german research center for artificial intelligence
    (dfki), kaiserslautern, germany
  Figure 1 Link: articels_figures_by_rev_year\2023\Hitchhikers_Guide_to_SuperResolution_Introduction_and_Recent_Advances\figure_1.jpg
  Figure 1 caption: 'Example images from different SR datasets: Set5 [31], Set14 [1],
    Manga109 [32], General100 [33], BSDS100 [2], and BSDS200 [2]. The ratio of the
    size differences is preserved.'
  Figure 10 Link: articels_figures_by_rev_year\2023\Hitchhikers_Guide_to_SuperResolution_Introduction_and_Recent_Advances\figure_10.jpg
  Figure 10 caption: Architecture design of MDSR [38]. It utilizes multi-path learning
    to select between multiple scaling factor paths (denoted by 2x, 3x, 4x on the
    bottom right of the blue boxes). Also, all paths share an intermediate feature
    extraction block to save parameters.
  Figure 2 Link: articels_figures_by_rev_year\2023\Hitchhikers_Guide_to_SuperResolution_Introduction_and_Recent_Advances\figure_2.jpg
  Figure 2 caption: Principle of DDPMs. The Gaussian diffusion process adds noise
    iteratively. The iterative refinement process reverts the process. The task of
    the SR model is to predict the noise added between two iterations. The predicted
    noise is then used to revert one iteration.
  Figure 3 Link: articels_figures_by_rev_year\2023\Hitchhikers_Guide_to_SuperResolution_Introduction_and_Recent_Advances\figure_3.jpg
  Figure 3 caption: Channel-attention mechanism [74]. It reduces a feature map in
    the spatial dimensions and extracts weighting values by using several FC layers
    that are element-wise multiplied to the initial feature map.
  Figure 4 Link: articels_figures_by_rev_year\2023\Hitchhikers_Guide_to_SuperResolution_Introduction_and_Recent_Advances\figure_4.jpg
  Figure 4 caption: Spatial-attention mechanism [76]. It extracts informations by
    inspecting the relationship between two positions (first matrix multiplication)
    and returns the importance of each position as a feature map (second matrix multiplication).
    MM denotes the matrix multiplication.
  Figure 5 Link: articels_figures_by_rev_year\2023\Hitchhikers_Guide_to_SuperResolution_Introduction_and_Recent_Advances\figure_5.jpg
  Figure 5 caption: 'Visualization of different upsampling locations within a neural
    network: (upper left) post-upsampling, (bottom left) pre-upsampling, (upper right)
    progressive upsampling, and (bottom right) iterative up-and-down upsampling.'
  Figure 6 Link: articels_figures_by_rev_year\2023\Hitchhikers_Guide_to_SuperResolution_Introduction_and_Recent_Advances\figure_6.jpg
  Figure 6 caption: Architecture designs of SRCNN [39], FSRCNN [33], ESPCN [66] and
    LapSRNMS-LapSRN [44]. They are grouped together in this work as simple network
    designs, because they use only convolutional operations and upsampling methods.
  Figure 7 Link: articels_figures_by_rev_year\2023\Hitchhikers_Guide_to_SuperResolution_Introduction_and_Recent_Advances\figure_7.jpg
  Figure 7 caption: Architecture design of VDSR [98]. It applies a deep convolutional
    network as feature extractor but also uses a residual from the input to the final
    prediction. Therefore, the feature extractor can concentrate to add high-frequency
    details onto the interpolated image.
  Figure 8 Link: articels_figures_by_rev_year\2023\Hitchhikers_Guide_to_SuperResolution_Introduction_and_Recent_Advances\figure_8.jpg
  Figure 8 caption: Pixel value distribution of an example image from BSDS100 [2].
    It can be observed that the distribution of the LR and HR image are skewed. In
    contrary, the distribution of the difference between LR and HR looks normally
    distributed, which is easier learned by an SR model.
  Figure 9 Link: articels_figures_by_rev_year\2023\Hitchhikers_Guide_to_SuperResolution_Introduction_and_Recent_Advances\figure_9.jpg
  Figure 9 caption: Recurrent-based architecture designs of DRCN [105] (upper left),
    DRRN [65] (upper right), and MemNet [92] (bottom). The DRCN applies the same convolutional
    layer multiple times. The DRNN extends this idea to recursive application of a
    residual block. The MemNet introduces a gate unit to distill most important features
    from all intermediate convolution results in a recursive manner.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Brian B. Moser
  Name of the last author: Andreas Dengel
  Number of Figures: 15
  Number of Tables: 1
  Number of authors: 6
  Paper title: 'Hitchhiker''s Guide to Super-Resolution: Introduction and Recent Advances'
  Publication Date: 2023-02-10 00:00:00
  Table 1 caption:
    table_text: 'TABLE I Comparison (PSNRSSIM) of SR Approaches Discussed in This
      Work: Simple, Residual, Recurrent, Attention-Based, Lightweight and Wavelet
      Networks, as Well as NAS Derived Networks and Unsupervised Trained Models. The
      Ordering Reflects Roughly the Performance Ranking. However, Some Comparisons
      are not Fair Due to Different Settings, e.g., NAS versus Unsupervised Results.
      Also, Some Publications do not Report Results on the Commonly Used Datasets.
      Therefore, They are not Listed (e.g., WESPE [128], DSN [133], CinCGan [52])'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3243794
- Affiliation of the first author: biomedical signal and image processing laboratory
    (bisipl), school of electrical engineering, sharif university of technology, tehran,
    iran
  Affiliation of the last author: biomedical signal and image processing laboratory
    (bisipl), school of electrical engineering, sharif university of technology, tehran,
    iran
  Figure 1 Link: articels_figures_by_rev_year\2023\Tractable_Maximum_Likelihood_Estimation_for_Latent_Structure_Influence_Models_Wi\figure_1.jpg
  Figure 1 caption: Convergence of proposed algorithm in the estimation of LSIMs parameters
    from simulated multi-channel time-series for 3 and 5 channels CHMMs.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Tractable_Maximum_Likelihood_Estimation_for_Latent_Structure_Influence_Models_Wi\figure_2.jpg
  Figure 2 caption: Convergence of proposed EM-algorithm in the estimation of LSIM
    parameters considering the various number of real EEG channels and subjects.
  Figure 3 Link: articels_figures_by_rev_year\2023\Tractable_Maximum_Likelihood_Estimation_for_Latent_Structure_Influence_Models_Wi\figure_3.jpg
  Figure 3 caption: Convergence of proposed algorithm in the estimation of LSIM parameters
    considering a 64-channel ECoG.
  Figure 4 Link: articels_figures_by_rev_year\2023\Tractable_Maximum_Likelihood_Estimation_for_Latent_Structure_Influence_Models_Wi\figure_4.jpg
  Figure 4 caption: Analysis of AIC and BIC for HMM, CHMM and LSIM considering embedded
    Lorenz systems.
  Figure 5 Link: articels_figures_by_rev_year\2023\Tractable_Maximum_Likelihood_Estimation_for_Latent_Structure_Influence_Models_Wi\figure_5.jpg
  Figure 5 caption: Coupling weights of the selected model based on BIC for embedded
    Lorenz system.
  Figure 6 Link: articels_figures_by_rev_year\2023\Tractable_Maximum_Likelihood_Estimation_for_Latent_Structure_Influence_Models_Wi\figure_6.jpg
  Figure 6 caption: Analysis of AIC and BIC for HMM, CHMM and LSIM considering multichannel
    ECoG.
  Figure 7 Link: articels_figures_by_rev_year\2023\Tractable_Maximum_Likelihood_Estimation_for_Latent_Structure_Influence_Models_Wi\figure_7.jpg
  Figure 7 caption: Coupling weights of the selected model based on BIC for Macaque
    ECoG dataset.
  Figure 8 Link: articels_figures_by_rev_year\2023\Tractable_Maximum_Likelihood_Estimation_for_Latent_Structure_Influence_Models_Wi\figure_8.jpg
  Figure 8 caption: Log-likelihood convergence paths of exact and approximate EM algorithms
    (4-channel LSIM with five states and one Gaussian component). (a) Simulated data
    from a 4-channel CHMM (b) Real data from the 4 EEG channels of subject 2 in EPFL
    dataset.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Sajjad Karimi
  Name of the last author: Mohammad Bagher Shamsollahi
  Number of Figures: 8
  Number of Tables: 5
  Number of authors: 2
  Paper title: Tractable Maximum Likelihood Estimation for Latent Structure Influence
    Models With Applications to EEG & ECoG Processing
  Publication Date: 2023-02-10 00:00:00
  Table 1 caption:
    table_text: TABLE I LSIM and CHMM Learning Computation Times (seconds) for Hidden
      States (M) Per Channel Varying From 2 to 4
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Classification Accuracy for SVM, HMM, CHMM and LSIM in 3-Channel
      Simulated CHMMs
  Table 3 caption:
    table_text: TABLE III Classification Accuracy for SVM, HMM, CHMM and LSIM in 5-Channel
      Simulated CHMMs
  Table 4 caption:
    table_text: TABLE IV Verification AUC Results for Session 2 When the System is
      Trained With Data From Session 1
  Table 5 caption:
    table_text: TABLE V Verification AUC Results for Session 3 When the System is
      Trained With Data From Session 1
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3244130
- Affiliation of the first author: national engineering research center of visual
    technology, school of computer science, peking university, beijing, china
  Affiliation of the last author: national engineering research center of visual technology,
    school of computer science, peking university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2023\MILO_MultiBounce_Inverse_Rendering_for_Indoor_Scene_With_LightEmitting_Objects\figure_1.jpg
  Figure 1 caption: Three virtual balls (light-emitting, specular, and diffuse) are
    inserted into a real indoor scene. MILO renders photo-realistic images by reasonably
    demonstrating complex cast shadows, multi-bounce effect between scene objects
    and virtual objects, and virtual light illumination. For example, the specular
    reflection of the ceiling light is partially occluded by the diffuse ball. The
    green light source also illuminates the walls and the ceiling to green, which
    are reflected by the specular ball.
  Figure 10 Link: articels_figures_by_rev_year\2023\MILO_MultiBounce_Inverse_Rendering_for_Indoor_Scene_With_LightEmitting_Objects\figure_10.jpg
  Figure 10 caption: "Qualitative comparison for virtual object insertion on a synthetic\
    \ scene with Li et al. [25], Srinivasan et al. [42], Azinovi\u0107 et al. [2],\
    \ and David et al. [31]. Our method demonstrates lower error than other methods\
    \ on error maps. Li et al. [25]'s method inserts balls on image plane directly,\
    \ so the balls have circular shapes. Other methods insert balls in the 3D space\
    \ and use perspective camera model, so the balls have ellipse shape."
  Figure 2 Link: articels_figures_by_rev_year\2023\MILO_MultiBounce_Inverse_Rendering_for_Indoor_Scene_With_LightEmitting_Objects\figure_2.jpg
  Figure 2 caption: MILO pipeline. MILO-Net takes scene geometry as input to predict
    light-emitting and reflectance properties for differentiable path tracing. MILO-Renderer
    uses scene geometry, six parameter maps, and camera poses to render images. Rendered
    images are compared with captured images.
  Figure 3 Link: articels_figures_by_rev_year\2023\MILO_MultiBounce_Inverse_Rendering_for_Indoor_Scene_With_LightEmitting_Objects\figure_3.jpg
  Figure 3 caption: 'Three types of ambiguities during inverse rendering: (a) Multi-bounce
    ambiguity: the icon on the wall (on the left side) and the icon on the specular
    ball (on the right side) both render the same result on the specular ball; (b)
    Diffuse-emission ambiguity: the lower plate reflects light on the left side, while
    emitting light on the right side, consequently, the virtual ball on the right
    side has wrong shading and no shadow. (c) Specular uncertainty: the specular parameters
    are recovered only by the left side, however, if replacing the light source to
    a larger ball, the specular reflection is rendered correctly only in the small
    highlighted area on the right side.'
  Figure 4 Link: articels_figures_by_rev_year\2023\MILO_MultiBounce_Inverse_Rendering_for_Indoor_Scene_With_LightEmitting_Objects\figure_4.jpg
  Figure 4 caption: MILO-Net takes geometry points, geometry normal, and environment
    map (Envmap) points as inputs to predict all six fully spatially-varying parameter
    maps.
  Figure 5 Link: articels_figures_by_rev_year\2023\MILO_MultiBounce_Inverse_Rendering_for_Indoor_Scene_With_LightEmitting_Objects\figure_5.jpg
  Figure 5 caption: Rendering results using different importance strategies. All images
    are rendered using 256 samples per pixel. Our full importance sampling strategy
    achieves the best result. We further reduce noise by denoiser for final result
    visualization.
  Figure 6 Link: articels_figures_by_rev_year\2023\MILO_MultiBounce_Inverse_Rendering_for_Indoor_Scene_With_LightEmitting_Objects\figure_6.jpg
  Figure 6 caption: Input image examples and geometries of 4 synthetic scenes and
    5 real scenes. Our dataset covers different categories of rooms in indoor scenes.
  Figure 7 Link: articels_figures_by_rev_year\2023\MILO_MultiBounce_Inverse_Rendering_for_Indoor_Scene_With_LightEmitting_Objects\figure_7.jpg
  Figure 7 caption: Recovered parameter maps for two real scenes and the rendered
    images. The specular highlights (green circle) and cast shadows (blue double circle)
    are removed in diffuse and specular albedo. Only the ceiling lights are emitting
    light.
  Figure 8 Link: articels_figures_by_rev_year\2023\MILO_MultiBounce_Inverse_Rendering_for_Indoor_Scene_With_LightEmitting_Objects\figure_8.jpg
  Figure 8 caption: "Comparison with methods by Azinovi\u0107 et al. [2] and David\
    \ et al. [31] on synthetic scene. There are two different materials on this sink\
    \ (the specular top surface and the diffuse front surface). Methods by Azinovi\u0107\
    \ et al. [2] and David et al. [31] rely on object segmentation, resulting in artifacts\
    \ in the blue circle. Our method doesn't require object segmentation as input\
    \ and predicts the specular highlight in the blue circle correctly."
  Figure 9 Link: articels_figures_by_rev_year\2023\MILO_MultiBounce_Inverse_Rendering_for_Indoor_Scene_With_LightEmitting_Objects\figure_9.jpg
  Figure 9 caption: Recovery results of Light-emitting objects and materials using
    synthetic data. The recovered light-emitting object and surface reflectance are
    similar to ground truth. Please zoom in to see the details.
  First author gender probability: 0.53
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Bohan Yu
  Name of the last author: Boxin Shi
  Number of Figures: 17
  Number of Tables: 3
  Number of authors: 6
  Paper title: 'MILO: Multi-Bounce Inverse Rendering for Indoor Scene With Light-Emitting
    Objects'
  Publication Date: 2023-02-13 00:00:00
  Table 1 caption:
    table_text: TABLE I Comparison Among Recent Works on Indoor Scene Inverse Rendering
      Using Intrinsic Image Decomposition (IID) [25], [55], Lighting Estimation (LE)
      [30], [42], Inverse Light Transport (ILT) [6], [20], [50], and Differentiable
      Rendering (DR) [2], [31] (Including Ours) Methods. Inputs are Abbreviated as
      S. Img. (Single Image), M. Img. (Multi-View Image), Geo. (Geometry), and Seg.
      (Object Segmentation). Reflectance Models are Abbreviated as P-SVBRDF (Partially
      Spatially Varying BRDF) and F-SVBRDF (Fully Spatially Varying BRDF)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Detailed Information of 4 Synthetic Scenes and 5 Real Scenes.
      We Evaluate MILO on Different Combinations of Light Source Configurations
  Table 3 caption:
    table_text: TABLE III Quantitative Comparison With Four Methods on Synthetic Scenes.
      MILO Achieves the Best Quantitative Performance Regarding All Error Metrics
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3244658
- Affiliation of the first author: department of computer science, city university
    of hong kong, hong kong
  Affiliation of the last author: school of computer science and engineering, nanyang
    technological university, singapore
  Figure 1 Link: articels_figures_by_rev_year\2023\FlatteningNet_Deep_Regular_D_Representation_for_D_Point_Cloud_Analysis\figure_1.jpg
  Figure 1 caption: Given an irregular point cloud of arbitrary geometric and topological
    structures, Flattening-Net generates a regular point geometry image (PGI) that
    encodes point coordinates as pixel colors while preserving local neighborhood
    consistency effectively. In the first stage, the input 3D points are embedded
    onto a 2D unit square domain through the SFM module. In the second stage, we resample
    the embedding points on lattice grids to produce a regular PGI. As a generic representation
    modality for point clouds, PGI naturally fits into a rich variety of high-level
    and low-level downstream applications driven by specific task networks.
  Figure 10 Link: articels_figures_by_rev_year\2023\FlatteningNet_Deep_Regular_D_Representation_for_D_Point_Cloud_Analysis\figure_10.jpg
  Figure 10 caption: Visual comparison of point cloud upsampling results. Given (a)
    input sparse point clouds and (b) dense ground-truths, we present typical upsampling
    examples generated by (c) PU-Net [73], (d) MPU [84], (e) PU-GAN [74], (f) PUGeo-Net
    [75], and (g) our FlatNet-Ups.
  Figure 2 Link: articels_figures_by_rev_year\2023\FlatteningNet_Deep_Regular_D_Representation_for_D_Point_Cloud_Analysis\figure_2.jpg
  Figure 2 caption: Visualization of 2D PGI representation structures generated from
    diverse object-level and scene-level 3D point clouds, where we colorize input
    points, planar embeddings, and the resulting PGIs according to their correspondence
    relationships, such that pixel colors uniquely reflect spatial positions. It is
    observed that the generated PGIs have smooth color distributions, indicating that
    spatial continuity is effectively maintained during such a surface-to-plane mapping
    process.
  Figure 3 Link: articels_figures_by_rev_year\2023\FlatteningNet_Deep_Regular_D_Representation_for_D_Point_Cloud_Analysis\figure_3.jpg
  Figure 3 caption: Overall workflows of G2SD and S2PF in terms of learning point-wise
    mappings between 2D and 3D domains. (a) The G2SD architecture, formulated as an
    auto-encoder, tends to deform fixed 2D lattice grids to reconstruct the input
    3D point cloud. (b) The S2PF architecture solves the opposite problem, which explicitly
    maps the input 3D points onto a 2D plane driven by a repulsion loss.
  Figure 4 Link: articels_figures_by_rev_year\2023\FlatteningNet_Deep_Regular_D_Representation_for_D_Point_Cloud_Analysis\figure_4.jpg
  Figure 4 caption: Comparison of G2SD and S2PF in terms of regular geometry parameterization.
    Given a target shape represented by N spatial points, G2SD reconstructs an Nprime
    times 3 point cloud that is further reshaped as a 3 times nprime times nprime
    2D image, S2PF directly maps input points onto a 2D plane and then performs resampling
    on lattice grids to construct a 2D image. We treat point coordinates as pixel
    colors to visualize the corresponding three-channel images.
  Figure 5 Link: articels_figures_by_rev_year\2023\FlatteningNet_Deep_Regular_D_Representation_for_D_Point_Cloud_Analysis\figure_5.jpg
  Figure 5 caption: In grid resampling, we map irregular planar embeddings to regular
    pixels. In general, resampling with sparse grids may yield significant information
    loss (a), while resampling with dense grids can effectively reduce such loss (b).
  Figure 6 Link: articels_figures_by_rev_year\2023\FlatteningNet_Deep_Regular_D_Representation_for_D_Point_Cloud_Analysis\figure_6.jpg
  Figure 6 caption: Illustration of the proposed CSConv operator directly working
    on PGI representation structures to achieve efficient and scalable regional embedding.
    Given a geometry image block whose pixels are resampled from embedded spatial
    points, we sequentially partition the whole block scope into innermost-, intermediate-,
    and outermost-squares. Treating each square as a point set, we adopt shared MLPs
    followed by channel-wise max-pooling to output vectorized square-wise embeddings,
    which are concatenated in order and further fused through a separate FC layer
    into a structural codeword. In parallel, we perform absolute coordinates embedding
    to generate a positional codeword. Finally, we concatenate and fuse the structural
    and positional codewords to obtain the regional embedding vector.
  Figure 7 Link: articels_figures_by_rev_year\2023\FlatteningNet_Deep_Regular_D_Representation_for_D_Point_Cloud_Analysis\figure_7.jpg
  Figure 7 caption: Comparison of classification accuracy, FLOPs, and latency of different
    methods when dealing with increasing number of input points.
  Figure 8 Link: articels_figures_by_rev_year\2023\FlatteningNet_Deep_Regular_D_Representation_for_D_Point_Cloud_Analysis\figure_8.jpg
  Figure 8 caption: Visual comparison of point cloud reconstruction results generated
    by (b) Baseline-Rec-M, (c) Baseline-Rec-F, and (d) our FlatNet-Rec for auto-encoding
    (a) the input point clouds. In particular, we also show the PGI representation
    structures corresponding to (a) and (d).
  Figure 9 Link: articels_figures_by_rev_year\2023\FlatteningNet_Deep_Regular_D_Representation_for_D_Point_Cloud_Analysis\figure_9.jpg
  Figure 9 caption: Illustration of the processing pipeline of FlatNet-Ups that implements
    3D point cloud upsampling as 2D PGI super-resolution. Given (a) an input sparse
    point cloud patch, we generate (b) an LR-PGI and then apply standard bicubic image
    interpolation to obtain (c) an enlarged LR-PGI, which corresponds to (d) the coarsely
    upsampled patch. After that, the initial enlarged LR-PGI is refined into the resulting
    (e) HR-PGI, which corresponds to the desired (f) dense upsampling result.
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.86
  Name of the first author: Qijian Zhang
  Name of the last author: Ying He
  Number of Figures: 10
  Number of Tables: 12
  Number of authors: 6
  Paper title: 'Flattening-Net: Deep Regular 2D Representation for 3D Point Cloud
    Analysis'
  Publication Date: 2023-02-14 00:00:00
  Table 1 caption:
    table_text: TABLE I Quantitative Geometry Fidelity Metrics Computed Between the
      Generated PGIs and the Original Point Clouds on ModelNet40 and ShapeNetPart
      Datasets Under Different Number of Input Points
  Table 10 caption:
    table_text: TABLE X Quantitative Performance of Point Cloud Reconstruction on
      ModelNet40 Measured by Chamfer Distance (CD) and Model Size (MS) Corresponding
      to Different Number of Input Points
  Table 2 caption:
    table_text: TABLE II Trade-Off Between Redundancy and Accuracy (Geometry Fidelity)
      When Generating PGI Representation Structures of Various Resolutions From Input
      Point Clouds Uniformly Containing 10000 Points
  Table 3 caption:
    table_text: "TABLE III Quantitative Neighborhood Consistency Metrics Derived From\
      \ Different Choices of J J and J \xAF J\xAF on ModelNet40, Where the Number\
      \ of Guidance Points (i.e., N G NG) is Uniformly Configured as 256"
  Table 4 caption:
    table_text: "TABLE IV Overall Accuracy (OA) of Different Deep Shape Classification\
      \ Methods on ModelNet40. \u201C \u2217 \u201D Means That Point-Wise Normals\
      \ are Consumed as Additional Input Attributes"
  Table 5 caption:
    table_text: TABLE V Ablative Analysis of CSConv on ModelNet40 Classification
  Table 6 caption:
    table_text: TABLE VI Performance of Real-Scanned Point Cloud Object Classification
      on the OBJONLY and OBJBG Settings of ScanObjectNN, in Which Our Flattening-Net
      is Pretrained on ShapeNetCore and Directly Applied to Generate PGIs on ScanObjectNN
      Without Fine-Tuning
  Table 7 caption:
    table_text: "TABLE VII Performance of Part Segmentation on ShapeNetPart Measured\
      \ by Mean Intersection-Over-Union (mIoU), Where \u201C \u2217 \u201D Means That\
      \ Point-Wise Normals are Consumed as Additional Input Attributes"
  Table 8 caption:
    table_text: TABLE VIII Ablative Analysis of CSConv on ShapeNetPart Segmentation
  Table 9 caption:
    table_text: TABLE IX Performance of Large-Scale Indoor Scene Segmentation on Area-5
      of S3DIS Measured by Mean Class Accuracy (mAcc) and Mean Intersection-Over-Union
      (mIoU)
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3244828
- Affiliation of the first author: department of state key laboratory of information
    engineering in surveying, mapping and remote sensing, wuhan university, wuhan,
    china
  Affiliation of the last author: department of state key laboratory of information
    engineering in surveying, mapping and remote sensing, wuhan university, wuhan,
    china
  Figure 1 Link: articels_figures_by_rev_year\2023\RoReg_Pairwise_Point_Cloud_Registration_With_Oriented_Descriptors_and_Local_Rota\figure_1.jpg
  Figure 1 caption: (Left) The oriented descriptors encode local orientation information.
    By correctly aligning the orientation of two descriptors, we are able to estimate
    a local rotation that is exactly equal to the global rotation used in registering
    two scans (Right).
  Figure 10 Link: articels_figures_by_rev_year\2023\RoReg_Pairwise_Point_Cloud_Registration_With_Oriented_Descriptors_and_Local_Rota\figure_10.jpg
  Figure 10 caption: Pipeline of the local rotation estimation.
  Figure 2 Link: articels_figures_by_rev_year\2023\RoReg_Pairwise_Point_Cloud_Registration_With_Oriented_Descriptors_and_Local_Rota\figure_2.jpg
  Figure 2 caption: (Left) Aligning local patch with local reference framework (LRF)
    brings rotation-invariance. (Right) The rotation-invariance brought by LRF is
    sensitive to noise and point density variations.
  Figure 3 Link: articels_figures_by_rev_year\2023\RoReg_Pairwise_Point_Cloud_Registration_With_Oriented_Descriptors_and_Local_Rota\figure_3.jpg
  Figure 3 caption: "Comparison between the D3Feat [6] detector (upper) and the proposed\
    \ rotation-guided detector (lower). Blocks (I) and (III) show the training strategy\
    \ of existing methods and ours in a training batch with 4 samples. Since no other\
    \ point in the batch has a similar descriptor to the red point, baseline methods\
    \ (I) increase the red point's detection score in this case. In comparison, our\
    \ method checks the correctness of the estimated local rotation to predict detection\
    \ scores. The red point is a wall-joint point, which cannot estimate an accurate\
    \ local rotation due to its 180 \u2218 orientation ambiguity, and thus has a low\
    \ detection score. (II) and (IV) show the detection scores of D3Feat and ours\
    \ respectively. (II) shows that D3Feats [6] includes less matchable keypoints\
    \ like wall joints and tends to generate incorrect matches. In RoReg (IV), we\
    \ are able to rule out these ambiguous points in the detection."
  Figure 4 Link: articels_figures_by_rev_year\2023\RoReg_Pairwise_Point_Cloud_Registration_With_Oriented_Descriptors_and_Local_Rota\figure_4.jpg
  Figure 4 caption: Rotation coherence is useful in feature matching. (I) Due to symmetry,
    Region A and Region B have very similar geometry and both can be matched with
    Region C. However, (II) the correspondences between Region A and Region C cannot
    estimate a proper rotation in the SO(3) space because there is an additional reflective
    transformation. (III) In the correct correspondence between B and C, we are able
    to correctly estimate a rotation on every correspondence. (IV) Thus, our method
    is able to handle such symmetric regions.
  Figure 5 Link: articels_figures_by_rev_year\2023\RoReg_Pairwise_Point_Cloud_Registration_With_Oriented_Descriptors_and_Local_Rota\figure_5.jpg
  Figure 5 caption: Comparison between the classical RANSAC (upper) and the proposed
    OSE-RANSAC (lower). The classical RANSAC samples correspondence triplets to generate
    transformation hypotheses on every triplet with the Kabsch algorithm, which contain
    many spurious solutions with low inlier ratios. By utilizing local rotations,
    our OSE-RANSAC generates a hypothesis with only one correspondence, which reduces
    the searching space and improves the registration quality, especially when the
    inlier ratio is low.
  Figure 6 Link: articels_figures_by_rev_year\2023\RoReg_Pairwise_Point_Cloud_Registration_With_Oriented_Descriptors_and_Local_Rota\figure_6.jpg
  Figure 6 caption: Overview of RoReg. We first extract RoReg-Desc (Section III-B)
    and illustrate how to estimate local rotations from RoReg-Descs in Section III-C.
    Then, a set of keypoints are detected from the RoReg-Descs by the rotation-guided
    detector (Section III-D). The detected keypoints are matched by the rotation coherence
    matcher to build correspondences (Section III-E). Finally, transformations are
    estimated by the OSE-RANSAC (Section III-F).
  Figure 7 Link: articels_figures_by_rev_year\2023\RoReg_Pairwise_Point_Cloud_Registration_With_Oriented_Descriptors_and_Local_Rota\figure_7.jpg
  Figure 7 caption: "(a) Icosahedral group contains k \u03C0 rotations around axes\
    \ through edge centers or 2 k \u03C0 3 rotations around axes through face centers\
    \ or 2 k \u03C0 5 rotations around axes through vertices. (b) The neighborhood\
    \ set H . Different colors are drawn on the vertices of an icosahedron. H contains\
    \ the identity element and 12 rotations that permute the identity vertices to\
    \ the right 12 vertices. Note all 12 rotations are 72 \u2218 rotations."
  Figure 8 Link: articels_figures_by_rev_year\2023\RoReg_Pairwise_Point_Cloud_Registration_With_Oriented_Descriptors_and_Local_Rota\figure_8.jpg
  Figure 8 caption: Pipeline of extracting a RoReg-Desc on a point.
  Figure 9 Link: articels_figures_by_rev_year\2023\RoReg_Pairwise_Point_Cloud_Registration_With_Oriented_Descriptors_and_Local_Rota\figure_9.jpg
  Figure 9 caption: Rotation-equivariance and rotation-invariance of RoReg-Desc. Applying
    a rotation in the icosahedral group will bring a permutation to the rotation-equivariant
    part F of RoReg-Desc. The row-wise pooling results in a descriptor d that is invariant
    to rotations in icosahedral group.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.66
  Name of the first author: Haiping Wang
  Name of the last author: Bisheng Yang
  Number of Figures: 20
  Number of Tables: 13
  Number of authors: 9
  Paper title: 'RoReg: Pairwise Point Cloud Registration With Oriented Descriptors
    and Local Rotations'
  Publication Date: 2023-02-14 00:00:00
  Table 1 caption:
    table_text: TABLE I Evaluation Results on 3DMatch and 3DLoMatch
  Table 10 caption:
    table_text: TABLE X Quantitative Results Using Different Numbers of Correspondences
  Table 2 caption:
    table_text: TABLE II Quantitative Results on 3DMatch and 3DLoMatch Comparing With
      Direct Registration Methods
  Table 3 caption:
    table_text: TABLE III Evaluation Results on ETH
  Table 4 caption:
    table_text: TABLE IV Ablation Studies on Components of RoReg.
  Table 5 caption:
    table_text: TABLE V The Settings of Models Used in Section IV-D
  Table 6 caption:
    table_text: TABLE VI Ablation Studies of RoReg-Desc on Original and Rotated 3DMatch
  Table 7 caption:
    table_text: TABLE VII Quantitative Performance Comparison of Different Detection
      Loss Functions
  Table 8 caption:
    table_text: TABLE VIII Inlier Ratio on 3DMatch and 3DLoMatch Using Keypoints Detected
      by D3Feat [6] or Our Rotation-Guided Detector
  Table 9 caption:
    table_text: TABLE IX Evaluation Results on the 3DMatch and 3DLoMatch Datasets.
      Note That, Randomly Sampled Points are Used in This Experiment to Produce the
      Results
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3244951
- Affiliation of the first author: state key lab of cad&cg, college of computer science,
    zhejiang university, zhejiang, china
  Affiliation of the last author: state key lab of cad&cg, college of computer science,
    zhejiang university, zhejiang, china
  Figure 1 Link: articels_figures_by_rev_year\2023\Implicit_Neural_Representations_With_Structured_Latent_Codes_for_Human_Body_Mode\figure_1.jpg
  Figure 1 caption: The basic idea of Neural Body. Neural Body generates implicit
    3D representations of a human body at different video frames from the same set
    of latent codes, which are anchored to the vertices of a deformable mesh. For
    each frame, we transform the spatial locations of codes based on the human pose,
    and use a network to regress the density and color for any 3D location based on
    the structured latent codes. Then, images at any viewpoints can be synthesized
    by the volume rendering.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Implicit_Neural_Representations_With_Structured_Latent_Codes_for_Human_Body_Mode\figure_2.jpg
  Figure 2 caption: Novel view synthesis of a performer from a sparse multi-view video.
    Neural Body captures the 3D geometry and appearance of the performer, which can
    be used for 3D reconstruction and novel view synthesis.
  Figure 3 Link: articels_figures_by_rev_year\2023\Implicit_Neural_Representations_With_Structured_Latent_Codes_for_Human_Body_Mode\figure_3.jpg
  Figure 3 caption: Implicit neural representation with structured latent codes. (a)
    The structured latent codes are input into a SparseConvNet which outputs a latent
    code volume. This process diffuses the input codes defined on the surface to nearby
    3D space. (b) For any 3D point, its latent code is obtained using trilinear interpolation
    from its neighboring vertices in the latent code volume and passed into MLP networks
    for density and color regression.
  Figure 4 Link: articels_figures_by_rev_year\2023\Implicit_Neural_Representations_With_Structured_Latent_Codes_for_Human_Body_Mode\figure_4.jpg
  Figure 4 caption: "Qualitative comparison on the ZJU-MoCap dataset. The results\
    \ of two subjects on the left are novel views of training human poses, and the\
    \ results of two subjects on the right are renderings of novel human poses. \u201C\
    NV\u201D means Neural Volumes [11], \u201CNT\u201D means Neural Textures [48],\
    \ and \u201COurs\u201D means Neural Body with implicit surface model."
  Figure 5 Link: articels_figures_by_rev_year\2023\Implicit_Neural_Representations_With_Structured_Latent_Codes_for_Human_Body_Mode\figure_5.jpg
  Figure 5 caption: "Qualitative comparison on the Human3.6 M dataset. We present\
    \ novel views of training human poses of two subjects on the left, and show rendering\
    \ results of novel human poses of two subjects on the right. \u201COurs\u201D\
    \ means Neural Body with implicit surface model."
  Figure 6 Link: articels_figures_by_rev_year\2023\Implicit_Neural_Representations_With_Structured_Latent_Codes_for_Human_Body_Mode\figure_6.jpg
  Figure 6 caption: 3D reconstruction on the RenderPeople dataset. IDR is trained
    separately per frame, while other methods are trained on the whole video. Our
    method with implicit surface model has better reconstruction results than other
    methods.
  Figure 7 Link: articels_figures_by_rev_year\2023\Implicit_Neural_Representations_With_Structured_Latent_Codes_for_Human_Body_Mode\figure_7.jpg
  Figure 7 caption: "Qualitative results on monocular videos. Our method renders more\
    \ appearance details and reconstruct more geometric details than People-Snapshot\
    \ [19]. \u201COurs\u201D indicates Neural Body without implicit surface model."
  Figure 8 Link: articels_figures_by_rev_year\2023\Implicit_Neural_Representations_With_Structured_Latent_Codes_for_Human_Body_Mode\figure_8.jpg
  Figure 8 caption: "Novel view synthesis on the video \u201CMagdalena\u201D. The\
    \ rendered dress tends to be blurry. The reason may be that the dress's hem deforms\
    \ non-rigidly along with the human movement, and latent codes around the hem will\
    \ correspond to different human geometry and appearance at different video frames.\
    \ Consequently, different content will be encoded into the same latent code, averaging\
    \ the information of the latent code and degrading the performance of Neural Body.\
    \ The visualization also indicates that Neural Body does not correctly reconstruct\
    \ the geometry of hem. Note that this model is without implicit surface model."
  Figure 9 Link: articels_figures_by_rev_year\2023\Implicit_Neural_Representations_With_Structured_Latent_Codes_for_Human_Body_Mode\figure_9.jpg
  Figure 9 caption: Results of human poses that are very different from training poses.
    We animates human subjects in ZJU-MoCap dataset with human poses from Human3.6
    M dataset. The rendered faces and bodies are distorted and blurry, indicating
    that Neural Body has limited generalization ability on novel pose synthesis.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.87
  Name of the first author: Sida Peng
  Name of the last author: Hujun Bao
  Number of Figures: 9
  Number of Tables: 11
  Number of authors: 8
  Paper title: Implicit Neural Representations With Structured Latent Codes for Human
    Body Modeling
  Publication Date: 2023-02-16 00:00:00
  Table 1 caption:
    table_text: TABLE I Architecture of SparseConvNet. Each Layer Consists of Sparse
      Convolution, Batch Normalization and ReLU
  Table 10 caption:
    table_text: "TABLE X Results of Models Trained With Different Numbers of Training\
      \ Frames. We Train Models on 1, 60, 300, 600, and 1200 Frames of \u201CTwirl\u201D\
      . On Novel View Synthesis of Training Frame, the First Training Frame is Selected\
      \ for Test, and the Images From 1201-st Frame to 1400th Frame are Used for Evaluating\
      \ Novel Pose Synthesis"
  Table 2 caption:
    table_text: TABLE II The Number of Training Frames and Test Frames for Each Subject
      in the ZJU-MoCap Dataset
  Table 3 caption:
    table_text: "TABLE III Results of Novel View Synthesis on the ZJU-MoCap Dataset\
      \ in Terms of PSNR and SSIM Metrics. \u201COurs\u201D Means Neural Body With\
      \ Implicit Surface Model Introduced in Section III-E, \u201CNV\u201D Means Neural\
      \ Volumes, and \u201CNT\u201D Means Neural Textures. Results of Ani-NeRF Were\
      \ Obtained From [77]. Note That We Re-Trained Neural Body, so the Results of\
      \ \u201COURS\u201D are Slightly Different From the Conference Version [17]"
  Table 4 caption:
    table_text: "TABLE IV Results of Novel Pose Synthesis on the ZJU-MoCap Dataset\
      \ in Terms of PSNR and SSIM. \u201COurs\u201D Means Neural Body With Implict\
      \ Surface Model. Results of Ani-NeRF Were Obtained From [77]"
  Table 5 caption:
    table_text: "TABLE V Results of Novel View Synthesis on Human3.6 M. \u201COurs\u201D\
      \ Means Neural Body With Implicit Surface Model"
  Table 6 caption:
    table_text: "TABLE VI Results of Novel Pose Synthesis on Human3.6 M. \u201COurs\u201D\
      \ Means Neural Body With Implicit Surface Model"
  Table 7 caption:
    table_text: "TABLE VII Results of 3D Reconstruction on the RenderPeople Dataset\
      \ in Terms of Chamfer Distance and P2S (Lower is Better). \u201COurs\u201D Means\
      \ Neural Body With Implicit Surface Model. The Testing is Performed on Some\
      \ Sampled Frames With an Interval of 10 Frames"
  Table 8 caption:
    table_text: TABLE VIII Results of Single Frame Reconstruction on RenderPeople
      Dataset in Terms of Chamfer Distance and P2S (Lower is Better). The Testing
      is Performed on the First Frame. IDR [82] is Trained on the First Frame, and
      Other Methods are Trained on the Whole Video
  Table 9 caption:
    table_text: "TABLE IX Results of Models Trained With Different Numbers of Camera\
      \ Views on the Video \u201CTwirl\u201D of the ZJU-MoCap Dataset. We Select Six\
      \ Camera Views for Ablation Studies and Use the Remaining Views for Test"
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3245815
- Affiliation of the first author: department of information engineering, electronics,
    and telecommunications, sapienza university of rome, rome, italy
  Affiliation of the last author: department of information engineering, electronics,
    and telecommunications, sapienza university of rome, rome, italy
  Figure 1 Link: articels_figures_by_rev_year\2023\Multifractal_Characterization_of_Texts_for_Pattern_Recognition_On_the_Complexity\figure_1.jpg
  Figure 1 caption: The time series representation of a small piece of the Quran in
    Arabic language. Each stem represents the position in the overall frequency ranking
    of the lexical category related to a suitable token-word.
  Figure 10 Link: articels_figures_by_rev_year\2023\Multifractal_Characterization_of_Texts_for_Pattern_Recognition_On_the_Complexity\figure_10.jpg
  Figure 10 caption: Box plot related to the analyzed variables ( Delta tildealpha
    , Atildealpha , h(2) ) highlighting the relative means and standard deviations.
  Figure 2 Link: articels_figures_by_rev_year\2023\Multifractal_Characterization_of_Texts_for_Pattern_Recognition_On_the_Complexity\figure_2.jpg
  Figure 2 caption: Schematic presentation of the main parameters of a multifractal
    spectrum.
  Figure 3 Link: articels_figures_by_rev_year\2023\Multifractal_Characterization_of_Texts_for_Pattern_Recognition_On_the_Complexity\figure_3.jpg
  Figure 3 caption: Panel (a) Fluctuation diagram Fq(s) reporting the estimation of
    the Hurst exponent for the ancient Greek text concerning the portion of the New
    Testament written by Mark. Panel (b) The profile Y(k) of the underlying time series.
  Figure 4 Link: articels_figures_by_rev_year\2023\Multifractal_Characterization_of_Texts_for_Pattern_Recognition_On_the_Complexity\figure_4.jpg
  Figure 4 caption: "The tau (q) diagram \u2013 Panel (a) \u2013 and the multifractal\
    \ spectrum f(tildealpha ) as function of the H\xF6lder exponent tildealpha \u2013\
    \ Panel (b) \u2013 for the overall text excerpts analyzed."
  Figure 5 Link: articels_figures_by_rev_year\2023\Multifractal_Characterization_of_Texts_for_Pattern_Recognition_On_the_Complexity\figure_5.jpg
  Figure 5 caption: "Multifractal analysis for the ancient Greek language, specifically\
    \ for some Gospels within the New Testament (John, Mark and Revelation). (Left)\
    \ The generalized Hurst exponent h(q) , (center) the multifractal spectrum f(tildealpha\
    \ ) as function of the H\xF6lder exponent tildealpha . The diagrams report the\
    \ results for the original time series (normal), the shuffled time series (shuffle)\
    \ and the surrogate one (surrogate). (Right) The scaling behavior (fluctuation\
    \ function) for the q th order fluctuation parameter."
  Figure 6 Link: articels_figures_by_rev_year\2023\Multifractal_Characterization_of_Texts_for_Pattern_Recognition_On_the_Complexity\figure_6.jpg
  Figure 6 caption: "Multifractal analysis for the \u201CUlysses\u201D by J. Joyce\
    \ in English language, specifically the Ulysses I, from the beginning to the 10th\
    \ chapter. (Left) The generalized Hurst exponent h(q) , (center) the multifractal\
    \ spectrum f(tildealpha ) as function of the H\xF6lder exponent tildealpha . The\
    \ diagrams report the results for the original time series (normal), the shuffled\
    \ time series (shuffle) and the surrogate one (surrogate). (Right) The scaling\
    \ behavior (fluctuation function) parametrized by the q th order fluctuation parameter."
  Figure 7 Link: articels_figures_by_rev_year\2023\Multifractal_Characterization_of_Texts_for_Pattern_Recognition_On_the_Complexity\figure_7.jpg
  Figure 7 caption: "Fluctuation function \u2013 for q=2 \u2013 for the \u201CUlysses\u201D\
    \ by J. Joyce both in English and in the translation in Italian, subdivided in\
    \ two parts. Ulysses I is from the beginning to the 10th chapter, while Ulysses\
    \ II is from chapter 11 to the end."
  Figure 8 Link: articels_figures_by_rev_year\2023\Multifractal_Characterization_of_Texts_for_Pattern_Recognition_On_the_Complexity\figure_8.jpg
  Figure 8 caption: Panel (a) Multifractal strength parameter Delta tildealpha , averaged
    on 30 random trials and varying the degradation coefficient rho in [0,1] , for
    five representative texts. Panel (b) Degradation of the multifractal strength
    parameter Delta tildealpha measured for a pure deterministic binomial multifractal
    cascade [50]. The experiment is conducted with the same general parameter setting.
  Figure 9 Link: articels_figures_by_rev_year\2023\Multifractal_Characterization_of_Texts_for_Pattern_Recognition_On_the_Complexity\figure_9.jpg
  Figure 9 caption: Panel (a) Scatter plot obtained from the t-distributed stochastic
    neighbor embedding (t-SNE) algorithm reducing the dimensionality of the space
    generated by the three analyzed variables Delta tildealpha , Atildealpha , h(2)
    . The size of points is proportional to the multifractal strength measured by
    Delta tildealpha , while the color indicates the language family. Panel (b) In
    the off-diagonal is reported the scatter plot of each variable ( Delta tildealpha
    , Atildealpha , h(2) ) against the others, while in the diagonal are reported
    the histograms, where data-points (texts) and histograms are grouped by the linguistic
    strain.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Enrico De Santis
  Name of the last author: Antonello Rizzi
  Number of Figures: 14
  Number of Tables: 2
  Number of authors: 3
  Paper title: 'Multifractal Characterization of Texts for Pattern Recognition: On
    the Complexity of Morphological Structures in Modern and Ancient Languages'
  Publication Date: 2023-02-16 00:00:00
  Table 1 caption:
    table_text: 'TABLE I Summary for Each Analyzed Language. The ML-Postags Indicates
      Whether the Postags are Generated Manually or by a Machine Learning Technique.
      The Following Four Columns Indicate a Rough Classification of Languages in Polysynthetic,
      Agglutinative, Fusive or Isolating. The Last Column Indicates the Language Type,
      That Is: Hamito-Semitic, Semitic, Indo-European>germanic, Indo-European>neolatin,
      Indo-European'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Multifractal Indexes for the Analyzed Text Excerpts Grouped
      by Language. For Each Group and Each Index It is Reported the Group Mean and
      Variance (In Brackets)
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3245886
- Affiliation of the first author: department of computer science and engineering,
    the hong kong university of science and technology, hong kong
  Affiliation of the last author: department of computer science and engineering,
    the hong kong university of science and technology, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2023\OcclusionAware_Instance_Segmentation_Via_BiLayer_Network_Architectures\figure_1.jpg
  Figure 1 caption: Simplified illustration on BCNet's key contribution. Unlike previous
    segmentation approaches operating on a single image layer (i.e., directly on the
    input image), we decouple overlapping objects into two image layers, where the
    top layer deals with the occluding objects (occluder) and the bottom layer for
    occludee (which is also referred to as target object in other methods as they
    do not explicitly consider the occluder). The overlapping parts of the two image
    layers indicate the invisible region of the occludee, which is explicitly modeled
    by our occlusion-aware BCNet framework.
  Figure 10 Link: articels_figures_by_rev_year\2023\OcclusionAware_Instance_Segmentation_Via_BiLayer_Network_Architectures\figure_10.jpg
  Figure 10 caption: Qualitative results of Mask Scoring R-CNN [5] (top row) and our
    BCNet (middle row) on COCO test-dev set, both using ResNet-101-FPN and Faster
    R-CNN [23]. The bottom row visualizes squared heatmap of contour and mask predictions
    by the two GCN layers for the occluder and occludee in the same ROI region specified
    by the red bounding box, which also makes the final segmentation result of BCNet
    more explainable than previous methods.
  Figure 2 Link: articels_figures_by_rev_year\2023\OcclusionAware_Instance_Segmentation_Via_BiLayer_Network_Architectures\figure_2.jpg
  Figure 2 caption: Instance Segmentation on COCO [7] validation set by a) Mask R-CNN
    [1], b) PANet [2], c) Mask Scoring R-CNN [5], d) ASN [8], e) Occlusion R-CNN (ORCNN)
    [9], f) Cascade Mask R-CNN [3], g) TensorMask [10], h) CenterMask [11], i) HTC
    [6] and j) Our BCNet. Note that d) and e) are specially designed for amodalocclusion
    mask prediction. In this example, the bounding box is given to compare the quality
    of different regressed instance masks.
  Figure 3 Link: articels_figures_by_rev_year\2023\OcclusionAware_Instance_Segmentation_Via_BiLayer_Network_Architectures\figure_3.jpg
  Figure 3 caption: "A brief comparison of mask head architectures: (a) Mask R-CNN\
    \ [1], (b) CenterMask [11], (c) Cascade mask R-CNN [3], (d) HTC [6], (e) Mask\
    \ scoring R-CNN [5], (f) Iterative amodal segmentation [21], (g) ASN [8], (h)\
    \ ORCNN [9], where (f), (g) and (h) are specially designed for amodalocclusion\
    \ mask prediction, (i) Ours: BCNet. The input mathbf x denotes CNN feature after\
    \ ROI extraction. Conv is convolution layer with 3times 3 kernel, FC is the fully\
    \ connected layer, SAM is the spatial attention module. B t and M t respectively\
    \ denote box and mask head at t th stage. Unlike previous occlusion-aware mask\
    \ heads, which only regress both modal and amodal masks from the occludee, our\
    \ BCNet has a bilayer GCN structure and considers the interactions between the\
    \ top \u201Coccluder\u201D and bottom \u201Coccludee\u201D in the same ROI. The\
    \ occlusion perception branch explicitly models the occluding object by performing\
    \ joint mask and contour predictions, and distills essential occlusion information\
    \ for the second graph layer to segment target object (\u201Coccludee\u201D)."
  Figure 4 Link: articels_figures_by_rev_year\2023\OcclusionAware_Instance_Segmentation_Via_BiLayer_Network_Architectures\figure_4.jpg
  Figure 4 caption: Architecture of our BCNet for GCN-based Instance Segmentation
    with bilayer occluder-occludee relational modeling, which consists of three modules;
    (1) Backbone [70] with FPN for feature extraction from input image; (2) Detection
    branch [18] for predicting instance proposals; (3) BCNet with bilayer GCN structure
    for mask prediction. For cropped ROI feature, the first GCN explicitly models
    occluding regions (occluder) by simultaneously detecting occlusion contours and
    masks, which distills essential shape and position information to guide the second
    GCN in mask prediction for the occludee. We utilize the non-local operator [71],
    [72] detailed in Section III-B to implement the GCN layer. Visualization results
    are resized to squares.
  Figure 5 Link: articels_figures_by_rev_year\2023\OcclusionAware_Instance_Segmentation_Via_BiLayer_Network_Architectures\figure_5.jpg
  Figure 5 caption: 'Left: Architecture of our transformer-based BCNet built on [20]
    with bilayer transformer decoder. Right: Architecture of Mask2Former [20] for
    instance segmentation. Instead of adopting a single transformer decoder and only
    one set of instance queries, our bilayer transformer decoder models occluder-occludee
    relations by processing occluder and occludee queries in a cascaded manner. In
    the latter stage of the first transformer decoder, the learned shape and texture
    information of the occluder is injected to the second decoder to guide the target
    instance (occludee) segmentation by residue connection. MAL denotes the Masked
    cross-Attention Layer in [20]. Pixel decoder constructs a multi-scale feature
    pyramid from the original image for feeding into the transformer decoder.'
  Figure 6 Link: articels_figures_by_rev_year\2023\OcclusionAware_Instance_Segmentation_Via_BiLayer_Network_Architectures\figure_6.jpg
  Figure 6 caption: "Occlusion synthesis for producing Synthetic Occlusion Dataset\
    \ (SOD) by sampling both occluding and occluded instances from the collected Complete\
    \ Object Bank (COB), followed by grid searching the occluded positions in the\
    \ image. COB from COCO is produced by conditionally filtering out the objects\
    \ with bounding boxes overlapping rate over 5% and mask area smaller than 32\xD7\
    32, followed by manual selection."
  Figure 7 Link: articels_figures_by_rev_year\2023\OcclusionAware_Instance_Segmentation_Via_BiLayer_Network_Architectures\figure_7.jpg
  Figure 7 caption: "Qualitative results comparison of the amodal mask predictions\
    \ on COCOA [54] by AmodalMRCNN [9], ORCNN [9] and our method using ResNet-50,\
    \ where BCNet hallucinates a more reasonable shape for the baby carriage without\
    \ producing a large portion of segmentation error. We remove the \u201Cstuff\u201D\
    \ background for more clarity."
  Figure 8 Link: articels_figures_by_rev_year\2023\OcclusionAware_Instance_Segmentation_Via_BiLayer_Network_Architectures\figure_8.jpg
  Figure 8 caption: Qualitative instance segmentation results of CenterMask [11] (top
    row) and our BCNet (middle row) on COCO [7], both using ResNet-101-FPN and FCOS
    detector [18]. The bottom row visualizes squared heatmap of contour and mask predictions
    by the two GCN layers for the occluder and occludee in the same ROI region specified
    by the red bounding box, which also makes the final segmentation result of BCNet
    more explainable than previous methods.
  Figure 9 Link: articels_figures_by_rev_year\2023\OcclusionAware_Instance_Segmentation_Via_BiLayer_Network_Architectures\figure_9.jpg
  Figure 9 caption: Qualitative amodal results comparison between Mask R-CNN + ASN
    module [8] (top row) and our BCNet (bottom row) for the mask predictions on KINS
    test set [8], both using ResNet-101-FPN and Faster R-CNN detector [23], where
    the mask shape of the invisibleoccluded regions are more reasonably estimated
    by BCNet.
  First author gender probability: 0.96
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.93
  Name of the first author: Lei Ke
  Name of the last author: Chi-Keung Tang
  Number of Figures: 12
  Number of Tables: 13
  Number of authors: 3
  Paper title: Occlusion-Aware Instance Segmentation Via BiLayer Network Architectures
  Publication Date: 2023-02-17 00:00:00
  Table 1 caption:
    table_text: TABLE I Effect of the First GCN for Occlusion Modeling by Predicting
      Contours and Masks on COCO With ResNet-50-FPN Model
  Table 10 caption:
    table_text: TABLE X Comparison With SOTA Methods on COCO Test-Dev Set. Mask AP
      is Reported and All Entries are Single-Model Results. Note That HTC [6] Adopts
      3-Stage Cascade Refinement With Multiple Object Detectors and Mask Heads. All
      of the Methods are Trained on COCO train2017
  Table 2 caption:
    table_text: TABLE II Effect of the Second GCN for Detecting Occludee Contours
      for Final Mask Prediction Guided by the Output of First GCN
  Table 3 caption:
    table_text: TABLE III Effect of Bilayer Structure Using GCN Versus FCN Implementation
  Table 4 caption:
    table_text: TABLE IV Influence of the Object Detector (FCOS Versus Faster R-CNN
      Versus Query-Based detector [20]) on BCNet
  Table 5 caption:
    table_text: TABLE V Effect of the Bilayer Transformer Decoder for the Transformer-Based
      BCNet
  Table 6 caption:
    table_text: TABLE VI Results on the COCOA Dataset
  Table 7 caption:
    table_text: TABLE VII Results on the KINS Dataset
  Table 8 caption:
    table_text: TABLE VIII Results on COCO-OCC Split
  Table 9 caption:
    table_text: TABLE IX Results on the OCHuman [87] Val Using R50-FPN
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3246174
- Affiliation of the first author: moe key lab of artificial intelligence, ai institute,
    shanghai jiao tong university, shanghai, china
  Affiliation of the last author: huawei inc., shenzhen, guangdong, china
  Figure 1 Link: articels_figures_by_rev_year\2023\A_Survey_on_LabelEfficient_Deep_Image_Segmentation_Bridging_the_Gap_Between_Weak\figure_1.jpg
  Figure 1 caption: The taxonomy of label efficient deep image segmentation methods
    according to the type categorization of weak supervision (upper half) and the
    type categorization of segmentation problems. The interactions with filled dots
    and hollow dots indicate the segmentation problems with the certain types of weak
    supervision have been explored and have not been explored, respectively. For the
    former, some typical works are provided.
  Figure 10 Link: articels_figures_by_rev_year\2023\A_Survey_on_LabelEfficient_Deep_Image_Segmentation_Bridging_the_Gap_Between_Weak\figure_10.jpg
  Figure 10 caption: The mainstream pipeline for instance segmentation with box-level
    supervision.
  Figure 2 Link: articels_figures_by_rev_year\2023\A_Survey_on_LabelEfficient_Deep_Image_Segmentation_Bridging_the_Gap_Between_Weak\figure_2.jpg
  Figure 2 caption: Examples for each type of weak supervision compared with the full
    dense supervision.
  Figure 3 Link: articels_figures_by_rev_year\2023\A_Survey_on_LabelEfficient_Deep_Image_Segmentation_Bridging_the_Gap_Between_Weak\figure_3.jpg
  Figure 3 caption: The illustration of VADeR (image from [74]). The left is the image-level
    contrastive learning and the right is VADeR (pixel-wise contrastive learning).
  Figure 4 Link: articels_figures_by_rev_year\2023\A_Survey_on_LabelEfficient_Deep_Image_Segmentation_Bridging_the_Gap_Between_Weak\figure_4.jpg
  Figure 4 caption: Siamese structure based unsupervised dense representation learning.
  Figure 5 Link: articels_figures_by_rev_year\2023\A_Survey_on_LabelEfficient_Deep_Image_Segmentation_Bridging_the_Gap_Between_Weak\figure_5.jpg
  Figure 5 caption: The mainstream pipeline for semantic segmentation with image-level
    supervision.
  Figure 6 Link: articels_figures_by_rev_year\2023\A_Survey_on_LabelEfficient_Deep_Image_Segmentation_Bridging_the_Gap_Between_Weak\figure_6.jpg
  Figure 6 caption: Pseudo mask generation from seed areas. (Image from [98].).
  Figure 7 Link: articels_figures_by_rev_year\2023\A_Survey_on_LabelEfficient_Deep_Image_Segmentation_Bridging_the_Gap_Between_Weak\figure_7.jpg
  Figure 7 caption: The mainstream pipeline for instance segmentation with image-level
    supervision.
  Figure 8 Link: articels_figures_by_rev_year\2023\A_Survey_on_LabelEfficient_Deep_Image_Segmentation_Bridging_the_Gap_Between_Weak\figure_8.jpg
  Figure 8 caption: Illustration of instance-level seed area generation by peak back-propagation.
    (Image from [34].).
  Figure 9 Link: articels_figures_by_rev_year\2023\A_Survey_on_LabelEfficient_Deep_Image_Segmentation_Bridging_the_Gap_Between_Weak\figure_9.jpg
  Figure 9 caption: The mainstream pipeline for semantic segmentation with box-level
    supervision.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Wei Shen
  Name of the last author: Qi Tian
  Number of Figures: 15
  Number of Tables: 4
  Number of authors: 9
  Paper title: 'A Survey on Label-Efficient Deep Image Segmentation: Bridging the
    Gap Between Weak Supervision and Dense Prediction'
  Publication Date: 2023-02-17 00:00:00
  Table 1 caption:
    table_text: TABLE I Representative Works of Label-Efficient Deep Image Segmentation
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II The Mathematical Definitions for Image Segmentation With
      Different Types of Supervision
  Table 3 caption:
    table_text: "TABLE III Systematic Study on the Functions of the Heuristic Priors\
      \ in Dealing With Weak Supervision of Different Types. \u201C-\u201D Means \u201C\
      has Not Been Proposed Yet\u201D"
  Table 4 caption:
    table_text: TABLE IV Datasets and Evaluation Metrics Used for Label-Efficient
      Deep Image Segmentation
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3246102
- Affiliation of the first author: department of engineering science, university of
    oxford, oxford, u.k.
  Affiliation of the last author: department of electronic engineering, chinese university
    of hong kong, hong kong, sar, china
  Figure 1 Link: articels_figures_by_rev_year\2023\Handling_OpenSet_Noise_and_Novel_Target_Recognition_in_Domain_Adaptive_Semantic_\figure_1.jpg
  Figure 1 caption: Geometric illustration of SimT with the setting of 3 pseudo label
    classes and 5 ground truth label classes. Note that coordinates (x, y, z) of any
    points in blue triangle satisfy conditions of x+y+z=1 and 0 < x,y,z< 1 .
  Figure 10 Link: articels_figures_by_rev_year\2023\Handling_OpenSet_Noise_and_Novel_Target_Recognition_in_Domain_Adaptive_Semantic_\figure_10.jpg
  Figure 10 caption: Qualitative results of novel target recognition task in SYNTHIA
    rightarrow Cityscapes scenario. (a) Target image, (b) Ground truth, Predictions
    from (c) ZS3Net (Source only) [72], (d) BudaNet [13], (e) ours (SimT) w BAPA-Net
    [1]. Three open-set classes are terrain, truck, and train.
  Figure 2 Link: articels_figures_by_rev_year\2023\Handling_OpenSet_Noise_and_Novel_Target_Recognition_in_Domain_Adaptive_Semantic_\figure_2.jpg
  Figure 2 caption: Illustration of SimT for handling mixed closed-set and open-set
    label noises in DA semantic segmentation. The proposed framework contains a segmentation
    net and a SimT. Target images are passed through the segmentation net f(cdot)mathbf
    w to perform semantic segmentation, and supervision signals are corrected by the
    learnable SimT. SimT is estimated through the proposed (a) volume regularization,
    (b) anchor guidance, (c) convex guarantee, and (d) semantic constraint.
  Figure 3 Link: articels_figures_by_rev_year\2023\Handling_OpenSet_Noise_and_Novel_Target_Recognition_in_Domain_Adaptive_Semantic_\figure_3.jpg
  Figure 3 caption: Illustration of SimT for novel target recognition in DA semantic
    segmentation. The clean class posterior is constrained via the proposed mathcal
    LC2OLC and mathcal LSR .
  Figure 4 Link: articels_figures_by_rev_year\2023\Handling_OpenSet_Noise_and_Novel_Target_Recognition_in_Domain_Adaptive_Semantic_\figure_4.jpg
  Figure 4 caption: Qualitative results of UDA in SYNTHIA rightarrow Cityscapes scenario.
    (a) Target image, (b) Ground truth, Predictions from (c) source only model, (d)
    BAPA-Net [1], (e) ours (SimT).
  Figure 5 Link: articels_figures_by_rev_year\2023\Handling_OpenSet_Noise_and_Novel_Target_Recognition_in_Domain_Adaptive_Semantic_\figure_5.jpg
  Figure 5 caption: Qualitative results of UDA in Endovis17 rightarrow Endovis18 scenario.
    (a) Target image, (b) Ground truth, Predictions from (c) source only model, (d)
    IGNet [55], (e) ours (SimT).
  Figure 6 Link: articels_figures_by_rev_year\2023\Handling_OpenSet_Noise_and_Novel_Target_Recognition_in_Domain_Adaptive_Semantic_\figure_6.jpg
  Figure 6 caption: Ablation study for volume regularization ( alpha ), anchor guidance
    ( beta ), convex guarantee ( gamma ), and semantic constraint ( delta ) of SimT.
  Figure 7 Link: articels_figures_by_rev_year\2023\Handling_OpenSet_Noise_and_Novel_Target_Recognition_in_Domain_Adaptive_Semantic_\figure_7.jpg
  Figure 7 caption: Robustness to various types of noise. 'Baseline' models are AdaptSegNet
    [19], DSP [49] and BAPA-Net [1].
  Figure 8 Link: articels_figures_by_rev_year\2023\Handling_OpenSet_Noise_and_Novel_Target_Recognition_in_Domain_Adaptive_Semantic_\figure_8.jpg
  Figure 8 caption: The SimT visualization. (a) The optimized SimT, (b) confusion
    matrix of pseudo labels.
  Figure 9 Link: articels_figures_by_rev_year\2023\Handling_OpenSet_Noise_and_Novel_Target_Recognition_in_Domain_Adaptive_Semantic_\figure_9.jpg
  Figure 9 caption: The t-SNE visualization of embedded features in handling open-set
    noise task.
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.99
  Name of the first author: Xiaoqing Guo
  Name of the last author: Yixuan Yuan
  Number of Figures: 13
  Number of Tables: 10
  Number of authors: 4
  Paper title: Handling Open-Set Noise and Novel Target Recognition in Domain Adaptive
    Semantic Segmentation
  Publication Date: 2023-02-17 00:00:00
  Table 1 caption:
    table_text: "TABLE I Results of Adapting SYNTHIA to Cityscapes. MIoU Denotes Mean\
      \ IoU of 13 Classes, Excluding the Classes With \u2217 "
  Table 10 caption:
    table_text: TABLE X Analysis of Reparameterization Method
  Table 2 caption:
    table_text: TABLE II Results of Adapting Endovis17 to Endovis18
  Table 3 caption:
    table_text: TABLE III Ablation Study. 'pseudo Label' Denotes We Employ Pseudo
      Labels Generated by the Corresponding UDA Model
  Table 4 caption:
    table_text: TABLE IV ablation Study for Reparameterization Method in Handling
      Open-Set Noises
  Table 5 caption:
    table_text: TABLE V results of Semi-Supervised Semantic Segmentation on Cityscapes
      Dataset
  Table 6 caption:
    table_text: TABLE VI Results of Adapting SYNTHIA to Cityscapes. MIoU Denotes the
      Mean IoU of 3 Open-Set Classes
  Table 7 caption:
    table_text: TABLE VII Results on Adapting Endovis17 to Endovis18
  Table 8 caption:
    table_text: TABLE VIII Ablation Study for Novel Target Recognition
  Table 9 caption:
    table_text: TABLE IX effectiveness of Semantic Constraint L SimT Semantic LSemanticSimT
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3246392
- Affiliation of the first author: school of computer science, mcgill university,
    montreal, qc, canada
  Affiliation of the last author: department of computer science, city university
    of hong kong, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2023\Variational_Nested_Dropout\figure_1.jpg
  Figure 1 caption: The probability of tail index being sampled in different nested
    dropout realizations. Rippel et al. [48] and Cui et al. [11] adopt Geometric and
    Categorical distributions, which are static over different layers and the learning
    process. The proposed variational nested dropout (VND) learns the importances
    of nodes or representations from data. The two examples are from two different
    layers in a Bayesian nested neural network.
  Figure 10 Link: articels_figures_by_rev_year\2023\Variational_Nested_Dropout\figure_10.jpg
  Figure 10 caption: Results on Cifar10 for (a) VGG11, (b) MobileNetv2, and (c) ResNeXt-Cifar.
    Each curve plots performance versus the network width.
  Figure 2 Link: articels_figures_by_rev_year\2023\Variational_Nested_Dropout\figure_2.jpg
  Figure 2 caption: The multivariate Downhill samples under different temperatures
    tau . When tau rightarrow 0 , a clear cliff is observed as the dimension increases,
    which is beneficial for differentiating important or unimportant nodes. As tau
    increases, the shape becomes a slope where the gaps between importantunimportant
    nodes are smoother, which is beneficial for training.
  Figure 3 Link: articels_figures_by_rev_year\2023\Variational_Nested_Dropout\figure_3.jpg
  Figure 3 caption: Sampling process in a layer for calculating the data log-likelihood
    (Equation 8). A fully connected layer f(cdot) takes mathbf H as an input and outputs
    mathbf F . The variational ordering unit q(mathbf z|boldsymbolbeta ) generates
    ordered mask mathbf z=[zj]j . Nodes wij 's with the same color share an element
    zj . The gradient through stochastic nodes fracpartial mathbf Fpartial boldsymbolbeta
    can be estimated efficiently, to update the importance boldsymbolbeta .
  Figure 4 Link: articels_figures_by_rev_year\2023\Variational_Nested_Dropout\figure_4.jpg
  Figure 4 caption: Approximation to (16). Our approximation allows alpha >1 (c.f.,
    [30]) and does not push alpha rightarrow 0 to generate a collapsed model (c.f.,
    [42]).
  Figure 5 Link: articels_figures_by_rev_year\2023\Variational_Nested_Dropout\figure_5.jpg
  Figure 5 caption: "The correlation between two output sample sets of mathrmBN3,\
    \ . VGG11 and ResNeXt are used as the backbones. The models are trained on the\
    \ \u201Cin domain\u201D data. The correlation are evaluated on both \u201Cin domain\u201D\
    \ and \u201Cout of domain\u201D data. Red color indicates a high correlation."
  Figure 6 Link: articels_figures_by_rev_year\2023\Variational_Nested_Dropout\figure_6.jpg
  Figure 6 caption: The two step slimming to obtain a high performance deterministic
    network, mathrmSN3 . The triangle represents a 2-simplex with 3 classes. The left
    red points are the output of mathrmBN3, given input mathbf xast , mathrmBN3(mathbf
    xast ) . The right contours represent the Dirichlet distribution parameterized
    by mathrmSN3(mathbf xast) .
  Figure 7 Link: articels_figures_by_rev_year\2023\Variational_Nested_Dropout\figure_7.jpg
  Figure 7 caption: The graphical representation of the Gaussian-Bernoulli chain prior.
  Figure 8 Link: articels_figures_by_rev_year\2023\Variational_Nested_Dropout\figure_8.jpg
  Figure 8 caption: Digit images generated from 16 samples of mathbf z (the 4x4 grid
    of digits) using different latent variable (code) lengths (increasing from left
    to right).
  Figure 9 Link: articels_figures_by_rev_year\2023\Variational_Nested_Dropout\figure_9.jpg
  Figure 9 caption: The generated images given specific samples of mathbf h . Different
    colored blocks for hi represent different values sampled from a univariate Gaussian.
    The group of colored blocks at the top left corner of a image represents the sample
    mathbf h . In this example, the maximum length of mathbf h is 4.
  First author gender probability: 0.97
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yufei Cui
  Name of the last author: Chun Jason Xue
  Number of Figures: 13
  Number of Tables: 8
  Number of authors: 8
  Paper title: Variational Nested Dropout
  Publication Date: 2023-02-20 00:00:00
  Table 1 caption:
    table_text: TABLE I Performance of BN 3 BN 3and SNR-Based Pruning Methods
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Performance With Limited Parameters, SVB, Monte-Carlo Integration
      and Full-Width BN 3 BN 3
  Table 3 caption:
    table_text: TABLE III Training and Testing Time for BN 3 BN 3DT- BN 3 BN 3, IBNN
      and FN 3 FN 3
  Table 4 caption:
    table_text: TABLE IV Performance of SN 3 SN 3 With Different Sizes and Relative
      Sizes
  Table 5 caption:
    table_text: TABLE V Frechet Inception Distance (FID) Scores for Image Generation
  Table 6 caption:
    table_text: TABLE VI Inception Scores (IS) for Image Generation. Higher IS is
      Better
  Table 7 caption:
    table_text: TABLE VII Reconstruction Error for Image Generation. Lower Error is
      Better
  Table 8 caption:
    table_text: "TABLE VIII Numerical Results on Semantic Segmentation Using Generalized\
      \ Energy Distance ( \u21D3 \u21D3) with Different Numbers of Posterior Samples"
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3241945
- Affiliation of the first author: cooperative medianet innovation center, shanghai
    jiao tong university, shanghai, china
  Affiliation of the last author: astar centre for frontier ai research, singapore
  Figure 1 Link: articels_figures_by_rev_year\2023\Latent_ClassConditional_Noise_Model\figure_1.jpg
  Figure 1 caption: Dynamic label regression for LCCN. The images and noisy labels
    are respectively inputted to the classifier and the safeguarded Bayesian noise
    modeling to compute the prediction and the conditional transition. Then, the latent
    labels are sampled based on their product, and then used for the training of the
    classifier and the safeguarded Bayesian noise modeling. All components are trained
    end-to-end in a stochastic fashion.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Latent_ClassConditional_Noise_Model\figure_2.jpg
  Figure 2 caption: Latent Class-Conditional Noise model. ( x , tildey ) is the sample
    pair. y is the latent true label. phi is the noise transition. alpha is the Dirichlet
    hyperparameter. N is the sample number and K is the category number.
  Figure 3 Link: articels_figures_by_rev_year\2023\Latent_ClassConditional_Noise_Model\figure_3.jpg
  Figure 3 caption: Some variants of the Latent Class-Conditional Noise model. (a)
    LCCN is for the open-set noisy label setting, where compared to LCCN, phi in mathbb
    R(K+1)times K introduces the extra dimension that collapses the abnormal categories
    beyond the pre-defined classes. (b) LCCN+ considers the case that the true label
    y , i.e., groundtruth, is partially observed. We use y to train the classifier
    directly when it is available, otherwise infer it before the training. (c) DivideLCCN
    extends LCCN to the dual-model structure, which follows the empirical design [18],
    [19] for the cross-model training. (a) LCCN (b) LCCN+ (c) DivideLCCN.
  Figure 4 Link: articels_figures_by_rev_year\2023\Latent_ClassConditional_Noise_Model\figure_4.jpg
  Figure 4 caption: Colormap of the confusion matrices on CIFAR-10 with r=0.5. We
    utilize the log-scale for each element in the confusion matrix for better visualization.
    The left three maps are respectively learned by LCCN at the beginning, 30,000
    step and the end, and the right one is the groundtruth.
  Figure 5 Link: articels_figures_by_rev_year\2023\Latent_ClassConditional_Noise_Model\figure_5.jpg
  Figure 5 caption: Test accuracy of LCCN and S-adaptation in the training on CIFAR-10
    with r =0.5 (left), and the corresponding histograms for the change of noise transition
    phi via a mini-batch of samples (right).
  Figure 6 Link: articels_figures_by_rev_year\2023\Latent_ClassConditional_Noise_Model\figure_6.jpg
  Figure 6 caption: Correction ratio during training of LCCN on CIFAR-10 under r =0.5
    with some negatively corrected samples (the red box) and some positively corrected
    samples (the green box) in the high probabilities.
  Figure 7 Link: articels_figures_by_rev_year\2023\Latent_ClassConditional_Noise_Model\figure_7.jpg
  Figure 7 caption: Ablation study of the Dirichlet parameter alpha w.r.t. test accuracy.
  Figure 8 Link: articels_figures_by_rev_year\2023\Latent_ClassConditional_Noise_Model\figure_8.jpg
  Figure 8 caption: Some exemplars that are considered as the open-set label noise
    by LCCN in the training set. We intuitively summarize these photos into four categories
    based on their contents, multiple different objects (RED box), implicit categories
    (GREEN box), uncertain types (BLACK box) and confusing appearance (BLUE box),
    which are respectively marked by the color of the surrounded boxes.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.96
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Jiangchao Yao
  Name of the last author: Ivor W. Tsang
  Number of Figures: 8
  Number of Tables: 7
  Number of authors: 5
  Paper title: Latent Class-Conditional Noise Model
  Publication Date: 2023-02-22 00:00:00
  Table 1 caption:
    table_text: TABLE I Summary of Recently Popular Transition-Based Methods
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II The Average Accuracy ( % %) Over 5 Trials on CIFAR-10 and
      CIFAR-100 With Different Levels of the Symmetric Noise
  Table 3 caption:
    table_text: TABLE III The Average Accuracy ( % %) Over 5 Trials on CIFAR-10 and
      CIFAR-100 With Different Levels of the Asymmetric Noise
  Table 4 caption:
    table_text: TABLE IV The Average Accuracy ( % %) Over 5 Trials on CIFAR-10 and
      CIFAR-100 With Different Levels of the Open-Set Noise
  Table 5 caption:
    table_text: TABLE V The Average Accuracy Over 5 Trials on Clothing1M
  Table 6 caption:
    table_text: "TABLE VI The Learned Noise Transition on Clothing1M by LCCN \u2217\
      \ "
  Table 7 caption:
    table_text: TABLE VII The Accuracy on (mini)WebVision Dataset
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3247629
- Affiliation of the first author: school of mathematical sciences, university of
    science and technology of china, hefei, china
  Affiliation of the last author: school of mathematical sciences, university of science
    and technology of china, hefei, china
  Figure 1 Link: articels_figures_by_rev_year\2023\Fast_and_Robust_NonRigid_Registration_Using_Accelerated_MajorizationMinimization\figure_1.jpg
  Figure 1 caption: "Comparison of deformation graph construction by [16] and our\
    \ method on \u201Ccrane\u201D (left) and \u201Cmarch2\u201D (right) datasets of\
    \ [57]."
  Figure 10 Link: articels_figures_by_rev_year\2023\Fast_and_Robust_NonRigid_Registration_Using_Accelerated_MajorizationMinimization\figure_10.jpg
  Figure 10 caption: "Visualization of computational time and RMSE resulting from\
    \ different methods on 50 pairs of models constructed from the \u201Cjumping\u201D\
    \ dataset [57] with different profiles of added noise."
  Figure 2 Link: articels_figures_by_rev_year\2023\Fast_and_Robust_NonRigid_Registration_Using_Accelerated_MajorizationMinimization\figure_2.jpg
  Figure 2 caption: 'Left: function psi nu with different values of the parameter
    nu . Right: different surrogate functions for the function psi nu with nu =1 .'
  Figure 3 Link: articels_figures_by_rev_year\2023\Fast_and_Robust_NonRigid_Registration_Using_Accelerated_MajorizationMinimization\figure_3.jpg
  Figure 3 caption: "Comparison on partially overlapping data constructed from the\
    \ \u201Ccrane\u201D dataset [57]. We set kalpha =100, kbeta =10 for all variants."
  Figure 4 Link: articels_figures_by_rev_year\2023\Fast_and_Robust_NonRigid_Registration_Using_Accelerated_MajorizationMinimization\figure_4.jpg
  Figure 4 caption: "Comparison on two pairs of meshes from the \u201Csamba\u201D\
    \ dataset [57]. We set kalpha =10, kbeta =1 for all variants."
  Figure 5 Link: articels_figures_by_rev_year\2023\Fast_and_Robust_NonRigid_Registration_Using_Accelerated_MajorizationMinimization\figure_5.jpg
  Figure 5 caption: Deformation graphs and registration results using our method with
    different values of the radius parameter R , on a pair of models from the MPI
    Faust dataset [61]. We set kalpha =0.1,kbeta =0.001 . Here textn denotes the number
    of the deformation graph nodes, ti and to denote the computational time for the
    deformation graph construction and the numerical optimization respectively, and
    i denotes the number of iterations required for the numerical optimization solver
    to converge.
  Figure 6 Link: articels_figures_by_rev_year\2023\Fast_and_Robust_NonRigid_Registration_Using_Accelerated_MajorizationMinimization\figure_6.jpg
  Figure 6 caption: "Comparisons between the solver from [16] and our solver for the\
    \ same optimization problem on the \u201Cjumping\u201D dataset of [57]. We set\
    \ kalpha =10,kbeta =100 for both solvers."
  Figure 7 Link: articels_figures_by_rev_year\2023\Fast_and_Robust_NonRigid_Registration_Using_Accelerated_MajorizationMinimization\figure_7.jpg
  Figure 7 caption: "Visualization of the computational time and RMSE using different\
    \ methods on 50 pairs of models from the \u201Chandstand\u201D and \u201Cmarch1\u201D\
    \ datasets [57]."
  Figure 8 Link: articels_figures_by_rev_year\2023\Fast_and_Robust_NonRigid_Registration_Using_Accelerated_MajorizationMinimization\figure_8.jpg
  Figure 8 caption: "Comparison on mesh pairs from the \u201Chandstand\u201D dataset\
    \ (top) and the \u201Cmarch1\u201D dataset (bottom) [57]. We set alpha =10 for\
    \ N-ICP, alpha =10, beta = 1 for RPTS, alpha =1, beta =1 for SVR- ell 0 , and\
    \ kalpha =100, kbeta = 1 for our method."
  Figure 9 Link: articels_figures_by_rev_year\2023\Fast_and_Robust_NonRigid_Registration_Using_Accelerated_MajorizationMinimization\figure_9.jpg
  Figure 9 caption: Comparisons on the MPI Faust dataset [61]. The plot in the upper
    right visualizes the computational time and RMSE using different methods on 80
    pairs of models from the dataset, while the rendered images show the results on
    a particular pair.
  First author gender probability: 0.96
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.63
  Name of the first author: Yuxin Yao
  Name of the last author: Juyong Zhang
  Number of Figures: 17
  Number of Tables: 4
  Number of authors: 4
  Paper title: Fast and Robust Non-Rigid Registration Using Accelerated Majorization-Minimization
  Publication Date: 2023-02-22 00:00:00
  Table 1 caption:
    table_text: "TABLE I MeanMedian RMSE ( \xD7 10 \u22122 \xD710-2) and Average Computational\
      \ Time (s) Using Different Methods on 50 Pairs of Models From the \u201CHandstand\u201D\
      \ and \u201CMarch1\u201D Datasets [57]. We Set \u03B1=10 \u03B1=10 for N-ICP,\
      \ \u03B1=100,\u03B2=1 \u03B1=100,\u03B2=1 for RPTS, \u03B1=1,\u03B2=1 \u03B1\
      =1,\u03B2=1 for SVR- \u2113 0 \u21130, and k \u03B1 =100, k \u03B2 =1 k\u03B1\
      =100,k\u03B2=1 for Our Method"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE II Mean RMSE ( \xD7 10 \u22123 \xD710-3) and Average Computational\
      \ Time (s) Using Different Methods on Models From the MPI Faust Dataset [61].\
      \ We Set \u03B1=0.1 \u03B1=0.1 for N-ICP, \u03B1=0.1,\u03B2=0.001 \u03B1=0.1,\u03B2\
      =0.001 for RPTS, \u03B1=0.001,\u03B2=1 \u03B1=0.001,\u03B2=1 for SVR- \u2113\
      \ 0 \u21130, and k \u03B1 =0.1, k \u03B2 =0.001 k\u03B1=0.1,k\u03B2=0.001 for\
      \ Our Method"
  Table 3 caption:
    table_text: "TABLE III MeanMedian RMSE ( \xD7 10 \u22122 \xD710-2) and Average\
      \ Computational Time (s) Using Different Methods on 50 Pairs of Models Constructed\
      \ From the \u201CJumping\u201D Dataset [57] With Different Noise Profiles. We\
      \ Set \u03B1=10 \u03B1=10 for N-ICP, \u03B1=100,\u03B2=1 \u03B1=100,\u03B2=1\
      \ for RPTS, \u03B1=1,\u03B2=1 \u03B1=1,\u03B2=1 for SVR- \u2113 0 \u21130, and\
      \ k \u03B1 =100, k \u03B2 =1 k\u03B1=100,k\u03B2=1 for Our Method"
  Table 4 caption:
    table_text: "TABLE IV MeanMedian r s rs ( \xD7 10 \u22122 \xD710-2) and Average\
      \ Computational Time (s) Using Different Methods on 349 Pairs of Models From\
      \ DeepDeform Dataset [41]. For Lepard+NICP, There are Only 219 Valid Results.\
      \ We Set \u03B1=10 \u03B1=10 for N-ICP, \u03B1=100,\u03B2=1 \u03B1=100,\u03B2\
      =1 for RPTS, \u03B1=1,\u03B2=1,r=8 l \xAF \u03B1=1,\u03B2=1,r=8l\xAF for SVR-\
      \ \u2113 0 \u21130, and k \u03B1 =10, k \u03B2 =1,r=8 l \xAF k\u03B1=10,k\u03B2\
      =1,r=8l\xAF for Our Method"
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3247603
- Affiliation of the first author: key laboratory of information engineering in surveying,
    mapping and remote sensing, wuhan university, wuhan, china
  Affiliation of the last author: computer science and engineering department, university
    at buffalo, buffalo, ny, usa
  Figure 1 Link: articels_figures_by_rev_year\2023\Consistent_D_Hand_Reconstruction_in_Video_via_SelfSupervised_Learning\figure_1.jpg
  Figure 1 caption: 'Given a collection of unlabeled hand images or videos, we learn
    a 3D hand reconstruction network in a self-supervised manner. Top: the training
    uses unlabeled hand images from image collections or video sequences and their
    corresponding noisy detected 2D keypoints. Bottom: our model outputs accurate
    hand joints and shapes, as well as vivid hand textures.'
  Figure 10 Link: articels_figures_by_rev_year\2023\Consistent_D_Hand_Reconstruction_in_Video_via_SelfSupervised_Learning\figure_10.jpg
  Figure 10 caption: Boxplots of shape parameters in sequence predictions on the HO-3D
    testing set. SM1, AP13, and AP10 are sequences from HO-3D testing set. mathrm
    T & S loss reduces S.D. across all 10 dimensions of the shape parameters.
  Figure 2 Link: articels_figures_by_rev_year\2023\Consistent_D_Hand_Reconstruction_in_Video_via_SelfSupervised_Learning\figure_2.jpg
  Figure 2 caption: Overview of the proposed models. The mathrmS2HAND(V) on the right
    learns to reconstruct consistent 3D hands from video sequences without ground
    truth annotations based on mathrmS2HAND . Given an input image, the mathrmS2HAND
    model generates a 3D textured hand with its corresponding multiple 2D representations
    through a 3D reconstruction network and a 2D keypoints estimator. Effective loss
    functions and regularization terms are designed for self-supervised network training.
    Given a video sequence, the mathrmS2HAND(V) model produces sequential outputs
    from several weight-shared mathrmS2HAND models with temporal constraints. A quaternion
    loss and a T & S loss are presented to exploit continuous motion information to
    promote consistent hand reconstruction. During the inference, only the 3D reconstruction
    network is utilized and the mathrmS2HAND(V) acts just like a specially trained
    mathrmS2HAND due to weight sharing. The symbols used in this figure can be found
    in Section III-B and III-C.
  Figure 3 Link: articels_figures_by_rev_year\2023\Consistent_D_Hand_Reconstruction_in_Video_via_SelfSupervised_Learning\figure_3.jpg
  Figure 3 caption: (a)The joint skeleton structure. (b) A sample of bone rotation
    angles. The five bones ( overrightarrow0underline1,overrightarrow0underline5,overrightarrow0underline9,overrightarrow0underline13,overrightarrow0underline17
    ) on the palm are fixed. Each finger has 3 bones, and the relative orientation
    of each bone from its root bone is represented by azimuth, pitch, and roll.
  Figure 4 Link: articels_figures_by_rev_year\2023\Consistent_D_Hand_Reconstruction_in_Video_via_SelfSupervised_Learning\figure_4.jpg
  Figure 4 caption: Comparison between our quaternion loss and Slerp. The circle represents
    a 2D projective plane of 4D unit quaternion sphere. The red arch denotes the set
    of quaternion that satisfies (13), which ensures smooth orientation transition.
    The equation in each circle represents the corresponding prior. The remaining
    symbols can be found in Section III-C. As can be seen, both Slerp and quaternion
    loss has the prior to make sure (13) is satisfied. However Slerp has an additional
    prior phi (t) = t , while our Quaternion loss covers all possible phi (t) , which
    allows smooth orientation transition at all possible speed. .
  Figure 5 Link: articels_figures_by_rev_year\2023\Consistent_D_Hand_Reconstruction_in_Video_via_SelfSupervised_Learning\figure_5.jpg
  Figure 5 caption: Qualitative comparison to OpenPose [26] and MANO-CNN on the FreiHAND
    testing set. For OpenPose, we visualize the detected 2D keypoints. For our method
    and MANO-CNN, we visualize both the projected 2D keypoints and 3D mesh.
  Figure 6 Link: articels_figures_by_rev_year\2023\Consistent_D_Hand_Reconstruction_in_Video_via_SelfSupervised_Learning\figure_6.jpg
  Figure 6 caption: Comparisons between our proposed self-supervised (S.S.) methods
    and other SOTA fully-supervised (F.S.) methods on the STB dataset (hand-only scenario).
  Figure 7 Link: articels_figures_by_rev_year\2023\Consistent_D_Hand_Reconstruction_in_Video_via_SelfSupervised_Learning\figure_7.jpg
  Figure 7 caption: A comparison of 2D keypoint sets used or outputted at the training
    stage on FreiHAND. The fraction of frames within the maximum joint distance is
    plotted. Refer to Section IV-E1 for details.
  Figure 8 Link: articels_figures_by_rev_year\2023\Consistent_D_Hand_Reconstruction_in_Video_via_SelfSupervised_Learning\figure_8.jpg
  Figure 8 caption: Qualitative comparison of different motion-related constraints
    on the HO-3D testing set. Our mathrmS2HAND(V) with the quaternion loss achieves
    the best qualitative results.
  Figure 9 Link: articels_figures_by_rev_year\2023\Consistent_D_Hand_Reconstruction_in_Video_via_SelfSupervised_Learning\figure_9.jpg
  Figure 9 caption: Qualitative demonstration of the effectiveness of the mathrm T
    & S consistency loss. For each frame, we show output keypoints (left), output
    3D reconstruction (middle), and both sides of the output textures (with lighting)
    in flat hands (top-bottom-right). For each sequence, we show results without mathrm
    T & S loss on the top row and with mathrm T & S loss on the bottom row. mathrm
    T & S loss significantly improves the output appearance consistency in sequence
    predictions. .
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.61
  Name of the first author: Zhigang Tu
  Name of the last author: Junsong Yuan
  Number of Figures: 11
  Number of Tables: 9
  Number of authors: 7
  Paper title: Consistent 3D Hand Reconstruction in Video via Self-Supervised Learning
  Publication Date: 2023-02-22 00:00:00
  Table 1 caption:
    table_text: TABLE I Comparison of Main Results on the FreiHAND Testing Set. The
      Performance of Our Self-Supervised Method S 2 HAND S2 HAND is Comparable to
      the Recent Fully-Supervised and Weakly-Supervised methods. [19] Also Uses the
      Synthetic Training Data With 3D Supervision
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Comparison of Main Results on the HO-3D Testing Set. Without
      Using Any Object Information and Hand Annotation, Our S 2 HAND S2 HAND Model
      Performs Comparable With the Recent Fully-Supervised methods [23]. Further With
      the Temporal Constraints, Our S 2 HAND(V) S2 HAND (V) Even surpasses [23]
  Table 3 caption:
    table_text: TABLE III Ablation Studies on Different Losses Used in Our Method
      on the FreiHAND Testing Set. Refer to Section IV-E1 for Details
  Table 4 caption:
    table_text: TABLE IV Comparison of the Accuracy of Different Motion-Related Constraints
      on the HO-3D Testing Set. Quaternion Loss Shows Its Effectiveness Over the Similar
      Loss Functions on Modeling Smoothness
  Table 5 caption:
    table_text: TABLE V Comparison of the Smoothoness Performance of Different Motion-Related
      Constraints on the HO-3D Dataset. Quaternion Loss Gives the Smoothest Predictions
      and is Highly in Line With ACC and ACC-ERR
  Table 6 caption:
    table_text: TABLE VI Comparison of Self-Supervised Results and Weakly-Supervised
      Results. Refer to Section IV-E1 for Details
  Table 7 caption:
    table_text: TABLE VII Comparison of Different Configurations of the Quaternion
      Loss on the HO-3D Testing Set
  Table 8 caption:
    table_text: TABLE VIII Results of Absorbing Extra In-the-Wild Data From YT 3D
      on the HO-3D Testing Set. Represents Changing Camera Model From Perspective
      to the Orthogonal Model, Which Enables to Learn With In-the-Wild Data Without
      Camera Information
  Table 9 caption:
    table_text: TABLE IX Results of Hand Appearance Consistency of Our Methods on
      the HO-3D Testing Set. Texture S.d. Is Computed Using Lighted Textures Defined
      in (22)
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3247907
- Affiliation of the first author: college of computer science, sichuan university,
    chengdu, china
  Affiliation of the last author: college of computer science, sichuan university,
    chengdu, china
  Figure 1 Link: articels_figures_by_rev_year\2023\CrossModal_Retrieval_With_Partially_Mismatched_Pairs\figure_1.jpg
  Figure 1 caption: Toy example to illustrate our idea. Different from Positive Learning
    (PL) paradigm, our Complementary Contrastive Learning (CCL) solution utilizes
    negative (see blue balloon) instead of positive (see red balloon) information,
    thus embracing the robustness against PMPs.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\CrossModal_Retrieval_With_Partially_Mismatched_Pairs\figure_2.jpg
  Figure 2 caption: "Toy example to show the challenge of negative learning (NL, a.k.a.\
    \ complementary learning) for cross-modal retrieval. (a) shows that traditional\
    \ complementary learning cannot obtain the correct optimization direction, which\
    \ makes the anchor \u201CO\u201D apart from \u201CA\u201D but close to \u201C\
    C\u201D and \u201CD\u201D, because the complementary label is less informative\
    \ than the ordinary one. In addition, the anchor will suffer from the instability\
    \ issue as it will only affect by a single negative point at any instant, acting\
    \ like Brownian motion. More specifically, when the particle is very fine in flowing\
    \ fluid, there are only a few molecules around to interact with it, thus the random\
    \ interaction will produce an imbalance force to perturb the particle movement.\
    \ (b) illustrates that the resultant of all negative information could provide\
    \ a strong and correct optimization direction, thus helping our method to converge.\
    \ More intuitively, for larger particles, there are much more molecules all around\
    \ to interact with them, and thus the interaction forces from all directions will\
    \ cancel out the inter randomness and produce the correct resultant force along\
    \ the flowing direction."
  Figure 3 Link: articels_figures_by_rev_year\2023\CrossModal_Retrieval_With_Partially_Mismatched_Pairs\figure_3.jpg
  Figure 3 caption: Framework of the proposed method. First, the visual and textual
    samples are fed into the corresponding modality-specific networks fV and fT to
    extract the features fV(mathbf V) and fT(mathbf T) , respectively. Second, a nonparametric
    or parametric function g is conducted on the features to measure the cross-modal
    similarity between mathbf V and mathbf T . Then, our Cross-Modal Contrastive Learning
    module (CMCL) is adopted to compute the cross-modal matching probability. As the
    mismatched pairs will lead to inaccurate probability prediction, we propose a
    novel Complementary Contrastive Learning (CCL) loss to solve this problem by only
    using the negative information ( Y=0 ) to optimize our model. For positive information
    ( Y=1 ), our CCL will do nothing operation (NOP). Thanks to our complementary
    contrastive learning paradigm, the proposed method could be robust against PMPs
    because the negative information is less possible to be false than the positive
    one.
  Figure 4 Link: articels_figures_by_rev_year\2023\CrossModal_Retrieval_With_Partially_Mismatched_Pairs\figure_4.jpg
  Figure 4 caption: Comparison of robustness against PMPs with the mismatching rate
    of 0.6. This figure shows the pairwise similarity distributions of TP-FP (true
    positive pairs versus false positive pairs on the training set of MS-COCO), TN-FN
    (true negative pairs versus false negative pairs on the training set of MS-COCO),
    and PP-NP (positive pairs versus negative pairs on the validation set of MS-COCO)
    calculated by TR-HN, TR, CL, NL, and CCL, respectively.
  Figure 5 Link: articels_figures_by_rev_year\2023\CrossModal_Retrieval_With_Partially_Mismatched_Pairs\figure_5.jpg
  Figure 5 caption: Performance of different loss functions in SGR in terms of R1
    scores. The evaluation is conducted on the validation set of MS-COCO with MRate=0.6.
  Figure 6 Link: articels_figures_by_rev_year\2023\CrossModal_Retrieval_With_Partially_Mismatched_Pairs\figure_6.jpg
  Figure 6 caption: "The ability of our RCL to capture latent semantics for cross-modal\
    \ retrieval with MRate=0.6. The figure shows some retrieved examples of the image-to-text\
    \ (as shown in (a)\u2013(c)) and text-to-image (as shown in (d)\u2013(f)) for\
    \ RCL-SGR on the validation set of MS-COCO dataset. We show the top-3 retrieved\
    \ texts and images for each given image and text query, respectively. The correctly\
    \ matched ones are marked in green, and incorrectly matched in red. Specifically,\
    \ the correctly matched sentences are with green check marks, and the incorrectly\
    \ matched ones are with red words and X marks. The ground-truth matched images\
    \ are outlined in green boxes and unmatched in red boxes."
  Figure 7 Link: articels_figures_by_rev_year\2023\CrossModal_Retrieval_With_Partially_Mismatched_Pairs\figure_7.jpg
  Figure 7 caption: Parameter analysis of RCL-SAF in terms of average scores ( R1
    , R5 , and R10 ) for image-text matching with MRate=0.6 on the validation set
    of Flickr30K.
  Figure 8 Link: articels_figures_by_rev_year\2023\CrossModal_Retrieval_With_Partially_Mismatched_Pairs\figure_8.jpg
  Figure 8 caption: "The robustness of our RCL against PMPs with MRate=0.6. This figure\
    \ shows some mismatched and retrieved examples for our RCL-SGR on the training\
    \ set of the MS-COCO dataset. (a)\u2013(c) illustrate the mismatched (middle)\
    \ and top-5 retrieved (right) textual examples for each given image (left). The\
    \ correctly matched samples are marked by green check marks, and incorrectly matched\
    \ ones are marked by red X marks."
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Peng Hu
  Name of the last author: Xi Peng
  Number of Figures: 8
  Number of Tables: 8
  Number of authors: 5
  Paper title: Cross-Modal Retrieval With Partially Mismatched Pairs
  Publication Date: 2023-02-22 00:00:00
  Table 1 caption:
    table_text: TABLE I General Statistics of All Datasets in the Experiments. N tr
      Ntr, N va Nva, and N te Nte are the Number of Training, Validation, and Testing
      Sets in the Corresponding Dataset, Respectively
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Image-Text Matching With Different Mismatching Rates (MRate)
      on MS-COCO 1K and Flickr30K
  Table 3 caption:
    table_text: TABLE III Video-Text Retrieval With Different Mismatching Rates (MRate)
      on MSVD and MSR-VTT
  Table 4 caption:
    table_text: TABLE IV Image-Text Matching on CC152K
  Table 5 caption:
    table_text: TABLE V Comparison With NCR [43] Under Different Mismatching Rates
      (MRate) on MS-COCO and Flickr30K
  Table 6 caption:
    table_text: TABLE VI Comparison of SGR [5] With Different Presented Loss Functions
      Under the Mismatching Rates (MRate) of 0.6 on MS-COCO
  Table 7 caption:
    table_text: TABLE VII Image-Text Matching With the Mismatching Rate of 0.6 on
      MS-COCO 1K and Flickr30K
  Table 8 caption:
    table_text: TABLE VIII Comparison With Filtering-Based Baselines Under Different
      Mismatching Rates (MRate) on MS-COCO 1K and Flickr30K
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3247939
- Affiliation of the first author: national university of singapore, singapore
  Affiliation of the last author: bytedance, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2023\Learning_to_Augment_Poses_for_D_Human_Pose_Estimation_in_Images_and_Videos\figure_1.jpg
  Figure 1 caption: Pose estimation error in PA-MPJPE on the in-the-wild 3DPW [16]
    and MuPoTS-3D [11] datasets under cross-dataset evaluation setup of several well
    established models [5], [6], [7], [17], [18] trained with and without PoseAug
    and PoseAug-V. We see our auto-augmentation strategy significantly improves their
    generalization performances.
  Figure 10 Link: articels_figures_by_rev_year\2023\Learning_to_Augment_Poses_for_D_Human_Pose_Estimation_in_Images_and_Videos\figure_10.jpg
  Figure 10 caption: Pose estimation errors of Baseline and PoseAug-V of each frame
    on the challenging in-the-wild 3DPW dataset, where the key-frames correspond to
    hard cases that can increase the error of pose estimation, e.g., scale change
    (e.g., far from camera), unseen view points (e.g., down-top view), and novel actions
    (e.g., fencing). Thanks to the video data augmentation, PoseAug-V achieves better
    performance.
  Figure 2 Link: articels_figures_by_rev_year\2023\Learning_to_Augment_Poses_for_D_Human_Pose_Estimation_in_Images_and_Videos\figure_2.jpg
  Figure 2 caption: Overview of our PoseAug framework. The augmentor, estimator and
    discriminator are jointly trained end-to-end with an error-feedback training strategy.
    As such, the augmentor learns to augment data with guidance from the estimator
    and discriminator.
  Figure 3 Link: articels_figures_by_rev_year\2023\Learning_to_Augment_Poses_for_D_Human_Pose_Estimation_in_Images_and_Videos\figure_3.jpg
  Figure 3 caption: Augmentation operations with PoseAug. A source 3D pose is augmented
    by modifying its posture (via BA operation), body size (via BL operation) and
    view point and position (via RT operation).
  Figure 4 Link: articels_figures_by_rev_year\2023\Learning_to_Augment_Poses_for_D_Human_Pose_Estimation_in_Images_and_Videos\figure_4.jpg
  Figure 4 caption: Illustrations of the difference between original and part-aware
    KCS based discriminator. Given a novel and valid augmented pose, the original
    KCS based discriminator would wrongly classify it as fake as it does not appear
    in source data (H36 M), while the part-aware KCS based discriminator would recognize
    is as real and approve it, since it inspects local joint relations. It can be
    seen the part-aware KCS based discriminator can help the augmentor generate more
    diverse and plausible pose augmentation.
  Figure 5 Link: articels_figures_by_rev_year\2023\Learning_to_Augment_Poses_for_D_Human_Pose_Estimation_in_Images_and_Videos\figure_5.jpg
  Figure 5 caption: Augmentation operations with PoseAug-V. Given a source pose sequence,
    it first augments the end pose into a novel one; then generates the proper intermediate
    pose sequence conditioned on the start and the augmented end poses, forming new
    pose sequence with more diverse actions and trajectories. The augmented 3D pose
    sequence is projected to 2D plane with the generated virtual camera, which helps
    improve viewpoints diversity. For more details, please refer to the main text.
  Figure 6 Link: articels_figures_by_rev_year\2023\Learning_to_Augment_Poses_for_D_Human_Pose_Estimation_in_Images_and_Videos\figure_6.jpg
  Figure 6 caption: "Example 3D pose estimations from LSP, MPII, 3DHP and 3DPW. PoseAug's\
    \ results are shown in the left four columns. The rightmost column shows results\
    \ of Baseline\u2014VPose [17] trained wo PoseAug. Errors are highlighted by black\
    \ arrows."
  Figure 7 Link: articels_figures_by_rev_year\2023\Learning_to_Augment_Poses_for_D_Human_Pose_Estimation_in_Images_and_Videos\figure_7.jpg
  Figure 7 caption: Qualitative results of 3D pose estimations of VPose (T=27) trained
    wo and w PoseAug-V on 3DPW under cross-evaluation setup. The red-black poses are
    predictions and the green poses are ground-truths. Errors are highlighted by black
    arrows.
  Figure 8 Link: articels_figures_by_rev_year\2023\Learning_to_Augment_Poses_for_D_Human_Pose_Estimation_in_Images_and_Videos\figure_8.jpg
  Figure 8 caption: Ablation study on limited data setup. We report MPJPE for evaluation.
  Figure 9 Link: articels_figures_by_rev_year\2023\Learning_to_Augment_Poses_for_D_Human_Pose_Estimation_in_Images_and_Videos\figure_9.jpg
  Figure 9 caption: Distribution visualization on view point (top row) and position
    (bottom row) for original data H36 M, and augmented data from Li et al. [14],
    PoseAug (3 rd column) and PoseAug with extra 2D poses. We see PoseAug significantly
    improves diversity of view point and position.
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Jianfeng Zhang
  Name of the last author: Jiashi Feng
  Number of Figures: 10
  Number of Tables: 12
  Number of authors: 4
  Paper title: Learning to Augment Poses for 3D Human Pose Estimation in Images and
    Videos
  Publication Date: 2023-02-22 00:00:00
  Table 1 caption:
    table_text: TABLE I Quantitative Results. We Train PoseAug and PoseAug-V on H36
      M and Evaluate Them on Other Datasets Under Cross-Scaneario Setup
  Table 10 caption:
    table_text: TABLE X Ablation on startend Pose Augmentation on 3DHP and 3DPW Datasets
  Table 2 caption:
    table_text: TABLE II Applicability to Different Estimators. Performance Comparison
      in MPJPE for Various Pose Estimators Trained Without and With PoseAug on H36
      M and 3DHP Datasets. DET, CPN, HR and GT Denote 3D Pose Estimation Model Trained
      on Different 2D Pose Sources, Respectively. We Evaluate the Model on H36 M Test
      Set With the Corresponding 2D Pose Sources. On 3DHP Test Set, We Use GT 2D Poses
      as Input for Evaluating Model's Generalization. We Can Observe PoseAug Consistently
      Decreases Errors for All Datasets and Estimators
  Table 3 caption:
    table_text: TABLE III Ablation Study on Components of the Augmentor. We Report
      MPJPE on H36 M and 3DHP Datasets
  Table 4 caption:
    table_text: TABLE IV Ablation Study on Error Feedback Strategy on 3DPW. Augmentation
      Denotes the Combination of BA, BL, RT Operations
  Table 5 caption:
    table_text: TABLE V Ablation Study on the Discriminators D 2D D2D and D 3D D3D
      on H36 M and 3DHP. MPJPE is Used for Evaluation
  Table 6 caption:
    table_text: TABLE VI Ablation Study on Part-Aware KCS (PA-KCS). We Report MPJPE
      on 3DHP and PA-MPJPE on 3DPW
  Table 7 caption:
    table_text: TABLE VII Ablation Study on Augmentor of PoseAug-V. We Report MPJPE
      and PA-MPJPE for 3DHP and 3DPW, Respectively
  Table 8 caption:
    table_text: TABLE VIII Comparison Between Different Augmentation Schemes on 3DHP
      and 3DPW Datasets
  Table 9 caption:
    table_text: TABLE IX Ablation on the Number of Noise Priors on 3DHP and 3DPW Datasets
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3243400
- Affiliation of the first author: moe key laboratory of information fusion technology,
    school of automation, northwestern polytechnical university, xi'an, shaanxi, china
  Affiliation of the last author: school of life sciences, northwestern polytechnical
    university, xi'an, shaanxi, china
  Figure 1 Link: articels_figures_by_rev_year\2023\FewShot_Drug_Synergy_Prediction_With_a_PriorGuided_Hypernetwork_Architecture\figure_1.jpg
  Figure 1 caption: 'Overview of the HyperSynergy. HyperSynergy is composed of three
    modules: feature embedding network module, meta-generative network module and
    drug synergy prediction network module. The meta-generative network and drug synergy
    prediction network constitute the prior-guided hypernetwork. (a) In the meta-learning
    stage, we first sample many few-shot drug synergy prediction tasks from the base
    cell lines, and then learn the parameters of meta-generative network (which is
    composed of the Bayesian variational inference model and double-conditional weights
    generation network) to minimize the task-mean square error (i.e., the mean square
    error of query samples in a set of tasks). (b) In the fine-tuning stage, we first
    sample a few-shot drug synergy prediction task from the new cell lines (i.e.,
    data-poor cell lines), and then update the weights of the specific-task drug synergy
    prediction network by updating the input of weights generation network with gradient
    descent to minimize the mean square error of support samples. (c) Feature embedding
    network module. It first extracts the high-level drug features from two drug molecular
    graphs through multi-layer graph neural networks with skip-connection, and extracts
    the high-level gene expression features of the specific cell line through multi-layer
    convolutional neural networks, concatenating them as the embedding vector of a
    sample. (d) Meta-generative network module, which is composed of the Bayesian
    variational inference model and double-conditional weights generation network.
    The inference model aims to infer the prior distribution over task embedding vector
    of each cell line through the features of the support samples (e.g., 2 samples),
    and the task embedding vector sampled from the prior distribution is taken as
    the optimizable input of the weights generation network to generate the weights
    and biases for drug synergy prediction network. (e) Drug synergy prediction network
    module. It is a two-layer fully connected neural network.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\FewShot_Drug_Synergy_Prediction_With_a_PriorGuided_Hypernetwork_Architecture\figure_2.jpg
  Figure 2 caption: ROC and Precision-Recall curves of HyperSynergy and other seven
    methods on lower cell line similarity few-shot setting.
  Figure 3 Link: articels_figures_by_rev_year\2023\FewShot_Drug_Synergy_Prediction_With_a_PriorGuided_Hypernetwork_Architecture\figure_3.jpg
  Figure 3 caption: Scatter plot of the observed synergy scores and the predicted
    synergy scores with HyperSynergy, MatchMarker, TFSynergy, XGBoost, and DeepSynergy
    on data-rich setting.
  Figure 4 Link: articels_figures_by_rev_year\2023\FewShot_Drug_Synergy_Prediction_With_a_PriorGuided_Hypernetwork_Architecture\figure_4.jpg
  Figure 4 caption: "The influence of \u201Cunequal task setting\u201D on the performance\
    \ of HyperSynergy."
  Figure 5 Link: articels_figures_by_rev_year\2023\FewShot_Drug_Synergy_Prediction_With_a_PriorGuided_Hypernetwork_Architecture\figure_5.jpg
  Figure 5 caption: Parameter sensitivity analysis of HyperSynergy for the dimension
    of task embedding vector.
  Figure 6 Link: articels_figures_by_rev_year\2023\FewShot_Drug_Synergy_Prediction_With_a_PriorGuided_Hypernetwork_Architecture\figure_6.jpg
  Figure 6 caption: Results of HyperSynergy on different cell lines. (a) Box diagram
    of MSE of HyperSynergy on 21 meta-test cell lines. (b) Distribution of synergy
    scores of all samples in cell line SK-MEL-30 and SK-MES-1, respectively.
  Figure 7 Link: articels_figures_by_rev_year\2023\FewShot_Drug_Synergy_Prediction_With_a_PriorGuided_Hypernetwork_Architecture\figure_7.jpg
  Figure 7 caption: Influence of randomly sampled drug pairs. (a) Scatter diagram
    between MSE per task and drug pair similarity. (b) Scatter diagram between MSE
    per task and drug pair diversity.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.55
  Name of the first author: Qing-Qing Zhang
  Name of the last author: Jian-Yu Shi
  Number of Figures: 7
  Number of Tables: 3
  Number of authors: 4
  Paper title: Few-Shot Drug Synergy Prediction With a Prior-Guided Hypernetwork Architecture
  Publication Date: 2023-02-23 00:00:00
  Table 1 caption:
    table_text: "TABLE I Results (Mean \xB1 STD) of HyperSynergy and Other Seven Methods\
      \ on Few-Shot Setting in Terms of Three Regression Metrics"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE II Results (Mean \xB1 STD) of HyperSynergy and Other Seven\
      \ Methods on Zero-Shot Setting in Terms of Three regression(R) Metrics and Four\
      \ classification(C) Metrics"
  Table 3 caption:
    table_text: "TABLE III The Ablation Experimental Results (Mean \xB1STD) of HyperSynergy\
      \ on 10-Shot setting(training on 10-Shot, Test on 10-Shot) in Terms of Three\
      \ Regression Metrics"
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3248041
- Affiliation of the first author: school of computer science, fudan university, shanghai,
    china
  Affiliation of the last author: school of computer science, fudan university, shanghai,
    china
  Figure 1 Link: articels_figures_by_rev_year\2023\ADPL_Adaptive_Dual_Path_Learning_for_Domain_Adaptation_of_Semantic_Segmentation\figure_1.jpg
  Figure 1 caption: Illustration of visual inconsistency in single-domain adaptation
    pipelines. mathcal S is source image with ground-truth label, and mathcal T is
    target image. Gmathcal Srightarrow mathcal T represents image translation from
    domain- mathcal S to domain- mathcal T and vice versa. mathcal Sprime = Gmathcal
    Srightarrow mathcal T(mathcal S) and mathcal Tprime = Gmathcal Trightarrow mathcal
    S(mathcal T) are translated images in the corresponding domain. Mmathcal S and
    Mmathcal T are semantic segmentation models aligned in domain- mathcal S and domain-
    mathcal T , respectively. Visual inconsistency raised by image translation disturbs
    segmentation training in either the supervised part or the unsupervised one.
  Figure 10 Link: articels_figures_by_rev_year\2023\ADPL_Adaptive_Dual_Path_Learning_for_Domain_Adaptation_of_Semantic_Segmentation\figure_10.jpg
  Figure 10 caption: 'Visualization of different pseudo label generation strategies
    on GTA5 rightarrow Cityscapes by using DeepLab-V2 with ResNet101: (a) raw images
    from Cityscapes dataset; (b) pseudo labels generated by SPPLG (path- mathcal S
    ); (c) pseudo labels generated by SPPLG (path- mathcal T ); (d) pseudo labels
    generated by DPPLG-Average; (e) pseudo labels generated by DPPLG-Weighted (default);
    (f) ground-truth. Red rectangles highlight the differences.'
  Figure 2 Link: articels_figures_by_rev_year\2023\ADPL_Adaptive_Dual_Path_Learning_for_Domain_Adaptation_of_Semantic_Segmentation\figure_2.jpg
  Figure 2 caption: '(a) Overview of adaptive dual path learning (ADPL) framework.
    The model is optimized on source data mathcal S with pixel-level segmentation
    labels Ymathcal S and target data mathcal T without labels. ADPL consists of two
    complementary single-domain paths: path- mathcal S (learning is performed in source
    domain) and path- mathcal T (learning is conducted in target domain). Dual path
    image translation (DPIT) and dual path adaptive segmentation (DPAS) are proposed
    to make two paths interactive and promote each other. DPIT is a bidirectional
    image translation model supervised by a CycleGAN loss and a cross-domain perceptual
    loss which greatly maintains visual consistency, providing a source-to-target
    translation Gmathcal Srightarrow mathcal T(cdot) and a target-to-source translation
    Gmathcal Trightarrow mathcal S(cdot) . In DPAS, dual path pseudo label generation
    (DPPLG) module is proposed to produce pseudo labels hatYmathcal T and hatYmathcal
    Tprime for target images mathcal T and mathcal Tprime = Gmathcal Srightarrow mathcal
    T(mathcal T) by utilizing knowledge from two paths. Then the proposed Adaptive
    ClassMix is utilized to generate synthetic data by mixing two images (one raw
    image and one translated image aligned in the same domain) and their corresponding
    labels to optimize two segmentation models Mmathcal S and Mmathcal T . (b) Testing
    of ADPL. Only segmentation model Mmathcal T is used for inference.'
  Figure 3 Link: articels_figures_by_rev_year\2023\ADPL_Adaptive_Dual_Path_Learning_for_Domain_Adaptation_of_Semantic_Segmentation\figure_3.jpg
  Figure 3 caption: Using naive CycleGAN as image translator yields visual inconsistency
    between source images mathcal S and translated source images mathcal Sprime .
    Source images are from GTA5 [9] dataset.
  Figure 4 Link: articels_figures_by_rev_year\2023\ADPL_Adaptive_Dual_Path_Learning_for_Domain_Adaptation_of_Semantic_Segmentation\figure_4.jpg
  Figure 4 caption: Illustration of label correction strategy in Mmathcal T warm-up.
    This strategy revises the ground-truth labels Ymathcal S of content-changed areas
    with generated pixel-wise pseudo labels hatYmathcal Sprime of high confidence.
    The revised label Ymathcal Sprime together with mathcal Sprime are used for Mmathcal
    T training. Detailed revision is shown in green dashed rectangle.
  Figure 5 Link: articels_figures_by_rev_year\2023\ADPL_Adaptive_Dual_Path_Learning_for_Domain_Adaptation_of_Semantic_Segmentation\figure_5.jpg
  Figure 5 caption: Illustration of different dual path pseudo label generation (DPPLG)
    strategies. DPPLG aims to generate reliable pseudo labels for unlabeled target
    images by combining knowledge from two segmentation models Mmathcal T and Mmathcal
    S aligned in opposite domains. (a) DPPLG-Average averages two probability maps
    Pmathcal T(mathcal T) and Pmathcal S(mathcal Tprime ) predicted by Mmathcal T
    and Mmathcal S for pseudo-labeling. (b) DPPLG-Weighted weights two probability
    maps according to median confidence ( MCmathcal T and MCmathcal S ) calculated
    across target dataset.
  Figure 6 Link: articels_figures_by_rev_year\2023\ADPL_Adaptive_Dual_Path_Learning_for_Domain_Adaptation_of_Semantic_Segmentation\figure_6.jpg
  Figure 6 caption: Illustration of applying Adaptive ClassMix to generate synthetic
    data for segmentation model Mmathcal T training. Adaptive ClassMix selects half
    of the classes present in one image according to a pre-computed sampling probability
    and pastes the corresponding pixels to another image to generate a new synthetic
    image.
  Figure 7 Link: articels_figures_by_rev_year\2023\ADPL_Adaptive_Dual_Path_Learning_for_Domain_Adaptation_of_Semantic_Segmentation\figure_7.jpg
  Figure 7 caption: Quantitative comparison among different pseudo label generation
    strategies. We report precision vs recall of the generated pseudo labels for different
    strategies.
  Figure 8 Link: articels_figures_by_rev_year\2023\ADPL_Adaptive_Dual_Path_Learning_for_Domain_Adaptation_of_Semantic_Segmentation\figure_8.jpg
  Figure 8 caption: Visualization of feature space. We map features learned by different
    models to 2D space with UMAP [77]. Compared with others, our ADPL learns the most
    compact and distinguishable clusters.
  Figure 9 Link: articels_figures_by_rev_year\2023\ADPL_Adaptive_Dual_Path_Learning_for_Domain_Adaptation_of_Semantic_Segmentation\figure_9.jpg
  Figure 9 caption: 'Visualization of segmentation results on GTA5 rightarrow Cityscapes
    by using DeepLab-V2 with ResNet101: (a) raw images from Cityscapes dataset; (b)
    segmentation results of Mmathcal S(0) ; (c) segmentation results of Mmathcal T(0)
    ; (d) segmentation results of DPL [23]; (e) segmentation results of ADPL; (f)
    ground-truth. Red rectangles highlight the differences.'
  First author gender probability: 0.75
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.63
  Name of the first author: Yiting Cheng
  Name of the last author: Wenqiang Zhang
  Number of Figures: 11
  Number of Tables: 14
  Number of authors: 5
  Paper title: 'ADPL: Adaptive Dual Path Learning for Domain Adaptation of Semantic
    Segmentation'
  Publication Date: 2023-02-23 00:00:00
  Table 1 caption:
    table_text: "TABLE I Comparison With State-of-the-Art Methods on GTA5 \u2192 \u2192\
      Cityscapes Scenario. Bold: Best Result. Underline: Second Best Result"
  Table 10 caption:
    table_text: 'TABLE X Comparison of Different ClassMix Variants: Learning Without
      ClassMix (None), Naive ClassMix, Single-Direction Adaptive ClassMix (SA-ClassMix)
      and Adaptive ClassMix (A-ClassMix). We Report mIoU and Class-Wise IoU on M (1)
      T MT(1)'
  Table 2 caption:
    table_text: "TABLE II Comparison With State-of-the-Art Methods on SYNTHIA \u2192\
      \ \u2192 Cityscapes Scenario. Bold: Best Result. Underline: Second Best Result"
  Table 3 caption:
    table_text: "TABLE III Comparison With State-of-the-Art Methods on GTA5 \u2192\
      \ \u2192BDD100K Scenario. The Results of CyCADA [14] are Reproduced by MADAN+\
      \ [25]. \u2217 Denotes Multi-Source Domain Adaptation Method Trained on GTA5+SYNTHIA\
      \ \u2192 \u2192 BDD100K. Bold: Best Result. Underline: Second Best Result"
  Table 4 caption:
    table_text: TABLE IV Comparison of Different Image Translation Models
  Table 5 caption:
    table_text: TABLE V Comparison of Different Pseudo Label Generation Strategies
  Table 6 caption:
    table_text: TABLE VI Study on Online Pseudo Label Generation
  Table 7 caption:
    table_text: TABLE VII Ablation Study on Stage-Wise DPAS. Compared With DPL, our
      ADPL Achieves Better Performance While Having Less Training Cost
  Table 8 caption:
    table_text: TABLE VIII Study on M T MT Warm-Up. We Report the Final Performance
      of DPL and ADPL
  Table 9 caption:
    table_text: TABLE IX Ablation Study on Label Correction Strategy Used in M T MT
      Warm-Up
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3248294
- Affiliation of the first author: school of artificial intelligence, xidian university,
    xi'an, china
  Affiliation of the last author: school of artificial intelligence, xidian university,
    xi'an, china
  Figure 1 Link: articels_figures_by_rev_year\2023\Adaptive_SearchandTraining_for_Robust_and_Efficient_Network_Pruning\figure_1.jpg
  Figure 1 caption: Summary of the proposed iterative adaptive search and training
    method. After a coarse training, we first sample compact subnets from the target
    network using an adaptive search based on the weights; then we train the subnets
    and map the trained subnets to the target network. This search-train-map process
    will be repeated iteratively until the sampled subnets converge. The best performing
    subnetwork will be further fine-tuned by multi-teacher knowledge distillation
    into the final output pruned network.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Adaptive_SearchandTraining_for_Robust_and_Efficient_Network_Pruning\figure_2.jpg
  Figure 2 caption: Detailed illustration of the adaptive search-and-training method.
    First, we sample compact subnets of the target network based on weight alpha .
    Second, we search, train, and map the subnets to the target network. Note that
    such a search-train-map process will be iterated until the sampled subnets converge.
    The best performing subnetwork is finally fine-tuned into the output pruned network.
  Figure 3 Link: articels_figures_by_rev_year\2023\Adaptive_SearchandTraining_for_Robust_and_Efficient_Network_Pruning\figure_3.jpg
  Figure 3 caption: The structure of our proposed ThreshNet. ThreshNet consists of
    fully connected layers; for input of network weight with different sizes, we process
    them with different fully connected layers. The final output is a pruning threshold
    for the corresponding input layer, which is used as a guide to retain the corresponding
    portion of the weights. Retained channels are determined along with weights.
  Figure 4 Link: articels_figures_by_rev_year\2023\Adaptive_SearchandTraining_for_Robust_and_Efficient_Network_Pruning\figure_4.jpg
  Figure 4 caption: We show the performance of all sampled subnetworks during the
    search of resnet-110 on Cifar100 dataset. Before the search, the target network
    has been coarsely trained; after the search, the best performing subnetwork will
    be fine-tuned to convergence.
  Figure 5 Link: articels_figures_by_rev_year\2023\Adaptive_SearchandTraining_for_Robust_and_Efficient_Network_Pruning\figure_5.jpg
  Figure 5 caption: Ablation studies on distillation teachers and random selection.
    a) The impact of choosing different large networks as teacher networks on knowledge
    distillation; b) AccuracyFLOPs of the 200 sampled subnets on ResNet-50.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.95
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.55
  Name of the first author: Xiaotong Lu
  Name of the last author: Guangming Shi
  Number of Figures: 5
  Number of Tables: 6
  Number of authors: 6
  Paper title: Adaptive Search-and-Training for Robust and Efficient Network Pruning
  Publication Date: 2023-02-24 00:00:00
  Table 1 caption:
    table_text: TABLE I Pruning Rate for Different Gamma. The Pruned Network is the
      ResNet-20 Model Based on the CIFAR-10 Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE II Comparison of Different Pruning Algorithms for ResNet on\
      \ CIFAR10. 'FLOPS Pruned' Means the Calculation and Pruning Rate. 'acc Drop'\
      \ Means Accuracy and Performance Drop. The \u201C\u2713\u201D and \u201C\u2717\
      \u201D Under \u201Cpre-Trained\u201D and \u201Cfine-Tuned\u201D Indicate Whether\
      \ the Corresponding Method Needs to Be Pretrained Before Pruning or Optimized\
      \ Afterward, Respectively. Note That the Data of SFP and FPGM are From the Training\
      \ Log Published by the Authors, and the Data of TAS are From the Open Source\
      \ Code of the Author"
  Table 3 caption:
    table_text: "TABLE III Comparison of Different Pruning Algorithms for ResNet on\
      \ CIFAR-100. The Parameters in the Above Table are Consistent With TableII,\
      \ Where \u201Cours (Wo ThreshNet)\u201D Means Pruning With a Fixed Pruning Rate\
      \ for Each Layer in the Network"
  Table 4 caption:
    table_text: TABLE IV Comparison of Different Pruning Algorithms of ResNet50 on
      ImageNet
  Table 5 caption:
    table_text: TABLE V Comparison of Different Pruning Algorithms of MobileNet V2
      on ImageNet
  Table 6 caption:
    table_text: "TABLE VI Pruning ResNet-56 At a Pruning Rate of About 50% on CIFAR-10.\
      \ \u201Crandom\u201D Represents a Random Sample, and \u201Cgumbel-Softmax\u201D\
      \ Denotes the Increase of Randomness Using Gumbel-Softmax and \u2717 Means That\
      \ There is No Randomness and the Channel is Manually Pruned Sequentially According\
      \ to Its Corresponding Weight \u03B1 \u03B1 From the Largest to the Smallest"
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3248612
- Affiliation of the first author: tmcc, cs, nankai university, tianjin, china
  Affiliation of the last author: tmcc, cs, nankai university, tianjin, china
  Figure 1 Link: articels_figures_by_rev_year\2023\Localization_Distillation_for_Object_Detection\figure_1.jpg
  Figure 1 caption: "Existing KD pipelines for object detection. \u2460 Logit Mimicking:\
    \ classification KD in [24]. \u2461 Feature Imitation: recent popular methods\
    \ distill intermediate features based on various distillation regions, which usually\
    \ need adaptive layers to align the size of the student's feature map. \u2462\
    \ Pseudo BBox Regression: treating teachers' predicted bounding boxes as additional\
    \ regression targets [6], [60]."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Localization_Distillation_for_Object_Detection\figure_2.jpg
  Figure 2 caption: "Bottom edge for \u201Celephant\u201D and right edge for \u201C\
    surfboard\u201D are ambiguous to locate."
  Figure 3 Link: articels_figures_by_rev_year\2023\Localization_Distillation_for_Object_Detection\figure_3.jpg
  Figure 3 caption: "Illustration of localization distillation (LD) for an edge e\u2208\
    B . Only the localization branch is visualized here. S(\u22C5,\u03C4) is the generalized\
    \ SoftMax function with temperature \u03C4 . For a given detector, we first switch\
    \ the bounding box representation to probability distribution. Then, we determine\
    \ where to distill via region weighting on the main distillation region and the\
    \ valuable localization region. Finally, we calculate the LD loss between two\
    \ probability distributions predicted by the teacher and the student."
  Figure 4 Link: articels_figures_by_rev_year\2023\Localization_Distillation_for_Object_Detection\figure_4.jpg
  Figure 4 caption: Visual comparisons of SOTA feature imitation and our LD. We show
    the average L1 error of classification scores and box probability distributions
    between teacher and student at the P4, P5, P6 and P7 FPN levels. The teacher is
    ResNet-101 and the student is ResNet-50. The results are evaluated on MS COCO
    val2017.
  Figure 5 Link: articels_figures_by_rev_year\2023\Localization_Distillation_for_Object_Detection\figure_5.jpg
  Figure 5 caption: "Visual comparisons between the state-of-the-art feature imitation\
    \ and our LD. We show the per-location L1 error summation of the localization\
    \ head logits between the teacher and the student as the P5 (first row) and P6\
    \ (second row) FPN levels. The teacher is ResNet-101 and the student is ResNet-50.\
    \ We can see that compared to the GI imitation [10], our method (\u201CMain LD\
    \ + VLR LD\u201D) can significantly reduce the errors for almost all the locations.\
    \ Darker is better. Best viewed in color."
  Figure 6 Link: articels_figures_by_rev_year\2023\Localization_Distillation_for_Object_Detection\figure_6.jpg
  Figure 6 caption: "Average teacher-student error on (left) deep feature representation,\
    \ (middle) class logits, and (right) bbox logits. \u201COurs\u201D denotes \u201C\
    Main LD + VLR LD + Main KD\u201D. The curves are evaluated on MS COCO val2017."
  Figure 7 Link: articels_figures_by_rev_year\2023\Localization_Distillation_for_Object_Detection\figure_7.jpg
  Figure 7 caption: The 2D contour plots of AP landscapes in feature subspace. The
    AP landscapes are evaluated on MS COCO val2017.
  Figure 8 Link: articels_figures_by_rev_year\2023\Localization_Distillation_for_Object_Detection\figure_8.jpg
  Figure 8 caption: The average precision (AP) during the early training stage. The
    feature imitation significantly slows down the convergence and gets a sub-optimal
    generalization. Logit mimicking (Ours) can reduce the training difficulty in the
    early training stage.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.75
  Name of the first author: Zhaohui Zheng
  Name of the last author: Ming-Ming Cheng
  Number of Figures: 8
  Number of Tables: 8
  Number of authors: 7
  Paper title: Localization Distillation for Object Detection
  Publication Date: 2023-02-24 00:00:00
  Table 1 caption:
    table_text: TABLE I Ablations. We Show Ablation Experiments for LD and VLR on
      MS COCO val2017
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE II Evaluation of Selective Region Distillation for KD and Our\
      \ LD. The Teacher-Student Pair is ResNet-101 \u2192 \u2192ResNet-50 for COCO,\
      \ and ResNet-101 \u2192 \u2192ResNet-18 for VOC 07+12"
  Table 3 caption:
    table_text: TABLE III Quantitative Results of LD for Lightweight Detectors. The
      Teacher is ResNet-101. The Results are Reported on MS COCO val2017
  Table 4 caption:
    table_text: TABLE IV Quantitative Results of LD on Various Popular Dense Object
      Detectors. The Teacher is ResNet-101 and the Student is ResNet-50. The Results
      are Reported on MS COCO val2017
  Table 5 caption:
    table_text: TABLE V Quantitative Results of Rotated LD on the Popular Arbitrary-Oriented
      Object Detectors. The Teacher is ResNet-34 and the Student is ResNet-18. The
      Results are Reported on the Validation Set of DOTA-V1.0
  Table 6 caption:
    table_text: "TABLE VI Logit Mimicking Vs . Feature Imitation. \u201Cours\u201D\
      \ Means We Use the Selective Region Distillation, I.e., \u201Cmain LD + VLR\
      \ LD + Main KD\u201D. \u201C\u201D Denotes We Remove the \u201Cmain KD\u201D\
      . the Teacher is ResNet-101 and the Student is ResNet-50 [22]. The Results are\
      \ Reported on MS COCO val2017"
  Table 7 caption:
    table_text: 'TABLE VII The Average Pearson Correlation Coefficient Between the
      Teacher-Student Pair. ''GI'': GI Imitation. ''ours'': Our Logit Mimicking Scheme
      With the Selective Region Distillation. The Results are Evaluated on MS COCO
      val2017'
  Table 8 caption:
    table_text: "TABLE VIII Comparison With State-of-The-Art Methods on COCO val2017\
      \ and test-Dev2019. TS: Traning Schedule. '1\xD7': Single-Scale Training 12\
      \ Epochs. '2\xD7': Multi-Scale Training 24 Epochs"
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3248583
- Affiliation of the first author: zhejiang university, hangzhou, china
  Affiliation of the last author: zhejiang university, zhejiang university, hangzhou,
    china
  Figure 1 Link: articels_figures_by_rev_year\2023\Personalized_Latent_Structure_Learning_for_Recommendation\figure_1.jpg
  Figure 1 caption: An illustration of latent structure learning for recommendation,
    i.e., disentangling user-item latent factors and the dependencies of factors.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Personalized_Latent_Structure_Learning_for_Recommendation\figure_2.jpg
  Figure 2 caption: 'Schematic of the proposed PlanRec framework, which consists of
    three critical components: 1) shared structure learning that learns the shared
    dependencies of latent factors discovered by VAE based on three hypotheses (omitted
    here for simplicity); 2) personalized structure learning that probabilistically
    sample personalized structures conditioned on the shared structure and user embedding;
    3) uncertainty estimation that measures the personalization uncertainty, and accordingly
    balance the predictions of personalization and shared knowledge.'
  Figure 3 Link: articels_figures_by_rev_year\2023\Personalized_Latent_Structure_Learning_for_Recommendation\figure_3.jpg
  Figure 3 caption: A visualization of the correlation between users' interest consistency
    w.r.t. item categories (left) activeness (right) and personalization uncertainty
    estimated by PlanRec. Lines are obtained using linear regression.
  Figure 4 Link: articels_figures_by_rev_year\2023\Personalized_Latent_Structure_Learning_for_Recommendation\figure_4.jpg
  Figure 4 caption: A visualization of the learned shared structure and the personalized
    masking probabilities of four sampled users. We visualize the overall item category
    distributions of all malefemale users, and the user-specific distributions.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.75
  Name of the first author: Shengyu Zhang
  Name of the last author: Fei Wu
  Number of Figures: 4
  Number of Tables: 8
  Number of authors: 8
  Paper title: Personalized Latent Structure Learning for Recommendation
  Publication Date: 2023-02-24 00:00:00
  Table 1 caption:
    table_text: TABLE I Statistics of the Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II A Comparison Between PlanRec and Various Kinds of Baselines
      on Three Real-World Datasets. We Conduct Two-Sided Test, and p p-Value <0.05
      <0.05 Indicates That the Improvement Over the Best-Performing Baseline is Statistically
      Significant
  Table 3 caption:
    table_text: TABLE III Analysis of the Number of Sampled Personalized Structures
      N m Nm, as Defined Near (16)
  Table 4 caption:
    table_text: TABLE IV Analysis of the Robustness Against User Interest Shifts w.r.t.
      Item Category on the OOD MovieLens Dataset and User Purchasing Power on the
      OOD Synthetic Dataset
  Table 5 caption:
    table_text: TABLE V Latent Factor Disentanglement Analysis
  Table 6 caption:
    table_text: TABLE VI Disentangled Dependency Analysis
  Table 7 caption:
    table_text: TABLE VII Ablation Study by Adding Critical Components to the Base
      Model in a Cumulative Manner
  Table 8 caption:
    table_text: TABLE VIII Ablation Study by Adding Three Structure Regularizations
      to the Base Model in a Cumulative Manner
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3247563
- Affiliation of the first author: "computer vision lab of eth zurich, z\xFCrich,\
    \ switzerland"
  Affiliation of the last author: "computer vision lab of eth zurich, z\xFCrich, switzerland"
  Figure 1 Link: articels_figures_by_rev_year\2023\PDCNet_Enhanced_Probabilistic_Dense_Correspondence_Network\figure_1.jpg
  Figure 1 caption: Estimating dense correspondences between the query (a) and the
    reference (b) images. The query is warped according to the predicted flows (c)-(d).
    The baseline (c) does not estimate an uncertainty map and is therefore unable
    to filter out the inaccurate predictions at e.g. occluded and homogeneous regions.
    In contrast, our PDC-Net+ (d) not only learns more accurate correspondences, but
    also when to trust them. It predicts a robust uncertainty map that identifies
    accurate matches and excludes incorrect and unmatched pixels (red).
  Figure 10 Link: articels_figures_by_rev_year\2023\PDCNet_Enhanced_Probabilistic_Dense_Correspondence_Network\figure_10.jpg
  Figure 10 caption: Sparsification plots for AEPE (left) and PCK-5 (right) on MegaDepth
    (top) and KITTI-2015 (bottom), comparing multiple uncertainty measures when using
    PDC-Net+ (D). Smaller AUSE (in parenthesis) is better.
  Figure 2 Link: articels_figures_by_rev_year\2023\PDCNet_Enhanced_Probabilistic_Dense_Correspondence_Network\figure_2.jpg
  Figure 2 caption: Distribution of errors |haty-y| on MegaDepth [33] between the
    flow haty estimated by GLU-Net [22] and the ground-truth y .
  Figure 3 Link: articels_figures_by_rev_year\2023\PDCNet_Enhanced_Probabilistic_Dense_Correspondence_Network\figure_3.jpg
  Figure 3 caption: "Predictive log-density log p(y|X) (2)\u2013(3) for an inlier\
    \ (red), outlier (blue), and ambiguous (green) match. Our mixture model faithfully\
    \ represents the uncertainty also in the latter case."
  Figure 4 Link: articels_figures_by_rev_year\2023\PDCNet_Enhanced_Probabilistic_Dense_Correspondence_Network\figure_4.jpg
  Figure 4 caption: "The proposed architecture for flow and uncertainty estimation.\
    \ The correlation uncertainty module Utheta independently processes each 2D-slice\
    \ Cijcdot cdot of the correlation volume. Its output is combined with the estimated\
    \ mean flow mu and the mixture model parameters phi from the previous scale level.\
    \ These are then given to the uncertainty predictor, which finally estimates the\
    \ weight lbrace alpha mrbrace 1M and variance lbrace sigma 2mrbrace 1M parameters\
    \ of our constrained mixture model (2)\u2013(4)."
  Figure 5 Link: articels_figures_by_rev_year\2023\PDCNet_Enhanced_Probabilistic_Dense_Correspondence_Network\figure_5.jpg
  Figure 5 caption: Visualization of the estimated uncertainties by masking the warped
    query image to only show the confident flow predictions. The standard approach
    (c) uses a common decoder for both flow and uncertainty estimation. It generates
    overly confident predictions in the sky and grass. The uncertainty estimates are
    substantially improved in (d), when using the proposed architecture described
    in Section III-C. Adding the flow perturbations for self-supervised training (Section
    III-D) further improves the robustness and generalization of the uncertainties
    (e). For reference, we also visualize the flow and confidence mask (f) predicted
    by the recent state-of-the-art approach RANSAC-Flow [30].
  Figure 6 Link: articels_figures_by_rev_year\2023\PDCNet_Enhanced_Probabilistic_Dense_Correspondence_Network\figure_6.jpg
  Figure 6 caption: The synthetic image pair generation pipeline for our self-supervised
    training. Our pipeline is divided in three parts. First, we randomly generate
    a synthetic flow field tildeY , used to create an image pair from a base image.
    Next, perturbations are added to the synthetic flow field, leading to a new pair
    of query and reference images related by the background flow field Ybg . Finally,
    each object is iteratively added to the image pair, using a randomly sampled object
    flow Yfg . The final ground-truth flow field Y relating the image pair is also
    updated accordingly.
  Figure 7 Link: articels_figures_by_rev_year\2023\PDCNet_Enhanced_Probabilistic_Dense_Correspondence_Network\figure_7.jpg
  Figure 7 caption: Pair of query and reference images with independently moving objects
    A , B and C . Objects A and B are solely visible in the reference and query images
    respectively. The regions in the reference image that are occluded by the objects
    are represented with dashed red contours. They correspond to areas A , Bprime
    and Cprime in the occlusion mask. Our injective mask corresponds to Cprime , i.e.
    the black area in the lower right frame.
  Figure 8 Link: articels_figures_by_rev_year\2023\PDCNet_Enhanced_Probabilistic_Dense_Correspondence_Network\figure_8.jpg
  Figure 8 caption: Results on ETH3D [97]. AEPE (left), PCK-1 (center) and PCK-5 (right)
    are plotted w.r.t. the inter-frame interval length. Note that results in COTR
    [69] are provided only for AEPE. The corresponding table with the detailed metrics
    is presented in supplementary, Table 4, available online.
  Figure 9 Link: articels_figures_by_rev_year\2023\PDCNet_Enhanced_Probabilistic_Dense_Correspondence_Network\figure_9.jpg
  Figure 9 caption: Sparse correspondence identification on HPatches [85], evaluating
    the 2000 most confident matches for each method.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Prune Truong
  Name of the last author: Luc Van Gool
  Number of Figures: 14
  Number of Tables: 9
  Number of authors: 4
  Paper title: 'PDC-Net+: Enhanced Probabilistic Dense Correspondence Network'
  Publication Date: 2023-02-27 00:00:00
  Table 1 caption:
    table_text: "TABLE I PCK (%) Results on Sparse Correspondences of the MegaDepth\
      \ [33] and RobotCar [34], [96] Datasets. In the Top Part, Methods are Trained\
      \ on Different Data Than MegaDepth While Approaches in the Bottom Part are Trained\
      \ on the MegaDepth Training Set. We Additionally Compare the Run-Time (in ms)\
      \ of all Methods on 480\xD7480 480\xD7480 Images on a NVIDIA GeForce RTX 2080\
      \ Ti GPU"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Dense Correspondence Estimation Results on HPatches [85].
      PCK are in %
  Table 3 caption:
    table_text: TABLE III Optical Flow Results on the Training Splits of KITTI [35].
      The Upper Part Contains Generic Matching Networks, While the Bottom Part Lists
      Specialized Optical Flow Methods, not Trained on KITTI
  Table 4 caption:
    table_text: TABLE IV Two-View Geometry Estimation on the Outdoor Dataset YFCC100M
      [36]. The Top Section Compares Sparse Methods While the Bottom Section Contains
      Dense Methods
  Table 5 caption:
    table_text: "TABLE V Two-View Geometry Estimation on the Indoor Dataset ScanNet\
      \ [37]. The Top Section Compares Sparse Methods, the Middle Section Presents\
      \ Dense-to-Sparse Approaches, and the Bottom One Contains Dense Methods. The\
      \ Symbol \u2020 \u2020 Denotes That the Network was not Trained on ScanNet"
  Table 6 caption:
    table_text: "TABLE VI Visual Localization on the Aachen Day-Night Dataset [38],\
      \ [39]. We Follow the Fixed Evaluation Protocol of [38] and Report the Percentage\
      \ of Query Images Localized Within X X Meters and Y \u2218 Y\u2218 of the Ground-Truth\
      \ Pose"
  Table 7 caption:
    table_text: "TABLE VII Retrieval-Based Localization on the Aachen Day-Night Dataset\
      \ [39] for a Pose Error Threshold of 5m, 10 \u2218 \u2218 in %. Results for\
      \ the Retrieval Methods are Extracted From the Official Leaderboard [38]"
  Table 8 caption:
    table_text: "TABLE VIII Ablation Study. In the Top Part, Different Probabilistic\
      \ Models are Compared (Sections III-A\u2013III-B). In the Second Part, a Constrained\
      \ Mixture is Used, and Different Architectures for Uncertainty Estimation are\
      \ Compared (Section III-C). In the Third Part, we Analyze the Impact of Our\
      \ Training Data With Perturbations (Section III-D). PCK and Fl are in %"
  Table 9 caption:
    table_text: TABLE IX Ablation Study of PDC-Net+, Trained With Only the First Stage,
      Described in Section IV-A. In the Top Part, we Compare Training With a Single
      or Multiple Independently Moving Objects (Section III-E). In the Bottom Part,
      Different Masking Strategies During Training are Compared (Section III-F). All
      Metrics are Computed With a Single Forward-Pass (D). PCK and Fl are in %
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3249225
- Affiliation of the first author: key laboratory of mathematics mechanization, academy
    of mathematics and systems science, chinese academy of sciences, beijing, china
  Affiliation of the last author: key laboratory of mathematics mechanization, academy
    of mathematics and systems science, chinese academy of sciences, beijing, china
  Figure 1 Link: Not Available
  Figure 1 caption: Not Available
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Manolis C. Tsakiris
  Name of the last author: Manolis C. Tsakiris
  Number of Figures: Not Available
  Number of Tables: 0
  Number of authors: 1
  Paper title: "Low-Rank Matrix Completion Theory via Pl\xFCcker Coordinates"
  Publication Date: 2023-02-28 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3250325
- Affiliation of the first author: institute of artificial intelligence of beihang
    university, beijing, china
  Affiliation of the last author: ucas-terminus ai lab, university of chinese academy
    of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2023\Normalization_Techniques_in_Training_DNNs_Methodology_Analysis_and_Application\figure_1.jpg
  Figure 1 caption: Illustration of normalization operations discussed in this paper.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.55
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Lei Huang
  Name of the last author: Ling Shao
  Number of Figures: 1
  Number of Tables: 1
  Number of authors: 6
  Paper title: 'Normalization Techniques in Training DNNs: Methodology, Analysis and
    Application'
  Publication Date: 2023-02-28 00:00:00
  Table 1 caption:
    table_text: "TABLE I Summary of the Main Single-Mode Normalization Methods, Based\
      \ on Our Proposed Framework for Describing Normalizing-Activations-as-Functions\
      \ Methods. We Also Provide the Comparison of Computational Complexity. Note\
      \ That we Separate the Complexities of NOP and NRR by Two Terms. We Use O( \u03D5\
      \ \u03B3,\u03B2 (X)) O(\u03C6\u03B3,\u03B2(X)) to Denote the Complexity of the\
      \ Sub-Networks Designed for Generating the Affine Parameters"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3250241
- Affiliation of the first author: jd explore academy, beijing, china
  Affiliation of the last author: jd explore academy, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2023\Augmentation_Pathways_Network_for_Visual_Recognition\figure_1.jpg
  Figure 1 caption: 'Left: Examples of original images and their lightly augmented
    (randomly Resize, Crop, Flip) and heavily augmented (Gray, GridShuffle, RandAugment)
    versions. Middle: Improvement on Top-1 accuracy by applying two heavy augmentations
    (Gray and GridShuffle) on ImageNet and its subsets (ImageNet n , n indicates the
    number of images used per category). Standard network (ResNet-50) performs quite
    unstable, showing marginal or adverse effects. Right: Improvement on Top-1 accuracy
    by applying searched augmentation (RandAugment [13]: A collection of randomly
    selected heavy augmentations) on ImageNet. Augmentation policy searched for ResNet-50
    leads to performance drop on iResNet-50. In contrast, Augmentation Pathways (AP)
    based network can steadily benefit from a much wider range of augmentation policies
    for robust classification.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Augmentation_Pathways_Network_for_Visual_Recognition\figure_2.jpg
  Figure 2 caption: Illustration of standard CNN (Left) and our proposed Augmentation
    Pathways network (Right) for handling data augmentations. Details of the basic
    AP-Conv in purple dashed box is illustrated in Fig. 3.
  Figure 3 Link: articels_figures_by_rev_year\2023\Augmentation_Pathways_Network_for_Visual_Recognition\figure_3.jpg
  Figure 3 caption: The detailed structure of basic augmentation pathway based convolutional
    layer.
  Figure 4 Link: articels_figures_by_rev_year\2023\Augmentation_Pathways_Network_for_Visual_Recognition\figure_4.jpg
  Figure 4 caption: The 3rd-order homogeneous augmentation pathways network is extended
    from the basic AP but handle heavy augmentations under two different hyperparameters
    ( g for Grid Shuffle) according to the visual feature dependencies among input
    images.
  Figure 5 Link: articels_figures_by_rev_year\2023\Augmentation_Pathways_Network_for_Visual_Recognition\figure_5.jpg
  Figure 5 caption: The network architecture of our high-order heterogeneous augmentation
    pathways network. Four heterogeneous neural pathways (HeAP 4 ) are responding
    to four different input images (lightly augmented images, GridShuffled images
    with g=(2,4,7)). Note that only the main neural pathway in red color is activated
    during inference.
  Figure 6 Link: articels_figures_by_rev_year\2023\Augmentation_Pathways_Network_for_Visual_Recognition\figure_6.jpg
  Figure 6 caption: The structure of augmentation pathway based convolutional layer
    without sharing feature.
  Figure 7 Link: articels_figures_by_rev_year\2023\Augmentation_Pathways_Network_for_Visual_Recognition\figure_7.jpg
  Figure 7 caption: Top-1 accuracy (%) on ImageNet 100 by using RandAugment with different
    ( n,m ).
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.8
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.99
  Name of the first author: Yalong Bai
  Name of the last author: Tao Mei
  Number of Figures: 7
  Number of Tables: 8
  Number of authors: 5
  Paper title: Augmentation Pathways Network for Visual Recognition
  Publication Date: 2023-02-28 00:00:00
  Table 1 caption:
    table_text: TABLE I Examples of Data Augmentations With Their Hyperparameters.
      Gray, Blur, Gridshuffle, MPN are Manually Designed Heavy Augmentations. RandAugment
      is a Searched Augmentation Combination Including 14 Different Image Transformations
      (e.g., Shear, Equalize, Solarize, Posterize, Rotate. Most of Them are Heavy
      Transformations)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE II The performance on ImageNet Parameters MACs on ResNet, iResNet,\
      \ ResNeXt, MobileNet V2, ConvNeXt and Their Basic Augmentation Pathways (AP)\
      \ Version on Given Additional Heavy Augmentation Policy RandAugment (for Generating\
      \ \u03C6 \u03D5). Repro: Our Reproduction of Each Method With Their Original\
      \ Augmentation Settings"
  Table 3 caption:
    table_text: TABLE III Performance Comparison on ImageNet Subsets. AP-ResNet Achieves
      Significant Improvements With Different Heavy Data Augmentation Policies
  Table 4 caption:
    table_text: "TABLE IV Recognition Accuracy of: 1) 3rd-Order Augmentation Pathway\
      \ (AP 3 3) Based ResNet-50 by Equipping Additional Augmentation RandAugment\
      \ 2 ((n,m)\u2208(1,5),(2,9)) 2((n,m)\u2208(1,5),(2,9)), and 2) Heterogeneous\
      \ Augmentation Pathways (HeAP 4 4) Based Network by Equipping Additional Augmentation\
      \ RandAugment 3 ((n,m)\u2208(1,5),(2,9),(4,15)) 3((n,m)\u2208(1,5),(2,9),(4,15))"
  Table 5 caption:
    table_text: TABLE V AP-ResNet-50 wo Sharing Weights for GridShuffle(7)
  Table 6 caption:
    table_text: TABLE VI The Effect of Removing Cross Pathways Connections, and Randomly
      Feeding Inputs to Different Pathways. Heavy Augmentation is RandAugment
  Table 7 caption:
    table_text: TABLE VII The Impact of Cross Pathways Regularization Term S S and
      its Weight for AP-ResNet-50 With RandAugment
  Table 8 caption:
    table_text: TABLE VIII Accuracy After Introducing Aggressive Crop Operation
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3250330
- Affiliation of the first author: "computer vision laboratory, \xE9cole polytechnique\
    \ f\xE9d\xE9rale de lausanne, lausanne, switzerland"
  Affiliation of the last author: "computer vision laboratory, \xE9cole polytechnique\
    \ f\xE9d\xE9rale de lausanne, lausanne, switzerland"
  Figure 1 Link: articels_figures_by_rev_year\2023\Persistent_Homology_With_Improved_Locality_Information_for_More_Effective_Deline\figure_1.jpg
  Figure 1 caption: 2D and 3D delineation. (a) Aerial image and slice of a microscopy
    stack. (b) A network trained using a standard homology-based loss yields road
    and neurite interruptions. (c) One trained using our localized loss is more topologically
    accurate and produces predictions that closely resemble the ground truth (d).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Persistent_Homology_With_Improved_Locality_Information_for_More_Effective_Deline\figure_2.jpg
  Figure 2 caption: Filtration. When the distance map shown on the left is filtered
    by thresholding, the loop h emerges at scale bh and is filled at scale dh . This
    gives rise to the point (bh,dh) in the persistence diagram shown on the right.
    Here, thresholding means retaining all pixels whose value is lower than the threshold.
  Figure 3 Link: articels_figures_by_rev_year\2023\Persistent_Homology_With_Improved_Locality_Information_for_More_Effective_Deline\figure_3.jpg
  Figure 3 caption: Comparing filtration functions on synthetic data. The binary ground
    truth road annotation (top-left in each table part) contains four loops, marked
    with cyan dashed lines. We synthesized a predicted class affinity map (bottom-left
    in each part) by extending one road to the left and interrupting another. In consequence,
    loop B and D from the ground truth are joined into B' in the prediction, and A
    is split into A' and E'. For each filtration method, we show binary masks resulting
    from filtration at different scales, pairs of persistence diagrams, and their
    optimal matches.
  Figure 4 Link: articels_figures_by_rev_year\2023\Persistent_Homology_With_Improved_Locality_Information_for_More_Effective_Deline\figure_4.jpg
  Figure 4 caption: Sensitivity of the topological loss term C to the number of injected
    errors (a) Ground truth distance maps of road networks. (b) Distance maps corrupted
    by introducing false roads and interruptions. We randomly injected one error at
    a time, obtaining corrupt distance maps with 30 errors. We repeated this simulation
    10 times. (c,d) The cumulative distribution function of change in the loss term
    in response to injecting one error. In (c), C is evaluated using the filtration
    by thresholding distance maps, whereas in (d) we use our filtration. The probability
    of decreasing the existing loss term by injecting additional errors is around
    0.4, whereas for our loss term it drops to 0.2. We conclude that our loss term
    is more monotonic with respect to the error number.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Doruk Oner
  Name of the last author: Pascal Fua
  Number of Figures: 4
  Number of Tables: 4
  Number of authors: 5
  Paper title: Persistent Homology With Improved Locality Information for More Effective
    Delineation
  Publication Date: 2023-03-01 00:00:00
  Table 1 caption:
    table_text: TABLE I Validation Results on the Massachusetts Dataset. Our Loss
      Function Outperforms All PH-Based Loss Functions. We Report Means and Standard
      Deviations Over Three Independent Training Runs
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Our Loss Function Outperforms All PH-Based Loss Functions
      on the RTracer Dataset. We Report Means and Standard Deviations Over Cities
      From the Test Set
  Table 3 caption:
    table_text: TABLE III Comparative Results on the Neurons Dataset. Our Loss Outperforms
      All the Baselines. We Report Means and Standard Deviations Over Three Independent
      Training Runs
  Table 4 caption:
    table_text: TABLE IV Comparative Results on the Brain Dataset. Our Loss Outperforms
      All PH-Based Losses. Means and Standard Deviations Over Three Independent Training
      Runs as Presented
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3246921
- Affiliation of the first author: college of systems engineering, national university
    of defense technology, changsha, hunan, china
  Affiliation of the last author: school of automation, beijing institute of technology,
    beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2023\Contrastive_MultiView_Kernel_Learning\figure_1.jpg
  Figure 1 caption: Generation paradigm of the Contrastive Multi-view Kernel on images.
    For ease of expression, we consider only RGB and Depth images as the two data
    views. At the very beginning, the data mathbf xi and mathbf xprime i are encoded
    into a unified space with two mapping functions fmathbf W1(cdot) and fmathbf W2(cdot)
    . Then, they are projected into a Hilbert space with an implicit kernel mapping
    varphi (cdot) . Here, two representations of each data sample are considered as
    'positive pairs' (for which we want a high kernel similarity < boldsymbolphi i,
    boldsymbolphi prime i> ) while disjointed data samples are considered as 'negative
    pairs' (for which we want low kernel similarity < boldsymbolphi j, boldsymbolphi
    prime i> , thus promoting the diversity of the learned mappings). Note that the
    correlations between the samples are partially plotted for simplicity of the picture.
    Finally, the kernel matrices of each view can be obtained as mathbf K1 and mathbf
    K2 .
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Contrastive_MultiView_Kernel_Learning\figure_2.jpg
  Figure 2 caption: Temporary measurements in model building, including loss value,
    performances, kernel difference and representation difference, by the example
    of Gaussian CMK on BBC and BBCSport, respectively.
  Figure 3 Link: articels_figures_by_rev_year\2023\Contrastive_MultiView_Kernel_Learning\figure_3.jpg
  Figure 3 caption: Visualization of the Gaussian CMK on the 1st view of BBC dataset
    in the learning process before 100 epochs.
  Figure 4 Link: articels_figures_by_rev_year\2023\Contrastive_MultiView_Kernel_Learning\figure_4.jpg
  Figure 4 caption: Accuracy variations respect to the dimension d of latent representations.
    The solid line represents different CMK types, while the black dotted line refers
    to the best result of traditional kernels.
  Figure 5 Link: articels_figures_by_rev_year\2023\Contrastive_MultiView_Kernel_Learning\figure_5.jpg
  Figure 5 caption: Temporary measurements in model building, including loss value
    and performances, by the example of Contrastive Multiple Kernel k -means with
    Gaussian CMK on BBC and CiteSeer, respectively. The dashed line indicates corresponding
    loss is plotted for better understanding but not employed in the optimization.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.97
  Name of the first author: Jiyuan Liu
  Name of the last author: Yuanqing Xia
  Number of Figures: 5
  Number of Tables: 8
  Number of authors: 5
  Paper title: Contrastive Multi-View Kernel Learning
  Publication Date: 2023-03-06 00:00:00
  Table 1 caption:
    table_text: TABLE I Representatives of Traditional Kernel Function
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Details of the Used Datasets
  Table 3 caption:
    table_text: TABLE III Accuracy Comparison of Traditional Kernel and CMK on Kernel
      k k-Means
  Table 4 caption:
    table_text: TABLE IV Parameters of Traditional Kernel and CMK
  Table 5 caption:
    table_text: TABLE V Accuracy Comparison of Traditional Kernel and CMK on Classical
      Multiple Kernel Methods
  Table 6 caption:
    table_text: TABLE VI Accuracy Comparison Among Classical MKC Methods, CMK + +
      and CMKKM
  Table 7 caption:
    table_text: TABLE VII Accuracy Comparison of CMK (Evaluating With Kernel k k-Means)
      on the 1st View of BBC Dataset
  Table 8 caption:
    table_text: TABLE VIII Accuracy Comparison of CMK (Evaluating With Kernel k k-Means)
      and CMKKM on BBC Dataset
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3253211
- Affiliation of the first author: institute of data science, national cheng kung
    university, tainan, taiwan
  Affiliation of the last author: institute of data science, department statistics,
    center of data science (cds), miin wu school of computing, national cheng kung
    university, tainan, taiwan
  Figure 1 Link: articels_figures_by_rev_year\2023\Jointly_Defending_DeepFake_Manipulation_and_Adversarial_Attack_Using_Decoy_Mecha\figure_1.jpg
  Figure 1 caption: Detected results of the DeepFake with and without adversarial
    perturbation. From (a) to (f) are original face, manipulated face, manipulated
    face with adversarial noise, normalized absolute difference between (b) and (c),
    and the detection result of the manipulated images without (b) and with (c) adversarial
    noise, respectively. preal and pfake stand for the softmax probability obtained
    from the naive outcomes of our network sreal and sfake . With the proposed hypothesis
    testing, the detected result of (c) could be rectified to a correct one.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Jointly_Defending_DeepFake_Manipulation_and_Adversarial_Attack_Using_Decoy_Mecha\figure_2.jpg
  Figure 2 caption: Flowchart of training and testing inference. In the training phase,
    the deceptive model composed of two isolated sub-networks, was trained to map
    the real and fake images to the distinct distribution of scores. Both scores were
    used to determine whether an input image had been attacked, but only the real
    score was used for DeepFake detection.
  Figure 3 Link: articels_figures_by_rev_year\2023\Jointly_Defending_DeepFake_Manipulation_and_Adversarial_Attack_Using_Decoy_Mecha\figure_3.jpg
  Figure 3 caption: The proposed decoy mechanism. The transition probability by softmax
    function obtained from generated scores has a similar distribution to traditional
    networks trained by cross-entropy which might mislead the attacker. mathbf sreal
    and mathbf sfake denote real and fake scores, respectively. which are generated
    by the deceptive model for an input video.
  Figure 4 Link: articels_figures_by_rev_year\2023\Jointly_Defending_DeepFake_Manipulation_and_Adversarial_Attack_Using_Decoy_Mecha\figure_4.jpg
  Figure 4 caption: Curves of the training process of the deceptive model. (a) The
    proposed losses, (a) validation AUC curves (AUC-HT and AUC-SP are the AUC indices
    evaluated by hypothesis testing before and after softmax normalization), and (c)
    and (d) density plots of the real and fake sub-networks in our MesoD.
  Figure 5 Link: articels_figures_by_rev_year\2023\Jointly_Defending_DeepFake_Manipulation_and_Adversarial_Attack_Using_Decoy_Mecha\figure_5.jpg
  Figure 5 caption: The change in probability distribution by white-box attack. The
    results were obtained by MesoD after training on FaceForensics++ raw quality,
    the maximum distortion is 0.005.
  Figure 6 Link: articels_figures_by_rev_year\2023\Jointly_Defending_DeepFake_Manipulation_and_Adversarial_Attack_Using_Decoy_Mecha\figure_6.jpg
  Figure 6 caption: The change in probability distribution by black-box attack. The
    results were obtained by MesoD after training on FaceForensics++ raw quality,
    the maximum distortion is 0.016.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Guan-Lin Chen
  Name of the last author: Chih-Chung Hsu
  Number of Figures: 6
  Number of Tables: 5
  Number of authors: 2
  Paper title: Jointly Defending DeepFake Manipulation and Adversarial Attack Using
    Decoy Mechanism
  Publication Date: 2023-03-06 00:00:00
  Table 1 caption:
    table_text: TABLE I Comparison of the Proposed Deceptive Model for the Number
      of Parameters (Param) and Floating-Point Operations (FLOPs) of Backbone Networks
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Testing Accuracy (%) of the Backbone and Its Deceptive Version.
      the Detection Result With Transition Probability and Hypothesis Testing are
      Denoted by SP and HT, Respectively
  Table 3 caption:
    table_text: TABLE III Decoy to White-Box Attack. AUC Scores Accuracy (%) of the
      Backbone Network and Our Deceptive Model With Different White-Box Attack Settings.
      All Models are Trained on FaceForensics++ Raw Quality
  Table 4 caption:
    table_text: TABLE IV Decoy to Black-Box Attack. AUC Scores Accuracy (%) of the
      Backbone Networks and the Deceptive Models With Different Black-Box Attack Settings.
      FaceForensics++ Raw Quality Was Used to Train All Models. PGD Attack With Natural
      Evolutionary Strategies for Gradient Estimation is Referred to as NES-PGD
  Table 5 caption:
    table_text: TABLE V Detecting White-Box and Black-Box Attacks. AUC Scores and
      Accuracy (Acc.) for Detecting White-Box and Black-Box Attack Obtained by the
      Deceptive Models After Training on FaceForensics++ Raw Quality. W and B Denote
      Results Obtained by White-Box and Black-Box Attacks, Respectively
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3253390
- Affiliation of the first author: school of computer science and engineering, nanyang
    technological university, singapore
  Affiliation of the last author: school of computer science and engineering, nanyang
    technological university, singapore
  Figure 1 Link: articels_figures_by_rev_year\2023\Clustered_TaskAware_MetaLearning_by_Learning_From_Learning_Paths\figure_1.jpg
  Figure 1 caption: Overview of CTML. For each incoming task mathcal Ti , part (a)
    extracts path representation and performs clustering in the path representation
    space; part (b) extracts feature representation and performs clustering in the
    feature representation space; part (c) generates a task-aware modulation to be
    applied on the global initialization; part (d) reconstructs path cluster assignment
    from feature cluster assignment.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Clustered_TaskAware_MetaLearning_by_Learning_From_Learning_Paths\figure_2.jpg
  Figure 2 caption: Illustration of task learning under the CTML framework. Tasks
    with similar rehearsed learning paths will produce similar modulations on the
    global initialization theta 0 , resulting in more informed task-adaptive initializations
    that facilitate the actual task learning.
  Figure 3 Link: articels_figures_by_rev_year\2023\Clustered_TaskAware_MetaLearning_by_Learning_From_Learning_Paths\figure_3.jpg
  Figure 3 caption: "Visualization of learning paths. (a) 6 meta-test tasks (5-way\
    \ 1-shot) randomly sampled from the Aircraft, Flower, and Traffic Sign sub-datasets.\
    \ (b) PCA visualization (on the first two principal components) of the 5-step\
    \ rehearsed learning paths (upper subplot) and the corresponding path embeddings\
    \ (lower subplot) for the 6 tasks. (c) Visualization of z gate (how much to retain\
    \ for the current step input) and r gate (how much to retain for the previous\
    \ step memory) of GRU at each rehearsed step t for the 6 tasks. (d)-(f) PCA visualization\
    \ of the modulated initializations and the 5-step actual learning paths for the\
    \ 6 tasks generated by CTML-feat, CTML and CTML(approx.) respectively. The initial\
    \ point of the path in each mini-plot is indicated by the \u201Dblack star\u201D\
    . (g)-(i) Visualization of task modulation on 4 randomly selected filters (size\
    \ of 3 times 3 ) from the 4 convolutional blocks and the final read-out layer\
    \ (size of 800 times 5 ) for the first task (i.e., blue circle task) generated\
    \ by CTML-feat, CTML, and CTML(approx.) respectively."
  Figure 4 Link: articels_figures_by_rev_year\2023\Clustered_TaskAware_MetaLearning_by_Learning_From_Learning_Paths\figure_4.jpg
  Figure 4 caption: (a) Effect of varying kfeat and kpath for MovieLens-1 M. (b)-(c)
    Shortcut approximation for 2 users randomly sampled from MovieLens-1 M ( kfeat=8
    and kpath=16 ).
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.73
  Name of the first author: Danni Peng
  Name of the last author: Sinno Jialin Pan
  Number of Figures: 4
  Number of Tables: 6
  Number of authors: 2
  Paper title: Clustered Task-Aware Meta-Learning by Learning From Learning Paths
  Publication Date: 2023-03-06 00:00:00
  Table 1 caption:
    table_text: "TABLE I Few-Shot Classification Performance of MAML-Based Methods\
      \ on Mixture-of-Datasets With Conv-4 as the Backbone. We Sampled 1000 Tasks\
      \ for Meta-Testing. The Results are Reported in the Form of Mean Accuracy (%)\
      \ \xB1 Std Over 8 Trials. We Also Report the Inference Time (In Milliseconds)\
      \ Per Task for the 5-Way 1-Shot Scenario During Meta-Testing (The Inference\
      \ Time for the 5-Shot Scenario is Around the Same as the 1-Shot Scenario, Hence\
      \ Omitted From the Table)"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE II Few-Shot Classification Performance (Mean Accuracy (%) \xB1\
      \ Std Over 8 Trials) Including the non-MAML-Based Baselines for Two Backbones:\
      \ Conv-4 and ResNet-12"
  Table 3 caption:
    table_text: "TABLE III Few-Shot Classification Performance (Mean Accuracy (%)\
      \ \xB1 Std Over 8 Trials) of Variants of CTML"
  Table 4 caption:
    table_text: TABLE IV Few-Shot Classification Performance of MAML-Based Methods
      on miniImageNet, tieredImageNet, and CIFAR-FS. The Results are Reported in the
      Form of 95% Confidence Interval of Accuracy (%) Based on 600 Meta-Testing Tasks.
      All Results are Adopted Directly from [27], Except for Those Denoted With ,
      Which are Obtained From Our Reproduction
  Table 5 caption:
    table_text: TABLE V Performance on Meta-Dataset in the Form of 95% Confidence
      Interval of Accuracy (%) for fo-MAML and fo-Proto-MAML Before and After Applying
      CTML. Models are Trained on the First 8 Sub-Datasets and Tested on the Meta-Test
      Splits of All the 10 Sub-Datasets. Comparisons With the State-of-The-Art Methods
      Can Be Found in Appendix E.3, available in the online supplemental material
  Table 6 caption:
    table_text: TABLE VI Recommendation Performance on 3 Datasets in MAE and NDCG20.
      We Report the Mean Over 5 Trials, Where the Std Devs are All Less Than 0.001.
      Appendix E.4, available in the online supplemental material, Includes the Full
      Disentangled Results for Warm & Cold Users
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3250323
- Affiliation of the first author: department of computing, imperial college london,
    london, u.k.
  Affiliation of the last author: department of computing, imperial college london,
    london, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2023\FreeHeadGAN_Neural_Talking_Head_Synthesis_With_Explicit_Gaze_Control\figure_1.jpg
  Figure 1 caption: Given a source and a target image, first we extract 3D facial
    key-points from both images, as we condition synthesis on sketches of 3D landmarks.
    We preserve the source identity, by adapting the target key-points to the facial
    shape of the source, through the computation of canonical key-points. Then, we
    predict an optical flow field to warp the source image, and finally a rendering
    network hallucinates the final image, based on the warped image. We explicitly
    control the gaze of generated faces, by colour-coding the gaze direction in the
    eye cavities of sketches.
  Figure 10 Link: articels_figures_by_rev_year\2023\FreeHeadGAN_Neural_Talking_Head_Synthesis_With_Explicit_Gaze_Control\figure_10.jpg
  Figure 10 caption: Synthetic examples when using 1, 2, 4 or 8 source images in the
    N-shot scenario. The results improve in terms of image quality as we get access
    to more source inputs.
  Figure 2 Link: articels_figures_by_rev_year\2023\FreeHeadGAN_Neural_Talking_Head_Synthesis_With_Explicit_Gaze_Control\figure_2.jpg
  Figure 2 caption: Computation of canonical key-points during training. Using a pair
    of frames that depict the same person, we train Ecan to disentangle identity from
    expression and pose. We project the regressed 3D key-points into a canonical space
    and then try to reconstruct them, after swapping the canonical points of the source
    and target images.
  Figure 3 Link: articels_figures_by_rev_year\2023\FreeHeadGAN_Neural_Talking_Head_Synthesis_With_Explicit_Gaze_Control\figure_3.jpg
  Figure 3 caption: Outline of our gaze estimation method. Having defined an eyeball
    template mathbf Teye , we generate pseudo-ground truth meshes for the available
    gaze estimation databases and employ them to train our 3D eye mesh regression
    network Egaze , which given input images of the eyes produces a 3D mesh adhering
    to mathbf Teye .
  Figure 4 Link: articels_figures_by_rev_year\2023\FreeHeadGAN_Neural_Talking_Head_Synthesis_With_Explicit_Gaze_Control\figure_4.jpg
  Figure 4 caption: 'Overview of the image generation component. The generator is
    made up from two modules: a flow network and a rendering network. The flow network
    computes the optical flow for warping the reference image and features, according
    to the target sketch. The rendering network uses this visual information, in order
    to translate the target sketch into a photo-realistic image of the source. The
    two networks are trained jointly in an adversarial manner, with the assistance
    of image discriminators.'
  Figure 5 Link: articels_figures_by_rev_year\2023\FreeHeadGAN_Neural_Talking_Head_Synthesis_With_Explicit_Gaze_Control\figure_5.jpg
  Figure 5 caption: Illustration of optical flows and weights predicted by the flow
    network, in case four source images are provided instead of one (4-shot). The
    warped image is calculated from the source images, flows and weighs with the application
    of (11).
  Figure 6 Link: articels_figures_by_rev_year\2023\FreeHeadGAN_Neural_Talking_Head_Synthesis_With_Explicit_Gaze_Control\figure_6.jpg
  Figure 6 caption: End-to-end pipeline of Free-HeadGAN during inference, for the
    task of reenactment. We adapt the target key-points to the facial shape (identity)
    of the source.
  Figure 7 Link: articels_figures_by_rev_year\2023\FreeHeadGAN_Neural_Talking_Head_Synthesis_With_Explicit_Gaze_Control\figure_7.jpg
  Figure 7 caption: We evaluate the generative performance of Free-HeadGAN on the
    tasks of (a) same-identity reconstruction (self-reenactment) and (b) cross-identity
    reenactment.
  Figure 8 Link: articels_figures_by_rev_year\2023\FreeHeadGAN_Neural_Talking_Head_Synthesis_With_Explicit_Gaze_Control\figure_8.jpg
  Figure 8 caption: Visual comparison of our method with baselines on the task of
    self-reenactment.
  Figure 9 Link: articels_figures_by_rev_year\2023\FreeHeadGAN_Neural_Talking_Head_Synthesis_With_Explicit_Gaze_Control\figure_9.jpg
  Figure 9 caption: Qualitative evaluation of our method against baselines on the
    task of reenactment.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Michail Christos Doukas
  Name of the last author: Stefanos Zafeiriou
  Number of Figures: 12
  Number of Tables: 5
  Number of authors: 4
  Paper title: 'Free-HeadGAN: Neural Talking Head Synthesis With Explicit Gaze Control'
  Publication Date: 2023-03-07 00:00:00
  Table 1 caption:
    table_text: TABLE I Checklist of Key Features and Design Choices of Recent State-of-The-Art
      Systems That Support Full Head Animation and Reenactment
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Numerical Comparison With State-of-the-Art Methods on the
      Tasks of Self-Reenactment (Same-Identity Reconstruction) and Reenactment (Cross-Identity
      Motion Transfer) for VoxCeleb [54] Test Set
  Table 3 caption:
    table_text: TABLE III Evaluation of Free-HeadGAN N-Shot Extension, in Case More
      Than One Source Images are Available
  Table 4 caption:
    table_text: TABLE IV Ablation Study Results on the Significance of Free-HeadGAN
      Components on the Task of Reenactment
  Table 5 caption:
    table_text: TABLE V Comparison Between the 3D Mesh and 3D Vector Regression Approaches
      to Gaze Estimation, on Within-Dataset, Cross-Subject Experiments
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3253243
