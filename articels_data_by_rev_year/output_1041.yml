- Affiliation of the first author: department of computer science and engineering,
    university of notre dame, notre dame, in
  Affiliation of the last author: department of computer science and engineering,
    university of notre dame, notre dame, in
  Figure 1 Link: articels_figures_by_rev_year\2018\Learning_Hyperedge_Replacement_Grammars_for_Graph_Generation\figure_1.jpg
  Figure 1 caption: "A graph and one possible minimal-width clique tree for it. Ghosted\
    \ edges are not part of E \u03B7 ; they are shown only for explanatory purposes."
  Figure 10 Link: articels_figures_by_rev_year\2018\Learning_Hyperedge_Replacement_Grammars_for_Graph_Generation\figure_10.jpg
  Figure 10 caption: Eigenvector Centrality. Nodes are ordered by their eigenvector-values
    along the x -axis. Cosine distance between the original graph and HRG, Chung-Lu
    and Kronecker models are shown at the top of each plot where lower is better.
    In terms of cosine distance, the eigenvector of HRG is consistently closest to
    that of the original graph.
  Figure 2 Link: articels_figures_by_rev_year\2018\Learning_Hyperedge_Replacement_Grammars_for_Graph_Generation\figure_2.jpg
  Figure 2 caption: Binarization of a bag in a clique tree.
  Figure 3 Link: articels_figures_by_rev_year\2018\Learning_Hyperedge_Replacement_Grammars_for_Graph_Generation\figure_3.jpg
  Figure 3 caption: Pruning a clique tree to remove leaf nodes without internal vertices.
    Ghosted clique tree nodes show nodes that are pruned.
  Figure 4 Link: articels_figures_by_rev_year\2018\Learning_Hyperedge_Replacement_Grammars_for_Graph_Generation\figure_4.jpg
  Figure 4 caption: Example of hyperedge replacement grammar rule creation from an
    interior vertex of the clique tree. Note that lowercase letters inside vertices
    are for explanatory purposes only; only the numeric labels outside external vertices
    are actually part of the rule.
  Figure 5 Link: articels_figures_by_rev_year\2018\Learning_Hyperedge_Replacement_Grammars_for_Graph_Generation\figure_5.jpg
  Figure 5 caption: Example of hyperedge replacement grammar rule creation from the
    root node of the clique tree.
  Figure 6 Link: articels_figures_by_rev_year\2018\Learning_Hyperedge_Replacement_Grammars_for_Graph_Generation\figure_6.jpg
  Figure 6 caption: Example of hyperedge replacement grammar rule creation from a
    leaf vertex of the clique tree.
  Figure 7 Link: articels_figures_by_rev_year\2018\Learning_Hyperedge_Replacement_Grammars_for_Graph_Generation\figure_7.jpg
  Figure 7 caption: Complete set of production rules extracted from the example clique
    tree. Note that lowercase letters inside vertices are for explanatory purposes
    only; only the numeric labels outside external vertices are part of the rule.
  Figure 8 Link: articels_figures_by_rev_year\2018\Learning_Hyperedge_Replacement_Grammars_for_Graph_Generation\figure_8.jpg
  Figure 8 caption: When an HRG rule has two nonterminal symbols, one is overwhelmingly
    likely to be much larger than the other. This plot shows, for various grammar
    rules (one LHS per row, one RHS per colored line), the probability (log scale)
    of apportioning 1024 nodes between two nonterminal symbols. This plot is best
    viewed in color.
  Figure 9 Link: articels_figures_by_rev_year\2018\Learning_Hyperedge_Replacement_Grammars_for_Graph_Generation\figure_9.jpg
  Figure 9 caption: Degree Distribution. Dataset graphs exhibit a power law degree
    distribution that is well captured by existing graph generators as well as HRG.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Salvador Aguinaga
  Name of the last author: Tim Weninger
  Number of Figures: 19
  Number of Tables: 2
  Number of authors: 3
  Paper title: Learning Hyperedge Replacement Grammars for Graph Generation
  Publication Date: 2018-03-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Experimental Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Graphlet Statistics and Graphlet Correlation Distance (GCD)
      for Six Real-World Graphs
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2810877
- Affiliation of the first author: department of information engineering and computer
    science (disi), university of trento, trento, italy
  Affiliation of the last author: department of information engineering and computer
    science (disi), university of trento, trento, italy
  Figure 1 Link: articels_figures_by_rev_year\2018\Recurrent_Convolutional_Shape_Regression\figure_1.jpg
  Figure 1 caption: The Cascaded Regression Methods (CRMs) train the regressors independently.
    We convert CRM into an end-to-end recurrent process, and the shape regressors
    are learned jointly. The regressor at each step is generated dynamically based
    on the new input features and the memory of the recurrent regressor.
  Figure 10 Link: articels_figures_by_rev_year\2018\Recurrent_Convolutional_Shape_Regression\figure_10.jpg
  Figure 10 caption: The first-layer filters trained on the image batches. The filters
    are organized in the descending order of their respective variances. The size
    of each filter is 9 times 9 .
  Figure 2 Link: articels_figures_by_rev_year\2018\Recurrent_Convolutional_Shape_Regression\figure_2.jpg
  Figure 2 caption: 'The overview of the proposed approach. Top: the RNN with gated
    recurrent units unrolled in time. Bottom: the CNN architecture used for feature
    extraction. Note that feature extraction is performed at every recurrent step.'
  Figure 3 Link: articels_figures_by_rev_year\2018\Recurrent_Convolutional_Shape_Regression\figure_3.jpg
  Figure 3 caption: 'The architectures of different regressors: (a) convolutional
    recurrent GRU Regressor (b) traditional recurrent GRU regressor (c) the traditional
    linear regressor. The linear regressor is a degenerate version of the traditional
    GRU regressor.'
  Figure 4 Link: articels_figures_by_rev_year\2018\Recurrent_Convolutional_Shape_Regression\figure_4.jpg
  Figure 4 caption: Landmark localization for 5 recurrent steps. On the left, we show
    some examples, for which 5 recurrences are sufficient, while the examples on the
    right require additional recurrences.
  Figure 5 Link: articels_figures_by_rev_year\2018\Recurrent_Convolutional_Shape_Regression\figure_5.jpg
  Figure 5 caption: 'Comparison between different combinations of the depth of the
    augmented CNN layers and fully connected layers: running time versus error for
    the 300W dataset.'
  Figure 6 Link: articels_figures_by_rev_year\2018\Recurrent_Convolutional_Shape_Regression\figure_6.jpg
  Figure 6 caption: Average error & normalized shape increment vs the number of recurrent
    steps.
  Figure 7 Link: articels_figures_by_rev_year\2018\Recurrent_Convolutional_Shape_Regression\figure_7.jpg
  Figure 7 caption: Performance of RCSR trained with different recurrent steps (i.e.,
    5, 10, 20, 30) on the full test set of the 300-W dataset.
  Figure 8 Link: articels_figures_by_rev_year\2018\Recurrent_Convolutional_Shape_Regression\figure_8.jpg
  Figure 8 caption: Selected qualitative examples taken from the full set of the 300-W
    dataset.
  Figure 9 Link: articels_figures_by_rev_year\2018\Recurrent_Convolutional_Shape_Regression\figure_9.jpg
  Figure 9 caption: Different convolutional modules for feature extraction which are
    (1) the super resolution convolutional neural network (SRCNN); (2) the LeNet (3)
    the facial action unit classification (FAUC) neural network.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: WEI WANG
  Name of the last author: Nicu Sebe
  Number of Figures: 11
  Number of Tables: 4
  Number of authors: 3
  Paper title: Recurrent Convolutional Shape Regression
  Publication Date: 2018-03-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Landmark Localization Results on the 300-W Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Landmark Localization Results on the HELEN Dataset
  Table 3 caption:
    table_text: TABLE 3 Landmark Localization Results on the LFPW Dataset.
  Table 4 caption:
    table_text: TABLE 4 Evaluation of the Effect of Various Recurrent-Convolutional
      Architectures (Best Results in Bold)
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2810881
- Affiliation of the first author: king abdullah university of science & technology,
    thuwal, saudi arabia
  Affiliation of the last author: king abdullah university of science & technology,
    thuwal, saudi arabia
  Figure 1 Link: articels_figures_by_rev_year\2018\SurfCut_Surfaces_of_Minimal_Paths_from_Topological_Structures\figure_1.jpg
  Figure 1 caption: 'Overview of SurfCut. [Top, left]: From a seed point on the surface,
    a front is propagated, [Top, right]: Generalized local extrema, i.e., ridges,
    of a function of FM distance are extracted, [Bottom, left]: Locations with minimum
    Euclidean distance between curves are extracted forming the boundary, [Bottom,
    right]: The surface is extracted as generalized local minima, i.e., valley, of
    the FM distance.'
  Figure 10 Link: articels_figures_by_rev_year\2018\SurfCut_Surfaces_of_Minimal_Paths_from_Topological_Structures\figure_10.jpg
  Figure 10 caption: Illustration of valley extraction by Algorithm 3, which retracts
    the volume while preserving 1-faces on the surface boundary (red). This gives
    the surface of interest.
  Figure 2 Link: articels_figures_by_rev_year\2018\SurfCut_Surfaces_of_Minimal_Paths_from_Topological_Structures\figure_2.jpg
  Figure 2 caption: Illustration of some ascending and descending manifolds of a one-dimensional
    function.
  Figure 3 Link: articels_figures_by_rev_year\2018\SurfCut_Surfaces_of_Minimal_Paths_from_Topological_Structures\figure_3.jpg
  Figure 3 caption: '[Left two images]: Illustration of faces that form a cubical
    complex (left) and faces that do not form a cubical complex (0,1,2-faces are marked
    in red, green and orange). The missing 1-face and 0-faces circled in blue on the
    right are not in the complex, but they are sub-faces of other faces in the set.
    [Right two images]: Example of 1-face, 0-face free pairs, and 2-face, 1-face free
    pairs (circled in blue).'
  Figure 4 Link: articels_figures_by_rev_year\2018\SurfCut_Surfaces_of_Minimal_Paths_from_Topological_Structures\figure_4.jpg
  Figure 4 caption: "[Top left]: The evolving Fast Marching (FM) front at two different\
    \ time instances in orange and white. The function 1\u03D5 is the likelihood of\
    \ surface, and is visualized (red - high values, and blue - low values). Ridge\
    \ points of U E , the Euclidean length of minimal paths, lie on the surface of\
    \ interest. [Top right]: This is more easily seen in 2D where the local maxima\
    \ of the Euclidean path length (red balls) of minimal paths (dashed) are seen\
    \ to lie on the curve of interest. The green contour is a snapshot of the front.\
    \ [Bottom]: Schematic in 3D with front (blue), surface (green), and minimal paths\
    \ (orange). Orthogonal to the surface where the surface intersects the front,\
    \ the Euclidean path length decreases. Along the surface, the path lengths may\
    \ increase or decrease."
  Figure 5 Link: articels_figures_by_rev_year\2018\SurfCut_Surfaces_of_Minimal_Paths_from_Topological_Structures\figure_5.jpg
  Figure 5 caption: Schematic of quantities in the proof of Proposition 1.
  Figure 6 Link: articels_figures_by_rev_year\2018\SurfCut_Surfaces_of_Minimal_Paths_from_Topological_Structures\figure_6.jpg
  Figure 6 caption: '[1st image]: Front color coded with Euclidean path length U E
    (top view). Red indicates high values. The bottom view (not shown) is a symmetric
    flip. Topologically, U E forms a volcano structure (ridge, i.e., top of volcano,
    is darkest red), and inside the volcano is blue. [Subsequent images]: Illustration
    of iterations (from left to right) of Algorithm 1 on noise-less data to obtain
    the ridge curve (white) on the Fast Marching front (green) by computing the Morse
    complex of U E . The ridge curve lies on the surface of interest (red).'
  Figure 7 Link: articels_figures_by_rev_year\2018\SurfCut_Surfaces_of_Minimal_Paths_from_Topological_Structures\figure_7.jpg
  Figure 7 caption: Illustration of Algorithm 2 operating on noisy data to obtain
    the highest ridge.
  Figure 8 Link: articels_figures_by_rev_year\2018\SurfCut_Surfaces_of_Minimal_Paths_from_Topological_Structures\figure_8.jpg
  Figure 8 caption: "[Left]: Ridge curve (white) extraction by retracting the Fast\
    \ Marching front at two instants. [Right]: An example cut (red) of ridge curves,\
    \ forming the surface boundary. Notice that the cut matches with the end of high\
    \ 1\u03D5 (bright areas)."
  Figure 9 Link: articels_figures_by_rev_year\2018\SurfCut_Surfaces_of_Minimal_Paths_from_Topological_Structures\figure_9.jpg
  Figure 9 caption: Quantities defined in the proof of Proposition 3.
  First author gender probability: 0.82
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Marei Algarni
  Name of the last author: Ganesh Sundaramoorthi
  Number of Figures: 21
  Number of Tables: 2
  Number of authors: 2
  Paper title: 'SurfCut: Surfaces of Minimal Paths from Topological Structures'
  Publication Date: 2018-03-05 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Methods for Surface Extraction Given the Surface
      Boundary on the Synthetic Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Evaluation on Lung Dataset
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2811810
- Affiliation of the first author: "computer vision laboratory, \xE9cole polytechnique\
    \ f\xE9d\xE9rale de lausanne, lausanne, switzerland"
  Affiliation of the last author: "computer vision laboratory, \xE9cole polytechnique\
    \ f\xE9d\xE9rale de lausanne, lausanne, switzerland"
  Figure 1 Link: articels_figures_by_rev_year\2018\Beyond_Sharing_Weights_for_Deep_Domain_Adaptation\figure_1.jpg
  Figure 1 caption: Our two-stream architecture. One stream operates on the source
    data and the other on the target one. Their weights are not shared. Instead, we
    introduce loss functions that prevent corresponding weights from being too different
    from each other.
  Figure 10 Link: articels_figures_by_rev_year\2018\Beyond_Sharing_Weights_for_Deep_Domain_Adaptation\figure_10.jpg
  Figure 10 caption: Samples from Source and Target datasets with synthetic and real
    images respectively.
  Figure 2 Link: articels_figures_by_rev_year\2018\Beyond_Sharing_Weights_for_Deep_Domain_Adaptation\figure_2.jpg
  Figure 2 caption: "Correlation between the parameters \u03B8 s 1 and \u03B8 t 1\
    \ of the first convolutional layer of our two-stream architecture, regularized\
    \ by either the L 2 (a) or exponential (b) norm defined in Eq. (7) and Eq. (8),\
    \ respectively. The red dots denote the correlation between the corresponding\
    \ layer parameters. The green line shows the initial estimate for the linear transformation,\
    \ and the blue line illustrates its final estimate after the training process.\
    \ (Best viewed in color.)."
  Figure 3 Link: articels_figures_by_rev_year\2018\Beyond_Sharing_Weights_for_Deep_Domain_Adaptation\figure_3.jpg
  Figure 3 caption: Our UAV dataset. [Top] Synthetic and real training examples. [Bottom]
    Real samples from the test dataset.
  Figure 4 Link: articels_figures_by_rev_year\2018\Beyond_Sharing_Weights_for_Deep_Domain_Adaptation\figure_4.jpg
  Figure 4 caption: Evaluation of the best network architecture. The y -axis corresponds
    to the MMD 2 loss between the outputs of the corresponding streams that operate
    on real and synthetic data, respectively. The x -axis denotes the network configuration,
    where a ' + ' sign indicates that the corresponding network layers are regularized
    with a loss function and a '-' sign that the weights are shared for the corresponding
    layers. (Best seen in color).
  Figure 5 Link: articels_figures_by_rev_year\2018\Beyond_Sharing_Weights_for_Deep_Domain_Adaptation\figure_5.jpg
  Figure 5 caption: Several iterations of Adam [56] to minimize the objective function,
    defined in Eq. 1. This experiment was carried out on the UAV-200 benchmark with
    a network architecture whose first 4 layers are not shared and initialized from
    the pre-trained optimal configuration, selected by the MMD criterion, whose 3
    first layers only are not shared. [Left] Behavior of the different terms of the
    cost function, defined in Eq. 1. [Right] Contribution of different layers to the
    Lw term of the cost function. (Best seen in color).
  Figure 6 Link: articels_figures_by_rev_year\2018\Beyond_Sharing_Weights_for_Deep_Domain_Adaptation\figure_6.jpg
  Figure 6 caption: Average precision of our approach, tested on the validation dataset
    for different values of lambda w [Top], lambda u [Middle] and sigma [Bottom].
    We report the mean and standard deviation of the AP score across 5 runs of our
    method. In all three cases, the performance remains stable across a wide interval.
  Figure 7 Link: articels_figures_by_rev_year\2018\Beyond_Sharing_Weights_for_Deep_Domain_Adaptation\figure_7.jpg
  Figure 7 caption: Influence of the ratio of synthetic to real data. [Top] AP of
    our approach (violet stars), DDC (blue triangles), and training using real data
    only (red circles) as a function of the number of real samples used given a constant
    number of synthetic ones. [Bottom] AP of our approach (violet stars) and DDC (blue
    triangles) as a function of the number of synthetic examples used given a small
    and constant number of real ones. (Best seen in color).
  Figure 8 Link: articels_figures_by_rev_year\2018\Beyond_Sharing_Weights_for_Deep_Domain_Adaptation\figure_8.jpg
  Figure 8 caption: Some examples from three domains in the Office dataset.
  Figure 9 Link: articels_figures_by_rev_year\2018\Beyond_Sharing_Weights_for_Deep_Domain_Adaptation\figure_9.jpg
  Figure 9 caption: Office dataset. [Top] The network architecture that proved to
    be the best according to our MMD-based criterion. [Bottom] The y -axis corresponds
    to the mathop textMMD2 loss between the outputs of the corresponding streams that
    operate on Amazon and Webcam, respectively. Different configurations are depicted
    by different colors.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Artem Rozantsev
  Name of the last author: Pascal Fua
  Number of Figures: 11
  Number of Tables: 9
  Number of authors: 3
  Paper title: Beyond Sharing Weights for Deep Domain Adaptation
  Publication Date: 2018-03-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Statistics of Our Two UAV Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison to Other Domain Adaptation Techniques on the UAV-200
      (small) Dataset
  Table 3 caption:
    table_text: TABLE 3 Comparison of Our Method Against Several Baselines on the
      UAV-200 (full) Dataset
  Table 4 caption:
    table_text: TABLE 4 Comparison of Different Extreme Cases of Sharing and Not-Sharing
      the Weights on the Test Images of the UAV-200 Dataset
  Table 5 caption:
    table_text: TABLE 5 Comparison Against Other Domain Adaptation Techniques on the
      Office Benchmark
  Table 6 caption:
    table_text: TABLE 6 Comparison to Other Domain Adaptation Techniques on the Office
      Standard Benchmark
  Table 7 caption:
    table_text: TABLE 7 Comparison Against Other Domain Adaptation Techniques on the
      MNIST+USPS Standard Benchmark
  Table 8 caption:
    table_text: "TABLE 8 Evaluation on the SVHN \u2192 MNIST Domain Adaptation Task"
  Table 9 caption:
    table_text: TABLE 9 Regression Results for the Facial Pose Estimation Task
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2814042
- Affiliation of the first author: computer science and artificial intelligence laboratory,
    massachusetts institute of technology, cambridge, ma
  Affiliation of the last author: computer science and artificial intelligence laboratory,
    massachusetts institute of technology, cambridge, ma
  Figure 1 Link: articels_figures_by_rev_year\2018\What_Do_Different_Evaluation_Metrics_Tell_Us_About_Saliency_Models\figure_1.jpg
  Figure 1 caption: Different evaluation metrics score saliency models differently.
    Saliency maps are evaluated on how well they approximate human ground truth eye
    movements (a), represented either as discrete fixation locations or a continuous
    fixation map. For a given image, saliency maps corresponding to 8 saliency models
    (b) are scored under 8 different evaluation metrics (c), 6 similarity and 2 dissimilarity
    metrics. We highlighted the top 3 best scoring maps (in yellow, green, and blue,
    respectively) under each metric (per row) for the particular image in (a).
  Figure 10 Link: articels_figures_by_rev_year\2018\What_Do_Different_Evaluation_Metrics_Tell_Us_About_Saliency_Models\figure_10.jpg
  Figure 10 caption: The SIM and CC metrics measure the similarity between the saliency
    map (b) and ground truth fixation map (a). SIM measures the histogram intersection
    between two maps (Section 4.2.1), while CC measures their cross correlation (Section
    4.2.2). CC treats false positives and negatives symmetrically, but SIM places
    less emphasis on false positives. As a result, both saliency maps have similar
    SIM scores (c), but the saliency map on the right has a lower CC score because
    false positives lower the overall correlation (d).
  Figure 2 Link: articels_figures_by_rev_year\2018\What_Do_Different_Evaluation_Metrics_Tell_Us_About_Saliency_Models\figure_2.jpg
  Figure 2 caption: A series of experiments and corresponding visualizations can help
    us understand what behaviors of saliency models different evaluation metrics capture.
    Given a natural image and ground truth human fixations on the image as in Fig.
    1a, we evaluate saliency models, including the 4 baselines in column (a), at their
    ability to approximate ground truth. Visualizations of 8 common metrics (b-i)
    help elucidate the computations performed when scoring saliency models. In each
    visualization, higher density regions correspond either to matches or mismatches
    between the saliency map (from the first column) and the human ground truth (from
    Fig. 1a). The visualizations for all the metrics are explained in greater detail
    in Figs. 6-11 throughout the rest of the paper.
  Figure 3 Link: articels_figures_by_rev_year\2018\What_Do_Different_Evaluation_Metrics_Tell_Us_About_Saliency_Models\figure_3.jpg
  Figure 3 caption: The AUC metric evaluates a saliency map by how many ground truth
    fixations it captures in successive level sets. To compute AUC, a saliency map
    (left column) is treated as a binary classifier of fixations at various threshold
    values (THRESH) and an ROC curve is swept out (bottom left). Thresholding the
    saliency map produces level sets (columns 3-7). For each level set, the true positive
    (TP) rate is the proportion of fixations landing in the level set (green points).
    The false positive (FP) rate is the proportion of image pixels in the level set
    not covered in fixations (uncovered white regions). Fixations landing outside
    of a level set are false negatives (red points). We include 5 level sets colored
    to correspond to their TP and FP rates on the ROC curve. The AUC score for the
    saliency map is the area under the ROC curve.
  Figure 4 Link: articels_figures_by_rev_year\2018\What_Do_Different_Evaluation_Metrics_Tell_Us_About_Saliency_Models\figure_4.jpg
  Figure 4 caption: 'How true and false positives are calculated under different AUC
    metrics (AUC-Judd, AUC-Borji, sAUC): (a) In all cases, the true positive rate
    is calculated as the proportion of fixations falling into a thresholded saliency
    map (green over green plus red). These fixations are superimposed on 5 level sets
    of the CovSal [24] saliency map. (b) In AUC-Judd, the false positive rate is the
    proportion of non-fixated pixels in the thresholded saliency map (blue over blue
    plus yellow). (c) In AUC-Borji, this calculation is approximated by sampling negatives
    uniformly at random and computing the proportion of negatives in the thresholded
    region (blue over blue plus yellow). (d) In sAUC, negatives are sampled according
    to the distribution of fixations in other images instead of uniformly at random.
    Saliency models are scored similarly under the AUC-Judd and AUC-Borji metrics,
    but differently under sAUC due to the sampling of false positives.'
  Figure 5 Link: articels_figures_by_rev_year\2018\What_Do_Different_Evaluation_Metrics_Tell_Us_About_Saliency_Models\figure_5.jpg
  Figure 5 caption: The effect of true positive (TP) and false positive (FP) rates
    on AUC score. We compare the Judd [38] and Achanta [2] models by visualizing (left
    to right) the original saliency maps, equalized saliency maps (to highlight the
    level sets), ROC curves, and the first two level sets of both maps. Overlaid on
    the level sets are true positives (fixations classified salient) in green and
    false negatives (fixations classified non-salient) in red. Points on the ROC curve
    correspond to TP and FP rates of different level sets (both axes span 0 to 1).
    Judd accounts for more fixations in the first few level sets than Achanta, achieving
    a higher AUC score overall. The AUC score is driven most by the first few level
    sets, while the total number of levels sets and FP in later level sets have a
    significantly smaller impact. Achanta has a smaller range of saliency values,
    and thus fewer points on the ROC curve.
  Figure 6 Link: articels_figures_by_rev_year\2018\What_Do_Different_Evaluation_Metrics_Tell_Us_About_Saliency_Models\figure_6.jpg
  Figure 6 caption: Both AUC and sAUC measure the ability of a saliency map to classify
    fixated from non-fixated locations (Section 4.1.1). The main difference is that
    AUC prefers maps that account for center bias, while sAUC penalizes them. The
    saliency maps in (b) are compared on their ability to predict the ground truth
    fixations in (a). For a particular level set, the true positive rate is the same
    for both maps (c). The sAUC metric normalizes this value by fixations sampled
    from other images, more of which land in the center of the image, thus penalizing
    the rightmost model for its center bias (d). The AUC metric, however, samples
    fixations uniformly at random and prefers the center-biased model which better
    explains the overall viewing behavior (e).
  Figure 7 Link: articels_figures_by_rev_year\2018\What_Do_Different_Evaluation_Metrics_Tell_Us_About_Saliency_Models\figure_7.jpg
  Figure 7 caption: Both AUC and NSS evaluate the ability of a saliency map (b) to
    predict fixation locations (a). AUC is invariant to monotonic transformations
    (Section 4.1.1), while NSS is not. NSS normalizes a saliency map by the standard
    deviation of the saliency values (Section 4.1.2). AUC ignores low-valued false
    positives but NSS penalizes them. As a result, the rightmost map has a lower NSS
    score because more false positives means the normalized saliency value at fixation
    locations drops (c). The AUC score of the left and right maps is very similar
    since a similar number of fixations fall in equally-sized level sets of the two
    saliency maps (d).
  Figure 8 Link: articels_figures_by_rev_year\2018\What_Do_Different_Evaluation_Metrics_Tell_Us_About_Saliency_Models\figure_8.jpg
  Figure 8 caption: 'We compute the information of one model over another at predicting
    ground truth fixations. We visualize the information gain of the Judd model over
    the center prior baseline (top) and the bottom-up IttiKoch model (bottom). In
    blue are the image pixels where the Judd model makes better predictions than each
    model. In red is the remaining distance to the real information gain: i.e., image
    pixels at which the Judd model underestimates saliency.'
  Figure 9 Link: articels_figures_by_rev_year\2018\What_Do_Different_Evaluation_Metrics_Tell_Us_About_Saliency_Models\figure_9.jpg
  Figure 9 caption: The EMD and SIM metrics measure the similarity between the saliency
    map (b) and ground truth fixation map (a). EMD measures how much density needs
    to move before the two maps match (Section 4.2.4), while SIM measures the direct
    intersection between the maps (Section 4.2.1). EMD prefers sparser predictions,
    even if they do not perfectly align with fixated regions, while SIM penalizes
    misalignment. The saliency map on the left makes sparser predictions, resulting
    in a smaller intersection with the ground truth, and lower SIM score (c). However,
    the predicted density in this map is spatially closer to the ground truth density,
    achieving a better EMD score than the map on the right (d).
  First author gender probability: 0.99
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.81
  Name of the first author: Zoya Bylinskii
  Name of the last author: "Fr\xE9do Durand"
  Number of Figures: 13
  Number of Tables: 6
  Number of authors: 5
  Paper title: What Do Different Evaluation Metrics Tell Us About Saliency Models?
  Publication Date: 2018-03-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Most Common Metrics for Saliency Model Evaluation Are
      Analyzed in This Paper
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Different Metrics Use Different Representations of Ground
      Truth for Evaluating Saliency Models
  Table 3 caption:
    table_text: TABLE 3 Performance of Saliency Baselines (as Pictured in Fig. 2)
      with Scores Averaged Over MIT300 Benchmark Images
  Table 4 caption:
    table_text: TABLE 4 Properties of the 8 Evaluation Metrics in This Paper (Under
      Our Implementations)
  Table 5 caption:
    table_text: TABLE 5 Metrics Have Different Sensitivities to False Negatives
  Table 6 caption:
    table_text: TABLE 6 A Brief Overview of the Metric Analyses and Discussions Provided
      in This Paper, Highlighting Some of the Key Properties, Features, and Applications
      of Different Evaluation Metrics
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2815601
- Affiliation of the first author: parietal team, inria, cea, university paris-saclay,
    gif sur yvette, france
  Affiliation of the last author: parietal team, inria, cea, university paris-saclay,
    gif sur yvette, france
  Figure 1 Link: articels_figures_by_rev_year\2018\Recursive_Nearest_Agglomeration_ReNA_Fast_Clustering_for_Approximation_of_Struct\figure_1.jpg
  Figure 1 caption: "Illustration of the approximation of a signal: Piece-wise constant\
    \ approximation of a 1D signal contaminated with additive Gaussian noise, x\u2208\
    \ R 500 . This signal is a sample of our statistical problem. The approximation\
    \ is built by clustering features (here time-points) with a connectivity constraint\
    \ (i.e., using a spatially-constrained Ward clustering). Only 25 clusters can\
    \ preserve the structure of the signal and decrease the noise, as seen from the\
    \ signal-to-noise-ratio ( dB )."
  Figure 10 Link: articels_figures_by_rev_year\2018\Recursive_Nearest_Agglomeration_ReNA_Fast_Clustering_for_Approximation_of_Struct\figure_10.jpg
  Figure 10 caption: 'Spatial ICA reproducibility after dimension reduction: Reproducibility
    of 40 spatial independent components on fMRI for 93 subjects, with a fixed reduced
    dimension (see section 4.5). (Left) the similarity of downsampling, Ward, SLIC
    and ReNA with respect to the non-compressed components is high. (Middle) across
    two sessions, donwsampling yields components more consistent than raw data. Ward,
    SLIC and ReNA perform as well as raw data, while other approaches fail to do so.
    (Right) regarding computational time, ReNA outperforms downsampling, Ward and
    SLIC, and performs as well as single-linkage and random projections. It is 16
    time faster than working on raw data.'
  Figure 2 Link: articels_figures_by_rev_year\2018\Recursive_Nearest_Agglomeration_ReNA_Fast_Clustering_for_Approximation_of_Struct\figure_2.jpg
  Figure 2 caption: "Illustration of feature grouping: In images, feature-grouping\
    \ data approximation corresponds to a super-pixel approach. In a more general\
    \ setting, this approach consists in finding a data-driven reduction \u03A6 FG\
    \ using clustering of features. Then, the data x are reduced to \u03A6 FG x and\
    \ then used for further statistical analysis (e.g., classification)."
  Figure 3 Link: articels_figures_by_rev_year\2018\Recursive_Nearest_Agglomeration_ReNA_Fast_Clustering_for_Approximation_of_Struct\figure_3.jpg
  Figure 3 caption: 'The nearest neighbor grouping: The algorithm receives a data
    matrix X represented on a regular square lattice G . left) The nodes correspond
    to the feature values and the edges are the encoded topological structure. 1)
    Graph representation: We calculate the similarity matrix D . 2) Finding 1-NN:
    We proceed by finding the 1-nearest neighbors subgraph Q according to the similarity
    measure. 3) Getting the clusters and reduction step: We extract the connected
    components of Q and merge the connected nodes.'
  Figure 4 Link: articels_figures_by_rev_year\2018\Recursive_Nearest_Agglomeration_ReNA_Fast_Clustering_for_Approximation_of_Struct\figure_4.jpg
  Figure 4 caption: 'Illustration of the working principle of the Recursive Nearest
    Neighbor, ReNA: The white lines represent the edges of the connectivity graph.
    The algorithm receives the original sequence of images, considering each feature
    (i.e., pixel or voxel in the image) as a cluster. From now on, for each iteration,
    the nearest clusters are merged (i.e., removing edges from the connectivity graph),
    yielding a reduced graph, until the desired number of clusters is found.'
  Figure 5 Link: articels_figures_by_rev_year\2018\Recursive_Nearest_Agglomeration_ReNA_Fast_Clustering_for_Approximation_of_Struct\figure_5.jpg
  Figure 5 caption: 'Clusters obtained for the extended Yale B face dataset using
    various feature grouping schemes: ( k = 120 ). Single, average and complete linkage
    clustering fail to represent the spatial structure of the data, finding a huge
    cluster leaving only small islands apart. Downsampling fails to capture the global
    appearance. In contrast, methods yielding balanced clusters maintain this structure.
    Colors are random.'
  Figure 6 Link: articels_figures_by_rev_year\2018\Recursive_Nearest_Agglomeration_ReNA_Fast_Clustering_for_Approximation_of_Struct\figure_6.jpg
  Figure 6 caption: 'Assessment of various approximation techniques on synthetic and
    brain imaging data: Evaluation of the performance varying the number k of clusters.
    (top) Empirical measure of the distortion of the approximated distance. For a
    fraction of the signal between 5 and 30 percent Ward, SLIC and ReNA present a
    denoising effect, improving the approximation of the distances. In contrast, traditional
    agglomerative clustering fails to preserve distances in the reduced space. Downsampling
    displays an intermediate performance. (center) Regarding computation time, downsampling
    and random sampling outperform all the alternatives, followed by random projections
    and single-linkage. The proposed method is almost as fast as single-linkage. (bottom)
    Percolation behavior, measured via the size of the largest cluster. Ward, SLIC
    and ReNA are the best avoiding huge clusters. The vertical dashed lines indicate
    the useful value range for practical applications ( k in left[lfloor p20 rfloor,
    lfloor p10rfloor right] ).'
  Figure 7 Link: articels_figures_by_rev_year\2018\Recursive_Nearest_Agglomeration_ReNA_Fast_Clustering_for_Approximation_of_Struct\figure_7.jpg
  Figure 7 caption: 'Face prediction accuracy for various approximation schemes: Prediction
    accuracy as function of the feature space dimension obtained for various approximation
    schemes and classifiers for face recognition on the extended Yale B dataset. The
    clustering methods finding balanced clusters need less features to have a fair
    performance, and they obtain significantly higher scores than the percolating
    methods.'
  Figure 8 Link: articels_figures_by_rev_year\2018\Recursive_Nearest_Agglomeration_ReNA_Fast_Clustering_for_Approximation_of_Struct\figure_8.jpg
  Figure 8 caption: 'Computation time taken to reach a solution: Quality of the fit
    of a ell 2 penalized logistic regression as function of the computation time for
    a fixed number of clusters. In both datasets, Ward, SLIC and ReNA obtain significantly
    higher scores than estimation on non-reduced data with less computation time to
    reach a stable solution. Note that the time displayed does include cluster computation.'
  Figure 9 Link: articels_figures_by_rev_year\2018\Recursive_Nearest_Agglomeration_ReNA_Fast_Clustering_for_Approximation_of_Struct\figure_9.jpg
  Figure 9 caption: "Impact of reduction methods on prediction for various datasets:\
    \ (Top) Each bar represents the impact of the corresponding option on the prediction\
    \ accuracy, relatively to the mean prediction with non-reduced data. Downsampling\
    \ has the same performance as raw data. On the other hand, random projections,\
    \ Nystr\xF6m, single, average and complete linkage algorithms are consistently\
    \ the worst ones across datasets. Ward, SLIC and ReNA perform at least as good\
    \ as non-reduced data. (middle) Regarding the computation time to find a reduction,\
    \ single-linkage and ReNA are consistently the best among the clustering algorithms.\
    \ Random projections perform better than Nystr\xF6m when the number of samples\
    \ is large. Downsampling is the fastest across datasets. (Bottom) The time to\
    \ converge for single-linkage and ReNA is almost the same. Average, complete-linkage\
    \ and Ward are consistently the slowest. SLIC performs well on large datasets.\
    \ Nystr\xF6m and random projections are the fastest across datasets. Single-linkage\
    \ and ReNA are the fastest clustering methods. ReNA strikes a good trade off between\
    \ time and prediction accuracy."
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: "Andr\xE9s Hoyos-Idrobo"
  Name of the last author: Bertrand Thirion
  Number of Figures: 10
  Number of Tables: 1
  Number of authors: 4
  Paper title: 'Recursive Nearest Agglomeration (ReNA): Fast Clustering for Approximation
    of Structured Signals'
  Publication Date: 2018-03-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of the Datasets and the Tasks Performed with Them
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2815524
- Affiliation of the first author: ccce, nankai university, nankai, qu, china
  Affiliation of the last author: university of oxford, oxford, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2018\Deeply_Supervised_Salient_Object_Detection_with_Short_Connections\figure_1.jpg
  Figure 1 caption: Visual comparison of saliency maps produced by the HED-based method
    [26] and ours. Though saliency maps produced by deeper (4-6) side output (s-out)
    look similar, because of the introduced short connections, each shallower (1-3)
    side output can generate satisfactory saliency maps and hence a better output
    result.
  Figure 10 Link: articels_figures_by_rev_year\2018\Deeply_Supervised_Salient_Object_Detection_with_Short_Connections\figure_10.jpg
  Figure 10 caption: "Quantitative comparisons with 11 methods on 5 popular datasets.\
    \ The ResNet-101 [59] version of our approach (i.e. 'Ours\u2020') clearly outperforms\
    \ its VGGNet version. For fair comparison, we exclude 'Ours\u2020' and highlight\
    \ the best result of each column in bold. Here we use the initials of each dataset\
    \ for convenience."
  Figure 2 Link: articels_figures_by_rev_year\2018\Deeply_Supervised_Salient_Object_Detection_with_Short_Connections\figure_2.jpg
  Figure 2 caption: Illustration of different architectures. (a) Hypercolumn [37],
    (b) FCN-8s [29] (c) HED [26], (d) and (e) different patterns of our proposed architecture.
    As can be seen, a series of short connections are introduced in our architecture
    for combining the advantages of both deeper layers and shallower layers. More
    interestingly, the last one can be viewed as a generalized version of all the
    formers.
  Figure 3 Link: articels_figures_by_rev_year\2018\Deeply_Supervised_Salient_Object_Detection_with_Short_Connections\figure_3.jpg
  Figure 3 caption: The proposed network architecture. Our architecture is based on
    VGGNet [28] for better comparison with previous CNN-based methods. As there are
    totally 6 different scales in VGGNet, 6 side outputs are introduced, each of which
    is represented by different colors. Besides the side loss for each side output,
    a fusion loss is employed for capturing features of different levels.
  Figure 4 Link: articels_figures_by_rev_year\2018\Deeply_Supervised_Salient_Object_Detection_with_Short_Connections\figure_4.jpg
  Figure 4 caption: "Details of each side output. (n,k\xD7k) means that the number\
    \ of channels and the kernel size are n and k , respectively. \u201CLayer\u201D\
    \ means which layer the corresponding side output is connected to. \u201C1\u201D\
    \ \u201C2\u201D and \u201C3\u201D represent three convolutional layers that are\
    \ used in each side output. Note that the first two convolutional layers in each\
    \ side output are followed by a ReLU layer for nonlinear transformation."
  Figure 5 Link: articels_figures_by_rev_year\2018\Deeply_Supervised_Salient_Object_Detection_with_Short_Connections\figure_5.jpg
  Figure 5 caption: Illustration of short connections in Fig. 3.
  Figure 6 Link: articels_figures_by_rev_year\2018\Deeply_Supervised_Salient_Object_Detection_with_Short_Connections\figure_6.jpg
  Figure 6 caption: "Comparisons of different side output settings and their performance\
    \ on PASCALS dataset [55]. (c,k\xD7k)\xD7n means that there are n convolutional\
    \ layers with c channels and size k\xD7k . Note that the last convolutional layer\
    \ in each side output is unchanged as listed in Fig. 4. In each setting, we only\
    \ modify one parameter while keeping all others unchanged so as to emphasize the\
    \ importance of each chosen parameter."
  Figure 7 Link: articels_figures_by_rev_year\2018\Deeply_Supervised_Salient_Object_Detection_with_Short_Connections\figure_7.jpg
  Figure 7 caption: The performance of different architectures on PASCALS dataset
    [55]. '' represents the pattern used in this paper.
  Figure 8 Link: articels_figures_by_rev_year\2018\Deeply_Supervised_Salient_Object_Detection_with_Short_Connections\figure_8.jpg
  Figure 8 caption: Selected results from various datasets. We split the selected
    images into multiple groups, which are separated by solid lines. To better show
    the capability of processing different scenes for each approach, we highlight
    the features of images in each group.
  Figure 9 Link: articels_figures_by_rev_year\2018\Deeply_Supervised_Salient_Object_Detection_with_Short_Connections\figure_9.jpg
  Figure 9 caption: Precision (vertical axis) recall (horizontal axis) curves on three
    popular salient object datasets.
  First author gender probability: 0.88
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Qibin Hou
  Name of the last author: Philip H. S. Torr
  Number of Figures: 14
  Number of Tables: 0
  Number of authors: 6
  Paper title: Deeply Supervised Salient Object Detection with Short Connections
  Publication Date: 2018-03-14 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2815688
- Affiliation of the first author: college of computer science and the state key lab
    of cad&cg, zhejiang university, hangzhou, zhejiang, china
  Affiliation of the last author: computer and information science department and
    grasp laboratory, university of pennsylvania, philadelphia, pa
  Figure 1 Link: articels_figures_by_rev_year\2018\MonoCap_Monocular_Human_Motion_Capture_using_a_CNN_Coupled_with_a_Geometric_Prio\figure_1.jpg
  Figure 1 caption: Overview of proposed approach. (top-left) Input image sequence,
    (top-right) CNN-based heat map outputs representing the soft localization of 2D
    joints, (bottom-left) 3D pose dictionary, and (bottom-right) the recovered 3D
    pose sequence reconstruction. To fully account for uncertainty, the problem is
    addressed in a probabilistic framework where the 2D joint locations are modeled
    as latent variables and marginalized in an EM algorithm. Temporal smoothness in
    3D is also imposed.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\MonoCap_Monocular_Human_Motion_Capture_using_a_CNN_Coupled_with_a_Geometric_Prio\figure_2.jpg
  Figure 2 caption: CNN-based 2D joint regressor. The network is a fully convolutional
    neural network (F-CNN). The input is an image and the output is a multi-channel
    heat map with each channel capturing the spatial uncertainty distribution of a
    joint.
  Figure 3 Link: articels_figures_by_rev_year\2018\MonoCap_Monocular_Human_Motion_Capture_using_a_CNN_Coupled_with_a_Geometric_Prio\figure_3.jpg
  Figure 3 caption: "Sensitivity to model parameters. The mean reconstruction error\
    \ versus model parameter \u03B1 , \u03B2 , or \u03B3 is shown, respectively."
  Figure 4 Link: articels_figures_by_rev_year\2018\MonoCap_Monocular_Human_Motion_Capture_using_a_CNN_Coupled_with_a_Geometric_Prio\figure_4.jpg
  Figure 4 caption: Example comparative frame results on Human3.6M [37]. Each row
    includes two examples. The images from left-to-right correspond to the heat map
    (all joints shown simultaneously), the 2D pose found by greedily locating each
    joint separately according to the heat map, the estimated 2D pose by the proposed
    EM algorithm, and the estimated 3D pose visualized in a novel view. The original
    viewpoint is also shown. Notice that the errors in the 2D heat maps are corrected
    after considering the pose and temporal smoothness priors.
  Figure 5 Link: articels_figures_by_rev_year\2018\MonoCap_Monocular_Human_Motion_Capture_using_a_CNN_Coupled_with_a_Geometric_Prio\figure_5.jpg
  Figure 5 caption: Example comparative frame results on KTH Football II [73]. The
    images from left-to-right in each example correspond to the heat map (all joints
    shown simultaneously), the 2D pose found by greedily locating each joint separately
    according to the heat map response, the estimated 2D pose by the proposed EM algorithm,
    and the estimated 3D pose visualized in a novel view. The original viewpoint is
    also shown.
  Figure 6 Link: articels_figures_by_rev_year\2018\MonoCap_Monocular_Human_Motion_Capture_using_a_CNN_Coupled_with_a_Geometric_Prio\figure_6.jpg
  Figure 6 caption: Example successes on MPII [62]. In each example, the images from
    left-to-right correspond to the input image, the heat map (all joints shown simultaneously),
    the estimated 2D pose, and the estimated 3D pose visualized in a novel view. The
    original viewpoint is also shown.
  Figure 7 Link: articels_figures_by_rev_year\2018\MonoCap_Monocular_Human_Motion_Capture_using_a_CNN_Coupled_with_a_Geometric_Prio\figure_7.jpg
  Figure 7 caption: Example comparative frame results on MPII [62]. In each example,
    the images from left-to-right correspond to the heat map (all joints shown simultaneously),
    the 2D pose found by greedily locating each joint separately according to the
    heat map, the estimated 2D pose by the proposed EM algorithm, and the estimated
    3D pose visualized in a novel view. The original viewpoint is also shown. Notice
    that the errors in the 2D heat maps are corrected after considering the 3D pose
    prior.
  Figure 8 Link: articels_figures_by_rev_year\2018\MonoCap_Monocular_Human_Motion_Capture_using_a_CNN_Coupled_with_a_Geometric_Prio\figure_8.jpg
  Figure 8 caption: Example failures on MPII [62]. In each example, the images from
    left-to-right correspond to the input image, the heat map (all joints shown simultaneously),
    the estimated 2D pose, and the estimated 3D pose visualized in a novel view. The
    original viewpoint is also shown.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.69
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Xiaowei Zhou
  Name of the last author: Kostas Daniilidis
  Number of Figures: 8
  Number of Tables: 8
  Number of authors: 6
  Paper title: 'MonoCap: Monocular Human Motion Capture using a CNN Coupled with a
    Geometric Prior'
  Publication Date: 2018-03-15 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Mean Per Joint Errors (mm) on Human3.6M [37] Given 2D Joints
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Mean Reconstruction Errors (mm) on Human3.6M [37] Given 2D
      Joints
  Table 3 caption:
    table_text: TABLE 3 Mean Per Joint Errors (mm) on Human3.6M [37]
  Table 4 caption:
    table_text: TABLE 4 Mean Reconstruction Errors (mm) on Human3.6M [37]
  Table 5 caption:
    table_text: "TABLE 5 Mean Per Joint Errors (mm) on the Human3.6M Online Test Set\
      \ (\u201CH36MNOS10\u201D) [37]"
  Table 6 caption:
    table_text: TABLE 6 Estimation Errors for the Variants of the Proposed Approach
  Table 7 caption:
    table_text: TABLE 7 Mean Reconstruction Errors (mm) on HumanEva I [65]
  Table 8 caption:
    table_text: TABLE 8 Mean PCP Scores on KTH Football II [73]
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2816031
- Affiliation of the first author: key laboratory of intelligent information processing,
    institute of computing technology, chinese academy of sciences, beijing, china
  Affiliation of the last author: department of mathematics, and by courtesy, the
    department of computer science and engineering, hong kong university of science
    and technology, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2018\From_Social_to_Individuals_A_Parsimonious_Path_of_MultiLevel_Models_for_Crowdsou\figure_1.jpg
  Figure 1 caption: 'A two-level preference learning in MovieLens: (a) The common
    preference with six representative occupation group preference. (b) The purple
    is the common preference, the remaining 21 paths represent the occupation group
    preferences, the red are the three groups with most distinct preferences from
    the common, the blue are the three groups with most similar preferences to the
    common, and the green ones are the others.'
  Figure 10 Link: articels_figures_by_rev_year\2018\From_Social_to_Individuals_A_Parsimonious_Path_of_MultiLevel_Models_for_Crowdsou\figure_10.jpg
  Figure 10 caption: 'Left: Mean running time (20 times repeat) of SynPar-LBI with
    thread number changing from 1 to 16 in WorldCollege ranking dataset. Right: The
    linear speedup of parallel LBI in WorldCollege ranking dataset.'
  Figure 2 Link: articels_figures_by_rev_year\2018\From_Social_to_Individuals_A_Parsimonious_Path_of_MultiLevel_Models_for_Crowdsou\figure_2.jpg
  Figure 2 caption: Comparison of three loss functions.
  Figure 3 Link: articels_figures_by_rev_year\2018\From_Social_to_Individuals_A_Parsimonious_Path_of_MultiLevel_Models_for_Crowdsou\figure_3.jpg
  Figure 3 caption: 'Left: Mean running time (20 times repeat) of SynPar-LBI with
    thread number changing from 1 to 16 in simulated data. Right: The linear speedup
    of parallel LBI in simulated data.'
  Figure 4 Link: articels_figures_by_rev_year\2018\From_Social_to_Individuals_A_Parsimonious_Path_of_MultiLevel_Models_for_Crowdsou\figure_4.jpg
  Figure 4 caption: 'Left: Mean running time (20 times repeat) of SynPar-LBI with
    thread number changing from 1 to 16 in movie dataset. Right: The linear speedup
    of parallel LBI in movie dataset.'
  Figure 5 Link: articels_figures_by_rev_year\2018\From_Social_to_Individuals_A_Parsimonious_Path_of_MultiLevel_Models_for_Crowdsou\figure_5.jpg
  Figure 5 caption: (a) The common preference in MovieLens dataset. (b) Preference
    of seven groups with different age range in MovieLens dataset.
  Figure 6 Link: articels_figures_by_rev_year\2018\From_Social_to_Individuals_A_Parsimonious_Path_of_MultiLevel_Models_for_Crowdsou\figure_6.jpg
  Figure 6 caption: 'Left: Mean running time (20 times repeat) of SynPar-LBI with
    thread number changing from 1 to 16 in IQA dataset. Right: The linear speedup
    of parallel LBI in IQA dataset.'
  Figure 7 Link: articels_figures_by_rev_year\2018\From_Social_to_Individuals_A_Parsimonious_Path_of_MultiLevel_Models_for_Crowdsou\figure_7.jpg
  Figure 7 caption: 'LBI regularization path of delta exhibiting personalized ranking
    in IQA dataset (reference image 1). (Red: top 10 personalized ranking annotators;
    Green: middle 3; Blue: bottom 3).'
  Figure 8 Link: articels_figures_by_rev_year\2018\From_Social_to_Individuals_A_Parsimonious_Path_of_MultiLevel_Models_for_Crowdsou\figure_8.jpg
  Figure 8 caption: Ranking order comparison of common versus personalized rankings
    of nine representative annotators in IQA dataset (reference image 1).
  Figure 9 Link: articels_figures_by_rev_year\2018\From_Social_to_Individuals_A_Parsimonious_Path_of_MultiLevel_Models_for_Crowdsou\figure_9.jpg
  Figure 9 caption: 'LBI regularization path of gamma exhibiting position bias in
    IQA dataset (reference image 1). (Red: Top 10 position-biased annotators; Blue:
    Bottom 5 position-biased annotators).'
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: Qianqian Xu
  Name of the last author: Yuan Yao
  Number of Figures: 13
  Number of Tables: 7
  Number of authors: 5
  Paper title: 'From Social to Individuals: A Parsimonious Path of Multi-Level Models
    for Crowdsourced Preference Aggregation'
  Publication Date: 2018-03-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Coarse-Grained versus Fine-Grained Model (i.e., Ours) on Test
      Error (i.e., Mismatch Ratio) in Simulated Data
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Coarse-Grained versus Fine-Grained Model (i.e., Ours) on Test
      Error (i.e., Mismatch Ratio) in Movie Dataset
  Table 3 caption:
    table_text: TABLE 3 Occupations and Age Ranges in Movie Dataset
  Table 4 caption:
    table_text: TABLE 4 Coarse-Grained versus Fine-Grained Model (i.e., Ours) on Test
      Error (i.e., Mismatch Ratio) in IQA Dataset (Reference Image 1)
  Table 5 caption:
    table_text: TABLE 5 Top 10 Position-Biased Annotators in IQA Dataset (Reference
      Image 1)
  Table 6 caption:
    table_text: TABLE 6 Coarse-Grained versus Fine-Grained Model (i.e., Ours) on Test
      Error (i.e., Mismatch Ratio) in WorldCollege Ranking Dataset
  Table 7 caption:
    table_text: TABLE 7 Top 10 Position-Biased Annotators in WorldCollege Ranking
      Dataset
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2817205
- Affiliation of the first author: beijing institute of technology, beijing, china
  Affiliation of the last author: microsoft research, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2018\HighSpeed_Hyperspectral_Video_Acquisition_By_Combining_Nyquist_and_Compressive_S\figure_1.jpg
  Figure 1 caption: System schematic for HSHS video acquisition by combining Nyquist
    and compressive sampling.
  Figure 10 Link: articels_figures_by_rev_year\2018\HighSpeed_Hyperspectral_Video_Acquisition_By_Combining_Nyquist_and_Compressive_S\figure_10.jpg
  Figure 10 caption: Reconstruction results of two selected bands at different temporal
    locations under K=20 .
  Figure 2 Link: articels_figures_by_rev_year\2018\HighSpeed_Hyperspectral_Video_Acquisition_By_Combining_Nyquist_and_Compressive_S\figure_2.jpg
  Figure 2 caption: Data flow in the proposed system. The CASSI branch captures a
    low-frame-rate video while the PanCam branch captures a high-frame-rate video
    simultaneously. The PanCam measurement is then used to train an over-complete
    3D dictionary, with which the underlying 4D HSHS video is reconstructed from the
    joint videos.
  Figure 3 Link: articels_figures_by_rev_year\2018\HighSpeed_Hyperspectral_Video_Acquisition_By_Combining_Nyquist_and_Compressive_S\figure_3.jpg
  Figure 3 caption: 'An example demonstrating structural similarity between a panchromatic
    image and its corresponding hyperspectral images at different bands. From left
    to right: panchromatic image, hyperspectral images at 510 nm, 600 nm, and 700
    nm.'
  Figure 4 Link: articels_figures_by_rev_year\2018\HighSpeed_Hyperspectral_Video_Acquisition_By_Combining_Nyquist_and_Compressive_S\figure_4.jpg
  Figure 4 caption: Paradigm of the proposed 3S model. Spatial-temporal cubes are
    extracted from band-wise videos at the same spatial location and stacked into
    a matrix. This matrix, when represented on the over-complete 3D dictionary, should
    have sparse coefficients concentrating on a few rows.
  Figure 5 Link: articels_figures_by_rev_year\2018\HighSpeed_Hyperspectral_Video_Acquisition_By_Combining_Nyquist_and_Compressive_S\figure_5.jpg
  Figure 5 caption: Influence of different dictionary configurations on our reconstruction
    algorithm. (a) Training cube number. (b) Dictionary over-completeness.
  Figure 6 Link: articels_figures_by_rev_year\2018\HighSpeed_Hyperspectral_Video_Acquisition_By_Combining_Nyquist_and_Compressive_S\figure_6.jpg
  Figure 6 caption: Convergence of our reconstruction algorithm.
  Figure 7 Link: articels_figures_by_rev_year\2018\HighSpeed_Hyperspectral_Video_Acquisition_By_Combining_Nyquist_and_Compressive_S\figure_7.jpg
  Figure 7 caption: Reconstruction results of two selected bands at different temporal
    locations under K=2 . (a) Original frame. (b) Enlarged regions. (c) CASSI-TI.
    (d) TwIST. (e) DBR. (f) 3S. (Please see the electronic version for better visualization.)
  Figure 8 Link: articels_figures_by_rev_year\2018\HighSpeed_Hyperspectral_Video_Acquisition_By_Combining_Nyquist_and_Compressive_S\figure_8.jpg
  Figure 8 caption: Prototype system for HSHS video acquisition.
  Figure 9 Link: articels_figures_by_rev_year\2018\HighSpeed_Hyperspectral_Video_Acquisition_By_Combining_Nyquist_and_Compressive_S\figure_9.jpg
  Figure 9 caption: HSHS video reconstruction results of a virtual scene consisting
    of three fast moving shapes with distinct spectrum components displayed on an
    LCD screen. (a) One set of CASSI and PanCam measurements along with the moving
    scene. (b)-(d) Reconstruction results of three selected bands at one temporal
    location. (Please see the electronic version for better visualization, and also
    refer to the supplementary video, which can be found on the Computer Society Digital
    Library at http:doi.ieeecomputersociety.org10.1109TPAMI.2018.2817496 for more
    complete results.)
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.69
  Name of the first author: Lizhi Wang
  Name of the last author: Wenjun Zeng
  Number of Figures: 15
  Number of Tables: 3
  Number of authors: 6
  Paper title: High-Speed Hyperspectral Video Acquisition By Combining Nyquist and
    Compressive Sampling
  Publication Date: 2018-03-20 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Light Throughput Comparison of Different Snapshot Spectral
      Imagers
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Evaluation of Different Reconstruction Algorithms
  Table 3 caption:
    table_text: TABLE 3 RMSE of Spectral Signatures in Three Areas
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2817496
