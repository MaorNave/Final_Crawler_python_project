- Affiliation of the first author: department of computer science, university of north
    carolina at charlotte, charlotte, nc
  Affiliation of the last author: department of computer science, rutgers the state
    university of new jersey, division of computer and information sciences, room
    c324, hill center bldg for the mathematical sciences, piscataway, nj
  Figure 1 Link: articels_figures_by_rev_year\2014\Query_Specific_Rank_Fusion_for_Image_Retrieval\figure_1.jpg
  Figure 1 caption: Retrieval results of two query images (in the green boxes) in
    the Corel-5K dataset, using a holistic feature (GIST) at the first row and in
    theblue boxes, and BoW of local features (SIFT) at the second row and in the black
    boxes.
  Figure 10 Link: articels_figures_by_rev_year\2014\Query_Specific_Rank_Fusion_for_Image_Retrieval\figure_10.jpg
  Figure 10 caption: Three sets of retrieval results from the UKbench (top), Corel-5k
    (middle), and SFLandmarks (bottom) datasets, respectively. Top-4candidates are
    shown for the fusion results ( 3rm rd row in the purple boxes) of a query (in
    a green box on the left), using holistic features ( 1rmst row in the blue boxes),
    and local features ( 2rmnd row in the black boxes).
  Figure 2 Link: articels_figures_by_rev_year\2014\Query_Specific_Rank_Fusion_for_Image_Retrieval\figure_2.jpg
  Figure 2 caption: An example of graph construction, where the query q links to its
    reciprocal neighbors (i.e., q and the green discs in the green zone). d is a candidate
    at the first layer with its reciprocal neighbors in the blue zone, whose Jaccard
    coefficient to q is 37 ( of nodes in the intersection divided by of nodes in the
    union of the green and blue zones). The radius of the disc representing a node
    indicates the influence of decay coefficient alpha .
  Figure 3 Link: articels_figures_by_rev_year\2014\Query_Specific_Rank_Fusion_for_Image_Retrieval\figure_3.jpg
  Figure 3 caption: Fusion of two graphs where the green and yellow graphs are derived
    from two different retrieval methods.
  Figure 4 Link: articels_figures_by_rev_year\2014\Query_Specific_Rank_Fusion_for_Image_Retrieval\figure_4.jpg
  Figure 4 caption: Illustration of expanding Gprime (the green zone). Candidate nodes
    are connected to Gprime , and are denoted by dash lines.
  Figure 5 Link: articels_figures_by_rev_year\2014\Query_Specific_Rank_Fusion_for_Image_Retrieval\figure_5.jpg
  Figure 5 caption: N-S scores on the UKbench dataset, after iteratively building
    graphs and applying the Graph-density on each baseline method until converge,
    then the two new baseline retrieval sets are combined using the Graph-PageRank
    and Graph-density.
  Figure 6 Link: articels_figures_by_rev_year\2014\Query_Specific_Rank_Fusion_for_Image_Retrieval\figure_6.jpg
  Figure 6 caption: 'From left to right: we add random noise (from 0 to 100 percent)
    to the rank results of VOC, HSV, and both, respectively. In the first two cases,
    the corrupted results are fused with the the other feature without noise (HSV
    or VOC). We compare the baselines with fusion results of the rank aggregation,
    the density-based and PageRank-based graph fusion methods. In the third case,
    we fuse two types of corrupted results.'
  Figure 7 Link: articels_figures_by_rev_year\2014\Query_Specific_Rank_Fusion_for_Image_Retrieval\figure_7.jpg
  Figure 7 caption: The scope( r )-precision curves for the Corel-5K dataset.
  Figure 8 Link: articels_figures_by_rev_year\2014\Query_Specific_Rank_Fusion_for_Image_Retrieval\figure_8.jpg
  Figure 8 caption: The scope( r )-precision curves for the Corel-5K dataset when
    fusing results from three features.
  Figure 9 Link: articels_figures_by_rev_year\2014\Query_Specific_Rank_Fusion_for_Image_Retrieval\figure_9.jpg
  Figure 9 caption: Retrieval results on the SFLandmarks. Recall versus number of
    top database candidates of (a) query 803 images in the 1.06M PCIs and (b) query
    803 images in the 638k PFIs.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Shaoting Zhang
  Name of the last author: Dimitris N. Metaxas
  Number of Figures: 11
  Number of Tables: 4
  Number of authors: 5
  Paper title: Query Specific Rank Fusion for Image Retrieval
  Publication Date: 2014-08-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of N-S Scores on the UKbench Dataset with Recent
      Retrieval Methods and Other Rank Fusion Approaches
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Top-1 Precision (in Percent) on the Corel-5K Dataset
  Table 3 caption:
    table_text: TABLE 3 Comparison of the mAP (in Percent) on the Holidays Dataset
      with Recent Retrieval Methods and Other Fusion Approaches
  Table 4 caption:
    table_text: TABLE 4 The Average Query Time (in ms) and the Breakdown on the Test
      Datasets
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2346201
- Affiliation of the first author: research school of engineering, australian national
    university and nicta (national ict australia), canberra, australia
  Affiliation of the last author: research school of engineering, australian national
    university and nicta (national ict australia), canberra, australia
  Figure 1 Link: articels_figures_by_rev_year\2014\Generalized_Weiszfeld_Algorithms_for_Lq_Optimization\figure_1.jpg
  Figure 1 caption: 'Mechanical setup for the Fermat-Weber problem: Each pulley represents
    a fixed point. Strings are passed over the pulleys and unit weights are attached
    to one end of string while other ends are tied together at a point (shown in green).
    The equilibrium position of that point will be the L1 minimum.'
  Figure 10 Link: articels_figures_by_rev_year\2014\Generalized_Weiszfeld_Algorithms_for_Lq_Optimization\figure_10.jpg
  Figure 10 caption: Side-by-side comparison of the results of L1 and L2 averaging
    for each of the four methods of computing relative rotations.
  Figure 2 Link: articels_figures_by_rev_year\2014\Generalized_Weiszfeld_Algorithms_for_Lq_Optimization\figure_2.jpg
  Figure 2 caption: 'Weiszfeld Algorithm (Gradient Descent Form): (a) shows three
    fixed points (green) and a starting point (red) from which the sum of distances
    to fixed points (green) is to be minimized. (b) shows an updated point (red) after
    one iteration of the Weiszfeld algorithm in the descent direction.'
  Figure 3 Link: articels_figures_by_rev_year\2014\Generalized_Weiszfeld_Algorithms_for_Lq_Optimization\figure_3.jpg
  Figure 3 caption: 'Weiszfeld Algorithm on Manifold: (a) represents a manifold with
    some given fixed points (red) and a starting point (white). (b), the Weiszfeld
    algorithm is applied to the transformed points (green) in the tangent space (red
    plane) and an updated point (blue) is computed in a descent direction. This updated
    point is then mapped back to the manifold and the procedure is repeated until
    convergence.'
  Figure 4 Link: articels_figures_by_rev_year\2014\Generalized_Weiszfeld_Algorithms_for_Lq_Optimization\figure_4.jpg
  Figure 4 caption: 'Rotation Averaging: (a) represents two cameras with bf Rij as
    a relative rotation between them. We obtain several estimates of the rotation
    between these two cameras and then perform averaging on them to get a better estimate.
    (b), we apply the rotation averaging algorithm to estimate absolute rotations,
    bf Ri , from previously computed relative rotations bf Rij .'
  Figure 5 Link: articels_figures_by_rev_year\2014\Generalized_Weiszfeld_Algorithms_for_Lq_Optimization\figure_5.jpg
  Figure 5 caption: 'Multiple Rotation Averaging: Nodes of the above graph represent
    absolute rotations tt Ri and edges of the graph represent relative rotation tt
    Rij . After fixing a root node tt Ri0 we construct a spanning tree of the graph
    (represented by solid arrows). For each node tt Rj , we apply a single iteration
    of the Lq rotation averaging algorithm on its neighboring nodes mathcal N(j) to
    get an averaged estimate of tt Rj . This process is repeated for every node of
    the graph.'
  Figure 6 Link: articels_figures_by_rev_year\2014\Generalized_Weiszfeld_Algorithms_for_Lq_Optimization\figure_6.jpg
  Figure 6 caption: 'Data set: (a), (b) and (c) show multiple images of the Notre
    Dame cathedral.'
  Figure 7 Link: articels_figures_by_rev_year\2014\Generalized_Weiszfeld_Algorithms_for_Lq_Optimization\figure_7.jpg
  Figure 7 caption: The graph shows the result of L2 (top curve) and L1 (bottom curve)
    rotation averaging, used in computing the relative orientation of two cameras
    from repeatedly applying the 5 -point algorithm to estimate relative rotation.
    The graph shows the results for a single pair of images and is indicative, of
    the general qualitative behaviour. The plots show the error with respect to ground
    truth as a function of the number of samples taken. In this example, the L1 algorithm
    converges in this case to close to ground truth with about 10 samples.
  Figure 8 Link: articels_figures_by_rev_year\2014\Generalized_Weiszfeld_Algorithms_for_Lq_Optimization\figure_8.jpg
  Figure 8 caption: The plot shows the result of the Lq averaging method for different
    values of q in the presence of different percentage of outliers in the dataset.
    We modify some percentage of image point correspondences to represent outliers
    and use the five-point algorithm to estimate relative rotation. Errors are computed
    by using the ground truth values. The above plots show that the Lq method, for
    q =1 , is more robust to outliers than the rest of the values of q and the L2
    method.
  Figure 9 Link: articels_figures_by_rev_year\2014\Generalized_Weiszfeld_Algorithms_for_Lq_Optimization\figure_9.jpg
  Figure 9 caption: Whisker plots of the absolute orientation accuracy of the 595
    images of the Notre Dame data set. The top and bottom of the boxes represent the
    25 and 75 percent marks. The left graph shows the result of L1 averaging and the
    right graph the L2 averaging results. In each graph are shown the results arising
    from different methods of computing the essential matrices, and hence the rotations.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Khurrum Aftab
  Name of the last author: Jochen Trumpf
  Number of Figures: 10
  Number of Tables: 1
  Number of authors: 3
  Paper title: Generalized Weiszfeld Algorithms for Lq Optimization
  Publication Date: 2014-08-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Timing (on a 2.6 GHz Laptop) for the Computation of the 42,621
      Essential Matrices Using Various Methods, and Also the Time Taken for L 1 and
      L 2 Averaging over All Nodes
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2353625
- Affiliation of the first author: computer science division, university of california
    at berkeley, berkeley, ca
  Affiliation of the last author: department of cognitive science, university of california,
    san diego, la jolla, ca
  Figure 1 Link: articels_figures_by_rev_year\2014\Unsupervised_Object_Class_Discovery_via_SaliencyGuided_Multiple_Class_Learning\figure_1.jpg
  Figure 1 caption: 'An overview of the unsupervised object class discovery problem.
    The input is the same for different types of algorithms: a set of unlabeled images.
    On the other hand, since different algorithms have different purposes, the outputs
    of the algorithms will vary according to those purposes.'
  Figure 10 Link: articels_figures_by_rev_year\2014\Unsupervised_Object_Class_Discovery_via_SaliencyGuided_Multiple_Class_Learning\figure_10.jpg
  Figure 10 caption: Object detection results for novel images returned by image search
    engines. The first two rows illustrate exemplar successful detection results and
    the last row illustrates exemplar failure cases.
  Figure 2 Link: articels_figures_by_rev_year\2014\Unsupervised_Object_Class_Discovery_via_SaliencyGuided_Multiple_Class_Learning\figure_2.jpg
  Figure 2 caption: "The pipeline of the proposed bMCL algorithm: (a) saliency-scored\
    \ windows, (b) high-salience \u201Cprobably positive\u201D bags (in which we expect\
    \ the object to be present), (c) low-salience \u201Cprobably negative\u201D bags\
    \ (in which we expect only background to be present), (d) Bottom-up Multiple Class\
    \ Learning algorithm: different colors represent positive bags that belong to\
    \ different classes, and (e) object clustering and detection results."
  Figure 3 Link: articels_figures_by_rev_year\2014\Unsupervised_Object_Class_Discovery_via_SaliencyGuided_Multiple_Class_Learning\figure_3.jpg
  Figure 3 caption: (a) Localized objects from SIVAL [41]. (b) Original images from
    SIVAL [41].
  Figure 4 Link: articels_figures_by_rev_year\2014\Unsupervised_Object_Class_Discovery_via_SaliencyGuided_Multiple_Class_Learning\figure_4.jpg
  Figure 4 caption: 'Example of bags and instances. On the first row, red rectangles:
    the most salient windows as instances in the positive bag; yellow rectangles:
    the most salient window obtained by [21]. On the second row, green rectangles:
    the least salient windows from a large set of randomly sampled windows as instances
    in the negative bag; blue rectangles: the desired object window.'
  Figure 5 Link: articels_figures_by_rev_year\2014\Unsupervised_Object_Class_Discovery_via_SaliencyGuided_Multiple_Class_Learning\figure_5.jpg
  Figure 5 caption: Object categorization results with varying number of clusters
    K are measured by purity. We compare bMCL with saliency detection baseline (SD)
    and random guess (RAND).
  Figure 6 Link: articels_figures_by_rev_year\2014\Unsupervised_Object_Class_Discovery_via_SaliencyGuided_Multiple_Class_Learning\figure_6.jpg
  Figure 6 caption: 'Illustrative categorization results of four methods in the experiments,
    the left from SIVAL2 [41] and the right from 3D1 [46]. From top to bottom: bMCL,
    M 3 IC [56], BAMIC [58], UnSL [27] and MFC [28]. In bMCL, the yellow rectangle
    is the localized object and the white rectangle is the most salient window given
    by [21]. In UnSL, the learned object key points are overlayed (red points). See
    Section 6.1 for detailed discussion.'
  Figure 7 Link: articels_figures_by_rev_year\2014\Unsupervised_Object_Class_Discovery_via_SaliencyGuided_Multiple_Class_Learning\figure_7.jpg
  Figure 7 caption: 'Object detection results for novel images: the top from SIVAL3
    [41], the middle from CC [5], and the lower from 3D1 [46]. The rectangles are
    the localization results given by bMCL. Different colors represent the class labels
    returned by the algorithm.'
  Figure 8 Link: articels_figures_by_rev_year\2014\Unsupervised_Object_Class_Discovery_via_SaliencyGuided_Multiple_Class_Learning\figure_8.jpg
  Figure 8 caption: 'Red rectangles: bMCL object localization results with a single
    object class (from top to bottom: aeroplane, cow, and motorbike) on the challenging
    PASCAL VOC 07.'
  Figure 9 Link: articels_figures_by_rev_year\2014\Unsupervised_Object_Class_Discovery_via_SaliencyGuided_Multiple_Class_Learning\figure_9.jpg
  Figure 9 caption: "Illustrative clustering and localization results on Internet\
    \ images with keywords \u201Cbean\u201D and \u201Cbow\u201D."
  First author gender probability: 0.67
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jun-Yan Zhu
  Name of the last author: Zhuowen Tu
  Number of Figures: 10
  Number of Tables: 4
  Number of authors: 5
  Paper title: Unsupervised Object Class Discovery via Saliency-Guided Multiple Class
    Learning
  Publication Date: 2014-08-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Object Categorization Performance Is Measured in Terms of
      (a) Purity, (b) Clustering Accuracy, and (c) NMI
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results for the Single Class Recognition Experiment
  Table 3 caption:
    table_text: TABLE 3 Clustering Results of Images Returned by Image Search Engines
  Table 4 caption:
    table_text: TABLE 4 Clustering Results of Internet Images Associated with Double
      Meaning Queries
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2353617
- Affiliation of the first author: department of computer science, city university
    of hong kong, hong kong special administrative region, china
  Affiliation of the last author: department of computer science, city university
    of hong kong, hong kong special administrative region, china
  Figure 1 Link: articels_figures_by_rev_year\2014\Stereo_Matching_Using_Tree_Filtering\figure_1.jpg
  Figure 1 caption: Support weights computed from a synthetic image. (a) is the guidance
    image I , (b)-(f) are support weights received by the first image pixel (on the
    top left of I ). The support weight in (b)-(f) decreases from 1 to 0 as the color
    changes from red to blue. (b)-(c) are computed using joint bilateral filter with
    sigma R=0.05 . sigma S is set to infinity in (c) so that every pixel contributes
    its support. (d)-(f) are support weights computed using the MST derived from (a)
    with sigma =0.05, 0.1, 0.3 , respectively. Note that the support weights in (d)-(f)
    are more natural for cost aggregation than (b)-(c).
  Figure 10 Link: articels_figures_by_rev_year\2014\Stereo_Matching_Using_Tree_Filtering\figure_10.jpg
  Figure 10 caption: Performance of the proposed stereo algorithm on different image
    resolutions. (a) and (b) present the errors (percentages of bad pixels) obtained
    from the Teddy and Cones data sets. The dash lines is obtained with texture handling,
    and the yellow, blue and dark curves are computed from the full-resolution (1,800
    times 1,500), half-resolution (900 times 750), quarter-resolution (450 times 375)
    images, respectively.
  Figure 2 Link: articels_figures_by_rev_year\2014\Stereo_Matching_Using_Tree_Filtering\figure_2.jpg
  Figure 2 caption: 'Linear time cost aggregation on an MST. The aggregation contains
    two steps: upward aggregation and downward aggregation. Upward aggregation performs
    aggregation from leaf nodes to root node and stores the aggregation result CdAuparrow
    at each node. Downward aggregation performs aggregation from root node to leaf
    nodes based on the previous upward aggregation result CdAuparrow and the downward
    aggregation result of the corresponding parent node CdAdownarrow . See the text
    for details.'
  Figure 3 Link: articels_figures_by_rev_year\2014\Stereo_Matching_Using_Tree_Filtering\figure_3.jpg
  Figure 3 caption: Experiment on textured Rocks1 data set [23]. (a) is the left image
    and (b) is ground-truth disparity map. (c) and (d) are the disparity maps obtained
    from the proposed non-local aggregation algorithm with and without texture handling,
    respectively. (e) is the synthetic left image with Gaussian white noise and (f)
    is the corresponding disparity map obtained by replacing the proposed non-local
    cost aggregation algorithm with a simple Gaussian filter, and (g) and (h) are
    the corresponding disparity maps obtained from the proposed non-local aggregation
    algorithm with and without texture handling, respectively. The numbers under the
    disparity maps are the percentages of bad pixels (with disparity error threshold
    1).
  Figure 4 Link: articels_figures_by_rev_year\2014\Stereo_Matching_Using_Tree_Filtering\figure_4.jpg
  Figure 4 caption: Texture suppression. (a) is the input image and (b) is the filtered
    image obtained from the proposed non-local filter. (c) and (d) are the filtered
    image when mathcal D(s,r)=1 . (c) and (d) are computed when sigma is set to 0.03
    and 0.1, respectively. Note that the proposed non-local filter can achieve much
    stronger smoothing on textured regions while keeping major edges sharp when mathcal
    D(s,r)=1 and sigma is relatively small.
  Figure 5 Link: articels_figures_by_rev_year\2014\Stereo_Matching_Using_Tree_Filtering\figure_5.jpg
  Figure 5 caption: Non-local upsampling. (a) contains the input high-resolution color
    images and the corresponding low-resolution disparity maps (on the upper left)
    and (b) are the upsampled disparity maps obtained from the upsampling method proposed
    in Section 4.1. The spatial resolution is enhanced 64times .
  Figure 6 Link: articels_figures_by_rev_year\2014\Stereo_Matching_Using_Tree_Filtering\figure_6.jpg
  Figure 6 caption: Experimental results on the Middlebury data sets [23]. (a) are
    the left images. (b) and (c) are disparity maps obtained from the local guided
    image filter based cost aggregation method [13] and the proposed non-local cost
    aggregation method proposed in Section 2, respectively. (d) and (e) are disparity
    maps obtained from the proposed non-local cost aggregation and the texture handling
    method presented in Section 3 with respect to two different sigma values. The
    bold numbers under the images are the average errors (percentages of bad pixels)
    which show that the proposed aggregation algorithm outperforms the guided filter
    [13]. The corresponding quantitative evaluation is summarized in Table 2.
  Figure 7 Link: articels_figures_by_rev_year\2014\Stereo_Matching_Using_Tree_Filtering\figure_7.jpg
  Figure 7 caption: Experimental results on the Middlebury data sets [23]. (a)-(d)
    present visual comparison of two disparity refinement methods with numerical comparison
    provided in Table 3. (a) and (c) are disparity maps obtained using the local weighted
    median filter base disparity refinement method in [13] , and (b) and (d) are disparity
    maps obtained using the non-local disparity refinement method proposed in Section
    4.2. (a)-(b) use the local guided image filter based cost aggregation method in
    [13] and (c)-(d) use the non-local cost aggregation method proposed in Section
    2. (e) is the disparity maps obtained with the non-local cost aggregation method
    proposed in Section 2, the non-local disparity refinement method proposed in Section
    4.2 and the texture handling method proposed in Section 3 when sigma =0.045 .
    The bold numbers under the images are the average errors (percentages of bad pixels).
  Figure 8 Link: articels_figures_by_rev_year\2014\Stereo_Matching_Using_Tree_Filtering\figure_8.jpg
  Figure 8 caption: Non-local cost aggregation in the time domain for enforcing temporal
    coherency. (a) is the reference camera image, and (b)-(c) are disparity maps obtained
    using the local guided image filter based method [13] and the proposed non-local
    cost aggregation method, respectively. (d)-(f) are disparity maps obtained using
    the proposed non-local cost aggregation with extension to the time domain for
    enforcing temporal coherence.
  Figure 9 Link: articels_figures_by_rev_year\2014\Stereo_Matching_Using_Tree_Filtering\figure_9.jpg
  Figure 9 caption: Performance of the proposed stereo algorithm with respect to parameter
    sigma . (a)-(d) present the errors (percentages of bad pixels) as parameter sigma
    increases from 0.02 to 0.22 , and the black lines correspond to the average errors,
    with minimum value 6.65 percent in (a), 5.36 percent in (b), 6.62 percent in (c)
    and 5.06 percent in (d).
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Qingxiong Yang
  Name of the last author: Qingxiong Yang
  Number of Figures: 10
  Number of Tables: 3
  Number of authors: 1
  Paper title: Stereo Matching Using Tree Filtering
  Publication Date: 2014-08-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Comparison of the Local Weighted Median Filter
      Based Methods and the Proposed Non-Local Upsampling Method on the Middlebury
      Benchmark [23] with Error Threshold 1
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Comparison of the Local Guided Image Filter Based
      Cost Aggregation Method in [13] and the Proposed Non-Local Cost Aggregation
      Methods on the Middlebury Benchmark [23] with Error Threshold 1
  Table 3 caption:
    table_text: TABLE 3 Numerical Comparison of the Local Weighted Median Filter Base
      Disparity Refinement Method in [13] and the Proposed Non-Local Disparity Refinement
      Method on the Middlebury Benchmark [23] with Error Threshold 1
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2353642
- Affiliation of the first author: graduate school of information sciences, tohoku
    university, sendai, miyagi, japan
  Affiliation of the last author: department of computer science, university of north
    carolina at chapel hill, chapel hill, nc
  Figure 1 Link: articels_figures_by_rev_year\2014\Retrieving_Similar_Styles_to_Parse_Clothing\figure_1.jpg
  Figure 1 caption: Parsing pipeline. Retrieved images and predicted tags augment
    clothing parsing.
  Figure 10 Link: articels_figures_by_rev_year\2014\Retrieving_Similar_Styles_to_Parse_Clothing\figure_10.jpg
  Figure 10 caption: Data size and parsing performance when 1) items are unknown and
    2) items are known. While average recall tends to converge, average precision
    grows with data size.
  Figure 2 Link: articels_figures_by_rev_year\2014\Retrieving_Similar_Styles_to_Parse_Clothing\figure_2.jpg
  Figure 2 caption: 'Style descriptor: Compact representation for fashion images.'
  Figure 3 Link: articels_figures_by_rev_year\2014\Retrieving_Similar_Styles_to_Parse_Clothing\figure_3.jpg
  Figure 3 caption: Retrieval examples. The leftmost column shows query images with
    ground truth item annotation. The rest are retrieved images with associated tags
    in the top 25. Notice retrieved samples sometimes have missing item tags.
  Figure 4 Link: articels_figures_by_rev_year\2014\Retrieving_Similar_Styles_to_Parse_Clothing\figure_4.jpg
  Figure 4 caption: Tag prediction PR-plot. KNN performs better in the high-recall
    regime.
  Figure 5 Link: articels_figures_by_rev_year\2014\Retrieving_Similar_Styles_to_Parse_Clothing\figure_5.jpg
  Figure 5 caption: Parsing outputs at each step. Labels are MAP assignments of the
    scoring functions.
  Figure 6 Link: articels_figures_by_rev_year\2014\Retrieving_Similar_Styles_to_Parse_Clothing\figure_6.jpg
  Figure 6 caption: Transferred parse. We transfer likelihoods in nearest neighbors
    to the input via dense matching.
  Figure 7 Link: articels_figures_by_rev_year\2014\Retrieving_Similar_Styles_to_Parse_Clothing\figure_7.jpg
  Figure 7 caption: Parsing examples (best seen in color). Our method sometimes confuses
    similar items, but gives overall perceptually better results.
  Figure 8 Link: articels_figures_by_rev_year\2014\Retrieving_Similar_Styles_to_Parse_Clothing\figure_8.jpg
  Figure 8 caption: F-1 score of non-empty items. We observe significant performance
    gains, especially for large items.
  Figure 9 Link: articels_figures_by_rev_year\2014\Retrieving_Similar_Styles_to_Parse_Clothing\figure_9.jpg
  Figure 9 caption: Parsing performance over retrieval size when items are unknown.
    Larger retrieval size results in slightly better parsing, but also takes longer
    computation time.
  First author gender probability: 0.9
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Kota Yamaguchi
  Name of the last author: Tamara L. Berg
  Number of Figures: 13
  Number of Tables: 3
  Number of authors: 4
  Paper title: Retrieving Similar Styles to Parse Clothing
  Publication Date: 2014-08-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Low-Level Features for Parsing
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Parsing Performance for Final and Intermediate Results (MAP
      Assignments at Each Step) in Percentage
  Table 3 caption:
    table_text: TABLE 3 Pose Estimation Performance with or without Conditional Parsing
      Input
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2353624
- Affiliation of the first author: nicta, crl, canberra, act 2600, australia
  Affiliation of the last author: nicta, crl, canberra, act 2600, australia
  Figure 1 Link: articels_figures_by_rev_year\2014\Mirror_Surface_Reconstruction_from_a_Single_Image\figure_1.jpg
  Figure 1 caption: Reconstruction setup and sample input images.
  Figure 10 Link: articels_figures_by_rev_year\2014\Mirror_Surface_Reconstruction_from_a_Single_Image\figure_10.jpg
  Figure 10 caption: Evaluation of the shapes (depth maps) obtained either by solving
    polynomials or by integration for synthetic data. (a) Ground truth depth range
    for the Uniform Cubic B-spline Surface (UCBS) (4.48 cm-5.34 cm), the sphere (8.18
    cm-8.85 cm) and the ellipsoid (5.93 cm-7.58 cm)(from top to bottom). (b) Absolute
    difference between the depth obtained by solving polynomials and the ground truth.
    Note that most of the reconstruction errors are less than 0.01 cm. (c) Absolute
    depth difference between the depth obtained by integration order A and the ground
    truth. (d) Absolute difference between the depth obtained by integration order
    A and order B. Note that these differences are less than 0.005 cm. All error measurements
    are given in centimetres.
  Figure 2 Link: articels_figures_by_rev_year\2014\Mirror_Surface_Reconstruction_from_a_Single_Image\figure_2.jpg
  Figure 2 caption: Mirror surface reconstruction setup. A pinhole camera centred
    at O is observing a mirror surface P that reflects a reference plane Q in image
    I . A point bf m on Q is reflected to the image point bf v on I via the 3D mirror
    point bf p on P . We refer to bf m and bf v as reflection correspondences. The
    reflected ray bf l is determined by bf m and bf p . We denote by bf i the incident
    ray for image point bf v and by bf n the normal to P at bf p . tt R and tt T denote
    the pose of the reference plane w.r.t. the camera, and bf n satisfies the mirror
    reflection geometry, i.e., bisects the angle between bf i and bf l .
  Figure 3 Link: articels_figures_by_rev_year\2014\Mirror_Surface_Reconstruction_from_a_Single_Image\figure_3.jpg
  Figure 3 caption: Plane mirror. Assume that the camera is centred at O viewing a
    plane mirror P which is placed two units in front of the camera, namely, the translation
    vector of P is (0,0,2)top , rotated by 45 degree around the y -axis and reflecting
    a reference plane Q placed at x = -2 . We denote Oprime as virtual camera center.
    Here, we show the projected light path in xz plane. From the reflection geometry,
    it can be derived that the reflection correspondence on the reference plane P
    is bf m = (-2, 4y, 2-4x)top .
  Figure 4 Link: articels_figures_by_rev_year\2014\Mirror_Surface_Reconstruction_from_a_Single_Image\figure_4.jpg
  Figure 4 caption: Initialization of the unknown mirror. See text for details.
  Figure 5 Link: articels_figures_by_rev_year\2014\Mirror_Surface_Reconstruction_from_a_Single_Image\figure_5.jpg
  Figure 5 caption: Ellipsoidal mirror. Let Plambda represent an ellipsoidal mirror
    with foci bff1 and bff2 . The camera is placed at bff1 and the reference plane,
    Q , passes through the other focus bff2 . All rays emanating from bff1 pass through
    bff2 .
  Figure 6 Link: articels_figures_by_rev_year\2014\Mirror_Surface_Reconstruction_from_a_Single_Image\figure_6.jpg
  Figure 6 caption: Central catadioptric systems. Q denotes the reference plane. The
    camera is initially placed at O1 viewing a conic mirror P1 . Oprime denotes the
    virtual camera center. The imaging system formed by O2 and P2 is a scaled version
    of the original imaging system composed of O1 and P1 by a factor k such that Oprime
    is fixed. It generates the same image. Hence this forms an ambiguity if the relative
    pose between Q and O1 is unknown.
  Figure 7 Link: articels_figures_by_rev_year\2014\Mirror_Surface_Reconstruction_from_a_Single_Image\figure_7.jpg
  Figure 7 caption: Plane Mirror. Q denotes the reference plane. The camera is initially
    placed at O1 viewing a plane mirror P1 . Oprime denotes the virtual camera center.
    O2 is a new camera position viewing a new plane mirror P2 , which form an ambiguity
    if the relative pose between Q and the camera is unknown.
  Figure 8 Link: articels_figures_by_rev_year\2014\Mirror_Surface_Reconstruction_from_a_Single_Image\figure_8.jpg
  Figure 8 caption: Transparent object reconstruction setup. The camera is placed
    at O viewing the transparent object P . Consider a point bfv on the image plane
    I and its corresponding visual ray (incident ray) bfi which meets P at bfp . The
    refractive indices for the medium are denoted by nu 1 and nu 2 . The incident
    ray bfi is refracted and intersects the reference plane Q at point bfm . The refracted
    ray is determined by bfm and bfp . We define bfm and bfv as refraction correspondences.
  Figure 9 Link: articels_figures_by_rev_year\2014\Mirror_Surface_Reconstruction_from_a_Single_Image\figure_9.jpg
  Figure 9 caption: Planar refractive surface. Assume the planar refractive surface
    P is represented by , namely the translation vector is (0,0,2) . The reference
    plane Q is placed at z = 4 . The relative refractive index nu = 12 .
  First author gender probability: 0.79
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Miaomiao Liu
  Name of the last author: Mathieu Salzmann
  Number of Figures: 16
  Number of Tables: 0
  Number of authors: 3
  Paper title: Mirror Surface Reconstruction from a Single Image
  Publication Date: 2014-08-29 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2353622
- Affiliation of the first author: "department of cybernetics, czech technical university,\
    \ karlovo n\xE1m\u011Bst\xED 13,12135 praha, czech republic"
  Affiliation of the last author: "department of cybernetics, czech technical university,\
    \ karlovo n\xE1m\u011Bst\xED 13,12135 praha, czech republic"
  Figure 1 Link: articels_figures_by_rev_year\2014\Universality_of_the_Local_Marginal_Polytope\figure_1.jpg
  Figure 1 caption: A pair of objects lbrace u,vrbrace in E with |K|=3 labels.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\Universality_of_the_Local_Marginal_Polytope\figure_2.jpg
  Figure 2 caption: Elementary constructions.
  Figure 3 Link: articels_figures_by_rev_year\2014\Universality_of_the_Local_Marginal_Polytope\figure_3.jpg
  Figure 3 caption: Construction of the number frac58 .
  Figure 4 Link: articels_figures_by_rev_year\2014\Universality_of_the_Local_Marginal_Polytope\figure_4.jpg
  Figure 4 caption: The output min-sum problem for the polytope P=lbrace ,(x,y,z)mid
    x + 2y + 2z=3; ; -x + 3y =-1; ; x,y,zge 0,rbrace .
  Figure 5 Link: articels_figures_by_rev_year\2014\Universality_of_the_Local_Marginal_Polytope\figure_5.jpg
  Figure 5 caption: Eliminating an edge crossing.
  Figure 6 Link: articels_figures_by_rev_year\2014\Universality_of_the_Local_Marginal_Polytope\figure_6.jpg
  Figure 6 caption: Planar edge crossing using three labels.
  Figure 7 Link: articels_figures_by_rev_year\2014\Universality_of_the_Local_Marginal_Polytope\figure_7.jpg
  Figure 7 caption: (a) A drawing of SumTrees for p=6 , q=2 , I1=lbrace 1,3,4,5rbrace
    , I2=lbrace 2,3,4,5,6rbrace . (b) Crossing edges between two layers.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "Daniel Pr\u016F\u0161a"
  Name of the last author: "Tom\xE1\u0161 Werner"
  Number of Figures: 7
  Number of Tables: 0
  Number of authors: 2
  Paper title: Universality of the Local Marginal Polytope
  Publication Date: 2014-08-29 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2353626
- Affiliation of the first author: faculty of science, university of ontario institute
    of technology, oshawa, on, canada
  Affiliation of the last author: faculty of science, university of ontario institute
    of technology, oshawa, on, canada
  Figure 1 Link: articels_figures_by_rev_year\2014\Stereo_Reconstruction_of_Droplet_Flight_Trajectories\figure_1.jpg
  Figure 1 caption: Approximately 1,000 2D trajectories are extracted from a single
    experiment by each camera. Roughly a quarter (shown in this image) of the extracted
    2D trajectories are matched successfully to yield 3D flight paths.
  Figure 10 Link: articels_figures_by_rev_year\2014\Stereo_Reconstruction_of_Droplet_Flight_Trajectories\figure_10.jpg
  Figure 10 caption: Distribution of the misfits (in pixels) between measurements
    and quadratic drag model in the real world experiments. For brevity, this histogram
    combines the horizontal and vertical components of the misfit vectors.
  Figure 2 Link: articels_figures_by_rev_year\2014\Stereo_Reconstruction_of_Droplet_Flight_Trajectories\figure_2.jpg
  Figure 2 caption: 'Outlier detection: a single trajectory (red) is recorded by the
    cameras zeta 1 (left) and zeta 2 (right). A first pass fits the ODE parameters
    (black) using a robustified loss function (top). The outliers are then discarded
    and a second pass, using only the inliers (green), computes the final fit (bottom).'
  Figure 3 Link: articels_figures_by_rev_year\2014\Stereo_Reconstruction_of_Droplet_Flight_Trajectories\figure_3.jpg
  Figure 3 caption: Re-projection into the two virtual cameras of some simulated trajectories.
    The dotted line represents the noisy data, the solid line is the ground truth,
    and the dashed line shows the trajectory reconstructed from the noisy data.
  Figure 4 Link: articels_figures_by_rev_year\2014\Stereo_Reconstruction_of_Droplet_Flight_Trajectories\figure_4.jpg
  Figure 4 caption: Reconstruction of a trajectory from a small amount of noisy data
    ( sigma =1.83 pixels). Note how, in the first row, the quadratic polynomial model
    overfits the available data, causing it to behave extremely badly outside of the
    interval.
  Figure 5 Link: articels_figures_by_rev_year\2014\Stereo_Reconstruction_of_Droplet_Flight_Trajectories\figure_5.jpg
  Figure 5 caption: Like Fig. 4, but without any noise added to the data.
  Figure 6 Link: articels_figures_by_rev_year\2014\Stereo_Reconstruction_of_Droplet_Flight_Trajectories\figure_6.jpg
  Figure 6 caption: Selected frames captured by the high-speed stereo camera pair
    during a porcine blood experiment. The top row shows frames 12 , 400 , 800 and
    1,000 captured by camera 1 and the bottom row shows the corresponding frames for
    camera 2. Frame resolution is 1,280 times 800 . The riot ball is visible in the
    first frame. The darker blobs are blood droplets whereas the translucent blob
    are images of the ballistic gel. The exposure time for each frame is less than
    0.7 milliseconds. For this reason, the frames are grayscale and appear to be poorly
    illuminated, even though the scene is illuminated by fourteen 500 watts lamps.
  Figure 7 Link: articels_figures_by_rev_year\2014\Stereo_Reconstruction_of_Droplet_Flight_Trajectories\figure_7.jpg
  Figure 7 caption: Reconstruction of a real trajectory. Note that, considering only
    the first 0.1 s of flight (top), the ODE models were a near perfect fit for the
    overall trajectory, while the quadratic polynomial model behaved badly. When 0.2
    s are used, the curves are very similar, but the polynomial models still deviate
    from the data.
  Figure 8 Link: articels_figures_by_rev_year\2014\Stereo_Reconstruction_of_Droplet_Flight_Trajectories\figure_8.jpg
  Figure 8 caption: Experimental setup showing ballistic gel containing transfer blood.
    Experiments are captured using a stereo camera pair capable of recording high-speed
    video.
  Figure 9 Link: articels_figures_by_rev_year\2014\Stereo_Reconstruction_of_Droplet_Flight_Trajectories\figure_9.jpg
  Figure 9 caption: Calibration verification by shining a laser at the observed region.
    In these figures, thin blue circles depict the laser dot, blue lines represent
    the epipolar line corresponding to the the laser dot in the other camera, and
    the thick red circles show the re-projection of the 3D reconstruction of the laser
    dot. The left image shows a successful calibration. Here, the epipolar line crosses
    the laser dot and the re-projection coincides with the original location. The
    right image shows an inaccurate calibration. One of the cameras has moved, perhaps
    due to the vibrations generated by the recoil of the gun.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Luis A. Zarrabeitia
  Name of the last author: Dhavide A. Aruliah
  Number of Figures: 10
  Number of Tables: 6
  Number of authors: 3
  Paper title: Stereo Reconstruction of Droplet Flight Trajectories
  Publication Date: 2014-08-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of the Notation Used in Sections 3, 4, 5, 6, and 7
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Accuracy of the Estimated Parameters from Noiseless Data,
      Compared with the Ground Truth
  Table 3 caption:
    table_text: TABLE 3 Accuracy of the Estimated Parameters from Noisy Data, Compared
      with the Ground Truth
  Table 4 caption:
    table_text: TABLE 4 Comparison of Reconstruction Strategies
  Table 5 caption:
    table_text: TABLE 5 Prediction Accuracy, with and without Noise
  Table 6 caption:
    table_text: TABLE 6 Prediction Accuracy for Real Paths
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2353638
- Affiliation of the first author: school of computer science and software engineering,
    the university of western australia, perth, wa, australia
  Affiliation of the last author: school of computer science and software engineering,
    the university of western australia, perth, wa, australia
  Figure 1 Link: articels_figures_by_rev_year\2014\Deep_Reconstruction_Models_for_Image_Set_Classification\figure_1.jpg
  Figure 1 caption: Block diagram of the proposed Deep Reconstruction Models based
    image set classification framework. The framework constitutes training and testing.
    During training, we first define a Template DRM (TDRM) and initialize its parameters
    by unsupervised pre-training using Gaussian Restricted Boltzmann Machines. The
    initialized TDRM is then separately trained with training images of each class
    to learn class-specific DRMs. During testing, the learnt DRMs are used to reconstruct
    images of a test image set and a voting strategy is adopted for classification.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\Deep_Reconstruction_Models_for_Image_Set_Classification\figure_2.jpg
  Figure 2 caption: 'Structure of the Template Deep Reconstruction Model. The TDRM
    is based on an auto-encoder and has two parts: an encoder and a symmetric decoder.
    The encoder finds a low dimensional meaningful representation of the input data
    which is then used by the decoder to reconstruct the original input.'
  Figure 3 Link: articels_figures_by_rev_year\2014\Deep_Reconstruction_Models_for_Image_Set_Classification\figure_3.jpg
  Figure 3 caption: Performance Curves for different methods on all datasets. The
    CMC curves (a-h) show that the proposed method achieves the highest identification
    rates for all ranks on most of the dataset. The ROC curves for face verification
    experiments on YTC dataset in (i) show that the proposed method significantly
    outperforms the others. Figure best seen in colors.
  Figure 4 Link: articels_figures_by_rev_year\2014\Deep_Reconstruction_Models_for_Image_Set_Classification\figure_4.jpg
  Figure 4 caption: Example images of a person from PubFig dataset.
  Figure 5 Link: articels_figures_by_rev_year\2014\Deep_Reconstruction_Models_for_Image_Set_Classification\figure_5.jpg
  Figure 5 caption: Sample frames from videos of a person in the YTF database.
  Figure 6 Link: articels_figures_by_rev_year\2014\Deep_Reconstruction_Models_for_Image_Set_Classification\figure_6.jpg
  Figure 6 caption: Example images of a person from COX dataset.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.6
  Name of the first author: Munawar Hayat
  Name of the last author: Senjian An
  Number of Figures: 6
  Number of Tables: 4
  Number of authors: 3
  Paper title: Deep Reconstruction Models for Image Set Classification
  Publication Date: 2014-08-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance on HondaUCSD Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance Evaluation of All Methods on Different Datasets
  Table 3 caption:
    table_text: TABLE 3 Equal Error Rates of Different Methods
  Table 4 caption:
    table_text: TABLE 4 Timing Analysis
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2353635
- Affiliation of the first author: department of electrical engineering, laboratory
    of signal processing and speech communication, graz university of technology,
    styria, austria
  Affiliation of the last author: department of electrical engineering, laboratory
    of signal processing and speech communication, graz university of technology,
    styria, austria
  Figure 1 Link: articels_figures_by_rev_year\2014\On_Bayesian_Network_Classifiers_with_Reduced_Precision_Parameters\figure_1.jpg
  Figure 1 caption: 'Quantization error of log (Y) , where Y is beta-distributed.
    Error histograms are computed from 105 samples. Left: Y sim mathrmBetaleft( 1cdot
    103,9cdot 103 right) ; Right: Y sim mathrmBeta,left( 1cdot 105,9 cdot 105 right)
    ; in both cases mathbb Etextrm P(Y) left[ log Y right]=-2.3026 ; q is the quantization
    interval width; average quantization error (vertical red line).'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\On_Bayesian_Network_Classifiers_with_Reduced_Precision_Parameters\figure_2.jpg
  Figure 2 caption: "CRs and bounds on the CRs of BNCs with reduced precision parameters\
    \ for varying number of bits; worst-case bounds (grey \xD7 ), probabilistic bound\
    \ (cyan \xD76.24+ ), probabilistic bound assuming uniform quantization error (black\
    \ \u25A1 ), reduced precision MAP parameters (blue + ), full-precision MAP parameters\
    \ (green, no marker)."
  Figure 3 Link: articels_figures_by_rev_year\2014\On_Bayesian_Network_Classifiers_with_Reduced_Precision_Parameters\figure_3.jpg
  Figure 3 caption: Sample size dependence of the proposed performance bounds for
    MNIST data and BNCs with TAN-CMI structure; worst-case bounds (dashed grey), probabilistic
    bounds (dotted cyan), probabilistic bounds assuming uniform quantization error
    (dash-dotted black), reduced precision MAP parameters (solid blue), full-precision
    MAP parameters (green, horizontal); bounds are learned on 10 , 20 , 50 and 100
    percent of the training data (from bottom to top).
  Figure 4 Link: articels_figures_by_rev_year\2014\On_Bayesian_Network_Classifiers_with_Reduced_Precision_Parameters\figure_4.jpg
  Figure 4 caption: CRs of BNCs with MAP and MM parameters over varying number of
    bits; MAP parameters (red); MM parameters (blue).
  Figure 5 Link: articels_figures_by_rev_year\2014\On_Bayesian_Network_Classifiers_with_Reduced_Precision_Parameters\figure_5.jpg
  Figure 5 caption: 'CRs of BNCs with MAP parameters over varying number of bits,
    TAN-CMI and TAN-MM structures; Left: MNIST, Right: DC-Mall.'
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Sebastian Tschiatschek
  Name of the last author: Franz Pernkopf
  Number of Figures: 5
  Number of Tables: 4
  Number of authors: 2
  Paper title: On Bayesian Network Classifiers with Reduced Precision Parameters
  Publication Date: 2014-08-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Number of Parameters of BNCs for Different Datasets and Structures
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of the CRs of BNCs with MAP and MM Parameters;
      a Plus (Minus) Sign Indicates That for the Corresponding DatasetStructureNumber
      of Bits BNCs with MM (MAP) Parameters Have a Significantly Higher CR
  Table 3 caption:
    table_text: TABLE 3 Comparison of the Robustness of BNCs with MAP and MM Parameters;
      a Plus (Minus) Sign Indicates That for the Corresponding DatasetStructureNumber
      of Bits BNCs with MM (MAP) Parameters Are Significantly More Robust
  Table 4 caption:
    table_text: TABLE 4 Comparison of the Robustness of BNCs with TAN-CMI and TAN-MM
      Structures and MAP Parameters; a Plus Sign Indicates That for the Corresponding
      DatasetNumber of Bits BNCs with TAN-MM Structure Are Significantly More Robust
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2353620
