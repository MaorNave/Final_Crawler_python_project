- Affiliation of the first author: department of electronic engineering, shanghai
    jiao tong university, shanghai, china
  Affiliation of the last author: huawei inc., shenzhen, guangdong, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Seed_the_Views_Hierarchical_Semantic_Alignment_for_Contrastive_Representation_Le\figure_1.jpg
  Figure 1 caption: An overall performance comparison of the proposed pre-trained
    model on several widely benchmarks for self-supervised evaluation. Here, a standard
    ResNet-50 is used as backbone. CSML significantly improves the performance on
    various tasks.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Seed_the_Views_Hierarchical_Semantic_Alignment_for_Contrastive_Representation_Le\figure_2.jpg
  Figure 2 caption: "An overview of the proposed hierarchical semantic alignment framework.\
    \ The baseline is based on MoCo v2 [11], which requires a query encoder f q and\
    \ an asynchronously updated key encoder f k . Given an anchor image x a , we randomly\
    \ select a positive sample x p from the nearest neighborhood set \u03A9 , and\
    \ generate the mixed sample x . The hierarchical semantic alignment is enforced\
    \ by pulling the positive samples x a , x p , and x in the intermediate layers\
    \ as well as the last embedding space."
  Figure 3 Link: articels_figures_by_rev_year\2022\Seed_the_Views_Hierarchical_Semantic_Alignment_for_Contrastive_Representation_Le\figure_3.jpg
  Figure 3 caption: (a) Top-1 accuracy with different k in knn. (b) Top-1 accuracy
    and computational complexity comparisons for different models. (c) Top-1 accuracy
    comparisons for different training epochs of CSML and MoCo v2. The results of
    MoCo v2 are pretrained with 1024 batch size.
  Figure 4 Link: articels_figures_by_rev_year\2022\Seed_the_Views_Hierarchical_Semantic_Alignment_for_Contrastive_Representation_Le\figure_4.jpg
  Figure 4 caption: Top-1 linear classification accuracy of different methods at different
    stages using ResNet-50 as backbone.
  Figure 5 Link: articels_figures_by_rev_year\2022\Seed_the_Views_Hierarchical_Semantic_Alignment_for_Contrastive_Representation_Le\figure_5.jpg
  Figure 5 caption: Training curves of CSML (with and without multi-crop augmentation)
    and MoCo v2. The Top-20 KNN accuracy on ImageNet are reported.
  Figure 6 Link: articels_figures_by_rev_year\2022\Seed_the_Views_Hierarchical_Semantic_Alignment_for_Contrastive_Representation_Le\figure_6.jpg
  Figure 6 caption: (a) Top-1 accuracy comparisons on different categories of CSML
    and fully supervised model. Here we compare most successful categories of each
    method. (b) An illustration of the selected KNN samples using CSML and the supervised
    model. For each class, the first rows samples are selected by unsupervised model
    CSML, the second rows samples are selected by supervised model. The supervised
    model usually focuses on discriminative details such as texture, while the unsupervised
    model CSML focuses on contour information for discrimination. The advantage of
    supervised model lies in discriminating fine-grained sub-categories, e.g., supervised
    model selects the most KNN samples of the same class as mud turtle, while unsupervised
    model CSML selects other sub-categories of turtle, such as box turtle and terrapin.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Haohang Xu
  Name of the last author: Qi Tian
  Number of Figures: 6
  Number of Tables: 20
  Number of authors: 7
  Paper title: 'Seed the Views: Hierarchical Semantic Alignment for Contrastive Representation
    Learning'
  Publication Date: 2022-05-23 00:00:00
  Table 1 caption: TABLE 1 Top-1 Accuracy Under Linear Classification on ImageNet
    With ResNet-50 as Backbone
  Table 10 caption: TABLE 10 Transfer Learning Results Under Linear Classification
    on Small Scale Downstream Datasets
  Table 2 caption: TABLE 2 KNN Classification Accuracy on ImageNet
  Table 3 caption: TABLE 3 Top-1 Accuracy of Different Backbones Under Linear Evaluation
    on ImageNet
  Table 4 caption: TABLE 4 Top-1 Accuracy on ImageNet Under Linear Evaluation When
    Applying CSML to BYOL, SwAV and DINO, ResNet-50 is Adopted as Backbone
  Table 5 caption: TABLE 5 Transfer Learning Results on PASCAL VOC Detection (Averaged
    Over 5 Trials)
  Table 6 caption: TABLE 6 Semi-Supervised Learning by Fine-Tuning 1% and 10% Labeled
    Images on ImageNet
  Table 7 caption: TABLE 7 Top-1 Accuracy Under Linear Classification on Places205
    Dataset [59]
  Table 8 caption: "TABLE 8 Transfer Learning Accuracy (%) on COCO Detection and COCO\
    \ Instance Segmentation (Averaged Over 5 Trials) With 1\xD7 1\xD7 Learning Schedule"
  Table 9 caption: "TABLE 9 Transfer Learning Accuracy (%) on COCO Detection and COCO\
    \ Instance Segmentation (Averaged Over 5 Trials) With 2\xD7 2\xD7 Learning Schedule"
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3176690
- Affiliation of the first author: cnr-imati, consiglio nazionale delle ricerche,
    istituto di matematica applicata e tecnologie informatiche genova, genova, italy
  Affiliation of the last author: cnr-imati, consiglio nazionale delle ricerche, istituto
    di matematica applicata e tecnologie informatiche genova, genova, italy
  Figure 1 Link: articels_figures_by_rev_year\2022\FourierBased_and_Rational_Graph_Filters_for_Spectral_Processing\figure_1.jpg
  Figure 1 caption: "Level-sets of spectral kernels induced by the filter \u03D5(s):=(1\u2212\
    s ) \u22124 + (4s+1 ) \u22121 with (a) s:=r , (b) s:= r 2 , and (c) s:= r 3 centred\
    \ at a seed point (yellow dot). A larger value of s corresponds to a faster decay\
    \ of \u03D5 , thus providing a smaller support of the corresponding kernel. (c)\
    \ Reducing s , the filter becomes constant and the corresponding kernels resembles\
    \ the behaviour of the geodesic distance in a neighbour of the seed point."
  Figure 10 Link: articels_figures_by_rev_year\2022\FourierBased_and_Rational_Graph_Filters_for_Spectral_Processing\figure_10.jpg
  Figure 10 caption: Level-sets and colour-map of (a,right) a noisy function g:=f+delta
    achieved by adding a Gaussian noise delta to the function f in (a,left); (b) smoothed
    scalar function tildef achieved with 100 diffusion functions. (c) Approximation
    error ( y -axis) and number of diffusion functions ( x -axis) at 5 different scales.
  Figure 2 Link: articels_figures_by_rev_year\2022\FourierBased_and_Rational_Graph_Filters_for_Spectral_Processing\figure_2.jpg
  Figure 2 caption: "Input filters \u03C6 (red), real (green) and imagery (blue) parts\
    \ of its Fourier transform \u03C6 \u02C6 computed with the Fast Fourier Transform.\
    \ Level-sets of the spectral operator induced by \u03C6 , Im( \u03C6 \u02C6 )\
    \ , and Real( \u03C6 \u02C6 ) : (first row) \u03C6(s):= s \u22122 , (second row)\
    \ \u03C6(s):= s 2 ."
  Figure 3 Link: articels_figures_by_rev_year\2022\FourierBased_and_Rational_Graph_Filters_for_Spectral_Processing\figure_3.jpg
  Figure 3 caption: "At small scales ( t= 10 \u22123 , t= 10 \u22122 ), the distribution\
    \ of the level-sets of the diffusion kernel have a local and multi-scale behaviour\
    \ at the same seed points (a,b), (c,d). (e,f) At large scales ( t= 10 \u22122\
    \ , t=0.5 , t=1 ), the diffusion kernels are no more centred at the seed points\
    \ and have an analogous global behaviour (e,f). The kernel has been approximated\
    \ with a rational polynomial of the exp filter of degree (r,r) , r:=7 (c.f. Eq\
    \ (20))."
  Figure 4 Link: articels_figures_by_rev_year\2022\FourierBased_and_Rational_Graph_Filters_for_Spectral_Processing\figure_4.jpg
  Figure 4 caption: Level-sets of bi-harmonic functions centred at 3 seed points on
    flat (spine) and rounded (harm, leg) surface regions.
  Figure 5 Link: articels_figures_by_rev_year\2022\FourierBased_and_Rational_Graph_Filters_for_Spectral_Processing\figure_5.jpg
  Figure 5 caption: "Input graph, colour-map, and level-sets of spectral kernels induced\
    \ by (a) an analytic and (b-d) diffusion filters. (b-d) At small scales ( t= 10\
    \ \u22123 , t= 10 \u22122 ), the distribution of the level-sets of the diffusion\
    \ kernels have a local and multi-scale behaviour at the same seeds. At large scales\
    \ ( t= 10 \u22121 ), diffusion kernels are no more centred at seeds and have an\
    \ analogous global behaviour."
  Figure 6 Link: articels_figures_by_rev_year\2022\FourierBased_and_Rational_Graph_Filters_for_Spectral_Processing\figure_6.jpg
  Figure 6 caption: "Level sets of the spectral distances from a source point (white\
    \ dot) induced by the filter varphi and evaluated with the Pad\xE9-Chebyshev approximation\
    \ ( r=5 )."
  Figure 7 Link: articels_figures_by_rev_year\2022\FourierBased_and_Rational_Graph_Filters_for_Spectral_Processing\figure_7.jpg
  Figure 7 caption: Level-sets of rational graph filters centred at a seed point mathbf
    p (black dot on the elbow) induced by the Chebyshev rational basis Rk(Delta)delta
    mathbf p with an increasing degree.
  Figure 8 Link: articels_figures_by_rev_year\2022\FourierBased_and_Rational_Graph_Filters_for_Spectral_Processing\figure_8.jpg
  Figure 8 caption: Colormap and level-sets of the diffusion wavelet at a source point
    computed with the P.-C. approximation and the trucated approximation with 100
    eigenfunctions, which is affected by undulations far from the source point and
    at small scales.
  Figure 9 Link: articels_figures_by_rev_year\2022\FourierBased_and_Rational_Graph_Filters_for_Spectral_Processing\figure_9.jpg
  Figure 9 caption: ell infty -approximation error ( y -axis) of the reconstruction
    of the geometry of 3D shapes with respect to an increasing number ( x -axis) of
    Laplacian eigenfunctions (red curve), harmonic (blue curve), and diffusion (green
    curve) kernels. Indeed, harmonic and diffusion kernels are a valid alternative
    to the eigenfunctions, with additional properties; in fact, they can be centred
    at any seed point and diffusion kernels have a multi-scale local behaviour.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "Giuseppe Patan\xE8"
  Name of the last author: "Giuseppe Patan\xE8"
  Number of Figures: 12
  Number of Tables: 0
  Number of authors: 1
  Paper title: Fourier-Based and Rational Graph Filters for Spectral Processing
  Publication Date: 2022-05-23 00:00:00
  Table 1 caption: Not Available
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3177075
- Affiliation of the first author: school of electronic engineering, xidian university,
    xian, china
  Affiliation of the last author: department of computer science and engineering,
    texas a&m university, college station, tx, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\Group_Contrastive_SelfSupervised_Learning_on_Graphs\figure_1.jpg
  Figure 1 caption: The pipelines of GraphCL and GroupCL. These two methods share
    the same trunk, in which the data are successively processed by the data augmentation
    and the GNN encoder. GroupCL employs a group generator R Q to generate multiple
    graph-level embeddings for two views. The maximizing MI objectives are employed
    on the representations in the same group from different views. The minimizing
    MI objectives are employed on the representations in different groups from the
    same view. GraphCL adopts a sum pooling and projection head to generate one graph
    embedding.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Group_Contrastive_SelfSupervised_Learning_on_Graphs\figure_2.jpg
  Figure 2 caption: The computation pipeline of R Q . It takes the node embedding
    matrix U as input and outputs multiple graph-level embeddings. The node embedding
    matrix is firstly projected to the Key and Value matrices by two independent linear
    functions. Each vector in the Query matrix is involved in a dot product with the
    Key, leading to an attention weights vector. Then the attention vector is used
    to perform a weighted sum over the Value, generating one embedding.
  Figure 3 Link: articels_figures_by_rev_year\2022\Group_Contrastive_SelfSupervised_Learning_on_Graphs\figure_3.jpg
  Figure 3 caption: The cosine distance of queries of PROTEINS dataset (left) and
    MUTAG dataset (right) on the unsupervised learning task.
  Figure 4 Link: articels_figures_by_rev_year\2022\Group_Contrastive_SelfSupervised_Learning_on_Graphs\figure_4.jpg
  Figure 4 caption: Visualization of attention weights of BACE dataset on the transfer
    learning task. The first three rows exhibit the independent results of three representations,
    in which the nodes being paid large attention are highlighted by the red, green,
    and cyan circles correspondingly. The last row shows the results of all-representation
    attentions on one graph.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.78
  Name of the first author: Xinyi Xu
  Name of the last author: Shuiwang Ji
  Number of Figures: 4
  Number of Tables: 6
  Number of authors: 4
  Paper title: Group Contrastive Self-Supervised Learning on Graphs
  Publication Date: 2022-05-23 00:00:00
  Table 1 caption: TABLE 1 The Results of the Unsupervised Learning Experiment. We
    Run 10-Folder Cross-Validation and Report the Mean and the Standard Deviation
    of the Classification Accuracy
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The Results of the Transfer Learning Experiment. We Run
    10-Folder Cross-Validation and Report the Mean and the Standard Deviation of the
    ROC-AUC Scores
  Table 3 caption: TABLE 3 Ablation Study on MI Upper Bound Estimator. We Run 10-Folder
    Cross-Validation and Report the Mean and the Standard Deviation of the Classification
    Accuracy. The Better Performance is Highlighted by the Bold Number
  Table 4 caption: TABLE 4 Ablation Study on the Number of Groups p p. We Run 10-Folder
    Cross-Validation and Report the Mean and the Standard Deviation of the Classification
    Accuracy
  Table 5 caption: "TABLE 5 Ablation Study on Diversity Loss Weight \u03BB \u03BB"
  Table 6 caption: TABLE 6 Ablation Study on Objective Functions
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3177295
- Affiliation of the first author: department of computing, the hong kong polytechnic
    university, hung hom, hong kong
  Affiliation of the last author: department of computing, the hong kong polytechnic
    university, hung hom, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2022\MMNet_A_ModelBased_Multimodal_Network_for_Human_Action_Recognition_in_RGBD_Video\figure_1.jpg
  Figure 1 caption: Difficult action pairs (e.g., Actions 11 and 12, Actions 12 and
    30, and Actions 31 and 53) in the NTU RGB+D dataset that confuse skeleton-based
    models. The goal of this paper is to capture complementary features from the RGB
    modality to compensate for the limitation of skeleton-based methods.
  Figure 10 Link: articels_figures_by_rev_year\2022\MMNet_A_ModelBased_Multimodal_Network_for_Human_Action_Recognition_in_RGBD_Video\figure_10.jpg
  Figure 10 caption: Visualization of skeleton-focused representations with different
    sampling lengths of the RGB modality (ST-ROI is normalized).
  Figure 2 Link: articels_figures_by_rev_year\2022\MMNet_A_ModelBased_Multimodal_Network_for_Human_Action_Recognition_in_RGBD_Video\figure_2.jpg
  Figure 2 caption: "Architecture of proposed MMNet. B (i) , J (i) , and V (i) represent\
    \ the inputs of skeleton bones, skeleton joints, and RGB video, respectively.\
    \ w (i) are spatial attention weights derived from the graph representation of\
    \ the skeleton joints J (i) , which guides the focus of ST-ROI transformed from\
    \ RGB video input V (i) . After this model-based data fusion, the skeleton-focused\
    \ ST-ROI R \u2032 (i) will be fed to the ResNet to generate a modality-specific\
    \ prediction. y J (i) c and y B (i) c denote respective predictions from skeleton\
    \ joint and bone streams, which are aggregated through the modality-specific prediction\
    \ of the RGB modality y V (i) c to deliver the ensemble recognition result."
  Figure 3 Link: articels_figures_by_rev_year\2022\MMNet_A_ModelBased_Multimodal_Network_for_Human_Action_Recognition_in_RGBD_Video\figure_3.jpg
  Figure 3 caption: Construction process of ST-ROI. The top case has one subject,
    and the bottom case has two subjects. Both cases are based on the OpenPose 2D
    skeleton.
  Figure 4 Link: articels_figures_by_rev_year\2022\MMNet_A_ModelBased_Multimodal_Network_for_Human_Action_Recognition_in_RGBD_Video\figure_4.jpg
  Figure 4 caption: '(a) Structure of spatiotemporal skeleton graph. (b) Spatial sampling
    strategy of graph convolutional network. Different colors denote different subsets:
    green stars denote the vertex itself; yellow triangles denote the farther centrifugal
    subset; blue squares denote the closer centripetal subset.'
  Figure 5 Link: articels_figures_by_rev_year\2022\MMNet_A_ModelBased_Multimodal_Network_for_Human_Action_Recognition_in_RGBD_Video\figure_5.jpg
  Figure 5 caption: Model-based fusion scheme of our MMNet. It constructs a skeleton-focused
    representation of the RGB modality by multiplying joint weights by the ST-ROI.
  Figure 6 Link: articels_figures_by_rev_year\2022\MMNet_A_ModelBased_Multimodal_Network_for_Human_Action_Recognition_in_RGBD_Video\figure_6.jpg
  Figure 6 caption: Recognition accuracy per action on NTU RGB+D 60 (X-Sub). Action
    names in red and underlined are relatively more difficult actions.
  Figure 7 Link: articels_figures_by_rev_year\2022\MMNet_A_ModelBased_Multimodal_Network_for_Human_Action_Recognition_in_RGBD_Video\figure_7.jpg
  Figure 7 caption: Confusion matrices of MS-G3D and ensemble (12+6) on NTU RGB+D
    120 with X-Sub protocol. Darker color in off-diagonal areas on the right side
    confusion matrix comparing with the left one indicates the improvements.
  Figure 8 Link: articels_figures_by_rev_year\2022\MMNet_A_ModelBased_Multimodal_Network_for_Human_Action_Recognition_in_RGBD_Video\figure_8.jpg
  Figure 8 caption: Visualization of neuron activation values for different skeleton
    joints along their temporal positions and cropped sample frames of video input.
    This visualization shows the idea of selecting top- t positions to calculate joint
    weights for their corresponding body areas.
  Figure 9 Link: articels_figures_by_rev_year\2022\MMNet_A_ModelBased_Multimodal_Network_for_Human_Action_Recognition_in_RGBD_Video\figure_9.jpg
  Figure 9 caption: Improvement of our MMNet over baseline model CTR-GCN on the Toyota
    Smarthome dataset (21 of 31 actions are improved).
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Bruce X.B. Yu
  Name of the last author: Keith C.C. Chan
  Number of Figures: 10
  Number of Tables: 16
  Number of authors: 5
  Paper title: 'MMNet: A Model-Based Multimodal Network for Human Action Recognition
    in RGB-D Videos'
  Publication Date: 2022-05-26 00:00:00
  Table 1 caption: TABLE 1 Ablation Study for NTU RGB+D With X-Sub and X-View Protocols
  Table 10 caption: TABLE 10 Ablation Study for Northwestern-UCLA Multiview With Three
    Cross-View Settings
  Table 2 caption: TABLE 2 Improvements in Actions of NTU RGB+D That are Difficult
    to Address Using Skeleton-Based Methods
  Table 3 caption: TABLE 3 Comparison of NTU RGB+D With X-Sub and X-View Protocols
  Table 4 caption: TABLE 4 Action Recognition Improvements on NTU RGB+D 120 Dataset
    by Aggregating the Results of Ensemble (12+6) Compared With the Top 10 Accurate
    and Confused Actions of Skeleton-Based Method MS-G3D.
  Table 5 caption: TABLE 5 Ablation Study for NTU RGB+D 120 With X-Sub and X-Set Protocols
  Table 6 caption: TABLE 6 Ablation Study for Comparison With VPN on NTU RGB+D 120
    Under X-Sub and X-Set Protocols
  Table 7 caption: TABLE 7 Comparison of NTU RGB+D 120 With X-Sub and X-Set Protocols
  Table 8 caption: TABLE 8 Ablation Study for PKU-MMD With X-Sub and X-View Protocols
  Table 9 caption: TABLE 9 Comparison of PKU-MMD With X-Sub and X-View Protocols
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3177813
- Affiliation of the first author: college of information science and technology,
    jinan university, guangzhou, china
  Affiliation of the last author: department of mathematics, university of hong kong,
    hong kong, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Hypergraph_Collaborative_Network_on_Vertices_and_Hyperedges\figure_1.jpg
  Figure 1 caption: An illustration of the difference between a standard graph and
    a hypergraph. (a) is a paper-authorship network representing the author list of
    each paper. (b) is a pair-wise standard graph having a link between two papers
    if they share at least one common author. (c) is a hypergraph revealing a comprehensive
    view of the relations across papers and authors, different colors mean different
    hyperedges.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Hypergraph_Collaborative_Network_on_Vertices_and_Hyperedges\figure_2.jpg
  Figure 2 caption: "An overview of the proposed HCoN model. The superscripts \u201C\
    (0),\u201D \u201C(1),\u201D and \u201C(2)\u201D denote different layers. The latent\
    \ vertex representations are learned by aggregating the information from previous\
    \ vertices and hyperedges via network parameters Q v and Q e . Similarly, the\
    \ latent hyperedge representations are obtained by aggregating the information\
    \ from previous vertices and hyperedges via network parameters P v and P e . The\
    \ notations X (2) and Y (2) are output representations of vertices and hyperedges,\
    \ which are also represented by V and E in Eq. (11), respectively."
  Figure 3 Link: articels_figures_by_rev_year\2022\Hypergraph_Collaborative_Network_on_Vertices_and_Hyperedges\figure_3.jpg
  Figure 3 caption: Results in terms of F1-score, Accuracy, and 01 Loss of different
    methods using different label rates of training samples on the Citeseer co-citation
    dataset.
  Figure 4 Link: articels_figures_by_rev_year\2022\Hypergraph_Collaborative_Network_on_Vertices_and_Hyperedges\figure_4.jpg
  Figure 4 caption: Learning curves w.r.t. objective values and evaluation metrics,
    including training and test phases, on the Cora co-authorship dataset for vertex
    and hyperedge classifications, respectively. We rescale the loss values to the
    range space [0,1] via dividing them by their corresponding maximum values. Besides,
    we plot the final scaled hypergraph reconstruction loss values.
  Figure 5 Link: articels_figures_by_rev_year\2022\Hypergraph_Collaborative_Network_on_Vertices_and_Hyperedges\figure_5.jpg
  Figure 5 caption: Parameter sensitivity analysis on the Cora co-authorship dataset.
  Figure 6 Link: articels_figures_by_rev_year\2022\Hypergraph_Collaborative_Network_on_Vertices_and_Hyperedges\figure_6.jpg
  Figure 6 caption: Visualization of vertex samples represented by original features
    and different graph learning features on the Citeseer and Pumbed co-citation and
    Cora and DBLP co-authorship datasets for vertex classification. The color indicates
    the class label in dataset.
  Figure 7 Link: articels_figures_by_rev_year\2022\Hypergraph_Collaborative_Network_on_Vertices_and_Hyperedges\figure_7.jpg
  Figure 7 caption: Visualization of hyperedge samples represented by different graph
    learning features on the Pumbed co-citation dataset for hyperedge classification.
    The color indicates the class label in dataset.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.75
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Hanrui Wu
  Name of the last author: Michael Kwok-Po Ng
  Number of Figures: 7
  Number of Tables: 9
  Number of authors: 3
  Paper title: Hypergraph Collaborative Network on Vertices and Hyperedges
  Publication Date: 2022-05-26 00:00:00
  Table 1 caption: TABLE 1 Notations and Their Descriptions Used in This Paper
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Dataset Statistics Collected From [7]
  Table 3 caption: "TABLE 3 Results in Terms of Accuracy (%, Mean \xB1 Std. Deviation)\
    \ of Different Methods on the Semi-Supervised Vertex Classification Problem"
  Table 4 caption: "TABLE 4 Results (%, Mean \xB1 Std. Deviation) in Terms of Precision\
    \ ( \u2191 \u2191), Recall ( \u2191 \u2191), F1-Score ( \u2191 \u2191), Accuracy\
    \ ( \u2191 \u2191), Match Ratio ( \u2191 \u2191), and 01 Loss ( \u2193 \u2193\
    ) of Different Methods on the Semi-Supervised Hyperedge Classification Problem"
  Table 5 caption: "TABLE 5 Ablation Study in Terms of Accuracy (%, Mean \xB1 Std.\
    \ Deviation) on the Semi-Supervised Vertex Classification Problem"
  Table 6 caption: "TABLE 6 Ablation Study in Terms of Precision ( \u2191 \u2191),\
    \ Recall ( \u2191 \u2191), F1-Score ( \u2191 \u2191), Accuracy ( \u2191 \u2191\
    ), Match Ratio ( \u2191 \u2191), and 01 Loss ( \u2193 \u2193) on the Semi-Supervised\
    \ Hyperedge Classification Problem"
  Table 7 caption: "TABLE 7 Necessity Study in Terms of Accuracy (%, Mean \xB1 Std.\
    \ Deviation) on the Citeseer and Pubmed Co-Citation Datasets"
  Table 8 caption: "TABLE 8 Results (%, Mean \xB1 Std. Deviation) in Terms of Precision\
    \ ( \u2191 \u2191), Recall ( \u2191 \u2191), F1-Score ( \u2191 \u2191), Accuracy\
    \ ( \u2191 \u2191), Match Ratio ( \u2191 \u2191), and 01 Loss ( \u2193 \u2193\
    ) of Different Methods Using Different Label Rates of Training Samples on the\
    \ Citeseer Co-Citation Dataset for the Semi-Supervised Hyperedge Classification\
    \ Problem"
  Table 9 caption: TABLE 9 Wall-Clock Time (s) of Different Methods on the Pubmed
    Co-Citation Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3178156
- Affiliation of the first author: department of automation and the moe key laboratory
    of system control and information processing, shanghai jiao tong university, shanghai,
    china
  Affiliation of the last author: department of automation and the moe key laboratory
    of system control and information processing, shanghai jiao tong university, shanghai,
    china
  Figure 1 Link: articels_figures_by_rev_year\2022\Low_Dimensional_Trajectory_Hypothesis_is_True_DNNs_Can_Be_Trained_in_Tiny_Subspa\figure_1.jpg
  Figure 1 caption: "There are three parameters w(1),w(2),w(3) to optimize. But the\
    \ training trajectory w i i=0,\u2026,t could be in a two-dimensional subspace\
    \ spanned by e 1 and e 2 . If so, training in the low-dimensional space can have\
    \ comparable performance as training in the high-dimensional space."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Low_Dimensional_Trajectory_Hypothesis_is_True_DNNs_Can_Be_Trained_in_Tiny_Subspa\figure_2.jpg
  Figure 2 caption: Experiments for ResNet8 trained on CIFAR-10. (a) the PCA ratio
    of the first 30 epochs training trajectory in the principal directions. (b) the
    training performance of SGD and P-SGD (in 15 dimensional subspace).
  Figure 3 Link: articels_figures_by_rev_year\2022\Low_Dimensional_Trajectory_Hypothesis_is_True_DNNs_Can_Be_Trained_in_Tiny_Subspa\figure_3.jpg
  Figure 3 caption: 'The accuracy of ResNet20 on CIFAR-10 in different epochs. (a)
    SGD on 0.27M parameters: DLDR samples the first 50 epochs and there is learning
    rate decay after 100 epochs. (b) P-BFGS on 40 variables: trains from the initial,
    P-BFGS could surpass the DLDR sampling stage very quickly and achieve similar
    performance as (a).'
  Figure 4 Link: articels_figures_by_rev_year\2022\Low_Dimensional_Trajectory_Hypothesis_is_True_DNNs_Can_Be_Trained_in_Tiny_Subspa\figure_4.jpg
  Figure 4 caption: Wall-clock time consumption comparisons.
  Figure 5 Link: articels_figures_by_rev_year\2022\Low_Dimensional_Trajectory_Hypothesis_is_True_DNNs_Can_Be_Trained_in_Tiny_Subspa\figure_5.jpg
  Figure 5 caption: The performance under different levels of label noise.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.87
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.61
  Name of the first author: Tao Li
  Name of the last author: Xiaolin Huang
  Number of Figures: 5
  Number of Tables: 5
  Number of authors: 6
  Paper title: 'Low Dimensional Trajectory Hypothesis is True: DNNs Can Be Trained
    in Tiny Subspaces'
  Publication Date: 2022-05-26 00:00:00
  Table 1 caption: 'TABLE 1 Performance of Training ResNet8 in Low-Dimensional Subspaces
    for CIFAR-10 (Optimizer: SGD)'
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Test Accuracy on CIFAR-100 for Regular Training and Optimization
    in 40D Subspaces
  Table 3 caption: TABLE 3 Performance on Different Datasets (epochs)
  Table 4 caption: TABLE 4 The Effects of Independent Variables Number
  Table 5 caption: TABLE 5 P-BFGS on Well-Training Stage on ImageNet
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3178101
- Affiliation of the first author: college of computer science, sichuan university,
    chengdu, china
  Affiliation of the last author: college of computer science, sichuan university,
    chengdu, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Unsupervised_Contrastive_CrossModal_Hashing\figure_1.jpg
  Figure 1 caption: The limitations of existing methods. In this figure, we take a
    query as an example. (a) Shows the differencegap between contrastive learning
    (CL) and cross-modal hashing (CMH). More specifically, CL is optimized with continuous
    values in a differentiable manner. On the contrary, CMH is performed with binary
    codes and could not compute the gradients. In the figure, q and k are the query
    and key samples, respectively. (b) Shows the traditional max-margin ranking will
    ignore the cross-modal samples outside the margin, leading to more attention paid
    to false-negative ones (red circles). In the figure, green lines represent relevantpositive
    cross-modal correlations; orange lines represent irrelevantnegative cross-modal
    correlations; gray lines denote ignored correlations; blue items represent query
    samples; green items denote true-negative samples, and red items represent false-negative
    points.
  Figure 10 Link: articels_figures_by_rev_year\2022\Unsupervised_Contrastive_CrossModal_Hashing\figure_10.jpg
  Figure 10 caption: Convergence curves of our UCCH on the validation sets of IAPR
    TC-12 and MS-COCO. The code length is 128.
  Figure 2 Link: articels_figures_by_rev_year\2022\Unsupervised_Contrastive_CrossModal_Hashing\figure_2.jpg
  Figure 2 caption: The pipeline of the proposed method and we take a bimodal case
    as an example. In the example, two modality-specific networks learn unified binary
    representations for different modalities. The outputs of networks directly interact
    with the hash codes to learn the latent discrimination by using instance-level
    contrast without continuous relaxation, i.e., contrastive hashing learning ( L
    c ). The cross-modal ranking loss L r is utilized to bridge cross-modal hashing
    learning to cross-modal retrieval.
  Figure 3 Link: articels_figures_by_rev_year\2022\Unsupervised_Contrastive_CrossModal_Hashing\figure_3.jpg
  Figure 3 caption: Contrastive hashing learning adopts a contrastive loss to train
    the modality-specific networks by matching an image-text query ( x i , y i ) to
    a dictionary that is sampled from the hashing memory bank. By contrasting with
    the positive and negative keys, relevant cross-modal pairs directly approximate
    the corresponding unified binary codes and separate from their irrelevant pairs
    without continuous relaxation. The memory bank is driven by a momentum update
    with the corresponding pairs.
  Figure 4 Link: articels_figures_by_rev_year\2022\Unsupervised_Contrastive_CrossModal_Hashing\figure_4.jpg
  Figure 4 caption: The major difference between the max-margin ranking loss and our
    loss is that the former does not utilize the samples outside the margin whereas
    the latter does. To be specific, for a given query, one aims to retrieve the most
    relative sample from a given dictionary, where the query and dictionary lie into
    two modalities and different shapes denote different classes. Due to the existence
    of FNP, the vanilla max-margin ranking loss probably lead to wrong optimization
    result, as shown in (a). Instead of only pushing within-marginal between-pairs
    samples away, our loss simultaneously maximizes the intra-pair similarity while
    minimizing all inter-pair similarities. Thus, our ranking loss could fully utilize
    all negative samples so that the influence of FNP is alleviated.
  Figure 5 Link: articels_figures_by_rev_year\2022\Unsupervised_Contrastive_CrossModal_Hashing\figure_5.jpg
  Figure 5 caption: The precision-recall curves on the MIRFLICKR-25 K dataset. The
    code length is 128.
  Figure 6 Link: articels_figures_by_rev_year\2022\Unsupervised_Contrastive_CrossModal_Hashing\figure_6.jpg
  Figure 6 caption: The precision-recall curves on the IAPR TC-12 dataset. The code
    length is 128.
  Figure 7 Link: articels_figures_by_rev_year\2022\Unsupervised_Contrastive_CrossModal_Hashing\figure_7.jpg
  Figure 7 caption: The precision-recall curves on the NUS-WIDE dataset. The code
    length is 128.
  Figure 8 Link: articels_figures_by_rev_year\2022\Unsupervised_Contrastive_CrossModal_Hashing\figure_8.jpg
  Figure 8 caption: The precision-recall curves on the MS-COCO dataset. The code length
    is 128.
  Figure 9 Link: articels_figures_by_rev_year\2022\Unsupervised_Contrastive_CrossModal_Hashing\figure_9.jpg
  Figure 9 caption: Cross-modal retrieval performance of our UCCH in terms of MAP
    scores versus different values of beta on the validation sets of the IAPR TC-12
    and MS-COCO datasets, respectively. The code length is 128.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Peng Hu
  Name of the last author: Xi Peng
  Number of Figures: 13
  Number of Tables: 5
  Number of authors: 6
  Paper title: Unsupervised Contrastive Cross-Modal Hashing
  Publication Date: 2022-05-26 00:00:00
  Table 1 caption: TABLE 1 Performance Comparison in Terms of MAP Scores on the MIRFLICKR-25
    K and IAPR TC-12 Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Performance Comparison in Terms of MAP Scores on the NUS-WIDE
    and MS-COCO Datasets
  Table 3 caption: TABLE 3 Ablation Study on Different Datasets
  Table 4 caption: TABLE 4 Performance Comparison in Terms of Recallk Scores on Flickr30
    K
  Table 5 caption: TABLE 5 Efficiency Comparison on MIRFLICKR-25 K With 128 Code Length
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3177356
- Affiliation of the first author: center for research on intelligent perception and
    computing (cripac), national laboratory of pattern recognition (nlpr), center
    for excellence in brain science and intelligence technology (cebsit), institute
    of automation, chinese academy of sciences (casia), beijing, china
  Affiliation of the last author: center for research on intelligent perception and
    computing (cripac), national laboratory of pattern recognition (nlpr), center
    for excellence in brain science and intelligence technology (cebsit), institute
    of automation, chinese academy of sciences (casia), beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Efficient_Image_and_Sentence_Matching\figure_1.jpg
  Figure 1 caption: 'The pipeline of image and sentence matching. Given pairwise images
    and sentences (the number of pairs is N ), the goal is to obtain their cross-modal
    similarity matrix. The first stage is the feature representation, in which image
    and sentence backbone networks are used to obtain their representations, respectively.
    For images, their representations could be global image-level or local object-level
    ones, which is similar for sentences. The second stage is the similarity measurement,
    which includes two alternative methods. 1) N-to-N: simultaneously computing entire
    cross-modal similarities in a parallel manner, e.g., matrix multiplication on
    global representations [39]; and 2) 1-to-1: separately computing each or partial
    cross-modal similarities in a tangled manner, e.g., cross-modal attention on local
    representations [36].'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Efficient_Image_and_Sentence_Matching\figure_2.jpg
  Figure 2 caption: "The proposed Whitened Similarity Distillation (WSD) for efficient\
    \ image and sentence matching. Given a batch of paired images and sentences (batch\
    \ size N = 32), we feed them into the student and teacher models to predict two\
    \ cross-modal similarity matrices S and T , respectively, each of which has a\
    \ size of N\xD7N . For similarity distillation, we first obtain the differential\
    \ matrix D by subtracting T from S , and then orthogonally decompose it by the\
    \ Singular Value Decomposition (SVD) to obtain its singular values. After that,\
    \ we use one of two whitening-like transformations to remove or decay the largest\
    \ ones in either a hard or soft manner (the corresponding loss function is L hard\
    \ or L soft ). During model learning, we could also use the ranking loss L rank\
    \ based on the S to use the groundtruth information, which encourages positive\
    \ pairs of images and sentences to have larger similarities."
  Figure 3 Link: articels_figures_by_rev_year\2022\Efficient_Image_and_Sentence_Matching\figure_3.jpg
  Figure 3 caption: Plots of Bias and Variance during the model learning. The Bias
    quickly drops to near-zero at the 2-nd epoch, while the Variance fluctuates around
    large values.
  Figure 4 Link: articels_figures_by_rev_year\2022\Efficient_Image_and_Sentence_Matching\figure_4.jpg
  Figure 4 caption: Plots of original and logarithmic squared singular values. A small
    portion of original values are much larger than the others. After the logarithmic
    transformation, the value becomes more evenly distributed.
  Figure 5 Link: articels_figures_by_rev_year\2022\Efficient_Image_and_Sentence_Matching\figure_5.jpg
  Figure 5 caption: Comparison of frequency spectrums. D is the original differential
    matrix, and O 1 and O N are two orthogonal components corresponding to the largest
    and smallest singular values. We can see that the frequency spectrums of O 1 an
    O N captures the low-frequency information (in the small central area) and high-frequency
    information (in the large surrounding area), respectively.
  Figure 6 Link: articels_figures_by_rev_year\2022\Efficient_Image_and_Sentence_Matching\figure_6.jpg
  Figure 6 caption: Comparison of pseudo-covariance matrices. D T D and D D T are
    two original pseudo-covariance matrix measuring the covariances among column and
    row dimensions of D , respectively. D sT D s , D s D sT , D hT D h , and D h D
    hT are new pseudo-covariance matrices after soft and hard transformations, respectively.
    We can see that the diagonal values after transformations are more balanced and
    highlighted.
  Figure 7 Link: articels_figures_by_rev_year\2022\Efficient_Image_and_Sentence_Matching\figure_7.jpg
  Figure 7 caption: "Comparison of testing time. (a) Scatter plot of accuracy versus\
    \ testing time on the Flickr30k dataset. Our WSD and WSD \u2217 are much faster\
    \ than the others. (b) Time of feature representation by different models. Our\
    \ WSD can significantly reduce the time of feature representation. (c) Time of\
    \ similarity measurement by different models. The 1-to-1 models including SCAN,\
    \ IMRAM, GSMN and ADAPT are very slow while the fastest model is SAEM."
  Figure 8 Link: articels_figures_by_rev_year\2022\Efficient_Image_and_Sentence_Matching\figure_8.jpg
  Figure 8 caption: Failure cases of image retrieval by the WSD (ADAPT). The numbers
    in the top left corner are returned rankings of matched images (the smaller, the
    better) by the WSD (ADAPT), while those by its teacher model are all ones.
  Figure 9 Link: articels_figures_by_rev_year\2022\Efficient_Image_and_Sentence_Matching\figure_9.jpg
  Figure 9 caption: Plot of accuracy (in mR) versus model size (in Param) of all the
    distilled student models on the Flickr30k dataset.
  First author gender probability: 0.76
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Yan Huang
  Name of the last author: Liang Wang
  Number of Figures: 9
  Number of Tables: 10
  Number of authors: 3
  Paper title: Efficient Image and Sentence Matching
  Publication Date: 2022-05-27 00:00:00
  Table 1 caption: TABLE 1 The Configurations of Different State-of-the-Art Models
  Table 10 caption: TABLE 10 Comparison of Student Models Distilled From Different
    Teacher Models on the Flickr30k Dataset
  Table 2 caption: TABLE 2 Comparison With the State-of-the-Art Similarity Distillation
    Models on the Flickr30k Dataset
  Table 3 caption: TABLE 3 The Configurations of the Teacher Model and Four Different
    Student Models
  Table 4 caption: TABLE 4 Complementary With Ranking Loss on the Flickr30k Dataset
  Table 5 caption: TABLE 5 Comparison With the State-of-the-Art Methods on the Flickr30k
    and MSCOCO Datasets
  Table 6 caption: TABLE 6 Comparison With Two VSRN Variants Using Efficient Object
    Detection Models on the Flickr30k Dataset
  Table 7 caption: TABLE 7 Comparison of Different Percentage Parameters k k in Equations
    (9) and (12) on the Flickr30k Dataset
  Table 8 caption: TABLE 8 Comparison of Different Base Values b b on the Flickr30k
    Dataset
  Table 9 caption: TABLE 9 Comparison of Two Smallest Student Models on the Flickr30k
    Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3178485
- Affiliation of the first author: school of science, edith cowan university, joondalup,
    wa, australia
  Affiliation of the last author: school of science, edith cowan university, joondalup,
    wa, australia
  Figure 1 Link: articels_figures_by_rev_year\2022\Unsupervised_Learning_for_Maximum_Consensus_Robust_Fitting_A_Reinforcement_Learn\figure_1.jpg
  Figure 1 caption: 'The Search Tree. Top: An example of a 2D line fitting problem,
    where red points (indexed by numbers) are outliers to be removed. Bottom: Tree
    structure, i.e., certain subsets of points called a basis (see Section 3.1 for
    definition) are explored by a globally optimal algorithm [13] (left) and by our
    unsupervised learning method (right), where red nodes correspond to outliers shown
    in the top figure and the sequences from top red nodes to bottom ones in the trees
    show the successive points to be removed. Both methods, determine a set of candidate
    points, a basis, to eliminate. What differs is the way these two algorithms decide
    to eliminate these basis points. A uses a heuristic scoring, which may prioritise
    choices leading to exploring more of the tree, than the direct path. Our RL-based
    method learns to extract promising candidates and commits to removing those -
    avoiding some exploration costs. Explicitly, the admissible heuristic in A method
    brings the search into some subparts (green line) before discovering the optimal
    solution (red line). Our agent learns to remove outliers by traversing from the
    initial state to the goal state in the minimal number of steps (the states are
    numbered based on the index of the removed point). Observe that, in this example
    (not guaranteed in general), both methods terminate at the same solution (i.e.,
    both remove the same set of outliers).'
  Figure 10 Link: articels_figures_by_rev_year\2022\Unsupervised_Learning_for_Maximum_Consensus_Robust_Fitting_A_Reinforcement_Learn\figure_10.jpg
  Figure 10 caption: 'Linearized Fundamental Matrix Estimation on real datasets. Left:
    Consensus size (%). Right: Run times in log scale. Similar to the experiment on
    the ModelNet40 dataset, our method, on average, finds 2%-5% higher consensus with
    affordable time ( sim 1 second).'
  Figure 2 Link: articels_figures_by_rev_year\2022\Unsupervised_Learning_for_Maximum_Consensus_Robust_Fitting_A_Reinforcement_Learn\figure_2.jpg
  Figure 2 caption: Minimax Fitting and Basis. Illustration of a minimax fitting problem
    for a set of points in 2D. Blue dots represent measurements with largest residual,
    which form the basis set.
  Figure 3 Link: articels_figures_by_rev_year\2022\Unsupervised_Learning_for_Maximum_Consensus_Robust_Fitting_A_Reinforcement_Learn\figure_3.jpg
  Figure 3 caption: Visualization of states and actions. The initial state corresponds
    to the minimax fit of the original point set. The action, associated to a state,
    consists of removing a point in the basis and conduct minimax fit for the remaining
    points.
  Figure 4 Link: articels_figures_by_rev_year\2022\Unsupervised_Learning_for_Maximum_Consensus_Robust_Fitting_A_Reinforcement_Learn\figure_4.jpg
  Figure 4 caption: Structure of our proposed framework. Top row shows the (unrolled)
    operations (we use an instance of 2D line fitting as an example). Given a set
    of measurements, minimax is performed to obtain the initial state. Then, the state
    is encoded into a graph representation which is fed to the agent to predict the
    expected Q value (returns) of choosing each point in basis to eliminate. The agent
    then performs an action, receives a reward and moves to the next state. This process
    iterates until the agent reaches the goal state. Bottom row depicts the design
    of our network (agent).
  Figure 5 Link: articels_figures_by_rev_year\2022\Unsupervised_Learning_for_Maximum_Consensus_Robust_Fitting_A_Reinforcement_Learn\figure_5.jpg
  Figure 5 caption: '2D line fitting on (a) 100 and (b) 200 synthetic data points
    with various outlier rates. Left: Distribution of differences between predicted
    consensus and global optimal solution (obtained using A) with baseline models.
    Note: lower is better. Right: Run-time of our method compared to baseline models.
    Our method is very competitive for high number of points with high rate of outliers
    in terms of achieving optimal solution and less runtime. Note that A is dropped
    out in the comparison of runtime for N=200 since it is very slow in general for
    high number of outliers.'
  Figure 6 Link: articels_figures_by_rev_year\2022\Unsupervised_Learning_for_Maximum_Consensus_Robust_Fitting_A_Reinforcement_Learn\figure_6.jpg
  Figure 6 caption: '3D plane fitting on (a) 100 and (b) 200 points with varying outlier
    rates. Left: Difference between predicted consensus and global optimal solution
    (using A) with baseline models (lower is better). Right: Run-time of our method
    compared to baseline models.'
  Figure 7 Link: articels_figures_by_rev_year\2022\Unsupervised_Learning_for_Maximum_Consensus_Robust_Fitting_A_Reinforcement_Learn\figure_7.jpg
  Figure 7 caption: 'Comparison between different choices of reward functions (on
    training) using the (Left) ModelNet40 and (Right) KITTI datasets. lambda =0 corresponds
    to constant reward function: counting discarded points, 0< lambda < infty corresponds
    to reward function counting outliers and evaluating fitting residual (weighted
    with hyperparameter lambda ), lambda =infty corresponds to reward function evaluating
    fitting residual only. Unless lambda is too big (close to infty ) or too small
    (close to 0), different choices of lambda have relatively small impact on predicted
    results.'
  Figure 8 Link: articels_figures_by_rev_year\2022\Unsupervised_Learning_for_Maximum_Consensus_Robust_Fitting_A_Reinforcement_Learn\figure_8.jpg
  Figure 8 caption: Comparison of the performance of some variants of Deep Q learning
    adopted in training DDQN achieves the best performance for linearized fundamental
    matrix estimation on KITTI dataset while DQN-PER and DQN-Dueling Network have
    very similar performance.
  Figure 9 Link: articels_figures_by_rev_year\2022\Unsupervised_Learning_for_Maximum_Consensus_Robust_Fitting_A_Reinforcement_Learn\figure_9.jpg
  Figure 9 caption: "Comparison of the performance of SuperGlue and our methods on\
    \ ModelNet40 dataset. The distribution of consensus size per outlier rate is reported.\
    \ \u201CSuperGlue matches\u201D are the counts before pruning technical outliers\
    \ from those returned by SuperGlue. \u201CSuperGlue inliers\u201D are the counts\
    \ after pruning. On average, our method clearly outperforms Superglue on outlier\
    \ rates up to 40%. Our method never declares outliers to be inliers (and when\
    \ we intervene to strip out the outliers Superglue has incorrectly classified\
    \ as inliers (blue)), we still have much higher inlier counts (green). Even at\
    \ 50% outliers, where Superglue does often return more true inliers than our method,\
    \ more than half of the time it still returns few genuine inliers (horizontal\
    \ bars are the median of the distibutions)."
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Giang Truong
  Name of the last author: Syed Zulqarnain Gilani
  Number of Figures: 12
  Number of Tables: 3
  Number of authors: 5
  Paper title: 'Unsupervised Learning for Maximum Consensus Robust Fitting: A Reinforcement
    Learning Approach'
  Publication Date: 2022-05-27 00:00:00
  Table 1 caption: TABLE 1 Linearized Fundamental Matrix Estimation on (Semi-Synthetic)
    ModelNet40 Dataset
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison Between Some Variants of Deep Q Learning on
    Linearized Fundamental Matrix Estimation on KITTI Dataset (Test Performance)
  Table 3 caption: TABLE 3 Comparison Between Different Choices of Reward Functions
    During Evaluation BeforeAfter Local Tree Refinement
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3178442
- Affiliation of the first author: key laboratory of intelligent perception and systems
    for high-dimensional information of ministry of education, jiangsu key laboratory
    of image and video understanding for social security, school of computer science
    and engineering, nanjing university of science and technology, nanjing, jiangsu,
    china
  Affiliation of the last author: riken center for advanced intelligence project,
    tokyo, japan
  Figure 1 Link: articels_figures_by_rev_year\2022\ClassWise_Denoising_for_Robust_Learning_Under_Label_Noise\figure_1.jpg
  Figure 1 caption: "The pipeline comparison of (a) existing methods and (b) our method.\
    \ The existing strategy directly corrects all noisy labels in both the positive\
    \ and negative classes contained in the observed noisy dataset S \u02DC . In contrast,\
    \ our strategy introduces two virtual auxiliary sets S P \u02DC and S N \u02DC\
    \ as a bridge from noisy S \u02DC to clean S , and they help to correct the labels\
    \ of false negative and false positive data points in S \u02DC , respectively."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\ClassWise_Denoising_for_Robust_Learning_Under_Label_Noise\figure_2.jpg
  Figure 2 caption: Example images of binary classification datasets. (a) presents
    MNIST-binary, and (b) presents CIFAR-binary.
  Figure 3 Link: articels_figures_by_rev_year\2022\ClassWise_Denoising_for_Robust_Learning_Under_Label_Noise\figure_3.jpg
  Figure 3 caption: Example images of multi-class datasets. (a) presents Animal-10N,
    (b) presents Clothing-1M, and (c) presents CIFAR-100.
  Figure 4 Link: articels_figures_by_rev_year\2022\ClassWise_Denoising_for_Robust_Learning_Under_Label_Noise\figure_4.jpg
  Figure 4 caption: Comparison of CWD and LICS on variance and error in centroid estimation,
    where the first row shows the case under label flip rate (eta mathrmP=0.4, eta
    mathrmN=0.4) , and the second row shows the case under label flip rate (eta mathrmP=0.3,
    eta mathrmN=0.1) . The blue bar and orange bar indicate LICS and CWD, respectively.
    The numerical values are annotated above the bars.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Chen Gong
  Name of the last author: Masashi Sugiyama
  Number of Figures: 4
  Number of Tables: 8
  Number of authors: 8
  Paper title: Class-Wise Denoising for Robust Learning Under Label Noise
  Publication Date: 2022-05-30 00:00:00
  Table 1 caption: "TABLE 1 The Decomposition of Some Common Loss Functions [12],\
    \ [35], [36], Where Z=Yh(X) Z=Yh(X) is the Functional Margin, and [\u22C5 ] +\
    \ =max(\u22C5,0) [\xB7]+=max(\xB7,0)"
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Summary of Main Mathematical Notations
  Table 3 caption: TABLE 3 The Characteristics of Five Adopted UCI Datasets
  Table 4 caption: TABLE 4 Comparison of the Mean Test Accuracies (%) of Various Approaches
    on Five Adopted UCI Datasets
  Table 5 caption: TABLE 5 Comparison of the Mean Test Accuracies (%) of Various Approaches
    on MNIST-binary Dataset
  Table 6 caption: TABLE 6 Comparison of the Mean Test Accuracies (%) of Various Approaches
    on CIFAR-Binary Dataset
  Table 7 caption: TABLE 7 Comparison of Test Accuracies (%) of Various Approaches
    on Animal-10N and Clothing-1M Datasets
  Table 8 caption: TABLE 8 Comparison of the Mean Test Accuracies (%) of Various Approaches
    on CIFAR-100 Dataset
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3178690
