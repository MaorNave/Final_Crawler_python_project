- Affiliation of the first author: biorobotics institute of scuola superiore santanna,
    pisa, italy
  Affiliation of the last author: biorobotics institute of scuola superiore santanna,
    pisa, italy
  Figure 1 Link: articels_figures_by_rev_year\2019\Denoising_Autoencoders_for_Overgeneralization_in_Neural_Networks\figure_1.jpg
  Figure 1 caption: A linear classifier is trained to recognize exclusively pictures
    of digits 0 and 6. Digit 9 was never observed during training, but in this example
    it is wrongly classified as digit 6. This is an example of overgeneralization.
    A similar problem is fooling, whereby it is possible to generate images that are
    unrecognizable to humans but are nonetheless classified as one of the known classes
    with high confidence, for example here the noise-looking picture in the bottom-left
    corner that is classified as digit 0.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Denoising_Autoencoders_for_Overgeneralization_in_Neural_Networks\figure_2.jpg
  Figure 2 caption: "The system presented here is trained to classify points sampled\
    \ from three uniform ring distributions. A. 1000 data points are sampled from\
    \ each of the target distributions. B. Labeling y of each point in the input space\
    \ without scaling of the classifiers output by the confidence score. C. Labeling\
    \ y ~ of each point in the in=put space scaled by the computed confidence score.\
    \ Regions in white are assigned low confidence scores. D. Top: confidence score\
    \ without \u0393(x) . Bottom: estimate of the curvature of the log-distribution\
    \ of the data ( \u0393(x) ). The confidence score c ~ (x) is the product of the\
    \ two functions. The panel in B is the product of the classifiers output (C) and\
    \ the confidence score."
  Figure 3 Link: articels_figures_by_rev_year\2019\Denoising_Autoencoders_for_Overgeneralization_in_Neural_Networks\figure_3.jpg
  Figure 3 caption: "Visualization of a set of generated fooling samples from the\
    \ main results of Table 1 (MNIST). The samples from the plain CNN and the COOL\
    \ models were computed by trying to fool each systems output classification scores\
    \ above a threshold of 90 percent. As fooling was unsuccessful on the dAE model\
    \ in this case, the results reported here were taken from the simulations in which\
    \ fooling was performed directly on the output scaled by the confidence score\
    \ ( y ~ ). Classes for which fooling was never successful within the maximum number\
    \ of fooling iterations are marked as \u201Cnever\u201D."
  Figure 4 Link: articels_figures_by_rev_year\2019\Denoising_Autoencoders_for_Overgeneralization_in_Neural_Networks\figure_4.jpg
  Figure 4 caption: Comparison of the three models on a benchmark of open set recognition.
    The F-measure was computed for each model on problems created from the MNIST (left)
    and Fashion-MNIST (right) datasets by only using a limited number of known classes
    during training while testing on the full test set (e.g., training on classes
    0 and 3 but testing on all classes [0, 9]), requiring the models to be able to
    reject samples belonging to unknown classes. Higher values for the openness of
    a problem reflect a smaller number of classes used during training. The curves
    are averaged across 10 runs using different sub-sets of digits. Error bars denote
    standard deviation.
  Figure 5 Link: articels_figures_by_rev_year\2019\Denoising_Autoencoders_for_Overgeneralization_in_Neural_Networks\figure_5.jpg
  Figure 5 caption: ROC curves averaged over 10 1-class recognition problems, one
    for each class in MNIST (left) and Fashion-MNIST (right), for three models, the
    dAE model described in this paper, 1-Class SVM [19] and COOL [7].
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Giacomo Spigler
  Name of the last author: Giacomo Spigler
  Number of Figures: 5
  Number of Tables: 2
  Number of authors: 1
  Paper title: Denoising Autoencoders for Overgeneralization in Neural Networks
  Publication Date: 2019-04-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 MNIST Fooling Results
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Fashion-MNIST Fooling Results
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2909876
- Affiliation of the first author: "perception team, inria, universit\xE9 grenoble\
    \ alpes, montbonnot, france"
  Affiliation of the last author: "perception team, inria, universit\xE9 grenoble\
    \ alpes, montbonnot, france"
  Figure 1 Link: articels_figures_by_rev_year\2019\A_Comprehensive_Analysis_of_Deep_Regression\figure_1.jpg
  Figure 1 caption: Comparison of the training loss evolution with different optimizers
    for VGG-16 and ResNet-50.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\A_Comprehensive_Analysis_of_Deep_Regression\figure_2.jpg
  Figure 2 caption: Comparison of the training loss evolution with different batch
    size on VGG-16 and ResNet-50.
  Figure 3 Link: articels_figures_by_rev_year\2019\A_Comprehensive_Analysis_of_Deep_Regression\figure_3.jpg
  Figure 3 caption: Confidence intervals for the median MAE as a function of the dropout
    rate (%) for the four datasets (using VGG-16). The vertical gray line is the default
    configuration.
  Figure 4 Link: articels_figures_by_rev_year\2019\A_Comprehensive_Analysis_of_Deep_Regression\figure_4.jpg
  Figure 4 caption: Pre-processed examples for the Biwi (first and second rows), FLD
    (third row), Parse (fourth row), and MPII (fifth row) data sets.
  Figure 5 Link: articels_figures_by_rev_year\2019\A_Comprehensive_Analysis_of_Deep_Regression\figure_5.jpg
  Figure 5 caption: Performance improvement between the best and worst median runs
    for each configuration on (a) Biwi, (b) FLD, (c) Parse and (d) MPII, for four
    network variants and data pre-processing (Data), using VGG-16 (green and ascending
    lines) and ResNet-50 (red and descending lines). Horizontal blue lines indicate
    the improvement obtained by recent approaches when they outperform existing methods.
    Importantly, all the methods compared in this figure obtained statistically significant
    differences with p<0.001 , except for DO on Parse.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.93
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.95
  Name of the first author: "St\xE9phane Lathuili\xE8re"
  Name of the last author: Radu Horaud
  Number of Figures: 5
  Number of Tables: 14
  Number of authors: 4
  Paper title: A Comprehensive Analysis of Deep Regression
  Publication Date: 2019-04-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Network Baseline Specification
  Table 10 caption:
    table_text: TABLE 10 Impact of the Data Pre-Processing on VGG-16 and ResNet-50
  Table 2 caption:
    table_text: TABLE 2 Impact of the Loss Choice ( L L) on VGG-16 and ResNet-50
  Table 3 caption:
    table_text: TABLE 3 Impact of the Batch Normalization (BN) Layer on VGG-16 and
      ResNet-50
  Table 4 caption:
    table_text: TABLE 4 Impact of the Dropout (DO) Layer on VGG-16
  Table 5 caption:
    table_text: TABLE 5 Impact of the Finetunig Depth (FT) on VGG-16 and ResNet-50
  Table 6 caption:
    table_text: TABLE 6 Impact of the Regressed Layer (RL) for VGG-16
  Table 7 caption:
    table_text: TABLE 7 Impact of the Regressed Layer (RL) for ResNet-50
  Table 8 caption:
    table_text: TABLE 8 Impact of the Target and Input Representations (TIR) on VGG-16
      and ResNet-50
  Table 9 caption:
    table_text: TABLE 9 Impact of the Mirroring (Mirr.) on VGG16 and ResNet50
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2910523
- Affiliation of the first author: school of electronic information and communications,
    huazhong university of science and technology, wuhan, china
  Affiliation of the last author: microsoft research, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2019\Object_Detection_in_Videos_by_High_Quality_Object_Linking\figure_1.jpg
  Figure 1 caption: '(a) Static image detection: only the detections in the first
    two frames are correct. (b) Short tubelet detection: the detection in the third
    frame becomes correct due to object linking between the second and third frame.
    (c) Short tubelet linking: The detections in all the frames are correct due to
    short tubelet linking. Here the short video segment length is 2. For each frame,
    we only show the top-scoring box, where greenred boxes correspond to successfailure
    examples.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Object_Detection_in_Videos_by_High_Quality_Object_Linking\figure_2.jpg
  Figure 2 caption: The orange cuboid, bounding the movement of the object, is the
    target of the cuboid proposal stage. The tubelet, composed of the blue object
    boxes in the video segment, is the target of the short tubelet objection stage.
  Figure 3 Link: articels_figures_by_rev_year\2019\Object_Detection_in_Videos_by_High_Quality_Object_Linking\figure_3.jpg
  Figure 3 caption: Illustration of short tubelet linking. Boxes with the same color
    belong to the same tubelet. The short tubelets (a) from two temporally-overlapping
    video segments are linked together to form new long tubelets (b).
  Figure 4 Link: articels_figures_by_rev_year\2019\Object_Detection_in_Videos_by_High_Quality_Object_Linking\figure_4.jpg
  Figure 4 caption: Recall versus IoU threshold on the VID validation set. We show
    the results when keeping 50, 150, and 300 cuboid proposals for IoU threshold 0.5
    to 0.7.
  Figure 5 Link: articels_figures_by_rev_year\2019\Object_Detection_in_Videos_by_High_Quality_Object_Linking\figure_5.jpg
  Figure 5 caption: Visualization of some cuboid proposal results. Each column corresponds
    to a short video segment. The green boxes are the ground truths and the rest are
    the cuboid proposals. Boxes with the same color belong to the same cuboid proposals.
    For each video segment, we only show five proposals with the highest objectness
    scores for simplicity.
  Figure 6 Link: articels_figures_by_rev_year\2019\Object_Detection_in_Videos_by_High_Quality_Object_Linking\figure_6.jpg
  Figure 6 caption: "Short tubelet detection results. The \u201CLoose Criterion\u201D\
    \ is the standard detection mAP and the \u201CStrict Criterion\u201D checks whether\
    \ the instance IDs of the boxes in a tubelet are the same."
  Figure 7 Link: articels_figures_by_rev_year\2019\Object_Detection_in_Videos_by_High_Quality_Object_Linking\figure_7.jpg
  Figure 7 caption: Detection results for NMST-NMS and different short video segment
    lengths. Video segment length 1 means the static image detector.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Peng Tang
  Name of the last author: Jingdong Wang
  Number of Figures: 7
  Number of Tables: 2
  Number of authors: 6
  Paper title: Object Detection in Videos by High Quality Object Linking
  Publication Date: 2019-04-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Detection Results (mAP in % %) of Different Methods on the
      VID Validation Set Which Has four Subsets According to the Object Moving Speed
      or Occlusion
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average Precision (in % %) for Different Methods on the VID
      Dataset
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2910529
- Affiliation of the first author: department of electrical and computer engineering,
    university of california san diego, san diego, usa
  Affiliation of the last author: department of electrical and computer engineering,
    university of california san diego, san diego, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Learning_ComplexityAware_Cascades_for_Pedestrian_Detection\figure_1.jpg
  Figure 1 caption: "Decision tree of depth 2. Circles represent classifier nodes,\
    \ squares terminal nodes. \u03D5(x,v) is the feature of x used at node v , \u03B8\
    \ a threshold."
  Figure 10 Link: articels_figures_by_rev_year\2019\Learning_ComplexityAware_Cascades_for_Pedestrian_Detection\figure_10.jpg
  Figure 10 caption: "Stage configuration for \u201CBoosting\u201D and \u201CManual\u201D\
    \ cascades."
  Figure 2 Link: articels_figures_by_rev_year\2019\Learning_ComplexityAware_Cascades_for_Pedestrian_Detection\figure_2.jpg
  Figure 2 caption: Sample of the feature channels generated on a pedestrian image.
  Figure 3 Link: articels_figures_by_rev_year\2019\Learning_ComplexityAware_Cascades_for_Pedestrian_Detection\figure_3.jpg
  Figure 3 caption: "Eight 2\xD72 checkerboard-like filters used in this work. Red\
    \ (Green) is used to represent value +1 (-1)."
  Figure 4 Link: articels_figures_by_rev_year\2019\Learning_ComplexityAware_Cascades_for_Pedestrian_Detection\figure_4.jpg
  Figure 4 caption: CNN architecture used to extract small CNN features.
  Figure 5 Link: articels_figures_by_rev_year\2019\Learning_ComplexityAware_Cascades_for_Pedestrian_Detection\figure_5.jpg
  Figure 5 caption: MS-CNN proposal sub-network. Bold cubes are the network output
    tensors. c,b are the number of classes and bounding box coordinates, respectively.
  Figure 6 Link: articels_figures_by_rev_year\2019\Learning_ComplexityAware_Cascades_for_Pedestrian_Detection\figure_6.jpg
  Figure 6 caption: "MS-CNN object detection sub-network. \u201CTrunk CNN layers\u201D\
    \ are shared with the proposal sub-network. W and H are the width and height of\
    \ the input image. Green (blue) cubes represent object (context) region pooling."
  Figure 7 Link: articels_figures_by_rev_year\2019\Learning_ComplexityAware_Cascades_for_Pedestrian_Detection\figure_7.jpg
  Figure 7 caption: Cascade accuracy versus complexity for different features.
  Figure 8 Link: articels_figures_by_rev_year\2019\Learning_ComplexityAware_Cascades_for_Pedestrian_Detection\figure_8.jpg
  Figure 8 caption: "Stage configuration learned with different trade-off coefficient\
    \ \u03B7 . Only one in five stages is shown."
  Figure 9 Link: articels_figures_by_rev_year\2019\Learning_ComplexityAware_Cascades_for_Pedestrian_Detection\figure_9.jpg
  Figure 9 caption: Accuracy versus complexity for different trade-off coefficients
    eta .
  First author gender probability: 0.66
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Zhaowei Cai
  Name of the last author: Nuno Vasconcelos
  Number of Figures: 12
  Number of Tables: 7
  Number of authors: 3
  Paper title: Learning Complexity-Aware Cascades for Pedestrian Detection
  Publication Date: 2019-04-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Feature Complexity and Processing Unit
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Comparison to \u201CBoosting\u201D and \u201CManual\u201D\
      \ Cascades"
  Table 3 caption:
    table_text: TABLE 3 Comparison of Complexity Restricted and Sensitive Trees
  Table 4 caption:
    table_text: TABLE 4 Faster-RCNN Baseline Results
  Table 5 caption:
    table_text: TABLE 5 Embedding of Large CNNs in CompACT Cascades
  Table 6 caption:
    table_text: TABLE 6 Pedestrian Detection mAP Comparison on KITTI
  Table 7 caption:
    table_text: TABLE 7 The Results on CityPersons Validation Set
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2910514
- Affiliation of the first author: department of computer and electrical engineering
    and applied mathematics, university of salerno, fisciano, italy
  Affiliation of the last author: department of computer and electrical engineering
    and applied mathematics, university of salerno, fisciano, italy
  Figure 1 Link: articels_figures_by_rev_year\2019\Age_from_Faces_in_the_Deep_Learning_Revolution\figure_1.jpg
  Figure 1 caption: Problem formulations. RVE uses a regressor providing the age.
    CAE gives an age interval using a single (CAE-DE) or many binary classifiers (CAE-RF).
    DAE provides the label distribution, centered on the estimated age.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Age_from_Faces_in_the_Deep_Learning_Revolution\figure_2.jpg
  Figure 2 caption: Number of methods based on deep networks.
  Figure 3 Link: articels_figures_by_rev_year\2019\Age_from_Faces_in_the_Deep_Learning_Revolution\figure_3.jpg
  Figure 3 caption: "Performance, in terms of \u03F5 -error (see Section 3), achieved\
    \ by the different deep network architectures on the ChaLearn LAP databases."
  Figure 4 Link: articels_figures_by_rev_year\2019\Age_from_Faces_in_the_Deep_Learning_Revolution\figure_4.jpg
  Figure 4 caption: 'Ensembling techniques. (a) At description level (E-DES): the
    output layer combines the features learnt by different nets. (b) At decision level
    (E-DEC): the output layer combines the decisions taken by different networks.'
  Figure 5 Link: articels_figures_by_rev_year\2019\Age_from_Faces_in_the_Deep_Learning_Revolution\figure_5.jpg
  Figure 5 caption: 'Pose normalization by best score: the original image is rotated
    with six predefined angles and the detector applied; the face is normalized by
    using the angle with the best score.'
  Figure 6 Link: articels_figures_by_rev_year\2019\Age_from_Faces_in_the_Deep_Learning_Revolution\figure_6.jpg
  Figure 6 caption: '(a) P-MTL: the network is trained for age estimation in parallel
    with nets for other biometrics; all the layers are in common and each output neuron
    is associated to a task. (b) D-MTL: the different tasks may share some front layers,
    but are solved with specific nets, working in parallel.'
  Figure 7 Link: articels_figures_by_rev_year\2019\Age_from_Faces_in_the_Deep_Learning_Revolution\figure_7.jpg
  Figure 7 caption: Images from the most used datasets for real and apparent age estimation
    and age group classification.
  Figure 8 Link: articels_figures_by_rev_year\2019\Age_from_Faces_in_the_Deep_Learning_Revolution\figure_8.jpg
  Figure 8 caption: 'Datasets for age analysis: the larger is the radius of the circle,
    the higher is the number of methods using it.'
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Vincenzo Carletti
  Name of the last author: Mario Vento
  Number of Figures: 8
  Number of Tables: 8
  Number of authors: 4
  Paper title: Age from Faces in the Deep Learning Revolution
  Publication Date: 2019-04-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Surveys Regarding the Analysis of Facial Images Published
      in the Last Decade
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Classification of the Methods on the Basis of Problem Formulation,
      Ensembling, Used Deep Network and Learning Procedure (Preprocessing, Data Augmentation,
      Training and Multi-Task)
  Table 3 caption:
    table_text: TABLE 3 Publicly Available Datasets for Real Age Estimation
  Table 4 caption:
    table_text: TABLE 4 Publicly Available Datasets for Apparent Age Estimation
  Table 5 caption:
    table_text: TABLE 5 Publicly Available Datasets for Age Group Classification
  Table 6 caption:
    table_text: TABLE 6 Performance over Different Datasets in Terms of MAE
  Table 7 caption:
    table_text: "TABLE 7 \u03F5 \u03B5-Error on LAP Data"
  Table 8 caption:
    table_text: TABLE 8 Performance over Adience and Groups in Terms of Top-1 and
      1-Off
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2910522
- Affiliation of the first author: department of computer science and information
    engineering, national taiwan normal university, taipei, taiwan
  Affiliation of the last author: asus corporation, taipei city, taiwan
  Figure 1 Link: articels_figures_by_rev_year\2019\Multilabel_Deep_VisualSemantic_Embedding\figure_1.jpg
  Figure 1 caption: 'An image is often annotated with several tags: (left) Semantically
    similar, (middle) and (right) semantically different.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Multilabel_Deep_VisualSemantic_Embedding\figure_2.jpg
  Figure 2 caption: Infrastructure of the proposed label ranking model. The model
    learns (via CNN) an image-dependent transformation, which can be used to predict
    multiple labels for a new image.
  Figure 3 Link: articels_figures_by_rev_year\2019\Multilabel_Deep_VisualSemantic_Embedding\figure_3.jpg
  Figure 3 caption: Simplified example of formulating the multilabel image classification
    as a binary classification problem.
  Figure 4 Link: articels_figures_by_rev_year\2019\Multilabel_Deep_VisualSemantic_Embedding\figure_4.jpg
  Figure 4 caption: Visualization using t-SNE [35] of the label embeddings learned
    by the word2vec model. These labels are the 81 concepts defined in the NUS-WIDE
    dataset.
  Figure 5 Link: articels_figures_by_rev_year\2019\Multilabel_Deep_VisualSemantic_Embedding\figure_5.jpg
  Figure 5 caption: Effect of the transformed dimension to the classification performance.
    (top) Top-3 prediction (bottom) top-5 prediction.
  Figure 6 Link: articels_figures_by_rev_year\2019\Multilabel_Deep_VisualSemantic_Embedding\figure_6.jpg
  Figure 6 caption: Two examples of visualizing the image-conditioned word space (
    k = 2). Words being close to the origin are retrieved to describe the input image.
    Note that semantically similar words are still embedded nearby each other in the
    transformed word space.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.66
  Name of the first author: Mei-Chen Yeh
  Name of the last author: Yi-Nan Li
  Number of Figures: 6
  Number of Tables: 4
  Number of authors: 2
  Paper title: Multilabel Deep Visual-Semantic Embedding
  Publication Date: 2019-04-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Multilabel Image Classification Results on NUS-WIDE
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Multilabel Image Classification Results on MS COCO (for Top
      3 Prediction) and VOC 2007
  Table 3 caption:
    table_text: TABLE 3 Ablation Study on Effects of Different Visual Models
  Table 4 caption:
    table_text: TABLE 4 Top 3 Prediction Results
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2911065
- Affiliation of the first author: school of computer & information, hefei university
    of technology, hefei, anhui, china
  Affiliation of the last author: school of computer science & engineering, nanyang
    technological university, singapore
  Figure 1 Link: articels_figures_by_rev_year\2019\Learning_to_Compose_and_Reason_with_Language_Tree_Structures_for_Visual_Groundin\figure_1.jpg
  Figure 1 caption: "(a) A typical task of grounding the natural language \u201Ca\
    \ black dog on the left of the tree\u201D in the image, represented as a set of\
    \ object bounding boxes. The output should be the \u201Cdog\u201D grounded with\
    \ the red box. (b) Most recent advances focus on simple compositions of the language\
    \ such as (subject, predicate, object) triplet. (c) Our RvG-Tree decomposes the\
    \ sentence into more fine-grained compositions, and then accumulates the grounding\
    \ confidence score in a bottom-up fashion."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Learning_to_Compose_and_Reason_with_Language_Tree_Structures_for_Visual_Groundin\figure_2.jpg
  Figure 2 caption: "The overview of using RvG-Tree for natural language grounding.\
    \ Given a natural language sentence as input, we first prune the sentence and\
    \ then construct RvG-Tree (Section 3.2). Then, we have a score-feature classifier\
    \ (Section 3.3) to determine each node as the \u201Cscore node\u201D or \u201C\
    feature node\u201D, where the score node returns the recursive score and the feature\
    \ node returns the feature (Section 3.3). The final score of the root node is\
    \ accumulated recursively in a bottom-up fashion (Section 3.3) and the visual\
    \ region with the highest score is considered as the grounding result. Note that\
    \ all the nodes can be visualized by the corresponding confidence scores and only\
    \ qualitative regions are visualized."
  Figure 3 Link: articels_figures_by_rev_year\2019\Learning_to_Compose_and_Reason_with_Language_Tree_Structures_for_Visual_Groundin\figure_3.jpg
  Figure 3 caption: "(a). Given a flat sentence, RvG-Tree computes a score for every\
    \ parent candidate indicating how qualified each candidate is to be merged in\
    \ the next layer. By operating selection and merging recursively until RvG-Tree\
    \ reaches the root node, the tree is constructed and will be used in the inference\
    \ part. (b). RvG-Tree calculates grounding confidence scores in a recursive way.\
    \ Taking the l th node as an example, according to Eq. (6), the grounding score\
    \ returned by it is a sum of the scores from itself and its \u201Cscore child\u201D\
    ."
  Figure 4 Link: articels_figures_by_rev_year\2019\Learning_to_Compose_and_Reason_with_Language_Tree_Structures_for_Visual_Groundin\figure_4.jpg
  Figure 4 caption: Word cloud visualizations of what nodes are likely to be classified
    as (a) feature node or (b) score node. Every word frequency is increased by 1
    if it is a leaf under the feature or score node. We can see that feature nodes
    are generally related to visual concepts while score nodes are generally not visual.
  Figure 5 Link: articels_figures_by_rev_year\2019\Learning_to_Compose_and_Reason_with_Language_Tree_Structures_for_Visual_Groundin\figure_5.jpg
  Figure 5 caption: Examples of binary tree structures from (a) SCNLP constituency
    parser and (b) overall fined-tuned by Straight-Through Gumbel-Softmax estimator.
    We can see that the tree structure is more meaningful after fine-tuning.
  Figure 6 Link: articels_figures_by_rev_year\2019\Learning_to_Compose_and_Reason_with_Language_Tree_Structures_for_Visual_Groundin\figure_6.jpg
  Figure 6 caption: 'Qualitative results of correct grounding results from RefCOCOg.
    Red circle is the score node and black circle is the feature node. Each node is
    visualized by the score map or attention map of different regions: larger transparency
    indicates lower score. The ground-truth region is in green box and the result
    is in red box.'
  Figure 7 Link: articels_figures_by_rev_year\2019\Learning_to_Compose_and_Reason_with_Language_Tree_Structures_for_Visual_Groundin\figure_7.jpg
  Figure 7 caption: 'Qualitative results of incorrect grounding results from RefCOCOg.
    Red circle is the score node and black circle is the feature node. Each node is
    visualized by the score map or attention map of different regions: larger transparency
    indicates lower score. The ground-truth region is in green box and the result
    is in red box.'
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.61
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.63
  Name of the first author: Richang Hong
  Name of the last author: Hanwang Zhang
  Number of Figures: 7
  Number of Tables: 4
  Number of authors: 5
  Paper title: Learning to Compose and Reason with Language Tree Structures for Visual
    Grounding
  Publication Date: 2019-04-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Top-1 Accuracy% of Ablative Models on the Three Datasets with
      Ground-Truth Object Bounding Boxes
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Top-1 Accuracy% of Ablative Models on the Three Datasets with
      Detected Object Bounding Boxes
  Table 3 caption:
    table_text: TABLE 3 Top-1 Accuracy% of Various Grounding Models on the Three Datasets
      with Ground-Truth Object Bounding Boxes
  Table 4 caption:
    table_text: TABLE 4 Top-1 Accuracy% of Various Grounding Models on the Three Datasets
      with Detected Object Bounding Boxes
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2911066
- Affiliation of the first author: us army research laboratory, adelphi, usa
  Affiliation of the last author: us army research laboratory, adelphi, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Unsupervised_Deep_VisualInertial_Odometry_with_Online_Error_Correction_for_RGBD_\figure_1.jpg
  Figure 1 caption: "Generalized overview of our VIOLearner network (see Fig. 2 for\
    \ a more detailedaccurate representation of the architecture). Note the hierarchical\
    \ architecture where \u03B8 , the true change in pose between images, is estimated\
    \ at multiple scales and then corrected via convolutional processing of the Jacobian\
    \ of euclidean projection error at that scale with respect to a grid of source\
    \ coordinates. The final Level n computes m hypothesis reconstructions (and associated\
    \ estimates \u03B8 n,m ) and the \u03B8 n,m with the lowest paired euclidean projection\
    \ error between the reconstruction and true target image I j+1 is output as the\
    \ final network estimate \u03B8 n ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Unsupervised_Deep_VisualInertial_Odometry_with_Online_Error_Correction_for_RGBD_\figure_2.jpg
  Figure 2 caption: "VIOLearner: (a) IMU data is fed into a series of convolutional\
    \ neural network (CNN) layers which output a 3D affine matrix estimate of change\
    \ in camera pose \u03B8 0 between a source image I j and target image I j+1 .\
    \ The transform \u03B8 0 is applied to a downsampled source image via a spatial\
    \ transformer module and the euclidean error between the spatial transformation\
    \ and the downsampled target image is computed as E 0 . The Jacobian \u2202 E\
    \ 0 \u2202 G 0 of the error image E 0 is taken with respect to the source coordinates\
    \ G 0 , and fed to the next level. (b) The Jacobian \u2202 E i\u22121 \u2202 G\
    \ i\u22121 from the previous level is fed through CNN layers to produce an additive\
    \ refinement \u2202 \u03B8 i that is summed with the previous transform estimate\
    \ \u03B8 i\u22121 to produce a new transform estimate \u03B8 i . The Jacobian\
    \ \u2202 E i \u2202 G i is propagated forward. (c) In the final level, the previous\
    \ Jacobian \u2202 E n\u22121 \u2202 G n\u22121 is processed through CNN layers\
    \ for m hypothesis refinements."
  Figure 3 Link: articels_figures_by_rev_year\2019\Unsupervised_Deep_VisualInertial_Odometry_with_Online_Error_Correction_for_RGBD_\figure_3.jpg
  Figure 3 caption: The virtual Blocks environment exhibits visually unique imagery
    through use of an array of object materials and textures.
  Figure 4 Link: articels_figures_by_rev_year\2019\Unsupervised_Deep_VisualInertial_Odometry_with_Online_Error_Correction_for_RGBD_\figure_4.jpg
  Figure 4 caption: Trajectories for KITTI sequences 09 and 10. Ground truth (GT)
    and results from VIOLearner, SFMLearner, ORB-SLAM2, and OKVIS are shown.
  Figure 5 Link: articels_figures_by_rev_year\2019\Unsupervised_Deep_VisualInertial_Odometry_with_Online_Error_Correction_for_RGBD_\figure_5.jpg
  Figure 5 caption: Trajectories for AirSim sequences 0, 1, and 2. Ground truth (GT)
    and results from VIOLearner, SFMLearner, ORB-SLAM2, and OKVIS are shown.
  Figure 6 Link: articels_figures_by_rev_year\2019\Unsupervised_Deep_VisualInertial_Odometry_with_Online_Error_Correction_for_RGBD_\figure_6.jpg
  Figure 6 caption: "Results on VIOLearner and OKVIS trajectory estimation on the\
    \ Blocks environment given induced IMU orientation offset in the x-dimension.\
    \ Measurement errors are shown for each sequence with translational error percentage\
    \ (top row) on lengths 25 m - 100 m and rotational error in degrees per 100 m\
    \ (bottom row) on lengths 25 m - 100 m. In contrast to VIOLearner, after 20-30\
    \ \u2218 (depending on the sequence), OKVIS exhibits catastropic failure in translation\
    \ estimation."
  Figure 7 Link: articels_figures_by_rev_year\2019\Unsupervised_Deep_VisualInertial_Odometry_with_Online_Error_Correction_for_RGBD_\figure_7.jpg
  Figure 7 caption: Pose root mean squared error (RMSE) from poses computed by VIOLearner
    compared to ground truth after each level for KITTI sequences 09 and 10. We see
    a substantial decrease in error after repeated applications of our error correction
    module.
  Figure 8 Link: articels_figures_by_rev_year\2019\Unsupervised_Deep_VisualInertial_Odometry_with_Online_Error_Correction_for_RGBD_\figure_8.jpg
  Figure 8 caption: Sample images from the three fog conditions.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: E. Jared Shamwell
  Name of the last author: William D. Nothwang
  Number of Figures: 8
  Number of Tables: 9
  Number of authors: 4
  Paper title: Unsupervised Deep Visual-Inertial Odometry with Online Error Correction
    for RGB-D Imagery
  Publication Date: 2019-04-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Network Architectures of VIOLearner
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparisons to VO Approaches on KITTI Sequences 00, 02, 05,
      07, and 08
  Table 3 caption:
    table_text: TABLE 3 Comparisons to VIO Approaches on KITTI Odometry Sequence 10
  Table 4 caption:
    table_text: TABLE 4 Comparisons to VO and VIO Approaches on KITTI Sequences 09
      and 10
  Table 5 caption:
    table_text: TABLE 5 AirSim Trajectory Estimation from VIOLearner RGB-D, VIOLearner
      RGB, SFMLearner, OKVIS, and ORB-SLAM2
  Table 6 caption:
    table_text: TABLE 6 Absolute Trajectory Error (ATE) on KITTI 09 and KITTI 10 Averaged
      over Three or Five Multi-Frame Snippets (Reproduced from [34])
  Table 7 caption:
    table_text: TABLE 7 Results from Networks Trained with 1-8 Hypothesis Pathways
      on KITTI and AirSim
  Table 8 caption:
    table_text: TABLE 8 Robustness Evaluation of VIOLearner to Shifts in the Visual
      Domain
  Table 9 caption:
    table_text: TABLE 9 Robustness Evaluation of VIOLearner to Novel TrajectoriesMotions
      Not Seen During Training
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2909895
- Affiliation of the first author: microsoft, redmond, usa
  Affiliation of the last author: cvssp, university of surrey, guildford, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2019\Weakly_Supervised_Learning_with_MultiStream_CNNLSTMHMMs_to_Discover_Sequential_P\figure_1.jpg
  Figure 1 caption: 'Example showing a video segment of continuous sign language and
    the three aligned streams (from top to bottom): The sign glosses, the mouth shapes
    described by phonemes and the hand shapes. Vertical bars illustrate the synchronisation
    constraints across all streams, horizontal bars represent the garbage class.'
  Figure 10 Link: articels_figures_by_rev_year\2019\Weakly_Supervised_Learning_with_MultiStream_CNNLSTMHMMs_to_Discover_Sequential_P\figure_10.jpg
  Figure 10 caption: 'Saliency maps showing the CNN-LSTMs highest activations. Generated
    with Grad-Cam [76], from top to bottom: Sign-gloss stream, mouth shape stream,
    hand shape stream. It can be seen that the hand model focuses on the right hand,
    the mouth model on the mouth and the sign-gloss swaps between both hands and the
    mouth.'
  Figure 2 Link: articels_figures_by_rev_year\2019\Weakly_Supervised_Learning_with_MultiStream_CNNLSTMHMMs_to_Discover_Sequential_P\figure_2.jpg
  Figure 2 caption: Single CNN-HMM Stream. Showing initialisation and iterative label
    and temporal segmentation refinement in an expectation maximisation fashion. We
    first linearly partition the input stream (1. Flat Start), train a CNN-LSTM model
    and use this model to re-estimate a new segmentation.
  Figure 3 Link: articels_figures_by_rev_year\2019\Weakly_Supervised_Learning_with_MultiStream_CNNLSTMHMMs_to_Discover_Sequential_P\figure_3.jpg
  Figure 3 caption: Multi-stream (3-stream) CNN-HMM with synchronisation at the sign
    end. Three independent CNN-LSTM models are trained on the same full frame input,
    while having different loss functions yielding classifiers for sign-gloss, mouth
    & hand shape modalities. In a hybrid multi-stream HMM framework the networks model
    HMM emission probabilities. All streams can evolve different in time, but have
    to recombine at the sign ends which have been chosen as synchronisation points.
    The HMM is used to re-estimate the frame labelling, improving the modelling in
    several EM iterations.
  Figure 4 Link: articels_figures_by_rev_year\2019\Weakly_Supervised_Learning_with_MultiStream_CNNLSTMHMMs_to_Discover_Sequential_P\figure_4.jpg
  Figure 4 caption: A SignWriting entry describing the sign RAIN in DGS.
  Figure 5 Link: articels_figures_by_rev_year\2019\Weakly_Supervised_Learning_with_MultiStream_CNNLSTMHMMs_to_Discover_Sequential_P\figure_5.jpg
  Figure 5 caption: Twelve exemplary manually annotated hand shape classes are shown.
    Three labelled frames per class demonstrate intra-class variance and inter-class
    similarities. Hand-icons from [58].
  Figure 6 Link: articels_figures_by_rev_year\2019\Weakly_Supervised_Learning_with_MultiStream_CNNLSTMHMMs_to_Discover_Sequential_P\figure_6.jpg
  Figure 6 caption: Ground truth hand shape label count of all 3,361 annotations.
    45 out of 60 classes are present in the data & could be labelled. If several hand
    shapes appear close to one label counting bar, each hand shape alone amounts to
    the mentioned fraction of labels. Hand-icons from [58].
  Figure 7 Link: articels_figures_by_rev_year\2019\Weakly_Supervised_Learning_with_MultiStream_CNNLSTMHMMs_to_Discover_Sequential_P\figure_7.jpg
  Figure 7 caption: 'Development set WER [%] on glosses with cropped hand and face
    inputs: Compare 1-stream against 2-stream (gloss, mouth) and 3-stream (gloss,
    mouth, hand).'
  Figure 8 Link: articels_figures_by_rev_year\2019\Weakly_Supervised_Learning_with_MultiStream_CNNLSTMHMMs_to_Discover_Sequential_P\figure_8.jpg
  Figure 8 caption: 'Mouthings (precisionrecall), clean conditions: Compare 1-stream
    against 2-stream (gloss and mouth) and 3-stream (gloss, mouth, hand).'
  Figure 9 Link: articels_figures_by_rev_year\2019\Weakly_Supervised_Learning_with_MultiStream_CNNLSTMHMMs_to_Discover_Sequential_P\figure_9.jpg
  Figure 9 caption: 'Hands (accuracy): Compare 1-stream against 2-stream (gloss and
    hand) and 3-stream (gloss, mouth, hand).'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Oscar Koller
  Name of the last author: Richard Bowden
  Number of Figures: 10
  Number of Tables: 6
  Number of authors: 4
  Paper title: Weakly Supervised Learning with Multi-Stream CNN-LSTM-HMMs to Discover
    Sequential Parallelism in Sign Language Videos
  Publication Date: 2019-04-14 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Statistics of the PHOENIX 2014 T Recognition (\u201CGloss\u201D\
      ) & Translation (\u201CGerman\u201D) Set"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Annotation Statistics for the RWTH-PHOENIX-Weather Continuous
      Mouthing Challenge [1] Annotation Fraction [%] for Each of the Employed 11 Visemes
  Table 3 caption:
    table_text: TABLE 3 Achieved Word Error Rates on RWTH-PHOENIX-Weather 2014 T Continuous
      Sign Recognition Corpus with Cropped Hand and Face Inputs
  Table 4 caption:
    table_text: TABLE 4 Comparison to the State of the Art on RWTH-PHOENIX-Weather
      2014 Continuous Sign Recognition Corpus, Showing the 1-Stream and the 2-Stream
      Systems
  Table 5 caption:
    table_text: TABLE 5 Results on the Mouthing Sequences (Accuracy in [%], the Higher
      the Better)
  Table 6 caption:
    table_text: TABLE 6 Comparison of Proposed Multi-Stream CNN-LSTM-HMM Against Previously
      Published Results on the 1-Million-Hands Test Data
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2911077
- Affiliation of the first author: department of information and communication engineering,
    university of tokyo, bunkyo, japan
  Affiliation of the last author: department of information and communication engineering,
    university of tokyo, bunkyo, japan
  Figure 1 Link: articels_figures_by_rev_year\2019\Significance_of_SoftmaxBased_Features_in_Comparison_to_Distance_Metric_LearningB\figure_1.jpg
  Figure 1 caption: "Depiction of MNIST dataset. (a) Two-dimensional features obtained\
    \ by siamese network. (b) Two-dimensional features extracted from softmax-based\
    \ classifier; these features are well separated by angle but not by euclidean\
    \ norm. (c) Three-dimensional features extracted from softmax-based classifier;\
    \ we normalized these to have unit L2 norm and depict them in an azimuth\u2013\
    elevation coordinate system. The three-dimensional features are well separated\
    \ by their classes."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Significance_of_SoftmaxBased_Features_in_Comparison_to_Distance_Metric_LearningB\figure_2.jpg
  Figure 2 caption: Illustration of learning processes for softmax-based classification
    network and siamese-based DML network. For softmax, the gradient is defined by
    the distance between a sample and a fixed one-hot vector; for siamese by the distance
    between samples.
  Figure 3 Link: articels_figures_by_rev_year\2019\Significance_of_SoftmaxBased_Features_in_Comparison_to_Distance_Metric_LearningB\figure_3.jpg
  Figure 3 caption: GoogLeNet [14] architecture we use in this paper. We extracted
    the features of the red-colored layers. For (a), we applied PCA to reduce the
    number of feature dimensions. For (b) and (c), the dimensionality is reduced by
    the fcreduction layer.
  Figure 4 Link: articels_figures_by_rev_year\2019\Significance_of_SoftmaxBased_Features_in_Comparison_to_Distance_Metric_LearningB\figure_4.jpg
  Figure 4 caption: Comparisons between softmax-based features and lifted structured
    feature embedding [9] on NMI (clustering) and RecallK (retrieval) scores for the
    test set of the Caltech UCSD Birds 200-2011 (CUB) dataset. The dimension of the
    feature used in the retrieval experiments is 64.
  Figure 5 Link: articels_figures_by_rev_year\2019\Significance_of_SoftmaxBased_Features_in_Comparison_to_Distance_Metric_LearningB\figure_5.jpg
  Figure 5 caption: Comparisons between softmax-based features and lifted structured
    feature embedding [9] on NMI (clustering) and RecallK (retrieval) scores for the
    test set of the Stanford Cars 196 (CAR) dataset. The dimension of the feature
    used in the retrieval experiments is 64.
  Figure 6 Link: articels_figures_by_rev_year\2019\Significance_of_SoftmaxBased_Features_in_Comparison_to_Distance_Metric_LearningB\figure_6.jpg
  Figure 6 caption: Comparisons between softmax-based features and lifted structured
    feature embedding [9] on NMI (clustering) and RecallK (retrieval) scores for the
    test set of the Online Products (OP) dataset. The dimension of the feature used
    in the retrieval experiments is 64.
  Figure 7 Link: articels_figures_by_rev_year\2019\Significance_of_SoftmaxBased_Features_in_Comparison_to_Distance_Metric_LearningB\figure_7.jpg
  Figure 7 caption: 'CUB: NMI (clustering), and RecallK (retrieval) scores for test
    set of the Caltech UCSD Birds 200-2011 dataset under different dataset sizes.
    The feature dimensionality is fixed at 1,024.'
  Figure 8 Link: articels_figures_by_rev_year\2019\Significance_of_SoftmaxBased_Features_in_Comparison_to_Distance_Metric_LearningB\figure_8.jpg
  Figure 8 caption: 'CAR: NMI (clustering), and RecallK (retrieval) scores for test
    set of the Stanford Cars 196 dataset under different dataset sizes. The feature
    dimensionality is fixed at 256.'
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: Shota Horiguchi
  Name of the last author: Kiyoharu Aizawa
  Number of Figures: 8
  Number of Tables: 6
  Number of authors: 3
  Paper title: Significance of Softmax-Based Features in Comparison to Distance Metric
    Learning-Based Features
  Publication Date: 2019-04-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Properties of Datasets Used in Our Experiments
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 CUB: NMI (Clustering) and RecallK (Retrieval) Scores for
      the Test Set of the Caltech UCSD Birds 200-2011 (CUB) Dataset'
  Table 3 caption:
    table_text: 'TABLE 3 CAR: NMI (Clustering) and RecallK (Retrieval) Scores for
      the Test Set of the Stanford Cars 196 (CAR) Dataset'
  Table 4 caption:
    table_text: 'TABLE 4 OP: NMI (Clustering) and RecallK (Retrieval) Scores for the
      Test Set of the Online Product (OP) Dataset'
  Table 5 caption:
    table_text: 'TABLE 5 NMI Scores for the Test Set of the Caltech UCSD Birds 200-2011
      (CUB) Dataset: Comparisons of the Distance Metric with a Probability Invariant
      Shift and L2 Normalization'
  Table 6 caption:
    table_text: 'TABLE 6 NMI Scores for the Test Set of the Stanford Cars 196 (CAR)
      Dataset: Comparisons of the Distance Metric with a Probability Invariant Shift
      and L2 Normalization'
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2911075
