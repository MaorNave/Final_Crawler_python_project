- Affiliation of the first author: school of computer science, and the center for
    optical imagery analysis and learning (optimal), northwestern polytechnical university,
    xian, china
  Affiliation of the last author: school of computer science, and the center for optical
    imagery analysis and learning (optimal), northwestern polytechnical university,
    xian, china
  Figure 1 Link: articels_figures_by_rev_year\2018\Detecting_Coherent_Groups_in_Crowd_Scenes_by_Multiview_Clustering\figure_1.jpg
  Figure 1 caption: The pipeline of the proposed framework. First, an orientation
    graph is built according to the feature points orientation similarities. Then,
    a structural context descriptor is proposed to describe the structures of points.
    Third, the graphs are integrated by a novel self-weighted multiview clustering
    method. Finally, a merging approach is designed to combine the coherent subgroups.
  Figure 10 Link: articels_figures_by_rev_year\2018\Detecting_Coherent_Groups_in_Crowd_Scenes_by_Multiview_Clustering\figure_10.jpg
  Figure 10 caption: Performance comparison of MMSC and SMC L2 on four datasets. We
    can see that MMSC is sensitive to the value of gamma , while SMC L2 sustains good
    performance on different datasets.
  Figure 2 Link: articels_figures_by_rev_year\2018\Detecting_Coherent_Groups_in_Crowd_Scenes_by_Multiview_Clustering\figure_2.jpg
  Figure 2 caption: Crowd frames with low and high frame rates. Yellow points indicate
    the feature points, and the green circles indicate the neighborhood of the corresponding
    points (red color).
  Figure 3 Link: articels_figures_by_rev_year\2018\Detecting_Coherent_Groups_in_Crowd_Scenes_by_Multiview_Clustering\figure_3.jpg
  Figure 3 caption: Details about the construction of context graph.
  Figure 4 Link: articels_figures_by_rev_year\2018\Detecting_Coherent_Groups_in_Crowd_Scenes_by_Multiview_Clustering\figure_4.jpg
  Figure 4 caption: Comparison of groups (A) before and (B) after merging. Scatters
    with different colors indicate different detected groups, and arrows indicate
    motion orientations.
  Figure 5 Link: articels_figures_by_rev_year\2018\Detecting_Coherent_Groups_in_Crowd_Scenes_by_Multiview_Clustering\figure_5.jpg
  Figure 5 caption: Representative results of different group detection methods, and
    the changing trends of w o and w c with the number of iterations. Scatters with
    different colors indicate different detected groups, and arrows indicate motion
    orientations. The results of MPF L1 are consistent with the ground truth.
  Figure 6 Link: articels_figures_by_rev_year\2018\Detecting_Coherent_Groups_in_Crowd_Scenes_by_Multiview_Clustering\figure_6.jpg
  Figure 6 caption: The relative improvements of (A) MPF L1 and (B) MPF L2 compared
    with HC, CF, CT, CDC, and MCC.
  Figure 7 Link: articels_figures_by_rev_year\2018\Detecting_Coherent_Groups_in_Crowd_Scenes_by_Multiview_Clustering\figure_7.jpg
  Figure 7 caption: Performance of CF, CT, and MCC with varying k . Scatters with
    different colors indicate different detected groups, and arrows indicate motion
    orientations.
  Figure 8 Link: articels_figures_by_rev_year\2018\Detecting_Coherent_Groups_in_Crowd_Scenes_by_Multiview_Clustering\figure_8.jpg
  Figure 8 caption: Representative results of MPF L2 on MPT-20x100 dataset. Each point
    corresponds to a pedestrian (points are on the bottom of pedestrians). Scatters
    with different colors indicate different detected groups, and arrows indicate
    motion orientations.
  Figure 9 Link: articels_figures_by_rev_year\2018\Detecting_Coherent_Groups_in_Crowd_Scenes_by_Multiview_Clustering\figure_9.jpg
  Figure 9 caption: Clustering results on the block diagonal synthetic data by SMC
    L1 and SMC L2 methods.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Qi Wang
  Name of the last author: Feiping Nie
  Number of Figures: 12
  Number of Tables: 6
  Number of authors: 3
  Paper title: Detecting Coherent Groups in Crowd Scenes by Multiview Clustering
  Publication Date: 2018-10-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Comparison on Group Detection
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of MPF L2 and Its Variants
  Table 3 caption:
    table_text: TABLE 3 Performance on Group Number Estimation
  Table 4 caption:
    table_text: TABLE 4 Efficiency Comparison with Particle-Based Methods
  Table 5 caption:
    table_text: "TABLE 5 ACC (Mean \xB1 Standard Deviation%) on Multiview Datasets"
  Table 6 caption:
    table_text: "TABLE 6 F-score (Mean \xB1 Standard Deviation%) on Multiview Datasets"
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2875002
- Affiliation of the first author: australian centre for robotic vision, university
    of adelaide, adelaide, australia
  Affiliation of the last author: australian centre for robotic vision, university
    of adelaide, adelaide, australia
  Figure 1 Link: articels_figures_by_rev_year\2018\Approximate_Fisher_Information_Matrix_to_Characterize_the_Training_of_Deep_Neura\figure_1.jpg
  Figure 1 caption: "The evaluation of using different learning rates and mini-batch\
    \ sizes (a) and corresponding C \xAF K and L K values (b) at the last epoch for\
    \ the testing set of CIFAR-10 [9]. For (a) and (b), the intermediate points (i.e.,\
    \ mini-batch sizes) on each lr line are ignored for clarity, except for the top\
    \ five configurations that produce lowest testing errors. The stability of the\
    \ proposed measures over the first 10 epochs is shown in (c), illustrated by a\
    \ subset of the models with lr0.1. Finally, in (d), the subset of the models in\
    \ (c) has been used as \u201Cbeacons\u201D to guide the dynamic sampling training\
    \ (they share the same colors), i.e., we tune the runtime mini-batch size during\
    \ the training in order to push the C \xAF K and L K values close to the optimum\
    \ region in order to achieve accurate classification and fast training \u2013\
    \ in this example, s32-to-128 is located closer to the optimum region center than\
    \ the other dynamic sampling models: s16-to-256, s32-to-512, and s512-to-32 -\
    \ also s32-to-128 shows the lowest testing error amongst these four models (hereafter,\
    \ we use the following notation to represent the model hyper-parameter values:\
    \ smini\u2212batchsize\u2212lrlearningratevalue )."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Approximate_Fisher_Information_Matrix_to_Characterize_the_Training_of_Deep_Neura\figure_2.jpg
  Figure 2 caption: "An illustration of c,l k (the first row) and C,L k (the second\
    \ row) of tested ResNet models. These models are trained on CIFAR-10 with the\
    \ configurations of mini-batch sizes | B k |=8,\u2026,512 and initial learning\
    \ rate \u03B1 1 =0.1 , \u03B1 k is reduced 10 fold at k\u2208161 st epoch and\
    \ k\u2208241 st epochs. In general, the sampled measures c,l k are noisy, leading\
    \ to our proposal of the cumulative measures C,L k to assess the entire training\
    \ procedure. The x -axis is shown by epoch instead of iteration to align the readings\
    \ of different training configurations because the use of small mini-batch sizes\
    \ increases the number of iterations to complete an epoch."
  Figure 3 Link: articels_figures_by_rev_year\2018\Approximate_Fisher_Information_Matrix_to_Characterize_the_Training_of_Deep_Neura\figure_3.jpg
  Figure 3 caption: "This graph shows how mini-batch sizes | B k | and initial learning\
    \ rate \u03B1 k affect the training performance, which is also related to the\
    \ proposed measures C \xAF K and L K , on four common benchmarks and two model\
    \ architectures. We connect the models that use the same \u03B1 k value. Each\
    \ line omits the intermediate points for clarity except the the top five points\
    \ over all configurations with lowest testing errors. The gray texts, i.e., s8,\
    \ 2048, indicate the | B k | value at each end of the \u03B1 k -connected lines."
  Figure 4 Link: articels_figures_by_rev_year\2018\Approximate_Fisher_Information_Matrix_to_Characterize_the_Training_of_Deep_Neura\figure_4.jpg
  Figure 4 caption: "The proposed measurements (a) and (b) C \xAF K and (c) and (d)\
    \ L K for the training of the (a) and (c) ResNet and (b) and (d) DenseNet on CIFAR-10,\
    \ as a function of the number of training iterations (instead of epochs). The\
    \ black dotted vertical lines indicate the last iterations for the respective\
    \ experiments with the same batch size (the results of s8,512 are excluded to\
    \ avoid a cluttered presentation)."
  Figure 5 Link: articels_figures_by_rev_year\2018\Approximate_Fisher_Information_Matrix_to_Characterize_the_Training_of_Deep_Neura\figure_5.jpg
  Figure 5 caption: "This graph illustrates the \u201Ctravelling history\u201D of\
    \ C \xAF K and L K of sevral dynamic sampling models. Each model is represented\
    \ by a curve, where the plotted C \xAF K and L K values are extracted from 5,20,40,60,80,100%\
    \ of the total training epochs, forming six points on each line. The \u201Cbeacon\u201D\
    \ models are the corresponding s16, ..., 512-lr0.1 models from Fig. 3 (for each\
    \ dataset), which are denoted by dotted curves (not represented in the legend).\
    \ The mini-batch size of each dotted line is marked at the place where the training\
    \ is at 5 percent epochs. Furthermore, each dynamic sampling method is designed\
    \ to share the initial mini-batch size with one of the beacons, so we can observe\
    \ how they move from the \u201Croots\u201D in the graph."
  Figure 6 Link: articels_figures_by_rev_year\2018\Approximate_Fisher_Information_Matrix_to_Characterize_the_Training_of_Deep_Neura\figure_6.jpg
  Figure 6 caption: The (a) training and (b) testing error on CIFAR-10 of selected
    s32-to-512 (left), s512-to-32 (center) dynamic sampling models, and the s32-to-512-MS
    (right) multi-step learning rate decay variant.
  Figure 7 Link: articels_figures_by_rev_year\2018\Approximate_Fisher_Information_Matrix_to_Characterize_the_Training_of_Deep_Neura\figure_7.jpg
  Figure 7 caption: The proposed measurements quantifying the training of a toy input-500-500-output
    MLP network.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.55
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Zhibin Liao
  Name of the last author: Gustavo Carneiro
  Number of Figures: 7
  Number of Tables: 1
  Number of authors: 4
  Paper title: Approximate Fisher Information Matrix to Characterize the Training
    of Deep Neural Networks
  Publication Date: 2018-10-16 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 The Comparison between the Best Beacon Model s (at lr0.1),\
      \ and the Best Dynamic Sampling Models - \u2205 \u2205, and -MS"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2876413
- Affiliation of the first author: software school, xiamen university, xiamen, china
  Affiliation of the last author: software school, xiamen university, xiamen, china
  Figure 1 Link: articels_figures_by_rev_year\2018\Neural_Machine_Translation_with_Deep_Attention\figure_1.jpg
  Figure 1 caption: Illustration of the proposed DeepAtt. We use blue and red color
    to indicate the source and target side, respectively. The yellow and gray color
    denotes the information flow for target word prediction and attention respectively.
    Notice that we draw the encoder on the right and the decoder on the left for clarity.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Neural_Machine_Translation_with_Deep_Attention\figure_2.jpg
  Figure 2 caption: BLEU score and translation length on different length groups of
    source sentences.
  Figure 3 Link: articels_figures_by_rev_year\2018\Neural_Machine_Translation_with_Deep_Attention\figure_3.jpg
  Figure 3 caption: BLEU score of different systems on all test sets under different
    numbers of layers.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.93
  Name of the first author: Biao Zhang
  Name of the last author: Jinsong Su
  Number of Figures: 3
  Number of Tables: 7
  Number of authors: 3
  Paper title: Neural Machine Translation with Deep Attention
  Publication Date: 2018-10-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Case-Insensitive BLEU Scores on the Chinese-English Translation
      Task
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 AER Scores of Word Alignments
  Table 3 caption:
    table_text: TABLE 3 Case-Insensitive BLEU Scores on Specific Context Words
  Table 4 caption:
    table_text: TABLE 4 Examples Generated by Different Systems
  Table 5 caption:
    table_text: TABLE 5 Case-Insensitive BLEU Scores of Advanced Systems on the Chinese-English
      Translation Task
  Table 6 caption:
    table_text: TABLE 6 Case-Sensitive BLEU Scores on WMT14 English-German Translation
      Task
  Table 7 caption:
    table_text: TABLE 7 Case-sensitive BLEU Scores on WMT14 English-French Translation
      Task
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2876404
- Affiliation of the first author: school of electronic information and communications,
    huazhong university of science and technology, wuhan, china
  Affiliation of the last author: departments of cognitive science and computer science,
    johns hopkins university, baltimore, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\PCL_Proposal_Cluster_Learning_for_Weakly_Supervised_Object_Detection\figure_1.jpg
  Figure 1 caption: "Different proposals cover different parts of objects. All these\
    \ proposals can be classified as \u201Cbird\u201D but only the green boxes, which\
    \ have enough IoU with groundtruth, contribute to correct detections."
  Figure 10 Link: articels_figures_by_rev_year\2018\PCL_Proposal_Cluster_Learning_for_Weakly_Supervised_Object_Detection\figure_10.jpg
  Figure 10 caption: Some detection results for class bicycle, bus, cat, chair, dog,
    motorbike, person, and train (in each image only the top-scoring box is shown).
    Green rectangle indicates success cases (IoU > 0.5), and red rectangle indicates
    failure cases (IoU < 0.5).
  Figure 2 Link: articels_figures_by_rev_year\2018\PCL_Proposal_Cluster_Learning_for_Weakly_Supervised_Object_Detection\figure_2.jpg
  Figure 2 caption: The proposals (b), of an image (a), can be grouped into different
    proposal clusters (c). Proposals with the same color in (c) belong to the same
    cluster (red indicates background).
  Figure 3 Link: articels_figures_by_rev_year\2018\PCL_Proposal_Cluster_Learning_for_Weakly_Supervised_Object_Detection\figure_3.jpg
  Figure 3 caption: "(a) Conventional MIL networks transfer the instance classification\
    \ (object detection) problem to a bag classification (image classification) problem.\
    \ (b) We propose to generate proposal clusters and assign proposals the label\
    \ of the corresponding object class for each cluster. (c) We propose to treat\
    \ each proposal cluster as a small new bag. \u201C0\u201D, \u201C1\u201D, and\
    \ \u201C2\u201D indicate the \u201Cbackground\u201D, \u201Cmotorbike\u201D, and\
    \ \u201Ccar\u201D, respectively."
  Figure 4 Link: articels_figures_by_rev_year\2018\PCL_Proposal_Cluster_Learning_for_Weakly_Supervised_Object_Detection\figure_4.jpg
  Figure 4 caption: 'The architecture of our method. All arrows are utilized during
    the forward process of training, only the solid ones have back-propagation computations,
    and only the blue ones are used during testing. During the forward process of
    training, an image and its proposal boxes are fed into the CNN which involves
    a series of convolutional layers, an SPP layer, and two fully connected layers
    to produce proposal features. These proposal features are branched into many streams:
    the first one for the basic MIL network and the other ones for iterative instance
    classifier refinement. Each stream outputs a set of proposal scores and generates
    proposal clusters consequently. Based on these proposal clusters, supervisions
    are generated to compute losses for the next stream. During the back-propagation
    process of training, proposal features and classifiers are trained according to
    the network losses. All streams share the same proposal features.'
  Figure 5 Link: articels_figures_by_rev_year\2018\PCL_Proposal_Cluster_Learning_for_Weakly_Supervised_Object_Detection\figure_5.jpg
  Figure 5 caption: "Results on VOC 2007 for different refinement times and different\
    \ training strategies, where \u201CPCL-xx-H\u201D and \u201CPCL-xx-G\u201D indicate\
    \ the highest scoring proposal based method and the graph-based method to generate\
    \ proposal clusters, respectively, \u201CPCL-OL-x\u201D and \u201CPCL-OB-x\u201D\
    \ indicate the directly assigning label method and the treating clusters as bags\
    \ method to train the network online, respectively, and \u201CPCL-AB-x\u201D indicates\
    \ using the alternating training strategy."
  Figure 6 Link: articels_figures_by_rev_year\2018\PCL_Proposal_Cluster_Learning_for_Weakly_Supervised_Object_Detection\figure_6.jpg
  Figure 6 caption: Results on VOC 2007 for different IoU threshold I t .
  Figure 7 Link: articels_figures_by_rev_year\2018\PCL_Proposal_Cluster_Learning_for_Weakly_Supervised_Object_Detection\figure_7.jpg
  Figure 7 caption: Results on VOC 2007 for different IoU threshold Iprime t .
  Figure 8 Link: articels_figures_by_rev_year\2018\PCL_Proposal_Cluster_Learning_for_Weakly_Supervised_Object_Detection\figure_8.jpg
  Figure 8 caption: Some learned proposal clusters. The proposals with white are cluster
    centers. Proposals with the same color belong to the same cluster. We omit the
    background cluster for simplification.
  Figure 9 Link: articels_figures_by_rev_year\2018\PCL_Proposal_Cluster_Learning_for_Weakly_Supervised_Object_Detection\figure_9.jpg
  Figure 9 caption: Some visualization comparisons among the WSDDN [17], the WSDDN+context
    [18], and our method (PCL) (in each image only the top-scoring box is shown).
    Green rectangle indicates success cases (IoU > 0.5), red rectangle indicates failure
    cases (IoU < 0.5), and yellow rectangle indicates groundtruths. The first four
    rows show examples that our method outperforms other two methods (with larger
    IoU). The fifth row shows examples that our method is worse than other two methods
    (with smaller IoU). The last row shows failure examples for both three methods.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Peng Tang
  Name of the last author: Alan Yuille
  Number of Figures: 10
  Number of Tables: 7
  Number of authors: 7
  Paper title: 'PCL: Proposal Cluster Learning for Weakly Supervised Object Detection'
  Publication Date: 2018-10-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Results (AP in % %) for Different Methods on the VOC 2007
      Test Set
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results (CorLoc in % %) for Different Methods on the VOC 2007
      Trainval Set
  Table 3 caption:
    table_text: TABLE 3 Results (AP in % %) for Different Methods on the VOC 2012
      Test Set
  Table 4 caption:
    table_text: TABLE 4 Results (CorLoc in % %) for Different Methods on the VOC 2012
      Trainval Set
  Table 5 caption:
    table_text: TABLE 5 Results (mAP in % %) for Different Methods on the ImageNet
      Dataset
  Table 6 caption:
    table_text: TABLE 6 Results (mAP0.5 and mAP[.5, .95] in % %) of Different Methods
      on the MS-COCO Dataset
  Table 7 caption:
    table_text: "TABLE 7 Runtime Comparisons between Our Method (\u201CPCL\u201D in\
      \ Table) and Our Basic MIL Network [17] (\u201CBasic\u201D in Table)"
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2876304
- Affiliation of the first author: data and web science group, university of mannheim,
    mannheim, germany
  Affiliation of the last author: max planck institute for informatics, saarbruecken,
    germany
  Figure 1 Link: articels_figures_by_rev_year\2018\Motion_Segmentation__Multiple_Object_Tracking_by_Correlation_CoClustering\figure_1.jpg
  Figure 1 caption: 'Left: Frames 100, 110, and 120 of the sequence MOT16-08 [50].
    Right: Segmentation and tracking result are depicted as color-labeled point trajectories
    and bounding boxes, respectively. Formulating bottom-up motion segmentation and
    top-down multiple object tracking as a joint co-clustering problem, combines advantages
    of both approaches and is tolerant even to strong partial occlusion, indicated
    by the black arrow. It establishes links between low-level concepts (point trajectories)
    and high-level concepts (bounding boxes).'
  Figure 10 Link: articels_figures_by_rev_year\2018\Motion_Segmentation__Multiple_Object_Tracking_by_Correlation_CoClustering\figure_10.jpg
  Figure 10 caption: The average pedestrian shape template and the trajectory-tracklet
    edges used for the comparison to subgraph multicut [67].
  Figure 2 Link: articels_figures_by_rev_year\2018\Motion_Segmentation__Multiple_Object_Tracking_by_Correlation_CoClustering\figure_2.jpg
  Figure 2 caption: Here, we visualize an exemplary graph G built on a two-frame video
    sequence showing two walking pedestrians. (a) At a high level, bounding boxes
    describe feasible detections of humans. At a low level, trajectories describe
    feasible motions of points. (b) Both are represented here by nodes in a graph.
    Nodes drawn as rectangles represent bounding boxes, nodes drawn as circles represent
    point trajectories. (c) An optimal decomposition of the graph defines, first,
    a grouping of point trajectories, second, a clustering of bounding boxes, and
    third, an assignment of point trajectories to bounding boxes.
  Figure 3 Link: articels_figures_by_rev_year\2018\Motion_Segmentation__Multiple_Object_Tracking_by_Correlation_CoClustering\figure_3.jpg
  Figure 3 caption: "Edges e lh between high and low level nodes. For every detection\
    \ v , the template T v is evaluated at the spatial location of every trajectory\
    \ u\u2208 V low . An edge with an attractive cost c lh e is introduced if u intersects\
    \ with T v in a location of high object probability (green edges). If u misses\
    \ the template T v and the distance d sp2 (u,v) to the center of T v is larger\
    \ than a threshold \u03C3 (indicated by the gray circle), an edge with repulsive\
    \ edge cost is introduced (red). If u intersects with T v in a location of low\
    \ object probability and the distance is smaller than \u03C3 , no edge is introduced."
  Figure 4 Link: articels_figures_by_rev_year\2018\Motion_Segmentation__Multiple_Object_Tracking_by_Correlation_CoClustering\figure_4.jpg
  Figure 4 caption: Examples of the faster R-CNN object detections on images from
    FBMS59 sequences [54]. The first row shows the best 20 detections. The second
    row shows three exemplary templates T generated with DeepLab [17], [55] on these
    detections.
  Figure 5 Link: articels_figures_by_rev_year\2018\Motion_Segmentation__Multiple_Object_Tracking_by_Correlation_CoClustering\figure_5.jpg
  Figure 5 caption: Examples of CCC segmentation results densified by the variational
    method of Ochs et al. [52] on three sequences of the FBMS59 [54] benchmark.
  Figure 6 Link: articels_figures_by_rev_year\2018\Motion_Segmentation__Multiple_Object_Tracking_by_Correlation_CoClustering\figure_6.jpg
  Figure 6 caption: Comparison of the proposed CCC model and the trajectory multicut
    (MCe) [40] on the marple6 sequence of FBMS59. While MCe cannot properly segment
    the persons, the tracking information from the bounding box subgraph helps our
    joint model to segment the two men throughout the sequence despite scaling and
    rotational motion. Additionally, static, consistently detected objects like the
    car in the first part of the sequence are segmented as well. As these are not
    annotated, this causes over-segmentation penalty on the FBMS59 metrics.
  Figure 7 Link: articels_figures_by_rev_year\2018\Motion_Segmentation__Multiple_Object_Tracking_by_Correlation_CoClustering\figure_7.jpg
  Figure 7 caption: Segmentation and tracking results of the proposed CCC model and
    the trajectory multicut (MCe) [40] on the horses06 sequence of FBMS59. MCe cannot
    segment the person and the horse next to him due to the difficult motion and strong
    partial occlusions.
  Figure 8 Link: articels_figures_by_rev_year\2018\Motion_Segmentation__Multiple_Object_Tracking_by_Correlation_CoClustering\figure_8.jpg
  Figure 8 caption: The average pedestrian shape template used for the computation
    of pairwise terms between pedestrian detections and trajectories.
  Figure 9 Link: articels_figures_by_rev_year\2018\Motion_Segmentation__Multiple_Object_Tracking_by_Correlation_CoClustering\figure_9.jpg
  Figure 9 caption: Results of the proposed correlation co-clustering model on the
    tud-crossing sequence from MOT15.
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Margret Keuper
  Name of the last author: Bernt Schiele
  Number of Figures: 10
  Number of Tables: 8
  Number of authors: 5
  Paper title: Motion Segmentation & Multiple Object Tracking by Correlation Co-Clustering
  Publication Date: 2018-10-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Results for the FBMS-59 Training and Test Set
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Multi-Target Tracking Results on the 2D MOT 2015 Benchmark
  Table 3 caption:
    table_text: TABLE 3 Multi-Target Tracking Results on the MOT16 Benchmark
  Table 4 caption:
    table_text: TABLE 4 Multi-Target Tracking Results on the MOT17 Challenge
  Table 5 caption:
    table_text: TABLE 5 Motion Segmentation on the Multi-Target Tracking Sequence
      Tud-Crossing
  Table 6 caption:
    table_text: TABLE 6 Motion Segmentation on the Tud-Crossing Sequence from MOT15
  Table 7 caption:
    table_text: TABLE 7 Segmentation Evaluation on the PETS-S2L2 Sequence from MOT15
  Table 8 caption:
    table_text: TABLE 8 Tracking Result on Multi-Target Tracking Sequences TUD-Campus,
      TUD-Crossing [3], and ParkingLot [80]
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2876253
- Affiliation of the first author: max-planck-institute for informatics, saarbrcken,
    germany
  Affiliation of the last author: max-planck-institute for informatics, saarbrcken,
    germany
  Figure 1 Link: articels_figures_by_rev_year\2018\HighFidelity_Monocular_Face_Reconstruction_Based_on_an_Unsupervised_ModelBased_F\figure_1.jpg
  Figure 1 caption: Our deep model-based face autoencoder enables unsupervised end-to-end
    learning of semantic parameters, such as pose, shape, expression, skin reflectance,
    and illumination. An optional landmark-based surrogate loss enables faster convergence
    and improved reconstruction results, see Section 6. Both scenarios require no
    supervision of the semantic parameters during training.
  Figure 10 Link: articels_figures_by_rev_year\2018\HighFidelity_Monocular_Face_Reconstruction_Based_on_an_Unsupervised_ModelBased_F\figure_10.jpg
  Figure 10 caption: Comparison to Jackson et al. [45]. Our approach obtains higher
    quality reconstructions while also estimating the reflectance and incident scene
    illumination.
  Figure 2 Link: articels_figures_by_rev_year\2018\HighFidelity_Monocular_Face_Reconstruction_Based_on_an_Unsupervised_ModelBased_F\figure_2.jpg
  Figure 2 caption: "Quantitative evaluation of stochastic sampling on real data.\
    \ Even drastic sampling of \u22482% of vertices only marginally reduces the quality\
    \ of the reconstruction results."
  Figure 3 Link: articels_figures_by_rev_year\2018\HighFidelity_Monocular_Face_Reconstruction_Based_on_an_Unsupervised_ModelBased_F\figure_3.jpg
  Figure 3 caption: Qualitative comparison of MoFA with and without stochastic sampling.
    Stochastic sampling of vertices lets us train networks much faster with comparable
    results to networks trained using all vertices.
  Figure 4 Link: articels_figures_by_rev_year\2018\HighFidelity_Monocular_Face_Reconstruction_Based_on_an_Unsupervised_ModelBased_F\figure_4.jpg
  Figure 4 caption: Our approach enables the regression of high quality pose, shape,
    expression, skin reflectance and illumination from just a single monocular image
    (images from CelebA [64]).
  Figure 5 Link: articels_figures_by_rev_year\2018\HighFidelity_Monocular_Face_Reconstruction_Based_on_an_Unsupervised_ModelBased_F\figure_5.jpg
  Figure 5 caption: Sample images of our real world training corpus.
  Figure 6 Link: articels_figures_by_rev_year\2018\HighFidelity_Monocular_Face_Reconstruction_Based_on_an_Unsupervised_ModelBased_F\figure_6.jpg
  Figure 6 caption: Comparison to Richardson et al. [12], [13] on 300-VW [65], [66],
    [67] (left) and LFW [68] (right). Our approach obtains higher reconstruction quality
    and provides estimates of colored reflectance and illumination. Note, in [12],
    [13] the grayscale reflectance is not regressed but obtained via optimization.
    We on the other hand regress all parameters (including reflectance) at once.
  Figure 7 Link: articels_figures_by_rev_year\2018\HighFidelity_Monocular_Face_Reconstruction_Based_on_an_Unsupervised_ModelBased_F\figure_7.jpg
  Figure 7 caption: Comparison to Tran et al. [14] on LFW [68]. Our approach obtains
    visually similar quality. Here, we show the full face model, but training only
    uses the frontal part (cf. Fig. 1, right).
  Figure 8 Link: articels_figures_by_rev_year\2018\HighFidelity_Monocular_Face_Reconstruction_Based_on_an_Unsupervised_ModelBased_F\figure_8.jpg
  Figure 8 caption: Comparison to the monocular reconstruction approach of [4] on
    CelebA [64]. Our approach obtains similar or higher quality, while being orders
    of magnitude faster (4 ms versus approx 500 ms).
  Figure 9 Link: articels_figures_by_rev_year\2018\HighFidelity_Monocular_Face_Reconstruction_Based_on_an_Unsupervised_ModelBased_F\figure_9.jpg
  Figure 9 caption: We compare to our implementation of the high quality off-line
    monocular reconstruction approach of [8]. We obtain similar quality without requiring
    landmarks as input. Without landmarks, [8] often gets stuck in a local minimum.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ayush Tewari
  Name of the last author: Christian Theobalt
  Number of Figures: 24
  Number of Tables: 2
  Number of authors: 7
  Paper title: High-Fidelity Monocular Face Reconstruction Based on an Unsupervised
    Model-Based Face Autoencoder
  Publication Date: 2018-10-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Evaluation on Real Data
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Geometric Error on 180 Meshes of the FaceWarehouse [71] Dataset
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2876842
- Affiliation of the first author: college of intelligence and computing, tianjin
    university, tianjin, china
  Affiliation of the last author: school of electrical and information engineering,
    university of sydney, sydney, australia
  Figure 1 Link: articels_figures_by_rev_year\2018\Generalized_Latent_MultiView_Subspace_Clustering\figure_1.jpg
  Figure 1 caption: "Illustration of multi-view latent representation. Observations\
    \ X (v) V v=1 ( V\u22652 ) corresponding to different views are partially projected\
    \ by P (v) V v=1 from one underlying latent representation H ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Generalized_Latent_MultiView_Subspace_Clustering\figure_2.jpg
  Figure 2 caption: Illustration of the proposed generalized Latent Multi-view Subspace
    Clustering (gLMSC). The latent representation non-linearly encodes the information
    from multiple views with neural networks for uncovering the data distribution
    in subspaces. Our model can also be considered as an unsupervised multi-view representation
    learning method, where the learned representation could be used for other potential
    applications. For comparison, the dashlines indicate the linear LMSC (lLMSC) mentioned
    in Section 3.1.
  Figure 3 Link: articels_figures_by_rev_year\2018\Generalized_Latent_MultiView_Subspace_Clustering\figure_3.jpg
  Figure 3 caption: Experiments to evaluate the robustness of multi-view and single-view
    methods on synthetic data.
  Figure 4 Link: articels_figures_by_rev_year\2018\Generalized_Latent_MultiView_Subspace_Clustering\figure_4.jpg
  Figure 4 caption: Visualization of different views and latent representation with
    t-SNE on the MSRCV1 dataset.
  Figure 5 Link: articels_figures_by_rev_year\2018\Generalized_Latent_MultiView_Subspace_Clustering\figure_5.jpg
  Figure 5 caption: "Results of our method when using different parameters: \u03BB\
    \ (top row) and k (bottom row)."
  Figure 6 Link: articels_figures_by_rev_year\2018\Generalized_Latent_MultiView_Subspace_Clustering\figure_6.jpg
  Figure 6 caption: Convergence of our method. For a better view, the plots are normalized
    into the range [0, 1].
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Changqing Zhang
  Name of the last author: Dong Xu
  Number of Figures: 6
  Number of Tables: 4
  Number of authors: 7
  Paper title: Generalized Latent Multi-View Subspace Clustering
  Publication Date: 2018-10-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Main Notations Used Throughout the Paper
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance Comparison of Different Clustering Methods
  Table 3 caption:
    table_text: TABLE 3 Performance Comparison of Different Clustering Methods
  Table 4 caption:
    table_text: TABLE 4 Performance Comparison between Single View and the Learned
      Latent Representation
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2877660
- Affiliation of the first author: department of electronic engineering, chinese university
    of hong kong, hong kong
  Affiliation of the last author: department of electronic engineering, chinese university
    of hong kong, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2018\Visibility_Constrained_Generative_Model_for_DepthBased_D_Facial_Pose_Tracking\figure_1.jpg
  Figure 1 caption: Our identity-adaptive facial pose tracking system is robust to
    occlusions and expressions. (a) Poses are estimated under heavy occlusions. The
    face models are overlaid with the point clouds. The visible face regions marked
    in red. (b) Poses are tracked under varying expressions. The estimated face identities
    are not interfered by expressions.
  Figure 10 Link: articels_figures_by_rev_year\2018\Visibility_Constrained_Generative_Model_for_DepthBased_D_Facial_Pose_Tracking\figure_10.jpg
  Figure 10 caption: The pose accuracy comparisons between generic and personalized
    face models, in terms of statistics of rotation and translation errors. The left
    column shows the histograms of angle errors, and the right column visualizes the
    histograms of translation errors. The brown bars indicate the performance by the
    personalized face model while the blue ones are for the generic face model.
  Figure 2 Link: articels_figures_by_rev_year\2018\Visibility_Constrained_Generative_Model_for_DepthBased_D_Facial_Pose_Tracking\figure_2.jpg
  Figure 2 caption: "The statistics of the face model trained in the FaceWarehouse\
    \ dataset [25]. (a) Overall shape variation. (b)\u2013(c) Shape variations by\
    \ w id and w exp , respectively. (d) Shape variation by the residual term in Eq.\
    \ (2). The shape variation is set as one standard deviation of the marginalized\
    \ per-vertex distribution."
  Figure 3 Link: articels_figures_by_rev_year\2018\Visibility_Constrained_Generative_Model_for_DepthBased_D_Facial_Pose_Tracking\figure_3.jpg
  Figure 3 caption: "Overview of the propose probabilistic framework, which consists\
    \ of 1) robust facial motion tracking and 2) online switchable identity adaptation.\
    \ For both components, the generative model p (t) M (f) acts as the key intermediate\
    \ and it is updated immediately with the feedback of the identity adaptation.\
    \ The input to the system is the depth map while the output is the rigid pose\
    \ parameter \u03B8 (t) and the updated face identity parameters \u03BC (t) id\
    \ , \u03A3 (t) id that encode the identity distribution p (t) ( w id ) . Note\
    \ that the color image is for illustration but not used in our system."
  Figure 4 Link: articels_figures_by_rev_year\2018\Visibility_Constrained_Generative_Model_for_DepthBased_D_Facial_Pose_Tracking\figure_4.jpg
  Figure 4 caption: Samples of the occluded faces.
  Figure 5 Link: articels_figures_by_rev_year\2018\Visibility_Constrained_Generative_Model_for_DepthBased_D_Facial_Pose_Tracking\figure_5.jpg
  Figure 5 caption: Illustration of the ray visibility constraint. A profiled face
    model and a curve on the point cloud are in front of a depth camera. (a) A part
    of face points fit the curve, while the rest points are occluded. (b) The face
    model is completely occluded. (c) An infeasible case where the face model occludes
    the point cloud curve.
  Figure 6 Link: articels_figures_by_rev_year\2018\Visibility_Constrained_Generative_Model_for_DepthBased_D_Facial_Pose_Tracking\figure_6.jpg
  Figure 6 caption: "(a) The signed distance through a ray v \u20D7 q n , where n\
    \ n is the normal vector at the surface point p n that intersects v \u20D7 q n\
    \ . (b) Visibility of the face model with respect to the point cloud. The occlusion\
    \ space is marked in blue, while the visible points are marked in red. (c) Illustration\
    \ of the surface distribution N( y n |0, \u03C3 2 o ) and the projected face distribution\
    \ p Q\u2192P ( y n ;\u03B8) , where the overlapping interval in between illustrates\
    \ whether the face model is visible."
  Figure 7 Link: articels_figures_by_rev_year\2018\Visibility_Constrained_Generative_Model_for_DepthBased_D_Facial_Pose_Tracking\figure_7.jpg
  Figure 7 caption: "Examples of our rigid pose estimation by the generic face model.\
    \ (a)\u2013(b) Color images and the corresponded point clouds. (c) Initial alignment\
    \ provided by the proposed face localization method. (d) The proposed rigid pose\
    \ estimation results."
  Figure 8 Link: articels_figures_by_rev_year\2018\Visibility_Constrained_Generative_Model_for_DepthBased_D_Facial_Pose_Tracking\figure_8.jpg
  Figure 8 caption: Comparison of the rigid pose estimation methods with the generic
    face model. (a) and (b) Color image and its corresponded point cloud. (c) Two
    views of the initial alignment provided by face localization. (d) Result by iterative
    closest points (ICP) [43]. (e) Result by maximizing the log-likeligood log pmathcal
    Qrightarrow mathcal P(mathbf y;boldsymboltheta ) . (f) Result by minimizing the
    ray visibility score. (g) The augmented RVS method by particle swarm optimization.
  Figure 9 Link: articels_figures_by_rev_year\2018\Visibility_Constrained_Generative_Model_for_DepthBased_D_Facial_Pose_Tracking\figure_9.jpg
  Figure 9 caption: Tracking results on the (a) Biwi and (b) ICT-3DHP dataset with
    the personalized face models. Our system is robust to the profiled faces and occlusions,
    and is also effective to facial expression variations.
  First author gender probability: 0.95
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: Lu Sheng
  Name of the last author: King Ngi Ngan
  Number of Figures: 17
  Number of Tables: 4
  Number of authors: 5
  Paper title: Visibility Constrained Generative Model for Depth-Based 3D Facial Pose
    Tracking
  Publication Date: 2018-10-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Facial Pose Datasets Summary
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Component-Wise Runtime Comparison (MATLAB Platform)
  Table 3 caption:
    table_text: TABLE 3 Evaluations on Biwi Dataset
  Table 4 caption:
    table_text: TABLE 4 Evaluations on ICT-3DHP Dataset
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2877675
- Affiliation of the first author: "computer science department, technion\u2014israel\
    \ institute of technology, haifa, israel"
  Affiliation of the last author: "computer science department, technion\u2014israel\
    \ institute of technology, haifa, israel"
  Figure 1 Link: articels_figures_by_rev_year\2018\Efficient_InterGeodesic_Distance_Computation_and_Fast_Classical_Scaling\figure_1.jpg
  Figure 1 caption: "Reconstruction of e (a column of E ) on a flat surface. Left:\
    \ The true values of e . The chosen n=13 samples are marked by red points. Middle\
    \ and right: The reconstructed function e \u2217 =Mr using the Dirichlet and Laplacian\
    \ energies. For comparison, we colored the function according to the absolute\
    \ error | e \u2217 \u2212e| ."
  Figure 10 Link: articels_figures_by_rev_year\2018\Efficient_InterGeodesic_Distance_Computation_and_Fast_Classical_Scaling\figure_10.jpg
  Figure 10 caption: Approximated distance as a function of the true distance, using
    SMDS, FMDS, NMDS, Geodesics in Heat and the best low-rank reconstruction. For
    better visualization, the plots are shifted horizontally by i=0,1,2,3 , respectively.
    In this experiment we used n=100 .
  Figure 2 Link: articels_figures_by_rev_year\2018\Efficient_InterGeodesic_Distance_Computation_and_Fast_Classical_Scaling\figure_2.jpg
  Figure 2 caption: "Reconstruction of e (a column of E ) on a curved surface. Left:\
    \ The curved surface. Middle: The true values of the distance function e , measured\
    \ from the middle point of the surface, and sampled at n=30 red points. Right:\
    \ The reconstructed function e \u2217 =Mr using the suggested energy."
  Figure 3 Link: articels_figures_by_rev_year\2018\Efficient_InterGeodesic_Distance_Computation_and_Fast_Classical_Scaling\figure_3.jpg
  Figure 3 caption: "Average reconstruction error w.r.t. the penalty parameter \u03BC\
    \ ."
  Figure 4 Link: articels_figures_by_rev_year\2018\Efficient_InterGeodesic_Distance_Computation_and_Fast_Classical_Scaling\figure_4.jpg
  Figure 4 caption: Demonstration of the partition of E , assuming the columns were
    chosen as the first ones. R s and C s are a part of the matrices R and C . Note
    that C and C s are subsets of R and R s .
  Figure 5 Link: articels_figures_by_rev_year\2018\Efficient_InterGeodesic_Distance_Computation_and_Fast_Classical_Scaling\figure_5.jpg
  Figure 5 caption: "The reconstruction error with respect to n 1 , given by \u2225\
    \ E \u2212E \u2225 F \u2225E \u2225 F , using the learning approach (CUR) and\
    \ its proposed modification. E is computed from the giraffe shape in the SHREC\
    \ database (see experimental results)."
  Figure 6 Link: articels_figures_by_rev_year\2018\Efficient_InterGeodesic_Distance_Computation_and_Fast_Classical_Scaling\figure_6.jpg
  Figure 6 caption: Shapes in different poses from the SHREC database, and their corresponding
    canonical forms obtained by NMDS. Here, each shape consists of 10 4 vertices.
  Figure 7 Link: articels_figures_by_rev_year\2018\Efficient_InterGeodesic_Distance_Computation_and_Fast_Classical_Scaling\figure_7.jpg
  Figure 7 caption: "Canonical forms of giraffe and hand shapes, using n=100 samples\
    \ for the compared methods. The stress error 100 p 2 \u2225Z Z T + 1 2 JEJ \u2225\
    \ F of each embedding is displayed at the bottom of its corresponding form."
  Figure 8 Link: articels_figures_by_rev_year\2018\Efficient_InterGeodesic_Distance_Computation_and_Fast_Classical_Scaling\figure_8.jpg
  Figure 8 caption: Distance of approximated canonical forms to the true one with
    respect the number of samples n , using different methods.
  Figure 9 Link: articels_figures_by_rev_year\2018\Efficient_InterGeodesic_Distance_Computation_and_Fast_Classical_Scaling\figure_9.jpg
  Figure 9 caption: The reconstruction error of D with respect to the number of samples
    n , using different methods.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Gil Shamai
  Name of the last author: Ron Kimmel
  Number of Figures: 16
  Number of Tables: 1
  Number of authors: 3
  Paper title: Efficient Inter-Geodesic Distance Computation and Fast Classical Scaling
  Publication Date: 2018-10-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 (a) Accuracy, (b) Query, and (c) Preprocessing Time of Geodesic
      Distance Approximations Using FMDS, NMDS and CTP
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2877961
- Affiliation of the first author: department of computational and data sciences,
    video analytics lab, indian institute of science, bangalore, india
  Affiliation of the last author: department of computational and data sciences, video
    analytics lab, indian institute of science, bangalore, india
  Figure 1 Link: articels_figures_by_rev_year\2018\PictionaryStyle_Word_Guessing_on_HandDrawn_Object_Sketches_Dataset_Analysis_and_\figure_1.jpg
  Figure 1 caption: We propose a deep recurrent model of Pictionary-style word guessing.
    Such models can enable social robots to participate in real-life game scenarios
    as shown above. Picture credit:Trisha Mittal.
  Figure 10 Link: articels_figures_by_rev_year\2018\PictionaryStyle_Word_Guessing_on_HandDrawn_Object_Sketches_Dataset_Analysis_and_\figure_10.jpg
  Figure 10 caption: Distribution of ratings for human and machine-generated guesses.
  Figure 2 Link: articels_figures_by_rev_year\2018\PictionaryStyle_Word_Guessing_on_HandDrawn_Object_Sketches_Dataset_Analysis_and_\figure_2.jpg
  Figure 2 caption: 'The time-line of a typical Sketch-QA guessing session: Every
    time a stroke is added, the subject either inputs a best-guess word of the object
    being drawn (stroke 5,10). In case existing strokes do not offer enough clues,
    heshe requests the next stroke be drawn. After the final stroke (15), the subject
    is informed the objects ground-truth category.'
  Figure 3 Link: articels_figures_by_rev_year\2018\PictionaryStyle_Word_Guessing_on_HandDrawn_Object_Sketches_Dataset_Analysis_and_\figure_3.jpg
  Figure 3 caption: Here, x -axis denotes the categories. y -axis denotes the number
    of sketches within the category with multiple guesses. The categories are shown
    sorted by the number of sketches which elicited multiple guesses.
  Figure 4 Link: articels_figures_by_rev_year\2018\PictionaryStyle_Word_Guessing_on_HandDrawn_Object_Sketches_Dataset_Analysis_and_\figure_4.jpg
  Figure 4 caption: The distribution of first guess locations normalized ([0,1]) over
    sequence lengths ( y -axis) across categories ( x -axis).
  Figure 5 Link: articels_figures_by_rev_year\2018\PictionaryStyle_Word_Guessing_on_HandDrawn_Object_Sketches_Dataset_Analysis_and_\figure_5.jpg
  Figure 5 caption: Categories sorted by the median location of first guess.
  Figure 6 Link: articels_figures_by_rev_year\2018\PictionaryStyle_Word_Guessing_on_HandDrawn_Object_Sketches_Dataset_Analysis_and_\figure_6.jpg
  Figure 6 caption: 'Some examples of misclassifications: Human guesses are shown
    in blue. Ground-truth category labels are in pink.'
  Figure 7 Link: articels_figures_by_rev_year\2018\PictionaryStyle_Word_Guessing_on_HandDrawn_Object_Sketches_Dataset_Analysis_and_\figure_7.jpg
  Figure 7 caption: "The architecture for our deep neural model of word guessing.\
    \ The rectangular bars correspond to guess-word embeddings. M3 corresponds to\
    \ the CNN regressor whose penultimate layers outputs are used as input features\
    \ to the LSTM model. \u201C\u201D reflects our choice of modelling no guess as\
    \ a pre-defined non-word embedding. See Section 6 for details."
  Figure 8 Link: articels_figures_by_rev_year\2018\PictionaryStyle_Word_Guessing_on_HandDrawn_Object_Sketches_Dataset_Analysis_and_\figure_8.jpg
  Figure 8 caption: Architecture for the two-phase baseline. The first phase (blue
    dotted line) is used to predict location of the transition to the word-guessing
    phase (output 1). Starting from transition location, the second-phase (red dotted
    line) sequentially outputs word-embedding predictions until the end of stroke
    sequence.
  Figure 9 Link: articels_figures_by_rev_year\2018\PictionaryStyle_Word_Guessing_on_HandDrawn_Object_Sketches_Dataset_Analysis_and_\figure_9.jpg
  Figure 9 caption: Examples of guesses generated by our model on test set sequences.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Ravi Kiran Sarvadevabhatla
  Name of the last author: R. Venkatesh Babu
  Number of Figures: 11
  Number of Tables: 7
  Number of authors: 4
  Paper title: 'Pictionary-Style Word Guessing on Hand-Drawn Object Sketches: Dataset,
    Analysis and Deep Network Models'
  Publication Date: 2018-10-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Distribution of Possible Number of Guesses and Count of
      Number of Sequences Which Elicited Them
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Accuracy of Human Guesses for Various Matching Criteria (Section
      4.1)
  Table 3 caption:
    table_text: TABLE 3 Quantifying the Suitability of Matching Criteria Combination
      for Characterizing Human-Level Sketch Object Recognition Accuracy
  Table 4 caption:
    table_text: TABLE 4 Category Level Performance of Human and Machine Classifiers
  Table 5 caption:
    table_text: "TABLE 5 Comparing Human and Machine Classifiers for the Possible\
      \ Prediction Combinations \u2013 \u2713 Indicates Correct and \u2717 Indicates\
      \ Incorrect Prediction"
  Table 6 caption:
    table_text: TABLE 6 Sequence-Level Accuracies over the Validation Set Are Shown
  Table 7 caption:
    table_text: TABLE 7 Overall Average Sequence-Level Accuracy on Test Set Are Shown
      for Guessing Models (CNNs Only Baseline [First Row], Two-Phase Baseline [Second]
      and Our Proposed Model [Third])
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2877996
