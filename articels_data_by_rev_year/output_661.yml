- Affiliation of the first author: "departamento de teor\xEDa de la se\xF1al y comunicaciones,\
    \ escuela superior de ingenier\xEDa, universidad de sevilla, avda. descubrimientos\
    \ sn, seville, spain"
  Affiliation of the last author: i3s laboratory, umr 7271, university of nice sophia
    antipolis, cnrs, cs 40121, sophia antipolis cedex, france
  Figure 1 Link: articels_figures_by_rev_year\2016\On_the_Link_Between_LPCA_and_ICA\figure_1.jpg
  Figure 1 caption: "Criterion E|y| as a function of angle \u03B8 for an orthogonal\
    \ mixture of two sources with (a) uniform distribution and (b) Laplacian distribution.\
    \ In the first case, the independent sources maximize the criterion, whereas they\
    \ minimize it in the second case. The criterion values have been estimated from\
    \ random unitary mixtures of independent source realizations composed of 10 4\
    \ samples."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\On_the_Link_Between_LPCA_and_ICA\figure_2.jpg
  Figure 2 caption: 'Contour plots of criterion E|y| as a function of g 1 and g 2
    (solid lines) and evolution of the iterative L1-PCA algorithm (7) (dashed lines)
    in a three-source scenario. (a) Sources fulfilling condition C1 (uniform distributions):
    The algorithm converges to a valid separating solution. (b) Sources fulfilling
    conditions C2 (Laplacian distributions): the algorithm fails to converge to a
    valid separating solution.'
  Figure 3 Link: articels_figures_by_rev_year\2016\On_the_Link_Between_LPCA_and_ICA\figure_3.jpg
  Figure 3 caption: Criterion E|y| as a function of g 1 and g 2 for a mixture of three
    sources satisfying condition C2 (Laplacian distributions).
  Figure 4 Link: articels_figures_by_rev_year\2016\On_the_Link_Between_LPCA_and_ICA\figure_4.jpg
  Figure 4 caption: Independent source extraction performance as a function of the
    sample size T , for N=2 sources with uniform distributions [plots (a)-(b)] and
    Laplacian distributions [plots (c)-(d)].
  Figure 5 Link: articels_figures_by_rev_year\2016\On_the_Link_Between_LPCA_and_ICA\figure_5.jpg
  Figure 5 caption: Independent source extraction performance as a function of the
    outlier contamination rate, for N=3 sources with uniform distributions [plots
    (a)-(b)] and Laplacian distributions [plots (c)-(d)]. The observation window length
    is fixed at T=100 samples.
  Figure 6 Link: articels_figures_by_rev_year\2016\On_the_Link_Between_LPCA_and_ICA\figure_6.jpg
  Figure 6 caption: Plots of statistical criteria obtained from random unitary mixtures
    of two sources with uniform distributions corrupted by outliers. Each line represents
    one of 25 independent data realizations, whereas the thick line plots the average
    of the 25 independent runs. (a) L1-norm criterion E|y| . (b) ICA criterion J(w)
    with G(y)=logcosh(y) . In the second case, the local maxima deviate significantly
    from valid separating solutions and, as a result, maximization of the criterion
    fails to extract an independent component in most cases.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "Rub\xE9n Mart\u0131n-Clemente"
  Name of the last author: Vicente Zarzoso
  Number of Figures: 6
  Number of Tables: 1
  Number of authors: 2
  Paper title: On the Link Between L1-PCA and ICA
  Publication Date: 2016-06-22 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Percentage of Monte Carlo Runs Where the Maximum Number of
      Iterations Is Reached in the Experiments of Figs. 4 and 5 (without Outliers):
      ''Uni'': Uniform Sources. ''Lap'': Laplacian Sources'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2557797
- Affiliation of the first author: mathematics and informatics department, university
    of barcelona, and the computer vision center, catalonia, spain
  Affiliation of the last author: microsoft research, cambridge, united kingdom
  Figure 1 Link: Not Available
  Figure 1 caption: Not Available
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.63
  Name of the first author: Sergio Escalera
  Name of the last author: Jamie Shotton
  Number of Figures: Not Available
  Number of Tables: 0
  Number of authors: 4
  Paper title: "Guest Editors\u2019 Introduction to the Special Issue on Multimodal\
    \ Human Pose Recovery and Behavior Analysis"
  Publication Date: 2016-06-30 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2557878
- Affiliation of the first author: department of research, google, mountain view,
    ca
  Affiliation of the last author: department of research, google, mountain view, ca
  Figure 1 Link: articels_figures_by_rev_year\2016\Show_and_Tell_Lessons_Learned_from_the__MSCOCO_Image_Captioning_Challenge\figure_1.jpg
  Figure 1 caption: NIC, our model, is based end-to-end on a neural network consisting
    of a vision CNN followed by a language generating RNN. It generates complete sentences
    in natural language from an input image, as shown on the example above.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Show_and_Tell_Lessons_Learned_from_the__MSCOCO_Image_Captioning_Challenge\figure_2.jpg
  Figure 2 caption: "LSTM: the memory block contains a cell c which is controlled\
    \ by three gates. In blue we show the recurrent connections\u2014the output m\
    \ at time t\u22121 is fed back to the memory at time t via the three gates; the\
    \ cell value is fed back via the forget gate; the predicted word at time t\u2212\
    1 is fed back in addition to the memory output m at time t into the Softmax for\
    \ word prediction."
  Figure 3 Link: articels_figures_by_rev_year\2016\Show_and_Tell_Lessons_Learned_from_the__MSCOCO_Image_Captioning_Challenge\figure_3.jpg
  Figure 3 caption: LSTM model combined with a CNN image embedder (as defined in [24])
    and word embeddings. The unrolled connections between the LSTM memories are in
    blue and they correspond to the recurrent connections in Fig. 2. All LSTMs share
    the same parameters.
  Figure 4 Link: articels_figures_by_rev_year\2016\Show_and_Tell_Lessons_Learned_from_the__MSCOCO_Image_Captioning_Challenge\figure_4.jpg
  Figure 4 caption: "Flickr-8k: NIC: Predictions produced by NIC on the Flickr8k test\
    \ set (average score: 2.37); Pascal: NIC: (average score: 2.45); COCO-1k: NIC:\
    \ A subset of 1,000 images from the MSCOCO test set with descriptions produced\
    \ by NIC (average score: 2.72); Flickr-8k: ref: These are results from [16] on\
    \ Flickr8k rated using the same protocol, as a baseline (average score: 2.08);\
    \ Flickr-8k: GT: we rated the groundtruth labels from Flickr8k using the same\
    \ protocol. This provides us with a \u201Ccalibration\u201D of the scores (average\
    \ score: 3.89)."
  Figure 5 Link: articels_figures_by_rev_year\2016\Show_and_Tell_Lessons_Learned_from_the__MSCOCO_Image_Captioning_Challenge\figure_5.jpg
  Figure 5 caption: A selection of evaluation results, grouped by human rating.
  Figure 6 Link: articels_figures_by_rev_year\2016\Show_and_Tell_Lessons_Learned_from_the__MSCOCO_Image_Captioning_Challenge\figure_6.jpg
  Figure 6 caption: A selection of evaluation images, comparing the captions obtained
    by our original model (InitialModel) and the model submitted to the competition
    (BestModel).
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Oriol Vinyals
  Name of the last author: Dumitru Erhan
  Number of Figures: 6
  Number of Tables: 10
  Number of authors: 4
  Paper title: 'Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning
    Challenge'
  Publication Date: 2016-07-07 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Scores on the MSCOCO Development Set for Two Models: NIC,
      Which Was the Model Which We Developed in [46], and NICv2, Which Was the Model
      After We Tuned and Refined Our System for the MSCOCO Competition'
  Table 10 caption:
    table_text: TABLE 10 Human Generated Scores of the Top Five Competition Submissions
  Table 2 caption:
    table_text: TABLE 2 BLEU-1 Scores
  Table 3 caption:
    table_text: TABLE 3 N-Best Examples from the MSCOCO Test Set
  Table 4 caption:
    table_text: TABLE 4 Recallk and Median Rank on Flickr8k
  Table 5 caption:
    table_text: TABLE 5 Recallk and Median Rank on Flickr30k
  Table 6 caption:
    table_text: TABLE 6 Nearest Neighbors of a Few Example Words
  Table 7 caption:
    table_text: TABLE 7 Pearson Correlation and Human Rankings Found in the MSCOCO
      Official Website Competition Table for Several Automatic Metrics (Using 40 Ground
      Truth Captions in the Test Set)
  Table 8 caption:
    table_text: TABLE 8 A Summary of All the Improvements Which We Introduced for
      the MSCOCO Competition
  Table 9 caption:
    table_text: TABLE 9 Automatic Scores of the Top Five Competition Submissions
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2587640
- Affiliation of the first author: department of computer science, university of central
    florida, orlando, fl
  Affiliation of the last author: department of electrical and computer engineering,
    university of illinois, urbana, il
  Figure 1 Link: articels_figures_by_rev_year\2016\Joint_Intermodal_and_Intramodal_Label_Transfers_for_Extremely_Rare_or_Unseen_Cla\figure_1.jpg
  Figure 1 caption: Illustration of semantic label propagation from text to images
    by the learned transfer function. On the left is the labeled text corpus S , and
    at the bottom is the labeled image corpus T . The proposed I2LT transfers the
    labels from both corpora to annotate a test image at the top right corner. The
    output label is given by a discriminant function f(z) . Note that the test image
    is not associated with any text document. Hence, the transfer function T is applied
    for the intermodal label transfer f inter from source corpus S , along with the
    intramodal label transfer f intra from the image corpus T .
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Joint_Intermodal_and_Intramodal_Label_Transfers_for_Extremely_Rare_or_Unseen_Cla\figure_2.jpg
  Figure 2 caption: Examples of images over the different categories.
  Figure 3 Link: articels_figures_by_rev_year\2016\Joint_Intermodal_and_Intramodal_Label_Transfers_for_Extremely_Rare_or_Unseen_Cla\figure_3.jpg
  Figure 3 caption: Average error rate of different algorithms with varying number
    of training images.
  Figure 4 Link: articels_figures_by_rev_year\2016\Joint_Intermodal_and_Intramodal_Label_Transfers_for_Extremely_Rare_or_Unseen_Cla\figure_4.jpg
  Figure 4 caption: Average error rate of different algorithms with various percentage
    of training images from each concept, from 10 to 50 percent with an increment
    of 10 percent.
  Figure 5 Link: articels_figures_by_rev_year\2016\Joint_Intermodal_and_Intramodal_Label_Transfers_for_Extremely_Rare_or_Unseen_Cla\figure_5.jpg
  Figure 5 caption: Comparison of Average Precision (AP) for 81 concepts on NUS-WIDE
    dataset. The plot is better viewed in color.
  Figure 6 Link: articels_figures_by_rev_year\2016\Joint_Intermodal_and_Intramodal_Label_Transfers_for_Extremely_Rare_or_Unseen_Cla\figure_6.jpg
  Figure 6 caption: Average error rate of different algorithms with varying number
    of image-text pairs to learn the intermodal transfer function.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Guo-Jun Qi
  Name of the last author: Thomas Huang
  Number of Figures: 6
  Number of Tables: 10
  Number of authors: 4
  Paper title: Joint Intermodal and Intramodal Label Transfers for Extremely Rare
    or Unseen Classes
  Publication Date: 2016-07-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Number of Occurrence Pairs of Texts and Images for Each
      Category
  Table 10 caption:
    table_text: TABLE 10 The Training and Testing Time with 2,000 Co-Occurrence Pairs
      and 10 Training Examples (in Seconds)
  Table 2 caption:
    table_text: TABLE 2 The Number of Positive and Negative Images for Each Category
  Table 3 caption:
    table_text: TABLE 3 The Number of Wiki Articles for Each Category
  Table 4 caption:
    table_text: TABLE 4 Comparison of Error Rate of Different Algorithms with (a)
      Two Training Images (b) Ten Training Images
  Table 5 caption:
    table_text: TABLE 5 The Number of Topics (i.e., the Rank of Matrix S ) Used for
      Learning the Transfer Function in Topic Space from 2,000 Co-Occurrence Pairs
      with Two and Ten Training Examples
  Table 6 caption:
    table_text: TABLE 6 Comparison of Mean Average Precision (MAPs) on NUS-WIDE Dataset
  Table 7 caption:
    table_text: TABLE 7 Comparison of CCA-Type Cross-Modal Retrieval Models and the
      Proposed I2LT Algorithm
  Table 8 caption:
    table_text: TABLE 8 Comparison of Zero-Shot Classifiers on CUB200 and Flower102
  Table 9 caption:
    table_text: TABLE 9 Comparison of Zero-Shot Classifiers on the Extended CUB Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2587643
- Affiliation of the first author: department of electrical and electronics engineering,
    eskisehir osmangazi university, eskisehir, turkey
  Affiliation of the last author: department of electrical and electronics engineering,
    eskisehir osmangazi university, eskisehir, turkey
  Figure 1 Link: articels_figures_by_rev_year\2016\Best_Fitting_Hyperplanes_for_Classification\figure_1.jpg
  Figure 1 caption: The separating hyperplane S returned by the SVM classifier separates
    people and dog classes. All samples under the separating hyperplane are assigned
    to the dog class. When there are test samples coming from the unknown classes,
    such as chair and fish, these samples will be erroneously assigned to the dog
    class with high confidence scores. Adding another parallel hyperplane H helps
    to localize dog class samples better, and misclassifications can be reduced.
  Figure 10 Link: articels_figures_by_rev_year\2016\Best_Fitting_Hyperplanes_for_Classification\figure_10.jpg
  Figure 10 caption: AP scores (percent) on selected ImageNet classes.
  Figure 2 Link: articels_figures_by_rev_year\2016\Best_Fitting_Hyperplanes_for_Classification\figure_2.jpg
  Figure 2 caption: "In the proposed method, positive class samples (shown with red\
    \ triangles) lie between two parallel hyperplanes characterized by w \u22A4 +\
    \ x+ b + =1\u2212\u0394 and w \u22A4 + x+ b + =\u22121+\u0394 . The negative samples\
    \ shown with blue circles can lie on both sides of the fitting hyperplane (the\
    \ fitting hyperplane, w \u22A4 + x+ b + =0 , is shown with the dashed line), and\
    \ they are separated from the positive samples with a margin of at least 2\u0394\
    || w + || in the separable case."
  Figure 3 Link: articels_figures_by_rev_year\2016\Best_Fitting_Hyperplanes_for_Classification\figure_3.jpg
  Figure 3 caption: "Illustration of the cost function, J pos (t)= R pos (t)+ R pos\
    \ (\u2212t) , of the positive samples. R pos (t)= H \u22121+\u0394 (t)\u2212 H\
    \ s\u22122+\u0394 (t) can be written as the sum of the convex Hinge loss and a\
    \ concave loss function. \u0394 is set to 0.3 and s=\u22120.20 ."
  Figure 4 Link: articels_figures_by_rev_year\2016\Best_Fitting_Hyperplanes_for_Classification\figure_4.jpg
  Figure 4 caption: "Illustrations of the Ramp loss functions, R pos (t)=min(1\u2212\
    s,max(0,\u22121+\u0394\u2212t)) (top left figure) and R neg (t)=min(1+\u0394\u2212\
    s,max(0,1+\u0394\u2212t)) (bottom left). Each loss can be written as the sum of\
    \ the convex Hinge loss (center) and the concave loss (right), i.e., R pos (t)=\
    \ H \u22121+\u0394 (t)\u2212 H s\u22122+\u0394 (t) and R neg (t)= H 1+\u0394 (t)\u2212\
    \ H s (t) , where H a (t)=max(0,a\u2212t) is the classical Hinge loss. Here, the\
    \ parameters are set to \u0394=0.3 and s=\u22120.20 ."
  Figure 5 Link: articels_figures_by_rev_year\2016\Best_Fitting_Hyperplanes_for_Classification\figure_5.jpg
  Figure 5 caption: "The Symmetric Ramp Loss function, J neg (t)= R neg (t)+ R neg\
    \ (\u2212t) , for negative samples. The parameter s is set to \u2212 0.20."
  Figure 6 Link: articels_figures_by_rev_year\2016\Best_Fitting_Hyperplanes_for_Classification\figure_6.jpg
  Figure 6 caption: The common vector determines the minimum distance from the origin
    to the affine hull of the samples. All points on an affine hull collapse to the
    same point when they are projected onto the common vector, and thus the common
    vector can be seen as the normal of an affine hull.
  Figure 7 Link: articels_figures_by_rev_year\2016\Best_Fitting_Hyperplanes_for_Classification\figure_7.jpg
  Figure 7 caption: The convex loss function for positive samples, Rpos(t) = mathrmarg,max(|t|-(1-Delta),0)
    . It can also be written as the sum of two Hinge losses, Rpos(t)=H1+Delta(t)+H1+Delta(-t)
    .
  Figure 8 Link: articels_figures_by_rev_year\2016\Best_Fitting_Hyperplanes_for_Classification\figure_8.jpg
  Figure 8 caption: Synthetic data created by using normal distributions. The support
    vectors returned by the proposed method are denoted by circles around the samples.
    Our proposed method finds a better fitting hyperplane (its normal vector is shown
    by the black line) compared to the GEPSVM's (its normal is the green line).
  Figure 9 Link: articels_figures_by_rev_year\2016\Best_Fitting_Hyperplanes_for_Classification\figure_9.jpg
  Figure 9 caption: (a) Precision-Recall and (b) ROC curves for FDDD dataset.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Hakan Cevikalp
  Name of the last author: Hakan Cevikalp
  Number of Figures: 11
  Number of Tables: 6
  Number of authors: 1
  Paper title: Best Fitting Hyperplanes for Classification
  Publication Date: 2016-07-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 AP Scores (%) on the FDDB Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 AP Scores (%) on the INRIA Person Dataset
  Table 3 caption:
    table_text: TABLE 3 Average Precision Scores (%) on PASCAL VOC 2007 Detection
      and Classification Datasets
  Table 4 caption:
    table_text: TABLE 4 Classification Rates (%) on the Caltech-256 and USPS Digit
      Datasets
  Table 5 caption:
    table_text: TABLE 5 Classification Rates (%) on the Open Set USPS Dataset
  Table 6 caption:
    table_text: TABLE 6 Classification Rates (%) as a Function of Dimensionality for
      the Open Set Recognition Setup
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2587647
- Affiliation of the first author: department of electronic engineering, chinese university
    of hong kong, shatin, nt, hong kong
  Affiliation of the last author: department of electronic engineering, chinese university
    of hong kong, shatin, nt, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2016\DeepIDNet_Object_Detection_with_Deformable_Part_Based_Convolutional_Neural_Netwo\figure_1.jpg
  Figure 1 caption: The motivation of this paper in new pretraining scheme (a) and
    jointly learning feature representation and deformable object parts shared by
    multiple object classes at different semantic levels (b). In (a), a model pretrained
    on image-level annotation is more robust to size and location change while a model
    pretrained on object-level annotation is better in representing objects with tight
    bounding boxes. In (b), when ipod rotates, its circular pattern moves horizontally
    at the bottom of the bounding box. Therefore, the circular patterns have smaller
    penalty moving horizontally but higher penalty moving vertically. The curvature
    part of the circular pattern are often at the bottom right positions of the circular
    pattern. Magnitudes of deformation penalty are normalized to make them comparable
    across the two examples in (a) for visualization. Best viewed in color.
  Figure 10 Link: articels_figures_by_rev_year\2016\DeepIDNet_Object_Detection_with_Deformable_Part_Based_Convolutional_Neural_Netwo\figure_10.jpg
  Figure 10 caption: Fraction of high-scored false positives on VOC-2007 that are
    due to poor localization (Loc), confusion with similar objects (Sim), confusion
    with other VOC objects (Oth), or confusion with background or unlabeled objects
    (BG).
  Figure 2 Link: articels_figures_by_rev_year\2016\DeepIDNet_Object_Detection_with_Deformable_Part_Based_Convolutional_Neural_Netwo\figure_2.jpg
  Figure 2 caption: Overview of our approach. Detailed description is given in Section
    3.1. Texts in red highlight the steps that are not present in RCNN [1].
  Figure 3 Link: articels_figures_by_rev_year\2016\DeepIDNet_Object_Detection_with_Deformable_Part_Based_Convolutional_Neural_Netwo\figure_3.jpg
  Figure 3 caption: 'Architecture of DeepID-Net with three parts: (a) baseline deep
    model, which is ZF [20] in our single-model detector; (b) layers of part filters
    with variable sizes and def-pooling layers; (c) deep model to obtain 1,000-class
    image classification scores. The 1,000-class image classification scores are used
    to refine the 200-class bounding box classification scores.'
  Figure 4 Link: articels_figures_by_rev_year\2016\DeepIDNet_Object_Detection_with_Deformable_Part_Based_Convolutional_Neural_Netwo\figure_4.jpg
  Figure 4 caption: The relationship between the operations in the DPM and the CNN.
  Figure 5 Link: articels_figures_by_rev_year\2016\DeepIDNet_Object_Detection_with_Deformable_Part_Based_Convolutional_Neural_Netwo\figure_5.jpg
  Figure 5 caption: "Def-pooling layer. The part detection map and the deformation\
    \ penalty are summed up. Block-wise max pooling is then performed on the summed\
    \ map to obtain the output B of size H s y \xD7 W s x ( 3\xD71 in this example)."
  Figure 6 Link: articels_figures_by_rev_year\2016\DeepIDNet_Object_Detection_with_Deformable_Part_Based_Convolutional_Neural_Netwo\figure_6.jpg
  Figure 6 caption: The learned deformation penalty for different visual patterns.
    The penalties in map 1 are low at diagonal positions. The penalties in map 2 and
    3 are low at vertical and horizontal locations separately. The penalties in map
    4 are high at the bottom right corner and low at the upper left corner.
  Figure 7 Link: articels_figures_by_rev_year\2016\DeepIDNet_Object_Detection_with_Deformable_Part_Based_Convolutional_Neural_Netwo\figure_7.jpg
  Figure 7 caption: The learned part filters visualized using deepdraw [76].
  Figure 8 Link: articels_figures_by_rev_year\2016\DeepIDNet_Object_Detection_with_Deformable_Part_Based_Convolutional_Neural_Netwo\figure_8.jpg
  Figure 8 caption: Repeated visual patterns in multiple object classes.
  Figure 9 Link: articels_figures_by_rev_year\2016\DeepIDNet_Object_Detection_with_Deformable_Part_Based_Convolutional_Neural_Netwo\figure_9.jpg
  Figure 9 caption: The SVM weights on image classification scores (a) for the object
    detection class volleyball (b).
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.93
  Name of the first author: Wanli Ouyang
  Name of the last author: Xiaoou Tang
  Number of Figures: 17
  Number of Tables: 10
  Number of authors: 14
  Paper title: 'DeepID-Net: Object Detection with Deformable Part Based Convolutional
    Neural Networks'
  Publication Date: 2016-07-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Detection mAP (Percent) on VOC-2007 Test
  Table 10 caption:
    table_text: TABLE 10 Ablation Study of the Overall Pipeline for Single Model on
      ILSVRC2014 val 2
  Table 2 caption:
    table_text: TABLE 2 VOC-2007 Test Detection Average Precision (Percent) for RCNN
      Using VGG and Our Approach
  Table 3 caption:
    table_text: TABLE 3 Detection mAP (Percent) on ILSVRC2014 for Top Ranked Approaches
      with Single Model (sgl) and Average Model (avg)
  Table 4 caption:
    table_text: TABLE 4 Models Used for Model Averaging
  Table 5 caption:
    table_text: TABLE 5 Study of Bounding Box (bbox) Rejection and Baseline Deep Model
      on ILSVRC2014 val 2
  Table 6 caption:
    table_text: TABLE 6 Study of Bounding Box (bbox) Rejection at the Training and
      Testing Stage without Context or Def-Pooling
  Table 7 caption:
    table_text: TABLE 7 Study of Number of Classes Used for Pretraining
  Table 8 caption:
    table_text: TABLE 8 Investigation on Def-Pooling for Different Baseline Net Structures
      on ILSVRC2014 val 2
  Table 9 caption:
    table_text: TABLE 9 Ablation Study of the Two Pretraining Schemes in Section 3.3
      for Different Net Structures on ILSVRC2014 val 2
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2587642
- Affiliation of the first author: "vision and natural computation group, universit\xE9\
    \ paris 06, paris, france"
  Affiliation of the last author: "vision and natural computation group, universit\xE9\
    \ paris 06, paris, france"
  Figure 1 Link: articels_figures_by_rev_year\2016\HOTS_A_Hierarchy_of_EventBased_TimeSurfaces_for_Pattern_Recognition\figure_1.jpg
  Figure 1 caption: "Illustration of event-based encoding of visual signals. (a) ATIS\
    \ camera [22] . (b) Log of the luminance of a pixel located at [x,y ] T . (c)\
    \ Asynchronous temporal contrast events generated with respect to the predefined\
    \ threshold \u0394logI."
  Figure 10 Link: articels_figures_by_rev_year\2016\HOTS_A_Hierarchy_of_EventBased_TimeSurfaces_for_Pattern_Recognition\figure_10.jpg
  Figure 10 caption: 'Letters & Digits experiment: Distance measurements between learning
    and testing presentations of patterns. One presentation of each learnt pattern
    is shown to the system. Each line presents data related to a particular trained
    pattern. Each section in between dashed vertical lines corresponds to the presentation
    of a test stimulus. Histograms show normalized distances obtained during the experiment
    so that the recognized objects is the smallest bar in each column (marked with
    a star). White bars code for standard distance, grey bars for normalized distance
    and black bars for Bhattacharyya distance. These three distances all lead to a
    100 percent recognition rate.'
  Figure 2 Link: articels_figures_by_rev_year\2016\HOTS_A_Hierarchy_of_EventBased_TimeSurfaces_for_Pattern_Recognition\figure_2.jpg
  Figure 2 caption: "Definition of a time-surface from the spatio-temporal cloud of\
    \ events. A time-surface describes the recent time history of events in the spatial\
    \ neighborhood of an event. This figure shows how the time-surface for an event\
    \ happening at pixel x 0 =[ x 0 , y 0 ] T at time t 0 is computed. The event-driven\
    \ time-based vision sensor (a) is filming a scene and outputs events shown in\
    \ (b) where ON events are represented on the left hand picture and OFF events\
    \ on the right hand one. For clarity, we continue by only showing values associated\
    \ to OFF events. When an OFF event e v i =[ x 0 , t i ,\u22121] arrives, we consider\
    \ the times of most recent OFF events in the spatial neighborhood (c) where brighter\
    \ pixels represent more recent events. Extracting a spatial receptive field allows\
    \ to build the event-context T i (x,p) (d) associated with that event. Exponential\
    \ decay kernels are then applyed to the obtained values (e) and their values at\
    \ t i constitute the time-surface itself. (f) shows these values as a surface.\
    \ This representation will be used in the following figures and the label of the\
    \ axes will be removed for better clarity."
  Figure 3 Link: articels_figures_by_rev_year\2016\HOTS_A_Hierarchy_of_EventBased_TimeSurfaces_for_Pattern_Recognition\figure_3.jpg
  Figure 3 caption: 'Example of some time-surfaces for simple movements of objects.
    First column shows a representation of the stimulus. The second column shows corresponding
    data from the ATIS sensor where white dots are ON events and black dots are OFF
    events. The third column shows the time-surface obtained from these events and
    computed for the event located in the center of the circle in the second column:
    the first, positive, half is obtained from the ON events and the second, negative,
    half is obtained from the OFF events. (a) A horizontal bar moving downwards. (b)
    A vertical bar moving rightward. (c) Corner moving to the top-right.'
  Figure 4 Link: articels_figures_by_rev_year\2016\HOTS_A_Hierarchy_of_EventBased_TimeSurfaces_for_Pattern_Recognition\figure_4.jpg
  Figure 4 caption: View of the proposed hierarchical model. From left to right, a
    moving digit (a) is presented to the ATIS camera (b) which produces ON and OFF
    events (c) which are fed into Layer 1 . The events are convolved with exponential
    kernels (d) to build event contexts from spatial receptive field of sidelength
    (2 R 1 +1) . These contexts are clustered into N 1 features (e). When a feature
    is matched, it produces an event (f). Events from the N 1 features constitute
    the output of the layer (g). Each layer k (gray boxes) takes input from its previous
    layer and feeds the next by reproducing steps (d)-(g). The output of Layer k is
    presented between Layer k and k+1 ((g),(h),(i)). To compute event contexts, each
    layer considers a receptive field of sidelength (2 R k +1) around each pixel.
    The event contexts are compared to the different features (represented as surfaces
    in the gray boxes as explained in Section 3.3) and the closest one is matched.
    The images next to each features show the activation of their associated features
    in each layer. These activations constitute the output of the layer. The output
    (i) of the last layer is then fed to the classifier (j) which will recognize the
    object.
  Figure 5 Link: articels_figures_by_rev_year\2016\HOTS_A_Hierarchy_of_EventBased_TimeSurfaces_for_Pattern_Recognition\figure_5.jpg
  Figure 5 caption: 'Flipped cards experiment: Pattern database. The database for
    this experiment consists of the four suits (spades, hearts, clubs, and diamonds)
    found in a card deck. They are captured by a sensitive DVS sensor as the cards
    are flipped in front of it (white dots represent ON events and black dots OFF
    events).'
  Figure 6 Link: articels_figures_by_rev_year\2016\HOTS_A_Hierarchy_of_EventBased_TimeSurfaces_for_Pattern_Recognition\figure_6.jpg
  Figure 6 caption: 'Flipped cards experiment: Patterns'' signatures. Histograms of
    feature activation numbers for the four suits moving in front of the camera. X-axis
    is the index of the feature shown in the supplemental material, which can be found
    on the Computer Society Digital Library at http:doi.ieeecomputersociety.org10.1109TPAMI.2016.2574707,
    Y-axis is the number of activations of the feature during the stimulus presentation.
    Each column corresponds to one suit. The snapshots show how the pips evolve during
    one particular presentation (each snapshot is taken at a regular time interval).
    Each pattern outputs a different signature that allows its recognition.'
  Figure 7 Link: articels_figures_by_rev_year\2016\HOTS_A_Hierarchy_of_EventBased_TimeSurfaces_for_Pattern_Recognition\figure_7.jpg
  Figure 7 caption: 'Flipped cards experiment: Distance measurements between learning
    and testing presentations of patterns. The system is presented with nine different
    presentations of each learnt pattern. Each line presents data related to a particular
    trained pattern. Each section in between dashed vertical lines corresponds to
    the presentation of a test stimulus. Histograms show normalized distances obtained
    during the experiment so that the recognized objects is the smallest bar in each
    column (marked with a star). White bars code for standard distance, grey bars
    for normalized distance and black bars for Bhattacharyya distance. These three
    distances lead to respective performances of 94 , 100 and 97 percent.'
  Figure 8 Link: articels_figures_by_rev_year\2016\HOTS_A_Hierarchy_of_EventBased_TimeSurfaces_for_Pattern_Recognition\figure_8.jpg
  Figure 8 caption: 'Letters & Digits experiment: Pattern database. The database for
    this experiment consists of the 26 letters of the roman alphabet and the digits
    0 to 9. They are captured by a DVS sensor as the characters are moving in front
    of it (white dots represent ON events and black dots OFF events).'
  Figure 9 Link: articels_figures_by_rev_year\2016\HOTS_A_Hierarchy_of_EventBased_TimeSurfaces_for_Pattern_Recognition\figure_9.jpg
  Figure 9 caption: 'Letters & Digits experiment: Pattern signatures for some of the
    input classes. For each letter and digit the trained histogram used as a signature
    by the classifier is shown. The snapshot shows an accumulation of events from
    the sensor (White dots for ON events and black dots for OFF events). The histograms
    present the signatures: X-axis is the index of the feature, Y-axis is the number
    of activations of the feature during the stimulus presentation. The signatures
    of all the letters & digits are presented in the supplemental material, available
    online.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Xavier Lagorce
  Name of the last author: Ryad B. Benosman
  Number of Figures: 13
  Number of Tables: 1
  Number of authors: 5
  Paper title: 'HOTS: A Hierarchy of Event-Based Time-Surfaces for Pattern Recognition'
  Publication Date: 2016-07-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison with State of the Art
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2574707
- Affiliation of the first author: mitsubishi electric research labs (merl), cambridge,
    ma
  Affiliation of the last author: "inria grenoble rh\xF4ne-alpes, univ. grenoble alpes,\
    \ ljk, saint-martin-d'h\xE8res, france"
  Figure 1 Link: articels_figures_by_rev_year\2016\A_Unifying_Model_for_Camera_Calibration\figure_1.jpg
  Figure 1 caption: (a) The camera as black box, with one pixel and its camera ray.
    (b) The pixel sees a point on a calibration object, whose coordinates are identified
    in a frame associated with the object. (c) Same as (b), for another position of
    the calibration object. (d) If the object's motion is known, the two points on
    the calibration object can be placed in the same coordinate frame (here the same
    one as in (c)). The camera ray is then determined by joining them.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\A_Unifying_Model_for_Camera_Calibration\figure_2.jpg
  Figure 2 caption: 'Left: An omnidirectional image taken with a fisheye and the region
    of calibration grids occupied in four other images (shown using convex hulls of
    grid points). Middle: We show the five calibrated grid positions, which are used
    to compute the camera rays. Right: Example of a complete calibration for a fisheye
    camera from 23 overlapping grids.'
  Figure 3 Link: articels_figures_by_rev_year\2016\A_Unifying_Model_for_Camera_Calibration\figure_3.jpg
  Figure 3 caption: Complete distortion correction of a fisheye image shown in the
    middle. Note that heavily distorted lines are corrected in the undistorted images.
    Only very close to the image border, residual distortions remain, which seems
    acceptable for such a generic calibration approach.
  Figure 4 Link: articels_figures_by_rev_year\2016\A_Unifying_Model_for_Camera_Calibration\figure_4.jpg
  Figure 4 caption: Calibration of a stereo system using the axial algorithm with
    three images and a total of 481 rays. (a) Computed camera rays and axis. (b) Camera
    rays of the stereo system after clustering them and enforcing rays in each cluster
    to cut a single 3D point.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Srikumar Ramalingam
  Name of the last author: Peter Sturm
  Number of Figures: 4
  Number of Tables: 6
  Number of authors: 2
  Paper title: A Unifying Model for Camera Calibration
  Publication Date: 2016-07-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Coupled Variables in the Trifocal Calibration Tensor for the
      General 2D Camera
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Coefficients of the Trifocal Calibration Tensor for the General
      2D Camera and a Linear Calibration Object
  Table 3 caption:
    table_text: TABLE 3 Coupled Variables in the Trifocal Calibration Tensors for
      a General 3D Camera
  Table 4 caption:
    table_text: TABLE 4 Coupled Variables in the Bifocal Matching Tensor for a Central
      2D Camera
  Table 5 caption:
    table_text: TABLE 5 Coupled Variables in the Bifocal Matching Tensors for a 3D
      Single Center Camera
  Table 6 caption:
    table_text: TABLE 6 Nature of Solutions on Applying the Calibration Algorithms
      on Cameras of Specific Types
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2592904
- Affiliation of the first author: "department of environment perception, daimler\
    \ ag, b\xF6blingen, germany"
  Affiliation of the last author: department of computer science, tu darmstadt, darmstadt,
    germany
  Figure 1 Link: articels_figures_by_rev_year\2016\TreeStructured_Models_for_Efficient_MultiCue_Scene_Labeling\figure_1.jpg
  Figure 1 caption: Example results of our proposed method on three kinds of superpixels,
    and comparison to ground truth annotation.
  Figure 10 Link: articels_figures_by_rev_year\2016\TreeStructured_Models_for_Efficient_MultiCue_Scene_Labeling\figure_10.jpg
  Figure 10 caption: Breakdown of the 163 ms runtime reported with Stixel superpixels
    in Table 2 into the individual components of our overall system. The two tree-structured
    models we focus on in this work, i.e., encode-and-classify trees and the multi-cue
    segmentation tree, only use about 10 percent of the overall time, clearly indicating
    its efficiency.
  Figure 2 Link: articels_figures_by_rev_year\2016\TreeStructured_Models_for_Efficient_MultiCue_Scene_Labeling\figure_2.jpg
  Figure 2 caption: Method overview. Arrows indicate the contribution of each available
    cue (rows) to the individual processing steps (columns). Central to our approach
    are the proposed encode-and-classify trees, which generate pixel-level classification
    and texton histograms at the same time, and the segmentation tree. Pixel classification
    and texton histograms are detailed in Section 3.1, the other cues in Section 5.1.
  Figure 3 Link: articels_figures_by_rev_year\2016\TreeStructured_Models_for_Efficient_MultiCue_Scene_Labeling\figure_3.jpg
  Figure 3 caption: "Our proposed encode-and-classify tree structure for pixel-level\
    \ classification and texton extraction (bottom). Encoder nodes define a sub-tree\
    \ that operates on small local patches of size S 1 \xD7 S 1 to obtain highly discriminative\
    \ texton histograms. The remaining nodes have full access to all pixels in the\
    \ larger region S 2 \xD7 S 2 for accurate classification. Example of a texton\
    \ map generated using one tree and the pixel-level labeling result (top)."
  Figure 4 Link: articels_figures_by_rev_year\2016\TreeStructured_Models_for_Efficient_MultiCue_Scene_Labeling\figure_4.jpg
  Figure 4 caption: Segmentation tree and resulting CRF model. Regions are encoded
    by false colors, CRF nodes by black dots, and parent-child-relations by lines.
  Figure 5 Link: articels_figures_by_rev_year\2016\TreeStructured_Models_for_Efficient_MultiCue_Scene_Labeling\figure_5.jpg
  Figure 5 caption: 'Three superpixel variants we employ at the lowest level of our
    region proposal tree. From left to right: SEOF [10], GBIS [11], and Stixels [12].
    In all variants, superpixels on the ground surface and sky region are already
    grouped (see text) and not considered in the segmentation tree. Superpixels with
    invalid depth information are ignored and visualized transparently.'
  Figure 6 Link: articels_figures_by_rev_year\2016\TreeStructured_Models_for_Efficient_MultiCue_Scene_Labeling\figure_6.jpg
  Figure 6 caption: 'Encoding accuracy (red) and pixel classification accuracy (blue)
    as a function of patch size. Note that each curve is normalized such that its
    maximum value is 1 . We observe that smaller patches are better for encoding,
    while medium to large patches are good for pixel classification. This trend is
    consistent on both datasets: Daimler Urban Segmentation (DUS) and KITTI. The maximizing
    patch sizes are highlighted for each curve.'
  Figure 7 Link: articels_figures_by_rev_year\2016\TreeStructured_Models_for_Efficient_MultiCue_Scene_Labeling\figure_7.jpg
  Figure 7 caption: Encoding accuracy (red, left axis) and histogram length (blue,
    right axis) as a function of tree depth. It can be seen that with increased depth
    encoding accuracy saturates rather quickly, but the histogram length and thus
    histogram classification runtime grows continuously. This shows that limiting
    the number of encoder nodes, i.e., the histogram length, is important to set the
    optimal trade-off between accuracy and runtime. We set the histogram length to
    1,000, as highlighted.
  Figure 8 Link: articels_figures_by_rev_year\2016\TreeStructured_Models_for_Efficient_MultiCue_Scene_Labeling\figure_8.jpg
  Figure 8 caption: Overview of different annotation and prediction formats. Object
    instance prediction (d) is generated from Algorithm 2. Note the false positive
    vehicle prediction to the right of the pedestrians.
  Figure 9 Link: articels_figures_by_rev_year\2016\TreeStructured_Models_for_Efficient_MultiCue_Scene_Labeling\figure_9.jpg
  Figure 9 caption: 'Qualitative results on the DUS dataset compared to the ground
    truth and the best performing baseline. From left to right: ground truth, baseline
    [46], and our results for SEOF, GBIS and Stixels superpixels.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Marius Cordts
  Name of the last author: Stefan Roth
  Number of Figures: 10
  Number of Tables: 3
  Number of authors: 5
  Paper title: Tree-Structured Models for Efficient Multi-Cue Scene Labeling
  Publication Date: 2016-07-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Standard RDFs and Encode-and-Classify (E&C) Trees Compared
      on the DUS Dataset with Different Patch Size Combinations. Accuracy is Given
      as Average F-score, Relative to the Maximum Possible Encoding and Classification
      Performance, According to Fig. 6. Using Our E&C Trees with a Smaller Patch Size
      S 1 for Encoding and a Larger One for Classification S 2 Yields a Performance
      Close to the Individual Maxima of Standard RDFs. At the Same Time, They are
      More Efficient Compared to Using Two Separate Standard Models for Encoding and
      Classification (Center Column)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Results on the DUS Dataset with Official TrainTest
      Split Compared to Five Baselines (Left). We Show Pixel Accuracy (IoU), Object
      Accuracy (F-score), and Runtime. Additionally, We Report Results Using Stixels,
      Where Detections (-DT), Point Tracks (-PT), and Their Combination (-DT, -PT)
      are Removed from the Full System to Demonstrate their Influence on the Overall
      Performance (Right)
  Table 3 caption:
    table_text: TABLE 3 Quantitative Results on the KITTI Dataset Compared to Three
      Baselines
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2592911
- Affiliation of the first author: department of mathematics and computer science,
    open university of israel, ra'anana, israel
  Affiliation of the last author: department of electrical engineering, technion,
    haifa, israel
  Figure 1 Link: articels_figures_by_rev_year\2016\SIFTing_Through_Scales\figure_1.jpg
  Figure 1 caption: 'Dense matches of different objects in different scales. Top:
    Source and target input images. Bottom: Source image warped onto target using
    the recovered flows: Using DSIFT (bottom left) and our SLS descriptor (bottom
    right), overlaid on the target and manually cropped to demonstrate the alignment.
    DSIFT fails to capture the scale differences and produces an output in the same
    scale as the input. SLS captures scale changes at each pixel: the output produced
    by using SLS has the appearance of the Source image in the scale and position
    of the target.'
  Figure 10 Link: articels_figures_by_rev_year\2016\SIFTing_Through_Scales\figure_10.jpg
  Figure 10 caption: Make3d depth transfer. Estimated depth maps of an image from
    the Make3d data [38], [39]. The SLS result is the most similar to the ground truth.
  Figure 2 Link: articels_figures_by_rev_year\2016\SIFTing_Through_Scales\figure_2.jpg
  Figure 2 caption: Effects of scale differences on DSIFT versus our own SLS descriptor.
    Source images warped onto target image using correspondences obtained by the SIFT
    flow algorithm [8], [9] and the DSIFT descriptor, compared against the SLS descriptor
    ( Section 3.3). The results in the bottom two rows should appear similar to the
    top-right image. DSIFT descriptors provide some scale invariance despite a single
    arbitrary scale selection (left column, middle row). The SLS descriptors provide
    scale invariance across far greater scale differences (bottom).
  Figure 3 Link: articels_figures_by_rev_year\2016\SIFTing_Through_Scales\figure_3.jpg
  Figure 3 caption: "SIFT behavior through scales. Two images separated by a \xD7\
    \ 2 scale factor. Top: SIFT descriptors extracted at a detected interest point,\
    \ near a corner structure in the image. Bottom: Descriptors extracted at a low\
    \ contrast region where no interest point was detected. In both cases, SIFTs were\
    \ extracted at scales ranging from 10 to 35. We illustrate the SIFT descriptor\
    \ histogram values for each set of descriptors. These demonstrate that (a) even\
    \ in low contrast areas, SIFT values are not uniform and (b) the values of the\
    \ SIFT descriptors gradually change through scales."
  Figure 4 Link: articels_figures_by_rev_year\2016\SIFTing_Through_Scales\figure_4.jpg
  Figure 4 caption: 'SIFT-to-SIFT distances between two sets. Top: Two images of different
    size. SIFT descriptors are extracted at a low contrast area where no interest
    point was detected at 24 scales. Bottom: SIFT descriptor distance matrix for the
    various scales. It demonstrates that matching differently scaled descriptors around
    (a) corresponding points: SIFTs from the Target image match those at higher scales
    in the Source, implying that setting the same scale to all pixels in both images
    may lead to poor matches. (b) non-corresponding points: the distance between these
    descriptors is significantly larger, suggesting that they would not match. (c)
    the same point: the self SIFT distance matrix shows that SIFTs change gradually
    across scales, suggesting that descriptors are a smooth function of scale.'
  Figure 5 Link: articels_figures_by_rev_year\2016\SIFTing_Through_Scales\figure_5.jpg
  Figure 5 caption: 'Descriptor to SIFT subspace mean distance. Left: The images.
    Right: Mean distance from the descriptors in the set to the corresponding SIFT
    Subspace per pixel (Eq. (3)). Large portions of the images do not have the corner
    structures necessary for accurate scale selection and SIFT descriptor extraction.
    It is in those image regions that subspaces fit best.'
  Figure 6 Link: articels_figures_by_rev_year\2016\SIFTing_Through_Scales\figure_6.jpg
  Figure 6 caption: 'If SIFTs were 2D: A visualization of matching 2d SIFT descriptors,
    looking at multiple SIFTS taken at different scales. (Left) The distance between
    two sets is the distance between the two nearest points. (Right) The distance
    between the two subspaces is related to the angle between them. See Section 3.2.'
  Figure 7 Link: articels_figures_by_rev_year\2016\SIFTing_Through_Scales\figure_7.jpg
  Figure 7 caption: Auto-crop to the ROI. Dense matches directly formed, without estimating
    Epipolar Geometry, between the first and last images of the Oxford Corridor sequence
    [34] (left column). On the right, notice the large areas where no information
    is available in Image 2 to correspond with parts of Image 1. These areas are automatically
    cropped to include only the area onto which pixels from the second image were
    warped.
  Figure 8 Link: articels_figures_by_rev_year\2016\SIFTing_Through_Scales\figure_8.jpg
  Figure 8 caption: Dense flow with scene motion. Image pairs presenting different
    scale changes in different parts of the scene, due to camera and scene motion.
    Correspondences from Source to Target images estimated using [9], comparing DSIFT
    [8], SID [19], Segmented SID and segmented SIFT, both from [37] and our SLS, shown
    here with the automatically determined crop region in white (Section 3.5).
  Figure 9 Link: articels_figures_by_rev_year\2016\SIFTing_Through_Scales\figure_9.jpg
  Figure 9 caption: Dense flow between different scenes in different scales. Correspondences
    from Source to Target images estimated using [9], comparing DSIFT [8], SID [19],
    Segmented SID and segmented SIFT, both from [37] and our SLS, shown here with
    the automatically determined crop region in white (Section 3.5).
  First author gender probability: 0.74
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.99
  Name of the first author: Tal Hassner
  Name of the last author: Lihi Zelnik-Manor
  Number of Figures: 14
  Number of Tables: 3
  Number of authors: 4
  Paper title: SIFTing Through Scales
  Publication Date: 2016-07-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Results on the Scaled-Middlebury Benchmark
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Depth Transfer on Make3d Data\u2014Relative Error, Log-10\
      \ Error and RMSE"
  Table 3 caption:
    table_text: TABLE 3 Run-Time Comparison
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2592916
