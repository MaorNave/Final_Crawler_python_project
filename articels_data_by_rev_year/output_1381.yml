- Affiliation of the first author: department of national laboratory of pattern recognition,
    institute of automation, chinese academy of sciences, beijing, china
  Affiliation of the last author: department of national laboratory of pattern recognition,
    institute of automation, chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2019\LocalAggregation_Graph_Networks\figure_1.jpg
  Figure 1 caption: Two intrinsic differences between (a) Euclidean and (b) non-Euclidean
    structured signals. On Euclidean domains, local inputs at each location (vertex)
    are permutation-ordered and dimension-equal, which allow a single sharable filter
    to aggregate these local inputs. On non-Euclidean domains, however, local inputs
    at each vertex are permutation-unordered and dimension-unequal, which are beyond
    the scope of the traditional convolution. For clarity, we exemplify these two
    properties in the figure (b) where the neighbors of the red and blue vertices
    are depicted in the red and blue circles, respectively. Specifically, the neighbors
    of the red and blue vertices are unordered, and their numbers are unequal.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\LocalAggregation_Graph_Networks\figure_2.jpg
  Figure 2 caption: Invariance. (a) Classification errors during training (thick line)
    and validation (fine line), respectively. (b) Rotation. (c) Shift. (d) Scale.
  Figure 3 Link: articels_figures_by_rev_year\2019\LocalAggregation_Graph_Networks\figure_3.jpg
  Figure 3 caption: The root mean square error (rmse) during training and validation
    stages with diverse basis functions and activation functions.
  Figure 4 Link: articels_figures_by_rev_year\2019\LocalAggregation_Graph_Networks\figure_4.jpg
  Figure 4 caption: Training and validation errors with diverse (a) depth of LAGNs,
    (b) polynomial orders, (c) initializations, and (d) number of samples.
  Figure 5 Link: articels_figures_by_rev_year\2019\LocalAggregation_Graph_Networks\figure_5.jpg
  Figure 5 caption: The feature maps learned by the experimental (left) LAGN and (right)
    TradCNN on the STL-10 dataset. Specifically, the input images are illustrated
    on the upward side, and the sixteen feature maps in the first layers of the LAGN
    and TradCNN are shown orderly.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.78
  Name of the first author: Jianlong Chang
  Name of the last author: Chunhong Pan
  Number of Figures: 5
  Number of Tables: 7
  Number of authors: 6
  Paper title: Local-Aggregation Graph Networks
  Publication Date: 2019-05-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Various Polynomial Basis Functions
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Classification Accuracies on MNIST
  Table 3 caption:
    table_text: TABLE 3 Classification Accuracies on 20NEWS
  Table 4 caption:
    table_text: TABLE 4 Squared Correlations on DPP4
  Table 5 caption:
    table_text: TABLE 5 RMSE Among Various Methods on the Taxi Flow Data
  Table 6 caption:
    table_text: TABLE 6 Time Consumption to Process One Epoch on MNIST During Training
  Table 7 caption:
    table_text: TABLE 7 Classification Accuracies of TradCNNs and LAGNs on STL-10
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2915591
- Affiliation of the first author: realsense, intel corp. ltd., haifa, israel
  Affiliation of the last author: computer science department, technion, israel
  Figure 1 Link: articels_figures_by_rev_year\2019\Intel_RealSense_SR_Coded_Light_Depth_Camera\figure_1.jpg
  Figure 1 caption: Example of a 3D scan obtained with the SR300 camera.
  Figure 10 Link: articels_figures_by_rev_year\2019\Intel_RealSense_SR_Coded_Light_Depth_Camera\figure_10.jpg
  Figure 10 caption: Examples of the Morphological filter operation. (A) validating
    an invalid center pixel since it is part of well determined slope. (B) invalidating
    a spurious central pixel. (Light color denotes valid pixels).
  Figure 2 Link: articels_figures_by_rev_year\2019\Intel_RealSense_SR_Coded_Light_Depth_Camera\figure_2.jpg
  Figure 2 caption: 5 bit Grey code in time and image space.
  Figure 3 Link: articels_figures_by_rev_year\2019\Intel_RealSense_SR_Coded_Light_Depth_Camera\figure_3.jpg
  Figure 3 caption: SR300 3D imaging system.
  Figure 4 Link: articels_figures_by_rev_year\2019\Intel_RealSense_SR_Coded_Light_Depth_Camera\figure_4.jpg
  Figure 4 caption: SR300 assembly.
  Figure 5 Link: articels_figures_by_rev_year\2019\Intel_RealSense_SR_Coded_Light_Depth_Camera\figure_5.jpg
  Figure 5 caption: Flow chart of the depth reconstruction pipeline.
  Figure 6 Link: articels_figures_by_rev_year\2019\Intel_RealSense_SR_Coded_Light_Depth_Camera\figure_6.jpg
  Figure 6 caption: Illustration of the operation of the vertical code filter. The
    original location of the detected transition in pixel (i,j) (in black) is corrected
    (red) to align with the transition in the rows above and below.
  Figure 7 Link: articels_figures_by_rev_year\2019\Intel_RealSense_SR_Coded_Light_Depth_Camera\figure_7.jpg
  Figure 7 caption: Illustration of the operation of the horizontal code filter. Detected
    transitions are marked by black circles. The original location of the detected
    transition in pixel (i,j) is corrected (red) to align with the next and previous
    transitions.
  Figure 8 Link: articels_figures_by_rev_year\2019\Intel_RealSense_SR_Coded_Light_Depth_Camera\figure_8.jpg
  Figure 8 caption: Illustration of the operation of the consistency filter code filter.
    Detected transitions are marked by black circles. The code values between transitions
    are interpolated and mis-detected codes are fixed.
  Figure 9 Link: articels_figures_by_rev_year\2019\Intel_RealSense_SR_Coded_Light_Depth_Camera\figure_9.jpg
  Figure 9 caption: Flow chart of the depth post-processing pipeline
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Aviad Zabatani
  Name of the last author: Ron Kimmel
  Number of Figures: 13
  Number of Tables: 1
  Number of authors: 10
  Paper title: "Intel\xAE RealSense\u2122 SR300 Coded Light Depth Camera"
  Publication Date: 2019-05-10 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Characterization Report for Latest SR300 Package, Shown at
      Different Distance from the Camera
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2915841
- Affiliation of the first author: department of engineering science, university of
    oxford, oxford, united kingdom
  Affiliation of the last author: department of electrical and electronic engineering,
    imperial college london, london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2019\HHPatches_A_Benchmark_and_Evaluation_of_Handcrafted_and_Learned_Local_Descriptor\figure_1.jpg
  Figure 1 caption: Illustrative timeline of significant trends in development of
    local feature descriptors. For example SIFT based descriptors, introduced in 1999,
    have received continuous acclaim while for other descriptor families the interest
    has faded more quickly.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\HHPatches_A_Benchmark_and_Evaluation_of_Handcrafted_and_Learned_Local_Descriptor\figure_2.jpg
  Figure 2 caption: Examples of the image sequences contributing to H Patches; note
    the diversity of scenes and nuisance factors, including viewpoint (left), illumination
    (right), focus, reflections and other changes.
  Figure 3 Link: articels_figures_by_rev_year\2019\HHPatches_A_Benchmark_and_Evaluation_of_Handcrafted_and_Learned_Local_Descriptor\figure_3.jpg
  Figure 3 caption: "Construction of regions for patch extraction. Each detected feature\
    \ frame (yellow) is reprojected in the ref. image with a random transformation\
    \ T for Easy, Hard or Tough. Using the ground truth homography H gt , these regions\
    \ are reprojected to target images where the patch is extracted from a measurement\
    \ region visualised by dashed ellipse for factor \u03C1=3 ."
  Figure 4 Link: articels_figures_by_rev_year\2019\HHPatches_A_Benchmark_and_Evaluation_of_Handcrafted_and_Learned_Local_Descriptor\figure_4.jpg
  Figure 4 caption: Average overlap of the Hessian and Hessian-Affine detectors on
    the viewpoint sequences of .[94] Line color encodes dataset and line style a detector.
    The selected overlaps of the Easy and Hard variants are visualised with a dotted
    line.
  Figure 5 Link: articels_figures_by_rev_year\2019\HHPatches_A_Benchmark_and_Evaluation_of_Handcrafted_and_Learned_Local_Descriptor\figure_5.jpg
  Figure 5 caption: Geometric noise visualized on a sample set of patches. Left column
    shows the reference patches and the other columns show the corresponding patches
    from Easy, Hard and Tough distributions.
  Figure 6 Link: articels_figures_by_rev_year\2019\HHPatches_A_Benchmark_and_Evaluation_of_Handcrafted_and_Learned_Local_Descriptor\figure_6.jpg
  Figure 6 caption: Patch verification results for the state of the art feature descriptors,
    on the Hard geometric noise. (left) ROC curves for Balanced scenario - ROC curve
    (middle) Performance on the Imbalanced scenario - PR curve. Note that the ranking
    of the best performing descriptors remains similar between the Balanced and Imbalanced
    tasks. (right) Performance of Intra-sequence and Inter-sequence negative pair
    sampling. Intra-sequence sampling leads to a more challenging verification task,
    due to repetitive patterns and self-similarities that are commonly present on
    images.
  Figure 7 Link: articels_figures_by_rev_year\2019\HHPatches_A_Benchmark_and_Evaluation_of_Handcrafted_and_Learned_Local_Descriptor\figure_7.jpg
  Figure 7 caption: Patch retrieval results for several state of the art feature descriptors,
    measured in mAP for the Easy, Hard and Tough settings. We can observe that the
    performance significantly decreases with the size of distractor pool.
  Figure 8 Link: articels_figures_by_rev_year\2019\HHPatches_A_Benchmark_and_Evaluation_of_Handcrafted_and_Learned_Local_Descriptor\figure_8.jpg
  Figure 8 caption: Verification, matching and retrieval results for descriptors trained
    on Liberty and tested on the all sequences of the mathbb H Patches. Colour bars
    are means over all the variants of each task. Colour of the marker indicates Easy,
    Hard, and Tough geometric noise. The type of the marker corresponds to different
    experimental settings i.e., negative pair patches are sampled from the same image
    sequences Intra-sequence or different Inter-sequence, the results are averaged
    for viewpoint Viewp or illumination Illum sequences (see Section 7.3, 7.4 and
    7.5 for details). Figures titled Normalisation onLiberty show results for best
    normalisation found using the Liberty dataset.
  Figure 9 Link: articels_figures_by_rev_year\2019\HHPatches_A_Benchmark_and_Evaluation_of_Handcrafted_and_Learned_Local_Descriptor\figure_9.jpg
  Figure 9 caption: Verification, matching and retrieval results for descriptors as
    an average over three splits of mathbb H Patches (Set A, Set B, Set C). This figure
    uses same visualisation as Fig. 8. Figures titled Normalisation on mathbb H Patches
    show results using descriptors normalised with a best method found on the training
    set of the split. In this case, all descriptors are learnt on Liberty. Bottom
    figures report the average results for two descriptors trained and tested over
    three different splits of the mathbb H Patches dataset.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Vassileios Balntas
  Name of the last author: Krystian Mikolajczyk
  Number of Figures: 9
  Number of Tables: 12
  Number of authors: 6
  Paper title: 'H

    H-Patches: A Benchmark and Evaluation of Handcrafted and Learned Local Descriptors'
  Publication Date: 2019-05-10 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Contradictory Conclusions Reported in the Literature While
      Evaluating the Same Descriptors on the Same Benchmark (Oxford Affine Covariant
      Features [4])
  Table 10 caption:
    table_text: TABLE 10 Performance of TFeat-M When Trained and Tested within the
      Same Domain
  Table 2 caption:
    table_text: "TABLE 2 Matching Performance Measured by Mean Average Precision (mAP)\
      \ for Different Magnification \u03C1 \u03C1 of Region Size Parameter"
  Table 3 caption:
    table_text: TABLE 3 Comparison of Existing and H H Patches Dataset
  Table 4 caption:
    table_text: TABLE 4 The Range of Geometric Noise Distributions, in Units of a
      Patch Scale
  Table 5 caption:
    table_text: TABLE 5 Basic Properties of the Selected Descriptors
  Table 6 caption:
    table_text: "TABLE 6 Verification Task \u2014 Mean Average Precision (mAP) for\
      \ the Imbalanced Scenario and Different Levels of Geometric Noise"
  Table 7 caption:
    table_text: "TABLE 7 Image Matching Task \u2014 Mean Average Precision (mAP) for\
      \ Different Levels of Geometric Noise"
  Table 8 caption:
    table_text: "TABLE 8 Image Matching Task \u2014 Success Rate % for Different Levels\
      \ of Geometric Noise"
  Table 9 caption:
    table_text: TABLE 9 Performance (mAP) of Several Descriptors on Random Subsets
      of H H Patches
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2915233
- Affiliation of the first author: universitat oberta de catalunya, barcelona, spain
  Affiliation of the last author: universitat oberta de catalunya, barcelona, spain
  Figure 1 Link: articels_figures_by_rev_year\2019\Context_Based_Emotion_Recognition_Using_EMOTIC_Dataset\figure_1.jpg
  Figure 1 caption: How is this kid feeling? Try to recognize his emotional states
    from the person bounding box, without scene context.
  Figure 10 Link: articels_figures_by_rev_year\2019\Context_Based_Emotion_Recognition_Using_EMOTIC_Dataset\figure_10.jpg
  Figure 10 caption: Co-variance between 26 emotion categories. Each row represents
    the occurrence probability of every other category given the category of that
    particular row.
  Figure 2 Link: articels_figures_by_rev_year\2019\Context_Based_Emotion_Recognition_Using_EMOTIC_Dataset\figure_2.jpg
  Figure 2 caption: Sample images in the EMOTIC dataset along with their annotations.
  Figure 3 Link: articels_figures_by_rev_year\2019\Context_Based_Emotion_Recognition_Using_EMOTIC_Dataset\figure_3.jpg
  Figure 3 caption: Examples of annotated people in EMOTIC dataset for each of the
    26 emotion categories (Table 1). The person in the red bounding box is annotated
    by the corresponding category.
  Figure 4 Link: articels_figures_by_rev_year\2019\Context_Based_Emotion_Recognition_Using_EMOTIC_Dataset\figure_4.jpg
  Figure 4 caption: Examples of annotated images in EMOTIC dataset for each of the
    3 continuous dimensions Valence, Arousal & Dominance. The person in the red bounding
    box has the corresponding value of the given dimension.
  Figure 5 Link: articels_figures_by_rev_year\2019\Context_Based_Emotion_Recognition_Using_EMOTIC_Dataset\figure_5.jpg
  Figure 5 caption: AMT interface designs (a) For Discrete Categories annotations
    & (b) For Continuous Dimensions annotations.
  Figure 6 Link: articels_figures_by_rev_year\2019\Context_Based_Emotion_Recognition_Using_EMOTIC_Dataset\figure_6.jpg
  Figure 6 caption: Annotations of five different annotators for 3 images in EMOTIC.
  Figure 7 Link: articels_figures_by_rev_year\2019\Context_Based_Emotion_Recognition_Using_EMOTIC_Dataset\figure_7.jpg
  Figure 7 caption: Representation of agreement between multiple annotators. Categories
    sorted in decreasing order according to the average number of annotators who agreed
    for that category.
  Figure 8 Link: articels_figures_by_rev_year\2019\Context_Based_Emotion_Recognition_Using_EMOTIC_Dataset\figure_8.jpg
  Figure 8 caption: (a) Kappa values (sorted) and (b) Standard deviation (sorted),
    for each annotated person in validation set.
  Figure 9 Link: articels_figures_by_rev_year\2019\Context_Based_Emotion_Recognition_Using_EMOTIC_Dataset\figure_9.jpg
  Figure 9 caption: Dataset Statistics. (a) Number of people annotated for each emotion
    category; (b), (c) & (d) Number of people annotated for every value of the three
    continuous dimensions viz.Valence, Arousal & Dominance.
  First author gender probability: 0.84
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.99
  Name of the first author: Ronak Kosti
  Name of the last author: Agata Lapedriza
  Number of Figures: 15
  Number of Tables: 4
  Number of authors: 4
  Paper title: Context Based Emotion Recognition Using EMOTIC Dataset
  Publication Date: 2019-05-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Proposed Emotion Categories with Definitions
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Instruction Summary for Each HIT
  Table 3 caption:
    table_text: TABLE 3 Average Precision (AP) Obtained on Test Set per Category
  Table 4 caption:
    table_text: TABLE 4 Average Absolute Error (AAE) Obtained on Test Set per Each
      Continuous Dimension
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2916866
- Affiliation of the first author: rapid-rich object search lab, school of electrical
    and electronic engineering, nanyang technological university, singapore
  Affiliation of the last author: rapid-rich object search lab, school of electrical
    and electronic engineering, nanyang technological university, singapore
  Figure 1 Link: articels_figures_by_rev_year\2019\NTU_RGBD__A_LargeScale_Benchmark_for_D_Human_Activity_Understanding\figure_1.jpg
  Figure 1 caption: 'Illustration of the configuration of 25 body joints in our dataset.
    The labels of these joints are: (1) base of spine, (2) middle of spine, (3) neck,
    (4) head, (5) left shoulder, (6) left elbow, (7) left wrist, (8) left hand, (9)
    right shoulder, (10) right elbow, (11) right wrist, (12) right hand, (13) left
    hip, (14) left knee, (15) left ankle, (16) left foot, (17) right hip, (18) right
    knee, (19) right ankle, (20) right foot, (21) spine, (22) tip of left hand, (23)
    left thumb, (24) tip of right hand, (25) right thumb.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\NTU_RGBD__A_LargeScale_Benchmark_for_D_Human_Activity_Understanding\figure_2.jpg
  Figure 2 caption: Illustration of the body part feature generation network.
  Figure 3 Link: articels_figures_by_rev_year\2019\NTU_RGBD__A_LargeScale_Benchmark_for_D_Human_Activity_Understanding\figure_3.jpg
  Figure 3 caption: "Illustration of estimating the semantic relevance score between\
    \ the novel actions text description (name) and the body parts text description\
    \ (name). Here we take the action \u201Cmake ok sign\u201D with the body part\
    \ \u201Cright hand\u201D as an example. Each word in a text description is fed\
    \ to the pre-trained Word2Vec model to produce its embedding (a 300-dimensional\
    \ vector), and the representation of a text description is obtained by averaging\
    \ the embedding of all the words in it. Finally, the semantic relevance is estimated\
    \ by calculating the cosine similarity between the two representations."
  Figure 4 Link: articels_figures_by_rev_year\2019\NTU_RGBD__A_LargeScale_Benchmark_for_D_Human_Activity_Understanding\figure_4.jpg
  Figure 4 caption: "Examples of semantic relevance scores between actions name and\
    \ each body parts name. In the 1st and 3rd columns, the body parts (joints) with\
    \ larger scores are labeled with red circles. In the 2nd and 4th columns, we show\
    \ the scores of several body parts. These scores are obtained by using pre-trained\
    \ word embedding model [28]. Semantically, \u201Cright foot\u201D and \u201Cleft\
    \ foot\u201D are very relevant to \u201Chopping, one foot jumping\u201D, while\
    \ \u201Cright hand\u201D and \u201Cleft hand\u201D are relevant to \u201Cwield\
    \ knife towards other person\u201D."
  Figure 5 Link: articels_figures_by_rev_year\2019\NTU_RGBD__A_LargeScale_Benchmark_for_D_Human_Activity_Understanding\figure_5.jpg
  Figure 5 caption: Evaluation of using different sizes of training set for action
    recognition with different methods. In this figure, ratio 1.0 means the full training
    set is used for network training.
  Figure 6 Link: articels_figures_by_rev_year\2019\NTU_RGBD__A_LargeScale_Benchmark_for_D_Human_Activity_Understanding\figure_6.jpg
  Figure 6 caption: Evaluation of using different sizes of training set for action
    recognition with different data modalities. The skeleton data modality is evaluated
    using the Spatio-Temporal LSTM [48]. The RGB and depth data modalities are both
    evaluated using the two-stream framework [111].
  Figure 7 Link: articels_figures_by_rev_year\2019\NTU_RGBD__A_LargeScale_Benchmark_for_D_Human_Activity_Understanding\figure_7.jpg
  Figure 7 caption: Confusion matrix of the RGB data modality.
  Figure 8 Link: articels_figures_by_rev_year\2019\NTU_RGBD__A_LargeScale_Benchmark_for_D_Human_Activity_Understanding\figure_8.jpg
  Figure 8 caption: Confusion matrix of Internal Feature Fusion [94].
  Figure 9 Link: articels_figures_by_rev_year\2019\NTU_RGBD__A_LargeScale_Benchmark_for_D_Human_Activity_Understanding\figure_9.jpg
  Figure 9 caption: Sample frames of the NTU RGB+D 120 dataset. The first four rows
    show the variety in human subjects, camera views, and environmental conditions.
    The fifth row depicts the intra-class variation of the performances. The last
    row illustrates the RGB, RGB+joints, depth, depth+joints, and IR modalities of
    a sample frame.
  First author gender probability: 0.9
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: Jun Liu
  Name of the last author: Alex C. Kot
  Number of Figures: 9
  Number of Tables: 9
  Number of authors: 6
  Paper title: 'NTU RGB+D 120: A Large-Scale Benchmark for 3D Human Activity Understanding'
  Publication Date: 2019-05-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of the Proposed NTU RGB+D 120 Dataset and Some
      of the Other Publicly Available Datasets for 3D Action Recognition
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of the Dataset Version Introduced in This Paper
      and the One Released in Our Preliminary Conference Paper [47]
  Table 3 caption:
    table_text: TABLE 3 The Cameras Height and Distance to the Subjects in Each Collection
      Setup
  Table 4 caption:
    table_text: TABLE 4 The Results of Different Methods, Which Are Designed for 3D
      Human Activity Analysis, Using the Cross-Subject and Cross-Setup Evaluation
      Criteria on the NTU RGB+D 120 Dataset
  Table 5 caption:
    table_text: TABLE 5 Evaluation of Using Different Data Modalities (RGB, Depth,
      and 3D Skeleton Data) for Action Recognition on the NTU RGB+D 120 Dataset
  Table 6 caption:
    table_text: TABLE 6 Action Recognition Results of Different Data Modalities on
      the NTU RGB+D 120 Dataset
  Table 7 caption:
    table_text: TABLE 7 Action Recognition Results of Different Methods on the NTU
      RGB+D 120 Dataset
  Table 8 caption:
    table_text: TABLE 8 The Results of One-Shot 3D Action Recognition on the NTU RGB+D
      120 Dataset
  Table 9 caption:
    table_text: TABLE 9 The Results of Using Different Sizes of Auxiliary Training
      Set to Learn the Feature Generation Network, for One-Shot Recognition on the
      Novel Action Classes
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2916873
- Affiliation of the first author: research school of engineering, australian national
    university, canberra, act, australia
  Affiliation of the last author: research school of engineering, australian national
    university, canberra, act, australia
  Figure 1 Link: articels_figures_by_rev_year\2019\Semantic_Face_Hallucination_SuperResolving_Very_LowResolution_Face_Images_with_S\figure_1.jpg
  Figure 1 caption: "Comparison with the state-of-the-art CNN based face hallucination\
    \ methods. (a) 16\xD716 LR input image. (b) 128\xD7128 HR original image (not\
    \ used in training). (c) The corresponding HR image of the nearest neighbor of\
    \ the given LR image in the dataset after compensating for misalignments. (d)\
    \ Result of VDSR [7], which is a CNN based generic super-resolution method. (e)\
    \ Result of VDSR \u2020 [7] retrained with LR and HR face image pairs. (f) Result\
    \ of CBN [5]. (g) Result of TDAE [4]. (h) Our result."
  Figure 10 Link: articels_figures_by_rev_year\2019\Semantic_Face_Hallucination_SuperResolving_Very_LowResolution_Face_Images_with_S\figure_10.jpg
  Figure 10 caption: "Discussions on the variants of our network. (a) 16\xD716 LR\
    \ input images. (b) 128\xD7128 HR ground-truth images. (c) Results of using a\
    \ shared CNN branch EN s to encode attributes in super-resolution. (d) Results\
    \ of using all neutral attributes. (e) Results of using completely wrong attributes.\
    \ (f) Results without embedding attribute information. (g) Results of using a\
    \ standard discriminative network. (h) Our results."
  Figure 2 Link: articels_figures_by_rev_year\2019\Semantic_Face_Hallucination_SuperResolving_Very_LowResolution_Face_Images_with_S\figure_2.jpg
  Figure 2 caption: 'The architecture of our attribute embedded upsampling network.
    The network consists of two parts: an upsampling network and a discriminative
    network. The upsampling network takes LR faces and attribute vectors as inputs
    while the discriminative network takes realsuper-resolved HR face images and attribute
    vectors as inputs.'
  Figure 3 Link: articels_figures_by_rev_year\2019\Semantic_Face_Hallucination_SuperResolving_Very_LowResolution_Face_Images_with_S\figure_3.jpg
  Figure 3 caption: "Ablation study of our network. (a) 16\xD716 LR input image. (b)\
    \ 128\xD7128 HR ground-truth image, its ground-truth attributes are male and old.\
    \ (c) Result without using an autoencoder. Here, the attribute vectors are replicated\
    \ and then concatenated with the LR input directly. (d) Result without using skip\
    \ connections in the autoencoder. (e) Result by only using an \u2113 2 loss. (f)\
    \ Result without using the attribute embedding but with a standard discriminative\
    \ network. In this case, the network is similar to the decoder in [4]. (g) Result\
    \ without using the perceptual loss. (h) Our final result."
  Figure 4 Link: articels_figures_by_rev_year\2019\Semantic_Face_Hallucination_SuperResolving_Very_LowResolution_Face_Images_with_S\figure_4.jpg
  Figure 4 caption: Comparison with the state-of-the-arts methods on male images.
    (a) Unaligned LR inputs. (b) Original HR images. (c) Bicubic interpolation. (d)
    Results of Ma et al.s method [9]. (e) Results of Shi et al.s method [22]. (f)
    Results of Jiang et al.s method (TLcR-RL) [24]. (g) Results of Kim et al.s method
    (VDSR) [7]. (h) Results of Ledig et al.s method (SRGAN) [54]. (i) Results of Zhu
    et al.s method (CBN) [5]. (j) Results of Cao et al.s method [6]. (k) Results of
    Yu and Poriklis method (TDAE) [4]. (l) Results of Chen et al.s method (FSRNet)
    [35]. (m) Results of Yu et al.s method [36]. (n) Results of Lee et al.s method
    (AACNN) [40]. (o) Our results.
  Figure 5 Link: articels_figures_by_rev_year\2019\Semantic_Face_Hallucination_SuperResolving_Very_LowResolution_Face_Images_with_S\figure_5.jpg
  Figure 5 caption: Comparison with the state-of-the-arts methods on male images.
    (a) Unaligned LR inputs. (b) Original HR images. (c) Bicubic interpolation. (d)
    Results of Ma et al.s method [9]. (e) Results of Shi et al.s method [22]. (f)
    Results of Jiang et al.s method (TLcR-RL) [24]. (g) Results of Kim et al.s method
    (VDSR) [7]. (h) Results of Ledig et al.s method (SRGAN) [54]. (i) Results of Zhu
    et al.s method (CBN) [5]. (j) Results of Cao et al.s method [6]. (k) Results of
    Yu and Poriklis method (TDAE) [4]. (l) Results of Chen et al.s method (FSRNet)
    [35]. (m) Results of Yu et al.s method [36]. (n) Results of Lee et al.s method
    (AACNN) [40]. (o) Our results.
  Figure 6 Link: articels_figures_by_rev_year\2019\Semantic_Face_Hallucination_SuperResolving_Very_LowResolution_Face_Images_with_S\figure_6.jpg
  Figure 6 caption: Comparison with the state-of-the-arts methods on female images.
    (a) Unaligned LR inputs. (b) Original HR images. (c) Bicubic interpolation. (d)
    Results of Ma et al.s method [9]. (e) Results of Shi et al.s method [22]. (f)
    Results of Jiang et al.s method (TLcR-RL) [24]. (g) Results of Kim et al.s method
    (VDSR) [7]. (h) Results of Ledig et al.s method (SRGAN) [54]. (i) Results of Zhu
    et al.s method (CBN) [5]. (j) Results of Cao et al.s method [6]. (k) Results of
    Yu and Poriklis method (TDAE) [4]. (l) Results of Chen et al.s method (FSRNet)
    [35]. (m) Results of Yu et al.s method [36]. (n) Results of Lee et al.s method
    (AACNN) [40]. (o) Our results.
  Figure 7 Link: articels_figures_by_rev_year\2019\Semantic_Face_Hallucination_SuperResolving_Very_LowResolution_Face_Images_with_S\figure_7.jpg
  Figure 7 caption: Comparison with the state-of-the-arts methods on female images.
    (a) Unaligned LR inputs. (b) Original HR images. (c) Bicubic interpolation. (d)
    Results of Ma et al.s method [9]. (e) Results of Shi et al.s method [22]. (f)
    Results of Jiang et al.s method (TLcR-RL) [24]. (g) Results of Kim et al.s method
    (VDSR) [7]. (h) Results of Ledig et al.s method (SRGAN) [54]. (i) Results of Zhu
    et al.s method (CBN) [5]. (j) Results of Cao et al.s method [6]. (k) Results of
    Yu and Poriklis method (TDAE) [4]. (l) Results of Chen et al.s method (FSRNet)
    [35]. (m) Results of Yu et al.s method [36]. (n) Results of Lee et al.s method
    (AACNN) [40]. (o) Our results.
  Figure 8 Link: articels_figures_by_rev_year\2019\Semantic_Face_Hallucination_SuperResolving_Very_LowResolution_Face_Images_with_S\figure_8.jpg
  Figure 8 caption: Results of the state-of-the-arts methods on unaligned LR face
    images. (a) Unaligned LR inputs. (b) Original HR images. (c) Results of Zhu et
    al.s method (CBN) [5]. (d) Results of Chen et al.s method (FSRNet) [35]. (e) Our
    results.
  Figure 9 Link: articels_figures_by_rev_year\2019\Semantic_Face_Hallucination_SuperResolving_Very_LowResolution_Face_Images_with_S\figure_9.jpg
  Figure 9 caption: 'Our method can fine-tune the super-resolved results by adjusting
    the attributes. From top to bottom: the LR input faces, the HR ground-truth faces,
    our results with ground-truth attributes, our results by adjusting attributes.
    (a) Reversing genders of super-resolved faces. (b) Aging upsampled faces. (c)
    Removing makeups. (d) Changing noses. (The first two columns: making noses pointy,
    and the last two columns: making noses bigger.) (e) Adding and removing beard.
    (f) Narrowing and opening eyes. (g) Making and removing bushy Eyebrows. (h) Making
    lips bigger. (i) Opening and closing mouths.'
  First author gender probability: 0.6
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Xin Yu
  Name of the last author: Fatih Porikli
  Number of Figures: 10
  Number of Tables: 5
  Number of authors: 4
  Paper title: 'Semantic Face Hallucination: Super-Resolving Very Low-Resolution Face
    Images with Supplementary Attributes'
  Publication Date: 2019-05-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Evaluations on the Test Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Classification Results Impacted by Tuning Attributes
  Table 3 caption:
    table_text: TABLE 3 Ablation Study on Our Proposed Network
  Table 4 caption:
    table_text: TABLE 4 Embedding Attributes into Different Layers of D D
  Table 5 caption:
    table_text: TABLE 5 Quantitative Evaluations of Impact of Different Losses
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2916881
- Affiliation of the first author: "pattern recognition lab, friedrich-alexander-universit\xE4\
    t (fau) erlangen-n\xFCrnberg, erlangen, germany"
  Affiliation of the last author: "it security infrastructures lab, fau erlangen-n\xFC\
    rnberg, erlangen, germany"
  Figure 1 Link: articels_figures_by_rev_year\2019\Toward_Bridging_the_SimulatedtoReal_Gap_Benchmarking_SuperResolution_on_Real_Dat\figure_1.jpg
  Figure 1 caption: Can experiments on simulated data predict the behavior of SR on
    real data? Our study reveals that this is not the case. (a) VDSR [14] and its
    color-coded error w.r.t. the ground truth on simulated low-resolution data. (b)
    VDSR on our real acquisitions of the same scene. The simulation considerably overestimates
    the performance both visually and quantitatively. We benchmark SR algorithms on
    captured data to overcome shortcomings of simplistic simulations.
  Figure 10 Link: articels_figures_by_rev_year\2019\Toward_Bridging_the_SimulatedtoReal_Gap_Benchmarking_SuperResolution_on_Real_Dat\figure_10.jpg
  Figure 10 caption: "Robustness analysis of MFSR w.r.t. photometric variations. The\
    \ x -axis denote the number of photometric outliers within a set of K=11 frames.\
    \ The y -axis depicts the normalized PSNR and IFC averaged over 14 scenes with\
    \ global motion and 3\xD7 magnification."
  Figure 2 Link: articels_figures_by_rev_year\2019\Toward_Bridging_the_SimulatedtoReal_Gap_Benchmarking_SuperResolution_on_Real_Dat\figure_2.jpg
  Figure 2 caption: Overview of our data collection and benchmark. In our data collection,
    we capture multiple frames at the actual pixel resolution to obtain ground truth
    high-resolution images via frame averaging. We employ hardware binning on the
    sensor to gain captured low-resolution images and include postprocessing (e.g.
    imagevideo compression) to collect multiple versions of this low-resolution data
    (see Section 3). In the benchmark, we use 1) full-reference and no-reference measures
    to quantitatively assess super-resolved data with and without exploiting the ground
    truth (see Sections 4 and 5), and 2) observer studies to evaluate image quality
    according to human perception (see Section 6).
  Figure 3 Link: articels_figures_by_rev_year\2019\Toward_Bridging_the_SimulatedtoReal_Gap_Benchmarking_SuperResolution_on_Real_Dat\figure_3.jpg
  Figure 3 caption: Overview of the scenes covered by our SupER database.
  Figure 4 Link: articels_figures_by_rev_year\2019\Toward_Bridging_the_SimulatedtoReal_Gap_Benchmarking_SuperResolution_on_Real_Dat\figure_4.jpg
  Figure 4 caption: Proposed hardware setup and an example scene. Our data comprise
    challenging conditions for SR like local object motion and photometric variations.
  Figure 5 Link: articels_figures_by_rev_year\2019\Toward_Bridging_the_SimulatedtoReal_Gap_Benchmarking_SuperResolution_on_Real_Dat\figure_5.jpg
  Figure 5 caption: "Correlation between the benchmarks of SR algorithms on simulated\
    \ data and our captured LR data in terms of different full-reference quality measures\
    \ for 2\xD72 (top row) and 4\xD74 binning (bottom row). The individual algorithms\
    \ are categorized either as SISR (shown with blue color map) or MFSR (shown with\
    \ red color map). Algorithms located below the line of equal image quality perform\
    \ worse on real data compared to simulated data. For each quality measure, the\
    \ corresponding Spearman rank correlation \u03C1 is shown."
  Figure 6 Link: articels_figures_by_rev_year\2019\Toward_Bridging_the_SimulatedtoReal_Gap_Benchmarking_SuperResolution_on_Real_Dat\figure_6.jpg
  Figure 6 caption: "Benchmark of the SR algorithms for global (a), mixed (b), and\
    \ local motion (c) using the mean normalized PSNR and IFC for 2\xD7 , 3\xD7 and\
    \ 4\xD7 magnification. The algorithms are categorized either as SISR (shown with\
    \ blue color map) or MFSR (shown with red color map)."
  Figure 7 Link: articels_figures_by_rev_year\2019\Toward_Bridging_the_SimulatedtoReal_Gap_Benchmarking_SuperResolution_on_Real_Dat\figure_7.jpg
  Figure 7 caption: "SR methods under global motion on the newspapers dataset ( 3\xD7\
    \ magnification). Top: SR on raw data. MFSR (e.g., WNUISR and IRWSR) outperforms\
    \ SISR algorithms (e.g., A+ and VDSR) w.r.t. the recovery of fine structures like\
    \ text and reconstruction algorithms with sparsity priors (e.g., IRWSR) enhanced\
    \ the recovery of HR details compared to interpolation-based methods (e.g., WNUISR).\
    \ Bottom: SR under H.265HEVC coding (quantization QP30). All methods are affected\
    \ by compression artifacts and become indistinguishable."
  Figure 8 Link: articels_figures_by_rev_year\2019\Toward_Bridging_the_SimulatedtoReal_Gap_Benchmarking_SuperResolution_on_Real_Dat\figure_8.jpg
  Figure 8 caption: "Robustness analysis of SR w.r.t. video compression. The x -axis\
    \ denotes the compression level in terms of H.265HEVC quantization. The y -axis\
    \ depicts the normalized PSNR and IFC averaged over 14 scenes with global motion\
    \ and 3\xD7 magnification. The individual algorithms are categorized either as\
    \ SISR (shown with blue color map) or MFSR (shown with red color map)."
  Figure 9 Link: articels_figures_by_rev_year\2019\Toward_Bridging_the_SimulatedtoReal_Gap_Benchmarking_SuperResolution_on_Real_Dat\figure_9.jpg
  Figure 9 caption: "SR on the dynamic coffee dataset ( 3\xD7 magnification). Top:\
    \ Mixed motion due movements of a coffee cup. In contrast to SISR (e.g. A+ [12]),\
    \ simple interpolation-based MFSR (e.g. NUISR [74]) is prone to erroneous optical\
    \ flow caused by occlusions near object boundaries, while robust reconstruction\
    \ (e.g. IRWSR [23]) and deep learning methods (e.g. VSRnet [25]) partly compensate\
    \ for uncertainties. Bottom: Local object motion without camera motion. Except\
    \ VSRnet [25] that exploits training data, MFSR cannot effectively enhance the\
    \ resolution."
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "Thomas K\xF6hler"
  Name of the last author: Christian Riess
  Number of Figures: 16
  Number of Tables: 4
  Number of authors: 6
  Paper title: 'Toward Bridging the Simulated-to-Real Gap: Benchmarking Super-Resolution
    on Real Data'
  Publication Date: 2019-05-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Parameters of the Basler acA2000-50gm Camera [55] and the
      f1.8 16 mm Fixed-Focus Length Lens for Our Data Collection [56]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Overview of Motion Types Captured in Our Database
  Table 3 caption:
    table_text: TABLE 3 Comparison of Our SupER Database to Other Publicly Available
      Benchmark Datasets
  Table 4 caption:
    table_text: TABLE 4 Categorization of the SR Algorithms in Our Benchmark
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2917037
- Affiliation of the first author: computer science department, lakehead university,
    thunder bay, ontario, canada
  Affiliation of the last author: department of electrical and computer engineering,
    university of windsor, ontario, canada
  Figure 1 Link: articels_figures_by_rev_year\2019\Recomputation_of_the_Dense_Layers_for_Performance_Improvement_of_DCNN\figure_1.jpg
  Figure 1 caption: Schematic Diagram of the proposed method.
  Figure 10 Link: articels_figures_by_rev_year\2019\Recomputation_of_the_Dense_Layers_for_Performance_Improvement_of_DCNN\figure_10.jpg
  Figure 10 caption: 'Top-1 Testing Accuracy of Places365: Our method with VGG16 Vs
    VGG-16.'
  Figure 2 Link: articels_figures_by_rev_year\2019\Recomputation_of_the_Dense_Layers_for_Performance_Improvement_of_DCNN\figure_2.jpg
  Figure 2 caption: "Learning strategy of the proposed method. \u27F5 represents the\
    \ feedforward operations of the proposed method, while \u27F5 represents the error-inverse\
    \ operations of the proposed method. (a) Step 1: update parameters ( a 1 ,\u2026\
    , a n ) in each fully connected layer. (b) Step 2: update the adjustment of the\
    \ outputs ( P 1 ,\u2026, P n ) in each fully connected layer."
  Figure 3 Link: articels_figures_by_rev_year\2019\Recomputation_of_the_Dense_Layers_for_Performance_Improvement_of_DCNN\figure_3.jpg
  Figure 3 caption: Testing accuracy performance comparison of AlexNet with or without
    learning rate used in the proposed method.
  Figure 4 Link: articels_figures_by_rev_year\2019\Recomputation_of_the_Dense_Layers_for_Performance_Improvement_of_DCNN\figure_4.jpg
  Figure 4 caption: Testing accuracy performance comparison of AlexNet with or without
    learning rate used in the proposed method.
  Figure 5 Link: articels_figures_by_rev_year\2019\Recomputation_of_the_Dense_Layers_for_Performance_Improvement_of_DCNN\figure_5.jpg
  Figure 5 caption: 'Illustration of the key re-calculation operations with the proposed
    method in fully-connected layers. longleftarrow represents the feedforward operations
    of the proposed method, while longleftarrow represents the error-inverse operations
    of the proposed method. (a) Step 1: update parameters ( mathbf an ) in the n th
    fully connected layer. (b) Step 2.1: update the current output error of the n
    th layer with the updated parameters ( mathbf an ). (c) Step 2.2: update the desired
    adjustment of n-1 th layer mathbf Pn-1 with updated error mathbf en .'
  Figure 6 Link: articels_figures_by_rev_year\2019\Recomputation_of_the_Dense_Layers_for_Performance_Improvement_of_DCNN\figure_6.jpg
  Figure 6 caption: Dropout strategy for updating parameters. For instance, the initial
    parameters of 20 neurons are distributed in space alpha . After updating the neurons
    using the proposed method, the parameters of 20 neurons are distributed in space
    beta . Finally, if the dropout rate equals 50 percent, we only randomly select
    ten neurons from space alpha , and the other ten neurons from space beta to finalize
    the 20 recalculated neurons in space lambda .
  Figure 7 Link: articels_figures_by_rev_year\2019\Recomputation_of_the_Dense_Layers_for_Performance_Improvement_of_DCNN\figure_7.jpg
  Figure 7 caption: Retraining fully connected layers of VGG16 with our proposed method
    (CIFAR10). longleftarrow represents the feedforward operations of the proposed
    method, while longleftarrow represents the error-inverse operations of the proposed
    method.
  Figure 8 Link: articels_figures_by_rev_year\2019\Recomputation_of_the_Dense_Layers_for_Performance_Improvement_of_DCNN\figure_8.jpg
  Figure 8 caption: 'Top-1 Testing Accuracy of CIFAR10100 and SUN397: Our method with
    CNN models versus Original CNN models.'
  Figure 9 Link: articels_figures_by_rev_year\2019\Recomputation_of_the_Dense_Layers_for_Performance_Improvement_of_DCNN\figure_9.jpg
  Figure 9 caption: 'Top-1 Testing Accuracy of Places365: Our method with AlexNet
    versus AlexNet.'
  First author gender probability: 0.66
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yimin Yang
  Name of the last author: Thangarajah Akilan
  Number of Figures: 12
  Number of Tables: 7
  Number of authors: 4
  Paper title: Recomputation of the Dense Layers for Performance Improvement of DCNN
  Publication Date: 2019-05-20 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Notations Used in the Paper
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Scene-15 Classification Accuracy for Our Method Against Leading
      Alternate Approaches without Data Argumentation
  Table 3 caption:
    table_text: TABLE 3 Experimental Settings under the Condition of Transfer Learning
  Table 4 caption:
    table_text: TABLE 4 Experimental Settings under the Condition of Training from
      Scratch
  Table 5 caption:
    table_text: TABLE 5 Classification Accuracy for Our Method Against Other Leading
      Methods without Data Augmentation
  Table 6 caption:
    table_text: TABLE 6 Top-1 Classification Accuracy with Both Our Method and Original
      BP Method
  Table 7 caption:
    table_text: TABLE 7 Classification Accuracy on Scene-Centric Databases for the
      Deep Features of Object-Centric Databases (ImageNet)
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2917685
- Affiliation of the first author: department of electrical engineering and computer
    science, northwestern university, evanston, il, usa
  Affiliation of the last author: department of electrical engineering and computer
    science, northwestern university, evanston, il, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Learning_Visual_Instance_Retrieval_from_Failure_Efficient_Online_Local_Metric_Ad\figure_1.jpg
  Figure 1 caption: The overall idea of our proposed online local metric adaptation
    algorithm illustrated in the context of P-RID. Unlike existing offline learning-based
    methods that learn a single global metric or feature embedding for all probe and
    gallery samples, we exploit negative samples to learn a dedicated local metric
    for each online probe to adapt the offline learned global feature space to an
    instance-specific discriminative local feature space (called OLMANS feature space).
    The hard negatives in NDB around the local hypersphere of the query probe are
    pushed far away so the final retrieval result in OLMANS feature space is improved.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Learning_Visual_Instance_Retrieval_from_Failure_Efficient_Online_Local_Metric_Ad\figure_2.jpg
  Figure 2 caption: 'The improvement of ranking result by our OLMANS on VIPeR [36].
    BLUE boxes: input probes, RED: gallery targets. For each case, the top row is
    the result from the baseline [5], and the bottom row is our result. (Best view
    in color and enlarged).'
  Figure 3 Link: articels_figures_by_rev_year\2019\Learning_Visual_Instance_Retrieval_from_Failure_Efficient_Online_Local_Metric_Ad\figure_3.jpg
  Figure 3 caption: "The local metric M i for a single probe p i can push the closest\
    \ negative sample y j of p i away from its local hypersphere \u03A9( p i ) ."
  Figure 4 Link: articels_figures_by_rev_year\2019\Learning_Visual_Instance_Retrieval_from_Failure_Efficient_Online_Local_Metric_Ad\figure_4.jpg
  Figure 4 caption: "The local metric M i for a set-based probe P i can pull the same-instance\
    \ samples together meanwhile push the closest negative samples y j away from the\
    \ local hypersphere \u03A9( P i )"
  Figure 5 Link: articels_figures_by_rev_year\2019\Learning_Visual_Instance_Retrieval_from_Failure_Efficient_Online_Local_Metric_Ad\figure_5.jpg
  Figure 5 caption: The influence of baseline quality. The x -axis means the maximum
    iteration time for offline learning and the y -axis is the identification rate
    (Rank1, Rank5 and Rank10 on VIPeR).
  Figure 6 Link: articels_figures_by_rev_year\2019\Learning_Visual_Instance_Retrieval_from_Failure_Efficient_Online_Local_Metric_Ad\figure_6.jpg
  Figure 6 caption: The influence of baseline metric choice. (a) and (d) are the results
    on VIPeR and GRID directly using the euclidean distance; (b) and (e) are XQDA
    [6] results; (c) and (f) are MLAPG [5] results.
  Figure 7 Link: articels_figures_by_rev_year\2019\Learning_Visual_Instance_Retrieval_from_Failure_Efficient_Online_Local_Metric_Ad\figure_7.jpg
  Figure 7 caption: The influence of parameter lambda . The x-axis means the value
    of lambda and the y-axis is the identification rate. The results at Rank1, Rank5
    and Rank10 on VIPeR are shown.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.69
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.86
  Name of the first author: Jiahuan Zhou
  Name of the last author: Ying Wu
  Number of Figures: 7
  Number of Tables: 12
  Number of authors: 2
  Paper title: 'Learning Visual Instance Retrieval from Failure: Efficient Online
    Local Metric Adaptation from Negative Samples'
  Publication Date: 2019-05-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Statistics of Different P-RID Benchmarks
  Table 10 caption:
    table_text: TABLE 10 Average Learning Time (Seconds) on VIPeR
  Table 2 caption:
    table_text: TABLE 2 Comparison Results with the Global Metric Learning Methods
      on VIPeR Using the Same LOMO Feature
  Table 3 caption:
    table_text: TABLE 3 Comparison with the Global Metric Learning Methods on GRID
      Using the Same LOMO Feature
  Table 4 caption:
    table_text: TABLE 4 Comparison Results on CUHK03, Market1501, and DukeMTMC-reID
  Table 5 caption:
    table_text: TABLE 5 State-of-the-Art Comparison Results on on MSMT17
  Table 6 caption:
    table_text: TABLE 6 Comparison with the State-of-the-Art Re-Ranking Method
  Table 7 caption:
    table_text: TABLE 7 The Influence of Baseline Metric Choice
  Table 8 caption:
    table_text: TABLE 8 The Influence of Baseline Feature Choices on VIPeR and GRID
      under Different Metrics (10-Folds Average Rank1 Performance Is Reported)
  Table 9 caption:
    table_text: TABLE 9 The Influence of Different NDBs on VIPeR
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2918208
- Affiliation of the first author: department of automation, tsinghua university,
    beijing, china
  Affiliation of the last author: department of computer science, cornell university,
    ithaca, ny, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Convolutional_Networks_with_Dense_Connectivity\figure_1.jpg
  Figure 1 caption: A 5-layer dense block with a growth rate of k=4 . Each layer takes
    all preceding feature-maps as input.
  Figure 10 Link: articels_figures_by_rev_year\2019\Convolutional_Networks_with_Dense_Connectivity\figure_10.jpg
  Figure 10 caption: Comparison of DenseNet and its variants with partial dense connectivity,
    in terms of parameter efficiency (Left) and computational efficiency (Right).
  Figure 2 Link: articels_figures_by_rev_year\2019\Convolutional_Networks_with_Dense_Connectivity\figure_2.jpg
  Figure 2 caption: A deep DenseNet with three dense blocks. The layers between two
    adjacent blocks are referred to as transition layers and change feature map sizes
    via convolution and pooling.
  Figure 3 Link: articels_figures_by_rev_year\2019\Convolutional_Networks_with_Dense_Connectivity\figure_3.jpg
  Figure 3 caption: 'DenseNet layer forward pass: original implementation (left) and
    efficient implementation (right). Solid boxes correspond to tensors allocated
    in memory, where as translucent boxes are pointers. Solid arrows represent computation,
    and dotted arrows represent memory pointers. The efficient implementation stores
    the output of the concatenation and pre-activation batch normalizationReLU operations
    in temporary storage buffers, whereas the original implementation allocates new
    memory.'
  Figure 4 Link: articels_figures_by_rev_year\2019\Convolutional_Networks_with_Dense_Connectivity\figure_4.jpg
  Figure 4 caption: 'Left: Comparison of the parameter efficiency on C10+ between
    DenseNet variations. Middle: Comparison of the parameter efficiency between DenseNet
    and (pre-activation) ResNets. DenseNet requires about 13 of the parameters as
    ResNet to achieve comparable accuracy. Right: Training and testing curves of the
    1001-layer pre-activation ResNet [36] with more than 10M parameters and a 100-layer
    DenseNet with only 0.8M parameters.'
  Figure 5 Link: articels_figures_by_rev_year\2019\Convolutional_Networks_with_Dense_Connectivity\figure_5.jpg
  Figure 5 caption: Comparison of the DenseNet and ResNet Top-1 (single model and
    single-crop) error rates on the ImageNet classification dataset as a function
    of learned parameters (left), flops (middle, and GPU memory footprint at training
    time (right). Training GPU memory measured using the efficient LuaTorch DenseNet
    implementation with a batch size of 64.
  Figure 6 Link: articels_figures_by_rev_year\2019\Convolutional_Networks_with_Dense_Connectivity\figure_6.jpg
  Figure 6 caption: The average absolute filter weights of convolutional layers in
    a trained DenseNet. The color of pixel (s,ell) encodes the average L1 norm (normalized
    by the number of input feature maps) of the weights connecting convolutional layer
    s to layer ell within a dense block. The three columns highlighted by black rectangles
    correspond to the two transition layers and the classification layer. The first
    row encodes those weights connected to the input layer of the dense block.
  Figure 7 Link: articels_figures_by_rev_year\2019\Convolutional_Networks_with_Dense_Connectivity\figure_7.jpg
  Figure 7 caption: 'Left and Middle: GPU memory consumption as a function of network
    depthnumber of parameters. Each model is a DenseNet with k=12 features added per
    layer. The efficient implementation can train much deeper models with less memory.
    Right: Computation time (on a NVIDIA Maxwell Titan-X).'
  Figure 8 Link: articels_figures_by_rev_year\2019\Convolutional_Networks_with_Dense_Connectivity\figure_8.jpg
  Figure 8 caption: Top-1 validation error on ImageNet as a function of the computational
    cost (measured by flops) of different DenseNets, with varying growth rate (Left),
    varying width of the bottleneck layers (Middle) and varying compression ratio
    at transition layers (Right).
  Figure 9 Link: articels_figures_by_rev_year\2019\Convolutional_Networks_with_Dense_Connectivity\figure_9.jpg
  Figure 9 caption: Comparison of DenseNet and its variant with full dense connectivity,
    in terms of parameter efficiency (Left) and computational efficiency (Right).
  First author gender probability: 0.95
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Gao Huang
  Name of the last author: Kilian Q. Weinberger
  Number of Figures: 12
  Number of Tables: 3
  Number of authors: 5
  Paper title: Convolutional Networks with Dense Connectivity
  Publication Date: 2019-05-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 DenseNet Architectures for ImageNet
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Error Rates (%) on CIFAR and SVHN Datasets
  Table 3 caption:
    table_text: TABLE 3 The Top-1 and Top-5 Error Rates on the ImageNet Validation
      Set, with Single-Crop 10-Crop Testing
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2918284
