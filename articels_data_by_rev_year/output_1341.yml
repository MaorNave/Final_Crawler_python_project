- Affiliation of the first author: department of computer science and information
    engineering, national chiayi university, chiayi, r.o.c.
  Affiliation of the last author: department of computer science and information engineering,
    national chiayi university, chiayi, r.o.c.
  Figure 1 Link: articels_figures_by_rev_year\2019\Learning_to_Index_for_Nearest_Neighbor_Search\figure_1.jpg
  Figure 1 caption: A 2D euclidean space, where A, B, and C are clusters, a, b, and
    c are the respective centroids, and q is the query. The clusters can be ranked
    based on their query-centroid distances, which is widely used in existing NN search
    methods. However, the distance-based ranking does not reflect the NN distribution
    among the clusters well.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Learning_to_Index_for_Nearest_Neighbor_Search\figure_2.jpg
  Figure 2 caption: Probability-based ranking versus distance-based ranking. Data
    points are from the wine dataset (UCI machine learning repository), a thirteen-feature
    set labeled by three different classes. We project the data points to a 2D space
    constituted by the top two eigenvectors with the highest eigenvalues for visualization
    purposes.
  Figure 3 Link: articels_figures_by_rev_year\2019\Learning_to_Index_for_Nearest_Neighbor_Search\figure_3.jpg
  Figure 3 caption: Top-1 recall of the first-level retrieval in SIFT1B.
  Figure 4 Link: articels_figures_by_rev_year\2019\Learning_to_Index_for_Nearest_Neighbor_Search\figure_4.jpg
  Figure 4 caption: Top-1 recall of the first-level retrieval in DEEP1B.
  Figure 5 Link: articels_figures_by_rev_year\2019\Learning_to_Index_for_Nearest_Neighbor_Search\figure_5.jpg
  Figure 5 caption: Top-1 recall of the second-level retrieval in SIFT1B.
  Figure 6 Link: articels_figures_by_rev_year\2019\Learning_to_Index_for_Nearest_Neighbor_Search\figure_6.jpg
  Figure 6 caption: Top-1 recall of the second-level retrieval in DEEP1B.
  Figure 7 Link: articels_figures_by_rev_year\2019\Learning_to_Index_for_Nearest_Neighbor_Search\figure_7.jpg
  Figure 7 caption: R1 versus runtime in DEEP1B.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Chih-Yi Chiu
  Name of the last author: Yin-Chih Liao
  Number of Figures: 7
  Number of Tables: 5
  Number of authors: 3
  Paper title: Learning to Index for Nearest Neighbor Search
  Publication Date: 2019-03-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Configurations of the Index Structures in the Reference Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Top-1 Recall and Runtime (Milliseconds) of the First-Level
      Retrieval for Different First-Level Codebooks in SIFT1B RVQ
  Table 3 caption:
    table_text: "TABLE 3 Top-1 Recall and Runtime (Milliseconds) of the Second-Level\
      \ Retrieval for Indexing PCA-Compressed and Original Data in SIFT1B and DEEP1B\
      \ RVQ 4096\xD74096 4096\xD74096"
  Table 4 caption:
    table_text: "TABLE 4 Top-k Recall (k = 1, 10, 100, and 1000) in SIFT1B and DEEP1B\
      \ RVQ 4096\xD74096 4096\xD74096"
  Table 5 caption:
    table_text: TABLE 5 Top-1 RecallA (R1, R10, and R100), Runtime (milliseconds),
      and Memory Consumption (bytes) of the State-of-the-Art Methods
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2907086
- Affiliation of the first author: division of biostatistics, university of california
    at berkeley, berkeley, usa
  Affiliation of the last author: department of statistics, purdue university, west
    lafayette, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Tensor_Graphical_Model_NonConvex_Optimization_and_Statistical_Inference\figure_1.jpg
  Figure 1 caption: An illustration of generated triangle graph (left) in Simulations
    1 and four nearest neighbor graph (right) in Simulations 2. In this illustration,
    the dimension is 100.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Tensor_Graphical_Model_NonConvex_Optimization_and_Statistical_Inference\figure_2.jpg
  Figure 2 caption: 'The first row: Averaged computational time of each method in
    Simulations 1&2 , respectively. The second row: Averaged estimation error of Kronecker
    product of precision matrices of each method in Simulations 1&2 , respectively.
    Results for the direct Glasso method in Scenario s3 is omitted due to its extremely
    slow computation.'
  Figure 3 Link: articels_figures_by_rev_year\2019\Tensor_Graphical_Model_NonConvex_Optimization_and_Statistical_Inference\figure_3.jpg
  Figure 3 caption: Averaged estimation errors of precision matrices in Frobenius
    norm and max norm of each method in Simulations 1&2 , respectively. The first
    row is for Simulation 1, and the second row is for Simulation 2.
  Figure 4 Link: articels_figures_by_rev_year\2019\Tensor_Graphical_Model_NonConvex_Optimization_and_Statistical_Inference\figure_4.jpg
  Figure 4 caption: "QQ plots for fixed zero entry [ \u03A9 \u2217 1 ] 6,1 . From\
    \ left column to right column is scenario s1, s2 and s3. The first row is simulation\
    \ 1, and the second is simulation 2."
  Figure 5 Link: articels_figures_by_rev_year\2019\Tensor_Graphical_Model_NonConvex_Optimization_and_Statistical_Inference\figure_5.jpg
  Figure 5 caption: Analysis of the advertisement clicking data. Shown are differential
    dependence patterns between PC (red lines) and mobile (black lines) identified
    by our inference procedure. From left to right are advertisements, publishers,
    days of weeks.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.92
  Name of the first author: Xiang Lyu
  Name of the last author: Guang Cheng
  Number of Figures: 5
  Number of Tables: 3
  Number of authors: 6
  Paper title: 'Tensor Graphical Model: Non-Convex Optimization and Statistical Inference'
  Publication Date: 2019-03-26 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Empirical FDP FDP , Its Theoretical Limit \u03C4 \u03C4,\
      \ and Power Power (All in % %) of Our Inference Procedure under FDP Control\
      \ for the Kronecker Product of Precision Matrices in Scenario s1, s2, and s3"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Model Selection Performance Comparison Among Glasso, P-MLE,
      and Our FDP Control Procedure
  Table 3 caption:
    table_text: TABLE 3 Brain Regions of Potentially Differential Connectivity Pattern
      Identified by Our Inference Procedure
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2907679
- Affiliation of the first author: department of computer science, rutgers university,
    new brunswick, usa
  Affiliation of the last author: department of computer science, rutgers university,
    new brunswick, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Towards_Efficient_UNets_A_Coupled_and_Quantized_Approach\figure_1.jpg
  Figure 1 caption: "Illustration of dense U-Net, stacked U-Nets and coupled U-Nets.\
    \ The CU-Net is a hybrid of dense U-Net and stacked U-Nets, integrating the merits\
    \ of both dense connectivity and multi-stage top-down and bottom-up refinement.\
    \ The coupled U-Nets can save \u223C70% parameters and \u223C30% inference time\
    \ of stacked U-Nets. Each block in the coupled U-Nets is a bottleneck module which\
    \ is different from the dense block."
  Figure 10 Link: articels_figures_by_rev_year\2019\Towards_Efficient_UNets_A_Coupled_and_Quantized_Approach\figure_10.jpg
  Figure 10 caption: Naive implementation versus memory-efficient implementation.
    The order -1 coupling, batch size 16 and a 12 GB GPU are used. The naive implementation
    can only support 9 U-Nets at most. In contrast, the memory-efficient implementation
    allows training 16 U-Nets, which nearly doubles the depth of CU-Net.
  Figure 2 Link: articels_figures_by_rev_year\2019\Towards_Efficient_UNets_A_Coupled_and_Quantized_Approach\figure_2.jpg
  Figure 2 caption: Illustration of single residual U-Net and dense U-Net. Each top-down
    or bottom-up block in the residual U-Net is a residual block of several residual
    modules. In contrast, each block in the dense U-Net is a dense block with several
    densely connected layers. The dense connections promote the feature reuse inside
    each block. Therefore, we could largely reduce the parameters of single U-Net
    by replacing each residual block with a dense block.
  Figure 3 Link: articels_figures_by_rev_year\2019\Towards_Efficient_UNets_A_Coupled_and_Quantized_Approach\figure_3.jpg
  Figure 3 caption: Illustration of order - K coupling. For simplicity, each dot represents
    one U-Net. The red lines are shortcut connections for the same semantic blocks
    in different U-Nets. The initial input and the U-Net outputs pass through the
    blue lines. Order -0 coupling (Top) strings U-Nets together only by their inputs
    and outputs, i.e., stacked U-Nets. Order -1 coupling (Middle) has shortcut connections
    only for adjacent U-Nets. Similarly, order -2 coupling (Bottom) has shortcut connections
    for 3 nearby U-Nets.
  Figure 4 Link: articels_figures_by_rev_year\2019\Towards_Efficient_UNets_A_Coupled_and_Quantized_Approach\figure_4.jpg
  Figure 4 caption: Implementation of CU-Net. The left figure shows 2 U-Nets coupled
    together through the red dot lines. Each U-Net has its own supervision. For simplicity,
    we only show a pair of top-down and bottom-up semantic blocks. The right figure
    gives the detailed implementation of a pair of semantic blocks in the second U-Net.
    Basically, m features from the former block of current U-Net and n features from
    the same semantic block of the preceding U-Net are concatenated and transformed
    to 4n features by a conv1x1. A conv3x3 then generates n new features and another
    conv1x1 compresses the m+n input and n generated features to m features. The bottom-up
    block needs to concatenate the additional m skipped features.
  Figure 5 Link: articels_figures_by_rev_year\2019\Towards_Efficient_UNets_A_Coupled_and_Quantized_Approach\figure_5.jpg
  Figure 5 caption: Illustration of iterative refinement. The same order - K CU-Net
    is used twice in the iterative refinement. In the first iteration, the input of
    the last U-Net is generated on the basis of the initial input. Then they are concatenated
    and further aggregated in a dense block. The updated input is forwarded through
    the order - K CU-Net to get the final output. Given a long CU-Net cascade, the
    iterative refinement has the potential to reduce its depth by half and still maintain
    comparable accuracy.
  Figure 6 Link: articels_figures_by_rev_year\2019\Towards_Efficient_UNets_A_Coupled_and_Quantized_Approach\figure_6.jpg
  Figure 6 caption: Illustration of memory efficient implementation. It is for the
    Concat-BN-ReLU-Conv( 1times 1 ) in each bottleneck structure. ReLU is not shown
    since it is an in-place operation with no additional memory request. The efficient
    implementation pre-allocates two fixed memory space to store the concatenated
    and normalized features of connected blocks. In contrast, the naive implementation
    always allocates new memories for them, causing high memory consumption.
  Figure 7 Link: articels_figures_by_rev_year\2019\Towards_Efficient_UNets_A_Coupled_and_Quantized_Approach\figure_7.jpg
  Figure 7 caption: Validation PCKh curves of 2 coupled U-Nets(CU-Net-2) under different
    hyper-parameters m and n . The converged curve reaches higher for larger m and
    n . But the gap between adjacent curves becomes smaller. m=128 and n=32 is a good
    trade-off of accuracy and efficiency. Besides, Larger m and n also make the curve
    smoother.
  Figure 8 Link: articels_figures_by_rev_year\2019\Towards_Efficient_UNets_A_Coupled_and_Quantized_Approach\figure_8.jpg
  Figure 8 caption: Validation PCKh curves of single dense U-Net and 8 coupled U-Nets
    (CU-Net-8) with 1 and 4 supervisions. They have equivalent amounts of parameters.
    The CU-Net-8 get higher converged PCKh than the single dense U-Net. The additional
    intermediate supervisions bring more PCKh gains. But its curve fluctuates more
    before the convergence.
  Figure 9 Link: articels_figures_by_rev_year\2019\Towards_Efficient_UNets_A_Coupled_and_Quantized_Approach\figure_9.jpg
  Figure 9 caption: Relation of PCKh(%), parameters and order - K coupling on MPII
    validation set. The parameter number of CU-Net grows approximately linearly with
    the order of coupling. However, the PCKh first increases and then decreases. A
    small order 1 or 2 would be a good balance for prediction accuracy and parameter
    efficiency.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Zhiqiang Tang
  Name of the last author: Dimitris N. Metaxas
  Number of Figures: 11
  Number of Tables: 14
  Number of authors: 4
  Paper title: 'Towards Efficient U-Nets: A Coupled and Quantized Approach'
  Publication Date: 2019-03-26 00:00:00
  Table 1 caption:
    table_text: TABLE 1 PCKhs of the CU-Net with Varied Intermediate Supervisions
      on the MPII Validation Set
  Table 10 caption:
    table_text: TABLE 10 Detailed PCKh Comparison of Different Quantization Configurations
      for order order-1 CU-Net-2 on MPII Validation Sets
  Table 2 caption:
    table_text: TABLE 2 Comparison of Different Hyper-Parameters m m and n n Measured
      by the Parameter Number and the PCKh on the MPII Validation Set
  Table 3 caption:
    table_text: 'TABLE 3 CU-Net versus Single Dense U-Net on MPII Validation Set Measured
      by PCKh (%) and Parameter '
  Table 4 caption:
    table_text: TABLE 4 Order Order-1 CU-Net-8 versus order order-7 CU-Net-8, Measured
      by Training and Validation PCKhs (%) on MPII
  Table 5 caption:
    table_text: TABLE 5 Order Order-1 CU-Net versus Stacked Residual U-Nets on MPII
      Validation Set Measured by PCKh (%), Parameter Number, and Inference Time
  Table 6 caption:
    table_text: TABLE 6 NME (%) on 300-W Using order order-1 CU-Net-4 with Iterative
      Refinement, Detection, and Regression Supervisions
  Table 7 caption:
    table_text: TABLE 7 Iterative order order-1 CU-Net-4 versus Non-Iterative order
      order-1 CU-Net-8 on 300-W Measured by NME (%)
  Table 8 caption:
    table_text: TABLE 8 Performance of Different Combinations of Bit-Width Values
      on the 300-W Dataset Measured by NME (%)
  Table 9 caption:
    table_text: TABLE 9 Performance of Different Quantization Configurations for order
      order-1 CU-Net-2 on the MPII Validation Dataset Measured by PCKh (%), Training
      Memory, Model Size, and Balance Index
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2907634
- Affiliation of the first author: institute for computer graphics and vision, graz
    university of technology, graz, austria
  Affiliation of the last author: institute for computer graphics and vision, graz
    university of technology, graz, austria
  Figure 1 Link: articels_figures_by_rev_year\2019\Generalized_Feedback_Loop_for_Joint_HandObject_Pose_Estimation\figure_1.jpg
  Figure 1 caption: "Overview of our approach for hand pose estimation. First, we\
    \ localize the hand in the camera frame \u24EA and crop a patch around the hand\
    \ location. Then, we use a first CNN \u2460 to predict an initial estimate of\
    \ the 3D pose given an input depth image of the hand. The pose is used to synthesize\
    \ a depth image \u2461, which is used together with the input depth image to derive\
    \ a pose update \u2462. The update is applied to the pose and the process is iterated.\
    \ In this work, we follow this general approach and further show how to extend\
    \ it to joint hand-object pose estimation."
  Figure 10 Link: articels_figures_by_rev_year\2019\Generalized_Feedback_Loop_for_Joint_HandObject_Pose_Estimation\figure_10.jpg
  Figure 10 caption: 'Training samples used for joint hand-object pose estimation.
    Top row: Semi-synthetic training data comprising real depth images of hands with
    rendered objects. The interaction between hand and object cannot be accurately
    modeled easily, but the solution space of possible object locations and possible
    object poses can be significantly reduced, which enables training our proposed
    method. Bottom row: Additional synthetic depth data is used to accurately model
    the hand-object interaction, but it does not capture the sensor characteristics.'
  Figure 2 Link: articels_figures_by_rev_year\2019\Generalized_Feedback_Loop_for_Joint_HandObject_Pose_Estimation\figure_2.jpg
  Figure 2 caption: 'Samples generated by the synthesizer CNN for different poses
    from the test set. Top: Ground truth depth image. Middle: Synthesized depth image
    using our learned hand model. Bottom: Color-coded, pixel-wise difference between
    the depth images. Red represents large errors, blue represents small errors. The
    synthesizer CNN is able to render convincing depth images for a very large range
    of poses. The largest errors are located near the occluding contours of the hand,
    which are noisy in the ground truth images.'
  Figure 3 Link: articels_figures_by_rev_year\2019\Generalized_Feedback_Loop_for_Joint_HandObject_Pose_Estimation\figure_3.jpg
  Figure 3 caption: Synthesized images for physically impossible poses. Note the colors
    that indicate different fingers. (a) Shows a feasible pose with its synthesized
    image. (b) Shows the synthesized image for the same pose after swapping the ring
    and middle finger positions. In (c) the ring and middle finger are flipped downwards,
    and in (d) the wrist joints are flipped upwards.
  Figure 4 Link: articels_figures_by_rev_year\2019\Generalized_Feedback_Loop_for_Joint_HandObject_Pose_Estimation\figure_4.jpg
  Figure 4 caption: Network architecture of the localizer CNN. All layers have rectified-linear
    units, except the last layer which has linear units. C denotes a convolutional
    layer with the number of filters and the filter size inscribed, FC a fully connected
    layer with the number of neurons, and P a max-pooling layer with the window size.
    The initial hand crop from the depth image is fed to the network that predicts
    the location of the MCP of the hand to correct an inaccurate hand localization.
  Figure 5 Link: articels_figures_by_rev_year\2019\Generalized_Feedback_Loop_for_Joint_HandObject_Pose_Estimation\figure_5.jpg
  Figure 5 caption: Network architecture of the predictor CNN used for initial pose
    prediction. C denotes a convolutional layer with the number of filters and the
    filter size inscribed, FC a fully-connected layer with the number of neurons,
    D a Dropout layer [65] with the probability of dropping a neuron, R a residual
    module [66] with the number of filters and filter size, and P a max-pooling layer
    with the window size. The cropped depth image is fed to the ResNet that predicts
    the 3D hand pose. The last layer is a bottleneck layer with 30 neurons that incorporates
    the pose prior. [11].
  Figure 6 Link: articels_figures_by_rev_year\2019\Generalized_Feedback_Loop_for_Joint_HandObject_Pose_Estimation\figure_6.jpg
  Figure 6 caption: Network architecture of the synthesizer CNN used to generate depth
    images of hands given their poses. The input of the network is the hand pose.
    The fully connected hidden layers create a 2,048 dimensional latent representation
    at sf F sf C4 which is reshaped into 32 feature maps of size 8times 8 . The feature
    maps are gradually enlarged by successive unpooling and convolution operations.
    The last convolution layer combines the feature maps to derive a single depth
    image of size 128 times 128 . All layers have rectified-linear units, except the
    last layer which has tanh units. sf C denotes a convolutional layer with the number
    of filters and the filter size inscribed, sf F sf C a fully connected layer with
    the number of neurons, and sf U sf P an unpooling layer with the upscaling factor.
  Figure 7 Link: articels_figures_by_rev_year\2019\Generalized_Feedback_Loop_for_Joint_HandObject_Pose_Estimation\figure_7.jpg
  Figure 7 caption: Network architecture of the updater CNN. The network contains
    several convolutional layers that use a filter stride to reduce the size of the
    feature maps. The final feature maps are fed into a fully connected network. All
    layers have rectified-linear units, except the last layer which has linear units.
    The pose update is used to refine the initial pose and the refined pose is again
    fed into the synthesizer CNN to iterate the whole procedure. As in Fig. 6, sf
    C denotes a convolutional layer, and mathsfFC a fully connected layer.
  Figure 8 Link: articels_figures_by_rev_year\2019\Generalized_Feedback_Loop_for_Joint_HandObject_Pose_Estimation\figure_8.jpg
  Figure 8 caption: Our iterative pose optimization in high-dimensional space, schematized
    here in 2D. We start at an initial pose ( boldsymbol times ) and want to converge
    to the ground truth pose ( boldsymbol circ ), that maximizes image similarity.
    Our updater CNN generates updates for each pose ( boldsymbol + ) that bring us
    closer. The updates are predicted from the synthesized image of the current pose
    estimate and the observed depth image.
  Figure 9 Link: articels_figures_by_rev_year\2019\Generalized_Feedback_Loop_for_Joint_HandObject_Pose_Estimation\figure_9.jpg
  Figure 9 caption: Overview of our joint hand-object pose estimation method. We show
    the predictors for the initial poses with one iteration of the updaters. The input
    to our method is a crop from the depth camera frame that contains the hand with
    the object estimated from the center-of-mass in the depth camera frame. From this
    input, we localize the hand and the object separately, by predicting the 2D locations
    and depth using the localizer CNNs. We apply a Spatial Transformer Network (STN)
    to crop a region of interest around the predicted location. On this centered crop,
    we predict an initial pose using the predictor CNNs, which are then used to synthesize
    depth images of the hand and the object using the synthesizer CNNs. Using an Inverse
    STN (ISTN), we paste back the different inputs, i.e., the synthesized object and
    the synthesized hand, onto the input image. These images are stacked together
    ( otimes ) and serve as input to the updater CNNs that predict one update for
    the pose of the hand and one for the object. The updates are added to the poses
    and the procedure is iterated several times.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Markus Oberweger
  Name of the last author: Vincent Lepetit
  Number of Figures: 16
  Number of Tables: 2
  Number of authors: 3
  Paper title: Generalized Feedback Loop for Joint Hand-Object Pose Estimation
  Publication Date: 2019-03-27 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Evaluation on the NYU Dataset [16]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Results on the DexterHO Dataset [30]
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2907951
- Affiliation of the first author: department of computer science, drexel university,
    philadelphia, usa
  Affiliation of the last author: department of computer science, drexel university,
    philadelphia, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Recognizing_Material_Properties_from_Images\figure_1.jpg
  Figure 1 caption: "Materials are unique in that they have a many characteristic\
    \ locally-recognizable visual attributes. Wool, for example, often appears \u201C\
    fuzzy\u201D, and metals are typically \u201Cshiny\u201D. In this work we show\
    \ that material attributes can be recognized from purely local information, that\
    \ we can discover these visual attributes automatically (again from purely local\
    \ information), and that such a process can be applied to large-scale material\
    \ image datasets via an end-to-end formulation in a Convolutional Neural Network."
  Figure 10 Link: articels_figures_by_rev_year\2019\Recognizing_Material_Properties_from_Images\figure_10.jpg
  Figure 10 caption: 'Material Attribute-Category CNN (MAC-CNN) Architecture: We introduce
    auxiliary fully-connected attribute layers to each spatial pooling layer, and
    combine the per-layer predictions into a final attribute output via an additional
    set of weights. The loss functions attached to the attribute layers encourage
    the extraction of attributes that match the human material representation encoded
    in perceptual distances. The first set of attribute layers acts as a set of weak
    learners to extract attributes wherever they are present. The final layer combines
    them to form a single prediction.'
  Figure 2 Link: articels_figures_by_rev_year\2019\Recognizing_Material_Properties_from_Images\figure_2.jpg
  Figure 2 caption: "Per-pixel material trait predictions on unseen test images. Each\
    \ RGB color channel corresponds to the predicted probability of a single selected\
    \ material trait. The recognized traits clearly divide the images into regions\
    \ of similar material appearance. Of particular importance is the fact that we\
    \ can successfully recognize material properties like \u201Cfuzzy\u201D on the\
    \ left, despite the fact that our training data did not include any animals. The\
    \ shape of each statue on the right is that of a person, but we see from the recognized\
    \ material traits that the material is in fact metal."
  Figure 3 Link: articels_figures_by_rev_year\2019\Recognizing_Material_Properties_from_Images\figure_3.jpg
  Figure 3 caption: Materials such as the plastic in these images exhibit a wide range
    of appearances depending on the object and scene. Despite this, we can intuitively
    recognize visual attributes (smooth and translucent, for example) shared across
    different instances of the material. In our preliminary work we show that these
    attributes, which we refer to as visual material traits, are locally-recognizable
    and can be used to recognize material categories like plastic or metal.
  Figure 4 Link: articels_figures_by_rev_year\2019\Recognizing_Material_Properties_from_Images\figure_4.jpg
  Figure 4 caption: "Sample material image patches. Asking annotators to merely \u201C\
    describe\u201D the patches is an ambiguous question. Patches may look similar\
    \ even though the annotator does not have a concrete word to define the similarity.\
    \ We instead ask only for binary visual similarity decisions."
  Figure 5 Link: articels_figures_by_rev_year\2019\Recognizing_Material_Properties_from_Images\figure_5.jpg
  Figure 5 caption: "Projection of materials into a 2D similarity subspace allow us\
    \ evaluate our collected annotations. Coordinates indicate perceived similarity\
    \ to water and leather respectively. The locations of the two material categories\
    \ corresponding to the axes are marked. We would expect that, in this case, water\
    \ would lie furthest along the \u201Cwater\u201D axis and likewise with leather.\
    \ We can also see that water occasionally looks like leather. Materials with common\
    \ visual properties, such as the smoothness of plastic and glass, tend to lie\
    \ close to each other. Finally, no material appears strongly similar to both leather\
    \ and water. This is expected as the two materials do generally exhibit different\
    \ appearances."
  Figure 6 Link: articels_figures_by_rev_year\2019\Recognizing_Material_Properties_from_Images\figure_6.jpg
  Figure 6 caption: t-SNE [34] embedding of materials from the raw feature space (a)
    and from our discovered attributes (b). We embed a set of material image patches
    from the FMD [15] into 2D space via t-SNE using raw features and predicted attribute
    probabilities as the input space for the embeddings. Though t-SNE has been shown
    to perform well in high-dimensional input spaces, it fails to separate material
    categories from the raw feature space. Material categories are, however, clearly
    more separable with our attribute space.
  Figure 7 Link: articels_figures_by_rev_year\2019\Recognizing_Material_Properties_from_Images\figure_7.jpg
  Figure 7 caption: Per-pixel discovered attribute probabilities for four attributes
    (one per column). These images show that the discovered attributes exhibit patterns
    similar to those of known material traits. The first attribute, for example, appears
    consistently within the woven hat and the koala; the second attribute tends to
    indicate smooth regions. The third attribute shows we are discovering attributes
    that can appear both sparsely and densely in an image, depending on the context.
    These are all properties shared with visual material traits. Attributes from a
    random A do not exhibit any of these properties.
  Figure 8 Link: articels_figures_by_rev_year\2019\Recognizing_Material_Properties_from_Images\figure_8.jpg
  Figure 8 caption: Correlation between discovered attribute predictions and material
    traits. Groups of attributes can collectively indicate the presence of a material
    trait. Metallic, for example, correlates positively with attribute 0 and negatively
    with attribute 8.
  Figure 9 Link: articels_figures_by_rev_year\2019\Recognizing_Material_Properties_from_Images\figure_9.jpg
  Figure 9 caption: Accuracy versus training set size. The line is the average of
    3 random splits. Error bars indicate the minimum and maximum of the splits. Accuracy
    does not continue to increase as we use larger training datasets. This shows that
    we have successfully extracted as much local information as possible from human
    perception.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.8
  Name of the first author: Gabriel Schwartz
  Name of the last author: Ko Nishino
  Number of Figures: 17
  Number of Tables: 0
  Number of authors: 2
  Paper title: Recognizing Material Properties from Images
  Publication Date: 2019-03-27 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2907850
- Affiliation of the first author: school of computer science and information engineering,
    hefei university of technology, hefei, china
  Affiliation of the last author: school of information technology and mathematical
    sciences, university of south australia, adelaide, australia
  Figure 1 Link: articels_figures_by_rev_year\2019\MultiSource_Causal_Feature_Selection\figure_1.jpg
  Figure 1 caption: "Example of BN and interventions. (a) A simple BN representing\
    \ dependencies among five variables; (b) An example of an intervention on variable\
    \ \u201Csprinkler\u201D."
  Figure 10 Link: articels_figures_by_rev_year\2019\MultiSource_Causal_Feature_Selection\figure_10.jpg
  Figure 10 caption: AUC of NB using the features selected by MCFS and its rivals
    in Experiment 3.
  Figure 2 Link: articels_figures_by_rev_year\2019\MultiSource_Causal_Feature_Selection\figure_2.jpg
  Figure 2 caption: The ALARM Bayesian network.
  Figure 3 Link: articels_figures_by_rev_year\2019\MultiSource_Causal_Feature_Selection\figure_3.jpg
  Figure 3 caption: A local causal structure around education learned from the original
    educational attainment dataset.
  Figure 4 Link: articels_figures_by_rev_year\2019\MultiSource_Causal_Feature_Selection\figure_4.jpg
  Figure 4 caption: AUC of NB using the features selected by MCFS and its rivals in
    Experiment 1.
  Figure 5 Link: articels_figures_by_rev_year\2019\MultiSource_Causal_Feature_Selection\figure_5.jpg
  Figure 5 caption: AUC of KNN using the features selected by MCFS and its rivals
    in Experiment 1.
  Figure 6 Link: articels_figures_by_rev_year\2019\MultiSource_Causal_Feature_Selection\figure_6.jpg
  Figure 6 caption: AUC of SVM using the features selected by MCFS and its rivals
    in Experiment 1.
  Figure 7 Link: articels_figures_by_rev_year\2019\MultiSource_Causal_Feature_Selection\figure_7.jpg
  Figure 7 caption: AUC of NB using the features selected by MCFS and its rivals in
    Experiment 2.
  Figure 8 Link: articels_figures_by_rev_year\2019\MultiSource_Causal_Feature_Selection\figure_8.jpg
  Figure 8 caption: AUC of KNN using the features selected by MCFS and its rivals
    in Experiment 2.
  Figure 9 Link: articels_figures_by_rev_year\2019\MultiSource_Causal_Feature_Selection\figure_9.jpg
  Figure 9 caption: AUC of SVM using the features selected by MCFS and its rivals
    in Experiment 2.
  First author gender probability: 0.53
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.79
  Name of the first author: Kui Yu
  Name of the last author: Thuc Duy Le
  Number of Figures: 12
  Number of Tables: 18
  Number of authors: 5
  Paper title: Multi-Source Causal Feature Selection
  Publication Date: 2019-03-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of Compared Methods in Our Experiments
  Table 10 caption:
    table_text: "TABLE 10 Running Time (in Seconds) and Number of Selected Features\
      \ on \u201CVTUB\u201D"
  Table 2 caption:
    table_text: TABLE 2 Synthetic Datasets Used in the Experiments
  Table 3 caption:
    table_text: "TABLE 3 Prediction Accuracy of MCFS Against Its Rivals When \u201C\
      HR\u201D Is the Target (In the Table, (X) Denotes that an Algorithm Succeeded\
      \ by Returning a Non-Empty Feature Set for X Times Out of the Full 5 Runs)"
  Table 4 caption:
    table_text: "TABLE 4 Prediction Accuracy of MCFS Against the Intersections of\
      \ Features Selected by Its Rivals When \u201CHR\u201D Is the Target (In the\
      \ Table, (X) Denotes that an Algorithm Succeeded by Returning a Non-Empty Feature\
      \ Set for X Times Out of the Full 5 Runs)"
  Table 5 caption:
    table_text: "TABLE 5 Prediction Accuracy of MCFS Against Unions of Features Selected\
      \ Its Rivals on \u201CHR\u201D"
  Table 6 caption:
    table_text: "TABLE 6 Running Time (in Seconds) and Number of Selected Features\
      \ on \u201CHR\u201D"
  Table 7 caption:
    table_text: "TABLE 7 Prediction Accuracy of MCFS Against Its Rivals on \u201C\
      VTUB\u201D (In the Table, (X) Denotes that an Algorithm Succeeded by Returning\
      \ a Non-Empty Feature Set for X Times Out of the Full 5 Runs)"
  Table 8 caption:
    table_text: "TABLE 8 Prediction Accuracy of MCFS Against Intersections of Features\
      \ Selected Its Rivals on \u201CVTUB\u201D (In the Table, (X) Denotes that an\
      \ Algorithm Succeeded by Returning a Non-Empty Feature Set for X Times Out of\
      \ the Full 5 Runs)"
  Table 9 caption:
    table_text: "TABLE 9 Prediction Accuracy of MCFS Against Unions of Features Selected\
      \ Its Rivals on \u201CVTUB\u201D"
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2908373
- Affiliation of the first author: college of information sciences and technology,
    the pennsylvania state university, state college, usa
  Affiliation of the last author: department of statistics, the pennsylvania state
    university, state college, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Aggregated_Wasserstein_Distance_and_State_Registration_for_Hidden_Markov_Models\figure_1.jpg
  Figure 1 caption: "(a) Experiment scheme for varying \u03BC and varying \u03A3 .\
    \ A re-estimated \u03D5 \u02C6 0 is denoted as the dashed blue line. (b) (c) Mean\
    \ estimates of W 2 ( \u03D5 \u02C6 0 , \u03D5 i ) (blue) and KL( \u03D5 \u02C6\
    \ 0 , \u03D5 i ) (orange) and their 3\u03C3 confidence intervals w.r.t different\
    \ Gaussian \u03D5 i . (b) is for varying \u03BC , and (c) is for varying \u03A3\
    \ ."
  Figure 10 Link: articels_figures_by_rev_year\2019\Aggregated_Wasserstein_Distance_and_State_Registration_for_Hidden_Markov_Models\figure_10.jpg
  Figure 10 caption: Testing accuracies with respect to the iteration number in Adaboost
    (number of weak classifiers selected). (a) Motion classification by Adaboost on
    6 joints. (b) Motion classification by Adaboost on 27 joints. The iteration number
    means the number of features incrementally acquired in Adaboost.
  Figure 2 Link: articels_figures_by_rev_year\2019\Aggregated_Wasserstein_Distance_and_State_Registration_for_Hidden_Markov_Models\figure_2.jpg
  Figure 2 caption: "A simple registration example about how T 2 in \u039B 2 is registered\
    \ towards \u039B 1 such that it can be compared with T 1 in \u039B 1 . For this\
    \ example, W encodes a \u201Chard matching\u201D between states in \u039B 1 and\
    \ \u039B 2 ."
  Figure 3 Link: articels_figures_by_rev_year\2019\Aggregated_Wasserstein_Distance_and_State_Registration_for_Hidden_Markov_Models\figure_3.jpg
  Figure 3 caption: "Top: Intuition for w i,j solved in MAW. w i,j represents a proportion\
    \ of \u03D5 1,i (more precisely, w i,j \u03D5 1,i of \u03C0 1,i \u03D5 1,i ) is\
    \ transported to \u03D5 2,j ( w i,j \u03D5 2,j of \u03C0 2,i \u03D5 2,j ). Bottom:\
    \ Intuition for w \u2217 i,j solved in IAW. Imagine the blue dots under the slash\
    \ lined area of \u03D5 1,i are matched to red dots under the slash lined area\
    \ of \u03D5 2,j based on the point-wise transport plan \u03A0 n (see Eq. (22)).\
    \ Suppose these dots are sampled from the two mixture distributions respectively.\
    \ Let w \u2217 i,j quantify the proportion of \u03D5 1,i that is transported to\
    \ \u03D5 2,j . Eq. (21) determines w \u2217 i,j based on the true Wasserstein\
    \ transport plan for the two continuous mixture distributions, which is approximated\
    \ here by the optimal transport plan \u03A0 n between two sets of sampled points."
  Figure 4 Link: articels_figures_by_rev_year\2019\Aggregated_Wasserstein_Distance_and_State_Registration_for_Hidden_Markov_Models\figure_4.jpg
  Figure 4 caption: Precision-recall plot for the study to compare KL, KLvargmm, KLvarhmm,
    MAW and IAWs sensitivity to the perturbation of HMMs parameters. (best viewed
    in color).
  Figure 5 Link: articels_figures_by_rev_year\2019\Aggregated_Wasserstein_Distance_and_State_Registration_for_Hidden_Markov_Models\figure_5.jpg
  Figure 5 caption: NN Retrieval experiment Scale versus AUCs. under (a) the perturbation
    of mu , (b) the perturbation of Sigma and (c) the perturbation of transition matrix.
    (best viewed in color).
  Figure 6 Link: articels_figures_by_rev_year\2019\Aggregated_Wasserstein_Distance_and_State_Registration_for_Hidden_Markov_Models\figure_6.jpg
  Figure 6 caption: Analysis of how the accuracy of the estimated HMMs affects results
    of the NN Retrieval experiment (perturbation of mu ). T = M times d times tau
    , where M = 3 , d = 3 . The case tau = infty means the true parameters of the
    seed HMMs are used (instead of estimated parameters).
  Figure 7 Link: articels_figures_by_rev_year\2019\Aggregated_Wasserstein_Distance_and_State_Registration_for_Hidden_Markov_Models\figure_7.jpg
  Figure 7 caption: 'Visualization of CMU motion capture data. Top: Jump. Bottom:
    Walk.'
  Figure 8 Link: articels_figures_by_rev_year\2019\Aggregated_Wasserstein_Distance_and_State_Registration_for_Hidden_Markov_Models\figure_8.jpg
  Figure 8 caption: Precision recall plot for motion retrieval. The plot for 6 joint-groups,
    i.e., root12 , headneckthorax12 , rbody12 , lbody12 , rleg6 , lleg6 , are displayed
    separately.
  Figure 9 Link: articels_figures_by_rev_year\2019\Aggregated_Wasserstein_Distance_and_State_Registration_for_Hidden_Markov_Models\figure_9.jpg
  Figure 9 caption: Precision recall plot for MAW based motion retrieval under data
    missing for certain dimension(s) setting. The plot for 6 joint-groups, i.e., root12
    , headneckthorax12 , rbody12 , lbody12 , rleg6 , lleg6 , are displayed separately.
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.66
  Name of the first author: Yukun Chen
  Name of the last author: Jia Li
  Number of Figures: 11
  Number of Tables: 3
  Number of authors: 3
  Paper title: Aggregated Wasserstein Distance and State Registration for Hidden Markov
    Models
  Publication Date: 2019-03-31 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of the Parameters Setup for Parameter Perturbation
      Experiments
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Synthetic Data per Distance Computation Time Comparison
  Table 3 caption:
    table_text: TABLE 3 Phoneme K-NN Classification Accuracy Comparison
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2908635
- Affiliation of the first author: "institute for communications engineering, technical\
    \ university of munich, m\xFCnchen, germany"
  Affiliation of the last author: know-center gmbh, graz, austria
  Figure 1 Link: articels_figures_by_rev_year\2019\Learning_Representations_for_Neural_NetworkBased_Classification_Using_the_Inform\figure_1.jpg
  Figure 1 caption: "(a)-(c): The line segment depicts the set [0,1], from which the\
    \ feature RV X takes its values. Red (black) color indicates feature values corresponding\
    \ to class Y=0 ( Y=1 ). (a): The one-dimensional feature variable has a discrete\
    \ distribution with mass points as indicated by the circles. The size of the circles\
    \ is proportional to the probability mass. (b): Training based on a data set D\
    \ . Crosses indicate data points. (c): The one-dimensional feature variable has\
    \ a continuous distribution with support indicated by the thick lines, the probability\
    \ masses on each interval are identical to the probability masses of the points\
    \ in (a). (d): The function f implemented by a DNN with a one hidden layer with\
    \ two neurons, ReLU activation functions, and a single output neuron. The parameters\
    \ leading to this function are \u0398 0 =[1;1],[\u2212a;\u2212a\u2212b] and \u0398\
    \ 1 =[1,\u22121],0 . (e)-(g) show the mutual information I(X;L) as a function\
    \ of the parameter a , for b=0.25 , evaluated on a grid of a ranging from 0 to\
    \ 5 in steps of 0.05. It can be seen that the mutual information is piecewise\
    \ constant. The missing values in (g) indicate that the mutual information is\
    \ infinite at the respective positions."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Learning_Representations_for_Neural_NetworkBased_Classification_Using_the_Inform\figure_2.jpg
  Figure 2 caption: 'Representational simplicity and robustness in binary classification:
    The top figure on the L.H.S. illustrates the two-dimensional input space and the
    support of the input X in R 2 . The rest of the figures on L.H.S. show various
    functions of X 1 (since Y only depends on X 1 ) implementable using a ReLU-based
    DNN. The figures on R.H.S. show the output RVs Y ~ when X is transformed via the
    corresponding functions on the L.H.S. Red (black) color indicates feature values
    corresponding to class Y=0 ( Y=1 ).'
  Figure 3 Link: articels_figures_by_rev_year\2019\Learning_Representations_for_Neural_NetworkBased_Classification_Using_the_Inform\figure_3.jpg
  Figure 3 caption: "Robustness in binary classification. The L.H.S. shows the feature\
    \ space, with X 1 on the horizontal and X 2 on the vertical axis. One can see\
    \ that X is distributed on [0,12]\xD7[0,\u03B5] if Y=0 and on [0,\u03B5]\xD7[12,1]\
    \ if Y=1 , for 0<\u03B5\u226A1 . The R.H.S. shows the supports of the distributions\
    \ P Y ~ |Y=0 and P Y ~ |Y=1 , obtained by two different DNNs with identical IB\
    \ functionals. The blue dot represents a noisy feature or a data point not in\
    \ the training set. See text for details."
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.94
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Rana Ali Amjad
  Name of the last author: Bernhard C. Geiger
  Number of Figures: 3
  Number of Tables: 0
  Number of authors: 2
  Paper title: Learning Representations for Neural Network-Based Classification Using
    the Information Bottleneck Principle
  Publication Date: 2019-04-02 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2909031
- Affiliation of the first author: school of electrical engineering, kaist, daejeon,
    republic of korea
  Affiliation of the last author: graduate school of culture technology, and the school
    of eletrical engineering, kaist, daejeon, republic of korea
  Figure 1 Link: articels_figures_by_rev_year\2019\Globally_Optimal_Inlier_Set_Maximization_for_Atlanta_World_Understanding\figure_1.jpg
  Figure 1 caption: Atlanta frame estimation from RGB-D data (surface normals). (a)
    Examples of 3D point clouds from the NYUv2 RGB-D dataset [14], where the red and
    blue boxes show the reference color image and a top-down view of the 3D point
    clouds, respectively. (b) The Manhattan frames on the input normal distributions
    and the corresponding segmentations, where the estimated solutions do not cover
    the scenes well [15], [16]. On the other hand, (c) the Atlanta frames estimated
    by our approach represent nearly the entire scenes.
  Figure 10 Link: articels_figures_by_rev_year\2019\Globally_Optimal_Inlier_Set_Maximization_for_Atlanta_World_Understanding\figure_10.jpg
  Figure 10 caption: 'Illustration of two RANSAC approaches designed for three Atlanta
    directions: (a) 4-line RANSAC, and (b) 2-normal RANSAC.'
  Figure 2 Link: articels_figures_by_rev_year\2019\Globally_Optimal_Inlier_Set_Maximization_for_Atlanta_World_Understanding\figure_2.jpg
  Figure 2 caption: "Parametrization of Atlanta frame V . Left: Vertical direction\
    \ v v and the first horizontal direction v h 1 are defined by rotation R . Right:\
    \ Each additional horizontal direction v h m (m\u22652) can be obtained by rotating\
    \ v h 1 around v v by the angle \u03B1 m ."
  Figure 3 Link: articels_figures_by_rev_year\2019\Globally_Optimal_Inlier_Set_Maximization_for_Atlanta_World_Understanding\figure_3.jpg
  Figure 3 caption: 'Example of an efficient measurement space (one of NYUv2 RGB-D
    data): (a) EGI on the 3D sphere. (b) 2D-EGI. (c, d) 2D integral image and row-wise
    1D integral image of the 2D-EGI, respectively.'
  Figure 4 Link: articels_figures_by_rev_year\2019\Globally_Optimal_Inlier_Set_Maximization_for_Atlanta_World_Understanding\figure_4.jpg
  Figure 4 caption: Illustration of inlier region concepts. (a) Toy example of the
    slice inlier region, which tightly circumscribes the original inlier region in
    the 2D domain (black dashed circle) using a set of bar-shaped rectangles. Each
    color indicates each bar-shaped rectangle. (b, e) show inlier regions on the sphere
    for the normal case (b) and extreme case including pole (e). These corresponding
    inlier regions defined by rectangular [25] and the proposed slice inlier regions
    in the EGI domain are visualized in (c, f) and (d, g), respectively. As it approaches
    the pole region, the rectangular inlier region loosens the relaxation gap.
  Figure 5 Link: articels_figures_by_rev_year\2019\Globally_Optimal_Inlier_Set_Maximization_for_Atlanta_World_Understanding\figure_5.jpg
  Figure 5 caption: "Two \u201Cedge case\u201D AF models: (a) infinite AF V \u221E\
    \ and (b) minimum AF V 1 . Both AFs are defined by only a rotation matrix."
  Figure 6 Link: articels_figures_by_rev_year\2019\Globally_Optimal_Inlier_Set_Maximization_for_Atlanta_World_Understanding\figure_6.jpg
  Figure 6 caption: Toy example of the bound computation scheme for a band shape inlier
    region. To determine the inlier cardinality of the band shape inlier region (yellow
    region), we simply subtract the cardinality values of the two circular inlier
    regions (green regions) using the efficient bound computation in Section 6.3 from
    the total number of surface normals.
  Figure 7 Link: articels_figures_by_rev_year\2019\Globally_Optimal_Inlier_Set_Maximization_for_Atlanta_World_Understanding\figure_7.jpg
  Figure 7 caption: Illustration of the horizon bins mathbf h and automatic selection
    of M by BIC of the Gaussian mixture model. Each color in mathbf h describes the
    inlier cardinality. The log-likelihood can be simply computed based on the weighted
    sum with the Gaussian kernels and mathbf h .
  Figure 8 Link: articels_figures_by_rev_year\2019\Globally_Optimal_Inlier_Set_Maximization_for_Atlanta_World_Understanding\figure_8.jpg
  Figure 8 caption: 'Representative results obtained for synthetic data on lines (top
    row) and surface normals (bottom row). (a, e): Convergence of the lower and upper
    bounds of the number of inliers. (b, f): Distribution of the number of inliers
    detected by RANSAC (4-line and 2-normal RANSAC) on the same problem instance,
    see main text for more details. (c, g): Convergence of the volume of the BnB search
    space. (d, h): Evolution of the number of BnB cubes.'
  Figure 9 Link: articels_figures_by_rev_year\2019\Globally_Optimal_Inlier_Set_Maximization_for_Atlanta_World_Understanding\figure_9.jpg
  Figure 9 caption: Execution time (log scale) of the original problem, rectangular
    bound and slice bound, w.r.t. (a) the number of surface normals and (b) the number
    of Atlanta directions.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Kyungdon Joo
  Name of the last author: Jean-Charles Bazin
  Number of Figures: 14
  Number of Tables: 0
  Number of authors: 4
  Paper title: Globally Optimal Inlier Set Maximization for Atlanta World Understanding
  Publication Date: 2019-04-09 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2909863
- Affiliation of the first author: school of information science and technology, university
    of science and technology of china, hefei, china
  Affiliation of the last author: school of information science and technology, university
    of science and technology of china, hefei, china
  Figure 1 Link: articels_figures_by_rev_year\2019\ContextAware_Visual_Policy_Network_for_FineGrained_Image_Captioning\figure_1.jpg
  Figure 1 caption: "The evolution of the encoder-decoder framework for image captioning.\
    \ LP: language policy. VP: visual policy. v t : visual feature at step t . y t\
    \ : predicted word at step t . y gt t : ground-truth word at step t . (a) The\
    \ traditional framework focuses only on word prediction by exposing the ground-truth\
    \ word y gt t\u22121 as input to step t for language generation. (b) RL-based\
    \ framework focuses on sequence training by directly feeding the predicted word\
    \ y t\u22121 to LP at step t . (c) Our proposed framework explicitly takes historical\
    \ visual actions v i<t as visual context at step t ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\ContextAware_Visual_Policy_Network_for_FineGrained_Image_Captioning\figure_2.jpg
  Figure 2 caption: The intuition of using visual context in fine-grained image captioning.
    The proposed CAVP is the first RL-based image captioning model which incorporates
    visual context into sequential visual reasoning.
  Figure 3 Link: articels_figures_by_rev_year\2019\ContextAware_Visual_Policy_Network_for_FineGrained_Image_Captioning\figure_3.jpg
  Figure 3 caption: 'Overview of the proposed RL-based image sentence captioning framework.
    It consists of the proposed CAVP for visual feature composition and the language
    policy for sentence generation. CAVP contains four sub-policy (SP) networks: Single
    SP, Context SP, Composition SP, and Output SP. t is the current time step and
    y t is the predicted word.'
  Figure 4 Link: articels_figures_by_rev_year\2019\ContextAware_Visual_Policy_Network_for_FineGrained_Image_Captioning\figure_4.jpg
  Figure 4 caption: Overview of the proposed hierarchical CAVP-LP framework for image
    paragraph captioning, consisting of a sentence-level CAVP-LP and a word-level
    CAVP-LP.
  Figure 5 Link: articels_figures_by_rev_year\2019\ContextAware_Visual_Policy_Network_for_FineGrained_Image_Captioning\figure_5.jpg
  Figure 5 caption: The performance comparison of the CAVP model and the Up-Down method.
    All SPICE category scores are improved by CAVP.
  Figure 6 Link: articels_figures_by_rev_year\2019\ContextAware_Visual_Policy_Network_for_FineGrained_Image_Captioning\figure_6.jpg
  Figure 6 caption: Qualitative examples where top matrix shows the output policy
    network action probabilities and the bottom image shows the decision with maximum
    probability for composition features. The red bounding boxes are the context regions
    and the blue bounding boxes are the current regions which concatenated with context
    regions.
  Figure 7 Link: articels_figures_by_rev_year\2019\ContextAware_Visual_Policy_Network_for_FineGrained_Image_Captioning\figure_7.jpg
  Figure 7 caption: For each generated word, we visualized the attended image regions,
    outlining the region with the maximum policy probability in bounding box. The
    red bounding boxes are the visual context representation regions and the blue
    bounding boxes are the regions decided by single policy network.
  Figure 8 Link: articels_figures_by_rev_year\2019\ContextAware_Visual_Policy_Network_for_FineGrained_Image_Captioning\figure_8.jpg
  Figure 8 caption: Examples of image paragraph captioning results of our model. For
    each image, a paragraph description with a variable number of sentences is generated.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.62
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Zheng-Jun Zha
  Name of the last author: Feng Wu
  Number of Figures: 8
  Number of Tables: 7
  Number of authors: 5
  Paper title: Context-Aware Visual Policy Network for Fine-Grained Image Captioning
  Publication Date: 2019-04-09 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Performance Comparisons on MS-COCO \u201CKarpathy\u201D Offline\
      \ Split"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Highest Ranking Published Image Captioning Results on the
      Online MSCOCO Test Server
  Table 3 caption:
    table_text: TABLE 3 Ablation Performance on MS-COCO
  Table 4 caption:
    table_text: TABLE 4 Efficiency Comparison in Terms of Parameter Number, Training
      Time (hour) and Testing Time (msimage)
  Table 5 caption:
    table_text: "TABLE 5 Ablation Performance on the MS-COCO \u201CKarpathy\u201D\
      \ Offline Split with Respect to Various Metrics as the Reward"
  Table 6 caption:
    table_text: TABLE 6 Performance Comparison on Image Paragraph Captioning Task
  Table 7 caption:
    table_text: TABLE 7 Ablation Performance on Image Paragraph Captioning Task
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2909864
