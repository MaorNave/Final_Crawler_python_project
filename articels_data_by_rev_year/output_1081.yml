- Affiliation of the first author: "institute for robotics and cognitive systems,\
    \ university of l\xFCbeck, l\xFCbeck, germany"
  Affiliation of the last author: "institute for robotics and cognitive systems, university\
    \ of l\xFCbeck, l\xFCbeck, germany"
  Figure 1 Link: articels_figures_by_rev_year\2018\Efficient_Registration_of_HighResolution_Feature_Enhanced_Point_Clouds\figure_1.jpg
  Figure 1 caption: Flowchart of the registration framework.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Efficient_Registration_of_HighResolution_Feature_Enhanced_Point_Clouds\figure_2.jpg
  Figure 2 caption: "Evaluation datasets: a) Two point clouds of the Dragon dataset\
    \ scanned from different angles, before the reconstruction. b) Teapot without\
    \ (left) and with (right) Gaussian distributed noise, using a variance of \u03C3\
    \ 2 =3 per point. c) High-resolution point clouds with 327,323 points each, used\
    \ for efficiency analysis. The point clouds are displaced by a randomly generated\
    \ transformation. d.1) Generated coloured hemisphere, from which d.2) two different\
    \ but partially overlapping patches were selected. Afterwards, each selected region\
    \ d.3) and d.4) was sampled separately."
  Figure 3 Link: articels_figures_by_rev_year\2018\Efficient_Registration_of_HighResolution_Feature_Enhanced_Point_Clouds\figure_3.jpg
  Figure 3 caption: "Registration errors with respect to the Gaussian distributed\
    \ noise level, characterised by variance \u03C3 2 ."
  Figure 4 Link: articels_figures_by_rev_year\2018\Efficient_Registration_of_HighResolution_Feature_Enhanced_Point_Clouds\figure_4.jpg
  Figure 4 caption: Results of the efficiency experiments. The top graph shows the
    dependency of the algorithms' runtime with respect to the number of points. The
    bottom graph visualises the registration error with respect to the number of points
    per cloud.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Philipp Jauer
  Name of the last author: Floris Ernst
  Number of Figures: 4
  Number of Tables: 2
  Number of authors: 5
  Paper title: Efficient Registration of High-Resolution Feature Enhanced Point Clouds
  Publication Date: 2018-04-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Results of a Reconstruction Experiment to Evaluate the Accuracy
      and Repeatability of the Registration Algorithms
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Result of the Coloured Hemisphere Patches Matching to Evaluate
      the Influence of Features Enhancements on the Registrations Quality
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2831670
- Affiliation of the first author: department of computer science, university of central
    florida, orlando, fl
  Affiliation of the last author: department of computer science, university of central
    florida, orlando, fl
  Figure 1 Link: articels_figures_by_rev_year\2018\Egocentric_Meets_TopView\figure_1.jpg
  Figure 1 caption: 'Left) A set of 5 egocentric videos. Right) A top-view video capturing
    the scene. The viewers are highlighted using red circles in the top-view video.
    We aim to answer the two following questions: 1) Does this set of egocentric videos
    belong to the viewers visible in the top-view video? 2) Assuming they do, which
    viewer is capturing which egocentric video?.'
  Figure 10 Link: articels_figures_by_rev_year\2018\Egocentric_Meets_TopView\figure_10.jpg
  Figure 10 caption: The cumulative matching curve demonstrates the performance of
    the proposed spectral and matching based optimization methods (red and magenta),
    and compares them with the baseline graph matching method introduced in [1] (cyan)
    in terms of jointly performing the two tasks. It shows that our proposed algorithms
    outperform the baseline by more than 5.03 and 8.6 percent, respectively.
  Figure 2 Link: articels_figures_by_rev_year\2018\Egocentric_Meets_TopView\figure_2.jpg
  Figure 2 caption: The input to our framework is a set of egocentric videos (in this
    case 5 videos), and one top-view video. The goal is to assign the egocentric videos
    to the people recording them. A graph is formed on the set of egocentric videos
    (each node being one of the egocentric videos), and another graph is formed on
    the top-view video (each node representing one of the targets present in the video).
    Using spectral graph matching, a soft assignment is found between the two graphs,
    and using a soft-to-hard assignment, each egocentric video is assigned to one
    of the viewers in the top-view video. This assignment addresses the second question
    in Fig. 1.
  Figure 3 Link: articels_figures_by_rev_year\2018\Egocentric_Meets_TopView\figure_3.jpg
  Figure 3 caption: Adapting our method for evaluating top-view videos. We form a
    graph on the set of egocentric videos and compare this graph to other graphs built
    on different top-view videos. The top-view videos are ranked based on how similar
    their graphs are to the egocentric graph. The performance of this ranking helps
    us answer our first question.
  Figure 4 Link: articels_figures_by_rev_year\2018\Egocentric_Meets_TopView\figure_4.jpg
  Figure 4 caption: Expected FOV for three different viewers in the top-view video
    alongside with their corresponding egocentric frames. The Top-FOV shown for the
    identity highlighted in green has a high overlap with the one highlighted in yellow,
    therefore we expect their egocentric videos (color-coded accordingly) to have
    relatively similar visual content. In contrast, the FOV of the identity highlighted
    in red does not have as much overlap with other FOVs, thus we do not expect their
    egocentric videos to be visually similar.
  Figure 5 Link: articels_figures_by_rev_year\2018\Egocentric_Meets_TopView\figure_5.jpg
  Figure 5 caption: (a) Two different examples of the 2D features extracted from the
    nodes of the graphs for which the values are color-coded. Left column shows the
    2D matrices extracted from the pairwise similarities of the GIST feature descriptors
    U GIST , middle shows the 2D matrices computed by intersection over union of the
    FOV in the top-view camera U IOU , and the rightmost column shows the result of
    the 2D cross correlation between the two. (b) The same concept, but between two
    edges. The leftmost column shows the pairwise similarity between GIST descriptors
    of one egocentric camera to another B GIST . Middle, shows pairwise intersection
    over unions of the FOVs of the pair of viewers B IOU , and the rightmost column
    illustrates their 2D cross correlation. The similarities between B GIST and B
    IOU capture the affinity of two nodesedges in the two graphs.
  Figure 6 Link: articels_figures_by_rev_year\2018\Egocentric_Meets_TopView\figure_6.jpg
  Figure 6 caption: Examples of 1D features capturing the number of humans in different
    frames. Left column shows the summation of the detection scores at every frame
    for their corresponding egocentric videos. Right column shows the number of visible
    people in three different viewer's Top-FOV over time. The x axis encodes the frame
    number in which the number of humans was measured. Both vectors are normalized
    and then compared to each other. The similarity between the two patterns shows
    the discriminative power of this feature especially if the video is long enough.
    However, our experiments show that in most cases where human detection results
    are not that confident or the video length is too short, this feature by itself
    does not results in a high assignment accuracy.
  Figure 7 Link: articels_figures_by_rev_year\2018\Egocentric_Meets_TopView\figure_7.jpg
  Figure 7 caption: Sample frames of top-view and egocentric videos are shown. The
    time-delays of the egocentric videos with respect to the top-view video are also
    shown. Color-coding denotes the viewer identities.
  Figure 8 Link: articels_figures_by_rev_year\2018\Egocentric_Meets_TopView\figure_8.jpg
  Figure 8 caption: The cumulative matching curve demonstrates the performance of
    the proposed spectral and matching based optimization methods (red and magenta)
    and compares them with the baseline graph matching method introduced in [1]. It
    shows that our proposed algorithms outperform the baselines by more than 1.3 and
    8.3 percent, respectively.
  Figure 9 Link: articels_figures_by_rev_year\2018\Egocentric_Meets_TopView\figure_9.jpg
  Figure 9 caption: (a) shows the cumulative matching curve for ranking the viewers
    in the top-view video. The green and blue curves belong to ranking based on the
    cross correlation between the 2D, and cross correlation between the 1D unary scores,
    respectively (Not incorporating pairwise features). The cyan curve is the result
    of the graph matching method (Section 3.3), and the magenta and red curves are
    the results of the two iterative approaches introduced in Section 3.4 with two
    different initializations. The dashed black line shows random ranking accuracy.
    (b) shows the assignment accuracy mean and standard deviations based on random
    assignment, using the number of humans, using unary features, using spectral graph
    matching, and using our two iterative approaches with two different initializations.
    The best performance in both (a) and (b) is achieved by the matching score based
    iterative optimization, when the time-delays are initialized by the median of
    the suggested values.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.94
  Name of the first author: Shervin Ardeshir
  Name of the last author: Ali Borji
  Number of Figures: 14
  Number of Tables: 0
  Number of authors: 2
  Paper title: Egocentric Meets Top-View
  Publication Date: 2018-05-01 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2832121
- Affiliation of the first author: state key lab of cad&cg, zhejiang university, hangzhou,
    zhejiang, china
  Affiliation of the last author: microsoft research, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2018\PhysicallyBased_Simulation_of_Cosmetics_via_Intrinsic_Image_Decomposition_with_F\figure_1.jpg
  Figure 1 caption: An intrinsic image decomposition example. (a) Input image. (b)
    Albedo layer. (c) Diffuse shading layer. (d) Specular highlights layer. (e) Geometry.
    The layers for diffuse shading and specular highlights have been intensity-rescaled
    for better visualization.
  Figure 10 Link: articels_figures_by_rev_year\2018\PhysicallyBased_Simulation_of_Cosmetics_via_Intrinsic_Image_Decomposition_with_F\figure_10.jpg
  Figure 10 caption: Different eye shadows. (a) Without makeup. (bc) Eye shadows with
    spatially-uniform reflectance. (defg) Eye shadows containing glitter. The first
    row contains our simulation results, and the second row shows images of real makeup
    as references.
  Figure 2 Link: articels_figures_by_rev_year\2018\PhysicallyBased_Simulation_of_Cosmetics_via_Intrinsic_Image_Decomposition_with_F\figure_2.jpg
  Figure 2 caption: Light paths through cosmetic and skin layers. (a) For computing
    albedo. (b) For computing specular highlights.
  Figure 3 Link: articels_figures_by_rev_year\2018\PhysicallyBased_Simulation_of_Cosmetics_via_Intrinsic_Image_Decomposition_with_F\figure_3.jpg
  Figure 3 caption: Different foundation thickness. (a) Without makeup. (b) Real image
    with heavy foundation. (c) Synthesized image with heavy foundation. (d) Synthesized
    image with light foundation. The first row shows color images and the second row
    shows the corresponding diffuse shading layers. The third row displays close-up
    views of approximately the same areas of the color images and diffuse shading
    layers. Please zoom into the electronic file for closer inspection.
  Figure 4 Link: articels_figures_by_rev_year\2018\PhysicallyBased_Simulation_of_Cosmetics_via_Intrinsic_Image_Decomposition_with_F\figure_4.jpg
  Figure 4 caption: "Simulation of eye shadow containing glitter. (a) Without makeup.\
    \ (b) Real makeup references. (c) Our simulated result with spatially-varying\
    \ reflectance from glitter. (d) Our simulated result I M with only spatially-uniform\
    \ reflectance parameters \u0398 . The second row shows (b) the extracted glitter\
    \ distribution map R g (P) and (c) the synthesized glitter distribution R g .\
    \ The second row is intensity-rescaled for better visualization."
  Figure 5 Link: articels_figures_by_rev_year\2018\PhysicallyBased_Simulation_of_Cosmetics_via_Intrinsic_Image_Decomposition_with_F\figure_5.jpg
  Figure 5 caption: 'Makeup maps (top row) and corresponding makeup results (bottom
    row) for video input. Leftmost column: in face texture space T . Other columns:
    transformed maps for different video frames.'
  Figure 6 Link: articels_figures_by_rev_year\2018\PhysicallyBased_Simulation_of_Cosmetics_via_Intrinsic_Image_Decomposition_with_F\figure_6.jpg
  Figure 6 caption: Comparisons with previous image-based makeup transfer methods
    [6], [7]. (a) Target images. (b) Reference images with makeup, which are used
    by [6], [7] but not by our method. The reference images without makeup that are
    used in [7] are not shown. (c) Results of [7]. (d) Results of [6]. (e) Results
    of [49], which uses a general intrinsic image decomposition method. (f) Our results
    using the proposed facial intrinsic image decomposition method. (g) Target faces
    actually wearing the cosmetics, which serves as unaligned ground truth. In the
    first row, the target and reference images were captured of the same face under
    different lighting conditions. In the second row, the target and reference images
    are of different faces under the same lighting condition. The yellow boxes highlight
    errors in makeup transfer. Please zoom in to view details.
  Figure 7 Link: articels_figures_by_rev_year\2018\PhysicallyBased_Simulation_of_Cosmetics_via_Intrinsic_Image_Decomposition_with_F\figure_7.jpg
  Figure 7 caption: Results and comparisons under different illumination environments.
    (a) Target images. (b) Results of [7]. (c) Results of [6]. The makeup style references
    used by [6], [7] are inserted in the lower right corners. (d) Our results. (e)
    Real makeup images as ground truth references.
  Figure 8 Link: articels_figures_by_rev_year\2018\PhysicallyBased_Simulation_of_Cosmetics_via_Intrinsic_Image_Decomposition_with_F\figure_8.jpg
  Figure 8 caption: Different foundations. (a) Without makeup. (b) Liquid, radiant.
    (c) Liquid, latte. (d) Liquid, natural. (e) Pressed powder, natural. (f) Loose
    powder, natural. The first row shows images of real makeup as references, and
    the second, third and fourth rows present the simulation results of our method,
    TAAZ and Modiface.
  Figure 9 Link: articels_figures_by_rev_year\2018\PhysicallyBased_Simulation_of_Cosmetics_via_Intrinsic_Image_Decomposition_with_F\figure_9.jpg
  Figure 9 caption: Different lipsticks. (a) Without makeup. (b) Ruby. (c) Fiery.
    (d) Rose. (e) Fascination. The first row shows images of real makeup as references,
    and the second, third and fourth rows present simulation results of our method,
    TAAZ and Modiface. Note that the lip configurations in the references do not necessarily
    match those of the input image.
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Chen Li
  Name of the last author: Stephen Lin
  Number of Figures: 13
  Number of Tables: 2
  Number of authors: 4
  Paper title: Physically-Based Simulation of Cosmetics via Intrinsic Image Decomposition
    with Facial Priors
  Publication Date: 2018-05-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Intrinsic Image Layers Affected by Different Cosmetics
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Perceptual Study Results
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2832059
- Affiliation of the first author: eon reality inc, irvine, ca
  Affiliation of the last author: key lab of intelligent information processing, institute
    of computing technology, chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2018\Image_Projective_Invariants\figure_1.jpg
  Figure 1 caption: Geometric deformations caused by 2D projective transformations.
  Figure 10 Link: articels_figures_by_rev_year\2018\Image_Projective_Invariants\figure_10.jpg
  Figure 10 caption: The images of two celebrated paintings.
  Figure 2 Link: articels_figures_by_rev_year\2018\Image_Projective_Invariants\figure_2.jpg
  Figure 2 caption: This illustration shows the total amount of pixels in the domain
    will change when f(x,y) is transformed into g(u,v) after the projective transformation.
  Figure 3 Link: articels_figures_by_rev_year\2018\Image_Projective_Invariants\figure_3.jpg
  Figure 3 caption: The images which are used to test the stability and discriminability
    of PIs .
  Figure 4 Link: articels_figures_by_rev_year\2018\Image_Projective_Invariants\figure_4.jpg
  Figure 4 caption: "The ARE of PIs - X - N , where X\u2208G,L,W and N\u22085,7,9,\u2026\
    ,15 ."
  Figure 5 Link: articels_figures_by_rev_year\2018\Image_Projective_Invariants\figure_5.jpg
  Figure 5 caption: Visualizing the stability and discriminability of PIs .
  Figure 6 Link: articels_figures_by_rev_year\2018\Image_Projective_Invariants\figure_6.jpg
  Figure 6 caption: Our butterfly image database.
  Figure 7 Link: articels_figures_by_rev_year\2018\Image_Projective_Invariants\figure_7.jpg
  Figure 7 caption: The precision-recall curves obtained by using PIs - X - N ( Xin
    lbrace G,L,Wrbrace , Nin lbrace 5,7,9,ldots,15rbrace ), AMIs , HMs , ZMs and GHMs
    on our butterfly image database.
  Figure 8 Link: articels_figures_by_rev_year\2018\Image_Projective_Invariants\figure_8.jpg
  Figure 8 caption: The texture image database.
  Figure 9 Link: articels_figures_by_rev_year\2018\Image_Projective_Invariants\figure_9.jpg
  Figure 9 caption: Classification accuracies of texture images using PIs - X - N
    ( Xin lbrace G,L,Wrbrace , Nin lbrace 5,7,9,ldots,15rbrace ), AMIs , HMs , ZMs
    and GHMs .
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.66
  Name of the first author: Erbo Li
  Name of the last author: Hua Li
  Number of Figures: 12
  Number of Tables: 3
  Number of authors: 4
  Paper title: Image Projective Invariants
  Publication Date: 2018-05-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Six 2D Projective Transformations (r = 1)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Numerical Values of PIs PIs-W-7 of 60 Test Images
  Table 3 caption:
    table_text: TABLE 3 The Numerical Values of PIs PIs- W W-7, AMIs AMIs and HMs
      HMs of Real Images
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2832060
- Affiliation of the first author: school of computer science and engineering, nanjing
    university of science and technology, nanjing, jiangsu, china
  Affiliation of the last author: school of engineering, university of california,
    merced, ca
  Figure 1 Link: articels_figures_by_rev_year\2018\Learning_to_Deblur_Images_with_Exemplars\figure_1.jpg
  Figure 1 caption: "A challenging example. (a) Blurred face image. (b)-(d) Results\
    \ of Cho and Lee [4], Krishnan et al. [7], and Xu et al. [9]. (e)-(f) Intermediate\
    \ results of Krishnan et al. [7] and Xu et al. [9]. (g) Restored salient edges\
    \ by our exemplar-based method visualized by Poisson reconstruction. (h) Deblurred\
    \ image by our method (with the support size of 75\xD775 pixels)."
  Figure 10 Link: articels_figures_by_rev_year\2018\Learning_to_Deblur_Images_with_Exemplars\figure_10.jpg
  Figure 10 caption: An example from the synthesized profile face test dataset.
  Figure 2 Link: articels_figures_by_rev_year\2018\Learning_to_Deblur_Images_with_Exemplars\figure_2.jpg
  Figure 2 caption: Effect of salient edges in kernel estimation. (a) True image and
    kernel. (h) Blurred image. (b)-(f) Extracted salient edges of facial components
    from the clear images visualized by Poisson reconstruction. (g) The ground-truth
    edges of (a). (i)-(n) Deblurred results by using edges (b)-(g), respectively.
  Figure 3 Link: articels_figures_by_rev_year\2018\Learning_to_Deblur_Images_with_Exemplars\figure_3.jpg
  Figure 3 caption: Extracted salient edges (see Section 3.2.1 for details). (a) Input
    image. (b) Initial contour. (c) Refined contour.
  Figure 4 Link: articels_figures_by_rev_year\2018\Learning_to_Deblur_Images_with_Exemplars\figure_4.jpg
  Figure 4 caption: Kernel estimation accuracy (KS stands for kernel similarity) with
    respect to restored salient edges from different facial components. The x -axis
    (b)-(g) represent 6 facial components in Figs. 2b, 2c, 2d, 2e, 2f, and 2g.
  Figure 5 Link: articels_figures_by_rev_year\2018\Learning_to_Deblur_Images_with_Exemplars\figure_5.jpg
  Figure 5 caption: Effect of noise on the proposed matching criterion.
  Figure 6 Link: articels_figures_by_rev_year\2018\Learning_to_Deblur_Images_with_Exemplars\figure_6.jpg
  Figure 6 caption: Extracted salient edges by [32] and our method. (a) Input blurred
    images. (b) Some intermediate feature maps generated by proposed network. (c)
    Restored edges by [32]. (d) Restored edges by the proposed CNN.
  Figure 7 Link: articels_figures_by_rev_year\2018\Learning_to_Deblur_Images_with_Exemplars\figure_7.jpg
  Figure 7 caption: Examples of synthetic blur kernels.
  Figure 8 Link: articels_figures_by_rev_year\2018\Learning_to_Deblur_Images_with_Exemplars\figure_8.jpg
  Figure 8 caption: 'Quantitative comparisons with several state-of-the-art single-image
    blind deblurring methods: Shan et al. [3], Cho and Lee [4], Xu and Jia [5], Krishnan
    et al. [7], Levin et al. [2], Zhong et al. [44], Xu et al. [9], Sun et al. [17],
    and Michaeli and Irani [18].'
  Figure 9 Link: articels_figures_by_rev_year\2018\Learning_to_Deblur_Images_with_Exemplars\figure_9.jpg
  Figure 9 caption: An example from the synthesized frontal face test dataset.
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jinshan Pan
  Name of the last author: Ming-Hsuan Yang
  Number of Figures: 19
  Number of Tables: 4
  Number of authors: 4
  Paper title: Learning to Deblur Images with Exemplars
  Publication Date: 2018-05-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 CNN Architecture for Structure Prediction
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average Run Time (s) on 480 Test Images
  Table 3 caption:
    table_text: TABLE 3 Effect of the Filter Size in the First Layer on Image Deblurring
  Table 4 caption:
    table_text: TABLE 4 Effect of the Different Filter Size Settings in the Convolution
      Layer of the Proposed Network
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2832125
- Affiliation of the first author: university of chinese academy of sciences, beijing,
    china
  Affiliation of the last author: center for research on intelligent perception and
    computing, national laboratory of pattern recognition, institute of automation,
    cas center for excellence in brain science and intelligence technology, chinese
    academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2018\Aggregating_Randomized_ClusteringPromoting_Invariant_Projections_for_Domain_Adap\figure_1.jpg
  Figure 1 caption: A toy example of the sought subspaces with different structures,
    (a). Promoting source class-clustering, (b). Promoting domain-irrelevant class-clustering
    (confusion), respectively. (Best viewed in colors.)
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Aggregating_Randomized_ClusteringPromoting_Invariant_Projections_for_Domain_Adap\figure_2.jpg
  Figure 2 caption: Overview of the proposed 'sampling-and-fusion' framework. Several
    coupled source-target domain subsets are generated via randomly instances and
    feature sampling. Then for each source-target domain pair, we learn the optimal
    domain-invariant projection via the proposed domain-invariant object function.
    Finally these classifiers are trained on the whole source data and be fused in
    an ensemble manner to predict the whole unlabeled target data. (Best viewed in
    colors.)
  Figure 3 Link: articels_figures_by_rev_year\2018\Aggregating_Randomized_ClusteringPromoting_Invariant_Projections_for_Domain_Adap\figure_3.jpg
  Figure 3 caption: 'Ablation study of our algorithm (SCTC: sourcetarget class clustering).
    [Office-Caltech: SURF & PIE: l 2 -normalization & Office-Home: ResNet50-P 5 ]'
  Figure 4 Link: articels_figures_by_rev_year\2018\Aggregating_Randomized_ClusteringPromoting_Invariant_Projections_for_Domain_Adap\figure_4.jpg
  Figure 4 caption: "Parameter study of our algorithm: averaged accuracy with parameter\
    \ \u03BB . [Office-Caltech: SURF & PIE: l 2 -normalization & Office-Home: ResNet50-P\
    \ 5 ]"
  Figure 5 Link: articels_figures_by_rev_year\2018\Aggregating_Randomized_ClusteringPromoting_Invariant_Projections_for_Domain_Adap\figure_5.jpg
  Figure 5 caption: "Averaged recognition accuracy (%) on the Office-Caltech dataset\
    \ with SURF features using the 'full training' protocol (a-c) and MNIST-USPS dataset\
    \ (d-f) w.r.t. each sampling density \u03B4 s , \u03B4 t and \u03B4 f while the\
    \ other two densities are kept fixed at 1."
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jian Liang
  Name of the last author: Tieniu Tan
  Number of Figures: 5
  Number of Tables: 9
  Number of authors: 4
  Paper title: Aggregating Randomized Clustering-Promoting Invariant Projections for
    Domain Adaptation
  Publication Date: 2018-05-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Statistics of the Six Benchmark Domain-Adaptation Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Recognition Accuracies (%) on the Office31 Dataset
  Table 3 caption:
    table_text: TABLE 3 Recognition Accuracies (%) on the Office-Caltech Dataset using
      the Protocol [9], [10]
  Table 4 caption:
    table_text: TABLE 4 Averaged Recognition Accuracies (%) on the Office-Caltech
      Dataset with various Features under the 'Splitting' Protocol [67], [68]
  Table 5 caption:
    table_text: "TABLE 5 Recognition Accuracies (%) on the PIE Dataset with two Preprocessing\
      \ Tools, l 2 l2-Normalization and z z-Score Standardization ( \u2020 \u2020\
      \ Last 6 Columns)"
  Table 6 caption:
    table_text: TABLE 6 Recognition Accuracies (%) on the Office-Home Dataset with
      ResNet50-P 5 5 Features
  Table 7 caption:
    table_text: TABLE 7 Recognition Accuracies (%) on the MNIST - USPS and COIL20
      Datasets
  Table 8 caption:
    table_text: "TABLE 8 Averaged Recognition Accuracy \xB1 \xB1 Standard Deviation\
      \ (%) on Five 'High-Dimensional' (d > > 1,000) Datasets ( \u03B4 f =0.5,K=10\
      \ \u03B4f=0.5,K=10)"
  Table 9 caption:
    table_text: "TABLE 9 Averaged Recognition Accuracy \xB1 \xB1 Standard Deviation\
      \ (%) on the Office-Caltech Dataset with SURF Features under the 'Full-Training'\
      \ Protocol for WMV-1-NN w.r.t. Different Sampling Densities [ \u03B4 s , \u03B4\
      \ t , \u03B4 f \u03B4s,\u03B4t,\u03B4f]"
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2832198
- Affiliation of the first author: school of electronic engineering and computer science,
    queen mary university of london, london, united kingdom
  Affiliation of the last author: vision semantics ltd., london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2018\Imbalanced_Deep_Learning_by_Minority_Class_Incremental_Rectification\figure_1.jpg
  Figure 1 caption: 'Imbalanced training data class distributions: (a) clothing attributes
    (X-Domain [11]), (b) facial attributes (CelebA [12]).'
  Figure 10 Link: articels_figures_by_rev_year\2018\Imbalanced_Deep_Learning_by_Minority_Class_Incremental_Rectification\figure_10.jpg
  Figure 10 caption: '(a) Simulated imbalanced training data distributions on CIFAR-100
    [34]. (b) Performance gains of ResNet32 by the CRL on differently imbalanced training
    data. Metric: Mean class-balanced accuracy (%).'
  Figure 2 Link: articels_figures_by_rev_year\2018\Imbalanced_Deep_Learning_by_Minority_Class_Incremental_Rectification\figure_2.jpg
  Figure 2 caption: Visualisation of deep feature distributions learned by the ResNet32
    [36] models trained on (a) class balanced data, (b) class imbalanced data, and
    (c) class imbalanced data with our CRL model. Customised CIFAR-100 object category
    training image sets were used (more details in Section 4.3). In the illustration,
    we showed only 3 (2 majority and 1 minority) classes for the visual clarity sake.
    It is observed that the minority class (Min-1) is overwhelmed more severely by
    the two majority classes (Maj-1, Maj-2) when deploying the existing CNN model
    given class imbalanced training data. The proposed CRL approach rectifies clearly
    this undesired model induction bias inherent to existing deep learning architectures.
  Figure 3 Link: articels_figures_by_rev_year\2018\Imbalanced_Deep_Learning_by_Minority_Class_Incremental_Rectification\figure_3.jpg
  Figure 3 caption: Overview of the proposed Class Rectification Loss (CRL) regularisation
    for large scale class imbalanced deep learning.
  Figure 4 Link: articels_figures_by_rev_year\2018\Imbalanced_Deep_Learning_by_Minority_Class_Incremental_Rectification\figure_4.jpg
  Figure 4 caption: Illustration of (a) inter-class structure rectification around
    decision boundary in the CRL, as complementary to (b) the cross-entropy loss with
    single-class independent modelling (indicated by dashed arrow).
  Figure 5 Link: articels_figures_by_rev_year\2018\Imbalanced_Deep_Learning_by_Minority_Class_Incremental_Rectification\figure_5.jpg
  Figure 5 caption: 'Illustration of minority class hard sample mining. (a) Class-level
    mining: For each minority class c , hard-positives are those samples from class
    c but with low class prediction scores on c by the current model (red solid circles).
    Hard-negatives are those with high class c prediction scores but on the wrong
    class (blue solid circles). (b) Instance-level mining: For each sample (dotted
    red box) of a minority class c , hard-positives are the samples of class c (solid
    red box) further away from the given sample of class c in the feature space (pointed
    by a red arrow). Hard-negatives are those close to the given sample but from different
    classes (pointed by blue arrow). Top-3 hard positive and negative samples are
    shown in the two examples for conceptual illustration.'
  Figure 6 Link: articels_figures_by_rev_year\2018\Imbalanced_Deep_Learning_by_Minority_Class_Incremental_Rectification\figure_6.jpg
  Figure 6 caption: Performance additional gain over the DeepID2 by the LMLE and CRL
    models on the 40 CelebA binary-class facial attributes [12]. Attributes are sorted
    from left to right in increasing class imbalance ratio.
  Figure 7 Link: articels_figures_by_rev_year\2018\Imbalanced_Deep_Learning_by_Minority_Class_Incremental_Rectification\figure_7.jpg
  Figure 7 caption: Examples (3 pairs) of facial attribute recognition (imbalance
    ratio in bracket). For each pair (attribute), DeepID2 missed both, whilst CRL
    identified the image with green box but failed the image with red box.
  Figure 8 Link: articels_figures_by_rev_year\2018\Imbalanced_Deep_Learning_by_Minority_Class_Incremental_Rectification\figure_8.jpg
  Figure 8 caption: Performance additional gain over the MTCT by the LMLE and CRL
    models on 9 X-Domain multi-class clothing attributes [11] with the imbalance ratios
    (numbers under the bars) increasing from left to right.
  Figure 9 Link: articels_figures_by_rev_year\2018\Imbalanced_Deep_Learning_by_Minority_Class_Incremental_Rectification\figure_9.jpg
  Figure 9 caption: 'Examples of clothing attribute recognition by the CRL model,
    with false attribute prediction in red (Red box: clothing auto-detection).'
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.61
  Name of the first author: Qi Dong
  Name of the last author: Xiatian Zhu
  Number of Figures: 12
  Number of Tables: 13
  Number of authors: 3
  Paper title: Imbalanced Deep Learning by Minority Class Incremental Rectification
  Publication Date: 2018-05-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparing Large Datasets w.r.t. Training Data Imbalance in
      Terms of Class Imbalance Ratio (the Sample Size Ratio Between the Smallest and
      Largest Classes)
  Table 10 caption:
    table_text: TABLE 10 Comparing Hard Mining Schemes (ClassInstance) and CRL Loss
      Functions (Relative (Rel), Absolute (Abs), and Distribution (Dis))
  Table 2 caption:
    table_text: TABLE 2 Statistics of the Three Datasets Utilised in Our Evaluations
  Table 3 caption:
    table_text: TABLE 3 Facial Attribute Recognition on the CelebA Benchmark [12]
  Table 4 caption:
    table_text: TABLE 4 Performance Additional Gain Over the DeepID2 by LMLE and CRL
      on Bottom-20 and Top-20 CelebA Facial Attributes in Mean Class-Balanced Accuracy
      (%)
  Table 5 caption:
    table_text: TABLE 5 Clothing Attributes Recognition on the X-Domain Dataset [11]
  Table 6 caption:
    table_text: TABLE 6 Performance Additional Gain Over the MTCT by LMLE and CRL
      on Bottom-1 and Top-8 X-Domain Clothing Attributes in Mean Accuracy (%)
  Table 7 caption:
    table_text: TABLE 7 Evaluation of CRL on Clothing Attribute Recognition with the
      DeepFashion Benchmark [30]
  Table 8 caption:
    table_text: TABLE 8 Object Classification Performance (%) on CIFAR-100 [34]
  Table 9 caption:
    table_text: "TABLE 9 Effect of the CRL in Different CNN Models Given Class Imbalanced\
      \ Training Data ( \u03B3=1 \u03B3=1)"
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2832629
- Affiliation of the first author: "insitut f\xFCr informationsverarbeitung, gottfried\
    \ wilhelm leibniz universit\xE4t hannover, hannover, germany"
  Affiliation of the last author: "insitut f\xFCr informationsverarbeitung, gottfried\
    \ wilhelm leibniz universit\xE4t hannover, hannover, germany"
  Figure 1 Link: articels_figures_by_rev_year\2018\OcclusionAware_Method_for_Temporally_Consistent_Superpixels\figure_1.jpg
  Figure 1 caption: 'Top row: Original sequence with frame numbers. Mid row: Subset
    of superpixels manually selected in frame 15 and shown as color-coded labels.
    The superpixels in the frames 22 and 30 are generated with our approach and are
    displayed using the same label colors to indicate temporal consistency. Bottom
    row: The soccer players are cut out based on the selected superpixels. Best viewed
    in color.'
  Figure 10 Link: articels_figures_by_rev_year\2018\OcclusionAware_Method_for_Temporally_Consistent_Superpixels\figure_10.jpg
  Figure 10 caption: Results for the 2D benchmark metrics. The left column shows the
    results for the BuffaloXiph [39] data set, the right column for the BVDS [40]
    data set. Note that the metrics are plotted over the number of superpixels per
    frame. Lower values are better except for the superpixel compactness (CO). For
    BVDS the curve of sGBH is out of the range in the variance of area diagram. It
    starts at about 7.5 and linearly increases up to 13.
  Figure 2 Link: articels_figures_by_rev_year\2018\OcclusionAware_Method_for_Temporally_Consistent_Superpixels\figure_2.jpg
  Figure 2 caption: 'Top row: Frame 1 and 60 of the soccer sequence. Second row: Label
    maps with temporal consistency but without a method to cope with structural changes
    in the video volume. The superpixels in the later part of the sequence are squeezed
    together on the left side of the frame, while on the right side the size of the
    superpixels has to grow to fully occupy the newly uncovered image regions. Third
    row: Label maps created with our approach. The superpixels are temporally consistent
    and have an equal size over the whole sequence. The silhouette of the player was
    marked for visualization purposes. Best viewed in color.'
  Figure 3 Link: articels_figures_by_rev_year\2018\OcclusionAware_Method_for_Temporally_Consistent_Superpixels\figure_3.jpg
  Figure 3 caption: The three subfigures exemplarily show pixels at the border between
    two superpixels (green and blue). In (b) and (c), a pixel is marked (orange) and
    it is examined if a label change (to blue) would violate the spatial coherence
    constraint. The eight neighboring pixels (marked by the circling arrow) are plotted
    as an array on the bottom of each subfigure. While the label in the array of (b)
    only changes once and thus a change is valid, the label in (c) changes a second
    time from blue to green which indicates an invalid change. Best viewed in color.
  Figure 4 Link: articels_figures_by_rev_year\2018\OcclusionAware_Method_for_Temporally_Consistent_Superpixels\figure_4.jpg
  Figure 4 caption: 'Sliding window approach. Bottom row: Frames inside the sliding
    window (non-transparent) are divided into three groups. Top row: Corresponding
    label maps.'
  Figure 5 Link: articels_figures_by_rev_year\2018\OcclusionAware_Method_for_Temporally_Consistent_Superpixels\figure_5.jpg
  Figure 5 caption: The sliding window is initialized from the front to the back positions.
    After J iterations (cf. Algorithm 1), the sliding window is shifted and the segmentation
    of the current frame moves into the area of the past frames. As these segmentations
    are not altered anymore, they can be stored to disk which reduces the delay of
    the algorithm to F+1 frames.
  Figure 6 Link: articels_figures_by_rev_year\2018\OcclusionAware_Method_for_Temporally_Consistent_Superpixels\figure_6.jpg
  Figure 6 caption: Schematic example of two adjacent superpixels being propagated
    towards each other by optical flow. The overlapping areas (dark blue) can yield
    essential information about the structure of the video as it indicates the occlusion
    of an image region. Similarly, a gap in the propagated superpixel labeling indicates
    a disoccluded region (light blue). Best viewed in color.
  Figure 7 Link: articels_figures_by_rev_year\2018\OcclusionAware_Method_for_Temporally_Consistent_Superpixels\figure_7.jpg
  Figure 7 caption: "The left superpixel (red) is propagated (indicated by the arrow)\
    \ towards two stationary superpixels (yellow, green). Without the knowledge that\
    \ the yellow superpixel becomes occluded by the red superpixel (left column) its\
    \ newly calculated spatial center will have an offset compared to the propagated\
    \ center. The offset of the spatial center will lead to a shift of the yellow\
    \ superpixel as the spatial term of Eq. (2) enforces the homogeneous size constraint\
    \ (indicated by the dotted circle). This effect is further propagated and results\
    \ in a shift of the neighboring (green) superpixel. By utilizing the occlusion\
    \ information to compute the \u201Ctrue\u201D spatial center (right column) the\
    \ drift can be avoided resulting in a more accurate superpixel movement."
  Figure 8 Link: articels_figures_by_rev_year\2018\OcclusionAware_Method_for_Temporally_Consistent_Superpixels\figure_8.jpg
  Figure 8 caption: Color coded plots for the 3D segmentation accuracy barzeta operatornameSA(alpha,
    P) , temporal extent barzeta operatornameTEX(alpha, P) , and 3D undersegmentation
    error barzeta operatornameUE(alpha, P) . For each rectangle in (a) to (c) the
    metrics were averaged over all sequences of the Segtrack [38] data set and seven
    levels of coarseness. The color maps are adjusted to the minimum and maximum values
    of the normalized metrics. Here, the scale of the 3D undersegmentation error is
    inverted, so red means better in all plots. In (d) the multiplicative combination
    is shown.
  Figure 9 Link: articels_figures_by_rev_year\2018\OcclusionAware_Method_for_Temporally_Consistent_Superpixels\figure_9.jpg
  Figure 9 caption: Results for spatio-temporal benchmark metrics. The left column
    shows the results for the BuffaloXiph [39] data set, the right column for the
    BVDS [40] data set. Note that the metrics are plotted over the number of supervoxels.
    Higher values are better except for the 3D undersegmentation error (3D UE).
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Matthias Reso
  Name of the last author: "J\xF6rn Ostermann"
  Number of Figures: 14
  Number of Tables: 1
  Number of authors: 4
  Paper title: Occlusion-Aware Method for Temporally Consistent Superpixels
  Publication Date: 2018-05-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Misclassification Error Rate (in Percent) for the Interactive
      Video Segmentation Task
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2832628
- Affiliation of the first author: computer science and artificial intelligence laboratory,
    massachusetts institute of technology, cambridge, ma
  Affiliation of the last author: laboratory of information and decision systems,
    massachusetts institute of technology, cambridge, ma, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\Dynamic_Clustering_Algorithms_via_SmallVariance_Analysis_of_Markov_Chain_Mixture\figure_1.jpg
  Figure 1 caption: "(1a - 1c): Accuracy contours and CPU time histogram for D-Means.\
    \ (1d - 1e): Comparison with Gibbs sampling, variational inference, and particle\
    \ learning. Shaded region indicates 1\u03C3 interval; in (1e), only upper half\
    \ is shown. (1f): Comparison of accuracy when enforcing (Gibbs, D-Means) and not\
    \ enforcing (Gibbs NT, D-Means NT) correct cluster tracking. (1g): An example\
    \ 5 sequential frames from clustering using D-Means; note the consistency of datapoint\
    \ labelling (color) over time."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Dynamic_Clustering_Algorithms_via_SmallVariance_Analysis_of_Markov_Chain_Mixture\figure_2.jpg
  Figure 2 caption: '(2a - 2c): Accuracy contours and CPU time histogram for SD-Means
    on the synthetic ring data. (2d - 2e) An example 5 sequential frames from clustering
    using SD-Means (2d) and D-Means (2e).'
  Figure 3 Link: articels_figures_by_rev_year\2018\Dynamic_Clustering_Algorithms_via_SmallVariance_Analysis_of_Markov_Chain_Mixture\figure_3.jpg
  Figure 3 caption: MST kernel to the center point.
  Figure 4 Link: articels_figures_by_rev_year\2018\Dynamic_Clustering_Algorithms_via_SmallVariance_Analysis_of_Markov_Chain_Mixture\figure_4.jpg
  Figure 4 caption: Four example GPs to be clustered.
  Figure 5 Link: articels_figures_by_rev_year\2018\Dynamic_Clustering_Algorithms_via_SmallVariance_Analysis_of_Markov_Chain_Mixture\figure_5.jpg
  Figure 5 caption: Typical set of latent functions discovered by D-Means.
  Figure 6 Link: articels_figures_by_rev_year\2018\Dynamic_Clustering_Algorithms_via_SmallVariance_Analysis_of_Markov_Chain_Mixture\figure_6.jpg
  Figure 6 caption: Typical set of latent functions discovered by SD-Means.
  Figure 7 Link: articels_figures_by_rev_year\2018\Dynamic_Clustering_Algorithms_via_SmallVariance_Analysis_of_Markov_Chain_Mixture\figure_7.jpg
  Figure 7 caption: '(7a): Histogram of label accuracy for D-Means and SD-Means. (7b):
    Histogram of the number of clusters learned for D-Means and SD-Means. (7c): Histogram
    of computational time for D-Means and SD-Means.'
  Figure 8 Link: articels_figures_by_rev_year\2018\Dynamic_Clustering_Algorithms_via_SmallVariance_Analysis_of_Markov_Chain_Mixture\figure_8.jpg
  Figure 8 caption: 'Results of the GP aircraft trajectory clustering. (8a): A map
    (labeled with major US city airports) showing the overall aircraft flows for 12
    trajectories, with colors and 1 sigma confidence ellipses corresponding to takeoff
    region (multiple clusters per takeoff region), colored dots indicating mean takeoff
    position for each cluster, and lines indicating the mean trajectory for each cluster.
    (8b): A track of plane counts for the 12 clusters during the week, with color
    intensity proportional to the number of takeoffs at each time.'
  Figure 9 Link: articels_figures_by_rev_year\2018\Dynamic_Clustering_Algorithms_via_SmallVariance_Analysis_of_Markov_Chain_Mixture\figure_9.jpg
  Figure 9 caption: 'Video color quantization results. (9a): Four frames from the
    original video (top row) and quantized video (bottom row) with D-Means. (9b):
    The number of clusters over the whole video (top) and a zoomed window on frames
    3550-4350 (bottom). Note the smoothness of the number of clusters over time using
    D-Means with respect to DP-Means. (9c): A sequence of 4 frames from the original
    video (top row), quantized video with D-Means (middle row), and quantized video
    with DP-Means (bottom row). Note the flicker in the quantization, particularly
    in the upper region of the frames, when using DP-Means in contrast to using D-Means.
    (9d): The difference between each frame and the previous frame for quantized video,
    normalized by the same quantity for the original video. D-Means tracks the original
    video (dashed horizontal black line) more closely than both K-Means and DP-Means.
    Missing values are when the original video had no frame difference (normalization
    by 0).'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Trevor Campbell
  Name of the last author: Jonathan How
  Number of Figures: 9
  Number of Tables: 2
  Number of authors: 3
  Paper title: Dynamic Clustering Algorithms via Small-Variance Analysis of Markov
    Chain Mixture Models
  Publication Date: 2018-05-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Mean Computational Time & Accuracy on Synthetic Ring Data
      Over 50 Trials
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Mean Computational Time & Accuracy on Hand-Labeled Aircraft
      Trajectory Data
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2833467
- Affiliation of the first author: department of electrical engineering, university
    of washington, seattle, wa
  Affiliation of the last author: department of electrical engineering, university
    of washington, seattle, wa
  Figure 1 Link: articels_figures_by_rev_year\2018\Unsupervised_Deep_Learning_of_Compact_Binary_Descriptors\figure_1.jpg
  Figure 1 caption: The basic idea of our proposed method. We enforce three criteria,
    and optimize the parameters of the network with back-propagation. Our approach
    dose not require labels provided by the dataset, and is more practical to real-world
    applications in comparison to supervised binary descriptors. This figure shows
    the inference stage of our approach. Given an input image, our network encodes
    it as a binary feature descriptor.
  Figure 10 Link: articels_figures_by_rev_year\2018\Unsupervised_Deep_Learning_of_Compact_Binary_Descriptors\figure_10.jpg
  Figure 10 caption: Query images and their corresponding top 10 ranked images retrieved
    from ILSVRC2012 dataset. Upper two rows are the correct retrieval results. The
    bottom row shows our failure case. Images with red boarder are the false positives.
  Figure 2 Link: articels_figures_by_rev_year\2018\Unsupervised_Deep_Learning_of_Compact_Binary_Descriptors\figure_2.jpg
  Figure 2 caption: An overview of the proposed unsupervised deep learning framework
    for computing binary descriptors. We create a new fully-connected layer on the
    top of the existing deep neural network, and employ the proposed objective functions
    to learn compact yet discriminative binary descriptor. The proposed approach takes
    two inputs from the original image and the geometric transformed one into two
    towers of our network, respectively. Both of the networks share and update the
    same parameters. Then, we learn binary descriptors through the optimization of
    the proposed objective functions. Note that the binary descriptor is computed
    by applying only one tower of our network in the test phase.
  Figure 3 Link: articels_figures_by_rev_year\2018\Unsupervised_Deep_Learning_of_Compact_Binary_Descriptors\figure_3.jpg
  Figure 3 caption: 'ROC curves of the proposed DeepBit descriptors and the compared
    binary descriptors, across all the splits of training and testing configurations
    on the Brown datasets. In parentheses: The bit length of the binary descriptor
    (b), and the 95 percent error rates.'
  Figure 4 Link: articels_figures_by_rev_year\2018\Unsupervised_Deep_Learning_of_Compact_Binary_Descriptors\figure_4.jpg
  Figure 4 caption: Comparative evaluation of the matching time with different descriptors.
    The estimated computational cost were computed for 100K pairs from a test dataset
    on a desktop with the POPCOUNT instruction and averaged over 100 runs.
  Figure 5 Link: articels_figures_by_rev_year\2018\Unsupervised_Deep_Learning_of_Compact_Binary_Descriptors\figure_5.jpg
  Figure 5 caption: Correctly matched patches and mismatched ones from the Brown dataset.
    Top row shows the patches from Liberty classified as matched pairs; the first
    three are correctly classified, but the fourth is mismatched, which describes
    different architectures. Middle row shows the image pairs from Notredame classified
    as the matched pairs; the fourth is mismatched although both of them share similar
    pattern. Bottom row shows the patches from Yosemite classified as matched pairs;
    the last one is mismatched, which are visually similar but belong to different
    locations.
  Figure 6 Link: articels_figures_by_rev_year\2018\Unsupervised_Deep_Learning_of_Compact_Binary_Descriptors\figure_6.jpg
  Figure 6 caption: PrecisionRecall curves of different unsupervised hashing methods
    on the CIFAR-10 dataset with respect to 16, 32 and 64 bits, respectively.
  Figure 7 Link: articels_figures_by_rev_year\2018\Unsupervised_Deep_Learning_of_Compact_Binary_Descriptors\figure_7.jpg
  Figure 7 caption: Distribution of the binary codes when training with different
    weights of the objective function E2 . The generated binary codes are evenly distributed
    when the mean value for each bit is close to 0.5. The results show that our proposed
    objective function is effective for learning balanced binary codes.
  Figure 8 Link: articels_figures_by_rev_year\2018\Unsupervised_Deep_Learning_of_Compact_Binary_Descriptors\figure_8.jpg
  Figure 8 caption: Performance comparison (mAP, %) of DeepBit with different combinations
    of hyperparameters. The bit length is 32. (a) Different parameters of beta and
    gamma while alpha is set to 1, (b) Different scaling factors and rotation ranges.
  Figure 9 Link: articels_figures_by_rev_year\2018\Unsupervised_Deep_Learning_of_Compact_Binary_Descriptors\figure_9.jpg
  Figure 9 caption: Query patches and the corresponding top ranked 10 patches retrieved
    from RomePatches dataset. The code length of our binary descriptor is 512.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.51
  Name of the first author: Kevin Lin
  Name of the last author: Ming-Ting Sun
  Number of Figures: 13
  Number of Tables: 12
  Number of authors: 5
  Paper title: Unsupervised Deep Learning of Compact Binary Descriptors
  Publication Date: 2018-05-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Statistics of Datasets Used in the Experiments
  Table 10 caption:
    table_text: TABLE 10 The Categorization Accuracy (Mean%) for Different Features
      on the Flower-102 Dataset
  Table 2 caption:
    table_text: TABLE 2 Comparison of the Proposed Binary Descriptor to the State-of-the-Art
      Binary Descriptors, in Terms of 95 Percent Error Rates (ERR) Across All the
      Splits of Training and Testing Configurations
  Table 3 caption:
    table_text: TABLE 3 Performance Comparison (mAP, %) of Different Unsupervised
      Hashing Algorithms on the CIFAR-10 Dataset
  Table 4 caption:
    table_text: TABLE 4 Performance Comparison (mAP, %) of Different Unsupervised
      Hashing Algorithms on the CIFAR-10 Dataset
  Table 5 caption:
    table_text: TABLE 5 Performance Comparison (mAP, %) of Different Binary Descriptors
      on the RomePatches Dataset
  Table 6 caption:
    table_text: TABLE 6 The mAP at Top 1,000 Returned Images and Precision at n n
      Samples of Methods on the ILSVRC2012 Validation Set
  Table 7 caption:
    table_text: TABLE 7 Performance Comparison of Instances Retrieval Performance
      (mAP, %) of Different Methods on Paris, Oxford, and Holidays Datasets
  Table 8 caption:
    table_text: TABLE 8 Performance Comparison (Precision, %) of the Top 4 Returned
      Images of Different Methods on UKB Dataset
  Table 9 caption:
    table_text: TABLE 9 The Categorization Accuracy (Mean%) for Different Features
      on the Flower-17 Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2833865
