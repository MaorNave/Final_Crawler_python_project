- Affiliation of the first author: department of statistics and data science and department
    of computer science, university of central florida, orlando, fl, usa
  Affiliation of the last author: department of mathematics, university of california,
    los angeles, los angeles, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2023\Matrix_Completion_With_CrossConcentrated_Sampling_Bridging_Uniform_Sampling_and_\figure_1.jpg
  Figure 1 caption: Visual illustrations of different sampling schemes. From left
    to right, sampling methods change from the uniform sampling style to the CUR sampling
    style with the same total observations rate. Colored pixels indicate observed
    entries, while black pixels mean missing entries.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Matrix_Completion_With_CrossConcentrated_Sampling_Bridging_Uniform_Sampling_and_\figure_2.jpg
  Figure 2 caption: Visual results for image inpainting from the CCS-based samples
    via ScalePGD algorithm [32]. See a more detailed setting in Section IV-B.
  Figure 3 Link: articels_figures_by_rev_year\2023\Matrix_Completion_With_CrossConcentrated_Sampling_Bridging_Uniform_Sampling_and_\figure_3.jpg
  Figure 3 caption: 'Empirical phase transition in the overall sampling rate alpha
    , the percentage of selected rows and columns delta , and uniform sampling rates
    on the selected submatrices p . Row 1: 3D-view of the empirical phase transition
    of ICURC. Row 2: 2D view of empirical phase transition of ICURC (in the red box),
    ScaledPGD (in the blue box), and SVP (in the green box). Left: r=5 . Middle: r=10
    . Right: r=15 . One can see that as rank increases, the required overall sampling
    rate increases correspondingly. Additionally, the CCS model provides flexibility
    in obtaining a sufficient amount of data to ensure completing the missing data
    successfully and the performance of the ICURC algorithm from the CCS-based samples
    is comparable to that of the state-of-the-art algorithms (SVP and ScaledPGD) from
    the uniform-sampling-based samples.'
  Figure 4 Link: articels_figures_by_rev_year\2023\Matrix_Completion_With_CrossConcentrated_Sampling_Bridging_Uniform_Sampling_and_\figure_4.jpg
  Figure 4 caption: 'Empirical phase transitions of ICURC in overall observation size
    s and problem size n . The column (resp. row) number of the concentrated column
    (resp. row) submatrix equals to crlog 2(n) . Row 1: r = 5 . Row 2: r = 10 . Row
    3: r = 15 . Left: c = 0.25 . Middle: c = 0.5 . Right: c = 1 . The required samples
    for guaranteed matrix completion are independent of the size of the concentrated
    submatrices.'
  Figure 5 Link: articels_figures_by_rev_year\2023\Matrix_Completion_With_CrossConcentrated_Sampling_Bridging_Uniform_Sampling_and_\figure_5.jpg
  Figure 5 caption: Visual results for image inpainting by setting rank r = 20 and
    the percentage of selected rows and columns delta = 10 % . ScaledPGD and SVP are
    based on the uniform sampling model with the same observed number of entries as
    the one based on CCS. All algorithms achieve visually reliable results.
  Figure 6 Link: articels_figures_by_rev_year\2023\Matrix_Completion_With_CrossConcentrated_Sampling_Bridging_Uniform_Sampling_and_\figure_6.jpg
  Figure 6 caption: 'Bar-plot results on the recommendation system data. The performances
    are measured in HR, NMAE, and runtime. Top: overall observation rate alpha = 10
    % . Middle: alpha = 20 % . Bottom: alpha = 30 % . When overall observation rate
    alpha is fixed, the ICURC performs better under various CCS conditions compared
    with other methods in uniform sampling.'
  Figure 7 Link: articels_figures_by_rev_year\2023\Matrix_Completion_With_CrossConcentrated_Sampling_Bridging_Uniform_Sampling_and_\figure_7.jpg
  Figure 7 caption: 'Bar-plot results on link prediction datasets. The performances
    are measured in Precision, AUC, and runtime. Top: overall observation rate alpha
    = 10 % . Middle: alpha = 20 % . Bottom: alpha = 30 % . When overall observation
    rate alpha is fixed, the ICURC performs better under various CCS conditions compared
    with other methods in uniform sampling.'
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.75
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: HanQin Cai
  Name of the last author: Deanna Needell
  Number of Figures: 7
  Number of Tables: 4
  Number of authors: 4
  Paper title: 'Matrix Completion With Cross-Concentrated Sampling: Bridging Uniform
    Sampling and CUR Sampling'
  Publication Date: 2023-03-23 00:00:00
  Table 1 caption:
    table_text: TABLE I Table of Notation
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Image Inpainting Results on Building and Window Datasets.
      Under Various Setups of CCS, ICURC can Achieve Higher SNR With Shorter Runtime
      Compared With Other Methods
  Table 3 caption:
    table_text: "TABLE III Datasets Information for Collaborative Filtering. Here,\
      \ \u03B1 \u03B1 is the Overall Observation Rate"
  Table 4 caption:
    table_text: "TABLE IV Information for Link Prediction Datasets. Here, \u03B1 \u03B1\
      \ is the Overall Observation Rate"
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3261185
- Affiliation of the first author: department of computer science, university of bucharest,
    bucharest, romania
  Affiliation of the last author: department of computer science, center for research
    in computer vision (crcv), university of central florida, orlando, fl, usa
  Figure 1 Link: articels_figures_by_rev_year\2023\Diffusion_Models_in_Vision_A_Survey\figure_1.jpg
  Figure 1 caption: The rough number of papers on diffusion models per year.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Diffusion_Models_in_Vision_A_Survey\figure_2.jpg
  Figure 2 caption: Images generated by Stable Diffusion [10] based on various text
    prompts, via the https:beta.dreamstudio.aidream platform.
  Figure 3 Link: articels_figures_by_rev_year\2023\Diffusion_Models_in_Vision_A_Survey\figure_3.jpg
  Figure 3 caption: 'A generic framework composing three alternative formulations
    of diffusion models based on: stochastic differential equations (SDEs), denoising
    diffusion probabilistic models (DDPMs) and noise conditioned score networks (NCSNs).
    In general, a diffusion model consists of two processes. The first one, called
    the forward process, transforms data into noise, while the second one is a generative
    process that reverses the effect of the forward process. This latter process learns
    to transform the noise back into data. We illustrate these processes for all three
    formulations. The forward SDE shows that a change over time in x is modeled by
    a function f plus a stochastic component partial omega sim mathcal N(0,partial
    t) scaled by sigma (t) . We underline that different choices of f and sigma will
    lead to different diffusion processes. This is why the SDE formulation is a generalization
    of the other two. The reverse (generative) SDE shows how to change x in order
    to recover the data from pure noise. We keep the random component and modify the
    deterministic one using the gradients of the log probability nabla x log pt(x)
    , so that x moves to regions where the data density p(x) is high. DDPMs sample
    the data points during the forward process from a normal distribution mathcal
    N!(xt;! sqrt1!-!beta t!cdot ! xt-1, beta t!cdot !mathbf I) , where beta tll 1
    . This iterative sampling slowly destroys information in data, and replaces it
    with Gaussian noise. The sampling is illustrated via the reparametrization trick
    (see details in Section II-A). The reverse process of DDPM also performs iterative
    sampling from a normal distribution, but the mean mu theta (xt,t) of the distribution
    is derived by subtracting the noise, estimated by a neural network, from the image
    at the previous step xt . The variance is equal to the one used in the forward
    process. The initial image going into the reverse process contains only Gaussian
    noise. The forward process of NCSN simply adds normal noise to the image at the
    previous step. This can also be seen as sampling from a normal distribution mathcal
    N(xt;xt-1,(sigma t2-sigma t-12))cdot mathbf I) , with the mean being the image
    at the previous step. The reverse process of NCSN is based on an algorithm described
    in Section II-B. Best viewed in color.'
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Florinel-Alin Croitoru
  Name of the last author: Mubarak Shah
  Number of Figures: 3
  Number of Tables: 1
  Number of authors: 4
  Paper title: 'Diffusion Models in Vision: A Survey'
  Publication Date: 2023-03-27 00:00:00
  Table 1 caption:
    table_text: "TABLE I Our Multi-Perspective Categorization of Diffusion Models\
      \ Applied in Computer Vision. To Classify Existing Models, We Consider Three\
      \ Criteria: The Task, the Denoising Condition, and the Underlying Approach (architecture).\
      \ Additionally, We List the Data Sets on Which the Surveyed Models are Applied.\
      \ We Use the Following Abbreviations in the Architecture Column: D3PM (Discrete\
      \ Denoising Diffusion Probabilistic Models), DSB (Diffusion Schr\xF6dinger Bridge),\
      \ BDDM (Bilateral Denoising Diffusion Models), PNDM (Pseudo Numerical Methods\
      \ for Diffusion Models), ADM (Ablated Diffusion Model), D2C (Diffusion-Decoding\
      \ Models With Contrastive Representations), CCDF (Come-Closer-Diffuse-Faster),\
      \ VQ-DDM (Vector Quantised Discrete Diffusion Model), BF-CNN (Bias-Free CNN),\
      \ FDM (Flexible Diffusion Model), RVD (Residual Video Diffusion), RaMViD (Random\
      \ Mask Video Diffusion)"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3261988
- Affiliation of the first author: school of mathematics and computer science, panzhihua
    university, panzhihua, china
  Affiliation of the last author: department of computer science and technology, institute
    for artificial intelligence, state key laboratory of intelligent technology and
    systems, thu-bosch jcml center, thbi, idgmcgovern institute for brain research,
    bnrist, tsinghua university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2023\Extracting_Semantic_Knowledge_From_GANs_With_Unsupervised_Learning\figure_1.jpg
  Figure 1 caption: Semantic segmentation obtained by using the average feature of
    each class compared to using a trained linear classifier. The prediction from
    a pretrained DeepLabV3 [11] is regarded as the groundtruth. The linear classifier
    is supervised by a DeepLabV3 network pretrained on CelebAMask-HQ dataset [12]
    according to Xu et al. [7].
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Extracting_Semantic_Knowledge_From_GANs_With_Unsupervised_Learning\figure_2.jpg
  Figure 2 caption: "An illustration of clustering on linearly separable data. Top-left,\
    \ the groundtruth cluster assignments, and the decision boundaries of the linear\
    \ SVM trained on the groundtruth. Other panels, results of different clustering\
    \ algorithms. Decision boundaries are visualized as solid lines, except for AHC\
    \ which has no such boundary. \u201Cx\u201D denotes the centroids of K-means."
  Figure 3 Link: articels_figures_by_rev_year\2023\Extracting_Semantic_Knowledge_From_GANs_With_Unsupervised_Learning\figure_3.jpg
  Figure 3 caption: The pipeline of re-purposing pretrained GANs for downstream tasks,
    UFGS and USCS. First, we propose the K-means with Linear Separability Heuristic
    (KLiSH) to cluster GAN's features. Then, we generate datasets with synthesized
    images and segmentation masks together from GANs. Finally, we train different
    models on the synthesized datasets and perform semantic segmentation on real images
    and semantic-conditional image synthesis. Both applications require no human annotations
    at all.
  Figure 4 Link: articels_figures_by_rev_year\2023\Extracting_Semantic_Knowledge_From_GANs_With_Unsupervised_Learning\figure_4.jpg
  Figure 4 caption: Demonstration of MIoU-permuted labels.
  Figure 5 Link: articels_figures_by_rev_year\2023\Extracting_Semantic_Knowledge_From_GANs_With_Unsupervised_Learning\figure_5.jpg
  Figure 5 caption: "Results of K-means, AHC, KASP, and KLiSH on face images and car\
    \ images. The first row shows the generated images and the other rows show the\
    \ clustering results obtained with different cluster numbers, denoted by the number\
    \ after \u201C\u201D. \u201Cfinal\u201D refers to the selected number of clusters\
    \ for downstream tasks, which is 26, 11, 30, and 26 from left to right, respectively."
  Figure 6 Link: articels_figures_by_rev_year\2023\Extracting_Semantic_Knowledge_From_GANs_With_Unsupervised_Learning\figure_6.jpg
  Figure 6 caption: Demonstration of a merging step. The first two columns show the
    images and clusters. The confidence maps lbrace mathbf skdagger rbrace k=1M of
    the image are visualized (partially) in other columns. lbrace mathbf skdagger
    rbrace k=1M take value within [0, 1] and are visualized using the color bar on
    the right. The cluster in yellow box (say cluster p ) has the smallest IoU ((4))
    with K=35. The cluster in green box (say cluster q ) has the largest ECoS ((5))
    with cluster p . Cluster p and q are then merged in this step.
  Figure 7 Link: articels_figures_by_rev_year\2023\Extracting_Semantic_Knowledge_From_GANs_With_Unsupervised_Learning\figure_7.jpg
  Figure 7 caption: "Comparison of clustering algorithms on various datasets and GANs.\
    \ \u201Cfinal\u201D refers to 18, 9, 16, 15, and 28 from left to right."
  Figure 8 Link: articels_figures_by_rev_year\2023\Extracting_Semantic_Knowledge_From_GANs_With_Unsupervised_Learning\figure_8.jpg
  Figure 8 caption: UFGS results on real images from CelebAMask-HQ (rows 1 and 2)
    and PASCAL VOC (rows 3 and 4).
  Figure 9 Link: articels_figures_by_rev_year\2023\Extracting_Semantic_Knowledge_From_GANs_With_Unsupervised_Learning\figure_9.jpg
  Figure 9 caption: The qualitative results of USCS on various GANs and datasets.
    The semantic masks are drawn by users and the modifications on the masks are indicated
    by dashed boxes. The image on the right of each mask is generated by our USCS
    model.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Jianjin Xu
  Name of the last author: Xiaolin Hu
  Number of Figures: 9
  Number of Tables: 7
  Number of authors: 3
  Paper title: Extracting Semantic Knowledge From GANs With Unsupervised Learning
  Publication Date: 2023-03-27 00:00:00
  Table 1 caption:
    table_text: TABLE I The Semantics Segmentation Performance (Measured in mIoU)
      of Linear Classifier or Centroids of Features. Please See Section IV-B for Details
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE II The Semantic Extraction Performance (In mIoU%) of SVM and\
      \ LSE on Various GANs and Datasets. \u0394 \u0394 Denotes the Relative Difference,\
      \ \u0394= SVM\u2212LSE LSE \u0394=SVM-LSELSE"
  Table 3 caption:
    table_text: "TABLE III Quantitative Evaluation of Clustering Algorithms on Face\
      \ Image Generators. The Final Number of Clusters Used is Indicated by a \u2020\
      \ \u2020. For All Metrics, the Larger the Better. The Clustering Results are\
      \ Reported as the Best One Among the 5 Trials"
  Table 4 caption:
    table_text: TABLE IV The Indices of GAN Layers Selected for Clustering
  Table 5 caption:
    table_text: TABLE V Evaluation on Clustering Algorithms Using Different Layer
      Choices
  Table 6 caption:
    table_text: TABLE VI The Performance of Two UFGS Methods and Several Few-Shot
      Learning Methods
  Table 7 caption:
    table_text: "TABLE VII IoU(%) Per Class of DeepLabV3 Trained on Synthetic Datasets\
      \ Generated With Different Annotation Methods. The Evaluation is Conducted on\
      \ Real Images From the Test Split of CelebAMask-HQ. \u201Ceye-G,\u201D \u201C\
      u-Lip,\u201D \u201Cl-Lip\u201D Denotes \u201Ceyeglasses,\u201D \u201Cupper Lip,\u201D\
      \ and \u201Clower Lip,\u201D Respectively. The Mouth Class is a Union of \u201C\
      teeth,\u201D \u201Cu-Lip,\u201D and \u201Cl-Lip\u201D. Our UFGS Achieved Close\
      \ Performance With One-Shot LSE on Most Classes and Outperformed One-Shot LSE\
      \ on Hard Classes Like \u201Ceyeglasses\u201D and \u201Ccloth\u201D"
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3262140
- Affiliation of the first author: school of computer science and engineering, nanjing
    university of science and technology, nanjing, jiangsu, china
  Affiliation of the last author: school of computer science and engineering, nanjing
    university of science and technology, nanjing, jiangsu, china
  Figure 1 Link: articels_figures_by_rev_year\2023\Progressive_InstanceAware_Feature_Learning_for_Compositional_Action_Recognition\figure_1.jpg
  Figure 1 caption: "Compositional examples in Something-Something [1]. We annotate\
    \ all instances (i.e., hands and objects) by the cyan boxes and highlight the\
    \ major difference by the red dashed boxes or lines in each group of comparisons.\
    \ By observing the appearance changes of objects (e.g., untearable spoon and torn\
    \ paper), humans can easily distinguish between (a) and (b). In contrast, humans\
    \ understand \u201Ccloser to\u201D in (c) and \u201Caway from\u201D in (d) by\
    \ observing the relative displacement (shown as the red dashed line) among instances\
    \ rather than by the objects' appearances. Motivated by this, we aim to fuse different\
    \ types of information to understand compositional actions. Best viewed in color."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Progressive_InstanceAware_Feature_Learning_for_Compositional_Action_Recognition\figure_2.jpg
  Figure 2 caption: "Overview of the proposed framework. It takes T frames sampled\
    \ from a video, the associated position, and identity information (distinguishing\
    \ only \u201Chand\u201D or \u201Cobject\u201D) of each instance as inputs. This\
    \ framework is composed of three steps. Position-aware Appearance Feature Extraction\
    \ (PAFE) (in Section III-B1): pastes a set of boxes on the video to extract instance-centric\
    \ appearance features and concatenate them with position features into hybrid\
    \ features; Identity-aware Feature Interaction (IFI) (in Section III-B2): builds\
    \ pairwise relationships between instance-centric hybrid features with the consideration\
    \ of object identity (only identifying the hand or object). Semantic-aware Position\
    \ Prediction (SPP) (in Section III-B3): recovers part of the high-level information\
    \ from semantic features (output from IFI) by predicting the future position information\
    \ (e.g., coordinate and offset) of each instance. Best viewed in color."
  Figure 3 Link: articels_figures_by_rev_year\2023\Progressive_InstanceAware_Feature_Learning_for_Compositional_Action_Recognition\figure_3.jpg
  Figure 3 caption: Differences between STIN [11] and our approach.
  Figure 4 Link: articels_figures_by_rev_year\2023\Progressive_InstanceAware_Feature_Learning_for_Compositional_Action_Recognition\figure_4.jpg
  Figure 4 caption: Visualization results of Semantic-aware Position Prediction (SPP)
    on Something-Else [11]. Each row contains a sequence of frames uniformly sampled
    from the raw video. The first image in each row shows the overall motion tendency
    of the object. The colored points are the center coordinates of instances. For
    ease of viewing, only one instance's position information is drawn in each sample.
  Figure 5 Link: articels_figures_by_rev_year\2023\Progressive_InstanceAware_Feature_Learning_for_Compositional_Action_Recognition\figure_5.jpg
  Figure 5 caption: '(a): Top-5 categories that our approach exceeds (blue bar) or
    lags behind (orange bar) I3D + STIN [11]. The numbers represent the difference
    between the two models in terms of top-1 accuracy. (b-e): Predictions of I3D [5],
    [6], I3D + STIN [11], and our approach on some examples from the Something-Else
    dataset. All instances are annotated by cyan boxes, and the correct and incorrect
    predictions are highlighted in green and red, respectively. Best viewed in color.'
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.93
  Name of the first author: Rui Yan
  Name of the last author: Jinhui Tang
  Number of Figures: 5
  Number of Tables: 8
  Number of authors: 5
  Paper title: Progressive Instance-Aware Feature Learning for Compositional Action
    Recognition
  Publication Date: 2023-03-27 00:00:00
  Table 1 caption:
    table_text: "TABLE I Results of the Proposed Methods Built on Different Video\
      \ Backbones. The Baseline Method Simply Average-Pools the Feature Maps Extracted\
      \ From Visual Backbones Into Video Features. \u201C Fr\u201D Denotes the Number\
      \ of Frames Used in Models"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE II Results of the Proposed Methods Built With Different Interaction\
      \ Methods. NL \u2217 [16], STRG \u2217 [6], STIN \u2217 [11], and Transformer\
      \ \u2217 [57] are Built on the Proposed Basic Instance-Centric Joint representations\
      \ (extracted by PAFE) for Fair Comparisons"
  Table 3 caption:
    table_text: TABLE III Results of the Proposed Methods Built With Different Temporal
      Fusion Methods
  Table 4 caption:
    table_text: "TABLE IV Effect of Different Position Predictions. \u201CGT\u201D\
      \ and \u201CDET\u201D are Short for Ground-Truth and Detection, Respectively"
  Table 5 caption:
    table_text: "TABLE V Ablation Study on Something-Else With the Compositional Setting.\
      \ \u201CPAFE,\u201D \u201CIFI\u201D and \u201CSPP\u201D are Short for Position-Aware\
      \ Appearance Feature Extraction, Identity-Aware Feature Interaction and Semantic-Aware\
      \ Position Prediction, Respectively. \u201Cglobal Fea.\u201D Represents the\
      \ Appearance Features Extracted From the Whole Frames. \u201CGT\u201D and \u201C\
      DET\u201D are Short for Ground-Truth and Detection, Respectively"
  Table 6 caption:
    table_text: "TABLE VI Compositional Action Recognition on the Something-Else Dataset.\
      \ For a Fair Comparison, the Numbers of Frames Fed Into the Video Backbone are\
      \ Specified. \u201Censemble\u201D Represents Weighted Fusion of the Prediction\
      \ Scores From Two Different Models Trained Separately. All Ensemble Methods\
      \ Fuse the Scores From Different Models Via \u201Cnaive Sum\u201D Unless Stated\
      \ Otherwise"
  Table 7 caption:
    table_text: "TABLE VII Few-Shot Compositional Action Recognition accuracy ( %\
      \ %) on the Something-Else Dataset. \u201C Fr\u201D Denotes the Number of Frames\
      \ Used in Models. \u201Cbase,\u201D \u201C5-S,\u201D and \u201C10-S\u201D Denote\
      \ the Base Set, 5-Shot Set and 10-Shot Set, Respectively. \u201Censemble\u201D\
      \ Indicates That the Model is Combined With I3D in an Ensemble Way. Only top-1\
      \ Accuracy of Models is Reported Here Due to the Limit Space"
  Table 8 caption:
    table_text: "TABLE VIII Compositional Action Recognition accuracy ( % %) on the\
      \ IKEA-Assembly dataset [15]. Macro is Short for Macro-Recall That Averages\
      \ the Recall of Each Class; Micro is Short for Micro-Accuracy Computes the Accuracy\
      \ Globally Without Distinguishing Between Different Classes.\u201Cours\u201D\
      \ Denotes \u201CPAFE + IFI + SPP\u201D"
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3261659
- Affiliation of the first author: guangdong laboratory of machine perception and
    intelligent computing, shenzhen msu-bit university, shenzhen, china
  Affiliation of the last author: department of computer science, city university
    of hong kong, kowloon, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2023\Measuring_Perceptual_Color_Differences_of_Smartphone_Photographs\figure_1.jpg
  Figure 1 caption: Sample pictures of the same natural scene captured by six flagship
    smartphones using the night mode. It is clear that they reproduce similar structural
    details but different color appearances. (a) Apple iPhone 12 Pro. (b) HUAWEI Mate40
    Pro. (c) OnePlus 7 Pro. (d) Samsung S21 Ultra. (e) OPPO Find X3 Pro. (f) Xiaomi
    11 Ultra.
  Figure 10 Link: articels_figures_by_rev_year\2023\Measuring_Perceptual_Color_Differences_of_Smartphone_Photographs\figure_10.jpg
  Figure 10 caption: Reference image recovery. Starting from (b) a Gaussian noise
    image and (h) a tone-altered image of the reference, we recover images by optimizing
    the predicted CD to a reference image, using different DNN-based CD measures.
    (c)-(g) and (i)-(m) are recovered from the Gaussian noise image and the tone-altered
    image of the reference, respectively.
  Figure 2 Link: articels_figures_by_rev_year\2023\Measuring_Perceptual_Color_Differences_of_Smartphone_Photographs\figure_2.jpg
  Figure 2 caption: Major components of the ISP pipeline.
  Figure 3 Link: articels_figures_by_rev_year\2023\Measuring_Perceptual_Color_Differences_of_Smartphone_Photographs\figure_3.jpg
  Figure 3 caption: Representative images from the proposed SPCD dataset. (a) Human
    and animal. (b) Cityscape, landscape, and food. (c) Background complexity. (d)
    Lighting condition. (e) Weather condition. (f) Camera mode.
  Figure 4 Link: articels_figures_by_rev_year\2023\Measuring_Perceptual_Color_Differences_of_Smartphone_Photographs\figure_4.jpg
  Figure 4 caption: Pairwise feature distributions with the corresponding convex hulls
    of SPCD. (a) Brightness against Colorfulness. (b) Contrast against Colorfulness.
  Figure 5 Link: articels_figures_by_rev_year\2023\Measuring_Perceptual_Color_Differences_of_Smartphone_Photographs\figure_5.jpg
  Figure 5 caption: The graphical user interface for subjective testing.
  Figure 6 Link: articels_figures_by_rev_year\2023\Measuring_Perceptual_Color_Differences_of_Smartphone_Photographs\figure_6.jpg
  Figure 6 caption: Empirical distributions of 30,000 perceptual CDs in SPCD.
  Figure 7 Link: articels_figures_by_rev_year\2023\Measuring_Perceptual_Color_Differences_of_Smartphone_Photographs\figure_7.jpg
  Figure 7 caption: "System diagram of CD-Net for perceptual CD assessment. The parameterization\
    \ of convolution is denoted as \u201Cfilter | kernel size | input channel \xD7\
    \ output channel\u201D. The number of trainable parameters for each layer is given\
    \ at the bottom, yielding a total of 14,464."
  Figure 8 Link: articels_figures_by_rev_year\2023\Measuring_Perceptual_Color_Differences_of_Smartphone_Photographs\figure_8.jpg
  Figure 8 caption: Scatter plots of varDelta E against varDelta V .
  Figure 9 Link: articels_figures_by_rev_year\2023\Measuring_Perceptual_Color_Differences_of_Smartphone_Photographs\figure_9.jpg
  Figure 9 caption: Visualization of local CD maps between two photographic images
    (a) and (f), where a warmer color indicates a larger CD between two pixelspatches.
    For CD methods with different scales, we compensate it using (10), followed by
    linear scaling to the range of [0, 255] .
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.97
  Name of the first author: Zhihua Wang
  Name of the last author: Kede Ma
  Number of Figures: 10
  Number of Tables: 8
  Number of authors: 8
  Paper title: Measuring Perceptual Color Differences of Smartphone Photographs
  Publication Date: 2023-03-28 00:00:00
  Table 1 caption:
    table_text: TABLE I The Conversion Between the Grayscale Grade Levels and the
      Perceptual CDs, Measured by a Tele-Spectroradiometer and Predicted by (1)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Min, Max, Median and Mean STRESS, SRCC, and PLCC Between
      Two Randomized Subgroups With Equal Size Across 100 Splits
  Table 3 caption:
    table_text: "TABLE III STRESS, SRCC, and PLCC Between Predicted CDs ( \u0394E\
      \ \u0394E) and Perceptual CDs ( \u0394V \u0394V) in SPCD. The Top Section Lists\
      \ Representative CD Formulae Developed From Homogeneous Color Patches. The Second\
      \ Section Contains CD Measures Adapted for Natural Photographic Images. The\
      \ Third Section Includes General-Purpose Image Quality Models. The Fourth Section\
      \ Consists of JND Measures. The Fifth Section Gives Trained DNNs With Frequently\
      \ Used Backbones as Reference. The Top Two Methods are Highlighted in Boldface"
  Table 4 caption:
    table_text: TABLE IV Ablation Analysis of the Front-End Filter Bank
  Table 5 caption:
    table_text: TABLE V Ablation Analysis of the Last Convolution Channel Number
  Table 6 caption:
    table_text: TABLE VI Ablation Analysis of the Image Size Discrepancy During Training
      and Testing
  Table 7 caption:
    table_text: 'TABLE VII Generalizability Evaluation on the TID2013 Subset, Which
      Contains Three Types of Color-Related Distortions: Quantization Noise, Image
      Color Quantization With Dither, and Chromatic Aberration'
  Table 8 caption:
    table_text: "TABLE VIII Generalizability Evaluation on the COM Dataset and Its\
      \ Four Sub-Datasets: BFD-P, Leeds, Witt, and RIT-DuPont. The Top Two Methods\
      \ Targeted for Natural Photographic Images are Highlighted in Boldface. PLCC\
      \ on RIT-DuPont is Not Computable, Thus Indicated by \u201C\u2014\u201D"
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3262424
- Affiliation of the first author: reler, ccai, zhejiang university, hangzhou, zhejiang,
    china
  Affiliation of the last author: reler, ccai, zhejiang university, hangzhou, zhejiang,
    china
  Figure 1 Link: articels_figures_by_rev_year\2023\LocalGlobal_Context_Aware_Transformer_for_LanguageGuided_Video_Segmentation\figure_1.jpg
  Figure 1 caption: Our main idea. Previous LVS models are mainly built upon 3D CNNs,
    bounded by the local observations from short durations. They thus fail to identify
    the target referent in the early stage beyond the 'fall' event. In contrast, Locater
    is fully aware of both local and global context, enabling a holistic understanding
    of entire video content and correctly grounding the phrase over the whole video.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\LocalGlobal_Context_Aware_Transformer_for_LanguageGuided_Video_Segmentation\figure_2.jpg
  Figure 2 caption: Illustration of our Locater. Building upon Transformer encoder-decoder
    architecture, Locater maintains a finite memory to collect and retain both global
    and local temporal context. By referencing the memory, Locater can reach a holistic
    understanding of video content and query the entire video with the expression,
    with linear computation complexity and constant storage consumption. See Section
    III-B for details.
  Figure 3 Link: articels_figures_by_rev_year\2023\LocalGlobal_Context_Aware_Transformer_for_LanguageGuided_Video_Segmentation\figure_3.jpg
  Figure 3 caption: Attention visualization of frame-specific queries (10).
  Figure 4 Link: articels_figures_by_rev_year\2023\LocalGlobal_Context_Aware_Transformer_for_LanguageGuided_Video_Segmentation\figure_4.jpg
  Figure 4 caption: Representative video examples from our proposed (a) A2D-S +textM
    , (b) A2D-S +textS , and (c) A2D-S +,!textT datasets (Section IV-A).
  Figure 5 Link: articels_figures_by_rev_year\2023\LocalGlobal_Context_Aware_Transformer_for_LanguageGuided_Video_Segmentation\figure_5.jpg
  Figure 5 caption: Visual comparison results (Section V-D) on A2D-S test [2] (left)
    and R-YTVOS val [4] (right). Referents and corresponding descriptions are highlighted
    in the same colour. In right column, we show qualitative results of CSTM [16]
    (1st row) and Locater (2nd row).
  Figure 6 Link: articels_figures_by_rev_year\2023\LocalGlobal_Context_Aware_Transformer_for_LanguageGuided_Video_Segmentation\figure_6.jpg
  Figure 6 caption: Typical failure cases on A2D-S test. See Section V-H for more
    details.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.66
  Name of the first author: Chen Liang
  Name of the last author: Yi Yang
  Number of Figures: 6
  Number of Tables: 10
  Number of authors: 6
  Paper title: Local-Global Context Aware Transformer for Language-Guided Video Segmentation
  Publication Date: 2023-03-28 00:00:00
  Table 1 caption:
    table_text: TABLE I An Example of Applying Semantic-Role Labeling to the Video
      Description for the Construction of A2D-S + S S+ and A2D-S + T T+. See Section
      IV-A for Details
  Table 10 caption:
    table_text: TABLE X A Set of Ablation Studies (Section V-G) on A2D-S test and
      A2D-S + +. We Report the Average Score on All Three Subsets of A2D-S + +
  Table 2 caption:
    table_text: TABLE II Dataset Statistics of A2D-S test and Our A2D-S + M,S,T M,S,T+
      (Section IV-B)
  Table 3 caption:
    table_text: 'TABLE III Quantitative Results on A2D-S test [2] (Section V-A). (:
      ViT-B With First 6 Layers as Backbone. Same for Other Tables)'
  Table 4 caption:
    table_text: TABLE IV Quantitative Results on J-HMDB-S [2] (Section V-B)
  Table 5 caption:
    table_text: TABLE V Quantitative Results on R-YTVOS val [4] (Section V-C)
  Table 6 caption:
    table_text: TABLE VI Quantitative Results on A2D-S + A2D-S+. Locater IMG IMG Refers
      to Locater Without Using Memory (Section V-E)
  Table 7 caption:
    table_text: TABLE VII Quantitative Results With Different Number of Objects. Results
      are Reported in Terms of mIoU. Locater IMG IMG Refers to Locater Without Using
      Memory (Section V-E)
  Table 8 caption:
    table_text: TABLE VIII Benchmarking Results on test-Dev Set of RVOS Track in YTB-VOS
      21 21 challenge [14] (Section V-F)
  Table 9 caption:
    table_text: TABLE IX Benchmarking Results on test-Challenge set2 of RVOS Track
      in YTB-VOS 21 21 challenge [14] (Section V-F)
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3262578
- Affiliation of the first author: department of computer science, purdue university,
    west lafayette, in, usa
  Affiliation of the last author: oppo research, seattle, wa, usa
  Figure 1 Link: articels_figures_by_rev_year\2023\CaCo_Both_Positive_and_Negative_Samples_are_Directly_Learnable_via_CooperativeAd\figure_1.jpg
  Figure 1 caption: "This figure compares existing contrastive learning methods \u2013\
    \ MoCo and AdCo \u2013 with the proposed CaCo. Both MoCo and AdCo only has negative\
    \ samples from the memory bank, and MoCo queues these samples from the encoder\
    \ output over past minibatches without training them directly. In contrast, given\
    \ a query anchor mathbf z , CaCo directly learns the cooperative positive mathbf\
    \ bj+ and adversarial negatives mathbf bj, jne j+ from the shared memory bank\
    \ through backward gradients ( mathrmgrad )."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\CaCo_Both_Positive_and_Negative_Samples_are_Directly_Learnable_via_CooperativeAd\figure_2.jpg
  Figure 2 caption: The diagram of CaCo. CaCo trains a visual representation encoder
    and a cooperative-adversarial memory bank at the same time. For a encoded query
    q , we used its counterpart key k by the key encoder to identify its most probable
    positive out of memory bank as its positive pair, while all other embeddings in
    the memory bank are its negative pairs. Then the network is optimized by minimizing
    the contrastive loss, the memory bank is coopertive-adversarial optimized as discussed
    in the paper.
  Figure 3 Link: articels_figures_by_rev_year\2023\CaCo_Both_Positive_and_Negative_Samples_are_Directly_Learnable_via_CooperativeAd\figure_3.jpg
  Figure 3 caption: Analysis of different initialization methods for CaCo. The first
    panel of each subfigure is the plot of Mean Maximum Positive Probability and NN
    accuracy (based on 20% of training set) over 200 epochs of pretraining under different
    settings. The second panel shows the percentage of category-space change in the
    memory bank over 200 epochs of pretraining. The third, forth and fifth panel show
    the histogram of category-space of the memory bank for epoch 1, epoch 10 and epoch
    200, respectively. The first row shows the default AdCo initialization and the
    second row shows the random embedding initialization.
  Figure 4 Link: articels_figures_by_rev_year\2023\CaCo_Both_Positive_and_Negative_Samples_are_Directly_Learnable_via_CooperativeAd\figure_4.jpg
  Figure 4 caption: Analysis of CaCo performances on ImageNet-1 K and ImageNet-100
    datasets, both based on AdCo initialization. The first panel of each subfigure
    is the plot of Mean Maximum Positive Probability and NN accuracy (based on 20%
    of training set) over 200 epochs of pretraining under different settings. The
    second panel shows the percentage of category-space change in the memory bank
    over 200 epochs of pretraining. The third, forth and fifth panel show the histogram
    of classes in the memory bank for epoch 1, epoch 10 and epoch 200, respectively.
  Figure 5 Link: articels_figures_by_rev_year\2023\CaCo_Both_Positive_and_Negative_Samples_are_Directly_Learnable_via_CooperativeAd\figure_5.jpg
  Figure 5 caption: The surrogate images of the Most Probable Positive (MPP) and random
    negatives from the learned memory bank for some query examples.
  Figure 6 Link: articels_figures_by_rev_year\2023\CaCo_Both_Positive_and_Negative_Samples_are_Directly_Learnable_via_CooperativeAd\figure_6.jpg
  Figure 6 caption: t-SNE visualization of learned representations on CIFAR10, PETS
    and VOC2017 datasets, where different colors represent different classes on these
    datasets. The encoder is pre-trained by CaCo over 800 epochs on Imagenet1K.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Xiao Wang
  Name of the last author: Guo-Jun Qi
  Number of Figures: 6
  Number of Tables: 11
  Number of authors: 4
  Paper title: 'CaCo: Both Positive and Negative Samples are Directly Learnable via
    Cooperative-Adversarial Contrastive Learning'
  Publication Date: 2023-03-28 00:00:00
  Table 1 caption:
    table_text: TABLE I Top-1 Accuracy Under the Linear Evaluation on Imagenet1K Dataset
      With the ResNet-50 Backbone. All Compared Methods Use Single-Crop Augmentations
      Pre-Trained Over 200 Epochs
  Table 10 caption:
    table_text: TABLE X Top-1 Accuracy Under the Linear Evaluation on Imagenet1K With
      the Pretrained ResNet-50 Backbone. The Table Compares Different Methods (MoCo-V2
      Versus CaCo) Using Different Types of Augmentations When Being Pre-Trained Over
      200 Epochs
  Table 2 caption:
    table_text: TABLE II Top-1 Accuracy Under the Linear Evaluation on Imagenet1K
      With the ResNet-50 Backbone. The Table Compares the Methods Using a Single Crop
      Augmentation Pre-Trained With More Epochs
  Table 3 caption:
    table_text: TABLE III Top-1 Accuracy Under the Linear Evaluation on Imagenet1K
      With the ResNet-50 Backbone. The Table Compares the Methods Using Same Multi-Crop
      Augmentation in SwAV [23] Pre-Trained With More Epochs
  Table 4 caption:
    table_text: TABLE IV Running Time Comparison of Different Methods. GPU timeEpoch
      Means Sum of Time Cost of All GPUs to Train an Epoch. Here the Time are Measured
      Under the Asymmetrical Settings to Have Fair Comparison With Early Methods
  Table 5 caption:
    table_text: TABLE V Top-1 Accuracy Under the Linear Evaluation on ImageNet100
      With the ResNet-50 Backbone. The Table Compares Different Self-Supervised Learning
      Methods Pre-Trained With 200 Epochs
  Table 6 caption:
    table_text: "TABLE VI Transfer Learning Results on Various Datasets With ResNet-50\
      \ Pretrained With Imagenet1K Over 800 Epochs. The Results are Obtained With\
      \ \u2020 \u2020 https:github.comfacebookresearchmoco Under the CC-BY-NC 4.0\
      \ License and \u2021 \u2021 https:github.commaple-Research-LabAdCo Under MIT\
      \ License, Other Results Were Reported Directly in the Other Papers"
  Table 7 caption:
    table_text: TABLE VII Transfer Learning Results on Object Detection Tasks
  Table 8 caption:
    table_text: 'TABLE VIII Top-1 Accuracy Under the Linear Evaluation on ImageNet
      With the ResNet-50 Backbone. The Table Compares the Methods Over 200 Epochs
      of Pretraining. Parameters: The Parameter Counts of Negative Samples, as to
      BYOL and SimSiam, It Means the Parameter Counts of the Predictor'
  Table 9 caption:
    table_text: TABLE IX Top-1 Accuracy Under the Linear Evaluation on ImageNet With
      the ResNet-50 Backbone. The Table Compares Different Settings Over 200 Epochs
      of Pretraining. Here None Setting Denotes Both Positive and Negative are From
      Current Mini-Batch; Positive Setting Denotes the Positive Embedding is From
      Memory Bank and the Negative Embedding is From the Current Mini-Batch; Negative
      Setting Suggests the Positive Embedding is From Current Mini-Batch and the Negative
      Embedding is From the Memory Bank; P+n is Our CaCo Setting, Where Both Positive
      and Negative are From the Memory Bank. Here the positive Pool and negative Pool
      Suggests the Total Number of Embeddings That We Selected From to Serve as Positive
      and Negatives, Respectively
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3262608
- Affiliation of the first author: school of information science and technology, fudan
    university, shanghai, china
  Affiliation of the last author: school of information science and technology, fudan
    university, shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2023\PerformanceAware_Approximation_of_Global_Channel_Pruning_for_Multitask_CNNs\figure_1.jpg
  Figure 1 caption: Visualization of the co-influence among different filters during
    the pruning process. We take a single-stage detector VGG16-SSD [14] as an example,
    and use the Grad-CAM technology [15] to generate the attention map of each filter
    in SSD. The upper and lower rows respectively illustrate the attention maps of
    filters in layer (a) conv32, (b) conv41, (c) extras.3, and (d) extras.5 before
    and after several pruning steps. Column (a) shows the response decrease after
    pruning other filters, while the other three columns show the response increase
    after pruning other filters.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\PerformanceAware_Approximation_of_Global_Channel_Pruning_for_Multitask_CNNs\figure_2.jpg
  Figure 2 caption: The distinction between different pruning strategies in a single
    pruning iteration. The red dashed circle denotes the set of filters to be pruned
    at each iteration. (a) A general multitask model, in which each filter has different
    sensitivity to different tasks. The red circle refers to the filter that is most
    sensitive to Task 1, and the green circle refers to the filter that is most sensitive
    to Task 2. (b) Layer-wise channel pruning, which evaluates and prunes filters
    in a single layer and fine-tunes the model at each iteration. The filters in the
    gray area are not considered in the current pruning iteration. (c) Previous global
    channel pruning, which evaluates and prunes filters across all layers based on
    static filters' relations at each pruning iteration (represented in the unchanged
    colors of filters). (d) Our PAGCP method, which evaluates and prunes the filters
    in a single layer dynamically based on the previously pruned structure, and globally
    considers both intra- and inter-layer filter relations for the compression performance
    of multitask models (represented in the color change of filters). The layer pruning
    sequence is determined by each layer's contribution to the total FLOPs reduction.
  Figure 3 Link: articels_figures_by_rev_year\2023\PerformanceAware_Approximation_of_Global_Channel_Pruning_for_Multitask_CNNs\figure_3.jpg
  Figure 3 caption: The overview of the proposed Performance-Aware Global Channel
    Pruning (PAGCP) framework. Given an original well-trained multitask model, we
    sort all target layers in a new sequence based on each layer's contribution to
    total FLOPs reduction (computed by subtracting the FLOPs of the pruned model from
    the FLOPs of the original model), and compress each layer in such sequence. Specially,
    the original model produces an initial loss boldsymbolmathcal L(0) for the estimation
    of pruning ratio in lF1 . Then at each step i , we select the task with largest
    performance drop when masking gamma filters in lFi as the most sensitive task
    for the target filters to be pruned. In the compressor, based on the selected
    task and the saliency criterion, we maximize the compression ratio of lFi under
    local constraints with boldsymbolmathcal L(i-1) generated from step i-1 as a reference.
    The compressor outputs a list of the pruned filters and updates boldsymbolmathcal
    L(i) for step i+1 . After all layers are compressed, we reorder the pruned layers
    by evaluating their compression contributions again, and compress the top P layers
    with the highest pruning ratios. Finally, we retrain the pruned model and repeat
    the above procedure until the reduction requirement of FLOPs or parameters is
    satisfied.
  Figure 4 Link: articels_figures_by_rev_year\2023\PerformanceAware_Approximation_of_Global_Channel_Pruning_for_Multitask_CNNs\figure_4.jpg
  Figure 4 caption: The pruning scheme of skip connections in our method. Filters
    with the same index across different layers share the same group. Input FM1 and
    FM2 generated by different layers denote the general input feature maps of the
    skip connection layer. Filters of Group 1 and Group 3 are finally removed in this
    figure, as well as the corresponding channels of output feature maps FM3 .
  Figure 5 Link: articels_figures_by_rev_year\2023\PerformanceAware_Approximation_of_Global_Channel_Pruning_for_Multitask_CNNs\figure_5.jpg
  Figure 5 caption: Hyper-parameter analysis. Column 1 figure denotes the study on
    the pruning ratio during the initialization of the pruning sequence. Column 2
    figure shows the study on the masking ratio during the pruning in each layer.
    Column 3 figure presents the study on the global performance drop alpha . Column
    4 shows the study on the initial drop threshold d1 and the filtering ratio of
    pruning layers P . Except for Column 4 figure, where all schemes iteratively prune
    SSD300 with 70% FLOPs reduction, the rest experiments in the other three charts
    are conducted by pruning SSD300 only once.
  Figure 6 Link: articels_figures_by_rev_year\2023\PerformanceAware_Approximation_of_Global_Channel_Pruning_for_Multitask_CNNs\figure_6.jpg
  Figure 6 caption: Widths of layers in the pruned SSD300 at each pruning iteration.
  Figure 7 Link: articels_figures_by_rev_year\2023\PerformanceAware_Approximation_of_Global_Channel_Pruning_for_Multitask_CNNs\figure_7.jpg
  Figure 7 caption: The sensitivity in each layer of SSD300 with two tasks for the
    model. The vertical and horizontal axis denote the layer sequence and the pruning
    ratio, respectively. Bars with different colors represent different task-sensitive
    groups.
  Figure 8 Link: articels_figures_by_rev_year\2023\PerformanceAware_Approximation_of_Global_Channel_Pruning_for_Multitask_CNNs\figure_8.jpg
  Figure 8 caption: The detection results of the pruned YOLOv5m on COCO2017, where
    different colors of bounding boxes refer to different classes and the number on
    the top left of each bounding box refers to the prediction confidence.
  Figure 9 Link: articels_figures_by_rev_year\2023\PerformanceAware_Approximation_of_Global_Channel_Pruning_for_Multitask_CNNs\figure_9.jpg
  Figure 9 caption: The multitask prediction results of the pruned MTI-Net on NYUD-v2,
    where Row 1 and 3 are the semantic segmentation maps and Row 2 and 4 are the corresponding
    depth maps.
  First author gender probability: 0.54
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Hancheng Ye
  Name of the last author: Bin Wang
  Number of Figures: 9
  Number of Tables: 10
  Number of authors: 5
  Paper title: Performance-Aware Approximation of Global Channel Pruning for Multitask
    CNNs
  Publication Date: 2023-03-28 00:00:00
  Table 1 caption:
    table_text: TABLE I Tasks Choices for NUYD-V2 and PASCAL Context, Where Seg.,
      Dep., Sal., and Norm., Refer to Semantic Segmentation, Depth Estimation, Saliency
      Estimation, and Surface Normal Estimation, Respectively
  Table 10 caption:
    table_text: TABLE X Acceleration of the Inference Latency (msimage) for Various
      Pruned Models on Both Cloud and Mobile Platforms. Models Suffixed With 'p' are
      Pruned by PAGCP
  Table 2 caption:
    table_text: "TABLE II Compression Results on PASCAL VOC. params Denotes the Parameter\
      \ Numbers. \u201C \u2663 \u2663\u201D Denotes the Model is Trained on VOC07\
      \ Trainval Dataset. \u2193 \u2193 Denotes the Absolute Decrease of the Metric,\
      \ and \u2193 \u2193(%) Denotes the Relative Decrease (%) of the Metric"
  Table 3 caption:
    table_text: "TABLE III Compression Results on COCO. \u201C \u2020 \u2020\u201D\
      \ Denotes the Model Input Size is 300\xD7300 Pixels"
  Table 4 caption:
    table_text: TABLE IV Compression Results of SSD300 on PASCAL VOC At Each Iteration
  Table 5 caption:
    table_text: TABLE V Compression Results on Conventional Multitask Learning Benchmarks.
      F. And P. Denotes FLOPs and Parameters, Respectively. S2Mseg, S2Mdep, S2Msal
      and S2Mnorm Denote the Compressed MTI-Net Models Pruned by PAGCP on the Semantic
      Segmentation, Depth Estimation, Saliency Estimation and Surface Normal Estimation,
      Respectively, and Then Finetuned on the Multitask Setting
  Table 6 caption:
    table_text: "TABLE VI Compression Results of SSD300 Based on Four Algorithms.\
      \ \u201C \u2020 \u2020\u201D Denotes the Results From Several Iterations of\
      \ Pruning. \u201C \u2663 \u2663\u201D Denotes the Result From One Pruning Iteration"
  Table 7 caption:
    table_text: TABLE VII Compression Results of SSD300 Based on Four Performance
      Drop Constraints With One Pruning Iteration
  Table 8 caption:
    table_text: "TABLE VIII The Study of the Performance-Aware Criterion on NYUD-V2.\
      \ The \u201Cinit. Pred.\u201D Represents the Output Set of the Corresponding\
      \ Task At the Initial Stage. The \u201Cfinal Pred.\u201D Refers to the Final\
      \ Output At the Final Stage"
  Table 9 caption:
    table_text: TABLE IX Compression Results of ResNet50 on ImageNet
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3260903
- Affiliation of the first author: school of software, shandong university, zibo,
    shandong province, china
  Affiliation of the last author: school of software, shandong university, zibo, shandong
    province, china
  Figure 1 Link: articels_figures_by_rev_year\2023\MissingnessPatternAdaptive_Learning_With_Incomplete_Data\figure_1.jpg
  Figure 1 caption: When all features (x,y,z) are observable, we have an optimal separating
    plane in (a). When only (x,y) are observable, the best separating line is the
    solid line in (b). The projection of optimal separating plane in (b) is the dashed
    line. If we train one model for both cases, we will probably end with a compromise
    of them and get an inferior result.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\MissingnessPatternAdaptive_Learning_With_Incomplete_Data\figure_2.jpg
  Figure 2 caption: The flowchart of our proposed method. In the learning process,
    given a set of samples with different missingness patterns, we provide a dictionary
    H for generating missingness pattern-specific functions. We then restrict H with
    a low-rank constraint that introduces correlations between models for different
    missingness patterns. After a rigorous generalization error bound analysis, we
    apply our method into both linear and non-linear models with efficient training
    process.
  Figure 3 Link: articels_figures_by_rev_year\2023\MissingnessPatternAdaptive_Learning_With_Incomplete_Data\figure_3.jpg
  Figure 3 caption: Examples for mathbf x , mathbf m and barmathbf m .
  Figure 4 Link: articels_figures_by_rev_year\2023\MissingnessPatternAdaptive_Learning_With_Incomplete_Data\figure_4.jpg
  Figure 4 caption: The average accuracy.
  Figure 5 Link: articels_figures_by_rev_year\2023\MissingnessPatternAdaptive_Learning_With_Incomplete_Data\figure_5.jpg
  Figure 5 caption: Effect of parameters on dataset bands.
  Figure 6 Link: articels_figures_by_rev_year\2023\MissingnessPatternAdaptive_Learning_With_Incomplete_Data\figure_6.jpg
  Figure 6 caption: Effect of parameters on dataset horse.
  Figure 7 Link: articels_figures_by_rev_year\2023\MissingnessPatternAdaptive_Learning_With_Incomplete_Data\figure_7.jpg
  Figure 7 caption: Convergence rate.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.8
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.96
  Name of the first author: Yongshun Gong
  Name of the last author: Yilong Yin
  Number of Figures: 7
  Number of Tables: 7
  Number of authors: 7
  Paper title: Missingness-Pattern-Adaptive Learning With Incomplete Data
  Publication Date: 2023-03-29 00:00:00
  Table 1 caption:
    table_text: TABLE I Symbol Description
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Summary of Datasets
  Table 3 caption:
    table_text: "TABLE III Classification Accuracy (mean \xB1 std) With Original Datasets.\
      \ The Best Results are Bold and the Second Best are Underlined"
  Table 4 caption:
    table_text: "TABLE IV Classification Accuracy (mean \xB1 std) on Datasets With\
      \ Additional Missing Values. The Best Results are Bold and the Second Best are\
      \ Underlined"
  Table 5 caption:
    table_text: "TABLE V Classification Accuracy (mean \xB1 std) on Sensorless Drive\
      \ Diagnosis Dataset. The Best Results are Bold and the Second Best are Underlined"
  Table 6 caption:
    table_text: "TABLE VI Classification Accuracy (mean \xB1 std) on MNIST Dataset.\
      \ The Best Results are Bold and the Second Best are Underlined"
  Table 7 caption:
    table_text: "TABLE VII Classification Accuracy (mean \xB1 std) on Avila Dataset.\
      \ The Best Results are Bold and the Second Best are Underlined"
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3262784
- Affiliation of the first author: s-lab and school of computer science and engineering,
    nanyang technological university, singapore
  Affiliation of the last author: school of computer science and engineering, nanyang
    technological university, singapore
  Figure 1 Link: articels_figures_by_rev_year\2023\Automatic_Transformation_Search_Against_Deep_Leakage_From_Gradients\figure_1.jpg
  Figure 1 caption: Reconstruction attacks (a) and our proposed approach (b) in collaborative
    learning systems.
  Figure 10 Link: articels_figures_by_rev_year\2023\Automatic_Transformation_Search_Against_Deep_Leakage_From_Gradients\figure_10.jpg
  Figure 10 caption: Visual results and the PSNR values of the reconstruction attacks
    with the hybrid policies chosen using the privacy scores without (Row 1&2) and
    with variance (Row 3&4).
  Figure 2 Link: articels_figures_by_rev_year\2023\Automatic_Transformation_Search_Against_Deep_Leakage_From_Gradients\figure_2.jpg
  Figure 2 caption: Pipeline to identify optimal transformation policies using our
    privacy and accuracy scores.
  Figure 3 Link: articels_figures_by_rev_year\2023\Automatic_Transformation_Search_Against_Deep_Leakage_From_Gradients\figure_3.jpg
  Figure 3 caption: Visualization of calculating Spri .
  Figure 4 Link: articels_figures_by_rev_year\2023\Automatic_Transformation_Search_Against_Deep_Leakage_From_Gradients\figure_4.jpg
  Figure 4 caption: The reconstruction processes of different transformations used
    in Fig. 3.
  Figure 5 Link: articels_figures_by_rev_year\2023\Automatic_Transformation_Search_Against_Deep_Leakage_From_Gradients\figure_5.jpg
  Figure 5 caption: Correlation between PSNR and Spri .
  Figure 6 Link: articels_figures_by_rev_year\2023\Automatic_Transformation_Search_Against_Deep_Leakage_From_Gradients\figure_6.jpg
  Figure 6 caption: Spri-Sacc distributes of different numbers of policies. The numbers
    of policies for (a), (b), (c) are 500, 1500, and 5000 respectively. We observe
    that the privacy score range of 1500 policies is the same as that of 5000. Therefore,
    we set 1500 as the policy search space in our experiment.
  Figure 7 Link: articels_figures_by_rev_year\2023\Automatic_Transformation_Search_Against_Deep_Leakage_From_Gradients\figure_7.jpg
  Figure 7 caption: Model performance of ResNet20 on CIFAR100 during the training
    process.
  Figure 8 Link: articels_figures_by_rev_year\2023\Automatic_Transformation_Search_Against_Deep_Leakage_From_Gradients\figure_8.jpg
  Figure 8 caption: 'Visual results and the PSNR values of the reconstruction attacks
    [13] with and without our defense on CIFAR100. Row 1: clean samples, Row 2: reconstructed
    samples without transformation, Row 3: transformed samples, Row 4: reconstructed
    samples with transformation. The adopted transformations are the corresponding
    Hybrid policies in Table II.'
  Figure 9 Link: articels_figures_by_rev_year\2023\Automatic_Transformation_Search_Against_Deep_Leakage_From_Gradients\figure_9.jpg
  Figure 9 caption: Visual results and the PSNR values of the reconstruction attacks
    [13] with and without our defense on F-MNIST.
  First author gender probability: 0.95
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.66
  Name of the first author: Wei Gao
  Name of the last author: Yang Liu
  Number of Figures: 17
  Number of Tables: 10
  Number of authors: 8
  Paper title: Automatic Transformation Search Against Deep Leakage From Gradients
  Publication Date: 2023-03-29 00:00:00
  Table 1 caption:
    table_text: TABLE I Summary of the 50 Transformations
  Table 10 caption:
    table_text: TABLE X PSNR (db) of Different Initializations for Each Architecture
      on CIFAR100
  Table 2 caption:
    table_text: TABLE II PSNR (db) and Model Accuracy (%) of Different Transformation
      Configurations for Each Architecture and Dataset
  Table 3 caption:
    table_text: TABLE III The PSNR Values (db) Between the Reconstructed and Transformed
      Images Under Different Attacks
  Table 4 caption:
    table_text: TABLE IV Comparisons With Existing Defense Methods Under the Adam+cosine
      Attack
  Table 5 caption:
    table_text: TABLE V Comparisons With Existing Dataset Condensation Methods
  Table 6 caption:
    table_text: TABLE VI Comparisons Between Policies Chosen from [15] and the Variance
      Integrated Algorithm on CIFAR100 and F-MNIST With ResNet20
  Table 7 caption:
    table_text: TABLE VII Comparisons With Top 3 Augmentations
  Table 8 caption:
    table_text: TABLE VIII PSNR (db) and Model Accuracy (%) of Different Transformation
      Policies With Different Architectures and Datasets
  Table 9 caption:
    table_text: 'TABLE IX Transferability Results: Applying the Same Policies From
      CIFAR100 to F-MNIST and ImageNet'
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3262813
