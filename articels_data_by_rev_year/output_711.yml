- Affiliation of the first author: university of bretagne sud, irisa laboratory, rennes,
    france
  Affiliation of the last author: normandie university, ur, litis, saint-etienne-du-rouvray,
    france
  Figure 1 Link: articels_figures_by_rev_year\2016\Optimal_Transport_for_Domain_Adaptation\figure_1.jpg
  Figure 1 caption: "Illustration of the proposed approach for domain adaptation.\
    \ (left) Dataset for training, i.e., source domain, and testing, i.e., target\
    \ domain. Note that a classifier estimated on the training examples clearly does\
    \ not fit the target data. (middle) A data dependent transportation map T \u03B3\
    0 is estimated and used to transport the training samples onto the target domain.\
    \ Note that this transformation is usually non linear. (right) The transported\
    \ labeled samples are used for estimating a classifier in the target domain."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Optimal_Transport_for_Domain_Adaptation\figure_2.jpg
  Figure 2 caption: "Illustration of the optimal transport problem. (a) Monge problem\
    \ over 2D domains. T is a push-forward from \u03A9 s to \u03A9 t . (b) Kantorovich\
    \ relaxation over 1D domains: \u03B3 can be seen as a joint probability distribution\
    \ with marginals \u03BC s and \u03BC t . (c) Illustration of the solution of the\
    \ Kantorovich relaxation computed between two ellipsoidal distributions in 2D.\
    \ The grey line between two points indicate a non-zero coupling between them."
  Figure 3 Link: articels_figures_by_rev_year\2016\Optimal_Transport_for_Domain_Adaptation\figure_3.jpg
  Figure 3 caption: Illustration of the classification decision boundary produced
    by OT-Laplace over the two moons example for increasing rotation angles. The source
    domain is represented as coloured points. The target domain is depicted as points
    in grey (best viewed with colors).
  Figure 4 Link: articels_figures_by_rev_year\2016\Optimal_Transport_for_Domain_Adaptation\figure_4.jpg
  Figure 4 caption: Examples from the datasets used in the visual adaptation experiment.
    five random samples from one class are given for all the considered domains.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Nicolas Courty
  Name of the last author: Alain Rakotomamonjy
  Number of Figures: 4
  Number of Tables: 5
  Number of authors: 4
  Paper title: Optimal Transport for Domain Adaptation
  Publication Date: 2016-10-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Mean Error Rate over 10 Realizations for the Two Moons Simulated
      Example
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Summary of the Domains Used in the Visual Adaptation Experiment
  Table 3 caption:
    table_text: TABLE 3 Overall Recognition Accuracies in Percent Obtained over All
      Domains Pairs Using the SURF Features
  Table 4 caption:
    table_text: TABLE 4 Results of Adaptation by Optimal Transport Using DeCAF Features
  Table 5 caption:
    table_text: TABLE 5 Results of Semi-Supervised Adaptation with Optimal Transport
      Using the SURF Features
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2615921
- Affiliation of the first author: advanced digital sciences center, singapore
  Affiliation of the last author: university of illinois, urbana-champaign, il
  Figure 1 Link: articels_figures_by_rev_year\2016\PatchMatch_Filter_EdgeAware_Filtering_Meets_Randomized_Search_for_Visual_Corresp\figure_1.jpg
  Figure 1 caption: Problems with PatchMatch [6] and CostFilter [34] for correspondence
    field estimation. (a,b) Input images. (c) ANNF of PatchMatch (with the same color
    coding for optical flow). (d) Ground-truth flow [1]. (e) Flow map of CostFilter
    [34]. (f) Flow map of our PMF method, running 10-times faster than [34] under
    fair settings. Average endpoint error of (e) 0.0837 and (f) 0.0825.
  Figure 10 Link: articels_figures_by_rev_year\2016\PatchMatch_Filter_EdgeAware_Filtering_Meets_Randomized_Search_for_Visual_Corresp\figure_10.jpg
  Figure 10 caption: Advantages of our improved search strategies proposed in Secttion
    4.3. a) Better initialization. b) Non-local neighbor propagation ( iteration =
    3).
  Figure 2 Link: articels_figures_by_rev_year\2016\PatchMatch_Filter_EdgeAware_Filtering_Meets_Randomized_Search_for_Visual_Corresp\figure_2.jpg
  Figure 2 caption: (a) SLIC superpixels of approximate size 64, 256 and 1024 pixels.
    Fig. courtesy from [3]. (b) Bounding-box B( c k ) containing the superpixel S(k)
    centered at pixel c k and r -pixel extended subimage R( c k ) .
  Figure 3 Link: articels_figures_by_rev_year\2016\PatchMatch_Filter_EdgeAware_Filtering_Meets_Randomized_Search_for_Visual_Corresp\figure_3.jpg
  Figure 3 caption: 'Generalized affinity graph and improved strategies: superpixel-induced
    enrichment and initialization.'
  Figure 4 Link: articels_figures_by_rev_year\2016\PatchMatch_Filter_EdgeAware_Filtering_Meets_Randomized_Search_for_Visual_Corresp\figure_4.jpg
  Figure 4 caption: "Strength of the cross-scale consistency constraint in matching\
    \ large low-textured regions. (a, b) Input Baby2 stereo image pair. (c) Ground-truth\
    \ depth map. (d, e) Depth map and error map of the PMF algorithm (without post-processing).\
    \ (f) Depth map of the PMBP method [8] with a strong regularization weight b (When\
    \ b=0 , the resulting PM-stereo method [9] struggles with the low-textured regions.).\
    \ (g, h) Depth map and error map of the fPMF algorithm (without post-processing).\
    \ (i) The binary classification map \u03A5 superimposed on the left input image\
    \ (green pixels denote the classified textureless regions, otherwise textured\
    \ regions). It is generated to adaptively adjust the cross-scale consistency constraint\
    \ in fPMF."
  Figure 5 Link: articels_figures_by_rev_year\2016\PatchMatch_Filter_EdgeAware_Filtering_Meets_Randomized_Search_for_Visual_Corresp\figure_5.jpg
  Figure 5 caption: Time-accuracy trade-off study of PMF methods.
  Figure 6 Link: articels_figures_by_rev_year\2016\PatchMatch_Filter_EdgeAware_Filtering_Meets_Randomized_Search_for_Visual_Corresp\figure_6.jpg
  Figure 6 caption: After applying PMF for a few iterations, optical flow estimation
    for the RubberWhale images quickly converges.
  Figure 7 Link: articels_figures_by_rev_year\2016\PatchMatch_Filter_EdgeAware_Filtering_Meets_Randomized_Search_for_Visual_Corresp\figure_7.jpg
  Figure 7 caption: 'Visual results. Top row (left to right): Segmented Teddy image,
    PMF-S (w CLMF-0) result and close-up comparison. Middle row (left to right): Segmented
    Cones image, PMF-S (w GF) result and close-up comparison. Bottom row (left to
    right): Synthesized novel-view images with PMF-C and PMF-S.'
  Figure 8 Link: articels_figures_by_rev_year\2016\PatchMatch_Filter_EdgeAware_Filtering_Meets_Randomized_Search_for_Visual_Corresp\figure_8.jpg
  Figure 8 caption: Visual comparison of the stereo results estimated by PatchMatch
    Stereo (PM) [9], PMBP [8], PMF-S [26] , and fPMF-S for Bowling2 (top) and Baby2
    (bottom) that contain large textureless regions.
  Figure 9 Link: articels_figures_by_rev_year\2016\PatchMatch_Filter_EdgeAware_Filtering_Meets_Randomized_Search_for_Visual_Corresp\figure_9.jpg
  Figure 9 caption: Results on Schefflera, Teddy and HumanEva by a) PMF-OF b) CostFilter
    [34] c) DPOF [21] d) MDP-Flow2 [42].
  First author gender probability: 0.95
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Jiangbo Lu
  Name of the last author: Minh N. Do
  Number of Figures: 12
  Number of Tables: 7
  Number of authors: 6
  Paper title: 'PatchMatch Filter: Edge-Aware Filtering Meets Randomized Search for
    Visual Correspondence'
  Publication Date: 2016-10-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Complexity Comparison of Three Different Techniques
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Middlebury Stereo Evaluation [2] for Error Threshold = 0.5.
  Table 3 caption:
    table_text: TABLE 3 Stereo Evaluation Results for Teddy and Cones when Error Threshold
      = 0.5 [Captured on 29072015]
  Table 4 caption:
    table_text: TABLE 4 Quantitative Stereo Result Evaluation (wo Post-Processing)
      on Seven Middleburry 2006 Datasets with Error Threshold 0.5
  Table 5 caption:
    table_text: TABLE 5 Middlebury Quantitative Flow Evaluation Results Measured with
      Average Endpoint Error (AEE) for Three Challenging Scenes
  Table 6 caption:
    table_text: TABLE 6 Evaluation of Different PMF-Based Approaches on the MPI Sintel
      Training Dataset
  Table 7 caption:
    table_text: TABLE 7 Optical Flow Performance on the MPI Sintel Dataset
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2616391
- Affiliation of the first author: nokia bell labs, murray hill, nj
  Affiliation of the last author: department of electrical, computer and systems engineering,
    rensselaer polytechnic institute, troy, ny
  Figure 1 Link: articels_figures_by_rev_year\2016\Hierarchical_Context_Modeling_for_Video_Event_Recognition\figure_1.jpg
  Figure 1 caption: "Events \u201Cloading\u201D with large intra-class variation."
  Figure 10 Link: articels_figures_by_rev_year\2016\Hierarchical_Context_Modeling_for_Video_Event_Recognition\figure_10.jpg
  Figure 10 caption: Confusion matrices for the recognition of six person-vehicle
    interaction events on VIRAT 2.0 Ground Dataset with the SVM-STIP, SVM-Context,
    and the proposed Deep Model.
  Figure 2 Link: articels_figures_by_rev_year\2016\Hierarchical_Context_Modeling_for_Video_Event_Recognition\figure_2.jpg
  Figure 2 caption: The integration of contexts from three levels.
  Figure 3 Link: articels_figures_by_rev_year\2016\Hierarchical_Context_Modeling_for_Video_Event_Recognition\figure_3.jpg
  Figure 3 caption: The definition of event neighborhood, in which the blue rectangle
    indicates the event bounding box, and the dashed green rectangle is the extended
    rectangle. The shaded region within the extended rectangle but outside of the
    event bounding is the spatial neighborhood. The event neighborhood is the union
    of the spatial neighborhoods over T frames.
  Figure 4 Link: articels_figures_by_rev_year\2016\Hierarchical_Context_Modeling_for_Video_Event_Recognition\figure_4.jpg
  Figure 4 caption: Extracting appearance context feature from event neighborhood.
    (a) SIFT key points in the neighborhood of each frame; (b) BOW histogram feature.
  Figure 5 Link: articels_figures_by_rev_year\2016\Hierarchical_Context_Modeling_for_Video_Event_Recognition\figure_5.jpg
  Figure 5 caption: Extracting interaction context feature with a 2D histogram that
    captures the co-occurrence frequencies of words of event objects and contextual
    objects.
  Figure 6 Link: articels_figures_by_rev_year\2016\Hierarchical_Context_Modeling_for_Video_Event_Recognition\figure_6.jpg
  Figure 6 caption: The model capturing semantic level contexts, where mathbf hp and
    mathbf ho are the first layer hidden units representing person and object middle
    level representation, mathbf hr is the second layer hidden units capturing interactions,
    and mathbf y stands for the event class label.
  Figure 7 Link: articels_figures_by_rev_year\2016\Hierarchical_Context_Modeling_for_Video_Event_Recognition\figure_7.jpg
  Figure 7 caption: The model combining semantic level contexts with observations,
    where vectors mathbf p and mathbf o denote the person and object observations,
    and mathbf e and mathbf c represent the event and context observations respectively.
  Figure 8 Link: articels_figures_by_rev_year\2016\Hierarchical_Context_Modeling_for_Video_Event_Recognition\figure_8.jpg
  Figure 8 caption: The model capturing prior level contexts, where mathbf s represents
    the global scene observation, mathbf m-1 denotes the recognition measurement of
    the previous event, mathbf hs denotes the hidden units representing different
    possible scene states, mathbf y-1 denotes the previous event, and mathbf y stands
    for the current event.
  Figure 9 Link: articels_figures_by_rev_year\2016\Hierarchical_Context_Modeling_for_Video_Event_Recognition\figure_9.jpg
  Figure 9 caption: The proposed deep context model integrating image level, semantic
    level, and prior level contexts, where the shaded units represent the hidden units,
    the striped units represent the observed units that would be available both in
    training and testing, and the units in grid are event label units which are available
    in training and not available in testing.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: Xiaoyang Wang
  Name of the last author: Qiang Ji
  Number of Figures: 10
  Number of Tables: 8
  Number of authors: 2
  Paper title: Hierarchical Context Modeling for Video Event Recognition
  Publication Date: 2016-10-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance of Appearance Context Feature with Different Neighborhood
      Size on Six Events of VIRAT 2.0
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance of Interaction Context Features with Different
      Vocabulary Size on Six Events of VIRAT 2.0
  Table 3 caption:
    table_text: TABLE 3 Performance of Proposed Context Features Combined with Baseline
      STIP Features on Six Events of VIRAT 2.0
  Table 4 caption:
    table_text: TABLE 4 Performances of SVM-STIP, SVM-Context, Model-BM and Deep Model
      on VIRAT 1.0 Ground Dataset
  Table 5 caption:
    table_text: TABLE 5 The Comparison of Our Model with State-of-the-Art Approaches
      on VIRAT 1.0 Ground Dataset
  Table 6 caption:
    table_text: TABLE 6 Performances of SVM-STIP, SVM-Context, Model-BM Baselines
      and the Proposed Deep Model Compared with BN [47] for Six Events on VIRAT 2.0
      Dataset
  Table 7 caption:
    table_text: TABLE 7 Comparisons with State-of-the-Art Methods for Recognition
      of All Events from VIRAT
  Table 8 caption:
    table_text: TABLE 8 Overall Recognition Accuracies Compared to State-of-the-Art
      Methods on UT-Interaction Dataset
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2616308
- Affiliation of the first author: department of computer sciences, technion-israel
    institute of technology, haifa, israel
  Affiliation of the last author: department of computer sciences, technion-israel
    institute of technology, haifa, israel
  Figure 1 Link: articels_figures_by_rev_year\2016\Learn_on_Source_Refine_on_Target_A_Model_Transfer_Learning_Framework_with_Random\figure_1.jpg
  Figure 1 caption: Simple box example and resulting decision trees.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Learn_on_Source_Refine_on_Target_A_Model_Transfer_Learning_Framework_with_Random\figure_2.jpg
  Figure 2 caption: Simple concept shift example.
  Figure 3 Link: articels_figures_by_rev_year\2016\Learn_on_Source_Refine_on_Target_A_Model_Transfer_Learning_Framework_with_Random\figure_3.jpg
  Figure 3 caption: Induced decision tree with distributions.
  Figure 4 Link: articels_figures_by_rev_year\2016\Learn_on_Source_Refine_on_Target_A_Model_Transfer_Learning_Framework_with_Random\figure_4.jpg
  Figure 4 caption: Error rates for different target training set sizes. The x-axis
    is the ratio between available source and target training examples.
  Figure 5 Link: articels_figures_by_rev_year\2016\Learn_on_Source_Refine_on_Target_A_Model_Transfer_Learning_Framework_with_Random\figure_5.jpg
  Figure 5 caption: Cumulative distribution functions of margins for disagreeing base
    classifiers.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.95
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.58
  Name of the first author: Noam Segev
  Name of the last author: Ran El-Yaniv
  Number of Figures: 5
  Number of Tables: 7
  Number of authors: 5
  Paper title: 'Learn on Source, Refine on Target: A Model Transfer Learning Framework
    with Random Forests'
  Publication Date: 2016-10-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Test Results of Transfer Forests on Synthetic Challenges
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Dataset Information
  Table 3 caption:
    table_text: TABLE 3 Low Resolution and Inverted Versions of Digits from the MNIST
      Dataset
  Table 4 caption:
    table_text: "TABLE 4 Test Error Rates Compared to Benchmarks and Competing Algorithms\u2014\
      Lowest Error in Boldface"
  Table 5 caption:
    table_text: TABLE 5 Models Transfer Times in MS. STRUT and SER Times Are Shown
      for a Serial Execution
  Table 6 caption:
    table_text: "TABLE 6 Test Error Rates of the MIX Transfer Forest and the Instance\
      \ Transfer Algorithms\u2014Lowest Error in Boldface"
  Table 7 caption:
    table_text: TABLE 7 Error Measurements for Transfer Forests on Synthetic Source-Target
      Transformations
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2618118
- Affiliation of the first author: xidian university, xi'an, china
  Affiliation of the last author: microsoft research, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2016\Adaptive_Nonlocal_Sparse_Representation_for_DualCamera_Compressive_Hyperspectral\figure_1.jpg
  Figure 1 caption: Flowchart of the proposed method for DCCHI reconstruction.
  Figure 10 Link: articels_figures_by_rev_year\2016\Adaptive_Nonlocal_Sparse_Representation_for_DualCamera_Compressive_Hyperspectral\figure_10.jpg
  Figure 10 caption: Spectrum accuracy evaluation. (a) Two selected spatial points
    in the original hyperspectral image (synthesized to the RGB format). (b) and (c)
    The reconstructed spectra of point 1 and point 2 by different methods.
  Figure 2 Link: articels_figures_by_rev_year\2016\Adaptive_Nonlocal_Sparse_Representation_for_DualCamera_Compressive_Hyperspectral\figure_2.jpg
  Figure 2 caption: PSNR results of reconstruction on four hyperspectral images with
    a fixed cube width and variable cube lengths.
  Figure 3 Link: articels_figures_by_rev_year\2016\Adaptive_Nonlocal_Sparse_Representation_for_DualCamera_Compressive_Hyperspectral\figure_3.jpg
  Figure 3 caption: PSNR results of reconstruction on four hyperspectral images with
    a fixed cube length and variable cube widths.
  Figure 4 Link: articels_figures_by_rev_year\2016\Adaptive_Nonlocal_Sparse_Representation_for_DualCamera_Compressive_Hyperspectral\figure_4.jpg
  Figure 4 caption: An intuitive example for comparing different similarity metrics.
    A 3D cube is visualized by stacking each 2D spatial section as one column of a
    2D patch, where different columns in the 2D patch denote different spectral bands
    in the 3D cube. (a) An exemplar cube taken from the initial reconstruction result.
    (b) A set of similar cubes found according to the internal, external, and joint
    similarities, respectively. (c) The correspondingly reconstructed cubes. (d) The
    groundtruth cube.
  Figure 5 Link: articels_figures_by_rev_year\2016\Adaptive_Nonlocal_Sparse_Representation_for_DualCamera_Compressive_Hyperspectral\figure_5.jpg
  Figure 5 caption: "PSNR results of reconstruction on four hyperspectral images with\
    \ different fixed joint similarities ( \u03B2 r ij =0.25,0.5,0.75 ), the internal\
    \ similarity ( \u03B2 r ij =1 ), the external similarity ( \u03B2 r ij =0 ), and\
    \ the adaptive joint similarity (marked as 'A')."
  Figure 6 Link: articels_figures_by_rev_year\2016\Adaptive_Nonlocal_Sparse_Representation_for_DualCamera_Compressive_Hyperspectral\figure_6.jpg
  Figure 6 caption: Average similarity weight over all the cubes against the iteration
    number on four hyperspectral images.
  Figure 7 Link: articels_figures_by_rev_year\2016\Adaptive_Nonlocal_Sparse_Representation_for_DualCamera_Compressive_Hyperspectral\figure_7.jpg
  Figure 7 caption: PSNR and SAM comparison of different methods. (a) PSNR results,
    and (b) SAM results.
  Figure 8 Link: articels_figures_by_rev_year\2016\Adaptive_Nonlocal_Sparse_Representation_for_DualCamera_Compressive_Hyperspectral\figure_8.jpg
  Figure 8 caption: Visual comparison of reconstruction results by different methods
    on the Toy data (one spectral band selected).
  Figure 9 Link: articels_figures_by_rev_year\2016\Adaptive_Nonlocal_Sparse_Representation_for_DualCamera_Compressive_Hyperspectral\figure_9.jpg
  Figure 9 caption: Visual comparison of reconstruction results by different methods
    on the Beads data (one spectral band selected).
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.67
  Name of the first author: Lizhi Wang
  Name of the last author: Wenjun Zeng
  Number of Figures: 13
  Number of Tables: 2
  Number of authors: 5
  Paper title: Adaptive Nonlocal Sparse Representation for Dual-Camera Compressive
    Hyperspectral Imaging
  Publication Date: 2016-10-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 RMSE Comparison of the Reconstructed Spectrum by Different
      Methods
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 RMSE Comparison of the Reconstructed Spectrum by Different
      Methods
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2621050
- Affiliation of the first author: institute of artificial intelligence & robotics,
    xi'an jiaotong university, xi'an, shaanxi, china
  Affiliation of the last author: institute of artificial intelligence & robotics,
    xi'an jiaotong university, xi'an, shaanxi, china
  Figure 1 Link: articels_figures_by_rev_year\2016\Video_Object_Discovery_and_CoSegmentation_with_Extremely_Weak_Supervision\figure_1.jpg
  Figure 1 caption: "Problem setting: Input\u2014multiple videos capturing a common\
    \ category of objects. Some of which may contain irrelevant frames. Output\u2014\
    a label for each frame indicating if it is relevant, and a detailed pixel labeling\
    \ of the common object for each relevant frame identified."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Video_Object_Discovery_and_CoSegmentation_with_Extremely_Weak_Supervision\figure_2.jpg
  Figure 2 caption: The flowchart of our video object discovery and co-segmentation
    method.
  Figure 3 Link: articels_figures_by_rev_year\2016\Video_Object_Discovery_and_CoSegmentation_with_Extremely_Weak_Supervision\figure_3.jpg
  Figure 3 caption: The spatio-temporal auto-context feature.
  Figure 4 Link: articels_figures_by_rev_year\2016\Video_Object_Discovery_and_CoSegmentation_with_Extremely_Weak_Supervision\figure_4.jpg
  Figure 4 caption: Some visual example results of our methods and five state-of-the-art
    single video segmentation methods on eight videos that contain only one object
    on the SegTrack dataset.
  Figure 5 Link: articels_figures_by_rev_year\2016\Video_Object_Discovery_and_CoSegmentation_with_Extremely_Weak_Supervision\figure_5.jpg
  Figure 5 caption: Some visual example results of our methods and 12 state-of-the-art
    methods on the VCoSeg dataset.
  Figure 6 Link: articels_figures_by_rev_year\2016\Video_Object_Discovery_and_CoSegmentation_with_Extremely_Weak_Supervision\figure_6.jpg
  Figure 6 caption: Some visual example results of our methods and four state-of-the-art
    methods on the MOViCS dataset. The different colors in the 2 nd , 3 rd , and 5
    th rows denote the class labels.
  Figure 7 Link: articels_figures_by_rev_year\2016\Video_Object_Discovery_and_CoSegmentation_with_Extremely_Weak_Supervision\figure_7.jpg
  Figure 7 caption: "The example relevant ( \u221A ) and irrelevant ( \xD7 ) frames,\
    \ the pixel-wise ground truth foreground labels (binary mask) for relevant frames,\
    \ and the statistical details of the XJTU-Stevens video co-segmentation and classification\
    \ dataset. For the airplane category, 11(47) denotes that the numbers of all videos,\
    \ videos only containing relevant frames, and videos containing irrelevant frames\
    \ are 11, 4, and 7, respectively; \u201C1763(170261)\u201D denotes that the numbers\
    \ of all frames, relevant frames, and irrelevant frames are 1,763, 1,702, and\
    \ 61, respectively."
  Figure 8 Link: articels_figures_by_rev_year\2016\Video_Object_Discovery_and_CoSegmentation_with_Extremely_Weak_Supervision\figure_8.jpg
  Figure 8 caption: Some visual example results of our methods and three multi-class
    video co-segmentation methods on the XJTU-Stevens video co-segmentation and classification
    dataset. The different colors in the 2nd and 4th rows denote the class labels.
  Figure 9 Link: articels_figures_by_rev_year\2016\Video_Object_Discovery_and_CoSegmentation_with_Extremely_Weak_Supervision\figure_9.jpg
  Figure 9 caption: (a) The numbers of misclassified frames and (b) the average IoU
    scores of our method by varying the value of Nc .
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Le Wang
  Name of the last author: Nanning Zheng
  Number of Figures: 9
  Number of Tables: 6
  Number of authors: 6
  Paper title: Video Object Discovery and Co-Segmentation with Extremely Weak Supervision
  Publication Date: 2016-10-26 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Principal Notations
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Average IoU Scores of Our Methods and Five Competing Single
      Video Segmentation Methods on Eight Videos That Contain Only One Object on the
      SegTrack Dataset
  Table 3 caption:
    table_text: TABLE 3 The Average IoU Scores of Our Methods and 12 State-of-the-Art
      Methods on the VCoSeg Dataset
  Table 4 caption:
    table_text: TABLE 4 The Average IoU Scores of Our Methods and Four State-of-the-Art
      Methods on the MOViCS Dataset
  Table 5 caption:
    table_text: TABLE 5 The Number of Misclassified Frames of Our Methods by Varying
      the Number of Manually Annotated Frames (I, II or III in the 2nd Row), and Three
      Multi-Class Video Co-Segmentation Methods on the XJTU-Stevens Video Co-Segmentation
      and Classification Dataset
  Table 6 caption:
    table_text: TABLE 6 The Average IoU Scores of Our Methods and Three Multi-Class
      Video Co-Segmentation Methods on the XJTU-Stevens Video Co-Segmentation and
      Classification Dataset
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2612187
- Affiliation of the first author: university of amsterdam, amsterdam, wx, the netherlands
  Affiliation of the last author: university of amsterdam, amsterdam, wx, the netherlands
  Figure 1 Link: articels_figures_by_rev_year\2016\Learning_to_Recognize_Human_Activities_Using_Soft_Labels\figure_1.jpg
  Figure 1 caption: "Comparison between the traditional way of annotation and our\
    \ way (soft labeling). The video is sampled from the CAD-120 dataset [1]. Each\
    \ image in the picture represents a video segment. In this example, the subject\
    \ performs two activities, \u201Cmoving\u201D (green) and \u201Cplacing\u201D\
    \ (blue). Labels of the first two and last two frames are easy to assign. However,\
    \ assigning labels for the frames between the two activities is purely based on\
    \ personal preferences (traditional way). Instead of assigning an arbitrary label,\
    \ we propose to assign 0.5 to both labels (our way) and let the learning algorithm\
    \ to determine which label is correct."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Learning_to_Recognize_Human_Activities_Using_Soft_Labels\figure_2.jpg
  Figure 2 caption: "The graphical representation of our model. Input nodes x (black)\
    \ are observed during both training and testing. y is the target labels to be\
    \ predicted, i.e., the true activity labels. \u03C0 k are the soft labels, i.e.,\
    \ labels that contain confidence values. The soft labels are only present during\
    \ training but not testing."
  Figure 3 Link: articels_figures_by_rev_year\2016\Learning_to_Recognize_Human_Activities_Using_Soft_Labels\figure_3.jpg
  Figure 3 caption: "A transition segment where the label is \u201Cwrongly\u201D classified\
    \ by the soft labeling model. The \u201Cgroundtruth\u201D label provided by the\
    \ CAD120 dataset is reaching, and the prediction of our model is moving. We can\
    \ see that the person is performing two activities at the same time, reaching\
    \ for an apple (left hand) and moving a cup (right hand). Assigning a single label\
    \ to these frames is problematic. In contrast, the soft labeling method is able\
    \ to capture both of the activities and the inappropriate annotation is corrected\
    \ by learning with soft labeling."
  Figure 4 Link: articels_figures_by_rev_year\2016\Learning_to_Recognize_Human_Activities_Using_Soft_Labels\figure_4.jpg
  Figure 4 caption: The test performance of activity recognition on the CAD-120 dataset.
    We show the change of performance with the increasing amount of noise at transition
    labeling. The colors encode the two different segmentation methods, i.e., uniform
    segmentation (purple) and graph-based segmentation (blue). The type of the lines
    is used to distinguish three approaches, i.e., the baseline (dotted line), the
    state-of-the-art (dashed line) and the proposed soft labeling approach (solid
    line). We report the performance in accuracy (a), precision (b), recall (c), and
    F1-score (d).
  Figure 5 Link: articels_figures_by_rev_year\2016\Learning_to_Recognize_Human_Activities_Using_Soft_Labels\figure_5.jpg
  Figure 5 caption: Visualization of the latent components. The columns are six activities
    and rows refer to the four latent components. Due to the limitation of space,
    here only six activities are illustrated.
  Figure 6 Link: articels_figures_by_rev_year\2016\Learning_to_Recognize_Human_Activities_Using_Soft_Labels\figure_6.jpg
  Figure 6 caption: Execution time of the inference algorithm.
  Figure 7 Link: articels_figures_by_rev_year\2016\Learning_to_Recognize_Human_Activities_Using_Soft_Labels\figure_7.jpg
  Figure 7 caption: GUI for data labeling.
  Figure 8 Link: articels_figures_by_rev_year\2016\Learning_to_Recognize_Human_Activities_Using_Soft_Labels\figure_8.jpg
  Figure 8 caption: Visualization of the action labels that are assigned by multiple
    annotators. Each row corresponds with the labeling results of one annotator. Note
    that the videos are randomly assigned to the annotators. In this case, there are
    only four annotators working on this part of the video.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Ninghang Hu
  Name of the last author: "Ben Kr\xF6se"
  Number of Figures: 8
  Number of Tables: 3
  Number of authors: 4
  Paper title: Learning to Recognize Human Activities Using Soft Labels
  Publication Date: 2016-10-26 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of Notations
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance of F1-Score on the CAD-120 Dataset
  Table 3 caption:
    table_text: TABLE 3 Test Performance of the Proposed Model on the ACCOMPANY Dataset
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2621761
- Affiliation of the first author: department of software engineering, southwest jiaotong
    university, chengdu, sichuan, china
  Affiliation of the last author: school of engineering, university of california,
    merced, ca
  Figure 1 Link: articels_figures_by_rev_year\2016\Evaluation_of_Segmentation_Quality_via_Adaptive_Composition_of_Reference_Segment\figure_1.jpg
  Figure 1 caption: An illustrative example between a machine segmentation and labeled
    segmentations by humans. (a) An input image. (b) An image segmentation of (a).
    (c) Different parts (shown in different colors) of the segmentation in (b) are
    similar to those segments labeled by humans in (d). In order to better evaluate
    image segmentation algorithms, it is important to have a good reference segmentation
    composed from labeled segments by humans.
  Figure 10 Link: articels_figures_by_rev_year\2016\Evaluation_of_Segmentation_Quality_via_Adaptive_Composition_of_Reference_Segment\figure_10.jpg
  Figure 10 caption: Sample pairs in the segmentation evaluation dataset.
  Figure 2 Link: articels_figures_by_rev_year\2016\Evaluation_of_Segmentation_Quality_via_Adaptive_Composition_of_Reference_Segment\figure_2.jpg
  Figure 2 caption: Proposed evaluation framework based on adaptive composition of
    the reference segmentation.
  Figure 3 Link: articels_figures_by_rev_year\2016\Evaluation_of_Segmentation_Quality_via_Adaptive_Composition_of_Reference_Segment\figure_3.jpg
  Figure 3 caption: "An example to compose a reference segmentation for segmentation\
    \ S with labeled segmentations G 1 and G 2 . The optimal labelings l G 1 , l G\
    \ 2 of G 1 and G 2 generate a reference segmentation G \u2217 , which matches\
    \ S closely."
  Figure 4 Link: articels_figures_by_rev_year\2016\Evaluation_of_Segmentation_Quality_via_Adaptive_Composition_of_Reference_Segment\figure_4.jpg
  Figure 4 caption: An example of inconsistent boundaries among different human-labeled
    segmentations from the Berkeley Segmentation Dataset [29]. The whiter pixels indicate
    that more human subjects mark them as boundary.
  Figure 5 Link: articels_figures_by_rev_year\2016\Evaluation_of_Segmentation_Quality_via_Adaptive_Composition_of_Reference_Segment\figure_5.jpg
  Figure 5 caption: "Examples of composed references for human labeled segmentations.\
    \ For each example, the first row shows the original image, two human labeled\
    \ segmentations (labeled in green) of it and the corresponding composed reference\
    \ segmentations G \u2217 (labeled in red). The second row shows the human labeled\
    \ segmentations used to compose the reference segmentations. The images are from\
    \ the Berkeley Segmentation Dataset [29]."
  Figure 6 Link: articels_figures_by_rev_year\2016\Evaluation_of_Segmentation_Quality_via_Adaptive_Composition_of_Reference_Segment\figure_6.jpg
  Figure 6 caption: "Examples of composed references for machine segmentations. For\
    \ each example, the first row shows the original image, two machine segmentations\
    \ (labeled in green) of it and the corresponding composed reference segmentations\
    \ G \u2217 (labeled in red). The second row shows human labeled segmentations\
    \ used to compose the reference segmentations. The images are from the Berkeley\
    \ Segmentation Dataset [29]."
  Figure 7 Link: articels_figures_by_rev_year\2016\Evaluation_of_Segmentation_Quality_via_Adaptive_Composition_of_Reference_Segment\figure_7.jpg
  Figure 7 caption: User interface of the developed image segmentation tool.
  Figure 8 Link: articels_figures_by_rev_year\2016\Evaluation_of_Segmentation_Quality_via_Adaptive_Composition_of_Reference_Segment\figure_8.jpg
  Figure 8 caption: Sample images and the labeled segmentations in the developed dataset.
  Figure 9 Link: articels_figures_by_rev_year\2016\Evaluation_of_Segmentation_Quality_via_Adaptive_Composition_of_Reference_Segment\figure_9.jpg
  Figure 9 caption: Distribution of labeled segmentations in the proposed and BSDS500
    datasets.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Bo Peng
  Name of the last author: Ming-Hsuan Yang
  Number of Figures: 16
  Number of Tables: 4
  Number of authors: 4
  Paper title: Evaluation of Segmentation Quality via Adaptive Composition of Reference
    Segmentations
  Publication Date: 2016-10-27 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of the BSDS500 and Proposed Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Parameter Settings of the Four Algorithms for Generating Segmentations
      in Our Evaluation Dataset
  Table 3 caption:
    table_text: TABLE 3 Evaluation Results with the Meta-Measure
  Table 4 caption:
    table_text: TABLE 4 Evaluation Results by Different Measures
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2622703
- Affiliation of the first author: interdisciplinary centre for security, reliability,
    and trust, university of luxembourg, esch-sur-alzette, luxembourg
  Affiliation of the last author: interdisciplinary centre for security, reliability,
    and trust, university of luxembourg, esch-sur-alzette, luxembourg
  Figure 1 Link: articels_figures_by_rev_year\2016\RealTime_Enhancement_of_Dynamic_Depth_Videos_with_NonRigid_Deformations\figure_1.jpg
  Figure 1 caption: Results of different SR methods for a scale factor of r=4 applied
    to a low resolution dynamic depth video captured with a ToF camera at a rate of
    50 frames per ms. (a) Raw frame, (b) Bicubic interpolation, (c) SISR [31], (d)
    UP-SR [8], (e) Proposed recUP-SR. Units are in mm.
  Figure 10 Link: articels_figures_by_rev_year\2016\RealTime_Enhancement_of_Dynamic_Depth_Videos_with_NonRigid_Deformations\figure_10.jpg
  Figure 10 caption: 'Results of SR on the hand deforming sequence of Section 5.2.2:
    (a) Raw LR, (b) aTGV-SF (flow)+recUP-SR (SR), (c) proposed recUP-SR (flow+SR).'
  Figure 2 Link: articels_figures_by_rev_year\2016\RealTime_Enhancement_of_Dynamic_Depth_Videos_with_NonRigid_Deformations\figure_2.jpg
  Figure 2 caption: 'Flow chart of recUP-SR: A new multi-frame depth super-resolution
    algorithm for dynamic depth videos of non-rigidly deforming objects.'
  Figure 3 Link: articels_figures_by_rev_year\2016\RealTime_Enhancement_of_Dynamic_Depth_Videos_with_NonRigid_Deformations\figure_3.jpg
  Figure 3 caption: Correcting amplitude images using a standardization step [54].
    (a) and (b) show the original LR amplitude images for a dynamic scene containing
    a hand moving towards the camera where the intensity (amplitude) values differ
    significantly depending on the object distance from the camera. The corrected
    amplitude images for the same scene are presented in (c) and (d), where the intensity
    consistency is preserved.
  Figure 4 Link: articels_figures_by_rev_year\2016\RealTime_Enhancement_of_Dynamic_Depth_Videos_with_NonRigid_Deformations\figure_4.jpg
  Figure 4 caption: "3D Plotting of one super-resolved depth frame with r=4 using:\
    \ (b) bicubic interpolation, (c) UP-SR [8], (d) SISR [31], (e) proposed recUP-SR\
    \ with L=3,K=7,\u03BB=2.5 . (a) LR noisy depth frame. (f) 3D ground truth. Color\
    \ bar in mm."
  Figure 5 Link: articels_figures_by_rev_year\2016\RealTime_Enhancement_of_Dynamic_Depth_Videos_with_NonRigid_Deformations\figure_5.jpg
  Figure 5 caption: Comparison with multi-modal fusion methods. (a) Raw LR depth image.
    (b) HR 2D image. (c) PWAS [25], (d) UML [26], (e) aTGV [28], (f) Proposed recUP-SR
    and corresponding full video is available through this link. Color bar in mm.
  Figure 6 Link: articels_figures_by_rev_year\2016\RealTime_Enhancement_of_Dynamic_Depth_Videos_with_NonRigid_Deformations\figure_6.jpg
  Figure 6 caption: 3D RMSE in mm of the super-resolved hand sequence in Section 5.2.1
    using recUP-SR.
  Figure 7 Link: articels_figures_by_rev_year\2016\RealTime_Enhancement_of_Dynamic_Depth_Videos_with_NonRigid_Deformations\figure_7.jpg
  Figure 7 caption: Tracking results for different depth values randomly chosen from
    the super-resolved hand sequence in Section 5.2.1 with different SR scale factors
    r=1,r=2, and r=4 , are plotted in (a), (b), and (c), respectively. The corresponding
    filtered velocities are shown in (d), (e), and (f), respectively.
  Figure 8 Link: articels_figures_by_rev_year\2016\RealTime_Enhancement_of_Dynamic_Depth_Videos_with_NonRigid_Deformations\figure_8.jpg
  Figure 8 caption: 'Estimated range flow on the hand deforming sequence of Section
    5.2.2: (a) aTGV-SF [29], (b) proposed recUP-SR. Arrows represent the lateral motion.
    The color represents the radial motion in mm. Full video available through this
    link.'
  Figure 9 Link: articels_figures_by_rev_year\2016\RealTime_Enhancement_of_Dynamic_Depth_Videos_with_NonRigid_Deformations\figure_9.jpg
  Figure 9 caption: 'Results of SR on the hand deforming sequence of Section 5.2.2:
    (a) aTGV-SF (flow+SR) [29] (b) recUP-SR (flow) + aTGV-SF (SR) (c) aTGV-SF (flow)
    + recUP-SR (SR), (d) proposed recUP-SR (flow+SR).'
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Kassem Al Ismaeil
  Name of the last author: "Bj\xF6rn Ottersten"
  Number of Figures: 16
  Number of Tables: 1
  Number of authors: 5
  Paper title: Real-Time Enhancement of Dynamic Depth Videos with Non-Rigid Deformations
  Publication Date: 2016-10-27 00:00:00
  Table 1 caption:
    table_text: TABLE 1 3D RMSE in mm for the Super-Resolved Dancing Girl Sequence
      Using Different SR Methods
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2622698
- Affiliation of the first author: department of electrical and computer engineering,
    carnegie mellon university, pittsburgh, pa
  Affiliation of the last author: department of electrical and computer engineering,
    carnegie mellon university, pittsburgh, pa
  Figure 1 Link: articels_figures_by_rev_year\2016\Shape_and_SpatiallyVarying_Reflectance_Estimation_from_Virtual_Exemplars\figure_1.jpg
  Figure 1 caption: Recovery of surface normals and spatially-varying BRDF. We propose
    a framework for per-pixel estimation of surface normal and BRDF in the setting
    of photometric stereo. Shown above are the estimated shape and rendered images
    of a visually-complex object. The results were obtained from 250 images.
  Figure 10 Link: articels_figures_by_rev_year\2016\Shape_and_SpatiallyVarying_Reflectance_Estimation_from_Virtual_Exemplars\figure_10.jpg
  Figure 10 caption: Evaluations on the benchmark photometric stereo dataset [50].
    Shown are the mean and median of the angular errors measured in degrees for both
    the gradient descent method and the state-of-the-art techniques. For each object,
    the best performing algorithm for both mean and median angular error is marked
    in red. The proposed technique outperforms the benchmarked techniques in a majority
    of scenes. The numbers for the benchmarked algorithms are reported from [50].
  Figure 2 Link: articels_figures_by_rev_year\2016\Shape_and_SpatiallyVarying_Reflectance_Estimation_from_Virtual_Exemplars\figure_2.jpg
  Figure 2 caption: Accuracy of BRDF models on the MERL database [25]. For the 100
    materials in the database, we plot the approximation accuracy in relative RMS
    error [26] (also see (8)) for the proposed, bivariate [8], Cook-Torrance [21],
    and the isotropic Ward [7] models. For the proposed model, we use a leave-one-out
    scheme, wherein for each BRDF the remaining 99 BRDFs in the database are used
    to form the dictionary. The proposed model outperforms competing models both quantitatively
    (top) as well as in visual perception (bottom).
  Figure 3 Link: articels_figures_by_rev_year\2016\Shape_and_SpatiallyVarying_Reflectance_Estimation_from_Virtual_Exemplars\figure_3.jpg
  Figure 3 caption: The error as a function of candidate normals for a few test examples.
    We can observe that the global minima is compact and the error increases largely
    monotonically in its vicinity. This motivates our coarse-to-fine search strategy.
  Figure 4 Link: articels_figures_by_rev_year\2016\Shape_and_SpatiallyVarying_Reflectance_Estimation_from_Virtual_Exemplars\figure_4.jpg
  Figure 4 caption: Normal and BRDF estimation with varying number of images. We estimate
    the angular errors for the coarse-to-fine (in dot green) and the gradient descent
    method (solid green line). We also estimate the relative BRDF errors for both
    per-pixel (in dot red) and rank-1 prior (solid red line) under perfect knowledge
    of the surface normals. Finally, we test the entire estimation pipeline by measuring
    the accuracy of BRDFs using the surface normals from the gradient descent scheme
    (orange solid line). The plots were obtained by averaging across all 100 BRDFs
    in the MERL database and 20,000 randomly-generated normals per material.
  Figure 5 Link: articels_figures_by_rev_year\2016\Shape_and_SpatiallyVarying_Reflectance_Estimation_from_Virtual_Exemplars\figure_5.jpg
  Figure 5 caption: Normal estimation for different materials. We fix the number of
    input imageslighting directions to 253. For each material BRDF, we compute average
    error over 1,000 randomly-generated surface normals for both the coarse-to-fine
    search strategy (in red) and the gradient descent method (in green). The gradient
    descent scheme outperforms both competing methods [30], [31] in 88 out of 100
    materials.
  Figure 6 Link: articels_figures_by_rev_year\2016\Shape_and_SpatiallyVarying_Reflectance_Estimation_from_Virtual_Exemplars\figure_6.jpg
  Figure 6 caption: Normal estimation for different dictionary size. We fix the number
    of input imageslighting directions to 253. For different dictionary size, we compute
    average error over 1,000 randomly-generated surface normals for both coarse-to-fine
    search (in red) and the gradient descent (in green) methods.
  Figure 7 Link: articels_figures_by_rev_year\2016\Shape_and_SpatiallyVarying_Reflectance_Estimation_from_Virtual_Exemplars\figure_7.jpg
  Figure 7 caption: Normal estimation for different type of materials in the dictionary.
    We fix the number of input imageslighting directions to 253. The numbers in the
    legend indicates the size of the corresponding dictionary. We observe that a mismatch
    in material type always leads to poor normal estimates. Shown are average errors
    over 1,000 randomly-generated surface normals.
  Figure 8 Link: articels_figures_by_rev_year\2016\Shape_and_SpatiallyVarying_Reflectance_Estimation_from_Virtual_Exemplars\figure_8.jpg
  Figure 8 caption: "BRDF evaluation with low rank prior for a synthetic object. We\
    \ show a spherical object whose per-pixel BRDF is a linear combination of the\
    \ three materials shown. The color coded sphere shows the relative abundances\
    \ of the three materials in each color channel. We show rendered images using\
    \ the ground truth, the per-pixel estimate as well as the low-rank estimate for\
    \ different values of rank, K . For each value of the solution rank K , we include\
    \ the corresponding value of \u03B2 used in the optimization at the top of the\
    \ plot. Finally, we present the relative BRDF error as a function of the rank."
  Figure 9 Link: articels_figures_by_rev_year\2016\Shape_and_SpatiallyVarying_Reflectance_Estimation_from_Virtual_Exemplars\figure_9.jpg
  Figure 9 caption: Joint evaluation for the surface normals and BRDFs. We compare
    on synthetic synthetic data for the proposed technique with the joint scheme of
    surface normals estimating using [30] and BRDF estimation using [40]. Insets on
    the top-left are the angular errors and euclidean intensity differences for the
    relighting, shown for both the joint scheme and the proposed approach. The proposed
    technique outperforms the competing methods in both normal estimation and novel
    image synthesis.
  First author gender probability: 0.96
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Zhuo Hui
  Name of the last author: Aswin C. Sankaranarayanan
  Number of Figures: 13
  Number of Tables: 2
  Number of authors: 2
  Paper title: Shape and Spatially-Varying Reflectance Estimation from Virtual Exemplars
  Publication Date: 2016-11-01 00:00:00
  Table 1 caption:
    table_text: ''
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: ''
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2623613
