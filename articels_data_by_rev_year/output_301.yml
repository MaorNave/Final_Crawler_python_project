- Affiliation of the first author: "media integration and communication center (micc),\
    \ universit\xE0 di firenze, firenze 50134, italy"
  Affiliation of the last author: "media integration and communication center (micc),\
    \ universit\xE0 di firenze, firenze 50134, italy"
  Figure 1 Link: articels_figures_by_rev_year\2014\Person_ReIdentification_by_Iterative_ReWeighted_Sparse_Ranking\figure_1.jpg
  Figure 1 caption: Our feature descriptor. (a) An Epanechnikov kernel weights the
    contribution of each pixel to HS and RGB histograms computed on overlapping stripes
    (b) and (c). Overlapping HOG descriptors are concatenated with these (d).
  Figure 10 Link: articels_figures_by_rev_year\2014\Person_ReIdentification_by_Iterative_ReWeighted_Sparse_Ranking\figure_10.jpg
  Figure 10 caption: Iterative ranking and its effect on recall. (a) Average recognition
    rates with cutoffs for each iteration. (b) RER and recognition rate as function
    of rank.
  Figure 2 Link: articels_figures_by_rev_year\2014\Person_ReIdentification_by_Iterative_ReWeighted_Sparse_Ranking\figure_2.jpg
  Figure 2 caption: 'Basis expansion for MvsS re-identification on ETHZ1. Top: (left)
    probe sample, (right) the first 15 samples in the gallery, two instances for each
    subject ( N=2 ). Bottom: reconstruction coefficients for least squares ( lambda
    =0 ), sparse ( lambda =0.2 ) and nearest neighbour ( lambda =0.6 ). Each color
    represents a single subject which has two instances.'
  Figure 3 Link: articels_figures_by_rev_year\2014\Person_ReIdentification_by_Iterative_ReWeighted_Sparse_Ranking\figure_3.jpg
  Figure 3 caption: '(a) Ranking with limited information from a single basis expansion
    (MvsS, N = 2). Ranking decisions must be made on the basis of little information
    (low coefficient energy) or no information (zero coefficient energy). In the middle
    are reconstruction coefficients, at the bottom the corresponding normalized reconstruction
    errors for each multi-shot probe. (b) Effects of soft- and hard-weighting. Top:
    reconstruction coefficients from the first solution widehatboldsymbolalphas at
    iteration s ; Middle: refined reconstruction coefficients after soft-weighting
    widehatboldsymbolalphaprime s ; and Bottom: coefficients widehatboldsymbolalphas+1
    at iteration s+1 after hard-weighting. (c) The effects of soft-weighting on performance
    on the i-LIDS dataset.'
  Figure 4 Link: articels_figures_by_rev_year\2014\Person_ReIdentification_by_Iterative_ReWeighted_Sparse_Ranking\figure_4.jpg
  Figure 4 caption: Comparative performance evaluation on VIPeR and i-LIDS. (a) SvsS
    on VIPeR. (b) SvsS on i-LIDS. (c) MvsS on i-LIDS (N in lbrace 2,3rbrace ) . (d)
    MvsM on i-LIDS (N in lbrace 2,3rbrace ) . Dashed curves distinguish techniques
    that set aside a portion of the dataset for learning. In the legends we report
    the normalized area under the CMC curve (nAUC), when available.
  Figure 5 Link: articels_figures_by_rev_year\2014\Person_ReIdentification_by_Iterative_ReWeighted_Sparse_Ranking\figure_5.jpg
  Figure 5 caption: Performance on CAVIAR4REID with respect to the state-of-the-art.
    (a) SvsS. (b) MvsM for N in lbrace 3,5rbrace . In the legends we report the nAUC,
    when available.
  Figure 6 Link: articels_figures_by_rev_year\2014\Person_ReIdentification_by_Iterative_ReWeighted_Sparse_Ranking\figure_6.jpg
  Figure 6 caption: Comparison with state-of-the-art descriptors and analysis of the
    contribution of each descriptor component. (a) State-of-the-art descriptors in
    our ISR framework on i-LIDS. Solid lines represent MvsM (N = 2) and dashed lines
    represent SvsS. (b) ISR and our descriptor with different background models on
    VIPeR. (c) ISR and our descriptor with different pooling models on VIPeR. (d)
    The contribution of each descriptor component on the VIPeR dataset.
  Figure 7 Link: articels_figures_by_rev_year\2014\Person_ReIdentification_by_Iterative_ReWeighted_Sparse_Ranking\figure_7.jpg
  Figure 7 caption: Sensitivity to (a) illumination and (b) viewpoint changes.
  Figure 8 Link: articels_figures_by_rev_year\2014\Person_ReIdentification_by_Iterative_ReWeighted_Sparse_Ranking\figure_8.jpg
  Figure 8 caption: Rank-1 accuracy of ISR with our descriptor and PS over a range
    of misalignment error. Images are random samples of misaligned imagery.
  Figure 9 Link: articels_figures_by_rev_year\2014\Person_ReIdentification_by_Iterative_ReWeighted_Sparse_Ranking\figure_9.jpg
  Figure 9 caption: (a) Rank-1 accuracy on VIPeR and i-LIDS for SvsS. Accuracy is
    plotted for varying sparseness ( lambda ), including least squares ( lambda =
    0 ) and the nearest neighbor ( lambda approx 0.6 ) solutions. (b) Comparison of
    ISR with the Nearest Subspace Classifier on CAVIAR4REID. In the legend we report
    the number of instances per person ( N ) and number of learned subspaces ( k ).
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Giuseppe Lisanti
  Name of the last author: Alberto Del Bimbo
  Number of Figures: 11
  Number of Tables: 2
  Number of authors: 4
  Paper title: Person Re-Identification by Iterative Re-Weighted Sparse Ranking
  Publication Date: 2014-11-10 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance at Rank-1 with Respect to the State-of-the-Art
      on ETHZ
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance at Rank-1 with Respect to the State-of-the-Art
      on VIPeR, iLIDS and CAVIAR4REID
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2369055
- Affiliation of the first author: universite paris-est, ecole des ponts paristech,
    france
  Affiliation of the last author: ecole centrale de paris, france
  Figure 1 Link: articels_figures_by_rev_year\2014\A_Framework_for_Efficient_Structured_MaxMargin_Learning_of_HighOrder_MRF_Models\figure_1.jpg
  Figure 1 caption: (a) In MRFCRF training, one aims to learn a mapping f:Xrightarrow
    Y between a typically high-dimensional input space X and an output space of MRFCRF
    variables Y . In stereo matching, for instance, the elements of the input space
    X correspond to stereoscopic images, and the elements of the output space Y correspond
    to disparity maps. (b) In general, the mapping f(mathbf x) is defined as minimizing
    the energy EG(mathbf u(mathbf ymidmathbf w),mathbf h(mathbf ymidmathbf w)) of
    an MRFCRF model whose unary and higher-order potentials mathbf u(mathbf ymidmathbf
    w) , mathbf h(mathbf ymidmathbf w) are parameterized by mathbf w (the potentials
    also depend on mathbf x , but this is omitted here to simplify notation). Therefore,
    to fully specify this mapping it suffices to estimate mathbf w , which is what
    parameter learning aims to achieve in this case. (c) Our framework reduces, in
    a principled manner, the training of a complex MRF model into the parallel training
    of a series of easy-to-handle slave MRFs. The latter can be freely chosen so as
    to fully exploit the problem structure, which, in addition to efficiency, contributes
    a sufficient amount of flexibility and generality to our method.
  Figure 10 Link: articels_figures_by_rev_year\2014\A_Framework_for_Efficient_Structured_MaxMargin_Learning_of_HighOrder_MRF_Models\figure_10.jpg
  Figure 10 caption: Primal objective function during training of high-order Potts
    MRFs.
  Figure 2 Link: articels_figures_by_rev_year\2014\A_Framework_for_Efficient_Structured_MaxMargin_Learning_of_HighOrder_MRF_Models\figure_2.jpg
  Figure 2 caption: Pseudocode of learning via dual decomposition.
  Figure 3 Link: articels_figures_by_rev_year\2014\A_Framework_for_Efficient_Structured_MaxMargin_Learning_of_HighOrder_MRF_Models\figure_3.jpg
  Figure 3 caption: (a) Learnt pairwise potential V(cdot ) , (b) primal objective
    (30), (c) and average test error as a function of time for the image denoising
    problem.
  Figure 4 Link: articels_figures_by_rev_year\2014\A_Framework_for_Efficient_Structured_MaxMargin_Learning_of_HighOrder_MRF_Models\figure_4.jpg
  Figure 4 caption: (a) Noisy test image (b) Denoised image when using a function
    V(cdot ) estimated during the course of the learning algorithm (c) Denoised result
    when using the final V(cdot ) (d) Ground truth image. We also show below each
    image the corresponding MRF energy computed using the final estimated V(cdot )
    .
  Figure 5 Link: articels_figures_by_rev_year\2014\A_Framework_for_Efficient_Structured_MaxMargin_Learning_of_HighOrder_MRF_Models\figure_5.jpg
  Figure 5 caption: (a) Learnt function f(cdot ) and (b) primal objective (30) as
    a function of time for two different graph decompositions in the case of stereo-matching.
  Figure 6 Link: articels_figures_by_rev_year\2014\A_Framework_for_Efficient_Structured_MaxMargin_Learning_of_HighOrder_MRF_Models\figure_6.jpg
  Figure 6 caption: (a) Disparity maps for the 'Sawtooth', 'Poster' and 'Bull' stereo
    pairs. (b) Three disparity maps computed for the stereo pair 'Venus' using functions
    f(cdot ) estimated at different iterations of our learning algorithm (the final
    result is the one shown on the right).
  Figure 7 Link: articels_figures_by_rev_year\2014\A_Framework_for_Efficient_Structured_MaxMargin_Learning_of_HighOrder_MRF_Models\figure_7.jpg
  Figure 7 caption: (a) Learning objective function during MRF training with the hand
    data set. Boxplots of Dice coefficients for (b) 2D hand segmentation, and (c)
    3D left ventricle segmentation (the Dice coefficient is a similarity measure between
    sets X and Y , defined as frac2|Xcap Y||X|+|Y| ).
  Figure 8 Link: articels_figures_by_rev_year\2014\A_Framework_for_Efficient_Structured_MaxMargin_Learning_of_HighOrder_MRF_Models\figure_8.jpg
  Figure 8 caption: 2D hand segmentation results.
  Figure 9 Link: articels_figures_by_rev_year\2014\A_Framework_for_Efficient_Structured_MaxMargin_Learning_of_HighOrder_MRF_Models\figure_9.jpg
  Figure 9 caption: 3D Segmentation results on cardiac CT volumes. (Top row) Our results.
    (Bottom row) ASM results.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Nikos Komodakis
  Name of the last author: Nikos Paragios
  Number of Figures: 10
  Number of Tables: 0
  Number of authors: 3
  Paper title: A Framework for Efficient Structured Max-Margin Learning of High-Order
    MRF Models
  Publication Date: 2014-11-10 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2368990
- Affiliation of the first author: "willow project-team, inria, and d\xE9partement\
    \ d'informatique, de l'ecole normale sup\xE9rieure, ensinriacnrs umr 8548, paris,\
    \ france"
  Affiliation of the last author: "willow project-team, inria, and d\xE9partement\
    \ d'informatique, de l'ecole normale sup\xE9rieure, ensinriacnrs umr 8548, paris,\
    \ france"
  Figure 1 Link: articels_figures_by_rev_year\2014\Pose_Estimation_and_Segmentation_of_Multiple_People_in_Stereoscopic_Movies\figure_1.jpg
  Figure 1 caption: "Illustration of the steps of our proposed framework on a sample\
    \ frame (a) from the movie \u201CStreetDance\u201D. We compute the disparity map\
    \ (b) from the stereo pair. Occlusion-aware unary costs based on disparity and\
    \ articulated pose mask are computed for all the people detected in the scene.\
    \ In (c) we show the unary cost for the person labelled 1 . Pairwise smoothness\
    \ costs computed from disparity, motion, and colour features are shown in (d).\
    \ The range of values in (b, c, d) is denoted by the red (low)\u2014blue (high)\
    \ spectrum of colours. The estimated articulated pose for person 1 is shown in\
    \ (e). (f) shows the final segmentation result, where each colour represents a\
    \ unique person, and the numbers denote the front (0) to back (4) ordering of\
    \ people. (Best viewed in colour.)"
  Figure 10 Link: articels_figures_by_rev_year\2014\Pose_Estimation_and_Segmentation_of_Multiple_People_in_Stereoscopic_Movies\figure_10.jpg
  Figure 10 caption: 'Comparison of segmentation performed: (a) individually on each
    frame; and (b) temporally on video. We overlay the result of our person detector
    on each image. We observe that the temporal consistency term reduces leaking (Row
    1, rightmost person). It also helps segment more people in the scene accurately
    (Rows 2 and 3). (Best viewed in colour.)'
  Figure 2 Link: articels_figures_by_rev_year\2014\Pose_Estimation_and_Segmentation_of_Multiple_People_in_Stereoscopic_Movies\figure_2.jpg
  Figure 2 caption: "A graphical illustration of our model, where the observed variables\
    \ are shaded. The variable d i in the graph represents the features computed at\
    \ each pixel i in the video. For clarity, we show 4 pixels from a frame, and two\
    \ of the temporal links (dashed line), which connect pixels in one frame to the\
    \ next. The person label x i and disparity parameters \u03C4 are inferred given\
    \ the image features d i , and the pose parameters \u0398 ."
  Figure 3 Link: articels_figures_by_rev_year\2014\Pose_Estimation_and_Segmentation_of_Multiple_People_in_Stereoscopic_Movies\figure_3.jpg
  Figure 3 caption: Illustration of the occlusion-based unary costs for the example
    in Fig. 1 . From left to right we show the unary costs for persons labelled 0-4
    and the background. The cost for a pixel to take a label (person or background)
    is denoted by the red (low)-blue (high) spectrum of colours. Here we observe the
    effect of accumulating the label likelihoods in a front-to-back order. For example,
    in the illustration for Person 4, a low cost (red) for taking label 4 is observed
    only for the pixels that are not occluded by the other people in front. ( Best
    viewed in colour.)
  Figure 4 Link: articels_figures_by_rev_year\2014\Pose_Estimation_and_Segmentation_of_Multiple_People_in_Stereoscopic_Movies\figure_4.jpg
  Figure 4 caption: "Articulated pose masks for three mixture components are shown\
    \ for some of the body parts. The pose masks for each part capture a different\
    \ configuration of the pose. For instance, the masks for \u201CLeft wrist\u201D\
    \ show three different locations of the lower arm: stretched out, partially bent\
    \ over the shoulder, and lying by the torso. ( Best viewed in colour.)"
  Figure 5 Link: articels_figures_by_rev_year\2014\Pose_Estimation_and_Segmentation_of_Multiple_People_in_Stereoscopic_Movies\figure_5.jpg
  Figure 5 caption: Estimated poses and masks on sample frames. Given a pose estimate
    (a), we compute a pose-specific mask (b) using per-mixture part masks learnt from
    manually segmented training data. In (c) we show a scaled version of the masks,
    doubling the actual distances between part masks. This visually explains how each
    per-mixture mask is contributing to the final mask. In (b,c), the cost for a pixel
    to take a person label is denoted by the red (low) - blue (high) spectrum of colours.
    (Best viewed in colour.)
  Figure 6 Link: articels_figures_by_rev_year\2014\Pose_Estimation_and_Segmentation_of_Multiple_People_in_Stereoscopic_Movies\figure_6.jpg
  Figure 6 caption: "The front-to-back ordering of people in a scene is determined\
    \ by \u03C4 l , the disparity parameter in the potential (7), estimated for each\
    \ person (shown at the top). The optimal set \u03C4 \u2217 is estimated jointly\
    \ for all the people by solving (9) over a candidate set. Here we show the effect\
    \ of picking wrong \u03C4 l for two people, which implies wrong ordering (shown\
    \ at the bottom). This results in poor unary cost functions and a higher overall\
    \ cost, due to the additional negative evidence in the form of (1\u2212 \u03B2\
    \ m i ) as defined in (5). The colours red, yellow and blue in the unary cost\
    \ figures represent low, medium and high costs respectively. Unaries (here for\
    \ persons 2 and 4) are combined (third column) by taking their per-pixel minimum,\
    \ as described in Section 4.1. Note the lower cost (more red) of the combined\
    \ unary for the correct person ordering. (Best viewed in colour.)"
  Figure 7 Link: articels_figures_by_rev_year\2014\Pose_Estimation_and_Segmentation_of_Multiple_People_in_Stereoscopic_Movies\figure_7.jpg
  Figure 7 caption: Precision-recall curves for person detection based on Yang and
    Ramanan [19] (Y. & R.; dashed lines) and Felzenszwalb et al. [36] (solid lines)
    methods. For both methods we report the performance of the appearance (HOG) and
    disparity (HOGdisp) based detectors, as well as the jointly trained appearance
    and disparity based detector (HOGcomb). HOGcomb, the detector based on [36] performs
    significantly better than the other models. (Best viewed in colour.)
  Figure 8 Link: articels_figures_by_rev_year\2014\Pose_Estimation_and_Segmentation_of_Multiple_People_in_Stereoscopic_Movies\figure_8.jpg
  Figure 8 caption: "Pose estimation results. Buffy-HOG is the upper-body model from\
    \ [19], and Streetdance- corresponds to our models trained on appearance orand\
    \ disparity features extracted from the 3D movie Streetdance. (a) Mean-APK curves,\
    \ which are produced by varying the \u03B3 threshold. (b) & (c) Precision-recall\
    \ curves for left elbow and left wrist respectively. Using disparity cues improves\
    \ the recall of the pose estimator for elbows, and combining them with appearance\
    \ cues shows a good initial precision performance. Estimating the wrist position\
    \ remains a challenge, and the overall performance for this part is similar to\
    \ [19]. (Best viewed in colour.)"
  Figure 9 Link: articels_figures_by_rev_year\2014\Pose_Estimation_and_Segmentation_of_Multiple_People_in_Stereoscopic_Movies\figure_9.jpg
  Figure 9 caption: "Qualitative results on images from the movies \u201CStreetDance\u201D\
    \ and \u201CPina\u201D. Each row shows the original image and the corresponding\
    \ segmentation. Rows 1 and 2 demonstrate successful handling of occlusion between\
    \ several people. The method can also handle non-trivial poses, as shown by Rows\
    \ 3 and 4. The segmentation results are generally accurate, although some inaccuracies\
    \ still remain on very difficult examples. For instance, in Row 1, the segmentation\
    \ for the people in the background for persons 3 and 5, due to the weak disparity\
    \ cue for these people far away from the camera. The numbers denote the front\
    \ (low values) to back (high values) ordering of people. ( Best viewed in colour.)"
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Guillaume Seguin
  Name of the last author: Ivan Laptev
  Number of Figures: 15
  Number of Tables: 4
  Number of authors: 4
  Paper title: Pose Estimation and Segmentation of Multiple People in Stereoscopic
    Movies
  Publication Date: 2014-11-10 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Evaluating Pose Estimation
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Evaluation of Pixel-Wise Person Segmentation on Our Inria
      3DMovie Dataset
  Table 3 caption:
    table_text: TABLE 3 Evaluation of Pixel-Wise Person Segmentation on Our Inria
      3DMovie Dataset Using Ground Truth Components
  Table 4 caption:
    table_text: TABLE 4 Evaluation of Pixel-Wise Person Segmentation on the H2view
      Dataset
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2369050
- Affiliation of the first author: school of software engineering, tongji university,
    shanghai, china
  Affiliation of the last author: microsoft research asia, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2014\Sketch_Matching_on_Topology_Product_Graph\figure_1.jpg
  Figure 1 caption: Examples of large irregularity and variations in hand drawn sketches.
    (a) Two regular shape templates; (b)-(e) sketches from four different users. See
    Section 4 for more details on our dataset.
  Figure 10 Link: articels_figures_by_rev_year\2014\Sketch_Matching_on_Topology_Product_Graph\figure_10.jpg
  Figure 10 caption: Example partial matching results using six different methods.
    The leftmost sketch in the shadow is the partial query. For each query the top
    10 matched sketch shapes (from left to right) from the database are shown. The
    positive results are labeled with green frames.
  Figure 2 Link: articels_figures_by_rev_year\2014\Sketch_Matching_on_Topology_Product_Graph\figure_2.jpg
  Figure 2 caption: Examples of endpoint refinement in [12]. (a) a raw sketch; (b)
    pulling endpoints; (c) deleting extra points.
  Figure 3 Link: articels_figures_by_rev_year\2014\Sketch_Matching_on_Topology_Product_Graph\figure_3.jpg
  Figure 3 caption: Examples of topology relations between primitives.
  Figure 4 Link: articels_figures_by_rev_year\2014\Sketch_Matching_on_Topology_Product_Graph\figure_4.jpg
  Figure 4 caption: Fine-grained topology graph. (a) a sketch; (b) its topology graph,
    shown with topology and geometry features.
  Figure 5 Link: articels_figures_by_rev_year\2014\Sketch_Matching_on_Topology_Product_Graph\figure_5.jpg
  Figure 5 caption: Construction of topology product graph. (a)(b) two topology graphs;
    (c) their topology product graph. The common structures are shown in blue. The
    solid edges in (c) denote the corresponding nodes with the same topology relation
    (identically labeled edges). The dashed edges denote that no topology relation
    is presented (no connected edges). The edge weight measures the dissimilarity
    of the geometry features on two corresponding edges.
  Figure 6 Link: articels_figures_by_rev_year\2014\Sketch_Matching_on_Topology_Product_Graph\figure_6.jpg
  Figure 6 caption: 'An example of sketch synthesis from regular shape in [23]. Left:
    the regular shape. Middle to right: synthesized sketches by three levels of deformation.'
  Figure 7 Link: articels_figures_by_rev_year\2014\Sketch_Matching_on_Topology_Product_Graph\figure_7.jpg
  Figure 7 caption: Examples of hand drawn sketches in our new dataset.
  Figure 8 Link: articels_figures_by_rev_year\2014\Sketch_Matching_on_Topology_Product_Graph\figure_8.jpg
  Figure 8 caption: Comparison of six methods for complete and partial matching. (a)
    PR curves for complete matching; (b) PR curves for partial matching; (c) Mean
    Average Precisions for complete and partial matching.
  Figure 9 Link: articels_figures_by_rev_year\2014\Sketch_Matching_on_Topology_Product_Graph\figure_9.jpg
  Figure 9 caption: Comparison of six methods for partial matching. (a)(b) Average
    precisionrecall for different K s; (c) Average precision with respect to the incompleteness
    of the query shape for K=1 .
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.73
  Name of the first author: Shuang Liang
  Name of the last author: Yichen Wei
  Number of Figures: 11
  Number of Tables: 1
  Number of authors: 4
  Paper title: Sketch Matching on Topology Product Graph
  Publication Date: 2014-11-10 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Definition of Topology Relations between Two Primitives P
      1 and P 2
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2369031
- Affiliation of the first author: department of computer science, university of illinois
    at urbana-champaign, champaign, il.
  Affiliation of the last author: department of computer science, university of illinois
    at urbana-champaign, champaign, il.
  Figure 1 Link: articels_figures_by_rev_year\2014\Learning_Discriminative_Collections_of_Part_Detectors_for_Object_Recognition\figure_1.jpg
  Figure 1 caption: "Averaged patches of top 15 detections on held-out set for a subset\
    \ of \u201Cdog\u201D part detectors that model different parts, poses, and shapes.\
    \ See Fig. 5 for more examples."
  Figure 10 Link: articels_figures_by_rev_year\2014\Learning_Discriminative_Collections_of_Part_Detectors_for_Object_Recognition\figure_10.jpg
  Figure 10 caption: Relative changes in detection performance on validation using
    different combinations of region features over baseline features. Baseline uses
    shape, aspect ratio, and relative size.
  Figure 2 Link: articels_figures_by_rev_year\2014\Learning_Discriminative_Collections_of_Part_Detectors_for_Object_Recognition\figure_2.jpg
  Figure 2 caption: Overview of our part-based detection. Our approach is to train
    a large number of part detectors with a single positive exemplar (patch or whole
    object), select a subset of diverse and discriminative candidates, and refine
    models by incorporating additional consistent training examples. Parts are used
    to classify bottom-up region proposals into object categories using a boosting
    classifier, and part predictions are used to predict the the object bounding box.
  Figure 3 Link: articels_figures_by_rev_year\2014\Learning_Discriminative_Collections_of_Part_Detectors_for_Object_Recognition\figure_3.jpg
  Figure 3 caption: An example of a learned multiresolution part model for a bicycle
    wheel. The top visualizes the positive weights, whereas the bottom visualizes
    the negative. The left model is the standard resolution part model. The right
    model is the double resolution. Notice how the double resolution better captures
    the spoke orientations.
  Figure 4 Link: articels_figures_by_rev_year\2014\Learning_Discriminative_Collections_of_Part_Detectors_for_Object_Recognition\figure_4.jpg
  Figure 4 caption: Illustration of our sigmoid learner.
  Figure 5 Link: articels_figures_by_rev_year\2014\Learning_Discriminative_Collections_of_Part_Detectors_for_Object_Recognition\figure_5.jpg
  Figure 5 caption: Averages of patches from the top 15 detections on the held-out
    validation set for a random sampling of the parts trained for each category on
    the PASCAL VOC2010 training set. Note the diversity and spatial consistency of
    most parts. For dogs, different parts on similar portions of the face seem to
    account for differences across breeds. Some parts correspond to the face (left),
    others to the whole object (next to left), and others to a small detail, such
    as the eye or nose.
  Figure 6 Link: articels_figures_by_rev_year\2014\Learning_Discriminative_Collections_of_Part_Detectors_for_Object_Recognition\figure_6.jpg
  Figure 6 caption: All confidently detected parts the target category are overlaid
    on each image. As can be seen, parts are typically well-localized, and the most
    distinctive pieces of the object such as the animal face or vehicle wheels are
    detected in a manner robust to occlusion.
  Figure 7 Link: articels_figures_by_rev_year\2014\Learning_Discriminative_Collections_of_Part_Detectors_for_Object_Recognition\figure_7.jpg
  Figure 7 caption: All confidently detected parts from an incorrect category are
    overlaid on each image. Visualizations of incorrect detections typically yield
    intuitive explanations. Dog face parts are detected on the cat; sheep-standing-right
    parts are detected on the cow-standing-right; an airplane tail is hallucinated
    out of the skyline and edge of the van windshield; bicycle wheels are detected
    on the motorbike.
  Figure 8 Link: articels_figures_by_rev_year\2014\Learning_Discriminative_Collections_of_Part_Detectors_for_Object_Recognition\figure_8.jpg
  Figure 8 caption: Detection performance with varying number of positive examples.
    The BCP detection model is trained with only part responses (no region features)
    using varying number of positives. Error bars represent standard error of four
    independent trials.
  Figure 9 Link: articels_figures_by_rev_year\2014\Learning_Discriminative_Collections_of_Part_Detectors_for_Object_Recognition\figure_9.jpg
  Figure 9 caption: Fraction of top N detections (N = number of positive instances)
    due to localization error (blue), similar categories (red), dissimilar categories
    (green), background (orange), and correct (cyan) using analysis code from [25].
    For each category, the first row is our method; the second row, DPM [3]. Our method
    consistently has less confusion with background and more confusion with similar
    objects.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Kevin J Shih
  Name of the last author: Derek Hoiem
  Number of Figures: 13
  Number of Tables: 6
  Number of authors: 3
  Paper title: Learning Discriminative Collections of Part Detectors for Object Recognition
  Publication Date: 2014-11-20 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Evaluation of Part Detection and Spatial Consistency for
      Each Refinement Method Using Three Criteria: Mean partAP over All Parts of a
      Category (mAP), the Mean AP for Detecting the Top Three Keypoints for Each Part
      Type (3KP), and the Maximum AP for Each Keypoint over All Parts (xKP)'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Detection Comparison on PASCAL VOC2010 Test Set
  Table 3 caption:
    table_text: TABLE 3 Detection Validation with Different Classifiers and Localization
      Methods on PASCAL VOC2010 Validation
  Table 4 caption:
    table_text: TABLE 4 Multiresolution Part Comparison on PASCAL VOC2010 Validation
      Set
  Table 5 caption:
    table_text: TABLE 5 Part Sharing Comparison on PASCAL VOC2010 Validation Set
  Table 6 caption:
    table_text: TABLE 6 Results and Comparisons for Ground Truth Region Experiments
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2366122
- Affiliation of the first author: computer engineering and networks lab., eth zurich,
    switzerland
  Affiliation of the last author: computer engineering and networks lab., eth zurich,
    switzerland
  Figure 1 Link: articels_figures_by_rev_year\2014\A_Semidefinite_Programming_Based_Search_Strategy_for_Feature_Selection_with_Mutu\figure_1.jpg
  Figure 1 caption: Synergy between x and y features. While information of each individual
    feature about the class label (ellipse or line) is almost zero, their joint information
    can almost completely remove the class label ambiguity.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\A_Semidefinite_Programming_Based_Search_Strategy_for_Feature_Selection_with_Mutu\figure_2.jpg
  Figure 2 caption: Comparing the search strategies for mRMR measure with the Friedman
    test and its corresponding post-hoc analysis. The y-axis is the classification
    accuracy difference and x-axis indicates the names of the compared algorithms.
  Figure 3 Link: articels_figures_by_rev_year\2014\A_Semidefinite_Programming_Based_Search_Strategy_for_Feature_Selection_with_Mutu\figure_3.jpg
  Figure 3 caption: Comparing the search strategies for mRMR. Results of the post-hoc
    tests for each classifier.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.93
  Name of the first author: Tofigh Naghibi
  Name of the last author: Beat Pfister
  Number of Figures: 3
  Number of Tables: 7
  Number of authors: 3
  Paper title: A Semidefinite Programming Based Search Strategy for Feature Selection
    with Mutual Information Measure
  Publication Date: 2014-11-20 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Approximation Ratios of BE and COBRA for Different NP Values
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Datasets Descriptions
  Table 3 caption:
    table_text: TABLE 3 Estimating P by Searching over an Admissible Set that Minimizes
      the Classification Error-Rate
  Table 4 caption:
    table_text: TABLE 4 Comparison of COBRA with the Greedy Search Methods over Different
      Datasets
  Table 5 caption:
    table_text: TABLE 5 The Average (over 50 Similarity Ratios) Similarity Ratio for
      Four Datasets
  Table 6 caption:
    table_text: TABLE 6 Comparison of COBRA with QPFS and SOSS over Five Datasets
  Table 7 caption:
    table_text: TABLE 7 The Performance of the Classification Algorithms When Trained
      with COBRA Features Optimized for the LDA Classifier
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2372791
- Affiliation of the first author: school of software engineering, tongji university,
    shanghai, china
  Affiliation of the last author: advanced institute of translational medicine and
    the school of software engineering, tongji university, shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2014\D_Palmprint_Identification_Using_BlockWise_Features_and_Collaborative_Representa\figure_1.jpg
  Figure 1 caption: 3D palmprint acquisition device developed in [19].
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\D_Palmprint_Identification_Using_BlockWise_Features_and_Collaborative_Representa\figure_2.jpg
  Figure 2 caption: The first row displays three 3D palmprint ROIs, shown in image
    format while the second row displays their corresponding ST maps. (a) and (b)
    are captured from the same palm but in different sessions. (b) and (c) are from
    different palms.
  Figure 3 Link: articels_figures_by_rev_year\2014\D_Palmprint_Identification_Using_BlockWise_Features_and_Collaborative_Representa\figure_3.jpg
  Figure 3 caption: Illustration for the proposed CR-based framework for 3D palmprint
    identification.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.64
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Lin Zhang
  Name of the last author: Jianwei Lu
  Number of Figures: 3
  Number of Tables: 4
  Number of authors: 4
  Paper title: 3D Palmprint Identification Using Block-Wise Features and Collaborative
    Representation
  Publication Date: 2014-11-24 00:00:00
  Table 1 caption:
    table_text: TABLE 1 ST Labels Defined by Signs of Surface Curvatures [43]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Algorithm for CR-Based 3D Palmprint Identification
  Table 3 caption:
    table_text: TABLE 3 Recognition Rates by Using Different Features
  Table 4 caption:
    table_text: TABLE 4 Experimental Results by Using Various Methods
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2372764
- Affiliation of the first author: department of electrical and computer engineering,
    carnegie mellon university, pittsburgh, pa
  Affiliation of the last author: department of electrical and computer engineering,
    carnegie mellon university, pittsburgh, pa
  Figure 1 Link: articels_figures_by_rev_year\2014\ZeroAliasing_Correlation_Filters_for_Object_Recognition\figure_1.jpg
  Figure 1 caption: An example illustrating the differences between linear and circular
    correlation. Here, the input signal in (a) is correlated with itself by appropriately
    padding the input signal with zeros. The result is the linear correlation output
    (shown in (b)). This is accomplished with a DFT of size N=2times 16-1=31 . The
    circular correlation output (in part (c)) is generated with a DFT of size 16 .
    Because the signals are not zero-padded, the output in part (c) is a circular
    correlation. In this example, the effects of aliasing due to circular correlation
    are very obvious; in other examples, the effects may be less noticeable, but still
    can affect performance.
  Figure 10 Link: articels_figures_by_rev_year\2014\ZeroAliasing_Correlation_Filters_for_Object_Recognition\figure_10.jpg
  Figure 10 caption: "Vehicle \u201CPickup\u201D and background."
  Figure 2 Link: articels_figures_by_rev_year\2014\ZeroAliasing_Correlation_Filters_for_Object_Recognition\figure_2.jpg
  Figure 2 caption: "Overview of our proposed zero-aliasing correlation filters (ZACFs)\
    \ approach. Conventional CF designs result in templates that are non-zero for\
    \ all values. This means that, during the optimization, the correlation between\
    \ the template and the training images is in fact a circular correlation. In our\
    \ design, constraining the tail of the template using ZA constraints guarantees\
    \ that the optimization step corresponds to a linear correlation. This results\
    \ in correlation planes that resemble the original design criteria\u2013in this\
    \ case, a sharp peak with low side-lobes."
  Figure 3 Link: articels_figures_by_rev_year\2014\ZeroAliasing_Correlation_Filters_for_Object_Recognition\figure_3.jpg
  Figure 3 caption: Comparison of the conventional MACE, TDMACE and ZAMACE filters.
  Figure 4 Link: articels_figures_by_rev_year\2014\ZeroAliasing_Correlation_Filters_for_Object_Recognition\figure_4.jpg
  Figure 4 caption: Spatial domain plots of the templates before cropping. Note that
    the conventional MACE template has large values outside of the ranges of the desired
    template size, where as the ZAMACE template does not, as the ZA constraints force
    these values to zero. In these plots, we display the absolute value of the templates.
  Figure 5 Link: articels_figures_by_rev_year\2014\ZeroAliasing_Correlation_Filters_for_Object_Recognition\figure_5.jpg
  Figure 5 caption: Example correlation outputs for one of the training images. Note
    that the ZAMACE filter yields an output that is much sharper than the original
    MACE filter design.
  Figure 6 Link: articels_figures_by_rev_year\2014\ZeroAliasing_Correlation_Filters_for_Object_Recognition\figure_6.jpg
  Figure 6 caption: Illustration of the ZA constraints required for the 2-D formulation
    of the ZAMACE template.
  Figure 7 Link: articels_figures_by_rev_year\2014\ZeroAliasing_Correlation_Filters_for_Object_Recognition\figure_7.jpg
  Figure 7 caption: Illustration of the proximal step, which is performed in the spatial
    domain. All DFTs and Inverse-DFTs are in 2-D.
  Figure 8 Link: articels_figures_by_rev_year\2014\ZeroAliasing_Correlation_Filters_for_Object_Recognition\figure_8.jpg
  Figure 8 caption: "Computational times for filter design comparing RACF and the\
    \ proximal gradient method compared to a closed form solution. The \u201Ctraining\
    \ image size\u201D on the horizontal axis refers to N where the training image\
    \ size is Ntimes N ."
  Figure 9 Link: articels_figures_by_rev_year\2014\ZeroAliasing_Correlation_Filters_for_Object_Recognition\figure_9.jpg
  Figure 9 caption: Example images from different classes of vehicles.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Joseph A. Fernandez
  Name of the last author: B. V. K. Vijaya Kumar
  Number of Figures: 13
  Number of Tables: 6
  Number of authors: 4
  Paper title: Zero-Aliasing Correlation Filters for Object Recognition
  Publication Date: 2014-11-26 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance Comparison of Baseline CFs and ZACFs on the ORL
      Data Set
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance Comparison of Baseline CFs and ZACFs on the FRGC
      Data Set
  Table 3 caption:
    table_text: TABLE 3 CF Recognition Performance without Retraining, ATR Algorithm
      Development Image Database
  Table 4 caption:
    table_text: TABLE 4 CF Recognition Performance with Retraining, ATR Algorithm
      Development Image Database
  Table 5 caption:
    table_text: TABLE 5 Eye Localization Performance (%)
  Table 6 caption:
    table_text: 'TABLE 6 Object Detection: Average Precision (%)'
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2375215
- Affiliation of the first author: "department of electrical and computer engineering,\
    \ drexel university, 3120\u201340 market street, suite 313, philadelphia, pa"
  Affiliation of the last author: national research center for mathematics and computer
    science, the netherlands
  Figure 1 Link: articels_figures_by_rev_year\2014\Normalized_Compression_Distance_of_Multisets_with_Applications\figure_1.jpg
  Figure 1 caption: Example frames from two retinal progenitor cell image sequences
    showing segmentation (blue lines) and tracking (red lines) results. The type of
    cells the RPCs will eventually produce can be predicted by analyzing the multidimensional
    time sequence data obtained from the segmentation and tracking results. The NCD
    for multisets significantly improves the accuracy of the predictions.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\Normalized_Compression_Distance_of_Multisets_with_Applications\figure_2.jpg
  Figure 2 caption: Example MNIST digits. Classification accuracy for this application
    was improved by combining the proposed NCD for multisets with the pairwise NCD.
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Andrew R. Cohen
  Name of the last author: "Paul M.B. Vit\xE1nyi"
  Number of Figures: 2
  Number of Tables: 1
  Number of authors: 2
  Paper title: Normalized Compression Distance of Multisets with Applications
  Publication Date: 2014-11-26 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Partitioning Algorithm for Identifying Maximally Separated\
      \ Subsets for Each Class (Multiset) X , Partition X into Two Subsets A and B\
      \ Such That NC D 1 (AB)\u2212NC D 1 (A)\u2212NC D 1 (B) Is a Maximum"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2375175
- Affiliation of the first author: "center for visual computing, ecole centrale paris,\
    \ ch\xE2tenay-malabry, france"
  Affiliation of the last author: computer science department, stanford university,
    stanford, ca
  Figure 1 Link: articels_figures_by_rev_year\2014\Parameter_Estimation_and_Energy_Minimization_for_RegionBased_Semantic_Segmentati\figure_1.jpg
  Figure 1 caption: (a) Neighboring regions r1 , r2 and r3 , shown using solid lines,
    consist of a single super-pixel. The regions shown using dashed lines are formed
    by two super-pixels. Specifically, r4 = r1cup r2 , r5 = r1cup r3 and r6 = r2cup
    r3 . (b) The potentials corresponding to the clique of size 6 formed by the regions.
    The branches (horizontal lines) along the trellis (the vertical line on top of
    a region) represent the different labels that each region may take. We consider
    a two label case here. The unary potential overlinetheta r(i) is shown next to
    the i th branch of the trellis on top of region r . The pairwise potential overlinetheta
    rrprime (i,j) is shown next to the connection between the i th branch of r and
    the jth branch of rprime . The only non-zero potential overlinetheta rrprime (1,1)
    > 0 corresponds to selecting both the regions r and rprime . The optimal assignment
    of the clique must have an energy greater than 0 since at least two neighboring
    regions must be selected. (c) The optimal solution of the lp relaxation. The value
    of yr(i) is shown next to the i th branch of r and the value of yrrprime (i,j)
    is shown next to the connection between the i th and j th branches of r and rprime
    respectively. Note that the solution satisfies all cycles inequalities, that is,
    sum (r,rprime ) in cal EC yrrprime (0,0)+yrrprime (1,1) ge 1 , where cal EC is
    a cycle. Hence the solution lies within the feasible region of the relaxation.
    However, it can be easily verified that its objective function value is 0 , thereby
    proving that the relaxation is not tight.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\Parameter_Estimation_and_Energy_Minimization_for_RegionBased_Semantic_Segmentati\figure_2.jpg
  Figure 2 caption: Outputs obtained using segmentation-consistent inference during
    different iterations of spl. (a) Images annotated with generic classes. Column
    2 shows the segmentation (where the checkered patterns indicate generic classes).
    In columns 3-5, pixels labeled using the correct specific-class by segmentation-consistent
    inference are shown in white, while pixel labeled using the wrong specific-class
    are shown in black (we used the ground-truth specific-class segmentations of these
    images only for the purpose of illustration; these ground-truth segmentation were
    not used during training). A blue surrounding box on the output implies that the
    example was selected as easy by spl, while a red surrounding box indicates that
    is wasn't selected during the specified iteration. Note that spl discards the
    image where the cow (row 2) is incorrectly labeled. (b) Images annotated using
    bounding boxes. Column 2 shows the segmentation obtained using bounding box inference.
    The objects have been accurately segmented. Furthermore, spl discards the image
    where the sky (row 2) is incorrectly labeled.
  Figure 3 Link: articels_figures_by_rev_year\2014\Parameter_Estimation_and_Energy_Minimization_for_RegionBased_Semantic_Segmentati\figure_3.jpg
  Figure 3 caption: The first two rows show the results obtained for images from the
    voc2009 test set. Note that, unlike our approach that learns the parameters using
    lsvm on a large dataset, cll mislabels background pixels into the wrong specific
    classes. The last two rows show images from the sbd test set. While our approach
    is able to identify most of the foreground pixels correctly, cll mislabels them
    as background.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: M. Pawan Kumar
  Name of the last author: Daphne Koller
  Number of Figures: 3
  Number of Tables: 4
  Number of authors: 4
  Paper title: Parameter Estimation and Energy Minimization for Region-Based Semantic
    Segmentation
  Publication Date: 2014-12-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of the Notation Used in the Paper
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Mean and Standard Deviation of the Energy and the Pixel-Wise
      Accuracy over Four Folds Is Shown
  Table 3 caption:
    table_text: TABLE 3 Accuracies for the voc2009 Test Set
  Table 4 caption:
    table_text: TABLE 4 Accuracies for the sbd Test Set
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2372766
