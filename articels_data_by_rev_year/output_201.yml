- Affiliation of the first author: national key laboratory for novel software technology,
    nanjing university and collaborative innovation center of novel software technology
    and industrialization, nanjing, china
  Affiliation of the last author: national key laboratory for novel software technology,
    nanjing university and collaborative innovation center of novel software technology
    and industrialization, nanjing, china
  Figure 1 Link: articels_figures_by_rev_year\2014\Towards_Making_Unlabeled_Data_Never_Hurt\figure_1.jpg
  Figure 1 caption: There are multiple large-margin low-density separators coinciding
    well with labeled data (cross and triangle).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\Towards_Making_Unlabeled_Data_Never_Hurt\figure_2.jpg
  Figure 2 caption: Parameter influence with 10 labeled examples.
  Figure 3 Link: articels_figures_by_rev_year\2014\Towards_Making_Unlabeled_Data_Never_Hurt\figure_3.jpg
  Figure 3 caption: Training and testing time (in seconds) of s3vm and s4vms on uci
    data sets with linear kernel.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.69
  Name of the first author: Yu-Feng Li
  Name of the last author: Zhi-Hua Zhou
  Number of Figures: 3
  Number of Tables: 7
  Number of authors: 2
  Paper title: Towards Making Unlabeled Data Never Hurt
  Publication Date: 2014-01-13 00:00:00
  Table 1 caption:
    table_text: Table 1 Characteristics of the data sets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "Table 2 Comparison of accuracy (mean\xB1std.) Entries of semi-supervised\
      \ methods (S3VM, S3VM-c, S3VM-p, S3VM-us, S3VM best s , S3VM min s , S3VM com\
      \ s , S4VMa and S4VMs) are boldedunderlined if they are significantly betterworse\
      \ than SVM (paired t -tests at 95 percent significance level). '-' marks cases\
      \ suffering from high computational cost or memory overhead."
  Table 3 caption:
    table_text: "Table 3 Comparison of accuracy (mean\xB1std.) with out-of-sample\
      \ extension"
  Table 4 caption:
    table_text: "Table 4 Accuracy of SVM and accuracy improvements of S4VMs and S3VM\
      \ against SVM on different numbers of labeled data The accuracy Improvement\
      \ of algo against SVM is calculated by (ac c algo \u2212ac c svm ) . 'lin' stands\
      \ for the linear kernel."
  Table 5 caption:
    table_text: Table 5 Accuracy of SVM and accuracy improvements of S4VMs and S3VM
      on different numbers of unlabeled data
  Table 6 caption:
    table_text: "Table 6 Comparison of accuracy (Mean\xB1std.) when the balance constraint\
      \ is violated"
  Table 7 caption:
    table_text: "Table 7 Accuracy of other S3VMs (mean\xB1std.)"
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2299812
- Affiliation of the first author: australian centre for visual technologies and the
    computer vision group of the university of adelaide, sa, australia
  Affiliation of the last author: australian centre for visual technologies and the
    computer vision group of the university of adelaide, sa, australia
  Figure 1 Link: articels_figures_by_rev_year\2014\A_Hybrid_Loss_for_Multiclass_and_Structured_Prediction\figure_1.jpg
  Figure 1 caption: Training error with various number of classes. alpha = 0.5 for
    the hybrid loss. Fisher consistency analyses the behaviour of a loss observing
    the entire data population. Thus the training data are the entire data, so are
    the testing data. Consequently, the training error is the testing error.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\A_Hybrid_Loss_for_Multiclass_and_Structured_Prediction\figure_2.jpg
  Figure 2 caption: Performance of the hybrid, hinge, and log losses on non-dominantdominant
    mixtures. Points denote pairs of test accuracies for models trained on one of
    60 data sets using the losses named on the axes. Score (ab) denotes the vertical
    loss with a wins and b losses (ties not counted).
  Figure 3 Link: articels_figures_by_rev_year\2014\A_Hybrid_Loss_for_Multiclass_and_Structured_Prediction\figure_3.jpg
  Figure 3 caption: Estimated probabilities of the true label Dyi(xi) and most likely
    label Dyiast(xi) . Sentences are sorted according to Dyi(xi) and Dyiast(xi) respectively
    in ascending order. D=12 is shown as the straight black dot line. About 700 sentences
    out of 2,012 in the testing set and 2,000 sentences out of 8,936 in the training
    set have no dominant label.
  Figure 4 Link: articels_figures_by_rev_year\2014\A_Hybrid_Loss_for_Multiclass_and_Structured_Prediction\figure_4.jpg
  Figure 4 caption: Confusion matrices on the tvhi data set. For each class the best
    are highlighted by green rectangles. The hybrid loss achieves the best classification
    accuracy on three out of five action classes, i.e., other, hug and kiss.
  Figure 5 Link: articels_figures_by_rev_year\2014\A_Hybrid_Loss_for_Multiclass_and_Structured_Prediction\figure_5.jpg
  Figure 5 caption: 'Visualisation of some action predictions using different losses.
    First column: the input images; second column : the hinge loss results; third
    column: the log loss results; fourth column: the hybrid loss results. The pgms
    are superimposed on the images with green node and red node indicate correct and
    incorrect predictions respectively. Note ot, hs, hf, hg, ks denote five action
    classes: others, handshake, highfive, hug, kiss. The edges (line segments in pink)
    come from the interaction annotation in the data set, and are used to model the
    dependency between two action variables of subjects.'
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.55
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Qinfeng Shi
  Name of the last author: Zhenhua Wang
  Number of Figures: 5
  Number of Tables: 2
  Number of authors: 5
  Paper title: A Hybrid Loss for Multiclass and Structured Prediction
  Publication Date: 2014-02-14 00:00:00
  Table 1 caption:
    table_text: Table 1 Accuracy, precision, recall and F1 score on the CONLL2000
      text chunking task
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Table 2 Accuracy, precision, recall and F1 score on the BaseNP chunking
      task for training on increasing portions of training set
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2306414
- Affiliation of the first author: nicta, australia
  Affiliation of the last author: institut universitaire de france and cnrs, university
    of nice sophia antipolis, nice, france
  Figure 1 Link: articels_figures_by_rev_year\2014\Gentle_Nearest_Neighbors_Boosting_over_Proper_Scoring_Rules\figure_1.jpg
  Figure 1 caption: Plot phi of row D in Table 1 (left) and its matching posterior
    estimate hatpphi,h as a function of h in bb R (right).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\Gentle_Nearest_Neighbors_Boosting_over_Proper_Scoring_Rules\figure_2.jpg
  Figure 2 caption: Weight update wprime computed as a function of w and g(c,j) buildrelrm
    .over=eta (c,j)nj , when yic yjc = 1 and varepsilon = 1over 2 (see Table 1). The
    corresponding bcls are the binary logistic loss (left) and matsushita's loss (right).
    The black grid depicts the plane of equation wprime = w .
  Figure 3 Link: articels_figures_by_rev_year\2014\Gentle_Nearest_Neighbors_Boosting_over_Proper_Scoring_Rules\figure_3.jpg
  Figure 3 caption: "Average ( mu ) \xB1 Std dev. ( sigma ) for accuracy (left), f-measure\
    \ (center) and recall (right) over the small domains for all algorithms. In each\
    \ plot, algorithms are ordered from left to right in decreasing average of the\
    \ metric."
  Figure 4 Link: articels_figures_by_rev_year\2014\Gentle_Nearest_Neighbors_Boosting_over_Proper_Scoring_Rules\figure_4.jpg
  Figure 4 caption: 'Ranking results: colors indicate the number of times an algorithm
    ranked among the top-tier (green), second-tier (blue) and third-tier (red, for
    the worst eight algorithms) among all algorithms, over all small domains. For
    each color, the lighter the tone, the worse the rank. For example, dark green
    is rank 1, the lightest green is rank 7 and dark blue is rank 8. Algorithms are
    ordered from left to right in decreasing average of the metric at hand.'
  Figure 5 Link: articels_figures_by_rev_year\2014\Gentle_Nearest_Neighbors_Boosting_over_Proper_Scoring_Rules\figure_5.jpg
  Figure 5 caption: 'Ranking results contd: number of times each algorithm performed
    significantly better than the others (blue) or worse (red) according to student
    paired t -test ( p=.1 ). Algorithms order follow Figs. 3 and 4 (see text).'
  Figure 6 Link: articels_figures_by_rev_year\2014\Gentle_Nearest_Neighbors_Boosting_over_Proper_Scoring_Rules\figure_6.jpg
  Figure 6 caption: 'Manifold classification patterns for the accuracy (UP, commented),
    f-measure (bottom left) and recall (bottom right), for all 22 algorithms (see
    text); colors in hexagons cluster types of algorithms: red = gnnb, yellow = svm,
    pink = adaboost, cyan = SGD, blue = UNN, green = NN (see text and [27] for details).'
  Figure 7 Link: articels_figures_by_rev_year\2014\Gentle_Nearest_Neighbors_Boosting_over_Proper_Scoring_Rules\figure_7.jpg
  Figure 7 caption: Relative variation (in percent) of gnnbover unn, expressed as
    a function of the number of boosting rounds t . Positive values mean better results
    for GNNB; a dashed rectangle indicates the zone of negative values.
  Figure 8 Link: articels_figures_by_rev_year\2014\Gentle_Nearest_Neighbors_Boosting_over_Proper_Scoring_Rules\figure_8.jpg
  Figure 8 caption: "Left: frequency of cases among classes for the proportion of\
    \ examples used per class by GNNB; right: GNNB\u2217 ( k=200 ) versus sgd1 (conventions\
    \ follow Fig. 7)."
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Richard Nock
  Name of the last author: Michel Barlaud
  Number of Figures: 8
  Number of Tables: 5
  Number of authors: 5
  Paper title: Gentle Nearest Neighbors Boosting over Proper Scoring Rules
  Publication Date: 2014-02-24 00:00:00
  Table 1 caption:
    table_text: "Table 1 From left to right: examples of balanced convex losses \u03C8\
      \ \u03D5 (A, B, C, D; we let ln denote the base- e logarithm, and log z (x)\
      \ = . ln(x)ln(z) ); permissible functions \u03D5 ; value of \u03C0 \u2217 as\
      \ defined in (41); expression of update \u03B4 j in (10) for \u03B5= 1 2 ;expression\
      \ of the weight update in (11) (see text for details)"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Table 2 Domains used in our experiments, ordered in increasingnumber
      of classes, and then examples
  Table 3 caption:
    table_text: Table 3 Performance of our divide-and-conquer approach on large domains
      for gnnb( log ), using top-1 and top-5 accuracies
  Table 4 caption:
    table_text: "Table 4 Results on caltech (accuracy, f-measure and recall are \xD7\
      100 ) f is the number of features, and k=200 for nn, gnnb."
  Table 5 caption:
    table_text: Table 5 Results on SUN (conventions follow Table 4)
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2307877
- Affiliation of the first author: stanford university, menlo park, california
  Affiliation of the last author: department of engineering, university of cambridge,
    trumpington street, cambridge, england
  Figure 1 Link: articels_figures_by_rev_year\2014\Pitman_Yor_Diffusion_Trees_for_Bayesian_Hierarchical_Clustering\figure_1.jpg
  Figure 1 caption: 'A sample from the Dirichlet Diffusion Tree with N=4 datapoints.
    Top: the location of the Brownian motion for each of the four paths. Bottom: the
    corresponding tree structure. Each branch point corresponds to an internal tree
    node.'
  Figure 10 Link: articels_figures_by_rev_year\2014\Pitman_Yor_Diffusion_Trees_for_Bayesian_Hierarchical_Clustering\figure_10.jpg
  Figure 10 caption: Optimal trees learnt by the greedy EM algorithm for the DDT and
    PYDT on a synethic data set with D=2, N=100 .
  Figure 2 Link: articels_figures_by_rev_year\2014\Pitman_Yor_Diffusion_Trees_for_Bayesian_Hierarchical_Clustering\figure_2.jpg
  Figure 2 caption: 'A sample from the Pitman-Yor Diffusion Tree with N=4 datapoints
    and a(t)=1(1-t), theta =1, alpha =0 . Top: the location of the Brownian motion
    for each of the four paths. Bottom: the corresponding tree structure. Each branch
    point corresponds to an internal tree node.'
  Figure 3 Link: articels_figures_by_rev_year\2014\Pitman_Yor_Diffusion_Trees_for_Bayesian_Hierarchical_Clustering\figure_3.jpg
  Figure 3 caption: The effect of varying theta on the log probability of two tree
    structures (i.e., the product of the terms in Equation (22) over the segments
    in the tree), indicating the types of tree preferred. Small theta < 1 favours
    binary trees while larger values of theta favors higher order branching points.
  Figure 4 Link: articels_figures_by_rev_year\2014\Pitman_Yor_Diffusion_Trees_for_Bayesian_Hierarchical_Clustering\figure_4.jpg
  Figure 4 caption: A sample from the Pitman-Yor Diffusion Tree with N=20 datapoints
    and a(t)=1(1-t), theta =1, alpha =0 showing the branching structure including
    non-binary branch points.
  Figure 5 Link: articels_figures_by_rev_year\2014\Pitman_Yor_Diffusion_Trees_for_Bayesian_Hierarchical_Clustering\figure_5.jpg
  Figure 5 caption: Samples from the Pitman-Yor Diffusion Tree with N=1,000 datapoints
    in D=2 dimensions and a(t)=c(1-t) . As theta increases more obvious clusters appear.
  Figure 6 Link: articels_figures_by_rev_year\2014\Pitman_Yor_Diffusion_Trees_for_Bayesian_Hierarchical_Clustering\figure_6.jpg
  Figure 6 caption: 'Two measures of tree imbalance for samples from the binary Pitman-Yor
    Diffusion Tree with theta = -2alpha for varying alpha and N=100 . Solid lines:
    expected values calculated using recursion formulae. Points: empirical indices
    calculated using generated trees. Left: Colless''s index of balance, see Equation
    (37). Right: Proportion of unbalanced nodes, see Equation (38).'
  Figure 7 Link: articels_figures_by_rev_year\2014\Pitman_Yor_Diffusion_Trees_for_Bayesian_Hierarchical_Clustering\figure_7.jpg
  Figure 7 caption: A hierarchical partitioning of the integers 1,ldots,7 showing
    the underlying tree structure.
  Figure 8 Link: articels_figures_by_rev_year\2014\Pitman_Yor_Diffusion_Trees_for_Bayesian_Hierarchical_Clustering\figure_8.jpg
  Figure 8 caption: A draw from a S=10 -level nested Chinese restaurant process.
  Figure 9 Link: articels_figures_by_rev_year\2014\Pitman_Yor_Diffusion_Trees_for_Bayesian_Hierarchical_Clustering\figure_9.jpg
  Figure 9 caption: Steps involved in the slice sampling procedure. Width of the grey
    region perpendicular to each edge shows the unnormalised posterior F(x) on the
    remaining tree cal R , and the extent of this region shows cal I . Atoms in F(x)
    at the nodes are represented both schematically and mathematically as rectangles.
    a) Initial tree. b) Randomly chosen subtree is detached at x0 and we sample y
    sim U[0,F(x0)] . c) x1 sim hboxUniform[cal I1] is rejected because F(x1) < y and
    cal I1 is shrunk to give cal I2 . d) x2 sim hboxUniform[cal I2] is accepted because
    F(x2) > y . e) Subtree is reattached at x2 .
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: David A. Knowles
  Name of the last author: Zoubin Ghahramani
  Number of Figures: 15
  Number of Tables: 0
  Number of authors: 2
  Paper title: Pitman Yor Diffusion Trees for Bayesian Hierarchical Clustering
  Publication Date: 2014-03-21 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2313115
- Affiliation of the first author: australian national university and national ict,
    australia
  Affiliation of the last author: department of computing, macquarie university, sydney,
    australia
  Figure 1 Link: articels_figures_by_rev_year\2014\Differential_Topic_Models\figure_1.jpg
  Figure 1 caption: Differential topic modeling using the TPYP. The top level is an
    abstract space that generates each sub-space for each group of documents. Each
    group's vocabulary subspace is formed by taking a transformation from the top
    abstract space.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\Differential_Topic_Models\figure_2.jpg
  Figure 2 caption: Graphical model of differential topic modeling.
  Figure 3 Link: articels_figures_by_rev_year\2014\Differential_Topic_Models\figure_3.jpg
  Figure 3 caption: "Illustrates the latent variables associated with the mikw=6 words\
    \ of index w with topic k in collection i : each table has a single \u201Chead\
    \ of table\u201D marked in red and there are tikw=3 in total. The head must choose\
    \ a single word in the abstract space to associate with, its entry is in vecvikw\
    \ ."
  Figure 4 Link: articels_figures_by_rev_year\2014\Differential_Topic_Models\figure_4.jpg
  Figure 4 caption: An example of topic differences between the blogs of Daily Kos
    (green boxes, size proportional to the frequency of the topic), Democrats, and
    Right Wing News (red boxes), Republicans, best viewed in color. The arcs represent
    the similarity strength of topic pairs. Word sizes are proportional to their frequencies.
  Figure 5 Link: articels_figures_by_rev_year\2014\Differential_Topic_Models\figure_5.jpg
  Figure 5 caption: Some word association structures on MLJ data set. The words in
    the eclipses are from the global vocabulary, each corresponds to a set of words
    (in the colored boxes) in each group, represented by the statistics tildeqkwvi
    in (6), the numbers following the words represent the strength of the correlations
    in range [0, 1] . Best viewed in color.
  Figure 6 Link: articels_figures_by_rev_year\2014\Differential_Topic_Models\figure_6.jpg
  Figure 6 caption: Perplexities versus topics on the five data sets, best viewed
    in color.
  Figure 7 Link: articels_figures_by_rev_year\2014\Differential_Topic_Models\figure_7.jpg
  Figure 7 caption: "Comparison of test perplexity for different algorithms and data\
    \ sets. Each point corresponds to one parameter setting. \u201CA-B\u201D in the\
    \ legend indicates the algorithm \u201CA\u201D and data set \u201CB\u201D, while\
    \ subscripts \u201Cccp\u201D and \u201Ccc\u201D mean the data set with and without\
    \ tokenisation preprocessing, \u201Cwn\u201D and \u201Cco\u201D mean the two kinds\
    \ of transformation matrices. Postscript \u201C - 1\u201D in (c) means the original\
    \ data sets without stop word removal."
  Figure 8 Link: articels_figures_by_rev_year\2014\Differential_Topic_Models\figure_8.jpg
  Figure 8 caption: 'Results on image data sets. Left: an example topic. Top: average
    of patches in its top 49 visual words; bottom: the locations of these patches
    on the images. Right: object localization. x-axis: topics considered; y-axis:
    average localization ratio over topics (larger is better).Dash line: scores on
    groups; Solid: average scores. Best viewed in color.'
  Figure 9 Link: articels_figures_by_rev_year\2014\Differential_Topic_Models\figure_9.jpg
  Figure 9 caption: "10 topics for the three groups \u201CV\u201D, \u201CX\u201D and\
    \ \u201CZ\u201D from TII (left) and LDA (right). The first column contains random\
    \ samples for three groups, the others are the corresponding 10 topics. The second\
    \ column of TII topics reveals different structures among the characters while\
    \ the other columns represent shared structures."
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: Changyou Chen
  Name of the last author: Lan Du
  Number of Figures: 9
  Number of Tables: 2
  Number of authors: 5
  Paper title: Differential Topic Models
  Publication Date: 2014-03-21 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Two Topic Hierarchies for GENT Data Set
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Classification Accuracies on the Three Data Sets
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2313127
- Affiliation of the first author: amazon berlin, berlin, germany
  Affiliation of the last author: gatsby computational neuroscience unit, csml, university
    college, london, u.k
  Figure 1 Link: articels_figures_by_rev_year\2014\Latent_IBP_Compound_Dirichlet_Allocation\figure_1.jpg
  Figure 1 caption: 'Ordered word frequencies of the four benchmark corpora that will
    be considered in the experiments (see Section 5 for a detailed description). Let
    fw be the frequency of word w in the corpus. It can be observed that the ranked
    word frequencies follow Zipf''s law, which is an example of a power-law distribution:
    p(fw)propto fw-c , where c is a positive constant. Like many natural phenomena,
    human languages including english exhibit this property. Intuitively, this means
    that human languages have a very heavy tail: few words are extremely frequent,
    while many words are very infrequent.'
  Figure 10 Link: articels_figures_by_rev_year\2014\Latent_IBP_Compound_Dirichlet_Allocation\figure_10.jpg
  Figure 10 caption: Histogram of the number of topics per word for 20 newsgroup.
    Most words are assigned to a relatively small number of topics in ftm and lida,
    which increases the diversity of the topics.
  Figure 2 Link: articels_figures_by_rev_year\2014\Latent_IBP_Compound_Dirichlet_Allocation\figure_2.jpg
  Figure 2 caption: Ordered feature frequencies drawn from a three-parameter IBP with
    parameters eta =10 and delta = 1 . The features are power-law distributed when
    sigma >0 .
  Figure 3 Link: articels_figures_by_rev_year\2014\Latent_IBP_Compound_Dirichlet_Allocation\figure_3.jpg
  Figure 3 caption: Binary matrices sampled from a three-parameter IBP. The mass parameter
    eta is constant, the concentration parameter delta increases across columns, while
    the discount parameter sigma increases across rows. In all cases the expected
    number of non-zero entries is eta D . The amount of features that is shared decreases
    when the concentration parameter delta increases. The discount parameter sigma
    is responsible for the power-law characteristic. The top left matrix is drawn
    from a one-parameter IBP, while the matrix below is drawn from a two-parameter
    IBP with the same mass parameter, but a larger concentration parameter.
  Figure 4 Link: articels_figures_by_rev_year\2014\Latent_IBP_Compound_Dirichlet_Allocation\figure_4.jpg
  Figure 4 caption: Left column matrices are generated from a dirichlet distribution
    (like in hdp-lda) with mass parameter alpha in 10,0.1,0.01 . It can be observed
    that the amount of sparsity for each column of the matrix is similar; if one is
    interested in sparse topics, then all of them show a similar level of sparsity.
    The matrices in right column are generated from an ICDP (like in lida). It can
    be observed that the amount of sparsity varies across the columns of the matrix.
  Figure 5 Link: articels_figures_by_rev_year\2014\Latent_IBP_Compound_Dirichlet_Allocation\figure_5.jpg
  Figure 5 caption: 'Graphical models for the different configurations: HDP-LDA: solid
    arrows only; four-parameter ftm: solid + dashed arrows; lida: solid + dashed +
    dotted arrows. Nodes indicate random variables (gray: observed variables; blue:
    latent variables; orange: hyperparameters). Rectangle plates correspond to repetitions.'
  Figure 6 Link: articels_figures_by_rev_year\2014\Latent_IBP_Compound_Dirichlet_Allocation\figure_6.jpg
  Figure 6 caption: Training log-perplexity for 10 randomly generated corpora. It
    can be observed that the sampler converges relatively quickly.
  Figure 7 Link: articels_figures_by_rev_year\2014\Latent_IBP_Compound_Dirichlet_Allocation\figure_7.jpg
  Figure 7 caption: "Toy example\u2014convergence of the hyperparameters based on\
    \ 10 random initialisations and fixed sigma . The correct value is indicated by\
    \ the constant dashed line. In all cases, the hyperparameter samples converge\
    \ to a value close to the ground truth. However, alpha and eta show a small bias."
  Figure 8 Link: articels_figures_by_rev_year\2014\Latent_IBP_Compound_Dirichlet_Allocation\figure_8.jpg
  Figure 8 caption: Convergence of the hyperparameters for nips when zeta =0.25 .
    The blue, red and green curves correspond respectively to sigma =0 , sigma =0.10
    and sigma =0.25 .
  Figure 9 Link: articels_figures_by_rev_year\2014\Latent_IBP_Compound_Dirichlet_Allocation\figure_9.jpg
  Figure 9 caption: Histogram of the number of topics per document. The FTM and lida
    assign more topics to the documents compared to HDP-LDA. We do not note a significant
    difference between ftm and lida, even though lida is consistently outperforming
    FTM in terms of test log-perplexity.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "C\xE9dric Archambeau"
  Name of the last author: Guillaume Bouchard
  Number of Figures: 10
  Number of Tables: 3
  Number of authors: 3
  Paper title: Latent IBP Compound Dirichlet Allocation
  Publication Date: 2014-03-28 00:00:00
  Table 1 caption:
    table_text: Table 1 Data characteristics and average log-perplexity (with standard
      errors) on held-out data LIDA outperforms all other methods, except on 20 newsgroups,
      where the four-parameter FTM performs best.
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Table 2 Examples of random topics associated with reuters the topics
      were matched using the minimum mean absolute error with respect to the topics
      extracted by FTM.
  Table 3 caption:
    table_text: Table 3 Examples of random topics associated with NIPS The topics
      were matched using the minimum mean absolute error with respect to the topics
      extracted by FTM.
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2313122
- Affiliation of the first author: school of computer science, carnegie mellon university,
    5000 forbes avenue, smith hall, pittsburgh, pa
  Affiliation of the last author: school of computer science, carnegie mellon university,
    5000 forbes avenue, smith hall, pittsburgh, pa
  Figure 1 Link: articels_figures_by_rev_year\2014\DataDriven_Objectness\figure_1.jpg
  Figure 1 caption: We estimate the likelihood of each region in the input image to
    be an object, i.e., the objectness , by using a large pool of regions from prior
    data.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\DataDriven_Objectness\figure_2.jpg
  Figure 2 caption: Examples of high objectness image regions, their corresponding
    matched exemplar regions and the boundaries of segments weighted by their objectness
    estimated using our approach. The input image regions and the majority of the
    matched exemplar regions belong to different object categories.
  Figure 3 Link: articels_figures_by_rev_year\2014\DataDriven_Objectness\figure_3.jpg
  Figure 3 caption: Exemplar regions matched using HOG features. The left side of
    the vertical bar shows the input image segment, while the right shows the matched
    database exemplar regions.
  Figure 4 Link: articels_figures_by_rev_year\2014\DataDriven_Objectness\figure_4.jpg
  Figure 4 caption: Unary properties of image segments. (a) Regions that are more
    similar to the exemplar regions are more likely to be objects, (b) compared with
    the ones that are less similar to the exemplar regions.
  Figure 5 Link: articels_figures_by_rev_year\2014\DataDriven_Objectness\figure_5.jpg
  Figure 5 caption: Mutual consistency between nearest exemplar regions. Our algorithm
    needs to discover the high objectness regions which match with exemplar regions
    that are mutually consistent (a), because when the database is very large, almost
    any image region can be matched with several objects (b).
  Figure 6 Link: articels_figures_by_rev_year\2014\DataDriven_Objectness\figure_6.jpg
  Figure 6 caption: Different exemplar regions have different prior probability to
    appear in a daily environment and matching input regions. (a) Regions that match
    well with exemplar regions with high prior probability also have high objectness.
    (b) Regions that match with exemplar regions that have low prior probability of
    occurrence are less likely to be object regions. (c),(d) We down-weigh the exemplar
    regions that are frequently matched with many image regions.
  Figure 7 Link: articels_figures_by_rev_year\2014\DataDriven_Objectness\figure_7.jpg
  Figure 7 caption: Comparing our approach with existing techniques [2] [3][29]. (a)
    and (b) compares the generalizability of different approaches to different segmentation
    methods. (c) compares more approaches using the bounding box overlap criteria.
    (d) shows the effect of different features in our approach.
  Figure 8 Link: articels_figures_by_rev_year\2014\DataDriven_Objectness\figure_8.jpg
  Figure 8 caption: Comparison results of our approach with existing techniques. (Best
    viewed in color.)
  Figure 9 Link: articels_figures_by_rev_year\2014\DataDriven_Objectness\figure_9.jpg
  Figure 9 caption: We apply our approach to object discovery and compare with our
    previous approach of Kang et al. [1] in both object discovery performance and
    computational time. Please see text for details.
  First author gender probability: 0.83
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Hongwen Kang
  Name of the last author: Takeo Kanade
  Number of Figures: 9
  Number of Tables: 1
  Number of authors: 4
  Paper title: Data-Driven Objectness
  Publication Date: 2014-04-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Examples of the 10 Objects with the Highest Prior Probability
      and the 10 Objects with the Lowest Prior Probability
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2315811
- Affiliation of the first author: google inc., 1600 amphitheatre parkway, mountain
    view, ca
  Affiliation of the last author: institute for adaptive and neural computation, school
    of informatics, university of edinburgh, 10 crichton street, edinburgh eh8 9ab,
    united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2014\The_Supervised_Hierarchical_Dirichlet_Process\figure_1.jpg
  Figure 1 caption: The supervised HDP model where the observed variables are the
    words wij denoting word j in document i and the document label yi .
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\The_Supervised_Hierarchical_Dirichlet_Process\figure_2.jpg
  Figure 2 caption: Results for the test data sets after five-fold cross-validation.
    Classification results are given for (a) the newswire data set and (b) the movie
    snippet data set. Regression results ( R2 ) for the entire data set are given
    for (c) the movie reviews data set and (d) the document popularity data set. sLDA
    Gibbs performance is shown for each number of topics. Variational EM performed
    as well as or worse than Gibbs sampling and is omitted for space. For sHDP, the
    performance with MAP parameters, with parameters sampled from their posteriors
    and with a two-step supervised approach where an unsupervised HDP model is learnt
    and a GLM model trained on top of that is shown. The upper and lower bars show
    the minimum and maximum performance of each method relative to the performance
    of the sampled sHDP (minimum and maximum taken over the five folds). This allows
    the reader to see whether a method performs better or worse than sampled sHDP
    across all the folds.
  Figure 3 Link: articels_figures_by_rev_year\2014\The_Supervised_Hierarchical_Dirichlet_Process\figure_3.jpg
  Figure 3 caption: Chain convergence for (TOP) the sHDP model and (BOTTOM) the Gibbs
    sampled sLDA model with 40 topics. Gelman-Rubin-Brooks plots show how Gelman and
    Rubin's shrink factor for the L2 norm of the regression coefficients and the L2
    norm of the residuals changes across iterations during inference for the movie
    regression data set. This is shown for four parallel MCMC chains with different
    starting values. Values close to 1 indicate convergence. From these graphs it
    can be seen that the Gibbs sampled sLDA model is slower to mix compared to the
    sHDP model.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Andrew M. Dai
  Name of the last author: Amos J. Storkey
  Number of Figures: 3
  Number of Tables: 3
  Number of authors: 2
  Paper title: The Supervised Hierarchical Dirichlet Process
  Publication Date: 2014-04-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Most Positive and Negative Learnt Topics, in Terms of
      Their Regression Coefficients, from the Movie Review Regression Data Set with
      sHDP
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Strongest Topics for the Movie Review Regression Data Set
      Using the Alternative Two-Step Approach Where a Supervised GLM Model Is Learnt
      on Top of a Set of Unsupervised Topics from the HDP
  Table 3 caption:
    table_text: TABLE 3 The Most Positive and Negative Learnt Topics, in Terms of
      Regression Coefficients, from the Newswires Data Set with sHDP
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2315802
- Affiliation of the first author: department of electrical engineering, columbia
    university, new york, ny
  Affiliation of the last author: departments of eecs and statistics, uc berkeley,
    berkeley, ca
  Figure 1 Link: articels_figures_by_rev_year\2014\Nested_Hierarchical_Dirichlet_Processes\figure_1.jpg
  Figure 1 caption: An example of path structures for the nested Chinese restaurant
    process (nCRP) and the nested hierarchical Dirichlet process (nHDP) for hierarchical
    topic modeling. With the nCRP, the topics for a document are restricted to lying
    along a single path. With the nHDP, each document has access to the entire tree,
    but a document-specific distribution on paths will place high probability on a
    particular subtree. In both models a word follows a path to its topic. This path
    is deterministic in the case of the nCRP, and drawn from a highly probable document-specific
    subset of paths in the case of the nHDP.
  Figure 10 Link: articels_figures_by_rev_year\2014\Nested_Hierarchical_Dirichlet_Processes\figure_10.jpg
  Figure 10 caption: 'Wikipedia: The adaptively learned step size.'
  Figure 2 Link: articels_figures_by_rev_year\2014\Nested_Hierarchical_Dirichlet_Processes\figure_2.jpg
  Figure 2 caption: 'The New York Times: Average predictive log likelihood on a held-out
    test set as a function of training documents seen.'
  Figure 3 Link: articels_figures_by_rev_year\2014\Nested_Hierarchical_Dirichlet_Processes\figure_3.jpg
  Figure 3 caption: 'New York Times: The total size of the tree as a function of documents
    seen. We show the smallest number of nodes containing 95, 99 and 99.9 percent
    of the posterior mass.'
  Figure 4 Link: articels_figures_by_rev_year\2014\Nested_Hierarchical_Dirichlet_Processes\figure_4.jpg
  Figure 4 caption: 'The New York Times: Per-document statistics from the test set
    using the tree at the final step of the algorithm. (left) The average number of
    words per tree level. (right) The average number of nodes per level with more
    than one expected observation.'
  Figure 5 Link: articels_figures_by_rev_year\2014\Nested_Hierarchical_Dirichlet_Processes\figure_5.jpg
  Figure 5 caption: 'New York Times: The adaptively learned step size.'
  Figure 6 Link: articels_figures_by_rev_year\2014\Nested_Hierarchical_Dirichlet_Processes\figure_6.jpg
  Figure 6 caption: "Tree-structured topics from The New York Times. The shaded node\
    \ is the top-level node and lines indicate dependencies within the tree. In general,\
    \ topics are learning in increasing levels of specificity. For clarity, we have\
    \ removed grammatical variations of the same word, such as \u201Cscientist\u201D\
    \ and \u201Cscientists.\u201D"
  Figure 7 Link: articels_figures_by_rev_year\2014\Nested_Hierarchical_Dirichlet_Processes\figure_7.jpg
  Figure 7 caption: 'Wikipedia: Average predictive log likelihood on a held-out test
    set as a function of training documents seen.'
  Figure 8 Link: articels_figures_by_rev_year\2014\Nested_Hierarchical_Dirichlet_Processes\figure_8.jpg
  Figure 8 caption: 'Wikipedia: The total size of the tree as a function of documents
    seen. We show the smallest number of nodes containing 95, 99 and 99.9 percent
    of the posterior mass.'
  Figure 9 Link: articels_figures_by_rev_year\2014\Nested_Hierarchical_Dirichlet_Processes\figure_9.jpg
  Figure 9 caption: 'Wikipedia: Per-document statistics from the test set using the
    tree at the final step of the algorithm. (left) The average number of words per
    tree level. (right) The average number of nodes per level with more than one expected
    observation.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: John Paisley
  Name of the last author: Michael I. Jordan
  Number of Figures: 14
  Number of Tables: 2
  Number of authors: 4
  Paper title: Nested Hierarchical Dirichlet Processes
  Publication Date: 2014-04-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 A List of the Local and Global Variables and Their Respective
      q Distributions for the nHDP Topic Model
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of the nHDP with the nCRP in the Batch Inference
      Setting Using the Predictive Log Likelihood
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2318728
- Affiliation of the first author: department of computer science and sheffield institute
    for translational neuroscience, university of sheffield, sheffield, south yorkshire,
    united kingdom
  Affiliation of the last author: department of computer science and sheffield institute
    for translational neuroscience, university of sheffield, sheffield, south yorkshire,
    united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2014\Fast_Nonparametric_Clustering_of_Structured_TimeSeries\figure_1.jpg
  Figure 1 caption: A hierarchical Gaussian process model of gene expression of a
    single gene during Drosophila development. In each pane, the y-axis represents
    normalised log gene-expression level, and the x-axis represents time in hours.
    The left-most frame shows the posterior distribution for the function f(t) , and
    subsequent frames represent biological replicates, which we model as hierarchical
    groups. The right-most pane represents a hypothetical future replicate. Posterior
    means are represented as solid lines and shaded areas represent two standard deviations.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\Fast_Nonparametric_Clustering_of_Structured_TimeSeries\figure_2.jpg
  Figure 2 caption: A graphical models representation of our hierarchical Gaussian
    process clustering method. Gaussian processes are represented by infinite self-connected
    plates (note that all variables in a GP are jointly distributed). Hyper-parameters
    of the GPs and the DP concentration alpha are shown as solid dots. The right-hand
    plate represents the Dirichlet process, and the left hand plate represents N independent
    data groups to be clustered. The inner plate represents a single level of structure
    below the clustering, indexed by g , with functions represented as yn(g)(t) .
  Figure 3 Link: articels_figures_by_rev_year\2014\Fast_Nonparametric_Clustering_of_Structured_TimeSeries\figure_3.jpg
  Figure 3 caption: A graphical models representation of our hierarchical Gaussian
    process clustering method, after variables have been collapsed using the standard
    GP methodology. This model enables the d-separation test which shows that by approximating
    the distribution of the latent variables bf Z with q(bf Z) , the remainder of
    the model will marginalise analytically.
  Figure 4 Link: articels_figures_by_rev_year\2014\Fast_Nonparametric_Clustering_of_Structured_TimeSeries\figure_4.jpg
  Figure 4 caption: 'Cluster allocation diagrams for the synthetic data. In each,
    the data are indexed vertically and the clusters are index horizontally: white
    square indicates allocation of a datum to a cluster, grey squares represent uncertain
    allocations. The hierarchical GP model finds most of the correct structure, with
    some confusion in the first and second(eighth) cluster. The non-structured GP-DP
    fails to correctly account for structure in the data, and uses too many clusters
    to model the variance. The DP Gaussian mixture model is unable to discern the
    clusters correctly.'
  Figure 5 Link: articels_figures_by_rev_year\2014\Fast_Nonparametric_Clustering_of_Structured_TimeSeries\figure_5.jpg
  Figure 5 caption: 'The synthetic data set, shown in the clustering formation inferred
    by the hierarchical model. Each pane represents one cluster, and data assigned
    to that cluster are represented as thin lines joining the observations. Posterior
    means and two standard deviations of f are shown as solid lines and shaded areas.
    We note that this Figure omits some additional structure: each datum is modelled
    as a GP (not shown) whose prior mean is that common to the cluster.'
  Figure 6 Link: articels_figures_by_rev_year\2014\Fast_Nonparametric_Clustering_of_Structured_TimeSeries\figure_6.jpg
  Figure 6 caption: An example of the structure inferred within a single cluster of
    the Drosophila data set. The function f which governs the behaviour of the cluster
    is shown in the bottom left panel. Each row represents a single gene in the cluster,
    with the left-most pane representing the inferred function for that gene, and
    subsequent panes representing the inferred function for individual replicates.
    The bottom row shows the predictive distribution for a hypothetic extra gene in
    the cluster.
  Figure 7 Link: articels_figures_by_rev_year\2014\Fast_Nonparametric_Clustering_of_Structured_TimeSeries\figure_7.jpg
  Figure 7 caption: 'Convergence of our method on the Drosophila data set, using two
    random restarts. The same restarts were applied to both methods. The conjugate
    Riemann method uses Hestenes-Steifel conjugacy on the manifold, whist the VBEM
    procedure is effectively steepest descent. Both types of optimization show that
    there are plateaus in the objective function: the conjugate method quickly escapes
    these, whilst VBEM can only move according to the local gradient, and becomes
    stuck.'
  Figure 8 Link: articels_figures_by_rev_year\2014\Fast_Nonparametric_Clustering_of_Structured_TimeSeries\figure_8.jpg
  Figure 8 caption: 'Eight of the 10 Clusters found in the mouse cartilage data using
    periodic GPs for clustering. In this case, the only information shared between
    genes in a group must be captured by a periodic GP. We are fortunate enough to
    be given the period of the rhythm in advance: it is 24 hrs as enforced by the
    light-dark cycle. Although there are other effects in the data, in this case we
    do not wish to use them in clustering: they are thus modelled on a gene-by-gene
    basis as a RBF (and i.i.d. noise) GP.'
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: James Hensman
  Name of the last author: Neil D. Lawrence
  Number of Figures: 8
  Number of Tables: 3
  Number of authors: 3
  Paper title: Fast Nonparametric Clustering of Structured Time-Series
  Publication Date: 2014-04-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Notation for Variables Used in the Model
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of the Hierarchical and Non-Hierarchical Methods
      on the Drosophila Data
  Table 3 caption:
    table_text: TABLE 3 Timing of Clustering Each of the Data Sets Using VBEM and
      the Riemann Approach
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2318711
