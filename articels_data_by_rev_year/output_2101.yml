- Affiliation of the first author: "cnrs, i3s, universit\xE9 c\xF4te dazur, sophia\
    \ antipolis, france"
  Affiliation of the last author: "cnrs, i3s, universit\xE9 c\xF4te dazur, sophia\
    \ antipolis, france"
  Figure 1 Link: articels_figures_by_rev_year\2021\TRACK_A_New_Method_From_a_ReExamination_of_Deep_Architectures_for_Head_Motion_Pr\figure_1.jpg
  Figure 1 caption: "360 \u2218 video streaming principle. The user requests the next\
    \ video segment at time t , if the future orientations of the user ( \u03B8 t+1\
    \ , \u03C6 t+1 ),\u2026,( \u03B8 t+H , \u03C6 t+H ) were known, the bandwidth\
    \ consumption could be reduced by sending in higher quality only the areas corresponding\
    \ to the future FoV."
  Figure 10 Link: articels_figures_by_rev_year\2021\TRACK_A_New_Method_From_a_ReExamination_of_Deep_Architectures_for_Head_Motion_Pr\figure_10.jpg
  Figure 10 caption: Transfer Entropy (TE) TEVrightarrow P(t,s) between Vt+s and Pt+s
    (averaged over t and videos) for all the datasets used in NOSSDAV17, PAMI18, CVPR18,
    and MM18, with the addition of MMSys18.
  Figure 2 Link: articels_figures_by_rev_year\2021\TRACK_A_New_Method_From_a_ReExamination_of_Deep_Architectures_for_Head_Motion_Pr\figure_2.jpg
  Figure 2 caption: For each time-stamp t , all the next positions from t until t+H
    are predicted.
  Figure 3 Link: articels_figures_by_rev_year\2021\TRACK_A_New_Method_From_a_ReExamination_of_Deep_Architectures_for_Head_Motion_Pr\figure_3.jpg
  Figure 3 caption: 'The building blocks in charge, at each time step, of processing
    positional information P t and content information V t , that are visual features
    learned end-to-end or obtained from a saliency extractor module (omitted in this
    scheme). Left: MM18 [11]. Right: CVPR18. [10]'
  Figure 4 Link: articels_figures_by_rev_year\2021\TRACK_A_New_Method_From_a_ReExamination_of_Deep_Architectures_for_Head_Motion_Pr\figure_4.jpg
  Figure 4 caption: The deep-position-only baseline based on an encoder-decoder (seq2seq)
    architecture.
  Figure 5 Link: articels_figures_by_rev_year\2021\TRACK_A_New_Method_From_a_ReExamination_of_Deep_Architectures_for_Head_Motion_Pr\figure_5.jpg
  Figure 5 caption: 'Top: Comparison with MM18 [11], H=2.5 seconds. Bottom: Comparison
    with CVPR18 [10], prediction horizon H=1 sec. CVPR18-repro is introduced in Section
    4.1, the model TRACK in Sec. 6.2.'
  Figure 6 Link: articels_figures_by_rev_year\2021\TRACK_A_New_Method_From_a_ReExamination_of_Deep_Architectures_for_Head_Motion_Pr\figure_6.jpg
  Figure 6 caption: 'Left: Distribution of difficulty in the CVPR18 dataset. Right:
    Error as a function of the difficulty for the CVPR18-repro model.'
  Figure 7 Link: articels_figures_by_rev_year\2021\TRACK_A_New_Method_From_a_ReExamination_of_Deep_Architectures_for_Head_Motion_Pr\figure_7.jpg
  Figure 7 caption: Mutual information I(Pt;Pt+s) between Pt and Pt+s (averaged over
    t and videos) for all the datasets used in NOSSDAV17, PAMI18, CVPR18 and MM18,
    with the addition of MMSys18.
  Figure 8 Link: articels_figures_by_rev_year\2021\TRACK_A_New_Method_From_a_ReExamination_of_Deep_Architectures_for_Head_Motion_Pr\figure_8.jpg
  Figure 8 caption: 'Prediction error on the MMSys18 dataset. The deep-position-only
    baseline is tested on the 5 videos above, and trained on the others (see [29,
    Sec. I] or [14], [15]). Top left: Average results on all five test videos. Rest:
    Detailed result per video category (Exploration, Moving Focus, Ride, Static Focus).
    Legend is identical in all sub-figures.'
  Figure 9 Link: articels_figures_by_rev_year\2021\TRACK_A_New_Method_From_a_ReExamination_of_Deep_Architectures_for_Head_Motion_Pr\figure_9.jpg
  Figure 9 caption: Prediction error averaged on test videos of the datasets of NOSSDAV17
    (left) and MM18 (right). We refer to the supplemental material [29, Sec. II] or
    [14] for the train-test video split used for the deep-position-only baseline (identical
    to original methods). Legend is identical in both sub-figures.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "Miguel Fabi\xE1n Romero Rond\xF3n"
  Name of the last author: "Fr\xE9d\xE9ric Precioso"
  Number of Figures: 22
  Number of Tables: 4
  Number of authors: 4
  Paper title: "TRACK: A New Method From a Re-Examination of Deep Architectures for\
    \ Head Motion Prediction in 360 \u2218 \u2218 Videos"
  Publication Date: 2021-04-05 00:00:00
  Table 1 caption: TABLE 1 Taxonomy of Existing Dynamic Head-Prediction Methods
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 Comparison With PAMI18 [9]: Mean Overlap Scores of FoV\
    \ Prediction, Prediction Horizon H\u224830ms H\u224830ms (1 frame)"
  Table 3 caption: TABLE 3 Comparison With ChinaCom18 [12], Prediction Horizon H=1
    H=1 Second
  Table 4 caption: 'TABLE 4 Comparison With NOSSDAV17: Performance of Tile- and Orientation-Based
    Networks of [13] Compared Against Our Deep-Position-Only Baseline, Prediction
    Horizon H=1 H=1 Second'
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3070520
- Affiliation of the first author: department of computer science and automation,
    indian institute of science, bengaluru, karnataka, india
  Affiliation of the last author: department of computer science and automation, indian
    institute of science, bengaluru, karnataka, india
  Figure 1 Link: articels_figures_by_rev_year\2021\Contradistinguisher_A_Vapniks_Imperative_to_Unsupervised_Domain_Adaptation\figure_1.jpg
  Figure 1 caption: "Demonstration of difference in domain alignment and proposed\
    \ method CUDA on the 2-dimensional blobs synthetic toy-dataset for domain distributions\
    \ from popular scikit\u2212learn [46]. We provide two example settings of different\
    \ data distributions for domains D0 and D1 in both the directions of domain adaptation\
    \ D0\u2194D1 separated by the vertical lines. In each setting, the top row corresponds\
    \ to the domain alignment approach. The bottom row corresponds to the proposed\
    \ method CUDA compared to their respective domain alignment in the top row in\
    \ both D0\u2194D1 domain adaptation tasks. The yellow dotted lines indicate the\
    \ domain alignment process to superimpose the target domain onto the source domain,\
    \ thereby morphing both the domains. similarly, the two sub-columns indicate the\
    \ experiments with swapped source and target domains. Unlike the domain alignment\
    \ approach, where the classifier is learnt only on source domain, CUDA demonstrates\
    \ the contradistinguisher jointly learnt to classify on both the domains. As seen\
    \ above, swapping domains affect the classifier learnt in domain alignment because\
    \ the classifier depends on the source domain. However, because of joint learning\
    \ on both the domains simultaneously, contradistinguisher shows almost the same\
    \ decision boundary irrespective of the source domain, i.e., irrespective of the\
    \ direction of the domain adaptation, i.e., D0\u2192D1 or D1\u2192D0 . (Best viewed\
    \ in color.)"
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Contradistinguisher_A_Vapniks_Imperative_to_Unsupervised_Domain_Adaptation\figure_2.jpg
  Figure 2 caption: 'Architecture of the proposed method CUDA with Contradistinguisher
    (Encoder and Classifier). Three optimization objectives with their respective
    inputs involved in training of CUDA: (i) Source supervised (2), (ii) Target unsupervised
    (5) and Adversarial regularization (9).'
  Figure 3 Link: articels_figures_by_rev_year\2021\Contradistinguisher_A_Vapniks_Imperative_to_Unsupervised_Domain_Adaptation\figure_3.jpg
  Figure 3 caption: "Contour plots show the probability contours along with clear\
    \ decision boundaries on different toy-dataset settings trained using CUDA. (source\
    \ domain: \xD7, target domain: + , class 0: blue, class 1: red.) (Best viewed\
    \ in color.)"
  Figure 4 Link: articels_figures_by_rev_year\2021\Contradistinguisher_A_Vapniks_Imperative_to_Unsupervised_Domain_Adaptation\figure_4.jpg
  Figure 4 caption: 'Illustrations of samples from all the three domains of high resolution
    Office-31 [44] dataset with one instance per each class from every domain (column
    1,4,7,10: A , 2,5,8,11: D , 3,6,9,12: W ). (Best viewed in color.)'
  Figure 5 Link: articels_figures_by_rev_year\2021\Contradistinguisher_A_Vapniks_Imperative_to_Unsupervised_Domain_Adaptation\figure_5.jpg
  Figure 5 caption: 'Illustrations of samples from all the three data-splits of VisDA-2017
    [45] dataset with one instance per each class from every domain (row 1: V syn
    source domain synthetic images (training set), row 2: V real target domain real-world
    images (validation set), row 3: V real target domain real-world images (testing
    set)). It should be noted that unlike the Office-31 dataset and other standard
    benchmark domain adaptation datasets discussed in [43], most of the real-world
    images in the target domain of the VisDA-2017 dataset contains multiple true labels,
    which are only annotated with only one of the multiple labels. (Best viewed in
    color.)'
  Figure 6 Link: articels_figures_by_rev_year\2021\Contradistinguisher_A_Vapniks_Imperative_to_Unsupervised_Domain_Adaptation\figure_6.jpg
  Figure 6 caption: 'Row 1 and 2: t-SNE [69] plots for embeddings from the output
    of contradistinguisher with samples from Office-31 [44] dataset as input corresponding
    to the highest mean accuracy setting ss+tu+su+ta indicated in Table 2 for single-source
    domain adaptation using ResNet-152 [65] as the fixed encoder. Row 3: t-SNE [69]
    plots for embeddings from the output of contradistinguisher corresponding to the
    samples from Office-31 [44] dataset in high-resolution visual tasks after applying
    softmax trained with CUDA with ResNet-50 [65] as the encoder in a multi-source
    domain adaptation setting as indicated in Table 3. We can observe the clear class-wise
    clustering among all the 31 classes in the Office-31 [44] datasets. We achieve
    high accuracies in spite of having only a few hundred training samples in each
    domain. (Best viewed in color.)'
  Figure 7 Link: articels_figures_by_rev_year\2021\Contradistinguisher_A_Vapniks_Imperative_to_Unsupervised_Domain_Adaptation\figure_7.jpg
  Figure 7 caption: The t-SNE plots of the unseen test set samples corresponding to
    the CUDA result in Table 4. The t-SNE plots show clear clustering of all the 10
    classes in Digits datasets distinctively. (Best viewed in color.)
  Figure 8 Link: articels_figures_by_rev_year\2021\Contradistinguisher_A_Vapniks_Imperative_to_Unsupervised_Domain_Adaptation\figure_8.jpg
  Figure 8 caption: "The t-SNE plots of CUDACUDA \u2217 shows the clear clustering\
    \ of all the twelve classes of VisDA-2017 distinctively compared to the t-SNE\
    \ plots of BSPCAN. The t-SNE plots of CUDACUDA \u2217 represent some important\
    \ visual semantics of the image embeddings obtained from contradistinguisher in\
    \ the following manner. (i) The vehicular classes such as bus, car, train, and\
    \ truck can be seen clustered closely as semantically these classes are similar\
    \ to each other (region bounded in red). (ii) The two-wheeler classes such as\
    \ bicycle and motorcycle are clustered closely as these are semantically similar\
    \ to each other compared to vehicular classes that are clustered exactly opposite\
    \ (region bounded in green). (iii) Irrespective of the approach used, there is\
    \ always confusion between knife and skateboard classes. This confusion between\
    \ knife and skateboard classes represented in the confusion matrices, which is\
    \ also clearly seen in the t-SNE plots as well, can be attributed to the nature\
    \ of images of these classes in the dataset on close observation (region bounded\
    \ in blue). (iv) The remaining classes such as aeroplane, horse, person and plant\
    \ can be seen clustered independently and distinctively as these classes have\
    \ almost no visual semantic similarities to one another. (Best viewed in color.)"
  Figure 9 Link: articels_figures_by_rev_year\2021\Contradistinguisher_A_Vapniks_Imperative_to_Unsupervised_Domain_Adaptation\figure_9.jpg
  Figure 9 caption: We indicate few samples that are misclassified by the contradistinguisher
    in the following subcaption format originallabel|predictedlabel. In most cases,
    the original ground truth labels are dubious, and the predicted labels make more
    sense realistically. Subplots (6), (7), (11), (12), (14), (18), (23) and (25)
    shows that the object is identified based on the shape and not if the object is
    present only in the foreground. This indicates that the contradistinguisher makes
    the predictions based on the clearly visible shapes and not the presence of the
    object in the foregroundbackground. The visualization of the features responsible
    for the respective predicted outcome indicates the shape bias as mostly the features
    are detected as edges corresponding to the shape of the object in the image. This
    shows the importance of shape bias to achieve high performance in transfer learning
    and domain adaptation tasks.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.8
  Name of the first author: Sourabh Balgi
  Name of the last author: Ambedkar Dukkipati
  Number of Figures: 9
  Number of Tables: 6
  Number of authors: 2
  Paper title: 'Contradistinguisher: A Vapniks Imperative to Unsupervised Domain Adaptation'
  Publication Date: 2021-04-06 00:00:00
  Table 1 caption: TABLE 1 Details of Visual Domain Adaptation Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Target Domain Accuracy (%) on High Resolution Office-31
    [44] Dataset Containing Three Domains
  Table 3 caption: TABLE 3 Target Domain Accuracy (%) on High Resolution Office-31
    [44] Dataset Under Multi-Source Domain Adaptation Setting by Combining Two Domains
    Into a Single Source Domain and the Remaining Domain as the Target Domain With
    ResNet-50 [65] as the Encoder
  Table 4 caption: TABLE 4 Target Domain Accuracy Reported on the Test Set (%) on
    all 5 Combinations of Digits Datasets Under Multi-Source Domain Adaptation Setting
  Table 5 caption: TABLE 5 Results on VisDA-2017 Dataset Reproduced From the Current
    State-of-the-Art Method BSP, CAN and Our Proposed Method CUDA Reported on Both
    the Validation Set and Test Set
  Table 6 caption: TABLE 6 Total Classification Accuracy (%) on VisDA-2017 Dataset
    Reported on Both the Validation Set and Test Set
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3071225
- Affiliation of the first author: college of science, china agricultural university,
    beijing, china
  Affiliation of the last author: college of science, china agricultural university,
    beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\A_Safe_Feature_Elimination_Rule_for_L__LRegularized_Logistic_Regression\figure_1.jpg
  Figure 1 caption: "Flowchart of sequential SFER for solving L1-LR with a series\
    \ of \u03BB ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\A_Safe_Feature_Elimination_Rule_for_L__LRegularized_Logistic_Regression\figure_2.jpg
  Figure 2 caption: The geometric representation of W s SFER and W s Slores .
  Figure 3 Link: articels_figures_by_rev_year\2021\A_Safe_Feature_Elimination_Rule_for_L__LRegularized_Logistic_Regression\figure_3.jpg
  Figure 3 caption: Comparison of six methods on screening ratio with different parameter
    values.
  Figure 4 Link: articels_figures_by_rev_year\2021\A_Safe_Feature_Elimination_Rule_for_L__LRegularized_Logistic_Regression\figure_4.jpg
  Figure 4 caption: "Number of the half-spaces selected in SFER with the increase\
    \ of \u03BB \u03BB max on the arcene data set."
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Xianli Pan
  Name of the last author: Yitian Xu
  Number of Figures: 4
  Number of Tables: 4
  Number of authors: 2
  Paper title: A Safe Feature Elimination Rule for L 1 L1-Regularized Logistic Regression
  Publication Date: 2021-04-06 00:00:00
  Table 1 caption: TABLE 1 Comparison of SFER and Existing Safe Screening Rules
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Statistics of Ten Benchmark Data Sets
  Table 3 caption: TABLE 3 Experimental Results of Seven Algorithms on Ten Benchmark
    Data Sets
  Table 4 caption: TABLE 4 Time Comparison of GAP-Warm and SFER-Warm on Four Data
    Sets
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3071138
- Affiliation of the first author: vanguard group, valley forge, pa, usa
  Affiliation of the last author: department of applied and computational mathematics
    and statistics, university of notre dame, notre dame, in, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\The_Loss_Surface_of_Deep_Linear_Networks_Viewed_Through_the_Algebraic_Geometry_L\figure_1.jpg
  Figure 1 caption: "The mean of N R as a function of \u039B max for H=1 and d 1 =2\
    \ ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\The_Loss_Surface_of_Deep_Linear_Networks_Viewed_Through_the_Algebraic_Geometry_L\figure_2.jpg
  Figure 2 caption: "The mean of N R for different value of d x (circles), d y (triangles)\
    \ and m (diamonds) where \u039B max =1 . For circles: H=1 , m=2 , and d y = d\
    \ 1 =2 . For triangles: H=1 , m=2 , and d x = d 1 =2 . For diamonds: H=1 and d\
    \ x = d y = d 1 =2 ."
  Figure 3 Link: articels_figures_by_rev_year\2021\The_Loss_Surface_of_Deep_Linear_Networks_Viewed_Through_the_Algebraic_Geometry_L\figure_3.jpg
  Figure 3 caption: "The index distribution for different d x and \u039B max . For\
    \ the left plot, H=1 , m=5 , d x = d y = d 1 =2 , and \u039B\u2208[0, \u039B max\
    \ ] . For the right plot, H=1 , m=2 , d y = d 1 =2 , and \u039B max =1 ."
  Figure 4 Link: articels_figures_by_rev_year\2021\The_Loss_Surface_of_Deep_Linear_Networks_Viewed_Through_the_Algebraic_Geometry_L\figure_4.jpg
  Figure 4 caption: "The index distribution for different m where H=1 , d x =2 , d\
    \ y =2 , and \u039B max =1 ."
  Figure 5 Link: articels_figures_by_rev_year\2021\The_Loss_Surface_of_Deep_Linear_Networks_Viewed_Through_the_Algebraic_Geometry_L\figure_5.jpg
  Figure 5 caption: "The minimum, mean, and maximum of global minimum loss function\
    \ value at real solutions of 1000 samples with different \u039B max . The other\
    \ parameters are H=1 , m=5 , and d 1 = d x = d y =2 ."
  Figure 6 Link: articels_figures_by_rev_year\2021\The_Loss_Surface_of_Deep_Linear_Networks_Viewed_Through_the_Algebraic_Geometry_L\figure_6.jpg
  Figure 6 caption: The loss surfaces near two solutions found by Bertini. The top
    figure shows the loss surfaces for Solution 1 against variable w 1 12 and w 0
    25 . The bottom figure shows the loss surfaces for zero solution against variable
    w 1 12 and w 0 25 .
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.89
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Dhagash Mehta
  Name of the last author: Jonathan D. Hauenstein
  Number of Figures: 6
  Number of Tables: 4
  Number of authors: 4
  Paper title: The Loss Surface of Deep Linear Networks Viewed Through the Algebraic
    Geometry Lens
  Publication Date: 2021-04-06 00:00:00
  Table 1 caption: TABLE 1 Upper Bounds on the Number Solutions for (4) Based on CBB
    and BKK, With Comparison to the Dedieu-Malajovich Number N DM NDM Which are Independent
    of the Parameter Values
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Computational Results of N C NC, N R NR, N DM NDM, and
    max(I) max(I) for the Cases m>1 m>1
  Table 3 caption: TABLE 3 All Local Minima Arising From (11) With H=1 H=1, m=5 m=5,
    d x = d y =3 dx=dy=3, and d 1 =2 d1=2 Such That There are Local Minima Which are
    not Global Minima
  Table 4 caption: TABLE 4 The Nontrivial Optima of (12)
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3071289
- Affiliation of the first author: department of computer science, brown university,
    providence, ri, usa
  Affiliation of the last author: facebook ai research, menlo park, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\SampleEfficient_Neural_Architecture_Search_by_Learning_Actions_for_Monte_Carlo_T\figure_1.jpg
  Figure 1 caption: Starting from the entire model space, at each search stage we
    learn an action (or a set of linear constraints) to separate good from bad models
    for providing distinctive rewards for better searching. Fig. 10 in the appendix
    provides a visualization of the partitioning process in LaNAS, which can be found
    on the Computer Society Digital Library at http:doi.ieeecomputersociety.org10.1109TPAMI.2021.3071343.
  Figure 10 Link: articels_figures_by_rev_year\2021\SampleEfficient_Neural_Architecture_Search_by_Learning_Actions_for_Monte_Carlo_T\figure_10.jpg
  Figure 10 caption: 'A visualization of partitioning eggholder function using LaNAS:
    eggholder is a popular benchmark function for black-box optimization, (a) depicts
    its function surface, contour and definition. (b) LaNAS builds a tree of height
    = 5 for searching v ; after collecting 500 samples, we visualize each partitions
    Omega j represented at node0 rightarrow node15 in (c) rightarrow (f), by splitting
    Omega based on its parent constraint. As node0 rightarrow node15 recursively splitting
    Omega , the final Omega j at node15 only contains the most promising region in
    Omega for sampling (see (f) versus (c)), and the bad region (blue lines in contours)
    are clearly separated from good region (red lines in contours) from (c) rightarrow
    (f).'
  Figure 2 Link: articels_figures_by_rev_year\2021\SampleEfficient_Neural_Architecture_Search_by_Learning_Actions_for_Monte_Carlo_T\figure_2.jpg
  Figure 2 caption: 'Illustration of motivation: (a) visualizes the MCTS search trees
    using sequential and global action space. The node value (i.e., accuracy) is higher
    if the color is darker. (b) For a given node, the reward distributions for its
    children. d is the average distance over all nodes. global better separates the
    search space by network quality and provides distinctive rewards in recognizing
    a promising path. (c) As a result, global finds the best network much faster than
    sequential. This motivates us to learn actions to partition the search space for
    the efficient architecture search.'
  Figure 3 Link: articels_figures_by_rev_year\2021\SampleEfficient_Neural_Architecture_Search_by_Learning_Actions_for_Monte_Carlo_T\figure_3.jpg
  Figure 3 caption: 'An overview of LaNAS: Each iteration of LaNAS comprises a search
    and learning phase. The search phase uses MCTS to sample networks, while the learning
    phase learns a linear model between network hyper-parameters and accuracies.'
  Figure 4 Link: articels_figures_by_rev_year\2021\SampleEfficient_Neural_Architecture_Search_by_Learning_Actions_for_Monte_Carlo_T\figure_4.jpg
  Figure 4 caption: "Integrating with one-shot NAS: Before LaNAS comes into play,\
    \ we pre-train a supernet with a random mask at each iteration until it converges,\
    \ i.e., decoupling the training and search so that we can benchmark different\
    \ algorithms on the same supernet. During the search phase, the supernet remains\
    \ static. When LaNAS evaluate a network a i , we transform the supernet to a i\
    \ by multiplying the mask corresponding to a i as shown in the figure. Opr stands\
    \ for a layer type; we name edges from a\u2192e , and each edge can be one of\
    \ the predefined layers or none. The figure shows there are three possibilities\
    \ for a compounded edge, represented either by a 1x3 one-hot vector to choose\
    \ a layer type to activate the edge, or a 1x3 zero vector to deactivate the edge."
  Figure 5 Link: articels_figures_by_rev_year\2021\SampleEfficient_Neural_Architecture_Search_by_Learning_Actions_for_Monte_Carlo_T\figure_5.jpg
  Figure 5 caption: The cell structure of supernet used in searching nasnet. The supernet
    structure of normal and reduction cell are same. (a) Each edge is a compound edge,
    consisting of 4 independent edges with the same inputoutput to represent 4 layer
    types. (b) Each node allows for two inputs from previous nodes. To specify a NASNet
    architecture, we use 5 variables for defining connectivity among nodes, and 10
    variables for defining the layer type of every edge. Supernet can transform to
    any network in the search space by applying the mask.
  Figure 6 Link: articels_figures_by_rev_year\2021\SampleEfficient_Neural_Architecture_Search_by_Learning_Actions_for_Monte_Carlo_T\figure_6.jpg
  Figure 6 caption: "The top row shows the time-course of test regrets of different\
    \ methods (test regret between current best accuracy v + and the best in dataset\
    \ v \u2217 with the interquartile range), while the bottom row illustrates Cumulative\
    \ Distribution Function (CDF) of v + for each method at 4\u2217 10 4 unique valid\
    \ samples. ConvNet-60K compensates NASBench to test the case of |D|=|\u03A9| ,\
    \ and supernet compensates for the case of |\u03A9|\u226B| \u03A9 nasbench | ,\
    \ where |D|,|\u03A9| are the size of the dataset and search space, respectively.\
    \ LaNAS consistently demonstrates the best performance in 3 cases."
  Figure 7 Link: articels_figures_by_rev_year\2021\SampleEfficient_Neural_Architecture_Search_by_Learning_Actions_for_Monte_Carlo_T\figure_7.jpg
  Figure 7 caption: 'Ablation study: (a) the effect of different tree heights and
    select in MCTS. The number in each entry is samples to reach global optimal. (b)
    the choice of predictor for splitting search space. (c) the effect of samples
    for initialization toward the search performance. (d) the effect of hyper-parameter
    c in UCB on NASBench performance.'
  Figure 8 Link: articels_figures_by_rev_year\2021\SampleEfficient_Neural_Architecture_Search_by_Learning_Actions_for_Monte_Carlo_T\figure_8.jpg
  Figure 8 caption: Evaluations of search dynamics:(a) KL-divergence of pj and pj
    dips and bounces back. barv-barv continues to grow, showing the average metric
    barv over different nodes becomes higher when the search progresses. (b) sample
    distribution pj approximates dataset distribution pj when the number of samples
    n in [200, 700] . The search algorithm then zooms into the promising sub-domain,
    as shown by the growth of barvj when n in [700, 5000] .
  Figure 9 Link: articels_figures_by_rev_year\2021\SampleEfficient_Neural_Architecture_Search_by_Learning_Actions_for_Monte_Carlo_T\figure_9.jpg
  Figure 9 caption: Comparisons of pi bayes to pi random in sampling from the selected
    partition Omega j .
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.74
  Name of the first author: Linnan Wang
  Name of the last author: Yuandong Tian
  Number of Figures: 10
  Number of Tables: 4
  Number of authors: 5
  Paper title: Sample-Efficient Neural Architecture Search by Learning Actions for
    Monte Carlo Tree Search
  Publication Date: 2021-04-07 00:00:00
  Table 1 caption: TABLE 1 Methods Used in Experiments and Their Attributes
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Results on CIFAR-10 Using the NASNet Search Space
  Table 3 caption: TABLE 3 Transferring LaNet From CIFAR-10 to ImageNet Using the
    NASNet Search Space
  Table 4 caption: TABLE 4 Results on ImageNet Using the EfficientNet Search Space
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3071343
- Affiliation of the first author: school of computer science, faculty of engineering,
    university of sydney, darlington, nsw, australia
  Affiliation of the last author: school of computer science, faculty of engineering,
    university of sydney, darlington, nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2021\Variance_Reduced_Methods_for_NonConvex_Composition_Optimization\figure_1.jpg
  Figure 1 caption: "The QC comparison of GD, SVRG, and SCVR at different values of\
    \ m 0 , which is from m= n m 0 , m 0 \u22650 ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Variance_Reduced_Methods_for_NonConvex_Composition_Optimization\figure_2.jpg
  Figure 2 caption: 'Comparison of the performance (objective value and gradient)
    of SCVR with five different sizes of K : n 13 , 2 n 13 , 5 n 13 , 10 n 13 , and
    20 n 13 on dataset Olivetti faces, where K is the number of inner iterations and
    n is the number of samples. From top to bottom: objective versus query complexity
    and gradient versus query complexity.'
  Figure 3 Link: articels_figures_by_rev_year\2021\Variance_Reduced_Methods_for_NonConvex_Composition_Optimization\figure_3.jpg
  Figure 3 caption: 'Nonlinear embedding problem: Comparison of performances with
    seven different algorithms: GD, SGD, SCGD, Acc-SCGD, SVRG, SCVR, and SCVRII. From
    top to bottom, Objective versus QC and Gradient versus QC, respectively. From
    left to right, we apply three different datasets: Olivetti faces, COIL-20, and
    MNIST.'
  Figure 4 Link: articels_figures_by_rev_year\2021\Variance_Reduced_Methods_for_NonConvex_Composition_Optimization\figure_4.jpg
  Figure 4 caption: Visualization of Olivetti faces, COIL-20 and MNIST data sets by
    t-SNE performed on SCVR.
  Figure 5 Link: articels_figures_by_rev_year\2021\Variance_Reduced_Methods_for_NonConvex_Composition_Optimization\figure_5.jpg
  Figure 5 caption: 'Reinforcement learning: Comparison of performances with seven
    different algorithms: GD, SGD, SCGD, Acc-SCGD, SVRG, and SCVR. From top to bottom,
    Objective versus QC and Gradient versus QC, respectively. From left to right,
    we apply three different states: S=500, 1000, and 2000.'
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Liu Liu
  Name of the last author: Dacheng Tao
  Number of Figures: 5
  Number of Tables: 1
  Number of authors: 3
  Paper title: Variance Reduced Methods for Non-Convex Composition Optimization
  Publication Date: 2021-04-07 00:00:00
  Table 1 caption: TABLE 1 Comparison of the Query Complexity of Different Algorithms
    for the Non-Convex Problem
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3071594
- Affiliation of the first author: department of computer science, city university
    of hong kong, kowloon, hong kong
  Affiliation of the last author: department of computer science, city university
    of hong kong, kowloon, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2021\Active_FineTuning_From_gMAD_Examples_Improves_Blind_Image_Quality_Assessment\figure_1.jpg
  Figure 1 caption: Failures of two DNN-based BIQA models, MEON [19] and deepIQA [18],
    when competing with a full-reference IQA method, MS-SSIM [27], in the gMAD competition
    on the Waterloo Exploration Database [28]. (a) Bestworst-quality images according
    to MEON, with near-identical quality reported by MS-SSIM. (b) Bestworst-quality
    images according to MS-SSIM with near-identical quality reported by MEON. (c)
    Bestworst-quality images according to deepIQA with near-identical quality reported
    by MS-SSIM. (d) Bestworst-quality images according to MS-SSIM with near-identical
    quality reported by deepIQA. Visual inspection of the image pairs (a) and (b)
    indicates that MEON does not handle ringing artifacts well, which result from
    JPEG2000 compression. This suggests that exposing MEON to more diverse JPEG2000-compressed
    images during training may be a potential way of improving its robustness. Similarly,
    it is quite clear, from the images pairs (c) and (d), that deepIQA makes inaccurate
    quality predictions for Gaussian-blurred images possibly due to its patch-based
    training strategy.
  Figure 10 Link: articels_figures_by_rev_year\2021\Active_FineTuning_From_gMAD_Examples_Improves_Blind_Image_Quality_Assessment\figure_10.jpg
  Figure 10 caption: gMAD image pairs with the maximum fidelity losses (i.e., the
    worst-case samples) selected in (a) mathcal L(1) , (b) mathcal L(2) , and (c)
    mathcal L(3) , respectively, when VIF [57] is the defender and our model is the
    attacker.
  Figure 2 Link: articels_figures_by_rev_year\2021\Active_FineTuning_From_gMAD_Examples_Improves_Blind_Image_Quality_Assessment\figure_2.jpg
  Figure 2 caption: The active fine-tuning cycle for improving BIQA models. We start
    with a differentiable parametric BIQA model, seek a small number of image pairs
    by letting it compete with a set of full-reference IQA methods in gMAD [29], collect
    human opinions on the visual quality of the selected images, and fine-tune it
    from the combination of existing IQA databases and the newly annotated gMAD set.
  Figure 3 Link: articels_figures_by_rev_year\2021\Active_FineTuning_From_gMAD_Examples_Improves_Blind_Image_Quality_Assessment\figure_3.jpg
  Figure 3 caption: "The network architecture of our BIQA model. The parameterization\
    \ of convolution is denoted as \u201Cfilter support | input channel \xD7 output\
    \ channel.\u201D The number of parameters for each layer is given at the bottom,\
    \ summing up to 154,865."
  Figure 4 Link: articels_figures_by_rev_year\2021\Active_FineTuning_From_gMAD_Examples_Improves_Blind_Image_Quality_Assessment\figure_4.jpg
  Figure 4 caption: Sample images from the large-scale unlabeled set S for gMAD competition.
    (a) Amphibian. (b) Bird. (c) Fish. (d) Flower. (e) Fruit. (f) Furniture. (g) Geological
    formation. (h) Mammal. (i) Musical instrument. (j) Reptile. (k) Tool. (l) Vehicle.
    Images are cropped for improved visibility.
  Figure 5 Link: articels_figures_by_rev_year\2021\Active_FineTuning_From_gMAD_Examples_Improves_Blind_Image_Quality_Assessment\figure_5.jpg
  Figure 5 caption: Graphical user interface for subjective testing.
  Figure 6 Link: articels_figures_by_rev_year\2021\Active_FineTuning_From_gMAD_Examples_Improves_Blind_Image_Quality_Assessment\figure_6.jpg
  Figure 6 caption: The empirical distributions of (a) p( x r , y r ) and (b) p( x
    a , y a ) on L (1) . It is clear that full-reference IQA methods (as attackers)
    can easily falsify our BIQA model, and vice versa.
  Figure 7 Link: articels_figures_by_rev_year\2021\Active_FineTuning_From_gMAD_Examples_Improves_Blind_Image_Quality_Assessment\figure_7.jpg
  Figure 7 caption: "The progress of our method in terms of the mean fidelity loss\
    \ ( \xB1 standard error) on the gMAD sets, when playing the role of the defender\
    \ and the attacker, respectively."
  Figure 8 Link: articels_figures_by_rev_year\2021\Active_FineTuning_From_gMAD_Examples_Improves_Blind_Image_Quality_Assessment\figure_8.jpg
  Figure 8 caption: gMAD image pairs with the maximum fidelity losses (i.e., the worst-case
    samples) selected in (a) L (1) , (b) L (2) , and (c) L (3) , respectively, when
    our model is the defender and VSI [56] is the attacker.
  Figure 9 Link: articels_figures_by_rev_year\2021\Active_FineTuning_From_gMAD_Examples_Improves_Blind_Image_Quality_Assessment\figure_9.jpg
  Figure 9 caption: gMAD image pairs with the maximum fidelity losses selected in
    (a) L (1) , (b) L (2) , and (c) L (3) , respectively, when our model is the defender
    and MDSI [58] is the attacker.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.97
  Name of the first author: Zhihua Wang
  Name of the last author: Kede Ma
  Number of Figures: 12
  Number of Tables: 6
  Number of authors: 2
  Paper title: Active Fine-Tuning From gMAD Examples Improves Blind Image Quality
    Assessment
  Publication Date: 2021-04-08 00:00:00
  Table 1 caption: TABLE 1 Summary of IQA Databases
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Correlation (SRCC and PLCC) Between Model Predictions and
    MOSs on T T
  Table 3 caption: TABLE 3 Correlation (SRCC and PLCC) Results on the gMAD Image Sets
  Table 4 caption: TABLE 4 Correlation (SRCC and PLCC) of Model Predictions by f w
    fw Against Human Ratings on T T After Simple Fine-Tuning on D 2 D2 and Active
    Fine-Tuning on Both D 2 D2 and D 3 D3
  Table 5 caption: TABLE 5 Results of the D-Test, L-Test, and P-Test on the Waterloo
    Exploration Database
  Table 6 caption: TABLE 6 Synthetic-to-Real Generalization of Our Method on Two Authentically
    Distorted Datasets - SPAQ [61] and KonIQ-10k [62]
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3071759
- Affiliation of the first author: department of cybernetics, visual recognition group,
    czech technical university, prague, czechia
  Affiliation of the last author: department of cybernetics, visual recognition group,
    czech technical university, prague, czechia
  Figure 1 Link: articels_figures_by_rev_year\2021\GraphCut_RANSAC_Local_Optimization_on_Spatially_Coherent_Structures\figure_1.jpg
  Figure 1 caption: Inlier correspondences (green dots) of a rigid motion model, i.e.,
    a fundamental matrix, initialized by a minimal sample (a). Inliers obtained by
    (b) standard thresholding of the residual; (c) the proposed graph-cut-based selection
    considering spatial coherence. All other points are marked by gray circles. The
    graph-cut-based selection (c) returns more inliers compared to the traditional
    thresholding (b).
  Figure 10 Link: articels_figures_by_rev_year\2021\GraphCut_RANSAC_Local_Optimization_on_Spatially_Coherent_Structures\figure_10.jpg
  Figure 10 caption: Relative pose fitting with varying parameters. The average translation
    (left; in degrees) and rotation (middle; in degrees) errors and the processing
    time (right; in ms) are plotted as the function of the confidence (top) and maximum
    iteration number (bottom). The reported values are the average errors over 4000
    scenes from datasets TUM, KITTI, T&T, and CPC. The compared methods are the proposed
    Graph-Cut RANSAC combined with MSAC [24] and MAGSAC++ [17] scoring techniques,
    MSAC [24], RANSAC [1], USAC [37], and NG-RANSAC [18]. In the bottom-right plot,
    the time of NG-RANSAC goes up to 3.4 seconds. In addition, NG-RANSAC model loading
    takes 1.4 seconds on average.
  Figure 2 Link: articels_figures_by_rev_year\2021\GraphCut_RANSAC_Local_Optimization_on_Spatially_Coherent_Structures\figure_2.jpg
  Figure 2 caption: "Example loss functions used for robust model fitting \u2013 RANSAC\
    \ [1], MSAC [25], MLESAC [24], MAGSAC++ [17]."
  Figure 3 Link: articels_figures_by_rev_year\2021\GraphCut_RANSAC_Local_Optimization_on_Spatially_Coherent_Structures\figure_3.jpg
  Figure 3 caption: "The effect of spatial coherence weight \u03BB on the inlier selection\
    \ of a 2D line. The inliers (red points) of a line (green) initialized by a minimal\
    \ sample (blue crosses) are shown. The top row shows the results of a single graph-cut\
    \ run using different values for \u03BB when the Potts model (5) is applied. The\
    \ bottom one shows the labeling results of a single graph-cut run when using the\
    \ proposed spatial coherence model (6). The inlier-outlier threshold is shown\
    \ by green dashed lines. The edges of the neighborhood graph are grey line segments."
  Figure 4 Link: articels_figures_by_rev_year\2021\GraphCut_RANSAC_Local_Optimization_on_Spatially_Coherent_Structures\figure_4.jpg
  Figure 4 caption: Example image pairs from the datasets used for homography estimation
    evaluation; with inlier correspondence visualization.
  Figure 5 Link: articels_figures_by_rev_year\2021\GraphCut_RANSAC_Local_Optimization_on_Spatially_Coherent_Structures\figure_5.jpg
  Figure 5 caption: Example scenes from the datasets used for 6D pose estimation.
    (Left) The input images passed to the EPOS method [45]. EPOS returns a set of
    2D-3D correspondences and object masks. (Right) The 3D objects rendered using
    the poses estimated by GC-RANSAC from the predicted 2D-3D correspondences. Courtesy
    of T. Hodan.
  Figure 6 Link: articels_figures_by_rev_year\2021\GraphCut_RANSAC_Local_Optimization_on_Spatially_Coherent_Structures\figure_6.jpg
  Figure 6 caption: Example image pairs from the datasets used for epipolar geometry
    estimation; with inlier correspondence visualization.
  Figure 7 Link: articels_figures_by_rev_year\2021\GraphCut_RANSAC_Local_Optimization_on_Spatially_Coherent_Structures\figure_7.jpg
  Figure 7 caption: Maximum iteration number study. The avg. log 10 error (px) and
    the run-time (ms) on manually selected inliers are plotted as the function of
    the max. iteration number. The confidence was set to 0.99.
  Figure 8 Link: articels_figures_by_rev_year\2021\GraphCut_RANSAC_Local_Optimization_on_Spatially_Coherent_Structures\figure_8.jpg
  Figure 8 caption: Fundamental matrix fitting. The cumulative distribution functions
    of the SGD errors (in pixels) on four datasets, each consisting of 1000 image
    pairs. Being accurate is interpreted as a curve close to the top-left corner.
    The confidence and maximum iteration number were set to 0.99 and 5000, respectively.
  Figure 9 Link: articels_figures_by_rev_year\2021\GraphCut_RANSAC_Local_Optimization_on_Spatially_Coherent_Structures\figure_9.jpg
  Figure 9 caption: Confidence study. The avg. log 10 error (in pixels) and the run-time
    (in milliseconds) are plotted as the function of the required confidence. The
    max. iteration number was 1000000.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Daniel Barath
  Name of the last author: Jiri Matas
  Number of Figures: 15
  Number of Tables: 1
  Number of authors: 2
  Paper title: 'Graph-Cut RANSAC: Local Optimization on Spatially Coherent Structures'
  Publication Date: 2021-04-08 00:00:00
  Table 1 caption: TABLE 1 The Errors and Failure Ratios (in Percentage) are Reported
    for All Methods (1st Row) on All Problems (1st col.) and Datasets (2nd col.)
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3071812
- Affiliation of the first author: northwestern polytechnical university, xian, p.r.
    china
  Affiliation of the last author: northwestern polytechnical university, xian, p.r.
    china
  Figure 1 Link: articels_figures_by_rev_year\2021\Reconstructive_SequenceGraph_Network_for_Video_Summarization\figure_1.jpg
  Figure 1 caption: The sequence-graph model for video summarization. Two shots are
    displayed as examples. The frames in each shot are taken as sequences and encoded
    by LSTM. The shots are modeled as a complete graph, where all pairwise dependencies
    are captured.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Reconstructive_SequenceGraph_Network_for_Video_Summarization\figure_2.jpg
  Figure 2 caption: The overview of the proposed Reconstructive Sequence-Graph Network
    (RSGN). Specifically, the LSTM units are bidirectional. The red and black nodes
    denote the key and non-key shots, respectively. The two virtual green nodes stand
    for the encoded video and summary content.
  Figure 3 Link: articels_figures_by_rev_year\2021\Reconstructive_SequenceGraph_Network_for_Video_Summarization\figure_3.jpg
  Figure 3 caption: The summarization results of RSGN and RSGN uns . The images are
    sampled from the summaries generated by RSGN. The curves denote the distributions
    of importance scores. The gray curves depict the ground truth score, while the
    redblue curves depict the score predicted by the supervisedunsupervised model,
    respectively.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Bin Zhao
  Name of the last author: Xuelong Li
  Number of Figures: 3
  Number of Tables: 7
  Number of authors: 4
  Paper title: Reconstructive Sequence-Graph Network for Video Summarization
  Publication Date: 2021-04-09 00:00:00
  Table 1 caption: TABLE 1 The Comparison of Baselines on the SumMe and TVsum Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The Comparison of Different Edge Types of Our Graph Encoder
    on the SumMe and TVsum Datasets
  Table 3 caption: TABLE 3 The Comparison of Unsupervised Approaches on the SumMe
    and TVsum Datasets
  Table 4 caption: TABLE 4 The Comparison of Supervised Approaches on the SumMe and
    TVsum Datasets
  Table 5 caption: TABLE 5 The Results With Different Training Settings on the SumMe
    and TVsum Datasets
  Table 6 caption: TABLE 6 Rank-Based Evaluation Results on SumMe and TVsum
  Table 7 caption: TABLE 7 The Summarization Results on the VTW Dataset
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3072117
- Affiliation of the first author: school of computer and communication sciences,
    epfl, cvlab, lausanne, switzerland
  Affiliation of the last author: school of computer and communication sciences, epfl,
    cvlab, lausanne, switzerland
  Figure 1 Link: articels_figures_by_rev_year\2021\Robust_Differentiable_SVD\figure_1.jpg
  Figure 1 caption: "Influence of the covariance matrix size. (a) Probability that\
    \ the difference between two eigenvalues of a covariance matrix is smaller than\
    \ a threshold\u2014 2 \u221210 ,\u2026, 2 \u221214 \u2014as a function of its\
    \ dimension. To compute it for each dimension between 6 and 600, we randomly generated\
    \ 10,000 covariance matrices and counted the proportion for which at least two\
    \ eigenvalues were less than a specific threshold from each other. Given the dimension\
    \ d , we randomly sampled n=2d data points, X d\xD7n , whose row-wise mean is\
    \ 0. The covariance matrix is then obtained by computing X X \u22A4 . (b) Rate\
    \ at which training fails for ZCA normalization on CIFAR10. For each dimension,\
    \ we made 18 attempts."
  Figure 10 Link: articels_figures_by_rev_year\2021\Robust_Differentiable_SVD\figure_10.jpg
  Figure 10 caption: Comparisons between our SVD-WCT, the GD-WCT [13], the PI-WCT
    [24] and the Clip-WCT on the Artworks dataset [50].
  Figure 2 Link: articels_figures_by_rev_year\2021\Robust_Differentiable_SVD\figure_2.jpg
  Figure 2 caption: "Applications of differentiable eigendecomposition: ZCA normalization\
    \ & colorstyle transfer (a) Original n data points X\u2208 R 2\xD7n , whose two\
    \ dimensions are highly correlated. (b) PCA whitening removes the correlations,\
    \ but simultaneously rotates the data, as indicated by the red points. For PCA,\
    \ the whitening matrix is S PCA = \u039B \u221212 V T , where \u039B and V denote\
    \ the diagonal eigenvalue matrix and eigenvector matrix of the covariance matrix\
    \ X X \u22A4 . The points are transformed as X \u2032 = S PCA X . (c) By contrast,\
    \ ZCA whitening, which also decorrelates the data, preserves the original data\
    \ orientation. For ZCA, the whitening matrix is S ZCA =V \u039B \u221212 V T ,\
    \ and the points are also transformed as X \u2032 = S ZCA X . Because of its unique\
    \ property, ZCA whitening can used in a decorrelated batch normalization layer\
    \ to decorrelate the features [4]. (d) Color transfer can be achieved by first\
    \ whitening the pixel values in image A by multiplying them with V A \u039B \u2212\
    12 A V T A . The whitened pixels are then colored according to image B by multiplying\
    \ them with V B \u039B 12 B V T B . (e) Style transfer can be achieved by doing\
    \ similar operations to those performed in color transfer. The difference is that\
    \ the whitening and coloring transformations are performed at the level of deep\
    \ feature maps instead of raw pixels. See Section 5.3 for more detail."
  Figure 3 Link: articels_figures_by_rev_year\2021\Robust_Differentiable_SVD\figure_3.jpg
  Figure 3 caption: Original gradient descent direction and the ones after gradient
    clipping and Taylor expansion. We observe that the direction is better preserved
    with the Taylor expansion.
  Figure 4 Link: articels_figures_by_rev_year\2021\Robust_Differentiable_SVD\figure_4.jpg
  Figure 4 caption: "Influence of K . (a) R K+1 (x)=( \u03BB k \u03BB 1 ) K+1 as a\
    \ function of the eigenvalue ratio \u03BB k \u03BB 1 and of the Taylor expansion\
    \ degree K . (b) Contour plot of the surface shown in (a)."
  Figure 5 Link: articels_figures_by_rev_year\2021\Robust_Differentiable_SVD\figure_5.jpg
  Figure 5 caption: Convergence curves of ResNet18 on CIFAR100 with different hyperparameter
    values. The matrix dimension d=64 .
  Figure 6 Link: articels_figures_by_rev_year\2021\Robust_Differentiable_SVD\figure_6.jpg
  Figure 6 caption: Convergence curves for different matrix dimensions d of our Taylor-based
    method (denoted as SVD-Taylor), the PI-based one of [24] (denoted as SVD-PI),
    and the standard power iteration method [37]. Both SVD-Taylor and SVD-PI use standard
    SVD in the forward pass. Since our approach yields better gradients, it converges
    faster at the beginning. Furthermore, although all methods converge to the same
    training error, our approach yields better testing accuracy. All methods are trained
    with 350 epochs. The learning rate is decreased by 0.1 every 100 epochs. The x
    axis represents the time consumed and y axis denotes the training error.
  Figure 7 Link: articels_figures_by_rev_year\2021\Robust_Differentiable_SVD\figure_7.jpg
  Figure 7 caption: 'Computation speed: Backpropogation time for ResNet18 on CIFAR10
    with different ZCA layers and different matrix dimensions. Taylor expansion is
    always faster than PI. For fair comparison, the Power Iteration number and the
    Taylor expansion degree are set to the same value 9. The computation time is averaged
    over the first 10 epochs. The batch size is 128. This experiment was performed
    on an Nvidia V100 GPU.'
  Figure 8 Link: articels_figures_by_rev_year\2021\Robust_Differentiable_SVD\figure_8.jpg
  Figure 8 caption: Network architecture to transfer the style from an image xB to
    a content image xA . The content encoder EAC extracts content features mathcal
    Xmathcal A from xA , while the style encoder EBS computes style features mathcal
    Xmathcal B from xB . Following [13], the style features are then fed to 5 pairs
    of multilayer perceptron networks (MLP), resulting in 5 coloring transformation
    matrices (mathcal Sc(1), mu c(1)) , (mathcal Sc(2), mu c(2)) , ldots , (mathcal
    Sc(5), mu c(5)) ranging from high-level to low-level features. WCT is then performed
    to transfer the style from mathcal Xmathcal B to the content of mathcal Xmathcal
    A in the generator GB , and finally obtain the stylized image xA . The discriminator
    DB , used in an adversarial fashion, helps to improve the realism of xA by discriminating
    between real images from domain B and generated images.
  Figure 9 Link: articels_figures_by_rev_year\2021\Robust_Differentiable_SVD\figure_9.jpg
  Figure 9 caption: "Qualitative comparisons on the Artworks dataset [50]. Our method\
    \ SVD-WCT generates images with sharper details than GD-WCT [13]. Furthermore,\
    \ the original style can be completely replaced by the new style. For instance,\
    \ the colors of the train on the top left row and of the flower shown in the middle\
    \ left row have changed completely. By contrast, with GD-WCT [13], the original\
    \ style sometimes remains and the details are blurry. Better viewed with 8\xD7\
    \ zoom in."
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Wei Wang
  Name of the last author: Mathieu Salzmann
  Number of Figures: 10
  Number of Tables: 10
  Number of authors: 5
  Paper title: Robust Differentiable SVD
  Publication Date: 2021-04-12 00:00:00
  Table 1 caption: TABLE 1 Notation
  Table 10 caption: TABLE 10 Reference Time (ms) of Different Models
  Table 2 caption: TABLE 2 ResNet18 & ResNet50 Share the Same Structure, But the Blocks
    and the FC Layer are Different
  Table 3 caption: TABLE 3 ResNet18 on CIFAR100 With Matrix Dimension d=64 d=64
  Table 4 caption: "TABLE 4 Errors and Success Rates of ResNet18 With Standard SVD,\
    \ Power Iteration (PI), SVD-PI & Our SVD-Taylor on CIFAR10 With the Image Size\
    \ of 32\xD732 32\xD732"
  Table 5 caption: "TABLE 5 Error Rates of ResNet1850 With Our ZCA Layer on CIFAR100\
    \ Dataset With the Image Size is 32\xD732 32\xD732"
  Table 6 caption: "TABLE 6 Error Rates of ResNet1850 With Our ZCA Layer on Tiny ImageNet\
    \ val. Set With the Image Size of 64\xD764 64\xD764"
  Table 7 caption: TABLE 7 Error Rate (%) of Covariance Pooling Methods With AlexNet
    on ImageNet val. Set
  Table 8 caption: TABLE 8 Classification Accuracy (%) of the Real Style Images, and
    Stylized Images Synthesized With the Baseline GD-WCT [13] and With Our SVD-WCT
  Table 9 caption: TABLE 9 Sharpness Scores
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3072422
