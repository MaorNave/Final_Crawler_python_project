- Affiliation of the first author: department of computer science, the university
    of liverpool, ashton building, liverpool l69 3bx, united kingdom
  Affiliation of the last author: department of electrical engineering and electronics,
    the university of liverpool, brownlow hill, liverpool l69 3gj, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2015\A_New_Measure_for_Analyzing_and_Fusing_Sequences_of_Objects\figure_1.jpg
  Figure 1 caption: 'Data visualization applying seriation to two synthetic datasets.
    (a, b): Original 2D datasets, with labels A-C and F-J corresponding to structures.
    (c, d): Symmetric distance matrices using a random initial sample placement. (e,
    f): Rowcolumn ordered distance maps, with labels A-K corresponding to individual
    blocks.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\A_New_Measure_for_Analyzing_and_Fusing_Sequences_of_Objects\figure_2.jpg
  Figure 2 caption: "Dendrogram (of cophenetic correlation coefficient = 0.98) computed\
    \ using complete linkage and the averaged \u03B3 P values from Table 2. Groups\
    \ with linkage values (shown on the horizontal axis) of 0.8 and over are drawn\
    \ in different shades."
  Figure 3 Link: articels_figures_by_rev_year\2015\A_New_Measure_for_Analyzing_and_Fusing_Sequences_of_Objects\figure_3.jpg
  Figure 3 caption: Dissimilarity matrices of the Elutriation dataset, ordered using
    the output from four different algorithms.
  Figure 4 Link: articels_figures_by_rev_year\2015\A_New_Measure_for_Analyzing_and_Fusing_Sequences_of_Objects\figure_4.jpg
  Figure 4 caption: ARGAR errors for consensus seriation results obtained with the
    proposed, Hamming, Spearman and Kendall measures, showing also the average of
    the original algorithms comprising each T i .
  Figure 5 Link: articels_figures_by_rev_year\2015\A_New_Measure_for_Analyzing_and_Fusing_Sequences_of_Objects\figure_5.jpg
  Figure 5 caption: Seriated maps for the individual algorithms of subset T 1 as well
    as the proposed consensus method, for the dataset Alpha CI.
  Figure 6 Link: articels_figures_by_rev_year\2015\A_New_Measure_for_Analyzing_and_Fusing_Sequences_of_Objects\figure_6.jpg
  Figure 6 caption: Seriated maps for the individual algorithms of subset T 4 as well
    as the proposed consensus method, for the dataset Colon tissues.
  Figure 7 Link: articels_figures_by_rev_year\2015\A_New_Measure_for_Analyzing_and_Fusing_Sequences_of_Objects\figure_7.jpg
  Figure 7 caption: "RGAR errors for the sequence generating algorithms in T 1 and\
    \ all consensus methods plotted against all window sizes \u03B4 ."
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.97
  Name of the first author: John Yannis Goulermas
  Name of the last author: Tingting Mu
  Number of Figures: 7
  Number of Tables: 2
  Number of authors: 3
  Paper title: A New Measure for Analyzing and Fusing Sequences of Objects
  Publication Date: 2015-08-20 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of the Main Formulations of the Raw and the Coefficient
      Forms of the Proposed Permutation Measure and Other Existing Ones
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Comparisons between the Sixteen Evaluated Algorithms Based\
      \ on the Proximity Coefficient \u03B3 P"
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2470671
- Affiliation of the first author: department of engineering enzo ferrari, university
    of modena and reggio emilia, modena, italy
  Affiliation of the last author: department of engineering enzo ferrari, university
    of modena and reggio emilia, modena, italy
  Figure 1 Link: articels_figures_by_rev_year\2015\Socially_Constrained_Structural_Learning_for_Groups_Detection_in_Crowd\figure_1.jpg
  Figure 1 caption: Examples of social groups detected in different crowds.
  Figure 10 Link: articels_figures_by_rev_year\2015\Socially_Constrained_Structural_Learning_for_Groups_Detection_in_Crowd\figure_10.jpg
  Figure 10 caption: F-1 scores obtained by all combinations of traintest pair sequences
    in MPT- 20 x 100 . Results were clustered (diagonal blocks C1-C4 from left to
    right) to highlight similar notion of group among sequences.
  Figure 2 Link: articels_figures_by_rev_year\2015\Socially_Constrained_Structural_Learning_for_Groups_Detection_in_Crowd\figure_2.jpg
  Figure 2 caption: 'Highlights of social groups properties: (a) hierarchical coherence,
    (b) density invariance and (c) transitivity.'
  Figure 3 Link: articels_figures_by_rev_year\2015\Socially_Constrained_Structural_Learning_for_Groups_Detection_in_Crowd\figure_3.jpg
  Figure 3 caption: 'Features: physical identity (a) and social identity (b,c) provide
    a computational interpretation of the concept of group membership, while (d) evaluates
    the likeliness of the existence of a shared goal between pedestrians.'
  Figure 4 Link: articels_figures_by_rev_year\2015\Socially_Constrained_Structural_Learning_for_Groups_Detection_in_Crowd\figure_4.jpg
  Figure 4 caption: Proxemics (a) are modeled as a GMM in Eq. (5) and (b) unveil physical
    identity through mutual positions inside the proxemic bubble.
  Figure 5 Link: articels_figures_by_rev_year\2015\Socially_Constrained_Structural_Learning_for_Groups_Detection_in_Crowd\figure_5.jpg
  Figure 5 caption: Visual example of causality probability. The vertical line is
    the S of Eq. (9) while the shaded area is dtextca .
  Figure 6 Link: articels_figures_by_rev_year\2015\Socially_Constrained_Structural_Learning_for_Groups_Detection_in_Crowd\figure_6.jpg
  Figure 6 caption: Intersecting heat maps are generated by converging trajectories,
    which project on the xy plane their shared goal.
  Figure 7 Link: articels_figures_by_rev_year\2015\Socially_Constrained_Structural_Learning_for_Groups_Detection_in_Crowd\figure_7.jpg
  Figure 7 caption: Differences in the way losses account for errors. Singletons are
    white. Figures (a, c, e) depict solution mathbf yi and the links considered by
    the respective losses, while (b, d, f) color pedestrians according to solution
    mathbf y and show the links on which the two solutions mathbf yi and mathbf y
    disagree.
  Figure 8 Link: articels_figures_by_rev_year\2015\Socially_Constrained_Structural_Learning_for_Groups_Detection_in_Crowd\figure_8.jpg
  Figure 8 caption: Comparison against baseline and [2] on MPT- 20 x 100 .
  Figure 9 Link: articels_figures_by_rev_year\2015\Socially_Constrained_Structural_Learning_for_Groups_Detection_in_Crowd\figure_9.jpg
  Figure 9 caption: Results on MPT- 20 x 100 highlight the complexity of each scene.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Francesco Solera
  Name of the last author: Rita Cucchiara
  Number of Figures: 15
  Number of Tables: 6
  Number of authors: 3
  Paper title: Socially Constrained Structural Learning for Groups Detection in Crowd
  Publication Date: 2015-08-20 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Proxemics Characterization as Found in Hall's Theory
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Comparative Results on Publicly Available Dataset Using the\
      \ G -MITRE Loss of Section 6.3 and the Positive Pairwise Loss \u0394 + PW of\
      \ [26]"
  Table 3 caption:
    table_text: 'TABLE 3 Datasets: Number of Pedestrians (p), Groups (g) and Density
      Metrics'
  Table 4 caption:
    table_text: TABLE 4 Evaluation of Our Proposal When Trained with Different Loss
      Functions
  Table 5 caption:
    table_text: TABLE 5 Spatial Depiction, Training Efficacy and Groups Predictability
      of the Clusters of Sequences of Fig. 10
  Table 6 caption:
    table_text: TABLE 6 Performance of Detector [49], Tracker [50] and Group Detection
      Algorithms (in Terms of G-MITRE) in a Fully Automatic Pipeline
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2470658
- Affiliation of the first author: helsinki institute for information technology hiit,
    department of computer science, aalto university, finland
  Affiliation of the last author: helsinki institute for information technology hiit,
    department of computer science, aalto university, finland
  Figure 1 Link: articels_figures_by_rev_year\2015\Archetypal_Analysis_for_Nominal_Observations\figure_1.jpg
  Figure 1 caption: "Illustration of (top) standard archetypal analysis for real-valued\
    \ observations and (bottom) the extension we propose for nominal observations.\
    \ Figure (a) shows a solution with three archetypes Z . Figure (b) shows the generating\
    \ observations (in color) for each archetype. Z \u22C51 and Z \u22C52 have two\
    \ generating observations, whereas Z \u22C53 has one generating observation. Figure\
    \ (c) shows the H values of the solution in a ternary plot. Observations outside\
    \ the archetypes solution are projected onto the boundary. Table (d) shows an\
    \ excerpt from the dataset with nominal features. Table (e) shows the solution\
    \ with three archetypes. Z \u22C53 , for example, is a married female who likes\
    \ golf or tennis. A few of the generating observations are visible in (d) with\
    \ respective colors. Figure (f) shows the H values; as in (c) the individual samples\
    \ are expressed as compositions of archetypes."
  Figure 10 Link: articels_figures_by_rev_year\2015\Archetypal_Analysis_for_Nominal_Observations\figure_10.jpg
  Figure 10 caption: "Simplex visualization for the German credit dataset. The individual\
    \ factors mathbf hj are projected into the circle spanned by the K archetypes\
    \ (see [9] for a detailed explanation of the simplex visualization). Figure (a)\
    \ highlights the two options \u201Cunskilled-resident\u201D (blue) and \u201C\
    highly qualified employee\u201D (purple) of the Job attributes. We can see that\
    \ \u201Cunskilled-resident\u201D observations are mainly located towards A1; and\
    \ \u201Chighly qualified employee\u201D observations are mainly located towards\
    \ A4. Figure (b) highlights the two options \u201Cgood\u201D (red) and \u201C\
    bad\u201D (blue) for the Credit risk attribute. We can see that \u201Cgood\u201D\
    \ observations are basically located everywhere, but \u201Cbad\u201D observations\
    \ are located towards A3 and A4."
  Figure 2 Link: articels_figures_by_rev_year\2015\Archetypal_Analysis_for_Nominal_Observations\figure_2.jpg
  Figure 2 caption: "(a) Plate diagram of probabilistic archetypal analysis as presented\
    \ in [9] : here w\u223CDir(1),h\u223CDir(1) and x\u223Cf(\u0398Wh) . (b) Plate\
    \ diagram of probabilistic archetypal analysis for nominal observations discussed\
    \ in this article. We investigate Dirichlet priors over the coefficient vectors:\
    \ here w\u223CDir(\u03B7) and h\u223CDir(\u03B1) . Additionally, z 1 \u223CMult(h)\
    \ , z 2 \u223CMult( w z 1 ) , and w i \u223CMult( \u0398 i \u22C5 z 2 ) . The\
    \ diagram generalizes two previous cases in [9]: the Bernoulli observation model\
    \ ( f is Bernoulli distribution) is achieved when v i =2 ( v i being the number\
    \ of categories in i th nominal feature) and t=1 for each i=1,\u2026,d ( t being\
    \ the number of instances of feature i ), whereas the multinomial observation\
    \ model ( f is multinomial distribution) is achieved when d=1 ( d being the number\
    \ of features). However, it also allows other observation models such as multiple-choice\
    \ questions when t=1 , but d and v i can be arbitrary, and multi-view textual\
    \ representation when d , t and v i are all arbitrary."
  Figure 3 Link: articels_figures_by_rev_year\2015\Archetypal_Analysis_for_Nominal_Observations\figure_3.jpg
  Figure 3 caption: "The figures illustrate the difference between EM and VB approaches.\
    \ Figures (a) and (b) show multiple EM and VB solutions (shown in different colors)\
    \ achieved on the same set of observations on a two dimensional simplex with K=10\
    \ archetypes. Figure (a') shows the variation of objective function with varying\
    \ number of archetypes K : it stabilizes at K=6 . Figures (a\u201D) and (b\u201D\
    ) show the coefficient matrix H for EM and VB solution. While EM solution assigns\
    \ weights to all archetypes, VB solution sets certain weights to zero: the archetypes\
    \ with nonzero factor values are called 'active'. Figure (b') shows the number\
    \ of active archetypes from different trials for each K . Since the problem is\
    \ simple the VB solution almost always indicates to the correct number of archetypes."
  Figure 4 Link: articels_figures_by_rev_year\2015\Archetypal_Analysis_for_Nominal_Observations\figure_4.jpg
  Figure 4 caption: "The figures illustrate the difference between EM and VB approaches.\
    \ Figures (a) and (b) show multiple EM and VB solutions achieved on the same set\
    \ of binary observations in eight dimensions with K=8 archetypes: the archetypes\
    \ are binarized and then shown in decimal. Figure (a') shows the variation of\
    \ objective function with varying number of archetypes: this displays a monotonic\
    \ increase in likelihood value without a clear elbow. Figures (a\u201D) and (b\u201D\
    ) show the coefficient matrix H for EM and VB solution. While EM solution assigns\
    \ weights to all archetypes, VB solution sets certain weights to zero: the archetypes\
    \ with nonzero factor values are called 'active'. Figure (b') shows the number\
    \ of active archetypes from different trials for each K . Since the problem is\
    \ difficult the VB solution indicates different number of archetypes but around\
    \ the right ballpark. If we consider the best objective value then it finds the\
    \ correct archetypes."
  Figure 5 Link: articels_figures_by_rev_year\2015\Archetypal_Analysis_for_Nominal_Observations\figure_5.jpg
  Figure 5 caption: "The figure illustrates the dependence of number of inferred archetypes\
    \ on hyperparameter alpha as a \u201Cbubble chart\u201D, the width of the bubble\
    \ is the fraction of times the number of inferred archetypes is y given hyperparameter\
    \ value x . We observe that as hyperparameter is increased, more active archetypes\
    \ have been found."
  Figure 6 Link: articels_figures_by_rev_year\2015\Archetypal_Analysis_for_Nominal_Observations\figure_6.jpg
  Figure 6 caption: "The figure shows the inferred number of archetypes for varying\
    \ number of true archetypes found by VB solution for two different sample sizes\
    \ n as a \u201Cbubble chart\u201D, the width of the bubble is the fraction of\
    \ times the number of inferred archetypes is y given true number of archetypes\
    \ x ."
  Figure 7 Link: articels_figures_by_rev_year\2015\Archetypal_Analysis_for_Nominal_Observations\figure_7.jpg
  Figure 7 caption: 'Comparison among topic modeling, probabilistic archetypal analysis,
    and clustering. The gray circles are observations, blue circles are topics, green
    circles are archetypal profiles, and red circles are centroids. We observe that:
    1) archetypal profiles offer better interpretability when the observations are
    close to each other, and 2) topics can be abstract whereas archetypal profiles
    are closely related to the observations. Refer to Section 2.2 for more information.'
  Figure 8 Link: articels_figures_by_rev_year\2015\Archetypal_Analysis_for_Nominal_Observations\figure_8.jpg
  Figure 8 caption: 'Comparison of archetypal analysis and clustering of binary observations
    on four datasets. The datasets (a)-(d) are arranged along increasing difficulty
    of the task: while dna has clear cluster structure, random does not have any.
    For each sub-figure the top (black and white) boxes show the assignments of observations
    to archetypes (left) or clusters (right). The flatter bottom (black and white)
    boxes show the representative observations as prototypes in the same order as
    the groups in top (black and white) boxes. The assignments are similar if the
    task is easier. The flatter boxes in between the two black and white boxes show
    the respective protopypes (archetypal profiles for AA and cluster centers for
    clustering method). It can be observed that the prototypes achieved by the two
    methods are more different if the task is more difficult. The square boxes on
    the right side of the prototype boxes show the pairwise distance between prototypes.
    For better visibility, the values are normalized by the largest distance achieved
    by both methods. It is observed that archetypes are further apart than the cluster
    centers. For more details see Section 3.'
  Figure 9 Link: articels_figures_by_rev_year\2015\Archetypal_Analysis_for_Nominal_Observations\figure_9.jpg
  Figure 9 caption: The VB lower bound versus the number of active archetypes for
    the experiments (Section 4). The best solution out of 100 trials is marked by
    a red plus.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Sohan Seth
  Name of the last author: Manuel J.A. Eugster
  Number of Figures: 11
  Number of Tables: 3
  Number of authors: 2
  Paper title: Archetypal Analysis for Nominal Observations
  Publication Date: 2015-08-20 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Prototypes Obtained by Archetypal Analysis and Bernoulli Mixture
      Modeling for Austrian Guest Survey (Section 4.1)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Prototypes Obtained by Archetypal Analysis and Latent Dirichlet
      Allocation for German Credit Dataset (Section 4.2)
  Table 3 caption:
    table_text: TABLE 3 Prototypes Obtained by Archetypal Analysis and Bernoulli Mixture
      Modeling for SUN Attribute Dataset (Section 4.3)
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2470655
- Affiliation of the first author: electrical and computer engineering department
    and the scientific computing and imaging institute, university of utah, salt lake
    city, ut
  Affiliation of the last author: electrical and computer engineering department and
    the scientific computing and imaging institute, university of utah, salt lake
    city, ut
  Figure 1 Link: articels_figures_by_rev_year\2015\Semantic_Image_Segmentation_with_Contextual_Hierarchical_Models\figure_1.jpg
  Figure 1 caption: 'Results of CHM on different tasks. First row: Semantic segmentation
    (Stanford background dataset [18]. Second row: Horse segmentation (Weizmann dataset
    [17]. Third row: Membrane detection (mouse neuropil dataset [9]). Fourth row:
    Edge detection (Berkeley dataset [19]). See Section 4 for details.'
  Figure 10 Link: articels_figures_by_rev_year\2015\Semantic_Image_Segmentation_with_Contextual_Hierarchical_Models\figure_10.jpg
  Figure 10 caption: Test samples of edge detection on BSDS 500 [19] dataset. (a)
    Input image, (b) gPb-OWT-UCM [53], (c) Sketch tokens [58], (d) SCG [12], (e) SE
    [11], (f) CHM, (g) Groundtruth. CHM is able to capture finer details like upper
    stairs in the first row, steeples in the second row, and wheels in the third row.
  Figure 2 Link: articels_figures_by_rev_year\2015\Semantic_Image_Segmentation_with_Contextual_Hierarchical_Models\figure_2.jpg
  Figure 2 caption: Illustration of the contextual hierarchical model. The blue classifiers
    are learned during the bottom-up step and the red classifier is learned during
    the top-down step. In the bottom-up step, each classifier takes the outputs of
    lower classifiers as well as the input image as input. The height of the hierarchy,
    L, is three in this model but it can be extended to any arbitrary number.
  Figure 3 Link: articels_figures_by_rev_year\2015\Semantic_Image_Segmentation_with_Contextual_Hierarchical_Models\figure_3.jpg
  Figure 3 caption: F-value of the classifiers trained at the original resolution
    in the CHM with LDNN and random forest. The overfitting in the random forest makes
    it useless in the CHM architecture.
  Figure 4 Link: articels_figures_by_rev_year\2015\Semantic_Image_Segmentation_with_Contextual_Hierarchical_Models\figure_4.jpg
  Figure 4 caption: Test results of the Weizmann horse dataset. (a) Input image, (b)
    MSANN [50] , (c) CHM-RF, (d) CHM-LDNN, (e) ground truth images. The CHM-LDNN is
    more successful in completing the body of horses.
  Figure 5 Link: articels_figures_by_rev_year\2015\Semantic_Image_Segmentation_with_Contextual_Hierarchical_Models\figure_5.jpg
  Figure 5 caption: 'Test samples of semantic segmentation on Stanford background
    dataset [18] . First row: Input image. Second row: CHM. Third row: CHM with intra-class
    connection. Fourth row: Groundtruth. Using intra-class contextual information
    improves the performance.'
  Figure 6 Link: articels_figures_by_rev_year\2015\Semantic_Image_Segmentation_with_Contextual_Hierarchical_Models\figure_6.jpg
  Figure 6 caption: The confusion matrix of CHM results on the Stanford background
    dataset [18] . The overall class-average accuracy is 74.32 %.
  Figure 7 Link: articels_figures_by_rev_year\2015\Semantic_Image_Segmentation_with_Contextual_Hierarchical_Models\figure_7.jpg
  Figure 7 caption: Performance of CHM on the Stanford Background dataset using different
    number of levels.
  Figure 8 Link: articels_figures_by_rev_year\2015\Semantic_Image_Segmentation_with_Contextual_Hierarchical_Models\figure_8.jpg
  Figure 8 caption: Per class accuracy of different methods on SIFT flow dataset [71].
    The classes are sorted from most frequent to least frequent.
  Figure 9 Link: articels_figures_by_rev_year\2015\Semantic_Image_Segmentation_with_Contextual_Hierarchical_Models\figure_9.jpg
  Figure 9 caption: Precision-recall curves of CHM in comparison with other methods
    for BSDS 500 dataset [19].
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Mojtaba Seyedhosseini
  Name of the last author: Tolga Tasdizen
  Number of Figures: 14
  Number of Tables: 8
  Number of authors: 2
  Paper title: Semantic Image Segmentation with Contextual Hierarchical Models
  Publication Date: 2015-08-27 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Testing Performance of Different Methods on the Weizmann Horse
      Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Testing Performance of Different Methods on Stanford Background
      Dataset [18] : Pixelwise Accuracy, Class-Average Accuracy, and Computation Time'
  Table 3 caption:
    table_text: TABLE 3 Testing Performance of Different Methods on the SIFT Flow
      Dataset
  Table 4 caption:
    table_text: TABLE 4 Testing Performance of Different Methods on BSDS 500 Dataset
      [19]
  Table 5 caption:
    table_text: TABLE 5 Testing Performance of Different Methods on NYU Depth Dataset
      [20] Using RGB (Top), and RGBD (Bottom) Modalities
  Table 6 caption:
    table_text: TABLE 6 Testing Performance of Different Methods on NYU Depth Dataset
      [20] Using BSDS 500 Dataset [19] for Training
  Table 7 caption:
    table_text: TABLE 7 Testing Performance of Different Methods for the Mouse Neuropil
      and Drosophila VNC Datasets
  Table 8 caption:
    table_text: TABLE 8 Pixel Error ( 1 -F-value) and Training Time (Hours) of Different
      Methods on ISBI Challenge [79] Test Set
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2473846
- Affiliation of the first author: multimedia ip center, korea electronics and technology
    institute, seongnam-si, gyeonggido, republic of korea
  Affiliation of the last author: school of information and communications, gwangju
    institute of science and technology, gwangju, republic of korea
  Figure 1 Link: articels_figures_by_rev_year\2015\Interacting_Multiview_Tracker\figure_1.jpg
  Figure 1 caption: Tracking results from videos with low contrast, drastic lighting
    changes, and pose variations (best viewed on high-resolution displays). The proposed
    algorithm (IMT) performs favorably against three top-ranked trackers (i.e., Struck
    [13], SCM [37], and ASLA [16]) from a recent benchmark study [31] . Quantitative
    results are presented in Table 3 and Fig. 7.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Interacting_Multiview_Tracker\figure_2.jpg
  Figure 2 caption: "As trackers are constructed using different features, corresponding\
    \ posterior distributions ( p(x|z) ) are of different scales. \u03C3 u denotes\
    \ the standard deviation of u ."
  Figure 3 Link: articels_figures_by_rev_year\2015\Interacting_Multiview_Tracker\figure_3.jpg
  Figure 3 caption: Components of the proposed tracking algorithm.
  Figure 4 Link: articels_figures_by_rev_year\2015\Interacting_Multiview_Tracker\figure_4.jpg
  Figure 4 caption: "Graphical model: Hidden variable (object state x t , a selected\
    \ tracker index m t , TPM \u03A9 t ) and observation (observed image z t ). 1)\
    \ The TPM is updated using the current observation. 2) The tracker selection is\
    \ conducted by updating the tracker probability based on the current observation\
    \ and the TPM. 3) Each object state is estimated based on current observation,\
    \ tracker selection, tracker interaction, and TPM."
  Figure 5 Link: articels_figures_by_rev_year\2015\Interacting_Multiview_Tracker\figure_5.jpg
  Figure 5 caption: Representation update examples. The transient and stable features
    are shown in the red and blue boxes, respectively. The learned principal components
    are shown in the green boxes. The yellow circles demonstrate the updated stable
    features at different frames.
  Figure 6 Link: articels_figures_by_rev_year\2015\Interacting_Multiview_Tracker\figure_6.jpg
  Figure 6 caption: Changes of interaction probabilities on the diagonal of the TPM
    and tracker probabilities. Each color line represents one type of trackers. Each
    color box represents one type of appearance changes. The results are obtained
    by running the IMT 10 times.
  Figure 7 Link: articels_figures_by_rev_year\2015\Interacting_Multiview_Tracker\figure_7.jpg
  Figure 7 caption: 'The area under curve (AUC) of each success plot [31]. OPE: Running
    the trackers throughout each sequence with initializations of the ground truth
    positions. TRE: Running the trackers with initialization from the ground truth
    position at different frames. SRE: Running the trackers with initialization from
    the different bounding boxes at the first frame. In all evaluation metrics, the
    IMT performs well against the other state-of-the-art methods.'
  Figure 8 Link: articels_figures_by_rev_year\2015\Interacting_Multiview_Tracker\figure_8.jpg
  Figure 8 caption: Experimental results of state-of-the-art tracking methods.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.54
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.56
  Name of the first author: Ju Hong Yoon
  Name of the last author: Kuk-Jin Yoon
  Number of Figures: 8
  Number of Tables: 3
  Number of authors: 3
  Paper title: Interacting Multiview Tracker
  Publication Date: 2015-08-27 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Interaction Probability Basis of the i th Tracker \u03C9\
      \ i s =[ \u03C9 1,i s , \u03C9 2,i s , \u03C9 3,i s ] Where s Denotes the Basis\
      \ Index"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average Tracking Success Rate on 16 Benchmark Sequences in
      Table 3
  Table 3 caption:
    table_text: TABLE 3 Success Rate Using the Same Default Parameters
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2473862
- Affiliation of the first author: department of computer science, boston university,
    boston, ma
  Affiliation of the last author: department of computer science, boston university,
    boston, ma
  Figure 1 Link: articels_figures_by_rev_year\2015\Exploiting_Surroundedness_for_Saliency_Detection_A_Boolean_Map_Approach\figure_1.jpg
  Figure 1 caption: (a) Image from the MIT dataset [19] (left) and its eye tracking
    data (right). (b) Saliency maps estimated by (from left to right) AIM [9], LG
    [10] and our method. AIM and LG measure an image patch's saliency based on its
    rarity. Our method, based on global structural information, is less responsive
    to the elements in the background.
  Figure 10 Link: articels_figures_by_rev_year\2015\Exploiting_Surroundedness_for_Saliency_Detection_A_Boolean_Map_Approach\figure_10.jpg
  Figure 10 caption: Speed performance comparison.
  Figure 2 Link: articels_figures_by_rev_year\2015\Exploiting_Surroundedness_for_Saliency_Detection_A_Boolean_Map_Approach\figure_2.jpg
  Figure 2 caption: The Pipeline of BMS. An image is first represented by a set of
    randomly generated Boolean maps. For each Boolean map, a binary activation map
    is produced by suppressing unsurrounded regions. Then a real-valued attention
    map is obtained by normalizing the activation map. At last, attention maps are
    linearly combined.
  Figure 3 Link: articels_figures_by_rev_year\2015\Exploiting_Surroundedness_for_Saliency_Detection_A_Boolean_Map_Approach\figure_3.jpg
  Figure 3 caption: The pipeline of the attention map computation. See the text for
    details.
  Figure 4 Link: articels_figures_by_rev_year\2015\Exploiting_Surroundedness_for_Saliency_Detection_A_Boolean_Map_Approach\figure_4.jpg
  Figure 4 caption: "In the test image (top left), there are two surrounded square\
    \ regions. The one with higher contrast is slightly blurred on its border. The\
    \ image values range in [0,1] , and we have added Gaussian noise with \u03C3=0.05\
    \ to it. Given that the seeds are the image border pixels, four types of distance\
    \ transforms are shown: MBD (top middle), geodesic distance (bottom left), fuzzy\
    \ distance (bottom middle) and max-arc distance (bottom right). The mean activation\
    \ map in BMS (top right) is computed using sample step \u03B4=0.02 . The values\
    \ of all the maps are re-scaled for visualization. See text for more discussion."
  Figure 5 Link: articels_figures_by_rev_year\2015\Exploiting_Surroundedness_for_Saliency_Detection_A_Boolean_Map_Approach\figure_5.jpg
  Figure 5 caption: (a) Test image. (b) Mean attention map in our formulation. (c)
    MBD transform. (d) Geodesic distance transform. See text for more discussion.
  Figure 6 Link: articels_figures_by_rev_year\2015\Exploiting_Surroundedness_for_Saliency_Detection_A_Boolean_Map_Approach\figure_6.jpg
  Figure 6 caption: sAUC scores against the level of blur. For each plot, the x axis
    is the Gaussian blur STD relative to the largest dimension of the saliency maps,
    and the y axis is the sAUC score.
  Figure 7 Link: articels_figures_by_rev_year\2015\Exploiting_Surroundedness_for_Saliency_Detection_A_Boolean_Map_Approach\figure_7.jpg
  Figure 7 caption: The influence of distance-to-center re-weighting on the NSS scores.
    On each plot, the blue bars represent the NSS scores under the optimal blurring
    computed without distance-to-center re-weighting, and the red bars represent the
    NSS scores under the optimal blurring computed with DTC re-weighting.
  Figure 8 Link: articels_figures_by_rev_year\2015\Exploiting_Surroundedness_for_Saliency_Detection_A_Boolean_Map_Approach\figure_8.jpg
  Figure 8 caption: Sample images and saliency maps of compared models. The second
    column shows the eye fixation heat maps, which are obtained by blurring the eye
    fixation maps.
  Figure 9 Link: articels_figures_by_rev_year\2015\Exploiting_Surroundedness_for_Saliency_Detection_A_Boolean_Map_Approach\figure_9.jpg
  Figure 9 caption: A closer look at the input images in Figs. 8a and 8b.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Jianming Zhang
  Name of the last author: Stan Sclaroff
  Number of Figures: 14
  Number of Tables: 3
  Number of authors: 2
  Paper title: 'Exploiting Surroundedness for Saliency Detection: A Boolean Map Approach'
  Publication Date: 2015-08-27 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Eye Tracking Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Compared Models
  Table 3 caption:
    table_text: TABLE 3 Evaluation Scores. The best score on each dataset is shown
      in red. The 2nd and 3rd best are underlined.
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2473844
- Affiliation of the first author: institute of industrial science, university of
    tokyo, japan
  Affiliation of the last author: institute of industrial science, university of tokyo,
    japan
  Figure 1 Link: articels_figures_by_rev_year\2015\Separating_Reflective_and_Fluorescent_Components_Using_High_Frequency_Illuminati\figure_1.jpg
  Figure 1 caption: (a) The scene captured under white light. (b) The recovered reflective
    component. (c) The recovered fluorescent component.
  Figure 10 Link: articels_figures_by_rev_year\2015\Separating_Reflective_and_Fluorescent_Components_Using_High_Frequency_Illuminati\figure_10.jpg
  Figure 10 caption: Recovered reflectance r(lambda) , fluorescence emission e(lambda)
    and absorption a(lambda) spectra of the red () and yellow ( ) sheets.
  Figure 2 Link: articels_figures_by_rev_year\2015\Separating_Reflective_and_Fluorescent_Components_Using_High_Frequency_Illuminati\figure_2.jpg
  Figure 2 caption: An example of absorption and emission spectra from the McNamara
    and Boswell Fluorescence Spectral Dataset [15].
  Figure 3 Link: articels_figures_by_rev_year\2015\Separating_Reflective_and_Fluorescent_Components_Using_High_Frequency_Illuminati\figure_3.jpg
  Figure 3 caption: An example of a captured scene (a). When a reflective-fluorescent
    point in the scene is lit by the illuminant (b), which is a high frequency binary
    illumination pattern in the wavelength domain, each lit wavelength includes both
    reflective and fluorescent components while the unlit wavelengths have only the
    fluorescent component. (c) shows its complement.
  Figure 4 Link: articels_figures_by_rev_year\2015\Separating_Reflective_and_Fluorescent_Components_Using_High_Frequency_Illuminati\figure_4.jpg
  Figure 4 caption: Sinusoidal illuminant patterns. The blue and pink solid lines
    denote two illumination patterns. There is a phase shift between them.
  Figure 5 Link: articels_figures_by_rev_year\2015\Separating_Reflective_and_Fluorescent_Components_Using_High_Frequency_Illuminati\figure_5.jpg
  Figure 5 caption: The percentage of absorption spectra in the McNamara and Boswell
    fluorescence spectral dataset where k1 = k2 given different the period of the
    illumination. The smaller the period of the illumination, the more absorption
    spectra satisfy our requirement that k1 = k2 .
  Figure 6 Link: articels_figures_by_rev_year\2015\Separating_Reflective_and_Fluorescent_Components_Using_High_Frequency_Illuminati\figure_6.jpg
  Figure 6 caption: Absorption and emission spectra of two fluorescent materials.
  Figure 7 Link: articels_figures_by_rev_year\2015\Separating_Reflective_and_Fluorescent_Components_Using_High_Frequency_Illuminati\figure_7.jpg
  Figure 7 caption: All test errors sorted in ascending order. 67 percent of cases
    were below the average error of 0.012.
  Figure 8 Link: articels_figures_by_rev_year\2015\Separating_Reflective_and_Fluorescent_Components_Using_High_Frequency_Illuminati\figure_8.jpg
  Figure 8 caption: Examples of estimated absorption spectra and their root-mean-square-errors.
  Figure 9 Link: articels_figures_by_rev_year\2015\Separating_Reflective_and_Fluorescent_Components_Using_High_Frequency_Illuminati\figure_9.jpg
  Figure 9 caption: Evaluation of our separation method on a pink sheet (). (a) Two
    high frequency illuminations. (c) and (e) show the recovered reflectance and fluorescence
    emission spectra under these high frequency illuminations, respectively. (b) Two
    low frequency illuminations. (d) and (f) show the recovered reflectance and fluorescence
    emission spectra under these low frequency illuminations, respectively. The red
    lines show the ground truths and the blue lines show the estimated results.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ying Fu
  Name of the last author: Yoichi Sato
  Number of Figures: 21
  Number of Tables: 1
  Number of authors: 5
  Paper title: Separating Reflective and Fluorescent Components Using High Frequency
    Illumination in the Spectral Domain
  Publication Date: 2015-08-27 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Mean Percent Difference between k 1 and k 2 for 183 Absorption
      Spectra on CIE Standard Illuminants [36] with the Ideal Sinusoidal Pattern Filters
      and Real Filters
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2473839
- Affiliation of the first author: department of mathematics and computer science,
    the open university of israel, raanana, israel
  Affiliation of the last author: the department of mathematics and computer science,
    the open university of israel, israel and with the university of southern california,
    information sciences institute (isi), ca
  Figure 1 Link: articels_figures_by_rev_year\2015\Dense_Correspondences_across_Scenes_and_Scales\figure_1.jpg
  Figure 1 caption: "Dense correspondences between the same semantic content (\u201C\
    smiley\u201D) in different scenes and scales. Top: Input images. Bottom: Image\
    \ hallucination results produced by warping the colors of the target onto the\
    \ source using the estimated flow from source to target. A good result has the\
    \ colors of the target located in the same position as their matching semantic\
    \ regions in the source. Results show the output of SIFT flow using DSIFT, without\
    \ local scale selections (bottom left), and our method with scale propagation\
    \ (bottom right)."
  Figure 10 Link: articels_figures_by_rev_year\2015\Dense_Correspondences_across_Scenes_and_Scales\figure_10.jpg
  Figure 10 caption: 'Example LMO benchmark [15] results. From left to right: Input
    test image; the most similar reference image; the per pixel labels of the reference
    image; labels estimated using DSIFT; labels estimated using our Match-aware scale
    propagation; finally, ground truth labels of the input image.'
  Figure 2 Link: articels_figures_by_rev_year\2015\Dense_Correspondences_across_Scenes_and_Scales\figure_2.jpg
  Figure 2 caption: Visualizing three means of scale propagation. (a) Input images.
    (b) Sparse interest point detections, using the SIFT, DoG-based feature detector
    implemented by vlfeat [7] . Detections are visualized according to estimated scales.
    (c-e) Per-pixel scale estimates, S I (p) , color-coded. (c) Geometric scale propagation
    (Section 3.1); (d) Image-aware propagation (Section 3.2); (e) Match-aware propagation,
    described in Section 3.3. Note how in (e) similar scale distributions are apparent
    for both images. Color-bars on the right provide legends for actual scale values.
  Figure 3 Link: articels_figures_by_rev_year\2015\Dense_Correspondences_across_Scenes_and_Scales\figure_3.jpg
  Figure 3 caption: "Effect of wrong scale estimates on flow accuracy using SIFT flow\
    \ [6] . Top: Scale-maps for 20, 50, and 80 percent scale assignment errors, visualized\
    \ by color coding scales (color-bar on the right). The correct scale is the default\
    \ value of 2.667 for all pixels. Mid: Visualizing the assigned scales, for every\
    \ 15th pixel. Bottom: Angular errors (left) and endpoint errors (right), \xB1\
    \ SE, for increasing errors in scale estimates. Evidently, flow remains accurate\
    \ up until about 20 percent errors rates."
  Figure 4 Link: articels_figures_by_rev_year\2015\Dense_Correspondences_across_Scenes_and_Scales\figure_4.jpg
  Figure 4 caption: 'Image hallucination results. Each row presents dense correspondences
    established from source to target image, illustrated by warping the target back
    to the source using the estimated flow. The following methods and representations
    are compared, from left to right: DSIFT [7], SID [8], SLS [9], Segmentation aware
    SID (Seg. SID) [29], DAISY-Flow [35] and SIFT descriptors extracted using our
    Match-aware scale propagation. In nearly all these examples NRDC [34] failed to
    find matches between the two images and is therefore omitted from this figure.
    Good results should have the colors of the target photos, warped to the shapes
    appearing in the source photos.'
  Figure 5 Link: articels_figures_by_rev_year\2015\Dense_Correspondences_across_Scenes_and_Scales\figure_5.jpg
  Figure 5 caption: "Image hallucination results\u2014comparison of proposed methods.\
    \ Each row presents dense correspondences established from a source image to its\
    \ target, illustrated by warping the target back to its sources using the estimated\
    \ flow. We compare our proposed methods for propagating scales, from left to right:\
    \ Geometric scale propagation (Section 3.1), image-aware propagation (Section\
    \ 3.2), and match-aware propagation (Section 3.3). Each hallucination result provides\
    \ also a visualization of the estimated flow field. Flow legend is provided on\
    \ the bottom right."
  Figure 6 Link: articels_figures_by_rev_year\2015\Dense_Correspondences_across_Scenes_and_Scales\figure_6.jpg
  Figure 6 caption: Quantitative results on the Moseg benchmark [45]. Average overlap
    between estimated and ground-truth segmentations for frame pairs separated by
    increasing temporal intervals. Our match-aware scale propagation (shaded blue
    line) is only outperformed by Seg. SID [29] and on-par with SID [8], despite being
    an order of magnitude faster and smaller than both.
  Figure 7 Link: articels_figures_by_rev_year\2015\Dense_Correspondences_across_Scenes_and_Scales\figure_7.jpg
  Figure 7 caption: Qualitative Moseg benchmark [45] results. Each result shows the
    ground truth segmentation of the objects in the image drawn in red over the warped
    target photos.
  Figure 8 Link: articels_figures_by_rev_year\2015\Dense_Correspondences_across_Scenes_and_Scales\figure_8.jpg
  Figure 8 caption: 'Make3D [48] benchmark, qualitative examples. Single image depth
    estimation results using the Depth Transfer of [14]. Left to right: Input image;
    depth estimated using the standard DSIFT representation; depth estimated using
    our match-aware scale propagation; the ground truth. See text for more details.'
  Figure 9 Link: articels_figures_by_rev_year\2015\Dense_Correspondences_across_Scenes_and_Scales\figure_9.jpg
  Figure 9 caption: LMO [15] semantic segmentation results. Numbers of correctly labeled
    pixels from each category. Results compare identical pipelines using DSIFT (left)
    and our Match-aware scale propagation (right).
  First author gender probability: 0.97
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.74
  Name of the first author: Moria Tau
  Name of the last author: Tal Hassner
  Number of Figures: 13
  Number of Tables: 4
  Number of authors: 2
  Paper title: Dense Correspondences across Scenes and Scales
  Publication Date: 2015-08-28 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Different Descriptor Dimensions, and Flow-Estimation
      Times
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results on the Scaled-Middlebury Benchmark
  Table 3 caption:
    table_text: TABLE 3 Results on the Middlebury Benchmark, Not Scaled
  Table 4 caption:
    table_text: TABLE 4 Make3D [48] Benchmark, Quantitative Results
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2474356
- Affiliation of the first author: school of computer science, the university of adelaide,
    sa, australia
  Affiliation of the last author: australian research council centre of excellence
    for robotic vision
  Figure 1 Link: articels_figures_by_rev_year\2015\Pedestrian_Detection_with_Spatially_Pooled_Features_and_Structured_Ensemble_Lear\figure_1.jpg
  Figure 1 caption: Architecture of our pooled features. In this example, sp-Cov are
    extracted from each fixed sized pooling region.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Pedestrian_Detection_with_Spatially_Pooled_Features_and_Structured_Ensemble_Lear\figure_2.jpg
  Figure 2 caption: ROC curves of our sp-Cov features and the conventional covariance
    detector [3] on INRIA test images.
  Figure 3 Link: articels_figures_by_rev_year\2015\Pedestrian_Detection_with_Spatially_Pooled_Features_and_Structured_Ensemble_Lear\figure_3.jpg
  Figure 3 caption: "Decision boundaries on the toy data set where each strong classifier\
    \ consists of 10 weak classifiers (horizontal and vertical decision stumps). Positive\
    \ and negative data are represented by \u2218 and \xD7 , respectively. The partial\
    \ AUC score in the FPR range [0,0.2] is also displayed. Our approach achieves\
    \ the best pAUC score compared to other asymmetric classifiers."
  Figure 4 Link: articels_figures_by_rev_year\2015\Pedestrian_Detection_with_Spatially_Pooled_Features_and_Structured_Ensemble_Lear\figure_4.jpg
  Figure 4 caption: Precision-recall curves of our approach and state-of-the-art detectors
    (DA-PDM [60], LSVM-MDPM-sv [61], LSVM-MDPM-us [13] and mBoW [62]) on the KITTI
    pedestrian detection test set.
  Figure 5 Link: articels_figures_by_rev_year\2015\Pedestrian_Detection_with_Spatially_Pooled_Features_and_Structured_Ensemble_Lear\figure_5.jpg
  Figure 5 caption: ROC curves of our approach and several state-of-the-art detectors
    (ACF+SDt [8] , MT-DPM+Context [63], MT-DPM [63] , MultiResC+2Ped [64], ACF-Caltech
    [37], MOCO [65], MF+Motion+2Ped [64] , DBN-Mut [14], Roerei [34], MultiResC [66],
    MultiFtr+Motion [38] , ACF [37], HOG [2] and VJ [5]) on the Caltech pedestrian
    test set.
  Figure 6 Link: articels_figures_by_rev_year\2015\Pedestrian_Detection_with_Spatially_Pooled_Features_and_Structured_Ensemble_Lear\figure_6.jpg
  Figure 6 caption: "Left: The spatial distribution of features selected by pAUCEns\
    \ T . White pixels indicate that a large number of low-level visual features are\
    \ selected in that area. These regions correspond to human head, shoulders and\
    \ feet. Right: The learned linear SVM model from the BING classifier. Each pixel\
    \ shows the SVM weight. Note the similarity between the learned SVM weights and\
    \ SVM weights of HOG (Fig. 6b in [2]), i.e., large SVM weights are near the head\
    \ and shoulder contour ( \u2227 -shape)."
  Figure 7 Link: articels_figures_by_rev_year\2015\Pedestrian_Detection_with_Spatially_Pooled_Features_and_Structured_Ensemble_Lear\figure_7.jpg
  Figure 7 caption: The change in the detection performance as we vary the threshold
    value of the BING detector (evaluated on the Caltech pedestrian test set). BING(thresh
    = 0 ) represents the proposed two-stage pedestrian detector, in which the first
    stage is the BING classifier with the threshold value of zero and and the second
    stage is the pAUCEns T detector described in Section 3.3.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Sakrapee Paisitkriangkrai
  Name of the last author: Anton van den Hengel
  Number of Figures: 7
  Number of Tables: 7
  Number of authors: 3
  Paper title: Pedestrian Detection with Spatially Pooled Features and Structured
    Ensemble Learning
  Publication Date: 2015-08-28 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Computational Complexity of Our Approach and AdaBoost
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Log-Average Miss Rate of Our Features with and without Applying
      Spatial Pooling
  Table 3 caption:
    table_text: TABLE 3 Log-Average Miss Rates of Various Feature Combinations
  Table 4 caption:
    table_text: TABLE 4 The pAUC Score on Protein-Protein Interaction Data Set
  Table 5 caption:
    table_text: TABLE 5 Average pAUC Scores in the FPR Range [0,0.1] and Their Standard
      Deviations on Vision Data Sets at Various Boosting Iterations
  Table 6 caption:
    table_text: TABLE 6 Proportion of Windows Rejected by Tuning the Threshold of
      the BING Classifier
  Table 7 caption:
    table_text: TABLE 7 Log-Average Miss Rate and Evaluation Time of Various AdaBoost
      Based Pedestrian Detectors with Different Soft Cascade's Rejection Thresholds
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2474388
- Affiliation of the first author: institute of high performance computing (ihpc),
    the agency for science, technology and research, singapore
  Affiliation of the last author: center for quantum computation & intelligent systems,
    university of technology, sydney, nsw 2006, australia
  Figure 1 Link: articels_figures_by_rev_year\2015\CoLabeling_for_MultiView_Weakly_Labeled_Learning\figure_1.jpg
  Figure 1 caption: Comparison between co-training and our co-labeling.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\CoLabeling_for_MultiView_Weakly_Labeled_Learning\figure_2.jpg
  Figure 2 caption: Organization of the three-layer structure for the multi-layer
    MKL in our co-labeling framework. The circles denote the input-output kernels,
    and the rectangles with different colors denote the combination coefficients at
    different layers, respectively. We impose different regularizers on the combination
    coefficients at each layer when combining them to obtain their parent node (e.g.,
    from the blue rectangles to the green rectangles).
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Xinxing Xu
  Name of the last author: Ivor W. Tsang
  Number of Figures: 2
  Number of Tables: 6
  Number of authors: 4
  Paper title: Co-Labeling for Multi-View Weakly Labeled Learning
  Publication Date: 2015-09-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Mean Average Precisions over 81 Concepts from Different Methods
      on the NUS-WIDE Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Summarization of the Datasets Used in Two-View SSL
  Table 3 caption:
    table_text: "TABLE 3 MAPs (Means \xB1 Standard Deviations (%)) over Five Classes\
      \ for Different Methods on the BBC and BBCSport Datasets"
  Table 4 caption:
    table_text: TABLE 4 Summarization of the Reuters Multilingual Dataset Used in
      Multi-View SSL
  Table 5 caption:
    table_text: "TABLE 5 MAPs (Means \xB1 Standard Deviations (%)) over Six Classes\
      \ and All Five Languages from Different Methods on the Reuters Multilingual\
      \ Dataset"
  Table 6 caption:
    table_text: "TABLE 6 APs (Means \xB1 Standard Deviations (%)) from Different Methods\
      \ on the 20 Newsgroups Data Set When Using Different Number of Normal Training\
      \ Documents (i.e., N )"
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2476813
