- Affiliation of the first author: department of computer science and engineering,
    shanghai jiao tong university, shanghai, china
  Affiliation of the last author: cowarobot company, ltd., anhui province key laboratory
    of multimodal cognitive computation, hefei, china
  Figure 1 Link: articels_figures_by_rev_year\2022\SCRDet_Detecting_Small_Cluttered_and_Rotated_Objects_via_InstanceLevel_Feature_D\figure_1.jpg
  Figure 1 caption: Small, cluttered and rotated objects in complex scene whereby
    rotation detection plays an important role. Red boxes indicate missing detection
    which are suppressed by non-maximum suppression (NMS).
  Figure 10 Link: articels_figures_by_rev_year\2022\SCRDet_Detecting_Small_Cluttered_and_Rotated_Objects_via_InstanceLevel_Feature_D\figure_10.jpg
  Figure 10 caption: Visual illustration of detection results on the datasets of COCO,
    ICDAR2015, S 2 TLD before (right) and after (left) using InLD.
  Figure 2 Link: articels_figures_by_rev_year\2022\SCRDet_Detecting_Small_Cluttered_and_Rotated_Objects_via_InstanceLevel_Feature_D\figure_2.jpg
  Figure 2 caption: 'The pipeline of our method (using RetinaNet [15] as an embodiment).
    Our SCRDet++ mainly consists of four modules: basic embodiment for feature extraction,
    Image-level denoising for removing common image noise, instance-level denoising
    module for suppressing instance noise (i.e., inter-class feature coupling and
    distraction between intra-class and background) and the class+box branch for predicting
    classification score and bounding box position. C and A represent the number of
    object categories and the number of anchor at each feature point, respectively.'
  Figure 3 Link: articels_figures_by_rev_year\2022\SCRDet_Detecting_Small_Cluttered_and_Rotated_Objects_via_InstanceLevel_Feature_D\figure_3.jpg
  Figure 3 caption: 'Images (left) and their feature maps before (middle) and after
    (right) the instance-level denoising operation. First row: non-object with object-like
    shape. Second row: inter-class feature coupling and intra-class feature boundary
    blurring. Third row: weak feature response.'
  Figure 4 Link: articels_figures_by_rev_year\2022\SCRDet_Detecting_Small_Cluttered_and_Rotated_Objects_via_InstanceLevel_Feature_D\figure_4.jpg
  Figure 4 caption: 'Feature maps corresponding to clean images (top) and to their
    noisy versions (bottom). The noise is randomly generated by a Gaussian function
    with a mean value of 0 and a variance of 0.005. The first and third columns: images;
    the rest columns: feature maps. The contrast between foreground and background
    in the feature map of the clean image is more obvious (second column), and the
    boundaries between dense objects are clearer (fourth column).'
  Figure 5 Link: articels_figures_by_rev_year\2022\SCRDet_Detecting_Small_Cluttered_and_Rotated_Objects_via_InstanceLevel_Feature_D\figure_5.jpg
  Figure 5 caption: Feature map with decoupled category-specific feature signals along
    channels. The abbreviation HA, SP, SH, and SV indicate Harbor, Swimming pool,
    Ship, and Small vehicle, respectively. Others include background and unseen categories
    that do not appear in the image. Features of different categories are decoupled
    into their respective channels (top and middle), while the features of object
    and background are enhanced and suppressed in spatial domain, respectively (bottom).
  Figure 6 Link: articels_figures_by_rev_year\2022\SCRDet_Detecting_Small_Cluttered_and_Rotated_Objects_via_InstanceLevel_Feature_D\figure_6.jpg
  Figure 6 caption: "Rotation box definitions (OpenCV definition). \u03B8 denotes\
    \ the acute angle to the x -axis, and for the other side we refer it as w . The\
    \ range of angle representation is [\u221290,0) ."
  Figure 7 Link: articels_figures_by_rev_year\2022\SCRDet_Detecting_Small_Cluttered_and_Rotated_Objects_via_InstanceLevel_Feature_D\figure_7.jpg
  Figure 7 caption: Boundary discontinuity of angle regression. Blue, green, red bounding
    box denotes anchorproposal, ground-truth, prediction box.
  Figure 8 Link: articels_figures_by_rev_year\2022\SCRDet_Detecting_Small_Cluttered_and_Rotated_Objects_via_InstanceLevel_Feature_D\figure_8.jpg
  Figure 8 caption: Detection results by two losses. For this dense arrangement case,
    the angle estimation error will also make the classification even harder.
  Figure 9 Link: articels_figures_by_rev_year\2022\SCRDet_Detecting_Small_Cluttered_and_Rotated_Objects_via_InstanceLevel_Feature_D\figure_9.jpg
  Figure 9 caption: Illustrations of the five categories and different lighting and
    weather conditions in our collected S 2 TLD dataset as released in the paper.
  First author gender probability: 0.72
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.87
  Name of the first author: Xue Yang
  Name of the last author: Tao He
  Number of Figures: 12
  Number of Tables: 11
  Number of authors: 6
  Paper title: 'SCRDet++: Detecting Small, Cluttered and Rotated Objects via Instance-Level
    Feature Denoising and Rotation Loss Smoothing'
  Publication Date: 2022-04-12 00:00:00
  Table 1 caption: TABLE 1 Ablative Study of Five Image Level Denoising Settings as
    Used in [20] on the OBB Task of DOTA-V1.0 Dataset
  Table 10 caption: TABLE 10 Accuracy (%) on DIOR
  Table 2 caption: TABLE 2 Ablative Study for Speed and Accuracy of InLD on OBB Task
    of DOTA
  Table 3 caption: TABLE 3 Ablative Study by Accuracy (%) of the Number of Dilated
    Convolution on Pyramid Levels and the InLD Loss L InLD LInLD in InLD on OBB Task
    of DOTA
  Table 4 caption: TABLE 4 Detailed Ablative Study by Accuracy (%) of the Effect of
    InLD on Two Traffic Light Datasets
  Table 5 caption: TABLE 5 Ablative Study by Accuracy (%) of ImLD, InLD and Their
    Combination (Numbers in Bracket Denote Relative Improvement Against Using InLD
    Alone) on Different Datasets and Different Detection Tasks
  Table 6 caption: TABLE 6 Ablative Study by Accuracy (%) of Each Component in Our
    Method on the OBB Task of DOTA-V1.0 Dataset
  Table 7 caption: TABLE 7 Ablative Study by Accuracy (%) of IoU-Smooth L1 Loss by
    Using It or Not in the Three Methods on the OBB Task of DOTA-V1.0 Dataset
  Table 8 caption: TABLE 8 Ablative Study by Accuracy (%) of IoU-Smooth L1 Loss on
    the OBB Task of DOTA-V1.0, DOTA-V1.5 and DOTA-V2.0
  Table 9 caption: TABLE 9 AP and mAP (%) Across Categories of OBB and HBB Task on
    DOTA
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3166956
- Affiliation of the first author: beijing key laboratory of mobile computing and
    pervasive device, institute of computing technology, chinese academy of sciences,
    beijing, china
  Affiliation of the last author: beijing key laboratory of mobile computing and pervasive
    device, institute of computing technology, chinese academy of sciences, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2022\Robust_Pose_Transfer_With_Dynamic_Details_Using_Neural_Video_Rendering\figure_1.jpg
  Figure 1 caption: With only an accessible Internet video as training data, our neural
    rendering framework is able to synthesize temporally coherent person images from
    given pose sequences, while keeping rich details compared with traditional rendering
    results using static textures.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Robust_Pose_Transfer_With_Dynamic_Details_Using_Neural_Video_Rendering\figure_2.jpg
  Figure 2 caption: "The pipeline of our training process. Pose labels I t\u22121\
    \ pose and I t pose are extracted from the temporal context of adjacent video\
    \ frames. The UV generator takes pose labels as input and predicts the UV coordinates\
    \ of each pixel, i.e., I uv . Afterwards, the hybrid texture is mapped to the\
    \ screen space according to the predicted UV coordinates and then translated to\
    \ human foreground images with dynamic details. Meanwhile, background image is\
    \ refined during the training process and final synthesized images are obtained\
    \ through a combination of foreground and background images. Note that adjacent\
    \ frames are trained as a whole for the implicit learning of temporal coherence\
    \ in the neural video renderer."
  Figure 3 Link: articels_figures_by_rev_year\2022\Robust_Pose_Transfer_With_Dynamic_Details_Using_Neural_Video_Rendering\figure_3.jpg
  Figure 3 caption: The illustration of D 2 G-Net. I ht fg is obtained by mapping
    the hybrid texture to the screen space. The translation network aims to translate
    the high-dimensional feature to RGB color with dynamic details under the current
    pose. Note that the pose label is conditioned on the feature level to guide the
    translation process.
  Figure 4 Link: articels_figures_by_rev_year\2022\Robust_Pose_Transfer_With_Dynamic_Details_Using_Neural_Video_Rendering\figure_4.jpg
  Figure 4 caption: Comparison with image-translation based approaches. We compare
    our approach with EDN [1] and V2V [2]. Our approach could produce more realistic
    imitating results due to the robust 3D representation, even when trained only
    with a short monocular video.
  Figure 5 Link: articels_figures_by_rev_year\2022\Robust_Pose_Transfer_With_Dynamic_Details_Using_Neural_Video_Rendering\figure_5.jpg
  Figure 5 caption: Compared with other neural rendering baselines, our approach produces
    much clearer details due to the embedded image-translation components, i.e., the
    hybrid texture representation and D 2 G-Net.
  Figure 6 Link: articels_figures_by_rev_year\2022\Robust_Pose_Transfer_With_Dynamic_Details_Using_Neural_Video_Rendering\figure_6.jpg
  Figure 6 caption: Comparison of pose-aware dynamic details. The two rows correspond
    to consecutive frames of t and t+1 , respectively. Compared with TNA [7] and DTL
    [9], our method better delineates details for different poses. We show the results
    of self-transfer for the reference of ground truth. More pose transfer results
    will be exhibited in the supplementary material, available online.
  Figure 7 Link: articels_figures_by_rev_year\2022\Robust_Pose_Transfer_With_Dynamic_Details_Using_Neural_Video_Rendering\figure_7.jpg
  Figure 7 caption: We visualize the effect of key components in our approach. Compared
    with other variants, our approach generates clearer high-frequency details (1st
    row) and pose-varying wrinkles (2nd row).
  Figure 8 Link: articels_figures_by_rev_year\2022\Robust_Pose_Transfer_With_Dynamic_Details_Using_Neural_Video_Rendering\figure_8.jpg
  Figure 8 caption: Examples of the same generated person with different backgrounds.
    Our approach is able to generate videos with arbitrary novel backgrounds by replacing
    the refined background image.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.51
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.64
  Name of the first author: Yang-Tian Sun
  Name of the last author: Lin Gao
  Number of Figures: 8
  Number of Tables: 2
  Number of authors: 6
  Paper title: Robust Pose Transfer With Dynamic Details Using Neural Video Rendering
  Publication Date: 2022-04-12 00:00:00
  Table 1 caption: TABLE 1 Quantitative Comparison With SOTA Methods
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The Ablation Study for the Effect of the Image Translation
    Component
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3166989
- Affiliation of the first author: intelligent systems lab, intel labs, santa clara,
    ca, usa
  Affiliation of the last author: intelligent systems lab, intel labs, santa clara,
    ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\Enhancing_Photorealism_Enhancement\figure_1.jpg
  Figure 1 caption: 'We train convolutional networks to enhance the photorealism of
    rendered images, using intermediate buffers produced by a conventional rendering
    engine. Left: frames from a modern computer game (GTA V) [27]. Right: same frames
    enhanced by our approach to mimic the style of Cityscapes [28]. Enhancements by
    our method are semantically consistent and non-trivial. For example, our method
    adds gloss to cars (1st row), reforests parched hills to mimic German climate
    (2nd row), and paves roads with smoother asphalt (3rd row). Insets magnify marked
    regions.'
  Figure 10 Link: articels_figures_by_rev_year\2022\Enhancing_Photorealism_Enhancement\figure_10.jpg
  Figure 10 caption: Scene layouts are different in GTA (top) and Cityscapes (bottom).
    This affects the probability of seeing certain classes at specific positions.
    For example, GTA is more likely to have sky at the top of the image, while Cityscapes
    is more likely to have trees there and a star logo on the hood of the car. Fig.
    11 illustrates the impact of this structural mismatch on photorealism enhancement.
  Figure 2 Link: articels_figures_by_rev_year\2022\Enhancing_Photorealism_Enhancement\figure_2.jpg
  Figure 2 caption: Targeting different real-world datasets. We train our method to
    enhance images from GTA (top left) with KITTI, Cityscapes, and Vistas as target
    datasets. Our method is able to reproduce the characteristic appearance of these
    datasets (e.g., sensor noise in KITTI, saturation in Cityscapes, fine textures
    in Vistas) while keeping the structure of the original GTA images. Insets show
    sample images from the respective target datasets.
  Figure 3 Link: articels_figures_by_rev_year\2022\Enhancing_Photorealism_Enhancement\figure_3.jpg
  Figure 3 caption: Our approach is built around an image enhancement network that
    transforms a rendered image. In addition to the image, the network ingests G-buffer
    feature tensors at multiple scales. The tensors represent rendering information
    from a conventional graphics pipeline, encoded by a G-buffer encoder network.
    We train both networks jointly via an LPIPS loss (to retain the structure of the
    rendered image) and a perceptual discriminator (to maximize the realism of the
    enhanced image).
  Figure 4 Link: articels_figures_by_rev_year\2022\Enhancing_Photorealism_Enhancement\figure_4.jpg
  Figure 4 caption: Image enhancement network. We replace the batch normalization
    layers within HRNet by rendering-aware denormalization (RAD), forming RAD blocks
    (RB). Each branch of the HRNet receives a G-buffer feature tensor at a matching
    scale (different scales are coded by color). Original feature streams are shown
    in gray. We omit initial stem convolutions as well as transition and fusion layers
    for clarity.
  Figure 5 Link: articels_figures_by_rev_year\2022\Enhancing_Photorealism_Enhancement\figure_5.jpg
  Figure 5 caption: For a rendered image (left), G-buffers represent information on
    geometry (e.g., normal, depth), materials (e.g., albedo, glossiness), and lighting
    (e.g., atmosphere). A semantic segmentation, which can be derived from the G-buffers
    [27], [87], provides further high-level information on the scene.
  Figure 6 Link: articels_figures_by_rev_year\2022\Enhancing_Photorealism_Enhancement\figure_6.jpg
  Figure 6 caption: "G-buffer encoder. The G-buffer encoder accounts for different\
    \ data types and varying spatial density of the G-buffers. It processes them via\
    \ multiple streams (0\u2013c), which are fused into a joint representation in\
    \ accordance with one-hot-encoded object IDs. The features are further transformed\
    \ via residual blocks (see Fig. 7), of which the orange blocks downscale the tensors.\
    \ The output scale of the feature tensors is red, the channel width is blue. Scales\
    \ match with branches in the image enhancement network."
  Figure 7 Link: articels_figures_by_rev_year\2022\Enhancing_Photorealism_Enhancement\figure_7.jpg
  Figure 7 caption: Residual blocks. In the G-buffer encoder and RAD modules we employ
    residual blocks. They consist of convolutional layers (kernel size 3) with spectral
    normalization [94] and ReLUs. Changes in channel width or downscaling are performed
    in Conv 1 and Conv P. If channel width and resolution are constant, the projection
    via Conv P is omitted.
  Figure 8 Link: articels_figures_by_rev_year\2022\Enhancing_Photorealism_Enhancement\figure_8.jpg
  Figure 8 caption: "Rendering-aware denormalization (RAD) modulates image feature\
    \ tensors via encoded geometry, material, lighting, and semantic information from\
    \ a conventional rendering pipeline. The image features are normalized via group\
    \ normalization, then scaled and shifted via per-element weights \u03B3,\u03B2\
    \ . The weights are learned and adapt to G-buffer features received from the G-buffer\
    \ encoder (Fig. 6). To better adapt the weights, we transform the G-buffer features\
    \ via three residual blocks within each RAD module."
  Figure 9 Link: articels_figures_by_rev_year\2022\Enhancing_Photorealism_Enhancement\figure_9.jpg
  Figure 9 caption: Perceptual discriminator. A perceptual discriminator evaluates
    the realism of enhanced images. It consists of pretrained robust segmentation
    (MSeg [74]) and perceptual (VGG [95]) networks (green). These provide high-level
    semantic information via label maps and perceptual feature tensors. The maps and
    tensors are ingested by discriminator networks, which produce a realism score
    map. For clarity, we only show a single discriminator network, indicated by the
    dashed rectangle.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: Stephan R. Richter
  Name of the last author: Vladlen Koltun
  Number of Figures: 20
  Number of Tables: 3
  Number of authors: 3
  Paper title: Enhancing Photorealism Enhancement
  Publication Date: 2022-04-12 00:00:00
  Table 1 caption: "TABLE 1 Comparison to Prior Work. All Methods Were Trained on\
    \ the Cityscapes Dataset. Performance Reported as Kernel Inception Distance \xD7\
    1000 \xD71000 (KID) and Semantically Aligned Kernel VGG Distance \xD71000 \xD7\
    1000 (sKVD). Lower is Better"
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Controlled Experiments. Each Specific Idea in Our Approach
    Outperforms the Respective Baselines. In Each Condition, We Train for 600K Iterations
    on GTA and Cityscapes. Lower is Better
  Table 3 caption: TABLE 3 Targeting Different Datasets
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3166687
- Affiliation of the first author: bnrist, kliss, school of software, blbci, thuibcs,
    tsinghua university, beijing, china
  Affiliation of the last author: bnrist, kliss, school of software, blbci, thuibcs,
    tsinghua university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Graph_Learning_on_Millions_of_Data_in_Seconds_Label_Propagation_Acceleration_on_\figure_1.jpg
  Figure 1 caption: The performance comparison of recent state-of-the-art graph learning
    methods for large-scale data on the SUSY dataset with 5 million samples.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Graph_Learning_on_Millions_of_Data_in_Seconds_Label_Propagation_Acceleration_on_\figure_2.jpg
  Figure 2 caption: The comparison of traditional graph learning framework and our
    proposed DDGL method. The red and blue points are the labeled data, and all other
    gray points are the unlabeled ones. The green points are new unlabeled data.
  Figure 3 Link: articels_figures_by_rev_year\2022\Graph_Learning_on_Millions_of_Data_in_Seconds_Label_Propagation_Acceleration_on_\figure_3.jpg
  Figure 3 caption: The visualization results of different methods.
  Figure 4 Link: articels_figures_by_rev_year\2022\Graph_Learning_on_Millions_of_Data_in_Seconds_Label_Propagation_Acceleration_on_\figure_4.jpg
  Figure 4 caption: Experimental results of different methods in the incremental procedure.
  Figure 5 Link: articels_figures_by_rev_year\2022\Graph_Learning_on_Millions_of_Data_in_Seconds_Label_Propagation_Acceleration_on_\figure_5.jpg
  Figure 5 caption: "Experimental results of our proposed DDGL method with respect\
    \ to different numbers of labeled data \u03B3 in each class."
  Figure 6 Link: articels_figures_by_rev_year\2022\Graph_Learning_on_Millions_of_Data_in_Seconds_Label_Propagation_Acceleration_on_\figure_6.jpg
  Figure 6 caption: Experimental results of our proposed DDGL method with respect
    to different feature dimensions.
  Figure 7 Link: articels_figures_by_rev_year\2022\Graph_Learning_on_Millions_of_Data_in_Seconds_Label_Propagation_Acceleration_on_\figure_7.jpg
  Figure 7 caption: Experimental results of our proposed DDGL method with respect
    to different numbers of landmarks on MNIST dataset.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Yubo Zhang
  Name of the last author: Yue Gao
  Number of Figures: 7
  Number of Tables: 10
  Number of authors: 6
  Paper title: 'Graph Learning on Millions of Data in Seconds: Label Propagation Acceleration
    on Graph Using Data Distribution'
  Publication Date: 2022-04-12 00:00:00
  Table 1 caption: TABLE 1 The Time Complexity Comparison of Graph-Based Methods
  Table 10 caption: TABLE 10 The Running Time on Testing Datasets (Seconds)
  Table 2 caption: TABLE 2 Description of the Six Testing Datasets
  Table 3 caption: TABLE 3 Experimental Comparison on the USPS Dataset in Terms of
    Average Classification Error Rate.(%)
  Table 4 caption: TABLE 4 Experimental Comparison on the Letter Dataset in Terms
    of Average Classification Error Rate.(%)
  Table 5 caption: TABLE 5 Experimental Comparison on the CIFAR-10 Dataset in Terms
    of Average Classification Error Rate.(%)
  Table 6 caption: TABLE 6 Experimental Comparison on the MNIST Dataset in Terms of
    Average Classification Error Rate.(%)
  Table 7 caption: TABLE 7 Experimental Comparison on the Extended MNIST Dataset in
    Terms of Average Classification Error Rate.(%)
  Table 8 caption: TABLE 8 Experimental Comparison on the SUSY Dataset in Terms of
    Average Classification Error Rate.(%)
  Table 9 caption: TABLE 9 The Numbers of Nonzero Elements in Corresponding Matrix
    for Different Methods on Extended MNIST Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3166894
- Affiliation of the first author: department of computer science and engineering,
    the chinese university of hong kong, hong kong
  Affiliation of the last author: department of computer science and engineering,
    the chinese university of hong kong, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2022\ScaleAware_Automatic_Augmentations_for_Object_Detection_With_Dynamic_Training\figure_1.jpg
  Figure 1 caption: Scale-aware search space. It contains image-level and instance-level
    augmentation. Image-level augmentation includes zoom-in and zoom-out functions
    with probabilities and magnitudes for search. In instance-level, we introduce
    scale-aware area ratios, which make operations adaptive to objects in different
    scales. Augmented images are further generalized with the Gaussian map.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\ScaleAware_Automatic_Augmentations_for_Object_Detection_With_Dynamic_Training\figure_2.jpg
  Figure 2 caption: An example image of removing context.
  Figure 3 Link: articels_figures_by_rev_year\2022\ScaleAware_Automatic_Augmentations_for_Object_Detection_With_Dynamic_Training\figure_3.jpg
  Figure 3 caption: An example of Gaussian-based instance-level augmentation. It removes
    the original hard boundary and the augmented areas are adjustable to the Gaussian
    variance.
  Figure 4 Link: articels_figures_by_rev_year\2022\ScaleAware_Automatic_Augmentations_for_Object_Detection_With_Dynamic_Training\figure_4.jpg
  Figure 4 caption: Examples on different instance-level operations with magnitudes
    random sampled.
  Figure 5 Link: articels_figures_by_rev_year\2022\ScaleAware_Automatic_Augmentations_for_Object_Detection_With_Dynamic_Training\figure_5.jpg
  Figure 5 caption: Full-image examples on instance-level operations with bounds of
    the magnitudes.
  Figure 6 Link: articels_figures_by_rev_year\2022\ScaleAware_Automatic_Augmentations_for_Object_Detection_With_Dynamic_Training\figure_6.jpg
  Figure 6 caption: The overall evolutionary algorithm framework of our search method
    for learning data augmentation policies. We adopt the evolutionary algorithm for
    search, where a population of data augmentation policies are randomly initialized
    and then evolved in iterations. During search, policies are sampled from the search
    space. Then, they are trained and evaluated by our estimation metric. The computed
    metrics serve as feedback to update. Better policies are generated in this framework
    over time.
  Figure 7 Link: articels_figures_by_rev_year\2022\ScaleAware_Automatic_Augmentations_for_Object_Detection_With_Dynamic_Training\figure_7.jpg
  Figure 7 caption: "Dynamic training paradigm for scale-aware augmentation. For image-level\
    \ augmentations, we count the accumulated loss for small, middle, and large scales.\
    \ Then we select the proper augmentation format from zoom-out, original, and zoom-in\
    \ in a heuristic decision manner. For instance-level augmentation, we construct\
    \ a learnable module with the differentiable top-k method to determine the suitable\
    \ instance for copy-paste. The candidate instance map in the figure is shown as\
    \ 3\xD73 instances just for simplification. In practice, we construct 10\xD710\
    \ instances in experiments."
  Figure 8 Link: articels_figures_by_rev_year\2022\ScaleAware_Automatic_Augmentations_for_Object_Detection_With_Dynamic_Training\figure_8.jpg
  Figure 8 caption: Coefficients between actual accuracy and metrics. Our metric presents
    a higher coefficient than the proxy accuracy [20].
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.91
  Name of the first author: Yukang Chen
  Name of the last author: Jiaya Jia
  Number of Figures: 8
  Number of Tables: 20
  Number of authors: 8
  Paper title: Scale-Aware Automatic Augmentations for Object Detection With Dynamic
    Training
  Publication Date: 2022-04-12 00:00:00
  Table 1 caption: TABLE 1 Analysis on the Context for Scales
  Table 10 caption: TABLE 10 Comparison on Larg-Scale Jittering
  Table 2 caption: TABLE 2 Improvement Details on RetinaNet ResNet-50
  Table 3 caption: TABLE 3 Comparison With AutoAug-Det on RetinaNet ResNet-50
  Table 4 caption: TABLE 4 Search on RetinaNet ResNet-50 With Different Metrics
  Table 5 caption: TABLE 5 Improvements Across Detection Frameworks
  Table 6 caption: TABLE 6 Improvements on PASCAL VOC With Faster R-CNN on ResNet-50
    Backbone
  Table 7 caption: TABLE 7 Comparison With State-of-the-Art Data Augmentation Methods
    on COCO for Object Detection
  Table 8 caption: TABLE 8 Searched Augmentation Policy
  Table 9 caption: "TABLE 9 Scale Variation Issue on a Clean Faster R-CNN in 1\xD7"
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3166905
- Affiliation of the first author: department of computer science, georgia state university,
    atlanta, ga, usa
  Affiliation of the last author: department of computer science, georgia state university,
    atlanta, ga, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\Contingency_Space_A_Semimetric_Space_for_Classification_Evaluation\figure_1.jpg
  Figure 1 caption: The metric surface of precision ( S 1 pre ) visualized in the
    contingency space. On the left, a contour plot is used to illustrate the surface,
    where the darker values represent higher precision and the contours are added
    to show the changes of the curvature. On the right, the surfaces actual 3D view
    is shown to better illustrate the bivariate distribution function representing
    precisions values.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Contingency_Space_A_Semimetric_Space_for_Classification_Evaluation\figure_2.jpg
  Figure 2 caption: "The metric surfaces of 12 performance evaluation metrics (listed\
    \ in Table 2) are visualized. The surfaces on the left are generated under the\
    \ assumption of having a balanced dataset (therefore, in the contingency space\
    \ C 1 ), and for those on the right, an imbalance ratio of 1:5 is used (therefore,\
    \ generated in C 5 ). The juxtaposition of the surfaces on the two sides sheds\
    \ light on the impact of class imbalance on metrics behavior (as discussed in\
    \ Section 5.1). The color scale on the right maps the interval [\u22121,1] to\
    \ a spectrum of dark blue to dark red, respectively. The contours are drawn only\
    \ to accent the curvatures of the surfaces and not to imply that the metrics form\
    \ piecewise surfaces."
  Figure 3 Link: articels_figures_by_rev_year\2022\Contingency_Space_A_Semimetric_Space_for_Classification_Evaluation\figure_3.jpg
  Figure 3 caption: Two learning paths of a convolutional neural network on subsets
    of the MNIST dataset are compared. On the left, the classifier is learning to
    distinguish between the digits 0 and 1 (problem A), while on the right, it does
    the same but on the digits 3 and 8 (problem B) which have more similar structures.
    The difference between the two learning paths can be used as a proxy to verify
    that problem A is an easier classification task for CNN than problem B.
  Figure 4 Link: articels_figures_by_rev_year\2022\Contingency_Space_A_Semimetric_Space_for_Classification_Evaluation\figure_4.jpg
  Figure 4 caption: 'Distributions of the length of learning paths obtained by training
    a CNN algorithm on MNIST hand-written digits, in two scenarios: (1) trained on
    digits 0s and 1s (left), and (2) trained on hand-written digits 3s and 8s. When
    repeating this experiment 100 times, the Kolmogorov-Smirnov test returns a very
    small p -value indicating that the two distributions are not similar.'
  Figure 5 Link: articels_figures_by_rev_year\2022\Contingency_Space_A_Semimetric_Space_for_Classification_Evaluation\figure_5.jpg
  Figure 5 caption: Changes in the metric surface of the f 1 score as the imbalance
    ratio changes linearly from 1:1 (far left) to 1:32 (far right).
  Figure 6 Link: articels_figures_by_rev_year\2022\Contingency_Space_A_Semimetric_Space_for_Classification_Evaluation\figure_6.jpg
  Figure 6 caption: "The metric surfaces of f 1 score for two class imbalance ratios;\
    \ 1:1 (the top surface) and 1:32 (the bottom surface). The volume confined between\
    \ the two metric surfaces is used to define the imbalance sensitivity, IS \u03BC\
    \ (r) , a proxy to quantify a metrics sensitivity to class imbalance."
  Figure 7 Link: articels_figures_by_rev_year\2022\Contingency_Space_A_Semimetric_Space_for_Classification_Evaluation\figure_7.jpg
  Figure 7 caption: "The imbalance sensitivity IS \u03BC (r) for 8 classification\
    \ evaluation metrics (listed in Table 2) are compared as the positive-to-negative\
    \ class-imbalance ratio changes linearly from 1:1 to 1:32. Metrics such as Tau\
    \ (tau), recall (rec), true skill statistic (tss), and Youden J index (j) are\
    \ imbalance agnostic while others are impacted logarithmically."
  Figure 8 Link: articels_figures_by_rev_year\2022\Contingency_Space_A_Semimetric_Space_for_Classification_Evaluation\figure_8.jpg
  Figure 8 caption: Customization of weighted Tau through its 3 weight scalars. This
    prepares this metric for objectives of different tasks.
  Figure 9 Link: articels_figures_by_rev_year\2022\Contingency_Space_A_Semimetric_Space_for_Classification_Evaluation\figure_9.jpg
  Figure 9 caption: Comparison of Tau with the metrics listed in Table 2 on evaluation
    of CNNs learning process on a multi-class dataset.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Azim Ahmadzadeh
  Name of the last author: Rafal A. Angryk
  Number of Figures: 9
  Number of Tables: 2
  Number of authors: 4
  Paper title: 'Contingency Space: A Semimetric Space for Classification Evaluation'
  Publication Date: 2022-04-13 00:00:00
  Table 1 caption: TABLE 1 A Reference Table of Basic Random Variables (R.V.) Commonly
    Used for Performance Evaluation of Binary, Classification Algorithms Notations
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Some of the Popular Classification Performance Metrics
    and Their Formulas Based on the Confusion Matrix
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3167007
- Affiliation of the first author: national laboratory of pattern recognition, institute
    of automation, center for research on intelligent perception and computing, chinese
    academy of sciences, beijing, china
  Affiliation of the last author: national laboratory of pattern recognition, institute
    of automation, center for research on intelligent perception and computing, chinese
    academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Learning_to_Adapt_Across_Dual_Discrepancy_for_CrossDomain_Person_ReIdentificatio\figure_1.jpg
  Figure 1 caption: Illustration of dual discrepancy in cross-domain person re-ID.
    We consider intra-domain variation and inter-domain shift simultaneously. In terms
    of the former, cross-camera variations lead to a biased retrieval. As for the
    latter, the discrepancy between the source domain and the target domain hinders
    the effective adaptation.
  Figure 10 Link: articels_figures_by_rev_year\2022\Learning_to_Adapt_Across_Dual_Discrepancy_for_CrossDomain_Person_ReIdentificatio\figure_10.jpg
  Figure 10 caption: Statistics of neighborhood in Market rightarrow Duke experiment
    during the duration of training. For each iteration, we report the mean value
    within in the corresponding mini-batch. (a) The number of detected neighbors.
    (b) The minimum of the similarities between the detected neighbors and the query.
    (c) The precision of the neighborhood selection. (d) The recall of the neighborhood
    selection.
  Figure 2 Link: articels_figures_by_rev_year\2022\Learning_to_Adapt_Across_Dual_Discrepancy_for_CrossDomain_Person_ReIdentificatio\figure_2.jpg
  Figure 2 caption: Framework of our method. First, mixed data is generated by the
    convex combination between source-target pairs. Then, it is fed into the network
    together with target data to acquire image embeddings. After the normalization
    by a BN layer, each type of embeddings is assigned to its corresponding component.
    (1) With the help of an augmented memory, the learning of target embeddings is
    supervised by intra-camera and inter-camera neighborhood consistency. (2) As for
    mixed embeddings, we maintain a dynamic classifier to cover the label space of
    each source-target pair adaptively.
  Figure 3 Link: articels_figures_by_rev_year\2022\Learning_to_Adapt_Across_Dual_Discrepancy_for_CrossDomain_Person_ReIdentificatio\figure_3.jpg
  Figure 3 caption: The visualization of ranking lists in intra-camera matching and
    inter-camera matching. We perform retrieval on DukeMTMC-reID using a model pre-trained
    on Market-1501. The green frame indicates positive matches, while the red frame
    indicates negative matches. The score on the top of each gallery image represents
    its cosine similarity with the probe.
  Figure 4 Link: articels_figures_by_rev_year\2022\Learning_to_Adapt_Across_Dual_Discrepancy_for_CrossDomain_Person_ReIdentificatio\figure_4.jpg
  Figure 4 caption: Visualization of similarity distribution in intra-camera matching
    and inter-camera matching. The features are extracted by the supervised baseline
    model trained on DukeMTMC-reID. The similarities are computed on Market-1501.
  Figure 5 Link: articels_figures_by_rev_year\2022\Learning_to_Adapt_Across_Dual_Discrepancy_for_CrossDomain_Person_ReIdentificatio\figure_5.jpg
  Figure 5 caption: "Evaluation with different values of \u03F5 in Eq. (12)."
  Figure 6 Link: articels_figures_by_rev_year\2022\Learning_to_Adapt_Across_Dual_Discrepancy_for_CrossDomain_Person_ReIdentificatio\figure_6.jpg
  Figure 6 caption: "Evaluation with different values of the Beta distribution parameter\
    \ \u03B1 in Eq. (14)."
  Figure 7 Link: articels_figures_by_rev_year\2022\Learning_to_Adapt_Across_Dual_Discrepancy_for_CrossDomain_Person_ReIdentificatio\figure_7.jpg
  Figure 7 caption: Evaluation with different values of sigma in Eq. (1).
  Figure 8 Link: articels_figures_by_rev_year\2022\Learning_to_Adapt_Across_Dual_Discrepancy_for_CrossDomain_Person_ReIdentificatio\figure_8.jpg
  Figure 8 caption: Evaluation with different values of eta 1 and eta 2 in Eq. (19).
  Figure 9 Link: articels_figures_by_rev_year\2022\Learning_to_Adapt_Across_Dual_Discrepancy_for_CrossDomain_Person_ReIdentificatio\figure_9.jpg
  Figure 9 caption: The visualization of the ranking lists on Market-1501. The green
    frame indicates positive matches, while the red frame indicates negative matches.
    The text at the end of each line indicates the variant that produces the ranking
    list.
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.63
  Name of the first author: Chuanchen Luo
  Name of the last author: Zhaoxiang Zhang
  Number of Figures: 11
  Number of Tables: 16
  Number of authors: 3
  Paper title: Learning to Adapt Across Dual Discrepancy for Cross-Domain Person Re-Identification
  Publication Date: 2022-04-13 00:00:00
  Table 1 caption: TABLE 1 Summary of Hyper-Parameters
  Table 10 caption: TABLE 10 Effect of Interpolation
  Table 2 caption: "TABLE 2 Evaluation With Different Values of Scaling Factor \u03C4\
    \ \u03C4 in Eq. (3)"
  Table 3 caption: "TABLE 3 Evaluation With Different Values of Threshold \u03B8 \u03B8\
    \ in Eq. (9)"
  Table 4 caption: TABLE 4 Ablation Studies on Market-1501 and DukeMTMC-ReID
  Table 5 caption: TABLE 5 Ablation Studies on MSMT17
  Table 6 caption: TABLE 6 Ablation Study of Self-Paced Scheme in Camera-Aware Neighborhood
    Consistency
  Table 7 caption: TABLE 7 Ablation Study of Negative Prototypes in Cross-Domain Mixup
  Table 8 caption: TABLE 8 Ablation Study of Virtual Prototype
  Table 9 caption: TABLE 9 Ablation Study of Employing L m Lm and L s Ls Simultaneously
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3167053
- Affiliation of the first author: inception institute of artificial intelligence,
    abu dhabi, uae
  Affiliation of the last author: terminus group, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Learning_Enriched_Features_for_Fast_Image_Restoration_and_Enhancement\figure_1.jpg
  Figure 1 caption: Framework of the proposed MIRNet-v2 that learns enriched feature
    representations for image restoration and enhancement. MIRNet-v2 is based on a
    recursive residual design. In the core of MIRNet-v2 is the multi-scale residual
    block (MRB) whose main branch is dedicated to maintaining spatially-precise high-resolution
    representations through the entire network and the complimentary set of parallel
    branches provide better contextualized features.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Learning_Enriched_Features_for_Fast_Image_Restoration_and_Enhancement\figure_2.jpg
  Figure 2 caption: Schematic for selective kernel feature fusion (SKFF). It operates
    on features from different resolution streams, and performs aggregation based
    on self-attention.
  Figure 3 Link: articels_figures_by_rev_year\2022\Learning_Enriched_Features_for_Fast_Image_Restoration_and_Enhancement\figure_3.jpg
  Figure 3 caption: "Architecture of residual contextual block (RCB). In the first\
    \ two group convolution layers, g represents the number of groups. \u2297 denotes\
    \ matrix multiplication."
  Figure 4 Link: articels_figures_by_rev_year\2022\Learning_Enriched_Features_for_Fast_Image_Restoration_and_Enhancement\figure_4.jpg
  Figure 4 caption: Visual comparisons for dual-pixel defocus deblurring on the DPDD
    dataset [14]. Compared to the other approaches, our MIRNet-v2 more effectively
    removes blur while preserving the fine image details.
  Figure 5 Link: articels_figures_by_rev_year\2022\Learning_Enriched_Features_for_Fast_Image_Restoration_and_Enhancement\figure_5.jpg
  Figure 5 caption: Image denoising comparisons. First two examples are from SIDD
    [10] and the last is from DND [115]. The proposed MIRNet-v2 better preserves fine
    texture and structural patterns in the denoised images.
  Figure 6 Link: articels_figures_by_rev_year\2022\Learning_Enriched_Features_for_Fast_Image_Restoration_and_Enhancement\figure_6.jpg
  Figure 6 caption: "Comparisons for \xD74 super-resolution on the RealSR [117] dataset.\
    \ The image produced by our MIRNet-v2 is more faithful to the ground-truth than\
    \ other competing methods (see lines near the right edge of the crops)."
  Figure 7 Link: articels_figures_by_rev_year\2022\Learning_Enriched_Features_for_Fast_Image_Restoration_and_Enhancement\figure_7.jpg
  Figure 7 caption: "Additional visual examples for \xD74 super-resolution, comparing\
    \ our MIRNet-v2 against the state-of-the-art approach [117]. Note that all example\
    \ crops are taken from different images."
  Figure 8 Link: articels_figures_by_rev_year\2022\Learning_Enriched_Features_for_Fast_Image_Restoration_and_Enhancement\figure_8.jpg
  Figure 8 caption: Visual comparison of low-light enhancement approaches on the LoL
    dataset [85]. The image produced by our method is visually closer to the ground-truth
    in terms of brightness and global contrast.
  Figure 9 Link: articels_figures_by_rev_year\2022\Learning_Enriched_Features_for_Fast_Image_Restoration_and_Enhancement\figure_9.jpg
  Figure 9 caption: Visual results of image enhancement on the MIT-Adobe FiveK [118]
    dataset. Compared to the state-of-the-art, our MIRNet-v2 makes better color and
    contrast adjustments and produces images that appear vivid, natural and pleasant.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Syed Waqas Zamir
  Name of the last author: Ling Shao
  Number of Figures: 9
  Number of Tables: 11
  Number of authors: 7
  Paper title: Learning Enriched Features for Fast Image Restoration and Enhancement
  Publication Date: 2022-04-13 00:00:00
  Table 1 caption: TABLE 1 Comparison Between MIRNet-v2 and MIRNet [9] Under the Same
    Experimental Settings for Image Denoising Task on the SIDD Benchmark Dataset [10]
  Table 10 caption: TABLE 10 Effect of Progressive Learning
  Table 2 caption: TABLE 2 Dual-Pixel Defocus Deblurring Comparisons on the DPDD Dataset
    [14]
  Table 3 caption: TABLE 3 Denoising Comparisons on SIDD [10] and DND [115] Datasets
  Table 4 caption: TABLE 4 Super-Resolution Evaluation on the RealSR Dataset [117]
  Table 5 caption: TABLE 5 Low-Light Image Enhancement Evaluation on the LoL Dataset
    [85]
  Table 6 caption: TABLE 6 Image Enhancement Comparisons on the MIT-Adobe FiveK Dataset
    [118]
  Table 7 caption: TABLE 7 Impact of Individual Components of MRB
  Table 8 caption: TABLE 8 Effect of Individual Components of RCB
  Table 9 caption: TABLE 9 Feature Aggregation
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3167175
- Affiliation of the first author: school of big data and software engineering, chongqing
    university, chongqing, china
  Affiliation of the last author: department of electrical and computer engineering,
    university of alberta, edmonton, ab, canada
  Figure 1 Link: articels_figures_by_rev_year\2022\Negation_of_the_Quantum_Mass_Function_for_Multisource_Quantum_Information_Fusion\figure_1.jpg
  Figure 1 caption: The QBBA entropy in Example 1.
  Figure 10 Link: articels_figures_by_rev_year\2022\Negation_of_the_Quantum_Mass_Function_for_Multisource_Quantum_Information_Fusion\figure_10.jpg
  Figure 10 caption: Variation tendency of QBBA negation and CvD negation in Example
    7.
  Figure 2 Link: articels_figures_by_rev_year\2022\Negation_of_the_Quantum_Mass_Function_for_Multisource_Quantum_Information_Fusion\figure_2.jpg
  Figure 2 caption: The QBBA entropy in Example 2.
  Figure 3 Link: articels_figures_by_rev_year\2022\Negation_of_the_Quantum_Mass_Function_for_Multisource_Quantum_Information_Fusion\figure_3.jpg
  Figure 3 caption: The measure of entropy in Example 3.
  Figure 4 Link: articels_figures_by_rev_year\2022\Negation_of_the_Quantum_Mass_Function_for_Multisource_Quantum_Information_Fusion\figure_4.jpg
  Figure 4 caption: Variation tendency of QBBA negation in Example 4.
  Figure 5 Link: articels_figures_by_rev_year\2022\Negation_of_the_Quantum_Mass_Function_for_Multisource_Quantum_Information_Fusion\figure_5.jpg
  Figure 5 caption: Variation of Echi entropy in terms of QBBA negation.
  Figure 6 Link: articels_figures_by_rev_year\2022\Negation_of_the_Quantum_Mass_Function_for_Multisource_Quantum_Information_Fusion\figure_6.jpg
  Figure 6 caption: Variation tendency of QBBA negation in Example 5.
  Figure 7 Link: articels_figures_by_rev_year\2022\Negation_of_the_Quantum_Mass_Function_for_Multisource_Quantum_Information_Fusion\figure_7.jpg
  Figure 7 caption: Variation tendency of QBBA negation in Example 6.
  Figure 8 Link: articels_figures_by_rev_year\2022\Negation_of_the_Quantum_Mass_Function_for_Multisource_Quantum_Information_Fusion\figure_8.jpg
  Figure 8 caption: Variation tendency of Yin et al.s negation in Example 6.
  Figure 9 Link: articels_figures_by_rev_year\2022\Negation_of_the_Quantum_Mass_Function_for_Multisource_Quantum_Information_Fusion\figure_9.jpg
  Figure 9 caption: Entropy measures in Example 6.
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Fuyuan Xiao
  Name of the last author: Witold Pedrycz
  Number of Figures: 14
  Number of Tables: 5
  Number of authors: 2
  Paper title: Negation of the Quantum Mass Function for Multisource Quantum Information
    Fusion With its Application to Pattern Classification
  Publication Date: 2022-04-14 00:00:00
  Table 1 caption: TABLE 1 Magnitude and Entropy of QBBA Negation in Example 4
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 Cardinality Ratio of | \u03C8 j \u27E9 |\u03C8j\u3009\
    \ to | \u03C8 k \u27E9 |\u03C8k\u3009 in Example 4"
  Table 3 caption: TABLE 3 Characteristics of Different Negation Methods
  Table 4 caption: TABLE 4 Average Accuracies of Pattern Classification and STDs for
    the Iris Data Set
  Table 5 caption: TABLE 5 Average Accuracies of Pattern Classification and STDs of
    the Motor Rotor Fault Diagnosis Data Set
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3167045
- Affiliation of the first author: huazhong university of science and technology,
    wuhan, china
  Affiliation of the last author: microsoft research asia, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\VoxelTrack_MultiPerson_D_Human_Pose_Estimation_and_Tracking_in_the_Wild\figure_1.jpg
  Figure 1 caption: The top and right area show the images captured by five synchronized
    cameras. The bottom-left area shows the 3D poses estimated by our approach. The
    numbers represent the identities of the estimates. The points with different colors
    represent the 3D trajectories of the root joints of different persons over time.
    We project the estimated 3D poses to images for visualization. The red pyramids
    represent the locations and orientations of the five cameras.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\VoxelTrack_MultiPerson_D_Human_Pose_Estimation_and_Tracking_in_the_Wild\figure_2.jpg
  Figure 2 caption: Overview of VoxelTrack for 3D pose tracking. Given multi-view
    images as input, it first estimates pixel-wise pose heatmaps and Re-ID features
    for each view. Then the heatmaps are warped to construct a 3D feature grid which
    is fed to a 3D pose network to estimate 3D poses. We estimate person-person occlusion
    relationships in each view by using the 3D poses and camera parameters. Finally,
    we perform 3D pose tracking with the 3D poses, occlusion-masks and Re-ID features
    as input.
  Figure 3 Link: articels_figures_by_rev_year\2022\VoxelTrack_MultiPerson_D_Human_Pose_Estimation_and_Tracking_in_the_Wild\figure_3.jpg
  Figure 3 caption: The network structure for estimating 2D pose heatmaps and Re-ID
    features. I v , F v , H v and G v represent the image, backbone feature map, 2D
    heatmap, and Re-ID feature map of camera v , respectively.
  Figure 4 Link: articels_figures_by_rev_year\2022\VoxelTrack_MultiPerson_D_Human_Pose_Estimation_and_Tracking_in_the_Wild\figure_4.jpg
  Figure 4 caption: The network structure for estimating 3D joint heatmaps and 3D
    poses. We first use a Joint Estimation Network (JEN) to get the 3D heatmaps of
    all the joints and then use a Ambiguity Resolution Network (ARN) to get the 3D
    poses. H v , D , V , U represent the 2D heatmap, empty discrete voxels, feature
    vectors of the voxels and the 3D heatmap, respectively.
  Figure 5 Link: articels_figures_by_rev_year\2022\VoxelTrack_MultiPerson_D_Human_Pose_Estimation_and_Tracking_in_the_Wild\figure_5.jpg
  Figure 5 caption: Some steps to compute the occlusion relationship based on depth.
    The person depth map is computed using camera parameters and 3D poses. For the
    minimum depth map, the color becomes deeper as the depth becomes larger.
  Figure 6 Link: articels_figures_by_rev_year\2022\VoxelTrack_MultiPerson_D_Human_Pose_Estimation_and_Tracking_in_the_Wild\figure_6.jpg
  Figure 6 caption: Visualization results on the Shelf dataset. The top is the 2D
    images captured by 5 cameras and the bottom is the 3D pose tracking results. Different
    numbers and colors represent different person identities.
  Figure 7 Link: articels_figures_by_rev_year\2022\VoxelTrack_MultiPerson_D_Human_Pose_Estimation_and_Tracking_in_the_Wild\figure_7.jpg
  Figure 7 caption: "Visualization results on the \u201C160906pizza1\u201D sequence\
    \ of the Panoptic dataset. The top is the 2D images captured by 5 cameras and\
    \ the bottom is the 3D pose tracking results. Different numbers and colors represent\
    \ different person identities."
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.69
  Name of the first author: Yifu Zhang
  Name of the last author: Wenjun Zeng
  Number of Figures: 7
  Number of Tables: 7
  Number of authors: 5
  Paper title: 'VoxelTrack: Multi-Person 3D Human Pose Estimation and Tracking in
    the Wild'
  Publication Date: 2022-04-15 00:00:00
  Table 1 caption: TABLE 1 Comparison to the State-of-the-Art Methods on the Panoptic
    Dataset
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison to the State-of-The-Art Methods in the Purely
    Monocular Scenario on the Panoptic Dataset
  Table 3 caption: TABLE 3 Comparison to the State-of-the-Art Methods on the Campus
    and the Shelf Datasets
  Table 4 caption: TABLE 4 Comparison to the State-of-The-Art Methods on the Human3.6
    M Dataset
  Table 5 caption: TABLE 5 Ablation Study of Voxel Size and Sparse Convolution on
    the Panoptic Dataset
  Table 6 caption: TABLE 6 Ablation Study of Number of Views, 2D Backbone, and Input
    Image Size on the Panoptic Dataset
  Table 7 caption: TABLE 7 Ablation Study of Re-ID Features, Occlusion Mask, and 3D
    Poses on the Panoptic Dataset
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3163709
