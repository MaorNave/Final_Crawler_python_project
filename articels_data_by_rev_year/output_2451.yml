- Affiliation of the first author: adsplab, school of ece, peking university, shenzhen,
    china
  Affiliation of the last author: moe key laboratory of computational linguistics
    school of eecs, peking university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Aligning_Source_Visual_and_Target_Language_Domains_for_Unpaired_Video_Captioning\figure_1.jpg
  Figure 1 caption: Illustration of the pipeline system (left) and our UVC-VI system
    (right) under the video-to-Chinese captioning scenario. The target caption generated
    by pipeline system suffers from visual irrelevancy (see words highlighted red)
    and disfluency (see words highlighted blue) errors. The words highlighted purple
    denote the detailed visual element that is included in the caption. For better
    understanding, we add the English translation below the Chinese captions in brackets.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Aligning_Source_Visual_and_Target_Language_Domains_for_Unpaired_Video_Captioning\figure_2.jpg
  Figure 2 caption: Visualization of the (a) the raw visual embedding V and (b) projected
    visual embedding VIM(V) in 2D space by t-SNE [81] (see Eq. (4)). For comparison,
    we show the textual embedding T in the target language domain. We plot the scatter
    diagrams for 1,000 samples. As we can see, our VIM can align the latent space
    of V and T effectively, i.e., successfully project the visual embedding of source
    video domain into the target language domain.
  Figure 3 Link: articels_figures_by_rev_year\2021\Aligning_Source_Visual_and_Target_Language_Domains_for_Unpaired_Video_Captioning\figure_3.jpg
  Figure 3 caption: The examples of video captions generated by the Base Model and
    UVC-VI for different languages, i.e., French, German, Chinese and English, under
    the unpaired setting, where the video-caption pairs in target language are not
    available. As we can see, without the training on the pairs of video and target
    caption, our approach can generate fluent and desirable video captions for different
    languages.
  Figure 4 Link: articels_figures_by_rev_year\2021\Aligning_Source_Visual_and_Target_Language_Domains_for_Unpaired_Video_Captioning\figure_4.jpg
  Figure 4 caption: The examples of image captions generated by the Base Model and
    our UVC-VI for different languages, i.e., French, German, Chinese and English,
    under the unpaired setting. As we can see, our UVC-VI can be extended to unpaired
    image captioning, where the image-caption pairs in target language are not available,
    to generate desirable and fluent image captions.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Fenglin Liu
  Name of the last author: Xu Sun
  Number of Figures: 4
  Number of Tables: 7
  Number of authors: 6
  Paper title: Aligning Source Visual and Target Language Domains for Unpaired Video
    Captioning
  Publication Date: 2021-12-02 00:00:00
  Table 1 caption: TABLE 1 Training Datasets Needed for Different Approaches
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Performance of Automatic Evaluations on the MSVD and MSR-VTT
    Video Captioning Datasets Under the Unpaired Setting
  Table 3 caption: "TABLE 3 Performance of Human Evaluation for Comparing \u201CUVC-VI\u201D\
    \ With \u201CBase Model\u201D and \u201CSGN [7] + Google Translator [67]\u201D\
    , Under Three Language Application Scenarios, i.e., French, German and Chinese,\
    \ Where the Video-Caption Pairs in Target Language are not Available"
  Table 4 caption: TABLE 4 Quantitative Analysis of Our Approach Which is Performed
    on the Test Set of MSR-VTT Video Captioning Dataset [8] Under the Unpaired Setting
  Table 5 caption: TABLE 5 We Further Report the SPICE F-Scores [82] on the Test Set
    of MSR-VTT Dataset for a Better Understanding of the Differences of the Target
    Captions Generated by Different Methods
  Table 6 caption: TABLE 6 Analysis of the Multimodal Collaborative Encoder (MCE)
  Table 7 caption: TABLE 7 Performance of Our Proposed UVC-VI on the Test Split of
    MSCOCO Image Captioning Dataset [76] Under the Unpaired Setting, i.e., Unpaired
    Image Captioning, Where the Image-Caption Pairs in Target Language are not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3132229
- Affiliation of the first author: brain lab, northwestern polytechnical university,
    xian, shaanxi, china
  Affiliation of the last author: beijing university of chinese medicine, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2021\BackgroundClick_Supervision_for_Temporal_Action_Localization\figure_1.jpg
  Figure 1 caption: Illustration of click-level supervision. The ground truth is shown
    above the frame sequence. (a) The action-click supervision (shown in orange) makes
    a random click within each action instance, records the timestamp and classification
    label, used by SF-Net [11]. (b) The proposed background-click supervision (shown
    in red) makes a random click within each background segment and record the timestamp.
  Figure 10 Link: articels_figures_by_rev_year\2021\BackgroundClick_Supervision_for_Temporal_Action_Localization\figure_10.jpg
  Figure 10 caption: Qualitative comparisons between the proposed BackTAL and SF-Net
    [11] on THUMOS14 dataset, where the start time and end time of each action instance
    is depicted. For the second visualization, please view in zoom and pay attention
    to the tennis ball to distinguish action frames from backgrounds.
  Figure 2 Link: articels_figures_by_rev_year\2021\BackgroundClick_Supervision_for_Temporal_Action_Localization\figure_2.jpg
  Figure 2 caption: Performance analysis of a baseline method for weakly supervised
    action localization. (a) From the diagnosing results, it can be found that the
    majority of errors come from Background Error. (b) Given class activation sequence,
    the majority of top- k frames fall into action segments.
  Figure 3 Link: articels_figures_by_rev_year\2021\BackgroundClick_Supervision_for_Temporal_Action_Localization\figure_3.jpg
  Figure 3 caption: Framework of the proposed BackTAL. BackTAL first extracts video
    features, then uses three convolutional layers to classify each frame and estimate
    the class activation sequence. Finally, it performs top- k aggregation and predicts
    the video-level classification score. Based on the background-click supervision,
    BackTAL adopts the affinity module (see Section 3.4) to mine the feature information
    and estimate the local attention mask. This assists in calculating frame-specific
    temporal convolution. Besides, BackTAL explores the score separation module (see
    Section 3.3) and mines the position information. This can enlarge the response
    gap between action frames and background frames.
  Figure 4 Link: articels_figures_by_rev_year\2021\BackgroundClick_Supervision_for_Temporal_Action_Localization\figure_4.jpg
  Figure 4 caption: Class activation sequence before (a) and after (b) employing the
    score separation module. We can find that the score separation module contributes
    to suppress the responses of background frames and enhance the responses of action
    frames, which contributes to separate adjacent action instances.
  Figure 5 Link: articels_figures_by_rev_year\2021\BackgroundClick_Supervision_for_Temporal_Action_Localization\figure_5.jpg
  Figure 5 caption: Visualization of the local similarity mask. Given a video containing
    the shotput action, we select an action frame (shown in orange), and calculate
    similarities between the selected frame and its local neighbors. The generated
    local similarity mask exhibits high response for action frames and low response
    for background frames.
  Figure 6 Link: articels_figures_by_rev_year\2021\BackgroundClick_Supervision_for_Temporal_Action_Localization\figure_6.jpg
  Figure 6 caption: Processes to annotate the background-click information, illustrated
    with a video containing the action LongJump. First, we sparsely extract frames
    from the video with 2fps. Then, when meeting a background segment, the annotator
    randomly clicks a frame and annotates it as background. Afterwards, the video-level
    classification label is recorded at the end of the video. Finally, the annotation
    file can be generated for the complete video.
  Figure 7 Link: articels_figures_by_rev_year\2021\BackgroundClick_Supervision_for_Temporal_Action_Localization\figure_7.jpg
  Figure 7 caption: "Statistics of background-click annotations on THUMOS14 dataset.\
    \ The x -axis indicates the relative position for each annotation, while the y\
    \ -axis indicates percentage of annotated frames. We can find that background-click\
    \ annotations approximately exhibit the uniform distribution. \u201CA1\u201D,\
    \ \u201CA2\u201D and \u201CA3\u201D indicate three different annotators."
  Figure 8 Link: articels_figures_by_rev_year\2021\BackgroundClick_Supervision_for_Temporal_Action_Localization\figure_8.jpg
  Figure 8 caption: Trade-off between annotation cost and action localization performance
    on THUMOS14 dataset. We compare BackTAL with recent methods that employ weak supervision,
    weak supervision with extra information, and full supervision. Annotation costs
    for weakly supervised methods [11], [14], [19] and fully supervised methods [4],
    [23], [24] are taken from [11]. The x -axis is the log-scale.
  Figure 9 Link: articels_figures_by_rev_year\2021\BackgroundClick_Supervision_for_Temporal_Action_Localization\figure_9.jpg
  Figure 9 caption: Visualization of the local attention mask. For each example, we
    show the attention mask calculated between a selected action frame (shown in orange)
    or a background frame (shown in red) and its corresponding neighboring frames.
  First author gender probability: 0.79
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.95
  Name of the first author: Le Yang
  Name of the last author: Jianxin Chen
  Number of Figures: 10
  Number of Tables: 13
  Number of authors: 6
  Paper title: Background-Click Supervision for Temporal Action Localization
  Publication Date: 2021-12-02 00:00:00
  Table 1 caption: TABLE 1 Comparison Experiments on THUMOS14 Dataset
  Table 10 caption: TABLE 10 Complexity Comparison Between Our BackTAL and Recent
    Action Localization Methods, in Terms of Model Parameters (M) and Computational
    FLOPs (G)
  Table 2 caption: TABLE 2 Comparison Experiments on ActivityNet v1.2 Dataset
  Table 3 caption: TABLE 3 Comparison Experiments on HACS Dataset, in Comparison With
    a Fully-Supervised SSN [28] and a Weakly-Supervised BaS-Net [14]
  Table 4 caption: TABLE 4 Comparison Experiments on BEOID Dataset, Measured by mAP
    Under Different tIoU Threshold
  Table 5 caption: TABLE 5 Effectiveness of Click Supervision
  Table 6 caption: TABLE 6 Comparison of the Background-Click Annotation With the
    Action-Click Annotation on THUMOS14 Dataset
  Table 7 caption: TABLE 7 Ablation Studies About the Efficacy of the Score Separation
    Module and the Affinity Module on THUMOS14 Dataset
  Table 8 caption: TABLE 8 Ablation Studies About Mining Position Information in Different
    Manners
  Table 9 caption: TABLE 9 Ablation Studies About the Impact of the Neighboring Frame
    Number, Measured by mAP (%) Under IoU Threshold 0.5 on THUMOS14 Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3132058
- Affiliation of the first author: school of electrical and data engineering, faculty
    of engineering and information technology, university of technology sydney, ultimo,
    nsw, australia
  Affiliation of the last author: school of electrical and data engineering, faculty
    of engineering and information technology, university of technology sydney, ultimo,
    nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2021\Bridging_the_Gap_Between_FewShot_and_ManyShot_Learning_via_Distribution_Calibrat\figure_1.jpg
  Figure 1 caption: Training a classifier from few-shot features makes the classifier
    overfit to the few examples (Left). Classifier trained with features sampled from
    calibrated distribution has better generalization ability (Right).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Bridging_the_Gap_Between_FewShot_and_ManyShot_Learning_via_Distribution_Calibrat\figure_2.jpg
  Figure 2 caption: "t-SNE visualization of our distribution estimation. Different\
    \ colors represent different classes. \u2605 represents support set features,\
    \ x in figure (c) represents query set features, \u25B2 in figure (b) represents\
    \ generated features."
  Figure 3 Link: articels_figures_by_rev_year\2021\Bridging_the_Gap_Between_FewShot_and_ManyShot_Learning_via_Distribution_Calibrat\figure_3.jpg
  Figure 3 caption: 'Left: Accuracy when increasing the power in Tukeys transformation
    when training with (red) or without (blue) the generated features. Right: Accuracy
    when increasing the number of generated features with the features are transformed
    by Tukeys transformation (red) and without Tukeys transformation (blue).'
  Figure 4 Link: articels_figures_by_rev_year\2021\Bridging_the_Gap_Between_FewShot_and_ManyShot_Learning_via_Distribution_Calibrat\figure_4.jpg
  Figure 4 caption: The left shows t-SNE[60] visualization of feature distributions
    of 5 randomly selected base classes. The middle and the right show the feature
    distributions of 5 randomly selected novel classes before Tukeys transformation
    and after Tukeys transformation, respectively.
  Figure 5 Link: articels_figures_by_rev_year\2021\Bridging_the_Gap_Between_FewShot_and_ManyShot_Learning_via_Distribution_Calibrat\figure_5.jpg
  Figure 5 caption: "The effect of different values of k and \u03B1 ."
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.76
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Shuo Yang
  Name of the last author: Min Xu
  Number of Figures: 5
  Number of Tables: 6
  Number of authors: 4
  Paper title: Bridging the Gap Between Few-Shot and Many-Shot Learning via Distribution
    Calibration
  Publication Date: 2021-12-03 00:00:00
  Table 1 caption: "TABLE 1 The Class Mean Similarity (\u201Cmean sim\u201D) and Class\
    \ Variance Similarity (\u201Cvar sim\u201D) Between Arctic fox and Different Classes"
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 5way1shot and 5way5shot Classification Accuracy (%) on
    miniImageNet with 95% Confidence Intervals
  Table 3 caption: TABLE 3 5way1shot and 5way5shot Classification Accuracy (%) on
    CUB With 95% Confidence Intervals
  Table 4 caption: TABLE 4 5way1shot and 5way5shot Classification Accuracy (%) on
    Tieredimagenet With 95% Confidence Intervals
  Table 5 caption: TABLE 5 Ablation Study on miniImageNet 5way1shot and 5way5shot
    Showing Accuracy (%) With 95% Confidence Intervals
  Table 6 caption: TABLE 6 5way1shot Classification Accuracy (%) on miniImageNet With
    Different Backbones and Classifiers
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3132021
- Affiliation of the first author: dut-ru international school of information science
    & engineering, dalian university of technology, dalian, liaoning, china
  Affiliation of the last author: key laboratory of machine perception (ministry of
    education), school of artificial intelligence, peking university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Investigating_BiLevel_Optimization_for_Learning_and_Vision_From_a_Unified_Perspe\figure_1.jpg
  Figure 1 caption: Illustrating the problem of BLO. (a) first shows a standard BLO
    problem with the situation of multiple solutions of f . Green curves indicate
    LL objectives denoted by f , whose corresponding minimizers given by S(x) are
    shown as green dots. The red curve represents the UL objective F whose minimizer
    is shown as the red dot. (b) further illustrates that, in general, not all points
    (green dots) in S(x) could minimize the UL objective denoted by F .
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Investigating_BiLevel_Optimization_for_Learning_and_Vision_From_a_Unified_Perspe\figure_2.jpg
  Figure 2 caption: "Schematic diagram of HO. The UL subproblem involves optimization\
    \ of hyper-parameters x based on ( D tr , D val ) , while the LL subproblem involves\
    \ optimization of weight parameters y , aiming to find the learning algorithm\
    \ g y (\u22C5) based on D tr ."
  Figure 3 Link: articels_figures_by_rev_year\2021\Investigating_BiLevel_Optimization_for_Learning_and_Vision_From_a_Unified_Perspe\figure_3.jpg
  Figure 3 caption: "Illustrating the training process of meta learning. The whole\
    \ process is visualized to learn new tasks quickly by drawing upon related tasks\
    \ on corresponding data sets. It can be decomposed into two parts: the \u201C\
    base-learner\u201D trained for operating a given task and the \u201Cmeta-learner\u201D\
    \ trained to learn how to optimize the base-learner."
  Figure 4 Link: articels_figures_by_rev_year\2021\Investigating_BiLevel_Optimization_for_Learning_and_Vision_From_a_Unified_Perspe\figure_4.jpg
  Figure 4 caption: 'Illustration of two architectures that are generally applied
    to multi-task and meta learning: MFL and MIL. Both of them can be separated into
    two parts: meta-parameters denoted by x (blue blocks) and parameters denoted by
    y j (green blocks). (a) shows meta-parameters for features shared across tasks
    and parameters of the logistic regression layer. (b) shows meta (initial) parameters
    shared across tasks and parameters of the task specific layer.'
  Figure 5 Link: articels_figures_by_rev_year\2021\Investigating_BiLevel_Optimization_for_Learning_and_Vision_From_a_Unified_Perspe\figure_5.jpg
  Figure 5 caption: Schematic diagram of NAS. Derived from a predefined search space
    A , NAS first selects an architecture A to transport into the performance estimation
    strategy, then returns the estimated performance of A to the search strategy.
  Figure 6 Link: articels_figures_by_rev_year\2021\Investigating_BiLevel_Optimization_for_Learning_and_Vision_From_a_Unified_Perspe\figure_6.jpg
  Figure 6 caption: "Illustrating the architecture of GAN. The generator G is represented\
    \ as a deterministic feed forward neural network (red blocks), through which a\
    \ fixed random noise v is passed to output G(v) . The discriminator D is another\
    \ neural network (green blocks) which maps the sampled real-world image u\u223C\
    \ p data and G(v) to a binary classification probability."
  Figure 7 Link: articels_figures_by_rev_year\2021\Investigating_BiLevel_Optimization_for_Learning_and_Vision_From_a_Unified_Perspe\figure_7.jpg
  Figure 7 caption: "Illustrating the schematic diagram of AC learning. First the\
    \ actor \u03C0 interacts with the environment to learn the state-action value-function\
    \ Q \u03C0 (s,a) , and then the actor \u03C0 is again obtained based on Q \u03C0\
    \ (s,a) ."
  Figure 8 Link: articels_figures_by_rev_year\2021\Investigating_BiLevel_Optimization_for_Learning_and_Vision_From_a_Unified_Perspe\figure_8.jpg
  Figure 8 caption: 'Summary of the mainstream gradient-based BLOs. We categorize
    these existing approaches into two main groups, i.e., w and wo LLS assumptions.
    When solving BLOs with LLS assumption, these methods can be further divided into
    two categories: EGBR and IGBR. As for EGBRs, they can be solved by different AD
    techniques (as denoted by the dashed rectangle). Very recently, two algorithms
    have also been proposed to address BLOs without the LLS assumption. In particular,
    they actually introduce a bi-level gradient aggregation or a value-function-based
    interior-point method to calculate the indirect gradient.'
  Figure 9 Link: articels_figures_by_rev_year\2021\Investigating_BiLevel_Optimization_for_Learning_and_Vision_From_a_Unified_Perspe\figure_9.jpg
  Figure 9 caption: Illustrating the roadmap of different categories of gradient-based
    BLOs. In the left bottom region, the formulations in the solid rectangles (i.e.,
    singleton and optimistic) have been widely studied. In contrast, since gradient-based
    methods for pessimistic BLOs have not been properly investigated in existing literature,
    we denote this category of formulation by a dashed rectangle. In Section 9, we
    demonstrate that we can also obtain a practical pessimistic BLO scheme within
    our general algorithmic platform.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.64
  Name of the first author: Risheng Liu
  Name of the last author: Zhouchen Lin
  Number of Figures: 9
  Number of Tables: 4
  Number of authors: 5
  Paper title: 'Investigating Bi-Level Optimization for Learning and Vision From a
    Unified Perspective: A Survey and Beyond'
  Publication Date: 2021-12-06 00:00:00
  Table 1 caption: TABLE 1 Summary of Mathematical Notations
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Summary of Related Learning and Vision Applications That
    Can Be (re)Formulated as BLOs
  Table 3 caption: TABLE 3 Summarizing the Convergence Results of Mainstream Gradient-Based
    Methods for BLOs Within Our Framework
  Table 4 caption: TABLE 4 Comparison of the Time and Space Complexity for Several
    Gradient-Based Mainstream BLOs
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3132674
- Affiliation of the first author: Not Available
  Affiliation of the last author: Not Available
  Figure 1 Link: Not Available
  Figure 1 caption: Not Available
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.83
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.83
  Name of the first author: Kyoung Mu Lee
  Name of the last author: Kyoung Mu Lee
  Number of Figures: 0
  Number of Tables: 0
  Number of authors: 1
  Paper title: Editorial
  Publication Date: 2021-12-07 00:00:00
  Table 1 caption: Not Available
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3121153
- Affiliation of the first author: school of astronautics, school of artificial intelligence,
    optics and electronics (iopen), northwestern polytechnical university, xian, shaanxi,
    china
  Affiliation of the last author: school of artificial intelligence, optics and electronics
    (iopen), northwestern polytechnical university, xian, shaanxi, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Ratio_Sum_Versus_Sum_Ratio_for_Linear_Discriminant_Analysis\figure_1.jpg
  Figure 1 caption: Visualization of 3D test dataset and LDA projection plane are
    shown in (a). The visualization of data projected by LDA is shown in (b).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Ratio_Sum_Versus_Sum_Ratio_for_Linear_Discriminant_Analysis\figure_2.jpg
  Figure 2 caption: Visualization of 2D data of two original toy datasets.
  Figure 3 Link: articels_figures_by_rev_year\2021\Ratio_Sum_Versus_Sum_Ratio_for_Linear_Discriminant_Analysis\figure_3.jpg
  Figure 3 caption: Visualization of 3D test dataset, LDA and RSLDA projection plane
    are shown in (a). The visualization of data projected by RSLDA is shown in (b).
  Figure 4 Link: articels_figures_by_rev_year\2021\Ratio_Sum_Versus_Sum_Ratio_for_Linear_Discriminant_Analysis\figure_4.jpg
  Figure 4 caption: "ACC over different kernel parameters \u03B7 and subspace dimensions\
    \ m on Control and JAFFE."
  Figure 5 Link: articels_figures_by_rev_year\2021\Ratio_Sum_Versus_Sum_Ratio_for_Linear_Discriminant_Analysis\figure_5.jpg
  Figure 5 caption: Convergence curves of RSLDA over different parameters on Control
    and JAFFE.
  Figure 6 Link: articels_figures_by_rev_year\2021\Ratio_Sum_Versus_Sum_Ratio_for_Linear_Discriminant_Analysis\figure_6.jpg
  Figure 6 caption: ACC and NMI curves of compared methods over different reduced
    dimensions on two toy datasets.
  Figure 7 Link: articels_figures_by_rev_year\2021\Ratio_Sum_Versus_Sum_Ratio_for_Linear_Discriminant_Analysis\figure_7.jpg
  Figure 7 caption: Visualization of data after reduced to 2D by competitors on toy
    datasets. (a)-(e) represent that the visualization results of TRLDA, RTLDA, MMC,
    RSLDA and KRSLDA on Gaussian. (f)-(j) represent that the visualization results
    of TRLDA, RTLDA, MMC, RSLDA and KRSLDA on Moon.
  Figure 8 Link: articels_figures_by_rev_year\2021\Ratio_Sum_Versus_Sum_Ratio_for_Linear_Discriminant_Analysis\figure_8.jpg
  Figure 8 caption: ACC curves of compared methods over different subspace dimensions
    on eight public datasets. (a)-(h) show the average ACC results of all competitors
    on Ionosphere, Control, MNIST, Caltech101, JAFFE, LUNG, Arence and Umist, respectively.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jingyu Wang
  Name of the last author: Feiping Nie
  Number of Figures: 8
  Number of Tables: 4
  Number of authors: 3
  Paper title: Ratio Sum Versus Sum Ratio for Linear Discriminant Analysis
  Publication Date: 2021-12-07 00:00:00
  Table 1 caption: TABLE 1 An Example to Compare Ratio Sum Criterion and Sum Ratio
    Criterion
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Parameters of Test Dataset
  Table 3 caption: TABLE 3 Relevant Parameters of the Public Datasets Used in the
    Experiments
  Table 4 caption: TABLE 4 The Max Mean and its Standard Deviation (Std) of the Percentage
    of ACC and NMI of Compared Methods on Public Datasets, and m m is the Number of
    Features Corresponding to the Maximum Value
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3133351
- Affiliation of the first author: dauin department of control and computer engineering
    of politecnico di torino, torino, to, italy
  Affiliation of the last author: dauin department of control and computer engineering
    of politecnico di torino, torino, to, italy
  Figure 1 Link: articels_figures_by_rev_year\2021\Modeling_the_Background_for_Incremental_and_WeaklySupervised_Semantic_Segmentati\figure_1.jpg
  Figure 1 caption: The figure depicts the content of the background pixels in case
    of partial labels. In incremental learning (top), since we have labels only for
    pixels of novel classes in the current training step, the background may contain
    pixels of the old ones. In point-supervised learning, every class with at least
    one annotated point in the image is also present in the background. Image taken
    from the Pascal-VOC dataset [2].
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Modeling_the_Background_for_Incremental_and_WeaklySupervised_Semantic_Segmentati\figure_2.jpg
  Figure 2 caption: Overview of our method. At learning step t an image is processed
    by the old (top) and current (bottom) models, mapping the image to their respective
    output spaces. As in standard ICL methods, we apply a cross-entropy loss to learn
    new classes (blue block) and a distillation loss to preserve old knowledge (yellow
    block). In this framework, we model the semantic changes of the background across
    different learning steps by (i) initializing the new classifier using the weights
    of the old background one (left), (ii) comparing the pixel-level background ground
    truth in the cross-entropy with the probability of having either the background
    (black) or an old class (pink and grey bars) and (iii) relating the background
    probability given by the old model in the distillation loss with the probability
    of having either the background or a novel class (green bar). Image taken from
    the Pascal-VOC dataset [2].
  Figure 3 Link: articels_figures_by_rev_year\2021\Modeling_the_Background_for_Incremental_and_WeaklySupervised_Semantic_Segmentati\figure_3.jpg
  Figure 3 caption: 'Qualitative results on the 100-50 setting of the ADE20K dataset
    using different incremental methods. The image demonstrates the superiority of
    our approach on both new (e.g., building, floor, table) and old (e.g., car, wall,
    person) classes. From left to right: image, FT, LwF [15], ILT [27], LwF-MC [13],
    our method, and the ground-truth. Best viewed in color.'
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Fabio Cermelli
  Name of the last author: Barbara Caputo
  Number of Figures: 3
  Number of Tables: 7
  Number of authors: 5
  Paper title: Modeling the Background for Incremental and Weakly-Supervised Semantic
    Segmentation
  Publication Date: 2021-12-09 00:00:00
  Table 1 caption: TABLE 1 Mean IoU (in %) on the Pascal-VOC 2012 Dataset for Different
    Incremental Class Learning Scenarios
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Ablation Study of the Proposed Method on the Pascal-VOC
    2012 overlapped Setup
  Table 3 caption: TABLE 3 Mean IoU (in %) on the ADE20K Dataset for Different Incremental
    Class Learning Scenarios
  Table 4 caption: TABLE 4 Mean IoU (in %) on the Cityscapes Dataset for Different
    Incremental Class Learning Scenarios
  Table 5 caption: TABLE 5 Results on Point-Based Weakly Supervised Object Segmentation
    on Pascal-VOC (mIoU in %)
  Table 6 caption: TABLE 6 Results on Scribble-Based Weakly Supervised Object Segmentation
    on Pascal-VOC (mIoU in %)
  Table 7 caption: TABLE 7 Results on Point-Based Weakly Supervised Scene Parsing
    on ADE20K (mIoU in %)
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3133954
- Affiliation of the first author: department of automation beijing national research
    center for information science and technology (bnrist), tsinghua university, beijing,
    china
  Affiliation of the last author: department of automation beijing national research
    center for information science and technology (bnrist), tsinghua university, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2021\TemporalSpatial_Causal_Interpretations_for_VisionBased_Reinforcement_Learning\figure_1.jpg
  Figure 1 caption: Architecture diagram of our TSCI model and the agent model to
    be interpreted. The agent model consists of a feature extractor, a Gated Recurrent
    Unit (GRU) [60] and two fully-connected layers. TSCI model mainly involves an
    encoder-decoder structure with the encoder shared from the feature extractor.
    The agent model and the encoder are fixed during training.
  Figure 10 Link: articels_figures_by_rev_year\2021\TemporalSpatial_Causal_Interpretations_for_VisionBased_Reinforcement_Learning\figure_10.jpg
  Figure 10 caption: The temporal-spatial causal features of two RL agents that are
    trained using PPO and A2C respectively.
  Figure 2 Link: articels_figures_by_rev_year\2021\TemporalSpatial_Causal_Interpretations_for_VisionBased_Reinforcement_Learning\figure_2.jpg
  Figure 2 caption: Visualization of temporal-spatial causal features discovered by
    our method. The last four frames are explicitly considered for each state as shown
    by the dashed black rectangle (time goes from left to right). The frames on the
    diagonal are identical as shown by the dashed red rectangles.
  Figure 3 Link: articels_figures_by_rev_year\2021\TemporalSpatial_Causal_Interpretations_for_VisionBased_Reinforcement_Learning\figure_3.jpg
  Figure 3 caption: Comparing saliency maps generated by different methods including
    our method, SARFA [18], Gaussian perturbation method [17] and gradient-based method
    [16].
  Figure 4 Link: articels_figures_by_rev_year\2021\TemporalSpatial_Causal_Interpretations_for_VisionBased_Reinforcement_Learning\figure_4.jpg
  Figure 4 caption: Reliability evaluation metrics. Comparing against CXPlain objective
    [26] and imitation learning (IL) objective [49]. All results are averaged across
    seven evaluation episodes.
  Figure 5 Link: articels_figures_by_rev_year\2021\TemporalSpatial_Causal_Interpretations_for_VisionBased_Reinforcement_Learning\figure_5.jpg
  Figure 5 caption: "An example for how to intervene on the input state, which consists\
    \ of the last four consecutive frames \u2032\u2032 o t\u22123 \u22EF o \u2032\u2032\
    \ t ."
  Figure 6 Link: articels_figures_by_rev_year\2021\TemporalSpatial_Causal_Interpretations_for_VisionBased_Reinforcement_Learning\figure_6.jpg
  Figure 6 caption: Performance evaluations of temporal causal features when different
    intervention schemes are applied to the input state. The agent to be evaluated
    remains unchanged except for the input state.
  Figure 7 Link: articels_figures_by_rev_year\2021\TemporalSpatial_Causal_Interpretations_for_VisionBased_Reinforcement_Learning\figure_7.jpg
  Figure 7 caption: Visualization of temporal-spatial causal features discovered by
    the proposed TSCI model when different intervention schemes are applied to intervene
    the input state. Every row is the same state consisting of four consecutive frames
    (time goes from left to right). The black areas are left out during the retraining
    of TSCI model.
  Figure 8 Link: articels_figures_by_rev_year\2021\TemporalSpatial_Causal_Interpretations_for_VisionBased_Reinforcement_Learning\figure_8.jpg
  Figure 8 caption: Performance comparison of five agents that apply different forms
    of input state or structure. While the first agent applies a full-connected (fc)
    feedforward structure, the last four agents use the same recurrent structure and
    their input states are obtained by stacking different number of the last consecutive
    frames together. All results are averaged across five independent runs.
  Figure 9 Link: articels_figures_by_rev_year\2021\TemporalSpatial_Causal_Interpretations_for_VisionBased_Reinforcement_Learning\figure_9.jpg
  Figure 9 caption: "Comparison between the attentions of four recurrent agent models\
    \ that apply frame stacking in different ways. Each row visualizes the attention\
    \ of the agent whose input states s k are obtained by stacking m consecutive frames\
    \ \u2032\u2032 o k\u2212m+1 \u22EF o k\u22121 o \u2032\u2032 k together. Each\
    \ red dashed rectangular represents the input state s k at the k th timestep.\
    \ Here we consider the input states that only include the last four frames \u2032\
    \u2032 o \u2032\u2032 t\u22123:t , and thus not all the last four input states\
    \ \u2032\u2032 s \u2032\u2032 t\u22123:t are visualized due to the limitation\
    \ of space."
  First author gender probability: 0.75
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.75
  Name of the first author: Wenjie Shi
  Name of the last author: Cheng Wu
  Number of Figures: 11
  Number of Tables: 1
  Number of authors: 4
  Paper title: Temporal-Spatial Causal Interpretations for Vision-Based Reinforcement
    Learning
  Publication Date: 2021-12-09 00:00:00
  Table 1 caption: TABLE 1 The Performance Comparison of the Features Discovered by
    Different Methods
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3133717
- Affiliation of the first author: department of computer science and engineering,
    texas a&m university, college station, tx, usa
  Affiliation of the last author: department of computer science and engineering,
    texas a&m university, college station, tx, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\NonLocal_Graph_Neural_Networks\figure_1.jpg
  Figure 1 caption: An illustration of the proposed non-local aggregation framework.
    It consists of three steps, including local embedding, attention-guided sorting,
    and non-local aggregation. Details are described in Section 3.1.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\NonLocal_Graph_Neural_Networks\figure_2.jpg
  Figure 2 caption: (a) Comparisons of the homophily between the original graph and
    the re-connected graph given by our NLGCN on Chameleon and Squirrel. (b) Comparisons
    of the homophily between the original graph and the re-connected graph given by
    our NLMLP on Actor, Cornell, Texas, and Wisconsin. (c) Visualization of sorted
    node sequence after the attention-guided sorting for Cornell and Texas. The colors
    denote node labels. Details are explained in Section 4.5.
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.78
  Name of the first author: Meng Liu
  Name of the last author: Shuiwang Ji
  Number of Figures: 2
  Number of Tables: 6
  Number of authors: 3
  Paper title: Non-Local Graph Neural Networks
  Publication Date: 2021-12-09 00:00:00
  Table 1 caption: TABLE 1 Statistics of the Eleven Datasets Used in Our Experiments
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparisons Between MLP and Common GNNs in Terms of The
    Best Validation Accuracy
  Table 3 caption: TABLE 3 Comparisons Between Our NLMLP and Strong Baselines on the
    Four Disassortative Graph Datasets Belonging to the First Category as Defined
    in Section 4.3
  Table 4 caption: TABLE 4 Comparisons Between Our NLGCN, NLGAT and Strong Baselines
    on the Three Disassortative Graph Datasets Belonging to the Second Category as
    Defined in Section 4.3
  Table 5 caption: TABLE 5 Comparisons Between Our NLMLP, NLGCN, NLGAT and Baselines
    on All the 11 Datasets
  Table 6 caption: TABLE 6 Comparisons in Terms of Real Running Time (milliseconds)
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3134200
