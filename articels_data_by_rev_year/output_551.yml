- Affiliation of the first author: school of computer science, the university of adelaide,
    adelaide, s.a., australia
  Affiliation of the last author: school of computer science, the university of adelaide,
    adelaide, s.a., australia
  Figure 1 Link: articels_figures_by_rev_year\2016\Fast_Rotation_Search_with_Stereographic_Projections_for_D_Registration\figure_1.jpg
  Figure 1 caption: "Under the action of all possible rotations in B , m i may lie\
    \ only on a spherical patch centered at R c m i . However the bounding function\
    \ (9) assumes that m i may lie in the \u03B4 i -ball centred at R c m i ."
  Figure 10 Link: articels_figures_by_rev_year\2016\Fast_Rotation_Search_with_Stereographic_Projections_for_D_Registration\figure_10.jpg
  Figure 10 caption: Initial poses of point clouds and globally optimal results by
    Glob-GM under full and partial overlap scenarios.
  Figure 2 Link: articels_figures_by_rev_year\2016\Fast_Rotation_Search_with_Stereographic_Projections_for_D_Registration\figure_2.jpg
  Figure 2 caption: "Illustrating the idea of matchlists on 1D rotation search\u2014\
    an analogous idea exists for 3D rotation search. Here, the origin is at the centre\
    \ of the largest circle. (Left) Under the actions of all possible rotations in\
    \ an interval B\u2286[0,2\u03C0] , m 1 cannot match with any of the b j 's, whilst\
    \ m 2 can match (up to \u03F5 ) with b 3 and b 4 . (Right) For a subinterval B\
    \ \u2032 of B , we need only test m 2 for potential intersections (i.e., the matchlist\
    \ of B \u2032 is m 2 ), since it is not possible for m 1 to have a match under\
    \ B \u2032 ."
  Figure 3 Link: articels_figures_by_rev_year\2016\Fast_Rotation_Search_with_Stereographic_Projections_for_D_Registration\figure_3.jpg
  Figure 3 caption: "A solid ball l \u03F5 ( b j ) intersects the surface of the sphere\
    \ S 2\u03C0 ( m i ) at a spherical patch, which has a circular outline on the\
    \ sphere. Under stereographic projection, the spherical patch is projected to\
    \ become a circular patch."
  Figure 4 Link: articels_figures_by_rev_year\2016\Fast_Rotation_Search_with_Stereographic_Projections_for_D_Registration\figure_4.jpg
  Figure 4 caption: "The three types of patches arising from projecting spherical\
    \ patches. (a) Interior patch. This is the case shown in Fig. 3. (b) Exterior\
    \ patch. The spherical patch contains the pole, thus the \u201Ccontents\u201D\
    \ of the spherical patch are projected outside the circle. (c) Half-plane. The\
    \ pole lies exactly on the outline of the spherical patch."
  Figure 5 Link: articels_figures_by_rev_year\2016\Fast_Rotation_Search_with_Stereographic_Projections_for_D_Registration\figure_5.jpg
  Figure 5 caption: "Projection of a spherical patch S \u03B1 (x) . The diagrams show\
    \ the side view of Fig. 3, where the horizontal axis represents the xy -plane\
    \ \u03A9 . As explained in Fig. 4, three cases can arise: (a) an interior patch,\
    \ (b) an exterior patch, or (c) a half-plane. In the above diagrams, the bolded\
    \ segments on the horizontal axes indicate the resulting patches."
  Figure 6 Link: articels_figures_by_rev_year\2016\Fast_Rotation_Search_with_Stereographic_Projections_for_D_Registration\figure_6.jpg
  Figure 6 caption: A set of interior patches in the projection plane is indexed in
    a circular R-tree. The MBR at each node is also drawn. The tree structure is shown
    on the right. A query patch mathcal Lq is also shown; in this example, mathcal
    Lq does not intersect with the largest MBR at the root node, hence the search
    need not proceed beyond the root.
  Figure 7 Link: articels_figures_by_rev_year\2016\Fast_Rotation_Search_with_Stereographic_Projections_for_D_Registration\figure_7.jpg
  Figure 7 caption: 'Point clouds used in the evaluation of rotation search: bunny,
    armadillo, dragon, buddha, mine-a, mine-b, mine-c, parasaur, t-rex and chicken.'
  Figure 8 Link: articels_figures_by_rev_year\2016\Fast_Rotation_Search_with_Stereographic_Projections_for_D_Registration\figure_8.jpg
  Figure 8 caption: (a) Median run time versus problem size M . (b) Median run time
    versus problem size M for MCIRC-ML. In this experiment, we ensured that the input
    point clouds mathcal M and mathcal B are of equal size so as compare against [12].
  Figure 9 Link: articels_figures_by_rev_year\2016\Fast_Rotation_Search_with_Stereographic_Projections_for_D_Registration\figure_9.jpg
  Figure 9 caption: Evolution of upper and lower bounds as a function of iteration
    count in our Glob-GM method (Algorithm 3). The time instances where a result is
    updated and then refined by the local method (Loc-GM) are also plotted. The algorithm
    is terminated only when the upper bound equals the lower bound (at approx the
    2,750 th iteration).
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "\xC1lvaro Parra Bustos"
  Name of the last author: David Suter
  Number of Figures: 10
  Number of Tables: 3
  Number of authors: 4
  Paper title: Fast Rotation Search with Stereographic Projections for 3D Registration
  Publication Date: 2016-01-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparing the Performance of BnB Rotation Search Methods Using
      Different Bounds and Bound Evaluation Methods
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparing Performance of 3D Registration Methods on Point
      Clouds with Full Overlap
  Table 3 caption:
    table_text: TABLE 3 Comparing Performance of 3D Registration Methods on Point
      Clouds with Partial Overlap
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2517636
- Affiliation of the first author: electronics and telecommunications research institute
    (etri), 218 gajeong-ro, yuseong-gu, daejeon, republic of korea
  Affiliation of the last author: electronics and telecommunications research institute
    (etri), 218 gajeong-ro, yuseong-gu, daejeon, republic of korea
  Figure 1 Link: articels_figures_by_rev_year\2016\Nonparametric_Feature_Matching_Based_Conditional_Random_Fields_for_Gesture_Recog\figure_1.jpg
  Figure 1 caption: Overview of the proposed method is illustrated. From an input
    test sequence (a), multiple features are extracted as in (b). They are matched
    to training data (c), resulting in a log-likelihood matrix in (d), which is used
    as the unary term of the proposed CRF model. For parameter estimation of our CRF
    model, structured learning is performed based on the SSVM framework. In the log-likelihood
    matrix, the x- and y-axes denote the frame index and the gesture class, respectively.
    Dark pixels represent high likelihood regions. By minimizing the energy function
    defined by the CRF model, we can obtain frame-wise recognition results, that are
    denoted by yellow lines. Final recognized gestures are shown in (e).
  Figure 10 Link: articels_figures_by_rev_year\2016\Nonparametric_Feature_Matching_Based_Conditional_Random_Fields_for_Gesture_Recog\figure_10.jpg
  Figure 10 caption: "Performance of the proposed method based on multiple features\
    \ is illustrated in (a). \u201CP\u201D, \u201CD\u201D, \u201CB\u201D, and \u201C\
    A\u201D denote the joint position, joint distance, both hands, and active hand\
    \ based features, respectively. Jaccard index scores are illustrated for each\
    \ gesture category in (b)."
  Figure 2 Link: articels_figures_by_rev_year\2016\Nonparametric_Feature_Matching_Based_Conditional_Random_Fields_for_Gesture_Recog\figure_2.jpg
  Figure 2 caption: Our feature extraction processes are illustrated. All skeletal
    joints are normalized with respect to the neck joint as shown in (a). They are
    concatenated to produce the skeletal joint position feature . As a viewpoint invariant
    feature, the euclidean distances for all pairs of the skeletal joints are computed
    as in (b) to form the skeletal joint distance feature. To consider the detailed
    appearance of the left and right hands, HOG descriptors are extracted for the
    RGB image as in (c). By concatenating the HOG descriptors within several frames,
    the appearance features of the left and right hands are obtained.
  Figure 3 Link: articels_figures_by_rev_year\2016\Nonparametric_Feature_Matching_Based_Conditional_Random_Fields_for_Gesture_Recog\figure_3.jpg
  Figure 3 caption: "This figure shows the likelihood estimation step in the proposed\
    \ method. The training samples of the m th multi-modal feature belonging to the\
    \ gesture class y (t) are represented by circles in (a). They are matched to the\
    \ input feature x (t) m (denoted by the red triangle) and the nearest neighbor\
    \ vector x (t), y (t) m,NN (denoted by the red circle) is obtained. The euclidean\
    \ distance \u2225 x (t) m \u2212 x (t), y (t) m,NN \u2225 is then computed and\
    \ this process is repeated for all frame indexes t=1,\u2026,T and gesture classes\
    \ y (t) =1,\u2026,C to construct the negative log-likelihood matrix as in (b).\
    \ We can get M negative log-likelihood matrices for all feature types m=1,\u2026\
    ,M and then add all these matrices to produce the unary term in our CRF model\
    \ as shown in the Fig. 1d."
  Figure 4 Link: articels_figures_by_rev_year\2016\Nonparametric_Feature_Matching_Based_Conditional_Random_Fields_for_Gesture_Recog\figure_4.jpg
  Figure 4 caption: The size of the temporal windows and the PCA variances for the
    features x P , x D , and x A are determined by simple greedy optimization with
    the training dataset. The resultant mean Jaccard index scores are plotted in (a)
    and (b).
  Figure 5 Link: articels_figures_by_rev_year\2016\Nonparametric_Feature_Matching_Based_Conditional_Random_Fields_for_Gesture_Recog\figure_5.jpg
  Figure 5 caption: "Performance of the proposed method based on the skeletal joint\
    \ position feature is illustrated with the number of nearest neighbors in the\
    \ kNN approximation to the likelihood function. \u201CmJ\u201D, \u201CmP\u201D\
    , \u201CmR\u201D, \u201CmF 1 \u201D denote the mean Jaccard index, mean precision,\
    \ mean recall, and mean F 1 score, respectively. Note that the scale on the x-axis\
    \ is not linear."
  Figure 6 Link: articels_figures_by_rev_year\2016\Nonparametric_Feature_Matching_Based_Conditional_Random_Fields_for_Gesture_Recog\figure_6.jpg
  Figure 6 caption: Unary costs for one of the test sequences (sample index 701 )
    are illustrated in this figure. Colored texts and gray dotted lines denote the
    ground-truth gesture classes and their startingending frames, respectively.
  Figure 7 Link: articels_figures_by_rev_year\2016\Nonparametric_Feature_Matching_Based_Conditional_Random_Fields_for_Gesture_Recog\figure_7.jpg
  Figure 7 caption: "Performance of the proposed method based on a single feature\
    \ is illustrated in (a). \u201CmJ\u201D, \u201CmP\u201D, \u201CmR\u201D, \u201C\
    mF 1 \u201D denote the mean Jaccard index, mean precision, mean recall, and mean\
    \ F 1 score, respectively. Jaccard index scores are illustrated for each gesture\
    \ category in (b)."
  Figure 8 Link: articels_figures_by_rev_year\2016\Nonparametric_Feature_Matching_Based_Conditional_Random_Fields_for_Gesture_Recog\figure_8.jpg
  Figure 8 caption: The pose of fingers plays an important role in recognizing the
    ok gesture in (a). The gestures based on the motions of both hands are illustrated
    in (b)-(f).
  Figure 9 Link: articels_figures_by_rev_year\2016\Nonparametric_Feature_Matching_Based_Conditional_Random_Fields_for_Gesture_Recog\figure_9.jpg
  Figure 9 caption: The model parameters of our GLCRF-I are determined by the simple
    brute-force search in (a) and (b). It can also be done by using the SSVM algorithm
    as in (c).
  First author gender probability: 0.54
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.54
  Name of the first author: Ju Yong Chang
  Name of the last author: Ju Yong Chang
  Number of Figures: 14
  Number of Tables: 6
  Number of authors: 1
  Paper title: Nonparametric Feature Matching Based Conditional Random Fields for
    Gesture Recognition from Multi-Modal Video
  Publication Date: 2016-01-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance of the Proposed Method Based on a Single Feature
      Is Illustrated
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance of the Proposed Method Based on Multiple Features
      Is Illustrated
  Table 3 caption:
    table_text: TABLE 3 Results of ChaLearn LAP Challenge (Track 3) Are Illustrated
  Table 4 caption:
    table_text: TABLE 4 Results of Cross-Subject Test Are Illustrated
  Table 5 caption:
    table_text: TABLE 5 Evaluation Results for MSRC-12 Dataset Are Illustrated
  Table 6 caption:
    table_text: "TABLE 6 F 1 Scores with Tolerated Latency ( \u03B4=333ms ) Are Illustrated\
      \ for Five Instruction Modalities"
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2519021
- Affiliation of the first author: dais, universit ca' foscari venezia, venice, italy
  Affiliation of the last author: dais, universit ca' foscari venezia, venice, italy
  Figure 1 Link: articels_figures_by_rev_year\2016\An_Accurate_and_Robust_Artificial_Marker_Based_on_Cyclic_Codes\figure_1.jpg
  Figure 1 caption: Some examples of fiducial markers that differ both for the detection
    technique and for the pattern used for recognition. In the first two, detection
    happens by finding ellipses and the coding is respectively held by the color of
    the rings in Concentric Circles (a) and by the appearance of the sectors in Intersense
    (b). While ARToolkit (c) uses image correlation to differentiate markers, ARTag
    (d) relies on error-correcting binary codes. Finally, in (e) and (f) we show two
    instances of RUNE-43 and RUNE-129 respectively.
  Figure 10 Link: articels_figures_by_rev_year\2016\An_Accurate_and_Robust_Artificial_Marker_Based_on_Cyclic_Codes\figure_10.jpg
  Figure 10 caption: Real-world reconstruction scenario. ArUco and RUNETag were placed
    on top of a scanner turntable to provide the initial coarse estimation for rangemap
    alignment. See text for details.
  Figure 2 Link: articels_figures_by_rev_year\2016\An_Accurate_and_Robust_Artificial_Marker_Based_on_Cyclic_Codes\figure_2.jpg
  Figure 2 caption: Our proposed design divided into its functional parts. An instance
    of a three-layers RUNE-129 is displayed.
  Figure 3 Link: articels_figures_by_rev_year\2016\An_Accurate_and_Robust_Artificial_Marker_Based_on_Cyclic_Codes\figure_3.jpg
  Figure 3 caption: 'Steps of the ring detection: in (a) the feasible view directions
    are evaluated for each ellipse (with complexity O(n) ), in (b) for each compatible
    pair of ellipses the feasible rings are estimated (with complexity O( n 2 ) ),
    in (c) the dot votes are counted, the code is recovered and the best candidate
    ring is accepted (figure best viewed in color).'
  Figure 4 Link: articels_figures_by_rev_year\2016\An_Accurate_and_Robust_Artificial_Marker_Based_on_Cyclic_Codes\figure_4.jpg
  Figure 4 caption: Estimated normals orientation in spherical coordinates of three
    coplanar ellipses spanning positive (Left) and negative (Right) focal length values.
    Note how one of the two possible orientations converge to a common direction while
    the other does the opposite.
  Figure 5 Link: articels_figures_by_rev_year\2016\An_Accurate_and_Robust_Artificial_Marker_Based_on_Cyclic_Codes\figure_5.jpg
  Figure 5 caption: 'A synthetic representation of the marker dots normal voting scheme
    used to guess an initial value of the camera focal length. Left: RUNE-129 markers
    rendered by a virtual camera with known focal length and principal point. Center:
    the normal accumulator visualized on the unitary sphere. Right: Focal length distribution
    of the bins. See the text for a complete discussion on the voting procedure.'
  Figure 6 Link: articels_figures_by_rev_year\2016\An_Accurate_and_Robust_Artificial_Marker_Based_on_Cyclic_Codes\figure_6.jpg
  Figure 6 caption: Evaluation of the accuracy in the camera pose estimation with
    respect to different scene conditions. Examples of the detected features are shown
    for RUNE-129 (first image column) and ARToolkitPlus (second image column).
  Figure 7 Link: articels_figures_by_rev_year\2016\An_Accurate_and_Robust_Artificial_Marker_Based_on_Cyclic_Codes\figure_7.jpg
  Figure 7 caption: Accuracy of camera calibration when using a single RUNE-129 as
    a dot-based calibration target. Camera poses has been divided into three groups
    based on the maximum angle between the camera z -axis and the marker plane. A
    random subset of photos is used to test the calibration varying the number of
    target exposures. In all the experiments we achieve a good accuracy with a decreasing
    st.dev. when increasing the number of photos.
  Figure 8 Link: articels_figures_by_rev_year\2016\An_Accurate_and_Robust_Artificial_Marker_Based_on_Cyclic_Codes\figure_8.jpg
  Figure 8 caption: Comparison between a calibration performed with RUNETag and chessboard
    target.
  Figure 9 Link: articels_figures_by_rev_year\2016\An_Accurate_and_Robust_Artificial_Marker_Based_on_Cyclic_Codes\figure_9.jpg
  Figure 9 caption: 'Comparison between the pose accuracy for a single or stereo camera
    setup. Left: distance between two jointly moving markers as a function of the
    angle with respect to the first camera. Right: Angle around the marker plane normal
    as estimated by the first camera versus the stereo setup. Ideally, all the measures
    should lie on the 45 degrees red line.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.82
  Name of the first author: Filippo Bergamasco
  Name of the last author: Andrea Torsello
  Number of Figures: 15
  Number of Tables: 2
  Number of authors: 5
  Paper title: An Accurate and Robust Artificial Marker Based on Cyclic Codes
  Publication Date: 2016-01-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Number of RANSAC Tests Required for an Exhaustive Search of
      All Ellipses That Can Be Fitted Given Each Subset of Five Dot Candidates Found
      in an Image
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Recognition Rate of the Two Proposed Marker Configurations
      with Respect to the Percentage of Area Occluded
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2519024
- Affiliation of the first author: "electrical engineering department, technion \u2013\
    \ israel institute of technology and the media lab, m.i.t. \u2013 massachusetts\
    \ institute of technology"
  Affiliation of the last author: "electrical engineering department, technion \u2013\
    \ israel institute of technology"
  Figure 1 Link: articels_figures_by_rev_year\2016\Surface_Regions_of_Interest_for_Viewpoint_Selection\figure_1.jpg
  Figure 1 caption: The left image shows the regions of interest, where red is the
    most interesting and blue is the least. The other images present the surface from
    the two most descriptive viewpoints, as calculated by our algorithm.
  Figure 10 Link: articels_figures_by_rev_year\2016\Surface_Regions_of_Interest_for_Viewpoint_Selection\figure_10.jpg
  Figure 10 caption: 'Region of interest detection on a mesh versus a point cloud:
    Quantitative evaluation based on [41] compares the results of our technique for
    meshes and for point clouds. The Evaluation was performed on the Benchmark for
    3D Interest Point Detection Algorithms [41] with parameters sigma =0.05, n=8.
    For all three measures we get similar results for meshes and for point clouds.'
  Figure 2 Link: articels_figures_by_rev_year\2016\Surface_Regions_of_Interest_for_Viewpoint_Selection\figure_2.jpg
  Figure 2 caption: 'Detection of regions of interest: algorithm outline.'
  Figure 3 Link: articels_figures_by_rev_year\2016\Surface_Regions_of_Interest_for_Viewpoint_Selection\figure_3.jpg
  Figure 3 caption: 'Spin Image calculation: Two cylindrical coordinates are defined
    for each vertex v with a normal n : the radial coordinate r , which is the perpendicular
    distance to the normal, and the elevation coordinate e , which is the distance
    to the tangent plane.'
  Figure 4 Link: articels_figures_by_rev_year\2016\Surface_Regions_of_Interest_for_Viewpoint_Selection\figure_4.jpg
  Figure 4 caption: Computation of the shape extremities.
  Figure 5 Link: articels_figures_by_rev_year\2016\Surface_Regions_of_Interest_for_Viewpoint_Selection\figure_5.jpg
  Figure 5 caption: Regions of interest of representative objects, as computed by
    our algorithm.
  Figure 6 Link: articels_figures_by_rev_year\2016\Surface_Regions_of_Interest_for_Viewpoint_Selection\figure_6.jpg
  Figure 6 caption: We compare our results (left) with five state-of-the-art methods
    (right). Since their implementations are unavailable, we ran our code on the same
    models given in these papers and show the results side by side. Our approach considers
    global distinctness, which allows us to detect specific facial feature ignoring
    repeating patterns, (e.g., the dragon (a) and gargoyle (e)). Our special algorithm
    for extremity detection allows us to treat all the limbs equally (without preferring
    rear legs to the front ones (d)). Finally, our patch association allows us to
    detect large regions, rather than small, more isolated ones (e.g., the frog and
    the camel (b)).
  Figure 7 Link: articels_figures_by_rev_year\2016\Surface_Regions_of_Interest_for_Viewpoint_Selection\figure_7.jpg
  Figure 7 caption: Robustness. Our method is robust to various mesh manipulations,
    including remeshing, simplification and adding noise.
  Figure 8 Link: articels_figures_by_rev_year\2016\Surface_Regions_of_Interest_for_Viewpoint_Selection\figure_8.jpg
  Figure 8 caption: 'Evaluation on the Benchmark for 3D Interest Point Detection Algorithms
    [41]: Our algorithm''s performance graph is colored in red. The lower the graph,
    the better the result. The left and the middle columns show that the FNE and the
    WME drop faster with our algorithm, compared to the other methods. All the algorithms,
    except for HKS, find more interest points than the human users. For the FPE measure
    (right column), almost all the methods, including ours, produce similar results
    (except for HKS, since it detects very few key points).'
  Figure 9 Link: articels_figures_by_rev_year\2016\Surface_Regions_of_Interest_for_Viewpoint_Selection\figure_9.jpg
  Figure 9 caption: Detection of regions of interest on point clouds.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.74
  Name of the first author: George Leifman
  Name of the last author: Ayellet Tal
  Number of Figures: 19
  Number of Tables: 0
  Number of authors: 3
  Paper title: Surface Regions of Interest for Viewpoint Selection
  Publication Date: 2016-01-27 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2522437
- Affiliation of the first author: key laboratory for ubiquitous network and service
    software of liaoning province, school of software technology, dalian university
    of technology
  Affiliation of the last author: key laboratory for ubiquitous network and service
    software of liaoning province, school of software technology, dalian university
    of technology
  Figure 1 Link: articels_figures_by_rev_year\2016\Learning_to_Diffuse_A_New_Perspective_to_Design_PDEs_for_Visual_Analysis\figure_1.jpg
  Figure 1 caption: The pipelines of LTD for (a) saliency detection and (b) object
    tracking, respectively. The pink regions illustrate the core components of LTD
    framework. The blue regions show how to incorporate different kinds of prior knowledge
    into the diffusion learning process. In (a) the priors are collected by human
    perception for saliency detection, while in (b) we learn priors from training
    data for object tracking. We also shows the ground truth (GT for short) salient
    region and saliency maps computed by some state-of-the-art saliency detection
    methods on the bottom row of subfigure (a).
  Figure 10 Link: articels_figures_by_rev_year\2016\Learning_to_Diffuse_A_New_Perspective_to_Design_PDEs_for_Visual_Analysis\figure_10.jpg
  Figure 10 caption: Image retargeting results of seam carving [58] with CA, RC and
    LTD.
  Figure 2 Link: articels_figures_by_rev_year\2016\Learning_to_Diffuse_A_New_Perspective_to_Design_PDEs_for_Visual_Analysis\figure_2.jpg
  Figure 2 caption: "Illustration of the shift convex hull strategy in (a) and connection\
    \ relationship in (b)-(c). The red and yellow polygons in (a) denote C and C \u2032\
    \ , respectively. The red and yellow regions in (b)-(c) represent F c and B c\
    \ , respectively. Lines in (c) indicate that all nodes in B c are connected."
  Figure 3 Link: articels_figures_by_rev_year\2016\Learning_to_Diffuse_A_New_Perspective_to_Design_PDEs_for_Visual_Analysis\figure_3.jpg
  Figure 3 caption: Saliency diffusion with different guidance maps. (a) input image
    and GT salient region. (b)-(e) center prior u l , color prior u c , background
    diffusion prior u f , final guidance map u (top) and their corresponding saliency
    maps (bottom), respectively.
  Figure 4 Link: articels_figures_by_rev_year\2016\Learning_to_Diffuse_A_New_Perspective_to_Design_PDEs_for_Visual_Analysis\figure_4.jpg
  Figure 4 caption: 'Saliency diffusion with different heat source. (a) input image
    and GT salient region. (b) F c (inside red polygon) and u . (c)-(e) diffusion
    results using one candidate seed in F c : (c) background ( L=10.6175 ), (d) bad
    foreground ( L=1.6818 ) and (e) good foreground ( L=31.7404 ). (f) optimal seeds
    ( L=43.8589 ) and final saliency map. Here we report L values using the original
    saliency maps but normalize them for visual comparison.'
  Figure 5 Link: articels_figures_by_rev_year\2016\Learning_to_Diffuse_A_New_Perspective_to_Design_PDEs_for_Visual_Analysis\figure_5.jpg
  Figure 5 caption: Confidence maps calculated by different strategies. (a) A new
    frame at time t and the surrounding window mathcal Cprime(mathbf xt) . (b) Zoomed-in
    surrounding window and different candidate tracking windows (rectangles with dotted
    and solid lines). (c)-(f) are confidence maps defined by f , uf , ftimes uf and
    f-fb , respectively. The corresponding tracking windows are also plotted on these
    maps.
  Figure 6 Link: articels_figures_by_rev_year\2016\Learning_to_Diffuse_A_New_Perspective_to_Design_PDEs_for_Visual_Analysis\figure_6.jpg
  Figure 6 caption: 'Comparisons between conventional PDEs (bottom left) and LTD on
    image segmentation. The results of LTD with unsupervised and supervised losses
    are presented on the pink and blue regions, respectively. We also use dotted rectangles
    with different colors to distinguish step results of LTD: (a) heat source determined
    by different objective functions (red), (b) stable temperature of learned diffusion
    systems (yellow), and (c) final segmentation results (green). The training images
    with different labels are also illustrated on the rightmost.'
  Figure 7 Link: articels_figures_by_rev_year\2016\Learning_to_Diffuse_A_New_Perspective_to_Design_PDEs_for_Visual_Analysis\figure_7.jpg
  Figure 7 caption: Qualitative comparisons on images from MSRA (top), ECSSD (middle)
    and Berkeley (bottom) databases.
  Figure 8 Link: articels_figures_by_rev_year\2016\Learning_to_Diffuse_A_New_Perspective_to_Design_PDEs_for_Visual_Analysis\figure_8.jpg
  Figure 8 caption: The average precision-recall curves.
  Figure 9 Link: articels_figures_by_rev_year\2016\Learning_to_Diffuse_A_New_Perspective_to_Design_PDEs_for_Visual_Analysis\figure_9.jpg
  Figure 9 caption: The average precisions, recalls and F-measures using adaptive
    thresholding.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Risheng Liu
  Name of the last author: Zhongxuan Luo
  Number of Figures: 14
  Number of Tables: 3
  Number of authors: 6
  Paper title: 'Learning to Diffuse: A New Perspective to Design PDEs for Visual Analysis'
  Publication Date: 2016-01-27 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average Running Time (Seconds per Image) for Different Methods
      on MSRA-1000 Database
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average OR (Top, Higher is Better) and CLE (Bottom, Lower
      is Better) for 6 Example Videos
  Table 3 caption:
    table_text: TABLE 3 Average FPS for Particle Filter Based Trackers on 50 Videos
      in the Benchmark [77]
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2522415
- Affiliation of the first author: "inria grenoble rh\xF4ne-alpes, montbonnot saint-martin,\
    \ france"
  Affiliation of the last author: "inria grenoble rh\xF4ne-alpes, montbonnot saint-martin,\
    \ france"
  Figure 1 Link: articels_figures_by_rev_year\2016\EM_Algorithms_for_WeightedData_Clustering_with_Application_to_AudioVisual_Scene_\figure_1.jpg
  Figure 1 caption: Samples of the SIM dataset with no outliers (top row) and contaminated
    with 50 percent outliers (bottom row). The 600 inliers are generated from Gaussian
    mixtures while the 300 outliers are generated from a uniform distribution.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\EM_Algorithms_for_WeightedData_Clustering_with_Application_to_AudioVisual_Scene_\figure_2.jpg
  Figure 2 caption: Results obtained by fitting mixture models to the SIM-Mixed data
    in the presence of 50 percent outliers (see Table 4).
  Figure 3 Link: articels_figures_by_rev_year\2016\EM_Algorithms_for_WeightedData_Clustering_with_Application_to_AudioVisual_Scene_\figure_3.jpg
  Figure 3 caption: 'Audio-visual data acquisition and alignment. Top: left- and right-microphone
    signals. A temporal segment of 0.4 s is outlined in red. Middle: Binaural spectrogram
    that corresponds to the outlined segment. This spectrogram is composed of 50 binaural
    vectors, each one being associated with an audio frame (shown as a vertical rectangle).
    Bottom: video frames associated with a segment. A sound-source direction of arrival
    is extracted from each binaural vector and mapped onto the image plane, hence
    each green dot in the image plane corresponds to a DOA.'
  Figure 4 Link: articels_figures_by_rev_year\2016\EM_Algorithms_for_WeightedData_Clustering_with_Application_to_AudioVisual_Scene_\figure_4.jpg
  Figure 4 caption: Results obtained on the fake speaker, moving speaker and cocktail
    party sequences. The first column shows the audio (green) and visual (blue) observations,
    as well as a yellow bounding box that shows the ground-truth active speaker. The
    second, third and fourth columns show the mixture components obtained with the
    WD-EM, GMM+U and FM-uMST methods, respectively. The blue disks mark components
    that correspond to correct detections of active speakers, namely whenever there
    is an overlap between a component and the ground-truth bounding box.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.95
  Name of the first author: Israel Dejene Gebru
  Name of the last author: Radu Horaud
  Number of Figures: 4
  Number of Tables: 5
  Number of authors: 4
  Paper title: EM Algorithms for Weighted-Data Clustering with Application to Audio-Visual
    Scene Analysis
  Publication Date: 2016-01-27 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Datasets Used for Benchmarking and Their Characteristics:
      n Is the Number of Data Points, d Is the Dimension of the Data Space, and K
      Is Number of Clusters'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results Obtained with the MNIST, WAV, BCW, and Letter Recognition
      Datasets
  Table 3 caption:
    table_text: TABLE 3 Micro F 1 Scores Obtained on the Real Data Sets (MNIST, WAV,
      BCW and Letter Recognition)
  Table 4 caption:
    table_text: TABLE 4 DB Scores Obtained on the SIM-X Dataset (best and second best)
  Table 5 caption:
    table_text: 'TABLE 5 The Correct Detection Rates (CDR) Obtained with the Three
      Methods for Three Scenarios: Fake Speaker (FS), Moving Speakers (MS), and Cocktail
      Party (CP)'
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2522425
- Affiliation of the first author: tnt group at the leibniz-university of hannover,
    germany
  Affiliation of the last author: tnt group at the leibniz-university of hannover,
    germany
  Figure 1 Link: articels_figures_by_rev_year\2016\Human_Pose_Estimation_from_Video_and_IMUs\figure_1.jpg
  Figure 1 caption: Tracking result for two selected frames. (a) Video-based tracker.
    (b) Our proposed hybrid tracker.
  Figure 10 Link: articels_figures_by_rev_year\2016\Human_Pose_Estimation_from_Video_and_IMUs\figure_10.jpg
  Figure 10 caption: Mean XOR error (red) and mean angular error (blue) for different
    weighting parameter lambda of the hybrid tracker. The dashed lines show the respective
    error levels of the video tracker.
  Figure 2 Link: articels_figures_by_rev_year\2016\Human_Pose_Estimation_from_Video_and_IMUs\figure_2.jpg
  Figure 2 caption: General tracking procedure of the video tracker, using silhouettes
    as image features. Multi-view silhouettes are obtained by background subtraction.
    In parallel, the mesh model is adapted to the current pose and projected to the
    respective camera views. One seeks for the pose parameters that best explain the
    image evidence.
  Figure 3 Link: articels_figures_by_rev_year\2016\Human_Pose_Estimation_from_Video_and_IMUs\figure_3.jpg
  Figure 3 caption: 'Global frames: tracking frame F T and inertial frame F I . Local
    frame: sensor frame F S .'
  Figure 4 Link: articels_figures_by_rev_year\2016\Human_Pose_Estimation_from_Video_and_IMUs\figure_4.jpg
  Figure 4 caption: Sketch of the hybrid tracker pipeline. Silhouette and orientation
    features are obtained from the inputs and their consistencies are combined in
    a hybrid energy term. We search for the model pose, which results in the minimal
    hybrid energy.
  Figure 5 Link: articels_figures_by_rev_year\2016\Human_Pose_Estimation_from_Video_and_IMUs\figure_5.jpg
  Figure 5 caption: 'Integration of orientation data into the video-based tracker.
    Ground-truth orientation: clockwise down path from F S at time t to F T . Tracking
    orientation: anti-clockwise upper path from F S at time t to F T .'
  Figure 6 Link: articels_figures_by_rev_year\2016\Human_Pose_Estimation_from_Video_and_IMUs\figure_6.jpg
  Figure 6 caption: Alternative interpretation of integrating orientation data into
    the video-based tracker. In comparison to Fig. 5 the tracking orientation path
    is extended by a local infinitesimal rotation mathbf RB(Delta mathbf x) of the
    body frame FB .
  Figure 7 Link: articels_figures_by_rev_year\2016\Human_Pose_Estimation_from_Video_and_IMUs\figure_7.jpg
  Figure 7 caption: 'Sensor placement: 10 sensors are strapped to body extremities
    (shank, thigh, forearm, upper arm), chest and waist.'
  Figure 8 Link: articels_figures_by_rev_year\2016\Human_Pose_Estimation_from_Video_and_IMUs\figure_8.jpg
  Figure 8 caption: Frame-wise XOR and orientation error for a walking sequence. The
    hybrid tracker (blue) performs well for the entire sequence. The video tracker
    (red) shows some large orientation errors between frames 160 and 370. Interestingly,
    this is almost invisible in the XOR error curve.
  Figure 9 Link: articels_figures_by_rev_year\2016\Human_Pose_Estimation_from_Video_and_IMUs\figure_9.jpg
  Figure 9 caption: Frame-wise XOR and orientation error for a dynamic punching sequence.
    The video tracker (red) struggles to track the complex motion and cannot recover
    from frame 210 on. The hybrid tracker (blue) performs better with respect to both
    error metrics.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.91
  Name of the first author: Timo von Marcard
  Name of the last author: Bodo Rosenhahn
  Number of Figures: 14
  Number of Tables: 8
  Number of authors: 3
  Paper title: Human Pose Estimation from Video and IMUs
  Publication Date: 2016-01-27 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Mean Tracking Error Values \u03BC and Standard Deviations\
      \ \u03C3 for Video-Based and Hybrid Tracker for All Sequences of the Database"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Mean Angular Error \u03BC ang [deg] of the Validation Sensors\
      \ Attached to Thighs, Chest and Upper Arms for the Video-Based and Hybrid Tracker\
      \ for All Sequences of the Database"
  Table 3 caption:
    table_text: TABLE 3 Mean Tracking Error for Varying Weighting of Sensor Cues,
      Computed over All Sequences of the Database
  Table 4 caption:
    table_text: TABLE 4 Tracking Camera List for Three Experiments with a Designated
      Scenario
  Table 5 caption:
    table_text: TABLE 5 Tracking Error Statistics for Camera Setups Shown in Table
      4
  Table 6 caption:
    table_text: TABLE 6 Average Tracking Error for Varying Number of Camera Views
  Table 7 caption:
    table_text: TABLE 7 Tracking Error Statistics for Different Levels of Sensor Lag
  Table 8 caption:
    table_text: TABLE 8 Mean and Standard Deviations of the Joint Position Error for
      Validation Subsets of the HumanEva Dataset
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2522398
- Affiliation of the first author: armstrong institute for patient safety and quality,
    johns hopkins university, baltimore, md
  Affiliation of the last author: armstrong institute for patient safety and quality,
    johns hopkins university, baltimore, md
  Figure 1 Link: articels_figures_by_rev_year\2016\A_Model_Selection_Approach_for_Clustering_a_Multinomial_Sequence_with_NonNegativ\figure_1.jpg
  Figure 1 caption: Comparison of three approaches through ARI for the model selection
    performance. In all cases, our procedure either outperforms or nearly on par with
    the two baseline algorithms.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\A_Model_Selection_Approach_for_Clustering_a_Multinomial_Sequence_with_NonNegativ\figure_2.jpg
  Figure 2 caption: "Comparison of two clustering algorithms, specifically, the two-sample\
    \ test procedure (MT) in [16] and our clustering approach (NL) which is (aic o\
    \ nmf) When the vertex correspondence is fully known, our approach outperforms,\
    \ but when no vertex correspondence is known, the non-parametric two sample procedure\
    \ outperforms. When only fraction of the true correspondence is known, the unknown\
    \ correspondence is extrapolated from the known correspondence from the connectivity\
    \ pattern, using the \u201Cseeded\u201D graph matching algorithm from [17]."
  Figure 3 Link: articels_figures_by_rev_year\2016\A_Model_Selection_Approach_for_Clustering_a_Multinomial_Sequence_with_NonNegativ\figure_3.jpg
  Figure 3 caption: "Connectivity matrices for C. elegan's chemical and electrical\
    \ networks between neurons. Visually, while there are similarities between the\
    \ graphs representing two networks, it can be seen that there are also dissimilarities.\
    \ Our numerical experiment yields that K \u02C6 =2 , further corroborating that\
    \ two networks are sufficiently different."
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Nam H. Lee
  Name of the last author: Michael Rosen
  Number of Figures: 3
  Number of Tables: 4
  Number of authors: 4
  Paper title: A Model Selection Approach for Clustering a Multinomial Sequence with
    Non-Negative Factorization
  Publication Date: 2016-01-27 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Comparison of \u0394(K) and \u0394 AIC (K) in Terms of the\
      \ Values of \u0394(K) and \u0394 AIC (K) , for a Single-Instance of a 50\xD7\
      2 Data Matrix Generated Using a Two-Cluster Parameter, i.e., K \u2217 =2"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Comparison of \u0394 and \u0394 AIC in Terms of the Number\
      \ of Times that K \u02C6 =2 Out of 100 Monte Carlo Repetitions"
  Table 3 caption:
    table_text: TABLE 3 Averaged Value of ARI from a Monte Carlo Experiment Comparing
      Two Graphs
  Table 4 caption:
    table_text: "TABLE 4 Values of \u0394(K) for Estimating the Inner Dimension for\
      \ the Swimmer Dataset"
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2522443
- Affiliation of the first author: "it: instituto de telecomunica\xE7\xF5es, department\
    \ of computer science, university of beira interior, covilh\xE3, portugal"
  Affiliation of the last author: "it: instituto de telecomunica\xE7\xF5es, department\
    \ of computer science, university of beira interior, covilh\xE3, portugal"
  Figure 1 Link: articels_figures_by_rev_year\2016\Joint_Head_PoseSoft_Label_Estimation_for_Human_Recognition_InTheWild\figure_1.jpg
  Figure 1 caption: Examples of images acquired by a visual surveillance system, composed
    by a wide-view camera feeding a pan-tilt-zoom device that collects data from moving
    and at-a-distance targets (up to 40 meters away).
  Figure 10 Link: articels_figures_by_rev_year\2016\Joint_Head_PoseSoft_Label_Estimation_for_Human_Recognition_InTheWild\figure_10.jpg
  Figure 10 caption: 'Left: Probability density functions of the stability of labels
    per subject ( S t c (i) ). Right: Variations in the overall stability S t c with
    respect to the number of shape centroids considered.'
  Figure 2 Link: articels_figures_by_rev_year\2016\Joint_Head_PoseSoft_Label_Estimation_for_Human_Recognition_InTheWild\figure_2.jpg
  Figure 2 caption: Overview of the stochastic process that generates an arbitrary
    number of 3D head shapes (meshes). Based on anthropometric surveys (marker 1),
    a set of probability density functions for head lengths is defined (marker 2),
    and used to iteratively deform a base mesh, enabling to obtain head shapes of
    evidently different appearance (marker 3).
  Figure 3 Link: articels_figures_by_rev_year\2016\Joint_Head_PoseSoft_Label_Estimation_for_Human_Recognition_InTheWild\figure_3.jpg
  Figure 3 caption: "Representation of the 3D head centroids resulting of a 4\xD7\
    4 SOM. Note the similarity in sizeshape between adjacent elements, rooted in the\
    \ preservation of the topological properties of the input space that this kind\
    \ of maps offers."
  Figure 4 Link: articels_figures_by_rev_year\2016\Joint_Head_PoseSoft_Label_Estimation_for_Human_Recognition_InTheWild\figure_4.jpg
  Figure 4 caption: Examples of the 99 percent confidence ellipsoids that represent
    the deviations of the positions of landmarks in the head shape samples with respect
    to their centroid. These values are used in the convergence test of the algorithm
    to discriminate between genuinespurious head landmarks.
  Figure 5 Link: articels_figures_by_rev_year\2016\Joint_Head_PoseSoft_Label_Estimation_for_Human_Recognition_InTheWild\figure_5.jpg
  Figure 5 caption: Data structure that indexes the joint poseshape hypotheses, grouped
    according to the similarity of their landmark projections. In retrieval, the indices
    of the hypotheses complying the query landmarks are accumulated, such that the
    most voted hypotheses will be evaluated first.
  Figure 6 Link: articels_figures_by_rev_year\2016\Joint_Head_PoseSoft_Label_Estimation_for_Human_Recognition_InTheWild\figure_6.jpg
  Figure 6 caption: Pose refinement, according to a convex optimisation paradigm.
    Assuming that the initial hypothesis p is a good approximation of the solution,
    the probability of falling in local minima is relatively short. p is the optimized
    configuration.
  Figure 7 Link: articels_figures_by_rev_year\2016\Joint_Head_PoseSoft_Label_Estimation_for_Human_Recognition_InTheWild\figure_7.jpg
  Figure 7 caption: "Finding the 3D positions in the Euclidean space from where the\
    \ query landmarks might have been projected, according to a pose p and shape s\
    \ estimates. The || q . \u2212 x . | | 2 values are used to discriminate between\
    \ the spurious (in red) and genuine (in green) query landmarks."
  Figure 8 Link: articels_figures_by_rev_year\2016\Joint_Head_PoseSoft_Label_Estimation_for_Human_Recognition_InTheWild\figure_8.jpg
  Figure 8 caption: Examples of the data sets used in the empirical validation of
    the proposed method. The upper row regards the AFLW data set, whereas the bottom
    rows are from the LFW and SCface sets.
  Figure 9 Link: articels_figures_by_rev_year\2016\Joint_Head_PoseSoft_Label_Estimation_for_Human_Recognition_InTheWild\figure_9.jpg
  Figure 9 caption: 'Upper row: examples of pose estimates in images from the AFLW
    data set. Second row: boxplot of the pose estimation errors for the six degrees-of-freedom:
    yaw, pitch, roll rotation angles (in radians), plus the t x , t y , t z translation
    values. Bottom row: performance comparison with respect to a state-of-the-art
    pose estimator [48] in a subset of the AFLW set.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "Hugo Proen\xE7a"
  Name of the last author: Juan C. Moreno
  Number of Figures: 21
  Number of Tables: 1
  Number of authors: 5
  Paper title: Joint Head Pose/Soft Label Estimation for Human Recognition In-The-Wild
  Publication Date: 2016-01-27 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Types of Anthropometric Measurements Considered in This Paper
      and Their Levels of Linear Correlation
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2522441
- Affiliation of the first author: center for machine vision and signal analysis (cmvs),
    university of oulu, finland
  Affiliation of the last author: cmvs, university of oulu
  Figure 1 Link: articels_figures_by_rev_year\2016\Comments_on_the_Kinship_Face_in_the_Wild_Data_Sets\figure_1.jpg
  Figure 1 caption: Examples of image pairs from the KinFaceW-I data set showing similar
    capturing conditions. The image pairs are either cropped from the same image or
    taken in a very close time interval.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Comments_on_the_Kinship_Face_in_the_Wild_Data_Sets\figure_2.jpg
  Figure 2 caption: Examples of image pairs from the KinFaceW-II data set. Kinship
    pairs are cropped from the same image and show very similar characteristics.
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: "Miguel Bordallo L\xF3pez"
  Name of the last author: Abdenour Hadid
  Number of Figures: 2
  Number of Tables: 4
  Number of authors: 3
  Paper title: "Comments on the \u201CKinship Face in the Wild\u201D Data Sets"
  Publication Date: 2016-01-27 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Classification Accuracy (Percent) of Different Methods on
      the Different Subsets of KinFaceW-I Data Set
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Classification Accuracy (Percent) of Different Methods on
      the Different Subsets of KinFaceW-II Data Set
  Table 3 caption:
    table_text: TABLE 3 Mean Classification Accuracy (Percent) of the Simple Scoring
      Method on Different Databases
  Table 4 caption:
    table_text: TABLE 4 Kinship Verification Data Sets Available to the Kinship Research
      Community as of November 2015
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2522416
