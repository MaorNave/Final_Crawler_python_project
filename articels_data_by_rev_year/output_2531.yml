- Affiliation of the first author: facebook reality labs, burlingame, ca, usa
  Affiliation of the last author: northeastern university, boston, ma, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\ImageText_Embedding_Learning_via_Visual_and_Textual_Semantic_Reasoning\figure_1.jpg
  Figure 1 caption: The proposed Visual Semantic Reasoning Network (VSRN++) performs
    reasoning on the image regions as well as words in the text caption to generate
    representations. Through the alignment learning, the learned visual representation
    captures key objects (boxes in the caption) and semantic concepts (highlight parts
    in the caption) of a scene as in the corresponding text caption.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\ImageText_Embedding_Learning_via_Visual_and_Textual_Semantic_Reasoning\figure_2.jpg
  Figure 2 caption: An overview of the proposed Visual Semantic Reasoning Network.
    For the visual encoding part, starting from features of salient image regions
    obtained by the bottom-up attention model (Section 3.1), it first performs region
    relationship reasoning on these regions to generate features enhanced by both
    semantic and spatial location relationships (Section 3.2). Then the model applies
    gate and memory mechanisms to perform global semantic reasoning on the relationship
    enhanced features, select the discriminative information and gradually generate
    the representation for the whole image scene (Section 3.4). For the text encoding
    part, we design similar reasoning steps on the text caption with GCN and GRUs.
    This aims to improve the ability of capturing semantic context in the caption
    when generating the text embedding (Section 3.5). Finally, the whole model is
    trained with joint optimization of Sinkhorn-based image-text matching and sentence
    generation (Section 3.6). The attention of the visual representation (top right)
    is obtained by calculating correlations between the final image representation
    and each region feature (Section 4.5).
  Figure 3 Link: articels_figures_by_rev_year\2022\ImageText_Embedding_Learning_via_Visual_and_Textual_Semantic_Reasoning\figure_3.jpg
  Figure 3 caption: Qualitative results of the image-to-text retrieval for VSRN on
    MS-COCO test set. For each image query, we show the top-3 ranked text caption.
    Ground-truth matched sentences are with check marks, while some sentences sharing
    similar meanings as ground-truth ones are marked with gray underline. We also
    show the attention visualization of the final image representation besides its
    corresponding image. Our model generates interpretable image representation that
    captures key objects and semantic concepts in the scene. (Best viewed in color
    when zoomed in.)
  Figure 4 Link: articels_figures_by_rev_year\2022\ImageText_Embedding_Learning_via_Visual_and_Textual_Semantic_Reasoning\figure_4.jpg
  Figure 4 caption: Qualitative comparisons between VSRN and VSRN++ for image-to-text
    retrieval and text-to-image retrieval on MS-COCO test set. For each image query,
    we show the top-3 ranked text caption, where ground-truth matched sentences are
    with check marks. For each text query, we show the top-3 retrieved images, ranking
    from left to right. The true matches are outlined in green boxes and attention
    visualizations are shown.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Kunpeng Li
  Name of the last author: Yun Fu
  Number of Figures: 4
  Number of Tables: 6
  Number of authors: 5
  Paper title: Image-Text Embedding Learning via Visual and Textual Semantic Reasoning
  Publication Date: 2022-02-07 00:00:00
  Table 1 caption: TABLE 1 Quantitative Results of the Image-to-Text (Caption) Retrieval
    and Text-to-Image (Image) Retrieval on Flickr30K Test set in Terms of RecallK
    (RK)
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Quantitative Evaluation Results of the Image-to-Text (Caption)
    Retrieval and Text-to-Image (Image) Retrieval on MS-COCO 5-Fold 1K Test set in
    Terms of RecallK (RK)
  Table 3 caption: TABLE 3 Quantitative Evaluation Results of the Image-to-Text (Caption)
    Retrieval and Text-to-Image (Image) Retrieval on MS-COCO 5K Test set in Terms
    of RecallK (RK)
  Table 4 caption: TABLE 4 Comparisons of the Inference Time With Recent State-of-the-art
    Methods Whose Code is Publicly Available
  Table 5 caption: TABLE 5 Ablation Studies on the MS-COCO 5-Fold 1K Test set and
    Flickr 30K Test set
  Table 6 caption: TABLE 6 Studies to Analyze the Effects of Region Ordering
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3148470
- Affiliation of the first author: "department of engineering \u201Cenzo ferrari\u201D\
    , university of modena and reggio emilia, modena, italy"
  Affiliation of the last author: "department of engineering \u201Cenzo ferrari\u201D\
    , university of modena and reggio emilia, modena, italy"
  Figure 1 Link: articels_figures_by_rev_year\2022\From_Show_to_Tell_A_Survey_on_Deep_LearningBased_Image_Captioning\figure_1.jpg
  Figure 1 caption: Overview of the image captioning task and taxonomy of the most
    relevant approaches.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\From_Show_to_Tell_A_Survey_on_Deep_LearningBased_Image_Captioning\figure_2.jpg
  Figure 2 caption: 'Three of the most relevant visual encoding strategies for image
    captioning: (a) global CNN features; (b) fine-grained features extracted from
    the activation of a convolutional layer, together with an attention mechanism
    guided by the language model; (c) image region features coming from a detector,
    together with an attention mechanism.'
  Figure 3 Link: articels_figures_by_rev_year\2022\From_Show_to_Tell_A_Survey_on_Deep_LearningBased_Image_Captioning\figure_3.jpg
  Figure 3 caption: 'Summary of the two most recent visual encoding strategies for
    image captioning: (a) graph-based encoding of visual regions; (b) self-attention-based
    encoding over image region features.'
  Figure 4 Link: articels_figures_by_rev_year\2022\From_Show_to_Tell_A_Survey_on_Deep_LearningBased_Image_Captioning\figure_4.jpg
  Figure 4 caption: Vision Transformer encoding. The image is split into fixed-size
    patches, linearly embedded, added to position embeddings, and fed to a standard
    Transformer encoder.
  Figure 5 Link: articels_figures_by_rev_year\2022\From_Show_to_Tell_A_Survey_on_Deep_LearningBased_Image_Captioning\figure_5.jpg
  Figure 5 caption: 'LSTM-based language modeling strategies: (a) Single-Layer LSTM
    model conditioned on the visual feature; (b) LSTM with attention, as proposed
    in [42]; (c) LSTM with attention, in the variant proposed in [43]; (d) two-layer
    LSTM with attention, in the style of the bottom-up top-down approach [58]. In
    all figures, boldsymbolX represents a set of visual features, boldsymbolht is
    the LSTM hidden state at time t , and boldsymbolst is the visual sentinel.'
  Figure 6 Link: articels_figures_by_rev_year\2022\From_Show_to_Tell_A_Survey_on_Deep_LearningBased_Image_Captioning\figure_6.jpg
  Figure 6 caption: Schema of the Transformer-based language model. The caption generation
    is performed via masked self-attention over previously generated tokens and cross-attention
    with encoded visual features.
  Figure 7 Link: articels_figures_by_rev_year\2022\From_Show_to_Tell_A_Survey_on_Deep_LearningBased_Image_Captioning\figure_7.jpg
  Figure 7 caption: Schema of a BERT-like language model. A single stream of attentive
    layers processes both image regions and word tokens and generates the output caption.
  Figure 8 Link: articels_figures_by_rev_year\2022\From_Show_to_Tell_A_Survey_on_Deep_LearningBased_Image_Captioning\figure_8.jpg
  Figure 8 caption: 'Qualitative examples from some of the most common image captioning
    datasets: (a) image-caption pairs; (b) word clouds of the captions most common
    visual words.'
  Figure 9 Link: articels_figures_by_rev_year\2022\From_Show_to_Tell_A_Survey_on_Deep_LearningBased_Image_Captioning\figure_9.jpg
  Figure 9 caption: Relationship between CIDEr, number of parameters and other scores.
    Values of Div-1 and CLIP-S are multiplied by powers of 10 for readability.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Matteo Stefanini
  Name of the last author: Rita Cucchiara
  Number of Figures: 9
  Number of Tables: 2
  Number of authors: 6
  Paper title: 'From Show to Tell: A Survey on Deep Learning-Based Image Captioning'
  Publication Date: 2022-02-07 00:00:00
  Table 1 caption: TABLE 1 Overview of the Main Image Captioning Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Performance Analysis of Representative Image Captioning
    Approaches in Terms of Different Evaluation Metrics
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3148210
- Affiliation of the first author: department of electronic and computer engineering,
    the hong kong university of science and technology, clear water bay, hong kong
  Affiliation of the last author: department of electronic and computer engineering,
    the hong kong university of science and technology, clear water bay, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2022\Towards_High_Performance_Low_Complexity_Calibration_in_Appearance_Based_Gaze_Est\figure_1.jpg
  Figure 1 caption: "Images of two left eyes and their difference from ColumbiaGaze\
    \ dataset [22]. (a) Image with 10 \u2218 horizontal and 0 \u2218 vertical gaze\
    \ angle. (b) Image with 15 \u2218 horizontal and 0 \u2218 vertical gaze angle.\
    \ (c) The absolute difference between (a) and (b) (value is scaled for better\
    \ illustration)."
  Figure 10 Link: articels_figures_by_rev_year\2022\Towards_High_Performance_Low_Complexity_Calibration_in_Appearance_Based_Gaze_Est\figure_10.jpg
  Figure 10 caption: Cumulative distribution function of the sensitivity coefficients
    computed over the last convolutional layer.
  Figure 2 Link: articels_figures_by_rev_year\2022\Towards_High_Performance_Low_Complexity_Calibration_in_Appearance_Based_Gaze_Est\figure_2.jpg
  Figure 2 caption: Scatter plots of the estimated gaze angles versus the ground truth
    of subject p06 from MPIIGaze.
  Figure 3 Link: articels_figures_by_rev_year\2022\Towards_High_Performance_Low_Complexity_Calibration_in_Appearance_Based_Gaze_Est\figure_3.jpg
  Figure 3 caption: Overview. (a) Appearance-based gaze estimation estimates the gaze
    angles (pitch and yaw) from RGB images. However, there are some factors not observable
    from the images, such as (b) the subject-dependent offset between the optic axis
    and visual axis.
  Figure 4 Link: articels_figures_by_rev_year\2022\Towards_High_Performance_Low_Complexity_Calibration_in_Appearance_Based_Gaze_Est\figure_4.jpg
  Figure 4 caption: Error analysis of our subject-independent estimator [26] on the
    MPIIGaze dataset [16] for (a) yaw and (b) pitch. Circles and crosses indicate
    the means. Error bars indicate standard deviations.
  Figure 5 Link: articels_figures_by_rev_year\2022\Towards_High_Performance_Low_Complexity_Calibration_in_Appearance_Based_Gaze_Est\figure_5.jpg
  Figure 5 caption: Architecture of the proposed network. (a) The main network that
    outputs hatt(Xi,j) based on the input images Xi,j . (b) The dilated CNN that is
    the basic component of (a). (c) The regular CNN without dilated-convolutions used
    as a baseline. FC denotes fully-connected layers, Conv denotes convolutional layers,
    and Dilated-Conv denotes dilated-convolutional layers with r as the dilation rate.
  Figure 6 Link: articels_figures_by_rev_year\2022\Towards_High_Performance_Low_Complexity_Calibration_in_Appearance_Based_Gaze_Est\figure_6.jpg
  Figure 6 caption: The setting of dataset collection. Each subject (a) appears at
    nine different locations on the images and (b) is instructed to gaze at 72 crosses
    and the camera at each location.
  Figure 7 Link: articels_figures_by_rev_year\2022\Towards_High_Performance_Low_Complexity_Calibration_in_Appearance_Based_Gaze_Est\figure_7.jpg
  Figure 7 caption: Statistics of the NISLGaze (ours), MPIIGaze and EYEDIAP datasets
    in terms of face location ( boldsymboll ), head pose ( boldsymbolh ) and gaze
    directions ( boldsymbolg ).
  Figure 8 Link: articels_figures_by_rev_year\2022\Towards_High_Performance_Low_Complexity_Calibration_in_Appearance_Based_Gaze_Est\figure_8.jpg
  Figure 8 caption: Within-dataset evaluation. The mean angular error on MPIIGaze
    for SGTC ( S=9 ). The number in each box shows the mean angular error when the
    calibration gaze target is located inside the corresponding 5times 5 region. Mean
    errors are computed by averaging over the entire test set (except for the calibration
    set) and over 100 calibration gaze targets located at a 10times 10 grid spaced
    by 0.5circ in yaw and pitch inside each region. overlineE=4.5circ, underlineE=2.6circ
    .
  Figure 9 Link: articels_figures_by_rev_year\2022\Towards_High_Performance_Low_Complexity_Calibration_in_Appearance_Based_Gaze_Est\figure_9.jpg
  Figure 9 caption: Cross-dataset evaluation. Mean angular error of SGTC ( S=5 ) when
    calibrated at different gaze targets. Trained on MPIIGaze and NISLGaze. Tested
    on the ColumbiaGaze. overlineE=4.8circ, underlineE=3.6circ .
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Zhaokang Chen
  Name of the last author: Bertram E. Shi
  Number of Figures: 10
  Number of Tables: 7
  Number of authors: 2
  Paper title: Towards High Performance Low Complexity Calibration in Appearance Based
    Gaze Estimation
  Publication Date: 2022-02-07 00:00:00
  Table 1 caption: TABLE 1 Mean Angular Errors for Gaze Estimation Without Calibration
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 Estimation Error (mean \xB1 \xB1 SD in degrees) of Multiple\
    \ Gaze Target Calibration"
  Table 3 caption: "TABLE 3 Estimation Error (mean \xB1 \xB1 SD in degrees) of SGTC\
    \ on MPIIGaze"
  Table 4 caption: "TABLE 4 Estimation Error (mean \xB1 \xB1SD) for SGTC With Varying\
    \ Calibration Data Amounts"
  Table 5 caption: "TABLE 5 Estimation Error (mean \xB1 \xB1SD) for SGTC at Different\
    \ Head Locations"
  Table 6 caption: TABLE 6 Comparison of Calibration Methods on NISLGaze ( T=1,S=16S=5
    T=1,S=16S=5)
  Table 7 caption: TABLE 7 Mean Error Under Different Landmark Disturbances
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3148386
- Affiliation of the first author: department of engineering science, university of
    oxford, oxford, u.k.
  Affiliation of the last author: institute of automation, chinese academy of sciences,
    beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Deep_Learning_for_FreeHand_Sketch_A_Survey\figure_1.jpg
  Figure 1 caption: Diverse free-hand sketches in human daily life. The masks (rough
    and simplified) on the top right corner are from [16]. The scene-level sketch
    (cloud, trees, and giraffes) on the bottom right corner is from SketchyCOCO dataset
    [17].
  Figure 10 Link: articels_figures_by_rev_year\2022\Deep_Learning_for_FreeHand_Sketch_A_Survey\figure_10.jpg
  Figure 10 caption: Sketches (bus, car, cat) and ground truth annotations selected
    from sketch parsing paper [171]. The semantic parts and background are annotated
    by colors. Best viewed in color.
  Figure 2 Link: articels_figures_by_rev_year\2022\Deep_Learning_for_FreeHand_Sketch_A_Survey\figure_2.jpg
  Figure 2 caption: Drawing samples out-of-scope of our focus on free-hand sketch.
  Figure 3 Link: articels_figures_by_rev_year\2022\Deep_Learning_for_FreeHand_Sketch_A_Survey\figure_3.jpg
  Figure 3 caption: 'Sketch-specific representations. Representations from left to
    right: sparse matrix (black background with white lines), dense picture (white
    background with black lines), graph, stroke sequence. Both graph and stroke sequence
    representations are based on the key stroke points. In stroke sequence, each key
    point is denoted as a four-tuple, where the first two entries and the last two
    entries represent the coordinates and pen state, respectively. See details in
    text.'
  Figure 4 Link: articels_figures_by_rev_year\2022\Deep_Learning_for_FreeHand_Sketch_A_Survey\figure_4.jpg
  Figure 4 caption: Illustrations of the major domain-unique challenges of free-hand
    sketch. Each column is a photo-sketch pair. Sketch is highly abstract. A pyramid
    can be depicted as a triangle in sketch, and a few strokes depict a fancy handbag.
    Sketch is highly diverse. Different people draw distinctive sketches when given
    the identical reference, due to subjective salience (head versus body), and drawing
    style.
  Figure 5 Link: articels_figures_by_rev_year\2022\Deep_Learning_for_FreeHand_Sketch_A_Survey\figure_5.jpg
  Figure 5 caption: 'Novel applications that sketch supports. (a) A sketch is drawn
    wih the device on the back of the hand [52]; (b) A walking cycle sequence. Left:
    input hand-drawn sketch, middle: inflated 3D model with control points, right:
    walking cycle animation created by recording trajectories of individual control
    points specified by the user [53].'
  Figure 6 Link: articels_figures_by_rev_year\2022\Deep_Learning_for_FreeHand_Sketch_A_Survey\figure_6.jpg
  Figure 6 caption: Milestones of deep learning based free-hand sketch research, from
    the perspectives of task, dataset, supervision, and representation. Note that
    self-supervised learning is a branch of unsupervised learning.
  Figure 7 Link: articels_figures_by_rev_year\2022\Deep_Learning_for_FreeHand_Sketch_A_Survey\figure_7.jpg
  Figure 7 caption: A tree diagram of the sketch task taxonomy. Generative tasks are
    framed by dashed lines. Sketch domain-unique tasks are framed by green lines.
    Best viewed in color.
  Figure 8 Link: articels_figures_by_rev_year\2022\Deep_Learning_for_FreeHand_Sketch_A_Survey\figure_8.jpg
  Figure 8 caption: Image entropy histogram of 9K sketch stars [19]. The blue bars
    denote the bin counts within different entropy ranges. Some representative sketches
    corresponding to different entropy values are illustrated.
  Figure 9 Link: articels_figures_by_rev_year\2022\Deep_Learning_for_FreeHand_Sketch_A_Survey\figure_9.jpg
  Figure 9 caption: The pipeline of SketchRNN [5]. The dashed arrow line denotes the
    recurrent processing of LSTM decoder. For simplicity, the recurrent processing
    of bi-LSTM encoder is not shown here.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Peng Xu
  Name of the last author: Liang Wang
  Number of Figures: 18
  Number of Tables: 9
  Number of authors: 6
  Paper title: 'Deep Learning for Free-Hand Sketch: A Survey'
  Publication Date: 2022-02-07 00:00:00
  Table 1 caption: TABLE 1 Notation and Abbreviations Used in This Survey
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Summary of the Representative Sketch Datasets
  Table 3 caption: TABLE 3 Comparison of Representative Sketch Recognition Networks
  Table 4 caption: TABLE 4 Comparison of the Representative Sketch Generation Deep
    Models
  Table 5 caption: TABLE 5 Comparison of the Representative Pipelines of Deep Sketch-Photo
    Generation
  Table 6 caption: TABLE 6 Comparison of Recognition Accuracy for Different Network
    Architectures on a Subset [23] of QuickDraw [5]
  Table 7 caption: TABLE 7 Robustness of ResNet-18 CNN Backbone to Perturbations in
    the Form of Random Spatial Transformations
  Table 8 caption: TABLE 8 Comparison of Deep Learning and Traditional Approaches
    to Sketch-Related Tasks
  Table 9 caption: TABLE 9 Comparison of Different Deep Network Architectures for
    Sketch-Oriented Tasks
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3148853
- Affiliation of the first author: school of electronics and information technology,
    sun yat-sen university, guangzhou, china
  Affiliation of the last author: school of computer science and engineering, sun
    yat-sen university, guangzhou, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Egocentric_Action_Recognition_by_Automatic_Relation_Modeling\figure_1.jpg
  Figure 1 caption: Comparison between egocentric videos and videos recorded from
    a third-person point of view. In both videos, a person walks into a dining room
    holding a sandwich and places it on a table. The invisibility of the camera wearer
    and the blur due to his or her movements seen in the egocentric video are two
    of the typical features of this type of videos. These frames were taken from the
    CharadesEgo dataset [1].
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Egocentric_Action_Recognition_by_Automatic_Relation_Modeling\figure_2.jpg
  Figure 2 caption: Relations in egocentric videos. In (a), the camera wearer is receiving
    a camera from the interacting person. In (b), the camera wearer is taking the
    carrot. In (c), the camera wearer is cutting the carrot. The relations between
    the camera wearer and the interacting persons or objects are important for recognizing
    the actions. These frames were taken from the UTokyo PEV dataset [45], the EPIC-Kitchens-55
    dataset [46] and the EGTEA Gaze+ dataset [36].
  Figure 3 Link: articels_figures_by_rev_year\2022\Egocentric_Action_Recognition_by_Automatic_Relation_Modeling\figure_3.jpg
  Figure 3 caption: "Main idea. Given an egocentric video, we learn to localize the\
    \ interactors, i.e., the body parts of the camera wearer and the persons, objects\
    \ or environments involved in the action, for interactor feature extraction. To\
    \ model the relations between the camera wearer and the other interactors\u2014\
    an important relation in egocentric videos\u2014we learn to categorize the interactor\
    \ features in each frame into two groups, i.e., the egocentric and exocentric\
    \ group, to facilitate interactive relation modeling. Based on these interactor\
    \ features, we model the temporal relations (along the temporal dimension), the\
    \ interactive relations (between the two groups) and the contextual relations\
    \ (between the interactor features and the global features) with an ego-relational\
    \ LSTM network constructed by searching for candidate connections. These frames\
    \ were taken from the EPIC-Kitchens-55 dataset [46]."
  Figure 4 Link: articels_figures_by_rev_year\2022\Egocentric_Action_Recognition_by_Automatic_Relation_Modeling\figure_4.jpg
  Figure 4 caption: 'Overall framework: Sampled frames and their corresponding (pre-computed)
    optical flows from an egocentric video are used as the input to our model. The
    basic feature maps of each sampled frame are obtained through an RGB backbone,
    and the keypoints of each sampled frame are localized using an hourglass network.
    The interactor features are extracted based on the basic feature map and the keypoints
    (Section 3.1). The interactor features are then categorized into two groups, i.e.,
    an egocentric group and an exocentric group, to facilitate interactive relation
    modeling, while it is unnecessary to identify which group corresponds to the camera
    wearer or the other interactors (Section 3.2). To model different relations in
    egocentric videos, an ego-relational LSTM network is designed to automatically
    search for connections and model the different types of relations for action recognition;
    in this network, the dashed arrows indicate candidate connections (Section 3.3).
    The optical flow features obtained from a flow backbone network are aggregated
    through late fusion.'
  Figure 5 Link: articels_figures_by_rev_year\2022\Egocentric_Action_Recognition_by_Automatic_Relation_Modeling\figure_5.jpg
  Figure 5 caption: "Ego-relational LSTM. The ego-relational LSTM structure consists\
    \ of two symmetrical LSTM branches, i.e., an egocentric branch (red dashed box)\
    \ and an exocentric branch (green dashed box). The connections denoted by red,\
    \ green, and orange dashed arrows are candidate connections for explicit relation\
    \ modeling, and these connections would be searched to construct the final ego-relational\
    \ LSTM structure. The blue arrows are used by the shared state gate to control\
    \ the shared feature accumulation during each time step. It should be noted that\
    \ only the connections of the \u201Cego state\u201D at the t th time step are\
    \ shown here for brevity; however, the connections of the other states share the\
    \ same pattern."
  Figure 6 Link: articels_figures_by_rev_year\2022\Egocentric_Action_Recognition_by_Automatic_Relation_Modeling\figure_6.jpg
  Figure 6 caption: Two ego-relational LSTM networks with 16 time steps learned on
    the EPIC-Kitchens-55 datasets. The red and green rectangles and circles indicate
    the egocentric and exocentric branches. Note that it is unnecessary to identify
    which branch corresponds to the camera wearer or the other interactors, as mentioned
    in Section 3.2. The orange rounded rectangles represent the contextual features.
    The red, green and orange arrows denote the selected temporal, interactive and
    contextual connections, respectively. And the black arrows are the inherent connections
    in the LSTM network. The shared features and states are neglected here since they
    are not searched.
  Figure 7 Link: articels_figures_by_rev_year\2022\Egocentric_Action_Recognition_by_Automatic_Relation_Modeling\figure_7.jpg
  Figure 7 caption: Examples of video frames from the EPIC-Kitchens-55 dataset with
    detected keypoints, feature separation and the mean value of shared state gates.
    In the frames, the red and green areas are egocentric and exocentric groups of
    Gaussian-like soft masks generated from the keypoints, and the figures on the
    masks are the values of hatalpha n (the function 1!!1 is replaced with sigmoid
    function to obtain the values). Below the frames, the changes in shared state
    gate values (averaged across channels) are plotted, which indicates the proportion
    of selected shared features at each time step.
  Figure 8 Link: articels_figures_by_rev_year\2022\Egocentric_Action_Recognition_by_Automatic_Relation_Modeling\figure_8.jpg
  Figure 8 caption: Example frames and keypoints (red) with different separation loss
    weights gamma sep . The frames are taken from the EPIC-Kitchens-55 dataset.
  Figure 9 Link: articels_figures_by_rev_year\2022\Egocentric_Action_Recognition_by_Automatic_Relation_Modeling\figure_9.jpg
  Figure 9 caption: Example frames and keypoints (red) with different similarity loss
    weights gamma sim . The frames are taken from the EPIC-Kitchens-55 dataset.
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.55
  Name of the first author: Haoxin Li
  Name of the last author: Jian-Huang Lai
  Number of Figures: 9
  Number of Tables: 14
  Number of authors: 6
  Paper title: Egocentric Action Recognition by Automatic Relation Modeling
  Publication Date: 2022-02-07 00:00:00
  Table 1 caption: TABLE 1 Top-1 Action Recognition Accuracy (%) Comparison of Each
    Component of Our Model
  Table 10 caption: TABLE 10 Top-1 Classification Accuracy (%) wo and With Feature
    Separation
  Table 2 caption: TABLE 2 Top-1 Classification Accuracy (%) Comparison of Different
    Structures
  Table 3 caption: TABLE 3 Top-1 Classification Accuracy (%) Comparison of Different
    Numbers of Connections k k
  Table 4 caption: TABLE 4 Top-1 Classification Accuracy (%) Comparison of Different
    Numbers of LSTM Layers
  Table 5 caption: TABLE 5 Top-1 Classification Accuracy (%) Comparison of the Different
    Types of Connections
  Table 6 caption: TABLE 6 Top-1 Classification Accuracy (%) Comparison of Relational
    Features and Basic Features
  Table 7 caption: TABLE 7 Top-1 Classification Accuracy (%) Comparison Using Different
    Number of Keypoints
  Table 8 caption: "TABLE 8 Top-1 Classification Accuracy (%) Comparison With Different\
    \ Standard Deviations \u03C3 \u03C3"
  Table 9 caption: TABLE 9 Effects of Different Constraints on Top-1 Classification
    Accuracy (%)
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3148790
- Affiliation of the first author: carnegie mellon university, pittsburgh, pa, usa
  Affiliation of the last author: university of adelaide, adelaide, sa, australia
  Figure 1 Link: articels_figures_by_rev_year\2022\Reframing_Neural_Networks_Deep_Structure_in_Overcomplete_Representations\figure_1.jpg
  Figure 1 caption: (a) Deep frame approximation is a unifying framework for multilayer
    representation learning where inference is posed as the constrained optimization
    of a multi-layer reconstruction objective. (b) The problem structure allows for
    effective feed-forward approximation with the activations of a standard deep neural
    network. (c) More accurate approximations can be found using an iterative optimization
    algorithm with updates implemented as recurrent feedback connections.
  Figure 10 Link: articels_figures_by_rev_year\2022\Reframing_Neural_Networks_Deep_Structure_in_Overcomplete_Representations\figure_10.jpg
  Figure 10 caption: The effect of increasing depth in chain and residual networks.
    Validation error is compared against network depth for two different network widths.
    (a) Unlike chain networks, even very deep residual networks can be trained effectively
    for decreased validation error. (b) Despite having the same number of total parameters,
    residual connections also induce frame structures with lower minimum deep frame
    potentials.
  Figure 2 Link: articels_figures_by_rev_year\2022\Reframing_Neural_Networks_Deep_Structure_in_Overcomplete_Representations\figure_2.jpg
  Figure 2 caption: In comparison to (a) standard chain connections, skip connections
    like those in (b) ResNets and (c) DenseNets have demonstrated significant improvements
    in parameter efficiency and generalization performance. We provide one possible
    explanation for this phenomenon by approximating network activations as (d) solutions
    to deep frame approximation problems with different induced frame structures.
  Figure 3 Link: articels_figures_by_rev_year\2022\Reframing_Neural_Networks_Deep_Structure_in_Overcomplete_Representations\figure_3.jpg
  Figure 3 caption: Parameter count is not a good indicator of generalization performance
    for deep networks. Instead, we compare different network architectures via the
    minimum deep frame potential, a lower bound on the mutual coherence of their corresponding
    structured frames. In comparison to (a) chain networks, the skip connections in
    (b) ResNets and (c) DenseNets induce Gram matrix structures with more nonzero
    elements allowing for (d) lower deep frame potentials across network sizes. (e)
    This correlates with improved parameter efficiency giving lower validation error
    with fewer parameters.
  Figure 4 Link: articels_figures_by_rev_year\2022\Reframing_Neural_Networks_Deep_Structure_in_Overcomplete_Representations\figure_4.jpg
  Figure 4 caption: "A comparison between (a) projection onto a simplex, which corresponds\
    \ to nonnegative and \u2113 1 norm constraints, and (b) the rectified linear unit\
    \ (ReLU) nonlinear activation function, which is equivalent to a nonnegative softthresholding\
    \ proximal operator."
  Figure 5 Link: articels_figures_by_rev_year\2022\Reframing_Neural_Networks_Deep_Structure_in_Overcomplete_Representations\figure_5.jpg
  Figure 5 caption: "An example of the \u201Cexplaining away\u201D conditional dependence\
    \ provided by optimization-based inference. Sparse representations constructed\
    \ by feed-forward nonnegative soft thresholding (a) have many more non-zero elements\
    \ due to redundancy and spurious activations (c). On the other hand, sparse representations\
    \ found by ell 1 -penalized, nonnegative least-squares optimization (b) yield\
    \ a more parsimonious set of components (d) that optimally reconstruct approximations\
    \ of the data. This figure was adapted from [8]."
  Figure 6 Link: articels_figures_by_rev_year\2022\Reframing_Neural_Networks_Deep_Structure_in_Overcomplete_Representations\figure_6.jpg
  Figure 6 caption: Mutual coherence, visualized in red, of different frames in mathbb
    R2 . (a) When mutual coherence is high, some vectors may be very similar leading
    to representation instability and sensitivity to noise. (b) Mutual coherence is
    minimized at the Welch bound for equiangular tight frames where all vectors are
    equally distributed. (c) Mutual coherence is zero in the case of orthogonality,
    which can not occur for overcomplete frames.
  Figure 7 Link: articels_figures_by_rev_year\2022\Reframing_Neural_Networks_Deep_Structure_in_Overcomplete_Representations\figure_7.jpg
  Figure 7 caption: A visualization of a one-dimensional convolutional frame with
    two input channels, five output channels, and a filter size of three. (a) The
    filters are repeated over eight spatial dimensions resulting in (b) a block-Toeplitz
    structure that is revealed through row and column permutations. (c) The corresponding
    gram matrix can be efficiently computed by (d) repeating local filter interactions.
  Figure 8 Link: articels_figures_by_rev_year\2022\Reframing_Neural_Networks_Deep_Structure_in_Overcomplete_Representations\figure_8.jpg
  Figure 8 caption: Deep frame approximation inference compared between feed-forward
    deep networks and recurrent optimization for (a) chain networks and (b) ResNets
    with a base width of 16 filters and 2 residual blocks per group, and (c) DenseNets
    with a growth rate of 4 and 4 layers per group. Unrolled to an increasing number
    of iterations, recurrent optimization networks quickly converge to validation
    accuracies comparable to their feed-forward approximations.
  Figure 9 Link: articels_figures_by_rev_year\2022\Reframing_Neural_Networks_Deep_Structure_in_Overcomplete_Representations\figure_9.jpg
  Figure 9 caption: A comparison of fully-connected deep network architectures with
    varying depths and widths. Warmer colors indicate models with more total parameters.
    (a) Some very large networks cannot be trained effectively resulting in unusually
    high validation errors. (b) This can be remedied with deep frame potential regularization,
    resulting in high correlation between minimum frame potential and validation error.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Calvin Murdock
  Name of the last author: Simon Lucey
  Number of Figures: 14
  Number of Tables: 0
  Number of authors: 3
  Paper title: 'Reframing Neural Networks: Deep Structure in Overcomplete Representations'
  Publication Date: 2022-02-08 00:00:00
  Table 1 caption: Not Available
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3149445
- Affiliation of the first author: cvl, eth zurich, zurich, switzerland
  Affiliation of the last author: vail, oxford brookes university, oxford, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2022\ROAD_The_Road_Event_Awareness_Dataset_for_Autonomous_Driving\figure_1.jpg
  Figure 1 caption: 'Use of labels in ROAD to describe typical road scenarios. (a)
    A green car is in front of the AV while changing lanes, as depicted by the arrow
    symbol. The associated event will then carry the following labels: in vehicle
    lane (location), moving left (action). Once the event is completed, the location
    label will change to: in outgoing lane. (b) Autonomous vehicle turning left from
    lane 6 into lane 4: lane 4 will be the outgoing lane as the traffic is moving
    in the same direction as the AV. However, if the AV turns right from lane 6 into
    lane 4 (a wrong turn), then lane 4 will become the incoming lane as the vehicle
    will be moving into the incoming traffic. The overall philosophy of ROAD is to
    use suitable combinations of multiple label types to fully describe a road situation,
    and allow a machine learning algorithm to learn from this information.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\ROAD_The_Road_Event_Awareness_Dataset_for_Autonomous_Driving\figure_2.jpg
  Figure 2 caption: Number of instances of each class of individual label-types, in
    logarithmic scale.
  Figure 3 Link: articels_figures_by_rev_year\2022\ROAD_The_Road_Event_Awareness_Dataset_for_Autonomous_Driving\figure_3.jpg
  Figure 3 caption: Proposed 3D-RetinaNet architecture for online video processing.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.95
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Gurkirt Singh
  Name of the last author: Fabio Cuzzolin
  Number of Figures: 3
  Number of Tables: 10
  Number of authors: 16
  Paper title: 'ROAD: The Road Event Awareness Dataset for Autonomous Driving'
  Publication Date: 2022-02-14 00:00:00
  Table 1 caption: TABLE 1 Comparison of ROAD With Similar Datasets for Perception
    in Autonomous Driving in Terms of Diversity of Labels
  Table 10 caption: TABLE 10 Results (in video-mAP) of the Winning Entries to the
    ICCV 2021 ROAD Challenge Compared With the Slowfast and YOLOv5 Baselines, at a
    Detection Threshold of 0.2
  Table 2 caption: TABLE 2 ROAD Tasks and Attributes
  Table 3 caption: TABLE 3 Comparison of the Action Detection Performance (Frame-mAP0.5
    (f-mAP) and Video-mAP at Different IoU Thresholds) of the Proposed 3D-RetinaNet
    Baseline Model With the State-of-the-art on the UCF-101-24 Dataset
  Table 4 caption: TABLE 4 Splits of Training, Validation and Test Sets for the ROAD
    Dataset With Respect to Weather Conditions
  Table 5 caption: TABLE 5 Frame-Level Results (mAP % %) Averaged Across the Three
    Splits of ROAD
  Table 6 caption: TABLE 6 Video-Level Results (mAP % %) Averaged Across the Three
    ROAD Splits
  Table 7 caption: TABLE 7 Number of Video- and Frame-Level Instances for Each Label
    (Individual or Composite), Left
  Table 8 caption: TABLE 8 Comparison of Joint versus Product of Marginals Approaches
    With I3D Backbone
  Table 9 caption: TABLE 9 AV-Action Temporal Segmentation Results (Frame mAP % %)
    Averaged Across All Three Splits
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3150906
- Affiliation of the first author: key laboratory of intelligent interaction and applications
    (ministry of industry and information technology), school of artificial intelligence,
    optics and electronics (iopen), northwestern polytechnical university, xian, shaanxi,
    china
  Affiliation of the last author: key laboratory of intelligent interaction and applications
    (ministry of industry and information technology), school of artificial intelligence,
    optics and electronics (iopen), northwestern polytechnical university, xian, shaanxi,
    china
  Figure 1 Link: articels_figures_by_rev_year\2022\Centerless_Clustering\figure_1.jpg
  Figure 1 caption: The motivation of the proposed algorithms ( k -sums and k -sums-x).
    (1) The unified view of k -means and spectral clustering is firstly revisited.
    (2) Then, our model is proposed by making some modifications to this unified view.
    (3) Two versions of the proposed method have been developed, called k -sums and
    k -sums-x. The former ( k -sums) takes a k -NN graph as input and the latter takes
    the features as input.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Centerless_Clustering\figure_2.jpg
  Figure 2 caption: "Schematic diagram of calculating the product of d ~ T i and Y\
    \ . Red squares represent the squared euclidean distances between samples, and\
    \ blue circles denote the elements that are equal to \u03B3 ."
  Figure 3 Link: articels_figures_by_rev_year\2022\Centerless_Clustering\figure_3.jpg
  Figure 3 caption: Distribution of D 1 .
  Figure 4 Link: articels_figures_by_rev_year\2022\Centerless_Clustering\figure_4.jpg
  Figure 4 caption: Performance on Outlier.
  Figure 5 Link: articels_figures_by_rev_year\2022\Centerless_Clustering\figure_5.jpg
  Figure 5 caption: Schematic diagram of face representation.
  Figure 6 Link: articels_figures_by_rev_year\2022\Centerless_Clustering\figure_6.jpg
  Figure 6 caption: Performance with different k .
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Shenfei Pei
  Name of the last author: Xuelong Li
  Number of Figures: 6
  Number of Tables: 6
  Number of authors: 5
  Paper title: Centerless Clustering
  Publication Date: 2022-02-14 00:00:00
  Table 1 caption: TABLE 1 The Time Complexity of Algorithms
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The Performance on Grid-Like Datasets
  Table 3 caption: TABLE 3 Benchmark Datasets
  Table 4 caption: "TABLE 4 Performance of Algorithms Taking Graph as Input (Mean\
    \ ( \xB1 \xB1 Std))"
  Table 5 caption: "TABLE 5 Performance of Algorithms Taking Features as Input (Mean\
    \ ( \xB1 \xB1 Std))"
  Table 6 caption: TABLE 6 Performance on Large-Scale Datasets
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3150981
- Affiliation of the first author: robotics institute, university of michigan, ann
    arbor, mi, usa
  Affiliation of the last author: luddy school of informatics, computing and engineering,
    indiana university, bloomington, in, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\DoTA_Unsupervised_Detection_of_Traffic_Anomaly_in_Driving_Videos\figure_1.jpg
  Figure 1 caption: Overview of our method based on future object localization (FOL)
    using a sample video from our DoTA dataset. Annotated bounding boxes (filled)
    and predicted boxes are presented. For each time step, we collect FOL predictions
    of all traffic participants from different past time steps and compute the bounding
    box standard deviation, called consistency, as the anomaly score.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\DoTA_Unsupervised_Detection_of_Traffic_Anomaly_in_Driving_Videos\figure_2.jpg
  Figure 2 caption: Overview of the future object localization model. Blocks with
    dashed outlines are encoders, and solid lines are decoders. The decoder recurrences
    are unfolded to visualize the prediction horizon.
  Figure 3 Link: articels_figures_by_rev_year\2022\DoTA_Unsupervised_Detection_of_Traffic_Anomaly_in_Driving_Videos\figure_3.jpg
  Figure 3 caption: 'Overview of our unsupervised VAD methods. The three brackets
    correspond to: (1) Predicted bounding box accuracy method (green); (2) Predicted
    box mask accuracy method (orange); (3) Predicted bounding box consistency method
    (blue). All methods use multiple previous FOL outputs to compute anomaly scores.'
  Figure 4 Link: articels_figures_by_rev_year\2022\DoTA_Unsupervised_Detection_of_Traffic_Anomaly_in_Driving_Videos\figure_4.jpg
  Figure 4 caption: DoTA Samples. Spatial annotations are shown as shadowed bounding
    boxes. Short anomaly category labels with indicate non-ego anomalies.
  Figure 5 Link: articels_figures_by_rev_year\2022\DoTA_Unsupervised_Detection_of_Traffic_Anomaly_in_Driving_Videos\figure_5.jpg
  Figure 5 caption: DoTA dataset statistics.
  Figure 6 Link: articels_figures_by_rev_year\2022\DoTA_Unsupervised_Detection_of_Traffic_Anomaly_in_Driving_Videos\figure_6.jpg
  Figure 6 caption: Anomaly score maps computed by four methods. Ground truth anomalous
    regions are labeled by bounding boxes. Brighter color indicates higher score.
  Figure 7 Link: articels_figures_by_rev_year\2022\DoTA_Unsupervised_Detection_of_Traffic_Anomaly_in_Driving_Videos\figure_7.jpg
  Figure 7 caption: (a) STAUC values of different methods using different top N% ;
    (b) ROC curve and STROC curves of the Ensemble method with different top N% .
  Figure 8 Link: articels_figures_by_rev_year\2022\DoTA_Unsupervised_Detection_of_Traffic_Anomaly_in_Driving_Videos\figure_8.jpg
  Figure 8 caption: Network architectures of methods for video anomaly detection.
  Figure 9 Link: articels_figures_by_rev_year\2022\DoTA_Unsupervised_Detection_of_Traffic_Anomaly_in_Driving_Videos\figure_9.jpg
  Figure 9 caption: Per-frame anomaly scores and TARRs of three methods for two different
    videos. The left and right columns show sample video frames and corresponding
    score maps, with an arrow indicating their temporal position within the video.
    Note that TARR only exists in positive frames.
  First author gender probability: 0.96
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yu Yao
  Name of the last author: David J. Crandall
  Number of Figures: 9
  Number of Tables: 6
  Number of authors: 7
  Paper title: 'DoTA: Unsupervised Detection of Traffic Anomaly in Driving Videos'
  Publication Date: 2022-02-14 00:00:00
  Table 1 caption: TABLE 1 Comparison of Published Driving Video Anomaly Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Traffic Anomaly Categories in the DoTA Dataset
  Table 3 caption: TABLE 3 Comparison of Video Anomaly Techniques on the DoTA Dataset,
    According to the AUC and STAUC Metrics
  Table 4 caption: TABLE 4 Video Anomaly Detection Results for Each Type of Anomaly
    Class
  Table 5 caption: TABLE 5 Video Action Recognition Per-Class and Mean Top-1 Accuracies
    on DoTA
  Table 6 caption: TABLE 6 Online Video Action Detection Average Precisions on DoTA
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3150763
- Affiliation of the first author: school of interactive computing, georgia institute
    of technology, atlanta, ga, usa
  Affiliation of the last author: intel labs, santa clara, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\MSeg_A_Composite_Dataset_for_MultiDomain_Semantic_Segmentation\figure_1.jpg
  Figure 1 caption: MSeg unifies multiple semantic segmentation datasets by reconciling
    their taxonomies and resolving incompatible annotations. This enables training
    models that perform consistently across domains and generalize better. Input images
    in this figure were taken from datasets that were not seen during training.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\MSeg_A_Composite_Dataset_for_MultiDomain_Semantic_Segmentation\figure_2.jpg
  Figure 2 caption: Procedure for determining the set of categories in the MSeg taxonomy.
  Figure 3 Link: articels_figures_by_rev_year\2022\MSeg_A_Composite_Dataset_for_MultiDomain_Semantic_Segmentation\figure_3.jpg
  Figure 3 caption: 'Semantic classes in MSeg. Left: pixel counts of MSeg classes,
    in log scale. Right: percentage of pixels from each component dataset that contribute
    to each class. Any single dataset is insufficient for describing the visual world.'
  Figure 4 Link: articels_figures_by_rev_year\2022\MSeg_A_Composite_Dataset_for_MultiDomain_Semantic_Segmentation\figure_4.jpg
  Figure 4 caption: Visualization of a subset of the class mapping from each dataset
    to our unified taxonomy. This figure shows 40 of the 194 classes; see the appendix
    for the full list. Each filled circle means that a class with that name exists
    in the dataset, while an empty circle means that there is no pixel from that class
    in the dataset. A rectangle indicates that a split andor merge operation was performed
    to map to the specified class in MSeg. Rectangles are zoomed-in in the right panel.
    Merge operations are shown with straight lines and split operations are shown
    with dashed lines. (Best seen in color.)
  Figure 5 Link: articels_figures_by_rev_year\2022\MSeg_A_Composite_Dataset_for_MultiDomain_Semantic_Segmentation\figure_5.jpg
  Figure 5 caption: Zero-shot generalization on images from MSeg test datasets; ScanNet-20
    (top two rows), KITTI (middle row), and WildDash (bottom two rows). In the fourth
    row the objects on the left are semi-truck trailers, and only the MSeg model recognizes
    them.
  Figure 6 Link: articels_figures_by_rev_year\2022\MSeg_A_Composite_Dataset_for_MultiDomain_Semantic_Segmentation\figure_6.jpg
  Figure 6 caption: Qualitative results from SwiftNet (RVC 2020 champion) and our
    MSeg model (RVC 2020 runner-up without any retraining on RVC datasets) on images
    from the WildDash-v2 RVC test dataset. Taxonomies greatly influence predictions;
    SwiftNet and WildDash-v2 introduce a separate van class (see row 1), whereas we
    group van instances with the car category. SwiftNet and WildDash-v2 also introduce
    a separate lane marking class (see row 3), which we merge into road. Note that
    SwiftNets taxonomy incorporates an ego-vehicle class, whereas we treat ego-vehicle
    as unlabeled when training MSeg models (See row 3).
  Figure 7 Link: articels_figures_by_rev_year\2022\MSeg_A_Composite_Dataset_for_MultiDomain_Semantic_Segmentation\figure_7.jpg
  Figure 7 caption: Qualitative results of our MSeg-720 panoptic model. Images sampled
    from the test splits of WildDash v2, ADE20K, and ScanNet.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: John Lambert
  Name of the last author: Vladlen Koltun
  Number of Figures: 7
  Number of Tables: 12
  Number of authors: 5
  Paper title: 'MSeg: A Composite Dataset for Multi-Domain Semantic Segmentation'
  Publication Date: 2022-02-14 00:00:00
  Table 1 caption: TABLE 1 Component datasets in MSeg.
  Table 10 caption: TABLE 10 Instance Segmentation Accuracy on MSeg Training Datasets
  Table 2 caption: TABLE 2 List of Datasets We Do Not Currently Include in MSeg
  Table 3 caption: TABLE 3 Semantic Segmentation Accuracy (mIoU) on MSeg Test Datasets
    Using 1 Million Crops
  Table 4 caption: TABLE 4 Semantic Segmentation Accuracy (mIoU) on MSeg Training
    Datasets
  Table 5 caption: TABLE 5 Results From the WildDash-v1 Leaderboard at the Time of
    Submission
  Table 6 caption: TABLE 6 Controlled Evaluation of Unified Taxonomy and Mask Relabeling
  Table 7 caption: TABLE 7 Robust Vision Challenge (RVC) 2020 Results in the Semantic
    Segmentation Track, as Measured by Class mIoU
  Table 8 caption: TABLE 8 Semantic Segmentation Accuracy (mIoU) on MSeg Train and
    Test Datasets After a Longer Training
  Table 9 caption: TABLE 9 Quantifying the Effect of Taxonomy Resolution on Performance
    on Mapillary Vistas in the RVC 2020 Challenge
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3151200
