- Affiliation of the first author: department of cybernetics, czech technical university,
    prague, czech republic
  Affiliation of the last author: department of cybernetics, czech technical university,
    prague, czech republic
  Figure 1 Link: articels_figures_by_rev_year\2015\Globally_Optimal_HandEye_Calibration_Using_BranchandBound\figure_1.jpg
  Figure 1 caption: A gripper-camera rig motion.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Globally_Optimal_HandEye_Calibration_Using_BranchandBound\figure_2.jpg
  Figure 2 caption: "(a) Geometric interpretation of the \u03F5 -epipolar constraint\
    \ with parameter \u03F5 imposed by the correspondence u ij \u2194 v ij on the\
    \ position of t A i . The cyan vectors show examples of the admissible configurations\
    \ of t A i . (b) 3D view of the same."
  Figure 3 Link: articels_figures_by_rev_year\2015\Globally_Optimal_HandEye_Calibration_Using_BranchandBound\figure_3.jpg
  Figure 3 caption: "(a) If the apertures \u03B1 j , \u03B1 k are sufficiently large,\
    \ cones C ij and C ik intersect in up to four lines l 1 ijk , l 2 ijk , l 3 ijk\
    \ , l 4 ijk . (b) The linear constraints imposed by correspondences u ij \u2194\
    \ v ij , u ik \u2194 v ik are determined by the normals n 1 ijk , n 2 ijk , n\
    \ 3 ijk , n 4 ijk . (c) Since Inequality 2 holds, the nappes C + ij , C + ik intersect\
    \ in l 1 ijk and \u2212 l 3 ijk . However, Inequality 3 does not hold and the\
    \ cones C ij , C ik do not form the pyramid P ijk . (d) The projection of the\
    \ pyramid P ijk into the plane \u03C0 i determined the boundary vectors b 1 ijk\
    \ , b 2 ijk (a 2D projection of the situation in (b))."
  Figure 4 Link: articels_figures_by_rev_year\2015\Globally_Optimal_HandEye_Calibration_Using_BranchandBound\figure_4.jpg
  Figure 4 caption: Ball experiment. (a) Camera positions in the experiment, different
    colors encode positions of the 3D points at different FOV levels. (b) The maximum
    residual error of the obtained solutions for the various values of sigma (red
    line) and the distribution of the measured errors over all correspondences (boxes).
    (c) The mean Euclidean distance between the 3D points transformed to the gripper's
    coordinate systems using ground truth mathtt X and the 3D points transformed to
    the gripper's coordinate systems using the estimated mathtt X . The eight FOV
    levels were merged into three groups. (d) Loglog plot of the computational time
    as a function of the number of threads. (e) The mean number of remaining cubes
    plotted against the number of subdivision phases. Note that the computation starts
    after the fourth subdivision. (f) The mean residual error at the beginning of
    the respective subdivision phase. Different noise levels were clustered into three
    groups.
  Figure 5 Link: articels_figures_by_rev_year\2015\Globally_Optimal_HandEye_Calibration_Using_BranchandBound\figure_5.jpg
  Figure 5 caption: Planar experiment. (a) Camera positions in the experiment, different
    colors encode positions of the 3D points at different FOV levels. (b) Comparison
    of the proposed method with previous methods using Euclidean distance measure.
    (c) Grid deformation (not in scale). (d) The maximum (red line) and median (boxes)
    residual error of the obtained solutions for the various values of sigma .
  Figure 6 Link: articels_figures_by_rev_year\2015\Globally_Optimal_HandEye_Calibration_Using_BranchandBound\figure_6.jpg
  Figure 6 caption: Real data experiment. (a-b) Motoman MA1400. Close up of the camera-gripper
    rig, sample images from the sequence; Camera poses reconstruction (c-d) Mitsubishi
    MELFA-RV-6S. Close up of the camera-gripper rig, sample images from the sequence;
    Model resulting from SfM, cameras are denoted by red pyramids.
  Figure 7 Link: articels_figures_by_rev_year\2015\Globally_Optimal_HandEye_Calibration_Using_BranchandBound\figure_7.jpg
  Figure 7 caption: Real data experiment error. (a) Motoman MA1400 (b) Mitsubishi
    MELFA-RV-6S.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.91
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jan Heller
  Name of the last author: Tomas Pajdla
  Number of Figures: 7
  Number of Tables: 0
  Number of authors: 3
  Paper title: Globally Optimal Hand-Eye Calibration Using Branch-and-Bound
  Publication Date: 2015-08-17 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2469299
- Affiliation of the first author: department of electrical and computer engineering,
    university of california, san diego, la jolla, ca
  Affiliation of the last author: department of electrical and computer engineering,
    university of california, san diego, la jolla, ca
  Figure 1 Link: articels_figures_by_rev_year\2015\High_Accuracy_Monocular_SFM_and_Scale_Correction_for_Autonomous_Driving\figure_1.jpg
  Figure 1 caption: '(Top row) (a) Our monocular SFM yields camera trajectories close
    to the ground truth over several kilometers of real-world driving. (b) Our monocular
    system significantly outperforms prior works that also use the ground plane for
    scale correction. (c) Our performance is comparable to stereo-based visual SFM.
    [Bottom row: Object localization] Accuracy of applications like 3D object localization
    that rely on the ground plane is also enhanced. The green line is the horizon
    from the estimated ground plane.'
  Figure 10 Link: articels_figures_by_rev_year\2015\High_Accuracy_Monocular_SFM_and_Scale_Correction_for_Autonomous_Driving\figure_10.jpg
  Figure 10 caption: Examples of mixture of Gaussians fits to detection scores. Our
    fitting (red) closely reflects the variation in noisy detection scores (blue).
    Each peak corresponds to an object.
  Figure 2 Link: articels_figures_by_rev_year\2015\High_Accuracy_Monocular_SFM_and_Scale_Correction_for_Autonomous_Driving\figure_2.jpg
  Figure 2 caption: 'System architecture for every steady state frame. The acronyms
    above represent PGM: Pose-guided matching, LBA: local bundle adjustment, R: re-finding,
    U: Update motion model, ECS: Epipolar search, T: triangulation. The modules are
    depicted in their multithreading arrangement, in correct synchronization order
    but not to scale.'
  Figure 3 Link: articels_figures_by_rev_year\2015\High_Accuracy_Monocular_SFM_and_Scale_Correction_for_Autonomous_Driving\figure_3.jpg
  Figure 3 caption: "Mechanism of epipolar constrained search, triangulation and validation\
    \ by reprojection to existing poses. For current frame n , only 3D points that\
    \ are validated against all frames 1 to n\u22121 are retained. Only persistent\
    \ 3D points that survive for greater than L frames may be collected by the next\
    \ keyframe."
  Figure 4 Link: articels_figures_by_rev_year\2015\High_Accuracy_Monocular_SFM_and_Scale_Correction_for_Autonomous_Driving\figure_4.jpg
  Figure 4 caption: "Geometry of ground plane estimation. The camera height h is the\
    \ distance from its optical center to ground plane. The ground plane normal is\
    \ n . Thus, the ground plane is defined by ( n \u22A4 ,h ) \u22A4 ."
  Figure 5 Link: articels_figures_by_rev_year\2015\High_Accuracy_Monocular_SFM_and_Scale_Correction_for_Autonomous_Driving\figure_5.jpg
  Figure 5 caption: "Homography mapping for plane-guided dense stereo. For a hypothesized\
    \ ground plane n,h and relative camera pose (R,t) between frames k and k+1 , a\
    \ per-pixel mapping can be computed within a region of interest by using the homography\
    \ matrix G=R+ h \u22121 t n \u22A4 ."
  Figure 6 Link: articels_figures_by_rev_year\2015\High_Accuracy_Monocular_SFM_and_Scale_Correction_for_Autonomous_Driving\figure_6.jpg
  Figure 6 caption: Examples of 1D Gaussian fits to estimate parameters a k s for
    h , n 1 and n 3 of the dense stereo method respectively.
  Figure 7 Link: articels_figures_by_rev_year\2015\High_Accuracy_Monocular_SFM_and_Scale_Correction_for_Autonomous_Driving\figure_7.jpg
  Figure 7 caption: Distributions of the underlying parameters a s,h , a s, n 1 and
    a s, n 3 for the dense stereo cue in KITTI dataset. The parameters a s roughly
    correspond to the peakiness of the stereo SAD cost distribution, which indicates
    belief in the accuracy of dense stereo.
  Figure 8 Link: articels_figures_by_rev_year\2015\High_Accuracy_Monocular_SFM_and_Scale_Correction_for_Autonomous_Driving\figure_8.jpg
  Figure 8 caption: Fitting a model mathcal Cs to relate observation variance us to
    the belief in quantized underlying parameters mathbf cs of dense stereo, for h
    , n1 and n3 .
  Figure 9 Link: articels_figures_by_rev_year\2015\High_Accuracy_Monocular_SFM_and_Scale_Correction_for_Autonomous_Driving\figure_9.jpg
  Figure 9 caption: (a) Distribution of the underlying variable ap, h for the 3D points
    cue in the KITTI dataset. The parameter ap,h corresponds to variation in height
    of 3D points stemming from the ground plane, which indicates belief in accuracy
    of the 3D points cue. (b) Relating observation variance up, h to the quantized
    underlying variable cp, h .
  First author gender probability: 0.67
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Shiyu Song
  Name of the last author: Clark C. Guest
  Number of Figures: 19
  Number of Tables: 4
  Number of authors: 3
  Paper title: High Accuracy Monocular SFM and Scale Correction for Autonomous Driving
  Publication Date: 2015-08-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Rotation and Translation Errors for Our System
      versus Other State-of-the-Art Stereo and Monocular Systems
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Effectiveness of our Ground Plane Estimation Is Demonstrated
      by Replacing VISO2-M's Ground Plane Estimation Module with ours
  Table 3 caption:
    table_text: TABLE 3 The Effectiveness of Our Monocular SFM Architecture Is Demonstrated
      by Comparing the Raw SFM Performance (without the Scale Correction of Our Ground
      Plane Estimation) with the State-of-the-Art SFM System, EKFMonoSLAM [19]
  Table 4 caption:
    table_text: TABLE 4 The Effectiveness of the Feature Matching Mechanism in Section
      3.2 Is Demonstrated by Comparing the SFM Performance Using the Proposed Method
      against the More Commonly Used Chain Matching Method
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2469274
- Affiliation of the first author: college of computer science, zhejiang university,
    hangzhou, china
  Affiliation of the last author: college of computer science, zhejiang university,
    hangzhou, china
  Figure 1 Link: articels_figures_by_rev_year\2015\Online_metricweighted_linear_representations_for_robust_visual_tracking\figure_1.jpg
  Figure 1 caption: Illustration of our discriminative criterion based on metric-weighted
    linear representation. The first column displays the original frames; the second
    column shows the corresponding confidence maps without metric learning (i.e.,
    M is an identity matrix); and the third column exhibits the corresponding confidence
    maps with metric learning. Clearly, our metric-weighted criterion is more discriminative.
  Figure 10 Link: articels_figures_by_rev_year\2015\Online_metricweighted_linear_representations_for_robust_visual_tracking\figure_10.jpg
  Figure 10 caption: "Tracking results of different trackers over some representative\
    \ frames from the \u201Cplaneshow\u201D video sequence in the scenarios with shape\
    \ deformations, out-of-plane rotations, and pose variations."
  Figure 2 Link: articels_figures_by_rev_year\2015\Online_metricweighted_linear_representations_for_robust_visual_tracking\figure_2.jpg
  Figure 2 caption: Intuitive illustration of time-weighted reservoir sampling. The
    upper part corresponds to the foreground samples stored in the foreground buffer
    during tracking, and the lower part is associated with all the foreground samples
    collected in the entire tracking process. Clearly, time-weighted reservoir sampling
    encourages more recent samples to appear in the buffer, and meanwhile retain some
    old samples with a long lifespan.
  Figure 3 Link: articels_figures_by_rev_year\2015\Online_metricweighted_linear_representations_for_robust_visual_tracking\figure_3.jpg
  Figure 3 caption: Quantitative evaluation of the proposed tracker using different
    buffer sizes and particle numbers. The left half corresponds to the tracking results
    with different buffer sizes, while the right half is associated with the tracking
    results with different particle numbers.
  Figure 4 Link: articels_figures_by_rev_year\2015\Online_metricweighted_linear_representations_for_robust_visual_tracking\figure_4.jpg
  Figure 4 caption: Quantitative comparison of the proposed tracker with weighted
    reservoir sampling and batch mode learning in average VOR on three video sequences.
    Clearly, the tracking performance of our weighted reservoir sampling is very close
    to that of batch mode learning.
  Figure 5 Link: articels_figures_by_rev_year\2015\Online_metricweighted_linear_representations_for_robust_visual_tracking\figure_5.jpg
  Figure 5 caption: Quantitative evaluation of the proposed tracker with five different
    initialization configurations (obtained by moderate random perturbation on the
    original initialization setting) in VOR on three video sequences. It is clear
    that the proposed tracker is not very sensitive to different initialization configurations.
  Figure 6 Link: articels_figures_by_rev_year\2015\Online_metricweighted_linear_representations_for_robust_visual_tracking\figure_6.jpg
  Figure 6 caption: Quantitative evaluation of the proposed tracker with different
    settings for the trade-off control factor rho such that rho in lbrace 0.07, 0.08,
    0.10, 0.12, 0.14, 0.16, 0.19rbrace . It is observed that the proposed tracker
    is not very sensitive to the setting of rho .
  Figure 7 Link: articels_figures_by_rev_year\2015\Online_metricweighted_linear_representations_for_robust_visual_tracking\figure_7.jpg
  Figure 7 caption: "Tracking results of different trackers over some representative\
    \ frames from the \u201CLola\u201D video sequence in the scenarios with drastic\
    \ scale changes and body pose variations."
  Figure 8 Link: articels_figures_by_rev_year\2015\Online_metricweighted_linear_representations_for_robust_visual_tracking\figure_8.jpg
  Figure 8 caption: "Tracking results of different trackers over some representative\
    \ frames from the \u201Ciceball\u201D video sequence in the scenarios with partial\
    \ occlusions, out-of-plane rotations, body pose variations, and abrupt motion."
  Figure 9 Link: articels_figures_by_rev_year\2015\Online_metricweighted_linear_representations_for_robust_visual_tracking\figure_9.jpg
  Figure 9 caption: "Tracking results of different trackers over some representative\
    \ frames from the \u201Cfootball3\u201D video sequence in the scenarios with motion\
    \ blurring, partial occlusions, head pose variations, and background clutters."
  First author gender probability: 0.97
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.83
  Name of the first author: Xi Li
  Name of the last author: Yueting Zhuang
  Number of Figures: 18
  Number of Tables: 5
  Number of authors: 5
  Paper title: Online metric-weighted linear representations for robust visual tracking
  Publication Date: 2015-08-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Evaluation of the Proposed Tracker Using Different
      Linear Representations on Four Video Sequences
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Evaluation of the Proposed Tracker Using Different
      Sampling Methods on Five Video Sequences
  Table 3 caption:
    table_text: TABLE 3 Quantitative Evaluation of the Proposed Tracker with Different
      Metric Learning Configurations on Five Video Sequences
  Table 4 caption:
    table_text: TABLE 4 The Quantitative Comparison Results of the 15 Trackers Over
      All the Video Sequences
  Table 5 caption:
    table_text: TABLE 5 Quantitative Evaluation of the Proposed Tracker Using Different
      Learning Strategies on Eight Video Sequences
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2469276
- Affiliation of the first author: department of computer science and engineering,
    university of california, riverside, ca
  Affiliation of the last author: center for research in intelligent systems, university
    of california, riverside, ca
  Figure 1 Link: articels_figures_by_rev_year\2015\Semantic_Concept_CoOccurrence_Patterns_for_Image_Annotation_and_Retrieval\figure_1.jpg
  Figure 1 caption: An illustration of (a) a network of nodes representing the semantic
    concepts and the edges representing the co-occurrence relations, and (b) the discovered
    corresponding hierarchical community structure from the network that shows concept
    co-occurrence patterns at different levels.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Semantic_Concept_CoOccurrence_Patterns_for_Image_Annotation_and_Retrieval\figure_2.jpg
  Figure 2 caption: 'The flowchart of the proposed concept inference framework. The
    contributions are: (i) a co-occurrence pattern detection method that effectively
    explores hierarchical correlations among semantic concepts, (ii) random walk based
    approach to refine the concept signature representation based on detected concept
    co-occurrence patterns.'
  Figure 3 Link: articels_figures_by_rev_year\2015\Semantic_Concept_CoOccurrence_Patterns_for_Image_Annotation_and_Retrieval\figure_3.jpg
  Figure 3 caption: (a) Modularity versus level of the hierarchy. (b) Modularity versus
    the number of co-occurrence patterns.
  Figure 4 Link: articels_figures_by_rev_year\2015\Semantic_Concept_CoOccurrence_Patterns_for_Image_Annotation_and_Retrieval\figure_4.jpg
  Figure 4 caption: The recall rate of common concepts in the three datasets.
  Figure 5 Link: articels_figures_by_rev_year\2015\Semantic_Concept_CoOccurrence_Patterns_for_Image_Annotation_and_Retrieval\figure_5.jpg
  Figure 5 caption: (a),(b) show the image annotation performance of the approaches
    applied to the three datasets measured by Top-5 F 0.5 -measure with annotation
    length M = 5 .
  Figure 6 Link: articels_figures_by_rev_year\2015\Semantic_Concept_CoOccurrence_Patterns_for_Image_Annotation_and_Retrieval\figure_6.jpg
  Figure 6 caption: The annotations for the test images from the three datasets by
    our approach. They are compared with the ground-truth. Green labels are correctly
    predicted, red ones are wrongly predicted and blue ones have very close semantic
    meaning to the ground-truth.
  Figure 7 Link: articels_figures_by_rev_year\2015\Semantic_Concept_CoOccurrence_Patterns_for_Image_Annotation_and_Retrieval\figure_7.jpg
  Figure 7 caption: An example of the top-10 retrieved images by our proposed approach.
    The retrieved images are ranked based on their semantic distance to the query.
    The top row shows the correctly retrieved images with street view and the stop
    sign. In the middle row, the top retrieved images correctly match the bedroom
    scene represented in the query. And in the last row, the images with a beach scene
    and people are placed at the top positions.
  Figure 8 Link: articels_figures_by_rev_year\2015\Semantic_Concept_CoOccurrence_Patterns_for_Image_Annotation_and_Retrieval\figure_8.jpg
  Figure 8 caption: (a), (b) show the image retrieval performance of the approaches
    applied to the three datasets measured by Top-D MAP with varied number of retrieved
    images D.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.65
  Name of the first author: Linan Feng
  Name of the last author: Bir Bhanu
  Number of Figures: 8
  Number of Tables: 8
  Number of authors: 2
  Paper title: Semantic Concept Co-Occurrence Patterns for Image Annotation and Retrieval
  Publication Date: 2015-08-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Definition of Symbols Used in Section 3.1
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Summarization of the Usage and Motivation for Adopted
      Co-Occurrence Measures
  Table 3 caption:
    table_text: TABLE 3 Pairwise Co-Occurrence Scores for Example Concept Pairs by
      Using NGD, NTD, ALA and the Combination of the Three
  Table 4 caption:
    table_text: TABLE 4 Averaged Modularity Scores (Q) from 5th to 10th Level
  Table 5 caption:
    table_text: TABLE 5 Precisions at Different Annotation Lengths by Using Different
      Co-Occurrence Measures
  Table 6 caption:
    table_text: TABLE 6 The Top-5 F 0.5 Score and the Standard Deviation (Show in
      the Parentheses) of Automated Annotation with Different Training Set Sizes
  Table 7 caption:
    table_text: TABLE 7 Mean Average Precision for Different Sizes of Retrieved Images
      by Using Different Co-Occurrence Measures
  Table 8 caption:
    table_text: TABLE 8 Mean Average Precision of Top-10 Retrieved Images with Different
      Training Set Size
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2469281
- Affiliation of the first author: department of information engineering, the chinese
    university of hong kong, hong kong
  Affiliation of the last author: department of information engineering, the chinese
    university of hong kong, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2015\Learning_Deep_Representation_for_Face_Alignment_with_Auxiliary_Attributes\figure_1.jpg
  Figure 1 caption: (a) Examples of facial landmark detection by a single conventional
    CNN, the cascaded CNN [13], and the proposed Tasks-Constrained Deep Convolutional
    Network. More accurate detection can be achieved by optimizing the detection task
    jointly with relatedauxiliary tasks. (b) Average face images with different attributes.
    The image in blue rectangle is averaged among the whole training faces, while
    the one in red is from the smiling faces with frontal pose. It indicates that
    the input and solution space can be effectively divided into subsets, which are
    in different distributions. This lowers the learning difficulty.
  Figure 10 Link: articels_figures_by_rev_year\2015\Learning_Deep_Representation_for_Face_Alignment_with_Auxiliary_Attributes\figure_10.jpg
  Figure 10 caption: "Examples of improvement by attribute group of \u201Ceye\u201D\
    \ and \u201Cmouth\u201D."
  Figure 2 Link: articels_figures_by_rev_year\2015\Learning_Deep_Representation_for_Face_Alignment_with_Auxiliary_Attributes\figure_2.jpg
  Figure 2 caption: "Structure specification for TCDCN. A 60\xD760 image is taken\
    \ as input. In the first layer, we convolve it with 20 different 5\xD75 filters,\
    \ using a stride of 1. The obtained feature map is 56\xD756\xD720 , which is subsampled\
    \ to 28\xD728\xD720 with a 2\xD72 max-pooling operation. Similar operations are\
    \ repeated in layer 2, 3, 4, as the parameters shown in the figure. The last layer\
    \ is fully-connected. Then the output is obtained by regression."
  Figure 3 Link: articels_figures_by_rev_year\2015\Learning_Deep_Representation_for_Face_Alignment_with_Auxiliary_Attributes\figure_3.jpg
  Figure 3 caption: The TCDCN learns shared features for facial landmark detection
    and auxiliary tasks. The first row shows the face images and the second row shows
    the corresponding features in the shared feature space, where the face images
    with similar poses and attributes are close with each other. This reveals that
    the learned feature space is robust to pose, expression, and occlusion.
  Figure 4 Link: articels_figures_by_rev_year\2015\Learning_Deep_Representation_for_Face_Alignment_with_Auxiliary_Attributes\figure_4.jpg
  Figure 4 caption: "(a) Facial landmark localization error curve with and without\
    \ dynamic task coefficient. The error is measured in L2-norm with respect to the\
    \ ground truth of the 10 coordinates values (normalized to [0,1]) for the five\
    \ landmarks. (b) Task coefficients for the \u201CBig Nose\u201D and \u201CArched\
    \ Eyebrows\u201D attributes over the training process."
  Figure 5 Link: articels_figures_by_rev_year\2015\Learning_Deep_Representation_for_Face_Alignment_with_Auxiliary_Attributes\figure_5.jpg
  Figure 5 caption: Correlation of each attribute group with different landmarks.
  Figure 6 Link: articels_figures_by_rev_year\2015\Learning_Deep_Representation_for_Face_Alignment_with_Auxiliary_Attributes\figure_6.jpg
  Figure 6 caption: Normalized correlation of the attributes with different landmarks.
    The attributes are randomly selected from each attribute group. The correlation
    is normalized among the five landmarks.
  Figure 7 Link: articels_figures_by_rev_year\2015\Learning_Deep_Representation_for_Face_Alignment_with_Auxiliary_Attributes\figure_7.jpg
  Figure 7 caption: Pairwise correlation of the auxiliary tasks learned by TCDCN (best
    viewed in color).
  Figure 8 Link: articels_figures_by_rev_year\2015\Learning_Deep_Representation_for_Face_Alignment_with_Auxiliary_Attributes\figure_8.jpg
  Figure 8 caption: 'Comparison of different model variants of TCDCN: the mean error
    over different landmarks (left), and the overall failure rate (right).'
  Figure 9 Link: articels_figures_by_rev_year\2015\Learning_Deep_Representation_for_Face_Alignment_with_Auxiliary_Attributes\figure_9.jpg
  Figure 9 caption: Improvement over different landmarks by different attribute groups.
  First author gender probability: 0.89
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.93
  Name of the first author: Zhanpeng Zhang
  Name of the last author: Xiaoou Tang
  Number of Figures: 17
  Number of Tables: 5
  Number of authors: 4
  Paper title: Learning Deep Representation for Face Alignment with Auxiliary Attributes
  Publication Date: 2015-08-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Annotated Face Attributes in MAFL Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Comparison of Mean Error ( \xD7 10 \u22122 ) on MAFL Dataset\
      \ under Different Network Configurations"
  Table 3 caption:
    table_text: TABLE 3 Comparison of Different Deep Modes for Facial Landmark Detection
  Table 4 caption:
    table_text: TABLE 4 Mean Errors (Percent) on Helen [23] Dataset
  Table 5 caption:
    table_text: TABLE 5 Mean Errors (Percent) on 300-W [20] Dataset (68 Landmarks)
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2469286
- Affiliation of the first author: department of electrical engineering, indian institute
    of science, bangalore, india
  Affiliation of the last author: department of electrical engineering, indian institute
    of science, bangalore, india
  Figure 1 Link: articels_figures_by_rev_year\2015\Low_Resolution_Face_Recognition_Across_Variations_in_Pose_and_Illumination\figure_1.jpg
  Figure 1 caption: Flowchart of the training and testing stages of the proposed approach.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Low_Resolution_Face_Recognition_Across_Variations_in_Pose_and_Illumination\figure_2.jpg
  Figure 2 caption: Proposed reference based face recognition algorithm.
  Figure 3 Link: articels_figures_by_rev_year\2015\Low_Resolution_Face_Recognition_Across_Variations_in_Pose_and_Illumination\figure_3.jpg
  Figure 3 caption: Cumulative match characteristic curves for the reference-based
    approach and the modified reference-based approach that re-ranks the top ranked
    gallery images by computing the stereo cost against the probe image.
  Figure 4 Link: articels_figures_by_rev_year\2015\Low_Resolution_Face_Recognition_Across_Variations_in_Pose_and_Illumination\figure_4.jpg
  Figure 4 caption: Modified reference-based approach. For each query (left), top
    row shows the top 10 matches returned by the reference-based method, bottom row
    shows re-ranked result using the proposed modification.
  Figure 5 Link: articels_figures_by_rev_year\2015\Low_Resolution_Face_Recognition_Across_Variations_in_Pose_and_Illumination\figure_5.jpg
  Figure 5 caption: Rank-1 recognition accuracy of the proposed algorithms with the
    two super-resolution techniques SR1 [40] and SR2 [41]. Comparison with other approaches
    are also shown.
  Figure 6 Link: articels_figures_by_rev_year\2015\Low_Resolution_Face_Recognition_Across_Variations_in_Pose_and_Illumination\figure_6.jpg
  Figure 6 caption: Rank-1 recognition performance for different probe resolutions.
  Figure 7 Link: articels_figures_by_rev_year\2015\Low_Resolution_Face_Recognition_Across_Variations_in_Pose_and_Illumination\figure_7.jpg
  Figure 7 caption: 'MBGC [8] data- Top row: Sample gallery images. Bottom Row: Sample
    probe images of the corresponding subjects.'
  Figure 8 Link: articels_figures_by_rev_year\2015\Low_Resolution_Face_Recognition_Across_Variations_in_Pose_and_Illumination\figure_8.jpg
  Figure 8 caption: 'Example facial images of Choke Point database [10]. Top row:
    frontal gallery images, second row: corresponding probe images.'
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.64
  Name of the first author: Sivaram Prasad Mudunuri
  Name of the last author: Soma Biswas
  Number of Figures: 8
  Number of Tables: 4
  Number of authors: 2
  Paper title: Low Resolution Face Recognition Across Variations in Pose and Illumination
  Publication Date: 2015-08-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Rank-1 Recognition Performance for Four Different Probe Poses,
      Averaged over the Different Gallery Illuminations
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Rank-1 Recognition of the Modified Reference-Based Algorithm
      with Varying Number of Reference Images for Pose 041
  Table 3 caption:
    table_text: TABLE 3 Rank-1 Recognition of the Proposed Approach and Comparison
      with Existing Algorithms on SCFace and MBGC Databases
  Table 4 caption:
    table_text: TABLE 4 Rank-1 Recognition (%) of the Proposed Approach and Comparison
      with Existing Algorithms on ChokePoint Database [10]
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2469282
- Affiliation of the first author: "instituto de investigaci\xF3n en ingenier\xED\
    a de arag\xF3n (i3a), universidad de zaragoza, zaragoza, spain"
  Affiliation of the last author: "instituto de investigaci\xF3n en ingenier\xEDa\
    \ de arag\xF3n (i3a), universidad de zaragoza, zaragoza, spain"
  Figure 1 Link: articels_figures_by_rev_year\2015\Sequential_NonRigid_Structure_from_Motion_Using_Physical_Priors\figure_1.jpg
  Figure 1 caption: 'Simultaneous estimation of 3D non-rigid structure and camera
    trajectory. Summary of the experiments. Top: Reconstructed 3D mesh overlaid on
    a specific frame of the input sequence. Bottom: 3D view of the same mesh and camera
    trajectory for the whole sequence. Black crosses indicate the initial and final
    camera positions. In contrast to standard EKF-SLAM methods where the scene is
    considered to be rigid, the mesh continuously deforms and we estimate its shape
    in each frame of the sequence. Our approach is suitable for both extensible surfaces
    (like the Silicone Cloth) and non-extensible materials (like the Bending Paper).
    The Synthetic Plate, the Face, and the Laparoscopic data, exhibit intermediate
    levels of extensibility.'
  Figure 10 Link: articels_figures_by_rev_year\2015\Sequential_NonRigid_Structure_from_Motion_Using_Physical_Priors\figure_10.jpg
  Figure 10 caption: 'Results on the Actress sequence. Top: Selected frames 24, 44,
    64 and 84 with the 2D tracked features. Middle and Bottom rows: Two different
    views of the 3D reconstruction. Note, specially on the bottom row, that each of
    the 3D features has the associated uncertainty ellipsoid. In this case, though,
    the ellipsoids are very small as the inter-frame deformation is relatively small
    compared to the camera motion.'
  Figure 2 Link: articels_figures_by_rev_year\2015\Sequential_NonRigid_Structure_from_Motion_Using_Physical_Priors\figure_2.jpg
  Figure 2 caption: 'Thin-plate geometry. Top: Structure at rest. The mid-surface
    plane (in red) is used to describe the geometry of the deformation. Bottom: Cross
    sectional views of the deformed structure. Displacements u , v and w of the middle-plane.'
  Figure 3 Link: articels_figures_by_rev_year\2015\Sequential_NonRigid_Structure_from_Motion_Using_Physical_Priors\figure_3.jpg
  Figure 3 caption: Triangular discretization of the scene. W and L represent the
    global and the local reference systems, respectively.
  Figure 4 Link: articels_figures_by_rev_year\2015\Sequential_NonRigid_Structure_from_Motion_Using_Physical_Priors\figure_4.jpg
  Figure 4 caption: "Normalized triangle in natural coordinates N and real triangle\
    \ element in local coordinates L . They are related by the J g transformation.\
    \ Nodes are represented as black dots ( \u2219 ) and Hammer's integration points\
    \ as black diamonds ( \u29EB )."
  Figure 5 Link: articels_figures_by_rev_year\2015\Sequential_NonRigid_Structure_from_Motion_Using_Physical_Priors\figure_5.jpg
  Figure 5 caption: "Problem Formulation. A moving camera m observes a non-rigid structure\
    \ y . A set of Gaussian distributed forces \u0394s are acting on the structure,\
    \ resulting in a non-rigid deformation. Our goal, is to estimate both the camera\
    \ trajectory and surface deformation from only image observations. A set of rigid\
    \ points on the surface (black squares \u25A0 ) let us to disambiguate between\
    \ rigid relative motions of the camera and the surface."
  Figure 6 Link: articels_figures_by_rev_year\2015\Sequential_NonRigid_Structure_from_Motion_Using_Physical_Priors\figure_6.jpg
  Figure 6 caption: "Data association. Left: Some of the Fast interest points detected\
    \ in the first frame of the sequence and their rectangular patches. Right: Search\
    \ regions. Cyan ellipses correspond to the search regions if only a rigid model\
    \ was considered. Yellow ellipses represent the actual predicted search areas,\
    \ with an additional degree of uncertainty produced by the non-rigid component\
    \ of the model. Note that for the left-most points\u2014rigid boundary points\u2014\
    yellow and cyan ellipses coincide."
  Figure 7 Link: articels_figures_by_rev_year\2015\Sequential_NonRigid_Structure_from_Motion_Using_Physical_Priors\figure_7.jpg
  Figure 7 caption: 'Results on the Synthetic Elastic Plate. Left: 3D reconstruction
    results for selected frames 50, 650 and 950. Top: The reconstructed mesh (blue)
    is projected onto the input mesh (black), which is hardly visible as the projection
    is almost perfect. Middle: 3D view of the estimated mesh, where the red ellipsoids
    are the associated nodal uncertainties represented as a 95 percent confidence
    region. The real elasticity of the plate is coded with the white-black pattern,
    where white indicates larger elasticity, reaching levels where the patch area
    is increased by a factor 2times . Bottom: Ground truth 3D shape (black) and our
    estimate (blue), computed from the mean of the Gaussian distributions at each
    nodal position. Right: Estimated camera trajectory, seen from two viewpoints (X-Y
    and X-Z). Although our approach provides the whole 6-dof camera pose for each
    frame, for clarity we just plot the position its center, and the associated uncertainty
    at specific frames. Note that this path is consistent with the ground truth camera
    trajectory.'
  Figure 8 Link: articels_figures_by_rev_year\2015\Sequential_NonRigid_Structure_from_Motion_Using_Physical_Priors\figure_8.jpg
  Figure 8 caption: 'Synthetic Elastic Plate: Comparing EKF-FEM against SBA [34],
    EM-LDS [43], CSF [19], DCT [3] and BBN1-BBN2 [31]. Left: 3D reconstruction error
    for every method at each frame of the sequence. Middle: Cumulative histogram of
    the reconstruction error. Right: Significance of the reconstruction error values.
    Black: ground truth; Blue: reconstructed mesh. Observe that the EKF-FEM error
    is around 5 mm, and thus it provides very accurate reconstructions. Similar results
    are obtained with the BBN2, but at the expense requiring good training data, representative
    of all deformations undergone by the plate.'
  Figure 9 Link: articels_figures_by_rev_year\2015\Sequential_NonRigid_Structure_from_Motion_Using_Physical_Priors\figure_9.jpg
  Figure 9 caption: 'Synthetic Elastic Plate: Robustness against image noise, missing
    data and elastic parameter tuning. Mean 3D reconstruction error of all 1,000 frames
    as a function of the image noise, percentage of missing data, Poisson''s ratio
    nu , thickness surface h and normalized forces Delta mathbf s . In the right-most
    graph we simultaneously plot h and the inverse of the normalized forces Delta
    mathbf s . In all cases, the white-filled square shows the parameters used in
    our original EKF-FEM formulation. Additionally, as a baseline reference, we show
    the mean error of the DCT [3] and BBN1-BBN2 [31] from Fig. 8. Note that the results
    we obtain remain within reasonable bounds for a wide range the tuned parameters,
    demonstrating that a fine tuning of parameters is not required.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.67
  Name of the first author: Antonio Agudo
  Name of the last author: J. M. M. Montiel
  Number of Figures: 18
  Number of Tables: 1
  Number of authors: 4
  Paper title: Sequential Non-Rigid Structure from Motion Using Physical Priors
  Publication Date: 2015-08-18 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Parameter Selection: h Is the Surface Thickness, \u03BD Is\
      \ the Poisson's Ratio and \u0394s= \u0394 f \u2217 Eh Is the Normalized Force"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2469293
- Affiliation of the first author: school of arts, media and engineering and the school
    of electrical, computer and energy engineering, arizona state university, tempe,
    az
  Affiliation of the last author: school of arts, media and engineering and the school
    of electrical, computer and energy engineering, arizona state university, tempe,
    az
  Figure 1 Link: articels_figures_by_rev_year\2015\ReconstructionFree_Action_Inference_from_Compressive_Imagers\figure_1.jpg
  Figure 1 caption: 'Compressive sensing of a scene: Every frame of the scene is compressively
    sensed by optically correlating random patterns with the frame to obtain CS measurements.
    The temporal sequence of such CS measurements is the CS video.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\ReconstructionFree_Action_Inference_from_Compressive_Imagers\figure_2.jpg
  Figure 2 caption: Overview of our approach to action recognition from a compressively
    sensed test video. First, MACH [22] filters for different actions are synthesized
    offline from training examples and then compressed to obtain smashed filters.
    Next, the CS measurements of the test video are correlated with these smashed
    filters to obtain correlation volumes which are analyzed to determine the action
    in the test video.
  Figure 3 Link: articels_figures_by_rev_year\2015\ReconstructionFree_Action_Inference_from_Compressive_Imagers\figure_3.jpg
  Figure 3 caption: (a) 'Type-1' filter obtained for walking action where the training
    examples were from different viewpoints. (b) 'Type-2' filter obtained from the
    training examples by bringing all the training examples to the same viewpoint.
    In (a), two groups of human move in opposite directions and eventually merge into
    each other, thus making the filter ineffective. In (b), the merging effect is
    countered by transforming the training set to the same viewpoint.
  Figure 4 Link: articels_figures_by_rev_year\2015\ReconstructionFree_Action_Inference_from_Compressive_Imagers\figure_4.jpg
  Figure 4 caption: The mean PSRs for different viewpoints for both canonical filters
    and compensated filters are shown. The mean PSR values obtained using compensated
    filters are more than those obtained using canonical filters, thus corroborating
    the need of affine transforming the MACH filters to the viewpoint of the test
    example.
  Figure 5 Link: articels_figures_by_rev_year\2015\ReconstructionFree_Action_Inference_from_Compressive_Imagers\figure_5.jpg
  Figure 5 caption: Spatial localization of subject without reconstruction at compression
    ratio = 100 for different actions in Weizmann dataset. (a) Walking. (b) Two handed
    wave. (c) Jump in place.
  Figure 6 Link: articels_figures_by_rev_year\2015\ReconstructionFree_Action_Inference_from_Compressive_Imagers\figure_6.jpg
  Figure 6 caption: 'Localization error for Weizmann dataset. X-axis: Displacement
    from ground truth. Y-axis: Fraction of total number of frames for which the displacement
    of subject''s centre from ground truth is less than or equal to the value in x-axis.
    On average, for approximately 70 percent of the frames, the displacement of ground
    truth is less than or equal to 15 pixels, for compression ratios of 100, 200 and
    300.'
  Figure 7 Link: articels_figures_by_rev_year\2015\ReconstructionFree_Action_Inference_from_Compressive_Imagers\figure_7.jpg
  Figure 7 caption: Reconstruction-free spatial localization of subject for Oracle
    MACH (shown as yellow box) and STSF (shown as green box) at compression ratio
    = 100 for some correctly classfied instances of various actions in the UCF sports
    dataset. (a) Golf. (b) Kicking. (c) Skate-Boarding. Action localization is estimated
    reasonably well directly from CS measurements even though the measurements themselves
    do not bear any explicit information regarding pixel locations.
  Figure 8 Link: articels_figures_by_rev_year\2015\ReconstructionFree_Action_Inference_from_Compressive_Imagers\figure_8.jpg
  Figure 8 caption: 'Action localization: Each row corresponds to various instances
    of a particular action, and action localization in one frame for each of these
    instances is shown. The bounding boxes (yellow for Oracle MACH, and green for
    STSF at compression ratio = 100) in most cases correspond to the human, or the
    moving part. Note that these bounding boxes shown are obtained using a rudimentary
    procedure, without any training, as outlined earlier in the section. This suggests
    that joint training of features extracted from correlation volumes and annotated
    bounding boxes can lead to more accurate action localization results.'
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Kuldeep Kulkarni
  Name of the last author: Pavan Turaga
  Number of Figures: 8
  Number of Tables: 6
  Number of authors: 2
  Paper title: Reconstruction-Free Action Inference from Compressive Imagers
  Publication Date: 2015-08-18 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Action Recognition Rates for the Dataset Corresponding to\
      \ Fixed Viewing Angle of 15 \u2218 Using Compensated Filters Generated for Various\
      \ Viewing Angles"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Weizmann Dataset: Recognition Rates for Reconstruction-Free
      Recognition from Compressive Cameras for Different Compression Factors Are Stable
      Even at High Compression Factors of 500'
  Table 3 caption:
    table_text: TABLE 3 Confusion Matrix for UCF Sports Database at a Compression
      Factor = 100
  Table 4 caption:
    table_text: 'TABLE 4 UCF50 Dataset: The Recognition Rate for our Framework Is
      Stable Even at Very High Compression Ratios, While in the Case of Recon + IDT,
      Recognition Rates Are Much Lower'
  Table 5 caption:
    table_text: 'TABLE 5 UCF50 Dataset: Recognition Rates for Individual Classes at
      Compression Ratios, 1 (Oracle MACH), 100 and 400'
  Table 6 caption:
    table_text: 'TABLE 6 HMDB51 Dataset: The Recognition Rate for Our Framework Is
      Stable Even at Very High Compression Ratios, While in the Case of Recon+IDT,
      It Is Much Lower'
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2469288
- Affiliation of the first author: ntt communication science laboratories, soraku-gun,
    kyoto, japan
  Affiliation of the last author: department of engineering, university of cambridge,
    cambridge, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2015\Unsupervised_ManytoMany_Object_Matching_for_Relational_Data\figure_1.jpg
  Figure 1 caption: Generative process of two networks with our model. The cluster
    proportions and connectivity are shared by both of the networks. From the cluster
    proportions, a cluster assignment for each node is generated. For example, nodes
    6, 2 and 3 are assigned into the first cluster in Network1. The nodes assigned
    to the same cluster are considered matched; nodes 6, 2 and 3 in Network1 and nodes
    7, 9, 6 and 1 in Network2 are matched. Using the cluster assignments and connectivity,
    links are generated.
  Figure 10 Link: articels_figures_by_rev_year\2015\Unsupervised_ManytoMany_Object_Matching_for_Relational_Data\figure_10.jpg
  Figure 10 caption: 'Document clustering by ReMatch with the 20News data. The x-axis
    in each figure shows the category index: (1) computers, (2) recreation, (3) science,
    and (4) talk. The y-axis in each figure of cluster ell shows the probability that
    documents assigned to cluster ell are labeled with the category. The left blue
    bar shows the probability in the first network, and the right red bar shows that
    in the second network.'
  Figure 2 Link: articels_figures_by_rev_year\2015\Unsupervised_ManytoMany_Object_Matching_for_Relational_Data\figure_2.jpg
  Figure 2 caption: Graphical model representation of ReMatch for bipartite networks.
  Figure 3 Link: articels_figures_by_rev_year\2015\Unsupervised_ManytoMany_Object_Matching_for_Relational_Data\figure_3.jpg
  Figure 3 caption: "(a) Input networks. In this case, the inputs are two bipartite\
    \ networks, where type t=1 nodes are aligned vertically, and type t=2 nodes are\
    \ aligned horizontally. (b) Outputs of ReMatch: (top) cluster assignments, where\
    \ the nodes are sorted according to the assignments, and (bottom) the inferred\
    \ connectivity matrix \u03B7 ."
  Figure 4 Link: articels_figures_by_rev_year\2015\Unsupervised_ManytoMany_Object_Matching_for_Relational_Data\figure_4.jpg
  Figure 4 caption: Cluster connectivities of (a) matching unidentifiable networks
    and (b) matching identifiable networks. Each node represents a cluster, and the
    value at the edge represents its connectivity.
  Figure 5 Link: articels_figures_by_rev_year\2015\Unsupervised_ManytoMany_Object_Matching_for_Relational_Data\figure_5.jpg
  Figure 5 caption: Examples of synthetic data sets. Nodes are aligned by their cluster
    assignments. The bar chart shows the cluster proportions of the type in the network.
  Figure 6 Link: articels_figures_by_rev_year\2015\Unsupervised_ManytoMany_Object_Matching_for_Relational_Data\figure_6.jpg
  Figure 6 caption: Average matching adjusted Rand index with (a) different Dirichlet
    parameters, (b) different numbers of networks, (c) different numbers of clusters
    and (d) different numbers of nodes for each cluster with the synthetic Dirichlet
    data sets.
  Figure 7 Link: articels_figures_by_rev_year\2015\Unsupervised_ManytoMany_Object_Matching_for_Relational_Data\figure_7.jpg
  Figure 7 caption: Cluster assignments over iterations in the inference with a Balance
    data set. The horizontal axis is the node index, and the vertical axis is the
    cluster index. Each blue 'o' represents a node in the first network, and each
    red 'x' represents a node in the second network. Therefore, when 'o' and 'x' are
    overlapped, nodes from different networks are correctly assigned into the same
    cluster.
  Figure 8 Link: articels_figures_by_rev_year\2015\Unsupervised_ManytoMany_Object_Matching_for_Relational_Data\figure_8.jpg
  Figure 8 caption: Shared latent group structures discovered by ReMatch in the Movie
    data. The vertical axis represents the user index, and the horizontal axis represents
    the movie index.
  Figure 9 Link: articels_figures_by_rev_year\2015\Unsupervised_ManytoMany_Object_Matching_for_Relational_Data\figure_9.jpg
  Figure 9 caption: Shared latent group structures discovered by ReMatch in the 20News
    data. The vertical axis represents the word index, and the horizontal axis represents
    the document index.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Tomoharu Iwata
  Name of the last author: Zoubin Ghahramani
  Number of Figures: 12
  Number of Tables: 6
  Number of authors: 3
  Paper title: Unsupervised Many-to-Many Object Matching for Relational Data
  Publication Date: 2015-08-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average Matching Adjusted Rand Index, and Its Standard Error
      for the Synthetic Data Sets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Cross-Domain Recommendation Results Using the Movie Data
  Table 3 caption:
    table_text: TABLE 3 Average Matching Adjusted Rand Index and Their Standard Error
      for the 20News Data
  Table 4 caption:
    table_text: TABLE 4 Word Clustering by ReMatch with the 20 News Data
  Table 5 caption:
    table_text: TABLE 5 Multi-Lingual Word Clustering by ReMatch in the Wikipedia
      Data
  Table 6 caption:
    table_text: TABLE 6 Average Matching Adjusted Rand Index and Their Standard Error
      for the Wikipedia Data
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2469284
- Affiliation of the first author: key laboratory of machine perception (ministry
    of education), school of electronic engineering and computer science, peking university,
    beijing, china
  Affiliation of the last author: key laboratory of machine perception (ministry of
    education), school of electronic engineering and computer science, peking university,
    beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2015\Fast_Multidimensional_EllipsoidSpecific_Fitting_by_Alternating_Direction_Method_\figure_1.jpg
  Figure 1 caption: A comparison between algebraic fitting [5] and geometric fitting
    [1]. (a) They produce close results if there is little noise and the sample points
    are uniform. (b)-(c) With heavy noise, non-uniform sampling, or bad initialization
    of the parameters, geometric fitting may produce a bad local minimum solution
    due to its non-convexity.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Fast_Multidimensional_EllipsoidSpecific_Fitting_by_Alternating_Direction_Method_\figure_2.jpg
  Figure 2 caption: An example of ellipse fitting with the presence of noise. (a)-(c)
    are the cases with 5, 15, and 25 percent Gaussian noise, respectively. The LLS
    solution may degenerate to a hyperbola when noise are strong (c), while other
    methods remain stable due to the semi-definiteness constraint.
  Figure 3 Link: articels_figures_by_rev_year\2015\Fast_Multidimensional_EllipsoidSpecific_Fitting_by_Alternating_Direction_Method_\figure_3.jpg
  Figure 3 caption: Residual errors of Calafiore's, Yin et al.'s, and our ADMM methods,
    under different noise levels and different numbers of data points, measured in
    ell 2 norm (a), ell 1 norm (b), and ell infty norm (c).
  Figure 4 Link: articels_figures_by_rev_year\2015\Fast_Multidimensional_EllipsoidSpecific_Fitting_by_Alternating_Direction_Method_\figure_4.jpg
  Figure 4 caption: Two images of a checkerboard used in our experiment.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.64
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.67
  Name of the first author: Zhouchen Lin
  Name of the last author: Yameng Huang
  Number of Figures: 4
  Number of Tables: 2
  Number of authors: 2
  Paper title: Fast Multidimensional Ellipsoid-Specific Fitting by Alternating Direction
    Method of Multipliers
  Publication Date: 2015-08-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average Runtime (in Seconds) at Different Numbers of Sample
      Points in Different Dimensions
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Success Rate, Mean Reprojection Error (in Pixels), and Mean
      Runtime (in Seconds) in Camera Intrinsic Parameter Estimation
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2469283
