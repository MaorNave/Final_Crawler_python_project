- Affiliation of the first author: department of mathematics, the university of hong
    kong, hong kong, china
  Affiliation of the last author: department of mathematics, the university of hong
    kong, hong kong, china
  Figure 1 Link: articels_figures_by_rev_year\2021\A_Generalized_Framework_for_EdgePreserving_and_StructurePreserving_Image_Smoothi\figure_1.jpg
  Figure 1 caption: Our method is capable of (a) image detail enhancement, (b) clip-art
    compression artifacts removal, (c) guided depth map upsampling and (d) image texture
    removal. These applications are representatives of edge-preserving and structure-preserving
    image smoothing tasks which require contradictive smoothing properties.
  Figure 10 Link: articels_figures_by_rev_year\2021\A_Generalized_Framework_for_EdgePreserving_and_StructurePreserving_Image_Smoothi\figure_10.jpg
  Figure 10 caption: "Comparison of different neighborhood radiuses in our model in\
    \ terms of 8\xD7 guided depth map upsampling. (a) Guidance image and ground-truth\
    \ depth map. Result of our method of the EP & SP mode with (b) r d = r s =1,\u03BB\
    =1.4 and (c) r d = r s =5,\u03BB=0.5 . r represents the value of r d = r s =r\
    \ . (d) MAE comparison under different neighborhood radiuses."
  Figure 2 Link: articels_figures_by_rev_year\2021\A_Generalized_Framework_for_EdgePreserving_and_StructurePreserving_Image_Smoothi\figure_2.jpg
  Figure 2 caption: "Clip-art compression artifacts removal. (a) Input compressed\
    \ JPEG image. Smoothing result of (b) edge-preserving smoother SD filter [7],\
    \ (c) structure-preserving smoother RTV smoothing [8] and (d) our method of the\
    \ simultaneous edge-preserving and structure-preserving mode. Pay attention to\
    \ the difference between the \u201Cblack lines\u201D labeled with the red arrows\
    \ (small structures with strong edges) and the \u201Cshades\u201D labeled with\
    \ the blue arrows (large structures with weak edges) in different results."
  Figure 3 Link: articels_figures_by_rev_year\2021\A_Generalized_Framework_for_EdgePreserving_and_StructurePreserving_Image_Smoothi\figure_3.jpg
  Figure 3 caption: Plots of (a) different penalty functions and (c) the truncated
    Huber penalty function under different parameter settings. Their corresponding
    edge stopping functions are plotted in (b) and (d).
  Figure 4 Link: articels_figures_by_rev_year\2021\A_Generalized_Framework_for_EdgePreserving_and_StructurePreserving_Image_Smoothi\figure_4.jpg
  Figure 4 caption: "1D illustration of real smoothing results. The column labeled\
    \ with the line in each image is plotted. (a) Input. Smoothing result of (b) SD\
    \ filter [7] which adopts the Welschs penalty function for regularization, (c)\
    \ L 0 norm smoothing [5] which approximates L 0 norm with a series of truncated\
    \ L 2 norms for regularization, (d) our method of the EP-2 mode which adopts the\
    \ truncated Huber penalty in Eq. (1) ( a=\u03F5,b> I m ) for regularization, (e)\
    \ our method of the EP-2 mode which adopts the truncated Huber penalty in Eq.\
    \ (1) ( a=\u03F5,b< I m ) for regularization and (f) our method of the EP & SP\
    \ mode which adopts the truncated Huber penalty in Eq. (1) ( a=\u03F5,b< I m )\
    \ for both regularization and data fidelity."
  Figure 5 Link: articels_figures_by_rev_year\2021\A_Generalized_Framework_for_EdgePreserving_and_StructurePreserving_Image_Smoothi\figure_5.jpg
  Figure 5 caption: 1D signal with structures of different scales and amplitudes.
    Smoothing result of (a) TV- L 1 smoothing [31], (b) our method of the SP-1 mode,
    (c) WLS [1] (our method of the EP-1 mode), (d) our method of the SP-2 mode, (e)
    SD filter [7], (f) our method of the EP-2 mode and (g) our method of the EP &
    SP mode.
  Figure 6 Link: articels_figures_by_rev_year\2021\A_Generalized_Framework_for_EdgePreserving_and_StructurePreserving_Image_Smoothi\figure_6.jpg
  Figure 6 caption: Comparison of image texture removal results. (a) Input image.
    Result of (b) TV- L 1 smoothing[31], (e) our method of the SP-1 mode.
  Figure 7 Link: articels_figures_by_rev_year\2021\A_Generalized_Framework_for_EdgePreserving_and_StructurePreserving_Image_Smoothi\figure_7.jpg
  Figure 7 caption: Comparison of intensity shift in Image detail enhancement. (a)
    Input image. Result of (b) WLS [1] (our method of the EP-1 mode) and (c) our method
    of the SP-2 mode. 1D plot of the highlighted region in the result of (d) WLS [1]
    (our method of the EP-1 mode) and (e) our method of the SP-2 mode.
  Figure 8 Link: articels_figures_by_rev_year\2021\A_Generalized_Framework_for_EdgePreserving_and_StructurePreserving_Image_Smoothi\figure_8.jpg
  Figure 8 caption: Comparison of intensity shift in HDR tone mapping. Result of (a)
    WLS [1] (our method of the EP-1 mode) and (b) our method of the SP-2 mode.
  Figure 9 Link: articels_figures_by_rev_year\2021\A_Generalized_Framework_for_EdgePreserving_and_StructurePreserving_Image_Smoothi\figure_9.jpg
  Figure 9 caption: "Comparison between the EP-2 mode and the EP & SP mode of our\
    \ method. 8\xD7 guided depth map upsampling result of (a) our method of the EP-2\
    \ mode and (b) our method of the EP & SP mode. (c) 1D plots of the labeled regions.\
    \ (d) MAE comparison of different upsampling factors."
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Wei Liu
  Name of the last author: Michael Ng
  Number of Figures: 19
  Number of Tables: 6
  Number of authors: 6
  Paper title: A Generalized Framework for Edge-Preserving and Structure-Preserving
    Image Smoothing
  Publication Date: 2021-07-19 00:00:00
  Table 1 caption: TABLE 1 Parameter Settings for Different Tasks
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Quantitative Evaluation of HDR Tone Mapping Results
  Table 3 caption: TABLE 3 Quantitative Comparison of Clip-Art Compression Artifacts
    Removal Results
  Table 4 caption: TABLE 4 Quantitative Comparison on the Noisy Simulated ToF Data
  Table 5 caption: TABLE 5 Quantitative Comparison on Real ToF Data
  Table 6 caption: TABLE 6 Running Time (in seconds) of Different Methods for Different
    Image Sizes
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3097891
- Affiliation of the first author: college of oceanography and space informatics,
    china university of petroleum (east china), qingdao, shandong, china
  Affiliation of the last author: college of oceanography and space informatics, china
    university of petroleum (east china), qingdao, shandong, china
  Figure 1 Link: articels_figures_by_rev_year\2021\EnhancementRegistrationHomogenization_ERH_A_Comprehensive_Underwater_Visual_Reco\figure_1.jpg
  Figure 1 caption: Visual reconstruction of multiple underwater images with poor
    visibility.
  Figure 10 Link: articels_figures_by_rev_year\2021\EnhancementRegistrationHomogenization_ERH_A_Comprehensive_Underwater_Visual_Reco\figure_10.jpg
  Figure 10 caption: Reconstruction result of multiple non-rigid transformation underwater
    images. Our paradigm is compared with AutoStitch, APAP, SPHP, APNAP, and REW on
    six input images.
  Figure 2 Link: articels_figures_by_rev_year\2021\EnhancementRegistrationHomogenization_ERH_A_Comprehensive_Underwater_Visual_Reco\figure_2.jpg
  Figure 2 caption: The structure of the Enhancement-Registration-Homogenization (ERH)
    paradigm, which consists of three procedures, namely E-procedure (enhancement),
    R-procedure (registration), and H-procedure (homogenization). Their cascading
    operation could combine multiple original underwater images to reconstruct a high-quality
    and wide-field image.
  Figure 3 Link: articels_figures_by_rev_year\2021\EnhancementRegistrationHomogenization_ERH_A_Comprehensive_Underwater_Visual_Reco\figure_3.jpg
  Figure 3 caption: E-procedure. The original underwater images are enhanced by two
    steps of color balance compensation and weighted image fusion. At the first step,
    red channel compensation and white balance processing are successively performed
    on the two original underwater images to obtain the color balanced images. At
    the second step, the gamma correction and edge sharpening of the two input images
    are derived from the same color balanced image. Then, they are merged with the
    corresponding normalized weight maps based on the multi-scale fusion strategy.
    The final output of this procedure is two enhanced underwater images.
  Figure 4 Link: articels_figures_by_rev_year\2021\EnhancementRegistrationHomogenization_ERH_A_Comprehensive_Underwater_Visual_Reco\figure_4.jpg
  Figure 4 caption: R-procedure. The registration of the enhanced images is completed
    based on mesh optimization. First, the SIFT algorithm is conducted to complete
    the feature extraction and matching for the enhanced image pairs. After finishing
    the initial alignment, they are partitioned into a series of uniform quadrilaterals
    in the mesh. With the constraints of an objective function, the mesh is continuously
    optimized to achieve accurate registration. The final output of this procedure
    is a registered image with the inhomogeneous transition.
  Figure 5 Link: articels_figures_by_rev_year\2021\EnhancementRegistrationHomogenization_ERH_A_Comprehensive_Underwater_Visual_Reco\figure_5.jpg
  Figure 5 caption: H-procedure. The multi-scale composition is adopted to eliminate
    the inhomogeneous transition in the registered image. First, the construction
    of Gaussian pyramid is to decompose the registered image into pieces of sub-image
    with different resolutions. Then subtract the same size images of the Gaussian
    pyramid and the Upsampling pyramid to get the Laplacian pyramid. Finally, the
    final output of this procedure is a reconstructed image with the natural appearance.
  Figure 6 Link: articels_figures_by_rev_year\2021\EnhancementRegistrationHomogenization_ERH_A_Comprehensive_Underwater_Visual_Reco\figure_6.jpg
  Figure 6 caption: Underwater image enhancement with E-procedure. Each row of the
    left two columns show a pair of overlapping original underwater images, and those
    of the right show their corresponding enhanced results.
  Figure 7 Link: articels_figures_by_rev_year\2021\EnhancementRegistrationHomogenization_ERH_A_Comprehensive_Underwater_Visual_Reco\figure_7.jpg
  Figure 7 caption: 'Reconstruction of two rigid transformation underwater images.
    Our paradigm is compared with AutoStitch, APAP, SPHP, APNAP, and REW on the three
    pairs of underwater rigid images. R: only R-procedure is executed in our paradigm.
    ER: the R-procedure is executed based on the E-procedure. ERH: the three procedures
    operate in a cascade where the former procedure completes and continues to the
    next. The yellow circle marks the distortion or deformation of several methods
    in the reconstructed images and there are inhomogeneous transitions inside the
    rectangle.'
  Figure 8 Link: articels_figures_by_rev_year\2021\EnhancementRegistrationHomogenization_ERH_A_Comprehensive_Underwater_Visual_Reco\figure_8.jpg
  Figure 8 caption: 'Reconstruction of two non-rigid transformation underwater images.
    Our paradigm is compared with AutoStitch, APAP, SPHP, APNAP, and REW on the three
    pairs of underwater rigid images. R: only R-procedure is executed in our paradigm.
    ER: the R-procedure is executed based on the E-procedure. ERH: the three procedures
    operate in a cascade where the former procedure completes and continues to the
    next. The yellow circle marks the distortion or deformation of several methods
    in the reconstructed images and there are inhomogeneous transitions inside the
    rectangle.'
  Figure 9 Link: articels_figures_by_rev_year\2021\EnhancementRegistrationHomogenization_ERH_A_Comprehensive_Underwater_Visual_Reco\figure_9.jpg
  Figure 9 caption: Reconstruction result of multiple rigid transformation underwater
    images. Our paradigm is compared with AutoStitch, APAP, SPHP, APNAP, and REW on
    eight input images.
  First author gender probability: 0.87
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Huajun Song
  Name of the last author: Peng Ren
  Number of Figures: 15
  Number of Tables: 4
  Number of authors: 4
  Paper title: 'Enhancement-Registration-Homogenization (ERH): A Comprehensive Underwater
    Visual Reconstruction Paradigm'
  Publication Date: 2021-07-19 00:00:00
  Table 1 caption: TABLE 1 Underwater Images Enhancement Quantitative Evaluation Based
    on UCIQE [42], UIQM [43], and CCF [44] Metrics
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The Comparison of Feature Points Matching Between Eight
    Pairs of Original and Enhanced Underwater Images
  Table 3 caption: TABLE 3 Quantitative Evaluation of Reconstructing the Rigid Images
    and Non-Rigid Images Based on SIQE Metrics
  Table 4 caption: TABLE 4 Visual Evaluation of Reconstructing the Rigid Images and
    Non-Rigid Images Based on User Survey
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3097804
- Affiliation of the first author: state key lab of cad&cg, zhejiang university, hangzhou,
    china
  Affiliation of the last author: state key lab of cad&cg, zhejiang university, hangzhou,
    china
  Figure 1 Link: articels_figures_by_rev_year\2021\Fast_and_Robust_MultiPerson_D_Pose_Estimation_and_Tracking_From_Multiple_Views\figure_1.jpg
  Figure 1 caption: This work proposes a novel approach for fast and robust recovery
    of 3D poses of multiple people from a few camera views. The main challenge is
    to establish consistent correspondences of 2D observations among multiple views,
    e.g., 2D human-body keypoints in images, which may be noisy and incomplete.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Fast_and_Robust_MultiPerson_D_Pose_Estimation_and_Tracking_From_Multiple_Views\figure_2.jpg
  Figure 2 caption: Overview of the proposed approach. Given images from a few calibrated
    cameras (a), an off-the-shelf human pose detector is used to produce 2D bounding
    boxes and associated 2D poses in each view, which may be inaccurate and incomplete
    (b). Then, the detected bounding boxes are clustered by a novel multi-view matching
    algorithm. Each resulting cluster includes the bounding boxes of the same person
    in different views (c). The isolated bounding boxes that have no matches in other
    views are regarded as false detections and discarded. Finally, the 3D pose of
    each person is reconstructed from the corresponding bounding boxes and associated
    2D poses (d).
  Figure 3 Link: articels_figures_by_rev_year\2021\Fast_and_Robust_MultiPerson_D_Pose_Estimation_and_Tracking_From_Multiple_Views\figure_3.jpg
  Figure 3 caption: An illustration of cycle consistency. The green lines denote a
    set of consistent correspondences and the red lines show a set of inconsistent
    correspondences.
  Figure 4 Link: articels_figures_by_rev_year\2021\Fast_and_Robust_MultiPerson_D_Pose_Estimation_and_Tracking_From_Multiple_Views\figure_4.jpg
  Figure 4 caption: An illustration of the tree-structure skeleton (left) and manifold
    filtering (right).
  Figure 5 Link: articels_figures_by_rev_year\2021\Fast_and_Robust_MultiPerson_D_Pose_Estimation_and_Tracking_From_Multiple_Views\figure_5.jpg
  Figure 5 caption: The effect of different keyframes intervals for tracking by projection
    on the Shelf dataset. The bar plot reports the running time (ms) of different
    modules. The curve shows the accuracy, i.e., the average percentage of correctly
    estimated parts (PCP) of all people.
  Figure 6 Link: articels_figures_by_rev_year\2021\Fast_and_Robust_MultiPerson_D_Pose_Estimation_and_Tracking_From_Multiple_Views\figure_6.jpg
  Figure 6 caption: The comparison of 3D trajectory before and after filtering. The
    horizontal axis denotes the frame index and the vertical axis is the location
    of the left elbow.
  Figure 7 Link: articels_figures_by_rev_year\2021\Fast_and_Robust_MultiPerson_D_Pose_Estimation_and_Tracking_From_Multiple_Views\figure_7.jpg
  Figure 7 caption: Qualitative results on the Shelf (top) and CMU panoptic (bottom)
    datasets. The first row shows the 2D bounding boxes and pose detections. The second
    row shows the result of our matching algorithm where the colors indicate the correspondences
    of bounding boxes across views. The third row shows the 2D projections of the
    estimated 3D poses.
  Figure 8 Link: articels_figures_by_rev_year\2021\Fast_and_Robust_MultiPerson_D_Pose_Estimation_and_Tracking_From_Multiple_Views\figure_8.jpg
  Figure 8 caption: Failure case. The person in the yellow circle is only visible
    in Camera 3. In the matching procedure, its 2D pose is regarded as the outlier
    and thus the corresponding 3D pose is missing.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.69
  Name of the first author: Junting Dong
  Name of the last author: Xiaowei Zhou
  Number of Figures: 8
  Number of Tables: 3
  Number of authors: 7
  Paper title: Fast and Robust Multi-Person 3D Pose Estimation and Tracking From Multiple
    Views
  Publication Date: 2021-07-20 00:00:00
  Table 1 caption: TABLE 1 Ablative Study on the Campus and Shelf Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Quantitative Comparison on the Campus and Shelf Datasets
  Table 3 caption: TABLE 3 Quantitative Comparison of Different Affinity Scores for
    Tracking on the Campus and Shelf Datasets
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3098052
- Affiliation of the first author: department of electrical engineering (esat-stadius),
    ku leuven, leuven, belgium
  Affiliation of the last author: department of electrical engineering (esat-stadius),
    ku leuven, leuven, belgium
  Figure 1 Link: articels_figures_by_rev_year\2021\Random_Features_for_Kernel_Approximation_A_Survey_on_Algorithms_Theory_and_Beyon\figure_1.jpg
  Figure 1 caption: A taxonomy of representative random features based algorithms.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Random_Features_for_Kernel_Approximation_A_Survey_on_Algorithms_Theory_and_Beyon\figure_2.jpg
  Figure 2 caption: Taxonomy of theoretical results on random features.
  Figure 3 Link: articels_figures_by_rev_year\2021\Random_Features_for_Kernel_Approximation_A_Survey_on_Algorithms_Theory_and_Beyon\figure_3.jpg
  Figure 3 caption: "Relationship between the needed assumptions. The notation A\u21D0\
    B means that B is a stronger assumption than A."
  Figure 4 Link: articels_figures_by_rev_year\2021\Random_Features_for_Kernel_Approximation_A_Survey_on_Algorithms_Theory_and_Beyon\figure_4.jpg
  Figure 4 caption: Approximation error, time cost, and test accuracy of various algorithms
    with liblinear on two image classification datasets.
  Figure 5 Link: articels_figures_by_rev_year\2021\Random_Features_for_Kernel_Approximation_A_Survey_on_Algorithms_Theory_and_Beyon\figure_5.jpg
  Figure 5 caption: "Training error, test error, and approximation error of random\
    \ features regression with \u03BB= 10 \u22128 on the sonar dataset with n=208,d=60\
    \ and the sub-set of MNIST (class 1 versus class 2) with n=200,d=784 ."
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Fanghui Liu
  Name of the last author: Johan A. K. Suykens
  Number of Figures: 5
  Number of Tables: 8
  Number of authors: 4
  Paper title: 'Random Features for Kernel Approximation: A Survey on Algorithms,
    Theory, and Beyond'
  Publication Date: 2021-07-26 00:00:00
  Table 1 caption: TABLE 1 Commonly Used Parameters and Symbols
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison of Different Kernel Approximation Methods on
    Space and Time Complexities to Obtain Wx Wx
  Table 3 caption: TABLE 3 Comparison of Convergence Rates and Required Random Features
    for Kernel Approximation Error
  Table 4 caption: TABLE 4 Comparison of Learning Rates and Required Random Features
    for Expected Risk With the Squared Loss Function
  Table 5 caption: TABLE 5 Comparison of Learning Rates and Required Random Features
    for Expected Risk With a Lipschitz Continuous Loss Function
  Table 6 caption: TABLE 6 Dataset Statistics
  Table 7 caption: TABLE 7 Results Statistics on Several Datasets
  Table 8 caption: TABLE 8 Comparison of Problem Settings on Analysis of High Dimensional
    Random Features on Double Descent
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3097011
- Affiliation of the first author: chinese university of hong kong, hong kong
  Affiliation of the last author: chinese university of hong kong, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2021\Cylindrical_and_Asymmetrical_D_Convolution_Networks_for_LiDARBased_Perception\figure_1.jpg
  Figure 1 caption: (a) Range Image (2D projection) versus Cubic Partition versus
    Cylindrical Partition. From top row, it can be found that range image abandons
    the 3D topology, where 2d convolution processes points in different locations
    (far away from each other in green circles). From bottom part, cylindrical partition
    generates the more balanced point distribution than cubic partition (89 versus
    61 percent cells containing points). (b) Applying the regular 3D voxel partition
    and 3D convolution directly (i.e., 3DVoxel) gets limited performance gain compared
    to projection-based (2D) methods [1], [2], [3], [4], while our method achieves
    a remarkable performance gain by further tackling the inherent difficulty of outdoor
    LiDAR point clouds (showing results on SemanticKITTI dataset).
  Figure 10 Link: articels_figures_by_rev_year\2021\Cylindrical_and_Asymmetrical_D_Convolution_Networks_for_LiDARBased_Perception\figure_10.jpg
  Figure 10 caption: An example of multi-scan fusion. (b) and (c) represent the moving
    car and stationary car after the multi-scan fusion, respectively. Note that we
    use three frames to perform the multi-scan fusion.
  Figure 2 Link: articels_figures_by_rev_year\2021\Cylindrical_and_Asymmetrical_D_Convolution_Networks_for_LiDARBased_Perception\figure_2.jpg
  Figure 2 caption: '(1): Top row is the overall framework for LiDAR segmentation.
    Here, LiDAR point cloud is fed into MLP to get the point-wise features and then
    these features are reassigned based on the cylindrical partition. Asymmetrical
    3D convolution networks are then used to generate the voxel-wise outputs. Finally,
    a point-wise module is introduced to refine these outputs. (2): Middle part shows
    the workflow of LiDAR 3D detection, where point-wise refinement is not attended.
    (3): Bottom row elaborates four components, including Asymmetrical Downsample
    block (AD), Asymmetrical Upsample blcok (AU), Asymmetrical residual block (A)
    and Dimension-Decomposition based Context Modeling (DDCM).'
  Figure 3 Link: articels_figures_by_rev_year\2021\Cylindrical_and_Asymmetrical_D_Convolution_Networks_for_LiDARBased_Perception\figure_3.jpg
  Figure 3 caption: The proportion of non-empty cells at different distances between
    cylindrical and cubic partition (The results are calculated on the training set
    of SemanticKITTI). It can be found that cylinder partition makes a higher non-empty
    proportion and more balanced point distribution, especially for farther-away regions.
  Figure 4 Link: articels_figures_by_rev_year\2021\Cylindrical_and_Asymmetrical_D_Convolution_Networks_for_LiDARBased_Perception\figure_4.jpg
  Figure 4 caption: The pipeline of cylindrical partition. We first transform the
    Cartesian coordinate to Cylinder coordinate and then assign the point-wise features
    to the structured representation based on the Point-Cylinder mapping table. The
    cylindrical features are obtained via max-pooling these point-wise features inside
    each cylinder.
  Figure 5 Link: articels_figures_by_rev_year\2021\Cylindrical_and_Asymmetrical_D_Convolution_Networks_for_LiDARBased_Perception\figure_5.jpg
  Figure 5 caption: "Filter activation visualization (regular versus asymmetrical).\
    \ We visualize some filters at second downsampling block from regular 3D convolution\
    \ networks (with cubic partition) (Figs. 5a and 5b) and asymmetrical 3D convolution\
    \ networks (with cylindrical partition) (Figs. 5c and 5d), respectively. Since\
    \ each filter in 3D convolution networks has size of H\xD7W\xD7L, we use the mean\
    \ value of L dimension as the activation value. Fig (a) and (c) are extracted\
    \ from 100th filter and Fig (b) and (d) are extracted from 120th filter. It can\
    \ be observed that the filters from the proposed asymmetrical 3D convolution networks\
    \ are sparsely activated in some certain regions while the regular convolution\
    \ covers most parts."
  Figure 6 Link: articels_figures_by_rev_year\2021\Cylindrical_and_Asymmetrical_D_Convolution_Networks_for_LiDARBased_Perception\figure_6.jpg
  Figure 6 caption: An illustration of asymmetrical residual block, where two asymmetrical
    kernels are stacked to power the skeleton. It can be observed that asymmetrical
    residual block focuses on the horizontal and vertical kernels.
  Figure 7 Link: articels_figures_by_rev_year\2021\Cylindrical_and_Asymmetrical_D_Convolution_Networks_for_LiDARBased_Perception\figure_7.jpg
  Figure 7 caption: Upper bound of mIoU with different label encoding methods (i.e.,
    majority and minority encoding). It can be found that no matter what encoding
    methods are, the information loss always occurs, which is also the reason for
    point-wise refinement.
  Figure 8 Link: articels_figures_by_rev_year\2021\Cylindrical_and_Asymmetrical_D_Convolution_Networks_for_LiDARBased_Perception\figure_8.jpg
  Figure 8 caption: Detailed workflow of MLPs in Cylindrical Partition and Point-wise
    Refinement Module.
  Figure 9 Link: articels_figures_by_rev_year\2021\Cylindrical_and_Asymmetrical_D_Convolution_Networks_for_LiDARBased_Perception\figure_9.jpg
  Figure 9 caption: Visualization of single-scan semantic segmentation on SemanticKITTI
    validation set. The left is ground-truth and right is our prediction.
  First author gender probability: 0.61
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.89
  Name of the first author: Xinge Zhu
  Name of the last author: Dahua Lin
  Number of Figures: 13
  Number of Tables: 15
  Number of authors: 9
  Paper title: Cylindrical and Asymmetrical 3D Convolution Networks for LiDAR-Based
    Perception
  Publication Date: 2021-07-26 00:00:00
  Table 1 caption: TABLE 1 Results of Our Proposed Method and State-of-the-Art LiDAR
    Segmentation Methods on SemanticKITTI Single-Scan Test Set
  Table 10 caption: TABLE 10 Ablation Studies for Network Components on SemanticKITTI
    Validation Set
  Table 2 caption: TABLE 2 Results of Our Proposed Method and Other LiDAR Segmentation
    Methods on nuScenes Validation Set
  Table 3 caption: TABLE 3 Results of Our Proposed Method and State-of-the-Art LiDAR
    Segmentation Methods on SemanticKITTI Multi-Scan Test Set - Part I
  Table 4 caption: TABLE 4 Results of Our Proposed Method and State-of-the-Art LiDAR
    Segmentation Methods on SemanticKITTI Multi-Scan Test Set - Part II
  Table 5 caption: TABLE 5 Results of Our Proposed Method and Other Methods on A2D2
    Dataset - Part I
  Table 6 caption: TABLE 6 Results of Our Proposed Method and Other Methods on A2D2
    Dataset - Part II
  Table 7 caption: TABLE 7 LiDAR-Based Panoptic Segmentation Results on the Validation
    Set of SemanticKITTI
  Table 8 caption: TABLE 8 LiDAR-Based 3D Detection Results on nuScenes Dataset
  Table 9 caption: TABLE 9 LiDAR-Based 3D Detection Results on Waymo Validation Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3098789
- Affiliation of the first author: institute of digital media, peking university,
    beijing, china
  Affiliation of the last author: institute of digital media, peking university, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2021\Disentangled_Feature_Learning_Network_and_a_Comprehensive_Benchmark_for_Vehicle_\figure_1.jpg
  Figure 1 caption: Our VERI-Wild 2.0 dataset is collected from a large-scale surveillance
    system consisting of 274 cameras covering a city district of more than 200 k m
    2 .
  Figure 10 Link: articels_figures_by_rev_year\2021\Disentangled_Feature_Learning_Network_and_a_Comprehensive_Benchmark_for_Vehicle_\figure_10.jpg
  Figure 10 caption: Illustration of the top 6 ReID results. Three examples of the
    top 6 retrieval results are shown in (a), (b), and (c). The comparison results
    of different representations are shown in (d). The green and red boxes indicate
    the correct and wrong results, respectively.
  Figure 2 Link: articels_figures_by_rev_year\2021\Disentangled_Feature_Learning_Network_and_a_Comprehensive_Benchmark_for_Vehicle_\figure_2.jpg
  Figure 2 caption: When matching the pairs captured from the same or similar orientations
    (a and b), the specific details (labelled with yellow boxes) are important cues
    for vehicle matching and discrimination. However, these details often become useless
    when matching the pairs captured from different orientations (a and c), and in
    this case, the vehicles common characteristics, such as the vehicle design styles,
    become quite important for ReID.
  Figure 3 Link: articels_figures_by_rev_year\2021\Disentangled_Feature_Learning_Network_and_a_Comprehensive_Benchmark_for_Vehicle_\figure_3.jpg
  Figure 3 caption: Compared to the existing VehicleID [27], VeRI-776 [29], and CityFlow
    datasets [49], our collected VERI-Wild 2.0 dataset covers many more challenging
    factors for vehicle ReID in the wild, e.g., significant viewpoint and illumination
    variations, various backgrounds, severe occlusions, and abundant visually similar
    yet different samples.
  Figure 4 Link: articels_figures_by_rev_year\2021\Disentangled_Feature_Learning_Network_and_a_Comprehensive_Benchmark_for_Vehicle_\figure_4.jpg
  Figure 4 caption: The statistics of VERI-Wild 2.0, including the distributions of
    the image size, sample captured time, vehicle colors, types, and brands.
  Figure 5 Link: articels_figures_by_rev_year\2021\Disentangled_Feature_Learning_Network_and_a_Comprehensive_Benchmark_for_Vehicle_\figure_5.jpg
  Figure 5 caption: "Illustration of our vehicle ReID framework. (a) The overall architecture\
    \ of DFNet. The orientation common feature is generated by the generator( G )\
    \ via \u201COdd-One-Out\u201D adversary, and the specific feature is obtained\
    \ by the attention scheme. (b) Odd-One-Out\u201D adversarial learning is implemented\
    \ by the adversary between the cross-entropy loss in D and the uniform loss in\
    \ G . Note that, G is the common feature generator, consisting of the backbone\
    \ Resnet50 and the common branch. (c) Distance alignment module aligns the distance\
    \ distribution between two disentangled features."
  Figure 6 Link: articels_figures_by_rev_year\2021\Disentangled_Feature_Learning_Network_and_a_Comprehensive_Benchmark_for_Vehicle_\figure_6.jpg
  Figure 6 caption: At the testing stage, we use the designed adaptive matching scheme
    to get the hybrid similarities, on which the feature distance alignment and adaptive
    weights calculation mechanisms are performed. The adaptive matching scheme can
    adaptively emphasize different features for different matching pairs to achieve
    better performance.
  Figure 7 Link: articels_figures_by_rev_year\2021\Disentangled_Feature_Learning_Network_and_a_Comprehensive_Benchmark_for_Vehicle_\figure_7.jpg
  Figure 7 caption: Distance distributions of the common features (red solid curves),
    the specific features (green solid curves), and the aligned specific feature distance
    distribution (green dashed curves) that is converted towards the common feature
    distance using the proposed alignment module. We can observe that the aligned
    specific feature distance distribution well fits the common feature distance distribution.
    The Kullback-Leibler Divergence (KLD) between the two distributions is 0.003 only,
    demonstrating the effectiveness of the proposed distance alignment method.
  Figure 8 Link: articels_figures_by_rev_year\2021\Disentangled_Feature_Learning_Network_and_a_Comprehensive_Benchmark_for_Vehicle_\figure_8.jpg
  Figure 8 caption: Visualization of the response maps generated by the attention
    module.
  Figure 9 Link: articels_figures_by_rev_year\2021\Disentangled_Feature_Learning_Network_and_a_Comprehensive_Benchmark_for_Vehicle_\figure_9.jpg
  Figure 9 caption: "The discriminators classification accuracy in \u201COdd-One-Out\u201D\
    \ adversarial learning. After 20 epochs, the generator produces common features\
    \ with increasingly better quality, which can fool the discriminator and degrade\
    \ its accuracy."
  First author gender probability: 0.76
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Yan Bai
  Name of the last author: Ling-Yu Duan
  Number of Figures: 10
  Number of Tables: 15
  Number of authors: 5
  Paper title: Disentangled Feature Learning Network and a Comprehensive Benchmark
    for Vehicle Re-Identification
  Publication Date: 2021-07-26 00:00:00
  Table 1 caption: "TABLE 1 Comparisons Between the Preliminary Dataset Version \u201C\
    VERI-Wild\u201D, and the Extended Version \u201CVERI-Wild 2.0\u201D"
  Table 10 caption: TABLE 10 Subsets Split for ViewpointIlluminationWeather Analysis
  Table 2 caption: TABLE 2 Comparisons Among the VehicleID [27], VeRI-776 [29], CityFlow
    [49], and Our Newly Collected VERI-Wild 2.0 Dataset for Vehicle ReID
  Table 3 caption: TABLE 3 The Splitting for the Subsets of the Test Set (Sample Number)
  Table 4 caption: TABLE 4 Performance on the VERI-Wild (a) and VERI-Wild 2.0 (b)
    Datasets
  Table 5 caption: TABLE 5 Performance on the VehicleID Dataset
  Table 6 caption: TABLE 6 Performance on the VeRI-776 Dataset
  Table 7 caption: TABLE 7 Ablation Study of Common and Specific Feature
  Table 8 caption: TABLE 8 Common Feature Performance (mAP) of Adopting Different
    Adversarial Strategies
  Table 9 caption: TABLE 9 Performance (mAP) of Adopting Different Distance Alignment
    Module
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3099253
- Affiliation of the first author: school of engineering, westlake university, hangzhou,
    zhejiang, china
  Affiliation of the last author: department of automation, tsinghua university, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2021\PlugandPlay_Algorithms_for_Video_Snapshot_Compressive_Imaging\figure_1.jpg
  Figure 1 caption: "Schematic of a color video SCI system and its snapshot measurement\
    \ (showing in Bayer RGB mode). A \u201CRGGB\u201D Bayer pattern is shown."
  Figure 10 Link: articels_figures_by_rev_year\2021\PlugandPlay_Algorithms_for_Video_Snapshot_Compressive_Imaging\figure_10.jpg
  Figure 10 caption: "Real data: chopper wheel ( 256\xD7256\xD714 )."
  Figure 2 Link: articels_figures_by_rev_year\2021\PlugandPlay_Algorithms_for_Video_Snapshot_Compressive_Imaging\figure_2.jpg
  Figure 2 caption: Trade-off of quality and speed of various plug-and-play denoising
    algorithms for SCI reconstruction. Average PSNR of the six grays-scale datasets
    [50] are shown.
  Figure 3 Link: articels_figures_by_rev_year\2021\PlugandPlay_Algorithms_for_Video_Snapshot_Compressive_Imaging\figure_3.jpg
  Figure 3 caption: Reconstruction of color SCI using mosaic sensor measurements.
    (a) Color SCI reconstruction by independently reconstruct RGGB channels using
    grayscale image denoising and then perform demosicing (we proposed this in [50]).
    The raw measurement (and the mask) is divided into four color channels, R (red),
    G1 (green), G2 (green) and B (blue) and these channels are reconstructed separately
    using the PnP-GAP with FFDNet. Then these channels are interleaved and demosaiced
    to obtain the final color video. (b) Proposed (in this paper) joint reconstruction
    and demosaicing for color SCI. The raw measurement (and the mask) is sent to the
    proposed PnP framework using GAPADMM with color denoising by FFDNet or FastDVDnet
    to output the desired color video directly. Note the demosaicing and color video
    denoising are embedded in each iteration.
  Figure 4 Link: articels_figures_by_rev_year\2021\PlugandPlay_Algorithms_for_Video_Snapshot_Compressive_Imaging\figure_4.jpg
  Figure 4 caption: "Demonstration of the solution of ADMM and GAP for a two-dimensional\
    \ sparse signal, where x \u2217 denotes the truth. (a) In the noise-free case,\
    \ both ADMM and GAP have a large chance to converge to the true signal. (b) In\
    \ the noisy case, GAP will converge to the green dot (the cross-point of dash-green\
    \ line and vertical axis), whereas the solution of ADMM will be one of the two\
    \ red dots that the red circle crosses the vertical axis."
  Figure 5 Link: articels_figures_by_rev_year\2021\PlugandPlay_Algorithms_for_Video_Snapshot_Compressive_Imaging\figure_5.jpg
  Figure 5 caption: "Comparison of reconstructed frames of different PnP-GAP algorithms\
    \ (GAP-TV [23], DeSCI [25], PnP-FFDNet [50], and PnP-FastDVDnet) on six simulated\
    \ grayscale video SCI datasets of spatial size 256\xD7256 and B=8 ."
  Figure 6 Link: articels_figures_by_rev_year\2021\PlugandPlay_Algorithms_for_Video_Snapshot_Compressive_Imaging\figure_6.jpg
  Figure 6 caption: "Comparison of reconstructed frames of PnP-GAP algorithms (GAP-TV\
    \ [23], DeSCI [25], PnP-FFDNet [50], and PnP-FastDVDnet) on six simulated benchmark\
    \ color video SCI datasets of size 512\xD7512\xD73 and B=8 . Please refer to the\
    \ full videos in the supplementary material, which can be found on the Computer\
    \ Society Digital Library at http:doi.ieeecomputersociety.org10.1109TPAMI.2021.3099035."
  Figure 7 Link: articels_figures_by_rev_year\2021\PlugandPlay_Algorithms_for_Video_Snapshot_Compressive_Imaging\figure_7.jpg
  Figure 7 caption: Reconstructed frames of PnP-GAP algorithms (GAP-TV [23], PnP-FFDNet
    [50], and PnP-FastDVDnet) on four simulated large-scale video SCI datasets. Please
    refer to the full videos in the supplementary material, available online.
  Figure 8 Link: articels_figures_by_rev_year\2021\PlugandPlay_Algorithms_for_Video_Snapshot_Compressive_Imaging\figure_8.jpg
  Figure 8 caption: "Reconstruction quality by different settings of the noise level\
    \ \u03C3 k for the proposed PnP-FastDVDnet on the first measurement of the Beauty\
    \ data. Left: initialized by 0, right: initialized by GAP-TV. We assume the maximum\
    \ pixel value being 255 and iter in the () of the legend denotes iterations."
  Figure 9 Link: articels_figures_by_rev_year\2021\PlugandPlay_Algorithms_for_Video_Snapshot_Compressive_Imaging\figure_9.jpg
  Figure 9 caption: Reconstruction PSNR in dB (a) and SSIM (b), varying compression
    rates B from 8 to 48 of the proposed PnP methods (PnP-FFDNet and PnP-FastDVDnet)
    and GAP-TV [23] .
  First author gender probability: 0.6
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.76
  Name of the first author: Xin Yuan
  Name of the last author: Qionghai Dai
  Number of Figures: 13
  Number of Tables: 4
  Number of authors: 5
  Paper title: Plug-and-Play Algorithms for Video Snapshot Compressive Imaging
  Publication Date: 2021-07-26 00:00:00
  Table 1 caption: 'TABLE 1 Grayscale Benchmark Dataset: The Average Results of PSNR
    in dB (Left Entry in Each Cell) and SSIM (Right Entry in each cell) and Run Time
    Per MeasurementShot in Minutes by Different Algorithms on 6 Benchmark Datasets'
  Table 10 caption: Not Available
  Table 2 caption: 'TABLE 2 Mid-Scale Bayer Benchmark Dataset: The Average Results
    of PSNR in dB (Left Entry in Each Cell) and SSIM (Right Entry) and Running Time
    Per MeasurementShot in Minutes by Different Algorithms on 6 Benchmark Color Bayer
    Datasets'
  Table 3 caption: TABLE 3 Running Time (minutes) of Large-Scale Data Using Different
    Algorithms
  Table 4 caption: TABLE 4 Running Time (Seconds) of Real Data Using Different Algorithms
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3099035
- Affiliation of the first author: jd explore academy, china
  Affiliation of the last author: jd explore academy, china
  Figure 1 Link: articels_figures_by_rev_year\2021\EndEnd_Occluded_Face_Recognition_by_Masking_Corrupted_Features\figure_1.jpg
  Figure 1 caption: The upper part visualizes nine different occlusions on face images.
    The lower part demonstrates the accuracy comparison between baseline and our method.
    Note that the right legends indicate the occluded degrees of different occlusions
    (e.g., (4)-0.12 represents that the area of blocking mouth is 12 percent of the
    total face image area).
  Figure 10 Link: articels_figures_by_rev_year\2021\EndEnd_Occluded_Face_Recognition_by_Masking_Corrupted_Features\figure_10.jpg
  Figure 10 caption: "The values of TAR (True Accepted Rate) when FAR (False Accepted\
    \ Rate) is 1e\u22124 on LFW and \u201COcc-LFW-S\u201D with S \u2208[1.0,2.0,3.0]\
    \ from FC, 2D and 3D. FROM is corresponding to \u201C3D\u201D."
  Figure 2 Link: articels_figures_by_rev_year\2021\EndEnd_Occluded_Face_Recognition_by_Masking_Corrupted_Features\figure_2.jpg
  Figure 2 caption: The pipeline of the proposed FROM. FROM first takes a mini-batch
    which consists of different random occluded and occlusion-free (not paired) face
    images as input, and generates a feature pyramid (including X 1 , X 2 , X 3 ).
    Then X 3 is used to decode the masks, which are later applied to X 1 to mask out
    the corrupted feature elements for the final recognition. We also propose to leverage
    the occlusion patterns as the extra supervision to guide the feature masks learning.
    The whole network is trained end-to-end.
  Figure 3 Link: articels_figures_by_rev_year\2021\EndEnd_Occluded_Face_Recognition_by_Masking_Corrupted_Features\figure_3.jpg
  Figure 3 caption: Architectures of MD and OPP. (a) MD represents the Mask Decoder.
    (b) OPP means the Occluded Pattern Predictor.
  Figure 4 Link: articels_figures_by_rev_year\2021\EndEnd_Occluded_Face_Recognition_by_Masking_Corrupted_Features\figure_4.jpg
  Figure 4 caption: "Examples of proximate occlusion patterns (left) and the number\
    \ of patterns for each occlusion size (right) when the face image is divided into\
    \ 4\xD74 blocks. The value of (i,j) location in the numerical matrix means the\
    \ number of occlusion patterns with size i\xD7j ."
  Figure 5 Link: articels_figures_by_rev_year\2021\EndEnd_Occluded_Face_Recognition_by_Masking_Corrupted_Features\figure_5.jpg
  Figure 5 caption: Examples of input face images and their occlusions predicted by
    our FROM, where orange color indicates the occluded area. The first row is from
    Occ-WebFace, and the last row images are from Occ-LFW-1.5. By decoding the correct
    masks of the corresponding occlusion, corrupted features can be effectively removed.
  Figure 6 Link: articels_figures_by_rev_year\2021\EndEnd_Occluded_Face_Recognition_by_Masking_Corrupted_Features\figure_6.jpg
  Figure 6 caption: Illuminations of occluders we employ to construct the occluded
    face datasets. All of them are the most common objects that may block the face,
    including sunglasses, scarf, face mask, hand, eye mask, eyeglasses, book, phone,
    and cup.
  Figure 7 Link: articels_figures_by_rev_year\2021\EndEnd_Occluded_Face_Recognition_by_Masking_Corrupted_Features\figure_7.jpg
  Figure 7 caption: Examples of the AR Face dataset and our synthetically occluded
    face images. The first row images are occluded with random scale from 1:0.5:5
    on WebFace. The second and third rows images are occluded with 1.0,1.5,2.0 on
    LFW and Facescrub, respectively. The last row images are from the AR Face dataset.
  Figure 8 Link: articels_figures_by_rev_year\2021\EndEnd_Occluded_Face_Recognition_by_Masking_Corrupted_Features\figure_8.jpg
  Figure 8 caption: "The values of TAR (True Accepted Rate) when FAR (False Accepted\
    \ Rate) is 1e\u22124 on LFW and \u201COcc-LFW-S\u201D with S \u2208[1.0,2.0,3.0]\
    \ ."
  Figure 9 Link: articels_figures_by_rev_year\2021\EndEnd_Occluded_Face_Recognition_by_Masking_Corrupted_Features\figure_9.jpg
  Figure 9 caption: Visualization of feature distributions by converting 512D to 2D
    with t -SNE [36] and following normalization. Different markers with color represent
    different classes. Zoom in for better view.
  First author gender probability: 0.55
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Haibo Qiu
  Name of the last author: Dacheng Tao
  Number of Figures: 14
  Number of Tables: 11
  Number of authors: 5
  Paper title: End2End Occluded Face Recognition by Masking Corrupted Features
  Publication Date: 2021-07-26 00:00:00
  Table 1 caption: TABLE 1 Comparison Between PDSN [17] and FROM in Terms of FLOPs
    and Parameters
  Table 10 caption: TABLE 10 Face Identification Accuracy(%) Under Protocol Small
    on MF1
  Table 2 caption: "TABLE 2 Face Verification Accuracy (%) Comparisons Between no\
    \ Binarization and Binarization With Different Thresholds Under K=5,\u03BB=1.0\
    \ K=5,\u03BB=1.0"
  Table 3 caption: "TABLE 3 Face Identification Accuracies(%) for Different Values\
    \ of K K on MF1 and \u201COcc-MF1-S\u201D With S \u2208[1.0,1.5,2.0] \u2208[1.0,1.5,2.0]"
  Table 4 caption: "TABLE 4 Face Identification Accuracies(%) for Different Values\
    \ of \u03BB \u03BB (i.e., the Weight Coefficient of Eq. (6)) on MF1 and \u201C\
    Occ-MF1-S\u201D With S \u2208[1.0,1.5,2.0] \u2208[1.0,1.5,2.0]"
  Table 5 caption: 'TABLE 5 Comparison Between Three Baselines (Note: BN=Backbone
    Network, AUG=Occluded Data Augmentation, MD=Mask Decoder, OPP=Occlusion Pattern
    Predictor) and FROM'
  Table 6 caption: "TABLE 6 Face Identification Accuracies(%) on \u201COcc-MF1\u201D\
    \ With Nine Different Occluded Areas of Face Images"
  Table 7 caption: "TABLE 7 Face Identification Accuracies(%) of Different Masks Locations\
    \ and Dimensions on MF1 and \u201COcc-MF1-S\u201D With S \u2208[1.0,1.5,2.0] \u2208\
    [1.0,1.5,2.0]"
  Table 8 caption: "TABLE 8 Comparison of TAR (True Accepted Rate) When FAR (False\
    \ Accepted Rate) is Set to 1e\u22123 1e-3 Between the Regression Loss and Our\
    \ FROM"
  Table 9 caption: TABLE 9 Face Verification Comparison ( % %) on RMF2, LFW-SM and
    OLFW
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3098962
- Affiliation of the first author: college of biomedical engineering and instrument
    science, zhejiang university, hangzhou, zhejiang, china
  Affiliation of the last author: tencent data platform, shenzhen, guangdong, china
  Figure 1 Link: articels_figures_by_rev_year\2021\AlphaGAN_Fully_Differentiable_Architecture_Search_for_Generative_Adversarial_Net\figure_1.jpg
  Figure 1 caption: "Searching the operations and the intermediate latent W i i=1,\u2026\
    ,n of StyleGAN2. \u201CtRGB2\u201D, \u201CtRGB4\u201D, and \u201CtRGB(m+1)\u201D\
    \ denote the skip connection layers in the original StyleGAN2 [6], which are conv1x1\
    \ operations. The number (e.g., 2) after the underscore in \u201CtRGB2\u201D denotes\
    \ that \u201CtRGB2\u201D receives the latent W i identical to the latent received\
    \ by the convolutional layer L 2 ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\AlphaGAN_Fully_Differentiable_Architecture_Search_for_Generative_Adversarial_Net\figure_2.jpg
  Figure 2 caption: The topology of the generator and the cell in conventional GANs.
  Figure 3 Link: articels_figures_by_rev_year\2021\AlphaGAN_Fully_Differentiable_Architecture_Search_for_Generative_Adversarial_Net\figure_3.jpg
  Figure 3 caption: Tracking architectures during searching. alphaGAN (s) is denoted
    by blue color with plus marker and alphaGAN (l) is denoted by red color with triangle
    marker.
  Figure 4 Link: articels_figures_by_rev_year\2021\AlphaGAN_Fully_Differentiable_Architecture_Search_for_Generative_Adversarial_Net\figure_4.jpg
  Figure 4 caption: Duality gap of alphaGAN (l) and alphaGAN (s) during searching.
  Figure 5 Link: articels_figures_by_rev_year\2021\AlphaGAN_Fully_Differentiable_Architecture_Search_for_Generative_Adversarial_Net\figure_5.jpg
  Figure 5 caption: The generated images of the models with best FID on FFHQ, LSUN-church,
    and CelebA. We select the images with cherry picking.
  Figure 6 Link: articels_figures_by_rev_year\2021\AlphaGAN_Fully_Differentiable_Architecture_Search_for_Generative_Adversarial_Net\figure_6.jpg
  Figure 6 caption: The generated images with different intermediate latent mathcal
    Wi=1,2,3,4,5,6,7,8 under the same latent zconst and zvar. mathcal W8 is the output
    of the last fully-connected layer in the mapping network. We exploit the pretrained
    model (trained with 25000K images) released by the authors of StyleGAN2 [6].
  Figure 7 Link: articels_figures_by_rev_year\2021\AlphaGAN_Fully_Differentiable_Architecture_Search_for_Generative_Adversarial_Net\figure_7.jpg
  Figure 7 caption: "FID curve of the searched architecture and the original architecture\
    \ during re-training. We present the curves of re-training. \u201CCommon architecture\u201D\
    \ denotes the original architecture exploited by StyleGAN2."
  Figure 8 Link: articels_figures_by_rev_year\2021\AlphaGAN_Fully_Differentiable_Architecture_Search_for_Generative_Adversarial_Net\figure_8.jpg
  Figure 8 caption: "The curve of duality gap during searching StyleGAN2 on CelebA,\
    \ LSUN-church, and FFHQ under \u201Cmicro + dynamic leftlbrace mathcal Wirightrbrace\
    \ \u201D paradigm, respectively."
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.74
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.73
  Name of the first author: Yuesong Tian
  Name of the last author: Wei Liu
  Number of Figures: 8
  Number of Tables: 8
  Number of authors: 6
  Paper title: 'AlphaGAN: Fully Differentiable Architecture Search for Generative
    Adversarial Networks'
  Publication Date: 2021-07-26 00:00:00
  Table 1 caption: ''
  Table 10 caption: Not Available
  Table 2 caption: ''
  Table 3 caption: ''
  Table 4 caption: ''
  Table 5 caption: ''
  Table 6 caption: ''
  Table 7 caption: ''
  Table 8 caption: ''
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3099829
- Affiliation of the first author: shenzhen key lab of computer vision and pattern
    recognition, siat-sensetime joint lab, shenzhen institutes of advanced technology,
    chinese academy of sciences, beijing, china
  Affiliation of the last author: shenzhen key lab of computer vision and pattern
    recognition, siat-sensetime joint lab, shenzhen institutes of advanced technology,
    chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\RankSRGAN_Super_Resolution_Generative_Adversarial_Networks_With_Learning_to_Rank\figure_1.jpg
  Figure 1 caption: "The comparison of RankSRGAN and the state-of-the-art perceptual\
    \ SR methods on \xD74. NIQE: lower is better. PSNR: higher is better."
  Figure 10 Link: articels_figures_by_rev_year\2021\RankSRGAN_Super_Resolution_Generative_Adversarial_Networks_With_Learning_to_Rank\figure_10.jpg
  Figure 10 caption: "The comparison of RankSRGAN and the state-of-the-art perceptual\
    \ SR methods on \xD74. NIQE: lower is better."
  Figure 2 Link: articels_figures_by_rev_year\2021\RankSRGAN_Super_Resolution_Generative_Adversarial_Networks_With_Learning_to_Rank\figure_2.jpg
  Figure 2 caption: 'Overview of the proposed method. Stage 1: Generate pair-wise
    rank images by different SR models in the orientation of perceptual metrics. Stage
    2: Train Siamese-like Ranker network. Stage 3: Introduce rank-content loss derived
    from well-trained Ranker to guide GAN training. RankSRGAN consists of a generator(G),
    discriminator(D), a fixed Feature extractor(F) and Ranker(R).'
  Figure 3 Link: articels_figures_by_rev_year\2021\RankSRGAN_Super_Resolution_Generative_Adversarial_Networks_With_Learning_to_Rank\figure_3.jpg
  Figure 3 caption: The NIQE of RankSRGAN-MR exceeds that of SRGAN, ESRGAN and RankSRGAN-MC.
  Figure 4 Link: articels_figures_by_rev_year\2021\RankSRGAN_Super_Resolution_Generative_Adversarial_Networks_With_Learning_to_Rank\figure_4.jpg
  Figure 4 caption: The upper bound (average NIQE value) of SRGAN, ESRGAN, model rank
    and model classification.
  Figure 5 Link: articels_figures_by_rev_year\2021\RankSRGAN_Super_Resolution_Generative_Adversarial_Networks_With_Learning_to_Rank\figure_5.jpg
  Figure 5 caption: 'The comparison of RankSRGAN and the state-of-the-art perceptual
    SR methods. NIQE: lower is better.'
  Figure 6 Link: articels_figures_by_rev_year\2021\RankSRGAN_Super_Resolution_Generative_Adversarial_Networks_With_Learning_to_Rank\figure_6.jpg
  Figure 6 caption: Image space visualization. A generative network can yield different
    texture results by different constraints. In the standard SRGAN, VGG54 loss with
    high-level feature tends to generate better textual details while VGG22 loss with
    low-level features will produce to more smooth output. ESRGAN employs the relativistic
    discriminator to expand the solution space and generates SR results with better
    perceptual scores. However, existing methods just provide a single objective target.
    In the proposed RankSRGAN, we introduce Ranker to provide stage-wise guidance
    for generator. The Ranker could predict the scores of middle stages during the
    training procedure. Furthermore, the ranking score is defined as a rank-content
    loss to constrain the generator in the orientation of perceptual metrics.
  Figure 7 Link: articels_figures_by_rev_year\2021\RankSRGAN_Super_Resolution_Generative_Adversarial_Networks_With_Learning_to_Rank\figure_7.jpg
  Figure 7 caption: The effects of Ranker with different distortions. (a) Different
    rank datasets with image distortion (blur and noise), SR and interpolation. For
    rank dataset with SR, we employ NIQE as the perceptual metric to get the order
    of pair-wise images. For rank datasets with blur, noise and image interpolation,
    we directly use degradation levels and interpolation coefficients to generate
    rank labels. The well-trained Ranker with different rank datasets can generate
    images with different perceptual characteristics. (b) The convergence curves of
    RankSRGAN with different Rankers.
  Figure 8 Link: articels_figures_by_rev_year\2021\RankSRGAN_Super_Resolution_Generative_Adversarial_Networks_With_Learning_to_Rank\figure_8.jpg
  Figure 8 caption: 'The visual results of RankSRGAN with different rank dataset.
    [Rn, Rb, Rs, Ri]: Ranker trained by rank dataset with [noise distortion, blur
    distortion, image SR and image interpolation].'
  Figure 9 Link: articels_figures_by_rev_year\2021\RankSRGAN_Super_Resolution_Generative_Adversarial_Networks_With_Learning_to_Rank\figure_9.jpg
  Figure 9 caption: The histograms of NIQE score of GT, SRGAN, ESRGAN and RankSRGAN
    (ours). These graphs illustrate that RankSRGAN successfully achieves the best
    NIQE results. (Better viewed in color version).
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.53
  Name of the first author: Wenlong Zhang
  Name of the last author: Yu Qiao
  Number of Figures: 24
  Number of Tables: 8
  Number of authors: 4
  Paper title: 'RankSRGAN: Super Resolution Generative Adversarial Networks With Learning
    to Rank'
  Publication Date: 2021-07-26 00:00:00
  Table 1 caption: TABLE 1 The Performance of Super-Resolved Results With Three Perceptual
    Levels in PIRM-Test [20]
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The Performance of Ranker With Different Network Architectures
  Table 3 caption: TABLE 3 Average NIQE [21], PI [20] and PSNR Values on the Set14
    [59], BSD100 [60] and PIRM-Test [20]
  Table 4 caption: TABLE 4 Comparison With RankSRGAN and RankSRGAN-HR
  Table 5 caption: TABLE 5 The Distance Between S R 1 SR1 and S R 2 SR2 With Regression
    and Rank
  Table 6 caption: TABLE 6 The Performance of RankSRGAN With Different Rankers
  Table 7 caption: TABLE 7 The Performance of RankSRGAN With Different Rankers
  Table 8 caption: 'TABLE 8 The Performance of RankSRGAN With the Combination of Loss
    Functions (P: Perceptual Loss, R: Rank-Content Loss, M: MSE Loss)'
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3096327
