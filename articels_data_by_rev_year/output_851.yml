- Affiliation of the first author: department of electrical and computer engineering,
    university of california, san diego, la jolla, ca
  Affiliation of the last author: department of cognitive science, university of california,
    san diego, la jolla, ca
  Figure 1 Link: articels_figures_by_rev_year\2017\Generalizing_Pooling_Functions_in_CNNs_Mixed_Gated_and_Tree\figure_1.jpg
  Figure 1 caption: "Illustration of proposed \u201Cmixed\u201D max-average pooling\
    \ operations. x is referred to as an input and \u03B1 is the parameter balancing\
    \ the importance of the max pooling and the average pooling operations."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Generalizing_Pooling_Functions_in_CNNs_Mixed_Gated_and_Tree\figure_2.jpg
  Figure 2 caption: "Illustration of proposed \u201Cgated\u201D max-average pooling\
    \ operations. x is referred to as an input and \u03C9 denotes the gating function\
    \ balancing the importance of the max pooling and the average pooling operations."
  Figure 3 Link: articels_figures_by_rev_year\2017\Generalizing_Pooling_Functions_in_CNNs_Mixed_Gated_and_Tree\figure_3.jpg
  Figure 3 caption: "Illustration of proposed tree pooling operation (3 levels in\
    \ this figure). We indicate the input being pooled by x , gating masks by \u03C9\
    \ , and pooling filters by v (subscripted as appropriate)."
  Figure 4 Link: articels_figures_by_rev_year\2017\Generalizing_Pooling_Functions_in_CNNs_Mixed_Gated_and_Tree\figure_4.jpg
  Figure 4 caption: Controlled experiment on CIFAR10 investigating the relative benefit
    of selected pooling operations in terms of robustness to three types of data variation.
    The three kinds of variations we choose to investigate are rotation, translation,
    and scale. With each kind of variation, we modify the CIFAR10 test images according
    to the listed amount. We observe that, across all types and amounts of variation
    (except extreme down-scaling) the proposed pooling operations investigated here
    (gated max-avg and 2 level tree pooling) provide improved robustness to these
    transformations, relative to the standard choices of maxpool or avgpool.
  Figure 5 Link: articels_figures_by_rev_year\2017\Generalizing_Pooling_Functions_in_CNNs_Mixed_Gated_and_Tree\figure_5.jpg
  Figure 5 caption: Visualization of the learned pooling masks of weighted pooling
    and the proposed gated max-average pooling function on CIFAR-10 dataset. Here
    we denote weighted pooling to a single learned pooling filter without the tree
    structure (i.e., a singleton leaf node containing 9 parameters; one such singleton
    leaf node per pooling layer). We also visualize the output feature maps from different
    pooling methods, including max pooling, average pooling, weighted pooling, and
    gated max-average pooling. We can see that the feature responses of learnable
    pooling functions (weight and gated pooling) encode much of the structure in the
    image, as some of it is lost when pooling without learning (average or max pooling)
    is used.
  Figure 6 Link: articels_figures_by_rev_year\2017\Generalizing_Pooling_Functions_in_CNNs_Mixed_Gated_and_Tree\figure_6.jpg
  Figure 6 caption: 't-SNE embeddings of the output responses from different pooling
    operations on the CIFAR10 test set (with classes indicated). From top to bottom:
    Average, max, gated max-avg, and (2 level) tree pooling. The first and the second
    columns show the first and the second pooling layers, respectively. Best viewed
    in color.'
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.83
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Chen-Yu Lee
  Name of the last author: Zhuowen Tu
  Number of Figures: 6
  Number of Tables: 7
  Number of authors: 3
  Paper title: 'Generalizing Pooling Functions in CNNs: Mixed, Gated, and Tree'
  Publication Date: 2017-05-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Classification Error (in %) Comparison Between Baseline Model
      (Trained with Conventional Max Pooling) and Corresponding Networks in Which
      Max Pooling Is Replaced by the Pooling Operation Listed
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Classification Error (in %) Comparison Between our Baseline
      Model (Trained with Conventional Max Pooling) and Proposed Methods Involving
      Tree Pooling
  Table 3 caption:
    table_text: TABLE 3 Classification Error (%) on CIFAR10 (without Data Augmentation)
      Comparison Between Networks Made Deeper with Convolution Layers and Proposed
      Tree+(Gated) Max-Avg Pooling
  Table 4 caption:
    table_text: TABLE 4 Classification Error (in %) Reported by Recent Comparable
      Publications on Four Benchmark Datasets with a Single Model and No Data Augmentation,
      Unless Otherwise Indicated
  Table 5 caption:
    table_text: TABLE 5 ImageNet 2012 Test Error (in %)
  Table 6 caption:
    table_text: TABLE 6 Here We Provide Explicit Statement of the Experimental Conditions
      (Specifically, Network Layer Configurations) Explored in Tables 1, 2, and 4
  Table 7 caption:
    table_text: TABLE 7 Classification Error (in %) Comparison Between the Baselines
      (Max Pooling and Gated Convolutional Layer with Stride 2 Used in Place of Pooling)
      and the Proposed Gated Max-Avg Pooling
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2703082
- Affiliation of the first author: computer vision laboratory, eth zurich, switzerland
  Affiliation of the last author: computer vision laboratory, eth zurich, switzerland
  Figure 1 Link: articels_figures_by_rev_year\2017\Domain_Generalization_and_Adaptation_Using_Low_Rank_Exemplar_SVMs\figure_1.jpg
  Figure 1 caption: An illustration of the prediction matrix G(W) , where we observe
    the block diagonal property of G(W) in (a). The frames from the videos corresponding
    to the two blocks with large values in G(W) are also visually similar to each
    other in (b).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Domain_Generalization_and_Adaptation_Using_Low_Rank_Exemplar_SVMs\figure_2.jpg
  Figure 2 caption: "The performance of our LRE-LSSVMs method when varying different\
    \ parameters: (a) varying parameters C ~ 1 and \u03BB 1 , and (b) varying parameters\
    \ C 2 and \u03BB ~ 2 ."
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Wen Li
  Name of the last author: Luc Van Gool
  Number of Figures: 2
  Number of Tables: 5
  Number of authors: 5
  Paper title: Domain Generalization and Adaptation Using Low Rank Exemplar SVMs
  Publication Date: 2017-05-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Recognition Accuracies (%) of Different Methods for Domain
      Generalization
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Average Training Time and Standard Deviation (in Seconds)\
      \ of LRE-SVMs and LRE-LSSVMs Over 10 Rounds of Experiments on the IXMAS Dataset\
      \ (Cam 0,1 \u2192 Cam 2,3,4)"
  Table 3 caption:
    table_text: TABLE 3 Recognition Accuracies (%) of Different Methods for Domain
      Adaptation
  Table 4 caption:
    table_text: TABLE 4 Recognition Accuracies (%) of Different Methods for Domain
      Adaptation on the Office-Caltech Dataset
  Table 5 caption:
    table_text: TABLE 5 Recognition Accuracies (%) of Different Methods for Domain
      Adaptation with Evolving Target Domain, Where the Target Domain Distribution
      Is Gradually Changing
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2704624
- Affiliation of the first author: school of computer science and engineering, nanjing
    university of science and technology, nanjing, china
  Affiliation of the last author: department of electrical and computer engineering,
    national university of singapore, singapore
  Figure 1 Link: articels_figures_by_rev_year\2017\Personalized_Age_Progression_with_BiLevel_Aging_Dictionary_Learning\figure_1.jpg
  Figure 1 caption: A personalized aging face generated by the proposed method. This
    aging face contains the aging layer (e.g., wrinkles) and the personalized layer
    (e.g., mole). The former can be seen as the corresponding face in a linear combination
    of the aging patterns, while the latter is invariant in the aging process. Better
    view in color.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Personalized_Age_Progression_with_BiLevel_Aging_Dictionary_Learning\figure_2.jpg
  Figure 2 caption: Framework of the proposed age progression. D g denotes a aging
    dictionary of the g th age group. In the offline phase, we collect short-term
    aging face pairs and then train the aging dictionary. In the online phase, for
    an input face, we render its aging faces by bi-level optimization on the corresponding
    aging dictionaries.
  Figure 3 Link: articels_figures_by_rev_year\2017\Personalized_Age_Progression_with_BiLevel_Aging_Dictionary_Learning\figure_3.jpg
  Figure 3 caption: Convergence curves of the optimization procedure for solving D
    1 and D 2 .
  Figure 4 Link: articels_figures_by_rev_year\2017\Personalized_Age_Progression_with_BiLevel_Aging_Dictionary_Learning\figure_4.jpg
  Figure 4 caption: The comparisons with ground truth and other methods. Each group
    includes an input face, a ground truth and three aging results generated by different
    methods. The number or word under each face photo represents the age range (e.g.,
    61-80) or the age period (e.g., older). For convenience of comparison, black background
    has been added to each face photo. Better view in color.
  Figure 5 Link: articels_figures_by_rev_year\2017\Personalized_Age_Progression_with_BiLevel_Aging_Dictionary_Learning\figure_5.jpg
  Figure 5 caption: The comparisons with prior works. Each comparison group includes
    an input face and three aging results of CDL-PAP, BDL-PAP and prior work. The
    number under the face photo is the age range. Some worse aging results of BDL-PAP
    are enclosed by blue box. For convenience of comparison, black background has
    been added to each face photo.
  Figure 6 Link: articels_figures_by_rev_year\2017\Personalized_Age_Progression_with_BiLevel_Aging_Dictionary_Learning\figure_6.jpg
  Figure 6 caption: The comparisons of original face pairs and the synthetic pairs
    by BDL-PAP. The face images in each solid-line box are the same person. Original
    pair consists of younger face and older face, while synthetic pair consists of
    synthetic face and older face.
  Figure 7 Link: articels_figures_by_rev_year\2017\Personalized_Age_Progression_with_BiLevel_Aging_Dictionary_Learning\figure_7.jpg
  Figure 7 caption: Pair setting and comparative performance of face verification.
  Figure 8 Link: articels_figures_by_rev_year\2017\Personalized_Age_Progression_with_BiLevel_Aging_Dictionary_Learning\figure_8.jpg
  Figure 8 caption: The comparisons of BDL-PAP and BDL-AP on FG-NET.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: xiangbo shu
  Name of the last author: Shuicheng Yan
  Number of Figures: 8
  Number of Tables: 3
  Number of authors: 6
  Paper title: Personalized Age Progression with Bi-Level Aging Dictionary Learning
  Publication Date: 2017-05-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Equal Error Rates (EER) (%) of Cross-Age Face Verification
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Left Part Illustrates the Average Rating Scores and Standard
      Deviation Values from the User Study on the Comparisons of BDL-PAP and BDL-AP
  Table 3 caption:
    table_text: TABLE 3 Comparison of Running Time for Age Progression Synthesis
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2705122
- Affiliation of the first author: department of electronic engineering, tsinghua
    university, beijing, china
  Affiliation of the last author: department of computer science, university of toronto,
    toronto, on, canada
  Figure 1 Link: articels_figures_by_rev_year\2017\D_Object_Proposals_Using_Stereo_Imagery_for_Accurate_Object_Class_Detection\figure_1.jpg
  Figure 1 caption: 'Features in our model (from left to right): Left camera image,
    stereo 3D reconstruction, depth-based features and our prior. In the third image,
    occupancy is marked with yellow ( P in Eq. (1)) and purple denotes free space
    ( F in Eq. (2)). In the prior, the ground plane is green and blue to red indicates
    increasing prior value of object height.'
  Figure 10 Link: articels_figures_by_rev_year\2017\D_Object_Proposals_Using_Stereo_Imagery_for_Accurate_Object_Class_Detection\figure_10.jpg
  Figure 10 caption: 'Stereo versus LIDAR: 3D bbox Recall versus number of Candidates
    on Moderate data. IoU threshold is set to 0.25.'
  Figure 2 Link: articels_figures_by_rev_year\2017\D_Object_Proposals_Using_Stereo_Imagery_for_Accurate_Object_Class_Detection\figure_2.jpg
  Figure 2 caption: 'The single-stream network for 3D object detection: Input can
    be an RGB image or a 6-channel RGB-HHA image.'
  Figure 3 Link: articels_figures_by_rev_year\2017\D_Object_Proposals_Using_Stereo_Imagery_for_Accurate_Object_Class_Detection\figure_3.jpg
  Figure 3 caption: 'Two-stream network for 3D object detection: The convnet learns
    from RGB (top) and HHA [18] (bottom) images as input, and concatenates features
    from fc7 layers for multi-task prediction. The model is trained end-to-end.'
  Figure 4 Link: articels_figures_by_rev_year\2017\D_Object_Proposals_Using_Stereo_Imagery_for_Accurate_Object_Class_Detection\figure_4.jpg
  Figure 4 caption: "2D bounding box Recall versus number of Candidates. \u201COurs-G\u201D\
    : Class-independent proposals. \u201COurs\u201D: Class-dependent proposals. We\
    \ use an overlap threshold of 0.7 for Car, and 0.5 for Pedestrian and Cyclist,\
    \ following the KITTI evaluation protocol [15]. From left to right are for Easy,\
    \ Moderate, and Hard evaluation regimes, respectively."
  Figure 5 Link: articels_figures_by_rev_year\2017\D_Object_Proposals_Using_Stereo_Imagery_for_Accurate_Object_Class_Detection\figure_5.jpg
  Figure 5 caption: 2D bounding box Recall versus IoU for 500 proposals. The number
    next to the label indicates the average recall (AR).
  Figure 6 Link: articels_figures_by_rev_year\2017\D_Object_Proposals_Using_Stereo_Imagery_for_Accurate_Object_Class_Detection\figure_6.jpg
  Figure 6 caption: 2D bounding box Recall versus Distance with 2,000 proposals on
    moderate data. We use overlap threshold of 0.7 for Car, and 0.5 for Pedestrian,
    Cyclist .
  Figure 7 Link: articels_figures_by_rev_year\2017\D_Object_Proposals_Using_Stereo_Imagery_for_Accurate_Object_Class_Detection\figure_7.jpg
  Figure 7 caption: 3D bounding box Recall versus Candidates onmoderatedata. 3D IoU
    threshold is set to 0.25.
  Figure 8 Link: articels_figures_by_rev_year\2017\D_Object_Proposals_Using_Stereo_Imagery_for_Accurate_Object_Class_Detection\figure_8.jpg
  Figure 8 caption: AP mathrm2D versus proposals on Car for the Moderate setting.
  Figure 9 Link: articels_figures_by_rev_year\2017\D_Object_Proposals_Using_Stereo_Imagery_for_Accurate_Object_Class_Detection\figure_9.jpg
  Figure 9 caption: 'Stereo versus LIDAR: 2D bounding box Recall versus number of
    Candidates onModeratedata . We use an overlap threshold of 0.7 for Car, and 0.5
    for Pedestrian and Cyclist. By hybrid we mean the approach that uses both stereo
    and LIDAR for road plane estimation, and LIDAR for feature extraction.'
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Xiaozhi Chen
  Name of the last author: Raquel Urtasun
  Number of Figures: 12
  Number of Tables: 8
  Number of authors: 6
  Paper title: 3D Object Proposals Using Stereo Imagery for Accurate Object Class
    Detection
  Publication Date: 2017-05-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Running Time of Different Proposal Methods
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average Precision (AP 2D ) (in %) on the Test Set of the KITTI
      Object Detection Benchmark
  Table 3 caption:
    table_text: TABLE 3 AOS Scores (in %) on the Test Set of KITTI's Object Detection
      and Orientation Estimation Benchmark
  Table 4 caption:
    table_text: TABLE 4 Object Detection (Top) and Orientation Estimation (Bottom)
      Results on KITTI's Validation Set
  Table 5 caption:
    table_text: 'TABLE 5 Stereo versus LIDAR on 2D Object Detection and Orientation
      Estimation: AP 2D and AOS for Car on Validation Set of KITTI'
  Table 6 caption:
    table_text: 'TABLE 6 Stereo versus LIDAR on 3D Object Detection: AP 3D and ALP
      for Car on KITTI Validation'
  Table 7 caption:
    table_text: 'TABLE 7 Comparison of Different Architectures on 2D Object Detection
      and Orientation Estimation: AP 2D and AOS for Car on Validation Set of KITTI'
  Table 8 caption:
    table_text: 'TABLE 8 Comparison of Different Architectures on 3D Object Detection:
      AP 3D and ALP for Car on Validation Set of KITTI'
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2706685
- Affiliation of the first author: mitsubishi electric research laboratories (merl),
    cambridge, ma
  Affiliation of the last author: department of engineering science, university of
    oxford, oxford, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2017\Sequential_Optimization_for_Efficient_HighQuality_Object_Proposal_Generation\figure_1.jpg
  Figure 1 caption: Comparison of generic object proposal methods on VOC2007 test
    dataset [7] with at most 1,000 proposals per image and intersection-over-union
    (IoU) threshold equal to 0.5. All the competing results are produced by public
    code (see Tables 3 and 4 in Section 4 for more details).
  Figure 10 Link: articels_figures_by_rev_year\2017\Sequential_Optimization_for_Efficient_HighQuality_Object_Proposal_Generation\figure_10.jpg
  Figure 10 caption: Comparison of recall-overlap curves using different methods and
    numbers of proposals on VOC2007 test set.
  Figure 2 Link: articels_figures_by_rev_year\2017\Sequential_Optimization_for_Efficient_HighQuality_Object_Proposal_Generation\figure_2.jpg
  Figure 2 caption: Best overlap (BO) statistical comparison using at most 1,000 proposals
    per image and IoU threshold 0.5 on VOC2007 test dataset.
  Figure 3 Link: articels_figures_by_rev_year\2017\Sequential_Optimization_for_Efficient_HighQuality_Object_Proposal_Generation\figure_3.jpg
  Figure 3 caption: Statistical comparison based on percentage of objects versus best
    proposal deviation from the ground-truth bounding box per object with at most
    1,000 proposals per image and IoU threshold 0.5 on VOC2007 test dataset.
  Figure 4 Link: articels_figures_by_rev_year\2017\Sequential_Optimization_for_Efficient_HighQuality_Object_Proposal_Generation\figure_4.jpg
  Figure 4 caption: "Statistical behavior comparison when optimizing Eq. (15) on VOC2007\
    \ train+val (left) and COCO training (right) datasets using \u03B7=0.8 as the\
    \ threshold for measuring high-quality proposals."
  Figure 5 Link: articels_figures_by_rev_year\2017\Sequential_Optimization_for_Efficient_HighQuality_Object_Proposal_Generation\figure_5.jpg
  Figure 5 caption: Illustration of updating current red solid bounding box mathbf
    r(t) to next red dashed bounding box mathbf r(t+1) . Our estimation for the ground-truth
    bounding box mathbf s based on mathcal C(mathbf r(t)) succeeds in (a) where the
    pixels in mathcal C(mathbf r(t)) spread well, but fails in (b) where the pixels
    in mathcal C(mathbf r(t)) concentrate on few boundary fragments.
  Figure 6 Link: articels_figures_by_rev_year\2017\Sequential_Optimization_for_Efficient_HighQuality_Object_Proposal_Generation\figure_6.jpg
  Figure 6 caption: Statistical behavior comparison on DRMABO versus Delta using VOC2007
    training (top), validation (middle), and test (bottom) datasets, respectively,
    by minimizing Eq. (17). Here x -axis shows the indexes of all possible combinations
    in Delta , and y -axis shows the performance improvement w.r.t. that with the
    combination lbrace 0.1, 0.2, 0.3, 0.4, 0.5rbrace used in [9].
  Figure 7 Link: articels_figures_by_rev_year\2017\Sequential_Optimization_for_Efficient_HighQuality_Object_Proposal_Generation\figure_7.jpg
  Figure 7 caption: Distributions of objects based on their BO scores and the width
    and height of their ground-truth bounding boxes, given the proposals from BING++
    as inputs. For larger objects BING++ works better, in general.
  Figure 8 Link: articels_figures_by_rev_year\2017\Sequential_Optimization_for_Efficient_HighQuality_Object_Proposal_Generation\figure_8.jpg
  Figure 8 caption: DR comparison on VOC2007 test dataset by varying the maximum number
    of quantized scalesaspect-ratios in BING.
  Figure 9 Link: articels_figures_by_rev_year\2017\Sequential_Optimization_for_Efficient_HighQuality_Object_Proposal_Generation\figure_9.jpg
  Figure 9 caption: Timing distribution over components in BING++.
  First author gender probability: 0.88
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ziming Zhang
  Name of the last author: Philip H.S. Torr
  Number of Figures: 14
  Number of Tables: 8
  Number of authors: 7
  Paper title: Sequential Optimization for Efficient High-Quality Object Proposal
    Generation
  Publication Date: 2017-05-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance Comparison (%) Among Different BING's Derivatives
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Effect of Image Resize Operation on Performance (%) in BING++
  Table 3 caption:
    table_text: TABLE 3 DR (%) and Running Time (s) Comparison on VOC2007 Test Dataset
  Table 4 caption:
    table_text: TABLE 4 ABO & MABO Comparison (%) Between Different Proposal Algorithms
      on VOC2007 Test Dataset Using 1,000 Proposals
  Table 5 caption:
    table_text: TABLE 5 DR (%), MABO (%) and Running Time (s) Comparison on VOC2007
      Test Dataset with Images Resized to 19 of Their Original Sizes
  Table 6 caption:
    table_text: "TABLE 6 DR (%), MABO (%) and Running Time (s) Comparison on VOC2007\
      \ Test Dataset with Images Resized to 360\xD7400 Pixels"
  Table 7 caption:
    table_text: TABLE 7 AP & mAP Comparison (%) for Object Detection Using Fast R-CNN
      on VOC2007 Test Dataset with 1,000 Proposals with IoU Threshold 0.5
  Table 8 caption:
    table_text: TABLE 8 DR and MABO Comparison (%) on COCO Validation Dataset Using
      the Same (Learned) Parameters on VOC2007 for Each Method
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2707492
- Affiliation of the first author: department of computer science, university of colorado
    colorado springs, colorado springs, co, 80918
  Affiliation of the last author: department of computer science, university of colorado
    colorado springs, colorado springs, co, 80918
  Figure 1 Link: articels_figures_by_rev_year\2017\The_Extreme_Value_Machine\figure_1.jpg
  Figure 1 caption: "A solution from the proposed EVM algorithm trained on four classes:\
    \ dots, diamonds, squares, and stars. The colors in the isocontour rings show\
    \ a Psi -model (probability of sample inclusion) for each extreme vector (EV)\
    \ chosen by the algorithm, with red near 1 and blue near .005. Via kernel-free\
    \ non-linear modeling, the EVM supports open set recognition and can reject the\
    \ three \u201C?\u201D inputs that lie beyond the support of the training set as\
    \ \u201Cunknown.\u201D Each Psi -model has its own independent shape and scale\
    \ parameters learnt from the data, and supports a soft-margin. For example, the\
    \ \u03A8 -model for the blue dots corresponding to extreme vector A has a more\
    \ gradual fall off, due to the effect of the outlier star during training."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\The_Extreme_Value_Machine\figure_2.jpg
  Figure 2 caption: "Multi-class open set recognition performance on OLETTER. The\
    \ x -axis represents the percentage of classes in the test set that were unseen\
    \ during training. Error bars show standard deviation. The EVM is comparable to\
    \ the existing state-of-the-art W-SVM [12] in F1-Measure, but at a substantial\
    \ savings in training efficiency and scalability, as reflected in the vector ratio\
    \ (VR). The EVM's VR is an order of magnitude smaller than the two SVM-based models.\
    \ Both EVM and W-SVM algorithms have favorable performance degradation characteristics\
    \ as the problem becomes more open. The two other probabilistically calibrated\
    \ algorithms degrade far more rapidly. Hyperparameters of \u03C4=75 (tail size)\
    \ and k=4 (number of EVs to average over) were used in this evaluation, and were\
    \ selected using the same training set cross validation technique as the W-SVM."
  Figure 3 Link: articels_figures_by_rev_year\2017\The_Extreme_Value_Machine\figure_3.jpg
  Figure 3 caption: "Open world performance of the EVM and NNO algorithms on the open\
    \ world ImageNet benchmark in terms of both F1-mearsure (left) and accuracy (right).\
    \ Initial training considering 50 classes was performed, then classes were incrementally\
    \ added in groups of 50. The EVM dramatically outperforms NNO at all points evaluated\
    \ and the divergence of the respective surfaces suggests superior scalability\
    \ with respect to both the number of additional training classes and the number\
    \ of additional unknown classes. Hyperparameter values of k=6 and \u03C4=33998\
    \ were selected using the cross validation procedure discussed in the text."
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ethan M. Rudd
  Name of the last author: Terrance E. Boult
  Number of Figures: 3
  Number of Tables: 1
  Number of authors: 4
  Paper title: The Extreme Value Machine
  Publication Date: 2017-05-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Numbers of Extreme Vectors, Cumulative Numbers of Training
      Points Used, and Vector Ratios After Each Batch Is Added
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2707495
- Affiliation of the first author: school of computer science and engineering, nanyang
    technological university, singapore
  Affiliation of the last author: australian centre for robotic vision, australia
  Figure 1 Link: articels_figures_by_rev_year\2017\Exploring_Context_with_Deep_Structured_Models_for_Semantic_Segmentation\figure_1.jpg
  Figure 1 caption: An illustration of the prediction process of our method. Both
    our unary and pairwise potentials are formulated as multi-scale CNNs for capturing
    semantic relations between image regions. Our method outputs a low-resolution
    prediction after performing CRF inference, which is then up-sampled and refined
    in a standard post-processing stage to generate the final prediction.
  Figure 10 Link: articels_figures_by_rev_year\2017\Exploring_Context_with_Deep_Structured_Models_for_Semantic_Segmentation\figure_10.jpg
  Figure 10 caption: Prediction examples on the NYUDv2 dataset.
  Figure 2 Link: articels_figures_by_rev_year\2017\Exploring_Context_with_Deep_Structured_Models_for_Semantic_Segmentation\figure_2.jpg
  Figure 2 caption: An illustration of generating a feature map with FeatMap-Net and
    constructing the CRF graph.
  Figure 3 Link: articels_figures_by_rev_year\2017\Exploring_Context_with_Deep_Structured_Models_for_Semantic_Segmentation\figure_3.jpg
  Figure 3 caption: An illustration of constructing pairwise connections in a CRF
    graph. A node is connected to all other nodes which lie inside the range box (dashed
    box in the figure). Two types of spatial relations are described in the figure,
    which correspond to two types of pairwise potential functions.
  Figure 4 Link: articels_figures_by_rev_year\2017\Exploring_Context_with_Deep_Structured_Models_for_Semantic_Segmentation\figure_4.jpg
  Figure 4 caption: An overview of the proposed contextual deep structured model.
    Unary-Net and Pairwise-Net are shown here for generating potential function outputs.
  Figure 5 Link: articels_figures_by_rev_year\2017\Exploring_Context_with_Deep_Structured_Models_for_Semantic_Segmentation\figure_5.jpg
  Figure 5 caption: An illustration of generating feature vectors for CRF nodes and
    pairwise connections from the feature map output by FeatMap-Net. The symbol d
    denotes the feature dimension. We concatenate the corresponding features of two
    connected nodes in the feature map to obtain the CRF edge features.
  Figure 6 Link: articels_figures_by_rev_year\2017\Exploring_Context_with_Deep_Structured_Models_for_Semantic_Segmentation\figure_6.jpg
  Figure 6 caption: The details of our FeatMap-Net. An input image is first resized
    into 3 scales, then each resized image goes through six convolution blocks to
    output one feature map. Top 5 convolution blocks are shared for all scales. Every
    scale has a specific convolution block (Conv Block 6). We perform two-level sliding
    pyramid pooling and concatenate the pooled feature map to the original feature
    map. The symbol d denotes the feature dimension.
  Figure 7 Link: articels_figures_by_rev_year\2017\Exploring_Context_with_Deep_Structured_Models_for_Semantic_Segmentation\figure_7.jpg
  Figure 7 caption: Details for sliding pyramid pooling. We perform two-level sliding
    pyramid pooling on the feature map for capturing patch-background context, which
    encode rich background information and increase the field-of-view for the feature
    map.
  Figure 8 Link: articels_figures_by_rev_year\2017\Exploring_Context_with_Deep_Structured_Models_for_Semantic_Segmentation\figure_8.jpg
  Figure 8 caption: 'The detailed configuration of the networks: FeatMap-Net, Unary-Net
    and Pairwise-Net. K is the number of classes. The filter size for convolution
    and the number of filters are shown for all layers. For FeatMap-Net, the top 5
    convolution blocks share the same configuration as the convolution blocks in the
    VGG-16 network. The stride of the last max pooling layer is 1, and for the other
    max pooling layers we use the same stride setting as the VGG-16 network.'
  Figure 9 Link: articels_figures_by_rev_year\2017\Exploring_Context_with_Deep_Structured_Models_for_Semantic_Segmentation\figure_9.jpg
  Figure 9 caption: 'An illustration of our two-stage prediction process. The prediction
    process consists of two stages: the coarse-level prediction stage and the prediction
    refinement stage. We first perform CRF inference on our contextual model to generate
    a score map for coarse-level prediction, then we bilinearly unsmaple the score
    map and apply a boundary refinement method [23] to obtain the final prediction
    which has the same resolution as the input image.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Guosheng Lin
  Name of the last author: Ian Reid
  Number of Figures: 14
  Number of Tables: 10
  Number of authors: 4
  Paper title: Exploring Context with Deep Structured Models for Semantic Segmentation
  Publication Date: 2017-05-26 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Segmentation Results on the NYUDv2 Dataset (40 Classes)
  Table 10 caption:
    table_text: TABLE 10 Segmentation Results on the KITTI Dataset (10 Classes)
  Table 2 caption:
    table_text: TABLE 2 Ablation Experiments
  Table 3 caption:
    table_text: TABLE 3 Comparison with Unary Ensembles on the NYUDv2 Dataset (40
      Classes)
  Table 4 caption:
    table_text: TABLE 4 Individual Category Results on the PASCAL VOC 2012 Test Set
      (IoU Scores)
  Table 5 caption:
    table_text: TABLE 5 Segmentation Results on the Cityscapes Test Set
  Table 6 caption:
    table_text: TABLE 6 Segmentation Results on the PASCAL-Context Dataset (60 Classes)
  Table 7 caption:
    table_text: TABLE 7 Segmentation Results on SUN-RGBD Dataset (37 Classes)
  Table 8 caption:
    table_text: TABLE 8 Segmentation Results on the COCO Dataset (80 Classes)
  Table 9 caption:
    table_text: TABLE 9 Segmentation Results on SIFT-Flow Dataset (33 Classes)
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2708714
- Affiliation of the first author: s&h, milan, italy
  Affiliation of the last author: institute for computer graphics and vision, graz
    university of technology, graz, austria
  Figure 1 Link: articels_figures_by_rev_year\2017\Robust_D_Object_Tracking_from_Monocular_Images_Using_Stable_Parts\figure_1.jpg
  Figure 1 caption: Our method in action during a demonstrative technical intervention
    at CERN, Geneva. Detected parts are shown as colored rectangles. The appearance
    of the scene constantly changes and undergoes heavy occlusions. Despite these
    difficulties, we accurately estimate the 3D pose of the box, even if only one
    part is detected or in presence of false detections caused by the cluttered environment.
  Figure 10 Link: articels_figures_by_rev_year\2017\Robust_D_Object_Tracking_from_Monocular_Images_Using_Stable_Parts\figure_10.jpg
  Figure 10 caption: 'Results of the experiment described in Section 8.3: Detection
    error Cumulative Distribution Functions (CDF) for the BOX and the T-Less datasets
    for different detectors. Top row: BOX-Video 1. Middle row: BOX-Video 2. Bottom
    row: T-less dataset.'
  Figure 2 Link: articels_figures_by_rev_year\2017\Robust_D_Object_Tracking_from_Monocular_Images_Using_Stable_Parts\figure_2.jpg
  Figure 2 caption: Our representation of the 3D pose of an object part. (a) We consider
    seven 3D control points for each part, arranged to span 3 orthogonal directions.
    (b) Given an image patch of the part, we predict the 2D reprojections of these
    control points using a regressor, and the uncertainty of the predictions.
  Figure 3 Link: articels_figures_by_rev_year\2017\Robust_D_Object_Tracking_from_Monocular_Images_Using_Stable_Parts\figure_3.jpg
  Figure 3 caption: Detecting the parts. (a) An input image of the box. (b) The output
    of the CNN part-det for each image location. Each color corresponds to a different
    part. (c) The output after Gaussian smoothing. (d) The detected parts, corresponding
    to the local maximums in (c).
  Figure 4 Link: articels_figures_by_rev_year\2017\Robust_D_Object_Tracking_from_Monocular_Images_Using_Stable_Parts\figure_4.jpg
  Figure 4 caption: Architecture of CNN part-det for part detection. The last layer
    outputs the likelihoods of the patch to correspond to each part or to the background.
  Figure 5 Link: articels_figures_by_rev_year\2017\Robust_D_Object_Tracking_from_Monocular_Images_Using_Stable_Parts\figure_5.jpg
  Figure 5 caption: "Architecture of the CNN CNN cp\u2212pred\u2212p predicting the\
    \ projections of the control points."
  Figure 6 Link: articels_figures_by_rev_year\2017\Robust_D_Object_Tracking_from_Monocular_Images_Using_Stable_Parts\figure_6.jpg
  Figure 6 caption: 'Pose prior for an electric box: Projections of the box by each
    of the 9 Gaussians centers overlinemathbf pm .'
  Figure 7 Link: articels_figures_by_rev_year\2017\Robust_D_Object_Tracking_from_Monocular_Images_Using_Stable_Parts\figure_7.jpg
  Figure 7 caption: 'Qualitative results for our challenging datasets. Top: We track
    the box despite large changes in the background and in the lighting conditions
    on both sequences of the BOX dataset. Middle: Our method correctly estimates the
    3D pose of the can using the can tab only. Bottom: The pose of the door is retrieved
    starting from the door knob, the keyhole and the lock. The video sequences are
    provided as the supplementary material, which can be found on the Computer Society
    Digital Library at http:doi.ieeecomputersociety.org10.1109TPAMI.2017.2708711.'
  Figure 8 Link: articels_figures_by_rev_year\2017\Robust_D_Object_Tracking_from_Monocular_Images_Using_Stable_Parts\figure_8.jpg
  Figure 8 caption: Training images and control points we used for the BOX, the CAN
    and the DOOR datasets. The center of each part is shown in yellow. Control points
    are zoomed for better visualization.
  Figure 9 Link: articels_figures_by_rev_year\2017\Robust_D_Object_Tracking_from_Monocular_Images_Using_Stable_Parts\figure_9.jpg
  Figure 9 caption: 'Parts and test sequence from the T-less dataset. (a)-(d): Four
    items employed as parts of the scene. (e): Testing sequence. The fifth object
    in the scene is not employed, acting as a supplementary occlusion.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Alberto Crivellaro
  Name of the last author: Vincent Lepetit
  Number of Figures: 14
  Number of Tables: 3
  Number of authors: 6
  Paper title: Robust 3D Object Tracking from Monocular Images Using Stable Parts
  Publication Date: 2017-05-26 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Main Notations
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Experimental Results
  Table 3 caption:
    table_text: TABLE 3 Pose Estimation Results on the T-Less Dataset-Sequence 3
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2708711
- Affiliation of the first author: arc centre of excellence for robotic vision
  Affiliation of the last author: arc centre of excellence for robotic vision
  Figure 1 Link: articels_figures_by_rev_year\2017\Image_Captioning_and_Visual_Question_Answering_Based_on_Attributes_and_External_\figure_1.jpg
  Figure 1 caption: An example of the proposed V2L system in action. Attributes are
    predicted by our CNN-based attribute prediction model. Image captions are generated
    by our attribute-based captioning generation model. All of the predicted attributes
    and generated captions, combined with the external knowledge from a large-scale
    knowledge base, are fed to an LSTM to produce the answer to the asked question.
    Underlined words indicate the information required to answer the question.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Image_Captioning_and_Visual_Question_Answering_Based_on_Attributes_and_External_\figure_2.jpg
  Figure 2 caption: Our attribute-based image captioning framework. The image analysis
    module learns a mapping between an image and the semantic attributes through a
    CNN. The language module learns a mapping from the attributes vector to a sequence
    of words using an LSTM.
  Figure 3 Link: articels_figures_by_rev_year\2017\Image_Captioning_and_Visual_Question_Answering_Based_on_Attributes_and_External_\figure_3.jpg
  Figure 3 caption: 'Attribute prediction CNN: The model is initialized from VggNet
    [10] pre-trained on ImageNet. The model is then fine-tuned on the target multi-label
    dataset. Given a test image, a set of proposal regions are selected and passed
    to the shared CNN, and finally the CNN outputs from different proposals are aggregated
    with max pooling to produce the final multi-label prediction, which gives us the
    high-level image representation, V att (I).'
  Figure 4 Link: articels_figures_by_rev_year\2017\Image_Captioning_and_Visual_Question_Answering_Based_on_Attributes_and_External_\figure_4.jpg
  Figure 4 caption: Examples of predicted attributes and generated captions.
  Figure 5 Link: articels_figures_by_rev_year\2017\Image_Captioning_and_Visual_Question_Answering_Based_on_Attributes_and_External_\figure_5.jpg
  Figure 5 caption: 'Our proposed model: Given an image, a CNN is first applied to
    produce the attribute-based representation V att (I) . The internal textual representation
    is made up of image captions generated based on the image-attributes. The hidden
    state of the caption-LSTM after it has generated the last word in each caption
    is used as its vector representation. These vectors are then aggregated as V cap
    (I) with average-pooling. The external knowledge is mined from the KB and the
    responses are encoded by Doc2Vec, which produces a vector V know (I) . The 3 vectors
    V are combined into a single representation of scene content, which is input to
    the VQA LSTM model that interprets the question and generates an answer.'
  Figure 6 Link: articels_figures_by_rev_year\2017\Image_Captioning_and_Visual_Question_Answering_Based_on_Attributes_and_External_\figure_6.jpg
  Figure 6 caption: An example of SPARQL query language for the attribute 'dog'. The
    mined text-based knowledge are shown below.
  Figure 7 Link: articels_figures_by_rev_year\2017\Image_Captioning_and_Visual_Question_Answering_Based_on_Attributes_and_External_\figure_7.jpg
  Figure 7 caption: Performance on five question categories for different models.
  Figure 8 Link: articels_figures_by_rev_year\2017\Image_Captioning_and_Visual_Question_Answering_Based_on_Attributes_and_External_\figure_8.jpg
  Figure 8 caption: Some example cases where our final model gives the correct answer
    while the base line model VggNet-LSTM generates the wrong answer. All results
    are from the VQA dataset. More results can be found in the supplementary material,
    available in the online supplemental material.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Qi Wu
  Name of the last author: Anton van den Hengel
  Number of Figures: 8
  Number of Tables: 12
  Number of authors: 5
  Paper title: Image Captioning and Visual Question Answering Based on Attributes
    and External Knowledge
  Publication Date: 2017-05-26 00:00:00
  Table 1 caption:
    table_text: TABLE 1 BLEU-1,2,3,4 and PPL Metrics Compared to Other State-of-the-Art
      Methods and Our Baseline on Flickr8k and Flickr30K Datasets
  Table 10 caption:
    table_text: TABLE 10 Toronto COCO-QA Accuracy (Percent) Per Category
  Table 2 caption:
    table_text: TABLE 2 BLEU-1,2,3,4, METEOR, CIDEr and PPL Metrics Compared to Other
      State-of-the-Art Methods and Our Baseline on MS COCO Dataset
  Table 3 caption:
    table_text: TABLE 3 COCO Evaluation Server Results
  Table 4 caption:
    table_text: TABLE 4 Human Evaluation on 1,000 Sampled Results from MS COCO Validation
      Split
  Table 5 caption:
    table_text: TABLE 5 Visual Feature Input Dimension and Properties of RNN
  Table 6 caption:
    table_text: TABLE 6 Some Statistics about the DAQUAR, Toronto COCO-QA Dataset
      [15] and VQA Dataset [12]
  Table 7 caption:
    table_text: TABLE 7 Accuracy, WUPS Metrics Compared to Other State-of-the-Art
      Methods and Our Baseline on DAQUAR-All
  Table 8 caption:
    table_text: TABLE 8 Accuracy, WUPS Metrics Compared to Other State-of-the-Art
      Methods and Our Baseline on DAQUAR-Reduced
  Table 9 caption:
    table_text: TABLE 9 Accuracy, WUPS Metrics Compared to Other State-of-the-Art
      Methods and Our Baseline on Toronto COCO-QA Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2708709
- Affiliation of the first author: centre for ai, university of technology at sydney,
    ultimo, nsw, australia
  Affiliation of the last author: department of computer science, university of texas
    at san antonio, san antonio, tx
  Figure 1 Link: articels_figures_by_rev_year\2017\SIFT_Meets_CNN_A_Decade_Survey_of_Instance_Retrieval\figure_1.jpg
  Figure 1 caption: "Milestones of instance retrieval. After a survey of methods before\
    \ the year 2000 by Smeulders et al. [2], Sivic and Zisserman [3] proposed Video\
    \ Google in 2003, marking the beginning of the BoW model. Then, the hierarchical\
    \ k-means and approximate k-means were proposed by Stew\xE9nius and Nist\xE9r\
    \ [11] and Philbin et al. [12], respectively, marking the use of large codebooks\
    \ in retrieval. In 2008, J\xE9gou et al. [13] proposed Hamming Embedding, a milestone\
    \ in using medium-sized codebooks. Then, compact visual representations for retrieval\
    \ were proposed by Perronnin et al. [14] and J\xE9gou et al. [15] in 2010. Although\
    \ SIFT-based methods were still moving forward, CNN-based methods began to gradually\
    \ take over, following the pioneering work of Krizhevsky et al. [6]. In 2014,\
    \ Razavian et al. [7] proposed a hybrid method extracting multiple CNN features\
    \ from an image. Babenko et al. [8] were the first to fine-tune a CNN model for\
    \ generic instance retrieval. Both [9], [10] employ the column features from pre-trained\
    \ CNN models, and [10] inspires later state-of-the-art methods. These milestones\
    \ are the representative works of the categorization scheme in this survey."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\SIFT_Meets_CNN_A_Decade_Survey_of_Instance_Retrieval\figure_2.jpg
  Figure 2 caption: A general pipeline of SIFT- and CNN-based retrieval models. Features
    are computed from hand-crafted detectors for SIFT, and densely applied filters
    or image patches for CNN. In both methods, under small codebooks, encodingpooling
    is employed to produce compact vectors. In SIFT-based methods, the inverted index
    is necessary under largemedium-sized codebooks. The CNN features can also be computed
    in an end-to-end way using fine-tuned CNN models.
  Figure 3 Link: articels_figures_by_rev_year\2017\SIFT_Meets_CNN_A_Decade_Survey_of_Instance_Retrieval\figure_3.jpg
  Figure 3 caption: Two milestone clustering methods (a) hierarchical k-means (HKM)
    [11] and (b) approximate k-means (AKM) [12] for large codebook generation. Bold
    borders and blue discs are the clustering boundaries and centers of the first
    layer of HKM. Slim borders and red squares are the final clustering results in
    both methods.
  Figure 4 Link: articels_figures_by_rev_year\2017\SIFT_Meets_CNN_A_Decade_Survey_of_Instance_Retrieval\figure_4.jpg
  Figure 4 caption: The data structure of the inverted index. It physically contains
    K inverted lists, each consisting of some postings, which index the image ID and
    some binary signatures. During retrieval, a quantized feature will traverse the
    inverted list corresponding to its assigned visual word. Dashed line denotes soft
    quantization, in which multiple inverted lists are visited.
  Figure 5 Link: articels_figures_by_rev_year\2017\SIFT_Meets_CNN_A_Decade_Survey_of_Instance_Retrieval\figure_5.jpg
  Figure 5 caption: False match removal by (A) HE [13], (B) local-local feature fusion,
    and (C) local-global feature fusion.
  Figure 6 Link: articels_figures_by_rev_year\2017\SIFT_Meets_CNN_A_Decade_Survey_of_Instance_Retrieval\figure_6.jpg
  Figure 6 caption: The state of the art over the years on the (a) Holidays, (b) Ukbench,
    and (c) Oxford5k datasets. Six fine-grained categories are summarized (see Section
    2). For each year, the best accuracy of each category is reported. For the compact
    representations, results of 128-bit vectors are preferentially selected. The purple
    star denotes the results produced by 2,048-dim vectors [17] , the best performance
    in fine-tuned CNN methods. Methods with a pink asterisk denote using rotated images
    on Holidays, full-sized queries on Oxford5k, or spatial verification and QE on
    Oxford5k (see Table 5).
  Figure 7 Link: articels_figures_by_rev_year\2017\SIFT_Meets_CNN_A_Decade_Survey_of_Instance_Retrieval\figure_7.jpg
  Figure 7 caption: The impact of feature dimension on retrieval accuracy. Compact
    (fixed-length) representations are shown, i.e., SIFT small voc., hybrid CNN methods,
    pre-trained CNN methods, and fine-tuned CNN methods. Curves with a pink asterisk
    on the end indicates using rotated images or full-sized queries on Holidays and
    Oxford5k (see Table 5), resp.
  Figure 8 Link: articels_figures_by_rev_year\2017\SIFT_Meets_CNN_A_Decade_Survey_of_Instance_Retrieval\figure_8.jpg
  Figure 8 caption: Memory cost versus retrieval accuracy on Oxford5k. In the legend,
    the first 8 methods are based on large codebooks, while the last 7 use medium-sized
    codebooks. This figure shares the same legend with Fig. 7c except the newly added
    numbers (in black).
  Figure 9 Link: articels_figures_by_rev_year\2017\SIFT_Meets_CNN_A_Decade_Survey_of_Instance_Retrieval\figure_9.jpg
  Figure 9 caption: The impact of codebook size on SIFT-based methods using (a) large
    codebooks and (b) medium-sized codebooks on the Oxford5k dataset.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Liang Zheng
  Name of the last author: Qi Tian
  Number of Figures: 9
  Number of Tables: 6
  Number of authors: 3
  Paper title: 'SIFT Meets CNN: A Decade Survey of Instance Retrieval'
  Publication Date: 2017-05-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Major Differences between Various Types of Instance Retrieval
      Models
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Pre-Trained CNN Models That Can Be Used
  Table 3 caption:
    table_text: TABLE 3 Statistics of Instance-Level Datasets Having Been Used in
      Fine-Tuning
  Table 4 caption:
    table_text: TABLE 4 Statistics of Popular Instance-Level Datasets
  Table 5 caption:
    table_text: TABLE 5 Performance Summarization of Some Representative Methods of
      the Six Categories on the Benchmarks
  Table 6 caption:
    table_text: TABLE 6 A Summary of Efficiency and Accuracy Comparison between Different
      Categories
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2709749
