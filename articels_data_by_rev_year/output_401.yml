- Affiliation of the first author: key lab of intelligent information processing,
    chinese academy of sciences (cas), institute of computing technology (ict), cas,
    beijing, china
  Affiliation of the last author: key lab of intelligent information processing, chinese
    academy of sciences (cas), institute of computing technology (ict), cas, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2015\MultiView_Discriminant_Analysis\figure_1.jpg
  Figure 1 caption: The overview of Multi-view Discriminant Analysis. The samples
    from distinct views are projected into a discriminant common space by using v
    transforms, one for each view. Here, images from distinct views, e.g., photo,
    sketch, NIR are denoted in distinct colors and images from distinct classes are
    denoted in distinct shapes. (Best viewed in color)
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\MultiView_Discriminant_Analysis\figure_2.jpg
  Figure 2 caption: The 2D embeddings of Euclidean space, common space from MCCA and
    MvDA for the samples from 7 views on Multi-PIE dataset. Different classes are
    denoted in different colors and shapes.
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.64
  Name of the first author: Meina Kan
  Name of the last author: Xilin Chen
  Number of Figures: 2
  Number of Tables: 3
  Number of authors: 5
  Paper title: Multi-View Discriminant Analysis
  Publication Date: 2015-05-20 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 The Performance of MvDA-VC w.r.t. \u03BB in Terms of Mean\
      \ Accuracy (mACC) on Multi-PIE Dataset"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Evluation on Multi-PIE Dataset in Terms of Mean Accuracy (mACC)
  Table 3 caption:
    table_text: TABLE 3 Evaluation on CUFSF and HFB Datasets in Terms of rank-1 Recognition
      Rate
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2435740
- Affiliation of the first author: microsoft research, redmond, wa
  Affiliation of the last author: department of electrical engineering and computer
    science, uc berkeley, berkeley, ca
  Figure 1 Link: articels_figures_by_rev_year\2015\RegionBased_Convolutional_Networks_for_Accurate_Object_Detection_and_Segmentatio\figure_1.jpg
  Figure 1 caption: Object detection system overview. Our system (1) takes an input
    image, (2) extracts around 2000 bottom-up region proposals, (3) computes features
    for each proposal using a large CNN, and then (4) classifies each region using
    class-specific linear SVMs. We trained an R-CNN that achieves a mean average precision
    of 62.9 percent on PASCAL VOC 2010. For comparison, [21] reports 35.1 percent
    mAP using the same region proposals, but with a spatial pyramid and bag-of-visual-words
    approach. The popular deformable part models perform at 33.4 percent. On the 200-class
    ILSVRC2013 detection dataset, we trained an R-CNN with a mAP of 31.4 percent,
    a large improvement over OverFeat [19], which had the previous best result at
    24.3 percent mAP.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\RegionBased_Convolutional_Networks_for_Accurate_Object_Detection_and_Segmentatio\figure_2.jpg
  Figure 2 caption: Warped training samples from VOC 2007 train.
  Figure 3 Link: articels_figures_by_rev_year\2015\RegionBased_Convolutional_Networks_for_Accurate_Object_Detection_and_Segmentatio\figure_3.jpg
  Figure 3 caption: (Left) Mean average precision on the ILSVRC2013 detection test
    set. Methods preceeded by use outside training data (images and labels from the
    ILSVRC classification dataset in all cases). (Right) Box plots for the 200 average
    precision values per method. A box plot for the post-competition OverFeat result
    is not shown because per-class APs are not yet available. The red line marks the
    median AP, the box bottom and top are the 25th and 75th percentiles. The whiskers
    extend to the min and max AP of each method. Each AP is plotted as a green dot
    over the whiskers (best viewed digitally with zoom).
  Figure 4 Link: articels_figures_by_rev_year\2015\RegionBased_Convolutional_Networks_for_Accurate_Object_Detection_and_Segmentatio\figure_4.jpg
  Figure 4 caption: Top regions for six pool 5 units. Receptive fields and activation
    values are drawn in white. Some units are aligned to concepts, such as people
    (row 1) or text (4). Other units capture texture and material properties, such
    as dot arrays (2) and specular reflections (6).
  Figure 5 Link: articels_figures_by_rev_year\2015\RegionBased_Convolutional_Networks_for_Accurate_Object_Detection_and_Segmentatio\figure_5.jpg
  Figure 5 caption: "Distribution of top-ranked false positive (FP) types for R-CNNs\
    \ with TorontoNet. Each plot shows the evolving distribution of FP types as more\
    \ FPs are considered in order of decreasing score. Each FP is categorized into\
    \ 1 of 4 types: Loc\u2014poor localization (a detection with an IoU overlap with\
    \ the correct class between 0.1 and 0.5, or a duplicate); Sim\u2014confusion with\
    \ a similar category; Oth\u2014confusion with a dissimilar object category; BG\u2014\
    a FP that fired on background. Compared with DPM (see [64]), significantly more\
    \ of our errors result from poor localization, rather than confusion with background\
    \ or other object classes, indicating that the CNN features are much more discriminative\
    \ than HOG. Loose localization likely results from our use of bottom-up region\
    \ proposals and the positional invariance learned from pre-training the CNN for\
    \ whole-image classification. Column three shows how our simple bounding-box regression\
    \ method fixes many localization errors."
  Figure 6 Link: articels_figures_by_rev_year\2015\RegionBased_Convolutional_Networks_for_Accurate_Object_Detection_and_Segmentatio\figure_6.jpg
  Figure 6 caption: "Sensitivity to object characteristics. Each plot shows the mean\
    \ (over classes) normalized AP (see [64]) for the highest and lowest performing\
    \ subsets within six different object characteristics (occlusion, truncation,\
    \ bounding-box area, aspect ratio, viewpoint, part visibility). For example, bounding-box\
    \ area comprises the subsets extra-small, small, \u2026 , extra-large. We show\
    \ plots for our method (R-CNN) with and without fine-tuning and bounding-box regression\
    \ as well as for DPM voc-release5. Overall, fine-tuning does not reduce sensitivity\
    \ (the difference between max and min), but does substantially improve both the\
    \ highest and lowest performing subsets for nearly all characteristics. This indicates\
    \ that fine-tuning does more than simply improve the lowest performing subsets\
    \ for aspect ratio and bounding-box area, as one might conjecture based on how\
    \ we warp network inputs. Instead, fine-tuning improves robustness for all characteristics\
    \ including occlusion, truncation, viewpoint, and part visibility."
  Figure 7 Link: articels_figures_by_rev_year\2015\RegionBased_Convolutional_Networks_for_Accurate_Object_Detection_and_Segmentatio\figure_7.jpg
  Figure 7 caption: Different object proposal transformations. (A) the original object
    proposal at its actual scale relative to the transformed CNN inputs; (B) tightest
    square with context; (C) tightest square without context; (D) warp. Within each
    column and example proposal, the top row corresponds to p = 0 pixels of context
    padding while the bottom row has p = 16 pixels of context padding.
  Figure 8 Link: articels_figures_by_rev_year\2015\RegionBased_Convolutional_Networks_for_Accurate_Object_Detection_and_Segmentatio\figure_8.jpg
  Figure 8 caption: Example detections on the val 2 set from the configuration that
    achieved 31.0 percent mAP on val 2 . Each image was sampled randomly (these are
    not curated). All detections at precision greater than 0.5 are shown. Each detection
    is labeled with the predicted class and the precision value of that detection
    from the detector's precision-recall curve. Viewing digitally with zoom is recommended.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Ross Girshick
  Name of the last author: Jitendra Malik
  Number of Figures: 8
  Number of Tables: 7
  Number of authors: 4
  Paper title: Region-Based Convolutional Networks for Accurate Object Detection and
    Segmentation
  Publication Date: 2015-05-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Detection Average Precision (Percent) on VOC 2010 Test
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Detection Average Precision (Percent) on VOC 2007 Test
  Table 3 caption:
    table_text: TABLE 3 Detection Average Precision (Percent) on VOC 2007 Test for
      Two Different CNN Architectures
  Table 4 caption:
    table_text: TABLE 4 ILSVRC2013 Ablation Study of Data Usage Choices, Fine-Tuning,
      and Bounding-Box Regression
  Table 5 caption:
    table_text: TABLE 5 Segmentation Mean Accuracy (Percent) on VOC 2011 Validation
  Table 6 caption:
    table_text: TABLE 6 Per-Category Segmentation Accuracy (Percent) on the VOC 2011
      Validation Set
  Table 7 caption:
    table_text: TABLE 7 Segmentation Accuracy (Percent) on VOC 2011 Test
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2437384
- Affiliation of the first author: graduate school of advanced science and engineering,
    waseda university 41-304, 17 kikui-cho, shinjuku-ku, tokyo, japan
  Affiliation of the last author: "institute for systems and robotics, instituto superior\
    \ t\xE9cnico, universidade de lisboa, portugal"
  Figure 1 Link: articels_figures_by_rev_year\2015\On_Stereo_Confidence_Measures_for_Global_Methods_Evaluation_New_Model_and_Integr\figure_1.jpg
  Figure 1 caption: 'Top: Matching a pixel in one image to pixels at different disparities
    in another image. Middle: Cost for each disparity. Bottom: Confidence measure
    computed from the cost values. Dashed line indicates true disparity. Even if the
    minimum cost is wrong, true disparity should still be attributed some confidence.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\On_Stereo_Confidence_Measures_for_Global_Methods_Evaluation_New_Model_and_Integr\figure_2.jpg
  Figure 2 caption: "Distribution of costs at true disparity ( E 1, d \u2217 ) for\
    \ SSD (left) and BTSAD (right) cost functions on a 5\xD75 , 9\xD79 and 13\xD7\
    13 window. Horizontal axis represents the values of E 1, d \u2217 ."
  Figure 3 Link: articels_figures_by_rev_year\2015\On_Stereo_Confidence_Measures_for_Global_Methods_Evaluation_New_Model_and_Integr\figure_3.jpg
  Figure 3 caption: "The parametric models' cliff-maximum-and-tail of performance.\
    \ Both C(d\u2208GT ) badpx (first two rows) and AUC (last two rows) are shown\
    \ for the exponential and Merrell models. Results with the different cost functions\
    \ and window sizes are shown. Note how the curves and optimal parameters vary\
    \ both between images and cost functions. Figures for Matthies' model are not\
    \ shown since they can be obtained by linearly rescaling the horizontal axis of\
    \ the exponential model's figures (see equations (4), (5) and (7))."
  Figure 4 Link: articels_figures_by_rev_year\2015\On_Stereo_Confidence_Measures_for_Global_Methods_Evaluation_New_Model_and_Integr\figure_4.jpg
  Figure 4 caption: Performance of models with parameter values changes with prefiltering
    conditions. Results obtained from the Cones image of the indoors set.
  Figure 5 Link: articels_figures_by_rev_year\2015\On_Stereo_Confidence_Measures_for_Global_Methods_Evaluation_New_Model_and_Integr\figure_5.jpg
  Figure 5 caption: "C(d) given Merrell's model with ABP and ML parameters. Dashed\
    \ red line indicates true disparity d \u2217 as indicated by the dataset. Results\
    \ taken from pixel (364,150) of the Teddy image, as an example of ML's better\
    \ performance seen in Table 2. ML does not require ground-truth and leads here\
    \ to higher C( d \u2217 ) ."
  Figure 6 Link: articels_figures_by_rev_year\2015\On_Stereo_Confidence_Measures_for_Global_Methods_Evaluation_New_Model_and_Integr\figure_6.jpg
  Figure 6 caption: The KITTI residential area dataset [26] used for occupancy grid
    evaluation. Green regions on the bottom image represent ground-truth occupied
    cells. Blue points represent laser data at one of the frames.
  Figure 7 Link: articels_figures_by_rev_year\2015\On_Stereo_Confidence_Measures_for_Global_Methods_Evaluation_New_Model_and_Integr\figure_7.jpg
  Figure 7 caption: "Comparison of the performance of all models along time when used\
    \ with the occupancy grid algorithm. Each point represents a different instant\
    \ of time, while the first frame of the image sequence is marked with \u201Ct\
    \ = 0\u201D. \u201CMat ABP\u201D overlaps perfectly with \u201CExp ABP\u201D on\
    \ both cost functions, and \u201CMat ML\u201D overlaps perfectly with \u201CExp\
    \ ML\u201D for the BTSAD cost function."
  Figure 8 Link: articels_figures_by_rev_year\2015\On_Stereo_Confidence_Measures_for_Global_Methods_Evaluation_New_Model_and_Integr\figure_8.jpg
  Figure 8 caption: 'Reconstruction results obtained using a BTSAD 13 times 13 cost
    function with the two top models: Merrell''s model (top) and the HSM (bottom).
    Green squares represent true-positives (i.e., cells correctly classified as occupied),
    brown squares represent false-positives (i.e., cells incorrectly classified as
    occupied).'
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "Martim Brand\xE3o"
  Name of the last author: "Jos\xE9 Santos-Victor"
  Number of Figures: 8
  Number of Tables: 4
  Number of authors: 5
  Paper title: 'On Stereo Confidence Measures for Global Methods: Evaluation, New
    Model and Integration into Occupancy Grids'
  Publication Date: 2015-05-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average Best Performing Parameters Computed from the Indoors
      Set (Total 23 Images)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 On Average, How Close to Optimal Performance Do Models Get?\
      \ Distances Computed as |AU C Method (img)\u2212minAUC(img)|minAUC(img) and\
      \ | C Method (img)\u2212maxC(img)|maxC(img) Averaged over All Indoors Images"
  Table 3 caption:
    table_text: TABLE 3 Performance in AUC for All Models and Window Cost Functions,
      Averaged over a Test Set
  Table 4 caption:
    table_text: "TABLE 4 Performance in C(d\u2208GT ) badpx for All Models and Window\
      \ Cost Functions, Averaged over a Test Set"
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2437381
- Affiliation of the first author: department of electrical engineering and computer
    science, university of california at berkeley, berkeley, ca
  Affiliation of the last author: department of electrical engineering and computer
    science, university of california at berkeley, berkeley, ca
  Figure 1 Link: articels_figures_by_rev_year\2015\Intrinsic_Scene_Properties_from_a_Single_RGBD_Image\figure_1.jpg
  Figure 1 caption: 'In 1a we have the input to our model: an RGB image and a Kinect
    depth map from the NYU Depth Dataset [17]. In 1b we have the output of our model.
    Depth maps are visualized with hue corresponding to depth and luminance corresponding
    to slant, and surface normals are visualized with hue corresponding to orientation,
    and saturation and luminance corresponding to slant. Mixtures are visualized with
    hue corresponding to component in the mixture, and intensity corresponding to
    the probability assigned to that component. Illumination is visualized by rendering
    a coarse grid of spheres under the spatially-varying illumination. In 1c and 1d
    we show the reflectance and shading images produced by two intrinsic image techniques,
    where 1d is the state-of-the-art. See Figs. 6 and 7 for more examples.'
  Figure 10 Link: articels_figures_by_rev_year\2015\Intrinsic_Scene_Properties_from_a_Single_RGBD_Image\figure_10.jpg
  Figure 10 caption: After a model is recovered, the spherical harmonic illuminations
    can be replaced (here we use randomly generated illuminations) and the input image
    (left) can shown under a different illumination (right). The middle image is our
    attempt to produce similar re-lit images using only the inpainted depth maps in
    the NYU dataset, which look noticeably worse due to noise in the depth image and
    the fact that illumination and reflectance have not been decomposed.
  Figure 2 Link: articels_figures_by_rev_year\2015\Intrinsic_Scene_Properties_from_a_Single_RGBD_Image\figure_2.jpg
  Figure 2 caption: "A visualization of the embedding used in our shape and light\
    \ mixtures. In 2a, we have an input image. In 2b we have the output of multiscale\
    \ Pb on the input image, and in 2c we have the 16 smallest eigenvectors (ignoring\
    \ the eigenvector that is all 1 's) of mPb using the intervening contour cue [28].\
    \ Each shape's and light's \u201Cownership\u201D of the image is parametrized\
    \ by a 17-dimensional vector, which is projected onto the eigenvector basis and\
    \ passed through a softmax function to yield the probability of each pixel belonging\
    \ to each mixture component. 2d, 2e, and 2f are visualizations of three random\
    \ mixtures with eight components (such as U or V ) where the weight vectors (\
    \ \u03C8 or \u03C9 ) are generated randomly (sampled from a Gaussian)."
  Figure 3 Link: articels_figures_by_rev_year\2015\Intrinsic_Scene_Properties_from_a_Single_RGBD_Image\figure_3.jpg
  Figure 3 caption: We initialize the depth maps in our shape mixture by fitting a
    mixture of Gaussians to the (x,y,z) coordinates of depth-map pixels, and then
    fitting a plane to each Gaussian. 3a shows the raw depth map, 3b shows the posterior
    probability of each pixel under each mixture component, and 3c shows the fitted
    planes composed into one depth map according to hard assignments under the mixture
    of Gaussians.
  Figure 4 Link: articels_figures_by_rev_year\2015\Intrinsic_Scene_Properties_from_a_Single_RGBD_Image\figure_4.jpg
  Figure 4 caption: A visualization of how we introduce noise to Kinect images. In
    the first column we have a ground-truth depth-map, and in the second we have our
    corrupted version of it that we will use as a proxy for Kinect data. The third
    column shows the difference between the two, where we see the stripe-like noise
    introduced by quantization, as well as the noise near the boundaries of the objects
    introduced by shuffling and mis-aligning the image. The fourth column is a visualization
    of our error model, where we see error increases with depth, in accordance with
    our understanding of binocular stereo.
  Figure 5 Link: articels_figures_by_rev_year\2015\Intrinsic_Scene_Properties_from_a_Single_RGBD_Image\figure_5.jpg
  Figure 5 caption: 'Some test-set scenes from our pseudo-synthetic scene dataset.
    In 5a we have the input to our model: an RGB image and a noisy Kinect-like depth
    map. In 5b we have the depth map, surface normals, reflectance, shading, and spatially-varying
    illumination that our model produces, and the corresponding ground-truth scene
    properties on the bottom. In 5 c and 5d we show the shading and reflectance images
    produced by the best-performing intrinsic image algorithms.'
  Figure 6 Link: articels_figures_by_rev_year\2015\Intrinsic_Scene_Properties_from_a_Single_RGBD_Image\figure_6.jpg
  Figure 6 caption: 'In 6a we have the input to our model: an RGB image and a Kinect
    depth map from the NYU Depth Dataset [17]. In 6b we have the output of our model.
    Mixtures are visualized with hue corresponding to component, and intensity corresponding
    to probability. Illumination is visualized by rendering a coarse grid of spheres.'
  Figure 7 Link: articels_figures_by_rev_year\2015\Intrinsic_Scene_Properties_from_a_Single_RGBD_Image\figure_7.jpg
  Figure 7 caption: More results from the NYU Depth Dataset [17].
  Figure 8 Link: articels_figures_by_rev_year\2015\Intrinsic_Scene_Properties_from_a_Single_RGBD_Image\figure_8.jpg
  Figure 8 caption: After a model is recovered, the camera can be moved and the input
    image (left) can be shown from a different viewpoint (right). Such a warping could
    be produced using just the smoothed Kinect depth maps provided in the NYU dataset
    (middle), but these images have jagged artifacts at surface and normal discontinuities.
    Both renderings, of course, contain artifacts in occluded regions.
  Figure 9 Link: articels_figures_by_rev_year\2015\Intrinsic_Scene_Properties_from_a_Single_RGBD_Image\figure_9.jpg
  Figure 9 caption: "One output of our model is a denoised depth-map. In 9a we have\
    \ the RGB-D input to our model, demonstrating how noisy and incomplete the raw\
    \ Kinect depth map can be. 9b shows the inpainted normals and depth included in\
    \ the NYU dataset [17], where holes have been inpainted but there is still a great\
    \ deal of noise, and many fine-scale shape details are missing. 9c is from an\
    \ ablation of our model in which we just denoiseinpaint the raw depth map (\u201C\
    model H\u201D in our ablation study), and 9d is from our complete model. The NYU\
    \ depth map is noisy and does not model depth discontinuities, and our \u201C\
    denoising\u201D model tends to oversmooth the scene, but our complete model has\
    \ little noise while recovering much of the detail of the scene and correctly\
    \ separating objects into different layers."
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Jonathan T. Barron
  Name of the last author: Jitendra Malik
  Number of Figures: 10
  Number of Tables: 1
  Number of authors: 2
  Paper title: Intrinsic Scene Properties from a Single RGB-D Image
  Publication Date: 2015-06-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Our Results on the Test Set of Our Pseudo-Synthetic Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2439286
- Affiliation of the first author: "institut mines-t\xE9l\xE9comt\xE9l\xE9com lille,\
    \ cristal (umr 9189), lille, france"
  Affiliation of the last author: department of statistics, florida state university,
    fl, tallahassee
  Figure 1 Link: articels_figures_by_rev_year\2015\Action_Recognition_Using_RateInvariant_Analysis_of_Skeletal_Shape_Trajectories\figure_1.jpg
  Figure 1 caption: "Pre-shape space C , the trajectories \u03B1 1 and \u03B1 2 ,\
    \ the geodesic \u03B1(t) connecting arbitrary points on \u03B1 1 and \u03B1 2\
    \ , the tangent space at a shape R , and the shooting vector V from R towards\
    \ a point on C ."
  Figure 10 Link: articels_figures_by_rev_year\2015\Action_Recognition_Using_RateInvariant_Analysis_of_Skeletal_Shape_Trajectories\figure_10.jpg
  Figure 10 caption: Action sub-sequence detection in a long video of actor1 of Daily
    Activity 3D dataset. The test sequence is from a different actor.
  Figure 2 Link: articels_figures_by_rev_year\2015\Action_Recognition_Using_RateInvariant_Analysis_of_Skeletal_Shape_Trajectories\figure_2.jpg
  Figure 2 caption: Two examples of geodesic interpolation between arbitrary shapes
    using Eqn. 4.
  Figure 3 Link: articels_figures_by_rev_year\2015\Action_Recognition_Using_RateInvariant_Analysis_of_Skeletal_Shape_Trajectories\figure_3.jpg
  Figure 3 caption: Examples of mean and median shapes, and the decreases in energies
    during their computations.
  Figure 4 Link: articels_figures_by_rev_year\2015\Action_Recognition_Using_RateInvariant_Analysis_of_Skeletal_Shape_Trajectories\figure_4.jpg
  Figure 4 caption: Denoising trajectories using median filtering.
  Figure 5 Link: articels_figures_by_rev_year\2015\Action_Recognition_Using_RateInvariant_Analysis_of_Skeletal_Shape_Trajectories\figure_5.jpg
  Figure 5 caption: The shape sequence shown on the left is resampled using different
    sample sizes on the right and the bottom plot shows computational costs.
  Figure 6 Link: articels_figures_by_rev_year\2015\Action_Recognition_Using_RateInvariant_Analysis_of_Skeletal_Shape_Trajectories\figure_6.jpg
  Figure 6 caption: Examples of pairwise temporal alignment of trajectories using
    Eqn. (9) .
  Figure 7 Link: articels_figures_by_rev_year\2015\Action_Recognition_Using_RateInvariant_Analysis_of_Skeletal_Shape_Trajectories\figure_7.jpg
  Figure 7 caption: "Cross-sectional variance \u03C1(t) versus t before (red) and\
    \ after (blue) the temporal registration, for examples taken from MSR Action3D\
    \ dataset."
  Figure 8 Link: articels_figures_by_rev_year\2015\Action_Recognition_Using_RateInvariant_Analysis_of_Skeletal_Shape_Trajectories\figure_8.jpg
  Figure 8 caption: Impact of the temporal alignment and the changes in delta on SVM-based
    classification accuracy.
  Figure 9 Link: articels_figures_by_rev_year\2015\Action_Recognition_Using_RateInvariant_Analysis_of_Skeletal_Shape_Trajectories\figure_9.jpg
  Figure 9 caption: Classification rates of actions, taken individually, of our approach
    and Oreifej and Liu [15] (reported results are generated using the implementation
    available online), and Wang et al. [14], [18] .
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Boulbaba Ben Amor
  Name of the last author: Anuj Srivastava
  Number of Figures: 10
  Number of Tables: 6
  Number of authors: 3
  Paper title: Action Recognition Using Rate-Invariant Analysis of Skeletal Shape
    Trajectories
  Publication Date: 2015-06-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 NN-Classification Rate for Different Metrics
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison to Previous Studies on MSR Action-3D Dataset (Metric-Based
      Shape Analysis)
  Table 3 caption:
    table_text: TABLE 3 Comparison of Our Approach to Previous Methods on MSR Action-3D
      Dataset
  Table 4 caption:
    table_text: TABLE 4 Comparison of Our Approach to Previous Methods on 3D Action
      Pairs Dataset
  Table 5 caption:
    table_text: TABLE 5 Comparison of Our Approach to Previous Methods on MSR Daily
      Activity 3D Dataset
  Table 6 caption:
    table_text: TABLE 6 Computational Efficiency of the Proposed Pipeline
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2439257
- Affiliation of the first author: department of information engineering, the chinese
    university of hong kong, hong kong, china
  Affiliation of the last author: department of information engineering, the chinese
    university of hong kong, hong kong, china
  Figure 1 Link: articels_figures_by_rev_year\2015\Image_SuperResolution_Using_Deep_Convolutional_Networks\figure_1.jpg
  Figure 1 caption: The proposed super-resolution convolutional neural network surpasses
    the bicubic baseline with just a few training iterations, and outperforms the
    sparse-coding-based method [48] with moderate training. The performance may be
    further improved with more training iterations. More details are provided in Section
    4.4.1 (the Set5 dataset with an upscaling factor 3). The proposed method provides
    visually appealing reconstructed image.
  Figure 10 Link: articels_figures_by_rev_year\2015\Image_SuperResolution_Using_Deep_Convolutional_Networks\figure_10.jpg
  Figure 10 caption: The test convergence curve of SRCNN and results of other methods
    on the Set5 dataset.
  Figure 2 Link: articels_figures_by_rev_year\2015\Image_SuperResolution_Using_Deep_Convolutional_Networks\figure_2.jpg
  Figure 2 caption: Given a low-resolution image Y , the first convolutional layer
    of the SRCNN extracts a set of feature maps. The second layer maps these feature
    maps nonlinearly to high-resolution patch representations. The last layer combines
    the predictions within a spatial neighbourhood to produce the final high-resolution
    image F(Y) .
  Figure 3 Link: articels_figures_by_rev_year\2015\Image_SuperResolution_Using_Deep_Convolutional_Networks\figure_3.jpg
  Figure 3 caption: An illustration of sparse-coding-based methods in the view of
    a convolutional neural network.
  Figure 4 Link: articels_figures_by_rev_year\2015\Image_SuperResolution_Using_Deep_Convolutional_Networks\figure_4.jpg
  Figure 4 caption: Training with the much larger ImageNet dataset improves the performance
    over the use of 91 images.
  Figure 5 Link: articels_figures_by_rev_year\2015\Image_SuperResolution_Using_Deep_Convolutional_Networks\figure_5.jpg
  Figure 5 caption: The figure shows the first-layer filters trained on ImageNet with
    an upscaling factor 3. The filters are organized based on their respective variances.
  Figure 6 Link: articels_figures_by_rev_year\2015\Image_SuperResolution_Using_Deep_Convolutional_Networks\figure_6.jpg
  Figure 6 caption: Example feature maps of different layers.
  Figure 7 Link: articels_figures_by_rev_year\2015\Image_SuperResolution_Using_Deep_Convolutional_Networks\figure_7.jpg
  Figure 7 caption: A larger filter size leads to better results.
  Figure 8 Link: articels_figures_by_rev_year\2015\Image_SuperResolution_Using_Deep_Convolutional_Networks\figure_8.jpg
  Figure 8 caption: Comparisons between three-layer and four-layer networks.
  Figure 9 Link: articels_figures_by_rev_year\2015\Image_SuperResolution_Using_Deep_Convolutional_Networks\figure_9.jpg
  Figure 9 caption: Deeper structure does not always lead to better results.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.93
  Name of the first author: Chao Dong
  Name of the last author: Xiaoou Tang
  Number of Figures: 16
  Number of Tables: 5
  Number of authors: 4
  Paper title: Image Super-Resolution Using Deep Convolutional Networks
  Publication Date: 2015-06-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Results of Using Different Filter Numbers in SRCNN
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Average Results of PSNR (dB), SSIM, IFC, NQM, WPSNR (dB)
      and MSSIM on the Set5 Dataset
  Table 3 caption:
    table_text: TABLE 3 The Average Results of PSNR (dB), SSIM, IFC, NQM, WPSNR (dB)
      and MSSIM on the Set14 Dataset
  Table 4 caption:
    table_text: TABLE 4 The Average Results of PSNR (dB), SSIM, IFC, NQM, WPSNR (dB)
      and MSSIM on the BSD200 Dataset
  Table 5 caption:
    table_text: TABLE 5 Average PSNR (dB) of Different Channels and Training Strategies
      on the Set5 Dataset
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2439281
- Affiliation of the first author: university of michigan, ann arbor, mi
  Affiliation of the last author: department of computer science, stanford university,
    stanford, ca
  Figure 1 Link: articels_figures_by_rev_year\2015\Leveraging_the_Wisdom_of_the_Crowd_for_FineGrained_Recognition\figure_1.jpg
  Figure 1 caption: "The distinction between fine-grained categories is often very\
    \ subtle. It is crucial to identify the key features\u2014if the wrong features\
    \ are selected, the task can be very difficult. A small number of right features,\
    \ on the other hand, makes the task easy."
  Figure 10 Link: articels_figures_by_rev_year\2015\Leveraging_the_Wisdom_of_the_Crowd_for_FineGrained_Recognition\figure_10.jpg
  Figure 10 caption: Examples from the 10 different classes in the fine-grained BMW-10
    dataset.
  Figure 2 Link: articels_figures_by_rev_year\2015\Leveraging_the_Wisdom_of_the_Crowd_for_FineGrained_Recognition\figure_2.jpg
  Figure 2 caption: "In our approach, the crowd first plays the \u201CBubbles\u201D\
    \ game, trying to classify a blurred image into one of the two given categories.\
    \ During the game, the crowd is allowed to inspect circular regions (\u201Cbubbles\u201D\
    ), with a penalty of game points. In this process, discriminative regions are\
    \ revealed. Next, when a computer tries to recognize fine grained categories,\
    \ it collects the human selected bubbles and detects similar patterns on a image.\
    \ The detection responses are max-pooled to form a \u201CBubbleBank\u201D representation\
    \ that can be used for learning classifiers."
  Figure 3 Link: articels_figures_by_rev_year\2015\Leveraging_the_Wisdom_of_the_Crowd_for_FineGrained_Recognition\figure_3.jpg
  Figure 3 caption: The game UI. The goal is to correctly classify the center image
    into one of the two categories. A green bubble follows the cursor. The player
    can click to reveal the area inside the bubble. The more bubbles used, the fewer
    points the player can earn.
  Figure 4 Link: articels_figures_by_rev_year\2015\Leveraging_the_Wisdom_of_the_Crowd_for_FineGrained_Recognition\figure_4.jpg
  Figure 4 caption: "Examples of game results from AMT. The red boxes show zoomed-in\
    \ views of the bubbles. Top row: Bubbles drawn on images of \u201CCommon Tern\u201D\
    ' when compared against \u201CHerring Gull\u201D. Second row: Bubbles for \u201C\
    Common Tern\u201D on the same images of the top row when compared against \u201C\
    Arctic Tern\u201D. Third row: Bubbles for \u201CParakeet Auklet\u201D when compared\
    \ against \u201CHorned Puffin\u201D. Fourth row: Bubbles for \u201CParakeet Auklet\u201D\
    \ on the same images of the third row when compared against \u201CLeast Auklet\u201D\
    . Fifth row: Bubbles for \u201CBMW 3-Series 2007-2008\u201D when compared against\
    \ \u201CBMW 3-Series 2009-2011\u201D. Sixth row: Bubbles for \u201CBMW 3-Series\
    \ 2007-2008\u201D on the same images of the fifth row when compared against \u201C\
    BMW 5-Series 2007\u201D."
  Figure 5 Link: articels_figures_by_rev_year\2015\Leveraging_the_Wisdom_of_the_Crowd_for_FineGrained_Recognition\figure_5.jpg
  Figure 5 caption: Worker success rates for the Bubbles game on CUB-200-2010 (left)
    and BMW-10 (right). The vast majority of workers are able to pick the right category
    most of the time.
  Figure 6 Link: articels_figures_by_rev_year\2015\Leveraging_the_Wisdom_of_the_Crowd_for_FineGrained_Recognition\figure_6.jpg
  Figure 6 caption: Statistics of image area revealed in successful games. The area
    revealed in most of the successful games is small. Over 90 percent of the games
    use less than 10 percent of the object bounding box.
  Figure 7 Link: articels_figures_by_rev_year\2015\Leveraging_the_Wisdom_of_the_Crowd_for_FineGrained_Recognition\figure_7.jpg
  Figure 7 caption: 'Heat maps of bubbles averaged over multiple games played by multiple
    players. Top two rows: CUB-200-2010, bottom row: BMW-10.'
  Figure 8 Link: articels_figures_by_rev_year\2015\Leveraging_the_Wisdom_of_the_Crowd_for_FineGrained_Recognition\figure_8.jpg
  Figure 8 caption: An overview of how we estimate 3D geometry and extend BubbleBank
    to 3D. We use a handful of CAD models (a) and sample patches directly on the surface
    of the cars (b). These patches are rectified (c) to form a representation that
    is (to an extent) viewpoint invariant. We extract features by convolving the features
    for each bubble on this 3D representation (d) and pool over regions to form the
    final feature vector (e).
  Figure 9 Link: articels_figures_by_rev_year\2015\Leveraging_the_Wisdom_of_the_Crowd_for_FineGrained_Recognition\figure_9.jpg
  Figure 9 caption: Visualization of the points clicked on in games played on the
    BMW-10 dataset, projected into 3D using our method as described in Section 4.3.
    We observe that the majority of points clicked by users are on the front and rear
    of the car, which is indeed where discriminative features are typically located.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.66
  Name of the first author: Jia Deng
  Name of the last author: Li Fei-Fei
  Number of Figures: 13
  Number of Tables: 3
  Number of authors: 4
  Paper title: Leveraging the Wisdom of the Crowd for Fine-Grained Recognition
  Publication Date: 2015-06-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Results on CUB-14-2010
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results on CUB-200-2010
  Table 3 caption:
    table_text: TABLE 3 Results on BMW-10
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2439285
- Affiliation of the first author: institute of industrial science, the university
    of tokyo, tokyo, japan
  Affiliation of the last author: institute of industrial science, the university
    of tokyo, tokyo, japan
  Figure 1 Link: articels_figures_by_rev_year\2015\Reflectance_and_Fluorescence_Spectral_Recovery_via_Actively_Lit_RGB_Images\figure_1.jpg
  Figure 1 caption: (a) The scene captured under blue and green light. (b) Simulated
    relighting results with consideration of fluorescent effects. (c) The relighting
    results without considering fluorescence.
  Figure 10 Link: articels_figures_by_rev_year\2015\Reflectance_and_Fluorescence_Spectral_Recovery_via_Actively_Lit_RGB_Images\figure_10.jpg
  Figure 10 caption: Recovered reflectance boldsymbols , fluorescence absorption boldsymbola
    and emission boldsymbole spectra of the five fluorescent sheets in the color wheel
    of Fig. 4. The recovered results for five fluorescent sheets from top to bottom
    in the color wheel are shown from in the columns from left to right.
  Figure 2 Link: articels_figures_by_rev_year\2015\Reflectance_and_Fluorescence_Spectral_Recovery_via_Actively_Lit_RGB_Images\figure_2.jpg
  Figure 2 caption: An example of absorption and emission spectra from the McNamara
    and Boswell Fluorescence Spectral Dataset [28].
  Figure 3 Link: articels_figures_by_rev_year\2015\Reflectance_and_Fluorescence_Spectral_Recovery_via_Actively_Lit_RGB_Images\figure_3.jpg
  Figure 3 caption: Capture of reflective-fluorescent spectra at a single point in
    a scene. (a) When the reflective-fluorescent scene is illuminated, the reflectance
    spectrum can be measured at the same wavelength as the illuminant while the emission
    spectrum can be captured at longer wavelengths. (b) Varying the illuminant over
    different wavelengths and measuring the fluorescence emission spectrum at the
    same wavelength. The observation of different scaled emissions allow us to infer
    the fluorescence absorption each wavelength.
  Figure 4 Link: articels_figures_by_rev_year\2015\Reflectance_and_Fluorescence_Spectral_Recovery_via_Actively_Lit_RGB_Images\figure_4.jpg
  Figure 4 caption: Overview of the method. The input images are captured under varied
    illuminants. The reflectance spectrum and the chromaticity of the fluorescent
    component are optimized in a process of alternating iterations that exploits the
    illuminant-invariant chromaticity of fluorescence. After that, the fluorescence
    absorption and emission spectra are estimated. All these recovered spectra can
    be used to relight the reflective-fluorescent scene under new illuminants.
  Figure 5 Link: articels_figures_by_rev_year\2015\Reflectance_and_Fluorescence_Spectral_Recovery_via_Actively_Lit_RGB_Images\figure_5.jpg
  Figure 5 caption: The bases are used to describe (a) reflectance and (b) fluorescence
    absorption spectra.
  Figure 6 Link: articels_figures_by_rev_year\2015\Reflectance_and_Fluorescence_Spectral_Recovery_via_Actively_Lit_RGB_Images\figure_6.jpg
  Figure 6 caption: All test errors sorted in ascending order. 69 percent of cases
    were below the average error of 0.01.
  Figure 7 Link: articels_figures_by_rev_year\2015\Reflectance_and_Fluorescence_Spectral_Recovery_via_Actively_Lit_RGB_Images\figure_7.jpg
  Figure 7 caption: Examples of estimated emission spectra and their mean root square
    errors.
  Figure 8 Link: articels_figures_by_rev_year\2015\Reflectance_and_Fluorescence_Spectral_Recovery_via_Actively_Lit_RGB_Images\figure_8.jpg
  Figure 8 caption: Camera spectral sensitivity used in our experiments.
  Figure 9 Link: articels_figures_by_rev_year\2015\Reflectance_and_Fluorescence_Spectral_Recovery_via_Actively_Lit_RGB_Images\figure_9.jpg
  Figure 9 caption: The spectra of our nine colored illuminants and corresponding
    singular values.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ying Fu
  Name of the last author: Yoichi Sato
  Number of Figures: 16
  Number of Tables: 2
  Number of authors: 5
  Paper title: Reflectance and Fluorescence Spectral Recovery via Actively Lit RGB
    Images
  Publication Date: 2015-06-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average and Standard Deviations of the Converged upon Estimated
      Chromaticities under All 66 Initializations of E Are Shown in the Second and
      Third Columns
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Errors Between the Ground Truth and the Relit Results
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2439270
- Affiliation of the first author: allen institute for artificial intelligence, seattle,
    wa
  Affiliation of the last author: department of electrical and computer engineering,
    virginia tech, blacksburg, va
  Figure 1 Link: articels_figures_by_rev_year\2015\HumanMachine_CRFs_for_Identifying_Bottlenecks_in_Scene_Understanding\figure_1.jpg
  Figure 1 caption: Overview of the scene model of [7] that we analyze using human
    subjects. For clarity, not all connections in the model are shown here.
  Figure 10 Link: articels_figures_by_rev_year\2015\HumanMachine_CRFs_for_Identifying_Bottlenecks_in_Scene_Understanding\figure_10.jpg
  Figure 10 caption: Humans (H) and machines (M) have different performance for recognizing
    stuff and things segments. Humans are generally better at recognizing stuff, while
    machines are better at things recognition. Larger segments are generally easier
    to recognize. Note that in this experiment, Humans see only the segment pixels,
    while the machine classifier incorporates the information from the neighboring
    regions of the segments.
  Figure 2 Link: articels_figures_by_rev_year\2015\HumanMachine_CRFs_for_Identifying_Bottlenecks_in_Scene_Understanding\figure_2.jpg
  Figure 2 caption: Segment labeling interface. We ask the human subjects to choose
    the category that the segment belongs to. If the subjects are confused among a
    few categories, they have the option of choosing more than one answer.
  Figure 3 Link: articels_figures_by_rev_year\2015\HumanMachine_CRFs_for_Identifying_Bottlenecks_in_Scene_Understanding\figure_3.jpg
  Figure 3 caption: Human labeling results on isolated segments.
  Figure 4 Link: articels_figures_by_rev_year\2015\HumanMachine_CRFs_for_Identifying_Bottlenecks_in_Scene_Understanding\figure_4.jpg
  Figure 4 caption: Chow-Liu trees for humans and machine. The trees share several
    similarities.
  Figure 5 Link: articels_figures_by_rev_year\2015\HumanMachine_CRFs_for_Identifying_Bottlenecks_in_Scene_Understanding\figure_5.jpg
  Figure 5 caption: Human object recognition from image boundaries. We show subjects
    segments inside the object bounding box and ask them to recognize the category
    of the object. We show the segments with (left image) and without (right image)
    context.
  Figure 6 Link: articels_figures_by_rev_year\2015\HumanMachine_CRFs_for_Identifying_Bottlenecks_in_Scene_Understanding\figure_6.jpg
  Figure 6 caption: Human shape mask labeling interface. Human subjects were asked
    to draw the object boundaries along the segment contours.
  Figure 7 Link: articels_figures_by_rev_year\2015\HumanMachine_CRFs_for_Identifying_Bottlenecks_in_Scene_Understanding\figure_7.jpg
  Figure 7 caption: Human scene classification. Subjects were shown images at multiple
    resolutions. Subjects were asked to choose the scene category for each image.
  Figure 8 Link: articels_figures_by_rev_year\2015\HumanMachine_CRFs_for_Identifying_Bottlenecks_in_Scene_Understanding\figure_8.jpg
  Figure 8 caption: Average scenes for some example scene categories in MSRC.
  Figure 9 Link: articels_figures_by_rev_year\2015\HumanMachine_CRFs_for_Identifying_Bottlenecks_in_Scene_Understanding\figure_9.jpg
  Figure 9 caption: Impact of each component on machine scene understanding. Here
    we show the semantic segmentation, object detection, and scene recognition accuracies
    when a single component of the model is changed (removed, implemented by a machine
    (default), replaced by a human or replaced with ground truth). The evaluation
    measures are average per-class recall, average precision (AP), and average recall
    for segmentation, object detection, and scene recognition respectively.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.98
  Name of the first author: Roozbeh Mottaghi
  Name of the last author: Devi Parikh
  Number of Figures: 15
  Number of Tables: 5
  Number of authors: 5
  Paper title: Human-Machine CRFs for Identifying Bottlenecks in Scene Understanding
  Publication Date: 2015-06-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 MSRC-21 Dataset Information
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Human and Machine Segment Potentials Are Complementary
  Table 3 caption:
    table_text: TABLE 3 Segmentation Accuracy Obtained by Humans and by Resizing TextonBoost
      Window Size Outside the Model (in Isolation) and Inside the CRF Model
  Table 4 caption:
    table_text: TABLE 4 Additional Categories Used for Segment Classification in PASCAL
      VOC
  Table 5 caption:
    table_text: TABLE 5 Human Segmentation Accuracies with Increasing Information
      from the Model
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2437377
- Affiliation of the first author: state key lab of cad&cg, college of computer science,
    zhejiang university, hangzhou, china
  Affiliation of the last author: center for optical imagery analysis and learning
    (optimal), state key laboratory of transient optics and photonics, xi'an institute
    of optics and precision mechanics, chinese academy of sciences, xi'an, shaanxi,
    p.r. china
  Figure 1 Link: articels_figures_by_rev_year\2015\AOptimal_Projection_for_Image_Representation\figure_1.jpg
  Figure 1 caption: The average precision-scope curves of different algorithms for
    the first four feedback iterations. The AOP algorithm performs the best on the
    entire scope.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\AOptimal_Projection_for_Image_Representation\figure_2.jpg
  Figure 2 caption: Performance evaluation of the five learning algorithms for relevance
    feedback image retrieval. (a) Precision at top 10, (b) Precision at top 20, and
    (c) Precision at top 30.
  Figure 3 Link: articels_figures_by_rev_year\2015\AOptimal_Projection_for_Image_Representation\figure_3.jpg
  Figure 3 caption: Precision at top 20 for AOP with different values of the two parameters.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xiaofei He
  Name of the last author: Xuelong Li
  Number of Figures: 3
  Number of Tables: 1
  Number of authors: 4
  Paper title: A-Optimal Projection for Image Representation
  Publication Date: 2015-06-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Precision at Top 20 Returns of the 4 Algorithms after the
      1st Feedback Iteration (Mean%)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2439252
