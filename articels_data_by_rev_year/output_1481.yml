- Affiliation of the first author: university of tokyo, bunkyoku, tokyo, japan
  Affiliation of the last author: university of tokyo, tokyo, japan
  Figure 1 Link: articels_figures_by_rev_year\2019\Reconstruction_of_Geometric_and_Optical_Parameters_of_NonPlanar_Objects_with_Thi\figure_1.jpg
  Figure 1 caption: "Schematic diagram of thin film interference with multiple reflections.\
    \ n 1 , n 2 , and n 3 are the refractive indexes, d is the film thickness, \u03B8\
    \ 1 is the incident angle, and \u03B8 2 is the refracting angle. \u03B8 3 is the\
    \ outgoing angle."
  Figure 10 Link: articels_figures_by_rev_year\2019\Reconstruction_of_Geometric_and_Optical_Parameters_of_NonPlanar_Objects_with_Thi\figure_10.jpg
  Figure 10 caption: Estimated incident angle versus ground truth data. Panels (a)
    and (b) are the results for the S- and P-wave components, respectively. The ground
    truth is the preset angle of our experimental setup. Each mean value is the average
    of the estimated incident angles.
  Figure 2 Link: articels_figures_by_rev_year\2019\Reconstruction_of_Geometric_and_Optical_Parameters_of_NonPlanar_Objects_with_Thi\figure_2.jpg
  Figure 2 caption: "S- and P-polarization reflection intensity versus incident angle\
    \ \u03B8 1 . The blue line represents the perpendicular polarization (S-polarization\
    \ wave), while the green line shows the parallel polarization (P-polarization\
    \ wave)."
  Figure 3 Link: articels_figures_by_rev_year\2019\Reconstruction_of_Geometric_and_Optical_Parameters_of_NonPlanar_Objects_with_Thi\figure_3.jpg
  Figure 3 caption: Reflectance of a thin film at 10, 30, and 60 degrees shown as
    blue, green, and red curves, respectively. The refractive index was 1.37, and
    the film thickness was 400 nm. The circle represents the first peak value in the
    interference spectrum at each incident angle.
  Figure 4 Link: articels_figures_by_rev_year\2019\Reconstruction_of_Geometric_and_Optical_Parameters_of_NonPlanar_Objects_with_Thi\figure_4.jpg
  Figure 4 caption: "Optical measurement setup. The incident angle and the azimuth\
    \ angle are denoted as theta and \u03D5 , respectively. The H.S. camera in the\
    \ figure depicts a hyper-spectral camera with 81 bands. The x \u2032 - y \u2032\
    \ plane is defined as being perpendicular to the incident plane. The x - y plane\
    \ is parallel to the polarizer located in front of the camera. The azimuth angle\
    \ \u03D5 is measured in the x - y plane."
  Figure 5 Link: articels_figures_by_rev_year\2019\Reconstruction_of_Geometric_and_Optical_Parameters_of_NonPlanar_Objects_with_Thi\figure_5.jpg
  Figure 5 caption: Diagrams and optical photographs of the apparatus.
  Figure 6 Link: articels_figures_by_rev_year\2019\Reconstruction_of_Geometric_and_Optical_Parameters_of_NonPlanar_Objects_with_Thi\figure_6.jpg
  Figure 6 caption: "Estimated incident angle \u03B8 1 derived using our method. The\
    \ data in panels (a) and (b) are the results for \u03B8 1 being derived using\
    \ the S-wave and P-wave components, respectively. The scale bar indicates the\
    \ relationship between the image brightness and the incident angle \u03B8 1 .\
    \ The gray scale varies in increments of 10 degrees."
  Figure 7 Link: articels_figures_by_rev_year\2019\Reconstruction_of_Geometric_and_Optical_Parameters_of_NonPlanar_Objects_with_Thi\figure_7.jpg
  Figure 7 caption: "Film thickness d on the hemisphere estimated from the calculated\
    \ incident angle \u03B8 1 . The data in panels (a) and (b) show the film thickness\
    \ results d derived by using S-wave and P-wave components, respectively. The scale\
    \ bar indicates the relationship between the color values in the images and the\
    \ film thickness d . The color scale varies in increments of 10 nm."
  Figure 8 Link: articels_figures_by_rev_year\2019\Reconstruction_of_Geometric_and_Optical_Parameters_of_NonPlanar_Objects_with_Thi\figure_8.jpg
  Figure 8 caption: Schematic diagram of the experimental setup. A film was used as
    the target object with a thin film surface. Here, L means lens, P is the polarizer,
    A is the analyzer, SM is the spectrometer, and Diff is the diffuser.
  Figure 9 Link: articels_figures_by_rev_year\2019\Reconstruction_of_Geometric_and_Optical_Parameters_of_NonPlanar_Objects_with_Thi\figure_9.jpg
  Figure 9 caption: Peak wavelengths in the visual region as a function of the estimated
    incident angle. Each triangle is the averaged (mean) wavelength from the measured
    reflectance values. The -marks depict the calculated incident angle from the peak
    wavelength measured by an interference spectroscopy device. Panel (a) shows the
    data for the S-wave peak wavelength, while panel (b) shows the data for the P-wave
    peak wavelength.
  First author gender probability: 0.99
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yoshie Kobayashi
  Name of the last author: Katsushi Ikeuchi
  Number of Figures: 19
  Number of Tables: 0
  Number of authors: 6
  Paper title: Reconstruction of Geometric and Optical Parameters of Non-Planar Objects
    with Thin Film
  Publication Date: 2019-08-26 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2937515
- Affiliation of the first author: department of computer science, johns hopkins university,
    baltimore, md, usa
  Affiliation of the last author: department of computer science, johns hopkins university,
    baltimore, md, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Deep_Differentiable_Random_Forests_for_Age_Estimation\figure_1.jpg
  Figure 1 caption: (a) The large variation in facial appearance across different
    persons of the same age. (b) Facial images of a person from childhood to adulthood.
    Note that, Facial aging effects appear as changes in the shape of the face during
    childhood and changes in skin texture during adulthood, respectively.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Deep_Differentiable_Random_Forests_for_Age_Estimation\figure_2.jpg
  Figure 2 caption: "Illustration of (a) a deep label distribution learning forest\
    \ (DLDLF) and (b) a deep regression forest (DRF). Each forest consists of two\
    \ trees. The top red circles denote the output units of the function f parameterized\
    \ by \u0398 . Here, they are the units of a fully-connected (FC) layer in a CNN.\
    \ The blue and green circles are split nodes and leaf nodes, respectively. in\
    \ each forest, two index functions \u03C6 1 and \u03C6 2 are randomly assigned\
    \ to the two trees respectively before training and then fixed. The black dash\
    \ arrows indicate the correspondence between the split nodes of the two trees\
    \ and the output units of the FC layer. Note that, one output unit may correspond\
    \ to the split nodes belonging to different trees. Each tree has independent leaf\
    \ node distribution \u03C0 (denoted by distribution histograms and curves in the\
    \ leaf nodes of the DLDLF and the DRF, respectively). The output of the forest\
    \ is a mixture of the tree predictions. f(\u22C5;\u0398) and \u03C0 are learned\
    \ jointly in an end-to-end manner."
  Figure 3 Link: articels_figures_by_rev_year\2019\Deep_Differentiable_Random_Forests_for_Age_Estimation\figure_3.jpg
  Figure 3 caption: 'The subtree rooted at node n : T n and its left and right subtrees:
    T n l and T n r .'
  Figure 4 Link: articels_figures_by_rev_year\2019\Deep_Differentiable_Random_Forests_for_Age_Estimation\figure_4.jpg
  Figure 4 caption: "Generated label distribution for a facial image at the chronological\
    \ age of 20 ( \u03B1=20 )."
  Figure 5 Link: articels_figures_by_rev_year\2019\Deep_Differentiable_Random_Forests_for_Age_Estimation\figure_5.jpg
  Figure 5 caption: Some examples of MORPH [25], FG-NET [26] and CACD [27]. The number
    below each image is the chronological age of each subject.
  Figure 6 Link: articels_figures_by_rev_year\2019\Deep_Differentiable_Random_Forests_for_Age_Estimation\figure_6.jpg
  Figure 6 caption: "The dynamics of the averaged entropy H(\u0398;S) over the training\
    \ set S for DLDLFs (left) and DRFs (right) at the beginning of tree learning."
  Figure 7 Link: articels_figures_by_rev_year\2019\Deep_Differentiable_Random_Forests_for_Age_Estimation\figure_7.jpg
  Figure 7 caption: (a) Histogram of data samples with respect to age on MORPH [25]
    (Setup I). (b) Visualization of the learned leaf node distributions in our DLDLF.
    (c) Visualization of the learned leaf node distributions in our DRF. The distributions
    held by different leaf nodes are in different (gradually varied) colors, which
    are best viewed in color.
  Figure 8 Link: articels_figures_by_rev_year\2019\Deep_Differentiable_Random_Forests_for_Age_Estimation\figure_8.jpg
  Figure 8 caption: "Performance changes by varying (a) tree number, (b) tree depth\
    \ and (c) standard deviation \u03B1 on MORPH [25] (Setup I)."
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Wei Shen
  Name of the last author: Alan Yuille
  Number of Figures: 8
  Number of Tables: 6
  Number of authors: 6
  Paper title: Deep Differentiable Random Forests for Age Estimation
  Publication Date: 2019-08-26 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Performance Comparison on MORPH [25] (Setup I)(: The Value
      Is Read from the CS Curve Shown in the Reference)'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Performance Comparison on MORPH [25] (Setup II)(: The Value
      Is Read from the CS Curve Shown in the Reference)'
  Table 3 caption:
    table_text: TABLE 3 Performance Comparison on MORPH [25] (Setting III)
  Table 4 caption:
    table_text: 'TABLE 4 Performance Comparison on FG-NET [26](: The Value Is Read
      from the CS Curve Shown in the Reference)'
  Table 5 caption:
    table_text: TABLE 5 Performance Comparison on CACD (Measured by MAE) [27]
  Table 6 caption:
    table_text: TABLE 6 Ablation Study on MORPH [43] (Setup I)
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2937294
- Affiliation of the first author: b-dat and cicaeet, school of automation, nanjing
    university of information science and technology, nanjing, china
  Affiliation of the last author: school of computer science and information engineering,
    hefei university of technology, hefei, anhui, china
  Figure 1 Link: articels_figures_by_rev_year\2019\Matrix_Completion_with_Deterministic_Sampling_Theories_and_Methods\figure_1.jpg
  Figure 1 caption: The unseen future values of time series are essentially a special
    type of missing data.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Matrix_Completion_with_Deterministic_Sampling_Theories_and_Methods\figure_2.jpg
  Figure 2 caption: Illustrations of the sampled submatrices.
  Figure 3 Link: articels_figures_by_rev_year\2019\Matrix_Completion_with_Deterministic_Sampling_Theories_and_Methods\figure_3.jpg
  Figure 3 caption: "Left: The relative condition number \u03B3 \u03A9, \u03A9 T (\
    \ L 0 ) versus the missing rate 1\u2212 \u03C1 0 at m=500 . Middle: The relative\
    \ condition number versus the matrix size m . Right: Plotting the recovery performance\
    \ of convex optimization as a function of the missing rate."
  Figure 4 Link: articels_figures_by_rev_year\2019\Matrix_Completion_with_Deterministic_Sampling_Theories_and_Methods\figure_4.jpg
  Figure 4 caption: "Visualizing the configurations of \u03A9 used in our simulations.\
    \ The white points correspond to the locations of the observed entries. In these\
    \ two examples, 90 percent entries of the matrix are missing."
  Figure 5 Link: articels_figures_by_rev_year\2019\Matrix_Completion_with_Deterministic_Sampling_Theories_and_Methods\figure_5.jpg
  Figure 5 caption: "Comparing IsoDP with convex optimization and LRFD. The numbers\
    \ plotted on the above figures are the success rates within 20 random trials.\
    \ The white and black areas mean \u201Csucceed\u201D and \u201Cfail\u201D, respectively.\
    \ Here, the success is in a sense that PSN R dB \u2265 40."
  Figure 6 Link: articels_figures_by_rev_year\2019\Matrix_Completion_with_Deterministic_Sampling_Theories_and_Methods\figure_6.jpg
  Figure 6 caption: Visualizing the regions in which the isomeric condition holds.
  Figure 7 Link: articels_figures_by_rev_year\2019\Matrix_Completion_with_Deterministic_Sampling_Theories_and_Methods\figure_7.jpg
  Figure 7 caption: An example image from the Oxford dinosaur sequence and the locations
    of the observed entries in the data matrix of trajectories. In this dataset, 74.29
    percent entries of the trajectory matrix are missing.
  Figure 8 Link: articels_figures_by_rev_year\2019\Matrix_Completion_with_Deterministic_Sampling_Theories_and_Methods\figure_8.jpg
  Figure 8 caption: Some examples of the originally incomplete and fully restored
    trajectories. (a) The original incomplete trajectories. (b) The trajectories restored
    by convex optimization [10]. (c) The trajectories restored by LRFD [29]. (d) The
    trajectories restored by IsoDP.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Guangcan Liu
  Name of the last author: Meng Wang
  Number of Figures: 8
  Number of Tables: 2
  Number of authors: 4
  Paper title: 'Matrix Completion with Deterministic Sampling: Theories and Methods'
  Publication Date: 2019-08-27 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Mean Square Error (MSE) on the Oxford Dinosaur Sequence
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 MSE on the MovieLens Dataset
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2937869
- Affiliation of the first author: tklndst, college of computer science, nankai university,
    tianjin, china
  Affiliation of the last author: oxford university, oxford, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2019\ResNet_A_New_MultiScale_Backbone_Architecture\figure_1.jpg
  Figure 1 caption: Multi-scale representations are essential for various vision tasks,
    such as perceiving boundaries, regions, and semantic categories of the target
    objects. Even for the simplest recognition tasks, perceiving information from
    very different scales is essential to understand parts, objects (e.g., sofa, table,
    and cup in this example), and their surrounding context (e.g., on the table context
    contributes to recognizing the black blob).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\ResNet_A_New_MultiScale_Backbone_Architecture\figure_2.jpg
  Figure 2 caption: Comparison between the bottleneck block and the proposed Res2Net
    module (the scale dimension s=4 ).
  Figure 3 Link: articels_figures_by_rev_year\2019\ResNet_A_New_MultiScale_Backbone_Architecture\figure_3.jpg
  Figure 3 caption: The Res2Net module can be integrated with the dimension cardinality
    [56] (replace conv with group conv) and SE [25] blocks.
  Figure 4 Link: articels_figures_by_rev_year\2019\ResNet_A_New_MultiScale_Backbone_Architecture\figure_4.jpg
  Figure 4 caption: Visualization of class activation mapping [45], using ResNet-50
    and Res2Net-50 as backbone networks.
  Figure 5 Link: articels_figures_by_rev_year\2019\ResNet_A_New_MultiScale_Backbone_Architecture\figure_5.jpg
  Figure 5 caption: Test precision on the CIFAR-100 dataset with regard to the model
    size, by changing cardinality (ResNeXt-29), depth (ResNeXt), and scale (Res2Net-29).
  Figure 6 Link: articels_figures_by_rev_year\2019\ResNet_A_New_MultiScale_Backbone_Architecture\figure_6.jpg
  Figure 6 caption: Visualization of semantic segmentation results [8], using ResNet-101
    and Res2Net-101 as backbone networks.
  Figure 7 Link: articels_figures_by_rev_year\2019\ResNet_A_New_MultiScale_Backbone_Architecture\figure_7.jpg
  Figure 7 caption: Examples of salient object detection [24] results, using ResNet-50
    and Res2Net-50 as backbone networks, respectively.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.95
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Shang-Hua Gao
  Name of the last author: Philip Torr
  Number of Figures: 7
  Number of Tables: 10
  Number of authors: 6
  Paper title: 'Res2Net: A New Multi-Scale Backbone Architecture'
  Publication Date: 2019-08-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Top-1 and Top-5 Test Error on the ImageNet Dataset
  Table 10 caption:
    table_text: TABLE 10 Performance of Key-Points Estimation on the COCO Validation
      Set
  Table 2 caption:
    table_text: TABLE 2 Top-1 and Top-5 Test Error ( % %) of Deeper Networks on the
      ImageNet Dataset
  Table 3 caption:
    table_text: TABLE 3 Top-1 and Top-5 Test Error ( % %) of Res2Net-50 with Different
      Scales on the ImageNet Dataset
  Table 4 caption:
    table_text: TABLE 4 Top-1 Test Error ( % %) and Model Size on the CIFAR-100 Dataset
  Table 5 caption:
    table_text: TABLE 5 Object Detection Results on the PASCAL VOC07 and COCO Datasets,
      Measured Using AP ( % %) and APIoU = 0.5 ( % %)
  Table 6 caption:
    table_text: TABLE 6 Average Precision (AP) and Average Recall (AR) of Object Detection
      with Different Sizes on the COCO Dataset
  Table 7 caption:
    table_text: TABLE 7 Performance of Semantic Segmentation on PASCAL VOC12 Val Set
      Using Res2Net-50 with Different Scales
  Table 8 caption:
    table_text: TABLE 8 Performance of Instance Segmentation on the COCO Dataset Using
      Res2Net-50 with Different Scales
  Table 9 caption:
    table_text: TABLE 9 Salient Object Detection Results on Different Datasets, Measured
      Using F-Measure and Mean Absolute Error (MAE)
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2938758
- Affiliation of the first author: "chalmers university of technology, g\xF6teborg,\
    \ sweden"
  Affiliation of the last author: school of science, edith cowan university, joondalup,
    wa, australia
  Figure 1 Link: articels_figures_by_rev_year\2019\Deterministic_Approximate_Methods_for_Maximum_Consensus_Robust_Fitting\figure_1.jpg
  Figure 1 caption: "(a) Sample point set p j , q j N j=1 . (b) A plot of \u03A8(\u03B8\
    ) in R 2 based on the sample point set. Each unique color represents a specific\
    \ consensus value. Regions corresponding to the maximum consensus value are indicated\
    \ in yellow."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Deterministic_Approximate_Methods_for_Maximum_Consensus_Robust_Fitting\figure_2.jpg
  Figure 2 caption: Two-dimensional analogy of balanced (left) and unbalanced (right)
    data generated in our experiments. The results of RANSAC, least squares, and our
    method initialized with the former two methods are shown. Observe that least squares
    is heavily biased under unbalanced data, but EP is able to recover from the bad
    initialization. (For clarity, the results of AM variants are not plotted as they
    are very close to EP-RS and EP-LSQ).
  Figure 3 Link: articels_figures_by_rev_year\2019\Deterministic_Approximate_Methods_for_Maximum_Consensus_Robust_Fitting\figure_3.jpg
  Figure 3 caption: Results for linear regression ( d=8 dimensions). (a)(b) Balanced
    data; (c)(d) Unbalanced data.
  Figure 4 Link: articels_figures_by_rev_year\2019\Deterministic_Approximate_Methods_for_Maximum_Consensus_Robust_Fitting\figure_4.jpg
  Figure 4 caption: Qualitative results of local refinement methods on (a,b,c) fundamental
    matrix estimation, (d,e,f) linearized homography estimation (g,h,i) homography
    estimation with geometric distance, and (j,k,l) affinity estimation. Green and
    red lines represent detected inliers and outliers. For clarity, only 100 inliersoutliers
    are plotted.
  Figure 5 Link: articels_figures_by_rev_year\2019\Deterministic_Approximate_Methods_for_Maximum_Consensus_Robust_Fitting\figure_5.jpg
  Figure 5 caption: Qualitative results of EP-RS on triangulation.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.89
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Huu Le
  Name of the last author: David Suter
  Number of Figures: 5
  Number of Tables: 6
  Number of authors: 5
  Paper title: Deterministic Approximate Methods for Maximum Consensus Robust Fitting
  Publication Date: 2019-09-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Fundamental Matrix Estimation Results (with Algebraic Error)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Homography Estimation Results (with Algebraic Error)
  Table 3 caption:
    table_text: TABLE 3 Homography Estimation Results (with Geometric Transfer Error)
  Table 4 caption:
    table_text: TABLE 4 Affinity Estimation Results (with Geometric Transfer Error)
  Table 5 caption:
    table_text: TABLE 5 Triangulation Results (with Geometric Transfer Error)
  Table 6 caption:
    table_text: TABLE 6 Total Inliers and Runtime of Triangulation for 11,595 Selected
      Points with More Than 10 Views
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2939307
- Affiliation of the first author: department of computer science, cornell university,
    ithaca, ny, usa
  Affiliation of the last author: department of electrical engineering and data science
    institute, columbia university, new york, ny, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\On_the_Global_Geometry_of_SphereConstrained_Sparse_Blind_Deconvolution\figure_1.jpg
  Figure 1 caption: "Geometry on the \u2113 2 ball for fixed a 0 and generic x 0 .\
    \ Left: The objective \u03C6(a) in a low dimensional setting a\u2208 S 2 \u2013\
    \ dark blue represents small values while dark red represents large values. All\
    \ local minima are close to signed shift truncations of the ground truth a 0 ,\
    \ with a 0 itself achieving global minimum. The green lines indicate regions where\
    \ a are ill-posed as convolutional kernels. Right: A shift truncation a achieves\
    \ a local minimum of \u03C6(a) in a high dimensional setting. Shown here is the\
    \ ground truth y= a 0 \u229B x 0 , a 0 , and x 0 (top right) versus their respective\
    \ recovered quantities a\u229Bx,a , and x (bottom right)."
  Figure 10 Link: articels_figures_by_rev_year\2019\On_the_Global_Geometry_of_SphereConstrained_Sparse_Blind_Deconvolution\figure_10.jpg
  Figure 10 caption: 'Multi kernel blind deconvolution on real STM image: Input image
    (left) and recovered convolutional kernels and their corresponding activation
    signals (right).'
  Figure 2 Link: articels_figures_by_rev_year\2019\On_the_Global_Geometry_of_SphereConstrained_Sparse_Blind_Deconvolution\figure_2.jpg
  Figure 2 caption: "Geometry on the \u2113 1 ball: The trivial spike convolutional\
    \ kernel is the global minimizer, while the ground truth [13,13,13] becomes a\
    \ local minimizer."
  Figure 3 Link: articels_figures_by_rev_year\2019\On_the_Global_Geometry_of_SphereConstrained_Sparse_Blind_Deconvolution\figure_3.jpg
  Figure 3 caption: 'Zero padding a signed shift truncation: The original signed shift
    truncation (left) and the corresponding zero padded one (right).'
  Figure 4 Link: articels_figures_by_rev_year\2019\On_the_Global_Geometry_of_SphereConstrained_Sparse_Blind_Deconvolution\figure_4.jpg
  Figure 4 caption: "Function geometry with varying \u03BB : The objective \u03C6\
    (a) over the hemisphere for \u03BB= 10 \u22121 , 10 \u22123 , 10 \u22126 . Here\
    \ a 0 = P S 2 [[1,8,2]] and x 0 \u223CBer(.1)\u2299N(0,1) . The ground truth kernel\
    \ a 0 and its shift-truncations P S 2 [[8,2,0]] , P S 2 [[0,1,8]] are shown in\
    \ red, and sign-flips P S 2 [\u2212[8,2,0]] , P S 2 [\u2212[0,1,8]] are shown\
    \ in magenta. Notice that each signed shift truncation shown on the hemisphere\
    \ is close to a corresponding local minima, while as the objective landscape becomes\
    \ less regularized as \u03BB shrinks."
  Figure 5 Link: articels_figures_by_rev_year\2019\On_the_Global_Geometry_of_SphereConstrained_Sparse_Blind_Deconvolution\figure_5.jpg
  Figure 5 caption: "Recovery accuracy [7]. Left: Phase transition diagram from noise-free\
    \ simulated results. Right: Performance of Algorithm 1 in the presence of additive\
    \ noise in the measurement; the error increases for small \u03B8 due to a lack\
    \ of samples, whereas extremely large \u03B8 leads to algorithmic failure."
  Figure 6 Link: articels_figures_by_rev_year\2019\On_the_Global_Geometry_of_SphereConstrained_Sparse_Blind_Deconvolution\figure_6.jpg
  Figure 6 caption: 'STM data analysis. From left to right: The microscopy images,
    extracted convolutional kernels (defect patterns), and their respective Fourier
    magnitude images.'
  Figure 7 Link: articels_figures_by_rev_year\2019\On_the_Global_Geometry_of_SphereConstrained_Sparse_Blind_Deconvolution\figure_7.jpg
  Figure 7 caption: 'Blur kernel recovery error: Cumulative distributions of recovered
    blur kernel error from synthetic (left), noised (middle), and real (right) blurry
    images.'
  Figure 8 Link: articels_figures_by_rev_year\2019\On_the_Global_Geometry_of_SphereConstrained_Sparse_Blind_Deconvolution\figure_8.jpg
  Figure 8 caption: 'Non-blind restoration error: Cumulative distributions of deblurred
    image error from synthetic (left), noised (middle), and real (right) blurry images.'
  Figure 9 Link: articels_figures_by_rev_year\2019\On_the_Global_Geometry_of_SphereConstrained_Sparse_Blind_Deconvolution\figure_9.jpg
  Figure 9 caption: 'Multi kernel blind deconvolution on synthetic data: Input image
    (left) and the recovered convolutional kernels of Stage I and Stage II of the
    algorithm (right).'
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yuqian Zhang
  Name of the last author: John Wright
  Number of Figures: 10
  Number of Tables: 0
  Number of authors: 6
  Paper title: On the Global Geometry of Sphere-Constrained Sparse Blind Deconvolution
  Publication Date: 2019-09-03 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2939237
- Affiliation of the first author: Not Available
  Affiliation of the last author: Not Available
  Figure 1 Link: Not Available
  Figure 1 caption: Not Available
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.62
  Name of the first author: Li Liu
  Name of the last author: Rama Chellappa
  Number of Figures: Not Available
  Number of Tables: 0
  Number of authors: 6
  Paper title: "Guest Editors\u2019 Introduction to the Special Section on Compact\
    \ and Efficient Feature Representation and Learning in Computer Vision"
  Publication Date: 2019-09-04 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2935426
- Affiliation of the first author: tsinghua university, beijing, beijing, china
  Affiliation of the last author: tsinghua university, beijing, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2019\Learning_Partbased_Convolutional_Features_for_Person_ReIdentification\figure_1.jpg
  Figure 1 caption: "Partition strategies of several deep part models in person re-identification.\
    \ (a) to (e): Partitioned parts by GLAD [5], PDC [3], DPL [2], Hydra-plus [6]\
    \ and PAR [4], respectively. (f): The proposed PCB+RPP is able to accommodate\
    \ various part partition strategies such as uniform partition (shown in this figure),\
    \ pose estimation, and human parsing, and then refines each part. Both PAR [4]\
    \ and our method conduct \u201Csoft\u201D partition, but our method differs significantly\
    \ from [4], as detailed in Section 2."
  Figure 10 Link: articels_figures_by_rev_year\2019\Learning_Partbased_Convolutional_Features_for_Person_ReIdentification\figure_10.jpg
  Figure 10 caption: "Some failure parsing results by DeepLab-V2 [29] on Market-1501.\
    \ We visualize 3 body regions, i.e., the head, upper-body and lower-body. a) The\
    \ \u201Chead\u201D region is lost; b) the \u201Clower-body\u201D region is lost."
  Figure 2 Link: articels_figures_by_rev_year\2019\Learning_Partbased_Convolutional_Features_for_Person_ReIdentification\figure_2.jpg
  Figure 2 caption: "Framework of PCB. The input image goes forward through the stacked\
    \ convolutional layers in the backbone network to form a 3D tensor T . PCB extracts\
    \ several parts on T and then averages the column vectors in the same part into\
    \ a single column vector g . A following 1\xD71 kernel-sized convolutional layer\
    \ transforms p pieces of g into p pieces of dimension-reduced h . Finally, each\
    \ column vector h is input into a classifier, respectively. Each classifier is\
    \ implemented with a fully-connected (FC) layer and a sequential Softmax function.\
    \ During testing, either the p pieces of g or h are concatenated to form the final\
    \ descriptor of the input image."
  Figure 3 Link: articels_figures_by_rev_year\2019\Learning_Partbased_Convolutional_Features_for_Person_ReIdentification\figure_3.jpg
  Figure 3 caption: Three partition strategies that can be adopted by PCB, including
    (a) partition by pose estimation (PCB-P), (b) partition by human parsing (PCB-H)
    and (c) the uniform partition (PCB-U). In (a) and (b), three body parts (head,
    upper-body, lower-body) are shown. PCB-H may optionally take two more body parts
    (the foreground and the shoes). In (c), the number of stripes is set to 6 after
    experimental validation.
  Figure 4 Link: articels_figures_by_rev_year\2019\Learning_Partbased_Convolutional_Features_for_Person_ReIdentification\figure_4.jpg
  Figure 4 caption: 'Visualization of within-part inconsistency in PCB-U. Left: T
    is equally partitioned to p=6 horizontal stripes (parts) during training. Right:
    Every column vector in T is denoted with a small rectangle and painted in the
    color of its closest part. We observe that outlier column vectors are very common,
    indicating part partition errors. Please note that all the visualized pixels are
    on tensor T , rather than on the raw image. However, we superpose the raw image
    on the left only for intuitive illustration.'
  Figure 5 Link: articels_figures_by_rev_year\2019\Learning_Partbased_Convolutional_Features_for_Person_ReIdentification\figure_5.jpg
  Figure 5 caption: Structure of PCB in combination with refined part pooling (RPP).
    The 3D tensor T is denoted simply by a rectangle instead of a cube because we
    focus on the spatial partition. Layers before T are omitted because they remain
    unchanged compared with Fig. 2. A part classifier predicts the probability of
    each column vector belonging to the p parts. Then each part is sampled from all
    the column vectors with the corresponding probability as the sampling weight.
    GAP denotes global average pooling.
  Figure 6 Link: articels_figures_by_rev_year\2019\Learning_Partbased_Convolutional_Features_for_Person_ReIdentification\figure_6.jpg
  Figure 6 caption: "Comparison between two different partitioning approaches, i.e.,\
    \ partitioning the raw image and partitioning the tensor T , on Market-1501. We\
    \ evaluate the performance of using every single part-feature (i.e., \u201Cpart1\u201D\
    \ to \u201Cpart6\u201D) and the concatenated feature (i.e., \u201Call\u201D),\
    \ respectively."
  Figure 7 Link: articels_figures_by_rev_year\2019\Learning_Partbased_Convolutional_Features_for_Person_ReIdentification\figure_7.jpg
  Figure 7 caption: Parameters analysis. (a) The impact of image size. We use the
    original down-sampling rate and the half down-sampling rate. (b) The impact of
    part numbers. We compare PCB with and without RPP. (c) The impact of part overlapping.
    When the number of overlapped pixels is positive, the neighboring stripes on T
    overlap with each other. Instead, when the number of overlapped pixels is negative,
    the neighboring stripes are apart from each other.
  Figure 8 Link: articels_figures_by_rev_year\2019\Learning_Partbased_Convolutional_Features_for_Person_ReIdentification\figure_8.jpg
  Figure 8 caption: Visualization of the refined parts under various numbers of parts
    p . When p = 8 or 12, some parts repeat with others or become empty.
  Figure 9 Link: articels_figures_by_rev_year\2019\Learning_Partbased_Convolutional_Features_for_Person_ReIdentification\figure_9.jpg
  Figure 9 caption: "Some pose estimation examples generated by Open Pose [12] on\
    \ Market-1501. The corresponding body part partition results are shown. The detected\
    \ landmarks are annotated by red dots. The extracted \u201Chead\u201D, \u201C\
    upper-body\u201D and \u201Clower-body\u201D regions are bounded by red, green\
    \ and black bounding boxes, respectively. a) An acceptable result; b) a failure\
    \ case on \u201Chead\u201D; c) a failure case on \u201Cupper-body\u201D; d) a\
    \ failure case on \u201Clower-body\u201D."
  First author gender probability: 0.8
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yifan Sun
  Name of the last author: Shengjin Wang
  Number of Figures: 14
  Number of Tables: 8
  Number of authors: 6
  Paper title: Learning Part-based Convolutional Features for Person Re-Identification
  Publication Date: 2019-09-05 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of the Proposed Method with IDE and 2 Variants
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of the Proposed Method with the State of the Art
      on Market-1501
  Table 3 caption:
    table_text: TABLE 3 Comparison with the State of the Art on DukeMTMC-reID and
      CUHK03
  Table 4 caption:
    table_text: TABLE 4 Comparison with the State of the Art on MSMT17
  Table 5 caption:
    table_text: TABLE 5 Comparison of PCB-P, PCB-H and PCB-U on Market-1501 and DukeMTMC-reID
      Datasets
  Table 6 caption:
    table_text: TABLE 6 Evaluation of the Compatibility to Two Common Training Losses
  Table 7 caption:
    table_text: TABLE 7 Evaluation of PCB under the Cross-Dataset Setting
  Table 8 caption:
    table_text: TABLE 8 Comparison between RPP (Induced Training) and PAR (Attention
      Mechanism) on Market-1501
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2938523
- Affiliation of the first author: department of computer science, huaqiao university,
    xiamen, china
  Affiliation of the last author: department of computer science, hong kong baptist
    university, hong kong sar, china
  Figure 1 Link: articels_figures_by_rev_year\2019\MTFH_A_Matrix_TriFactorization_Hashing_Framework_for_Efficient_CrossModal_Retrie\figure_1.jpg
  Figure 1 caption: Two typical examples show that one image may be annotated with
    multi-labels and one text paragraph may depict multiple relevant images. Meanwhile,
    the heterogeneous modalities often have different feature dimensions, and the
    hash codes of heterogeneous modalities stored in database may have equal or unequal
    lengths in practice.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\MTFH_A_Matrix_TriFactorization_Hashing_Framework_for_Efficient_CrossModal_Retrie\figure_2.jpg
  Figure 2 caption: Single-modal retrieval results obtained by Fast Supervised Discrete
    Hashing (FSDH) [14] and tested with different hash lengths.
  Figure 3 Link: articels_figures_by_rev_year\2019\MTFH_A_Matrix_TriFactorization_Hashing_Framework_for_Efficient_CrossModal_Retrie\figure_3.jpg
  Figure 3 caption: The proposed generalized and flexible MTFH framework, which explicitly
    correlates the heterogeneous modalities. Note that, MTFH can handle both paired
    or unpaired multi-modal data collections, and equal or varying hash length encoding
    scenarios.
  Figure 4 Link: articels_figures_by_rev_year\2019\MTFH_A_Matrix_TriFactorization_Hashing_Framework_for_Efficient_CrossModal_Retrie\figure_4.jpg
  Figure 4 caption: Precision-recall curves obtained by different approaches and tested
    on different datasets, in which the representative code lengths, i.e., 32, 64
    and 128 bits, are selected for evaluation.
  Figure 5 Link: articels_figures_by_rev_year\2019\MTFH_A_Matrix_TriFactorization_Hashing_Framework_for_Efficient_CrossModal_Retrie\figure_5.jpg
  Figure 5 caption: The representative topK-precision curves tested on MIRFlickr and
    NUS-WIDE-100k datasets.
  Figure 6 Link: articels_figures_by_rev_year\2019\MTFH_A_Matrix_TriFactorization_Hashing_Framework_for_Efficient_CrossModal_Retrie\figure_6.jpg
  Figure 6 caption: Cross-modal retrieval results obtained by the proposed MTFH with
    varying hash length settings, and the best results are highlighted in bold.
  Figure 7 Link: articels_figures_by_rev_year\2019\MTFH_A_Matrix_TriFactorization_Hashing_Framework_for_Efficient_CrossModal_Retrie\figure_7.jpg
  Figure 7 caption: Cross-modal retrieval results by fixing the hash length of one
    modality and varying the hash length of another modality.
  Figure 8 Link: articels_figures_by_rev_year\2019\MTFH_A_Matrix_TriFactorization_Hashing_Framework_for_Efficient_CrossModal_Retrie\figure_8.jpg
  Figure 8 caption: The illustration of different hash representation for single-modal
    retrieval and cross-modal retrieval tasks.
  Figure 9 Link: articels_figures_by_rev_year\2019\MTFH_A_Matrix_TriFactorization_Hashing_Framework_for_Efficient_CrossModal_Retrie\figure_9.jpg
  Figure 9 caption: Effects of different optimization schemes, parameter values and
    training set sizes.
  First author gender probability: 0.6
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.71
  Name of the first author: Xin Liu
  Name of the last author: Yiu-Ming Cheung
  Number of Figures: 9
  Number of Tables: 8
  Number of authors: 4
  Paper title: 'MTFH: A Matrix Tri-Factorization Hashing Framework for Efficient Cross-Modal
    Retrieval'
  Publication Date: 2019-09-10 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Comparisons of Cross-Modal Retrieval Performance
      (mAP) on Different Datasets, and the Best Results Are Highlighted in Bold
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Representative Cross-Modal Retrieval Performance (mAP50) Obtained
      by Different Approaches, and the Best Results Are Highlighted in Bold
  Table 3 caption:
    table_text: TABLE 3 Recall Rates Obtained by MTFH and Tested with Different Hash
      Lengths on Wiki Dataset
  Table 4 caption:
    table_text: TABLE 4 Retrieval Results (mAP) of Unpaired Multi-Modal Data Collections,
      and the Best Results Are Highlighted in Bold
  Table 5 caption:
    table_text: TABLE 5 Results (mAP) of Single-Modal Retrieval on Paired Multi-Modal
      Data, and the Best Results Are Highlighted in Bold
  Table 6 caption:
    table_text: TABLE 6 Results (mAP) of Cross-Modal Retrieval on CNN Visual Features,
      and the Best Results Are Highlighted in Bold
  Table 7 caption:
    table_text: TABLE 7 Results (mAP) of Different Optimization Schemes on Wiki Dataset
  Table 8 caption:
    table_text: TABLE 8 The Retrieval Time Tested on 100 Queries (Seconds Averaged
      in Five Runs), and mAP Scores Recorded Under Similar Memory Budget
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2940446
- Affiliation of the first author: department of computer science, university of maryland,
    college park, md, usa
  Affiliation of the last author: department of computer science, university of maryland,
    college park, md, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\TopologyAware_NonRigid_Point_Cloud_Registration\figure_1.jpg
  Figure 1 caption: Non-rigid registration under a close-to-open topology change.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\TopologyAware_NonRigid_Point_Cloud_Registration\figure_2.jpg
  Figure 2 caption: Overview of our topology-aware non-rigid registration pipeline.
  Figure 3 Link: articels_figures_by_rev_year\2019\TopologyAware_NonRigid_Point_Cloud_Registration\figure_3.jpg
  Figure 3 caption: 'Topological event detections on our dataset. Each row corresponds
    to a different sequence. First column: source color frame with event mask overlays
    (blue for contact, red for separation). Second column: target color frame. Third
    column: our topological event detections overlaid on the source geometry (blue
    for contact, red for separation).'
  Figure 4 Link: articels_figures_by_rev_year\2019\TopologyAware_NonRigid_Point_Cloud_Registration\figure_4.jpg
  Figure 4 caption: 'Warping results on our dynamic topology dataset, with focus given
    to separation events. Each row corresponds to a different sequence. First column:
    our topological event detections overlaid on the source geometry (same as last
    column of Fig. 3, but rendered from a different viewpoint). Columns 2-5: warped
    source geometry under VO-SF, PD-Flow, F-Warp (our topology-agnostic baseline algorithm),
    and FB-Warp (our proposed approach).'
  Figure 5 Link: articels_figures_by_rev_year\2019\TopologyAware_NonRigid_Point_Cloud_Registration\figure_5.jpg
  Figure 5 caption: 'Long-term model-to-frame registration. The boxing sequence (first
    row) is from [8]. The hat and alex sequences (second and third rows) are from
    [15]. First column: color image at initial scene state. Second column: scene model
    in its initial state. Third column: color image at final scene state. Fourth and
    fifth columns: scene model in its final state after continuous model-to-frame
    deformation by our F-Warp (baseline) and FB-Warp (proposed) algorithms, respectively.
    The models for the alex sequence are rendered from a different viewpoint to better
    visualize the artifacts introduced by F-Warp (see Section 4.4).'
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Konstantinos Zampogiannis
  Name of the last author: Yiannis Aloimonos
  Number of Figures: 5
  Number of Tables: 3
  Number of authors: 3
  Paper title: Topology-Aware Non-Rigid Point Cloud Registration
  Publication Date: 2019-09-10 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Evaluation on the MPI Sintel Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Topology Change Event Detection
  Table 3 caption:
    table_text: TABLE 3 Registration Under Close-To-Open Topology
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2940655
