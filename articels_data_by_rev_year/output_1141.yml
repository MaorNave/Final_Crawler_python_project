- Affiliation of the first author: "max planck institute for informatics, saarbr\xFC\
    cken, germany"
  Affiliation of the last author: "max planck institute for informatics, saarbr\xFC\
    cken, germany"
  Figure 1 Link: articels_figures_by_rev_year\2018\ZeroShot_LearningA_Comprehensive_Evaluation_of_the_Good_the_Bad_and_the_Ugly\figure_1.jpg
  Figure 1 caption: "Zero-shot learning (ZSL) versus generalized zero-shot learning\
    \ (GZSL): At training time, for both cases the images and attributes of the seen\
    \ classes ( Y tr ) are available. At test time, in the ZSL setting, the learned\
    \ model is evaluated only on unseen classes ( Y ts ) whereas in GZSL setting,\
    \ the search space contains both training and test classes ( Y tr \u222A Y ts\
    \ ). To facilitate classification without labels, both tasks use some form of\
    \ side information, e.g., attributes. The attributes are annotated per class,\
    \ therefore the labeling cost is significantly reduced."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\ZeroShot_LearningA_Comprehensive_Evaluation_of_the_Good_the_Bad_and_the_Ugly\figure_2.jpg
  Figure 2 caption: Comparing AWA1 [1] and our AWA2 in terms of number of images (Left)
    and t-SNE embedding of the image features (the embedding is learned on AWA1 and
    AWA2 simultaneously, therefore the figures are comparable). AWA2 follows a similar
    distribution as AWA1 and it contains more examples.
  Figure 3 Link: articels_figures_by_rev_year\2018\ZeroShot_LearningA_Comprehensive_Evaluation_of_the_Good_the_Bad_and_the_Ugly\figure_3.jpg
  Figure 3 caption: 'Robustness of 10 methods evaluated on SUN, CUB, AWA1, aPY using
    3 validation set splits (results are on the same test split). Top: original split,
    Bottom: proposed split (Image embeddings = ResNet). We measure top-1 accuracy
    in %.'
  Figure 4 Link: articels_figures_by_rev_year\2018\ZeroShot_LearningA_Comprehensive_Evaluation_of_the_Good_the_Bad_and_the_Ugly\figure_4.jpg
  Figure 4 caption: "Ranking 12 models by setting parameters on three validation splits\
    \ on the standard (SS, left) and proposed (PS, right) setting. Element (i,j) indicates\
    \ number of times model i ranks at j th over all 4\xD73 observations. Models are\
    \ ordered by their mean rank (displayed in brackets)."
  Figure 5 Link: articels_figures_by_rev_year\2018\ZeroShot_LearningA_Comprehensive_Evaluation_of_the_Good_the_Bad_and_the_Ugly\figure_5.jpg
  Figure 5 caption: Zero-Shot Learning experiments on Imagenet, measuring Top-1, Top-5
    and Top-10 accuracy. 23 H = classes with 23 hops away from ImageNet1K training
    classes ( mathcal Ytr ), M500M1KM5K denote 500, 1K and 5K most populated classes,
    L500L1KL5K denote 500, 1K and 5K least populated classes, All = The remaining
    20K categories of ImageNet.
  Figure 6 Link: articels_figures_by_rev_year\2018\ZeroShot_LearningA_Comprehensive_Evaluation_of_the_Good_the_Bad_and_the_Ugly\figure_6.jpg
  Figure 6 caption: 'GZSL on Imagenet, measuring Top-1, Top-5 and Top-10 accuracy.
    23H: classes with 23 hops away from ImageNet1K mathcal Ytr , M500M1KM5K: 5001K5K
    most populated classes, L500L1KL5K: 5001K5K least populated classes, All: Remaining
    20K classes.'
  Figure 7 Link: articels_figures_by_rev_year\2018\ZeroShot_LearningA_Comprehensive_Evaluation_of_the_Good_the_Bad_and_the_Ugly\figure_7.jpg
  Figure 7 caption: 'Ranking 13 models on the proposed split (PS) in generalized zero-shot
    learning setting. Top-Left: Top-1 accuracy (T1) is measured on unseen classes
    (ts), Top-Right: T1 is measured on seen classes (tr), Bottom: T1 is measured on
    Harmonic mean (H).'
  Figure 8 Link: articels_figures_by_rev_year\2018\ZeroShot_LearningA_Comprehensive_Evaluation_of_the_Good_the_Bad_and_the_Ugly\figure_8.jpg
  Figure 8 caption: Zero-shot (left) and generalized zero-shot learning (right) results
    in the transductive learning setting on our Proposesd Split.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.7
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.99
  Name of the first author: Yongqin Xian
  Name of the last author: Zeynep Akata
  Number of Figures: 8
  Number of Tables: 6
  Number of authors: 4
  Paper title: "Zero-Shot Learning\u2014A Comprehensive Evaluation of the Good, the\
    \ Bad and the Ugly"
  Publication Date: 2018-07-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Statistics for SUN [16], CUB [17], AWA1 [1], Proposed AWA2,
      aPY [18] in Terms of Size, Granularity, Number of Attributes, Number of Classes
      in Y tr Ytr and Y ts Yts, Number of Images at Training and Test Time for Standard
      Split (SS) and Our Proposed Splits (PS)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Reproducing Zero-Shot Results with Methods That Have a Public
      Implementation: O = Original Results, R = Reproduced Using Provided Image Features
      and Code'
  Table 3 caption:
    table_text: TABLE 3 Zero-Shot Learning Results on SUN, CUB, AWA1, AWA2 and aPY
      Using SS = Standard Split, PS = Proposed Split with ResNet Features
  Table 4 caption:
    table_text: 'TABLE 4 Cross-Dataset Evaluation over AWA1 and AWA2 in Zero-Shot
      Learning Setting on the Proposed Splits: Left of the Colon Indicates the Training
      Set and Right of the Colon Indicates the Test Set, e.g., AWA1:AWA2 Means That
      the Model Is Trained on the Train Set of AWA1 and Evaluated on the Test Set
      of AWA2'
  Table 5 caption:
    table_text: 'TABLE 5 ImageNet with Different Splits: 23 H = Classes with 23 Hops
      Away from the Y tr Ytr of ImageNet1K, 5001K5K Most Populated Classes, 5001K5K
      Least Populated Classes, All = The Remaining 20K Categories of ImageNet ( Y
      ts Yts)'
  Table 6 caption:
    table_text: 'TABLE 6 Generalized Zero-Shot Learning on Proposed Split (PS) Measuring
      ts = Top-1 Accuracy on Y ts Yts, tr = Top-1 Accuracy on Y tr Ytr, H = Harmonic
      Mean (CMT: CMT with Novelty Detection)'
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2857768
- Affiliation of the first author: school of engineering, brown university, providence,
    ri, usa
  Affiliation of the last author: school of engineering, brown university, providence,
    ri, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\Solving_Square_Jigsaw_Puzzle_by_Hierarchical_Loop_Constraints\figure_1.jpg
  Figure 1 caption: "An example of puzzle assembly process with hierarchical loop\
    \ constraints. 1) The pairwise matches are selected as candidates when the pairwise\
    \ compatibility measures are greater than a threshold. 2) We discover small loops\
    \ of increasing order, hierarchical loops (consensus configurations), from the\
    \ collection of the pairwise matching candidates. The first small loops discovered,\
    \ made from four matched pairs, are of order 2. We then iteratively build higher\
    \ order loops from the lower order loops, e.g., 4 loops of order 2 are assembled\
    \ into loops of order 3, i.e., loops of 3 by 3 pieces, and so on. We continue\
    \ assembling loops of loops in a bottom-up fashion until the algorithm finds maximum\
    \ loop(s). 3) Independent of the pairwise compatibility measure, we add more pairwise\
    \ matches iteratively if they are harmonious with existing hierarchical loops\
    \ and, as a result, increase the order of existing hierarchical loops. 4) The\
    \ algorithm then proceeds top-down to merge unused loop assemblies onto the dominant\
    \ structures. This top-down merging is more permissive than the bottom-up assembly\
    \ and has no \u201Cloop\u201D constraint, as long as two hierarchical loops share\
    \ at least two pieces in consistent position they are merged. At a particular\
    \ level, remaining loops are considered for merging in order of priority, where\
    \ priority is determined by the mean of pixel variances on boundaries of the pieces\
    \ in that loops. After all possible merges are done for loops of a particular\
    \ order, the unused loops are broken into their sub-loops and the merging is attempted\
    \ again with smaller loops. If the reconstruction is not rectangular, trimming\
    \ and filling steps are required. The puzzle in this example is very challenging\
    \ because all the pieces are only 14 by 14 pixels. For illustration purpose, false\
    \ configurations are highlighted in red. Note that when the order of hierarchical\
    \ loops is equal to or greater than 3, there are no false configurations in this\
    \ example."
  Figure 10 Link: articels_figures_by_rev_year\2018\Solving_Square_Jigsaw_Puzzle_by_Hierarchical_Loop_Constraints\figure_10.jpg
  Figure 10 caption: Reconstruction performance of unknown orientation piece puzzle
    with various piece size and number of pieces. The size of pieces are S = 28, 14,
    10, 7 pixels. The number of puzzle pieces are K = 221, 432, 1064 from the MIT
    dataset. When the size of a piece is small S = 14 pixels, our algorithm reduces
    reconstruction error up to 75 percent.
  Figure 2 Link: articels_figures_by_rev_year\2018\Solving_Square_Jigsaw_Puzzle_by_Hierarchical_Loop_Constraints\figure_2.jpg
  Figure 2 caption: "An example of single iteration of increasing consensus configurations.\
    \ The squares and the numbers within represent pieces and their unique IDs respectively.\
    \ From the Bag of Consensus Configurations (BCC), \u2460 we search for order 3\
    \ incomplete consensus from three loops of order 2. Note that the assemblies in\
    \ the BCC are always square, and order N consensus configuration means that the\
    \ dimension of the assembly is N by N pieces. \u2461 we find pairwise matching\
    \ candidates to complete this order 3 incomplete consensus. Since pairwise dissimilarity\
    \ measures between top of piece 7 and bottom of pieces 9, 90 and 20 are lower\
    \ than the threshold ( \u03B8 ), the right side of piece 2 to the left side of\
    \ those pieces (each of ID 9,90,20 ) are considered as new pairwise matching candidates,\
    \ independent of the pairwise compatibility measure. Likewise, pieces returning\
    \ lower dissimilarity measures with the right side of piece ID 2 could propose\
    \ more pairwise matching candidates with piece ID 7. \u2462 If the new pairwise\
    \ match completes at least two consensus configurations of order 3, we accept\
    \ the pairwise matches. If the newly accepted pairwise match is true, this addition\
    \ sometimes generates 2 or more additional larger sized consensus configurations.\
    \ In this example, two consensus configurations of order 3 are generated additionally.\
    \ \u2463 If the candidate match conflicts with existing larger consensus configurations\
    \ (in this example, order 3 or larger configurations), we reject the pairwise\
    \ match. Otherwise, \u2464 we hold off the decision to the next iteration. We\
    \ note that the decision is made only by existing consensus configurations in\
    \ the BCC, independent of the pairwise compatibility strength."
  Figure 3 Link: articels_figures_by_rev_year\2018\Solving_Square_Jigsaw_Puzzle_by_Hierarchical_Loop_Constraints\figure_3.jpg
  Figure 3 caption: Importance of derivative information for defining dissimilarity
    between pieces. Defining dissimilarity between two pieces could be seen as establishing
    a distance metric between two discretized curves where the two curves represent
    the values along the boundaries of the two pieces. Prior works define L2 norm
    as dissimilarity of two curves. In (a), two curves are more dissimilar than two
    curves in (b). But, the L2 norm of the two curves in (b) is greater than that
    in (a) which is not desired. The L2 norm of the two curves does not exactly represent
    the dissimilarity between two curves. By considering the derivatives in a direction
    along the boundary, we can compensate the erroneous measure. The value of the
    L2 norm of the derivative curve error in (a) is greater than that in (b) which
    is correct. For this reason, we incorporate derivative information along the adjoining
    boundaries for defining pairwise compatibility measure.
  Figure 4 Link: articels_figures_by_rev_year\2018\Solving_Square_Jigsaw_Puzzle_by_Hierarchical_Loop_Constraints\figure_4.jpg
  Figure 4 caption: All cases of loop of four pieces. The circle and the cross represent
    a true positive and a false positive pairwise matching respectively.
  Figure 5 Link: articels_figures_by_rev_year\2018\Solving_Square_Jigsaw_Puzzle_by_Hierarchical_Loop_Constraints\figure_5.jpg
  Figure 5 caption: Precision lower bound of loop of four pairwise matching according
    to precision and recall of pairwise matching, m 1 =18, m 2 =24 .
  Figure 6 Link: articels_figures_by_rev_year\2018\Solving_Square_Jigsaw_Puzzle_by_Hierarchical_Loop_Constraints\figure_6.jpg
  Figure 6 caption: The precision of pair matches changes as a function of order of
    hierarchical loops (a) for different dissimilarity measures, (b) at different
    local matching thresholds, (c) at different noise levels with the MIT dataset,
    and (d) with different datasets (with the MGC measure). The leftmost point represents
    the performance of the local matching by itself with no loop constraints. The
    noise levels in (c) are high magnitude because the input images are 16 bit. The
    piece size is S=28 pixels.
  Figure 7 Link: articels_figures_by_rev_year\2018\Solving_Square_Jigsaw_Puzzle_by_Hierarchical_Loop_Constraints\figure_7.jpg
  Figure 7 caption: Pairwise compatibility measure performance comparison. The figures
    represent precision and recall curves for Sum of Squared Distance (SSD) [3], MGC
    [21] and proposed measure on unknown orientation piece puzzle, and precision and
    recall values when each side of the piece possesses a single neighbor which returns
    lowest dissimilarity for various datasets.
  Figure 8 Link: articels_figures_by_rev_year\2018\Solving_Square_Jigsaw_Puzzle_by_Hierarchical_Loop_Constraints\figure_8.jpg
  Figure 8 caption: Component analysis of the proposed compatibility measure. We analyzed
    the components of our proposed pairwise compatibility measure with unknown orientation
    piece puzzles from the MIT dataset (the size of piece is S=28 , the number of
    pieces is K=432 ). Our method performs better than the MGC [21] without derivative
    information. And, with the derivative information, the performance is enhanced
    more.
  Figure 9 Link: articels_figures_by_rev_year\2018\Solving_Square_Jigsaw_Puzzle_by_Hierarchical_Loop_Constraints\figure_9.jpg
  Figure 9 caption: Qualitative reconstruction performance on unknown orientation
    piece puzzles. This figure shows assembly results by Gallagher [21], proposed
    algorithm without Growing Hierarchical Loops step and proposed algorithm. The
    prior algorithms return false configurations in the red rectangle regions whereas
    our proposed algorithm correctly assembles those challenging pieces.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Kilho Son
  Name of the last author: David B. Cooper
  Number of Figures: 14
  Number of Tables: 3
  Number of authors: 3
  Paper title: Solving Square Jigsaw Puzzle by Hierarchical Loop Constraints
  Publication Date: 2018-07-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Reconstruction Performance on Known Orientation Piece Puzzles
      from the MIT, McGill and Pomeranz Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Reconstruction Performance on Unknown Orientation Piece puzzles
      from the MIT Dataset
  Table 3 caption:
    table_text: TABLE 3 Reconstruction Performance on Unknown Orientation Piece Puzzles
      from the McGill and the Pomeranz Datasets
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2857776
- Affiliation of the first author: inria galen team, gif-sur-yvette, france
  Affiliation of the last author: university of montreal, montreal, canada
  Figure 1 Link: articels_figures_by_rev_year\2018\Scattering_Networks_for_Hybrid_Representation_Learning\figure_1.jpg
  Figure 1 caption: A scattering network. A J concatenates the averaged signals (cf.
    Section 3.1).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Scattering_Networks_for_Hybrid_Representation_Learning\figure_2.jpg
  Figure 2 caption: Trees of computations for a Scattering Transform. (a) Corresponds
    to the traversal used in the ScatNet software package and (b) To our current implementation
    (PyScatWave).
  Figure 3 Link: articels_figures_by_rev_year\2018\Scattering_Networks_for_Hybrid_Representation_Learning\figure_3.jpg
  Figure 3 caption: Reconstructed images with subcaption indicating J,err( S J ),err(x)
    . See Section 3.4 for details of the reconstruction approach.
  Figure 4 Link: articels_figures_by_rev_year\2018\Scattering_Networks_for_Hybrid_Representation_Learning\figure_4.jpg
  Figure 4 caption: "Architecture of the SLE, which is a cascade of 3 1\xD71 convolutions\
    \ followed by 3 fully connected layers. The ReLU non-linearities are included\
    \ inside the F i blocks."
  Figure 5 Link: articels_figures_by_rev_year\2018\Scattering_Networks_for_Hybrid_Representation_Learning\figure_5.jpg
  Figure 5 caption: Histogram of F 1 amplitude for first and second order coefficients.
    The vertical lines indicate a threshold that is used in Section 4.3 to sparsify
    F 1 .
  Figure 6 Link: articels_figures_by_rev_year\2018\Scattering_Networks_for_Hybrid_Representation_Learning\figure_6.jpg
  Figure 6 caption: Energy Omega 1lbrace Frbrace (left) and Omega 2lbrace Frbrace
    (right) from Eq. (1) for given angular frequencies.
  Figure 7 Link: articels_figures_by_rev_year\2018\Scattering_Networks_for_Hybrid_Representation_Learning\figure_7.jpg
  Figure 7 caption: Adversarial examples obtained from a Scattering Transform followed
    by a linear classifier on ImageNet.
  Figure 8 Link: articels_figures_by_rev_year\2018\Scattering_Networks_for_Hybrid_Representation_Learning\figure_8.jpg
  Figure 8 caption: Samples generated by the Scattering-DCGAN. See Section 6.2 for
    details.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Edouard Oyallon
  Name of the last author: Eugene Belilovsky
  Number of Figures: 8
  Number of Tables: 11
  Number of authors: 7
  Paper title: Scattering Networks for Hybrid Representation Learning
  Publication Date: 2018-07-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of the Computation Time of a Scattering Transform
      on CPUGPU
  Table 10 caption:
    table_text: TABLE 10 Architecture of the Discriminator and Generator of the Scattering-DCGAN
  Table 2 caption:
    table_text: TABLE 2 Top 1 and Top 5 Percentage Accuracy Reported from One Single
      Crop on ILSVRC2012
  Table 3 caption:
    table_text: TABLE 3 ILSVRC-2012 Validation Accuracy (Single Crop) of Hybrid Scattering
      and 10 Layer ResNet, a Comparable 18 Layer ResNet, and Other Well Known Benchmarks
  Table 4 caption:
    table_text: TABLE 4 Accuracy of Scattering Compared to Similar Architectures on
      CIFAR10
  Table 5 caption:
    table_text: TABLE 5 Structure of Scattering and ResNet-10 Architectures Used in
      ImageNet Experiments
  Table 6 caption:
    table_text: TABLE 6 Structure of Scattering and Wide ResNet Hybrid Architectures
      Used in Small Sample Experiments
  Table 7 caption:
    table_text: TABLE 7 Mean Accuracy of a Hybrid Scattering in a Limited Sample Situation
      on CIFAR-10 Dataset
  Table 8 caption:
    table_text: TABLE 8 Mean Accuracy of a Hybrid CNN on the STL-10 Dataset
  Table 9 caption:
    table_text: TABLE 9 Comparison of the Top-1 Accuracy from Unsupervised and Self-Supervised
      Representation, on the ImageNet Dataset, Evaluated as Ours with a Linear Classifier
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2855738
- Affiliation of the first author: noahs ark laboratory, huawei technologies co.,
    ltd., beijing, p.r. china
  Affiliation of the last author: ubtech sydney artificial intelligence centre, school
    of information technologies, university of sydney, darlington, nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2018\Packing_Convolutional_Neural_Networks_in_the_Frequency_Domain\figure_1.jpg
  Figure 1 caption: Visualization of original filters and filters after discarding
    top-3 high frequency coefficients in the DCT frequency domain.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Packing_Convolutional_Neural_Networks_in_the_Frequency_Domain\figure_2.jpg
  Figure 2 caption: The diagram of the proposed CNN compression method in the frequency
    domain. Our method has two pipelines, the top row shows our compression algorithm
    and the bottom row illustrates the proposed speedup scheme in the DCT frequency
    domain. It is remarkable to note that the network compressed by the proposed method
    can be directly used in the frequency domain without decompression.
  Figure 3 Link: articels_figures_by_rev_year\2018\Packing_Convolutional_Neural_Networks_in_the_Frequency_Domain\figure_3.jpg
  Figure 3 caption: "The performance of the proposed approach with different \u03BB\
    \ and d \xAF \xAF \xAF ."
  Figure 4 Link: articels_figures_by_rev_year\2018\Packing_Convolutional_Neural_Networks_in_the_Frequency_Domain\figure_4.jpg
  Figure 4 caption: The performance of the proposed approach with different numbers
    of cluster centers K .
  Figure 5 Link: articels_figures_by_rev_year\2018\Packing_Convolutional_Neural_Networks_in_the_Frequency_Domain\figure_5.jpg
  Figure 5 caption: Comparison between data-free and data-driven methods by pruning
    different proportions of weights on MNIST.
  Figure 6 Link: articels_figures_by_rev_year\2018\Packing_Convolutional_Neural_Networks_in_the_Frequency_Domain\figure_6.jpg
  Figure 6 caption: Compression statistics for ResNet-50 (better viewed in color version).
  Figure 7 Link: articels_figures_by_rev_year\2018\Packing_Convolutional_Neural_Networks_in_the_Frequency_Domain\figure_7.jpg
  Figure 7 caption: 'Visualization of example filters learned on MNIST: (a) The original
    convolution filters, (b) filters after pruning, (c) convolution filters compressed
    by the proposed algorithm.'
  Figure 8 Link: articels_figures_by_rev_year\2018\Packing_Convolutional_Neural_Networks_in_the_Frequency_Domain\figure_8.jpg
  Figure 8 caption: "Compression statistics for ResNeXt-50 (32 \xD7 4d) (better viewed\
    \ in color version)."
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yunhe Wang
  Name of the last author: Dacheng Tao
  Number of Figures: 8
  Number of Tables: 6
  Number of authors: 4
  Paper title: Packing Convolutional Neural Networks in the Frequency Domain
  Publication Date: 2018-07-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison between Combinations of Different Components in
      the Proposed Algorithm
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Compression Statistics for AlexNet
  Table 3 caption:
    table_text: TABLE 3 Compression Statistics for VGG-16 Net
  Table 4 caption:
    table_text: TABLE 4 Compression Statistics for ResNets
  Table 5 caption:
    table_text: TABLE 5 An Overall Comparison of State-of-the-Art Methods for Deep
      Neural Network Compression and Speed-Up on the ILSVRC2012 Dataset, Where r c
      rc Is the Compression Ratio and r s rs Is the Speed-Up
  Table 6 caption:
    table_text: TABLE 6 Running Time of Different Networks per Image
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2857824
- Affiliation of the first author: national key laboratory for novel software technology,
    nanjing university, nanjing, china
  Affiliation of the last author: department of electronic engineering, shanghai jiao
    tong university, shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2018\ThiNet_Pruning_CNN_Filters_for_a_Thinner_Net\figure_1.jpg
  Figure 1 caption: The overall ThiNet pipeline, which is divided into two major parts.
    Given a pre-trained CNN model, it is pruned layer by layer using our proposed
    filter selection method, followed by post-processing to further reduce model parameters.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\ThiNet_Pruning_CNN_Filters_for_a_Thinner_Net\figure_2.jpg
  Figure 2 caption: Illustration of ThiNets pruning part. We first focus on the dotted
    box to determine several weak channels and their corresponding filters (highlighted
    in yellow in the first row). These channels (and their associated filters) have
    little contribution to the overall performance, thus can be discarded, leading
    to a pruned model. Finally, the network is fine-tuned to recover its accuracy.
    (This figure is best viewed in color.)
  Figure 3 Link: articels_figures_by_rev_year\2018\ThiNet_Pruning_CNN_Filters_for_a_Thinner_Net\figure_3.jpg
  Figure 3 caption: "Illustration of data sampling and relationship among variables.\
    \ We first randomly sample an element y from the activation tensor of layer i+1\
    \ with random spatial location and random channel index. According to the spatial\
    \ location and channel index of y , the corresponding filter W \u02C6 and sliding\
    \ window x can also be determined."
  Figure 4 Link: articels_figures_by_rev_year\2018\ThiNet_Pruning_CNN_Filters_for_a_Thinner_Net\figure_4.jpg
  Figure 4 caption: "Illustration of gcos: The filter weights are first divided into\
    \ g groups (highlighted in different colors), followed by 1times 1 convolution\
    \ to solve the information blocking problem. These 1times 1 filters are initialized\
    \ using the design idea of \u201Cminimizing activation reconstruction error\u201D\
    . (This figure is best viewed in color.)"
  Figure 5 Link: articels_figures_by_rev_year\2018\ThiNet_Pruning_CNN_Filters_for_a_Thinner_Net\figure_5.jpg
  Figure 5 caption: Illustration of the ResNet pruning strategy. For each residual
    block, we only prune the first two convolution layers, keeping the block output
    dimension unchanged.
  Figure 6 Link: articels_figures_by_rev_year\2018\ThiNet_Pruning_CNN_Filters_for_a_Thinner_Net\figure_6.jpg
  Figure 6 caption: 'Performance comparison of different channel selection methods:
    Pruning the VGG-16-GAP model on CUB-200 with different compression rates. For
    random selection, we run it 3 times and report the mean value. (This figure is
    best viewed in color and zoomed in.)'
  Figure 7 Link: articels_figures_by_rev_year\2018\ThiNet_Pruning_CNN_Filters_for_a_Thinner_Net\figure_7.jpg
  Figure 7 caption: 'Performance comparison of different channel selection methods:
    Pruning the VGG-16-GAP model on Indoor-67 with different compression rates. For
    random selection, we run it 3 times and report the mean value. (This figure is
    best viewed in color and zoomed in.)'
  Figure 8 Link: articels_figures_by_rev_year\2018\ThiNet_Pruning_CNN_Filters_for_a_Thinner_Net\figure_8.jpg
  Figure 8 caption: Example style transfer results using different base models. There
    is no obvious winner between our ThiNet model and VGG19 according to the transfer
    results, but ThiNet model is 1.52.5times faster in GPUCPU. (This figure is best
    viewed in color and zoomed in.)
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jian-Hao Luo
  Name of the last author: Weiyao Lin
  Number of Figures: 8
  Number of Tables: 9
  Number of authors: 6
  Paper title: 'ThiNet: Pruning CNN Filters for a Thinner Net'
  Publication Date: 2018-07-20 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Layer-Wise Pruning Results Using Different Channel Selection
      Criteria
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison Among Several State-of-the-Art Pruning Methods
      on VGG-16 and ILSVRC-12
  Table 3 caption:
    table_text: TABLE 3 Performance of Pruning ResNet-50 on ImageNet Using Different
      Compression Rates
  Table 4 caption:
    table_text: TABLE 4 ImageNet Performance of Small ThiNet Models Generated from
      VGG16
  Table 5 caption:
    table_text: TABLE 5 Image Classification on 5 Benchmark Datasets Using Different
      Networks
  Table 6 caption:
    table_text: TABLE 6 Fine-Grained Image Retrieval Using the SCDA Method [47]
  Table 7 caption:
    table_text: TABLE 7 Object Detection Comparison on the VOC2007 Test Dataset [50]
      Using the SSD Method [49]
  Table 8 caption:
    table_text: TABLE 8 Semantic Segmentation on VOC2011 [50]
  Table 9 caption:
    table_text: "TABLE 9 Style Transfer Inference Speed Comparison for 512\xD7512\
      \ 512\xD7512 Images"
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2858232
- Affiliation of the first author: dipartimento di scienze ambientali, universita
    ca foscari venezia, venezia mestre, italy
  Affiliation of the last author: dipartimento di scienze ambientali, universita ca
    foscari venezia, venezia mestre, italy
  Figure 1 Link: articels_figures_by_rev_year\2018\Dominant_Sets_for_Constrained_Image_Segmentation\figure_1.jpg
  Figure 1 caption: 'Left: An example of our interactive image segmentation method
    and its outputs, with different user annotation. Respectively from top to bottom,
    tight bounding box (Tight BB), loose bounding box (Loose BB), a scribble made
    (only) on the foreground object (Scribble on FG) and scribbles with errors. Right:
    Blue and Red dash-line boxes, show an example of our unsupervised and interactive
    co-segmentation methods, respectively.'
  Figure 10 Link: articels_figures_by_rev_year\2018\Dominant_Sets_for_Constrained_Image_Segmentation\figure_10.jpg
  Figure 10 caption: 'Some qualitative results of our unsupervised method tested on
    the image pair dataset. Upper row: Original image Lower row: Result of the proposed
    unsupervised algorithm.'
  Figure 2 Link: articels_figures_by_rev_year\2018\Dominant_Sets_for_Constrained_Image_Segmentation\figure_2.jpg
  Figure 2 caption: An example graph (left), corresponding affinity matrix (middle),
    and scaled affinity matrix built considering vertex 5 as a user constraint (right).
    Notation C i refers to the i th maximal clique.
  Figure 3 Link: articels_figures_by_rev_year\2018\Dominant_Sets_for_Constrained_Image_Segmentation\figure_3.jpg
  Figure 3 caption: "Overview of our interactive segmentation system. Left: Over-segmented\
    \ image (output of the UCM-OWT algorithm [41]) with a user scribble (blue label).\
    \ Middle: The corresponding affinity matrix, using each over-segments as a node,\
    \ showing its two parts: S , the constraint set which contains the user labels,\
    \ and V\u2216S , the part of the graph which takes the regularization parameter\
    \ \u03B1 . Right: The optimization RD (Replicator Dynamics), starts from the barycenter\
    \ and extracts the first dominant set and update x and M , for the next extraction\
    \ till all the dominant sets which contain the user labeled regions are extracted."
  Figure 4 Link: articels_figures_by_rev_year\2018\Dominant_Sets_for_Constrained_Image_Segmentation\figure_4.jpg
  Figure 4 caption: 'Left: Performance of interactive segmentation algorithms, on
    Grab-Cut dataset, for different percentage of synthetic scribbles from the error
    region. Right: Synthetic scribbles and error region'
  Figure 5 Link: articels_figures_by_rev_year\2018\Dominant_Sets_for_Constrained_Image_Segmentation\figure_5.jpg
  Figure 5 caption: 'Exemple results of the interactive segmentation algorithm tested
    on Grab-Cut dataset. (In each block of the red dashed line) Left: Original image
    with bounding boxes of [4]. Middle left: Result of the bounding box approach.
    Middle: Original image and scribbles (observe that the scribbles are only on the
    object of interest). Middle right: Results of the scribbled approach. Right: The
    ground truth. Blue box: Results of Semi-Supervised Normalized Cuts (SSNcut) [53].'
  Figure 6 Link: articels_figures_by_rev_year\2018\Dominant_Sets_for_Constrained_Image_Segmentation\figure_6.jpg
  Figure 6 caption: Overview of our unsupervised co-segmentation algorithm.
  Figure 7 Link: articels_figures_by_rev_year\2018\Dominant_Sets_for_Constrained_Image_Segmentation\figure_7.jpg
  Figure 7 caption: 'The challenges of co-segmentation. Example image pairs: (top
    left) Similar foreground objects with significant variation in background, (top
    right) foreground objects with similar background. The bottom part shows why user
    interaction is important for some cases. The bottom left is the image, bottom
    middle shows the objectness score, and the bottom right shows the user label.'
  Figure 8 Link: articels_figures_by_rev_year\2018\Dominant_Sets_for_Constrained_Image_Segmentation\figure_8.jpg
  Figure 8 caption: Overview of our interactive co-segmentation algorithm.
  Figure 9 Link: articels_figures_by_rev_year\2018\Dominant_Sets_for_Constrained_Image_Segmentation\figure_9.jpg
  Figure 9 caption: Precision, recall and F-measure of our unsupervised co-segmentation
    algorithm and other state-of-the art approaches on the image pair dataset.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Eyasu Zemene Zemene
  Name of the last author: Marcello Pelillo
  Number of Figures: 11
  Number of Tables: 4
  Number of authors: 3
  Paper title: "Dominant Sets for \u201CConstrained\u201D Image Segmentation"
  Publication Date: 2018-07-20 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Error Rates of Different Scribble-Based Approaches on the
      Grab-Cut Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Jaccard Index of Different Approaches\u2014First 5 Bounding-Box-Based\u2014\
      on Berkeley Dataset"
  Table 3 caption:
    table_text: TABLE 3 Error Rates of Different Bounding-Box Approaches with Different
      Level of Looseness as an Input, on the Grab-Cut Dataset
  Table 4 caption:
    table_text: TABLE 4 Results of Our Interactive Co-Segmentation Method on Image
      Pair Dataset Putting Scribbles on One of the Image Pairs
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2858243
- Affiliation of the first author: 4paradigm inc, beijing, china
  Affiliation of the last author: microsoft research asia, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2018\LargeScale_LowRank_Matrix_Learning_with_Nonconvex_Regularizers\figure_1.jpg
  Figure 1 caption: Parallelization of different matrix operations. Here, the number
    of threads q is equal to 3. Each dotted path denotes operation of a thread.
  Figure 10 Link: articels_figures_by_rev_year\2018\LargeScale_LowRank_Matrix_Learning_with_Nonconvex_Regularizers\figure_10.jpg
  Figure 10 caption: Testing RMSE versus CPU time (in seconds) on the hyperspectral
    images. The plot of corn is similar and thus not shown.
  Figure 2 Link: articels_figures_by_rev_year\2018\LargeScale_LowRank_Matrix_Learning_with_Nonconvex_Regularizers\figure_2.jpg
  Figure 2 caption: "Partitioning of variables U\u03A3 V \u22A4 and O , and three\
    \ threads are used ( q=3 )."
  Figure 3 Link: articels_figures_by_rev_year\2018\LargeScale_LowRank_Matrix_Learning_with_Nonconvex_Regularizers\figure_3.jpg
  Figure 3 caption: k and k X gd versus the number of iterations on the synthetic
    data set with m=500 . The plot of TNN is similar and thus not shown.
  Figure 4 Link: articels_figures_by_rev_year\2018\LargeScale_LowRank_Matrix_Learning_with_Nonconvex_Regularizers\figure_4.jpg
  Figure 4 caption: Objective versus CPU time for the capped- ell 1 and LSP on MovieLens-100K.
    The plot of TNN is similar and thus not shown.
  Figure 5 Link: articels_figures_by_rev_year\2018\LargeScale_LowRank_Matrix_Learning_with_Nonconvex_Regularizers\figure_5.jpg
  Figure 5 caption: RMSE versus CPU time on the MovieLens-1M and 10M data sets.
  Figure 6 Link: articels_figures_by_rev_year\2018\LargeScale_LowRank_Matrix_Learning_with_Nonconvex_Regularizers\figure_6.jpg
  Figure 6 caption: RMSE versus CPU time on the netflix and yahoo data sets.
  Figure 7 Link: articels_figures_by_rev_year\2018\LargeScale_LowRank_Matrix_Learning_with_Nonconvex_Regularizers\figure_7.jpg
  Figure 7 caption: Grayscale images used in the experiment.
  Figure 8 Link: articels_figures_by_rev_year\2018\LargeScale_LowRank_Matrix_Learning_with_Nonconvex_Regularizers\figure_8.jpg
  Figure 8 caption: Testing RMSE versus CPU time (in seconds) on the grayscale images.
    The plot of wall is similar and thus not shown.
  Figure 9 Link: articels_figures_by_rev_year\2018\LargeScale_LowRank_Matrix_Learning_with_Nonconvex_Regularizers\figure_9.jpg
  Figure 9 caption: Hyperspectral images used in the experiment, their sizes are 1312
    times 480 times 49 , 1312 times 528 times 49 and 1312 times 480 times 49 , respectively.
    One sample band of each image is shown.
  First author gender probability: 0.96
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.57
  Name of the first author: Quanming Yao
  Name of the last author: Tie-Yan Liu
  Number of Figures: 14
  Number of Tables: 11
  Number of authors: 4
  Paper title: Large-Scale Low-Rank Matrix Learning with Nonconvex Regularizers
  Publication Date: 2018-07-20 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of the Proposed Algorithms with Existing Algorithms
  Table 10 caption:
    table_text: TABLE 10 Videos Used in the Experiment
  Table 2 caption:
    table_text: TABLE 2 r rs for Some Popular Nonconvex Low-Rank Regularizers
  Table 3 caption:
    table_text: TABLE 3 Comparison of the Iteration Time Complexities, Convergence
      Rates and Space Complexity of Various Matrix Completion Solvers
  Table 4 caption:
    table_text: TABLE 4 Matrix Completion Performance on the Synthetic Data
  Table 5 caption:
    table_text: TABLE 5 Recommendation Data Sets Used in the Experiments
  Table 6 caption:
    table_text: TABLE 6 Matrix Completion Results on the MovieLens Data Sets
  Table 7 caption:
    table_text: TABLE 7 Results on Grayscale Image Impainting
  Table 8 caption:
    table_text: TABLE 8 Results on Hyperspectral Image Impainting
  Table 9 caption:
    table_text: TABLE 9 RPCA Performance on Synthetic Data
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2858249
- Affiliation of the first author: department of electrical and computer engineering,
    national university of singapore, singapore
  Affiliation of the last author: department of electrical and computer engineering,
    national university of singapore, singapore
  Figure 1 Link: articels_figures_by_rev_year\2018\DAided_DualAgent_GANs_for_Unconstrained_Face_Recognition\figure_1.jpg
  Figure 1 caption: Comparison of pose distribution in the IJB-A [7] dataset wo and
    w DA-GAN.
  Figure 10 Link: articels_figures_by_rev_year\2018\DAided_DualAgent_GANs_for_Unconstrained_Face_Recognition\figure_10.jpg
  Figure 10 caption: Verification result analysis for worst matched cases (left) and
    worst non-matched cases (right) on IJB-A [7] split1. Best viewed in the original
    zoomed-in pdf.
  Figure 2 Link: articels_figures_by_rev_year\2018\DAided_DualAgent_GANs_for_Unconstrained_Face_Recognition\figure_2.jpg
  Figure 2 caption: "Overview of the proposed unconstrained face recognition pipeline.\
    \ Left panel: The proposed Dual-Agent Generative Adversarial Network architecture.\
    \ The simulator extracts face RoI, performs saliency prediction (i.e., facebackground\
    \ segmentation), localizes landmark points and produces synthetic faces with arbitrary\
    \ poses, which are fed to DA-GAN for realism refinement. DA-GAN uses a fully convolutional\
    \ skip-net as the generator and an auto-encoder as the discriminator. The dual\
    \ agents are responsible for discriminating real versus fake (minimizing the loss\
    \ L adv ) and preserving identity information (minimizing the loss L ip ). Right\
    \ panel: The proposed \u201Crecognition via generation\u201D framework. We transfer-learn\
    \ two state-of-the-art deep neural networks, SE-ResNeXt-152 [15], [16] and GoogleNet-BN\
    \ [17], from source domain to target domain extended by DA-GAN. We combine the\
    \ complementary two-view information from the two models to train template-adapted\
    \ Support Vector Machines (SVMs) [1]. The resulted margins are robust and discriminative\
    \ for unconstrained face recognition. Best viewed in color."
  Figure 3 Link: articels_figures_by_rev_year\2018\DAided_DualAgent_GANs_for_Unconstrained_Face_Recognition\figure_3.jpg
  Figure 3 caption: Quality of refined results w.r.t. the network convergence measurement
    L con .
  Figure 4 Link: articels_figures_by_rev_year\2018\DAided_DualAgent_GANs_for_Unconstrained_Face_Recognition\figure_4.jpg
  Figure 4 caption: Refined results of DA-GAN.
  Figure 5 Link: articels_figures_by_rev_year\2018\DAided_DualAgent_GANs_for_Unconstrained_Face_Recognition\figure_5.jpg
  Figure 5 caption: "Refined results of DA-GAN under various poses with yaw angles\
    \ ranging from \u2212 90 \u2218 to \u2212 10 \u2218 at a stride of 10 \u2218 ."
  Figure 6 Link: articels_figures_by_rev_year\2018\DAided_DualAgent_GANs_for_Unconstrained_Face_Recognition\figure_6.jpg
  Figure 6 caption: "Refined results of DA-GAN under various poses with yaw angles\
    \ ranging from + 10 \u2218 to + 90 \u2218 at a stride of 10 \u2218 ."
  Figure 7 Link: articels_figures_by_rev_year\2018\DAided_DualAgent_GANs_for_Unconstrained_Face_Recognition\figure_7.jpg
  Figure 7 caption: Qualitative result comparison of DA-GAN with state-of-the-art
    GANs and three different network settings.
  Figure 8 Link: articels_figures_by_rev_year\2018\DAided_DualAgent_GANs_for_Unconstrained_Face_Recognition\figure_8.jpg
  Figure 8 caption: Feature space of real faces and DA-GAN synthetic faces.
  Figure 9 Link: articels_figures_by_rev_year\2018\DAided_DualAgent_GANs_for_Unconstrained_Face_Recognition\figure_9.jpg
  Figure 9 caption: Verification result analysis for best matched cases (left) and
    best non-matched cases (right) on IJB-A [7] split1. Best viewed in the original
    zoomed-in pdf.
  First author gender probability: 0.9
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jian Zhao
  Name of the last author: Jiashi Feng
  Number of Figures: 11
  Number of Tables: 3
  Number of authors: 6
  Paper title: 3D-Aided Dual-Agent GANs for Unconstrained Face Recognition
  Publication Date: 2018-07-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance Comparison of DA-GAN with State-of-the-Arts on
      IJB-A [7] Verification Protocol
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance Comparison of DA-GAN with State-of-the-Arts on
      IJB-A [7] Identification Protocol
  Table 3 caption:
    table_text: TABLE 3 Performance Comparison of DA-GAN with State-of-the-Arts on
      CFP [13]
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2858819
- Affiliation of the first author: google inc, new york, ny, usa
  Affiliation of the last author: computer science, hunter college of cuny, new york,
    ny, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\A_Kronecker_Product_Model_for_Repeated_Pattern_Detection_on_D_Urban_Images\figure_1.jpg
  Figure 1 caption: "Estimation of the spatial periods of fa\xE7ade shown in Fig.\
    \ 2. Left: Auto-Correlation sequences used for the estimation of N v =90 pixels\
    \ . Right: Estimation of N h =56 pixels ."
  Figure 10 Link: articels_figures_by_rev_year\2018\A_Kronecker_Product_Model_for_Repeated_Pattern_Detection_on_D_Urban_Images\figure_10.jpg
  Figure 10 caption: 'From left to right: same order as in Fig. 7. The results demonstrate
    that the proposed method is robust to occlusions and different architectural styles.'
  Figure 2 Link: articels_figures_by_rev_year\2018\A_Kronecker_Product_Model_for_Repeated_Pattern_Detection_on_D_Urban_Images\figure_2.jpg
  Figure 2 caption: "Top: Partitioning the fa\xE7ade into blocks using the spatial\
    \ periods estimated in Fig. 1, where the green lines shows the boundary of partition\
    \ blocks. Bottom: The corresponding 3\xD710 blocks viewed independently."
  Figure 3 Link: articels_figures_by_rev_year\2018\A_Kronecker_Product_Model_for_Repeated_Pattern_Detection_on_D_Urban_Images\figure_3.jpg
  Figure 3 caption: 'Rank estimation result for the image shown in Fig. 2. Top: Plot
    of the function in Eq. (36), where the global minimum comes up at index 4. Bottom:
    The enlarged figure area within the green box in (a), which shows the global minimum
    in a clearer way.'
  Figure 4 Link: articels_figures_by_rev_year\2018\A_Kronecker_Product_Model_for_Repeated_Pattern_Detection_on_D_Urban_Images\figure_4.jpg
  Figure 4 caption: "Top: Each color represents one group. Bottom: The reconstructed\
    \ fa\xE7ade image."
  Figure 5 Link: articels_figures_by_rev_year\2018\A_Kronecker_Product_Model_for_Repeated_Pattern_Detection_on_D_Urban_Images\figure_5.jpg
  Figure 5 caption: Illustration of classification and refinement steps.
  Figure 6 Link: articels_figures_by_rev_year\2018\A_Kronecker_Product_Model_for_Repeated_Pattern_Detection_on_D_Urban_Images\figure_6.jpg
  Figure 6 caption: "Left: The original fa\xE7ade. Right: The recovered \u201Dclean\u201D\
    \ fa\xE7ade structure hidden beneath the noise."
  Figure 7 Link: articels_figures_by_rev_year\2018\A_Kronecker_Product_Model_for_Repeated_Pattern_Detection_on_D_Urban_Images\figure_7.jpg
  Figure 7 caption: (a) Input image. (b) Partition grid showing the estimated periods.
    (c) Low-rank component generated by the method in Section 4.2. (d) Estimated 1text-0
    repeated patterns computed by Algorithm 3 and the refinement step. Each color
    represents one group. (e) Ground truth.
  Figure 8 Link: articels_figures_by_rev_year\2018\A_Kronecker_Product_Model_for_Repeated_Pattern_Detection_on_D_Urban_Images\figure_8.jpg
  Figure 8 caption: (a) A failure case in [3]. (b) Our method detects all patterns.
    (c) In [3], the bottom two patterns are not detected. (d) Ground truth.
  Figure 9 Link: articels_figures_by_rev_year\2018\A_Kronecker_Product_Model_for_Repeated_Pattern_Detection_on_D_Urban_Images\figure_9.jpg
  Figure 9 caption: "The first two rows fail at period detection due to strong illumination\
    \ variation. In the third row, the input fa\xE7ade contains a set of Penrose tiling\
    \ style windows in the middle top. (a) Original images. (b) Partition grid. (c)\
    \ Detected patterns. (d) Ground truth."
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Juan Liu
  Name of the last author: Ioannis Stamos
  Number of Figures: 11
  Number of Tables: 1
  Number of authors: 4
  Paper title: A Kronecker Product Model for Repeated Pattern Detection on 2D Urban
    Images
  Publication Date: 2018-07-23 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Performance Comparison between the Proposed Method and [1]\
      \ Regarding the Sample Mean and Variance of K \u2212K K-K"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2858795
- Affiliation of the first author: csail, mit, cambridge, ma, usa
  Affiliation of the last author: csail, mit, cambridge, ma, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\Interpreting_Deep_Visual_Representations_via_Network_Dissection\figure_1.jpg
  Figure 1 caption: Samples from the Broden Dataset. The ground truth for each concept
    is a pixel-wise dense annotation.
  Figure 10 Link: articels_figures_by_rev_year\2018\Interpreting_Deep_Visual_Representations_via_Network_Dissection\figure_10.jpg
  Figure 10 caption: Average IoU versus the number of detectors for the object class
    in Resnet152 trained on Places and ImageNet respectively. For a set of units detecting
    the same object class, we average their IoU.
  Figure 2 Link: articels_figures_by_rev_year\2018\Interpreting_Deep_Visual_Representations_via_Network_Dissection\figure_2.jpg
  Figure 2 caption: Scoring unit interpretability by evaluating the unit for semantic
    segmentation.
  Figure 3 Link: articels_figures_by_rev_year\2018\Interpreting_Deep_Visual_Representations_via_Network_Dissection\figure_3.jpg
  Figure 3 caption: Illustration of network dissection for measuring semantic alignment
    of units in a given CNN. Here one unit of the last convolutional layer of a given
    CNN is probed by evaluating its performance on various segmentation tasks. Our
    method can probe any convolutional layer.
  Figure 4 Link: articels_figures_by_rev_year\2018\Interpreting_Deep_Visual_Representations_via_Network_Dissection\figure_4.jpg
  Figure 4 caption: The annotation interface used by human raters on Amazon Mechanical
    Turk. Raters are shown descriptive text in quotes together with fifteen images,
    each with highlighted patches, and must evaluate whether the quoted text is a
    good description for the highlighted patches.
  Figure 5 Link: articels_figures_by_rev_year\2018\Interpreting_Deep_Visual_Representations_via_Network_Dissection\figure_5.jpg
  Figure 5 caption: 'Comparison of the interpretability of the convolutional layers
    of AlexNet, trained on classification tasks for Places (top) and ImageNet (bottom).Four
    units in each layer are shown with their semantics. The segmentation generated
    by each unit is shown on the three Broden images with highest activation. Top-scoring
    labels are shown above to the left, and human-annotated labels are shown above
    to the right. There is some disagreement: for example, raters mark the first conv4
    unit on Places as a windows detector, while the algorithm matches the chequered
    texture.'
  Figure 6 Link: articels_figures_by_rev_year\2018\Interpreting_Deep_Visual_Representations_via_Network_Dissection\figure_6.jpg
  Figure 6 caption: "Interpretability over changes in basis of the representation\
    \ of AlexNet conv5 trained on Places. The vertical axis shows the number of unique\
    \ interpretable concepts that match a unit in the representation. The horizontal\
    \ axis shows \u03B1 , which quantifies the degree of rotation."
  Figure 7 Link: articels_figures_by_rev_year\2018\Interpreting_Deep_Visual_Representations_via_Network_Dissection\figure_7.jpg
  Figure 7 caption: Visualizations of the best single-unit concept detectors of five
    concepts taken from individual units of AlexNet conv5 trained on Places (left),
    compared with the best linear-combination detectors of the same concepts taken
    from the same representation under a random rotation (right). For most concepts,
    both the IoU and the visualization of the top activating image patches confirm
    that individual units match concepts better than linear combinations. In other
    cases, (e.g., head detectors) visualization of a linear combination appears highly
    consistent, but the IoU reveals lower consistency when evaluated over the whole
    dataset.
  Figure 8 Link: articels_figures_by_rev_year\2018\Interpreting_Deep_Visual_Representations_via_Network_Dissection\figure_8.jpg
  Figure 8 caption: "Complete rotation ( \u03B1=1 ) repeated on AlexNet trained on\
    \ Places365 and ImageNet respectively. Rotation reduces the interpretability significantly\
    \ for both of the networks."
  Figure 9 Link: articels_figures_by_rev_year\2018\Interpreting_Deep_Visual_Representations_via_Network_Dissection\figure_9.jpg
  Figure 9 caption: Interpretability across different architectures trained on ImageNet
    and Places. Plot above shows the number of unique detectors, plot below shows
    the ratio of unique detectors (number of unique detectors divided by the total
    number of units).
  First author gender probability: 0.75
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Bolei Zhou
  Name of the last author: Antonio Torralba
  Number of Figures: 29
  Number of Tables: 3
  Number of authors: 4
  Paper title: Interpreting Deep Visual Representations via Network Dissection
  Publication Date: 2018-07-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Statistics of Each Label Type Included in the Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Collection of Tested CNN Models
  Table 3 caption:
    table_text: TABLE 3 Human Evaluation of Our Network Dissection Approach
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2858759
