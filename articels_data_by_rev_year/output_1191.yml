- Affiliation of the first author: university of applied sciences, stuttgart, germany
  Affiliation of the last author: university of applied sciences, stuttgart, germany
  Figure 1 Link: articels_figures_by_rev_year\2018\Segmentation_of_Laser_Point_Clouds_in_Urban_Areas_by_a_Modified_Normalized_Cut_M\figure_1.jpg
  Figure 1 caption: Point cloud with varying point density (after Fig. 1 of Shi and
    Malik 2000).
  Figure 10 Link: articels_figures_by_rev_year\2018\Segmentation_of_Laser_Point_Clouds_in_Urban_Areas_by_a_Modified_Normalized_Cut_M\figure_10.jpg
  Figure 10 caption: "Overview over point cloud sg2710 (ETH Z\xFCrich), automatic\
    \ segmentation."
  Figure 2 Link: articels_figures_by_rev_year\2018\Segmentation_of_Laser_Point_Clouds_in_Urban_Areas_by_a_Modified_Normalized_Cut_M\figure_2.jpg
  Figure 2 caption: NCut favours cuts at constrictions.
  Figure 3 Link: articels_figures_by_rev_year\2018\Segmentation_of_Laser_Point_Clouds_in_Urban_Areas_by_a_Modified_Normalized_Cut_M\figure_3.jpg
  Figure 3 caption: NCut segmentation of a facade with four windows. Colors indicate
    the segment affiliation of the points.
  Figure 4 Link: articels_figures_by_rev_year\2018\Segmentation_of_Laser_Point_Clouds_in_Urban_Areas_by_a_Modified_Normalized_Cut_M\figure_4.jpg
  Figure 4 caption: Blurred plane parameters cause insufficient partitioning (see
    text for an explanation).
  Figure 5 Link: articels_figures_by_rev_year\2018\Segmentation_of_Laser_Point_Clouds_in_Urban_Areas_by_a_Modified_Normalized_Cut_M\figure_5.jpg
  Figure 5 caption: Segmentation result for the point cloud of Fig. 3 by DWCut.
  Figure 6 Link: articels_figures_by_rev_year\2018\Segmentation_of_Laser_Point_Clouds_in_Urban_Areas_by_a_Modified_Normalized_Cut_M\figure_6.jpg
  Figure 6 caption: Colour-enriched point cloud containing a building edge.
  Figure 7 Link: articels_figures_by_rev_year\2018\Segmentation_of_Laser_Point_Clouds_in_Urban_Areas_by_a_Modified_Normalized_Cut_M\figure_7.jpg
  Figure 7 caption: Segmentation result for the point cloud of Fig. 6 using the hybrid
    algorithm.
  Figure 8 Link: articels_figures_by_rev_year\2018\Segmentation_of_Laser_Point_Clouds_in_Urban_Areas_by_a_Modified_Normalized_Cut_M\figure_8.jpg
  Figure 8 caption: Segmentation result for the point cloud of Fig. 6 using the hybrid
    algorithm, overlaid with seed point cloud.
  Figure 9 Link: articels_figures_by_rev_year\2018\Segmentation_of_Laser_Point_Clouds_in_Urban_Areas_by_a_Modified_Normalized_Cut_M\figure_9.jpg
  Figure 9 caption: Segmentation result for a complete building facade with the hybrid
    algorithm (point count = 1,100,000).
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Avishek Dutta
  Name of the last author: Michael Hahn
  Number of Figures: 15
  Number of Tables: 2
  Number of authors: 3
  Paper title: Segmentation of Laser Point Clouds in Urban Areas by a Modified Normalized
    Cut Method
  Publication Date: 2018-09-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Sample Parameter Set for DWCut
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Evaluation Measures for Dataset sg2710
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2869744
- Affiliation of the first author: department of mathematics, colorado state university,
    fort collins, usa
  Affiliation of the last author: department of mathematics, colorado state university,
    fort collins, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\Motion_Segmentation_via_Generalized_Curvatures\figure_1.jpg
  Figure 1 caption: Example poses. The left side shows 25 body pose points extracted
    by a Microsoft Kinect II. The right side shows 44 hand points (22 per hand) extracted
    by the Intel RealSense.
  Figure 10 Link: articels_figures_by_rev_year\2018\Motion_Segmentation_via_Generalized_Curvatures\figure_10.jpg
  Figure 10 caption: Similar to Fig. 9, this figure shows the third ( kappa 3 ) and
    fourth ( kappa 4 ) curvatures for the same video. Once again, although there are
    differences among the various curvatures, all tend to be high during transitions
    and low during motions.
  Figure 2 Link: articels_figures_by_rev_year\2018\Motion_Segmentation_via_Generalized_Curvatures\figure_2.jpg
  Figure 2 caption: Mean singular values averaged over 1,000 windows for the 10 dimensional
    helix used in Table 1. The blue curve connects singular values computed with no
    noise, while the orange curve shows the singular values with 20 percent data noise
    and the green curve shows the singular values with 50 percent data noise. Since
    the kth curvature depends on the (k+1)th eigenvalue, this explains the breakdowns
    in Table 1.
  Figure 3 Link: articels_figures_by_rev_year\2018\Motion_Segmentation_via_Generalized_Curvatures\figure_3.jpg
  Figure 3 caption: A synthetically generated 3D curve using the curvature and torsion
    given in Equation (11). The curve is sampled at 500 points from s=0 to s=30 .
    Color is used to encode velocity. The dark blue parts of the curve are where the
    sampled points are closest together, while the red parts of the curve show where
    they are farthest apart.
  Figure 4 Link: articels_figures_by_rev_year\2018\Motion_Segmentation_via_Generalized_Curvatures\figure_4.jpg
  Figure 4 caption: "The true and approximated first curvatures ( \u03BA 1 ) for the\
    \ synthetic data curve given in Equation (11) and Fig. 3. The true curvatures\
    \ is shown in blue, while the estimated curvatures are shown in green. The four\
    \ subplots corresponds to varied amounts of uniform noise added. Uniform noise\
    \ added is at 0, 30, 60, 90 percent of the mean distance between points."
  Figure 5 Link: articels_figures_by_rev_year\2018\Motion_Segmentation_via_Generalized_Curvatures\figure_5.jpg
  Figure 5 caption: "The true and approximated second curvatures ( \u03BA 2 ) for\
    \ the synthetic data curve given in Equation (11) and Fig. 3. As in Fig. 4, the\
    \ true second curvature is shown in blue, and the estimated in green. The four\
    \ subplots corresponds to varied amounts of uniform noise added. Uniform noise\
    \ is added of 0, 30, 60, 90 percent of mean distance between points."
  Figure 6 Link: articels_figures_by_rev_year\2018\Motion_Segmentation_via_Generalized_Curvatures\figure_6.jpg
  Figure 6 caption: Cosine of the angle between singular vectors ( kappa 1 and kappa
    2 ) for the synthetic data curve given in Equation (11) and Fig. 3. The angles
    for kappa 1 is shown in blue, while the angles for kappa 2 are shown in green.
    The dashed blue line represents the peak in kappa 1 and is the optimal window
    selected by GCA for kappa 1 . The dashed green line represents the peak in kappa
    2 and is the optimal window selected by GCA for kappa 2 . The four subplots corresponds
    to varied amounts of uniform noise added. Uniform noise added is at 0, 30, 60,
    90 percent of the mean distance between points.
  Figure 7 Link: articels_figures_by_rev_year\2018\Motion_Segmentation_via_Generalized_Curvatures\figure_7.jpg
  Figure 7 caption: "Frames from Kinect v2 Skeleton data of part of one PALKA video.\
    \ This video fragment shows four motions: (1) raising arms, (2) lowering arms,\
    \ (3) raise hands to the eyes (\u201Cgoggles\u201D) and (4) returning the hands\
    \ to a neutral position. The figure shows 8 of the poses in the video. The poses\
    \ are color coded to match points in Fig. 8."
  Figure 8 Link: articels_figures_by_rev_year\2018\Motion_Segmentation_via_Generalized_Curvatures\figure_8.jpg
  Figure 8 caption: Projection of the 75-dimensional pose curve from Fig. 7 onto its
    first two eigenvectors. Low curvature sections are shown in blue and correspond
    to motions, while high curvature sections are shown in red and correspond to transitions
    between motions. The indices 1-8 are color coded to match the poses in Fig. 7
    in the projected trajectory. Discarding the endpoints of the video, motion transitions
    occur around poses 3, 5, and 7.
  Figure 9 Link: articels_figures_by_rev_year\2018\Motion_Segmentation_via_Generalized_Curvatures\figure_9.jpg
  Figure 9 caption: Curvatures kappa 1 and kappa 2 computed using GCA on the PALKA
    video whose projection is shown in Fig. 8. The white areas show motions (raising
    or lowering arms, kicking outward or bringing the leg back). The gray areas show
    transitions between motions. As hypothesized, curvatures are low during motions
    and high during transitions, although some transitions show up more clearly in
    kappa 1 , while others show up more clearly in kappa 2 .
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.92
  Name of the first author: Robert T. Arn
  Name of the last author: Chris Peterson
  Number of Figures: 11
  Number of Tables: 2
  Number of authors: 6
  Paper title: Motion Segmentation via Generalized Curvatures
  Publication Date: 2018-09-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Estimated Curvatures for 10-Dimensional Helix Such That the
      True i ith-Curvature Is i i
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 A Comparison of Segmentation Results on PALKA Dataset
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2869741
- Affiliation of the first author: science & technology on integrated information
    system laboratory, chinese academy of sciences, beijing, china
  Affiliation of the last author: microsoft research, redmond, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\OrderPreserving_Optimal_Transport_for_Distances_between_Sequences\figure_1.jpg
  Figure 1 caption: "The two stroke sequences of the same online character differ\
    \ in local order at the first and the last three positions. (a) The alignments\
    \ generated by DTW are disturbed by such local order reversal since DTW preserves\
    \ the temporal positions strictly, where the misalignments are shown in bold.\
    \ (b) If only the stroke shape is considered, Sinkhorn (smoothed OT) aligns the\
    \ same stroke \u201Cdot\u201D appeared in different temporal positions. (c) The\
    \ proposed OPW is able to generate proper alignments that can tackle local order\
    \ distortion by jointly considering the shape matching and temporal orders. The\
    \ alignments shown in (b) and (c) are the largest transports associated with instances\
    \ of the second sequence in the learned OT."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\OrderPreserving_Optimal_Transport_for_Distances_between_Sequences\figure_2.jpg
  Figure 2 caption: "The two action sequences \u201Cstand up\u201D and \u201Csit down\u201D\
    \ only differ in temporal orders. (a) Due to the temporal constraint, frames are\
    \ forced to be aligned in sorted order by DTW, and hence the difference between\
    \ the two sequences can be well captured. (b) When the temporal information is\
    \ ignored, the Sinkhorn distance does not distinguish between the two sequences.\
    \ (c) With the temporal regularizations, some unfamiliar frames in the same temporal\
    \ positions are aligned by the proposed OPW. Therefore, for a sequence of one\
    \ action, its OPW distance to a sequence of the other action will be large than\
    \ another sequence from the same action. The alignments shown in (b) and (c) are\
    \ the largest transports associated with instances of the second sequence."
  Figure 3 Link: articels_figures_by_rev_year\2018\OrderPreserving_Optimal_Transport_for_Distances_between_Sequences\figure_3.jpg
  Figure 3 caption: "The two periodic sequences of the \u201Cjogging\u201D action\
    \ start from different states. The right arm is first lifted in the top sequence\
    \ while the left arm is first lifted in the bottom sequence. (a) Misalignments\
    \ are introduced by DTW due to the strict temporal constraint. (b) Sinkhorn aligns\
    \ the frames representing the most similar poses even when the relative temporal\
    \ positions of the two frames are far away. As a result, long connection lines\
    \ occur frequently in the alignments. The first few frames of one sequence are\
    \ aligned to the rear frames of the other sequence across a complete periodic\
    \ cycle. (c) The proposed OPW aligns each frame in one sequence to the frame with\
    \ the same pose of the temporally adjacent cycle in the other sequence. The generated\
    \ alignments exhibit periodic segments. The alignments shown in (b) and (c) are\
    \ the largest transports associated with instances of the second sequence in the\
    \ learned OT. (d) is a local finer illustration of (c), where the alignments with\
    \ the largest several transport probabilities w.r.t. some instances in the second\
    \ sequence are shown. The thicker the line, the larger the probability."
  Figure 4 Link: articels_figures_by_rev_year\2018\OrderPreserving_Optimal_Transport_for_Distances_between_Sequences\figure_4.jpg
  Figure 4 caption: The Prior distribution of the transport matrix.
  Figure 5 Link: articels_figures_by_rev_year\2018\OrderPreserving_Optimal_Transport_for_Distances_between_Sequences\figure_5.jpg
  Figure 5 caption: Samples of similar online Chinese characters.
  Figure 6 Link: articels_figures_by_rev_year\2018\OrderPreserving_Optimal_Transport_for_Distances_between_Sequences\figure_6.jpg
  Figure 6 caption: Performances with the increase of lambda (a) by the NM classifier
    on the Face (all) dataset; (b) by the NN classifiers on the Face (all) dataset;
    (c) by the NM classifier on the Action3D dataset; (d) by the NN classifiers on
    the Action3D dataset.
  Figure 7 Link: articels_figures_by_rev_year\2018\OrderPreserving_Optimal_Transport_for_Distances_between_Sequences\figure_7.jpg
  Figure 7 caption: Performances on the Face (all) dataset with the increase of sigma
    by (a) the NM classifier; (b) the NN classifiers.
  Figure 8 Link: articels_figures_by_rev_year\2018\OrderPreserving_Optimal_Transport_for_Distances_between_Sequences\figure_8.jpg
  Figure 8 caption: Performances on the MSR Sports Action3D dataset with the increase
    of sigma by (a) the NM classifier; (b) the NN classifiers.
  Figure 9 Link: articels_figures_by_rev_year\2018\OrderPreserving_Optimal_Transport_for_Distances_between_Sequences\figure_9.jpg
  Figure 9 caption: Performances on the MSR Sports Action3D dataset with the increase
    of (a) lambda 1 by the NM classifier; (b) lambda 1 by the NN classifiers; (c)
    lambda 2 by the NM classifier; (d) lambda 2 by the NN classifiers.
  First author gender probability: 0.96
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.95
  Name of the first author: Bing Su
  Name of the last author: Gang Hua
  Number of Figures: 9
  Number of Tables: 12
  Number of authors: 2
  Paper title: Order-Preserving Optimal Transport for Distances between Sequences
  Publication Date: 2018-09-14 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Results of the OPW Distance with Different Values of \u03BB\
      \ 1 \u03BB1 and \u03BB 2 \u03BB2 on the Chalearn Dataset"
  Table 10 caption:
    table_text: TABLE 10 Results on the HAS Dataset
  Table 2 caption:
    table_text: TABLE 2 The Optimal Parameters on Different Datasets for the NM Classifier
  Table 3 caption:
    table_text: TABLE 3 Results of the OPW Distance with Different Numbers of Iterations
      on the MSR Sports Action3D Dataset
  Table 4 caption:
    table_text: TABLE 4 Results on the FacesUCR Datasets
  Table 5 caption:
    table_text: TABLE 5 Results on the Face (All) Dataset
  Table 6 caption:
    table_text: TABLE 6 Results on the MSR Sports Action3D Dataset
  Table 7 caption:
    table_text: TABLE 7 Results on the MSR Daily Activity3D Dataset
  Table 8 caption:
    table_text: TABLE 8 Results on Similar Online Chinese Character Recognition
  Table 9 caption:
    table_text: TABLE 9 Results on the SAD Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2870154
- Affiliation of the first author: school of computer science and technology, college
    of intelligence and computing, tianjin university, tianjin, china
  Affiliation of the last author: department of management and innovation systems,
    university of salerno, fisciano, italy
  Figure 1 Link: articels_figures_by_rev_year\2018\Active_Camera_Relocalization_from_a_Single_Reference_Image_without_HandEye_Calib\figure_1.jpg
  Figure 1 caption: Single image ACR without hand-eye calibration. (a) Illustration
    of ACR process, wherein the camera actively approaches the reference (target)
    pose. (b) Our effective and inexpensive ACR platform, wherein the red box indicates
    the camera (eye) and the green box indicates the motion platform (hand). (c) Comparison
    of ACR and Manual Camera Relocalization (Manual-CR) for fine-grained change detection.
    Note, the real fine-grained changes are regular rectangles, but manual-CR via
    image registration contorts them to irregular shapes, thus make minute change
    detection unreliable. In contrast, our ACR physically relocalizes the camera,
    thus faithfully preserve the authentic structure of fine-grained changes. (d)
    Real 0.1mm-level fine-grained changes, occurred during June 2014 and July 2015,
    discovered by our ACR platform in Cave465 of Dunhuang Mogao Grottoes. We circle
    the minute changes in the ACR image and highlight corresponding areas.
  Figure 10 Link: articels_figures_by_rev_year\2018\Active_Camera_Relocalization_from_a_Single_Reference_Image_without_HandEye_Calib\figure_10.jpg
  Figure 10 caption: Accuracy comparison of our ACR with two state-of-the-art competitors,
    CRP [36] and homography-CR [6], in two scenes. Both competitors use the original
    implementations on their designated platforms. Final images are cropped into the
    same size for fair comparison.
  Figure 2 Link: articels_figures_by_rev_year\2018\Active_Camera_Relocalization_from_a_Single_Reference_Image_without_HandEye_Calib\figure_2.jpg
  Figure 2 caption: "Illustration of ACR process and the hand-eye relation. In the\
    \ ACR process, the eye (camera) and hand always maintain a certain hand-eye relative\
    \ pose X\u2243\u27E8 R X , t X \u27E9 . To relocalize the camera from the initial\
    \ pose P 0 A to the reference (target) pose P ref A with a camera motion M \u2217\
    \ A , we need hand to execute a different motion M \u2217 B . In practice, we\
    \ solve ACR by a series of estimated hand motions defined in Eq. (2). P i A and\
    \ P i B indicate the intermediate camera and hand poses, respectively."
  Figure 3 Link: articels_figures_by_rev_year\2018\Active_Camera_Relocalization_from_a_Single_Reference_Image_without_HandEye_Calib\figure_3.jpg
  Figure 3 caption: "Iterative changes of R i A , t i A and the 3D trajectory of current\
    \ camera position during ACR process. The first two rows show the updating of\
    \ \u03B8 A , i.e., the angle of R A , and the length of t A , respectively. The\
    \ third row is the 3D changing trajectory of current camera position, wherein\
    \ the green circle denotes the initial camera position and the red dot is the\
    \ target camera position. To illustrate the sufficient conditions of R A and t\
    \ A convergence, for each row, we show three particular cases, (a) ( \u03B8 X\
    \ = \u03C0 4.5 )< \u03C0 4 , (b) \u03C0 4 \u2264( \u03B8 X = \u03C0 3.5 )\u2264\
    \ \u03C0 3 , and (c) ( \u03B8 X = \u03C0 2.5 )> \u03C0 3 . Clearly, case (a) verifies\
    \ that both 3D relative rotation angle and translation length converge to zero\
    \ in the ACR process, thus the camera is finally relocalized to the target pose.\
    \ In case (b), only the relative rotation converges, whereas both the relative\
    \ translation and camera position cannot be relocalized. Case (c) shows a complete\
    \ divergence example."
  Figure 4 Link: articels_figures_by_rev_year\2018\Active_Camera_Relocalization_from_a_Single_Reference_Image_without_HandEye_Calib\figure_4.jpg
  Figure 4 caption: Working flow of the proposed effective algorithm for 6D active
    camera relocalization (ACR) without hand-eye calibration.
  Figure 5 Link: articels_figures_by_rev_year\2018\Active_Camera_Relocalization_from_a_Single_Reference_Image_without_HandEye_Calib\figure_5.jpg
  Figure 5 caption: Average convergence process of R A and t A in 100 laboratory tests,
    with random X and initial P A . Both the angle of R A and length of t A converge
    to zero in all 100 tests. (a) Shows the convergence curves of R A and (b) shows
    the convergence curves of t A . Note, just after bisection happened, t A may temporarily
    increase, so the average curves of t A in (b) are not necessarily monotonic decreasing.
  Figure 6 Link: articels_figures_by_rev_year\2018\Active_Camera_Relocalization_from_a_Single_Reference_Image_without_HandEye_Calib\figure_6.jpg
  Figure 6 caption: Comparison of convergence rate (iterations) (a) and relocalization
    accuracy (AFD) (b) of the proposed straightforward and effective ACR strategies
    on 50 ACR tests in 5 scenes, with 10 independent tests for each scene. The bottom
    tables show detailed mean and standard deviation statistics. The effective ACR
    approach exhibits both much faster convergence speed and higher accuracy on all
    5 scenes.
  Figure 7 Link: articels_figures_by_rev_year\2018\Active_Camera_Relocalization_from_a_Single_Reference_Image_without_HandEye_Calib\figure_7.jpg
  Figure 7 caption: Positive correlations between the 5-point estimation error and
    the magnitude of relative pose mathbf PmathrmA . (a) and (b) are two curves that
    measure the magnitude of mathbf PmathrmA by relative rotation & translation and
    Frobenius norm of [mathbf RmathrmA,mathbf tmathrmA] - [mathbf I, mathbf 0] , respectively.
  Figure 8 Link: articels_figures_by_rev_year\2018\Active_Camera_Relocalization_from_a_Single_Reference_Image_without_HandEye_Calib\figure_8.jpg
  Figure 8 caption: Comparison of ACR performance with known (ground truth) mathbf
    X and guessing hatmathbf X = mathbf I , in terms of convergence rate under comparable
    overall accuracy (a) and one-step ACR accuracy (b).
  Figure 9 Link: articels_figures_by_rev_year\2018\Active_Camera_Relocalization_from_a_Single_Reference_Image_without_HandEye_Calib\figure_9.jpg
  Figure 9 caption: An example of relocalization comparison of the two proposed ACR
    strategies. As visualized by FDF maps, the relocalization quality of straightforward
    strategy is much worse than that of the effective ACR approach, whose AFD is almost
    50 times smaller.
  First author gender probability: 0.74
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Fei-Peng Tian
  Name of the last author: Zhi-Qiang Liu
  Number of Figures: 18
  Number of Tables: 1
  Number of authors: 7
  Paper title: Active Camera Relocalization from a Single Reference Image without
    Hand-Eye Calibration
  Publication Date: 2018-09-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Physical Relocalization Accuracy Comparison of the Proposed
      ACR to Computational Rephotography [22]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2870646
- Affiliation of the first author: vgg, university of oxford, oxford, united kingdom
  Affiliation of the last author: vgg, university of oxford, oxford, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2018\Capturing_the_Geometry_of_Object_Categories_from_Video_Supervision\figure_1.jpg
  Figure 1 caption: We propose an architecture to learn the 3D geometry of object
    categories from videos only, without manual annotations. Once learned, the network
    can predict i) viewpoint, ii) depth, and iii) a point cloud, all from a single
    image of a new object instance.
  Figure 10 Link: articels_figures_by_rev_year\2018\Capturing_the_Geometry_of_Object_Categories_from_Video_Supervision\figure_10.jpg
  Figure 10 caption: Monocular depth prediction. Cumulative RMS depth reconstruction
    error for the LDOS data, when pixels are ranked by the predicted pixel-wise confidence.
  Figure 2 Link: articels_figures_by_rev_year\2018\Capturing_the_Geometry_of_Object_Categories_from_Video_Supervision\figure_2.jpg
  Figure 2 caption: "Overview of our architecture. As a preprocessing, structure from\
    \ motion (SfM) extracts egomotion and a depth map for every frame. For training,\
    \ our architecture takes pairs of frames f t , f t \u2032 and produces a viewpoint\
    \ estimate, a depth estimate, and a 3D geometry estimate. At test time, viewpoint,\
    \ depth, and 3D geometry are predicted from single images."
  Figure 3 Link: articels_figures_by_rev_year\2018\Capturing_the_Geometry_of_Object_Categories_from_Video_Supervision\figure_3.jpg
  Figure 3 caption: The core architecture of VpDR-Net. This figure describes the architecture
    of the hypercolumn (HC) module.
  Figure 4 Link: articels_figures_by_rev_year\2018\Capturing_the_Geometry_of_Object_Categories_from_Video_Supervision\figure_4.jpg
  Figure 4 caption: "The architecture of \u03A6 vp . Left: The layers of \u03A6 vp\
    \ . Right: Detail of the 3 \xD7 3 downsampling residual block."
  Figure 5 Link: articels_figures_by_rev_year\2018\Capturing_the_Geometry_of_Object_Categories_from_Video_Supervision\figure_5.jpg
  Figure 5 caption: "The architecture of \u03A6 depth . Illustration of the layers\
    \ specific to the depth prediction branch which predicts a depth map (Section\
    \ 3.2) and optionally an uncertainty map (Section 4.3)."
  Figure 6 Link: articels_figures_by_rev_year\2018\Capturing_the_Geometry_of_Object_Categories_from_Video_Supervision\figure_6.jpg
  Figure 6 caption: 'The architecture of Phi mathrmpcl . Left: The overview of the
    point cloud completion network, right: A detail of the fully connected residual
    block. Orange boxes denote the sizes of the layer outputs.'
  Figure 7 Link: articels_figures_by_rev_year\2018\Capturing_the_Geometry_of_Object_Categories_from_Video_Supervision\figure_7.jpg
  Figure 7 caption: Data augmentation. Training samples generated leveraging monocular
    depth estimation (ours, top) and using depth from ORB-slam2 (baseline, bottom).
    Missing pixels due to missing depth in red.
  Figure 8 Link: articels_figures_by_rev_year\2018\Capturing_the_Geometry_of_Object_Categories_from_Video_Supervision\figure_8.jpg
  Figure 8 caption: Viewpoint prediction. Qualitative comparison between our VpDR-Net
    and the baseline VPNet architecture trained on Freiburg Cars LDOS aligned with
    the method from [9] (VPNet-unsupervised). For each of the 4 considered object
    classes, the five most confident viewpoint predictions are visualized (sorted
    by the predicted confidence from left to right). Each predicted viewpoint is used
    to align the Pascal3D ground truth CAD model with the corresponding image.
  Figure 9 Link: articels_figures_by_rev_year\2018\Capturing_the_Geometry_of_Object_Categories_from_Video_Supervision\figure_9.jpg
  Figure 9 caption: Monocular depth prediction. Visualization of the predicted depth
    and confidence for different input images of the 4 considered classes. Depth maps
    are filtered by removing low confidence pixels. Lighter color corresponds to more
    confident regions.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.82
  Name of the first author: David Novotny
  Name of the last author: Andrea Vedaldi
  Number of Figures: 11
  Number of Tables: 4
  Number of authors: 3
  Paper title: Capturing the Geometry of Object Categories from Video Supervision
  Publication Date: 2018-09-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Viewpoint Prediction
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Viewpoint Prediction
  Table 3 caption:
    table_text: TABLE 3 Joint Viewpoint Prediction and Object Detection on Pascal3D
      Reporting the AVP Measure on the Validation Set
  Table 4 caption:
    table_text: TABLE 4 Point Cloud Prediction
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2871117
- Affiliation of the first author: surgical planning laboratory, harvard medical school,
    boston, usa
  Affiliation of the last author: surgical planning laboratory, harvard medical school,
    boston, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\Reweighting_and_Point_RANSACBased_PnnP_Solution_to_Handle_Outliers\figure_1.jpg
  Figure 1 caption: Demonstration of geometrical relationships with a bunny model.
    The mouth point is the control point o . In algorithm, all virtual points rotate
    and scale around the control point p o = q o = x o . We use the tail point to
    exemplify p i and its projection q i . Plane A is parallel to the imaging plane
    and passes the camera optical center. Without loss of generality and for clearer
    demonstration, in this figure we use focal length f=1 and all depths are distances
    between points and plane A.
  Figure 10 Link: articels_figures_by_rev_year\2018\Reweighting_and_Point_RANSACBased_PnnP_Solution_to_Handle_Outliers\figure_10.jpg
  Figure 10 caption: Runtime results with outlier-free synthetic data. Standard deviation
    of image noise sigma =5 pixels. The number of points increased from 100 to 1000.
    (Left) Ordinary 3D cases. (Right) Quasi-singular cases.
  Figure 2 Link: articels_figures_by_rev_year\2018\Reweighting_and_Point_RANSACBased_PnnP_Solution_to_Handle_Outliers\figure_2.jpg
  Figure 2 caption: "Demonstration of the updating method of the scale factor \u03BC\
    \ . One possible method is to update \u03BC according to the euclidean distances\
    \ between p i , q i , and o , which works for p 1 and q 1 because they have close\
    \ depths as o . However, this method may result in slow \u03BC updating rate for\
    \ p 2 and q 2 because \u2225 q 2 \u2212o\u2225\u2248\u2225 p 2 \u2212o\u2225 .\
    \ Hence, it is more efficient to compare v i and x i to move points p i to the\
    \ related lines of sight."
  Figure 3 Link: articels_figures_by_rev_year\2018\Reweighting_and_Point_RANSACBased_PnnP_Solution_to_Handle_Outliers\figure_3.jpg
  Figure 3 caption: "Iterative process with 2D space and 1D camera imaging plane.\
    \ P 1 , P 2 , P 3 , and P 4 are the object points; P 1 is selected as the control\
    \ point ( o=1 ). (a) The process p (k) \u2192 q (k+1) \u2192 p (k+1) \u2192...\
    \ makes the estimation pose approach the correct solution. (b) The rotation related\
    \ to p (k+1) is worse than that related to p (k) , which means the process is\
    \ approaching a local minima, which is a mirror-image form of the true object\
    \ shape."
  Figure 4 Link: articels_figures_by_rev_year\2018\Reweighting_and_Point_RANSACBased_PnnP_Solution_to_Handle_Outliers\figure_4.jpg
  Figure 4 caption: "In R1PP n P, the selection of control point o is related to the\
    \ convergence rate. (a) An example to illustrate this behavior with 2D space and\
    \ 1D camera imaging plane . According to the R and \u03BC updating methods in\
    \ Eqs. (15) and (16). \u2220(p\u2212 o 1 ,q\u2212 o 1 )>\u2220(p\u2212 o 2 ,q\u2212\
    \ o 2 ) and \u2225 x i \u2212 x o1 \u2225\u2225 v i \u2212 x o1 \u2225>\u2225\
    \ x i \u2212 x o2 \u2225\u2225 v i \u2212 x o2 \u2225 suggest that the iteration\
    \ process is more likely to have larger R and \u03BC updating rate when o is closer\
    \ to p . (b) A real-world example to illustrate this behavior, the radius of a\
    \ circle represents the required number of iterations when using this feature\
    \ point as the control point o ."
  Figure 5 Link: articels_figures_by_rev_year\2018\Reweighting_and_Point_RANSACBased_PnnP_Solution_to_Handle_Outliers\figure_5.jpg
  Figure 5 caption: The overall flow chart of R1PP n P.
  Figure 6 Link: articels_figures_by_rev_year\2018\Reweighting_and_Point_RANSACBased_PnnP_Solution_to_Handle_Outliers\figure_6.jpg
  Figure 6 caption: Experiments with synthetic data (ordinary 3D case, 50 percent
    of outliers) to demonstrate the iteration process of R1PP n P when the control
    point o is an outlier. Randomly colored lines are results with different control
    points. (a) The changes of estimated camera pose between frames k and k-1 are
    complex during the iteration process, based on which it is difficult to decide
    when to stop the process. (b) It is more robust and efficient to stop the process
    when no more inliers can be detected.
  Figure 7 Link: articels_figures_by_rev_year\2018\Reweighting_and_Point_RANSACBased_PnnP_Solution_to_Handle_Outliers\figure_7.jpg
  Figure 7 caption: Except for REPP n P and R1PP n P, most P n P methods cannot handle
    outliers.
  Figure 8 Link: articels_figures_by_rev_year\2018\Reweighting_and_Point_RANSACBased_PnnP_Solution_to_Handle_Outliers\figure_8.jpg
  Figure 8 caption: Accuracy with outlier-free synthetic data (ordinary 3D cases).
    Number of points was 100. Different levels of image noises were added.
  Figure 9 Link: articels_figures_by_rev_year\2018\Reweighting_and_Point_RANSACBased_PnnP_Solution_to_Handle_Outliers\figure_9.jpg
  Figure 9 caption: Accuracy with outlier-free synthetic data (quasi-singular cases).
    Number of points was 100. Different levels of image noises were added. PPnP is
    out of range. The accuracy of all P n P methods decreased significantly compared
    with those in ordinary 3D cases, as shown in Fig. 8.
  First author gender probability: 0.69
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Haoyin Zhou
  Name of the last author: Jayender Jagadeesan
  Number of Figures: 15
  Number of Tables: 0
  Number of authors: 3
  Paper title: 'Re-weighting and 1-Point RANSAC-Based P

    n

    nP Solution to Handle Outliers'
  Publication Date: 2018-09-23 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2871832
- Affiliation of the first author: department of computer science, ucf, orlando, usa
  Affiliation of the last author: department of computer science, ucf, orlando, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\Sparse_OneGrab_Sampling_with_Probabilistic_Guarantees\figure_1.jpg
  Figure 1 caption: The population includes eight lines and 50 percent gross outliers.
    (a) shows the data points and sampled subset when the size of the sample set is
    calculated using SWIFT. (b) shows the detected lines using SWIFT samples and mean-shift.
    (c) and (d) show the underestimated sample size and the detected lines, failing
    to find all the instances in the data.
  Figure 10 Link: articels_figures_by_rev_year\2018\Sparse_OneGrab_Sampling_with_Probabilistic_Guarantees\figure_10.jpg
  Figure 10 caption: The effect of image segmentation accuracy on the average number
    of inliers sampled in each model instance when we have three motions.
  Figure 2 Link: articels_figures_by_rev_year\2018\Sparse_OneGrab_Sampling_with_Probabilistic_Guarantees\figure_2.jpg
  Figure 2 caption: "Comparison of estimated r averaged over 200 independent trials\
    \ versus ground-truth of r when N\u2208 10 3 , 10 4 , 10 5 , \u03B5\u22082,5 ,\
    \ and C\u22085,50 ."
  Figure 3 Link: articels_figures_by_rev_year\2018\Sparse_OneGrab_Sampling_with_Probabilistic_Guarantees\figure_3.jpg
  Figure 3 caption: "Examine the difference between \u03B8 and \u03B5 on the accuracy\
    \ of estimated r in hypergeometric distribution. Here, N\u2208 10 3 , 10 4 , 10\
    \ 5 , \u03B5=5 and from left to right C is C\u22085,20,50 ."
  Figure 4 Link: articels_figures_by_rev_year\2018\Sparse_OneGrab_Sampling_with_Probabilistic_Guarantees\figure_4.jpg
  Figure 4 caption: "Examine the average accuracy of estimated r using multinomial\
    \ estimation in Equation (18). Here, N= 10 5 and from left to right \u03B5=20,2\
    \ and C\u22084,20 . The condition of r\u226BC is satisfied in (a) and violated\
    \ in (b)."
  Figure 5 Link: articels_figures_by_rev_year\2018\Sparse_OneGrab_Sampling_with_Probabilistic_Guarantees\figure_5.jpg
  Figure 5 caption: "Estimated values of lower bound and upper bound of r for 1\u2212\
    \u03B4 averaged over 200 independent trials versus the ground-truth when N= 10\
    \ 4 and \u03B5=4 . From left to right: C\u22085,20,50 ."
  Figure 6 Link: articels_figures_by_rev_year\2018\Sparse_OneGrab_Sampling_with_Probabilistic_Guarantees\figure_6.jpg
  Figure 6 caption: The effect of changing different parameters on the sample size
    r . (a) Increasing the value of varepsilon forces the method to select more samples
    in order to remain with the same probability. (b) and (c) Increasing C or decreasing
    theta forces the method to grab more samples to reach the same probability.
  Figure 7 Link: articels_figures_by_rev_year\2018\Sparse_OneGrab_Sampling_with_Probabilistic_Guarantees\figure_7.jpg
  Figure 7 caption: Using SWIFT to detect 3D Planes when 1-delta =0.9 . (a) Data is
    from the Castelvecchio dataset [21] where r = 42 and N=754 . (b) Blue points are
    outliers that are not grouped in any model when r = 43 and N=3,000 .
  Figure 8 Link: articels_figures_by_rev_year\2018\Sparse_OneGrab_Sampling_with_Probabilistic_Guarantees\figure_8.jpg
  Figure 8 caption: Detecting planes in 3D point cloud data collected using a Kinect.
  Figure 9 Link: articels_figures_by_rev_year\2018\Sparse_OneGrab_Sampling_with_Probabilistic_Guarantees\figure_9.jpg
  Figure 9 caption: Synthetic data for multibody structure from motion. The outliers
    are added after moving objects and computing the 2D projections. These outliers
    are not shown in this figure. (a) The 3D objects that are not necessarily planar.
    (b) 3D objects are moved randomly and independently. (c) The image is taken from
    (a). (d) The image is taken from (b) after moving the camera.
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Maryam Jaberi
  Name of the last author: Hassan Foroosh
  Number of Figures: 16
  Number of Tables: 2
  Number of authors: 3
  Paper title: Sparse One-Grab Sampling with Probabilistic Guarantees
  Publication Date: 2018-09-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance Comparison Among 3 Different Scenarios of (i)
      SWIFT Sample Size (ii) Underestimated Sample Size (iii) Overestimated Sample
      Size
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparing the Size of SWIFT Sampling, the Manually Sampled
      Points, and the Ground-Truth
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2871850
- Affiliation of the first author: national university of singapore, singapore
  Affiliation of the last author: national university of singapore, singapore
  Figure 1 Link: articels_figures_by_rev_year\2018\Anticipating_Where_People_will_Look_Using_Adversarial_Networks\figure_1.jpg
  Figure 1 caption: 'Problem illustration: Gaze anticipation on future frames within
    a few seconds on egocentric videos. Given the current frame, the task is to predict
    the future gaze locations. Our proposed DFG method solves this problem through
    synthesizing future frames (transparent ones) and predicting corresponding future
    gaze locations (red circles).'
  Figure 10 Link: articels_figures_by_rev_year\2018\Anticipating_Where_People_will_Look_Using_Adversarial_Networks\figure_10.jpg
  Figure 10 caption: Visualization of the convolution filters in the first (GP1) and
    the second last (GP4) 3D convolution layers of Temporal Saliency Prediction Module
    in our DFG model. (a) The filters in the first 3D convolution layer show low-level
    features, such as edges. (b) The regions of salient objects are highly activated
    in the second last convolution layer, such as the fonts on the oatmeal box.
  Figure 2 Link: articels_figures_by_rev_year\2018\Anticipating_Where_People_will_Look_Using_Adversarial_Networks\figure_2.jpg
  Figure 2 caption: 'Architecture of our proposed deep future gaze (DFG) model. It
    contains DFG-P and DFG-G pathways. In Generator in Future Frame Generation Module
    in DFG-G, latent representation of the current frame I t is extracted by 2D ConvNet.
    To explicitly untangle foreground and background, it then branches into two streams:
    one for learning the representation for the foreground and the mask; one for learning
    the representation of the background. These 3 streams are combined to generate
    future frames (blue boundaries). Based on the generated frames, Temporal Saliency
    Prediction Module predicts the temporal saliency maps. As a competitor to Generator,
    Discriminator uses a 3D ConvNet to distinguish the generated frames from real
    frames R t,t+N (black boundaries) by classifying its inputs to real or fake. DFG-P
    predicts the gaze spatial priors from the task at hand inferred from the latent
    representation of I t . Element-wise summation is performed on the gaze spatial
    prior maps and the temporal saliency maps to produce the anticipated gaze locations
    (red dots).'
  Figure 3 Link: articels_figures_by_rev_year\2018\Anticipating_Where_People_will_Look_Using_Adversarial_Networks\figure_3.jpg
  Figure 3 caption: Evaluation of gaze anticipation using area under the curve (AUC)
    on the current frame as well as 31 future frames in GTEA, OST and Hollywood2 dataset.
    Evaluation results in GTEAplus dataset are similar as GTEA. See Supplementary
    Material, available online, for evaluation results of gaze anticipation in GTEAplus
    Dataset. Larger is better. The algorithms in the legend are introduced in Section
    4.3.
  Figure 4 Link: articels_figures_by_rev_year\2018\Anticipating_Where_People_will_Look_Using_Adversarial_Networks\figure_4.jpg
  Figure 4 caption: Evaluation of gaze anticipation using Average Angular Error (AAE)
    on the current frame as well as 31 future frames in GTEA, OST and Hollywood2 Dataset.
    Evaluation results in GTEAplus dataset are similar as GTEA. See Supplementary
    Material, available online, for evaluation results of gaze anticipation in GTEAplus
    Dataset. Smaller is better. The algorithms in the legend are introduced in Section
    4.3.
  Figure 5 Link: articels_figures_by_rev_year\2018\Anticipating_Where_People_will_Look_Using_Adversarial_Networks\figure_5.jpg
  Figure 5 caption: Example results of gaze anticipation on GTEAplus egocentric video
    dataset. Our DFG model produces 31 future frames based on the current frame. From
    first to last rows, results on future frames 1, 5, 9, 17, 29 with respect to the
    current frame are shown. The leftmost column shows the ground truth (GT) with
    red circle denoting human gaze locations. Column 2, 3, 4 (FG, mask, BG) show the
    foreground F(cdot) , the mask M(cdot) , and the background B(cdot) learnt by Generator
    respectively. Column 5 shows the generated future frames (GEN). Column 6 and 7
    show the corresponding predicted temporal saliency maps from two pathways DFG-G
    and DFG-P in our model. Column 8 show the final integrated temporal saliency maps
    predicted by our model. Column 9 and onwards show the predicted temporal saliency
    maps by all baselines (See Section 4.3). Best viewed in color. See Supplementary
    Material, available online, for more qualitative examples.
  Figure 6 Link: articels_figures_by_rev_year\2018\Anticipating_Where_People_will_Look_Using_Adversarial_Networks\figure_6.jpg
  Figure 6 caption: Example results of gaze anticipation on Hollywood2 third person
    video dataset. The format and conventions follow those in Fig. 5.
  Figure 7 Link: articels_figures_by_rev_year\2018\Anticipating_Where_People_will_Look_Using_Adversarial_Networks\figure_7.jpg
  Figure 7 caption: Evaluation of average gaze anticipation performance over 31 future
    frames versus magnitude of head motions in GTEA.
  Figure 8 Link: articels_figures_by_rev_year\2018\Anticipating_Where_People_will_Look_Using_Adversarial_Networks\figure_8.jpg
  Figure 8 caption: Evaluation of average gaze anticipation performance over 31 future
    frames versus confidence of Discriminator in our model in GTEA.
  Figure 9 Link: articels_figures_by_rev_year\2018\Anticipating_Where_People_will_Look_Using_Adversarial_Networks\figure_9.jpg
  Figure 9 caption: Schematic description of human psychophysics experiment on gaze
    anticipation. In Training Phase 1, subjects are presented with all the video frames
    of 5 training video clips and their corresponding over-laid ground truth gaze
    locations denoted by red circles. In Training Phase 2, subjects are first presented
    with the current video frame with ground truth gaze location same as Training
    Phase 1 followed by a blank gray screen. Subjects use computer mouses to click
    on the anticipated gaze location for the t+1 th frame. Next, the ground truth
    video frame overlaid with ground truth gaze location (red circle) and mouse click
    location (blue cross) are shown. Repeat for all 5 training video clips. In the
    testing phase, subjects are only presented with the current frame. They have to
    use computer mouse to click on the predicted gaze location on the current frame
    as well as anticipated future gaze locations on blank gray screen for a total
    of 100 testing video clips (50 clips per dataset in GTEA and GTEAplus).
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Mengmi Zhang
  Name of the last author: Jiashi Feng
  Number of Figures: 10
  Number of Tables: 7
  Number of authors: 5
  Paper title: Anticipating Where People will Look Using Adversarial Networks
  Publication Date: 2018-09-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Averaged Gaze Anticipation Performance over Current Frame
      as well as 31 Future Frames Using Normalized Saliency Scanpath (NSS) and the
      Area Under the Precision-Recall Curve (PR)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Evaluation of Center Bias Effect over the Next 31 Frames
  Table 3 caption:
    table_text: TABLE 3 Average Spatial Bias and Human Performance over the Next 31
      Frames on GTEA and GTEAplus Datasets
  Table 4 caption:
    table_text: TABLE 4 Statistics of Camera and Gaze Motions
  Table 5 caption:
    table_text: TABLE 5 Ablation Study on GTEA, OST and Hollywood2 Datasets
  Table 6 caption:
    table_text: TABLE 6 Results of Gaze Prediction on the Current Frame
  Table 7 caption:
    table_text: TABLE 7 Evaluation of Gaze Anticipation on Frames at Time t+16 t+16
      and t+32 t+32
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2871688
- Affiliation of the first author: department of computer science, city university
    of hong kong, kowloon, hong kong
  Affiliation of the last author: codehatch corp., edmonton, canada
  Figure 1 Link: articels_figures_by_rev_year\2018\On_the_Effectiveness_of_Least_Squares_Generative_Adversarial_Networks\figure_1.jpg
  Figure 1 caption: Illustration of different behaviors of two loss functions. (a)
    Decision boundaries of two loss functions. Note that the decision boundary should
    go across real data distribution for a successful GANs learning. Otherwise, the
    learning process is saturated. (b) Decision boundary of the sigmoid cross entropy
    loss function. The orange area is the side of real samples, and the blue area
    is the side of fake samples. The non-saturating loss and the minimax loss will
    cause almost no gradient for the fake samples in magenta and green, respectively,
    when we use them to update the generator. (c) Decision boundary of the least squares
    loss function. It penalizes the fake samples (both in magenta and green), and
    as a result, it forces the generator to generate samples toward the decision boundary.
  Figure 10 Link: articels_figures_by_rev_year\2018\On_the_Effectiveness_of_Least_Squares_Generative_Adversarial_Networks\figure_10.jpg
  Figure 10 caption: Evaluation on datasets with small variability. All the tasks
    are conducted using the same network architecture as shown in Table 5. DCGANs
    succeed in learning on MNIST but fail on the two synthetic digit datasets with
    small variability, while LSGANs succeed in learning on all the three datasets.
  Figure 2 Link: articels_figures_by_rev_year\2018\On_the_Effectiveness_of_Least_Squares_Generative_Adversarial_Networks\figure_2.jpg
  Figure 2 caption: (a) The non-saturating loss and the minimax loss. (b) The least
    squares loss.
  Figure 3 Link: articels_figures_by_rev_year\2018\On_the_Effectiveness_of_Least_Squares_Generative_Adversarial_Networks\figure_3.jpg
  Figure 3 caption: Generated images on LSUN-bedroom.
  Figure 4 Link: articels_figures_by_rev_year\2018\On_the_Effectiveness_of_Least_Squares_Generative_Adversarial_Networks\figure_4.jpg
  Figure 4 caption: Generated images on different scene datasets.
  Figure 5 Link: articels_figures_by_rev_year\2018\On_the_Effectiveness_of_Least_Squares_Generative_Adversarial_Networks\figure_5.jpg
  Figure 5 caption: (a) and (b) Generated images on cat datasets. (c), (d), and (e)
    Comparison by zooming in on the details of the images. LSGANs generate cats with
    sharper and more exquisite hair and faces than the ones generated by NS-GANs.
  Figure 6 Link: articels_figures_by_rev_year\2018\On_the_Effectiveness_of_Least_Squares_Generative_Adversarial_Networks\figure_6.jpg
  Figure 6 caption: Interpolation result by LSGANs. The generated images show smooth
    transitions, which indicates that LSGANs have learned semantic representations
    in the latent space.
  Figure 7 Link: articels_figures_by_rev_year\2018\On_the_Effectiveness_of_Least_Squares_Generative_Adversarial_Networks\figure_7.jpg
  Figure 7 caption: (a) Comparison of FID on LSUN between NS-GANs and LSGANs during
    the learning process, which is aligned with iterations. (b) Comparison of FID
    on LSUN between WGANs-GP and LSGANs during the learning process, which is aligned
    with wall-clock time.
  Figure 8 Link: articels_figures_by_rev_year\2018\On_the_Effectiveness_of_Least_Squares_Generative_Adversarial_Networks\figure_8.jpg
  Figure 8 caption: Dynamic results of Gaussian kernel estimation for NS-GANs and
    LSGANs. The final column shows the distribution of real data.
  Figure 9 Link: articels_figures_by_rev_year\2018\On_the_Effectiveness_of_Least_Squares_Generative_Adversarial_Networks\figure_9.jpg
  Figure 9 caption: Comparison experiments between NS-GANs and LSGANs by excluding
    batch normalization (BN).
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xudong Mao
  Name of the last author: Stephen Paul Smolley
  Number of Figures: 12
  Number of Tables: 5
  Number of authors: 6
  Paper title: On the Effectiveness of Least Squares Generative Adversarial Networks
  Publication Date: 2018-09-23 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 The Network Architecture for Scene Generation, Where CONV\
      \ Denotes the Convolutional Layer, TCONV Denotes the Transposed Convolutional\
      \ Layer, FC Denotes the Fully-Connected Layer, BN Denotes the Batch Normalization,\
      \ LReLU Denotes the Leaky-ReLU, and (K3,S2,O256) Denotes a Layer with 3\xD7\
      3 3\xD73 Kernel, Stride 2, and 256 Output Filters"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Network Architecture for Cats Generation
  Table 3 caption:
    table_text: TABLE 3 FID Results of NS-GANs, WGANs-GP, and LSGANs on Four Datasets
  Table 4 caption:
    table_text: TABLE 4 Experiments on Gaussian Mixture Distribution
  Table 5 caption:
    table_text: TABLE 5 The Network Architecture for Stability Evaluation on Datasets
      with Small Variability
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2872043
- Affiliation of the first author: cristal laboratory, imt lille-douai, university
    of lille, lille, france
  Affiliation of the last author: university of florence, firenze, italy
  Figure 1 Link: articels_figures_by_rev_year\2018\A_Novel_Geometric_Framework_on_Gram_Matrix_Trajectories_for_Human_Behavior_Under\figure_1.jpg
  Figure 1 caption: Overview of the proposed approach. Given a landmark sequence,
    the Gram matrices are computed for each landmark configuration to build trajectories
    on S + (d,n) . A moving shape is hence assimilated to an ellipsoid traveling along
    d -dimensional subspaces of R n , with d S + used to compare static ellipsoids.
    Dynamic Time Warping (DTW) is then used to align and compare trajectories in a
    rate-invariant manner. Finally, the ppfSVM is used on these trajectories for classification.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\A_Novel_Geometric_Framework_on_Gram_Matrix_Trajectories_for_Human_Behavior_Under\figure_2.jpg
  Figure 2 caption: 'A pictorial representation of the positive semidefinite cone
    S + (d,n) . Viewing matrices G 1 and G 2 as ellipsoids in R n ; their closeness
    consists of two contributions: d 2 G (squared Grassmann distance) and d 2 P d
    (squared Riemannian distance in P d ).'
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.93
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Anis Kacem
  Name of the last author: Stefano Berretti
  Number of Figures: 2
  Number of Tables: 5
  Number of authors: 4
  Paper title: A Novel Geometric Framework on Gram Matrix Trajectories for Human Behavior
    Understanding
  Publication Date: 2018-09-28 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Overall Accuracy (%) on the UT-Kinect, Florence3D, SBU Interaction,
      and SYSU-3D Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Baseline Experiments on the UT-Kinect, SBU, SYSU3D, and Florence3D
      Datasets
  Table 3 caption:
    table_text: TABLE 3 Comparative Study of the Proposed Approach with Baseline Experiments
      on the P-BME Dataset
  Table 4 caption:
    table_text: TABLE 4 Overall Accuracy (%) on CK+ and MMI Datasets
  Table 5 caption:
    table_text: TABLE 5 Overall Accuracy on Oulu-CASIA and AFEW Dataset (Following
      the EmotiW13 protocol [70])
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2872564
