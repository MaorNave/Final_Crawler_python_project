- Affiliation of the first author: department of computer science, university of bristol,
    bristol, united kingdom
  Affiliation of the last author: department of computer science, university of bristol,
    bristol, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2020\The_EPICKITCHENS_Dataset_Collection_Challenges_and_Baselines\figure_1.jpg
  Figure 1 caption: 'From Top: Frames from the 32 environments; Narrations by participants
    used to annotate action segments; Active object bounding box annotations.'
  Figure 10 Link: articels_figures_by_rev_year\2020\The_EPICKITCHENS_Dataset_Collection_Challenges_and_Baselines\figure_10.jpg
  Figure 10 caption: Routine activity over four days (rows), during breakfast for
    the same subject (P22). Coloured segments illustrate the actions performed in
    the video, with colours indicating action classes and gaps corresponding to background
    (unlabelled frames). Action segments are normalised by the video length. In total
    245 action classes were present. We highlight six classes to demonstrate the variability
    in routines over several days.
  Figure 2 Link: articels_figures_by_rev_year\2020\The_EPICKITCHENS_Dataset_Collection_Challenges_and_Baselines\figure_2.jpg
  Figure 2 caption: Head-mounted GoPro used in dataset recording.
  Figure 3 Link: articels_figures_by_rev_year\2020\The_EPICKITCHENS_Dataset_Collection_Challenges_and_Baselines\figure_3.jpg
  Figure 3 caption: Instructions used to collect video narrations from our participants.
  Figure 4 Link: articels_figures_by_rev_year\2020\The_EPICKITCHENS_Dataset_Collection_Challenges_and_Baselines\figure_4.jpg
  Figure 4 caption: 'Top (left to right): time of day of the recording, histogram
    of sequence durations and pie chart of high-level goals; Bottom: Wordles of narrations
    in native languages (English, Italian, Spanish, Greek and Chinese).'
  Figure 5 Link: articels_figures_by_rev_year\2020\The_EPICKITCHENS_Dataset_Collection_Challenges_and_Baselines\figure_5.jpg
  Figure 5 caption: Example temporal annotations for three consecutive actions.
  Figure 6 Link: articels_figures_by_rev_year\2020\The_EPICKITCHENS_Dataset_Collection_Challenges_and_Baselines\figure_6.jpg
  Figure 6 caption: 'Distance between narration timestamp and ground truth action
    segments. Top: normalised using the action segment length. Bottom: in seconds.
    The narration timestamp is relative to the speech recorded by the participants.
    Ground truth segments refer to the video start and end times.'
  Figure 7 Link: articels_figures_by_rev_year\2020\The_EPICKITCHENS_Dataset_Collection_Challenges_and_Baselines\figure_7.jpg
  Figure 7 caption: Object annotation selection, our method (green) versus best annotation
    per frame (orange). Other annotators are in blue.
  Figure 8 Link: articels_figures_by_rev_year\2020\The_EPICKITCHENS_Dataset_Collection_Challenges_and_Baselines\figure_8.jpg
  Figure 8 caption: 'From Top: Frequency of verb classes in action segments; Frequency
    of noun clusters in action segments, by category; Frequency of noun clusters in
    bounding box annotations, by category; Mean and standard deviation of bounding
    box, by category.'
  Figure 9 Link: articels_figures_by_rev_year\2020\The_EPICKITCHENS_Dataset_Collection_Challenges_and_Baselines\figure_9.jpg
  Figure 9 caption: "Left: Frequently co-occurring verbnouns in action segments [e.g.,\
    \ (openclose, cupboarddrawerfridge), (peel, carrotonionpotatopeach), (adjust,\
    \ heat)]; Mid: Next-action excluding repetitive instances of the same action [e.g.,\
    \ pour \u2192 mix, peel \u2192 cut or peel \u2192 throw, turn-on \u2192 wash].\
    \ Right: Co-occurring bounding boxes within the same annotated frame [e.g., (cup,\
    \ coffee), (knife, chopping board), (tap, sponge)]."
  First author gender probability: 0.9
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Dima Damen
  Name of the last author: Michael Wray
  Number of Figures: 16
  Number of Tables: 10
  Number of authors: 11
  Paper title: 'The EPIC-KITCHENS Dataset: Collection, Challenges and Baselines'
  Publication Date: 2020-05-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparative Overview of Relevant Datasets
  Table 10 caption:
    table_text: TABLE 10 Baseline Results for the Action Anticipation Challenge
  Table 2 caption:
    table_text: TABLE 2 Extracts from 6 Transcription Files in .sbv Format
  Table 3 caption:
    table_text: TABLE 3 Sample Video Summaries
  Table 4 caption:
    table_text: TABLE 4 Sample Verb and Noun Classes
  Table 5 caption:
    table_text: 'TABLE 5 Statistics of Test Splits: Seen (S1) and Unseen (S2) Kitchens:
      Number of Verb, Noun, Action, and Zero-Shot (ZS) Classes'
  Table 6 caption:
    table_text: TABLE 6 Baseline Results for the Object Detection Challenge
  Table 7 caption:
    table_text: TABLE 7 Object Detection mAP (IoU > > 0.5) the Ratio of the Object
      Bounding Boxs Area (A) to the Frame
  Table 8 caption:
    table_text: TABLE 8 Baseline Results for the Action Recognition Challenge
  Table 9 caption:
    table_text: "TABLE 9 Action Recognition Challenge\u2014Comparison of Temporal\
      \ Modelling Architectures"
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2991965
- Affiliation of the first author: computer science department, university of illinois
    at urbana-champaign, urbana, il, usa
  Affiliation of the last author: computer science department, university of illinois
    at urbana-champaign, urbana, il, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Contextual_Translation_Embedding_for_Visual_Relationship_Detection_and_Scene_Gra\figure_1.jpg
  Figure 1 caption: "Overview of our UVTransE visual relationship detection model.\
    \ Given an image, Faster R-CNN is fist used to detect objects. For each pair of\
    \ detected objects, appearance and spatial features are extracted and fed into\
    \ the visual module, which computes the UVTransE embedding: union \u2212 (subject\
    \ + object). The predicate embedding output by UVTransE may be optionally sent\
    \ to a Bi-GRU language model. Finally, triplets ( s , p , o ) are ranked based\
    \ on scores from the visual, language, and object detection modules."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Contextual_Translation_Embedding_for_Visual_Relationship_Detection_and_Scene_Gra\figure_2.jpg
  Figure 2 caption: Examples of relationship detection on the VRD test split. A triplet
    is correctly recognized if both the bounding boxes are correctly localized and
    the predicate matches the ground truth. The Missing GT column shows relationships
    that were marked as incorrect since they are not present in the ground truth.
    The Incorrect column shows legitimate mistakes. The last row shows our zero-shot
    results.
  Figure 3 Link: articels_figures_by_rev_year\2020\Contextual_Translation_Embedding_for_Visual_Relationship_Detection_and_Scene_Gra\figure_3.jpg
  Figure 3 caption: "Top three retrievals for a set of UnRel triplet queries with\
    \ our model. A relationship is marked as positive if the subject and object boxes\
    \ have IoU\u22650.3 with the ground truth. Otherwise, it is marked as an error\
    \ (red box around the entire image)."
  Figure 4 Link: articels_figures_by_rev_year\2020\Contextual_Translation_Embedding_for_Visual_Relationship_Detection_and_Scene_Gra\figure_4.jpg
  Figure 4 caption: "Example scene graphs generated on VG-IMP images. In the images,\
    \ green boxes are objects detected with IoU\u22650.5 , while orange boxes are\
    \ ground truth objects that are not detected by our pipeline. In the scene graphs,\
    \ green ellipses are true positive relations recognized by our model at Recall20,\
    \ orange ellipses are false negatives, and magenta ellipses are false positives\
    \ (sometimes due to missing ground truth)."
  Figure 5 Link: articels_figures_by_rev_year\2020\Contextual_Translation_Embedding_for_Visual_Relationship_Detection_and_Scene_Gra\figure_5.jpg
  Figure 5 caption: "Example scene graphs generated on Open Images. In the images,\
    \ green boxes are objects detected with IoU\u22650.5 , while orange ones are ground\
    \ truth objects that are not detected. In the scene graphs, attributes are represented\
    \ with cyan boxes. Green ellipses are true positive relations recognized by our\
    \ model at Recall20, orange ellipses are false negatives, and magenta ellipses\
    \ are false positives."
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Zih-Siou Hung
  Name of the last author: Svetlana Lazebnik
  Number of Figures: 5
  Number of Tables: 10
  Number of authors: 3
  Paper title: Contextual Translation Embedding for Visual Relationship Detection
    and Scene Graph Generation
  Publication Date: 2020-05-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Effect of C C on Recall50 on the Stanford VRD Dataset
  Table 10 caption:
    table_text: TABLE 10 Results on Open Images Challenge for the Top Three Teams
      on the Public Leaderboard Versus Our Methods
  Table 2 caption:
    table_text: TABLE 2 Comparisons of Baselines to Our Proposed Method on the Stanford
      VRD Dataset
  Table 3 caption:
    table_text: TABLE 3 Summary of State-of-the-Art Methods on the VRD Dataset
  Table 4 caption:
    table_text: TABLE 4 Full Test Set Performance on the Stanford VRD Dataset
  Table 5 caption:
    table_text: TABLE 5 Zero-Shot Performance on the Stanford VRD Dataset
  Table 6 caption:
    table_text: TABLE 6 Retrieval on UnRel (mAP) With IoU=0.3
  Table 7 caption:
    table_text: TABLE 7 Summary of State-of-the-Art Methods on the VG-IMP Dataset
  Table 8 caption:
    table_text: TABLE 8 Full Test Set Performance on the VG-IMP Dataset
  Table 9 caption:
    table_text: TABLE 9 Full Test Set Performance on the VG-VTransE Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2992222
- Affiliation of the first author: wangxuan institute of computer technology, peking
    university, beijing, china
  Affiliation of the last author: jd ai research, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Unpaired_Person_Image_Generation_With_Semantic_Parsing_Transformation\figure_1.jpg
  Figure 1 caption: 'Our overall framework for unpaired person image generation. We
    decompose the mapping into semantic parsing transformation ( H S ) and appearance
    generation ( H A ). Note that H A consists of two streams: a foreground generative
    network (in grey) and a background generative network (in orange) to render foreground
    and background, respectively. H S generates the semantic map S ~ p t under the
    target pose p t , and H A further generates the output I ~ p t guided by S ~ p
    t . Then we achieve the cycle to get the recovered input I ~ p s . For simplicity,
    we omit some inputs of the generator. The detailed illustration of H S and H A
    can be found in Figs. 2 and 4, respectively.'
  Figure 10 Link: articels_figures_by_rev_year\2020\Unpaired_Person_Image_Generation_With_Semantic_Parsing_Transformation\figure_10.jpg
  Figure 10 caption: Analysis of semantic parsing transformation. (a) Remove pose
    masks from the input. (b) Remove pose heat maps from the input. (c) Remove L adv
    S in Eq. (5). The results of the semantic maps generated by our semantic generative
    network are in the right.
  Figure 2 Link: articels_figures_by_rev_year\2020\Unpaired_Person_Image_Generation_With_Semantic_Parsing_Transformation\figure_2.jpg
  Figure 2 caption: "Semantic parsing transformation module. The semantic generator\
    \ H S consists of a semantic map encoder E S , a pose encoder E P , and a semantic\
    \ map generator G S , which can be formulated as H S = G S \u2218( E S , E P )\
    \ ."
  Figure 3 Link: articels_figures_by_rev_year\2020\Unpaired_Person_Image_Generation_With_Semantic_Parsing_Transformation\figure_3.jpg
  Figure 3 caption: In the process of pseudo label searching, the human bodies are
    decomposed into rigid parts and aligned by affine transformations.
  Figure 4 Link: articels_figures_by_rev_year\2020\Unpaired_Person_Image_Generation_With_Semantic_Parsing_Transformation\figure_4.jpg
  Figure 4 caption: "Appearance generation. We consider two streams: foreground and\
    \ background generation. The appearance generator is formulated as H A =( G F\
    \ A \u2218( E F A , E \u2032 S ))\u2295( G B A \u2218 E B A ) ."
  Figure 5 Link: articels_figures_by_rev_year\2020\Unpaired_Person_Image_Generation_With_Semantic_Parsing_Transformation\figure_5.jpg
  Figure 5 caption: Potential errors in the searched semantic map pair that might
    harm semantic parsing transformation.
  Figure 6 Link: articels_figures_by_rev_year\2020\Unpaired_Person_Image_Generation_With_Semantic_Parsing_Transformation\figure_6.jpg
  Figure 6 caption: Example results of images for DeepFashion (wo b.g.) by different
    methods (PG 2 [1], Def-GAN [5], UPIS [3], and V-UNet [4]). Our model better keeps
    clothing attributes (e.g., textures, clothing types).
  Figure 7 Link: articels_figures_by_rev_year\2020\Unpaired_Person_Image_Generation_With_Semantic_Parsing_Transformation\figure_7.jpg
  Figure 7 caption: Example results of images for DeepFashion (w b.g.) by different
    methods (PG 2 [1], Def-GAN [5], UPIS [3], and V-UNet [4]). Our model is able to
    retain the backgrounds from condition images and generate natural results.
  Figure 8 Link: articels_figures_by_rev_year\2020\Unpaired_Person_Image_Generation_With_Semantic_Parsing_Transformation\figure_8.jpg
  Figure 8 caption: Example results by different methods (PG 2 [1], Def-GAN [5], UPIS
    [3], and V-UNet [4]) on Market-1501. Our model generates better body shapes.
  Figure 9 Link: articels_figures_by_rev_year\2020\Unpaired_Person_Image_Generation_With_Semantic_Parsing_Transformation\figure_9.jpg
  Figure 9 caption: Ablation studies on semantic parsing transformation.
  First author gender probability: 0.67
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.99
  Name of the first author: Sijie Song
  Name of the last author: Tao Mei
  Number of Figures: 22
  Number of Tables: 4
  Number of authors: 5
  Paper title: Unpaired Person Image Generation With Semantic Parsing Transformation
  Publication Date: 2020-05-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Results on DeepFashion and Market-1501 Datasets
      (Based on Our Implementation)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 User Study Results on Pose-Guided Person Image Generation
  Table 3 caption:
    table_text: TABLE 3 Quantitative Results Under Different Configurations on DeepFashion
      (wo b.g.) and Market-1501 Datasets
  Table 4 caption:
    table_text: TABLE 4 Analysis of Pseudo Label Generation
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2992105
- Affiliation of the first author: department of computer science and engineering,
    university at buffalo, buffalo, ny, usa
  Affiliation of the last author: department of computer science and engineering,
    university at buffalo, buffalo, ny, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Chart_Mining_A_Survey_of_Methods_for_Automated_Chart_Analysis\figure_1.jpg
  Figure 1 caption: Summary of the chart mining pipeline. The process starts with
    a collection of documents from which charts and their captions are extracted.
    Additional segmentation is required for multi-panel images. Charts are then identified
    by their type through image classification methods. An approximation of the data
    used to create each chart is produced by chart-specific data extraction models.
    Finally, the extracted data can be used to support multiple target applications.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Chart_Mining_A_Survey_of_Methods_for_Automated_Chart_Analysis\figure_2.jpg
  Figure 2 caption: Summary of approaches used for figure extraction. Depending on
    whether the input document is raster or vector, different segmentation mechanisms
    are applied to extract figure and caption candidates. In the next stage, each
    method needs to link each figure to its corresponding caption.
  Figure 3 Link: articels_figures_by_rev_year\2020\Chart_Mining_A_Survey_of_Methods_for_Automated_Chart_Analysis\figure_3.jpg
  Figure 3 caption: Segmentation of multi-panel figures. Figure images and their captions
    are analyzed. Most methods use a classification step to determine if segmentation
    is needed. Image-based segmentation results are refined to match caption analysis
    results. The output is a set of individual sub-figures with sub-captions.
  Figure 4 Link: articels_figures_by_rev_year\2020\Chart_Mining_A_Survey_of_Methods_for_Automated_Chart_Analysis\figure_4.jpg
  Figure 4 caption: 'Types of multi-panel figures. We show multi-panel figures using:
    (a) gaps, extracted from [36]; (b) image stitching, extracted from [37]; and (c)
    a mixture which combines gap-based and stitched panels, extracted from [37].'
  Figure 5 Link: articels_figures_by_rev_year\2020\Chart_Mining_A_Survey_of_Methods_for_Automated_Chart_Analysis\figure_5.jpg
  Figure 5 caption: Popular chart types examples. Approximated counts of chart mining
    papers published from 2004 to 2018 are presented using both a (a) line chart and
    an (b) area chart. This count is further divided in three time periods, and we
    show the number of papers related to popular chart types using a (c) bar chart,
    and a (d) doughnut chart. The overall proportion of the coverage given to these
    most popular chart types is depicted using a (e) pie chart, a (f) tree-map and
    (g) radar chart. Finally, we use synthetic data to show examples of (i) scatter
    chart, (j) bubble chart, and (k) box plot.
  Figure 6 Link: articels_figures_by_rev_year\2020\Chart_Mining_A_Survey_of_Methods_for_Automated_Chart_Analysis\figure_6.jpg
  Figure 6 caption: Overview of the chart data extraction process. In general, chart
    images are processed through multiple distinguishable steps which are executed
    in different order by existing methodologies. Most methods conclude the process
    with a data extraction step which is chart-type dependent. Afterwards, many works
    analyze the tabular data along with the contextual information to extract the
    chart messages.
  Figure 7 Link: articels_figures_by_rev_year\2020\Chart_Mining_A_Survey_of_Methods_for_Automated_Chart_Analysis\figure_7.jpg
  Figure 7 caption: 'Chart data extraction example. After the initial pre-processing,
    the data extraction process continues with: (a) text processing, (b) axis understanding,
    and (c) legend understanding. The order of these steps varies from method to method.
    Then, (d) data extraction takes place producing (e) tabular data. Methods for
    high-level chart understanding produce additional outputs such as the (f) chart
    messages.'
  Figure 8 Link: articels_figures_by_rev_year\2020\Chart_Mining_A_Survey_of_Methods_for_Automated_Chart_Analysis\figure_8.jpg
  Figure 8 caption: Examples of charts complexities. (a) Two scales for a single axis,
    from [104]. (b) Hierarchical tick values, from [105]. A (c) factorized legend,
    where data series are combinations of legend entries, from [106]. (d) Two dependent
    variables, from [107]. (e) Non-contiguous axis scale, from [108]. A chart with
    (f) cluttered data marks, where individual points are hard to identify, and the
    data region contains addition of a sub-chart, from [109].
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Kenny Davila
  Name of the last author: Venu Govindaraju
  Number of Figures: 8
  Number of Tables: 5
  Number of authors: 5
  Paper title: 'Chart Mining: A Survey of Methods for Automated Chart Analysis'
  Publication Date: 2020-05-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of Classes Handled by Multiple ChartFigure Classification
      Methods
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Summary of Datasets Used for Training and Evaluation Methods
      for Extraction of Figures From Documents
  Table 3 caption:
    table_text: TABLE 3 Summary of Datasets Used for Image Classification
  Table 4 caption:
    table_text: TABLE 4 Summary of Datasets Used for Chart Data Extraction and Other
      Related Applications
  Table 5 caption:
    table_text: TABLE 5 Summary of Datasets Used for Evaluation of Text Processing
      in Figures
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2992028
- Affiliation of the first author: university of florida, gainesville, fl, usa
  Affiliation of the last author: university of florida, gainesville, fl, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Intrinsic_Grassmann_Averages_for_Online_Linear_Robust_and_Nonlinear_Subspace_Lea\figure_1.jpg
  Figure 1 caption: The average of two subspaces.
  Figure 10 Link: articels_figures_by_rev_year\2020\Intrinsic_Grassmann_Averages_for_Online_Linear_Robust_and_Nonlinear_Subspace_Lea\figure_10.jpg
  Figure 10 caption: Top and bottom row contain outliers (identified in a rectangular
    box) and non-outliers frames of UCSD and YaleExtendedB data respectively.
  Figure 2 Link: articels_figures_by_rev_year\2020\Intrinsic_Grassmann_Averages_for_Online_Linear_Robust_and_Nonlinear_Subspace_Lea\figure_2.jpg
  Figure 2 caption: 'Expressed variance as a function of number of observations. Left:
    The mean and one standard deviation of the RIGA estimator computed over 150 trials.
    In each trial data are generated in R 50 and we estimate a K=2 dimensional subspace.
    Right: The performance of different estimators for varying step-sizes. Here data
    are generated in R 250 and we estimate a K=20 dimensional subspace.'
  Figure 3 Link: articels_figures_by_rev_year\2020\Intrinsic_Grassmann_Averages_for_Online_Linear_Robust_and_Nonlinear_Subspace_Lea\figure_3.jpg
  Figure 3 caption: "Expressed variance as a function of number of observations. The\
    \ performance of different estimators. Left: Data are generated in R 250 and we\
    \ set K=20 . Right: Data are generated in R 100 and we set K=10 . We observe that\
    \ our estimator is better than its competitors for other values of D>100 and K\u2265\
    10 ."
  Figure 4 Link: articels_figures_by_rev_year\2020\Intrinsic_Grassmann_Averages_for_Online_Linear_Robust_and_Nonlinear_Subspace_Lea\figure_4.jpg
  Figure 4 caption: "Performance of RIGA with varying D and K . We can see that for\
    \ moderately large D and K\u226510 , the performance of RIGA is very good."
  Figure 5 Link: articels_figures_by_rev_year\2020\Intrinsic_Grassmann_Averages_for_Online_Linear_Robust_and_Nonlinear_Subspace_Lea\figure_5.jpg
  Figure 5 caption: 'Left and Middle: Performance of GROUSE with varying D and K and
    learning rates of 0.0001 and 0.01 respectively. D ranges between 50 and 500 in
    increments of 50 and K ranges between 2 and 22 in increments of 5. The plots are
    color coded from hot to cold, with warmest being D=50,K=2 and coolest being D=500,K=22
    . We can see that though for relatively larger learning rate the performance of
    GROUSE is better, it is not stable. Right: Stability analysis comparison of GROUSE
    and RIGA (for a fixed N , we randomly generate a data matrix, X , from a Gaussian
    distribution on R 250 . We estimate K=20 dimensional subspace and report the mean
    and standard deviation over 200 runs on X with learning rate 0.0001. This plot
    is a close-up look at the leftmost plot with D=250 and K=20 .)'
  Figure 6 Link: articels_figures_by_rev_year\2020\Intrinsic_Grassmann_Averages_for_Online_Linear_Robust_and_Nonlinear_Subspace_Lea\figure_6.jpg
  Figure 6 caption: Results from KRIGA and SKPCA on synthetic data.
  Figure 7 Link: articels_figures_by_rev_year\2020\Intrinsic_Grassmann_Averages_for_Online_Linear_Robust_and_Nonlinear_Subspace_Lea\figure_7.jpg
  Figure 7 caption: Results from KRIGA and SKPCA on 3D synthetic data.
  Figure 8 Link: articels_figures_by_rev_year\2020\Intrinsic_Grassmann_Averages_for_Online_Linear_Robust_and_Nonlinear_Subspace_Lea\figure_8.jpg
  Figure 8 caption: Comparison of KPCA and KRIGA in terms of ARE and running time.
  Figure 9 Link: articels_figures_by_rev_year\2020\Intrinsic_Grassmann_Averages_for_Online_Linear_Robust_and_Nonlinear_Subspace_Lea\figure_9.jpg
  Figure 9 caption: Comparative analysis for different noise levels. For all the competing
    methods, we use the suggested parameters in [56]
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.92
  Name of the first author: Rudrasis Chakraborty
  Name of the last author: Baba C. Vemuri
  Number of Figures: 13
  Number of Tables: 0
  Number of authors: 4
  Paper title: Intrinsic Grassmann Averages for Online Linear, Robust and Nonlinear
    Subspace Learning
  Publication Date: 2020-05-04 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2992392
- Affiliation of the first author: department of computer science, the graduate center,
    the city university of new york, new york, ny, usa
  Affiliation of the last author: department of electrical engineering, the city college,
    and the department of computer science, the graduate center, city university of
    new york, new york, ny, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\SelfSupervised_Visual_Feature_Learning_With_Deep_Neural_Networks_A_Survey\figure_1.jpg
  Figure 1 caption: The general pipeline of self-supervised learning. The visual feature
    is learned through the process of training ConvNets to solve a pre-defined pretext
    task. After self-supervised pretext task training finished, the learned parameters
    serve as a pre-trained model and are transferred to other downstream computer
    vision tasks by fine-tuning. The performance on these downstream tasks is used
    to evaluate the quality of the learned features. During the knowledge transfer
    for downstream tasks, the general features from only the first several layers
    are unusually transferred to downstream tasks.
  Figure 10 Link: articels_figures_by_rev_year\2020\SelfSupervised_Visual_Feature_Learning_With_Deep_Neural_Networks_A_Survey\figure_10.jpg
  Figure 10 caption: The architecture of the generator in VideoGan for video generation
    with GAN proposed in [37]. The figure is from [37] with authors permission.
  Figure 2 Link: articels_figures_by_rev_year\2020\SelfSupervised_Visual_Feature_Learning_With_Deep_Neural_Networks_A_Survey\figure_2.jpg
  Figure 2 caption: Self-supervised visual feature learning schema. The ConvNet is
    trained by minimizing errors between pseudo labels P and predictions O of the
    ConvNet. Since the pseudo labels are generated based on the structure of the data,
    no human annotations are involved during the whole process.
  Figure 3 Link: articels_figures_by_rev_year\2020\SelfSupervised_Visual_Feature_Learning_With_Deep_Neural_Networks_A_Survey\figure_3.jpg
  Figure 3 caption: 'Categories of pretext tasks for self-supervised visual feature
    learning: generation-based, context-based, free semantic label-based, and cross
    modal-based.'
  Figure 4 Link: articels_figures_by_rev_year\2020\SelfSupervised_Visual_Feature_Learning_With_Deep_Neural_Networks_A_Survey\figure_4.jpg
  Figure 4 caption: The pipeline of Generative Adversarial Networks [35]. By playing
    the two-player game, the discriminator forces the generator to generate realistic
    images, while the generator forces the discriminator to improve its differentiation
    ability.
  Figure 5 Link: articels_figures_by_rev_year\2020\SelfSupervised_Visual_Feature_Learning_With_Deep_Neural_Networks_A_Survey\figure_5.jpg
  Figure 5 caption: Qualitative illustration of image inpainting task. Given an image
    with a missing region (a), a human artist has no trouble inpainting it (b). Automatic
    inpainting using context encoder proposed in [21] trained with L2 reconstruction
    loss and adversarial loss is shown in (c). This figure is reproduced based on
    [21].
  Figure 6 Link: articels_figures_by_rev_year\2020\SelfSupervised_Visual_Feature_Learning_With_Deep_Neural_Networks_A_Survey\figure_6.jpg
  Figure 6 caption: The architecture of image colorization proposed in [20]. The figure
    is from [20] with authors permission.
  Figure 7 Link: articels_figures_by_rev_year\2020\SelfSupervised_Visual_Feature_Learning_With_Deep_Neural_Networks_A_Survey\figure_7.jpg
  Figure 7 caption: The visualization of the Jigsaw Image Puzzle [22]. (a) is an image
    with 9 sampled image patches, (b) is an example of shuffled image patches, and
    (c) shows the correct order of the sampled 9 patches. Figure is reproduced based
    on [22].
  Figure 8 Link: articels_figures_by_rev_year\2020\SelfSupervised_Visual_Feature_Learning_With_Deep_Neural_Networks_A_Survey\figure_8.jpg
  Figure 8 caption: An example of an indoor scene generated by a game engine [84].
    For each synthetic image, the corresponding depth, instance segmentation, and
    optical flow can be automatically generated by the engine.
  Figure 9 Link: articels_figures_by_rev_year\2020\SelfSupervised_Visual_Feature_Learning_With_Deep_Neural_Networks_A_Survey\figure_9.jpg
  Figure 9 caption: The architecture for utilizing synthetic and real-world images
    for self-supervised feature learning [53]. Figure is reproduced based on [53].
  First author gender probability: 0.82
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.74
  Name of the first author: Longlong Jing
  Name of the last author: Yingli Tian
  Number of Figures: 14
  Number of Tables: 9
  Number of authors: 2
  Paper title: 'Self-Supervised Visual Feature Learning With Deep Neural Networks:
    A Survey'
  Publication Date: 2020-05-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of Commonly Used Datasets of Images, Videos, Audios,
      and 3D Object Data
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Summary of Self-Supervised Image Feature Learning Methods
      Based on the Category of Pretext Tasks
  Table 3 caption:
    table_text: TABLE 3 Summary of Self-Supervised Video Feature Learning Methods
      Based on the Category of Pretext Tasks
  Table 4 caption:
    table_text: TABLE 4 Linear Classification on ImageNet and Places Datasets Using
      Activations From the Convolutional Layers of an AlexNet as Features
  Table 5 caption:
    table_text: TABLE 5 Comparison of the Self-Supervised Image Feature Learning Methods
      on Classification, Detection, and Segmentation on Pascal VOC Dataset
  Table 6 caption:
    table_text: TABLE 6 Comparison of Existing Self-Supervised Methods for Action
      Recognition on UCF101 and HMDB51 Datasets
  Table 7 caption:
    table_text: TABLE 7 Comparison of Existing Self-Supervised Methods for Audio Classification
      Task on DCASE and ESC50 Datasets
  Table 8 caption:
    table_text: TABLE 8 Summary of Self-Supervised 3D Feature Learning Methods Based
      on the Category of Pretext Tasks
  Table 9 caption:
    table_text: TABLE 9 The Comparison With the State-of-the-Art Methods for 3D Shape
      Recognition on ModelNet40 Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2992393
- Affiliation of the first author: czech institute of informatics, robotics and cybernetics
    (ciirc), czech technical university in prague, 6, prague, czechia
  Affiliation of the last author: faculty of electrical engineering, visual recognition
    group (vrg), czech technical university in prague, 6, prague, czechia
  Figure 1 Link: articels_figures_by_rev_year\2020\Minimal_Solvers_for_Rectifying_From_RadiallyDistorted_Conjugate_Translations\figure_1.jpg
  Figure 1 caption: Input (top left) is a distorted view of a scene plane with translational
    symmetries and reflections, and the outputs (top right, bottom) are the radially
    undistorted image and the rectified scene plane. The method is fully automatic.
  Figure 10 Link: articels_figures_by_rev_year\2020\Minimal_Solvers_for_Rectifying_From_RadiallyDistorted_Conjugate_Translations\figure_10.jpg
  Figure 10 caption: "Comparison of two error measures after 25 iterations of a simple\
    \ ransac for different solvers with increasing levels of white noise added to\
    \ the affine covariant region correspondences, where the normalized division model\
    \ parameter is set to -4 (see Section 3.1), which is similar to the distortion\
    \ of a GoPro Hero 4. Results are for imaged translated coplanar repeats: (a) Reports\
    \ the root mean square transfer error \u0394 xfer RMS . With the exception of\
    \ the H DES 222 l\u03BB solver, the proposed solvers are significantly more robust\
    \ for both types of repeats on both error measures; however H DES 222 l\u03BB\
    \ requires the most correspondences, and (b) reports the relative error of the\
    \ estimated division model parameter. The H 2 l\u03BB solver uses best minimal\
    \ solution selection, which improves its performance compared to H RND 2 l\u03BB\
    \ , which randomly selects a solution."
  Figure 2 Link: articels_figures_by_rev_year\2020\Minimal_Solvers_for_Rectifying_From_RadiallyDistorted_Conjugate_Translations\figure_2.jpg
  Figure 2 caption: 'Direct affine rectification. The rectification hierarchy is traversed
    from left to right. The proposed solvers directly rectify from distorted local
    features. The state-of-the art requires sampled undistortions, scene lines [8],
    [9], [13], or more region correspondences [14]. Color denotes how coplanar repeats
    relate: blue for translation and red for reflection. Marker type denotes the possible
    correspondence configurations: circles for three translated point correspondences
    and filled circles for two pairs of two translated point correspondences on a
    scene plane. Reflections (in red) can be detected as two point correspondences
    translating over different distances in the same direction. The distorted and
    undistorted images of the scene planes vanishing line are denoted l ~ and l ,
    and the distorted and undistorted reflection axis is similarly denoted a ~ and
    a , where a is its rectification. Point correspondences (circles) are extracted
    from region correspondences (solid polylines), which reduces the required input
    to one or two region correspondences.'
  Figure 3 Link: articels_figures_by_rev_year\2020\Minimal_Solvers_for_Rectifying_From_RadiallyDistorted_Conjugate_Translations\figure_3.jpg
  Figure 3 caption: 'The proposed solver variants can use: (a) three points translated
    over the same distance in one-direction, (b) three points translated in one direction
    with one point (blue) at an arbitrary distance, (c) two point pairs translated
    over two distances in two directions, and (d) two point pairs translated in two
    directions with one pair translating the same distance and one pair at different
    distances.'
  Figure 4 Link: articels_figures_by_rev_year\2020\Minimal_Solvers_for_Rectifying_From_RadiallyDistorted_Conjugate_Translations\figure_4.jpg
  Figure 4 caption: 'The proposed solvers give accurate undistortions and rectifications
    across all fields-of-view. The distorted image of the vanishing line is rendered
    in green. Left-to-right with increasing levels of distortion: (a) GoPro Hero 4
    at the medium-FOV setting, (b) GoPro Hero 4 at the wide-FOV setting, (c) and a
    Samyang 7.5mm fisheye lens. The outputs are the undistorted (middle row) and rectified
    images (bottom row). Note the stability of the undistortion estimates for the
    GoPro images. The rotunda image is rectified from features extracted mostly from
    the wrought iron fence below the rotunda. Focal lengths are 35mm equivalents.'
  Figure 5 Link: articels_figures_by_rev_year\2020\Minimal_Solvers_for_Rectifying_From_RadiallyDistorted_Conjugate_Translations\figure_5.jpg
  Figure 5 caption: "The geometry of a radially-distorted conjugate translations.\
    \ A translation of coplanar scene points X i , X j , X k by U induces a conjugate\
    \ translation H u in the undistorted image as viewed by camera P . Joined conjugately-translated\
    \ point correspondences x i \u2194 x \u2032 i , x j \u2194 x \u2032 j and x k\
    \ \u2194 x \u2032 k must meet at the vanishing point u . Vanishing line l is the\
    \ set of all vanishing points of translation directions. The division model images\
    \ lines as circles, thus the distorted vanishing point u ~ is given by the intersection\
    \ of three circles, two of which are coincident with the radially-distorted conjugately-translated\
    \ point correspondences x ~ i \u2194 x ~ \u2032 i , x ~ j \u2194 x ~ \u2032 j\
    \ and x ~ k \u2194 x ~ \u2032 k , and the third is given by the distorted vanishing\
    \ line l ~ . Radially-distorted conjugately-translated points are related by f\
    \ d ( H u f( x ~ ,\u03BB),\u03BB) , where f d (\u22C5,\u03BB) is the division-model\
    \ distortion function."
  Figure 6 Link: articels_figures_by_rev_year\2020\Minimal_Solvers_for_Rectifying_From_RadiallyDistorted_Conjugate_Translations\figure_6.jpg
  Figure 6 caption: "The geometry of the EVL constraints. The scene plane \u03A0 contains\
    \ the preimage of radially-distorted conjugately-translated affine-covariant regions,\
    \ equivalently, 3 translated points in the direction U . This configuration had\
    \ 3 additional translation directions V 1 , V 2 , V 3 that can be used to design\
    \ a solver. In the image plane \u03C0 , the joins of each of the images of the\
    \ 3 pairs of parallel lines (colored red, green and blue) meet at the imaged scene\
    \ planes vanishing line l . Each incidence of a vanishing point u , v 1 , v 2\
    \ and v 3 with l generates a scalar constraint equation. Two equations are needed\
    \ to estimate l and three are necessary to jointly estimate l and \u03BB . Note\
    \ that u can be estimated from one of 3 meets of distinct joins of undistorted\
    \ point correspondences, but only 1 such meet can be used as a constraint in the\
    \ EVL formulation."
  Figure 7 Link: articels_figures_by_rev_year\2020\Minimal_Solvers_for_Rectifying_From_RadiallyDistorted_Conjugate_Translations\figure_7.jpg
  Figure 7 caption: "EVL solver results on fisheye images. The distorted image of\
    \ the vanishing line is rendered in green in the input images on the top row.\
    \ Results were produced using the H 2 l\u03BB with 1-correspondence sampling in\
    \ a ransac framework. The H 2 l\u03BB solver runs in 0.5 \u03BC s. Surprisingly,\
    \ reasonable rectifications are possible using the 1-parameter division model\
    \ for the extreme distortions of fisheye lenses. Focal lengths are reported as\
    \ 35mm equivalent."
  Figure 8 Link: articels_figures_by_rev_year\2020\Minimal_Solvers_for_Rectifying_From_RadiallyDistorted_Conjugate_Translations\figure_8.jpg
  Figure 8 caption: Repeat detection, description, and representation. (a) Center
    of gravity (white cross) and curvature extrema (orange circles) of a detected
    MSER (orange contour [40]). Patches are normalized to a square and oriented to
    define an affine frame as in [41], (b) Bases are reflected for detecting axial
    symmetries. The RootSIFT transform embeds the local texture [47], [48]. (c) Affine
    frames are mapped to the image.
  Figure 9 Link: articels_figures_by_rev_year\2020\Minimal_Solvers_for_Rectifying_From_RadiallyDistorted_Conjugate_Translations\figure_9.jpg
  Figure 9 caption: "(a) The log 10 RMS warp error \u0394 warp RMS is reported for\
    \ noiseless scenes generated as described in Sections 8.1.1 and 8.1.3. Hidden\
    \ variable trick solvers are solid; solvers generated without simplified constraints\
    \ equations are dashed. The hidden-variable trick increases stability. The EVL\
    \ H 2 l\u03BB solver is the most stable since it does not require solving a complicated\
    \ polynomial system of equations. (b) Reports the RMS error \u0394 warp RMS after\
    \ 25 iterations of a simple ransac for the bench of solvers with increasing levels\
    \ of white noise added to the affine-covariant region correspondences, where the\
    \ normalized division model parameter is set to -4, which is similar to the distortion\
    \ of a GoPro Hero 4. Results are for radial-distorted conjugate translations.\
    \ The proposed solvers demonstrate excellent robustness to noise, and the EVL\
    \ solver H 2 l\u03BB is competitive with H DES 222 l\u03BB , which requires two\
    \ more correspondences. The H 2 l\u03BB solver uses best minimal solution selection,\
    \ which improves its performance compared to H RND 2 l\u03BB , which randomly\
    \ selects a solution."
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: James Pritts
  Name of the last author: "Ond\u0159ej Chum"
  Number of Figures: 13
  Number of Tables: 4
  Number of authors: 5
  Paper title: Minimal Solvers for Rectifying From Radially-Distorted Conjugate Translations
  Publication Date: 2020-05-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Scene Assumptions
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Common Denotations
  Table 3 caption:
    table_text: "TABLE 3 Proposed Solvers (rows 3\u20137) versus State of the Art"
  Table 4 caption:
    table_text: TABLE 4 Runtime Analysis
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2992261
- Affiliation of the first author: john hopcroft center and moe key lab of artificial
    intelligence ai institute, shanghai jiao tong university, shanghai, china
  Affiliation of the last author: university of california, los angeles, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Extraction_of_an_Explanatory_Graph_to_Interpret_a_CNN\figure_1.jpg
  Figure 1 caption: An explanatory graph represents the compositional hierarchy of
    object parts encoded in conv-layers of a CNN. Each filter in a pre-trained CNN
    may be activated by different object parts. Our method disentangles object parts
    from each filter in an unsupervised manner.
  Figure 10 Link: articels_figures_by_rev_year\2020\Extraction_of_an_Explanatory_Graph_to_Interpret_a_CNN\figure_10.jpg
  Figure 10 caption: Schematic illustration of an And-Or graph for semantic object
    parts. The AOG encodes a four-layer hierarchy for each semantic part, i.e., the
    semantic part (OR node), part templates (AND node), latent parts (OR nodes, those
    from the explanatory graph), and neural activation units (terminal nodes). In
    the AOG, the OR node of semantic part contains a number of alternative appearance
    candidates as children. Each OR node of a latent part encodes a list of neural
    activation units as alternative deformation candidates. Each AND node (e.g., a
    part template) uses a number of latent parts to describe its compositional regions.
  Figure 2 Link: articels_figures_by_rev_year\2020\Extraction_of_an_Explanatory_Graph_to_Interpret_a_CNN\figure_2.jpg
  Figure 2 caption: Schematic illustration of the explanatory graph. The explanatory
    graph encodes spatial and co-activation relationships between object parts in
    the explanatory graph. Nodes in high layers help localize nodes in low layers.
    From another perspective, we can regard low-layer nodes represent compositional
    parts of high-layer nodes.
  Figure 3 Link: articels_figures_by_rev_year\2020\Extraction_of_an_Explanatory_Graph_to_Interpret_a_CNN\figure_3.jpg
  Figure 3 caption: "Schematic illustration of related nodes V and V \u2032 . The\
    \ related nodes keep similar spatial relationships among different images. Circle\
    \ centers represent the prior part positions, e.g., \u03BC V and \u03BC V \u2032\
    \ . Red arrows denote relative displacements between the inferred positions and\
    \ prior positions, e.g., p V \u2212 \u03BC V . In particular, the middle sub-figure\
    \ illustrates different variables in a one-dimensional space for simplicity."
  Figure 4 Link: articels_figures_by_rev_year\2020\Extraction_of_an_Explanatory_Graph_to_Interpret_a_CNN\figure_4.jpg
  Figure 4 caption: A four-layer explanatory graph. For clarity, we put all nodes
    of different filters in the same conv-layer on the same plane and only show 1
    percent of the nodes with 10 percent of their edges from two perspectives.
  Figure 5 Link: articels_figures_by_rev_year\2020\Extraction_of_an_Explanatory_Graph_to_Interpret_a_CNN\figure_5.jpg
  Figure 5 caption: "Image patches corresponding to different nodes in explanatory\
    \ graphs. We visualized nodes in explanatory graphs that were learned for two\
    \ types of CNNs, i.e., CNNs learned for a single category and CNNs for multiple\
    \ categories. (1) The top nine layers visualized nodes corresponding to CNNs,\
    \ each learned for a single category. We used two methods to infer the image patch\
    \ for each node. In top nine rows, part location was inferred as x = argmax x\u2208\
    X: d x =d S I V\u2192x . In following three rows, parts were localized via x =\
    \ argmax x\u2208X: d x =d | f x \u22C5 \u2202y \u2202 f x | . (2) The bottom four\
    \ layers visualized image patches of graph nodes, when the CNN was learned to\
    \ classify multiple categories. In this case, each node usually encoded parts\
    \ shared by different categories. Texts before each group of image patches indicate\
    \ their corresponding categories. Part location was inferred as x = argmax x\u2208\
    X: d x =d S I V\u2192x . Please read texts for detailed explanations."
  Figure 6 Link: articels_figures_by_rev_year\2020\Extraction_of_an_Explanatory_Graph_to_Interpret_a_CNN\figure_6.jpg
  Figure 6 caption: Heatmaps of the distribution of object parts. We use a heatmap
    to visualize the spatial distribution of the top-50 percent object parts in the
    L th layer of the explanatory graph with the highest inference scores. We also
    compare heatmaps with the grad-CAM [27] of the feature map. Unlike the grad-CAM,
    our heatmaps mainly focus on the foreground of an object and uniformly pay attention
    to all parts, rather than only focus on most discriminative parts.
  Figure 7 Link: articels_figures_by_rev_year\2020\Extraction_of_an_Explanatory_Graph_to_Interpret_a_CNN\figure_7.jpg
  Figure 7 caption: Image synthesis based on the activation of nodes on an image.
    The explanatory graph only encodes major parts encoded in conv-layers with considerable
    information loss. Synthesis results demonstrate that the nodes are automatically
    learned to represent foreground appearance, and ignore background noises and trivial
    details of objects.
  Figure 8 Link: articels_figures_by_rev_year\2020\Extraction_of_an_Explanatory_Graph_to_Interpret_a_CNN\figure_8.jpg
  Figure 8 caption: Purity of part semantics (top-left). We compared object parts
    corresponding to nodes in the explanatory graph with features of raw filters.
    We draw raw feature maps of filters (left), the highest activation peaks on feature
    maps of filters (middle), and image regions corresponding to each node in the
    explanatory graph (right). Based on such visualization results, we use human users
    to annotate the semantic purity of each nodefilter.
  Figure 9 Link: articels_figures_by_rev_year\2020\Extraction_of_an_Explanatory_Graph_to_Interpret_a_CNN\figure_9.jpg
  Figure 9 caption: Notation for the computation of location instability.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.61
  Name of the first author: Quanshi Zhang
  Name of the last author: Song-Chun Zhu
  Number of Figures: 13
  Number of Tables: 9
  Number of authors: 6
  Paper title: Extraction of an Explanatory Graph to Interpret a CNN
  Publication Date: 2020-05-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Notation
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Location Instability of Nodes
  Table 3 caption:
    table_text: TABLE 3 Normalized Distance of Part Localization on the CUB200-2011
      Dataset [38]
  Table 4 caption:
    table_text: TABLE 4 Normalized Distance of Part Localization on the VOC Part Dataset
      [5]
  Table 5 caption:
    table_text: "TABLE 5 Accuracy of Part Localization Evaluated by \u201C IoU\u2265\
      0.5 IoU\u22650.5\u201D on the Pascal VOC Part dataset [5]"
  Table 6 caption:
    table_text: TABLE 6 Normalized Distance of Part Localization on the ILSVRC 2013
      DET Animal-Part Dataset [44]
  Table 7 caption:
    table_text: "TABLE 7 Accuracy of Part Localization Evaluated by \u201C IoU\u2265\
      0.5 IoU\u22650.5\u201D on the ILSVRC 2013 DET Animal-Part Dataset [44]"
  Table 8 caption:
    table_text: TABLE 8 Normalized Distance of Part Localization
  Table 9 caption:
    table_text: TABLE 9 Effects of the Edge Number M M
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2992207
- Affiliation of the first author: multipleye company ltd., seoul, south korea
  Affiliation of the last author: multipleye company ltd., seoul, south korea
  Figure 1 Link: articels_figures_by_rev_year\2020\EndtoEnd_Learning_for_Omnidirectional_Stereo_Matching_With_Uncertainty_Prior\figure_1.jpg
  Figure 1 caption: Illustration of the lens model and example of the camera rig.
    (a) We use flipped catadioptric coordinate system [31] for our ultra-wide FOV
    lens. (b) Example of a wide-baseline multi-camera rig system.
  Figure 10 Link: articels_figures_by_rev_year\2020\EndtoEnd_Learning_for_Omnidirectional_Stereo_Matching_With_Uncertainty_Prior\figure_10.jpg
  Figure 10 caption: 'Results on the synthetic data. Left: reference panorama image,
    rectified left color images, and grayscale fisheye images. Middle: predicted inverse
    depth maps by SweepNet, DispNet-CSS, and OmniMVS + 32 . Right: color-coded error
    maps of inverse depth index estimation (blue is low and red is high). Note that
    SweepNet and DispNet-CSS miss thin structures or occluded regions, or leave large
    untextured regions noisy.'
  Figure 2 Link: articels_figures_by_rev_year\2020\EndtoEnd_Learning_for_Omnidirectional_Stereo_Matching_With_Uncertainty_Prior\figure_2.jpg
  Figure 2 caption: Global sweeping approach for omnidirectional stereo. (a) We find
    stereo correspondences by comparing features warped onto each sphere from the
    input images. (b) There can be several observations on a ray in such a global
    sweeping approach.
  Figure 3 Link: articels_figures_by_rev_year\2020\EndtoEnd_Learning_for_Omnidirectional_Stereo_Matching_With_Uncertainty_Prior\figure_3.jpg
  Figure 3 caption: Overview of the proposed method. Each input image is fed into
    the 2D CNN for extracting feature maps. We project the unary feature maps into
    spherical features to build the matching cost volume. The final depth is acquired
    through cost volume computation by the 3D encoder-decoder architecture and softargmax.
  Figure 4 Link: articels_figures_by_rev_year\2020\EndtoEnd_Learning_for_Omnidirectional_Stereo_Matching_With_Uncertainty_Prior\figure_4.jpg
  Figure 4 caption: "(a) Example of a confident match on the pixel A . (b) Example\
    \ of an ambiguous match on the pixel B . In the training, since the softargmax\
    \ (Eq. (4)) outputs correct indices for both regions n \u2217 \u2243 n , it stops\
    \ updating the network for the ambiguous case (b). The entropy loss (Eq. (9))\
    \ penalizes such cases and drives the network to output a less ambiguous answer,\
    \ e.g., from the context in the surrounding pixels."
  Figure 5 Link: articels_figures_by_rev_year\2020\EndtoEnd_Learning_for_Omnidirectional_Stereo_Matching_With_Uncertainty_Prior\figure_5.jpg
  Figure 5 caption: "Interleaved spherical sweeping. Each input image is warped onto\
    \ the global spheres from S 0 to S N\u22121 . In the interleaved sweeping, the\
    \ pairs of spherical images from the opposite cameras are interleaved together\
    \ as shown in the figure."
  Figure 6 Link: articels_figures_by_rev_year\2020\EndtoEnd_Learning_for_Omnidirectional_Stereo_Matching_With_Uncertainty_Prior\figure_6.jpg
  Figure 6 caption: 'Examples of our proposed datasets. From left: 4 input fisheye
    images with visibility (left-top), reference panorama image, and ground truth
    inverse depth map.'
  Figure 7 Link: articels_figures_by_rev_year\2020\EndtoEnd_Learning_for_Omnidirectional_Stereo_Matching_With_Uncertainty_Prior\figure_7.jpg
  Figure 7 caption: "Depth map filtering result. Completeness and RMS error according\
    \ to the entropy threshold k . The completeness means the valid pixel ratio (%)\
    \ whose uncertainty H\u2264logk , and the RMS errors are averaged for the valid\
    \ pixels."
  Figure 8 Link: articels_figures_by_rev_year\2020\EndtoEnd_Learning_for_Omnidirectional_Stereo_Matching_With_Uncertainty_Prior\figure_8.jpg
  Figure 8 caption: Effectiveness of guiding regularization in training OmniMVS 32
    . (a) Validation errors in each epoch. Vertical lines indicate the epoch in which
    each network has the minimum validation error. (b) Mean and standard deviation
    of uncertainties of the validation set. Note that the uncertainty value H is converted
    to the number of confident indices in each ray by k= e H . Adding L ent greatly
    improves the reliability of depth estimation.
  Figure 9 Link: articels_figures_by_rev_year\2020\EndtoEnd_Learning_for_Omnidirectional_Stereo_Matching_With_Uncertainty_Prior\figure_9.jpg
  Figure 9 caption: "For the comparison purpose, we generate rectified image pairs\
    \ for conventional stereo algorithms. The rectified images are in 512\xD7512 and\
    \ 120\u2218 FOV. The predicted disparity maps are then merged into a H\xD7W omnidirectional\
    \ inverse depth index."
  First author gender probability: 0.86
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Changhee Won
  Name of the last author: Jongwoo Lim
  Number of Figures: 15
  Number of Tables: 5
  Number of authors: 3
  Paper title: End-to-End Learning for Omnidirectional Stereo Matching With Uncertainty
    Prior
  Publication Date: 2020-05-05 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Input Images Pass Separately From conv1 conv1 to transference
      transference, Then the Spherical Feature Maps are Merged by concat concat and
      fusion fusion
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison With the Published Datasets
  Table 3 caption:
    table_text: TABLE 3 Quantitative Evaluation of Guiding Regularization
  Table 4 caption:
    table_text: TABLE 4 Quantitative Comparison With Other Methods
  Table 5 caption:
    table_text: TABLE 5 Datasets Used in Training Each Method
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2992497
- Affiliation of the first author: department of mathematics and computer science,
    university of catania, catania, ct, italy
  Affiliation of the last author: department of mathematics and computer science,
    university of catania, catania, ct, italy
  Figure 1 Link: articels_figures_by_rev_year\2020\RollingUnrolling_LSTMs_for_Action_Anticipation_from_FirstPerson_Video\figure_1.jpg
  Figure 1 caption: Egocentric action anticipation. See text for notation.
  Figure 10 Link: articels_figures_by_rev_year\2020\RollingUnrolling_LSTMs_for_Action_Anticipation_from_FirstPerson_Video\figure_10.jpg
  Figure 10 caption: 'Qualitative examples (best seen on screen). Legend for attention
    weights: blue - RGB, orange - Flow, green - objects.'
  Figure 2 Link: articels_figures_by_rev_year\2020\RollingUnrolling_LSTMs_for_Action_Anticipation_from_FirstPerson_Video\figure_2.jpg
  Figure 2 caption: Video processing scheme adopted by the proposed method. In the
    example above, we set S enc =6 and S ant =8 .
  Figure 3 Link: articels_figures_by_rev_year\2020\RollingUnrolling_LSTMs_for_Action_Anticipation_from_FirstPerson_Video\figure_3.jpg
  Figure 3 caption: Overall architecture of the proposed Rolling-Unrolling LSTM architecture
    based on encoder-decoder models.
  Figure 4 Link: articels_figures_by_rev_year\2020\RollingUnrolling_LSTMs_for_Action_Anticipation_from_FirstPerson_Video\figure_4.jpg
  Figure 4 caption: Example of modality-specific branch with S enc =1 and S ant =3
    .
  Figure 5 Link: articels_figures_by_rev_year\2020\RollingUnrolling_LSTMs_for_Action_Anticipation_from_FirstPerson_Video\figure_5.jpg
  Figure 5 caption: Example of connection scheme used during SCP for time-step t=2
    .
  Figure 6 Link: articels_figures_by_rev_year\2020\RollingUnrolling_LSTMs_for_Action_Anticipation_from_FirstPerson_Video\figure_6.jpg
  Figure 6 caption: Example of the complete architecture with two branches and the
    Modality ATTention mechanism (MATT).
  Figure 7 Link: articels_figures_by_rev_year\2020\RollingUnrolling_LSTMs_for_Action_Anticipation_from_FirstPerson_Video\figure_7.jpg
  Figure 7 caption: Correlations between modality attention weights.
  Figure 8 Link: articels_figures_by_rev_year\2020\RollingUnrolling_LSTMs_for_Action_Anticipation_from_FirstPerson_Video\figure_8.jpg
  Figure 8 caption: Top-10 and bottom-10 actions which benefited from modality attention
    in terms of Top-5 Recall.
  Figure 9 Link: articels_figures_by_rev_year\2020\RollingUnrolling_LSTMs_for_Action_Anticipation_from_FirstPerson_Video\figure_9.jpg
  Figure 9 caption: Impact of the choice of S enc on performance.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Antonino Furnari
  Name of the last author: Giovanni Maria Farinella
  Number of Figures: 10
  Number of Tables: 10
  Number of authors: 2
  Paper title: Rolling-Unrolling LSTMs for Action Anticipation from First-Person Video
  Publication Date: 2020-05-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Egocentric Action Anticipation Results on the EPIC-KITCHENS
      Dataset
  Table 10 caption:
    table_text: TABLE 10 Recognition Results on EGTEA Gaze+
  Table 2 caption:
    table_text: TABLE 2 Egocentric Action Anticipation Results on the EPIC-Kitchens
      Test Set
  Table 3 caption:
    table_text: TABLE 3 Egocentric Action Anticipation Results on EGTEA Gaze+
  Table 4 caption:
    table_text: TABLE 4 Anticipation Results on ActivityNet
  Table 5 caption:
    table_text: TABLE 5 Ablation Study on EPIC-KITCHENS
  Table 6 caption:
    table_text: TABLE 6 Early Recognition Results on EPIC-KITCHENS
  Table 7 caption:
    table_text: TABLE 7 Early Recognition Results on EGTEA Gaze+
  Table 8 caption:
    table_text: TABLE 8 Early Recognition Results on ActivityNet
  Table 9 caption:
    table_text: TABLE 9 Egocentric Action Recognition Results on the EPIC-Kitchens
      Test Set
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2992889
