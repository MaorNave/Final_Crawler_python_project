- Affiliation of the first author: institute for information and system sciences,
    school of mathematics and statistics, xian jiaotong university, xian, china
  Affiliation of the last author: institute for information and system sciences, school
    of mathematics and statistics, xian jiaotong university, xian, china
  Figure 1 Link: articels_figures_by_rev_year\2018\ADMMCSNet_A_Deep_Learning_Approach_for_Image_Compressive_Sensing\figure_1.jpg
  Figure 1 caption: "The data flow graph of ADMM-CSNets. There are three types of\
    \ graph nodes in n th stage, corresponding to reconstruction layer ( X (n) ),\
    \ auxiliary variable update block ( Z (n) ) and multiplier update layer ( M (n)\
    \ ). For Basic-ADMM-CSNet, the auxiliary variable update operation ( Z (n) ) is\
    \ decomposed to a convolution layer ( C (n) ) and a non-linear transform layer\
    \ ( S (n) ). For Generic-ADMM-CSNet, the auxiliary variable update block ( Z (n)\
    \ ) is decomposed to N t iterations consisting of convolution layers ( C (n,k)\
    \ 1 , C (n,k) 2 ), non-linear activation layers ( H (n,k) ) and addition layers\
    \ ( A (n,k) ), where n\u22081,2,\u2026, N s and k\u22081,2,\u2026, N t ."
  Figure 10 Link: articels_figures_by_rev_year\2018\ADMMCSNet_A_Deep_Learning_Approach_for_Image_Compressive_Sensing\figure_10.jpg
  Figure 10 caption: Reconstructed T2-weighted images and error maps using the Cartesian
    sampling mask with 30 percent sampling rate. (a)-(f) Reconstructed images based
    on Zero-filling, RecPF, PBDW, PANO, GBRWT and the ADMM-CSNet learned from T1-weighted
    and PD-weighted MR images.
  Figure 2 Link: articels_figures_by_rev_year\2018\ADMMCSNet_A_Deep_Learning_Approach_for_Image_Compressive_Sensing\figure_2.jpg
  Figure 2 caption: Illustration of three types of graph nodes (i.e., layers) and
    their data flows in n th stage. The rectangular box represents the interested
    layer, and the circles represent the layers that have data communication (i.e.,
    connections) with the interested layers. The solid arrow indicates the data flow
    in the forward pass and dashed arrow indicates the backward flow in the backward
    pass when computing gradients in back-propagation.
  Figure 3 Link: articels_figures_by_rev_year\2018\ADMMCSNet_A_Deep_Learning_Approach_for_Image_Compressive_Sensing\figure_3.jpg
  Figure 3 caption: Illustration of a piecewise linear function determined by a set
    of control points.
  Figure 4 Link: articels_figures_by_rev_year\2018\ADMMCSNet_A_Deep_Learning_Approach_for_Image_Compressive_Sensing\figure_4.jpg
  Figure 4 caption: Examples of three types of under-sampling mask.
  Figure 5 Link: articels_figures_by_rev_year\2018\ADMMCSNet_A_Deep_Learning_Approach_for_Image_Compressive_Sensing\figure_5.jpg
  Figure 5 caption: Average test PSNR values for increasing L-BFGS iterations. (a)
    Curves on a real-valued brain data using pseudo radial mask with 20 percent sampling
    rate ( N s =4 ). (b) Curves on a complex-valued brain data using Cartesian sampling
    mask with 30 percent sampling rate ( N s =10 ).
  Figure 6 Link: articels_figures_by_rev_year\2018\ADMMCSNet_A_Deep_Learning_Approach_for_Image_Compressive_Sensing\figure_6.jpg
  Figure 6 caption: Examples of reconstructed images on a brain data using 30 percent
    sampling rate and the Cartesian sampling mask. (a) The ground-truth image. (b)-(h)
    Reconstructed images based on Zero-filling, TV, RecPF, PBDW, PANO, FDLCP and ADMM-CSNet.
    Their NRMSEs are 0.158, 0.095, 0.093, 0.085, 0.086, 0.082 and 0.067, respectively.
    Please zoom in for better comparison.
  Figure 7 Link: articels_figures_by_rev_year\2018\ADMMCSNet_A_Deep_Learning_Approach_for_Image_Compressive_Sensing\figure_7.jpg
  Figure 7 caption: Examples of learned filters and the corresponding non-linear activation
    functions. The first two rows are initial filters and functions, the last two
    rows are learned filters and functions.
  Figure 8 Link: articels_figures_by_rev_year\2018\ADMMCSNet_A_Deep_Learning_Approach_for_Image_Compressive_Sensing\figure_8.jpg
  Figure 8 caption: Reconstruction accuracies in NRMSE and PSNR under different sampling
    rates using the 2D random sampling mask.
  Figure 9 Link: articels_figures_by_rev_year\2018\ADMMCSNet_A_Deep_Learning_Approach_for_Image_Compressive_Sensing\figure_9.jpg
  Figure 9 caption: (a) Scatter plot of average NRMSEs and testing time for different
    methods on 20 percent sampled complex-valued brain data using the pseudo radial
    sampling mask.(b) Average testing NRMSEs using different number of stages with
    20 percent sampling rate.
  First author gender probability: 0.76
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Yan Yang
  Name of the last author: Zongben Xu
  Number of Figures: 17
  Number of Tables: 10
  Number of authors: 4
  Paper title: 'ADMM-CSNet: A Deep Learning Approach for Image Compressive Sensing'
  Publication Date: 2018-11-28 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Comparison of Average Reconstruction Accuracy on a Brain\
      \ Dataset with Different Sampling Rates and Cartesian Sampling Mask Using NRMSE\
      \ ( \xD7 10 \u22122 \xD710-2), PSNR (dB) and SSIM ( \xD7 10 \u22122 \xD710-2),\
      \ and Average Testing Time for Reconstructing a Complex-Valued Image with Size\
      \ of 256\xD7256 256\xD7256"
  Table 10 caption:
    table_text: "TABLE 10 Comparison of Average Reconstruction Accuracy on 10 Standard\
      \ Test Images in Size of 256\xD7256 256\xD7256 Using Randomly Permuted Coded\
      \ Diffraction Measurements and Walsh-Hadamard Measurements"
  Table 2 caption:
    table_text: TABLE 2 Comparisons of Different ADMM-CSNets
  Table 3 caption:
    table_text: TABLE 3 Performance Comparison for Deep Learning Methods on Brain
      Data
  Table 4 caption:
    table_text: TABLE 4 Comparison of Different Methods on Chest Data
  Table 5 caption:
    table_text: TABLE 5 Comparison of Different Methods on T2-Weighted Images
  Table 6 caption:
    table_text: TABLE 6 Comparison of Different Methods under Different Noise Levels
  Table 7 caption:
    table_text: TABLE 7 Comparison of Different Initializations
  Table 8 caption:
    table_text: TABLE 8 Comparison of Different Network Architectures
  Table 9 caption:
    table_text: TABLE 9 Comparison of Different Filter Numbers and Filter Sizes
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2883941
- Affiliation of the first author: department of electrical and computer engineering,
    tufts university, medford, usa
  Affiliation of the last author: department of electrical and computer engineering,
    tufts university, medford, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\A_Comprehensive_Database_for_Benchmarking_Imaging_Systems\figure_1.jpg
  Figure 1 caption: 'Heterogeneous face recognition in real-life scenarios: images
    captured through different sources, such as (a) social media; (b) drivers licenses,
    passports, and other identification documents; (c) night-vision surveillance cameras;
    (d) Thermal cameras; and (e) 3D cameras.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\A_Comprehensive_Database_for_Benchmarking_Imaging_Systems\figure_2.jpg
  Figure 2 caption: Example images selected from the Tufts face database. The Tufts
    database has a wide range of nationalities, ages, and ethnic backgrounds.
  Figure 3 Link: articels_figures_by_rev_year\2018\A_Comprehensive_Database_for_Benchmarking_Imaging_Systems\figure_3.jpg
  Figure 3 caption: (a) and (b) are two sets of frontal images of an individual with
    the different constraints for both visible and thermal. The images in the first
    row of each set illustrate visual images of a participant with various facial
    expressions; and the images in the second row of each set provide corresponding
    expressions in thermal imaging.
  Figure 4 Link: articels_figures_by_rev_year\2018\A_Comprehensive_Database_for_Benchmarking_Imaging_Systems\figure_4.jpg
  Figure 4 caption: Selecting facial components using FACES 4.0.
  Figure 5 Link: articels_figures_by_rev_year\2018\A_Comprehensive_Database_for_Benchmarking_Imaging_Systems\figure_5.jpg
  Figure 5 caption: Database naming convention. Detailed explanation is presented
    in Section 4.1. We believe our image naming convention will provide a convenient
    way for indexing.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.6
  Name of the first author: Karen Panetta
  Name of the last author: Xin Yuan
  Number of Figures: 5
  Number of Tables: 5
  Number of authors: 11
  Paper title: A Comprehensive Database for Benchmarking Imaging Systems
  Publication Date: 2018-11-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Advantages and Limitations of Popular Imaging Sensors
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 A Comparison of Popular Face Datasets
  Table 3 caption:
    table_text: TABLE 3 Detailed Camera Settings for the Tufts Database
  Table 4 caption:
    table_text: TABLE 4 Various Attributes of the Files in the Database
  Table 5 caption:
    table_text: TABLE 5 Contents in the Tufts Database (Best Quality)
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2884458
- Affiliation of the first author: department of electrical engineering and computer
    science, york university, toronto, canada
  Affiliation of the last author: department of electrical engineering and computer
    science, york university, toronto, canada
  Figure 1 Link: articels_figures_by_rev_year\2018\Incremental_Learning_Through_Deep_Adaptation\figure_1.jpg
  Figure 1 caption: "Overview of proposed method. For newly learned domains, controller\
    \ modules are attached to convolutions of a base network, whose parameter are\
    \ frozen. A switching variable \u03B1 allows to switch the behavior of the network\
    \ between the original behaviour of the convolution and a re-parametrized one\
    \ for the new domain. \u03B1 can be determined either manually or via a sub-network\
    \ (\u201CDataset Decider\u201D) which determines the source domain of the image,\
    \ switching accordingly between different sets of control parameters. \u03B1 also\
    \ controls which of the classifiers to apply. Other layers (e.g., non-linearities,\
    \ batch-normalization, skip layers) not shown for presentation purposes. We visualize\
    \ one added task, though an arbitrary number of tasks can be added."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Incremental_Learning_Through_Deep_Adaptation\figure_2.jpg
  Figure 2 caption: "Controller modules: Filters of a convolutional layer of a base\
    \ network are modified by re-combining their weights through a controller module,\
    \ where a switching \u03B1 variable can choose between the original filters F\
    \ l and newly created ones F a l . We show a controller module for a single added\
    \ task, but any number of controllers can be added, with \u03B1 then being a vector\
    \ instead of scalar."
  Figure 3 Link: articels_figures_by_rev_year\2018\Incremental_Learning_Through_Deep_Adaptation\figure_3.jpg
  Figure 3 caption: Demonstration of different possible limits of proposed method
    on a toy example. Transferring from a network where the first layer contains features
    from an orthogonal sub-space to that required for a task can result in chance
    accuracy. Please see text for details.
  Figure 4 Link: articels_figures_by_rev_year\2018\Incremental_Learning_Through_Deep_Adaptation\figure_4.jpg
  Figure 4 caption: (a) Controller initialization schemes. Mean loss averaged over
    5 experiments for different ways of initializing controller modules, overlaid
    with minimal and maximal values. Random initialization performs the worst (random).
    Approximating the behavior of a fine-tuned network is slightly better (linearapprox)
    and initializing by mimicking the base network (diagonal) performs the best (b)
    Predictability of a control networks overall accuracy average over all datasets,
    given its transferability measure.
  Figure 5 Link: articels_figures_by_rev_year\2018\Incremental_Learning_Through_Deep_Adaptation\figure_5.jpg
  Figure 5 caption: Transferability of various datasets to each other (ft-last) fine
    tuning only the last layer (full) fine-tuning all layers (ft-full-bn-off) fine
    tuning all layers while disallowing batch-normalization layers weights to be updated.
    Overall, networks tend to be more easily transferable to problems from related
    domains (e.g., natural drawing). Zoom in to see numbers. It is recommended to
    view this figure in color on-line.
  Figure 6 Link: articels_figures_by_rev_year\2018\Incremental_Learning_Through_Deep_Adaptation\figure_6.jpg
  Figure 6 caption: '(a) Accuracy versus learning method. Using only the last layer
    (feature extractor) performs worst. Finetune: Vanilla fine-tuning. Diagonal :
    Our controller modules with a diagonal combination matrix. Linear: Our full method.
    On average, our full method outperforms vanilla fine tuning. (b) Accuracy versus
    quantization: With as low as 8 bits, we see no significant effect of network quantization
    on our method, showing they can be applied together.'
  Figure 7 Link: articels_figures_by_rev_year\2018\Incremental_Learning_Through_Deep_Adaptation\figure_7.jpg
  Figure 7 caption: Shifting representations. Using a single base network Nsketch
    , we check the methods sensitivity to varying values of alpha by varying it in
    the range [0,1] . Increasing alpha shifts the network away from the base representation
    and towards learned tasks - gradually lowering performance on the base task (diamonds)
    and improving on the learned ones (full circles). The relatively slow decrease
    of the performance on sketch (blue diamonds) and increase in that of Plankton
    (blue circles) indicates a similarity between the learned representations.
  Figure 8 Link: articels_figures_by_rev_year\2018\Incremental_Learning_Through_Deep_Adaptation\figure_8.jpg
  Figure 8 caption: (a) Our method (linear) initially converges to a high accuracy
    faster than fine-tuning. The weaker variant of our method converges as fast as
    feature-extraction but reaches an overall higher accuracy. The learning rate is
    reduced at epoch 25. (b) Zoom in on top-right of (a).
  Figure 9 Link: articels_figures_by_rev_year\2018\Incremental_Learning_Through_Deep_Adaptation\figure_9.jpg
  Figure 9 caption: Mean classification accuracy (normalized, averaged over datasets)
    w.r.t no. parameters. Our method achieve better performance over baselines for
    a large range of parameter budgets. For very few parameters diagonal (ours) outperforms
    features extraction. To obtain maximal accuracy our full method requires far fewer
    parameters (see linear versus finetune).
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Amir Rosenfeld
  Name of the last author: John K. Tsotsos
  Number of Figures: 9
  Number of Tables: 3
  Number of authors: 2
  Paper title: Incremental Learning Through Deep Adaptation
  Publication Date: 2018-11-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Mean Transfer Learning Performance
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Perf: Accuracy (%, Higher is Better) on Various Datasets
      and Parameter Cost (par., Lower Is Better) for a Few Baselines and Several Variants
      of Our Method'
  Table 3 caption:
    table_text: TABLE 3 Results on Visual Decathlon Challenge
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2884462
- Affiliation of the first author: rheinmain university of applied sciences, wiesbaden,
    germany
  Affiliation of the last author: "technical university of munich, m\xFCnchen, germany"
  Figure 1 Link: articels_figures_by_rev_year\2018\A_RegionBased_GaussNewton_Approach_to_RealTime_Monocular_Multiple_Object_Trackin\figure_1.jpg
  Figure 1 caption: "A few examples of our proposed method estimating the pose of\
    \ a single or multiple complex objects under different challenging conditions.\
    \ These include cluttered scenesbackgrounds, strong occlusions as well as direct\
    \ sunlight. Top: The raw RGB input frames. Bottom: A mixed reality visualization\
    \ of the tracking result where the input images are virtually augmented with renderings\
    \ of the corresponding 3D models using the estimated poses. All results were obtained\
    \ within \u223C 16 ms per object. This shows the robustness and accuracy of our\
    \ approach in a variety of situations that are typically difficult for monocular\
    \ pose estimation."
  Figure 10 Link: articels_figures_by_rev_year\2018\A_RegionBased_GaussNewton_Approach_to_RealTime_Monocular_Multiple_Object_Trackin\figure_10.jpg
  Figure 10 caption: An example frame from the RBOT dataset with the duck model in
    the four different variants. Visually comparing the regular with the dynamic light
    version clarifies the impact of the lighting aspect on the appearance of the object
    region.
  Figure 2 Link: articels_figures_by_rev_year\2018\A_RegionBased_GaussNewton_Approach_to_RealTime_Monocular_Multiple_Object_Trackin\figure_2.jpg
  Figure 2 caption: 'Example images extracted from different related pose tracking
    datasets. Top left: [24], top right: [4], bottom left: [40] (cropped to fit aspect
    ratio) and bottom right: [34].'
  Figure 3 Link: articels_figures_by_rev_year\2018\A_RegionBased_GaussNewton_Approach_to_RealTime_Monocular_Multiple_Object_Trackin\figure_3.jpg
  Figure 3 caption: "An overview of our region-based pose estimation setting for a\
    \ single object. Left: The object pose T relative to a camera based on a color\
    \ image I c and a 3D model of the object. Middle: The objects silhouette I s generated\
    \ by projecting the 3D surface model into the 2D image plane using an estimated\
    \ pose T . Right: A combined 2D3D plot of the level-set pose embedding \u03A6\
    (x) in form of an euclidean signed distance transform of the projected silhouette."
  Figure 4 Link: articels_figures_by_rev_year\2018\A_RegionBased_GaussNewton_Approach_to_RealTime_Monocular_Multiple_Object_Trackin\figure_4.jpg
  Figure 4 caption: "Object segmentation using tclc-histograms. Top left: Schematic\
    \ 3D visualization of a tclc-histogram attached to a mesh vertex X i of a 3D squirrel\
    \ model. Top right: A color image I c of a heterogeneous squirrel in a cluttered\
    \ scene overlayed with the local regions. These are depicted by colored circles\
    \ where the RGB value relates to the coordinates of the corresponding X i . Bottom\
    \ left: Per pixel segmentation (visualized as P \xAF f (x)\u2212 P \xAF b (x)>0\
    \ ) computed from the tclc-histograms (20). Bottom right: Segmentation result\
    \ from global color histograms (12) for comparison."
  Figure 5 Link: articels_figures_by_rev_year\2018\A_RegionBased_GaussNewton_Approach_to_RealTime_Monocular_Multiple_Object_Trackin\figure_5.jpg
  Figure 5 caption: 'A multi-object tracking scenario. Top left: Image I c of a driller
    behind a Buddha figurine. Top right: Estimated common silhouette mask I s , where
    the segments of C 1 , that appear due to the occlusion, are marked red. Bottom:
    Corresponding per pixel segmentation computed from the tclc-histograms of each
    object. This shows that even in case of these rather dark, mutually occluding
    objects, the segmentation strategy produces high quality results.'
  Figure 6 Link: articels_figures_by_rev_year\2018\A_RegionBased_GaussNewton_Approach_to_RealTime_Monocular_Multiple_Object_Trackin\figure_6.jpg
  Figure 6 caption: 'The two depth map types used within our approach, where brighter
    pixels are closer to the camera. Left: The usual depth map I d corresponding to
    the closest surface points. Right: The reverse depth map I r d corresponding to
    the most distant surface points.'
  Figure 7 Link: articels_figures_by_rev_year\2018\A_RegionBased_GaussNewton_Approach_to_RealTime_Monocular_Multiple_Object_Trackin\figure_7.jpg
  Figure 7 caption: The two level-sets corresponding to mathbf C1 and mathbf C2 of
    Fig. 5, visualized in the pm 8 px band around the contours (grey pixels). Here,
    the distance values of Phi 1(mathbf x) that are influenced by the occlusion of
    mathbf C1 are marked red (bright inside and dark outside of Omega f1 ).
  Figure 8 Link: articels_figures_by_rev_year\2018\A_RegionBased_GaussNewton_Approach_to_RealTime_Monocular_Multiple_Object_Trackin\figure_8.jpg
  Figure 8 caption: "Left: Visual results of a pose tracking experiment, where a hand-held\
    \ screwdriver was rotated 360circ around its X -axis (top row: examples from the\
    \ input sequence with the frame numbers depicted, middle row: results of the original\
    \ first-order gradient descent implementation of PWP3D [26], bottom row: results\
    \ of the Gauss-Newton-like optimization of [35]). Right: A corresponding plot\
    \ of the estimated rotation around the X -axis for both methods. While the oscillations\
    \ around frames 105\u2013150 and 500\u2013600 indicate that the gradient descent\
    \ time steps are already near the limit of stability, the algorithm cannot reliably\
    \ estimate the full 360circ rotation leading to a failure of PWP3D (see last frame\
    \ central row)."
  Figure 9 Link: articels_figures_by_rev_year\2018\A_RegionBased_GaussNewton_Approach_to_RealTime_Monocular_Multiple_Object_Trackin\figure_9.jpg
  Figure 9 caption: An overview of all eighteen models included in the RBOT dataset.
    The well-textured models from the Rigid Pose tracking dataset [24] are marked
    with blacksquare . The weakly-textured models from the LINE-MOD detection dataset
    [13] are marked with blacklozenge . Here, all models are rendered at the same
    pose for scale comparison.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Henning Tjaden
  Name of the last author: Daniel Cremers
  Number of Figures: 11
  Number of Tables: 2
  Number of authors: 4
  Paper title: A Region-Based Gauss-Newton Approach to Real-Time Monocular Multiple
    Object Tracking
  Publication Date: 2018-12-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 AUC Scores (Higher Is Better) of Our Approach in the OPT Dataset
      Compared to the Results Presented in [40]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Tracking Success Rates (in %) of the Proposed Method in Comparison
      to That of [36] in the RBOT Dataset
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2884990
- Affiliation of the first author: signal processing and speech communication laboratory,
    graz university of technology, graz, austria
  Affiliation of the last author: signal processing and speech communication laboratory,
    graz university of technology, graz, austria
  Figure 1 Link: articels_figures_by_rev_year\2018\Bayesian_Neural_Networks_with_Weight_Sharing_Using_Dirichlet_Processes\figure_1.jpg
  Figure 1 caption: "Graphical illustration of the DP NN model with layerwise weight\
    \ sharing. Observed variables and hyperparameters are indicated as shaded circles.\
    \ The dashed circle indicates that \u03B2 is only relevant for regression tasks.\
    \ For global weight sharing, the dependency on l is dropped."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Bayesian_Neural_Networks_with_Weight_Sharing_Using_Dirichlet_Processes\figure_2.jpg
  Figure 2 caption: "(a) Classification errors (CE) and fraction of used weights (compared\
    \ to NNs without sharing) of BFGS-optimized NNs with random weight sharing and\
    \ posterior samples of DP BNNs using tanh. (b) CE and fraction of used weights\
    \ over \u03B1 for DP BNN ReLU and RND BNN ReLU with two hidden layers of 100 neurons.\
    \ (c) CE of DP BNN ReLU over number of averaged samples. (d) Average runtime for\
    \ several \u03B1 of sampling 200 weight sets using AHMC and SGMCMC, respectively,\
    \ and a single Gibbs cycle of configuration sampling ( z ) for two values of s\
    \ ( s=\u221E corresponds to the likelihood interpolation not being used). The\
    \ runtime differences between SGLD and SGHMC (shown as SGMCMC) are negligible."
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Wolfgang Roth
  Name of the last author: Franz Pernkopf
  Number of Figures: 2
  Number of Tables: 3
  Number of authors: 2
  Paper title: Bayesian Neural Networks with Weight Sharing Using Dirichlet Processes
  Publication Date: 2018-12-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average Test Classification Errors [%] and Standard Deviations
      over Five Runs on MNIST and Variants Thereof
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average Test Log-Likelihoods and Standard Deviations (Top)
      and Average Test Root Mean Squared Errors and Standard Deviations (Bottom) on
      Various UCI Regression Data Sets Obtained Using 5-Fold Cross-Validation
  Table 3 caption:
    table_text: TABLE 3 Average Test Classification Errors [%] and Standard Deviations
      Over five Runs of Stochastic MCMC Methods (SGLD and SGHMC) for Weight Sampling
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2884905
- Affiliation of the first author: mathematical institute, university of oxford, oxford,
    united kingdom
  Affiliation of the last author: mathematical institute, university of oxford, oxford,
    united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2018\Persistence_Paths_and_Signature_Features_in_Topological_Data_Analysis\figure_1.jpg
  Figure 1 caption: "Orbits. The dataset consists of 500 orbits. Each orbit is a set\
    \ of 1001 points in IR 2 , ( x n , y n ):n=0,\u2026,1000\u2282 IR 2 , generated\
    \ by one out of five discrete dynamical systems with a random initial value (\
    \ x 0 , y 0 ) . The five dynamical systems are given by taking the parameter r\u2208\
    2.5,3.5,4,4.1,4.3 , which thus acts as label, and update rule x n+1 = x n +r y\
    \ n (1\u2212 y n )mod1 , and y n+1 = y n +r x n (1\u2212 x n )mod1 where ( x 0\
    \ , y 0 ) is chosen uniformly at random in (0,1 ) 2 . For each of the labels r\
    \ we generated 100 orbits and used 50 to 50 percent as train-test split for the\
    \ resulting 500 orbits. Above shows one orbit for each value of r=2.5,3.5,4,4.1\
    \ (left to right and top to bottom)."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Persistence_Paths_and_Signature_Features_in_Topological_Data_Analysis\figure_2.jpg
  Figure 2 caption: Textures. The OutexTC00000 dataset of surface textures [35]. The
    data set consists of 240 images for training, 240 for testing, as prescribed by
    the test suite. Each sample carries one out of 24 labels. Above shows texture
    of four different labels.
  Figure 3 Link: articels_figures_by_rev_year\2018\Persistence_Paths_and_Signature_Features_in_Topological_Data_Analysis\figure_3.jpg
  Figure 3 caption: 'Shapes. The synthetic shapes dataset [33], [36] consists of point
    cloud data with six labels: random, circle, sphere, clusters, clusters-of-clusters
    and torus. Within each label, there are 50 point clouds containing 500 points
    each. The Gaussian noise model used in each case has standard deviation 0.1. We
    use a 50 to 50 percent test-train split and the persistence diagrams provided
    by [36] https:github.combziiujpcodebooks. Note that our learning method is different
    to that of [33], [36] where K -medoids clustering is used.'
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ilya Chevyrev
  Name of the last author: Harald Oberhauser
  Number of Figures: 3
  Number of Tables: 2
  Number of authors: 3
  Paper title: Persistence Paths and Signature Features in Topological Data Analysis
  Publication Date: 2018-12-07 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Mean Accuracy ( \xB1 \xB1 Standard Deviation)"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Best Truncation Level M M
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2885516
- Affiliation of the first author: "department of engineering \u201Cenzo ferrari\u201D\
    , university of modena and reggio emilia, modena, italy"
  Affiliation of the last author: "department of engineering \u201Cenzo ferrari\u201D\
    , university of modena and reggio emilia, modena, italy"
  Figure 1 Link: articels_figures_by_rev_year\2018\FacefromDepth_for_Head_Pose_Estimation_on_Depth_Images\figure_1.jpg
  Figure 1 caption: Visual examples of the proposed framework output in indoor (first
    row) and automotive (second and third row) settings. Head pose angles are reported
    as colored arrows. Depth maps, Face-from-Depth and Motion Image inputs are depicted
    on the left of each frame.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\FacefromDepth_for_Head_Pose_Estimation_on_Depth_Images\figure_2.jpg
  Figure 2 caption: Example of reliability of the FfD network on depth images. Two
    consecutive frames have been selected from a sequence with an abrupt illumination
    change (from light to dark). In the first column the auto equalized RGB, then
    the corresponding depth maps and finally the FfD reconstruction output.
  Figure 3 Link: articels_figures_by_rev_year\2018\FacefromDepth_for_Head_Pose_Estimation_on_Depth_Images\figure_3.jpg
  Figure 3 caption: Architecture of the Head Localization network with corresponding
    kernel size (k), number of feature maps (n) and stride (s) indicated for each
    convolutional layer.
  Figure 4 Link: articels_figures_by_rev_year\2018\FacefromDepth_for_Head_Pose_Estimation_on_Depth_Images\figure_4.jpg
  Figure 4 caption: Architecture of the Face-from-Depth network.
  Figure 5 Link: articels_figures_by_rev_year\2018\FacefromDepth_for_Head_Pose_Estimation_on_Depth_Images\figure_5.jpg
  Figure 5 caption: Architecture of the head and shoulder pose estimation networks.
  Figure 6 Link: articels_figures_by_rev_year\2018\FacefromDepth_for_Head_Pose_Estimation_on_Depth_Images\figure_6.jpg
  Figure 6 caption: Test (a) and train (c) images on Pandora dataset, test (b) and
    train (d) images on Biwi dataset. For each block, gray-level images and then the
    corresponding depth faces are depicted in the first columns; face images taken
    from the method described in [10] are reported in the third column; finally, the
    output of the Face-from-Depth network proposed in this paper is depicted in the
    last column.
  Figure 7 Link: articels_figures_by_rev_year\2018\FacefromDepth_for_Head_Pose_Estimation_on_Depth_Images\figure_7.jpg
  Figure 7 caption: Sample frames from the Pandora dataset. As depicted, extreme poses
    and challenging camouflage can be present.
  Figure 8 Link: articels_figures_by_rev_year\2018\FacefromDepth_for_Head_Pose_Estimation_on_Depth_Images\figure_8.jpg
  Figure 8 caption: Overview of the whole POSEidon + framework. Depth input images
    are acquired by depth sensors (black) and provided to a head localization CNN
    (blue) to suitably crop the images around the upper-body or head regions. The
    head crop is used to produce the three inputs for the following networks (green),
    that are then merged to output the head pose (red). In particular, the Face-from-Depth
    architecture reconstructs gray-level face images from the corresponding depth
    maps, while the Motion Images are obtained by applying the Farneback algorithm.
    Finally, the upper-body crop is used for the shoulder pose estimation (orange).
    [best in color]
  Figure 9 Link: articels_figures_by_rev_year\2018\FacefromDepth_for_Head_Pose_Estimation_on_Depth_Images\figure_9.jpg
  Figure 9 caption: Error distribution of each POSEidon + components on Pandora dataset.
    On x -axis are reported the ground truth angles, on y -axis the distribution of
    error for each input type.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Guido Borghi
  Name of the last author: Rita Cucchiara
  Number of Figures: 9
  Number of Tables: 7
  Number of authors: 5
  Paper title: Face-from-Depth for Head Pose Estimation on Depth Images
  Publication Date: 2018-12-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Head Pose Estimation Results on Biwi
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Evaluation Metrics Computed on the Reconstructed Gray-Level
      Face Images with Biwi and Pandora Datasets
  Table 3 caption:
    table_text: TABLE 3 Results Obtained on Pandora Dataset with Head Pose Network
      Trained on Gray Level Images and Tested with the Original Gray-Level and Reconstructed
      Ones
  Table 4 caption:
    table_text: TABLE 4 Results of the Head Pose Estimation on Pandora Comparing Different
      System Architectures
  Table 5 caption:
    table_text: TABLE 5 Results for Head Pose Estimation Task on Pandora Dataset
  Table 6 caption:
    table_text: TABLE 6 Estimation Errors and Mean Accuracy of the Shoulder Pose Estimation
      on Pandora
  Table 7 caption:
    table_text: TABLE 7 Results on Biwi, ICT-3DHP and Pandora Dataset of the Complete
      POSEidon + + Pipeline (i.e., Head Localization, Cropping and Pose Estimation)
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2885472
- Affiliation of the first author: department of mathematics and statistics, mcmaster
    university, hamilton, canada
  Affiliation of the last author: department of mathematics and statistics, mcmaster
    university, hamilton, canada
  Figure 1 Link: articels_figures_by_rev_year\2018\Flexible_HighDimensional_Unsupervised_Learning_with_Missing_Data\figure_1.jpg
  Figure 1 caption: Scatterplot of one of the simulated datasets, where colours reflect
    true class.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Flexible_HighDimensional_Unsupervised_Learning_with_Missing_Data\figure_2.jpg
  Figure 2 caption: Plot of run time (in seconds) over 100 repetions under various
    n and r .
  Figure 3 Link: articels_figures_by_rev_year\2018\Flexible_HighDimensional_Unsupervised_Learning_with_Missing_Data\figure_3.jpg
  Figure 3 caption: Plot of BIC and AWE values versus number of latent factors q for
    the MGHFAMISS models fitted to the one hour and eight hour ozone data, where the
    maximum is highlighted in each case.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yuhong Wei
  Name of the last author: Paul D. McNicholas
  Number of Figures: 3
  Number of Tables: 10
  Number of authors: 3
  Paper title: Flexible High-Dimensional Unsupervised Learning with Missing Data
  Publication Date: 2018-12-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 True Model Parameters for the Simulated Data
  Table 10 caption:
    table_text: TABLE 10 The Averaged ARI and ERR Values for the Best MGHFAMISS and
      MSTFAMISS Models Based on BIC for the Original and Modified Wine Data Under
      Various Missingness Rates
  Table 2 caption:
    table_text: TABLE 2 Number of Missing Observations for Each Pattern
  Table 3 caption:
    table_text: TABLE 3 Simulation Results Based on 30 Replications for Missing Pattern
      1
  Table 4 caption:
    table_text: TABLE 4 Simulation Results Based on 30 Replications for Missing Pattern
      2
  Table 5 caption:
    table_text: TABLE 5 Simulation Results Based on 30 Replications for Missing Pattern
      3
  Table 6 caption:
    table_text: TABLE 6 Imputation Performance for MI-PGMM, MI-MGHFA, MGHFAMISS, and
      MSTFAMISS Models Under Various Missing Rates ( r r) for Pattern 1
  Table 7 caption:
    table_text: TABLE 7 Simulation Results Based on 30 Replications Using MGHFAMISS
      and k-POD for Pattern 1
  Table 8 caption:
    table_text: TABLE 8 Run Time (in Seconds) Over 100 Repetions Under Various n n
      and r r
  Table 9 caption:
    table_text: "TABLE 9 The Frequencies with Which Each of the MGHFAMISS Models (Run\
      \ for q=1,\u2026,7 q=1,...,7) Are Chosen by the BIC and AWE for the Original\
      \ and Modified Wine Data Under Various Missingness Rates; Frequencies Are 0\
      \ for q>3 q>3 and So Are Omitted"
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2885760
- Affiliation of the first author: school of information engineering, ningxia university,
    yinchuan
  Affiliation of the last author: department of automation, state key lab of intelligent
    technologies and systems, beijing national research center for information science
    and technology (bnrist), tsinghua university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2018\Learning_ReasoningDecision_Networks_for_Robust_Face_Alignment\figure_1.jpg
  Figure 1 caption: Our proposed reasoning-decision networks framework versus the
    conventional regression-based approach. For the given testing face (near-frontal)
    with a poor initialization, the cascaded regression-based approach learns to correct
    misalignment errors intensely based on initialized shape and may cause bias prediction.
    Different from those approaches, our RDN architecture reasons a plausible shape
    searching policy over the whole shape space, which maximizes the cumulated quality
    values received by the learned shape evaluation function. Based on the obtained
    shape quality values, we successfully achieve to refine low-quality initialization
    for robust face alignment. Our approach will automatically terminates after iterations
    until reaching the alignment accuracy threshold.
  Figure 10 Link: articels_figures_by_rev_year\2018\Learning_ReasoningDecision_Networks_for_Robust_Face_Alignment\figure_10.jpg
  Figure 10 caption: Representative results of our proposed RDN approach on the 300-VW
    dataset, where 68 landmarks were employed for evaluation. According to the qualitative
    analysis, we see that our RDN still achieves robust performance regarding with
    temporal changes due to head motions.
  Figure 2 Link: articels_figures_by_rev_year\2018\Learning_ReasoningDecision_Networks_for_Robust_Face_Alignment\figure_2.jpg
  Figure 2 caption: The architecture of our proposed RDN. Our RDN starts with a given
    facial image I and an entangled-initialized shape p 0 . Taken the t -th iteration
    as an example, our shape reasoning module predicts an action a t to refine the
    shape as p t and produces a subset of shape candidates via K -NN shape searching
    constraint. Our shape decision module evaluates the quality value V for each shape
    candidate and then chooses the highest scored one as the future initialization
    p t+1 . It should be noted that we regard the immediate shape p t as final prediction,
    instead of these outputting shape set produced by K-NN searching.
  Figure 3 Link: articels_figures_by_rev_year\2018\Learning_ReasoningDecision_Networks_for_Robust_Face_Alignment\figure_3.jpg
  Figure 3 caption: Landmark partition of two specific types, i.e., the 300-W and
    AFLW-Full datasets for standard annotations. Note that we did not plot the facial
    contour for better visualization in the 68-lms annotation.
  Figure 4 Link: articels_figures_by_rev_year\2018\Learning_ReasoningDecision_Networks_for_Robust_Face_Alignment\figure_4.jpg
  Figure 4 caption: Visualization of the clustering centers for both evaluation datasets
    (300-W and AFLW-Full), where orange points specify clusters and blue ones for
    all training samples pre-processed by Procrustes [65] analysis.
  Figure 5 Link: articels_figures_by_rev_year\2018\Learning_ReasoningDecision_Networks_for_Robust_Face_Alignment\figure_5.jpg
  Figure 5 caption: CED curves on 300-W including LFPW, HELEN, IBUG, respectively,
    where 68 landmarks were employed for evaluation. Our RDN model outperforms state-of-the-arts
    consistently on three datasets, which indicates the robustness of our RDN to large
    variations due to types of facial poses, diverse facial expressions, partial occlusions
    in wild conditions. Note that the comparison curves should be best viewed in the
    color pdf file.
  Figure 6 Link: articels_figures_by_rev_year\2018\Learning_ReasoningDecision_Networks_for_Robust_Face_Alignment\figure_6.jpg
  Figure 6 caption: CED curves on the COFW dataset principally regarding of the occlusion
    issue, where 29 landmarks were employed for evaluation. We fix the error-axis
    within the range of [0,0.15] in focus on significant comparisons. Note that the
    comparison curves are best viewed in the color pdf file.
  Figure 7 Link: articels_figures_by_rev_year\2018\Learning_ReasoningDecision_Networks_for_Robust_Face_Alignment\figure_7.jpg
  Figure 7 caption: CED curves on the 300-VW challenging set Category 3, which demonstrates
    the robustness of our RDN versus temporal face changes. We employed 68 landmarks
    for evaluation as the same setting in 300-W. Note that the comparison curves are
    best viewed in the color pdf file.
  Figure 8 Link: articels_figures_by_rev_year\2018\Learning_ReasoningDecision_Networks_for_Robust_Face_Alignment\figure_8.jpg
  Figure 8 caption: Representative results of the comparisons of our RDN approach
    (white lines) versus the conventional cascaded regression approach (green lines).
    We used MDM [12] as the baseline cascaded regression approach with deep neural
    networks and plotted the immediate outcomes for each stage. For our RDN, we plotted
    the outcomes of both the shape-reasoning and shape-decision networks. Note that
    even starting with a random and low-quality initialization drawn from the training
    samples, our RDN achieves more reasonable initialization after plausible reasoning
    and evaluation.
  Figure 9 Link: articels_figures_by_rev_year\2018\Learning_ReasoningDecision_Networks_for_Robust_Face_Alignment\figure_9.jpg
  Figure 9 caption: Representative results of our proposed RDN approach on the COFW
    dataset, where 29 landmarks were employed for evaluation. According to the qualitative
    analysis, we see that our RDN even achieves robust performance regarding with
    diverse partial occlusions.
  First author gender probability: 0.9
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.59
  Name of the first author: Hao Liu
  Name of the last author: Jie Zhou
  Number of Figures: 14
  Number of Tables: 5
  Number of authors: 5
  Paper title: Learning Reasoning-Decision Networks for Robust Face Alignment
  Publication Date: 2018-12-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparisons of Averaged Errors (100 Percent) Normalized by
      Eye-Center-Distance of Our RDN with Existing Methods (in Chronological Order)
      on the 300-W Commonset, Challengingset and Fullset, Respectively, Where 68 Landmarks
      Were Employed for Evaluation
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparisons of AUC and Failure Rate of Our Approach with Recent
      Compelling Methods on 300-W Testset (68-lms)
  Table 3 caption:
    table_text: TABLE 3 Comparisons of the Averaged Errors with Other Approaches on
      COFW Regarding of Occlusions, Where 29 Landmarks Were Employed for Evaluation
  Table 4 caption:
    table_text: TABLE 4 Comparisons of Averaged Errors on ALFW-Full (in Chronological
      Order, 19-lms)
  Table 5 caption:
    table_text: TABLE 5 Performance Effects of Our Approach with the Mean Shape versus
      Randomly Initialized Shapes on the 300-W Fullset
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2885298
