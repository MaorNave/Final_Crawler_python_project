- Affiliation of the first author: department of computer science, university of north
    carolina at charlotte, charlotte, nc, usa
  Affiliation of the last author: department of computer science, university of north
    carolina at charlotte, charlotte, nc, usa
  Figure 1 Link: articels_figures_by_rev_year\2023\muxGNN_Multiplex_Graph_Neural_Network_for_Heterogeneous_Graphs\figure_1.jpg
  Figure 1 caption: Schematic of a multilayer representation of a heterogeneous graph.
    (a) A multiplex network consisting of relation-specific layer graphs with separate
    node instantiations coupled across relations. (b) Node invariant coupling graph
    structure in which a node's relation-specific representations are directly coupled
    to the supra-node to produce a singular representation of a node in a heterogeneous
    graph. (c) Node equivariant coupling graph structure in which node-relation instantiations
    are coupled with one another to produce a set of representations characterizing
    a node in a heterogeneous graph.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\muxGNN_Multiplex_Graph_Neural_Network_for_Heterogeneous_Graphs\figure_2.jpg
  Figure 2 caption: Overall framework of the proposed muxGNN model. (a) The relation-specific
    representation of a node generated by local neighborhood aggregation across each
    relation in which a node participates. (b) The node-relation pair representations
    of a supra-node are fused through coupling weights learned via node invariant
    coupling attention (c) Node equivariant attention learns sets of coupling weights
    for a node with each relation viewed as the dominant layer.
  Figure 3 Link: articels_figures_by_rev_year\2023\muxGNN_Multiplex_Graph_Neural_Network_for_Heterogeneous_Graphs\figure_3.jpg
  Figure 3 caption: Ablation study and dimension analysis of muxGNN's coupling graph
    attention.
  Figure 4 Link: articels_figures_by_rev_year\2023\muxGNN_Multiplex_Graph_Neural_Network_for_Heterogeneous_Graphs\figure_4.jpg
  Figure 4 caption: Distributions of coupling attention values for each protein in
    the (a) Brain, (b) CNS, (c) Leukocyte, and (d) NS layers in the Tissue-PPI dataset.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Joshua Melton
  Name of the last author: Siddharth Krishnan
  Number of Figures: 4
  Number of Tables: 4
  Number of authors: 2
  Paper title: 'muxGNN: Multiplex Graph Neural Network for Heterogeneous Graphs'
  Publication Date: 2023-03-29 00:00:00
  Table 1 caption:
    table_text: TABLE I Summary of Datasets Used in Our Experiments. For the Graph
      Classification Datasets, the Average Number of Nodes, Edges, and Relation Layers
      is Reported
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE II Mean Performance (ROC-AUC %, F1 % and PR-AUC %) on Link
      Prediction in a Transductive Context. OOT: Out of Time (24 hrs)'
  Table 3 caption:
    table_text: TABLE III Mean Performance (ROC-AUC %, F1 % and PR-AUC %) on Link
      Prediction in an Inductive Context
  Table 4 caption:
    table_text: TABLE IV Mean Performance (Accuracy, Precision, Recall, and F1) of
      Different Methods on Data Provenance Graph Classification. Only Two Digits of
      Precision Reported by [29]. Recall and F-Score Not Reported by [27]
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3263079
- Affiliation of the first author: institute of artificial intelligence, beihang university,
    beijing, china
  Affiliation of the last author: institute of artificial intelligence, beihang university,
    beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2023\Efficient_Robustness_Assessment_via_Adversarial_SpatialTemporal_Focus_on_Videos\figure_1.jpg
  Figure 1 caption: Overview of the proposed AstFocus attack. It integrates a cooperative
    Multi-Agent Reinforcement Learning (MARL) module into the PGD attack with NES
    gradient estimation [22], and thus selects key frames and key patches within the
    video to reduce dimensions. In this way, an effective and efficient gradient estimation
    on the reduced space is achieved, and the evaluation's efficiency and accuracy
    are improved at the same time.
  Figure 10 Link: articels_figures_by_rev_year\2023\Efficient_Robustness_Assessment_via_Adversarial_SpatialTemporal_Focus_on_Videos\figure_10.jpg
  Figure 10 caption: Convergence of the proposed AstFocus attacks.
  Figure 2 Link: articels_figures_by_rev_year\2023\Efficient_Robustness_Assessment_via_Adversarial_SpatialTemporal_Focus_on_Videos\figure_2.jpg
  Figure 2 caption: Designed actions of the spatial agent. In each frame, we uniformly
    divide the frame into overlapped patches according to a predefined stride. All
    the patch candidates constitute the actions. For simplicity, the stride equals
    to the patch size in this example.
  Figure 3 Link: articels_figures_by_rev_year\2023\Efficient_Robustness_Assessment_via_Adversarial_SpatialTemporal_Focus_on_Videos\figure_3.jpg
  Figure 3 caption: Flowchart for the Policy network of the proposed spatial agent.
    It is used to identify the crucial regions of each video frame.
  Figure 4 Link: articels_figures_by_rev_year\2023\Efficient_Robustness_Assessment_via_Adversarial_SpatialTemporal_Focus_on_Videos\figure_4.jpg
  Figure 4 caption: Flowchart for the policy network of the proposed temporal agent.
    It is used to select the key video frames from the input video.
  Figure 5 Link: articels_figures_by_rev_year\2023\Efficient_Robustness_Assessment_via_Adversarial_SpatialTemporal_Focus_on_Videos\figure_5.jpg
  Figure 5 caption: Parameter tuning results of AstFocus attacks with different patch
    sizes. (A) The effects for fooling rate. (B) The effects for query number. (C)
    The effects for perturbation magnitude.
  Figure 6 Link: articels_figures_by_rev_year\2023\Efficient_Robustness_Assessment_via_Adversarial_SpatialTemporal_Focus_on_Videos\figure_6.jpg
  Figure 6 caption: Parameter tuning results of AstFocus attacks with different upper
    bounds of key frames. (A) The effects for fooling rate. (B) The effects for query
    number. (C) The effects for perturbation magnitude.
  Figure 7 Link: articels_figures_by_rev_year\2023\Efficient_Robustness_Assessment_via_Adversarial_SpatialTemporal_Focus_on_Videos\figure_7.jpg
  Figure 7 caption: Parameter tuning results of AstFocus attacks with different sample
    numbers n . (A) The effects for fooling rate. (B) The effects for query number.
    (C) The effects for perturbation magnitude.
  Figure 8 Link: articels_figures_by_rev_year\2023\Efficient_Robustness_Assessment_via_Adversarial_SpatialTemporal_Focus_on_Videos\figure_8.jpg
  Figure 8 caption: Parameter tuning results of AstFocus attacks with different reward
    weights. (A) The effects for fooling rate. (B) The effects for query numbers.
    (C) The effects for perturbation magnitude.
  Figure 9 Link: articels_figures_by_rev_year\2023\Efficient_Robustness_Assessment_via_Adversarial_SpatialTemporal_Focus_on_Videos\figure_9.jpg
  Figure 9 caption: Comparison between RL-based agent and random agents.
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.76
  Name of the first author: Xingxing Wei
  Name of the last author: Huanqian Yan
  Number of Figures: 13
  Number of Tables: 8
  Number of authors: 3
  Paper title: Efficient Robustness Assessment via Adversarial Spatial-Temporal Focus
    on Videos
  Publication Date: 2023-03-29 00:00:00
  Table 1 caption:
    table_text: "TABLE I Comparisons With Query-Based Black-Box Video Attack Methods.\
      \ \u201CTemporal\u201D Denotes Reducing Temporal Redundancy in the Video, \u201C\
      Spatial\u201D Denotes Reducing Spatial Redundancy, \u201CJointly\u201D Denotes\
      \ Jointly Learning for Reducing the Spatial and Temporal Redundancy in the Video"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II The Accuracy of Four Different Modes on Three Datasets
  Table 3 caption:
    table_text: TABLE III Effects of Various Agents to AstFocus Attacks in an Un-Targeted
      Setting
  Table 4 caption:
    table_text: TABLE IV Effects of Various Rewards to AstFocus Attacks in an Un-Targeted
      Setting
  Table 5 caption:
    table_text: "TABLE V The Comparative Results versus Four Different Threat Models\
      \ on UCF-101 Dataset. The Best Results are Highlighted in Red. The Symbol \u201C\
      -\u201D Means the Used NQ Exceeds the Maximum NQ. \u2191 \u2191 Denotes the\
      \ Larger, the Better, and \u2193 \u2193 Denotes the Smaller, the Better"
  Table 6 caption:
    table_text: "TABLE VI The Comparative Results versus Four Different Threat Models\
      \ on HMDB-51 Dataset. The Best Results are Highlighted in Red. The Symbol \u201C\
      -\u201D Means the Used NQ Exceeds the Maximum NQ. \u2191 \u2191 Denotes the\
      \ Larger, the Better, and \u2193 \u2193 Denotes the Smaller, the Better"
  Table 7 caption:
    table_text: "TABLE VII The Comparative Results versus Four Different Threat Models\
      \ on Kinetics-400 Dataset. The Best Results are Highlighted in Red. The Symbol\
      \ \u201C-\u201D Means the Used NQ Exceeds the Maximum NQ. \u2191 \u2191 Denotes\
      \ the Larger, the Better, and \u2193 \u2193 Denotes the Smaller, the Better"
  Table 8 caption:
    table_text: TABLE VIII Results of AstFocus Attack Against Defended C3D Method
      on HMDB-51
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3262592
- Affiliation of the first author: school of artificial intelligence and automation,
    wuhan, china
  Affiliation of the last author: school of artificial intelligence and automation,
    wuhan, china
  Figure 1 Link: articels_figures_by_rev_year\2023\CRNet_A_Fast_Continual_Learning_Framework_With_Random_Theory\figure_1.jpg
  Figure 1 caption: Illustration of three continual learning scenarios to shed light
    on the difficulty and generality. Take a sequence that contains two binary classification
    tasks as an example. With all data of the current task available but none of the
    previous, a model incrementally recognizes 2 classes ( C1!sim ! C2 ) and 4 classes
    ( C1!sim ! C4 ) in different scenarios.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\CRNet_A_Fast_Continual_Learning_Framework_With_Random_Theory\figure_2.jpg
  Figure 2 caption: The proposed CRNet architecture built by a randomly initialized
    base network for continual learning. Each task can be sequentially fed with the
    original inputs boldsymbolX or extracted features boldsymbolXmathcal F . A basic
    CRNet equips n groups of mapping feature nodes for fully exploiting the hidden
    information among inputs and m enhancement nodes as extra enhanced inputs to diversify
    expanded-input representations, followed by L hidden nodes and analytical outputs
    for decision making. The darker nodes indicate those can obtain more distinguishable
    representations.
  Figure 3 Link: articels_figures_by_rev_year\2023\CRNet_A_Fast_Continual_Learning_Framework_With_Random_Theory\figure_3.jpg
  Figure 3 caption: Average test accuracy on split CIFAR-100 in the Class-IL scenario.
    (a) Consecutively learning five 20-class tasks. (b) Consecutively learning ten
    10-class tasks.
  Figure 4 Link: articels_figures_by_rev_year\2023\CRNet_A_Fast_Continual_Learning_Framework_With_Random_Theory\figure_4.jpg
  Figure 4 caption: Large-scale experiments on ImageNet in the Class-IL scenario that
    are measured by average test accuracy, backward transfer, and forward transfer.
    Our approaches compare with the state-of-the-art methods under 10 random task
    orderings, with the mean andor standard deviation reported. (a) Consecutively
    learning ten 20-class tasks. (b) Consecutively learning fifty 4-class tasks. (c)
    Consecutively learning one hundred 2-class tasks.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Depeng Li
  Name of the last author: Zhigang Zeng
  Number of Figures: 4
  Number of Tables: 3
  Number of authors: 2
  Paper title: 'CRNet: A Fast Continual Learning Framework With Random Theory'
  Publication Date: 2023-03-29 00:00:00
  Table 1 caption:
    table_text: "TABLE I Experimental Results on FashionMNIST-105 Measuring Average\
      \ Test Accuracy (%), Backward Transfer, Forward Transfer, Accumulative Training\
      \ and Test Time (s), and the Final Number of Parameters ( \xD7 10 6 \xD7106).\
      \ All Methods are Run 10 Times, With the Mean and Standard Deviation Reported.\
      \ \u2020 \u2020 Denotes the Results Produced From Mask-Based Baselines WithWithout\
      \ Using Task Identities"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Parameter Analysis on the Trade-Off Coefficients in Our Methods.
      All the Results are Measured by Average Test Accuracy (%), Backward Transfer,
      and Forward Transfer (All the Higher the Better) Under 10 Random Runs, With
      the Mean and Standard Deviation Reported
  Table 3 caption:
    table_text: TABLE III Ablation Study of the Proposed Components on Split CIFAR-100.
      All the Results are Measured Average Test Accuracy (%) Under 10 Random Runs,
      With the Mean and Standard Deviation Reported
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3262853
- Affiliation of the first author: institute of artificial intelligence, huazhong
    university of science and technology, wuhan, china
  Affiliation of the last author: electronic information and communications, huazhong
    university of science and technology, wuhan, china
  Figure 1 Link: articels_figures_by_rev_year\2023\SDVLOAM_SemiDirect_VisualLiDAR_Odometry_and_Mapping\figure_1.jpg
  Figure 1 caption: (a) Our camera-LiDAR sensor system for collecting gray images
    and 3D point cloud measurements. (b) Exemplar input data collected by our camera-LiDAR
    sensor system. (c) The estimated trajectory overlaid on Google Map for visual
    illustration. (d) The 3D reconstruction result.
  Figure 10 Link: articels_figures_by_rev_year\2023\SDVLOAM_SemiDirect_VisualLiDAR_Odometry_and_Mapping\figure_10.jpg
  Figure 10 caption: The comparative result of trajectory Fig. 7(c) with and without
    sweep reconstruction, which are almost coincide. This result demonstrates that
    our method can increase the output frequency without affecting the accuracy of
    pose estimation.
  Figure 2 Link: articels_figures_by_rev_year\2023\SDVLOAM_SemiDirect_VisualLiDAR_Odometry_and_Mapping\figure_2.jpg
  Figure 2 caption: Illustration of the coordinate transformation of three coordinate
    systems. (cdot)w , (cdot)c and (cdot)l are defined as a 3D point in the world
    coordinates, in the camera coordinates and in the LiDAR coordinates respectively.
    The world coordinate coincides with (cdot)l at the starting position. In all coordinates
    the x-axis points to the right, the y-axis points upward, and the z-axis points
    forward. We assume that the transformation mathbf Tcl between camera and LiDAR
    is known and remains constant during operation.
  Figure 3 Link: articels_figures_by_rev_year\2023\SDVLOAM_SemiDirect_VisualLiDAR_Odometry_and_Mapping\figure_3.jpg
  Figure 3 caption: "Overview of our SDV-LOAM which consists of two main modules:\
    \ a semi-direct depth-enhanced visual odometry and a LiDAR odometry. The visual\
    \ module and LiDAR module are combined by a sweep reconstruction block, which\
    \ increases the input frequency of LiDAR point clouds to the same frequency as\
    \ the camera images. The yellow blocks highlight our main contributions compared\
    \ with existing methods. It is worth explaining that the \u201D60 Hz visual pose\u201D\
    \ and the \u201D60 Hz reconstructed cloud\u201D are the ideal frequency assuming\
    \ infinite computing resources. We use 60 Hz visual pose and reconstructed cloud\
    \ here for the convenience of description. In fact, both our vision and LiDAR\
    \ module can run at around 20 Hz."
  Figure 4 Link: articels_figures_by_rev_year\2023\SDVLOAM_SemiDirect_VisualLiDAR_Odometry_and_Mapping\figure_4.jpg
  Figure 4 caption: Illustration of point matching with propagation. We first finds
    correspondences between a host keyframe kj and the current frame ci by minimizing
    photometric error between 2D patches (black squares) in ci and kj . For an arbitrary
    3D point mathbf p1 , we project it from its host frame kj to ci to derive mathbf
    u1ci , then we take mathbf p1 's previously matched correspondence mathbf u1kn
    from kn that is closest to ci , and align mathbf u1kn and mathbf u1ci which have
    similar scale to refine the position of mathbf u1ci .
  Figure 5 Link: articels_figures_by_rev_year\2023\SDVLOAM_SemiDirect_VisualLiDAR_Odometry_and_Mapping\figure_5.jpg
  Figure 5 caption: Illustration of our sweep reconstruction algorithm. 10 Hz input
    raw point cloud Sj , Sj+1 and Sj+2 are overlapped and reconstructed to obtain
    60 Hz reconstructed point clouds Si+7 , Si+8 , ldots , Si+17 .
  Figure 6 Link: articels_figures_by_rev_year\2023\SDVLOAM_SemiDirect_VisualLiDAR_Odometry_and_Mapping\figure_6.jpg
  Figure 6 caption: Sensor configuration for experiments on our own platform. The
    sensor system is mounted on a car-like vehicle for outdoor experiments.
  Figure 7 Link: articels_figures_by_rev_year\2023\SDVLOAM_SemiDirect_VisualLiDAR_Odometry_and_Mapping\figure_7.jpg
  Figure 7 caption: The results of our outdoor experiments. (a) and (c) are the trajectory
    estimated by our system on two different vehicle routes. We overlaid (a) and (c)
    with Google Map to obtain (b) and (d) for better evaluation.
  Figure 8 Link: articels_figures_by_rev_year\2023\SDVLOAM_SemiDirect_VisualLiDAR_Odometry_and_Mapping\figure_8.jpg
  Figure 8 caption: The resulting global point cloud map of trajectory Fig. 7(a) and
    (c).
  Figure 9 Link: articels_figures_by_rev_year\2023\SDVLOAM_SemiDirect_VisualLiDAR_Odometry_and_Mapping\figure_9.jpg
  Figure 9 caption: The local screenshots of trajectories of Fig. 7(c) in the form
    of points. (a)-(d) are the results without sweep reconstruction while (e)-(h)
    are the results with sweep reconstruction. The resolution of points on the trajectory
    reflects the frequency of the output pose from our LiDAR odometry. The comparison
    between (a)-(d) and (e)-(h) demonstrates that our sweep reconstruction block can
    effectively improve the frequency of LiDAR pose estimation.
  First author gender probability: 0.61
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.66
  Name of the first author: Zikang Yuan
  Name of the last author: Xin Yang
  Number of Figures: 11
  Number of Tables: 6
  Number of authors: 5
  Paper title: "SDV-LOAM: Semi-Direct Visual\u2013LiDAR Odometry and Mapping"
  Publication Date: 2023-03-29 00:00:00
  Table 1 caption:
    table_text: TABLE I Ablation Study of Our Visual Odometry
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Pixel Error of Correspondences At Different Frame Intervals
      on KITTI 00-10
  Table 3 caption:
    table_text: TABLE III Relative Translational Error of Fast-LOAM and Fast-LOAM++
      in X-Y-Z Direction [%] [%]
  Table 4 caption:
    table_text: TABLE IV Ablation Study of Our LiDAR Odometry
  Table 5 caption:
    table_text: TABLE V Relative Translational Error Comparison of LiDAR-Assisted
      Depth-Enhanced VO on KITTI Odometry
  Table 6 caption:
    table_text: TABLE VI Relative Translational Error Comparison of Visual-LiDAR Odometry
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3262817
- Affiliation of the first author: school of computer science and engineering, nanyang
    technological university, singapore
  Affiliation of the last author: ucas-terminus ai lab, university of chinese academy
    of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2023\Unsupervised_Point_Cloud_Representation_Learning_With_Deep_Neural_Networks_A_Sur\figure_1.jpg
  Figure 1 caption: 'The general pipeline of unsupervised representation learning
    on point clouds: Deep neural networks are first pre-trained with unannotated point
    clouds via unsupervised learning over certain pre-text tasks. The learned unsupervised
    point cloud representations are then transferred to various downstream tasks to
    provide network initialization, with which the pre-trained networks are fine-tuned
    with a small amount of annotated task-specific point cloud data.'
  Figure 10 Link: articels_figures_by_rev_year\2023\Unsupervised_Point_Cloud_Representation_Learning_With_Deep_Neural_Networks_A_Sur\figure_10.jpg
  Figure 10 caption: An illustration of 3D spatial convolution including continuous
    and discrete convolutions. Parameters p and qi denote the center point and its
    neighboring points, respectively. The graph is reproduced based on [2].
  Figure 2 Link: articels_figures_by_rev_year\2023\Unsupervised_Point_Cloud_Representation_Learning_With_Deep_Neural_Networks_A_Sur\figure_2.jpg
  Figure 2 caption: Taxonomy of existing unsupervised methods in point cloud representation
    learning.
  Figure 3 Link: articels_figures_by_rev_year\2023\Unsupervised_Point_Cloud_Representation_Learning_With_Deep_Neural_Networks_A_Sur\figure_3.jpg
  Figure 3 caption: 'Illustration of object part segmentation: The first row shows
    a few object samples including airplane, motorcycle, and table from the ShapeNetPart
    dataset [14]. The second row shows segmentation ground truth with different parts
    as highlighted by different colors.'
  Figure 4 Link: articels_figures_by_rev_year\2023\Unsupervised_Point_Cloud_Representation_Learning_With_Deep_Neural_Networks_A_Sur\figure_4.jpg
  Figure 4 caption: 'Illustration of 3D bounding boxes in point cloud object detection:
    The two graphs show 3D bounding boxes in datasets ScanNet-V2 [18] and KITTI [19]
    which are cropped from [16] and [20], respectively.'
  Figure 5 Link: articels_figures_by_rev_year\2023\Unsupervised_Point_Cloud_Representation_Learning_With_Deep_Neural_Networks_A_Sur\figure_5.jpg
  Figure 5 caption: 'Illustration of semantic point cloud segmentation: For the point
    cloud sample from S3DIS [21] on the left, the graph on the right shows the corresponding
    ground truth where different categories are highlighted by different colors.'
  Figure 6 Link: articels_figures_by_rev_year\2023\Unsupervised_Point_Cloud_Representation_Learning_With_Deep_Neural_Networks_A_Sur\figure_6.jpg
  Figure 6 caption: 'Illustration of instance segmentation on point clouds: For the
    point cloud sample from ScanNet-V2 [18] on the left, the graph on the right shows
    the corresponding ground truth with different instances highlighted by different
    colors.'
  Figure 7 Link: articels_figures_by_rev_year\2023\Unsupervised_Point_Cloud_Representation_Learning_With_Deep_Neural_Networks_A_Sur\figure_7.jpg
  Figure 7 caption: A simplified architecture of PointNet [15] for point cloud object
    classification, where parameters n and m denote point number and feature dimension,
    respectively.
  Figure 8 Link: articels_figures_by_rev_year\2023\Unsupervised_Point_Cloud_Representation_Learning_With_Deep_Neural_Networks_A_Sur\figure_8.jpg
  Figure 8 caption: 'Schematic depiction of graph convolutional network (GCN): Each
    graph consists of multiple vertexes representing points Xi or features Zi (highlighted
    by circular dots), as well as edges connecting the vertexes representing point
    relations (shown as black lines). C denotes input channels, F denotes output feature
    dimensions, and Yi denotes labels.'
  Figure 9 Link: articels_figures_by_rev_year\2023\Unsupervised_Point_Cloud_Representation_Learning_With_Deep_Neural_Networks_A_Sur\figure_9.jpg
  Figure 9 caption: An illustration of SR-UNet [54] that adopts a unified U-Net [55]
    architecture for sparse convolution. The graph is reproduced based on [54].
  First author gender probability: 0.57
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Aoran Xiao
  Name of the last author: Ling Shao
  Number of Figures: 20
  Number of Tables: 11
  Number of authors: 6
  Paper title: 'Unsupervised Point Cloud Representation Learning With Deep Neural
    Networks: A Survey'
  Publication Date: 2023-03-29 00:00:00
  Table 1 caption:
    table_text: TABLE I Summary of Commonly Used Datasets for Training and Evaluations
      in Prior URL Studies With Point Clouds
  Table 10 caption:
    table_text: TABLE X Object Detection Performance on Dataset ONCE [30]. The Baseline
      is Trained From Scratch. Unsupervised Learning Methods are Used for Pre-Training
      Models. U small Usmall, U median Umedian, and U large Ularge Represent Small,
      Medium, and Large Amounts of Unlabelled Data That are Used for Unsupervised
      Learning, Respectively
  Table 2 caption:
    table_text: TABLE II Summary of Generation-Based Methods for Unsupervised Representation
      Learning of Point Clouds
  Table 3 caption:
    table_text: TABLE III Summary of Context-Based Methods for Unsupervised Representation
      Learning of Point Clouds
  Table 4 caption:
    table_text: 'TABLE IV Comparing Linear Shape Classification on ModelNet10 and
      ModelNet40 [27]: Linear SVM Classifiers are Trained With Representations Learned
      by Different Unsupervised Methods. Accuracy Highlighted by Was Obtained by Pre-Training
      With Multi-Modal Data. [t] Denotes Models With Modified Transformers. [ST] Denotes
      Models With Standard Transformers'
  Table 5 caption:
    table_text: "TABLE V Comparisons of Unsupervised Pre-Training Performance Over\
      \ the Object Classification Datasets ModelNet40 and OBJ-BG Split in ScanObjecNN.\
      \ Performance Numbers are Presented in the Format of \u201DAB,\u201D With \u201D\
      a\u201D Indicating Training Classification Models From Scratch With Random Initialization\
      \ and \u201Db\u201D Indicating Fine-Tuning Classification Models That are Initialized\
      \ With Unsupervised Pre-Trained Models. Performance Under \u201Ca\u201D May\
      \ Vary Due to Different Implementations as Reported in the Corresponding Papers"
  Table 6 caption:
    table_text: "TABLE VI Comparison of 3D URL Methods for Shape Part Segmentation\
      \ Over ShapeNetPart [14]. \u201Cunsup.\u201D Denotes Linear Classification of\
      \ the Learned Unsupervised Point Features. \u201Ctrans.\u201D is Presented in\
      \ a Format of \u201CAB,\u201D Where \u201Ca\u201D is Obtained With Segmentation\
      \ Models Trained From Scratch With Random Initialization, and \u201Cb\u201D\
      \ is Obtained by Fine-Tuning Segmentation Models That are Initialized With Unsupervised\
      \ Pre-Trained Models. We Also Provide Supervised Performances (\u201Csup.\u201D\
      ) of Different Backbone Models With Random Initialization (Extracted From the\
      \ Original Papers)"
  Table 7 caption:
    table_text: 'TABLE VII Semantic Segmentation on S3DIS [21]: It Compares Supervised
      Training With Random Weight Initialization and Fine-Tuning With Pre-Trained
      Weights Learned From Unsupervised Pre-Training Tasks. It Uses DGCNN as the Segmentation
      Model, Which is Trained on Different Single Areas and Tested on Area 5 (Upper
      Part) and Area 6 (Lower Part)'
  Table 8 caption:
    table_text: 'TABLE VIII Performances for Semantic Segmentation on S3DIS [21].
      Upper Part: Models are Tested on Area5 (Fold1) and Trained on the Rest of the
      Data. Lower Part: Six-Fold Cross-Validation Over Three Runs'
  Table 9 caption:
    table_text: "TABLE IX Comparison of Pre-Training Effects by Different Unsupervised\
      \ Learning Methods. The Benchmarking is 3D Object Detection Task Over Datasets\
      \ SUN RGB-D [28] and ScanNet-V2 [18]. \u201C0.25\u201C and \u201C0.5\u201C Represent\
      \ Per-Category Results of Average Precision (AP) With IoU Threshold 0.25 (mAP0.25)\
      \ and 0.5 (mAP0.5), Respectively"
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3262786
- Affiliation of the first author: department of electrical and electronic engineering,
    personal robotics laboratory, imperial college london, london, u.k.
  Affiliation of the last author: department of electrical and electronic engineering,
    personal robotics laboratory, imperial college london, london, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2023\Visible_and_Infrared_Image_Fusion_Using_Deep_Learning\figure_1.jpg
  Figure 1 caption: The benefit of visible and infrared image fusion. The first row
    shows visible images while the second row shows corresponding infrared images.
    The third row illustrates the fusion results using the MGFF method [18]. As can
    be seen, the fused images contain features from both visible and infrared images.
  Figure 10 Link: articels_figures_by_rev_year\2023\Visible_and_Infrared_Image_Fusion_Using_Deep_Learning\figure_10.jpg
  Figure 10 caption: Visual quality-oriented VIF v.s. application-oriented VIF. Person
    detection is used as an example. The visible and infrared images are provided
    in the M 3 FD dataset [115]. The fused image is generated by the authors using
    the MGFF [18] image fusion algorithm.
  Figure 2 Link: articels_figures_by_rev_year\2023\Visible_and_Infrared_Image_Fusion_Using_Deep_Learning\figure_2.jpg
  Figure 2 caption: The number of papers on deep learning-based VIF methods increases
    quickly. Deep learning-based general image fusion methods that can be applied
    to several image fusion tasks, including VIF, are also counted.
  Figure 3 Link: articels_figures_by_rev_year\2023\Visible_and_Infrared_Image_Fusion_Using_Deep_Learning\figure_3.jpg
  Figure 3 caption: Development timeline of deep learning-based VIF methods, with
    some key milestones.
  Figure 4 Link: articels_figures_by_rev_year\2023\Visible_and_Infrared_Image_Fusion_Using_Deep_Learning\figure_4.jpg
  Figure 4 caption: Three stages of VIF methods, i.e., feature extraction, feature
    fusion, and image reconstruction.
  Figure 5 Link: articels_figures_by_rev_year\2023\Visible_and_Infrared_Image_Fusion_Using_Deep_Learning\figure_5.jpg
  Figure 5 caption: Single-branch and dual-branch architectures for handling visible
    and infrared images. In some dual-branch studies, the two deep learning models
    share weights as denoted by the dashed line.
  Figure 6 Link: articels_figures_by_rev_year\2023\Visible_and_Infrared_Image_Fusion_Using_Deep_Learning\figure_6.jpg
  Figure 6 caption: Main architectures of CNN-based VIF methods. Note that the CNN-based
    model can contain single branch or dual branches. It can also contain other components,
    e.g., image decomposition.
  Figure 7 Link: articels_figures_by_rev_year\2023\Visible_and_Infrared_Image_Fusion_Using_Deep_Learning\figure_7.jpg
  Figure 7 caption: Basic architectures of AE-based VIF methods. First, an autoencoder
    is trained to reconstruct the input image. Then, the trained encoder and decoder
    are used to generate a fused image with the help of a fusion rule.
  Figure 8 Link: articels_figures_by_rev_year\2023\Visible_and_Infrared_Image_Fusion_Using_Deep_Learning\figure_8.jpg
  Figure 8 caption: Basic architectures of unsupervised GAN-based VIF methods.
  Figure 9 Link: articels_figures_by_rev_year\2023\Visible_and_Infrared_Image_Fusion_Using_Deep_Learning\figure_9.jpg
  Figure 9 caption: An example architecture of transformer-based VIF methods. A CNN
    model is used to extract features, which are fused using a transformer block.
    The fused image is obtained by using another CNN model.
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xingchen Zhang
  Name of the last author: Yiannis Demiris
  Number of Figures: 11
  Number of Tables: 3
  Number of authors: 2
  Paper title: Visible and Infrared Image Fusion Using Deep Learning
  Publication Date: 2023-03-30 00:00:00
  Table 1 caption:
    table_text: TABLE I An Overview of Representative Deep Learning-Based VIF Methods.
      only Peer-Reviewed Methods are Summarized in This Table
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Some VIF Methods Published in Top Journals and Conferences.
      as Can Be Seen, It is Difficult to Know the Real Performance Comparison of VIF
      Methods Because Different Test Sets and Evaluation Metrics are Used
  Table 3 caption:
    table_text: TABLE III Quantitative Performance Comparison on VIFB. The Best Three
      Values of Each Metric are Marked in RED red RED , GREEN green GREEN and BLUE
      blue BLUE , Respectively. the Three Numbers After Method Names Denote the Number
      of Best Value, Second Best Value, and Third Best Value, Respectively. best Viewed
      in Color
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3261282
- Affiliation of the first author: research center for brain-inspired intelligence,
    state key laboratory of multimodal artificial intelligence systems, institute
    of automation, chinese academy of sciences, beijing, china
  Affiliation of the last author: research center for brain-inspired intelligence,
    state key laboratory of multimodal artificial intelligence systems, institute
    of automation, chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2023\Decoding_Visual_Neural_Representations_by_Multimodal_Learning_of_BrainVisualLing\figure_1.jpg
  Figure 1 caption: Dual coding of knowledge in the human brain. When we see a picture
    of elephant, we will spontaneously retrieve the knowledge of elephant in our mind.
    Then, the concept of elephant is encoded in the brain both visually and linguistically,
    where language, as a valid prior experience, contributes to shaping vision-derived
    representations.
  Figure 10 Link: articels_figures_by_rev_year\2023\Decoding_Visual_Neural_Representations_by_Multimodal_Learning_of_BrainVisualLing\figure_10.jpg
  Figure 10 caption: Decoding accuracy with different types of extra data. indicates
    that the comparison is significant (paired t-test, p < 0.05), and NS denotes that
    the comparison is not significant. The red dashed lines represent the chance level.
  Figure 2 Link: articels_figures_by_rev_year\2023\Decoding_Visual_Neural_Representations_by_Multimodal_Learning_of_BrainVisualLing\figure_2.jpg
  Figure 2 caption: Image stimuli, evoked brain activity and their corresponding textual
    data. We can only collect brain activity for a few categories, but we can easily
    collect images andor text data for almost all categories. Therefore, for seen
    classes, we assume that the brain activity, visual images and corresponding textual
    descriptions are available for training, whereas for novel classes, only visual
    images and textual descriptions are available for training. The test data are
    brain activity from the novel classes.
  Figure 3 Link: articels_figures_by_rev_year\2023\Decoding_Visual_Neural_Representations_by_Multimodal_Learning_of_BrainVisualLing\figure_3.jpg
  Figure 3 caption: Data preprocessing. We preprocess the raw inputs into feature
    representations with modality-specific feature extractors.
  Figure 4 Link: articels_figures_by_rev_year\2023\Decoding_Visual_Neural_Representations_by_Multimodal_Learning_of_BrainVisualLing\figure_4.jpg
  Figure 4 caption: "(a). Brain-Image-Text joint representation learning. The model\
    \ works in two collaborative parts\u2014multi-modality joint modeling and (both\
    \ intra- and inter-modality) MI regularization. In this step, trimodal data from\
    \ the seen classes and the large scale bimodal (image-text pairs) or unimodal\
    \ (imagetext) data from the novel classes are used for training. (b). Latent space\
    \ classifier training. A SVM classifier is trained from the latent representations\
    \ of visual and textual features of the novel classes. Note that the encoders\
    \ Ev and Et are frozen in this step, and only the SVM classifier (in grey module)\
    \ will be optimized. (c). Decoding neural representations. The latent representations\
    \ of the brain activity of the novel classes are passed to the neural decoder\
    \ (i.e., the trained SVM classifier) to obtain the decoded visual categories.\
    \ In this step, the encoder Eb and the SVM classifier are always frozen, hence\
    \ the label predicting space is identical to that in (b). The reason why the SVM\
    \ classifier can be generalized from (b) to (c) is that the latent representations\
    \ of these three modalities have been aligned in (a) thought the MoPoE formulation."
  Figure 5 Link: articels_figures_by_rev_year\2023\Decoding_Visual_Neural_Representations_by_Multimodal_Learning_of_BrainVisualLing\figure_5.jpg
  Figure 5 caption: The voxel stability maps in the visual cortex. For each voxel
    in each brain Regions of Interest (ROI), the stability score is quantified as
    the mean Pearson correlation coefficient across all pairwise combinations of the
    five repetitions (trials). Then, the stability score for each voxel is projected
    onto a cortical flat map constructed for an example subject (Subject 3 of DIR-Wiki
    dataset) using Pycortex [72]. For each ROI, only the top 15% voxels with larger
    stability scores are used in neural decoding tasks.
  Figure 6 Link: articels_figures_by_rev_year\2023\Decoding_Visual_Neural_Representations_by_Multimodal_Learning_of_BrainVisualLing\figure_6.jpg
  Figure 6 caption: The EEG channels and time window used in our experiments.
  Figure 7 Link: articels_figures_by_rev_year\2023\Decoding_Visual_Neural_Representations_by_Multimodal_Learning_of_BrainVisualLing\figure_7.jpg
  Figure 7 caption: Qualitative comparison of the (top-1) decoding accuracy gain for
    each test class when the textual feature is added (produced by the BraVL method,
    averaged across all 8 subjects, sorted in descending order).
  Figure 8 Link: articels_figures_by_rev_year\2023\Decoding_Visual_Neural_Representations_by_Multimodal_Learning_of_BrainVisualLing\figure_8.jpg
  Figure 8 caption: Projection of the textual feature contributions onto the visual
    cortex (produced by the BraVL method, Subject 3, DIR-Wiki). The green regions
    indicate that text features have a positive effect, and the red or orange regions
    indicate the opposite.
  Figure 9 Link: articels_figures_by_rev_year\2023\Decoding_Visual_Neural_Representations_by_Multimodal_Learning_of_BrainVisualLing\figure_9.jpg
  Figure 9 caption: The impact of the latent dimension, MI regularization coefficients
    lambda 1 and lambda 2 , stability selection ratio, as well as the number of MLP
    layers.
  First author gender probability: 0.96
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.71
  Name of the first author: Changde Du
  Name of the last author: Huiguang He
  Number of Figures: 14
  Number of Tables: 8
  Number of authors: 4
  Paper title: Decoding Visual Neural Representations by Multimodal Learning of Brain-Visual-Linguistic
    Features
  Publication Date: 2023-03-30 00:00:00
  Table 1 caption:
    table_text: TABLE I Overview of the Variational Joint Posterior Forms in Family
      of Multimodal VAEs
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Characteristics of the Brain-Visual-Linguistic Datasets Used
      in Our Experiments. For Brain Features on the GOD-Wiki and DIR-Wiki, Distinct
      Subjects Have Different Data Dimensions
  Table 3 caption:
    table_text: TABLE III Detailed BraVL Architecture. On DIR-Wiki, d b db is Equal
      to 1401, 1616 and 1355 for Subjects 1, 2 and 3, Respectively. On GOD-Wiki, d
      b db is Equal to 726, 945, 1034, 998 and 1009 for Subjects 1, 2, 3, 4 and 5,
      Respectively
  Table 4 caption:
    table_text: TABLE IV Neural Decoding Accuracy (%) of Several Methods That are
      Trained With the Visual (V), Textual (T) and Combined (V&T) Features, Respectively.
      Denotes the BraVL's Performance is Significantly Better Than the Compared Method
      (Paired T-Test, p < <0.05)
  Table 5 caption:
    table_text: TABLE V Compare Wiki Articles and Class Names as Textual Inputs. The
      Results are Averaged Across Subjects. Denotes Wiki Articles are Significantly
      Better Than Class Names (Paired T-Test, p < <0.05)
  Table 6 caption:
    table_text: TABLE VI Ablation Study of MI Regularizers. Indicates That Completed
      BraVL is Significantly Better Than the Compared One (Paired T-Test, p < <0.05)
  Table 7 caption:
    table_text: TABLE VII Ablation Results of Joint Posterior Approximation. Indicates
      That MoPoE is Significantly Better Than the Compared One (Paired T-Test, p <
      <0.05)
  Table 8 caption:
    table_text: TABLE VIII Intra-Subject and Inter-Subject Decoding Accuracy (%) of
      the Proposed BraVL Method Trained on the ThingsEEG-Wiki Dataset With the Visual
      (V), Textual (T) and Combined (V&T) Features, Respectively. 50200 50200-Way
      Decoding Means Predicting the Correct Class Out of 50200 50200 Novel Classes.
      V ft ft Means the Visual Feature Extractor (I.e., CORnet-S) Was Fine-Tuned on
      the Training Images of ThingsEEG-Text With 1654 Classes
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3263181
- Affiliation of the first author: information systems technology and design pillar,
    singapore university of technology and design, singapore
  Affiliation of the last author: information systems technology and design pillar,
    singapore university of technology and design, singapore
  Figure 1 Link: articels_figures_by_rev_year\2023\GradMDM_Adversarial_Attack_on_Dynamic_Networks\figure_1.jpg
  Figure 1 caption: 'A: Plot showing the trade-off between the two optimization objectives.
    MSE denotes the value of LtextMSE . B: Visualization of loss contours of two gating
    values, where gradients are represented by arrows. C: Visualization of loss contours
    with different alpha for mathcal LtextP . Higher alpha leads to larger deformation
    of the contour lines, with the gradient arrows pointing away from the dotted line.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\GradMDM_Adversarial_Attack_on_Dynamic_Networks\figure_2.jpg
  Figure 2 caption: 'A: Visualization of various gating values when optimizing using
    (6) to attack SkipNet. B: Illustration of gradient conflicts. (Top left) Optimizing
    according to (5). (Bottom Left) Optimizing according to (6). (Right) Applying
    our CGM according to (12). C: Visualization of the increased number of activated
    gates at lower MSE values using our GradMDM.'
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Jianhong Pan
  Name of the last author: Jun Liu
  Number of Figures: 2
  Number of Tables: 14
  Number of authors: 7
  Paper title: 'GradMDM: Adversarial Attack on Dynamic Networks'
  Publication Date: 2023-03-31 00:00:00
  Table 1 caption:
    table_text: TABLE I Performance Comparison of Dynamic Depth Networks on ImageNet
  Table 10 caption:
    table_text: TABLE X Attack Performance Comparison of Other Dynamic Networks on
      ImageNet
  Table 2 caption:
    table_text: TABLE II Performance Comparison of Dynamic Depth Networks on CIFAR-10
  Table 3 caption:
    table_text: TABLE III Performance Comparison of ManiDP on CIFAR-10 and ImageNet
  Table 4 caption:
    table_text: TABLE IV Ablation of Individual Components of GradMDM. PL and CGM
      Denote Power Loss and Complexity Gradient Masking, Respectively. GradMDM Uses
      the Combination of Both PL and CGM
  Table 5 caption:
    table_text: TABLE V Efficiency Comparison of Different Methods on ImageNet
  Table 6 caption:
    table_text: "TABLE VI Ablation of the Different Settings of \u03B3 \u03B3. The\
      \ Higher the Value of \u03B3 \u03B3, the More Weight Will Be Given to the Imperceptibility\
      \ Loss in Comparison to the Complexity Loss"
  Table 7 caption:
    table_text: TABLE VII Classification Accuracy on ImageNet After the Attack
  Table 8 caption:
    table_text: "TABLE VIII Ablation of the Different Settings of \u03B1 \u03B1. The\
      \ Higher the Value of \u03B1 \u03B1, the Larger the Deformation of the Contour\
      \ Lines in the Power Loss"
  Table 9 caption:
    table_text: TABLE IX Performance Comparison Against Other Baselines
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3263619
- Affiliation of the first author: audeering gmbh, gilching, germany
  Affiliation of the last author: audeering gmbh, gilching, germany
  Figure 1 Link: articels_figures_by_rev_year\2023\Dawn_of_the_Transformer_Era_in_Speech_Emotion_Recognition_Closing_the_Valence_Ga\figure_1.jpg
  Figure 1 caption: Proposed architecture built on wav2vec 2.0 HuBERT.
  Figure 10 Link: articels_figures_by_rev_year\2023\Dawn_of_the_Transformer_Era_in_Speech_Emotion_Recognition_Closing_the_Valence_Ga\figure_10.jpg
  Figure 10 caption: "Visualization of embeddings extracted with different models\
    \ overlayed with meta information for a combined dataset of MSP-Podcast and IEMOCAP.\
    \ We observe that the latent space of wav2vec 2.0 offers a better abstraction\
    \ from domain, gender, and speaker compared to the CNN14 baseline \u2013 even\
    \ without pre-training. However, only a pre-trained model is able to separate\
    \ low from high valence. To reduce the dimensionality of the latent space, we\
    \ applied T-SNE [63]."
  Figure 2 Link: articels_figures_by_rev_year\2023\Dawn_of_the_Transformer_Era_in_Speech_Emotion_Recognition_Closing_the_Valence_Ga\figure_2.jpg
  Figure 2 caption: CCC scores for arousal, dominance, valence (MSP-Podcast IEMOCAP),
    and sentiment (MOSI). All models have been trained for emotional dimension prediction
    using multitasking only on MSP-Podcast, and subsequently evaluated on its test
    set (in-domain), as well as to the test set of MOSI and the entire IEMOCAP dataset
    (cross-corpus).
  Figure 3 Link: articels_figures_by_rev_year\2023\Dawn_of_the_Transformer_Era_in_Speech_Emotion_Recognition_Closing_the_Valence_Ga\figure_3.jpg
  Figure 3 caption: Text and audio fusion results for arousal, dominance, and valence
    prediction on MSP-Podcast. Embeddings from the already fine-tuned models are concatenated
    with BERT embeddings extracted from automatic transcriptions, whereupon a two-layer
    feed-forward neural network is trained. We show the difference to results with
    the fine-tuned (ft) models from Fig. 2.
  Figure 4 Link: articels_figures_by_rev_year\2023\Dawn_of_the_Transformer_Era_in_Speech_Emotion_Recognition_Closing_the_Valence_Ga\figure_4.jpg
  Figure 4 caption: CCC performance for valence on the original and synthetic files
    on MSP-Podcast. We see that models with a high performance on the original files
    are more sensitive to sentiment (cf. left and center section). To prove that a
    fine-tuning of the transformer layers is required to learn linguistic content,
    we additionally show the correlation for models where the transformer layers were
    frozen (frz) during training (cf. Section IV-D).
  Figure 5 Link: articels_figures_by_rev_year\2023\Dawn_of_the_Transformer_Era_in_Speech_Emotion_Recognition_Closing_the_Valence_Ga\figure_5.jpg
  Figure 5 caption: Difference of fine-tuned (ft) to frozen (frz) CCC performance
    for arousal, dominance, and valence prediction on MSP-Podcast. The fine-tuned
    results are from Fig. 2, where transformer and output layers are jointly trained.
    For the frozen results, we keep all transformer layers frozen and simply train
    the output head. Results show that fine-tuning the transformer layer is worth
    the computational cost it incurs.
  Figure 6 Link: articels_figures_by_rev_year\2023\Dawn_of_the_Transformer_Era_in_Speech_Emotion_Recognition_Closing_the_Valence_Ga\figure_6.jpg
  Figure 6 caption: CCC scores for arousal, dominance, valence (MSP-PodcastIEMOCAP),
    and sentiment (MOSI) when augmenting the test data. The scores are averaged over
    all eleven different augmented versions of the test data.
  Figure 7 Link: articels_figures_by_rev_year\2023\Dawn_of_the_Transformer_Era_in_Speech_Emotion_Recognition_Closing_the_Valence_Ga\figure_7.jpg
  Figure 7 caption: Gender fairness scores for arousal, dominance, valence (MSP-Podcast
    IEMOCAP), and sentiment (MOSI). The gender fairness score is given by textCCCtextfemale
    - textCCCtextmale . A positive value indicates that the model under test performs
    better for female speaker and a negative value that it performs better for male
    speaker. A model with desired equal performance would have a gender fairness score
    of 0.
  Figure 8 Link: articels_figures_by_rev_year\2023\Dawn_of_the_Transformer_Era_in_Speech_Emotion_Recognition_Closing_the_Valence_Ga\figure_8.jpg
  Figure 8 caption: Speaker-level performance (CCC) on MSP-Podcast for the different
    models. We only use speakers with at least 200 test set samples for robust CCC
    estimates. All models show low CCC for at least one speaker on all 3 tasks. Speakers
    have been ordered according to the mean CCC over all dimensions and models.
  Figure 9 Link: articels_figures_by_rev_year\2023\Dawn_of_the_Transformer_Era_in_Speech_Emotion_Recognition_Closing_the_Valence_Ga\figure_9.jpg
  Figure 9 caption: CCC performance of randomly-initialized wav2vec 2.0 model (w2v2-L-wo-pretrain)
    on in-domain and cross-corpus arousal, dominance, valence sentiment prediction.
    We compare the performance with that of CNN14 and w2v2-L-robust. We observe that
    valence and sentiment benefit massively from pre-training, without which wav2vec
    2.0 performs worse than a classic CNN approach.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Johannes Wagner
  Name of the last author: "Bj\xF6rn W. Schuller"
  Number of Figures: 13
  Number of Tables: 3
  Number of authors: 7
  Paper title: 'Dawn of the Transformer Era in Speech Emotion Recognition: Closing
    the Valence Gap'
  Publication Date: 2023-03-31 00:00:00
  Table 1 caption:
    table_text: TABLE I State-of-The-Art 4-Class Emotion Recognition Performance on
      IEMOCAP Using Transformer-Based Architectures Ranked by unweighted average recall
      (UAR) weighted average recall (WAR). The Table Encodes Whether the Base (b)
      or Large (L) Architecture Was Used as Well as Whether the Pre-Trained Model
      Was Fine-Tuned for Speech Recognition (FT-SR). The Column FT-D Marks If the
      Transformer Layers Were Further Fine-Tuned During the Down-Stream Classification
      Task
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Transformer-Based Models Included in This Study and Details
      on the Data Used During Pre-Training. Models Comprised of Two Architecture Designs
      (wav2vec 2.0 and HuBERT), Each With Two Different Variants (Base and Large).
      For Each Model, We List Included dataset(s), Total Number of Hours (h), Number
      of Languages (eng If Only English), and Covered Domains (Read Speech, Telephone
      Conversions, Parliamentary Speech, Youtube)
  Table 3 caption:
    table_text: TABLE III Change in CCC for w2v2-L-robust Predictions on Augmented
      Data Compared to Its Predictions on Clean Data
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3263585
- Affiliation of the first author: department of automation, beijing national research
    center for information science and technology (bnrist), tsinghua university, beijing,
    china
  Affiliation of the last author: department of automation, beijing national research
    center for information science and technology (bnrist), tsinghua university, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2023\DepthGuided_Optimization_of_Neural_Radiance_Fields_for_Indoor_MultiView_Stereo\figure_1.jpg
  Figure 1 caption: Qualitative results for multi-view depth estimation on ScanNet
    [8]. Our method clearly surpasses leading multi-view estimation methods [36],
    [42] by building on top of neural radiance fields [40]. While also using test-time
    optimization, CVD [36] suffers from inaccurate estimation of flow correspondences.
    NeRF [40] fails to produce accurate geometry due to the inherent shape-radiance
    ambiguity [79] (See Fig. 3) in indoor scenes.With guided optimization, our method
    successfully integrates the learning-based depth priors into NeRF, significantly
    improving the geometry of the radiance fields.
  Figure 10 Link: articels_figures_by_rev_year\2023\DepthGuided_Optimization_of_Neural_Radiance_Fields_for_Indoor_MultiView_Stereo\figure_10.jpg
  Figure 10 caption: Results on view synthesis. The top two rows are rendering results
    on seen (training) views while the bottom two rows are on the novel views. With
    the adapted depth priors, our method improves the rendering quality for both seen
    and novel views. Better viewed when zoomed in.
  Figure 2 Link: articels_figures_by_rev_year\2023\DepthGuided_Optimization_of_Neural_Radiance_Fields_for_Indoor_MultiView_Stereo\figure_2.jpg
  Figure 2 caption: An overview of our method. First, We adopt conventional COLMAP
    to get sparse depth (after fusion), which is used to train a monocular depth network
    to get scene-specific depth priors. Then, we utilize the depth priors to guide
    volume sampling in the optimization of NeRF [40]. Finally, by computing the errors
    between the rendered images and the original input images we acquire confidence
    scores, which enables us to employ a confidence-based filter to improve the rendered
    depths.
  Figure 3 Link: articels_figures_by_rev_year\2023\DepthGuided_Optimization_of_Neural_Radiance_Fields_for_Indoor_MultiView_Stereo\figure_3.jpg
  Figure 3 caption: 'The inherent shape-radiance ambiguity [79] becomes a bottleneck
    in indoor scenes. Top row: (a) rendered RGB of NeRF [40]. (b) visualization of
    the sampled points along the camera ray at the position colored in red. The blue
    line indicates the groundtruth depth value. Bottom row: (c) the rendered depth
    map of NeRF [40]. (d) the groundtruth depth map. While NeRF produces high quality
    rendered image (PSNR: 31.53), the rendered depth largely deviates from the groundtruth.'
  Figure 4 Link: articels_figures_by_rev_year\2023\DepthGuided_Optimization_of_Neural_Radiance_Fields_for_Indoor_MultiView_Stereo\figure_4.jpg
  Figure 4 caption: Guided optimization of NeRF [40]. We adopt multi-view consistency
    check on adapted depth priors to get error maps, which help calculate adaptive
    depth ranges for each camera ray to sample points for NeRF optimization.
  Figure 5 Link: articels_figures_by_rev_year\2023\DepthGuided_Optimization_of_Neural_Radiance_Fields_for_Indoor_MultiView_Stereo\figure_5.jpg
  Figure 5 caption: The proposed coarse-to-fine strategy to acquire the depth priors
    with SfM reconstruction. We first train a coarse network supervised by sparse
    SfM points. Then, we can get a semi-dense point cloud with the coarse depth priors
    by depth fusion. The masked coarse depths are used to train the fine network.
  Figure 6 Link: articels_figures_by_rev_year\2023\DepthGuided_Optimization_of_Neural_Radiance_Fields_for_Indoor_MultiView_Stereo\figure_6.jpg
  Figure 6 caption: Illustration of two sampling methods. Compared with truncated
    uniform sampling, Gaussian sampling focuses on surface points and has the possibility
    to sample around regions that are away from the mean.
  Figure 7 Link: articels_figures_by_rev_year\2023\DepthGuided_Optimization_of_Neural_Radiance_Fields_for_Indoor_MultiView_Stereo\figure_7.jpg
  Figure 7 caption: Qualitative comparisons on ScanNet [8] dataset. 'Ours' indicates
    NerfingMVS++. Our method, without the post-filtering step, outperforms all compared
    methods in terms of depth quality. The filter further smooths the per-pixel estimated
    depth maps. Better viewed when zoomed in.
  Figure 8 Link: articels_figures_by_rev_year\2023\DepthGuided_Optimization_of_Neural_Radiance_Fields_for_Indoor_MultiView_Stereo\figure_8.jpg
  Figure 8 caption: Qualitative comparisons on NYU Depth V2 [59] dataset. 'Ours' indicates
    NerfingMVS++. Our method, without the post-filtering step, outperforms all compared
    methods in terms of depth quality. The filter further smooths the per-pixel estimated
    depth maps. Better viewed when zoomed in.
  Figure 9 Link: articels_figures_by_rev_year\2023\DepthGuided_Optimization_of_Neural_Radiance_Fields_for_Indoor_MultiView_Stereo\figure_9.jpg
  Figure 9 caption: The optimization of CVD [36] surprisingly degrades the quality
    of the depth priors due to unreliable flow correspondences, while our method achieves
    improvement with guided optimization of NeRF [40].
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Yi Wei
  Name of the last author: Jiwen Lu
  Number of Figures: 10
  Number of Tables: 10
  Number of authors: 4
  Paper title: Depth-Guided Optimization of Neural Radiance Fields for Indoor Multi-View
    Stereo
  Publication Date: 2023-03-31 00:00:00
  Table 1 caption:
    table_text: TABLE I Quantitative Comparisons for Multi-View Depth Estimation.
      The 'time' Indicates Averaged Per-Scene Time Cost, Which are Evaluated on a
      Single RTX 2080 Ti. Scores are Averaged Over 8 Scenes From ScanNet
  Table 10 caption:
    table_text: TABLE X Quantitative Comparisons for Novel View Synthesis. For All
      Methods, We Used COLMAP [56], [57] to Calculate Camera Poses. Numbers in Bold
      are Within 1% of the Best
  Table 2 caption:
    table_text: TABLE II Time Analysis of Each Module in the Proposed Method. The
      Time is the Average Training Time Over 8 Scenes on a Single RTX 2080 Ti
  Table 3 caption:
    table_text: TABLE III Comparison of the Effectiveness of Test-Time Optimization
      Between CVD [36] and Our Method. Both Methods Perform Optimization Over the
      Adapted Depth Priors, Which is Acquired by Training the Mannequin Challenge
      Depth network [29] With the Supervision From MVS Reconstruction [56], [57].
      Scores are Averaged Over 8 Scenes
  Table 4 caption:
    table_text: TABLE IV Quantitative Comparisons for Multi-View Depth Estimation.
      Scores are Averaged Over 8 Scenes From NYU Depth V2
  Table 5 caption:
    table_text: TABLE V Ablation Studies on Each Component of Our System. For the
      Experiments 'NeRF + Filter' and 'depth Priors + Filter,' We Compute the Confidence
      Scores by Using the Relative Errors Between the Prediction Depths and the Groundtruth.
      Results are Averaged Over 8 Scenes From ScanNet
  Table 6 caption:
    table_text: "TABLE VI Ablation Studies on the Design of the Proposed Guided Optimization\
      \ With Adaptive Ranges. 'bound' Denotes the Use of \u03B1 l \u03B1l and \u03B1\
      \ h \u03B1h in (6). For the Experiment Without Using Adaptive Depth Ranges for\
      \ Each Camera Ray, We Set a Fixed Relative Depth Range to [0.9,1.1] [0.9,1.1].\
      \ The Experiment Was Conducted Over 8 Scenes From ScanNet"
  Table 7 caption:
    table_text: TABLE VII Ablation Studies on the Sampling Distribution of Guided
      Optimization. Results are Averaged Over 8 Scenes From ScanNet
  Table 8 caption:
    table_text: TABLE VIII Ablation Studies on the Training Methods for Depth Priors.
      'SfM' Means We Directly Use Sparse SfM Points to Train the Monocular Depth Network.
      'MVS' is the Method Used in NerfingMVS. The First Three Lines Show the Accuracy
      of Depth Priors While Last Three Lines Show the Accuracy of Final Predicted
      Depths (After Post-Process Filtering). Results are Averaged Over 8 Scenes From
      ScanNet
  Table 9 caption:
    table_text: TABLE IX Comparison Between NeRF [40] and Our Method on Seen Views.
      Results are Averaged Over 8 Scenes From ScanNet. The Guided Optimization Helps
      NeRF to Focus on More Informative Regions and Improves its Capacity
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3263464
