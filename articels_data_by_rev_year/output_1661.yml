- Affiliation of the first author: school of computer science and engineering, southeast
    university, nanjing, china
  Affiliation of the last author: school of computer science and engineering, southeast
    university, nanjing, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Partial_MultiLabel_Learning_via_Credible_Label_Elicitation\figure_1.jpg
  Figure 1 caption: An exemplary partial multi-label learning scenario. In crowdsourcing
    image tagging, among the set of 7 candidate labels given by crowdsourcing annotators,
    only 4 of them are valid ones including house, tree, lavender, and France.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Partial_MultiLabel_Learning_via_Credible_Label_Elicitation\figure_2.jpg
  Figure 2 caption: Comparison of Particle-Vls (control approach) against other learning
    approaches with the Bonferroni-Dunn test. Approaches not connected with Particle-Vls
    in the CD diagram are considered to have significantly different performance from
    the control approach (CD=1.0867 at 0.05 significance level).
  Figure 3 Link: articels_figures_by_rev_year\2020\Partial_MultiLabel_Learning_via_Credible_Label_Elicitation\figure_3.jpg
  Figure 3 caption: Comparison of Particle-Map (control approach) against other learning
    approaches with the Bonferroni-Dunn test. Approaches not connected with Particle-Map
    in the CD diagram are considered to have significantly different performance from
    the control approach (CD=1.0867 at 0.05 significance level).
  Figure 4 Link: articels_figures_by_rev_year\2020\Partial_MultiLabel_Learning_via_Credible_Label_Elicitation\figure_4.jpg
  Figure 4 caption: 'Properties of Particle-Vls and Particle-Map change as parameter
    thr varies from 1 to 0.5 with an interval of 0.1. First row: the predictive performance
    of Particle-Vls on ranking loss, coverage, and average precision; Second row:
    the predictive performance of Particle-Map on ranking loss, coverage, and average
    precision; Third row: the mean size of credible labels elicited by Particle.'
  Figure 5 Link: articels_figures_by_rev_year\2020\Partial_MultiLabel_Learning_via_Credible_Label_Elicitation\figure_5.jpg
  Figure 5 caption: Running time (traintest) of each learning approach on the four
    real-world PML data sets (for histogram illustration, the y -axis corresponds
    to the value of log t+2 with t being the running time measured in seconds).
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.72
  Name of the first author: Min-Ling Zhang
  Name of the last author: Jun-Peng Fang
  Number of Figures: 5
  Number of Tables: 10
  Number of authors: 2
  Paper title: Partial Multi-Label Learning via Credible Label Elicitation
  Publication Date: 2020-04-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Pseudo-Code of Particle
  Table 10 caption:
    table_text: TABLE 10 Summary of Algorithmic Complexity of Each Learning Approach
  Table 2 caption:
    table_text: TABLE 2 Characteristics of the PML Experimental Data Sets
  Table 3 caption:
    table_text: TABLE 3 Experimental Results of Each Learning Approach on Synthetic
      as well as Real-World PML Data Sets in Terms of Hamming Loss, Where the Best
      Performance (the smaller the better) is Shown in Bold Face
  Table 4 caption:
    table_text: TABLE 4 Experimental Results of Each Learning Approach on Synthetic
      as well as Real-World PML Data Sets in Terms of Ranking Loss, Where the Best
      Performance (the smaller the better) is Shown in Bold Face
  Table 5 caption:
    table_text: TABLE 5 Experimental Results of Each Learning Approach on Synthetic
      as well as Real-World PML Data Sets in Terms of One-Error, Where the Best Performance
      (the smaller the better) is Shown in Bold Face
  Table 6 caption:
    table_text: TABLE 6 Experimental Results of Each Learning Approach on Synthetic
      as well as Real-World PML Data Sets in Terms of Coverage, Where the Best Performance
      (the smaller the better) is Shown in Bold Face
  Table 7 caption:
    table_text: TABLE 7 Experimental Results of Each Learning Approach on Synthetic
      as well as Real-World PML Data Sets in Terms of Average Precision, Where the
      Best Performance (the larger the better) is Shown in Bold Face
  Table 8 caption:
    table_text: TABLE 8 Summary of the Friedman Statistics F F F F in Terms of Each
      Evaluation Metric and the Critical Value at 0.05 Significance Level for Particle-Vls
      ( Learning Approaches n=7 n = 7 , Data Sets N=55 N = 55 )
  Table 9 caption:
    table_text: TABLE 9 Summary of the Friedman Statistics F F F F in Terms of Each
      Evaluation Metric and the Critical Value at 0.05 Significance Level for Particle-Map
      ( Learning Approaches n=7 n = 7 , Data Sets N=55 N = 55 )
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2985210
- Affiliation of the first author: research center for information technology innovation,
    academia sinica, taipei, taiwan
  Affiliation of the last author: department of electrical and computer engineering,
    virginia tech, blacksburg, va, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Show_Match_and_Segment_Joint_Weakly_Supervised_Learning_of_Semantic_Matching_and\figure_1.jpg
  Figure 1 caption: Joint Semantic Matching and Object Co-Segmentation. Semantic matching
    and object co-segmentation are two highly correlated tasks. However, existing
    methods often tackle these two tasks in isolation. In this paper, we exploit the
    complementary nature of the two tasks and propose a cross-task consistency loss
    to couple the learning of the two tasks. By leveraging cross-task information,
    our algorithm produces more accurate and consistent results on both tasks.
  Figure 10 Link: articels_figures_by_rev_year\2020\Show_Match_and_Segment_Joint_Weakly_Supervised_Learning_of_Semantic_Matching_and\figure_10.jpg
  Figure 10 caption: Sensitivity analysis of hyper-parameters. The performance of
    our approach remains stable when the weights for the five loss terms are within
    a reasonable range.
  Figure 2 Link: articels_figures_by_rev_year\2020\Show_Match_and_Segment_Joint_Weakly_Supervised_Learning_of_Semantic_Matching_and\figure_2.jpg
  Figure 2 caption: Separate Learning versus Joint Learning. Addressing semantic matching
    (left) or object co-segmentation (right) in isolation often suffers from the effect
    of background clutters (for semantic matching) or only focuses on segmenting the
    discriminative parts (for object co-segmentation) In this work, we exploit the
    property that the predicted object masks allow the model to suppress the negative
    impact due to background clutters while the estimated dense correspondence fields
    provide supervision for object co-segmentation. We couple the learning of both
    tasks through a cross-network consistency loss and show that joint learning improves
    the performance of both tasks.
  Figure 3 Link: articels_figures_by_rev_year\2020\Show_Match_and_Segment_Joint_Weakly_Supervised_Learning_of_Semantic_Matching_and\figure_3.jpg
  Figure 3 caption: "Overview of the Proposed Model. Our model is a two-stream network:\
    \ (top) semantic matching network and (bottom) object co-segmentation network.\
    \ Our model consists of four main CNN sub-networks: an encoder E (for extracting\
    \ features from the input images), a transformation predictor G (for estimating\
    \ the geometric transformation between an input image pair), a decoder D (for\
    \ producing object masks), and an ImageNet-pretrained and fixed ResNet-50 feature\
    \ extractor F (for computing the perceptual contrastive loss). The model training\
    \ is driven by four loss functions, including the foreground-guided matching loss\
    \ L matching , forward-backward consistency loss L cycle\u2212consis , perceptual\
    \ contrastive loss L contrast , and cross-network consistency loss L task\u2212\
    consis ."
  Figure 4 Link: articels_figures_by_rev_year\2020\Show_Match_and_Segment_Joint_Weakly_Supervised_Learning_of_Semantic_Matching_and\figure_4.jpg
  Figure 4 caption: "Illustration of the Perceptual Contrastive Loss L contrast .\
    \ The proposed perceptual contrastive loss is developed based on two criteria:\
    \ (1) low inter-image foreground object distinctness ( d + AB ) and (2) high intra-image\
    \ foreground-background discrepancy ( d \u2212 AB )."
  Figure 5 Link: articels_figures_by_rev_year\2020\Show_Match_and_Segment_Joint_Weakly_Supervised_Learning_of_Semantic_Matching_and\figure_5.jpg
  Figure 5 caption: Visual comparisons of object co-segmentation on the TSS dataset
    [6].
  Figure 6 Link: articels_figures_by_rev_year\2020\Show_Match_and_Segment_Joint_Weakly_Supervised_Learning_of_Semantic_Matching_and\figure_6.jpg
  Figure 6 caption: The advantage of joint semantic matching and object co-segmentation.
    We present four examples from the TSS dataset [6]. Integrating semantic matching
    with object co-segmentation helps improve the quality of co-segmentation.
  Figure 7 Link: articels_figures_by_rev_year\2020\Show_Match_and_Segment_Joint_Weakly_Supervised_Learning_of_Semantic_Matching_and\figure_7.jpg
  Figure 7 caption: Qualitative results of object co-segmentation on the Internet
    [23] dataset. Our method is capable of delineating accurate co-occurring object
    masks under large intra-class variations and background clutter.
  Figure 8 Link: articels_figures_by_rev_year\2020\Show_Match_and_Segment_Joint_Weakly_Supervised_Learning_of_Semantic_Matching_and\figure_8.jpg
  Figure 8 caption: Qualitative results of semantic matching. We present the qualitative
    comparisons of semantic matching with the state-of-the-art algorithms on the PF-PASCAL
    [7] top row) and PF-WILLOW [7] (bottom row) datasets.
  Figure 9 Link: articels_figures_by_rev_year\2020\Show_Match_and_Segment_Joint_Weakly_Supervised_Learning_of_Semantic_Matching_and\figure_9.jpg
  Figure 9 caption: Sensitivity analysis of the cutoff threshold m on object co- segmentation
    on the TSS dataset [6]. The performance of our approach remains stable when the
    cutoff threshold lies within a reasonable range.
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.61
  Name of the first author: Yun-Chun Chen
  Name of the last author: Jia-Bin Huang
  Number of Figures: 10
  Number of Tables: 9
  Number of authors: 4
  Paper title: 'Show, Match and Segment: Joint Weakly Supervised Learning of Semantic
    Matching and Object Co-Segmentation'
  Publication Date: 2020-04-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Semantic Matching Results on the TSS Dataset [6]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Experimental Results of Object Co-Segmentation on the TSS
      Dataset [6]
  Table 3 caption:
    table_text: TABLE 3 Experimental Results of Object Co-Segmentation on the Internet
      Dataset [23]
  Table 4 caption:
    table_text: TABLE 4 Experimental Results of Semantic Matching on the PF-PASCAL
      Dataset [7]
  Table 5 caption:
    table_text: TABLE 5 Experimental Results of Semantic Matching on the PF-WILLOW
      Dataset [7]
  Table 6 caption:
    table_text: TABLE 6 Experimental Results of Semantic Matching on the SPair-71k
      Dataset [24]
  Table 7 caption:
    table_text: TABLE 7 Ablation Studies of Object Co-Segmentation on the Internet
      Dataset [23]
  Table 8 caption:
    table_text: "TABLE 8 Ablation Study of Semantic Matching on the PF-WILLOW Dataset\
      \ [7] Under Three Different PCK Thresholds \u03B1 \u03B1"
  Table 9 caption:
    table_text: TABLE 9 Ablation Study of Object Co-Segmentation on the TSS Dataset
      [6] Using Different Post-Processing Methods
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2985395
- Affiliation of the first author: department of computational modeling, polytechnic
    institute, rio de janeiro state university, nova friburgo, rj, brazil
  Affiliation of the last author: school of engineering, brown university, providence,
    ri, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Camera_Pose_Estimation_Using_FirstOrder_Curve_Differential_Geometry\figure_1.jpg
  Figure 1 caption: (a) Widely separated views may not have enough keypoints in common,
    but share curve structure. (b) There may not be sufficient keypoints matching
    views of homogeneous objects such as a sculpture, but there is sufficient curve
    structure. (c) Each moving object requires its own set of features, demanding
    rich texture. (d) Non-rigid scenes face the same issue.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Camera_Pose_Estimation_Using_FirstOrder_Curve_Differential_Geometry\figure_2.jpg
  Figure 2 caption: 'Challenges in using curve fragments in multiview geometry: (a)
    instability with changes in viewpoint (b), zoom (c-h): a curve in one view broken
    in another, or linked to background, or becomes absent; a fragmentation at junctions
    gets linked in another view; different parts occluded in different views, and
    undergoing deformation from one view to the other. (i) Correspondence ambiguity
    along the curve.'
  Figure 3 Link: articels_figures_by_rev_year\2020\Camera_Pose_Estimation_Using_FirstOrder_Curve_Differential_Geometry\figure_3.jpg
  Figure 3 caption: Determining camera pose R , T given space curves in a world coordinate
    system and their projections in an image coordinate system (left); and an approach
    consisting of determining pose given 3D point-tangents (local curve models) their
    projections (right).
  Figure 4 Link: articels_figures_by_rev_year\2020\Camera_Pose_Estimation_Using_FirstOrder_Curve_Differential_Geometry\figure_4.jpg
  Figure 4 caption: Epipolar tangencies in curve-based relative pose estimation. a.
    An epipolar line on the left must correspond to the epipolar line on the right
    having tangency on the corresponding curve. This works for both static curves
    and occluding contours. b. Epipolar tangencies updated differentially through
    curvature.
  Figure 5 Link: articels_figures_by_rev_year\2020\Camera_Pose_Estimation_Using_FirstOrder_Curve_Differential_Geometry\figure_5.jpg
  Figure 5 caption: "Diagram of the mutual intersection of Eqs. (3.7)\u2013(3.9) in\
    \ the x 1 \u2013 x 2 plane."
  Figure 6 Link: articels_figures_by_rev_year\2020\Camera_Pose_Estimation_Using_FirstOrder_Curve_Differential_Geometry\figure_6.jpg
  Figure 6 caption: A parametrization of the ellipse by a parameter t .
  Figure 7 Link: articels_figures_by_rev_year\2020\Camera_Pose_Estimation_Using_FirstOrder_Curve_Differential_Geometry\figure_7.jpg
  Figure 7 caption: Our synthetic dataset comprises 100 random views of analytic space
    curves on a near-spherical configuration (top), at human scale. Sample views and
    3D curves are shown in the bottom row for stereo viewing with parallel eyes (left
    pair) or with crosed eyes (right pair).
  Figure 8 Link: articels_figures_by_rev_year\2020\Camera_Pose_Estimation_Using_FirstOrder_Curve_Differential_Geometry\figure_8.jpg
  Figure 8 caption: Distributions of reprojection (top row), rotation (middle), and
    translation errors (bottom), for using synthetic data of Fig. 7, without bundle
    adjustment (left column) and after bundle adjustment (right), for levels of positional
    ( x axes) and tangential noise (colors). The proposed P2PT stably matches ground
    truth and P3P, with only two correspondences. The rotation and translation errors
    on the right column are close to zero.
  Figure 9 Link: articels_figures_by_rev_year\2020\Camera_Pose_Estimation_Using_FirstOrder_Curve_Differential_Geometry\figure_9.jpg
  Figure 9 caption: The reprojection error distribution for real data (Capitol sequence
    and Middlebury Dino) using only two point-tangents.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ricardo Fabbri
  Name of the last author: Benjamin Kimia
  Number of Figures: 9
  Number of Tables: 1
  Number of authors: 3
  Paper title: Camera Pose Estimation Using First-Order Curve Differential Geometry
  Publication Date: 2020-04-06 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 The Number of 3D\u20132D Correspondences Needed to Solve\
      \ for Camera Pose and Intrinsic Parameters"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2985310
- Affiliation of the first author: mechanical and industrial engineering, university
    of illinois at chicago, chicago, il, usa
  Affiliation of the last author: mechanical and industrial engineering, university
    of illinois at chicago, chicago, il, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Adversarial_Attacks_on_Time_Series\figure_1.jpg
  Figure 1 caption: The top diagram shows the methodology of training the model distillation
    used in the white-box and black-box attacks. The bottom diagram is the methodology
    utilized to attack a time series classifier.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Adversarial_Attacks_on_Time_Series\figure_2.jpg
  Figure 2 caption: White-box attack on 1-NN DTW that is trained on all 42 datasets.
  Figure 3 Link: articels_figures_by_rev_year\2020\Adversarial_Attacks_on_Time_Series\figure_3.jpg
  Figure 3 caption: White-box attack on FCN that is trained on all 42 datasets.
  Figure 4 Link: articels_figures_by_rev_year\2020\Adversarial_Attacks_on_Time_Series\figure_4.jpg
  Figure 4 caption: Black-box attack on 1-NN DTW that is trained on all 42 datasets.
  Figure 5 Link: articels_figures_by_rev_year\2020\Adversarial_Attacks_on_Time_Series\figure_5.jpg
  Figure 5 caption: Black-box attack on FCN that is trained on all 42 datasets.
  Figure 6 Link: articels_figures_by_rev_year\2020\Adversarial_Attacks_on_Time_Series\figure_6.jpg
  Figure 6 caption: "A sample black-box and white-box attack on an FCN and 1-NN DTW\
    \ classifier that is trained on the dataset \u201CFordB\u201D. The last row of\
    \ the figure depicts the nearest neighbor of the original and adversarial time\
    \ series."
  Figure 7 Link: articels_figures_by_rev_year\2020\Adversarial_Attacks_on_Time_Series\figure_7.jpg
  Figure 7 caption: Black-box and white-box attacks on FCN and 1-NN DTW classifiers
    that are tested on Dtest without any retraining.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Fazle Karim
  Name of the last author: Houshang Darabi
  Number of Figures: 7
  Number of Tables: 5
  Number of authors: 3
  Paper title: Adversarial Attacks on Time Series
  Publication Date: 2020-04-10 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Wilcoxson Signed-Rank Test Comparing the Fraction of Successful
      Adversarial Between the Different Attacks
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Wilcoxson Signed-Rank Test Comparing GATN With FGSM
  Table 3 caption:
    table_text: TABLE 3 Wilcoxson Signed-Rank Test Comparing Testing Accuracy of Various
      Adversarial Attacks on Initial Time Series Classifiers and Time Series Classifiers
      Trained With Adversarial Samples
  Table 4 caption:
    table_text: TABLE 4 Wilcoxson Signed-Rank Test Comparing Various Adversarial Attacks
      on Initial Time Series Classifiers and Time Series Classifiers Trained With
      Adversarial Samples
  Table 5 caption:
    table_text: TABLE 5 Wilcoxson Signed-Rank Test Comparing the MSE Between the Different
      Attacks
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2986319
- Affiliation of the first author: "kite\u2013toronto rehab\u2013university health\
    \ network, toronto, on, canada"
  Affiliation of the last author: "kite\u2013toronto rehab\u2013university health\
    \ network, toronto, on, canada"
  Figure 1 Link: articels_figures_by_rev_year\2020\Analysis_of_the_Hands_in_Egocentric_Vision_A_Survey\figure_1.jpg
  Figure 1 caption: Hand-based approaches in FPV categorized by amount of detail and
    semantic content.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Analysis_of_the_Hands_in_Egocentric_Vision_A_Survey\figure_2.jpg
  Figure 2 caption: 'Diagram of hand localization tasks in egocentric vision. Hand
    detection and segmentation have often been used in combination, for example to
    segment ROIs previously obtained with a hand detector, or to classify as hand
    or not hand previously segmented regions. Since they provide the global position
    of the hands within the frame, they are chosen as the basis for other localization
    approaches, such as hand identification, hand tracking, hand pose estimation,
    fingertip detection (: Hand identification is now typically incorporated within
    the hand detection step).'
  Figure 3 Link: articels_figures_by_rev_year\2020\Analysis_of_the_Hands_in_Egocentric_Vision_A_Survey\figure_3.jpg
  Figure 3 caption: Diagram of the hand interpretation areas in egocentric vision.
    Grasp analysis and gesture recognition focus directly on describing the hand.
    In actioninteraction and activity recognition, the hand is instrumental in describing
    the users behaviour.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.82
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Andrea Bandini
  Name of the last author: "Jos\xE9 Zariffa"
  Number of Figures: 3
  Number of Tables: 1
  Number of authors: 2
  Paper title: 'Analysis of the Hands in Egocentric Vision: A Survey'
  Publication Date: 2020-04-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 List of Available Datasets With Hand-Based Annotations in
      FPV
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2986648
- Affiliation of the first author: school of mathematics and statistics, southwest
    university, chongqing, china
  Affiliation of the last author: department of mathematics and statistics, university
    of ottawa, ottawa, on, canada
  Figure 1 Link: articels_figures_by_rev_year\2020\LowTubalRank_Plus_Sparse_Tensor_Recovery_With_Prior_Subspace_Information\figure_1.jpg
  Figure 1 caption: An example of face recognition using subspace prior information.
  Figure 10 Link: articels_figures_by_rev_year\2020\LowTubalRank_Plus_Sparse_Tensor_Recovery_With_Prior_Subspace_Information\figure_10.jpg
  Figure 10 caption: "Fraction of correct recoveries across 10 trials, as a function\
    \ of sparsity of E ( x -axis) and of rank t (L) ( y -axis). There exist three\
    \ sizes of L\u2208 R n 1 \xD7 n 2 \xD7 n 3 : (a) n 1 =100 , n 2 =50 , n 3 =3 ;\
    \ (b) n 1 =100 , n 2 =50 , n 3 =5 ; (c) n 1 =100 , n 2 =50 , n 3 =10 . In all\
    \ cases, we set r new =\u230A0.25r\u230B and r extra =\u230A0.25r\u230B ."
  Figure 2 Link: articels_figures_by_rev_year\2020\LowTubalRank_Plus_Sparse_Tensor_Recovery_With_Prior_Subspace_Information\figure_2.jpg
  Figure 2 caption: "An illustration of the t-SVD of an n 1 \xD7 n 2 \xD7 n 3 tensor."
  Figure 3 Link: articels_figures_by_rev_year\2020\LowTubalRank_Plus_Sparse_Tensor_Recovery_With_Prior_Subspace_Information\figure_3.jpg
  Figure 3 caption: 'An illustration of a tensor: red cubes represent non-zero elements
    and yellow cubes represent zero elements. (a) A low-tubal-rank tensor has sparsity.
    (b) A sparse tensor has low tubal rank.'
  Figure 4 Link: articels_figures_by_rev_year\2020\LowTubalRank_Plus_Sparse_Tensor_Recovery_With_Prior_Subspace_Information\figure_4.jpg
  Figure 4 caption: Convergence results of mod-TPCP. (a) Value of Chg at each step.
    (b) Values of RelErrorL,RelErrorE and RelErrorX at each step.
  Figure 5 Link: articels_figures_by_rev_year\2020\LowTubalRank_Plus_Sparse_Tensor_Recovery_With_Prior_Subspace_Information\figure_5.jpg
  Figure 5 caption: Comparison of logarithmic relative error (a) and tensor incoherence
    conditions (b) with varying the number of extra directions r extra .
  Figure 6 Link: articels_figures_by_rev_year\2020\LowTubalRank_Plus_Sparse_Tensor_Recovery_With_Prior_Subspace_Information\figure_6.jpg
  Figure 6 caption: Comparison of logarithmic relative error (a) and tensor incoherence
    conditions (b) with varying the number of new directions r new .
  Figure 7 Link: articels_figures_by_rev_year\2020\LowTubalRank_Plus_Sparse_Tensor_Recovery_With_Prior_Subspace_Information\figure_7.jpg
  Figure 7 caption: Comparison of logarithmic relative error (a) and tensor incoherence
    conditions (b) with varying the number of lateral slices n 2 .
  Figure 8 Link: articels_figures_by_rev_year\2020\LowTubalRank_Plus_Sparse_Tensor_Recovery_With_Prior_Subspace_Information\figure_8.jpg
  Figure 8 caption: Comparison of logarithmic relative error (a) and tensor incoherence
    conditions (b) with varying the number of frontal slices n 3 .
  Figure 9 Link: articels_figures_by_rev_year\2020\LowTubalRank_Plus_Sparse_Tensor_Recovery_With_Prior_Subspace_Information\figure_9.jpg
  Figure 9 caption: "Fraction of correct recoveries across 10 trials, as a function\
    \ of sparsity of E ( x -axis) and of rank t (L) ( y -axis). There exist three\
    \ sizes of L\u2208 R n 1 \xD7 n 2 \xD7 n 3 : (a) n 1 = n 2 =100 , n 3 =3 ; (b)\
    \ n 1 = n 2 =100 , n 3 =5 ; (c) n 1 = n 2 =100 , n 3 =10 . In all cases, we set\
    \ r new =\u230A0.25r\u230B and r extra =\u230A0.25r\u230B ."
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Feng Zhang
  Name of the last author: Chen Xu
  Number of Figures: 14
  Number of Tables: 4
  Number of authors: 4
  Paper title: Low-Tubal-Rank Plus Sparse Tensor Recovery With Prior Subspace Information
  Publication Date: 2020-04-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 PSNR and SSIM Results on 4 Test Videos With 10 percent Corrupted
      Ratios by Different Algorithms
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 PSNR and SSIM Results on 4 Test Videos With 20 percent Corrupted
      Ratios by Different Algorithms
  Table 3 caption:
    table_text: TABLE 3 PSNR and SSIM Results on 4 Test Videos With 30 percent Corrupted
      Ratios by Different Algorithms
  Table 4 caption:
    table_text: TABLE 4 PSNR and SSIM Results on 4 Test Videos With 40 percent Corrupted
      Ratios by Different Algorithms
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2986773
- Affiliation of the first author: department of electrical engineering, stanford
    university, stanford, usa
  Affiliation of the last author: department of electrical engineering, stanford university,
    stanford, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Neural_Sensors_Learning_Pixel_Exposures_for_HDR_Imaging_and_Video_Compressive_Se\figure_1.jpg
  Figure 1 caption: Illustration of our end-to-end neural sensor framework. The exposure
    program of a sensor (physical layer) is learned end-to-end with a decoder (digital
    layer) for applications like video compressive sensing. Here, we show a single
    coded exposure captured with our prototype camera and several frames of the high-speed
    video reconstructed from this image showing an exploding balloon.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Neural_Sensors_Learning_Pixel_Exposures_for_HDR_Imaging_and_Video_Compressive_Se\figure_2.jpg
  Figure 2 caption: "Diagram of the proposed neural sensor system.During the training\
    \ phase (top row) an electronic encoder\u2014spatially varying pixel exposures\
    \ parameterized by \u03D5 \u2014is jointly optimized with a digital decoder\u2014\
    a neural network parameterized by \u03C8 \u2014in simulation using an application-specific\
    \ loss function L . The optimized shutter functions are compiled into a pixel\
    \ code that is uploaded on the programmable sensor. During inference (bottom row),\
    \ the sensor captures images by applying these exposure functions and the pre-trained\
    \ network recovers the final image."
  Figure 3 Link: articels_figures_by_rev_year\2020\Neural_Sensors_Learning_Pixel_Exposures_for_HDR_Imaging_and_Video_Compressive_Se\figure_3.jpg
  Figure 3 caption: "A diagram illustrating different shutter functions. The top diagram\
    \ shows how we represent a shutter function: A solid line on a given slot n indicates\
    \ that for this slot: S i,j [t+n\u03B4t]=1 , otherwise S i,j [t+n\u03B4t]=0 .\
    \ The bottom rows show four classes of shutter functions (from the simplest to\
    \ the most general). For each class, four examples of shutter functions for different\
    \ pixels are represented by different colors."
  Figure 4 Link: articels_figures_by_rev_year\2020\Neural_Sensors_Learning_Pixel_Exposures_for_HDR_Imaging_and_Video_Compressive_Se\figure_4.jpg
  Figure 4 caption: "A zoomed-in diagram of the encoder. This diagram illustrates\
    \ the implementation details of our encoder for a given pixel (i,j) and a single\
    \ parameter \u03D5 i,j . The learned parameter is the real-valued \u03D5 i,j .\
    \ The quantized parameter \u03D5 i,j is the one compiled on the sensor in C i,j\
    \ =f( S i,j ) ."
  Figure 5 Link: articels_figures_by_rev_year\2020\Neural_Sensors_Learning_Pixel_Exposures_for_HDR_Imaging_and_Video_Compressive_Se\figure_5.jpg
  Figure 5 caption: 'HDR results in simulation. On the right pane: the images show
    ground truths, measurements and reconstructions for different HDR scenes captured
    with shutter functions of class (b) in simulation along with some baseline comparisons.
    The left pane shows other scenes captured with optimized shutters of class (b)
    .'
  Figure 6 Link: articels_figures_by_rev_year\2020\Neural_Sensors_Learning_Pixel_Exposures_for_HDR_Imaging_and_Video_Compressive_Se\figure_6.jpg
  Figure 6 caption: Video compressive sensing results in simulation. These images
    show individual coded images (measurements) along with the reconstructed video
    clips for several high-speed scenes in simulation, using shutter functions of
    class (c) and (e) .
  Figure 7 Link: articels_figures_by_rev_year\2020\Neural_Sensors_Learning_Pixel_Exposures_for_HDR_Imaging_and_Video_Compressive_Se\figure_7.jpg
  Figure 7 caption: "Examples of non-optimized and optimized shutter functions. Different\
    \ classes of shutter functions are shown for different pixels, when optimized\
    \ and non-optimized. For functions of class (b) we encode in a grey level image\
    \ the stopping time of the integrations. For all the other classes we choose a\
    \ subset of 8\xD78 pixels for which we show the shutter functions in time: black\
    \ means the pixel is off, white it is on."
  Figure 8 Link: articels_figures_by_rev_year\2020\Neural_Sensors_Learning_Pixel_Exposures_for_HDR_Imaging_and_Video_Compressive_Se\figure_8.jpg
  Figure 8 caption: "Experimental high-speed & HDR captures with our SCAMP-5 prototypes.\
    \ The images show measurements captured with our sensor as well as reconstructed\
    \ frames for different class of shutter functions ( (b),(c) and (e) ) and decoders\
    \ [FC-net for class (c) and U-Nets for class (b),(e) ]. For the HDR images produced\
    \ using class (b) , the tone mapped images are produced using a global tone mapping\
    \ I TM = I \u03B3 HDR with \u03B3=0.5 , low and high exposures are produced by\
    \ clipping the HDR image to a high and low range: eg I low (x,y)=max(min(I(x,y),\
    \ h low ), l low ),\u2200(x,y),h>l ."
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Julien N. P. Martel
  Name of the last author: Gordon Wetzstein
  Number of Figures: 8
  Number of Tables: 4
  Number of authors: 5
  Paper title: 'Neural Sensors: Learning Pixel Exposures for HDR Imaging and Video
    Compressive Sensing With Programmable Sensors'
  Publication Date: 2020-04-13 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Pairs of Encoding and Decoding Functions for Certain Class\
      \ of Shutter Functions For Compactness we Denote the Max Rectifier x + =max(x,0)\
      \ x+=max(x,0) and the Min Rectifier x \u2212 =\u2212min(x,0) x-=-min(x,0) as\
      \ well as the Backward Difference Operator \u2207x[n]=x[n]\u2212x[n\u22121]\
      \ \u2207x[n]=x[n]-x[n-1]"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results for HDR Imaging
  Table 3 caption:
    table_text: TABLE 3 Results for Video Compressive Sensing
  Table 4 caption:
    table_text: TABLE 4 Baseline Comparisons for Video Compressive Sensing
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2986944
- Affiliation of the first author: department of electrical and electronic engineering,
    imperial college london, london, united kingdom
  Affiliation of the last author: center for sensor systems (zess), university of
    siegen, siegen, germany
  Figure 1 Link: articels_figures_by_rev_year\2020\OneBit_TimeResolved_Imaging\figure_1.jpg
  Figure 1 caption: Conventional versus one-bit time-resolved imaging. (a) Raw data
    from conventional time-resolved imaging sensor [2], [3]. (b) One-bit measurements.
    (c) Result from our recovery algorithm.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\OneBit_TimeResolved_Imaging\figure_2.jpg
  Figure 2 caption: "(a): Block diagram of a one-bit Time-of-Flight (ToF) sensor for\
    \ time-resolved imaging based on \u03A3\u0394 quantization. (b) Electronic schematic\
    \ of a ToF pixel implementing this concept."
  Figure 3 Link: articels_figures_by_rev_year\2020\OneBit_TimeResolved_Imaging\figure_3.jpg
  Figure 3 caption: "(a) Experimentally-calibrated pulse using conventional time-resolved\
    \ measurements \u03D5 r (nT) and its estimate \u03D5 \u02DC r (nT) using (17)\
    \ given the one-bit samples q r,\u03D5 [n] in (22). The mean-squared-error in\
    \ approximation is on the order of 10 \u22125 . (b) One-bit samples q r,\u03D5\
    \ . (c) Fourier spectrum of \u03D5 r (nT) (black) and one-bit samples q r,\u03D5\
    \ (red). Note that \u03D5 r (nT) is approximately bandlimited and the spectrum\
    \ of q r,\u03D5 overlaps with that of \u03D5 r (nT) in the bandlimited part."
  Figure 4 Link: articels_figures_by_rev_year\2020\OneBit_TimeResolved_Imaging\figure_4.jpg
  Figure 4 caption: Raw data and recovery from corresponding one-bit measurements
    for a given time-resolved pixel and single light path ( K=1 ). (a) Time domain.
    (b) Fourier Domain. The improvement due to biasDC correction is also shown.
  Figure 5 Link: articels_figures_by_rev_year\2020\OneBit_TimeResolved_Imaging\figure_5.jpg
  Figure 5 caption: Bench marking one-bit time-resolved imaging. (a) Scene parameters
    leftlbrace Gamma mathbf rleft[ 0 right],tau mathbf rleft[ 0 right] rightrbrace
    estimated using raw data. This serves as our ground truth. (b) Recovery from one-bit
    measurements using first order scheme ( L=1 ). (c) Recovery from one-bit measurements
    using second order scheme ( L=2 ).
  Figure 6 Link: articels_figures_by_rev_year\2020\OneBit_TimeResolved_Imaging\figure_6.jpg
  Figure 6 caption: Raw data and recovery from corresponding one-bit measurements
    for a given time-resolved pixel and two light paths ( K=2 ). (a) Time domain.
    (b) Fourier Domain. The improvement due to biasDC correction is also shown.
  Figure 7 Link: articels_figures_by_rev_year\2020\OneBit_TimeResolved_Imaging\figure_7.jpg
  Figure 7 caption: One-bit time-resolved imaging. (a) Scene parameters leftlbrace
    Gamma mathbf rleft[ 0 right],tau mathbf rleft[ 0 right] rightrbrace and leftlbrace
    Gamma mathbf rleft[ 1 right],tau mathbf rleft[ 1 right] rightrbrace are estimated
    using raw data. This serves as our ground truth. (b) Recovery from one-bit measurements
    using first order scheme ( L=1 ). (c) Recovery from one-bit measurements using
    second order scheme ( L=2 ).
  Figure 8 Link: articels_figures_by_rev_year\2020\OneBit_TimeResolved_Imaging\figure_8.jpg
  Figure 8 caption: One-bit time-resolved imaging with higher order reflections (
    K=3) .
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Ayush Bhandari
  Name of the last author: Otmar Loffeld
  Number of Figures: 8
  Number of Tables: 1
  Number of authors: 3
  Paper title: One-Bit Time-Resolved Imaging
  Publication Date: 2020-04-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 One-Bit Recovery for K=3 K=3 Light Paths
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2986950
- Affiliation of the first author: dgene digital technology, baton rouge, usa
  Affiliation of the last author: division of computer science and engineering, louisiana
    state university, baton rouge, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Shape_and_Reflectance_Reconstruction_Using_Concentric_MultiSpectral_Light_Field\figure_1.jpg
  Figure 1 caption: 'Left: our concentric multi-spectral light field (CMSLF) array.
    We arrange the cameras on concentric circles, where each circle has the same number
    of cameras that capture at the same specific spectrum. A multi-spectral ring light
    surrounding the cameras provides direction-varying illumination for each camera
    circle. Right: our reconstruction results. (a) scene photograph; (b) recovered
    normal map; (c) recovered 3D surface; and (d) recovered reflectance map.'
  Figure 10 Link: articels_figures_by_rev_year\2020\Shape_and_Reflectance_Reconstruction_Using_Concentric_MultiSpectral_Light_Field\figure_10.jpg
  Figure 10 caption: Our desired multi-spectral light source arrangement. The shading
    variation of the light sources with respect to the wavelength form a fluctuated
    curve.
  Figure 2 Link: articels_figures_by_rev_year\2020\Shape_and_Reflectance_Reconstruction_Using_Concentric_MultiSpectral_Light_Field\figure_2.jpg
  Figure 2 caption: 'Multi-spectral Surface Camera (MSS-Cam) sampling. Top: MSS-Cam
    sampled at the correct depth; Bottom: MSS-Cam for the same point but sampled at
    an incorrect depth.'
  Figure 3 Link: articels_figures_by_rev_year\2020\Shape_and_Reflectance_Reconstruction_Using_Concentric_MultiSpectral_Light_Field\figure_3.jpg
  Figure 3 caption: "The specularity variations in MSS-Cam exhibit unique pattern\
    \ in our CMSLF. (a) The cone-shaped lighting directions result in a reflection\
    \ cone that is symmetric to the normal. (b) Because of concentric camera setting\
    \ on circles, the intensities from each column of the MSS-Cam will be changing\
    \ on a sinusoidal curve (single-peak in the interval [0,2\u03C0] ). (c) An MSS-Cam\
    \ with specularity. (d) We plot the pixel intensities from the same MSS-Cam column\
    \ and show that they form single-peak patterns. (e) The camera spanned angles\
    \ corresponding to the peaks of each curve in (d) form a sinusoidal curve."
  Figure 4 Link: articels_figures_by_rev_year\2020\Shape_and_Reflectance_Reconstruction_Using_Concentric_MultiSpectral_Light_Field\figure_4.jpg
  Figure 4 caption: The algorithmic pipeline for shape and reflectance reconstruction.
  Figure 5 Link: articels_figures_by_rev_year\2020\Shape_and_Reflectance_Reconstruction_Using_Concentric_MultiSpectral_Light_Field\figure_5.jpg
  Figure 5 caption: 'Synthetic results of a sphere object with three types of materials.
    Top: we show the input image of the sphere models, the recovered normal map, the
    error map of normal estimation in degree, and the recovered reflectance (from
    left to right); Bottom: we show the multi-spectral reflectance curve (red) of
    a randomly sampled point on each sphere in comparison with the ground truth reflectance
    curve (blue).'
  Figure 6 Link: articels_figures_by_rev_year\2020\Shape_and_Reflectance_Reconstruction_Using_Concentric_MultiSpectral_Light_Field\figure_6.jpg
  Figure 6 caption: Synthetic results for the buddha and plate. From left to right,
    we show the input model images, recovered normal maps, the recovered normal maps,
    the error maps of normal estimation in degree, the recovered 3D surfaces and the
    recovered reflectance images.
  Figure 7 Link: articels_figures_by_rev_year\2020\Shape_and_Reflectance_Reconstruction_Using_Concentric_MultiSpectral_Light_Field\figure_7.jpg
  Figure 7 caption: Comparison results on synthetic data. We compare the recovered
    depth map and 3D surfaces with two state-of-the-arts light field-based methods
    [30] and [18].
  Figure 8 Link: articels_figures_by_rev_year\2020\Shape_and_Reflectance_Reconstruction_Using_Concentric_MultiSpectral_Light_Field\figure_8.jpg
  Figure 8 caption: Performance analysis with respect to the number of cameras, light
    sources and image noise.
  Figure 9 Link: articels_figures_by_rev_year\2020\Shape_and_Reflectance_Reconstruction_Using_Concentric_MultiSpectral_Light_Field\figure_9.jpg
  Figure 9 caption: (a) Our prototype concentric multi-spectral light field (CMSLF)
    system. (b) The illumination spectra of our multi-spectral ring light source.
    (c) Sample multi-spectral images captured with our CMSLF.
  First author gender probability: 0.9
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.88
  Name of the first author: Mingyuan Zhou
  Name of the last author: Jinwei Ye
  Number of Figures: 14
  Number of Tables: 1
  Number of authors: 6
  Paper title: Shape and Reflectance Reconstruction Using Concentric Multi-Spectral
    Light Field
  Publication Date: 2020-04-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Evaluation of Multi-Spectral Reflectance Estimation
      Using the Standard Colorchecker Chart in Comparison With [6]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2986764
- Affiliation of the first author: ece department, carnegie mellon university, pittsburgh,
    usa
  Affiliation of the last author: ece department, carnegie mellon university, pittsburgh,
    usa
  Figure 1 Link: articels_figures_by_rev_year\2020\SweepCam__DepthAware_Lensless_Imaging_Using_Programmable_Masks\figure_1.jpg
  Figure 1 caption: Lensless focal stack. We show images reconstructed at three different
    depths using our proposed SweepCam technique, which is a lensless camera with
    a programmable mask.
  Figure 10 Link: articels_figures_by_rev_year\2020\SweepCam__DepthAware_Lensless_Imaging_Using_Programmable_Masks\figure_10.jpg
  Figure 10 caption: Comparison of different reconstruction methods on real data.
    As shown in (a), the scene contains two transparencies printed with boat pattern.
    White is printed to be transparent. Near plane is at 2.8 cm while the far plane
    is at 18 cm. (b)(c)(d) show various reconstruction techniques from static mask
    measurements. (b) deconvolves static mask measurements with PSFs at near and far
    planes, as [2]; (c) jointly estimates texture and depth of each pixel in the scene,
    as [3]. (d) is given per pixel depth as input and only solves for texture. (e)
    reconstructs from SweepCam measurements with the same number of frames using the
    fast algorithm.
  Figure 2 Link: articels_figures_by_rev_year\2020\SweepCam__DepthAware_Lensless_Imaging_Using_Programmable_Masks\figure_2.jpg
  Figure 2 caption: "Schematic of a lensless imager. A mask is placed at a distance\
    \ d from the sensor. Ray from point ( x 0 , z 0 ) reaches sensor pixel (x,\u2212\
    d) after crossing the mask at (x+ x 0 \u2212x z 0 +d d) ."
  Figure 3 Link: articels_figures_by_rev_year\2020\SweepCam__DepthAware_Lensless_Imaging_Using_Programmable_Masks\figure_3.jpg
  Figure 3 caption: "Kernels and their evolution. Top row shows PSF for three different\
    \ depth. Second row shows each PSF correlated with PSF from z 0 = 6.8 cm, as kernels\
    \ underlying blocks in the Gram matrix from Section 4.1. Third row shows applying\
    \ deconvolution kernel for PSF at z 0 on PSFs of different depth; the result is\
    \ high frequency artifacts for directly applying deconvolution kernel on captured\
    \ measurements. Last row shows applying deconvolution kernel for PSF at z 0 on\
    \ PSFs of focused measurements; the artifacts are reduced by two orders of magnitude\
    \ when reconstructing with focused measurements. Focused measurements are generated\
    \ as described in Section 4.3 with 13 \xD7 13 aperture locations across baseline\
    \ area 0.78 cm\xD70.78 cm."
  Figure 4 Link: articels_figures_by_rev_year\2020\SweepCam__DepthAware_Lensless_Imaging_Using_Programmable_Masks\figure_4.jpg
  Figure 4 caption: Captured and focused measurements from our lab prototype for scene
    with content on two planes. Focused measurements in both are generated with 13
    captures with total baseline of 0.78 cm. Notice that the measurements focused
    at 2.7 cm suppresses contribution from 18 cm, and vice versa.
  Figure 5 Link: articels_figures_by_rev_year\2020\SweepCam__DepthAware_Lensless_Imaging_Using_Programmable_Masks\figure_5.jpg
  Figure 5 caption: "Reducing interference from other depth via focusing. The left\
    \ column shows translation patterns of the mask, while the right column shows\
    \ of | \u03B2 z (\u03C9)| in (20) for depth z=3cm and \u03BD z 0 =4cm . Row two\
    \ and three show more effective of suppression of interference from z=3cm as number\
    \ of translations increase. Imaging parameters such as mask pixel pitch are taken\
    \ from our hardware prototype, given in Section 6."
  Figure 6 Link: articels_figures_by_rev_year\2020\SweepCam__DepthAware_Lensless_Imaging_Using_Programmable_Masks\figure_6.jpg
  Figure 6 caption: 'Comparison of different number of measurements and baseline on
    simulated data. Left column quantitative evaluates reconstruction performance
    in terms of SSIM. (a) shows results for different number of measurements: increasing
    number of static measurements mitigates noise in the measurement but results in
    little changes in SSIM. SweepCam-full reconstruction is severely under-determined
    for single measurement, but improves as number of measurements increases, and
    peaks when number of measurements match number of unknown depth planes. SweepCam-fast
    reconstruction has increasing SSIM as the number of measurements increase, since
    the interference between depth planes is reduced due to focusing. (b) shows results
    for different baseline: small baseline degrades performance. Bottom images shows
    one of the reconstructed all-in-focus images at three operating points noted in
    the left plots. The all-in-focus images are generated by selecting pixels from
    the reconstructed volume with ground truth depth map.'
  Figure 7 Link: articels_figures_by_rev_year\2020\SweepCam__DepthAware_Lensless_Imaging_Using_Programmable_Masks\figure_7.jpg
  Figure 7 caption: Image quality with varying light levels. We simulate light levels
    in terms of the fraction of the full well capacity at the brightest pixel on the
    sensor. Shot noise and read noise are simulated with sensor full well capacity
    and dynamic range for the Sony IMX174 sensor. We observe that SweepCam fast achieves
    better performance under noisy conditions.
  Figure 8 Link: articels_figures_by_rev_year\2020\SweepCam__DepthAware_Lensless_Imaging_Using_Programmable_Masks\figure_8.jpg
  Figure 8 caption: "Comparison of reconstructing with different image priors. \u201C\
    Tsukuba\u201D scene from Middlebury dataset (ground truth shown in Fig. 6) is\
    \ imaged at \u201Cparam A\u201D described in Fig. 6. From left to right, we show\
    \ results of reconstruction using different image priors. The left three images\
    \ are reconstructed at each depth plane separately following (7): using static\
    \ mask measurements with \u21132 norm squared, SweepCam measurements with \u2113\
    2 norm squared, and SweepCam measurements with \u21131 norm of wavelet coefficients,\
    \ respectively. The right three images are reconstructed from all depth planes\
    \ in the volume from SweepCam measurements following (6), using \u21132 norm squared,\
    \ \u21131 norm, \u21131 norm of wavelet coefficients on each image plane as priors,\
    \ respectively. As shown, the SweepCam-fast algorithm achieves reasonable quality\
    \ while it runs significantly faster than the other algorithms using more sophisticated\
    \ priors."
  Figure 9 Link: articels_figures_by_rev_year\2020\SweepCam__DepthAware_Lensless_Imaging_Using_Programmable_Masks\figure_9.jpg
  Figure 9 caption: Prototype hardware setup. The proposed design includes a programmable
    amplitude mask and a sensor. Our programmable mask is made of a transmissive LCoS
    sandwiched between two cross polarizers, one of which is mounted on precision
    rotation mount of optimal contrast.
  First author gender probability: 0.54
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Yi Hua
  Name of the last author: Aswin C. Sankaranarayanan
  Number of Figures: 12
  Number of Tables: 1
  Number of authors: 4
  Paper title: "SweepCam \u2014 Depth-Aware Lensless Imaging Using Programmable Masks"
  Publication Date: 2020-04-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average Run Time and Quality Comparison Between Reconstruction
      Methods Operating Under param A of Fig. 6
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2986784
