- Affiliation of the first author: xi'an jiaotong university, xi'an, china
  Affiliation of the last author: microsoft research, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2015\Accelerating_Very_Deep_Convolutional_Networks_for_Classification_and_Detection\figure_1.jpg
  Figure 1 caption: "Illustration of the decomposition. (a) An original layer with\
    \ complexity O(d k 2 c) . (b) An approximated layer with complexity reduced to\
    \ O( d \u2032 k 2 c)+O(d d \u2032 ) ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Accelerating_Very_Deep_Convolutional_Networks_for_Classification_and_Detection\figure_2.jpg
  Figure 2 caption: "PCA accumulative energy of the responses in each layer, presented\
    \ as the sum of largest d \u2032 eigenvalues (relative to the total energy when\
    \ d \u2032 =d ). Here the filter number d is 96 for Conv1, 256 for Conv2, and\
    \ 512 for Conv3-7 (detailed in Table 1). These figures are obtained from 3,000\
    \ randomly sampled training images."
  Figure 3 Link: articels_figures_by_rev_year\2015\Accelerating_Very_Deep_Convolutional_Networks_for_Classification_and_Detection\figure_3.jpg
  Figure 3 caption: PCA accumulative energy and the accuracy rates (top-5). Here the
    accuracy is evaluated using the linear solution (the nonlinear solution has a
    similar trend). Each layer is evaluated independently, with other layers not approximated.
    The accuracy is shown as the difference to no approximation.
  Figure 4 Link: articels_figures_by_rev_year\2015\Accelerating_Very_Deep_Convolutional_Networks_for_Classification_and_Detection\figure_4.jpg
  Figure 4 caption: 'Linear versus Nonlinear for SPP-10: single-layer performance
    of accelerating Conv1 to Conv7. The speedup ratios are computed by the theoretical
    complexity of that layer. The error rates are top-5 single-view, and shown as
    the increase of error rates compared with no approximation (smaller is better).'
  Figure 5 Link: articels_figures_by_rev_year\2015\Accelerating_Very_Deep_Convolutional_Networks_for_Classification_and_Detection\figure_5.jpg
  Figure 5 caption: 'Symmetric versus Asymmetric for SPP-10: the cases of two-layer
    and three-layer approximation. The speedup is computed by the complexity of the
    layers approximated. (a) Approximation of Conv6 & 7. (b) Approximation of Conv2,
    3, & 4. (c) Approximation of Conv5, 6, & 7.'
  Figure 6 Link: articels_figures_by_rev_year\2015\Accelerating_Very_Deep_Convolutional_Networks_for_Classification_and_Detection\figure_6.jpg
  Figure 6 caption: Comparisons with Jaderberg et al. spatial decomposition method
    [17] for SPP-10. The speedup ratios are theoretical speedups of the whole model.
    The error rates are top-5 single-view, and shown as the increase of error rates
    compared with no approximation (smaller is better).
  Figure 7 Link: articels_figures_by_rev_year\2015\Accelerating_Very_Deep_Convolutional_Networks_for_Classification_and_Detection\figure_7.jpg
  Figure 7 caption: Actual versus theoretical speedup ratios of VGG-16 using CPU and
    GPU implementations.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Xiangyu Zhang
  Name of the last author: Jian Sun
  Number of Figures: 7
  Number of Tables: 9
  Number of authors: 4
  Paper title: Accelerating Very Deep Convolutional Networks for Classification and
    Detection
  Publication Date: 2015-11-20 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Architecture of the SPP-10 Model [7]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Whole-Model Acceleration withwithout Rank Selection for SPP-10
  Table 3 caption:
    table_text: TABLE 3 Comparisons of Absolute Performance of SPP-10
  Table 4 caption:
    table_text: TABLE 4 Comparisons with the Same Decomposed Architecture Trained
      from Scratch
  Table 5 caption:
    table_text: TABLE 5 The Architecture of the VGG-16 Model [1]
  Table 6 caption:
    table_text: TABLE 6 Whole-Model Acceleration withwithout Rank Selection for VGG-16
  Table 7 caption:
    table_text: "TABLE 7 Accelerating the VGG-16 Model [1] Using a Speedup Ratio of\
      \ 3 \xD7 , 4 \xD7 , or 5 \xD7"
  Table 8 caption:
    table_text: TABLE 8 Absolute Performance of Accelerating the VGG-16 Model [1]
  Table 9 caption:
    table_text: TABLE 9 Object Detection mAP on the PASCAL VOC 2007 Test Set
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2502579
- Affiliation of the first author: computer science department, cornell university,
    ithaca, ny
  Affiliation of the last author: computer science department, cornell university,
    ithaca, ny
  Figure 1 Link: articels_figures_by_rev_year\2015\Modeling_D_Environments_through_Hidden_Human_Context\figure_1.jpg
  Figure 1 caption: 'Left: Previous approaches model the relations between observable
    entities, such as the objects. Right: In our work, we consider the relations between
    the objects and hidden humans. Our key hypothesis is that even when the humans
    are never observed, the human context is helpful.'
  Figure 10 Link: articels_figures_by_rev_year\2015\Modeling_D_Environments_through_Hidden_Human_Context\figure_10.jpg
  Figure 10 caption: Examples of learned object-affordance topics. An affordance is
    represented by the probability distribution of an object in a 5times5times3 space
    given a human pose. We show the projected top views and side views for different
    object classes.
  Figure 2 Link: articels_figures_by_rev_year\2015\Modeling_D_Environments_through_Hidden_Human_Context\figure_2.jpg
  Figure 2 caption: "An example of instantiated ILCRF for scene arrangement. Top row\
    \ shows learned object affordances in top-view heatmaps (it shows the probability\
    \ of the object's location, given a human pose in the center facing to the right).\
    \ Middle row shows a total of K CRFs sampled from our ILCRF algorithm\u2014each\
    \ CRF models the scene differently. Bottom row shows the distribution of the objects\
    \ and humans (in the top view of the room) computed from the sampled CRFs."
  Figure 3 Link: articels_figures_by_rev_year\2015\Modeling_D_Environments_through_Hidden_Human_Context\figure_3.jpg
  Figure 3 caption: Graphical models for scene modeling. (a) Conditional random field
    has been used to capture objects and their relations in a scene. (b) In our work,
    we model possible human configurations as latent nodes in a scene in order to
    capture human and object relations in a scene.
  Figure 4 Link: articels_figures_by_rev_year\2015\Modeling_D_Environments_through_Hidden_Human_Context\figure_4.jpg
  Figure 4 caption: Six types of human poses extracted from Kinect 3D data.
  Figure 5 Link: articels_figures_by_rev_year\2015\Modeling_D_Environments_through_Hidden_Human_Context\figure_5.jpg
  Figure 5 caption: The affordance of a laptop screen, visualized in top- and side-view.
  Figure 6 Link: articels_figures_by_rev_year\2015\Modeling_D_Environments_through_Hidden_Human_Context\figure_6.jpg
  Figure 6 caption: Graphical representations of our infinite latent CRF (ILCRF).
  Figure 7 Link: articels_figures_by_rev_year\2015\Modeling_D_Environments_through_Hidden_Human_Context\figure_7.jpg
  Figure 7 caption: Examples of learned affordance topics and object affordances as
    a mixture of those topics (see Section 4.4).
  Figure 8 Link: articels_figures_by_rev_year\2015\Modeling_D_Environments_through_Hidden_Human_Context\figure_8.jpg
  Figure 8 caption: Learning object affordances. This example shows the learned object
    affordances (top row, shown as heatmaps) and top sampled human configurations
    (bottom) through iterations. In Iteration1, the affordance is only based on the
    prior Bpsi which is same for all objects. Thus, the sampled human poses also randomly
    appear in the scene. In later iterations, the affordances diverge to different
    but reasonable functions and so do the sampled humans based on these affordances.
  Figure 9 Link: articels_figures_by_rev_year\2015\Modeling_D_Environments_through_Hidden_Human_Context\figure_9.jpg
  Figure 9 caption: Top sampled human poses in different scenes. The first two are
    from stitched point-cloud from multiple RGB-D views, and the last three scenes
    are shown in RGB-D single views.
  First author gender probability: 0.55
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yun Jiang
  Name of the last author: Ashutosh Saxena
  Number of Figures: 16
  Number of Tables: 4
  Number of authors: 3
  Paper title: Modeling 3D Environments through Hidden Human Context
  Publication Date: 2015-12-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of Gibbs Sampling in ILCRF for Two Applications
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Object and Attribute Labeling Results
  Table 3 caption:
    table_text: TABLE 3 Scene Arrangement Results on Partially-Filled Scenes and Empty
      Scenes in Synthetic Dataset, Evaluated by the Location and Height Difference
      to the Labeled Arrangements
  Table 4 caption:
    table_text: TABLE 4 Scene Arrangement Results on Five Real Empty Scenes (Three
      Offices and Two Apartments)
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2501811
- Affiliation of the first author: school of electrical and computer engineering,
    purdue university, west lafayette, in
  Affiliation of the last author: school of electrical and computer engineering, purdue
    university, west lafayette, in
  Figure 1 Link: articels_figures_by_rev_year\2015\Saying_What_Youre_Looking_For_Linguistics_Meets_Video_Search\figure_1.jpg
  Figure 1 caption: "Predicates which accept detections, denoted by a and b , formulated\
    \ around nine parameters. These predicates are used for the experiments in Section\
    \ 7. The function project projects a detection forward one frame using optical\
    \ flow. The functions flow\u2212orientation and flow\u2212magnitude compute the\
    \ angle and magnitude of the average optical-flow vector inside a detection. The\
    \ function a cx accesses the x coordinate of the center of a detection. The function\
    \ a width computes the width of a detection. The functions \u222A and \u2229 compute\
    \ the area of the union and intersection of two detections respectively. The function\
    \ |\u22C5 | \u2218 computes angular separation. Words are formed as regular expressions\
    \ over these predicates."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Saying_What_Youre_Looking_For_Linguistics_Meets_Video_Search\figure_2.jpg
  Figure 2 caption: Regular expressions which encode the meanings of each of the 15
    words or lexicalized phrases in the lexicon used for the experiments in Section
    7. These are composed from the predicates shown in Fig. 1. We use an extended
    regular-expression syntax where an exponent of t, allows a predicate to hold for
    t or more frames.
  Figure 3 Link: articels_figures_by_rev_year\2015\Saying_What_Youre_Looking_For_Linguistics_Meets_Video_Search\figure_3.jpg
  Figure 3 caption: Tracker lattices are used to track each participant. Word lattices
    constructed from word FSMs for each word in the sentence recognize collections
    of tracks for participants that exhibit the semantics of that word as encoded
    in the FSM. We take the cross product of multiple tracker and word lattices to
    simultaneously track participants and recognize words. This ensures that the resulting
    tracks are described by the desired sentence.
  Figure 4 Link: articels_figures_by_rev_year\2015\Saying_What_Youre_Looking_For_Linguistics_Meets_Video_Search\figure_4.jpg
  Figure 4 caption: "Different sentential queries lead to different cross products.\
    \ The sentence is parsed and the role of each participant, show in red, is determined.\
    \ A single tracker lattice is constructed for each participant. Words and lexicalized\
    \ phrases, shown in blue, have associated word lattices which encode their semantics.\
    \ The arrows between words and participants represent the track-to-role mappings,\
    \ \u03B8 , required to link the tracker and word lattices in a way that faithfully\
    \ encodes the sentential semantics. Some words, like determiners, shown in grey,\
    \ have no semantics beyond determining the parse tree and track-to-role mapping.\
    \ The dashed lines indicate that the argument order is essential for words which\
    \ have more than one role. In other words, predicates like rode and away from\
    \ are not symmetric. Detection sources are shown in black, in this case two object\
    \ detectors. The tracker associated with each participant has access to all detection\
    \ sources, hence the bipartite clique between the trackers and the detection sources."
  Figure 5 Link: articels_figures_by_rev_year\2015\Saying_What_Youre_Looking_For_Linguistics_Meets_Video_Search\figure_5.jpg
  Figure 5 caption: The grammar for sentential queries used for the experiments in
    Section 7 .
  Figure 6 Link: articels_figures_by_rev_year\2015\Saying_What_Youre_Looking_For_Linguistics_Meets_Video_Search\figure_6.jpg
  Figure 6 caption: (Top left) Comparison of average precision in the top 1, 3, 5,
    and 10 hits, over the SVO queries for both the baseline and the sentence tracker.
    (bottom left) Precisionrecall curve over the SVO queries for the sentence tracker.
    Results for synthetic (top row) and human (bottom row) queries in the top 1, 3,
    5, and 10 hits (right three columns). (second column) Fraction of queries with
    at least the the indicated number of hits, correct or ambiguous hits, and correct
    hits. (third column) Fraction of queries that have at least the indicated fraction
    of correct hits. (fourth column) Precision of returned hits as a function of threshold.
  Figure 7 Link: articels_figures_by_rev_year\2015\Saying_What_Youre_Looking_For_Linguistics_Meets_Video_Search\figure_7.jpg
  Figure 7 caption: Frames from hits returned for several synthetic and human queries.
    Some clips are returned for multiple queries. As indicated above, these hits were
    judged as correct or ambiguous for the associated query by human judges.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Daniel Paul Barrett
  Name of the last author: Jeffrey Mark Siskind
  Number of Figures: 7
  Number of Tables: 0
  Number of authors: 4
  Paper title: 'Saying What You''re Looking For: Linguistics Meets Video Search'
  Publication Date: 2015-12-03 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2505297
- Affiliation of the first author: cylab biometrics center and the department of electrical
    and computer engineering, carnegie mellon university, pittsburgh, pa
  Affiliation of the last author: cylab biometrics center and the department of electrical
    and computer engineering, carnegie mellon university, pittsburgh, pa
  Figure 1 Link: articels_figures_by_rev_year\2015\Towards_a_Unified_Framework_for_Pose_Expression_and_Occlusion_Tolerant_Automatic\figure_1.jpg
  Figure 1 caption: An overview of our approach showing how each stage in the process
    works on an image from the test set partition of the LFPW dataset. In all facial
    images with landmarks overlaid on them, yellow dots are used to indicate the locations
    of facial landmarks, blue line segments indicate that the landmark at their center
    is accurately localized, and red line segments indicate that the landmark at their
    center is misaligned or potentially occluded. This same color scheme is maintained
    in all figures containing fitting results produced by our approach.
  Figure 10 Link: articels_figures_by_rev_year\2015\Towards_a_Unified_Framework_for_Pose_Expression_and_Occlusion_Tolerant_Automatic\figure_10.jpg
  Figure 10 caption: Normalized fitting errors (percent) obtained (using a common
    set of 68 or 39 landmarks on the MPIE test set and 24 landmarks on the COFW test
    set) using various algorithms on faces with various expressions and occlusion
    levels on the (a) MPIE and (b) COFW test sets, respectively.
  Figure 2 Link: articels_figures_by_rev_year\2015\Towards_a_Unified_Framework_for_Pose_Expression_and_Occlusion_Tolerant_Automatic\figure_2.jpg
  Figure 2 caption: MPIE landmarking (markup) schemes for (a) profile faces ( 39 facial
    landmarks), (b) frontal faces ( 68 facial landmarks). The facial images in the
    figure are from the MPIE database.
  Figure 3 Link: articels_figures_by_rev_year\2015\Towards_a_Unified_Framework_for_Pose_Expression_and_Occlusion_Tolerant_Automatic\figure_3.jpg
  Figure 3 caption: Process by which seed landmark candidates are retained by our
    approach when fitting a facial image from the test set partition of the LFPW dataset.
    The process is shown only for one of the pose models (for a yaw angle of 0 to
    +15 degree), but is repeated to retain seed landmark candidates specific to each
    pose model.
  Figure 4 Link: articels_figures_by_rev_year\2015\Towards_a_Unified_Framework_for_Pose_Expression_and_Occlusion_Tolerant_Automatic\figure_4.jpg
  Figure 4 caption: The highest scoring aligned initial shapes for each of the M=10
    pose models for a facial image from the test set partition of the LFPW dataset.
  Figure 5 Link: articels_figures_by_rev_year\2015\Towards_a_Unified_Framework_for_Pose_Expression_and_Occlusion_Tolerant_Automatic\figure_5.jpg
  Figure 5 caption: Iterative process used in our shape refinement step demonstrated
    on a facial image from the test set partition of the LFPW dataset.
  Figure 6 Link: articels_figures_by_rev_year\2015\Towards_a_Unified_Framework_for_Pose_Expression_and_Occlusion_Tolerant_Automatic\figure_6.jpg
  Figure 6 caption: Qualitative landmark localization results produced by our approach
    (when trained on images from the MPIE dataset) on some images from the MPIE, LFPW,
    AFW, ibug, and COFW datasets from the top to bottom rows, respectively.
  Figure 7 Link: articels_figures_by_rev_year\2015\Towards_a_Unified_Framework_for_Pose_Expression_and_Occlusion_Tolerant_Automatic\figure_7.jpg
  Figure 7 caption: (a) The 29 point landmarking scheme for the COFW dataset and (b)
    The 25 landmarks common to both the 68 point MPIE landmarking scheme and the ground
    truth annotations available for the COFW dataset. The facial image in the figure
    is from the training set partition of the COFW dataset.
  Figure 8 Link: articels_figures_by_rev_year\2015\Towards_a_Unified_Framework_for_Pose_Expression_and_Occlusion_Tolerant_Automatic\figure_8.jpg
  Figure 8 caption: Cumulative Error Distribution curves for various algorithms obtained
    by averaging normalized fitting errors (percent) over common interior landmarks
    ( 24 for the COFW test set, 49 or 27 for the MPIE test set, and 49 for all other
    test sets) on the (a) MPIE, (b) LFPW, (c) AFW, (d) ibug, and (e) COFW test sets.
  Figure 9 Link: articels_figures_by_rev_year\2015\Towards_a_Unified_Framework_for_Pose_Expression_and_Occlusion_Tolerant_Automatic\figure_9.jpg
  Figure 9 caption: Normalized fitting errors (percent) as a function of yaw angle
    for the top performing algorithms calculated using a common set of landmarks (
    24 for the COFW test set, 68 or 39 for the MPIE test set, and 49 for all other
    test sets) on the (a) MPIE, (b) LFPW, (c) AFW, (d) ibug, and (e) COFW test sets.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Keshav Seshadri
  Name of the last author: Marios Savvides
  Number of Figures: 10
  Number of Tables: 6
  Number of authors: 2
  Paper title: Towards a Unified Framework for Pose, Expression, and Occlusion Tolerant
    Automatic Facial Alignment
  Publication Date: 2015-12-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance of Various Algorithms on Various Test Sets with
      Statistics Computed Using 68 (or 39 for the MPIE Test Set) Common Landmarks
      (Except in Cases Where an Alternative Number Is Indicated in Brackets)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance of Various Algorithms on Test Sets with Statistics
      Computed Using 24 (for the COFW Test Set), 49 or 27 (for the MPIE Test Set),
      and 49 (for All Other Test Sets) Common Interior Landmarks Localized by the
      Various Algorithms
  Table 3 caption:
    table_text: TABLE 3 Occlusion Prediction Performance of RCPR and Our Algorithm
      (Both Trained UsingCOFW and LFPW Training Set Images) When Localizing 29 Landmarks
      on the COFW Test Set
  Table 4 caption:
    table_text: TABLE 4 Average Fitting Times per Image for Various Algorithms
  Table 5 caption:
    table_text: TABLE 5 MNFE (Percent) Values Obtained on Test Sets by Each Stage
      of Our Approach by Averaging Over the Maximum Number of Landmarks Localized
      at that Stage
  Table 6 caption:
    table_text: TABLE 6 Comparison of MNFE (Percent) Values Obtained by Averaging
      Over 25 Landmarks on the COFW Test Set, 68 or 39 Landmarks on the MPIE Test
      Set, and 68 Landmarks on All Other Test Sets Using Various Shape Regularization
      Approaches with Models Trained on MPIE Images
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2505301
- Affiliation of the first author: school of computer science, the university of adelaide,
    australia
  Affiliation of the last author: arc centre of excellence for robotic vision and
    the school of computer science, the university of adelaide, australia
  Figure 1 Link: articels_figures_by_rev_year\2015\Learning_Depth_from_Single_Monocular_Images_Using_Deep_Convolutional_Neural_Fiel\figure_1.jpg
  Figure 1 caption: 'Examples of depth estimation results using the proposed deep
    convolutional neural fields model. First row: NYU v2 dataset; second row: Make3D
    dataset. From left to right: input image, ground-truth, our prediction.'
  Figure 10 Link: articels_figures_by_rev_year\2015\Learning_Depth_from_Single_Monocular_Images_Using_Deep_Convolutional_Neural_Fiel\figure_10.jpg
  Figure 10 caption: 'An illustration of the absolute error maps and the pixel-wise
    error histograms of our predictions (Left: NYU v2; Right: Make3D). The absolute
    error maps are shown in meters, with the color bar shown in the last row. For
    the error histogram plot, the horizontal axis shows the prediction error in meters
    (quantized into 20 bins), and the vertical axis shows the percentage of pixels
    in each bin.'
  Figure 2 Link: articels_figures_by_rev_year\2015\Learning_Depth_from_Single_Monocular_Images_Using_Deep_Convolutional_Neural_Fiel\figure_2.jpg
  Figure 2 caption: An illustration of our DCNF model for depth estimation. The input
    image is first over-segmented into superpixels. In the unary part, for a superpixel
    p , we crop the image patch centred around its centroid, then resize and feed
    it to a CNN which is composed of five convolutional and four fully-connected layers
    (details refer to Fig. 3). In the pairwise part, for a pair of neighboring superpixels
    (p,q) , we consider K types of similarities, and feed them into a fully-connected
    layer. The outputs of unary part and the pairwise part are then fed to the CRF
    structured loss layer, which minimizes the negative log-likelihood. Predicting
    the depths of a new image x is to maximize the conditional probability Pr(y|x)
    , which has closed-form solutions (see Section 2.3 for details).
  Figure 3 Link: articels_figures_by_rev_year\2015\Learning_Depth_from_Single_Monocular_Images_Using_Deep_Convolutional_Neural_Fiel\figure_3.jpg
  Figure 3 caption: Detailed network architecture of the unary part in Fig. 2.
  Figure 4 Link: articels_figures_by_rev_year\2015\Learning_Depth_from_Single_Monocular_Images_Using_Deep_Convolutional_Neural_Fiel\figure_4.jpg
  Figure 4 caption: "An overview of the unary part of the DCNF-FCSP model. For the\
    \ unary part, the input image is fed into a fully-convolutional network to produce\
    \ convolution maps ( d is the number of filters of the last fully-convolutional\
    \ layer). The obtained convolution maps, together with the superpixel segmentation\
    \ over the original input image, are fed to a superpixel pooling layer. The outputs\
    \ are n\xD71 d dimensional feature vectors for each of the n superpixels, which\
    \ are then followed by three fully-connected layers to produce the unary output\
    \ z . The pairwise part are omitted here since we use the same network architecture\
    \ as in the DCNF model ( Fig. 2). The unary output z and the pairwise output R\
    \ are used as input to the CRF loss layer, which minimizes the negative log-likelihood\
    \ (See Section 2.4 for details)."
  Figure 5 Link: articels_figures_by_rev_year\2015\Learning_Depth_from_Single_Monocular_Images_Using_Deep_Convolutional_Neural_Fiel\figure_5.jpg
  Figure 5 caption: The fully convolutional network architecture used in Fig. 4. The
    network takes input images of arbitrary size and output convolution maps.
  Figure 6 Link: articels_figures_by_rev_year\2015\Learning_Depth_from_Single_Monocular_Images_Using_Deep_Convolutional_Neural_Fiel\figure_6.jpg
  Figure 6 caption: An illustration of the superpixel pooling method, which mainly
    consists of convolution maps upsampling and superpixel pooling. The convolution
    maps are upsampled to the original image size by nearest neighbor interpolations,
    over which the superpixel masking is applied. Then average pooling is performed
    within each superpixel region, to produce the n convolution features. n is the
    number of superpixels in the image. d is the number of channels of the convolution
    maps.
  Figure 7 Link: articels_figures_by_rev_year\2015\Learning_Depth_from_Single_Monocular_Images_Using_Deep_Convolutional_Neural_Fiel\figure_7.jpg
  Figure 7 caption: Examples of qualitative comparisons on the NYUD2 dataset (Best
    viewed on screen). Color indicates depths (red is far, blue is close). Our method
    yields visually better predictions with sharper transitions, aligning to local
    details.
  Figure 8 Link: articels_figures_by_rev_year\2015\Learning_Depth_from_Single_Monocular_Images_Using_Deep_Convolutional_Neural_Fiel\figure_8.jpg
  Figure 8 caption: Examples of depth predictions on the Make3D dataset (Best viewed
    on screen). Depths are shown in log scale and in color (red is far, blue is close).
  Figure 9 Link: articels_figures_by_rev_year\2015\Learning_Depth_from_Single_Monocular_Images_Using_Deep_Convolutional_Neural_Fiel\figure_9.jpg
  Figure 9 caption: Examples of depth predictions on the KITTI dataset (Best viewed
    on screen). Depths are shown in log scale and in color (red is far, blue is close).
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Fayao Liu
  Name of the last author: Ian Reid
  Number of Figures: 12
  Number of Tables: 7
  Number of authors: 4
  Paper title: Learning Depth from Single Monocular Images Using Deep Convolutional
    Neural Fields
  Publication Date: 2015-12-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Baseline Comparisons on the NYU v2 Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Baseline Comparisons on the Make3D Dataset
  Table 3 caption:
    table_text: TABLE 3 Performance Comparisons of DCNF and DCNF-FCSP on the NYU v2
      Dataset
  Table 4 caption:
    table_text: TABLE 4 Performance Comparisons of DCNF and DCNF-FCSP on the Make3D
      Dataset
  Table 5 caption:
    table_text: TABLE 5 State-of-the-Art Comparisons on the NYU v2 Dataset
  Table 6 caption:
    table_text: TABLE 6 State-of-the-Art Comparisons on the Make3D Dataset
  Table 7 caption:
    table_text: TABLE 7 State-of-the-Art Comparisons on the KITTI Dataset
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2505283
- Affiliation of the first author: institute for infocomm research, 1 fusionopolis
    way, singapore
  Affiliation of the last author: school of electrical and electronic engineering,
    nanyang technological university, singapore
  Figure 1 Link: articels_figures_by_rev_year\2015\Multimodal_Multipart_Learning_for_Action_Recognition_in_Depth_Videos\figure_1.jpg
  Figure 1 caption: Three levels of the proposed hierarchical mixed norm for multimodal
    multipart learning. We combine two levels of regularization inside modality groups
    and between them for each part, followed by a sparsity inducing norm between the
    parts to apply part selection.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Amir Shahroudy
  Name of the last author: Gang Wang
  Number of Figures: 1
  Number of Tables: 6
  Number of authors: 4
  Paper title: Multimodal Multipart Learning for Action Recognition in Depth Videos
  Publication Date: 2015-12-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Subject-Wise Cross-Validation Performance Comparison of the
      Proposed Hierarchical Mixed Norm with Plain and Multipart Group Sparsity Norm
      on the MSR-DailyActivity Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance Comparison of the Proposed Method Using PlainStructuredHierarchical
      Norms on the Standard Evaluation Split of the MSR-DailyActivity Dataset
  Table 3 caption:
    table_text: TABLE 3 Performance Comparison on the Standard Evaluation Split of
      the MSR-DailyActivity Dataset Using Single Modality and Multimodal Features
  Table 4 caption:
    table_text: TABLE 4 Average Cross-Subject Performance for MSR-Action3D Dataset
      on Three Action Subsets of [5]
  Table 5 caption:
    table_text: TABLE 5 Performance Comparison for MSR-Action3D Dataset over All Action
      Classes
  Table 6 caption:
    table_text: TABLE 6 Performance Comparison for 3D Action Pairs Dataset
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2505295
- Affiliation of the first author: state key laboratory for manufacturing systems
    engineering, xi'an jiaotong university, xi'an, shaanxi, china
  Affiliation of the last author: department of biostatistics, university of texas
    health science center at houston, houston, tx
  Figure 1 Link: articels_figures_by_rev_year\2015\Clustering_TreeStructured_Data_on_Manifold\figure_1.jpg
  Figure 1 caption: Illustration of the support tree, the branch indexing scheme,
    and the degenerated T-A matrix.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Clustering_TreeStructured_Data_on_Manifold\figure_2.jpg
  Figure 2 caption: Examples of the degenerated T-A matrix, T-A matrix with one attribute,
    and T-A matrix with multiple attributes.
  Figure 3 Link: articels_figures_by_rev_year\2015\Clustering_TreeStructured_Data_on_Manifold\figure_3.jpg
  Figure 3 caption: Illustration of the coordinate system of a meta-tree space. The
    dashed axes represent the basis vectors spanning the meta-tree space and the solid-line
    axes are for an orthogonal coordinate system.
  Figure 4 Link: articels_figures_by_rev_year\2015\Clustering_TreeStructured_Data_on_Manifold\figure_4.jpg
  Figure 4 caption: Illustration of transforms between two trees.
  Figure 5 Link: articels_figures_by_rev_year\2015\Clustering_TreeStructured_Data_on_Manifold\figure_5.jpg
  Figure 5 caption: The transition path through a strict consensus tree in the meta-tree
    space. (a) Tree transition path, (b) Unfolded transition path.
  Figure 6 Link: articels_figures_by_rev_year\2015\Clustering_TreeStructured_Data_on_Manifold\figure_6.jpg
  Figure 6 caption: Trees of dataset 1 from group 1. (a) Set 1, (b) Set 2.
  Figure 7 Link: articels_figures_by_rev_year\2015\Clustering_TreeStructured_Data_on_Manifold\figure_7.jpg
  Figure 7 caption: Ten vessel trees extracted from the retinal images in STARE. (a)
    Five samples from subjects with retinopathy, (b) Five samples from normal subjects.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.95
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Na Lu
  Name of the last author: Hongyu Miao
  Number of Figures: 7
  Number of Tables: 7
  Number of authors: 2
  Paper title: Clustering Tree-Structured Data on Manifold
  Publication Date: 2015-12-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance of TAMBAC on 2-ary Trees
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance of TAMBAC on 2-ary Trees versus 3-ary Trees
  Table 3 caption:
    table_text: TABLE 3 Performance of Tambac on 3D Synthesized Vascular Trees
  Table 4 caption:
    table_text: TABLE 4 Performance of TAMBAC on Stare Data
  Table 5 caption:
    table_text: TABLE 5 Metric Comparison on 3D Vascular Trees
  Table 6 caption:
    table_text: TABLE 6 Metric Comparison on Stare Data
  Table 7 caption:
    table_text: TABLE 7 Clustering Accuracy of TAMBAC under Different Noise Conditions
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2505282
- Affiliation of the first author: center for research on intelligent perception and
    computing (cripac), national laboratory of pattern recognition (nlpr), institute
    of automation, chinese academy of sciences (casia), beijing, china
  Affiliation of the last author: center for research on intelligent perception and
    computing (cripac), national laboratory of pattern recognition (nlpr), institute
    of automation, chinese academy of sciences (casia), beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2015\Joint_Feature_Selection_and_Subspace_Learning_for_CrossModal_Retrieval\figure_1.jpg
  Figure 1 caption: The overview of the proposed method. U 1 and U 2 are projection
    matrices learned by our method for image and text spaces, respectively. They project
    different modalities of data into a common space, while performing feature selection
    on different feature spaces. The proposed method also preserves the inter-modality
    and intra-modality similarity relationships when mapping.
  Figure 10 Link: articels_figures_by_rev_year\2015\Joint_Feature_Selection_and_Subspace_Learning_for_CrossModal_Retrieval\figure_10.jpg
  Figure 10 caption: Performance variation for the Text query versus Image database
    task with respect to lambda 1 and lambda 2 when we fix beta for the Pascal VOC,
    NUS-WIDE and Wiki datasets respectively.
  Figure 2 Link: articels_figures_by_rev_year\2015\Joint_Feature_Selection_and_Subspace_Learning_for_CrossModal_Retrieval\figure_2.jpg
  Figure 2 caption: The multimodal graph is constructed according to inter-modality
    and intra-modality similarity relationships.
  Figure 3 Link: articels_figures_by_rev_year\2015\Joint_Feature_Selection_and_Subspace_Learning_for_CrossModal_Retrieval\figure_3.jpg
  Figure 3 caption: Performance of different methods on the Pascal VOC dataset, based
    on precision-scope curve (a-b) for K = 50 to 1,000 and precision-recall curve
    (c-d).
  Figure 4 Link: articels_figures_by_rev_year\2015\Joint_Feature_Selection_and_Subspace_Learning_for_CrossModal_Retrieval\figure_4.jpg
  Figure 4 caption: Performance of different methods on the NUS-WIDE dataset, based
    on precision-scope curve (a-b) for K = 1,000 to 20,000 and precision-recall curve
    (c-d).
  Figure 5 Link: articels_figures_by_rev_year\2015\Joint_Feature_Selection_and_Subspace_Learning_for_CrossModal_Retrieval\figure_5.jpg
  Figure 5 caption: Performance of different methods on the Wiki dataset, based on
    precision-scope curve (a-b) for K = 50 to 1,000 and precision-recall curve (c-d).
  Figure 6 Link: articels_figures_by_rev_year\2015\Joint_Feature_Selection_and_Subspace_Learning_for_CrossModal_Retrieval\figure_6.jpg
  Figure 6 caption: "An example of cross-modal retrieval using text query (i.e., the\
    \ tags \u201Daeroplane+sky+building+shadow\u201D) on the Pascal VOC dataset. Red\
    \ border indicates a incorrect retrieval result."
  Figure 7 Link: articels_figures_by_rev_year\2015\Joint_Feature_Selection_and_Subspace_Learning_for_CrossModal_Retrieval\figure_7.jpg
  Figure 7 caption: Four examples of cross-modal retrieval using text queries on the
    Wiki dataset. The text query and ground truth image are shown on the left; the
    top three images retrieved by our method are presented at the right.
  Figure 8 Link: articels_figures_by_rev_year\2015\Joint_Feature_Selection_and_Subspace_Learning_for_CrossModal_Retrieval\figure_8.jpg
  Figure 8 caption: Selected words on some categories by JFSSL from the Wiki dataset.
  Figure 9 Link: articels_figures_by_rev_year\2015\Joint_Feature_Selection_and_Subspace_Learning_for_CrossModal_Retrieval\figure_9.jpg
  Figure 9 caption: Performance variation for the Image query versus Text database
    task with respect to lambda 1 and lambda 2 when we fix beta for the Pascal VOC,
    NUS-WIDE and Wiki datasets respectively.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Kaiye Wang
  Name of the last author: Tieniu Tan
  Number of Figures: 12
  Number of Tables: 7
  Number of authors: 5
  Paper title: Joint Feature Selection and Subspace Learning for Cross-Modal Retrieval
  Publication Date: 2015-12-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 MAP Comparison of Different Methods on the Pascal VOC Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 MAP Comparison of Different Methods on the NUS-WIDE Dataset
  Table 3 caption:
    table_text: TABLE 3 MAP Comparison of Different Methods on the Wiki Dataset
  Table 4 caption:
    table_text: TABLE 4 MAP Comparison of Different Methods on the Wiki Dataset for
      the Image-to-Image Retrieval Task
  Table 5 caption:
    table_text: TABLE 5 MAP Comparison of Different Methods on the Wiki Dataset in
      the Three-Modality Case
  Table 6 caption:
    table_text: TABLE 6 Evaluation of Regularization Terms on the Wiki Dataset in
      the Three-Modality Case
  Table 7 caption:
    table_text: TABLE 7 MAP Comparison with Different Features on the Wiki Dataset
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2505311
- Affiliation of the first author: department of information engineering, the chinese
    university of hong kong, hong kong, china
  Affiliation of the last author: department of electronic engineering, the chinese
    university of hong kong, hong kong, china
  Figure 1 Link: articels_figures_by_rev_year\2015\Hybrid_Deep_Learning_for_Face_Verification\figure_1.jpg
  Figure 1 caption: The hybrid ConvNet-RBM model. The blue arrows show the forward
    propagation directions. Best viewed in color.
  Figure 10 Link: articels_figures_by_rev_year\2015\Hybrid_Deep_Learning_for_Face_Verification\figure_10.jpg
  Figure 10 caption: RBM prediction accuracies on LFW test data w.r.t. the number
    of hidden neurons. The accuracy keeps almost unchanged.
  Figure 2 Link: articels_figures_by_rev_year\2015\Hybrid_Deep_Learning_for_Face_Verification\figure_2.jpg
  Figure 2 caption: Architecture of the hybrid ConvNet-RBM model. For the convenience
    of illustration, we show the input and output layers of the RBM as two separate
    layers instead of a single visible layer.
  Figure 3 Link: articels_figures_by_rev_year\2015\Hybrid_Deep_Learning_for_Face_Verification\figure_3.jpg
  Figure 3 caption: Structure of one ConvNet. The map numbers and sizes are illustrated
    as the length, width, and height of cuboids for the input layer and all the convolutional
    and max-pooling layers, respectively. The 3D convolution kernel sizes of the convolutional
    layers and the 2D pooling region sizes of the max-pooling layers are shown as
    the small cuboids and squares inside the large cuboids of maps, respectively.
    Neuron numbers of the fully-connected layers are marked beside each layer.
  Figure 4 Link: articels_figures_by_rev_year\2015\Hybrid_Deep_Learning_for_Face_Verification\figure_4.jpg
  Figure 4 caption: 'Top: 10 color face regions of medium scales. The five regions
    in the top left and top right are the so-called global and local regions, respectively.
    Bottom: three scales of two particular regions. The region sizes, after being
    re-scaled to fit the ConvNet input dimensions, are marked above each region.'
  Figure 5 Link: articels_figures_by_rev_year\2015\Hybrid_Deep_Learning_for_Face_Verification\figure_5.jpg
  Figure 5 caption: Eight possible input modes for a pair of face regions.
  Figure 6 Link: articels_figures_by_rev_year\2015\Hybrid_Deep_Learning_for_Face_Verification\figure_6.jpg
  Figure 6 caption: "Examples of the learned 20 4\xD74 filter pairs of the first convolutional\
    \ layer of ConvNets taking color (line 1,2) and gray (line 3,4) input region pairs,\
    \ respectively. The upper and lower filters in each pair convolve with the two\
    \ face regions in comparison, respectively, and the results are added. We use\
    \ line 1-4 and column a-j to indicate each filter pair. For filter pairs in which\
    \ one filter varies greatly while the other remains near uniform (1c, 1g, 1h,\
    \ 2e, 2i, 3h, 3i, 4g, 4i, 4j), features are extracted from the two input regions\
    \ separately. For those filter pairs in which both filters vary, some kind of\
    \ relations between the two input regions are extracted. Among the latter, many\
    \ filter pairs extract simple relations such as addition (1b, 1f, 3g, 4f) or subtraction\
    \ (1i, 2b, 2c, 2d, 2f, 3c, 3e, 3f, 4b, 4c, 4d), while others extract more complex\
    \ relations. Interestingly, we find that filters in some filter pairs are similar\
    \ to those in some others, except that the order of the two filters are inversed\
    \ (1a versus 1e, 1h versus 2g, 2c versus 2d, 1i versus 2b, 3j versus 4b, 4i versus\
    \ 4j). This makes sense since face similarities should be invariant with the order\
    \ of the two face regions in comparison. Best viewed in color."
  Figure 7 Link: articels_figures_by_rev_year\2015\Hybrid_Deep_Learning_for_Face_Verification\figure_7.jpg
  Figure 7 caption: Training accuracies w.r.t. the number of training epoches, averaged
    over the 60 ConvNets trained on the 60 different region pairs, with ReLU (S0),
    tanh (S5), and abstanh (S6) neuron activation functions, respectively. Best viewed
    in color.
  Figure 8 Link: articels_figures_by_rev_year\2015\Hybrid_Deep_Learning_for_Face_Verification\figure_8.jpg
  Figure 8 caption: Face regions with the highest and lowest face verification accuracies,
    respectively.
  Figure 9 Link: articels_figures_by_rev_year\2015\Hybrid_Deep_Learning_for_Face_Verification\figure_9.jpg
  Figure 9 caption: Average RBM prediction accuracies based on features extracted
    from 1, 5, 15, 30, and 60 face region pairs. The accuracy is consistently improved
    when concatenating more features.
  First author gender probability: 0.54
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.93
  Name of the first author: Yi Sun
  Name of the last author: Xiaoou Tang
  Number of Figures: 12
  Number of Tables: 7
  Number of authors: 3
  Paper title: Hybrid Deep Learning for Face Verification
  Publication Date: 2015-12-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of Network Structures
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average Test Accuracies for ConvNets Taking Structures S0-S6
      Specified in Table 1
  Table 3 caption:
    table_text: TABLE 3 Test Accuracies of Various Features and Classifiers
  Table 4 caption:
    table_text: TABLE 4 Accuracy Comparison on Each Learning Step of Our Hybrid ConvNet-RBM
      Model
  Table 5 caption:
    table_text: TABLE 5 Accuracy Comparison between the Proposed Classification RBM
      and Single- and Multi-Layer Perceptrons for Face Verification in the Hybrid
      Deep Model
  Table 6 caption:
    table_text: TABLE 6 The Estimated Mean Accuracy and the Standard Error of the
      Mean of Our Hybrid ConvNet-RBM Model and the State-of-the-Art Methods under
      the LFW Unrestricted Protocol
  Table 7 caption:
    table_text: TABLE 7 The Estimated Mean Accuracy and the Standard Error of the
      Mean of Our Hybrid ConvNet-RBM Model and the State-of-the-Art Methods that Rely
      on Outside Training Data
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2505293
- Affiliation of the first author: department of computer science and engineering,
    university of california, riverside, ca
  Affiliation of the last author: department of computer science and engineering,
    university of california, riverside, ca
  Figure 1 Link: articels_figures_by_rev_year\2015\Social_Grouping_for_MultiTarget_Tracking_and_Head_Pose_Estimation_in_Video\figure_1.jpg
  Figure 1 caption: (Left) Social grouping behavior not only generally exists in one
    scene, but also usually persists (with the same group members) across wide areas.
    (Right) Given head images alone, it is sometimes difficult for human beings to
    correctly identify head pose directions in challenging scenarios. Social context
    provides strong evidence for this difficult problem.
  Figure 10 Link: articels_figures_by_rev_year\2015\Social_Grouping_for_MultiTarget_Tracking_and_Head_Pose_Estimation_in_Video\figure_10.jpg
  Figure 10 caption: Percentage of correctly linked pairs on the four video sequences
    with three cameras. The videos consist of 17, 24, 9, and 14 (64 in total) ground
    truth linked pairs respectively.
  Figure 2 Link: articels_figures_by_rev_year\2015\Social_Grouping_for_MultiTarget_Tracking_and_Head_Pose_Estimation_in_Video\figure_2.jpg
  Figure 2 caption: "(Left) Motion dependency problem for order-one association methods\
    \ [16]: though \u03C4 1 \u2212 \u03C4 2 and \u03C4 2 \u2212 \u03C4 3 can be reasonably\
    \ pairwise linked, the full trajectory is not probable. (Middle, Right) Social\
    \ context from \u03C4 4 gives strong evidence to disambiguate the dependency among\
    \ tracks, indicating \u03C4 1 \u2212 \u03C4 2 \u2212 \u03C4 3 is probable (middle)\
    \ or not (right)."
  Figure 3 Link: articels_figures_by_rev_year\2015\Social_Grouping_for_MultiTarget_Tracking_and_Head_Pose_Estimation_in_Video\figure_3.jpg
  Figure 3 caption: A factor graph showing how variables and cliques interact in the
    CRF. A graph of three head images and only two unary features are shown for simplicity.
    If there are more people in a group or more unary features, this graph can be
    straight-forwardly augmented.
  Figure 4 Link: articels_figures_by_rev_year\2015\Social_Grouping_for_MultiTarget_Tracking_and_Head_Pose_Estimation_in_Video\figure_4.jpg
  Figure 4 caption: "Structure-aware head pose angle difference. Nodes are head images\
    \ and dark blue arrows are head directions. Relative positions within group members\
    \ are considered. The difference is simply \u03B2\u2212\u03B1 . A positive number\
    \ implies social attraction."
  Figure 5 Link: articels_figures_by_rev_year\2015\Social_Grouping_for_MultiTarget_Tracking_and_Head_Pose_Estimation_in_Video\figure_5.jpg
  Figure 5 caption: 'Two social interaction modes with structure-aware head direction
    angle difference. Left: dynamical social interaction mode, fitted with two exponentials
    on either side of 0 degree. Right: static social interaction mode, fitted with
    two Gaussians on either side of 90 degrees. The specific distributions (exponential
    and Gaussian) are chosen due to their expressive power in this application and
    simplicity to express in the negative log space. The fitted distributions are
    rescaled and are for illustration only; their actual parameters are learned from
    training data.'
  Figure 6 Link: articels_figures_by_rev_year\2015\Social_Grouping_for_MultiTarget_Tracking_and_Head_Pose_Estimation_in_Video\figure_6.jpg
  Figure 6 caption: Some representative tracking results for CAVIAR dataset.
  Figure 7 Link: articels_figures_by_rev_year\2015\Social_Grouping_for_MultiTarget_Tracking_and_Head_Pose_Estimation_in_Video\figure_7.jpg
  Figure 7 caption: Topology of the cameras in the experiments.
  Figure 8 Link: articels_figures_by_rev_year\2015\Social_Grouping_for_MultiTarget_Tracking_and_Head_Pose_Estimation_in_Video\figure_8.jpg
  Figure 8 caption: One representative tracking result for PETS dataset.
  Figure 9 Link: articels_figures_by_rev_year\2015\Social_Grouping_for_MultiTarget_Tracking_and_Head_Pose_Estimation_in_Video\figure_9.jpg
  Figure 9 caption: Percentage of correctly linked pairs on the four video sequences
    with four cameras. The videos consist of 27, 5, 5, and 23 (60 in total) ground
    truth linked pairs respectively.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Zhen Qin
  Name of the last author: Christian R. Shelton
  Number of Figures: 13
  Number of Tables: 6
  Number of authors: 2
  Paper title: Social Grouping for Multi-Target Tracking and Head Pose Estimation
    in Video
  Publication Date: 2015-12-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Datasets Used for Each Task in the Experiments
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Comparison of the Tracking Result on the CAVIAR Dataset:
      75 Ground Truth (GT) Tracks'
  Table 3 caption:
    table_text: TABLE 3 Comparison of the Tracking Result on the PETS 2009 Dataset
  Table 4 caption:
    table_text: TABLE 4 Comparison of the Tracking Result on the TUD-Stadtmitte Dataset
  Table 5 caption:
    table_text: TABLE 5 Comparison of the Head Pose Estimation Results on the TownCentre,
      CAVIAR and PETS 2009 Dataset
  Table 6 caption:
    table_text: TABLE 6 Comparison of the Group Discovery Result on the PSUHub Dataset
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2505292
