- Affiliation of the first author: the university of tokyo, tokyo, japan
  Affiliation of the last author: microsoft research asia, china
  Figure 1 Link: articels_figures_by_rev_year\2015\Adherent_Raindrop_Modeling_Detection_and_Removal_in_Video\figure_1.jpg
  Figure 1 caption: (a-e) The various appearances of raindrops. (e-f) The detection
    and removal result by our method.
  Figure 10 Link: articels_figures_by_rev_year\2015\Adherent_Raindrop_Modeling_Detection_and_Removal_in_Video\figure_10.jpg
  Figure 10 caption: The detection pipeline. Our method can work in real time if using
    only the intensity change.
  Figure 2 Link: articels_figures_by_rev_year\2015\Adherent_Raindrop_Modeling_Detection_and_Removal_in_Video\figure_2.jpg
  Figure 2 caption: (a) Balance at a raindrop surface. A denotes a two-phase point.
    B denotes a three-phase point. T denotes a surface tension, and P for pressure.
    At two-phase point A , surface tension T and pressure P are balanced. Three-phase
    point is an intersection of water, air and glass, while two-phase point is an
    intersection between air-water. (b) Change of the angle of tangent along a raindrop
    boundary.
  Figure 3 Link: articels_figures_by_rev_year\2015\Adherent_Raindrop_Modeling_Detection_and_Removal_in_Video\figure_3.jpg
  Figure 3 caption: Smoothness and roundness of some shapes.
  Figure 4 Link: articels_figures_by_rev_year\2015\Adherent_Raindrop_Modeling_Detection_and_Removal_in_Video\figure_4.jpg
  Figure 4 caption: "(a) A raindrop is a contracted image of the environment. (b)\
    \ On the image plane, there is a smooth mapping \u03C6 starting from the raindrop\
    \ into the environment. (c) The contraction ratios from the environment to a raindrop\
    \ are significant."
  Figure 5 Link: articels_figures_by_rev_year\2015\Adherent_Raindrop_Modeling_Detection_and_Removal_in_Video\figure_5.jpg
  Figure 5 caption: The refraction model of two points on an image plane ( P e and
    P r ) that are originated from the same point in the environment. There are two
    refractions on the light path passing a raindrop. The camera lens cover or protecting
    shield is assumed to be a thin plane and thus can be neglected.
  Figure 6 Link: articels_figures_by_rev_year\2015\Adherent_Raindrop_Modeling_Detection_and_Removal_in_Video\figure_6.jpg
  Figure 6 caption: "Rows: The appearance and model of pixels on an image plane collecting\
    \ light from A: environment, B: raindrop, C: both. Columns: (a) The light path\
    \ model. Green light: the light coming from environment point; Blue light: the\
    \ light refracted by a raindrop. (b) Raindrop-plane-cut of the model in (a). Green\
    \ circle: the area of light collected. Blue circle: the raindrop. \u03B1 : percentage\
    \ of light collected from the raindrop. (b') Light path coverage when the raindrop\
    \ is small. (c) The appearance of the 3 situations in (b). (c') The appearance\
    \ of the 3 situations in (b')."
  Figure 7 Link: articels_figures_by_rev_year\2015\Adherent_Raindrop_Modeling_Detection_and_Removal_in_Video\figure_7.jpg
  Figure 7 caption: "a. \u03B1 -channel of a disk with various blur kernels. b. Raindrops\
    \ with varying f-stop. c. Raindrops with varying angles. Raindrop appearance is\
    \ highly directional."
  Figure 8 Link: articels_figures_by_rev_year\2015\Adherent_Raindrop_Modeling_Detection_and_Removal_in_Video\figure_8.jpg
  Figure 8 caption: The accumulated optic flow as a feature.
  Figure 9 Link: articels_figures_by_rev_year\2015\Adherent_Raindrop_Modeling_Detection_and_Removal_in_Video\figure_9.jpg
  Figure 9 caption: The accumulated intensity changes as a feature.
  First author gender probability: 0.77
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Shaodi You
  Name of the last author: Katsushi Ikeuchi
  Number of Figures: 26
  Number of Tables: 1
  Number of authors: 5
  Paper title: Adherent Raindrop Modeling, Detection and Removal in Video
  Publication Date: 2015-10-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Raindrop Dynamic of Scenes in Fig. 19
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2491937
- Affiliation of the first author: "vision4robotics group (acin\u2014technical university\
    \ of vienna), wien, austria"
  Affiliation of the last author: "vision4robotics group (acin\u2014technical university\
    \ of vienna), wien, austria"
  Figure 1 Link: articels_figures_by_rev_year\2015\A_Global_Hypothesis_Verification_Framework_for_D_Object_Recognition_in_Clutter\figure_1.jpg
  Figure 1 caption: Our 3D object recognition methodology leverages on both local
    and global features, with hypotheses gathered from the two pipelines merged into
    the final Global Hypothesis Verification stage. When available, color information
    can be deployed within the local pipeline.
  Figure 10 Link: articels_figures_by_rev_year\2015\A_Global_Hypothesis_Verification_Framework_for_D_Object_Recognition_in_Clutter\figure_10.jpg
  Figure 10 caption: Recognition Rate versus Occlusion Rate on the Queen's dataset
    (all 80 scenes). For Taati and Greenspan, the average recognition rate rather
    than the full chart is plotted, according to the data reported in [6].
  Figure 2 Link: articels_figures_by_rev_year\2015\A_Global_Hypothesis_Verification_Framework_for_D_Object_Recognition_in_Clutter\figure_2.jpg
  Figure 2 caption: "GC constraint ambiguity: in this toy example, let the three points\
    \ p 1 , p 2 , p 3 on the model (left-side) be associated with the respective ones\
    \ on the scene (right-side), thus forming the three correspondences c 1 , c 2\
    \ , c 3 . If the current consensus set only contains c 1 , by evaluating p 2 ,\
    \ all points belonging to the sphere centered in p 1 and of radius || p 2 \u2212\
    \ p 1 || will satisfy the GC constraint. If the consensus set contains both c\
    \ 1 and c 2 , when evaluating p 3 , all points lying on the intersection of the\
    \ two spheres centered in p 1 and in p 2 of radius, respectively, || p 3 \u2212\
    \ p 1 || and || p 3 \u2212 p 2 || , (depicted in red) will satisfy the constraint."
  Figure 3 Link: articels_figures_by_rev_year\2015\A_Global_Hypothesis_Verification_Framework_for_D_Object_Recognition_in_Clutter\figure_3.jpg
  Figure 3 caption: 'Cues for GHV. Top left: a solution visualized by super-imposing
    the set of active model hypotheses onto the sensed scene points displayed in red.
    Top right: scene labeling via smooth surface segmentation. Bottom left: classification
    of scene points between explained by a single hypothesis (blue) or multiple hypotheses
    (green), unexplained (red), penalized by the clutter cue (purple). Bottom right
    : classification of visible model points between inliers (orange) and outliers
    (green).'
  Figure 4 Link: articels_figures_by_rev_year\2015\A_Global_Hypothesis_Verification_Framework_for_D_Object_Recognition_in_Clutter\figure_4.jpg
  Figure 4 caption: 'Tonal registration across object''s smooth faces. The L -channel
    is displayed as a grayscale image. From left to right: relevant part of the scene,
    object hypothesis, visible smooth faces, object hypothesis after independent tonal
    registration of each smooth face. In both rows, thanks to independent re-mapping
    of each smooth face, model points get tonally registered to either darker of brighter
    scene surfaces.'
  Figure 5 Link: articels_figures_by_rev_year\2015\A_Global_Hypothesis_Verification_Framework_for_D_Object_Recognition_in_Clutter\figure_5.jpg
  Figure 5 caption: Results by the different optimization algorithms on datasets (a),
    (b). Suffix RM indicates the use of replace active hypothesis moves in addition
    to switch state moves. SA and TS are both configured with N max =100 . The plot
    displays the number of evaluated moves until convergence (green bars, left vertical
    axis in logarithmic scale) as well as the average cost (blue markers, right vertical
    axis) over the whole dataset. The F-score is also reported between parenthesis
    below each optimization method, so to highlight the impact on recognition results.
  Figure 6 Link: articels_figures_by_rev_year\2015\A_Global_Hypothesis_Verification_Framework_for_D_Object_Recognition_in_Clutter\figure_6.jpg
  Figure 6 caption: 'From left to right: scene, object and plane hypotheses, verified
    hypotheses for the 6 considered datasets.'
  Figure 7 Link: articels_figures_by_rev_year\2015\A_Global_Hypothesis_Verification_Framework_for_D_Object_Recognition_in_Clutter\figure_7.jpg
  Figure 7 caption: GGC versus IGC with the local pipeline (SHOT and, when applicable,
    SIFT).
  Figure 8 Link: articels_figures_by_rev_year\2015\A_Global_Hypothesis_Verification_Framework_for_D_Object_Recognition_in_Clutter\figure_8.jpg
  Figure 8 caption: Impact of color strategies on the Challenge dataset.
  Figure 9 Link: articels_figures_by_rev_year\2015\A_Global_Hypothesis_Verification_Framework_for_D_Object_Recognition_in_Clutter\figure_9.jpg
  Figure 9 caption: Recognition Rate versus Occlusion Rate on the Laser Scanner dataset
    (without rhino ).
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Aitor Aldoma
  Name of the last author: Markus Vincze
  Number of Figures: 11
  Number of Tables: 5
  Number of authors: 4
  Paper title: A Global Hypothesis Verification Framework for 3D Object Recognition
    in Clutter
  Publication Date: 2015-10-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Features Included in Vector f q
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Datasets Properties
  Table 3 caption:
    table_text: TABLE 3 Precision, Recall and F-Score on the Six Benchmark Datasets
  Table 4 caption:
    table_text: "TABLE 4 PrecisionRecall for Different Values of Parameter \u03C1\
      \ e"
  Table 5 caption:
    table_text: "TABLE 5 PrecisionRecall for Different Values of Parameter \u03C1\
      \ e without Planar Hypotheses"
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2491940
- Affiliation of the first author: institute of information science, beijing jiaotong
    university, beijing, china
  Affiliation of the last author: department of electrical and computer engineering,
    national university of singapore, singapore
  Figure 1 Link: articels_figures_by_rev_year\2015\HCP_A_Flexible_CNN_Framework_for_MultiLabel_Image_Classification\figure_1.jpg
  Figure 1 caption: Some examples from ImageNet [10] and Pascal VOC 2007 [13]. The
    foreground objects in single-label images are usually roughly aligned. However,
    the assumption of object alighment is not valid for multi-label images. Also note
    the partial visibility and occlusion between objects in the multi-label images.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\HCP_A_Flexible_CNN_Framework_for_MultiLabel_Image_Classification\figure_2.jpg
  Figure 2 caption: An illustration of the infrastructure of the proposed HCP. For
    a given multi-label image, a set of input hypotheses to the shared CNN is selected
    based on the proposals generated by the state-of-the-art objectness detection
    techniques, e.g., BING [9] or EdgeBoxes [44]. We feed the selected hypotheses
    into the shared CNN and fuse the outputs into a c -dimensional prediction vector
    with cross-hypothesis max-pooling operation, where c is the category number of
    the target multi-label dataset. The shared CNN is firstly pre-trained on the single-label
    image dataset, e.g., ImageNet and then fine-tuned with the multi-label images
    based on the squared loss function. Finally, we retrain the whole HCP to further
    fine-tune the parameters for multi-label image classification.
  Figure 3 Link: articels_figures_by_rev_year\2015\HCP_A_Flexible_CNN_Framework_for_MultiLabel_Image_Classification\figure_3.jpg
  Figure 3 caption: (a) Source image. (b) Hypothesis bounding boxes generated by BING.
    Different colors indicate different clusters, which are produced by normalized
    cut. (c) Hypotheses directly generated by the bounding boxes. (d) Hypotheses generated
    by the proposed HS method.
  Figure 4 Link: articels_figures_by_rev_year\2015\HCP_A_Flexible_CNN_Framework_for_MultiLabel_Image_Classification\figure_4.jpg
  Figure 4 caption: An illustration of the proposed HCP for a VOC 2007 test image.
    The second row indicates the generated hypotheses. The third row indicates the
    predicted results for the input hypotheses. The last row is the predicted result
    for the test image after cross-hypothesis max-pooling.
  Figure 5 Link: articels_figures_by_rev_year\2015\HCP_A_Flexible_CNN_Framework_for_MultiLabel_Image_Classification\figure_5.jpg
  Figure 5 caption: (a) The predicted result based on I-FT model. (b) The predicted
    result based on HCP model.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yunchao Wei
  Name of the last author: Shuicheng Yan
  Number of Figures: 5
  Number of Tables: 4
  Number of authors: 8
  Paper title: 'HCP: A Flexible CNN Framework for Multi-Label Image Classification'
  Publication Date: 2015-10-26 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Improvements of Hypothesis Fine-Tuning Based on Two Kinds
      of Shared CNNs (mAP in %).
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Classification Results (mAP in %) by Varying the Number of
      Hypotheses During the Testing Stage on VOC 2007
  Table 3 caption:
    table_text: TABLE 3 Classification Results (AP in %) Comparison for State-of-the-Art
      Approaches on VOC 2007 Test
  Table 4 caption:
    table_text: TABLE 4 Classification Results (AP in %) Comparison for State-of-the-Art
      Approaches on VOC 2012 Test
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2491929
- Affiliation of the first author: computer science department at the university of
    freiburg
  Affiliation of the last author: computer science department at the university of
    freiburg
  Figure 1 Link: articels_figures_by_rev_year\2015\Discriminative_Unsupervised_Feature_Learning_with_Exemplar_Convolutional_Neural_\figure_1.jpg
  Figure 1 caption: Exemplary patches sampled from the STL unlabeled dataset which
    are later augmented by various transformations to obtain surrogate data for the
    CNN training.
  Figure 10 Link: articels_figures_by_rev_year\2015\Discriminative_Unsupervised_Feature_Learning_with_Exemplar_Convolutional_Neural_\figure_10.jpg
  Figure 10 caption: Mean average precision on the Flickr dataset for various transformations.
    Except for the blur transformation, all networks perform consistently better than
    SIFT. The network trained with blur transformations can keep up with SIFT even
    on blur.
  Figure 2 Link: articels_figures_by_rev_year\2015\Discriminative_Unsupervised_Feature_Learning_with_Exemplar_Convolutional_Neural_\figure_2.jpg
  Figure 2 caption: Several random transformations applied to one of the patches extracted
    from the STL unlabeled dataset. The original ('seed') patch is in the top left
    corner.
  Figure 3 Link: articels_figures_by_rev_year\2015\Discriminative_Unsupervised_Feature_Learning_with_Exemplar_Convolutional_Neural_\figure_3.jpg
  Figure 3 caption: Influence of the number of surrogate training classes. The validation
    error on the surrogate data is shown in red. Note the different y-axes for the
    two curves.
  Figure 4 Link: articels_figures_by_rev_year\2015\Discriminative_Unsupervised_Feature_Learning_with_Exemplar_Convolutional_Neural_\figure_4.jpg
  Figure 4 caption: Classification performance on STL for different numbers of samples
    per class. Random filters can be seen as '0 samples per class'.
  Figure 5 Link: articels_figures_by_rev_year\2015\Discriminative_Unsupervised_Feature_Learning_with_Exemplar_Convolutional_Neural_\figure_5.jpg
  Figure 5 caption: Influence of removing groups of transformations during generation
    of the surrogate training data. Baseline (' 0 ' value) is applying all transformations.
    Each group of three bars corresponds to removing some of the transformations.
  Figure 6 Link: articels_figures_by_rev_year\2015\Discriminative_Unsupervised_Feature_Learning_with_Exemplar_Convolutional_Neural_\figure_6.jpg
  Figure 6 caption: 'Invariance properties of the feature representation learned by
    Exemplar-CNN. Top: transformations applied to an image patch (translation, rotation,
    contrast, saturation, color). Bottom: invariance of different feature representations.
    (a)-(c): Normalized Euclidean distance between feature vectors of the original
    and the translated image patches versus the magnitude of the transformation, (d)-(f):
    classification performance on transformed image patches versus the magnitude of
    the transformation for various magnitudes of transformations applied for creating
    the surrogate training data.'
  Figure 7 Link: articels_figures_by_rev_year\2015\Discriminative_Unsupervised_Feature_Learning_with_Exemplar_Convolutional_Neural_\figure_7.jpg
  Figure 7 caption: "Filters learned by first layers of 64c5-64c5-128f networks when\
    \ training on surrogate data from various dataset. Top \u2013 from STL-10, middle\
    \ \u2013 CIFAR-10, bottom \u2013 Caltech-101."
  Figure 8 Link: articels_figures_by_rev_year\2015\Discriminative_Unsupervised_Feature_Learning_with_Exemplar_Convolutional_Neural_\figure_8.jpg
  Figure 8 caption: Analysis of the matching performance depending on the patch size
    and the network layer at which features are computed.
  Figure 9 Link: articels_figures_by_rev_year\2015\Discriminative_Unsupervised_Feature_Learning_with_Exemplar_Convolutional_Neural_\figure_9.jpg
  Figure 9 caption: Scatter plots for different pairs of descriptors on the Flickr
    dataset (upper row) and the Mikolajczyk dataset (lower row). Each point in a scatter
    plot corresponds to one image pair, and its coordinates are the AP values obtained
    with the compared descriptors. AlexNet (supervised training) and the Exemplar-CNN
    yield features that outperform SIFT on most images of the Flickr dataset (a,b),
    but AlexNet is inferior to SIFT on the Mikolajczyk dataset. Features obtained
    with the unsupervised training procedure outperform the features from AlexNet
    on both datasets (c,f).
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Alexey Dosovitskiy
  Name of the last author: Thomas Brox
  Number of Figures: 11
  Number of Tables: 6
  Number of authors: 5
  Paper title: Discriminative Unsupervised Feature Learning with Exemplar Convolutional
    Neural Networks
  Publication Date: 2015-10-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Classification Accuracies on Several Datasets (in Percent)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Classification Accuracies with Random Filters
  Table 3 caption:
    table_text: TABLE 3 Classification Accuracies with Clustering (in Percent)
  Table 4 caption:
    table_text: TABLE 4 Dependence of Classification Performance (in %) on the Training
      and Testing Datasets
  Table 5 caption:
    table_text: TABLE 5 Classification Accuracy Depending on the Network Architecture
  Table 6 caption:
    table_text: TABLE 6 Feature Computation Times for a Patch of 113 by 113 Pixels
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2496141
- Affiliation of the first author: institute of systems and robotics, electrical and
    computer engineering department, university of coimbra, portugal
  Affiliation of the last author: robotic institute at the khalifa university, uae
  Figure 1 Link: articels_figures_by_rev_year\2015\Probabilistic_Social_Behavior_Analysis_by_Exploring_Body_MotionBased_Patterns\figure_1.jpg
  Figure 1 caption: Main components of the proposed multilevel framework for social
    behavior analysis. ac c bp denotes acceleration signals of bp body part motion.
    f LMA denotes the features extracted from LMA data, explained in Section 3.4,
    used as inputs in the estimation process of four Interpersonal Behaviors, which
    in turn are used to estimate a Social Role.
  Figure 10 Link: articels_figures_by_rev_year\2015\Probabilistic_Social_Behavior_Analysis_by_Exploring_Body_MotionBased_Patterns\figure_10.jpg
  Figure 10 caption: (a) LMA-Effort-Time signals (probability of the sustained state
    relating to a person's body parts); (b) correspondent second derivative signals;
    (c) PSD estimates obtained from (b).The selected features are defined by the first
    peak coefficients ( a , d ) of each periodogram, where a and d denote, respectively,
    the first peak amplitude and frequency, in the illustrated case corresponding
    to the periodogram of the right hand acceleration. In the presented example (a,d)cong
    (25,100) .
  Figure 2 Link: articels_figures_by_rev_year\2015\Probabilistic_Social_Behavior_Analysis_by_Exploring_Body_MotionBased_Patterns\figure_2.jpg
  Figure 2 caption: A sequence of body postures during a conversation scenario; the
    person in the right is leading the conversation.
  Figure 3 Link: articels_figures_by_rev_year\2015\Probabilistic_Social_Behavior_Analysis_by_Exploring_Body_MotionBased_Patterns\figure_3.jpg
  Figure 3 caption: 'Four LMA components: Body concerns poses of body parts related
    to the body centre; Space describes body parts motion trajectories; Shape illustrates
    whole human body deformation in three planes (vertical, horizontal, sagittal);
    Effort contains four dimensions, and each of those dimensions has bipolar states.
    Figures are from [24].'
  Figure 4 Link: articels_figures_by_rev_year\2015\Probabilistic_Social_Behavior_Analysis_by_Exploring_Body_MotionBased_Patterns\figure_4.jpg
  Figure 4 caption: The transformation process from a sequence of acceleration signals
    ac c bp to E f bp time parameter. The LMA Bayesian fusion is explained in Section
    3.3.
  Figure 5 Link: articels_figures_by_rev_year\2015\Probabilistic_Social_Behavior_Analysis_by_Exploring_Body_MotionBased_Patterns\figure_5.jpg
  Figure 5 caption: PSD samples of a sequence of acceleration signals ac c bp in a
    conversation scenario (concerns the leader of the conversation).
  Figure 6 Link: articels_figures_by_rev_year\2015\Probabilistic_Social_Behavior_Analysis_by_Exploring_Body_MotionBased_Patterns\figure_6.jpg
  Figure 6 caption: The general DBN for the IBs and SR estimation.
  Figure 7 Link: articels_figures_by_rev_year\2015\Probabilistic_Social_Behavior_Analysis_by_Exploring_Body_MotionBased_Patterns\figure_7.jpg
  Figure 7 caption: Histogram of the LMA parameters' states (listed in Table 3) of
    the Indicator's IB variable states. Blue and red bars identify the Influent and
    Influenced states, respectively.
  Figure 8 Link: articels_figures_by_rev_year\2015\Probabilistic_Social_Behavior_Analysis_by_Exploring_Body_MotionBased_Patterns\figure_8.jpg
  Figure 8 caption: Structure of a Bayesian program [5].
  Figure 9 Link: articels_figures_by_rev_year\2015\Probabilistic_Social_Behavior_Analysis_by_Exploring_Body_MotionBased_Patterns\figure_9.jpg
  Figure 9 caption: BP for the human movement model.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Kamrad Khoshhal Roudposhti
  Name of the last author: Jorge Dias
  Number of Figures: 23
  Number of Tables: 5
  Number of authors: 3
  Paper title: Probabilistic Social Behavior Analysis by Exploring Body Motion-Based
    Patterns
  Publication Date: 2015-10-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 A Brief Description of IBs with Their States, and Their Relevant
      LMA Components [27]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Relevant IBs States for the Leading Role [22]
  Table 3 caption:
    table_text: TABLE 3 List of LMA Parameters
  Table 4 caption:
    table_text: "TABLE 4 Effective LMA Components for Each IB [22], [27] ( X : Is\
      \ Not Relevant, and \u221A : Is Relevant)"
  Table 5 caption:
    table_text: TABLE 5 Positive Classification Rate (PCR) of the IBs and SR Models
      (Inft.:Influent, Infc.:Influenced, Act.:Active, Pas.:Passive, Mim.:Mimicry,
      Unc.:Uncoordinated, Con.:Consistent, Var.:Variable) Based on Trials
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2496209
- Affiliation of the first author: section for cognitive systems at the technical
    university of denmark
  Affiliation of the last author: section for cognitive systems at the technical university
    of denmark
  Figure 1 Link: articels_figures_by_rev_year\2015\Principal_Curves_on_Riemannian_Manifolds\figure_1.jpg
  Figure 1 caption: "Left: An initial example: observations (blue dots) are given\
    \ in the Poincar\xE9 half-plane. The intrinsic mean (red diamond) and the principal\
    \ geodesic (yellow dashed line) both do a poor job of summarizing the main trend\
    \ of the data due to the restrictive structure of geodesic curves. The principal\
    \ curve (purple line), in contrast, captures the main mode of variation well.\
    \ Right: Example geodesics in the Poincar\xE9 half-plane illustrate the geometry\
    \ of this space; geodesics are either vertical lines or half-circles."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Principal_Curves_on_Riemannian_Manifolds\figure_2.jpg
  Figure 2 caption: Self-consistency (9) implies that the expectation of all points
    that project to the same point on a curve equals the point of projection.
  Figure 3 Link: articels_figures_by_rev_year\2015\Principal_Curves_on_Riemannian_Manifolds\figure_3.jpg
  Figure 3 caption: "Synthetic data on the Poincar\xE9 half-plane along with the principal\
    \ geodesic and the principal curve. The principal curve is consistently better\
    \ at capturing the trend of the data."
  Figure 4 Link: articels_figures_by_rev_year\2015\Principal_Curves_on_Riemannian_Manifolds\figure_4.jpg
  Figure 4 caption: 'Left: Motion capture data is represented on the product manifold
    slSO(3 ) 32 ; here the data is shown in the first two (Euclidean) principal components
    in the tangent space at the intrinsic mean. The horizontal (yellow) line is then
    the principal geodesic. The (purple) closed curve is the principal curve mapped
    to the tangent space and projected into the first two principal components. The
    principal curve provides the best description of the data. Center: The cross-validation
    error of the smoothing parameter in the experiment corresponding to the left panel.
    Right: The relative orientation of the left femur of a person walking is represented
    as a point on the unit sphere. Here, the periodicity of walking implies that the
    data roughly lies on a small circle; this implies an intrinsic mean at the pole
    of the small circle. The principal geodesic, thus, fails to capture the trend
    of the data. The (closed) principal curve does better.'
  Figure 5 Link: articels_figures_by_rev_year\2015\Principal_Curves_on_Riemannian_Manifolds\figure_5.jpg
  Figure 5 caption: Motion capture data along with the corresponding projection onto
    both the principal geodesic and the principal curve. The principal curve captures
    the structure of data much better than the principal geodesic.
  Figure 6 Link: articels_figures_by_rev_year\2015\Principal_Curves_on_Riemannian_Manifolds\figure_6.jpg
  Figure 6 caption: Example images from the MNIST dataset. The orientation constitutes
    the main source of variation.
  Figure 7 Link: articels_figures_by_rev_year\2015\Principal_Curves_on_Riemannian_Manifolds\figure_7.jpg
  Figure 7 caption: MNIST images of the digit 1 is projected onto its first two (Euclidean)
    principal components, and a smoothly changing local diagonal metric is estimated
    as a local inverse covariance matrix. The left panel shows geodesics between data
    and the intrinsic mean; geodesics generally stay within the support of the data.
    The center panel shows geodesics originating at the intrinsic mean, shooting in
    different directions. The right panel shows the principal geodesic and the principal
    curve. Even when geodesics follow the trend of the data, the principal curve captures
    the overall trend better than the principal geodesic.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "S\xF8ren Hauberg"
  Name of the last author: "S\xF8ren Hauberg"
  Number of Figures: 7
  Number of Tables: 0
  Number of authors: 1
  Paper title: Principal Curves on Riemannian Manifolds
  Publication Date: 2015-10-29 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2496166
- Affiliation of the first author: department of cybernetics, centre for machine perception,
    czech technical university, praha, czech republic
  Affiliation of the last author: department of cybernetics, czech technical university,
    prague, czech republic
  Figure 1 Link: articels_figures_by_rev_year\2015\RealTime_LexiconFree_Scene_Text_Localization_and_Recognition\figure_1.jpg
  Figure 1 caption: Scene text localization and recognition by the proposed method.
  Figure 10 Link: articels_figures_by_rev_year\2015\RealTime_LexiconFree_Scene_Text_Localization_and_Recognition\figure_10.jpg
  Figure 10 caption: "The number of boundary inflexion points \u03BA . (a) Characters.\
    \ (b) Non-textual content."
  Figure 2 Link: articels_figures_by_rev_year\2015\RealTime_LexiconFree_Scene_Text_Localization_and_Recognition\figure_2.jpg
  Figure 2 caption: Overview of the method. For an input image the method finds a
    set of words, where each word is assigned a Unicode string and its position in
    a form of a rectangular bounding-box. Character candidates are effectively detected
    across multiple scales and image projections (channels). Note that this enumeration
    can be easily parallelized.
  Figure 3 Link: articels_figures_by_rev_year\2015\RealTime_LexiconFree_Scene_Text_Localization_and_Recognition\figure_3.jpg
  Figure 3 caption: "Intensity gradient magnitude channel \u2207 . (a) Source image.\
    \ (b) Projection output. (c) Extremal Regions at threshold \u03B8=24 (ERs bigger\
    \ than 30 percent of the image area excluded for better visualization)."
  Figure 4 Link: articels_figures_by_rev_year\2015\RealTime_LexiconFree_Scene_Text_Localization_and_Recognition\figure_4.jpg
  Figure 4 caption: "Processing with a Gaussian pyramid (the pyramid scale denoted\
    \ by s ). Characters formed of multiple small regions merge together into a single\
    \ region (left column). A single region which corresponds to characters \u201C\
    ME\u201D is broken into two regions and serifs are eliminated (right column)."
  Figure 5 Link: articels_figures_by_rev_year\2015\RealTime_LexiconFree_Scene_Text_Localization_and_Recognition\figure_5.jpg
  Figure 5 caption: "Incrementally computable descriptors. Regions already existing\
    \ at threshold \u03B8\u22121 marked grey, new pixels at threshold \u03B8 marked\
    \ red, the resulting region at threshold \u03B8 outlined with a dashed line."
  Figure 6 Link: articels_figures_by_rev_year\2015\RealTime_LexiconFree_Scene_Text_Localization_and_Recognition\figure_6.jpg
  Figure 6 caption: Typical number of regions and timings in each stage (character
    detection in a single channel only) on a standard 2 GHz PC.
  Figure 7 Link: articels_figures_by_rev_year\2015\RealTime_LexiconFree_Scene_Text_Localization_and_Recognition\figure_7.jpg
  Figure 7 caption: In the first stage of the sequential classification the probability
    p(character|r) of each ER is estimated using incrementally computable descriptors
    that exploit the inclusion relation of ERs. (a) A source image cut-out and the
    initial seed of the ER inclusion sequence (marked with a red cross). (b) The value
    of p(character|r) in the inclusion sequence, ERs passed to the second stage marked
    red.
  Figure 8 Link: articels_figures_by_rev_year\2015\RealTime_LexiconFree_Scene_Text_Localization_and_Recognition\figure_8.jpg
  Figure 8 caption: The horizontal crossings feature used in the 1st stage of ER classification.
  Figure 9 Link: articels_figures_by_rev_year\2015\RealTime_LexiconFree_Scene_Text_Localization_and_Recognition\figure_9.jpg
  Figure 9 caption: The precision-recall curve of the first stage of the sequential
    classifier obtained by cross-validation. The configuration used in the experiments
    marked red (recall 95.6 percent, precision 67.3 percent).
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "Luk\xE1\u0161 Neumann"
  Name of the last author: "Ji\u0159\xED Matas"
  Number of Figures: 22
  Number of Tables: 7
  Number of authors: 2
  Paper title: Real-Time Lexicon-Free Scene Text Localization and Recognition
  Publication Date: 2015-10-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Recall (R) and Precision (R) of Character Detection by ER
      Detectors in Individual Channels and Their Combinations
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison with Most Recent Text Localization Results on the
      ICDAR 2013 Dataset
  Table 3 caption:
    table_text: TABLE 3 Comparison with Most Recent End-to-End Text Recognition Results
      on the ICDAR 2013 Dataset (Case-Sensitive)
  Table 4 caption:
    table_text: TABLE 4 Comparison with the Most Recent End-to-End Word Detection
      and Recognition Results on the Street View Dataset
  Table 5 caption:
    table_text: "TABLE 5 ICDAR 2015 Robust Reading Competition [9]\u2014End-to-End\
      \ Incidental Text Recognition"
  Table 6 caption:
    table_text: "TABLE 6 ICDAR 2015 Robust Reading Competition [9]\u2014End-to-End\
      \ Video Text Recognition"
  Table 7 caption:
    table_text: "TABLE 7 ICDAR 2015 Robust Reading Competition [9]\u2014End-to-End\
      \ Focused Text Recognition"
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2496234
- Affiliation of the first author: department of information engineering and computer
    science, university of trento, trento, italy
  Affiliation of the last author: department of information engineering and computer
    science, university of trento, trento, italy
  Figure 1 Link: articels_figures_by_rev_year\2015\SALSA_A_Novel_Dataset_for_Multimodal_Group_Behavior_Analysis\figure_1.jpg
  Figure 1 caption: Analysis of round-table meetings (left, adopted from Mission Survival
    dataset [6]) has been attempted extensively- orderly spatial arrangement and sufficient
    separation between persons enable reliable extraction of behavioral cues for each
    person from such scenes. Exemplar SALSA frame (right) showing FCGs- varying illumination,
    low resolution of faces, extreme occlusions and crowdedness makes AASI highly
    challenging (best-viewed in color and under zoom).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\SALSA_A_Novel_Dataset_for_Multimodal_Group_Behavior_Analysis\figure_2.jpg
  Figure 2 caption: Existing datasets facilitating F-formation detection. Frame border
    colors encode sensing modalities.
  Figure 3 Link: articels_figures_by_rev_year\2015\SALSA_A_Novel_Dataset_for_Multimodal_Group_Behavior_Analysis\figure_3.jpg
  Figure 3 caption: 'Datasets for social interaction analysis: the first five consist
    on round-table meetings and span over hours, while the last four study social
    networksbehavior and span over daysmonths.'
  Figure 4 Link: articels_figures_by_rev_year\2015\SALSA_A_Novel_Dataset_for_Multimodal_Group_Behavior_Analysis\figure_4.jpg
  Figure 4 caption: Five annotated F-formations represented via connections between
    feet positions (crosses) of interacting targets. Corresponding O-spaces are denoted
    by the colored convex shapes.
  Figure 5 Link: articels_figures_by_rev_year\2015\SALSA_A_Novel_Dataset_for_Multimodal_Group_Behavior_Analysis\figure_5.jpg
  Figure 5 caption: Distributions of the big-five personality traits.
  Figure 6 Link: articels_figures_by_rev_year\2015\SALSA_A_Novel_Dataset_for_Multimodal_Group_Behavior_Analysis\figure_6.jpg
  Figure 6 caption: 'Synchronization procedure: Similarity scores for badgetarget
    IDs 5 , 10 and 16 as a function of the time-shift- a clear peak can be observed
    for all badges.'
  Figure 7 Link: articels_figures_by_rev_year\2015\SALSA_A_Novel_Dataset_for_Multimodal_Group_Behavior_Analysis\figure_7.jpg
  Figure 7 caption: Tracking on Part 1 - Poster (top row) and Part 2 - Party (bottom
    row). Best viewed in color.
  Figure 8 Link: articels_figures_by_rev_year\2015\SALSA_A_Novel_Dataset_for_Multimodal_Group_Behavior_Analysis\figure_8.jpg
  Figure 8 caption: Mean speaker recognition accuracy with different methods on SALSA.
  Figure 9 Link: articels_figures_by_rev_year\2015\SALSA_A_Novel_Dataset_for_Multimodal_Group_Behavior_Analysis\figure_9.jpg
  Figure 9 caption: F-formation detection results (F1 score). Head and body pose were
    automatically estimated from visual, infra-red and audio data.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Xavier Alameda-Pineda
  Name of the last author: Nicu Sebe
  Number of Figures: 9
  Number of Tables: 5
  Number of authors: 8
  Paper title: 'SALSA: A Novel Dataset for Multimodal Group Behavior Analysis'
  Publication Date: 2015-10-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Description of the Sensors used in SALSA
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Mean Tracking Statistics and Per-Target Occlusion Rates for
      the Four Views
  Table 3 caption:
    table_text: TABLE 3 Head and Body Pose Estimation Error (Degree)
  Table 4 caption:
    table_text: TABLE 4 F-formation Detection with Ground-Truth Data
  Table 5 caption:
    table_text: TABLE 5 Significant Pearson Correlations between the Big-Five Traits
      and IR and GT Network Features ( Denotes p<.01 )
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2496269
- Affiliation of the first author: department of electrical and computer engineering,
    university of maryland, college park, md
  Affiliation of the last author: department of electrical and computer engineering,
    university of maryland, college park, md
  Figure 1 Link: articels_figures_by_rev_year\2015\Face_Association_for_Videos_Using_Conditional_Random_Fields_and_MaxMargin_Markov\figure_1.jpg
  Figure 1 caption: Face association. A face association algorithm solves the correspondence
    problem between face detections and the identity labels.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Face_Association_for_Videos_Using_Conditional_Random_Fields_and_MaxMargin_Markov\figure_2.jpg
  Figure 2 caption: Face association pipeline. The flowchart of our face association
    pipeline.
  Figure 3 Link: articels_figures_by_rev_year\2015\Face_Association_for_Videos_Using_Conditional_Random_Fields_and_MaxMargin_Markov\figure_3.jpg
  Figure 3 caption: Context-aided face matching. The face appearance alone usually
    is not sufficient as a strong feature to perform association. Contextual information,
    such as clothing appearance and relative poses, can be incorporated to make a
    more confident decision.
  Figure 4 Link: articels_figures_by_rev_year\2015\Face_Association_for_Videos_Using_Conditional_Random_Fields_and_MaxMargin_Markov\figure_4.jpg
  Figure 4 caption: "The OAM and the probabilistic torso mask. From frame t\u2212\
    20 to t\u22122 there was partial occlusion, which is still present in the mean\
    \ of the S(stable) component of the recently updated OAM A t\u22121 (b). The occlusion\
    \ disappeared at frame t\u22122 . So in the current frame t we get a clean face\
    \ a t (a). (c) is the mean of the W(wander) component of A t\u22121 , which captures\
    \ this recent appearance change. (d) is produced by subtracting the posterior\
    \ mixture probability of S component from that of the W component. We can see\
    \ that the previously occluded region is much better accounted for by the W component\
    \ than by the S component. (e) The learned probabilistic mask of torso. The green\
    \ square marks the position of the reference face."
  Figure 5 Link: articels_figures_by_rev_year\2015\Face_Association_for_Videos_Using_Conditional_Random_Fields_and_MaxMargin_Markov\figure_5.jpg
  Figure 5 caption: Distributions of relative positions. The empirical distributions
    are visualized using histograms. The fitted Laplace distribution is plotted in
    red, and the Gaussian distribution is plotted in green. Parameters are set as
    the maximum likelihood estimates. The distribution of the x-direction distance
    variation is much larger than that of the y-direction, which makes sense since
    the human moves horizontally much more often than vertically.
  Figure 6 Link: articels_figures_by_rev_year\2015\Face_Association_for_Videos_Using_Conditional_Random_Fields_and_MaxMargin_Markov\figure_6.jpg
  Figure 6 caption: Samples used to recover the missing faces. Subject 2's face is
    missed by the face detector. Based on previous relative positionsize features
    and the current positionssizes of Subjects 1 and 3, we are able to generate samples
    (marked by the red bounding boxes) which form the candidates for the positionsize
    of the missed face. The green bounding box marks the final inferred face position.
  Figure 7 Link: articels_figures_by_rev_year\2015\Face_Association_for_Videos_Using_Conditional_Random_Fields_and_MaxMargin_Markov\figure_7.jpg
  Figure 7 caption: Sample face association results on the QMUL Multi-Face database.
    The three rows correspond to results for the frontal, fast, and turning sequences,
    from top to bottom.
  Figure 8 Link: articels_figures_by_rev_year\2015\Face_Association_for_Videos_Using_Conditional_Random_Fields_and_MaxMargin_Markov\figure_8.jpg
  Figure 8 caption: Sample face association results on the Big Bang Theory database.
  Figure 9 Link: articels_figures_by_rev_year\2015\Face_Association_for_Videos_Using_Conditional_Random_Fields_and_MaxMargin_Markov\figure_9.jpg
  Figure 9 caption: Sample face association results on the Buffy database.
  First author gender probability: 0.96
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.62
  Name of the first author: Ming Du
  Name of the last author: Rama Chellappa
  Number of Figures: 9
  Number of Tables: 5
  Number of authors: 2
  Paper title: Face Association for Videos Using Conditional Random Fields and Max-Margin
    Markov Networks
  Publication Date: 2015-11-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Face Association Algorithms on the QMUL Multi-Face
      Database
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Face Association Algorithms on the Big Bang
      Theory Database
  Table 3 caption:
    table_text: TABLE 3 Comparison of Face Association Algorithms on the Buffy Database
  Table 4 caption:
    table_text: TABLE 4 Comparison of Contextual Features on the Buffy Database (CRF)
  Table 5 caption:
    table_text: TABLE 5 Comparison of Contextual Features on the Buffy Database (
      M 3 Networks)
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2497689
- Affiliation of the first author: department of computer science and digital technologies,
    northumbria university, newcastle upon tyne, united kingdom
  Affiliation of the last author: state key lab of cad&cg, zhejiang university, hangzhou,
    zhejiang, china
  Figure 1 Link: articels_figures_by_rev_year\2015\Local_Feature_Discriminant_Projection\figure_1.jpg
  Figure 1 caption: 'Performance (percent) of linear SVMs with IFK in different lower-dimensional
    subspaces on the UIUC-Sports, Scene-15 and MIT Indoor datasets. Note that we only
    use one type of local descriptor: SIFT in single-scale patches.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Local_Feature_Discriminant_Projection\figure_2.jpg
  Figure 2 caption: The convergency of the objective function and the difference of
    variables with respect to the number of iteration.
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.71
  Name of the first author: Mengyang Yu
  Name of the last author: Xiaofei He
  Number of Figures: 2
  Number of Tables: 4
  Number of authors: 4
  Paper title: Local Feature Discriminant Projection
  Publication Date: 2015-11-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparing the Complexity of LFDP with Other Linear Algorithms
      on N Where K Is the Parameter of K-Means and k Is the Parameter of the KNN Algorithm
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Resource Requirements of Different Methods for the 900,000
      SIFT Features from the UIUC-Sports Dataset
  Table 3 caption:
    table_text: TABLE 3 Performance (Percent) of Linear SVMs with IFK After PCA, LDA
      and LFDP Reduction on Local Features
  Table 4 caption:
    table_text: TABLE 4 Comparing the Results (Percent) of LFDP with Different K Values
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2497686
