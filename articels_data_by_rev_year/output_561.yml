- Affiliation of the first author: state key laboratory of intelligent technology
    and systems, tsinghua national laboratory for information science and technology,
    department of electronic engineering, tsinghua university, beijing, china
  Affiliation of the last author: state key laboratory of intelligent technology and
    systems, tsinghua national laboratory for information science and technology,
    department of electronic engineering, tsinghua university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2016\Person_ReIdentification_by_Discriminative_Selection_in_Video_Ranking\figure_1.jpg
  Figure 1 caption: Person re-identification challenges in public space scenes [9].
    (a-b) The two images in each bounding box refer to the same person observed in
    different cameras.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Person_ReIdentification_by_Discriminative_Selection_in_Video_Ranking\figure_2.jpg
  Figure 2 caption: The training pipeline of the proposed discriminative video ranking
    framework. (a) Training image sequences, Q a i denotes the image sequence of person
    p i from camera a (see Section 3.1). (b,c) Generating candidate fragments for
    each sequence (see Section 3.2). (d) Creating cross-view fragment pairs as positive
    and negative instances and pooling them into positive and negative bags respectively
    (see Section 3.3 and Fig. 4). (e) Learning a ranking model by simultaneously selecting
    and ranking iteratively discriminative fragment pairs (see Section 3.3).
  Figure 3 Link: articels_figures_by_rev_year\2016\Person_ReIdentification_by_Discriminative_Selection_in_Video_Ranking\figure_3.jpg
  Figure 3 caption: (a) A person image sequence of 50 frames is shown, with the motion
    energy intensity for each frame given in (b). The red dots in (b) denote the automatically
    detected local minima and maxima temporal landmarks in the motion intensity profile,
    of which the corresponding frames are provided at the vertically-aligned positions
    in (c). (d) Two example video fragments with the landmark frames highlighted by
    red bounding boxes.
  Figure 4 Link: articels_figures_by_rev_year\2016\Person_ReIdentification_by_Discriminative_Selection_in_Video_Ranking\figure_4.jpg
  Figure 4 caption: An overview of constructing the positive and negative bags of
    inter-camera fragment-pairs. This is to generate the training data from image
    sequences for DVR model learning. Examples of three people are illustrated. In
    particular, si,jkappa denotes the j th fragment from the i th person image sequence
    captured by camera kappa , kappa in lbrace a, brbrace . We build separately a
    positive ( Bi+ ) and negative ( Bi- ) bag for the i th person. Consider the first
    person p1 as an example, the cross-view pairings (blue lines) on fragments from
    p1 are used to form the positive bag B1+ , whilst other pairings (red lines) between
    p1 and a different person pi, ine 1 , shall create the negative bag B1- .
  Figure 5 Link: articels_figures_by_rev_year\2016\Person_ReIdentification_by_Discriminative_Selection_in_Video_Ranking\figure_5.jpg
  Figure 5 caption: Example pairs of image sequences from the same people appearing
    in different camera views given by (a) iLIDS-VID [53], (b) PRID 2011 [54], (c)
    a 5 fps (frames per second) camera pair (19, 40) from HDA+ [55], and (d) a 2 fps
    camera pair (50, 57) from HDA+ [55]. Note, severe appearance changes occur in
    HDA+ due to explicit clothing variation for some people in HDA+ ((c,d)); whilst
    significant appearance changes also occur in iLIDS-VID due to severe occlusion
    (bottom pair of (a)), and mostly less extreme appearance changes exist in iLIDS-VID
    and PRID 2011 due to changes in viewpoint and lighting ((a,b)).
  Figure 6 Link: articels_figures_by_rev_year\2016\Person_ReIdentification_by_Discriminative_Selection_in_Video_Ranking\figure_6.jpg
  Figure 6 caption: 'Three examples of the GEI gait features and the DVR video fragment
    pairs. Top: from PRID 2011 ; Middle: from iLIDS-VID; Bottom: from HDA+( 5 fps).
    In each example, the leftmost thumbnail shows GEI gait features, while the remaining
    thumbnails present some examples of fragment pairs, with the automatically selected
    pairs marked by red bounding boxes. A fragment is visualised as the weighted average
    of all its frames with emphasis on its central frame.'
  Figure 7 Link: articels_figures_by_rev_year\2016\Person_ReIdentification_by_Discriminative_Selection_in_Video_Ranking\figure_7.jpg
  Figure 7 caption: 'Examples of automatically selected cross-camera fragment pairs
    (indicated with red bounding box) from the HDA+ dataset during person ReID by
    our DVR model. Top row: from HDA+( 5 fps); Bottom row: from HDA+( 2 fps). A fragment
    is visualised as the weighted average of all its frames with emphasis on its central
    frame.'
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Taiqing Wang
  Name of the last author: Shengjin Wang
  Number of Figures: 7
  Number of Tables: 6
  Number of authors: 4
  Paper title: Person Re-Identification by Discriminative Selection in Video Ranking
  Publication Date: 2016-01-27 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Compare Different DVR Variants
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Compare Different Video Fragment Representations Using the
      DVR(Single) Model
  Table 3 caption:
    table_text: TABLE 3 Comparison with Gait Recognition and Temporal Sequence Matching
      Methods
  Table 4 caption:
    table_text: TABLE 4 Comparing Spatial Appearance Feature Based ReID Methods
  Table 5 caption:
    table_text: TABLE 5 Complementary Effect of the DVR Model to Existing Spatial
      Appearance Feature Based Models
  Table 6 caption:
    table_text: TABLE 6 The Effect of Space-Time Video Fragment Selection
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2522418
- Affiliation of the first author: robotics institute, carnegie mellon university,
    pittsburgh, pa
  Affiliation of the last author: robotics institute, carnegie mellon university,
    pittsburgh, pa
  Figure 1 Link: articels_figures_by_rev_year\2016\SpatioTemporal_Matching_for_Human_Pose_Estimation_in_Video\figure_1.jpg
  Figure 1 caption: Detection and tracking of humans in three videos using STM. STM
    extracts trajectories in video (gray lines) and selects a subset of trajectories
    (a) that match with the 3D motion capture model (b) learned from the CMU motion
    capture data set. Better viewed in color.
  Figure 10 Link: articels_figures_by_rev_year\2016\SpatioTemporal_Matching_for_Human_Pose_Estimation_in_Video\figure_10.jpg
  Figure 10 caption: Comparison of human pose estimation on the Human 3.6M dataset.
    (a) Result of [5] and our method on three actions of two views, where the 3 D
    reconstruction estimated by our method is plotted on the right. (b) PCK accuracy
    for each action. (c) PCK accuracy for each camera view. (d) PCK accuracy of each
    joint. (e) PCK as a function of threshold alpha .
  Figure 2 Link: articels_figures_by_rev_year\2016\SpatioTemporal_Matching_for_Human_Pose_Estimation_in_Video\figure_2.jpg
  Figure 2 caption: Overview of the STM method. Given a video, STM extracts video
    features and selects a subset of video features that match a 3D motion capture
    model.
  Figure 3 Link: articels_figures_by_rev_year\2016\SpatioTemporal_Matching_for_Human_Pose_Estimation_in_Video\figure_3.jpg
  Figure 3 caption: Example of feature trajectories and their responses. (a) Geometrical
    configuration of 14 body joints shared across 3 D datasets. (b) Dense trajectories
    extracted from a video segment. (c) Feature response maps for 4 joints (see bottom-right
    corner).
  Figure 4 Link: articels_figures_by_rev_year\2016\SpatioTemporal_Matching_for_Human_Pose_Estimation_in_Video\figure_4.jpg
  Figure 4 caption: 'Spatio-temporal bilinear model learned from the CMU motion capture
    dataset. (a) Top: All the motion capture segments randomly selected from a set
    of kicking sequences. Bottom: The segments are spatially aligned via Procrustes
    alignment. (b) Clustering motion capture segments into four temporal clusters.
    (c) The bilinear bases estimated from the third cluster. Left: top- 2 shape bases
    ( s i ) where the shape deformation is visualized by black arrows. Top: top- 3
    DCT trajectory bases ( t j ). Bottom-right: bilinear reconstruction by combining
    each pair of shape and DCT bases ( t j s T i ).'
  Figure 5 Link: articels_figures_by_rev_year\2016\SpatioTemporal_Matching_for_Human_Pose_Estimation_in_Video\figure_5.jpg
  Figure 5 caption: Clustering motion capture segments into four clusters for different
    datasets. (a) CMU motion capture dataset [42]. (b) Berkeley MHAD dataset [9] .
    (c) Human3.6M dataset [10]. (d) CMU MAD dataset [11].
  Figure 6 Link: articels_figures_by_rev_year\2016\SpatioTemporal_Matching_for_Human_Pose_Estimation_in_Video\figure_6.jpg
  Figure 6 caption: Comparison of human pose estimation on the CMU motion capture
    dataset. (a) Original motion capture key-frames in 3 D with 50 ( beta = 0.04 )
    outliers that were synthetically generated. (b) Results of the greedy approach
    and our method on four 2 D projections.
  Figure 7 Link: articels_figures_by_rev_year\2016\SpatioTemporal_Matching_for_Human_Pose_Estimation_in_Video\figure_7.jpg
  Figure 7 caption: Comparison of human pose estimation error on the CMU motion capture
    dataset. (a) Mean error and std. for each method and each action as a function
    of the number of outliers when beta = 0.04 . (b) Mean error and std for each camera
    view when beta = 0.04 . (c) Mean error and std for all actions and cameras when
    beta = 0.04 . (d) Mean error and std for all actions and cameras as a function
    of the velocity ( beta ) of 50 random outliers.
  Figure 8 Link: articels_figures_by_rev_year\2016\SpatioTemporal_Matching_for_Human_Pose_Estimation_in_Video\figure_8.jpg
  Figure 8 caption: Comparison of human pose estimation on the Berkeley MHAD dataset.
    (a) Result of [5] and our method on three actions of two views, where the 3 D
    reconstruction estimated by our method is plotted on the right. Videos are available
    at www.f-zhou.comhpefig8avdo1.avi , www.f-zhou.comhpefig8avdo2.avi and www.f-zhou.comhpefig8avdo3.avi
    . (b) PCK accuracy for each action. (c) PCK accuracy for each camera view. (d)
    PCK accuracy of each joint. (e) PCK as a function of threshold alpha .
  Figure 9 Link: articels_figures_by_rev_year\2016\SpatioTemporal_Matching_for_Human_Pose_Estimation_in_Video\figure_9.jpg
  Figure 9 caption: Comparison of human pose estimation on the Berkeley MHAD dataset.
    (a) Errors with respect to the segment length ( n ). (b) Errors with respect to
    the bases number ( ks and kt ). (c) Errors with respect to the regularization
    weights ( lambda a and lambda s ). (d) Time cost of each step.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Feng Zhou
  Name of the last author: Fernando De la Torre
  Number of Figures: 11
  Number of Tables: 0
  Number of authors: 2
  Paper title: Spatio-Temporal Matching for Human Pose Estimation in Video
  Publication Date: 2016-02-04 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2526002
- Affiliation of the first author: institute of electrical measurement and measurement
    signal processing, tu graz, austria
  Affiliation of the last author: department of electrical engineering and computer
    science and the centre for vision research, york university, toronto, canada
  Figure 1 Link: articels_figures_by_rev_year\2016\Dynamic_Scene_Recognition_with_Complementary_Spatiotemporal_Features\figure_1.jpg
  Figure 1 caption: Examples for small inter class differences (a) and large intra
    class variations (b) from the YUPENN [5] (a) and the Maryland [4] (b) datasets.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Dynamic_Scene_Recognition_with_Complementary_Spatiotemporal_Features\figure_2.jpg
  Figure 2 caption: "Dynamically Pooled Complementary Features (DPCF) Overview. First,\
    \ spatial v s , temporal v t , and color v c features are extracted at each spatiotemporal\
    \ location x=(x,y,t ) \u22A4 from a temporal slice of the input video. Second,\
    \ features are encoded into a mid-level representation learned for the task and\
    \ also used to extract dynamic pooling energies. Third, the encoded features are\
    \ pooled via a novel dynamic spacetime pyramid that adapts to the temporal image\
    \ structure, as guided by the pooling energies. The pooled encodings are concatenated\
    \ into vectors that serve as the final representation for online recognition."
  Figure 3 Link: articels_figures_by_rev_year\2016\Dynamic_Scene_Recognition_with_Complementary_Spatiotemporal_Features\figure_3.jpg
  Figure 3 caption: "Spatial (first row) and temporal (second row) primitives for\
    \ one temporal slice of a Windmill sequence (a) from the YUPENN dataset. (b)-(i)\
    \ illustrate the spatial filtering results (1) for eight orientations \u03B8 .\
    \ (j)-(q) illustrate the dynamic energies, (4), for the following directions n\
    \ =( n x , n y , n t ) \u22A4 : static (j), rightward (k), downward (l), leftward\
    \ (m), upward (n), horizontal flicker (o), and vertical flicker (p). Further,\
    \ (q) illustrates the no structure channel, (5). The hottest colors (i.e., red)\
    \ indicate the largest responses across each frame."
  Figure 4 Link: articels_figures_by_rev_year\2016\Dynamic_Scene_Recognition_with_Complementary_Spatiotemporal_Features\figure_4.jpg
  Figure 4 caption: Distribution of dynamic pooling energies of a street sequence
    from the YUPENN Dataset. (b)-(f) show the decomposition of the sequence into a
    distribution of pooling energies indicating stationarityhomogeneity in (b) and
    coarse coherent motion for several directions in (c)-(f). Hotter colors (e.g.,
    red) correspond to larger filter responses.
  Figure 5 Link: articels_figures_by_rev_year\2016\Dynamic_Scene_Recognition_with_Complementary_Spatiotemporal_Features\figure_5.jpg
  Figure 5 caption: Dynamic spacetime pyramid. Larger responses (hotter colors) in
    the static energies guide pooling into coarse (L0) as well as fine grids (L1 and
    L2). In contrast, coherent motion energies limit pooling to the coarsest grid
    only (L0). Green arrows along the sides of the coherent motion plots indicate
    directional tunings. Input imagery is from the Street Sequence, as shown in Fig.
    4.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Christoph Feichtenhofer
  Name of the last author: Richard P. Wildes
  Number of Figures: 5
  Number of Tables: 5
  Number of authors: 3
  Paper title: Dynamic Scene Recognition with Complementary Spatiotemporal Features
  Publication Date: 2016-02-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Results for Combinations of Feature Primitives, Encodings
      and Pooling. The Average Recognition Accuracy in % for Classification with One
      Versus Rest Linear SVMs is Reported
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Overall Classification Accuracy for Different Codebook Sizes
      When Using LLC and IFV Encoded Features Pooled via the Proposed Dynamic Pooling
      Framework
  Table 3 caption:
    table_text: TABLE 3 Classification Accuracy (in %) for the Best Performing Approaches
      on the Maryland Dataset
  Table 4 caption:
    table_text: TABLE 4 Classification Accuracy (in %) for the Best Performing Approaches
      on the YUPENN Dataset
  Table 5 caption:
    table_text: TABLE 5 Confusion Matrices for DPCF for the Maryland (left) and YUPENN
      (right) Dataset. The Columns Show the Predicted Labels of the Classifier, while
      the Rows List the Actual Ground Truth Label
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2526008
- Affiliation of the first author: school of computer science and software engineering,
    the university of western australia, 35 stirling highway crawley, wa, australia
  Affiliation of the last author: school of computer science and software engineering,
    the university of western australia, 35 stirling highway crawley, wa, australia
  Figure 1 Link: articels_figures_by_rev_year\2016\Discriminative_Bayesian_Dictionary_Learning_for_Classification\figure_1.jpg
  Figure 1 caption: "Schematics of the proposed approach: For training, a set of probability\
    \ distributions over the dictionary atoms, i.e., \u2135 , is learned. We also\
    \ infer sets of Bernoulli distributions indicating the probabilities of selection\
    \ of the dictionary atoms in the expansion of data from each class. These distributions\
    \ are used for inferring the support of the sparse codes. The (parameters of)\
    \ Bernoulli distributions are later used for learning a classifier. The final\
    \ dictionary is learned by sampling the distributions in \u2135 , whereas the\
    \ sparse codes are computed as element-wise product of the support and the weights\
    \ (also inferred by the approach) of the codes. Combined, the dictionary and the\
    \ codes faithfully represent the training data. For testing, sparse codes of the\
    \ query over the dictionary are computed and fed to the classifier for labeling."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Discriminative_Bayesian_Dictionary_Learning_for_Classification\figure_2.jpg
  Figure 2 caption: "Examples of how recognition accuracy is affected with varying\
    \ dictionary size: \u03BA=0 for LC-KSVD1 and \u03C5=0 for D-KSVD in Eq. (3). All\
    \ other parameters are kept constant at optimal values reported in [9]. For the\
    \ AR database, 2,000 training instances are used and testing is performed with\
    \ 600 instances. For the Extended YaleB, half of the database is used for training\
    \ and the other half is used for testing. The instances are selected uniformly\
    \ at random."
  Figure 3 Link: articels_figures_by_rev_year\2016\Discriminative_Bayesian_Dictionary_Learning_for_Classification\figure_3.jpg
  Figure 3 caption: Graphical representation of the proposed discriminative Bayesian
    dictionary learning model.
  Figure 4 Link: articels_figures_by_rev_year\2016\Discriminative_Bayesian_Dictionary_Learning_for_Classification\figure_4.jpg
  Figure 4 caption: "Illustration of the discriminative character of the inferred\
    \ dictionary: From top, the four rows present results on AR database [1], Extended\
    \ YaleB [2], Caltech-101[3] and 15 Scene categories [4] , respectively. In each\
    \ plot, the x-axis represents k\u2208K and the y-axis shows the corresponding\
    \ probability of selection of the kth dictionary atom in the expansion of the\
    \ data. A plot represents a single \u03C0 c vector learned as a result of Bayesian\
    \ inference. For the first three rows, from left to right, the value of c (i.e.,\
    \ class label) is 1, 5, 10, 15, 20 and 25, respectively. For the fourth row the\
    \ value of c is 1, 3, 5, 7, 9 and 11 for the plots from left to right. Plots clearly\
    \ show distinct clusters of high probabilities for different classes."
  Figure 5 Link: articels_figures_by_rev_year\2016\Discriminative_Bayesian_Dictionary_Learning_for_Classification\figure_5.jpg
  Figure 5 caption: Examples from the face databases.
  Figure 6 Link: articels_figures_by_rev_year\2016\Discriminative_Bayesian_Dictionary_Learning_for_Classification\figure_6.jpg
  Figure 6 caption: Examples from Caltech-101 database [3]. The proposed approach
    achieves 100 percent accuracy on these classes.
  Figure 7 Link: articels_figures_by_rev_year\2016\Discriminative_Bayesian_Dictionary_Learning_for_Classification\figure_7.jpg
  Figure 7 caption: Examples images from eight different categories in 15 Scene categories
    dataset [4] .
  Figure 8 Link: articels_figures_by_rev_year\2016\Discriminative_Bayesian_Dictionary_Learning_for_Classification\figure_8.jpg
  Figure 8 caption: Examples from UCF sports action dataset [5].
  Figure 9 Link: articels_figures_by_rev_year\2016\Discriminative_Bayesian_Dictionary_Learning_for_Classification\figure_9.jpg
  Figure 9 caption: Size of the inferred dictionary, i.e., |mathcal K| , as a function
    of the Gibbs sampling iterations. The plots show first hundred iterations for
    each dataset.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Naveed Akhtar
  Name of the last author: Ajmal Mian
  Number of Figures: 9
  Number of Tables: 6
  Number of authors: 3
  Paper title: Discriminative Bayesian Dictionary Learning for Classification
  Publication Date: 2016-02-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Recognition Accuracy with Random-Face Features on the Extended
      YaleB Database [2]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Recognition Accuracy with Random-Face Features on the AR Database
      [1]
  Table 3 caption:
    table_text: TABLE 3 Classification Results Using Spatial Pyramid Features on the
      Caltech-101 Dataset [3]
  Table 4 caption:
    table_text: TABLE 4 Computation Time for Training and Testing on Caltech-101 Database
  Table 5 caption:
    table_text: TABLE 5 Classification Accuracy on 15 Scene Category Dataset [4] Using
      Spatial Pyramid Features
  Table 6 caption:
    table_text: TABLE 6 Classification Rates on UCF Sports Action [5]
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2527652
- Affiliation of the first author: google inc
  Affiliation of the last author: center for visual computing, centralesupelec, inria,
    university of paris-saclay, france
  Figure 1 Link: articels_figures_by_rev_year\2016\HigherOrder_Graph_Principles_towards_NonRigid_Surface_Registration\figure_1.jpg
  Figure 1 caption: Overview of our algorithmic framework for surface registration
    as described in Section 1.2.
  Figure 10 Link: articels_figures_by_rev_year\2016\HigherOrder_Graph_Principles_towards_NonRigid_Surface_Registration\figure_10.jpg
  Figure 10 caption: 'Matching result for the lion data: (matchedtotal = 1,1051,251
    ).'
  Figure 2 Link: articels_figures_by_rev_year\2016\HigherOrder_Graph_Principles_towards_NonRigid_Surface_Registration\figure_2.jpg
  Figure 2 caption: "An example showing the matching ambiguity when considering only\
    \ intrinsic information. The matching scores in (a) and (b) are the same according\
    \ to Eq. (6) based on the M\xF6bius transform, since the distances between the\
    \ matched features are identical. However, such ambiguity can be avoided by adding\
    \ extrinsic similarity information (e.g., normal and curvature)."
  Figure 3 Link: articels_figures_by_rev_year\2016\HigherOrder_Graph_Principles_towards_NonRigid_Surface_Registration\figure_3.jpg
  Figure 3 caption: "The finite element method assumes the transformation between\
    \ facets to be piecewise linear and f( ab \u2192 )= a \u2032 b \u2032 \u2192 ,f(\
    \ ac \u2192 )= a \u2032 c \u2032 \u2192 . Under the linearity assumption, the\
    \ Jacobian can be computed in a closed form for each pair of triangular facets\
    \ \u25B3abc\u21A6\u25B3 a \u2032 b \u2032 c \u2032 ."
  Figure 4 Link: articels_figures_by_rev_year\2016\HigherOrder_Graph_Principles_towards_NonRigid_Surface_Registration\figure_4.jpg
  Figure 4 caption: An expression deformation prior obtained by 3 D scanned data with
    markers. (a) and (c) show the 3D scan of the onset and peak of a facial expression
    with large shape deformations respectively. (b) and (d) are the corresponding
    triangular templates constructed from the 3D scan data. (d) also shows the CDCs
    of the template's triangular facets for the deformation from (b) to (d), using
    the color map shown in (e). The histogram of the CDC values are shown in (f) and
    (g).
  Figure 5 Link: articels_figures_by_rev_year\2016\HigherOrder_Graph_Principles_towards_NonRigid_Surface_Registration\figure_5.jpg
  Figure 5 caption: 2 D Illustration of candidate selection scheme.
  Figure 6 Link: articels_figures_by_rev_year\2016\HigherOrder_Graph_Principles_towards_NonRigid_Surface_Registration\figure_6.jpg
  Figure 6 caption: "An example showing candidate points obtained from different M\xF6\
    bius transforms and their clustering. For any point p from the source surface,\
    \ the clustering of its matching candidate points on the target surface gives\
    \ us a matching candidate pprime ."
  Figure 7 Link: articels_figures_by_rev_year\2016\HigherOrder_Graph_Principles_towards_NonRigid_Surface_Registration\figure_7.jpg
  Figure 7 caption: Comparisons between our algorithm and the tensor matching algorithm
    [31] . (a) shows the performance in partial matching when n=30 points is matched
    to n+n1 points. Our method is strongly robust to outliers. (b) shows the use of
    partial pairwise terms for matching constraints to overcome memory limit for full
    matching ( n1 = 0 ) and our method remains robust with increasing n .
  Figure 8 Link: articels_figures_by_rev_year\2016\HigherOrder_Graph_Principles_towards_NonRigid_Surface_Registration\figure_8.jpg
  Figure 8 caption: Performance analysis of our MRF optimization algorithm. (a) shows
    the optimality using the test cases described in Section 4.2. (b) shows the speedup
    using the parallel implementation of Algorithm 3. L is the number of labels for
    each node. We show the runtime per iteration since different inputs have different
    iteration counts.
  Figure 9 Link: articels_figures_by_rev_year\2016\HigherOrder_Graph_Principles_towards_NonRigid_Surface_Registration\figure_9.jpg
  Figure 9 caption: 'Matching result for the body data: (matchedtotal = 2,8613,376
    ).'
  First author gender probability: 0.55
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Yun Zeng
  Name of the last author: Nikos Paragios
  Number of Figures: 15
  Number of Tables: 1
  Number of authors: 5
  Paper title: Higher-Order Graph Principles towards Non-Rigid Surface Registration
  Publication Date: 2016-02-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison with a Recent Intrinsic Method for Dense Surface
      Registration [49]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2528240
- Affiliation of the first author: "swiss center for electronics and microtechnology\
    \ (csem), neuch\xE2tel, switzerland"
  Affiliation of the last author: computer vision laboratory, epfl, lausanne, switzerland
  Figure 1 Link: articels_figures_by_rev_year\2016\Reconstructing_Curvilinear_Networks_Using_Path_Classifiers_and_Integer_Programmi\figure_1.jpg
  Figure 1 caption: 2D and 3D datasets used to test our approach. (a) Aerial image
    of roads. (b) Maximum intensity projection (MIP) of a confocal image stack of
    blood vessels, which appear in red. It features cycles created by circulatory
    anastomoses or by capillaries connecting arteries to veins. (c) Minimum intensity
    projection of a brightfield stack of neurons. (d) MIP of a confocal stack of olfactory
    projection axons. (e) MIP of a brainbow [1] stack. As most images in this paper,
    they are best visualized in color.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Reconstructing_Curvilinear_Networks_Using_Path_Classifiers_and_Integer_Programmi\figure_2.jpg
  Figure 2 caption: Enforcing tree topology results in creation of spurious junctions.
    (a) Image with two crossing branches. The dots depict sample points (seeds) on
    the centerlines found by maximizing a tubularity measure. In 3D, these branches
    may be disjoint but the z -resolution is insufficient to see it and only a single
    sample is found at their intersection, which we color yellow. (b) The samples
    are connected by geodesic paths to form a graph. (c, d) The final delineation
    is obtained by finding a subgraph that minimizes a global cost function. In (c),
    we prevent samples from being used more than once, resulting in an erroneous delineation.
    In (d), we allow the yellow point to be used twice, and penalize creation of early
    terminations and spurious junctions, which yields a better result.
  Figure 3 Link: articels_figures_by_rev_year\2016\Reconstructing_Curvilinear_Networks_Using_Path_Classifiers_and_Integer_Programmi\figure_3.jpg
  Figure 3 caption: Algorithmic steps. (a) Aerial image of a suburban neighborhood.
    (b) 3D scale-space tubularity image. (c) Graph obtained by linking the seed points.
    They are shown in red with the path centerlines overlaid in green. (d) The same
    graph with probabilities assigned to edges using our path classification approach.
    Blue and transparent denote low probabilities, red and opaque high ones. Note
    that only the paths lying on roads appear in red. (e) Reconstruction obtained
    by our approach.
  Figure 4 Link: articels_figures_by_rev_year\2016\Reconstructing_Curvilinear_Networks_Using_Path_Classifiers_and_Integer_Programmi\figure_4.jpg
  Figure 4 caption: Scoring paths by classification versus Integration. (a) Tubular
    graph of Fig. 3 c with edge weights computed by integrating tubularity values
    along the paths instead of using our path classification approach. We use the
    same color scheme as in Fig. 3d to demonstrate how much less informative these
    weights are. (b) In one of the microscopy stacks of the DIADEM data, scoring paths
    by summing tubularity values in, from left to right, shortcuts, spurious branches,
    and missing branches denoted by the red circles at the top. Using our classification
    approach to scoring paths yields the right answer in all three cases, as shown
    in the bottom row.
  Figure 5 Link: articels_figures_by_rev_year\2016\Reconstructing_Curvilinear_Networks_Using_Path_Classifiers_and_Integer_Programmi\figure_5.jpg
  Figure 5 caption: "Three aspects of our feature extraction process. An extended\
    \ neighborhood of points around the path centerline C(s) is defined as the envelope\
    \ of cross-sectional circles shown in black. This neighborhood is divided into\
    \ R radius intervals highlighted by the yellow, green and red tubes (here R=3\
    \ ) and a histogram is created for each such interval. A point x contributes a\
    \ weighted vote to an angular bin according to the angle between the normal N(x)\
    \ and the image gradient \u2207I(x) at that point."
  Figure 6 Link: articels_figures_by_rev_year\2016\Reconstructing_Curvilinear_Networks_Using_Path_Classifiers_and_Integer_Programmi\figure_6.jpg
  Figure 6 caption: Flowchart of our approach to extracting appearance features from
    tubular paths of arbitrary length.
  Figure 7 Link: articels_figures_by_rev_year\2016\Reconstructing_Curvilinear_Networks_Using_Path_Classifiers_and_Integer_Programmi\figure_7.jpg
  Figure 7 caption: Considering geometric relationships between edges helps at junctions.
    (a) A closeup of the graph built by our algorithm at a branching point. (b) Minimizing
    a sum of individual path costs yields these overlapping paths. (c) Accounting
    for edge pair geometry yields the correct connectivity.
  Figure 8 Link: articels_figures_by_rev_year\2016\Reconstructing_Curvilinear_Networks_Using_Path_Classifiers_and_Integer_Programmi\figure_8.jpg
  Figure 8 caption: Reasoning about edge-pairs. (a) A loopy graph with a root vertex
    r , in red. Allowing vertex c , in green, to be used by the two different branches,
    denoted by blue and yellow arrows, produces a loopy solution instead of a tree.
    However, describing the crossing in terms of edge pairs e icj and e kcl being
    active, and e kcj and e icl being inactive makes it possible to eventually recover
    the correct tree topology. (b) Handling overlapping edges. (c) Enforcing crossover
    consistency.
  Figure 9 Link: articels_figures_by_rev_year\2016\Reconstructing_Curvilinear_Networks_Using_Path_Classifiers_and_Integer_Programmi\figure_9.jpg
  Figure 9 caption: 'Delineation results, best viewed in color. Top Rows: For each
    dataset, two minimal or maximal projections and overlaid delineation results.
    Each connected curvilinear network is shown in a distinct color. Bottom Row: Four
    road images with final delineations shifted and overlaid to allow comparisons.
    The renderings are created using the Vaa3D software [54].'
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: "Engin T\xFCretken"
  Name of the last author: Pascal Fua
  Number of Figures: 9
  Number of Tables: 4
  Number of authors: 6
  Paper title: Reconstructing Curvilinear Networks Using Path Classifiers and Integer
    Programming
  Publication Date: 2016-02-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Reconstruction Accuracy in Terms of the DIADEM [2] Metric
      on Four Test Stacks of the Confocal-Axons Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 DIADEM Scores for the Brainbow (BRBW i Columns) and Brightfield
      (BRF i Columns) Datasets
  Table 3 caption:
    table_text: TABLE 3 NetMets [53] Scores for the Brightfield Dataset
  Table 4 caption:
    table_text: TABLE 4 Run-Times for the Network Flow and Subset Formulations of
      Sections 6.4.3 and 6.5 Respectively
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2519025
- Affiliation of the first author: "ispgroup, elen department, icteam institute, universit\xE9\
    \ catholique de louvain (ucl), louvain-la-neuve, belgium"
  Affiliation of the last author: "ispgroup, elen department, icteam institute, universit\xE9\
    \ catholique de louvain (ucl), louvain-la-neuve, belgium"
  Figure 1 Link: articels_figures_by_rev_year\2016\Discriminative_and_Efficient_Label_Propagation_on_Complementary_Graphs_for_Multi\figure_1.jpg
  Figure 1 caption: (a) An example with two targets (red and blue) with associated
    detections at each time. Gray detections mean that no appearance feature is available.
    (b) Spatio-temporal graph that depicts the spatio-temporal association between
    the nodes, (c) Appearance graph that connects nodes even if they are far in time.
    (d) Exclusion graph in which edges connect nodes that coexist at the same time.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Discriminative_and_Efficient_Label_Propagation_on_Complementary_Graphs_for_Multi\figure_2.jpg
  Figure 2 caption: Trade-off between the processing time and the tracking accuracy
    for different observation window size for the TUD Stadtmitte dataset.
  Figure 3 Link: articels_figures_by_rev_year\2016\Discriminative_and_Efficient_Label_Propagation_on_Complementary_Graphs_for_Multi\figure_3.jpg
  Figure 3 caption: Processing times for the joint and the node-wise approaches for
    different size of the graph.
  Figure 4 Link: articels_figures_by_rev_year\2016\Discriminative_and_Efficient_Label_Propagation_on_Complementary_Graphs_for_Multi\figure_4.jpg
  Figure 4 caption: Processing time and speed-up factors for different number of processors
    ( P=1 to P=4 ) of the TUD Stadtmitte ( top row) and PETS (bottom row) datasets.
    For each case, we perform 10 runs of the algorithm which are drawn with the same
    color.
  Figure 5 Link: articels_figures_by_rev_year\2016\Discriminative_and_Efficient_Label_Propagation_on_Complementary_Graphs_for_Multi\figure_5.jpg
  Figure 5 caption: Effect of parameters on TUD Stadtmitte (top) and PETS (bottom)
    datasets. Each plot shows the effect of changing a single parameter while keeping
    other parameters fixed. Red squares correspond to the reference parameter values
    used in our experiments. The parameter is mentioned on the bottom right corner.
  Figure 6 Link: articels_figures_by_rev_year\2016\Discriminative_and_Efficient_Label_Propagation_on_Complementary_Graphs_for_Multi\figure_6.jpg
  Figure 6 caption: Sample graphs and label evolution on a subset of detections from
    PETS dataset. Top row shows the input detections and the three constructed graphs.
    For clarity, edges that have weights smaller than 10-2 are suppressed. Bottom
    row depicts the evolution of label of the nodes along with the corresponding labeling
    energy.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Amit Kumar K.C.
  Name of the last author: Christophe De Vleeschouwer
  Number of Figures: 6
  Number of Tables: 4
  Number of authors: 3
  Paper title: Discriminative and Efficient Label Propagation on Complementary Graphs
    for Multi-Object Tracking
  Publication Date: 2016-02-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Notations
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Tracking Results on the TUD Stadtmitte (179 Frames), TUD Crossing
      (201 Frames) and PETS 2009-S2L1 (795 Frames) Datasets
  Table 3 caption:
    table_text: TABLE 3 Results on the APIDIS Dataset (1,500 Frames)
  Table 4 caption:
    table_text: TABLE 4 Results of the Incremental Graph Construction and Label Propagation
      Approach
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2533391
- Affiliation of the first author: visual computing group, microsoft research, beijing,
    china
  Affiliation of the last author: visual computing group, microsoft research, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2016\An_Efficient_Joint_Formulation_for_Bayesian_Face_Verification\figure_1.jpg
  Figure 1 caption: "The 2-D data is projected to 1-D by x\u2212y . The two classes,\
    \ which are separable in the original joint representation, become inseparable\
    \ after the projection. In the context of face verification, \u201CClass1\u201D\
    \ and \u201CClass2\u201D would refer to the two hypotheses H I and H E ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\An_Efficient_Joint_Formulation_for_Bayesian_Face_Verification\figure_2.jpg
  Figure 2 caption: 'Comparison with other Bayesian face related works. The joint
    Bayesian method is consistently better other a wide range of different training
    data sizes and on two databases: LFW(left) and WDRef(right).'
  Figure 3 Link: articels_figures_by_rev_year\2016\An_Efficient_Joint_Formulation_for_Bayesian_Face_Verification\figure_3.jpg
  Figure 3 caption: Comparison with LDA and PLDA on LFW(left) and WDRef(right).
  Figure 4 Link: articels_figures_by_rev_year\2016\An_Efficient_Joint_Formulation_for_Bayesian_Face_Verification\figure_4.jpg
  Figure 4 caption: Comparison with state-of-the-art methods on LFW. Without outside
    training data (left) and with outside training data (right). Note that joint Bayesian
    automatically determines the subspace dimension and here we artificially truncate
    singular values to produce lower-dimensional models merely for purposes of comparison.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Dong Chen
  Name of the last author: Jian Sun
  Number of Figures: 4
  Number of Tables: 5
  Number of authors: 5
  Paper title: An Efficient Joint Formulation for Bayesian Face Verification
  Publication Date: 2016-02-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Efficient Training Pipeline of Joint Bayesian Algorithm
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 3 Roles of A and G in the Log-likelihood Ratio Metric
  Table 3 caption:
    table_text: TABLE 2 Efficient Testing Pipeline of Joint Bayesian Algorithm
  Table 4 caption:
    table_text: TABLE 4 Comparison over Multi-PIE Dataset
  Table 5 caption:
    table_text: TABLE 5 Performance Comparison on YouTube Faces Dataset
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2533383
- Affiliation of the first author: school of electrical, computer and energy engineering
    and school of arts, media and engineering, arizona state university, tempe, az
  Affiliation of the last author: school of electrical, computer and energy engineering
    and school of arts, media and engineering, arizona state university, tempe, az
  Figure 1 Link: articels_figures_by_rev_year\2016\Shape_Distributions_of_Nonlinear_Dynamical_Systems_for_VideoBased_Inference\figure_1.jpg
  Figure 1 caption: "Phase space reconstruction of Lorenz attractor by delay embedding.\
    \ (a) shows the 3 D view of trajectories of Lorenz attractor with control parameters\
    \ \u03C1=45.92,\u03C3=16.0 and \u03B2=4.0 . We can see that trajectories of Lorenz\
    \ system settle down and are confined within the attractor. The one-dimensional\
    \ time series (observed) of the Lorenz system is shown in (b). We see that a low-dimensional\
    \ nonlinear system can generate such complex and chaotic signal. (c) shows the\
    \ reconstructed phase space from observed time series of the Lorenz system using\
    \ delay embedding (). The above example illustrates that the reconstructed phase\
    \ space preserves certain topological properties of the original Lorenz attractor."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Shape_Distributions_of_Nonlinear_Dynamical_Systems_for_VideoBased_Inference\figure_2.jpg
  Figure 2 caption: "Examples of phase space reconstruction of corresponding time\
    \ series data of a subject performing Run and Walk action respectively. The embedding\
    \ parameters were selected as m=3 and \u03C4 as described in Section 3.4. This\
    \ example illustrates that the shape of the reconstructed phase space can be seen\
    \ as a discriminative feature for classification of actions. We use shape distributions\
    \ proposed by Osada et al. [39] as a representation for shape of phase space.\
    \ (c) and (f) together support our hypothesis that shape distribution (D2) can\
    \ be used for classification of actions."
  Figure 3 Link: articels_figures_by_rev_year\2016\Shape_Distributions_of_Nonlinear_Dynamical_Systems_for_VideoBased_Inference\figure_3.jpg
  Figure 3 caption: Illustration of the effect of time-series lengths on reconstructed
    phase space for nonlinear dynamical models like Lorenz and Rossler systems, and
    right-foot trajectory of a subject performing Run action. These examples clearly
    indicate that the shape of the reconstructed phase space does not change with
    time-series length, motivating feature extraction representative of the shape
    of the reconstructed phase space (as reported in Fig. 4).
  Figure 4 Link: articels_figures_by_rev_year\2016\Shape_Distributions_of_Nonlinear_Dynamical_Systems_for_VideoBased_Inference\figure_4.jpg
  Figure 4 caption: Illustration of stability of the dynamical shape distribution
    (D2) extracted from reconstructed phase space for different time-series length.
    (a) shows the stability of D2 distribution on Lorenz and Rossler systems while
    studies have reported significant error in estimation of largest Lyapunov exponent
    on these models (refer Table 1). (b) depicts the stability of D2 distribution
    for trajectory data collected from right-foot of a subject performing Run action.
  Figure 5 Link: articels_figures_by_rev_year\2016\Shape_Distributions_of_Nonlinear_Dynamical_Systems_for_VideoBased_Inference\figure_5.jpg
  Figure 5 caption: Illustration of the phase space reconstruction and dynamical shape
    feature extraction (D2 shape feature) using four examples of Run, Walk and Dance
    action classes each from the motion capture dataset [4]. As an example, phase
    space reconstruction of X-rotation time-series from right leg of subjects performing
    these actions is shown. Embedding parameters, m was selected to be 3 and tau was
    calculated by method explained in Section 3.4 . It is evident from these examples
    that the 'shape' of phase space is a representative feature for an action class
    and can be captured using shape distributions.
  Figure 6 Link: articels_figures_by_rev_year\2016\Shape_Distributions_of_Nonlinear_Dynamical_Systems_for_VideoBased_Inference\figure_6.jpg
  Figure 6 caption: Example actions from action class Tennis serve (a) and Two hand
    wave (b) from the MSR Action3D dataset. Skeleton data of 20 joints provided in
    the dataset will be used in our action recognition experiment. Shape distributions
    from reconstructed phase space using the hand trajectory from five instances each
    of tennis serve and two hand wave actions is shown here to illustrate the insensitivity
    of the framework to inter-class similarities.
  Figure 7 Link: articels_figures_by_rev_year\2016\Shape_Distributions_of_Nonlinear_Dynamical_Systems_for_VideoBased_Inference\figure_7.jpg
  Figure 7 caption: Proposed framework for movement quality assessment and action
    recognition by extraction of dynamical shape feature from reconstructed phase
    space. (a) shows the time-series of x -location of wrist marker; its respective
    reconstructed phase space is shown in (b). These two exemplar trajectories are
    collected from the stroke rehabilitation dataset [28] and belong to unimpaired
    and impaired subjects respectively. The corresponding dynamical shape feature
    represented by shape distribution is shown in (c). Similarity measure (e.g., Euclidean
    distance) can be used to classify these trajectories.
  Figure 8 Link: articels_figures_by_rev_year\2016\Shape_Distributions_of_Nonlinear_Dynamical_Systems_for_VideoBased_Inference\figure_8.jpg
  Figure 8 caption: Block diagram representation for learning a regressor for movement
    quality assessment using Functional Activity Score from the Wolf Motor Function
    Test.
  Figure 9 Link: articels_figures_by_rev_year\2016\Shape_Distributions_of_Nonlinear_Dynamical_Systems_for_VideoBased_Inference\figure_9.jpg
  Figure 9 caption: Comparison between impairment level (with 5 being least impaired
    and 1 being most impaired) given by actual WMFT score and MQS for 15 impaired
    subjects. The Pearson correlation coefficient was found to be 0.8527 with a two-tail
    P-value of 5.35times 10-5 , proving its statistical significance.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Vinay Venkataraman
  Name of the last author: Pavan Turaga
  Number of Figures: 9
  Number of Tables: 9
  Number of authors: 2
  Paper title: Shape Distributions of Nonlinear Dynamical Systems for Video-Based
    Inference
  Publication Date: 2016-02-25 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Experimental Results on Lorenz and Rossler Models for Given\
      \ Embedding Parameters ( m L =3 , \u03C4 L =11 , m R =3 , \u03C4 R =8 ) and\
      \ Different Time-Series Lengths"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Classification Rates for the Various Proposed Dynamical Shape
      Features of Phase Space on the Motion Capture Dataset with m=3 (and m=5 in Parentheses)
  Table 3 caption:
    table_text: TABLE 3 Confusion Table for Motion Capture Dataset Using DT2 as the
      Dynamical Shape Feature Achieving Mean Classification Rate of 99.37Percent When
      Compared to 89.7Percent Reported by Ali et al. in [4].
  Table 4 caption:
    table_text: TABLE 4 Classification Results for Cross-Subject Test Setting Where
      50Percent Subjects Were Used for Training and the Remaining 50Percent Subjects
      for Testing in Proposed Method Using Linear SVM with m=3 (and m=5 in Parentheses)
  Table 5 caption:
    table_text: TABLE 5 Classification Results for Cross-Subject Test Setting Where
      50Percent Subjects Were Used for Training and the Remaining 50Percent Subjects
      for Testing in Proposed Method Using Nearest-Neighbor Classifier
  Table 6 caption:
    table_text: TABLE 6 Comparison of Classification Rates for Different Methods Using
      Leave-One-Reach-Out Cross-Validation and Nearest Neighbor Classifier on the
      Stroke Rehabilitation Dataset
  Table 7 caption:
    table_text: TABLE 7 Comparison of Performance of the Proposed Dynamical Shape
      Features with the Performance of Traditional Methods Used for Movement Quality
      Analysis
  Table 8 caption:
    table_text: "TABLE 8 Comparison of Classification Rates for Various Approaches\
      \ on the Maryland \u201Cin-the-Wild\u201D Dataset (with m=3 )"
  Table 9 caption:
    table_text: "TABLE 9 Comparison of Classification Rates for Various Approaches\
      \ on the Yupenn \u201CStabilized\u201D Dynamic Dataset (with m=3 )"
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2533388
- Affiliation of the first author: rolls-roycentu corporate lab, nanyang technological
    university, block n4, 2a-32, nanyang avenue, singapore
  Affiliation of the last author: centre for quantum computation and intelligent systems,
    university of technology sydney, sydney, n.s.w., australia
  Figure 1 Link: articels_figures_by_rev_year\2016\Making_Trillion_Correlations_Feasible_in_Feature_Grouping_and_Selection\figure_1.jpg
  Figure 1 caption: "Distributions of correlated feature pairs in some established\
    \ datasets, wherein each bar denotes the percentage of feature pairs (the y-axis)\
    \ that satisfies a given correlation threshold interval (the x-axis), i.e., 1\u2212\
    \ CDF i ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Making_Trillion_Correlations_Feasible_in_Feature_Grouping_and_Selection\figure_2.jpg
  Figure 2 caption: 'Structural relationship of support-affiliated feature groups
    (denoted using dotted ellipse). SF k : support feature (parent denoted using full
    circle), AF k s : affiliated features (children of SF k as denoted by dotted circles).'
  Figure 3 Link: articels_figures_by_rev_year\2016\Making_Trillion_Correlations_Feasible_in_Feature_Grouping_and_Selection\figure_3.jpg
  Figure 3 caption: Feature group structures generated by the various feature grouping
    methods.
  Figure 4 Link: articels_figures_by_rev_year\2016\Making_Trillion_Correlations_Feasible_in_Feature_Grouping_and_Selection\figure_4.jpg
  Figure 4 caption: Testing accuracy (in percent) on real-world datasets.
  Figure 5 Link: articels_figures_by_rev_year\2016\Making_Trillion_Correlations_Feasible_in_Feature_Grouping_and_Selection\figure_5.jpg
  Figure 5 caption: Training time (in seconds) on real-world datasets (in logarithmic
    scale, averaged from 5 runs).
  Figure 6 Link: articels_figures_by_rev_year\2016\Making_Trillion_Correlations_Feasible_in_Feature_Grouping_and_Selection\figure_6.jpg
  Figure 6 caption: Redundancy rate on real-world datasets.
  Figure 7 Link: articels_figures_by_rev_year\2016\Making_Trillion_Correlations_Feasible_in_Feature_Grouping_and_Selection\figure_7.jpg
  Figure 7 caption: Number of AFs selected w.r.t. the number of SFs by GDM-PCC and
    GDM-SU.
  Figure 8 Link: articels_figures_by_rev_year\2016\Making_Trillion_Correlations_Feasible_in_Feature_Grouping_and_Selection\figure_8.jpg
  Figure 8 caption: Accuracy results on psoriasis for ECV.
  Figure 9 Link: articels_figures_by_rev_year\2016\Making_Trillion_Correlations_Feasible_in_Feature_Grouping_and_Selection\figure_9.jpg
  Figure 9 caption: Interpretability of selected support-affiliated feature groups.
  First author gender probability: 0.67
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Yiteng Zhai
  Name of the last author: Ivor W. Tsang
  Number of Figures: 9
  Number of Tables: 5
  Number of authors: 3
  Paper title: Making Trillion Correlations Feasible in Feature Grouping and Selection
  Publication Date: 2016-02-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Complexity Analysis for Each Iteration in GDM
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results on Synthetic Dataset of Various Methods
  Table 3 caption:
    table_text: TABLE 3 Characteristics of the Real-World Datasets Considered
  Table 4 caption:
    table_text: TABLE 4 Comparison of One-Class Learning Result between LIBSVM-OCSVM
      and GDM
  Table 5 caption:
    table_text: "TABLE 5 Clustering Performance Results Using Original and Selected\
      \ Pixels for Face Recognition from 5 Runs (i.e., the Value Is in the Form of\
      \ mean \xB1 std.)"
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2533384
