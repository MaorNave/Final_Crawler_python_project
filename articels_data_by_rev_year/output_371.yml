- Affiliation of the first author: computer science department, chestnut hill, ma
  Affiliation of the last author: department of computer science, boston university,
    boston, ma
  Figure 1 Link: articels_figures_by_rev_year\2015\Scale_and_Rotation_Invariant_Matching_Using_Linearly_Augmented_Trees\figure_1.jpg
  Figure 1 caption: Matching a template to a target object using linearly augmented
    tree (LAT) model. Our method allows arbitrary pairwise constraints defined on
    the basic tree edges and linear high-order constraints that couple all the model
    nodes.
  Figure 10 Link: articels_figures_by_rev_year\2015\Scale_and_Rotation_Invariant_Matching_Using_Linearly_Augmented_Trees\figure_10.jpg
  Figure 10 caption: 'Example matching of the proposed method on synthetic data: (a)
    fish one with zero clutter, (b) fish one with 50 clutter points, (c) fish two
    with zero clutter, (d) fish two with 25 clutter points, (e) Chinese character
    with zero clutter, (f) Chinese character with 10 clutter points, (g) random points
    with zero clutter and 0.1 distortion, and (h) random points with 25 percent clutter
    and 0.1 distortion. Ten points are randomly selected on each template for matching.
    We introduce different levels of clutter to the target images in the experiments.
    The object deformation for the fish and Chinese character is fixed. The distortion
    for random dot targets varies in different test cases.'
  Figure 2 Link: articels_figures_by_rev_year\2015\Scale_and_Rotation_Invariant_Matching_Using_Linearly_Augmented_Trees\figure_2.jpg
  Figure 2 caption: LAT model and trellises. Thick lines indicate the paths.
  Figure 3 Link: articels_figures_by_rev_year\2015\Scale_and_Rotation_Invariant_Matching_Using_Linearly_Augmented_Trees\figure_3.jpg
  Figure 3 caption: The trellises for tree matching in the first (a) and last (b)
    stage of iteration. S1, S2 and S3 denote the three model points and T1, T2, T3
    and T4 denote the four target candidates. The warm color indicates high value
    and cool color indicates low value. The tree optimization finds the matching such
    that the total edge value is the highest. The thick edges indicate the optimal
    matching. Note that links in this illustration are different from the ones in
    Fig. 5.
  Figure 4 Link: articels_figures_by_rev_year\2015\Scale_and_Rotation_Invariant_Matching_Using_Linearly_Augmented_Trees\figure_4.jpg
  Figure 4 caption: '(a): The proposals for the toy example. Sites 1-3 are nodes for
    the three template points, each of which may take value 1, 2, 3 or 4 that corresponds
    to the label of four target points. The proposal variables also include cosine
    and sin of the rotation angle and the scale. The auxiliary variables are in fact
    also proposal variables; their values are not shown. (b): Convergence of the toy
    example.'
  Figure 5 Link: articels_figures_by_rev_year\2015\Scale_and_Rotation_Invariant_Matching_Using_Linearly_Augmented_Trees\figure_5.jpg
  Figure 5 caption: Matching a three-point template. The proposed algorithm achieves
    optimum in 34 iterations, in which eight samples are shown. The gray levels of
    lines indicate the assignment strength.
  Figure 6 Link: articels_figures_by_rev_year\2015\Scale_and_Rotation_Invariant_Matching_Using_Linearly_Augmented_Trees\figure_6.jpg
  Figure 6 caption: 'Matching a fluffy animal. Row 1, from left to right: the template
    image, target image, model overlaid on the template image and target image superpixels.
    Rows 2-5: The matching improves as more proposals are included. Here we show the
    iterations of the optimal linear program (four linear programs are solved in the
    optimization). The matching result is obtained by rounding the floating point
    solution. The final matching result (in row five) is from the branch and bound
    procedure.'
  Figure 7 Link: articels_figures_by_rev_year\2015\Scale_and_Rotation_Invariant_Matching_Using_Linearly_Augmented_Trees\figure_7.jpg
  Figure 7 caption: 'Matching a person. Row 1: The template image, target image, model
    overlaid on the template image, and target image superpixels. Rows 2-5 show the
    iterations as more proposals are included. The matching result is obtained by
    rounding the floating point solution. The last matching result in row 5 is from
    the branch and bound procedure.'
  Figure 8 Link: articels_figures_by_rev_year\2015\Scale_and_Rotation_Invariant_Matching_Using_Linearly_Augmented_Trees\figure_8.jpg
  Figure 8 caption: (a) Energy curve for matching a fluffy animal. (b) Energy curve
    for matching person. (c) The number of iterations is determined by the number
    of complex constraints but is independent of the number of target points. (d)
    The proposed method is more efficient than the simplex method.
  Figure 9 Link: articels_figures_by_rev_year\2015\Scale_and_Rotation_Invariant_Matching_Using_Linearly_Augmented_Trees\figure_9.jpg
  Figure 9 caption: '(a-b): We use the ground truth point matching to test how sensitive
    our matching result is to the parameter setting. The template object''s size is
    200 pixels. As shown in the two test cases, the proposed method is not sensitive
    to parameter settings. (c) and (d) show the matching costs of strong and weak
    features on the INRIA Graffiti test images one and two. (c): Top 20 matching costs
    of strong features, whose lowest cost matching corresponds to the correct target.
    Each line corresponds to a feature point. (d): Top 20 matching costs of weak features,
    whose lowest cost matching does not find the target. Each line corresponds to
    a feature point.'
  First author gender probability: 0.9
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Hao Jiang
  Name of the last author: Stan Sclaroff
  Number of Figures: 22
  Number of Tables: 2
  Number of authors: 3
  Paper title: Scale and Rotation Invariant Matching Using Linearly Augmented Trees
  Publication Date: 2015-03-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Matching Error Comparison with Image Matching Methods
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Matching Error Comparison with Graph Matching Methods
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2409880
- Affiliation of the first author: lund university, sweden
  Affiliation of the last author: chalmers university of technology, sweden
  Figure 1 Link: articels_figures_by_rev_year\2015\Shortest_Paths_with_HigherOrder_Regularization\figure_1.jpg
  Figure 1 caption: A drawing of a heart. The thinner coronary vessels are long with
    high curvature, but have low torsion as they lie approximately in a plane.
  Figure 10 Link: articels_figures_by_rev_year\2015\Shortest_Paths_with_HigherOrder_Regularization\figure_10.jpg
  Figure 10 caption: Execution time as a function of regularization strength for the
    example in Fig. 8. For purely length regularization the running time is an increasing
    function with regularization strength. For purely curvature regularization the
    longest running times occur for moderately strong regularization.
  Figure 2 Link: articels_figures_by_rev_year\2015\Shortest_Paths_with_HigherOrder_Regularization\figure_2.jpg
  Figure 2 caption: The data term is interpolated using nearest neighbor.
  Figure 3 Link: articels_figures_by_rev_year\2015\Shortest_Paths_with_HigherOrder_Regularization\figure_3.jpg
  Figure 3 caption: "Example curve (x,y,z)=(cos6\u03C0t,sin6\u03C0t,t) ."
  Figure 4 Link: articels_figures_by_rev_year\2015\Shortest_Paths_with_HigherOrder_Regularization\figure_4.jpg
  Figure 4 caption: Error when computing the curvature and torsion using the middle
    point of a spline fit to three or four points on the curve in Fig. 3.
  Figure 5 Link: articels_figures_by_rev_year\2015\Shortest_Paths_with_HigherOrder_Regularization\figure_5.jpg
  Figure 5 caption: Example construction of a line graph.
  Figure 6 Link: articels_figures_by_rev_year\2015\Shortest_Paths_with_HigherOrder_Regularization\figure_6.jpg
  Figure 6 caption: The same path through a graph (edges shown in black) represented
    in three different ways. The graph nodes (shown in color) correspond to (a) nodes,
    (b) edges, and (c) edge pairs in the graph.
  Figure 7 Link: articels_figures_by_rev_year\2015\Shortest_Paths_with_HigherOrder_Regularization\figure_7.jpg
  Figure 7 caption: Synthetic 3D experiment for torsion with volume-rendered data
    cost. Darker regions correspond to lower cost. We run the segmentation with either
    very high curvature or very high torsion regularization. The underlying graph
    has 91,000 nodes (35 times 130 times 20), 13,286,000 edges (146-connectivity)
    and 1,939,756,000 edge pairs. Runnings times are 0.1670 s for curvature and 6.8991
    s for torsion (harder problems will take longer time to solve).
  Figure 8 Link: articels_figures_by_rev_year\2015\Shortest_Paths_with_HigherOrder_Regularization\figure_8.jpg
  Figure 8 caption: Example segmentation highlighting the difference between length
    and curvature segmentation. The start set Etextstart is the upper boundary of
    the image and the end set Etextend the bottom. The images to the left only uses
    length regularization and the images to the right only curvature regularization.
    No amount of length regularization is able to find the long and smooth river path.
    The underlying graph has 220,443 nodes (373 times 591) and 3,509,756 edges (16-connectivity).
    Figs. (i) and (j) show the order in which the nodes were visited for medium curvature
    with and without A star , from blue (early) to red (late).
  Figure 9 Link: articels_figures_by_rev_year\2015\Shortest_Paths_with_HigherOrder_Regularization\figure_9.jpg
  Figure 9 caption: Example showing the effect of refining the shortest path solution
    using local optimization. The image shows a close-up of the shortest path problem
    solved in Fig. 8, With regularization settings rho = 0 and sigma = 50.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "Johannes Ul\xE9n"
  Name of the last author: Fredrik Kahl
  Number of Figures: 15
  Number of Tables: 4
  Number of authors: 3
  Paper title: Shortest Paths with Higher-Order Regularization
  Publication Date: 2015-03-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Number of Nodes Visited
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Parameters Used in the Experiments
  Table 3 caption:
    table_text: TABLE 3 Quantitative Results for Coronary Artery Centerline Extraction
      on the Training Set of [28]
  Table 4 caption:
    table_text: "TABLE 4 Curve Characteristics for All Arteries in the Training Set\
      \ of [28] Given as Mean \xB1 One Standard Deviation"
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2409869
- Affiliation of the first author: robotics institute, carnegie mellon university
  Affiliation of the last author: robotics institute, carnegie mellon university
  Figure 1 Link: articels_figures_by_rev_year\2015\Generalized_Canonical_Time_Warping\figure_1.jpg
  Figure 1 caption: 'Temporal alignment of sequences recorded with different sensors
    (from top to bottom: video, motion capture and accelerometers) of three subjects
    kicking a ball.'
  Figure 10 Link: articels_figures_by_rev_year\2015\Generalized_Canonical_Time_Warping\figure_10.jpg
  Figure 10 caption: Comparison of algorithms for aligning facial expression across
    different subjects. (a) An example of two smiling expression sequences aligned
    by GCTW. The features of the two sequences are computed as the 18 landmark coordinates
    of the mouth given by a face tracker. (b) Alignment results. The position that
    corresponds to the peak of the expression is indicated by points on the curves
    in the top row. (c) Comparison of time warping paths. The position that corresponds
    to the peak of the expression is indicated by the intersection of the dashed lines.
    (d) Alignment errors.
  Figure 2 Link: articels_figures_by_rev_year\2015\Generalized_Canonical_Time_Warping\figure_2.jpg
  Figure 2 caption: "An example of DTW for aligning time series. (a) Two 1 -D time\
    \ series ( n x =7 and n y =8 ) and the optimal alignment between samples computed\
    \ by DTW. (b) Euclidean distances between samples, where the red curve denotes\
    \ the optimal warping path ( l=9 ). (c) DP policy at each pair of samples, where\
    \ the three arrow directions, \u2193,\u2198,\u2192 , denote the policy, \u03C0\
    (\u22C5,\u22C5)\u2208[1,0],[1,1],[0,1] , respectively. (d) A matrix-form interpretation\
    \ of DTW as stretching the two time series in matrix products. (e) Warping matrix\
    \ W x . (f) Warping matrix W y ."
  Figure 3 Link: articels_figures_by_rev_year\2015\Generalized_Canonical_Time_Warping\figure_3.jpg
  Figure 3 caption: "Approximating temporal warping using monotonic bases. (a) Five\
    \ common choices for monotonic bases. (b) An example of time warping XW(Qa)\u2208\
    \ R 1\xD770 of 1 -D time series X\u2208 R 1\xD750 . (c) The warping matrix. (d)\
    \ The warping function Qa is a linear combination of three basis functions including\
    \ a constant function ( q 1 ) and two monotonic functions ( q 2 and q 3 )."
  Figure 4 Link: articels_figures_by_rev_year\2015\Generalized_Canonical_Time_Warping\figure_4.jpg
  Figure 4 caption: An example of using Gauss-Newton for solving the sub-sequence
    alignment problem. (a) Two 1 -D sequences. (b) The contour of the objective function
    ( J a as defined in (13)) with respect to the weights of two bases. (c) The Gauss-Newton
    optimization procedure, the longer red sequence is warped to match the shorter
    blue sequence. (d) Warping function ( p ) as a combination of a linear function
    ( q 1 ) and a constant function ( q 2 ) used for scaling and translation respectively.
  Figure 5 Link: articels_figures_by_rev_year\2015\Generalized_Canonical_Time_Warping\figure_5.jpg
  Figure 5 caption: Comparison between Gauss-Newton and variants of DTW for temporal
    alignment. (a) An example of two 1-D time series and the alignment results calculated
    using DTW, DTW constrained in the Sakoe-Chiba band (DTW-SC) and the Itakura Parallelogram
    band (DTW-IP), DTW optimized in a multi-level scheme (FastDTW) and Gauss-Newton
    (GN). (b) Comparison of different warping paths. GN-Init denotes the initial warping
    used for GN. SC-Bound and IP-Bound denote the boundaries of SC band and IP band
    respectively. (c) Alignment errors. (d) Computational costs.
  Figure 6 Link: articels_figures_by_rev_year\2015\Generalized_Canonical_Time_Warping\figure_6.jpg
  Figure 6 caption: Comparison of temporal alignment algorithms as a function of degrees-of-freedom
    and complexity. l is the length of warping path. LS(n) and eig(n) denote the complexity
    of solving a least-squares of n variables and a generalized eigenvalue problem
    with two n -by- n matrices, respectively.
  Figure 7 Link: articels_figures_by_rev_year\2015\Generalized_Canonical_Time_Warping\figure_7.jpg
  Figure 7 caption: Comparison of temporal alignment algorithms on the synthetic dataset.
    (a) An example of three synthetic time series generated by performing a random
    spatio-temporal transformation of a 2 -D latent sequence Z and adding Gaussian
    noise in the third dimension. (b) The alignment results. (c) Mean and variance
    of the alignment errors. (d) Mean and variance of the computational cost (time
    in seconds).
  Figure 8 Link: articels_figures_by_rev_year\2015\Generalized_Canonical_Time_Warping\figure_8.jpg
  Figure 8 caption: Comparison of temporal alignment algorithms for aligning multi-feature
    video data. (a) An example of three aligned videos by GCTW. The left three sequences
    are the original frames after background subtraction, while the right three are
    the binary images, the Euclidean distance transforms and the solutions of the
    Poisson equation. (b) The alignment results. (c) Comparison of time warping paths.
    (d) Mean and variance of the alignment errors.
  Figure 9 Link: articels_figures_by_rev_year\2015\Generalized_Canonical_Time_Warping\figure_9.jpg
  Figure 9 caption: 'Activity classification. (a) DTW (dis)similarity matrices computed
    between videos using different features: binary images X i i and Euclidean distance
    transforms Y j j . A darker color indicates a smaller distance. (b) GCTW distance
    matrices. (c) Classification errors.'
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Feng Zhou
  Name of the last author: Fernando De la Torre
  Number of Figures: 13
  Number of Tables: 0
  Number of authors: 2
  Paper title: Generalized Canonical Time Warping
  Publication Date: 2015-03-18 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2414429
- Affiliation of the first author: sistema its s.r.l., rome, italy
  Affiliation of the last author: istituto sistemi complessi, consiglio nazionale
    delle ricerche, u.o.s. sapienza, rome, italy
  Figure 1 Link: articels_figures_by_rev_year\2015\GReTAA_Novel_Global_and_Recursive_Tracking_Algorithm_in_Three_Dimensions\figure_1.jpg
  Figure 1 caption: 'Input and output data of our tracking algorithm. Top row: examples
    of original images extracted from the video sequences taken during four field
    experiments of flocking birds and swarming insects (from left to right, experimental
    events E3 , E6 , E14 , and E15 , respectively. See Section 5 and Table 2 therein).
    The original images are slightly cropped and enhanced for the sake of readability.
    Bottom row: the 3D reconstruction of the full trajectories for each experimental
    event.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\GReTAA_Novel_Global_and_Recursive_Tracking_Algorithm_in_Three_Dimensions\figure_2.jpg
  Figure 2 caption: Scheme illustrating the main steps of the full tracking system
    (image processing and tracking algorithms). A small crop of the original images
    extracted from the video sequences of three synchronized cameras are shown on
    the first row. The segmented images are shown on the second row using blue color
    for the object borders, and red for their centers of mass. On the third row, we
    show one example of a (trifocal) stereoscopic link connecting the views of the
    same object in the three cameras, and one example of temporal link connecting
    the same object between subsequent frames in each camera sequence. In the fourth
    row, we show a crop of the temporal graphs for each camera view, which represents
    a useful visualization of the full set of temporal links assigned for each camera.
    The figures show only a small crop representing a few objects for nine time frames,
    and we indicated with grey color a cluster of paths stereoscopically linked across
    the three views. The fifth row illustrates the 2D paths reconstructed in the image
    space of each camera, obtained by simple propagation of the temporal links. The
    algorithm outputs all possible 2D paths at this step. Global optimization is used
    to match the correct 2D paths between the camera views, as shown on the sixth
    and last row, from which we finally obtain the 3D the trajectory using standard
    stereoscopic geometry.
  Figure 3 Link: articels_figures_by_rev_year\2015\GReTAA_Novel_Global_and_Recursive_Tracking_Algorithm_in_Three_Dimensions\figure_3.jpg
  Figure 3 caption: "Scheme illustrating a real case of confined data. Two objects\
    \ A and B occlude each other for two frames in the left view, while they always\
    \ appear as separate objects in the right view. During the first iteration of\
    \ the recursive divide and conquer approach, the event is divided into three intervals,\
    \ I 1 , I 2 , and I 3 . In the first and in the third intervals, there are no\
    \ tracking ambiguities. The propagation of the temporal links results only in\
    \ the two correct 2D paths, A and B , in each camera. We define the cost of a\
    \ pair of 2D paths as the sum of the costs of the links between them, i.e., the\
    \ number of missing stereoscopic links. The global optimization selects the correct\
    \ matches, (A,A) and (B,B) . In the second interval, there are no ambiguities\
    \ in the right view: the only 2D paths, A and B , are correct. Instead, in the\
    \ left view, we propagate the temporal links and we create 4 2D paths; the two\
    \ correct ones (green lines) AA and BB , and the two wrong ones (red lines) AB\
    \ and BA . Two of the possible set-cover solutions \u0393 are: the correct one\
    \ G\u2261(AA,A),(BB,B) with a cost equal to 0 , and the wrong one (AB,A),(BA,B)\
    \ with a cost equal to 4 . Here, the global optimization is essential to select\
    \ the correct solution. All the matched 2D paths are then analyzed at the second\
    \ iteration as meta-objects. At this iteration, there are no occlusions. Propagating\
    \ the temporal links, we create two 2D paths in each camera view, and the tracking\
    \ problem is correctly solved for the entire duration of the event."
  Figure 4 Link: articels_figures_by_rev_year\2015\GReTAA_Novel_Global_and_Recursive_Tracking_Algorithm_in_Three_Dimensions\figure_4.jpg
  Figure 4 caption: 'Scheme illustrating a real case of non-confined data, i.e., data
    corrupted by the images of objects in one camera view which do not belong to the
    group of interest and do not appear in the common field-of-view of all cameras
    (e.g., insects or birds flying in front of one camera only, or a pollen particle
    passing in front of a camera lens). A tracked object A and a pollen particle B
    occlude each other in the left view, while only A is visible in the right view.
    There are only two possible set-cover solutions, both characterized by the same
    cost equal to 4 : (AA,A),(BB,A) and (AB,A),(BA,A) . Both solutions would produce
    wrong trajectories, and the algorithm would fail to find the correct solution.
    Relaxing the set-cover constraint, the correct solution is found as the trajectory
    (AA,A) , while the objects belonging only to the 2D path BB in the left view are
    discarded.'
  Figure 5 Link: articels_figures_by_rev_year\2015\GReTAA_Novel_Global_and_Recursive_Tracking_Algorithm_in_Three_Dimensions\figure_5.jpg
  Figure 5 caption: "Making use of synthetic data, we compare the intrinsic complexity\
    \ of the tracking problem with and without the recursive divide and conquer scheme.\
    \ In panel ( a ), the number P of possible paths for a synthetic dataset of 1,024\
    \ objects is shown as a function of the considered temporal duration T . In panel(\
    \ b ), the dependence of \u03B2 on T . In panel ( c ), the values of H as a function\
    \ of T are plotted as yellow diamonds; these are compared to the values of H obtained\
    \ when the recursive divide and conquer strategy is applied (choosing \u03C4 1\
    \ =25 frames), and plotted as green circles. In panel ( d ), the computational\
    \ time is plotted versus H for several runs with different complexities, with\
    \ and without recursion as in panel ( c ). In panel ( e ), the MOTA (see Section\
    \ 4) values obtained on several runs with different complexities are plotted versus\
    \ H , with and without recursion as in panel ( c )."
  Figure 6 Link: articels_figures_by_rev_year\2015\GReTAA_Novel_Global_and_Recursive_Tracking_Algorithm_in_Three_Dimensions\figure_6.jpg
  Figure 6 caption: "Comparison of test-cases used for several tracking algorithms,\
    \ quantified in terms of temporal duration T and estimated number of tracked objects\
    \ N . The largest datasets processed by different tracking algorithms define the\
    \ Pareto frontier in the two-dimensional space lbrace T, Nrbrace . The points\
    \ are classified according to the field of investigation for which the respective\
    \ algorithms have been developed: fluid dynamics experiments ( blacksquare ),\
    \ biological experiments ( bullet ), and the experimental data processed using\
    \ GReTA and presented in this paper and listed in Table 2 ( triangle and bigtriangledown\
    \ for birds and insects, respectively). The numbers next to the symbols correspond\
    \ to the references to the papers from which T and N have been estimated\u2014\
    see Bibliography."
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Alessandro Attanasi
  Name of the last author: Massimiliano Viale
  Number of Figures: 6
  Number of Tables: 3
  Number of authors: 11
  Paper title: GReTA-A Novel Global and Recursive Tracking Algorithm in Three Dimensions
  Publication Date: 2015-03-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of the Synthetic Datasets Used to Validate the New
      Tracking Software
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Summary of the Field Events Analyzed with the New Tracking
      Software
  Table 3 caption:
    table_text: TABLE 3 Comparison of the Quality of the Output Trajectories Retrieved
      Using GReTA and the Ones Retrieved Using the Algorithms MHT and SDD-MHT, as
      Published by Wu et al. [34] (See Table IV Therein) on the Datasets Labeled Davis-08
      Sparse and Davis-08 Dense
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2414427
- Affiliation of the first author: centre for visual information technology, iiit
    hyderabad, hyderabad, india
  Affiliation of the last author: ecole centrale paris & inria saclay, france
  Figure 1 Link: articels_figures_by_rev_year\2015\Optimizing_Average_Precision_Using_Weakly_Supervised_Data\figure_1.jpg
  Figure 1 caption: The best mean average precision over all 10 action classes obtained
    during five-fold cross validation. The x -axis corresponds to the amount of supervision
    provided. The y -axis corresponds to the mean average precision. As the amount
    of supervision decreases, the gap in the performance of latent ap-svm and the
    baseline methods increases, thereby illustrating the importance of using the correct
    loss function and the correct learning formulation for weakly supervised learning.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Optimizing_Average_Precision_Using_Weakly_Supervised_Data\figure_2.jpg
  Figure 2 caption: A comparison between the ap loss and ap loss upper-bound values
    computed across iterations during training of latent ap-svm (top) and latent ssvm
    (bottom).
  Figure 3 Link: articels_figures_by_rev_year\2015\Optimizing_Average_Precision_Using_Weakly_Supervised_Data\figure_3.jpg
  Figure 3 caption: The best average precision values for the five statistically significant
    character classes obtained during five-fold cross validation on the 'trainval'
    set of the iiit 5k-word dataset. The x -axis corresponds to the characters. The
    y -axis corresponds to the average precision. Latent ap-svm provides statistically
    significant improvements over latent svm for four out of the five characters.
  Figure 4 Link: articels_figures_by_rev_year\2015\Optimizing_Average_Precision_Using_Weakly_Supervised_Data\figure_4.jpg
  Figure 4 caption: The average precision values for the five statistically significant
    characters obtained on the 'test' set of the iiit 5k-word dataset. The x -axis
    corresponds to the character categories. The y -axis corresponds to the average
    precision.
  Figure 5 Link: articels_figures_by_rev_year\2015\Optimizing_Average_Precision_Using_Weakly_Supervised_Data\figure_5.jpg
  Figure 5 caption: The best average precision values for the four statistically significant
    character classes obtained during five-fold cross validation on the 'trainval'
    set of the iiit 5k-word dataset. The x -axis corresponds to the characters. The
    y -axis corresponds to the average precision. Latent ap-svm provides statistically
    significant improvements over latent ssvm for three out of the four characters.
  Figure 6 Link: articels_figures_by_rev_year\2015\Optimizing_Average_Precision_Using_Weakly_Supervised_Data\figure_6.jpg
  Figure 6 caption: The average precision values for the four statistically significant
    characters obtained on the 'test' set of the iiit 5k-word dataset. The x -axis
    corresponds to the character categories. The y -axis corresponds to the average
    precision.
  Figure 7 Link: articels_figures_by_rev_year\2015\Optimizing_Average_Precision_Using_Weakly_Supervised_Data\figure_7.jpg
  Figure 7 caption: The best average precision values for all 22 character classes
    obtained during five-fold cross validation on the 'trainval' set of the iiit 5k-word
    dataset. The x -axis corresponds to the characters. The y -axis corresponds to
    the average precision.
  Figure 8 Link: articels_figures_by_rev_year\2015\Optimizing_Average_Precision_Using_Weakly_Supervised_Data\figure_8.jpg
  Figure 8 caption: The average precision values for all 22 characters obtained on
    the 'test' set of the iiit 5k-word dataset. The x -axis corresponds to the character
    categories. The y -axis corresponds to the average precision.
  Figure 9 Link: articels_figures_by_rev_year\2015\Optimizing_Average_Precision_Using_Weakly_Supervised_Data\figure_9.jpg
  Figure 9 caption: Localization results for some training set images. First row corresponds
    to latent svm and the second row corresponds to latent ap-svm.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Aseem Behl
  Name of the last author: M. Pawan Kumar
  Number of Figures: 9
  Number of Tables: 3
  Number of authors: 4
  Paper title: Optimizing Average Precision Using Weakly Supervised Data
  Publication Date: 2015-03-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Average Precision of Latent ap-svm and the Baseline Latent
      svm and Latent ssvm Methods under Weak Supervision
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Object Category Wise Detection ap (Percent) on Pascal VOC2007
      Test Set
  Table 3 caption:
    table_text: TABLE 3 Counter-Example with Two Positive and Negative Samples Each
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2414435
- Affiliation of the first author: college of engineering and computer science, australian
    national university, canberra and nicta, canberra.
  Affiliation of the last author: college of engineering and computer science, australian
    national university, canberra and nicta, canberra.
  Figure 1 Link: articels_figures_by_rev_year\2015\Kernel_Methods_on_Riemannian_Manifolds_with_Gaussian_RBF_Kernels\figure_1.jpg
  Figure 1 caption: Pedestrian detection. Detection-Error tradeoff curves for the
    proposed manifold MKL approach and state-of-the-art methods on the INRIA dataset.
    Our method outperforms existing manifold methods and Euclidean kernel methods.
    The curves for the baselines were reproduced from [3].
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Kernel_Methods_on_Riemannian_Manifolds_with_Gaussian_RBF_Kernels\figure_2.jpg
  Figure 2 caption: DTI segmentation. Segmentation of the corpus callosum with kernel
    k -means on Sym + 3 . The proposed kernel yields a cleaner segmentation.
  Figure 3 Link: articels_figures_by_rev_year\2015\Kernel_Methods_on_Riemannian_Manifolds_with_Gaussian_RBF_Kernels\figure_3.jpg
  Figure 3 caption: 2D motion segmentation. Comparison of the segmentations obtained
    with kernel k -means with our Riemannian kernel (KKM), LLE, LE and HLLE on Sym
    + 3 . Our KKM algorithm yields a much cleaner segmentation. The baseline results
    were reproduced from [5].
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Sadeep Jayasumana
  Name of the last author: Mehrtash Harandi
  Number of Figures: 3
  Number of Tables: 6
  Number of authors: 5
  Paper title: Kernel Methods on Riemannian Manifolds with Gaussian RBF Kernels
  Publication Date: 2015-03-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Properties of Different Metrics on Sym + d
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Properties of Different Metrics on G r n
  Table 3 caption:
    table_text: TABLE 3 Object Categorization
  Table 4 caption:
    table_text: TABLE 4 Texture Recognition
  Table 5 caption:
    table_text: TABLE 5 Face and Action Recognition
  Table 6 caption:
    table_text: TABLE 6 Pose Grouping
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2414422
- Affiliation of the first author: institute for mathematics and computing science,
    eindhoven university of technology, eindhoven, the netherlands
  Affiliation of the last author: johann bernoulli institute for mathematics and computer
    science, university of groningen, groningen, the netherlands
  Figure 1 Link: articels_figures_by_rev_year\2015\An_Unified_Multiscale_Framework_for_Planar_Surface_and_Curve_Skeletonization\figure_1.jpg
  Figure 1 caption: "Density advection model from the surface \u2202\u03A9 of a genus-0\
    \ input shape to its surface skeleton S \u03A9 , curve skeleton C S \u03A9 , and\
    \ object center C \u03A9 . Colors depict the importance \u03BB of different spatial\
    \ regions."
  Figure 10 Link: articels_figures_by_rev_year\2015\An_Unified_Multiscale_Framework_for_Planar_Surface_and_Curve_Skeletonization\figure_10.jpg
  Figure 10 caption: 'Comparison of surface skeletons: (1) Jalba et al. (MBS) [29];
    (2) our method; Original shapes (3) vs our skeleton-based reconstruction (4).
    All voxel models have a 5123 resolution. Last row: detailed comparison, including
    also (5) the surface-skeleton of Miklos et al. [44] .'
  Figure 2 Link: articels_figures_by_rev_year\2015\An_Unified_Multiscale_Framework_for_Planar_Surface_and_Curve_Skeletonization\figure_2.jpg
  Figure 2 caption: "Boundary \u0393 t at four moments, with (a-d) and without (e-h)\
    \ density-ordered thinning. Surfacecurve skeletons, four simplification levels\u2014\
    our method with (i-l) and without (m-p) density-ordered thinning; Reniers et al.\
    \ [53] (q-t). All skeletons are color-coded by importance \u03BB using a rainbow\
    \ colormap."
  Figure 3 Link: articels_figures_by_rev_year\2015\An_Unified_Multiscale_Framework_for_Planar_Surface_and_Curve_Skeletonization\figure_3.jpg
  Figure 3 caption: Density transport via advection vs diffusive advection (see Section
    4.3).
  Figure 4 Link: articels_figures_by_rev_year\2015\An_Unified_Multiscale_Framework_for_Planar_Surface_and_Curve_Skeletonization\figure_4.jpg
  Figure 4 caption: "Conservative advection vs. diffusive advection. Density is transported\
    \ on the surface skeleton from \u0393 t to \u0393 t+1 by left: conservative advection\
    \ and right: diffusive advection. Arrows show the directions in which density\
    \ is transported. The new density values at grid cells a , b and c are also shown."
  Figure 5 Link: articels_figures_by_rev_year\2015\An_Unified_Multiscale_Framework_for_Planar_Surface_and_Curve_Skeletonization\figure_5.jpg
  Figure 5 caption: Mass transport directions for the hand model over the surface
    skeleton (a) and curve skeleton (b), as computed by our model. See Section 4.3.
  Figure 6 Link: articels_figures_by_rev_year\2015\An_Unified_Multiscale_Framework_for_Planar_Surface_and_Curve_Skeletonization\figure_6.jpg
  Figure 6 caption: Selecting curve-skeleton points for importance boosting (Section
    4.4). The red arrow shows the upstream density-advection from b through x up to
    the curve-skeleton point c . The yellow arrow shows the second direction from
    which c receives density from the surface-skeleton.
  Figure 7 Link: articels_figures_by_rev_year\2015\An_Unified_Multiscale_Framework_for_Planar_Surface_and_Curve_Skeletonization\figure_7.jpg
  Figure 7 caption: "Progressively simplified skeletons: surface skeletons (a-c) and\
    \ curve skeletons (d-f) for six increasing \u03C4 values. See Section 4.4."
  Figure 8 Link: articels_figures_by_rev_year\2015\An_Unified_Multiscale_Framework_for_Planar_Surface_and_Curve_Skeletonization\figure_8.jpg
  Figure 8 caption: Comparison of importance-colored 2D skeletons computed with our
    method and the AFMM [69], for increasing skeleton-simplification levels.
  Figure 9 Link: articels_figures_by_rev_year\2015\An_Unified_Multiscale_Framework_for_Planar_Surface_and_Curve_Skeletonization\figure_9.jpg
  Figure 9 caption: 'Comparison of our 3D surface and curve skeletons with 10 related
    methods. Top 6 rows: curve skeletons. Bottom 6 rows: surface skeletons.'
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Andrei C. Jalba
  Name of the last author: Alexandru C. Telea
  Number of Figures: 12
  Number of Tables: 1
  Number of authors: 3
  Paper title: An Unified Multiscale Framework for Planar, Surface, and Curve Skeletonization
  Publication Date: 2015-03-24 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance Comparison
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2414420
- Affiliation of the first author: school of electronic engineering and computer science,
    peking university, no. 5 yiheyuan road haidian district, beijing, p.r. china
  Affiliation of the last author: department of computer science, university of texas
    at san antonio, san antonio, tx, usa
  Figure 1 Link: articels_figures_by_rev_year\2015\SemanticAware_CoIndexing_for_Image_Retrieval\figure_1.jpg
  Figure 1 caption: 'A sample query from the Holidays dataset: retrieval using 1000
    semantic attributes (first row); retrieval using a vocabulary tree of local features
    (second row); retrieval based on co-indexing of both local features and semantic
    attributes (third row), which returns near-duplicate images to the query followed
    by images with similar semantic attributes.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\SemanticAware_CoIndexing_for_Image_Retrieval\figure_2.jpg
  Figure 2 caption: "Illustration of the proposed co-indexing procedure. The inverted\
    \ indexes L of visual words to 4 images a , b , c and d are illustrated at the\
    \ top-left corner. Their semantic similarity matrix S \xAF \xAF \xAF is shown\
    \ at the bottom-left corner, which is decomposed to S \xAF \xAF \xAF 0 , S \xAF\
    \ \xAF \xAF + , S \xAF \xAF \xAF \u2212 of semantically neutral, relevant, and\
    \ irrelevant image pairs. S \xAF \xAF \xAF \u2212 and S \xAF \xAF \xAF + are employed\
    \ to modify the inverted indexes by isolated image deletion and nearest image\
    \ insertion."
  Figure 3 Link: articels_figures_by_rev_year\2015\SemanticAware_CoIndexing_for_Image_Retrieval\figure_3.jpg
  Figure 3 caption: 'Illustration of the co-indexing process: based on the semantic
    similarities, isolated images on the inverted indexes (red and yellow lines) of
    two visual words (red ball and yellow triangle) are deleted, marked by red cross
    marks; nearest images are inserted to the indexes, indicated by solid arrows.'
  Figure 4 Link: articels_figures_by_rev_year\2015\SemanticAware_CoIndexing_for_Image_Retrieval\figure_4.jpg
  Figure 4 caption: The proposed index structure for co-indexing.
  Figure 5 Link: articels_figures_by_rev_year\2015\SemanticAware_CoIndexing_for_Image_Retrieval\figure_5.jpg
  Figure 5 caption: Comparison of isolated image deletion on the UKbench, Holidays,
    and Oxford5K datasets.
  Figure 6 Link: articels_figures_by_rev_year\2015\SemanticAware_CoIndexing_for_Image_Retrieval\figure_6.jpg
  Figure 6 caption: Comparison of nearest image insertion on the UKbench, Holidays,
    and Oxford5K datasets.
  Figure 7 Link: articels_figures_by_rev_year\2015\SemanticAware_CoIndexing_for_Image_Retrieval\figure_7.jpg
  Figure 7 caption: Some examples of retrieval results on UKbench, Oxford5K, and Holidays
    datasets. FAUG denotes the feature augmentation in [43]. Co-indexing denotes the
    proposed approach.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.89
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Shiliang Zhang
  Name of the last author: Qi Tian
  Number of Figures: 7
  Number of Tables: 8
  Number of authors: 5
  Paper title: Semantic-Aware Co-Indexing for Image Retrieval
  Publication Date: 2015-03-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Retrieval Performance of Individual Methods
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance Comparison of Nearest Image Insertion with Automatically
      Selected K d and the Best Fixed K in Fig. 6
  Table 3 caption:
    table_text: TABLE 3 The Overall Performance of Semantic-Aware Co-Indexing on the
      UKbench, Holidays, and Oxford5K Datasets
  Table 4 caption:
    table_text: TABLE 4 The Performance of Co-Indexing the 2659-D Classemes T 16 5
      +CM Denotes the Proposed Co-Indexing Approach
  Table 5 caption:
    table_text: TABLE 5 The Performance of Co-Indexing the CNN Features
  Table 6 caption:
    table_text: TABLE 6 Performance Comparison of Co-Indexing CNN Features with the
      Recent Retrieval Methods (without Re-Ranking and Query Expansion)
  Table 7 caption:
    table_text: TABLE 7 The Performance of Semantic-Aware Co-Indexing with 1.3 Million
      Distractor Images
  Table 8 caption:
    table_text: "TABLE 8 The Performance of T 16 5 +SA in Large-Scale Image Search\
      \ When Using a Separate Table to Store the K d \xAF Nearest Images"
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2417573
- Affiliation of the first author: key laboratory of machine perception (ministry
    of education), school of electronics engineering and computer science, peking
    university, beijing, china
  Affiliation of the last author: key laboratory of machine perception (ministry of
    education), school of electronics engineering and computer science, peking university,
    beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2015\MultiView_Intact_Space_Learning\figure_1.jpg
  Figure 1 caption: View insufficiency assumption.
  Figure 10 Link: articels_figures_by_rev_year\2015\MultiView_Intact_Space_Learning\figure_10.jpg
  Figure 10 caption: Category recognition accuracy of different algorithms on different
    dimensional spaces.
  Figure 2 Link: articels_figures_by_rev_year\2015\MultiView_Intact_Space_Learning\figure_2.jpg
  Figure 2 caption: Example robust estimators.
  Figure 3 Link: articels_figures_by_rev_year\2015\MultiView_Intact_Space_Learning\figure_3.jpg
  Figure 3 caption: Reconstruction of 3-D vase model using MISL algorithm.
  Figure 4 Link: articels_figures_by_rev_year\2015\MultiView_Intact_Space_Learning\figure_4.jpg
  Figure 4 caption: Reconstruction of 3-D chair model using MISL algorithm.
  Figure 5 Link: articels_figures_by_rev_year\2015\MultiView_Intact_Space_Learning\figure_5.jpg
  Figure 5 caption: Reconstruction of 3-D compel model using MISL algorithm.
  Figure 6 Link: articels_figures_by_rev_year\2015\MultiView_Intact_Space_Learning\figure_6.jpg
  Figure 6 caption: Reconstruction of s-curve using MISL algorithm.
  Figure 7 Link: articels_figures_by_rev_year\2015\MultiView_Intact_Space_Learning\figure_7.jpg
  Figure 7 caption: Face recognition accuracies of different algorithms on different
    dimensional spaces.
  Figure 8 Link: articels_figures_by_rev_year\2015\MultiView_Intact_Space_Learning\figure_8.jpg
  Figure 8 caption: Convergence curves of MISL for different dimensional latent spaces.
  Figure 9 Link: articels_figures_by_rev_year\2015\MultiView_Intact_Space_Learning\figure_9.jpg
  Figure 9 caption: Confusion tables of different algorithms on 101 action classes
    and 5 action types.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Chang Xu
  Name of the last author: Chao Xu
  Number of Figures: 10
  Number of Tables: 1
  Number of authors: 3
  Paper title: Multi-View Intact Space Learning
  Publication Date: 2015-03-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average Performance of Different Algorithms
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2417578
- Affiliation of the first author: institute for infocomm research, agency for science,
    technology and research, singapore
  Affiliation of the last author: department of electrical and computer engineering,
    national university of singapore, singapore
  Figure 1 Link: articels_figures_by_rev_year\2015\NUSPRO_A_New_Visual_Tracking_Challenge\figure_1.jpg
  Figure 1 caption: Exemplar frames of the airplane, boat, car, helicopter , motorcycle,
    basketball, gymnastics, handball , racing, soccer, tennis and pedestrian, hat,
    interview, mask, politician, sunglasses and longseq sequences of the NUS-PRO database.
  Figure 10 Link: articels_figures_by_rev_year\2015\NUSPRO_A_New_Visual_Tracking_Challenge\figure_10.jpg
  Figure 10 caption: TRR curves of sequences containing shadow change, flash, dim
    light and camera shake challenges (best viewed on high resolution displays).
  Figure 2 Link: articels_figures_by_rev_year\2015\NUSPRO_A_New_Visual_Tracking_Challenge\figure_2.jpg
  Figure 2 caption: The NUS-PRO database is challenging due to 12 factors including
    shadow change, flash, dim light, clutter background, fast background change, rotation,
    shape deformation, scale change, partial occlusion, full occlusion, similar objects
    and camera shake.
  Figure 3 Link: articels_figures_by_rev_year\2015\NUSPRO_A_New_Visual_Tracking_Challenge\figure_3.jpg
  Figure 3 caption: Number of sequence for each factor.
  Figure 4 Link: articels_figures_by_rev_year\2015\NUSPRO_A_New_Visual_Tracking_Challenge\figure_4.jpg
  Figure 4 caption: Challenging factors in an image sequence.
  Figure 5 Link: articels_figures_by_rev_year\2015\NUSPRO_A_New_Visual_Tracking_Challenge\figure_5.jpg
  Figure 5 caption: Statistics of target size. The size of a target is measured by
    the square root of the area of its bounding box, which roughly corresponds to
    the side length of a target object.
  Figure 6 Link: articels_figures_by_rev_year\2015\NUSPRO_A_New_Visual_Tracking_Challenge\figure_6.jpg
  Figure 6 caption: A bounding box of an image, illustrated by white solid lines,
    is obtained by expanding the bounding box of four facial fiducial points (eye
    centers and mouth corners) by 40 percent of the eye-mouth distance (the black
    dashed line in the right image).
  Figure 7 Link: articels_figures_by_rev_year\2015\NUSPRO_A_New_Visual_Tracking_Challenge\figure_7.jpg
  Figure 7 caption: Sample annotated boundary (dotted line) and torso (solid line)
    based bounding boxes. For a non-rigid object, the torso based annotation contains
    fewer background pixels, which helps increase the overlap ratio [19].
  Figure 8 Link: articels_figures_by_rev_year\2015\NUSPRO_A_New_Visual_Tracking_Challenge\figure_8.jpg
  Figure 8 caption: Overlap ratio for the boundary based and torso based bounding
    box in the first frames of helicopter, sportsman and pedestrian sequences.
  Figure 9 Link: articels_figures_by_rev_year\2015\NUSPRO_A_New_Visual_Tracking_Challenge\figure_9.jpg
  Figure 9 caption: Sample frames of partial occlusion (cyan rectangles), full occlusion
    (red rectangles) and no occlusion (green rectangles).
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Annan Li
  Name of the last author: Shuicheng Yan
  Number of Figures: 17
  Number of Tables: 6
  Number of authors: 5
  Paper title: 'NUS-PRO: A New Visual Tracking Challenge'
  Publication Date: 2015-03-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Statistics of the NUS-PRO Database
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Three Criteria for Computing the Overlap Ratio
  Table 3 caption:
    table_text: TABLE 3 Evaluated Tracking Algorithms
  Table 4 caption:
    table_text: TABLE 4 Top 3 Methods with the Largest AUCs for 12 Factors
  Table 5 caption:
    table_text: TABLE 5 Representation and Search Models of the Top 3 Algorithms
  Table 6 caption:
    table_text: TABLE 6 Top Three Approaches with Three Largest Obtained Four Main
      Object Categories and the Whole Database
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2417577
