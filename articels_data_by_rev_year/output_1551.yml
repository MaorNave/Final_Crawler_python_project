- Affiliation of the first author: department of electrical and computer engineering,
    national university of singapore, singapore
  Affiliation of the last author: yitu tech, shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2019\Tensor_LowRank_Representation_for_Data_Recovery_and_Clustering\figure_1.jpg
  Figure 1 caption: Illustration of the proposed TLRR method for tensor data recovery
    and clustering. By exploiting intrinsic low-rank structure of the input tensor
    data X , TLRR can effectively recover the underlying low-rank tensor L in presence
    of sparse noise E , and cluster the samples in X (encoded by Z under dictionary
    A ).
  Figure 10 Link: articels_figures_by_rev_year\2019\Tensor_LowRank_Representation_for_Data_Recovery_and_Clustering\figure_10.jpg
  Figure 10 caption: Recovery performance of R-TLRR on FRGC 2.0. (a) Images of 0,
    10, 20, and 30 percent corruptions (from left to right). (b) Recovered images
    in (a) by R-TLRR (from left to right).
  Figure 2 Link: articels_figures_by_rev_year\2019\Tensor_LowRank_Representation_for_Data_Recovery_and_Clustering\figure_2.jpg
  Figure 2 caption: Illustration of the vec , ivec and squeeze operations.
  Figure 3 Link: articels_figures_by_rev_year\2019\Tensor_LowRank_Representation_for_Data_Recovery_and_Clustering\figure_3.jpg
  Figure 3 caption: Comparison of vector linear representation and tensor linear representation.
    (a) Reports the reconstruction errors of LRR and TLRR which respectively are based
    on vector and tenslr linear representations, the learnt block-diagonal structures
    by LRR and TLRR, when the testing data have vector linear relations. (b) Reports
    the results under the same metrics on the data with tensor linear relations. Best
    viewed in color pdf file.
  Figure 4 Link: articels_figures_by_rev_year\2019\Tensor_LowRank_Representation_for_Data_Recovery_and_Clustering\figure_4.jpg
  Figure 4 caption: Comparison of clustering accuracies of LRR and TLRR on the FRGC
    2.0 dataset [37].
  Figure 5 Link: articels_figures_by_rev_year\2019\Tensor_LowRank_Representation_for_Data_Recovery_and_Clustering\figure_5.jpg
  Figure 5 caption: "Illustration of effects of the tensor subspace number k on \u03BC\
    \ 1 ( L 0 ) and \u03BC 2 ( L 0 ) . We produce a random tensor L 0 \u2208 R 1000\xD7\
    1000\xD710 with rank t ( L 0 )=100 . We increase the subspace number k and set\
    \ the sample number as 100k per subspace (see case (a) in Section 9.2.1 for details\
    \ of producing testing data)."
  Figure 6 Link: articels_figures_by_rev_year\2019\Tensor_LowRank_Representation_for_Data_Recovery_and_Clustering\figure_6.jpg
  Figure 6 caption: Illustration of the low tubal rank property of the images in Berkeley
    Segmentation dataset. (a) Two randomly selected images. (b) Plots the singular
    values of bar boldsymbol X obtained by conducting linear transformation on the
    DFT result bar boldsymbol mathcal X of image tensor boldsymbolmathcal X . (c)
    Displays sum i=1n3 boldsymbolsi , where boldsymbolsi is the singular value vector
    (in a descending order) of the i th frontal slice bar boldsymbol X(i) of bar boldsymbol
    mathcal X .
  Figure 7 Link: articels_figures_by_rev_year\2019\Tensor_LowRank_Representation_for_Data_Recovery_and_Clustering\figure_7.jpg
  Figure 7 caption: Experimental settings of the three testing datasets.
  Figure 8 Link: articels_figures_by_rev_year\2019\Tensor_LowRank_Representation_for_Data_Recovery_and_Clustering\figure_8.jpg
  Figure 8 caption: Comparison of block-diagonal structures learned by the compared
    methods. Best viewed in color pdf file.
  Figure 9 Link: articels_figures_by_rev_year\2019\Tensor_LowRank_Representation_for_Data_Recovery_and_Clustering\figure_9.jpg
  Figure 9 caption: Clustering results (ACC, NMI, and PUR) on noisy FRGC 2.0. Best
    viewed in times 2 sized color pdf file.
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Pan Zhou
  Name of the last author: Shuicheng Yan
  Number of Figures: 14
  Number of Tables: 3
  Number of authors: 5
  Paper title: Tensor Low-Rank Representation for Data Recovery and Clustering
  Publication Date: 2019-11-21 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Notational Convention in This Article
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Clustering Results (ACC, NMI, and PUR) and the Algorithm Running
      Time (in Seconds) on the Three Testing Databases
  Table 3 caption:
    table_text: TABLE 3 PSNR Values on the YUV Video Sequences
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2954874
- Affiliation of the first author: "eth z\xFCrich, z\xE4urich, switzerland"
  Affiliation of the last author: arc centre of excellent for robotic vision, australian
    national university, canberra, act, australia
  Figure 1 Link: articels_figures_by_rev_year\2019\Superpixel_Soup_Monocular_Dense_D_Reconstruction_of_a_Complex_Dynamic_Scene\figure_1.jpg
  Figure 1 caption: Dense 3D reconstruction of a complex dynamic scene, where both
    the camera and the objects are moving with respect to each other. The top left
    shows a sample reconstruction on messi sequence from Youtube Object dataset [23].
    The top right shows the reconstruction on alley1 sequence from the MPI Sintel
    dataset [24].
  Figure 10 Link: articels_figures_by_rev_year\2019\Superpixel_Soup_Monocular_Dense_D_Reconstruction_of_a_Complex_Dynamic_Scene\figure_10.jpg
  Figure 10 caption: 'Dense 3D reconstruction of the objects that are undergoing non-rigid
    deformation over frames. Top row: Input reference frame from Back sequence [36],
    Paper sequence [42] [54] and t-shirt sequence [42] [54]. Bottom row: Qualitative
    3D reconstruction results for the respective deforming object.'
  Figure 2 Link: articels_figures_by_rev_year\2019\Superpixel_Soup_Monocular_Dense_D_Reconstruction_of_a_Complex_Dynamic_Scene\figure_2.jpg
  Figure 2 caption: Reconstructing a 3D surface from a soup of un-scaled superpixels
    via solving a 3D Superpixel Jigsaw puzzle problem.
  Figure 3 Link: articels_figures_by_rev_year\2019\Superpixel_Soup_Monocular_Dense_D_Reconstruction_of_a_Complex_Dynamic_Scene\figure_3.jpg
  Figure 3 caption: "Illustration shows the modeling of a continuous scene with a\
    \ piece wise rigid and planar assumption. Each superpixel is composed of a set\
    \ ( \u03A0 si ~ , M i ) where \u03A0 si ~ contains geometric parameters, such\
    \ as normal, anchor point, and boundary points of a plane in 3D and M i contains\
    \ the motion parameters, i.e., rotation and translation."
  Figure 4 Link: articels_figures_by_rev_year\2019\Superpixel_Soup_Monocular_Dense_D_Reconstruction_of_a_Complex_Dynamic_Scene\figure_4.jpg
  Figure 4 caption: Demonstration of as rigid as possible constraint. Superpixel segmentation
    in the reference frame is used to decompose the entire scene as a set of anchor
    points. Schematic representation shows the construction of K-NN around a particular
    anchor point (shown in red). We constrain the local 3D coordinate transformation
    both before and after motion (green shows K-NN the reference frame, yellow shows
    the relation in the next frame (after motion)). We want this transformation to
    be as rigid as possible.
  Figure 5 Link: articels_figures_by_rev_year\2019\Superpixel_Soup_Monocular_Dense_D_Reconstruction_of_a_Complex_Dynamic_Scene\figure_5.jpg
  Figure 5 caption: 3D Continuity energy favors continuous surface for the planes
    that shares the common boundary points. a)-d) The lesser the E cont is, smoother
    the surface becomes (color bar shows the energy).
  Figure 6 Link: articels_figures_by_rev_year\2019\Superpixel_Soup_Monocular_Dense_D_Reconstruction_of_a_Complex_Dynamic_Scene\figure_6.jpg
  Figure 6 caption: a) Superpixelled reference image. b) Individual superpixel depth
    with arbitrary scale (unorganised superpixel soup). c) recovered depth map using
    our approach (organised superpixel soup) d) ground-truth depth map.
  Figure 7 Link: articels_figures_by_rev_year\2019\Superpixel_Soup_Monocular_Dense_D_Reconstruction_of_a_Complex_Dynamic_Scene\figure_7.jpg
  Figure 7 caption: 'Qualitative results using our algorithm in a complex dynamic
    scene. Example images are taken from MPI Sintel dataset [24]. Top row: Input reference
    image from sleeping1, sleeping2, shaman3, temple2, alley2 sequence (from left
    to right). Middle row: Ground-truth depth map for the respective frames. Bottom
    row: Recovered depth map using our method.'
  Figure 8 Link: articels_figures_by_rev_year\2019\Superpixel_Soup_Monocular_Dense_D_Reconstruction_of_a_Complex_Dynamic_Scene\figure_8.jpg
  Figure 8 caption: 'Qualitative results using our algorithm for the outdoor scenes.
    Examples are taken from VKITTI dataset [32]. Top row: Input reference image. Middle
    row: Ground-truth depth map for the respective frames. Bottom row: Recovered depth
    map using our method.'
  Figure 9 Link: articels_figures_by_rev_year\2019\Superpixel_Soup_Monocular_Dense_D_Reconstruction_of_a_Complex_Dynamic_Scene\figure_9.jpg
  Figure 9 caption: 'Qualitative results on KITTI Dataset [31]. The second row shows
    the obtained depth map for the respective frames. Note: Dense ground-truth depth
    data is not available with this dataset.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.67
  Name of the first author: Suryansh Kumar
  Name of the last author: Hongdong Li
  Number of Figures: 18
  Number of Tables: 2
  Number of authors: 3
  Paper title: 'Superpixel Soup: Monocular Dense 3D Reconstruction of a Complex Dynamic
    Scene'
  Publication Date: 2019-11-22 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Performance Comparison: This Table Lists the MRE Errors'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Contribution of Each Individual Energy Term to the Overall
      Optimzation
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2955131
- Affiliation of the first author: northwestern polytechnical university, xian, china
  Affiliation of the last author: university of kentucky, lexington, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Detailed_Surface_Geometry_and_Albedo_Recovery_from_RGBD_Video_under_Natural_Illu\figure_1.jpg
  Figure 1 caption: Diverse rotation variations resolve local ambiguity. (a) Shows
    a sampled image of the object with a reference pixel marked as red. In (b),(c)
    and (d) we plot the energy map for this reference pixel with x-axis and y-axis
    representing the two degree of freedom of a surface normal. The cooler color in
    these figures corresponds to smaller energy value. As we can see, given a single
    image the solution lies in a large band as shown in (b). With three images the
    normal converges better as shown in (c). And finally, we will be able to find
    the optimal surface normal for the reference pixel if we have got enough images
    under rotation variations as shown in (d).
  Figure 10 Link: articels_figures_by_rev_year\2019\Detailed_Surface_Geometry_and_Albedo_Recovery_from_RGBD_Video_under_Natural_Illu\figure_10.jpg
  Figure 10 caption: Comparison results on Backpack and Turtle model. (a1) and (a2)
    are the reference color images of Backpack and Turtle, respectively. The output
    from shading refinement method [32] is shown in (b1) and (b2). The results computed
    by depth super-resolution [16] are displayed in (c1) and (c2). (d1) and (d2) are
    the meshes acquired using our method but without applying our locally robust matching
    procedure. (e1) and (e2) are the output meshes of our method but without applying
    our adaptive reference selection procedure. Finally, (f1) and (f2) are the meshes
    achieved by our approach with all the components enforced. The normal map is given
    in (g1) and (g2).
  Figure 2 Link: articels_figures_by_rev_year\2019\Detailed_Surface_Geometry_and_Albedo_Recovery_from_RGBD_Video_under_Natural_Illu\figure_2.jpg
  Figure 2 caption: System pipeline.
  Figure 3 Link: articels_figures_by_rev_year\2019\Detailed_Surface_Geometry_and_Albedo_Recovery_from_RGBD_Video_under_Natural_Illu\figure_3.jpg
  Figure 3 caption: Results on adaptive reference frame selection. (a) Shows the labeling
    results on the 3D model with different colors representing different key frames.
    (b) Shows the labeling results on two sampled key frame images by projecting the
    labeled 3D model onto those corresponding frames.
  Figure 4 Link: articels_figures_by_rev_year\2019\Detailed_Surface_Geometry_and_Albedo_Recovery_from_RGBD_Video_under_Natural_Illu\figure_4.jpg
  Figure 4 caption: Demonstration of correspondence matching. (a) is the reference
    frame and (b) is one sampled key frame. Image(b) is warped to the reference frame
    with current transformation, and (c) displays the warped image overlaid with image(a).
    (d) shows the overlaid result using the flow map computed from warped image and
    the reference image [9]. (e) is the overlaid image after applying our proposed
    lighting insensitive robust matching.
  Figure 5 Link: articels_figures_by_rev_year\2019\Detailed_Surface_Geometry_and_Albedo_Recovery_from_RGBD_Video_under_Natural_Illu\figure_5.jpg
  Figure 5 caption: Results on synthetic models. (a1-a3) is the rendered color image
    of the reference frame; (e1-e3) shows the normal map of the ground-truth mesh
    after over smoothing; (b1-b3) is the normal map computed after applying shading
    refinement on the reference frame with its error map displayed in (f1-f3); (c1-c3)
    and (g1-g3) are the normal map and its corresponding error map achieved by our
    method. Our recovered albedo map and its error map is also demonstrated in (d1-d3)
    and (h1-h3), respectively.
  Figure 6 Link: articels_figures_by_rev_year\2019\Detailed_Surface_Geometry_and_Albedo_Recovery_from_RGBD_Video_under_Natural_Illu\figure_6.jpg
  Figure 6 caption: Results when adding salt and pepper noise. (a) shows the computed
    normal map without our EM framework and (b) is its error map; The normal and error
    map after applying our EM optimization are shown in(c) and (d), respectively.
  Figure 7 Link: articels_figures_by_rev_year\2019\Detailed_Surface_Geometry_and_Albedo_Recovery_from_RGBD_Video_under_Natural_Illu\figure_7.jpg
  Figure 7 caption: Results on objects with specular reflection. We show the reference
    images on the first row. The specular proportion increases from left to right.
    The second row are the corresponding error map of the recovered surface normal
    without applying our EM optimization. The last row shows the error map of the
    recovered surface normal from our approach. The mean error (in degree) of the
    estimated surface normal is displayed at the right upper corner of each error
    map.
  Figure 8 Link: articels_figures_by_rev_year\2019\Detailed_Surface_Geometry_and_Albedo_Recovery_from_RGBD_Video_under_Natural_Illu\figure_8.jpg
  Figure 8 caption: Evaluation of our lighting insensitive local match refinement
    method. The reference frame and one sampled frame are shown in (a) and (b), respectively.
    We warp the sampled image back to the reference frame with the computed correspondences.
    (c) Shows the overlay of the reference image and the warped image without applying
    the chromaticity normalization. (d) The overlay of the warped image achieved from
    our robust matching approach.
  Figure 9 Link: articels_figures_by_rev_year\2019\Detailed_Surface_Geometry_and_Albedo_Recovery_from_RGBD_Video_under_Natural_Illu\figure_9.jpg
  Figure 9 caption: Comparison results on Frog, Shoe and Chinese Fan model. (a1),
    (a2), and (a3) are the reference color images of Frog, Shoe, and Chinese Fan respectively.
    The outputs from KinectFusion are shown in (b1), (b2), and (b3). The results computed
    by shading refinement method [32] are displayed in (c1), (c2), and (c3). (d1),
    (d2), and (d3) are the meshes computed by depth super-resolution method [16].
    Finally, (e1), (e2), and (e3) are the output meshes from our approach with their
    corresponding normal maps displayed in (f1), (f2), and (f3).
  First author gender probability: 0.68
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xinxin Zuo
  Name of the last author: Ruigang Yang
  Number of Figures: 13
  Number of Tables: 2
  Number of authors: 5
  Paper title: Detailed Surface Geometry and Albedo Recovery from RGB-D Video under
    Natural Illumination
  Publication Date: 2019-11-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Evaluation
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Quantitative Evaluation of the Alignment (Unit: Pixel Error)'
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2955459
- Affiliation of the first author: google ai, sunnyvale, ca, usa
  Affiliation of the last author: department of biomedical engineering, university
    of florida, gainesville, fl, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\TextGuided_Neural_Network_Training_for_Image_Recognition_in_Natural_Scenes_and_M\figure_1.jpg
  Figure 1 caption: Examples of the four datasets on which we deploy our method, crossing
    from natural scene images and medical pathology and radiology images. Dataset
    details are introduced in Section 6.1. The goal of this paper is to take advantage
    of the captionsreports beyond image labels to guide CNN to distill visual features
    and thereby to achieve more accurate and interpretable image recognition.
  Figure 10 Link: articels_figures_by_rev_year\2019\TextGuided_Neural_Network_Training_for_Image_Recognition_in_Natural_Scenes_and_M\figure_10.jpg
  Figure 10 caption: 'Left: The accuracy with varying drop rates using TandemNet on
    the BCIDR dataset. Right: The F1-C score with varying drop rates using TandemNet2
    on the COCO dataset. TandemNet can achieve stable good performance at the drop
    rate 0.5 for all datasets. But TandemNet2 clearly is more insensitive to the drop
    rate.'
  Figure 2 Link: articels_figures_by_rev_year\2019\TextGuided_Neural_Network_Training_for_Image_Recognition_in_Natural_Scenes_and_M\figure_2.jpg
  Figure 2 caption: 'Overview of the proposed method. Left (Section 3): The image
    and language models encode visual and semantic features, which are sent to TandemNetTandemNet2,
    which then outputs distilled features used by the prediction module for classification.
    Right (Section 4&5): The illustration of the inner workings of TandemNet and TandemNet2.'
  Figure 3 Link: articels_figures_by_rev_year\2019\TextGuided_Neural_Network_Training_for_Image_Recognition_in_Natural_Scenes_and_M\figure_3.jpg
  Figure 3 caption: Training and testing F1-C curves of on COCO train and val datasets.
  Figure 4 Link: articels_figures_by_rev_year\2019\TextGuided_Neural_Network_Training_for_Image_Recognition_in_Natural_Scenes_and_M\figure_4.jpg
  Figure 4 caption: ROC curves and AUC scores for thorax disease diagnosis on OpenI.
    Our models use ResNet18 as the backend image model. Both models significantly
    outperform the ResNet18 baseline.
  Figure 5 Link: articels_figures_by_rev_year\2019\TextGuided_Neural_Network_Training_for_Image_Recognition_in_Natural_Scenes_and_M\figure_5.jpg
  Figure 5 caption: Phrase-based object localization (the pointing game) on the COCO-VGnome-Loc
    dataset using TandemNet2. Five results are shown per image. Point proposals computed
    using attention maps are shown in blue circles. Input phrases and groundtruth
    bounding boxes are in the white color. Our method is able to localize multiple
    objects in an image crossing different natural scenes, and is proficient at localizing
    small objects (the 3rd row especially).
  Figure 6 Link: articels_figures_by_rev_year\2019\TextGuided_Neural_Network_Training_for_Image_Recognition_in_Natural_Scenes_and_M\figure_6.jpg
  Figure 6 caption: Localization (per-class) accuracy and mAP of TandemNet2 compared
    against CAM [25]. Our method achieves higher mAP and is better at localizing small
    objects. Clearer view in digital format.
  Figure 7 Link: articels_figures_by_rev_year\2019\TextGuided_Neural_Network_Training_for_Image_Recognition_in_Natural_Scenes_and_M\figure_7.jpg
  Figure 7 caption: Multilabel object localization results of TandemNet2 (wo text)
    compared to CAM. For each image, the groundtruth labels are shown below the input
    image. The top row is our results and the bottom row is CAMs results. Each label
    of the top-3 predictions corresponding to each attention map is shown in white
    text. Our method generates obviously more accurate and more concentrated attention
    maps, especially on small objects, e.g., carrot, sports ball, bottle, and chair
    in shown examples. Please refer to the main paper for full explanation.
  Figure 8 Link: articels_figures_by_rev_year\2019\TextGuided_Neural_Network_Training_for_Image_Recognition_in_Natural_Scenes_and_M\figure_8.jpg
  Figure 8 caption: 'Attention visualization of TandemNet. From left to right: Test
    images (the bottom shows disease labels), pathologists annotations, visual attention
    of TandemNet (wo text), and visual attention and corresponding text attention
    (the bottom shows text inputs) of TandemNet (w text). Best viewed in color. We
    can observe that the attention (w text) better matches the annotations. See text
    for further explanations.'
  Figure 9 Link: articels_figures_by_rev_year\2019\TextGuided_Neural_Network_Training_for_Image_Recognition_in_Natural_Scenes_and_M\figure_9.jpg
  Figure 9 caption: The average text attention per feature type to each disease label.
    The feature type name is specified in the introduction of the BCIDR dataset (in
    order).
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.64
  Name of the first author: Zizhao Zhang
  Name of the last author: Lin Yang
  Number of Figures: 10
  Number of Tables: 6
  Number of authors: 4
  Paper title: Text-Guided Neural Network Training for Image Recognition in Natural
    Scenes and Medicine
  Publication Date: 2019-11-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Multilabel Image Classification Results on COCO
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Multi-Label Image Classification Results on the VGnome Test
      Set
  Table 3 caption:
    table_text: TABLE 3 Pathology Image Diagnosis Results on BCIDR
  Table 4 caption:
    table_text: TABLE 4 Image-Text Joint Classification on COCO
  Table 5 caption:
    table_text: TABLE 5 Pointing Game Results on VGnome
  Table 6 caption:
    table_text: TABLE 6 Caption-Based Image Retrieval Results on COCO
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2955476
- Affiliation of the first author: "department of mathematics and operational research,\
    \ facult\xE9 polytechnique, universit\xE9 de mons, mons, belgium"
  Affiliation of the last author: "department of mathematics and operational research,\
    \ facult\xE9 polytechnique, universit\xE9 de mons, mons, belgium"
  Figure 1 Link: articels_figures_by_rev_year\2019\Generalized_Separable_Nonnegative_Matrix_Factorization\figure_1.jpg
  Figure 1 caption: Average accuracy (15) for the different algorithms on the fully
    randomly generated GS matrices.
  Figure 10 Link: articels_figures_by_rev_year\2019\Generalized_Separable_Nonnegative_Matrix_Factorization\figure_10.jpg
  Figure 10 caption: The first image highlights the 28 extracted pixels by GS-FGM
    for the ORL data set. The next images are the 12 subjects extracted by GS-FGM.
  Figure 2 Link: articels_figures_by_rev_year\2019\Generalized_Separable_Nonnegative_Matrix_Factorization\figure_2.jpg
  Figure 2 caption: Average relative approximation error (16) on the fully randomly
    generated GS matrices.
  Figure 3 Link: articels_figures_by_rev_year\2019\Generalized_Separable_Nonnegative_Matrix_Factorization\figure_3.jpg
  Figure 3 caption: Distance to ground truth (17) on the fully randomly generated
    GS matrices.
  Figure 4 Link: articels_figures_by_rev_year\2019\Generalized_Separable_Nonnegative_Matrix_Factorization\figure_4.jpg
  Figure 4 caption: Average accuracy (15) for the different algorithms on the middle-point
    GS matrices with adversarial noise.
  Figure 5 Link: articels_figures_by_rev_year\2019\Generalized_Separable_Nonnegative_Matrix_Factorization\figure_5.jpg
  Figure 5 caption: Average relative approximation error (16) for the different algorithms
    on the middle-point GS matrices with adversarial noise.
  Figure 6 Link: articels_figures_by_rev_year\2019\Generalized_Separable_Nonnegative_Matrix_Factorization\figure_6.jpg
  Figure 6 caption: Distance to ground truth (17) on the middle-point GS matrices
    with adversarial noise.
  Figure 7 Link: articels_figures_by_rev_year\2019\Generalized_Separable_Nonnegative_Matrix_Factorization\figure_7.jpg
  Figure 7 caption: The first image highlights the 14 extracted pixels by GS-FGM for
    the CBCL data set. The next images are the 35 subjects extracted by GS-FGM.
  Figure 8 Link: articels_figures_by_rev_year\2019\Generalized_Separable_Nonnegative_Matrix_Factorization\figure_8.jpg
  Figure 8 caption: The first image highlights the 39 extracted pixels by GS-FGM for
    the Frey data set. The next images are the 11 subjects extracted by GS-FGM.
  Figure 9 Link: articels_figures_by_rev_year\2019\Generalized_Separable_Nonnegative_Matrix_Factorization\figure_9.jpg
  Figure 9 caption: The first image highlights the 24 extracted pixels by GS-FGM for
    the Yale data set. The next images are the 14 subjects extracted by GS-FGM.
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Junjun Pan
  Name of the last author: Nicolas Gillis
  Number of Figures: 10
  Number of Tables: 4
  Number of authors: 2
  Paper title: Generalized Separable Nonnegative Matrix Factorization
  Publication Date: 2019-11-26 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Relative Approximation Quality in Percent for the Document
      Data Sets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Words and Documents Extracted by GS-FGM on the TDT30 Data
      Set
  Table 3 caption:
    table_text: TABLE 3 The Relative Approximation Quality in Percent for the Facial
      Image Data Sets
  Table 4 caption:
    table_text: TABLE 4 Computational Time in Seconds for the Different Algorithms
      on the Image Data Sets
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2956046
- Affiliation of the first author: Not Available
  Affiliation of the last author: Not Available
  Figure 1 Link: articels_figures_by_rev_year\2019\On_Symbiosis_of_Attribute_Prediction_and_Semantic_Segmentation\figure_1.jpg
  Figure 1 caption: Examples of how contextual layout assists attribute prediction
    in wild. The person (on left) and the dog (on right) should be respectively labeled
    with the attributes eating and catching. This is hard to agree upon if we would
    have taken these object instances in isolation, out of their contexts i.e., food
    and frisbee.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\On_Symbiosis_of_Attribute_Prediction_and_Semantic_Segmentation\figure_2.jpg
  Figure 2 caption: 'Examples of the segmentation masks generated by our semantic
    segmentation network [1] for previously unseen images. From left to right: background,
    hair, face skin, eyes, eyebrows, mouth, and nose.'
  Figure 3 Link: articels_figures_by_rev_year\2019\On_Symbiosis_of_Attribute_Prediction_and_Semantic_Segmentation\figure_3.jpg
  Figure 3 caption: "Left: Semantic segmentation-based Pooling (SSP). Right: Semantic\
    \ segmentation-based Gating (SSG). N S and N A , respectively, indicate the number\
    \ of labels in semantic segmentation and attribute prediction tasks. We assume\
    \ that the output tensor of activations from the previous layer to either SSP\
    \ or SSG is of shape C\xD7H\xD7W where C , H , and W , respectively, represent\
    \ the number of channels, height and width of the activations. Alternatively,\
    \ in Sec. 3.3, we will show that instead of using all N S semantic regions for\
    \ every channel, one can learn a single semantic mask per channel. This would\
    \ also unify the SSP and SSG architectures."
  Figure 4 Link: articels_figures_by_rev_year\2019\On_Symbiosis_of_Attribute_Prediction_and_Semantic_Segmentation\figure_4.jpg
  Figure 4 caption: "Architecture of the Symbiotic Augmentation (SA). The embedding\
    \ layers, \u03A6 S and \u03A6 A , respectively, utilize the output of semantic\
    \ segmentation and attribute prediction classifiers to augment the other task.\
    \ Similar to Fig. 3, N S and N A denote the number of output labels for semantic\
    \ segmentation and attribute prediction, where, l S and l A are their corresponding\
    \ loss functions (per-pixel softmax cross entropy, image-level sigmoid cross entropy).\
    \ Addition and multiplication are element-wise operations."
  Figure 5 Link: articels_figures_by_rev_year\2019\On_Symbiosis_of_Attribute_Prediction_and_Semantic_Segmentation\figure_5.jpg
  Figure 5 caption: "Inception-V3 [6] backbone architecture used in the Symbiotic\
    \ Augmentation (SA) experiments. X A and X S are used as input features to SA\
    \ (ref. Fig. 4). In order to generate X S , we \u2113 2 normalize the intermediate\
    \ activations and scale them by learnable \u03C6 \u2217 parameters. Refer to [6]\
    \ for the details of the Inception-V3 architecture."
  Figure 6 Link: articels_figures_by_rev_year\2019\On_Symbiosis_of_Attribute_Prediction_and_Semantic_Segmentation\figure_6.jpg
  Figure 6 caption: 'Top fifty activation maps of the last convolution layer sorted
    in descending order w.r.t the average activation values. Top: Basic attribute
    prediction model using global pooling. Bottom: SSP.'
  Figure 7 Link: articels_figures_by_rev_year\2019\On_Symbiosis_of_Attribute_Prediction_and_Semantic_Segmentation\figure_7.jpg
  Figure 7 caption: 'Learned weights of Phi A in Symbiotic Augmentation (SA), trained
    on CelebA and Helen. Note: 9 values associated with 3times 3 kernels are averaged.
    For better visualization, values in each row are normalized between 0 and 1.'
  Figure 8 Link: articels_figures_by_rev_year\2019\On_Symbiosis_of_Attribute_Prediction_and_Semantic_Segmentation\figure_8.jpg
  Figure 8 caption: 'Learned weights of embedding convolution layers in Symbiotic
    Augmentation (SA), trained on WIDER and LIP. Note: 9 values associated with 3times
    3 kernels are averaged. For better visualization, values in each row are normalized
    between 0 and 1.'
  Figure 9 Link: articels_figures_by_rev_year\2019\On_Symbiosis_of_Attribute_Prediction_and_Semantic_Segmentation\figure_9.jpg
  Figure 9 caption: 'Learned weights of Phi S in Symbiotic Augmentation (SA), trained
    on CelebA and Helen. Note: 9 values associated with 3times 3 kernels are averaged.
    For better visualization, values in each row are normalized between 0 and 1.'
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Mahdi M. Kalayeh
  Name of the last author: Mubarak Shah
  Number of Figures: 9
  Number of Tables: 7
  Number of authors: 2
  Paper title: On Symbiosis of Attribute Prediction and Semantic Segmentation
  Publication Date: 2019-11-26 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Attribute Prediction Performance Evaluated by the Classification
      Error, Average Precision, and Balanced Classification Accuracy [29] on the CelebA
      [12] Original and Pre-Cropped Image Sets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Attribute Prediction Performance Evaluated by the Classification
      Error and the Average Precision (AP) on LFWA [12] Dataset
  Table 3 caption:
    table_text: TABLE 3 Attribute Prediction Performance Evaluated by the Average
      Precision(%) on WIDER Attribute [23] Dataset
  Table 4 caption:
    table_text: TABLE 4 Attribute Prediction Performance Evaluated by the Average
      Precision(%) on Berkeley Attributes of People [15] Dataset
  Table 5 caption:
    table_text: TABLE 5 Detailed Per-Attribute Classification Accuracy(%) and Average
      Precision(%) Results of Our Proposed Models for Facial Attribute Prediction
  Table 6 caption:
    table_text: TABLE 6 Detailed Per-Attribute AP(%) Results of Our Proposed Models
      for Person Attribute Prediction
  Table 7 caption:
    table_text: TABLE 7 Effect of Leveraging Image-Level Attribute Supervision for
      Semantic Face Parsing, Evaluated on the Test Split of Helen Face [2], [43]
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2956039
- Affiliation of the first author: department of electrical and computer engineering,
    university of california, san diego, san diego, ca, usa
  Affiliation of the last author: department of electrical and computer engineering,
    university of california, san diego, san diego, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Cascade_RCNN_High_Quality_Object_Detection_and_Instance_Segmentation\figure_1.jpg
  Figure 1 caption: (a) and (b) detections by object detectors of increasing qualities,
    and (c) examples of increasing quality.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Cascade_RCNN_High_Quality_Object_Detection_and_Instance_Segmentation\figure_2.jpg
  Figure 2 caption: Bounding box localization, classification loss and detection performance
    of object detectors of increasing IoU threshold u .
  Figure 3 Link: articels_figures_by_rev_year\2019\Cascade_RCNN_High_Quality_Object_Detection_and_Instance_Segmentation\figure_3.jpg
  Figure 3 caption: "The architectures of different frameworks. \u201CI\u201D is input\
    \ image, \u201Cconv\u201D backbone convolutions, \u201Cpool\u201D region-wise\
    \ feature extraction, \u201CH\u201D network head, \u201CB\u201D bounding box,\
    \ and \u201CC\u201D classification. \u201CB0\u201D is proposals in all architectures."
  Figure 4 Link: articels_figures_by_rev_year\2019\Cascade_RCNN_High_Quality_Object_Detection_and_Instance_Segmentation\figure_4.jpg
  Figure 4 caption: IoU histograms of training samples of each cascade stage. The
    distribution of the 1st stage is the RPN output. Shown in red are the percentage
    of positives for the corresponding IoU threshold.
  Figure 5 Link: articels_figures_by_rev_year\2019\Cascade_RCNN_High_Quality_Object_Detection_and_Instance_Segmentation\figure_5.jpg
  Figure 5 caption: "Distribution of the distance vector \u0394 of (4) (without normalization)\
    \ at different cascade stages. Top: plot of ( \u03B4 x , \u03B4 y ) . Bottom:\
    \ plot of ( \u03B4 w , \u03B4 h ) . Red dots are outliers for the increasing IoU\
    \ thresholds of later stages, and the statistics shown are obtained after outlier\
    \ removal."
  Figure 6 Link: articels_figures_by_rev_year\2019\Cascade_RCNN_High_Quality_Object_Detection_and_Instance_Segmentation\figure_6.jpg
  Figure 6 caption: "Architectures of the Mask R-CNN (a) and three Cascade Mask R-CNN\
    \ strategies for instance segmentation (b)-(d). Beyond the definitions of Fig.\
    \ 3, \u201CS\u201D denotes a segmentation branch. Note that segmentations branches\
    \ do not necessarily share heads with the detection branch."
  Figure 7 Link: articels_figures_by_rev_year\2019\Cascade_RCNN_High_Quality_Object_Detection_and_Instance_Segmentation\figure_7.jpg
  Figure 7 caption: (a) Detection performance of individually trained detectors, with
    their own proposals (solid curves) or Cascade R-CNN stage proposals (dashed curves).
    (b) Results of adding ground truth to the proposal set.
  Figure 8 Link: articels_figures_by_rev_year\2019\Cascade_RCNN_High_Quality_Object_Detection_and_Instance_Segmentation\figure_8.jpg
  Figure 8 caption: Detection performance of all Cascade R-CNN detectors at all cascade
    stages.
  Figure 9 Link: articels_figures_by_rev_year\2019\Cascade_RCNN_High_Quality_Object_Detection_and_Instance_Segmentation\figure_9.jpg
  Figure 9 caption: (a) Localization performance of iterative BBox and Cascade R-CNN
    regressors. (b) Detection performance of the individual classifiers of the integral
    loss detector.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Zhaowei Cai
  Name of the last author: Nuno Vasconcelos
  Number of Figures: 9
  Number of Tables: 17
  Number of authors: 2
  Paper title: 'Cascade R-CNN: High Quality Object Detection and Instance Segmentation'
  Publication Date: 2019-11-28 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of the Cascade R-CNN With Iterative BBox and Integral
      Loss Detectors
  Table 10 caption:
    table_text: TABLE 10 The Instance Segmentation Comparison Among Three Strategies
      of the Cascade Mask R-CNN
  Table 2 caption:
    table_text: TABLE 2 Stage-Wise Performance of the Three-Stage Cascade R-CNN.
  Table 3 caption:
    table_text: TABLE 3 Ablation Experiments
  Table 4 caption:
    table_text: TABLE 4 The Impact of the Number of Stages in Cascade R-CNN
  Table 5 caption:
    table_text: TABLE 5 The Cascaded Classification Performance of the Three-Stage
      Cascade R-CNN
  Table 6 caption:
    table_text: TABLE 6 Performance of State-of-the-Art Single-Model Detectors on
      COCO test-dev
  Table 7 caption:
    table_text: TABLE 7 Performance of Cascade R-CNN Implementations With Multiple
      Detectors
  Table 8 caption:
    table_text: TABLE 8 Performance of Various Implementations of the Cascade R-CNN
      With the FPN Detector on Detectron, Using the 1x Schedule
  Table 9 caption:
    table_text: TABLE 9 Proposal Recall of Cascade R-CNN Stages
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2956516
- Affiliation of the first author: media analytics and computing lab, informatics
    and communication engineering department, school of informatics, xiamen university,
    xiamen, fujian, china
  Affiliation of the last author: school of software, tsinghua university, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2019\Plenty_is_Plague_FineGrained_Learning_for_Visual_Question_Answering\figure_1.jpg
  Figure 1 caption: A comparison between the traditional learning paradigm and our
    fine-grained learning paradigm.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Plenty_is_Plague_FineGrained_Learning_for_Visual_Question_Answering\figure_2.jpg
  Figure 2 caption: "Statistics of six question types from VQA1.0 [7]. Fig.a shows\
    \ the ages of humans that can answer each type of question. Fig.b gives the performance\
    \ of VQA models using visual and textual content on different types. These two\
    \ figures serve as an indicator of the \u201Cdifficulty diversity\u201D as introduced\
    \ in Section 1. Fig.c gives the proportion of each type of questions in the dataset,\
    \ which indicates the issue of \u201Clabel redundancy\u201D. These statistics\
    \ reflect the varying difficulties of different question types and the extremely\
    \ uneven data distribution, which leads to two key issues in VQA training, i.e.,\
    \ the \u201Cdifficulty diversity\u201D and the \u201Clabel redundancy\u201D. The\
    \ target of our fine-grained learning paradigm is to address these two issues\
    \ by evaluating the learning progress of the VQA model on each question type and\
    \ selecting the most suitable examples to improve the training efficiency and\
    \ the model performance."
  Figure 3 Link: articels_figures_by_rev_year\2019\Plenty_is_Plague_FineGrained_Learning_for_Visual_Question_Answering\figure_3.jpg
  Figure 3 caption: Overall framework of our fine-grained learning paradigm. Our paradigm
    starts with a fine-grained training set, which has much fewer examples than the
    complete training set. A learning agent, composed of an actor network and a critic
    network, constantly interacts with the model training process. It evaluates the
    learning progress of the VQA model and generates actions of data augmentations
    for specific question types. The specific training data are selected via the proposed
    selection schemes, and integrated to augment the fine-grained training set. Afterwards,
    the model will be trained on the fine-grained training set and the corresponding
    rewards are used for updating the learning agent.
  Figure 4 Link: articels_figures_by_rev_year\2019\Plenty_is_Plague_FineGrained_Learning_for_Visual_Question_Answering\figure_4.jpg
  Figure 4 caption: Learning curves of different learning paradigms with different
    proportions of training examples on VQA2.0 dataset.
  Figure 5 Link: articels_figures_by_rev_year\2019\Plenty_is_Plague_FineGrained_Learning_for_Visual_Question_Answering\figure_5.jpg
  Figure 5 caption: Comparisons of the training expenditures and the model performance
    between FG-A1C paradigms and the random sampling scheme on the VQA2.0 dataset.
  Figure 6 Link: articels_figures_by_rev_year\2019\Plenty_is_Plague_FineGrained_Learning_for_Visual_Question_Answering\figure_6.jpg
  Figure 6 caption: Learning curves of different learning paradigms with different
    proportions of training examples on VQA-CP v2 dataset.
  Figure 7 Link: articels_figures_by_rev_year\2019\Plenty_is_Plague_FineGrained_Learning_for_Visual_Question_Answering\figure_7.jpg
  Figure 7 caption: Comparisons of the training expenditures and the model performance
    between FG-A1C paradigms and the random sampling scheme on the VQA-CP dataset.
  Figure 8 Link: articels_figures_by_rev_year\2019\Plenty_is_Plague_FineGrained_Learning_for_Visual_Question_Answering\figure_8.jpg
  Figure 8 caption: Sample distributions of different learning paradigms on the VQA2.0
    and VQA-CP v2 datasets. These distributions reflect preferences of different sampling
    scheme.
  Figure 9 Link: articels_figures_by_rev_year\2019\Plenty_is_Plague_FineGrained_Learning_for_Visual_Question_Answering\figure_9.jpg
  Figure 9 caption: The sampled questions of different learning paradigms.
  First author gender probability: 0.89
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Yiyi Zhou
  Name of the last author: Yue Gao
  Number of Figures: 9
  Number of Tables: 4
  Number of authors: 6
  Paper title: 'Plenty is Plague: Fine-Grained Learning for Visual Question Answering'
  Publication Date: 2019-11-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Statistics of Question Types of VQA2.0 and VQA-CP-2.0
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Evaluation Results of SAN and BUTD With Different Learning
      Paradigms on the VQA2.0-Test-Dev
  Table 3 caption:
    table_text: TABLE 3 Evaluation Results of SAN and BUTD With Different Learning
      Paradigms on the VQA-CP-v2 Test Split
  Table 4 caption:
    table_text: TABLE 4 Evaluations of BUTD on VQA2.0 Test-Dev With Visual Genome
      Dataset
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2956699
- Affiliation of the first author: beijing laboratory of intelligent information technology,
    school of computer science, beijing institute of technology, beijing, china
  Affiliation of the last author: research school of engineering, australian national
    university, canberra, act, australia
  Figure 1 Link: articels_figures_by_rev_year\2019\Dynamical_Hyperparameter_Optimization_via_Deep_Reinforcement_Learning_in_Trackin\figure_1.jpg
  Figure 1 caption: 'Sample results of our hyperparameter optimization method on a
    real-time tracker: Siam-py [61]. By only optimizing the hyperparameters of Siam-py,
    we achieve a significant improvement. For instance, our method improves the distance
    precision [67] from 4.6 percent to an astounding score of 99.7 percent on Bolt
    (top row). It also outperforms the original versions of KCF [24] and SiamFc-3s
    [4].'
  Figure 10 Link: articels_figures_by_rev_year\2019\Dynamical_Hyperparameter_Optimization_via_Deep_Reinforcement_Learning_in_Trackin\figure_10.jpg
  Figure 10 caption: Precision and success plots with AUC for OPE on the NfS [17].
  Figure 2 Link: articels_figures_by_rev_year\2019\Dynamical_Hyperparameter_Optimization_via_Deep_Reinforcement_Learning_in_Trackin\figure_2.jpg
  Figure 2 caption: 'Our training process for handling the task of dynamical hyperparameter
    optimization. This process contains three stages: supervised learning, Q-learning
    with fixed action network, and Q-learning with heuristic initialization. In the
    1st stage, the original hyperparameters (HP-siam) are treated as the supervised
    labels to train the action network. The training data contains information from
    both the video input and a base tracker (blue dashed box). Such information may
    include the output of the base tracker or several intermediate variables, e.g.,
    the heat-map in tracking results (in Section 3.1). In the 2nd stage, we freeze
    the action network (with blue color) initialized from the 1st stage and train
    the other networks in the Q-function with deep Q-learning. After that, the trained
    networks in the Q-function are used as reliable initialization in the training
    process of the 3rd stage.'
  Figure 3 Link: articels_figures_by_rev_year\2019\Dynamical_Hyperparameter_Optimization_via_Deep_Reinforcement_Learning_in_Trackin\figure_3.jpg
  Figure 3 caption: 'Illustration of visual tracking with our action-prediction network
    based an adjustment process. Green arrows show the original tracking method. A
    tracker is applied on a search region to obtain a heat-map, which is used to predict
    the object bounding box, and provides a search region for the next frame. We add
    two parts to adjust the original tracker: initialization (blue arrows) and action-prediction
    (black arrows). During initialization, we run the original tracker on a search
    region in Frame 2 to obtain the initial heat-map. During action-prediction, the
    heat-map is fed into the action network to predict optimal hyperparameters to
    adjust the tracker.'
  Figure 4 Link: articels_figures_by_rev_year\2019\Dynamical_Hyperparameter_Optimization_via_Deep_Reinforcement_Learning_in_Trackin\figure_4.jpg
  Figure 4 caption: Structure of Q-Function. Convs represents three convolutional
    layers used to process a heat-map. FCs are the enlarging network with two fully
    connected layers. N is the dimension of the action space, i.e., the number of
    hyperparameters.
  Figure 5 Link: articels_figures_by_rev_year\2019\Dynamical_Hyperparameter_Optimization_via_Deep_Reinforcement_Learning_in_Trackin\figure_5.jpg
  Figure 5 caption: 'Sample results to illustrate the effectiveness of our hyperparameter
    optimization on the baseline: Siam-py [61]. From top to bottom, the sequences
    are bolt and freeman1 (from OTB-100 [68]). (a) The beginning of the tracking errors
    produced by the baseline tracker. Yellow bounding boxes are the results of baseline:
    Siam-py. Red bounding boxes are the corresponding adjusted results. (b) The normalized
    hyperparameter sequences (solid line) and their pre-sets (dashed line) in each
    video. The scale step, scale penalty, scale learning rate, window weight, and
    template learning rate are denoted as ss, sp, slr, ww, tlr for Siam-py, respectively.
    (c) The zoomed-in regions (green rectangles) in (b).'
  Figure 6 Link: articels_figures_by_rev_year\2019\Dynamical_Hyperparameter_Optimization_via_Deep_Reinforcement_Learning_in_Trackin\figure_6.jpg
  Figure 6 caption: The results of different hyperparameters, a random search method,
    and a Bayesian optimization approach on the OTB-100 [68] benchmark. HP-ss, HP-sp,
    HP-slr, HP-win, and HP-tlr indicate that the scale step, scale penalty, scale
    learning rate, window weight, and template learning rate are fixed. Ran-Sear and
    Bayesian are the random search and Bayesian optimization methods, respectively.
  Figure 7 Link: articels_figures_by_rev_year\2019\Dynamical_Hyperparameter_Optimization_via_Deep_Reinforcement_Learning_in_Trackin\figure_7.jpg
  Figure 7 caption: Comparison of NAF, NAF+Mu, and NAF+Mu+OT (Ours) in terms of loss,
    mean q, episode reward and mean IoU. The loss may increase with training since
    the Q -estimations are away from true reward returns for complex tasks. Still,
    they can be used to learn competent policies and obtain sufficiently good results.
    mean q is the average of the estimated Q -values of each episode.
  Figure 8 Link: articels_figures_by_rev_year\2019\Dynamical_Hyperparameter_Optimization_via_Deep_Reinforcement_Learning_in_Trackin\figure_8.jpg
  Figure 8 caption: Overlap success plots of OPE with AUC for 11 tracking challenges
    on OTB-100 [68]. The AUC scores of each tracker are shown in the legend. For completeness,
    the overall results are also included in the last plot.
  Figure 9 Link: articels_figures_by_rev_year\2019\Dynamical_Hyperparameter_Optimization_via_Deep_Reinforcement_Learning_in_Trackin\figure_9.jpg
  Figure 9 caption: Precision and success plots with AUC for OPE on OTB-100 [68].
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Xingping Dong
  Name of the last author: Fatih Porikli
  Number of Figures: 12
  Number of Tables: 3
  Number of authors: 6
  Paper title: Dynamical Hyperparameter Optimization via Deep Reinforcement Learning
    in Tracking
  Publication Date: 2019-11-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Details of Convolutional Layers
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of the Proposed Methods, HP-siam and HP-bacf, Siam-py
      [61], and Several Correlation Filter based Trackers on NfS [17]
  Table 3 caption:
    table_text: TABLE 3 Evaluation of VOT2018 Real-Time Challenge in Terms of EAO
      and Speed
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2956703
- Affiliation of the first author: school of remote sensing and information engineering,
    wuhan university, wuhan, hubei, p.r. china
  Affiliation of the last author: school of information science and technology, university
    of science and technology of china, hefei, anhui, p.r. china
  Figure 1 Link: articels_figures_by_rev_year\2019\Visual_Scanpath_Prediction_Using_IORROI_Recurrent_Mixture_Density_Network\figure_1.jpg
  Figure 1 caption: 'Architecture of the proposed scanpath prediction model, which
    includes four major components: An image feature extractor (light gray), the ROI
    generation module (light blue), the fixation duration prediction module (light
    red) and the saliency guidance network (light green).'
  Figure 10 Link: articels_figures_by_rev_year\2019\Visual_Scanpath_Prediction_Using_IORROI_Recurrent_Mixture_Density_Network\figure_10.jpg
  Figure 10 caption: Saliency maps of the sample image (a). (b) Shows the saliency
    map computed from the fixations predicted by our scanpath generation model. (c)
    - (f) Show the saliency maps predicted by the state-of-the-art deep image saliency
    models.
  Figure 2 Link: articels_figures_by_rev_year\2019\Visual_Scanpath_Prediction_Using_IORROI_Recurrent_Mixture_Density_Network\figure_2.jpg
  Figure 2 caption: 'The IOR-ROI LSTM module consists of two parts: The IOR-LSTM is
    used to capture IOR pattern; the ROI-LSTM is used to aggregate information about
    next possible ROIs; channel-wise attention lies in between the IOR-LSTM and ROI-LSTM
    which adaptively focuses on different attributes of the inhibited image feature
    maps.'
  Figure 3 Link: articels_figures_by_rev_year\2019\Visual_Scanpath_Prediction_Using_IORROI_Recurrent_Mixture_Density_Network\figure_3.jpg
  Figure 3 caption: Inner structure of the IOR-LSTM. It is a modified version of original
    ConvLSTM, better simulating working mechanism of the VWM.
  Figure 4 Link: articels_figures_by_rev_year\2019\Visual_Scanpath_Prediction_Using_IORROI_Recurrent_Mixture_Density_Network\figure_4.jpg
  Figure 4 caption: Sample scanpaths from the OSIE dataset. The three scanpaths are
    recorded from three different subjects for the same stimuli. Light blue circle
    indicates initial fixation and light red circle is the last fixation. Radius of
    a circle represents the fixation duration, and longer fixation duration is signified
    by larger circle.
  Figure 5 Link: articels_figures_by_rev_year\2019\Visual_Scanpath_Prediction_Using_IORROI_Recurrent_Mixture_Density_Network\figure_5.jpg
  Figure 5 caption: Fixation duration distribution of the eye tracking data from the
    OSIE and MIT-LOWRES datasets. The minimum fixation duration is 20 and 5 ms, the
    maximum fixation duration is 1,975 and 3,000 ms in the OSIE and MIT-LOWRES datasets
    respectively, thereby resulting in a positively skewed distribution with modal
    values between 150 and 300 ms.
  Figure 6 Link: articels_figures_by_rev_year\2019\Visual_Scanpath_Prediction_Using_IORROI_Recurrent_Mixture_Density_Network\figure_6.jpg
  Figure 6 caption: "An example of generated ground truth inhibition map with respect\
    \ to scanpath shown in (a). There are two refixations in the sample scanpath,\
    \ i.e., fixation 5 and fixation 7. (b)-(h) Demonstrates the inhibition map \u03A6\
    \ \u2217 t for each fixation prediction step."
  Figure 7 Link: articels_figures_by_rev_year\2019\Visual_Scanpath_Prediction_Using_IORROI_Recurrent_Mixture_Density_Network\figure_7.jpg
  Figure 7 caption: Joint distribution of saccade amplitudes and saccade orientations
    illustrated in polar charts. The radial axis indicates saccade amplitude expressed
    in visual angle unit and the angular axis represents saccade orientation. Plot
    (a) and (b) shows the joint distribution obtained from training set of the OSIE
    and MIT-LOWRES dataset, respectively.
  Figure 8 Link: articels_figures_by_rev_year\2019\Visual_Scanpath_Prediction_Using_IORROI_Recurrent_Mixture_Density_Network\figure_8.jpg
  Figure 8 caption: Normalized performance of our scanpath prediction model with different
    number of Gaussian mixtures in the MDN.
  Figure 9 Link: articels_figures_by_rev_year\2019\Visual_Scanpath_Prediction_Using_IORROI_Recurrent_Mixture_Density_Network\figure_9.jpg
  Figure 9 caption: Visualization of the generated scanpaths from each scanpath prediction
    model on five images with different number of dominant objects. For models that
    do not predict fixation duration (without the superscript), the fixation duration
    is visualized in the magnitude of 200 ms.
  First author gender probability: 0.51
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Wanjie Sun
  Name of the last author: Feng Wu
  Number of Figures: 13
  Number of Tables: 5
  Number of authors: 3
  Paper title: Visual Scanpath Prediction Using IOR-ROI Recurrent Mixture Density
    Network
  Publication Date: 2019-12-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Results of Two Metrics for Different Models in the OSIE Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results of Two Metrics for Different Models in the MIT-LOWRES
      Dataset
  Table 3 caption:
    table_text: TABLE 3 Performance of Saliency Prediction by the Proposed Scanpath
      Prediction Model and Four State-of-the-Art Deep Learning Based Saliency Detection
      Models
  Table 4 caption:
    table_text: TABLE 4 Ablation Results of Two Metrics for Different Models in the
      OSIE Dataset
  Table 5 caption:
    table_text: TABLE 5 Ablation Results of Two Metrics for Different Models in the
      MIT-LOWRES Dataset
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2956930
