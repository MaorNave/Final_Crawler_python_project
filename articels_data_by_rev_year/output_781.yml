- Affiliation of the first author: department of electrical engineering and computer
    science, mit, cambridge, ma
  Affiliation of the last author: department of electrical engineering and computer
    science, mit, cambridge, ma
  Figure 1 Link: articels_figures_by_rev_year\2017\The_Manhattan_Frame_ModelManhattan_World_Inference_in_the_Space_of_Surface_Norma\figure_1.jpg
  Figure 1 caption: Across scales, surface normals of man-made environments exhibit
    characteristic patterns. This work establishes the connection between 3D Manhattan-World
    structures and their surface-normal distributions via the Manhattan-Frame model.
  Figure 10 Link: articels_figures_by_rev_year\2017\The_Manhattan_Frame_ModelManhattan_World_Inference_in_the_Space_of_Surface_Norma\figure_10.jpg
  Figure 10 caption: Inferred MMF in scene and surface normal space.
  Figure 2 Link: articels_figures_by_rev_year\2017\The_Manhattan_Frame_ModelManhattan_World_Inference_in_the_Space_of_Surface_Norma\figure_2.jpg
  Figure 2 caption: Structure assumptions about scenes in terms of expressiveness.
    The proposed MMF subsumes both the Atlanta and the Manhattan World models.
  Figure 3 Link: articels_figures_by_rev_year\2017\The_Manhattan_Frame_ModelManhattan_World_Inference_in_the_Space_of_Surface_Norma\figure_3.jpg
  Figure 3 caption: An MW structure maps to a MF in the surface normal space and to
    three orthogonal VPs in the image plane.
  Figure 4 Link: articels_figures_by_rev_year\2017\The_Manhattan_Frame_ModelManhattan_World_Inference_in_the_Space_of_Surface_Norma\figure_4.jpg
  Figure 4 caption: Depictions of the two proposed MF noise models.
  Figure 5 Link: articels_figures_by_rev_year\2017\The_Manhattan_Frame_ModelManhattan_World_Inference_in_the_Space_of_Surface_Norma\figure_5.jpg
  Figure 5 caption: 'Left: The unit sphere S 2 in 3D. The blue plane on the sphere
    illustrates T p S 2 , the tangent space to S 2 at point p . Middle and right:
    2D vMF distributions.'
  Figure 6 Link: articels_figures_by_rev_year\2017\The_Manhattan_Frame_ModelManhattan_World_Inference_in_the_Space_of_Surface_Norma\figure_6.jpg
  Figure 6 caption: "The geometry of the approximation of Log \u03BC ( q i ) ."
  Figure 7 Link: articels_figures_by_rev_year\2017\The_Manhattan_Frame_ModelManhattan_World_Inference_in_the_Space_of_Surface_Norma\figure_7.jpg
  Figure 7 caption: Graphical model for a mixture of K MFs.
  Figure 8 Link: articels_figures_by_rev_year\2017\The_Manhattan_Frame_ModelManhattan_World_Inference_in_the_Space_of_Surface_Norma\figure_8.jpg
  Figure 8 caption: Timing breakdown for the three different RTMF algorithms. The
    error bars show the one- sigma range.
  Figure 9 Link: articels_figures_by_rev_year\2017\The_Manhattan_Frame_ModelManhattan_World_Inference_in_the_Space_of_Surface_Norma\figure_9.jpg
  Figure 9 caption: Rotation estimation accuracy. Percentages of points assigned to
    each MF axis is color-coded in the second row.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Julian Straub
  Name of the last author: John W. Fisher
  Number of Figures: 15
  Number of Tables: 1
  Number of authors: 5
  Paper title: "The Manhattan Frame Model\u2014Manhattan World Inference in the Space\
    \ of Surface Normals"
  Publication Date: 2017-02-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Algorithm Timings on NYU V2 Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2662686
- Affiliation of the first author: bjut faculty of information technology, beijing
    university of technology, beijing, china
  Affiliation of the last author: discipline of business analytics, the university
    of sydney, nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2017\MultiDimensional_Sparse_Models\figure_1.jpg
  Figure 1 caption: "Illustration of the correlation in a 2D image and its vectorized\
    \ 1D signal. (a) An image patch of size 32\xD732 ; (b) The 3D smooth surface of\
    \ the image patch; (c) The vectorized 1D signal. As shown in this figure, the\
    \ vectorized 1D signal breaks the structure of the 2D image."
  Figure 10 Link: articels_figures_by_rev_year\2017\MultiDimensional_Sparse_Models\figure_10.jpg
  Figure 10 caption: 'Visual quality comparison of SR results for Image 13 corresponding
    to Table 8. From left to right: original images, results of bicubic interpolation,
    [56], [57], and our 2D-SSM method, respectively.'
  Figure 2 Link: articels_figures_by_rev_year\2017\MultiDimensional_Sparse_Models\figure_2.jpg
  Figure 2 caption: "Illustration of a tensor X \u2013 \u2013 equivalently expressed\
    \ by X \u2013 \u2013 = \u2211 ijk B \u2013 \u2013 ijk ( u i \u2297 \u2013 \u2013\
    \ v j \u2297 \u2013 \u2013 w k ) and X \u2013 \u2013 = B \u2013 \u2013 \xD7 1\
    \ D 1 \xD7 2 D 2 \xD7 3 D 3 . An arbitrary tensor X \u2013 \u2013 is a linear\
    \ combination of a redundant basis u i \u2297 \u2013 \u2013 v j \u2297 \u2013\
    \ \u2013 w k (i,j,k) of the tensor space to which X \u2013 \u2013 belongs. The\
    \ base vector u i \u2297 \u2013 \u2013 v j \u2297 \u2013 \u2013 w k is the tensor\
    \ product of base vectors u i , v j , and w k . The basis u i i , v j j , and\
    \ w k k constitute the dictionaries D 1 , D 2 , and D 3 respectively. B \u2013\
    \ \u2013 is determined by coefficients B \u2013 \u2013 ijk ."
  Figure 3 Link: articels_figures_by_rev_year\2017\MultiDimensional_Sparse_Models\figure_3.jpg
  Figure 3 caption: "Illustration of 1D sparse models (a), (c) and 3D sparse models\
    \ (b), (d). X \u2013 \u2013 is a 3D input cube X \u2013 \u2013 and vec( X \u2013\
    \ \u2013 ) is its vectorized 1D signal. The curves on x -, y - and z -ordinate\
    \ reflect the distinct features in the 3D cube. (a) vec( X \u2013 \u2013 )=Dvec(\
    \ B \u2013 \u2013 ),\u2225vec( B \u2013 \u2013 ) \u2225 0 =K , where vec( X \u2013\
    \ \u2013 ) can be represented by linear combination of K atoms of 1D dictionary\
    \ D . The gray and white squares denote non-zeros and zero values, respectively.\
    \ (b) X \u2013 \u2013 = B \u2013 \u2013 \xD7 1 D 1 \xD7 2 D 2 \xD7 3 D 3 ,\u2225\
    \ B \u2013 \u2013 \u2225 0 =K , where X \u2013 \u2013 can be directly composed\
    \ by 3D dictionaries D 1 , D 2 , D 3 of the x -, y - and z - dimension, and a\
    \ sparse cube B . The gray cubes denote non-zero values. (c) \u03A9vec( X \u2013\
    \ \u2013 )=vec( B \u2013 \u2013 ) , which means vec( X \u2013 \u2013 ) multiplied\
    \ by 1D dictionary \u03A9 can lead to sparse vector vec( B \u2013 \u2013 ) satisfying\
    \ \u2225vec( B \u2013 \u2013 ) \u2225 0 =P\u2212K . vec( B \u2013 \u2013 ) is\
    \ the analysis representative vector with K zeros. Here black and white squares\
    \ denote non-zero and zero values, respectively. (d) B \u2013 \u2013 = X \u2013\
    \ \u2013 \xD7 1 \u03A9 1 \xD7 2 \u03A9 2 \xD7 3 \u03A9 3 ,\u2225 B \u2013 \u2013\
    \ \u2225 0 =P\u2212K , which means X \u2013 \u2013 multiplied by 3D dictionaries\
    \ \u03A9 1 , \u03A9 2 , and \u03A9 3 from x -, y - and z - dimension respectively\
    \ to generate a sparse cube B \u2013 \u2013 . Here white cubes denote zero values."
  Figure 4 Link: articels_figures_by_rev_year\2017\MultiDimensional_Sparse_Models\figure_4.jpg
  Figure 4 caption: '12 test images for image denoising: ''Lena'', ''Peppers'', ''House'',
    ''Barbara'', ''Fingerprint'', ''Boats'', ''Cameraman'', ''Couple'', ''Flintstones'',
    ''Hill'', ''Man'', and ''Bridge''.'
  Figure 5 Link: articels_figures_by_rev_year\2017\MultiDimensional_Sparse_Models\figure_5.jpg
  Figure 5 caption: "Denoising performance comparison on the 'Peppers' image ( \u03C3\
    =20 ). Left to right: Original image, original image patch, KSVD [50], EPLL [53],\
    \ LSSC [51] , SAPCA-BM3D [54], NCSR [52], our proposal 2D-SSM (PSNR =30.87dB,31.17dB,31.37dB,31.53dB\
    \ and 31.22dB )."
  Figure 6 Link: articels_figures_by_rev_year\2017\MultiDimensional_Sparse_Models\figure_6.jpg
  Figure 6 caption: "Visual quality comparison of denoising results for 'Lena' with\
    \ dictionaries of similar sizes in 1D and 2D models. From left to right: original\
    \ image, noise image, the denoised results by [35] and [36] with a dictionary\
    \ of size 4\xD749 , and our denoised result with a dictionary of size 2\xD78\xD7\
    7 . (PSNR = 35.54 dB [35], 35.74 dB [36], 38.37dB (Ours))."
  Figure 7 Link: articels_figures_by_rev_year\2017\MultiDimensional_Sparse_Models\figure_7.jpg
  Figure 7 caption: "Dictionaries used for denoising 'Lena': (a) Dictionary \u03A9\
    \ of size 4\xD749 by [35]; (b) Dictionary \u03A9 of size 4\xD749 by [36]. Every\
    \ small square denotes an atom; (c) Our dictionaries \u03A9 1 , \u03A9 2 of size\
    \ 2\xD78\xD77 . Each row is an atom; (d) Dictionary \u03A9= \u03A9 1 \u2297 \u03A9\
    \ 2 ."
  Figure 8 Link: articels_figures_by_rev_year\2017\MultiDimensional_Sparse_Models\figure_8.jpg
  Figure 8 caption: The test image set for image super-resolution performance evaluation.
  Figure 9 Link: articels_figures_by_rev_year\2017\MultiDimensional_Sparse_Models\figure_9.jpg
  Figure 9 caption: 'Our Learned dictionaries. From left to right: D h 1 , D h 2 and
    C (k) 1 , C (k) 2 5 k=1 , in which each column is the atom of one directional
    signal of the HR image patches and the LR feature patches.'
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Na Qi
  Name of the last author: Junbin Gao
  Number of Figures: 25
  Number of Tables: 16
  Number of authors: 6
  Paper title: Multi-Dimensional Sparse Models
  Publication Date: 2017-02-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Number of Dictionary Parameters in 1D Sparse Models and 3D
      Sparse Models. Notations of Parameters Are Shown in Fig. 3
  Table 10 caption:
    table_text: TABLE 10 Experiment Setting for MD Signal Denoising in MD-SSM, Where
      'TS' Denotes Each Tensor Size and 'DS' Denotes the Dictionary Size
  Table 2 caption:
    table_text: TABLE 2 MD Synthesis Dictionary Learning Algorithm
  Table 3 caption:
    table_text: TABLE 3 MD Analysis Dictionary Learning Algorithm
  Table 4 caption:
    table_text: TABLE 4 Time Complexity Analysis of Dictionary Update for MD and 1D
      Sparse Models in A Single Iteration Cycle and Memory Usage of Dictionary
  Table 5 caption:
    table_text: TABLE 5 Parameters Used for Training on the Corrupted Image
  Table 6 caption:
    table_text: 'TABLE 6 PSNR (dB) Results by Different Denoising Methods, in Each
      Cell, the Results of the 6 Denoising Methods Are Reported in the Following Order:
      KSVD [50], EPLL [53], LSSC [51], SAPCA-BM3D [54], NCSR [52] , and Our 2D-SSM'
  Table 7 caption:
    table_text: TABLE 7 Denoising Results Valued by PSNR. In this Test, the Sizes
      of the Dictionaries Used in Our MD Model Are Much Less Than Those in [34], [35],
      [36]
  Table 8 caption:
    table_text: "TABLE 8 Objective Evaluation of 3\xD7 SR Reconstructions in Terms\
      \ of PSNR (dB) and SSIM. The Total Dictionary Sizes of [56], [57], and Our Scheme\
      \ Are 225\xD760 , ( 324\xD730 + 111\xD75 ), and 2\xD739\xD732 , Respectively"
  Table 9 caption:
    table_text: TABLE 9 Time Complexity of Our Proposed MD-SSM for Image Super-Resolution
      in Comparison with That of [56].
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2663423
- Affiliation of the first author: science & technology on integrated information
    system laboratory, chinese academy of sciences, beijing, china
  Affiliation of the last author: department of electrical engineering and computer
    science, northwestern university, evanston, il
  Figure 1 Link: articels_figures_by_rev_year\2017\Discriminative_Dimensionality_Reduction_for_MultiDimensional_Sequences\figure_1.jpg
  Figure 1 caption: "Top: Individual sequence samples. A video of one person walking\
    \ corresponds to an individual action pattern \u201Cwalk\u201D; an online stroke\
    \ sequence corresponds to an individual Chinese character pattern. Middle: An\
    \ example of a concatenate sequence corresponding to three ordered action patterns\
    \ of \u201Cwalk\u201D, \u201Crun\u201D and \u201Cjump\u201D for a person continuously\
    \ performing these actions in this video, and the action segments are unavailable.\
    \ Bottom: An example of the effects of dimensionality reduction for sequence (DRS)\
    \ on a concatenate sequence corresponding to several Arabic letters, as shown\
    \ on the right."
  Figure 10 Link: articels_figures_by_rev_year\2017\Discriminative_Dimensionality_Reduction_for_MultiDimensional_Sequences\figure_10.jpg
  Figure 10 caption: Comparison of different approximations of the optimal AIMs in
    the subspace for the (a) M-LSDA by the HMM classifier, (b) M-LSDA by the DTW classifier,
    (c) D-LSDA by the HMM classifier, and (d) D-LSDA by the DTW classifier.
  Figure 2 Link: articels_figures_by_rev_year\2017\Discriminative_Dimensionality_Reduction_for_MultiDimensional_Sequences\figure_2.jpg
  Figure 2 caption: "Samples of two Arabic characters \u201CDaad\u201D and \u201C\
    Saad\u201D are represented as frame sequences in the bottom. Frames corresponding\
    \ to the same state in each sample are boxed and linked to the state with the\
    \ same color. The mean sequences, state pixel-wise standard deviations and state\
    \ variances are visualized on top of the model. Each state captures a local structure\
    \ of the character, and each character pattern can be represented with three ordered\
    \ structures. The two characters only differ in the second state."
  Figure 3 Link: articels_figures_by_rev_year\2017\Discriminative_Dimensionality_Reduction_for_MultiDimensional_Sequences\figure_3.jpg
  Figure 3 caption: The diagram of extracting model-based statistics. Each training
    sample is boxed by a dashed rectangle. An HMM is first trained for each class,
    and vectors in sequence samples are aligned to the corresponding states. Different
    states of HMMs are shown in different colors. Means and variances are then extracted
    from these states. The means of states from the same HMM are connected to form
    the mean sequence, and state variances are pooled to form the variance of the
    class.
  Figure 4 Link: articels_figures_by_rev_year\2017\Discriminative_Dimensionality_Reduction_for_MultiDimensional_Sequences\figure_4.jpg
  Figure 4 caption: The optimal alignment and the corresponding AIMs obtained by DTW.
    A lattice corresponds to an AIM with respect to one sequence, where filled grids
    indicate that the corresponding elements are 1 and empty grids indicate that the
    related elements are 0 .
  Figure 5 Link: articels_figures_by_rev_year\2017\Discriminative_Dimensionality_Reduction_for_MultiDimensional_Sequences\figure_5.jpg
  Figure 5 caption: Robustness of M-LSDA on processing sequences with redundant temporal
    information and noisyoutlier vectors.
  Figure 6 Link: articels_figures_by_rev_year\2017\Discriminative_Dimensionality_Reduction_for_MultiDimensional_Sequences\figure_6.jpg
  Figure 6 caption: Recognition rates of different DRS methods (a) by the HMM classifier
    on the HAS dataset; (b) by the DTW classifier on the HAS dataset; (c) by the HMM
    classifier on the SAD dataset; (d) by the DTW classifier on the SAD dataset.
  Figure 7 Link: articels_figures_by_rev_year\2017\Discriminative_Dimensionality_Reduction_for_MultiDimensional_Sequences\figure_7.jpg
  Figure 7 caption: Samples of the selected similar characters and their corresponding
    GBK codes.
  Figure 8 Link: articels_figures_by_rev_year\2017\Discriminative_Dimensionality_Reduction_for_MultiDimensional_Sequences\figure_8.jpg
  Figure 8 caption: Recognition rates and standard deviations of different DRS methods
    on similar character recognition with (a) the HMM classifier and (b) the DTW classifier.
  Figure 9 Link: articels_figures_by_rev_year\2017\Discriminative_Dimensionality_Reduction_for_MultiDimensional_Sequences\figure_9.jpg
  Figure 9 caption: 'Performances of different DRS methods on the ChaLearn dataset:
    (a) Accuracy with the HMM classifier; (b) F-score with rank pooling + SVM.'
  First author gender probability: 0.96
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.86
  Name of the first author: Bing Su
  Name of the last author: Ying Wu
  Number of Figures: 13
  Number of Tables: 4
  Number of authors: 4
  Paper title: Discriminative Dimensionality Reduction for Multi-Dimensional Sequences
  Publication Date: 2017-02-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Frequently Used Notations and Acronyms
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison with State-of-the-Art Results on the ChaLearn Dataset
  Table 3 caption:
    table_text: "TABLE 3 (a) WRR and (b) CAR on APTI Database with Font \u201CArabic\
      \ Transparent\u201D, Style \u201CPlain\u201D and Size \u201C24\u201D"
  Table 4 caption:
    table_text: TABLE 4 Comparison of Arabic Handwriting Recognition Systems on the
      IFNENIT Dataset
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2665545
- Affiliation of the first author: Not Available
  Affiliation of the last author: department of electrical and electronic engineering,
    imperial college london, london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2017\LatentClass_Hough_Forests_for__DoF_Object_Pose_Estimation\figure_1.jpg
  Figure 1 caption: 'An illustration of intermediate results of the iterative process.
    Column 1: Initial pose estimation and the corresponding Hough map. Columns 2-3:
    Foreground probability masks and the respective Hough maps after 5 and 10 iterations,
    respectively.'
  Figure 10 Link: articels_figures_by_rev_year\2017\LatentClass_Hough_Forests_for__DoF_Object_Pose_Estimation\figure_10.jpg
  Figure 10 caption: 'Rows 1-4 show, from left to right, the original RGB image, the
    final segmentation mask, the final Hough vote map and the augmented 3D axis of
    the estimated result. The final row shows some incorrect results, from left to
    right: one false positive leading to a false segmentation mask and wrong 3D rendering
    and finally a false negative.'
  Figure 2 Link: articels_figures_by_rev_year\2017\LatentClass_Hough_Forests_for__DoF_Object_Pose_Estimation\figure_2.jpg
  Figure 2 caption: "During training, a random patch T is selected (red frame) as\
    \ a template. The similarity between it and all other patches is measured and\
    \ splitted based on a threshold \u03C4 (dotted circle). This process is repeated\
    \ until the optimal T and \u03C4 are found and stored in the current node."
  Figure 3 Link: articels_figures_by_rev_year\2017\LatentClass_Hough_Forests_for__DoF_Object_Pose_Estimation\figure_3.jpg
  Figure 3 caption: The z -value check enhances the robustness against clutter and
    occluders. Blue patches indicate a true positive match and red ones a false positives
    one. Without the z-check, features of the planar background of the false patch
    could match the training patch and become a false positive.
  Figure 4 Link: articels_figures_by_rev_year\2017\LatentClass_Hough_Forests_for__DoF_Object_Pose_Estimation\figure_4.jpg
  Figure 4 caption: 'Inference process: Input LINEMOD patches are extracted from RGB-D
    images. For each iteration, a subset of trees are drawn from the forest with bagging.'
  Figure 5 Link: articels_figures_by_rev_year\2017\LatentClass_Hough_Forests_for__DoF_Object_Pose_Estimation\figure_5.jpg
  Figure 5 caption: F1-Scores for the eight different patch sizes and the three different
    forests in our validation dataset.
  Figure 6 Link: articels_figures_by_rev_year\2017\LatentClass_Hough_Forests_for__DoF_Object_Pose_Estimation\figure_6.jpg
  Figure 6 caption: The impact of bagging in our iterative process. Accuracy (F1-Scores)
    versus number of trees selected for bagging under four different forest sizes.
  Figure 7 Link: articels_figures_by_rev_year\2017\LatentClass_Hough_Forests_for__DoF_Object_Pose_Estimation\figure_7.jpg
  Figure 7 caption: The impact of the number of iterations on the performance of our
    system for our validation dataset on a fixed patch size of 23 and a forest with
    10 trees.
  Figure 8 Link: articels_figures_by_rev_year\2017\LatentClass_Hough_Forests_for__DoF_Object_Pose_Estimation\figure_8.jpg
  Figure 8 caption: Precision-Recall curves for different number of iterations and
    patch sizes.
  Figure 9 Link: articels_figures_by_rev_year\2017\LatentClass_Hough_Forests_for__DoF_Object_Pose_Estimation\figure_9.jpg
  Figure 9 caption: Average precision-recall curve over all objects in the dataset
    of LINEMOD [16] (left) and our Domestic Environment Dataset (right). The shaded
    region represents one standard deviation above and below the precision value at
    a given recall value.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Alykhan Tejani
  Name of the last author: Tae-Kyun Kim
  Number of Figures: 11
  Number of Tables: 5
  Number of authors: 5
  Paper title: Latent-Class Hough Forests for 6 DoF Object Pose Estimation
  Publication Date: 2017-02-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 F1-Scores for LINEMOD [14], the Method of Drost et al. [10]
      and Our Approach for Each Object Class for the Dataset of Hinterstoisser et
      al. [16] and Our Domestic Environment Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Matching Score and Speed on the Dataset of Hinterstoisser
      et al. [16] for LINEMOD [14], the Methods of Drost et al. [10], Rios Cabrera
      et al. [31] , Brachman et al. [4] and Our Approach
  Table 3 caption:
    table_text: TABLE 3 Impact of Different Modalities on the Performance of Our Method
      Based on a Smaller Validation Subset
  Table 4 caption:
    table_text: TABLE 4 Comparison of Two Versions of Our Latent-Class Hough Forests
      and the Method of Brachmann et al. [4]
  Table 5 caption:
    table_text: TABLE 5 Percentages of Accurate Detections and F1-Scores for Two Versions
      of Our Latent-Class Hough Forests and the Method of Brachmann et al. [4]
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2665623
- Affiliation of the first author: department of mechanical and aerospace engineering,
    seoul national university, seoul, korea
  Affiliation of the last author: department of electrical & systems engineering,
    university of pennsylvania, philadelphia, pa
  Figure 1 Link: articels_figures_by_rev_year\2017\Fluid_Dynamic_Models_for_BhattacharyyaBased_Discriminant_Analysis\figure_1.jpg
  Figure 1 caption: Low dimensional fluid flow fields to find the linear subspace
    that maximally separates the class-conditional densities. The optimal projection
    directions are intuitively related to the largest flow directions. Schematically,
    in the left figure, the class densities repel each other in the low-dimensional
    direction to maximize the separation. In the right figure where the two densities
    overlap, the densities are squeezed in directions to minimize the overlap. We
    show how to analytically estimate these optimal directions and how they can be
    used for discriminant analysis.
  Figure 10 Link: articels_figures_by_rev_year\2017\Fluid_Dynamic_Models_for_BhattacharyyaBased_Discriminant_Analysis\figure_10.jpg
  Figure 10 caption: '(a) A sample of the KOSPI index for one day as a function of
    time; (b) first and second optimal filters from the fluid model to discriminate
    when the index will have increased by more than 2 percent at the end of the next
    day; (c) ROC curves for classification using different filters: comparing the
    first filter of the fluid model, the first two filters of the fluid model, FDA,
    linear SVM, and SVM with RBF kernel.'
  Figure 2 Link: articels_figures_by_rev_year\2017\Fluid_Dynamic_Models_for_BhattacharyyaBased_Discriminant_Analysis\figure_2.jpg
  Figure 2 caption: The force field F 2 (x) on class 2 induced by class 1 for different
    configurations of two Gaussians. The field is a repulsive force pushing class
    2 in (a), and compresses the fluid in (b). This behavior illustrates the importance
    of considering both the locations and shapes of the distributions in our fluid
    model.
  Figure 3 Link: articels_figures_by_rev_year\2017\Fluid_Dynamic_Models_for_BhattacharyyaBased_Discriminant_Analysis\figure_3.jpg
  Figure 3 caption: "For different configurations of two-dimensional Gaussians in\
    \ (a) and (b), the solutions of the fluid model are shown as a plot of angles\
    \ \u03B8 compared to the optimal angles obtained by directly optimizing the Bayes\
    \ and Bhattacharyya criterion. In both examples (a) and (b), the mean separation\
    \ between the two classes is varied, and the solutions converge to the FDA solution\
    \ as this separation goes to infinity. On the other hand, when the mean separation\
    \ goes to zero, the solutions converge to the optimal equal mean analysis. The\
    \ fluid model is able to smoothly interpolate between these two regimes for intermediate\
    \ values of the mean separation."
  Figure 4 Link: articels_figures_by_rev_year\2017\Fluid_Dynamic_Models_for_BhattacharyyaBased_Discriminant_Analysis\figure_4.jpg
  Figure 4 caption: Classification results using projected nearest neighbors for five
    classes.
  Figure 5 Link: articels_figures_by_rev_year\2017\Fluid_Dynamic_Models_for_BhattacharyyaBased_Discriminant_Analysis\figure_5.jpg
  Figure 5 caption: Two-way classification results across all class pairs.
  Figure 6 Link: articels_figures_by_rev_year\2017\Fluid_Dynamic_Models_for_BhattacharyyaBased_Discriminant_Analysis\figure_6.jpg
  Figure 6 caption: Classification results on benchmark data for various subspace
    dimensionalities.
  Figure 7 Link: articels_figures_by_rev_year\2017\Fluid_Dynamic_Models_for_BhattacharyyaBased_Discriminant_Analysis\figure_7.jpg
  Figure 7 caption: (a) Sample signals and means of two homoscedastic Gaussian processes.
    (b) Fluid model filter (equivalent to FDA in this case) and mean difference filter.
    This fluid model filter has two positive and negative peaks indicating two points
    of input change (See Section 5.1.1). (c) ROC curve for classification using the
    fluid model filter and mean difference filter.
  Figure 8 Link: articels_figures_by_rev_year\2017\Fluid_Dynamic_Models_for_BhattacharyyaBased_Discriminant_Analysis\figure_8.jpg
  Figure 8 caption: (a) Sample signals and the mean of two heteroscedastic Gaussian
    processes. (b) Fluid model filter and FDA filter. (c) ROC curve for classification
    using fluid model filter, mean difference filter, and FDA filter.
  Figure 9 Link: articels_figures_by_rev_year\2017\Fluid_Dynamic_Models_for_BhattacharyyaBased_Discriminant_Analysis\figure_9.jpg
  Figure 9 caption: (a) A sample of walking motion; (b) a sample of running motion;
    (c) embeddings in the first two discriminative features.
  First author gender probability: 0.79
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yung-Kyun Noh
  Name of the last author: Daniel D. Lee
  Number of Figures: 10
  Number of Tables: 0
  Number of authors: 5
  Paper title: Fluid Dynamic Models for Bhattacharyya-Based Discriminant Analysis
  Publication Date: 2017-02-08 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2666148
- Affiliation of the first author: school of mechanical and aerospace engineering,
    seoul national university, seoul, korea
  Affiliation of the last author: department of electrical & systems engineering,
    university of pennsylvania, philadelphia, pa
  Figure 1 Link: articels_figures_by_rev_year\2017\Generative_Local_Metric_Learning_for_Nearest_Neighbor_Classification\figure_1.jpg
  Figure 1 caption: Metric change for grid data. When the metric is changed by extending
    the horizontal direction, the separation between data is increased along the horizontal
    line.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Generative_Local_Metric_Learning_for_Nearest_Neighbor_Classification\figure_2.jpg
  Figure 2 caption: Metric change for data from a probabilistic generative model.
    For random data from two different probability density functions, extending the
    horizontal line simply reduces the densities. There is no visible increase of
    separation along the horizontal direction between different data any greater than
    the increase of separation along the vertical direction.
  Figure 3 Link: articels_figures_by_rev_year\2017\Generative_Local_Metric_Learning_for_Nearest_Neighbor_Classification\figure_3.jpg
  Figure 3 caption: "The nearest neighbor x NN appears at a finite distance d N from\
    \ x 0 due to finite sampling. Given the data distribution p(x) , the average probability\
    \ density function over the surface of a D dimensional hypersphere is p ~ ( x\
    \ 0 )=p( x 0 )+ d 2 N 2D \u2207 2 p | x= x 0 for small d N ."
  Figure 4 Link: articels_figures_by_rev_year\2017\Generative_Local_Metric_Learning_for_Nearest_Neighbor_Classification\figure_4.jpg
  Figure 4 caption: Optimal local metrics are shown on the left for three example
    Gaussian distributions in a 5-dimensional space. The projected 2-dimensional distributions
    are represented as ellipses (one standard deviation from the mean), while the
    remaining 3 dimensions have isotropic distributions. The local p ~ p of the three
    classes is plotted on the right using a euclidean metric I and for the optimal
    metric A opt . The solution A opt tries to keep the ratio p ~ p over the different
    classes as similar as possible for different distance d N .
  Figure 5 Link: articels_figures_by_rev_year\2017\Generative_Local_Metric_Learning_for_Nearest_Neighbor_Classification\figure_5.jpg
  Figure 5 caption: (a) Gaussian synthetic data with varying dimensionalities. As
    the number of dimensions grows, accuracies of many methods degrade. On the other
    hand, GLML continues to improve its performance vastly over other methods. Experiments
    (b) sim (h) on benchmark datasets vary the number of training data per class.
    The speech dataset (i) TI46 is pronounced by eight men and eight women. The Fisher
    kernel and BM are omitted for (f) sim (i) and (h),(i) respectively, since their
    performances are much worse than the nearest neighbor classifier with euclidean
    and other metrics. DML-eig and PLML are (partly) missing in (d) sim (f),(h),(i)
    and (d) sim (f),(h) respectively, due to the complexity of the algorithms.
  Figure 6 Link: articels_figures_by_rev_year\2017\Generative_Local_Metric_Learning_for_Nearest_Neighbor_Classification\figure_6.jpg
  Figure 6 caption: (a) Non-Gaussian data for nearest neighbor classification. (b)
    Classification performance of using Gaussian generative models and nearest neighbor
    performance with euclidean, LMNN, and GLML metrics.
  Figure 7 Link: articels_figures_by_rev_year\2017\Generative_Local_Metric_Learning_for_Nearest_Neighbor_Classification\figure_7.jpg
  Figure 7 caption: Isomap embedding with (a) euclidean metric, (b) Large Margin Nearest
    Neighbor (LMNN) metric, (c) GLML metric, and (d) t-Distributed Stochastic Neighbor
    Embedding (t-SNE) embedding. The inset figures represent the label distribution.
  Figure 8 Link: articels_figures_by_rev_year\2017\Generative_Local_Metric_Learning_for_Nearest_Neighbor_Classification\figure_8.jpg
  Figure 8 caption: Nearest neighbor classification results with various metrics and
    5 nearest neighbors. Nearest neighbor search uses GLML, PLML, and DML-eig metric
    for (a) Caltech 101 dataset with SURF feature, (b) Caltech 101 dataset with trained
    AlexNet hidden layer, and (c) Caltech 256 dataset with trained AlexNet hidden
    layer.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.79
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yung-Kyun Noh
  Name of the last author: Daniel D. Lee
  Number of Figures: 8
  Number of Tables: 0
  Number of authors: 3
  Paper title: Generative Local Metric Learning for Nearest Neighbor Classification
  Publication Date: 2017-02-08 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2666151
- Affiliation of the first author: research center for information technology innovation,
    academia sinica, taipei, taiwan
  Affiliation of the last author: institute of information science, academia sinica,
    taipei, taiwan
  Figure 1 Link: articels_figures_by_rev_year\2017\Supervised_Learning_of_SemanticsPreserving_Hash_via_Deep_Convolutional_Neural_Ne\figure_1.jpg
  Figure 1 caption: An overview of our proposed supervised semantic-preserving deep
    hashing (SSDH) that takes AlexNet as an example. We construct the hash functions
    as a latent layer with K units between the image representation layer and classification
    outputs in a convolutional neural network (CNN). SSDH takes inputs from images
    and learns image representations, binary codes, and classification through the
    optimization of an objective function that combines a classification loss with
    desirable properties of hash codes. The learned codes preserve the semantic similarity
    between images and are compact for image search.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Supervised_Learning_of_SemanticsPreserving_Hash_via_Deep_Convolutional_Neural_Ne\figure_2.jpg
  Figure 2 caption: Binary codes for retrieval. Images are fed to the network, and
    their corresponding binary codes are obtained by binarizing the activations of
    the latent layer. For image retrieval, the binary codes of a query and of every
    image in the database are compared based on the Hamming distance. The images closest
    to the query are returned as the results.
  Figure 3 Link: articels_figures_by_rev_year\2017\Supervised_Learning_of_SemanticsPreserving_Hash_via_Deep_Convolutional_Neural_Ne\figure_3.jpg
  Figure 3 caption: 'Sample images from the Yahoo-1M and UT-ZAP50K datasets. Upper:
    Yahoo-1M images. The product images are of heterogeneous types, including those
    that are backgroundless or of cluttered backgrounds, with or without humans. Lower:
    UT-ZAP50K images.'
  Figure 4 Link: articels_figures_by_rev_year\2017\Supervised_Learning_of_SemanticsPreserving_Hash_via_Deep_Convolutional_Neural_Ne\figure_4.jpg
  Figure 4 caption: Comparative evaluation of different hashing algorithms on the
    CIFAR-10 dataset. (a) mAP curves with respect to different number of hash bits.
    (b) Precision curves with respect to different number of top retrieved samples
    when the 48-bit hash codes are used in the evaluation. (c) Precision within Hamming
    radius 2 curves with respect to different number of hash bits.
  Figure 5 Link: articels_figures_by_rev_year\2017\Supervised_Learning_of_SemanticsPreserving_Hash_via_Deep_Convolutional_Neural_Ne\figure_5.jpg
  Figure 5 caption: Comparative evaluation of different hashing algorithms on the
    MNIST dataset. (a) mAP curves with respect to different number of hash bits. (b)
    Precision curves with respect to different number of top retrieved samples when
    the 48-bit hash codes are used in the evaluation. (c) Precision within Hamming
    radius 2 curves with respect to different number of hash bits.
  Figure 6 Link: articels_figures_by_rev_year\2017\Supervised_Learning_of_SemanticsPreserving_Hash_via_Deep_Convolutional_Neural_Ne\figure_6.jpg
  Figure 6 caption: Comparative evaluation of different hashing algorithms on the
    NUS-WIDE dataset. (a) mAP curves of top 5,000 returned images with respect to
    different number of hash bits. (b) Precision curves with respect to different
    number of top retrieved samples when the 48-bit hash codes are used in the evaluation.
    (c) Precision within Hamming radius 2 curves with respect to different number
    of hash bits.
  Figure 7 Link: articels_figures_by_rev_year\2017\Supervised_Learning_of_SemanticsPreserving_Hash_via_Deep_Convolutional_Neural_Ne\figure_7.jpg
  Figure 7 caption: Precision curves with respect to different number of top retrieved
    samples on the SUN397 dataset. The number inside parentheses indicates the code
    length.
  Figure 8 Link: articels_figures_by_rev_year\2017\Supervised_Learning_of_SemanticsPreserving_Hash_via_Deep_Convolutional_Neural_Ne\figure_8.jpg
  Figure 8 caption: Precision curves with respect to different number of top retrieved
    samples on the Yahoo-1M dataset when the 128-bit hash codes are used in the evaluation.
    AlexNet-ft denotes that the features from layer F7 of AlexNet fine-tuned on Yahoo-1M
    are used in learning hash codes.
  Figure 9 Link: articels_figures_by_rev_year\2017\Supervised_Learning_of_SemanticsPreserving_Hash_via_Deep_Convolutional_Neural_Ne\figure_9.jpg
  Figure 9 caption: Precision curves with respect to different number of top retrieved
    samples on the UT-ZAP50K dataset when the 48-bit hash codes are used in the evaluation.
    AlexNet-ft denotes that the features from layer F7 of AlexNet fine-tuned on UT-ZAP50K
    are used in learning hash codes.
  First author gender probability: 0.66
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.64
  Name of the first author: Huei-Fang Yang
  Name of the last author: Chu-Song Chen
  Number of Figures: 9
  Number of Tables: 10
  Number of authors: 3
  Paper title: Supervised Learning of Semantics-Preserving Hash via Deep Convolutional
    Neural Networks
  Publication Date: 2017-02-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Statistics of Datasets Used in the Experiments
  Table 10 caption:
    table_text: TABLE 10 Classification Accuracy of Various Methods on CIFAR-10, SUN397,
      Yahoo-1M, and ILSVRC2012
  Table 2 caption:
    table_text: "TABLE 2 The mAPs (%) of SSDH with 48 Bits versus \u03B2 and \u03B3\
      \ while \u03B1 is Set to 1 on the CIFARI-10 Dataset"
  Table 3 caption:
    table_text: TABLE 3 Performance Comparison of Using L1- and L2-Losses in Eq. (7)
      on CIFAR-10 and MNIST Based on mAP (%)
  Table 4 caption:
    table_text: TABLE 4 Performance Comparison of Using L1- and L2-Margin Losses in
      Eq. (4) on NUS-WIDE Based on mAP (%) and Precision (%) at 500 Samples
  Table 5 caption:
    table_text: TABLE 5 mAP (%) of Various Methods at 128 Bits on the Yahoo-1M Dataset
  Table 6 caption:
    table_text: TABLE 6 The mAP at Top 1,000 Returned Images and Precision at k Samples
      of Methods on the ILSVRC2012 Validation Set
  Table 7 caption:
    table_text: TABLE 7 The mAPs of SSDH with Different Deep Models on CIFAR-10, NUS-WIDE,
      Yahoo-1M, and ILSVRC2012
  Table 8 caption:
    table_text: TABLE 8 Number of Parameters and Amount of Storage of Different Network
      Models with a 48-Bit Latent Layer (in CAFFE)
  Table 9 caption:
    table_text: TABLE 9 Comparison of the Instance-Level Retrieval Performance (mAP
      (%)) of SSDH with Other Approaches on the Paris and Oxford Datasets
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2666812
- Affiliation of the first author: collaborative innovation center of high performance
    computing, national university of defense technology, changsha, china
  Affiliation of the last author: guangdong province key laboratory of information
    security, guangzhou, p. r. china
  Figure 1 Link: articels_figures_by_rev_year\2017\Person_ReIdentification_by_Camera_Correlation_Aware_Feature_Augmentation\figure_1.jpg
  Figure 1 caption: 'Illustration of person re-id challenges [1]. Left: Great visual
    similarity among different people. Right: Large cross-view appearance variations
    of the same people, each person within a dotted box.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Person_ReIdentification_by_Camera_Correlation_Aware_Feature_Augmentation\figure_2.jpg
  Figure 2 caption: "Illustration of extracting the proposed HIPHOP feature. (a) A\
    \ resized input person image with 227\xD7227 pixel size. (b) Forward propagate\
    \ the resized image through the whole AlexNet architecture. (c) Obtain the feature\
    \ maps of the 1st and 2nd convolutional layers. (d) Compute the HIP descriptor\
    \ by pooling activation intensity into histograms across different horizontal\
    \ strips and feature maps. (e) Extract the HOP descriptor by ranking the localized\
    \ activations and then pooling the top- \u03BA feature map indices over all horizontal\
    \ strips and feature maps. (f) Construct the final HIPHOP feature by fusion."
  Figure 3 Link: articels_figures_by_rev_year\2017\Person_ReIdentification_by_Camera_Correlation_Aware_Feature_Augmentation\figure_3.jpg
  Figure 3 caption: An illustration of zero padding based feature augmentation. (a)
    The data distribution in the original feature space from camera view a (the blue
    curve) and b (the green curve). (b) The augmented feature space by zero padding.
    The dashed blue and green curves represent the projected features with respect
    to the projection basis indicated by the solid red line. The two dashed lines
    imply feature projection operation. Note that the probability density axis is
    not plotted in (b) for demonstration simplicity.
  Figure 4 Link: articels_figures_by_rev_year\2017\Person_ReIdentification_by_Camera_Correlation_Aware_Feature_Augmentation\figure_4.jpg
  Figure 4 caption: Pipeline of the proposed person re-id approach. Training state
    is indicated with orange arrows, and testing stage with blue arrows.
  Figure 5 Link: articels_figures_by_rev_year\2017\Person_ReIdentification_by_Camera_Correlation_Aware_Feature_Augmentation\figure_5.jpg
  Figure 5 caption: Example images from different person re-id datasets. For every
    dataset, two images in a column correspond to the same person.
  Figure 6 Link: articels_figures_by_rev_year\2017\Person_ReIdentification_by_Camera_Correlation_Aware_Feature_Augmentation\figure_6.jpg
  Figure 6 caption: Illustration of our HIP and HOP person descriptors. (a) Gabor
    filters used in ELF18 [19]. (b) Convolutional filters from the 1st AlexNet layer.
    (c) Horizontal stripes of six images from three different people (P-1, P-2 and
    P-3). (d) HOP histograms extracted from the corresponding image strips (i.e.,
    indicated with a rectangular of the same color as histogram bars) in (c). Partial
    HOP descriptors are shown for clear visualization.
  Figure 7 Link: articels_figures_by_rev_year\2017\Person_ReIdentification_by_Camera_Correlation_Aware_Feature_Augmentation\figure_7.jpg
  Figure 7 caption: "Evaluating the effect of \u03BA in the HOP descriptor."
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.55
  Name of the first author: Ying-Cong Chen
  Name of the last author: Jian-Huang Lai
  Number of Figures: 7
  Number of Tables: 11
  Number of authors: 4
  Paper title: Person Re-Identification by Camera Correlation Aware Feature Augmentation
  Publication Date: 2017-02-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparing Top Matching Ranks (%) on VIPeR and CUHK01
  Table 10 caption:
    table_text: TABLE 10 Comparing State-of-the-Art Methods on Market-1501 [33]
  Table 2 caption:
    table_text: TABLE 2 Evaluating the Effect of Our CVD Regularization
  Table 3 caption:
    table_text: TABLE 3 Comparison Between CRAFT and Domain Adaptation
  Table 4 caption:
    table_text: TABLE 4 Evaluating the Generality of CRAFT Instantialization
  Table 5 caption:
    table_text: TABLE 5 Evaluating the Effectiveness of Our HIPHOP Feature
  Table 6 caption:
    table_text: TABLE 6 Comparing State-of-the-Art Methods on VIPeR [30]
  Table 7 caption:
    table_text: TABLE 7 Comparing State-of-the-Art Methods on CUHK01 [31]
  Table 8 caption:
    table_text: TABLE 8 Comparing State-of-the-Art Methods on CUHK03 [26]
  Table 9 caption:
    table_text: TABLE 9 Comparing State-of-the-Art Methods on QMUL GRID [32]
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2666805
- Affiliation of the first author: department of systems and control engineering,
    tokyo institute of technology, tokyo, japan
  Affiliation of the last author: department of cybernetics, czech technical university
    in prague, praha, czechia
  Figure 1 Link: articels_figures_by_rev_year\2017\_Place_Recognition_by_View_Synthesis\figure_1.jpg
  Figure 1 caption: Matching across major changes in scene appearance is easier for
    similar viewpoints. (a) Query image. (b) The original database image cannot be
    matched to the query due to a major change in scene appearance combined with large
    change in the viewpoint. (c) Matching a more similar synthesized view is possible.
    (d) Illustration of locations of (a-c) on the map. The dots and arrows indicate
    the camera positions and view directions.
  Figure 10 Link: articels_figures_by_rev_year\2017\_Place_Recognition_by_View_Synthesis\figure_10.jpg
  Figure 10 caption: Benefits of higher image resolution. The plot shows the fraction
    of correctly recognized queries (Recall, y -axis) versus the number of top N retrieved
    database images ( x -axis).
  Figure 2 Link: articels_figures_by_rev_year\2017\_Place_Recognition_by_View_Synthesis\figure_2.jpg
  Figure 2 caption: 'Matching across illumination and structural changes in the scene.
    First row: The same query image is matched to a street-view image depicting the
    same place from a different viewpoint (a) and to a synthesized virtual view depicting
    the query place from the same viewpoint (d). Second row: Matching sparsely sampled
    RootSIFT descriptors across a major change in illumination is difficult for the
    same (e) as well as for the different (b) viewpoints. Third row: Densely sampled
    descriptors can be matched across a large change in illumination (c) and the matching
    is much easier when the viewpoint is similar (f). In all cases the tentative matches
    are shown in red and geometrically verified matches are shown in green. Note how
    the proposed method (f), based on densely sampled descriptors coupled with virtual
    view synthesis, obtains significantly higher inlier ratio (0.76) on this challenging
    image pair with major illumination and structural changes in the scene.'
  Figure 3 Link: articels_figures_by_rev_year\2017\_Place_Recognition_by_View_Synthesis\figure_3.jpg
  Figure 3 caption: Input data for view synthesis. (a) The street-view panorama. (b)
    The associated piece-wise planar depth-map. Brightness indicates distance. (c)
    The individual scene planes are shown in different colors.
  Figure 4 Link: articels_figures_by_rev_year\2017\_Place_Recognition_by_View_Synthesis\figure_4.jpg
  Figure 4 caption: "Combining street-view imagery with synthetic views. The figure\
    \ shows camera positions for part of the 247-Tokyo dataset. The positions of the\
    \ original street-view images are shown in orange, the positions of synthesized\
    \ views (5 \xD7 5m grid) are shown in grey, and the positions of query images\
    \ are shown in blue. The \u201Clarge 247-Tokyo\u201D database of geo-tagged images\
    \ (left) includes 374,676 views generated from 31,223 street-view panoramas and\
    \ 2,499,816 synthesized views generated at 208,318 virtual camera locations. The\
    \ area bounded by green rectangle (middle) indicates the subset of the 247 large\
    \ database that includes 75,984 views generated from 6,332 street-view panoramas.\
    \ The inset (right) shows a close-up of one road intersection."
  Figure 5 Link: articels_figures_by_rev_year\2017\_Place_Recognition_by_View_Synthesis\figure_5.jpg
  Figure 5 caption: 'Example query images from the newly collected 247 Tokyo dataset.
    Each place in the query set is captured at different times of day: (a) daytime,
    (b) sunset, and (c) night. For comparison, the database street-view image at a
    close-by position is shown in (d). Note the major changes in appearance (illumination
    changes in the scene) between the database image (d) and the query images (a,b,c).'
  Figure 6 Link: articels_figures_by_rev_year\2017\_Place_Recognition_by_View_Synthesis\figure_6.jpg
  Figure 6 caption: Overview of our approach for place recognition with view synthesis.
    Our approach has an off-line and an on-line stage. The off-line process generates
    expanded geo-tagged image database that includes Dense VLAD descriptors computed
    from perspective images of the original street-view images as well as the synthesized
    views. The position of the query image is estimated in the on-line stage by matching
    its Dense VLAD descriptor to descriptors in the expanded image database and retrieving
    the GPS positions of the top matches.
  Figure 7 Link: articels_figures_by_rev_year\2017\_Place_Recognition_by_View_Synthesis\figure_7.jpg
  Figure 7 caption: Evaluation on the 247-Tokyo dataset. The fraction of correctly
    recognized queries (Recall, y -axis) versus the number of top N retrieved database
    images ( x -axis) for the proposed method (Dense VLAD SYNTH) compared to the baseline
    methods (Dense VLAD, Dense FV, Sparse FV). The performance is evaluated for all
    test query images (a), as well as separately for daytime queries (b), and sunsetnight
    queries (c). The benefits of the proposed method (Dense VLAD SYNTH) is most prominent
    for difficult illuminations (c).
  Figure 8 Link: articels_figures_by_rev_year\2017\_Place_Recognition_by_View_Synthesis\figure_8.jpg
  Figure 8 caption: Comparison with the baseline methods on the 247-Tokyo dataset.
    Each plot shows the fraction of correctly recognized queries (Recall, y -axis)
    versus the number of top N retrieved database images ( x -axis).
  Figure 9 Link: articels_figures_by_rev_year\2017\_Place_Recognition_by_View_Synthesis\figure_9.jpg
  Figure 9 caption: Benefits of synthetic views for the the NetVLAD descriptor [42]
    on the 247-Tokyo dataset. Using synthetic views (NetVLAD SYNTH) benefits also
    the state-of-the-art CNN-based descriptor (NetVLAD). For both NetVLAD and NetVLAD
    SYNTH the descriptor has been trained on images with missing pixels to learn to
    ignore artifacts in view synthesis. For comparison, place recognition performance
    for the proposed method using densely extracted RootSIFT descriptors (Dense VLAD
    SYNTH) is shown in black.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Akihiko Torii
  Name of the last author: Tomas Pajdla
  Number of Figures: 16
  Number of Tables: 2
  Number of authors: 5
  Paper title: 24/7 Place Recognition by View Synthesis
  Publication Date: 2017-02-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Number of (Perspective) Images in the 247 Tokyo Geotagged
      Database
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison with the Baseline Methods on the 247-Tokyo Dataset
      Using 4,096 Dimensional Image Descriptors
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2667665
- Affiliation of the first author: center for research on intelligent perception and
    computing, chinese academy of sciences, beijing, china
  Affiliation of the last author: center for research on intelligent perception and
    computing, chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2017\Demographic_Analysis_from_Biometric_Data_Achievements_Challenges_and_New_Frontie\figure_1.jpg
  Figure 1 caption: 'Demographic attributes from biometric data (image source: Google
    Images).'
  Figure 10 Link: articels_figures_by_rev_year\2017\Demographic_Analysis_from_Biometric_Data_Achievements_Challenges_and_New_Frontie\figure_10.jpg
  Figure 10 caption: Sample images from the MORPH II face database [6].
  Figure 2 Link: articels_figures_by_rev_year\2017\Demographic_Analysis_from_Biometric_Data_Achievements_Challenges_and_New_Frontie\figure_2.jpg
  Figure 2 caption: 'Biological and behavioral body traits used for person recognition
    (image source: Google Images).'
  Figure 3 Link: articels_figures_by_rev_year\2017\Demographic_Analysis_from_Biometric_Data_Achievements_Challenges_and_New_Frontie\figure_3.jpg
  Figure 3 caption: Right and left iris images of one European subject and one African
    subject from the UBIRIS.v2 database [45].
  Figure 4 Link: articels_figures_by_rev_year\2017\Demographic_Analysis_from_Biometric_Data_Achievements_Challenges_and_New_Frontie\figure_4.jpg
  Figure 4 caption: Major milestones in the history of automatic age estimation from
    biometric data.
  Figure 5 Link: articels_figures_by_rev_year\2017\Demographic_Analysis_from_Biometric_Data_Achievements_Challenges_and_New_Frontie\figure_5.jpg
  Figure 5 caption: Major milestones in the history of automatic gender estimation
    from biometric data.
  Figure 6 Link: articels_figures_by_rev_year\2017\Demographic_Analysis_from_Biometric_Data_Achievements_Challenges_and_New_Frontie\figure_6.jpg
  Figure 6 caption: Major milestones in the history of automatic race recognition
    from biometric data.
  Figure 7 Link: articels_figures_by_rev_year\2017\Demographic_Analysis_from_Biometric_Data_Achievements_Challenges_and_New_Frontie\figure_7.jpg
  Figure 7 caption: 'Four main stages involved in typical biometric demographic estimation
    systems (image source: Google Images).'
  Figure 8 Link: articels_figures_by_rev_year\2017\Demographic_Analysis_from_Biometric_Data_Achievements_Challenges_and_New_Frontie\figure_8.jpg
  Figure 8 caption: 'Richness of biometric data in our society (image source: Google
    Images).'
  Figure 9 Link: articels_figures_by_rev_year\2017\Demographic_Analysis_from_Biometric_Data_Achievements_Challenges_and_New_Frontie\figure_9.jpg
  Figure 9 caption: Fingerprints of different genders (originally shown in [81]).
  First author gender probability: 0.51
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yunlian Sun
  Name of the last author: Tieniu Tan
  Number of Figures: 15
  Number of Tables: 8
  Number of authors: 4
  Paper title: 'Demographic Analysis from Biometric Data: Achievements, Challenges,
    and New Frontiers'
  Publication Date: 2017-02-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Advanced Biometric Representations and Estimators for Demographic
      Analysis
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Machine and Human Performance on FG-NET and MORPH II Face
      Databases from [12]
  Table 3 caption:
    table_text: TABLE 3 ChaLearn LAP 2015 Final Ranking with the Best Three Results
  Table 4 caption:
    table_text: TABLE 4 Classification Accuracies (%) of Several Participant Algorithms
      in Paralinguistic Challenge 2010
  Table 5 caption:
    table_text: TABLE 5 Gender Classification Accuracies (%) on the CASIA B Gait Database
  Table 6 caption:
    table_text: TABLE 6 MAEs on the USF Gait Database
  Table 7 caption:
    table_text: TABLE 7 Gender Recognition Results (%) on the MIT Pedestrian Dataset
  Table 8 caption:
    table_text: TABLE 8 Race and Gender Classification Accuracies (%) on the UND and
      GFI Iris Datasets
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2669035
