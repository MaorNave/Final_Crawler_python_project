- Affiliation of the first author: smile clinic, department of informatics, university
    of sussex, united kingdom
  Affiliation of the last author: department of engineering, university of cambridge,
    cambridge, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2014\A_Very_Simple_SafeBayesian_Random_Forest\figure_1.jpg
  Figure 1 caption: Coding of a rooted and strictly binary tree. Internal nodes are
    denoted with circles and terminal nodes with squares. Each internal node has exactly
    two outgoing edges. The code for an internal node is 1 and 0 is for a terminal
    node. Codes are generated in depth-first order starting from 1 at the root node.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\A_Very_Simple_SafeBayesian_Random_Forest\figure_2.jpg
  Figure 2 caption: Sampling trees structures from the prior at lambda in lbrace 0.3,
    0.4, 0.5rbrace . Once we generate the structures, and fill up the internal nodes
    with splitting rules, we will completely specify decision trees.
  Figure 3 Link: articels_figures_by_rev_year\2014\A_Very_Simple_SafeBayesian_Random_Forest\figure_3.jpg
  Figure 3 caption: A visualisation of four trees with the highest (top row) and four
    trees with the lowest (bottom row) importance weights for a three-class problem
    'iris' dataset (best viewed in colour). The terminal nodes are colour encoded
    according to the label proportions of data points falling into the terminal nodes.
    The three colours, red, green, and blue, correspond to the three class labels.
    More homogeneous labels in the terminal node correspond to purer colour encoding.
    The terminal nodes with mixed class labels produce colour in RGB space. Some of
    the terminal nodes are coloured black as no data point falls into those particular
    nodes. The black internal nodes will not contribute to the importance weight of
    the tree as the associated terms in Equation (3) are 1 . Essentially, this allows
    us to have an in-built pruning of the trees structures.
  Figure 4 Link: articels_figures_by_rev_year\2014\A_Very_Simple_SafeBayesian_Random_Forest\figure_4.jpg
  Figure 4 caption: Operating curve of our Bayesian random forest based on the concept
    of power likelihood. Accuracy pm STE. The curves are generated based on training
    data. The point where mN=1 corresponds to Bayesian model averaging.
  Figure 5 Link: articels_figures_by_rev_year\2014\A_Very_Simple_SafeBayesian_Random_Forest\figure_5.jpg
  Figure 5 caption: Sensitivity analysis of the performance of S-Bayes RF on test
    set with respect to the choices of splitting probability lambda and number of
    trees texttrees . We observe that in general S-Bayes RF is robust to the ( lambda
    , texttrees )-parameter selection with the tendency of better accuracy performance
    with more trees and lambda close to 0.5 .
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Novi Quadrianto
  Name of the last author: Zoubin Ghahramani
  Number of Figures: 5
  Number of Tables: 4
  Number of authors: 2
  Paper title: A Very Simple Safe-Bayesian Random Forest
  Publication Date: 2014-10-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Accuracy across Five Random Repeats
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Performance Comparison of Bayesian Random Forest: Tree with\
      \ the Highest Importance Weight (Best Tree ), Weighted Ensemble of Trees with\
      \ \u03B2=1 (Bayesian Model Averaging), Weighted Ensemble of Trees with \u03B2\
      =5N ( S-Bayes RF)"
  Table 3 caption:
    table_text: "TABLE 3 Log Predictive Probabilities of Markov Chain Monte Carlo\
      \ of Bayesian Decision Tree and Weighted Ensemble of Trees with \u03B2=5N (S-Bayes\
      \ RF )"
  Table 4 caption:
    table_text: TABLE 4 Accuracy Results of Trees and Forest Variants on High Dimensional
      Images Dataset across Five Random Repeats
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2362751
- Affiliation of the first author: department of computer science and engineering,
    michigan state university, east lansing, mi
  Affiliation of the last author: department of computer science and engineering,
    michigan state university, east lansing, mi
  Figure 1 Link: articels_figures_by_rev_year\2014\Demographic_Estimation_from_Face_Images_Human_vs_Machine_Performance\figure_1.jpg
  Figure 1 caption: A wide variety of information can be gleaned from a face image,
    such as identity, age, gender, race, and scars, marks and tattoos (SMT).
  Figure 10 Link: articels_figures_by_rev_year\2014\Demographic_Estimation_from_Face_Images_Human_vs_Machine_Performance\figure_10.jpg
  Figure 10 caption: Age estimation performance with and without quality assessment
    by the proposed approach and human.
  Figure 2 Link: articels_figures_by_rev_year\2014\Demographic_Estimation_from_Face_Images_Human_vs_Machine_Performance\figure_2.jpg
  Figure 2 caption: "Overview of the proposed approach for automatic demographic estimation.\
    \ I and \u0393 are the input and preprocessed face images, respectively; \u03C8\
    \ and \u03A8 denote the BIF extraction process, and D\u2192S denotes the demographic\
    \ informative feature selection; x and x \u2032 denote the BIF feature vector\
    \ before and after feature selection, respectively; and QA(\u22C5) is the quality\
    \ assessment process."
  Figure 3 Link: articels_figures_by_rev_year\2014\Demographic_Estimation_from_Face_Images_Human_vs_Machine_Performance\figure_3.jpg
  Figure 3 caption: Different facial appearances of identical twins possibly due to
    extrinsic factors such as (a) environmental conditions (e.g., sunshine),2 and
    (b) lifestyle.3
  Figure 4 Link: articels_figures_by_rev_year\2014\Demographic_Estimation_from_Face_Images_Human_vs_Machine_Performance\figure_4.jpg
  Figure 4 caption: 'Examples of face preprocessing: (a) input images with pose variations
    from FG-NET and MORPH II; (b) intermediate gray-scale images after geometric normalization;
    and (c) output of preprocessing.'
  Figure 5 Link: articels_figures_by_rev_year\2014\Demographic_Estimation_from_Face_Images_Human_vs_Machine_Performance\figure_5.jpg
  Figure 5 caption: Major steps in calculating biologically inspired features.
  Figure 6 Link: articels_figures_by_rev_year\2014\Demographic_Estimation_from_Face_Images_Human_vs_Machine_Performance\figure_6.jpg
  Figure 6 caption: The top five (blue) and top 6 - 50 (green) most informative BIF
    features selected for estimating (a) age, (b) gender, and (c) race. The rectangle
    size indicates the scale of the corresponding BIF feature.
  Figure 7 Link: articels_figures_by_rev_year\2014\Demographic_Estimation_from_Face_Images_Human_vs_Machine_Performance\figure_7.jpg
  Figure 7 caption: "Learning a hierarchical age estimator: (a) binary decision tree\
    \ for classifying non-overlapping groups (e.g., male vs. female; white vs. black;\
    \ age < r 1 vs. age \u2265 r 1 ), and (b) within-group age regressors learned\
    \ from overlapping age groups."
  Figure 8 Link: articels_figures_by_rev_year\2014\Demographic_Estimation_from_Face_Images_Human_vs_Machine_Performance\figure_8.jpg
  Figure 8 caption: 'Demographic estimation obtained using the MTurk crowdsourcing
    service: (a) overview of the estimation process, and illustrations of three HITs
    for (b) age estimation, (c) gender classification, and (d) race classification.
    Images shown to the MTurk workers are exactly the same as those were input to
    our algorithm.'
  Figure 9 Link: articels_figures_by_rev_year\2014\Demographic_Estimation_from_Face_Images_Human_vs_Machine_Performance\figure_9.jpg
  Figure 9 caption: The difference in human age estimation performance between using
    3 , 6 , and 9 randomly selected (RS) MTurk workers and all 10 workers.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Hu Han
  Name of the last author: Anil K. Jain
  Number of Figures: 17
  Number of Tables: 7
  Number of authors: 4
  Paper title: 'Demographic Estimation from Face Images: Human vs. Machine Performance'
  Publication Date: 2014-10-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 A Summary of Machine and Human Performance on Age Estimation
      Reported in Literature
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 A Summary of Machine and Human Performance on Gender Classification
      Reported in Literature
  Table 3 caption:
    table_text: TABLE 3 A Summary of Machine and Human Performance on Race Classification
      Reported in Literature
  Table 4 caption:
    table_text: TABLE 4 Mean Absolute Error of Age Estimation on FG-NET, MORPH II,
      and PCSO Databases (in Years)
  Table 5 caption:
    table_text: TABLE 5 Confusion Matrix for Gender Classification (in Percent)
  Table 6 caption:
    table_text: TABLE 6 Confusion Matrix for Race Classification (in Percent)
  Table 7 caption:
    table_text: TABLE 7 Average Time (sec.) per Algorithmic Step in Our Prototype
      Demographic Estimation System
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2362759
- Affiliation of the first author: advanced digital sciences center, singapore
  Affiliation of the last author: department of electrical and computer engineering,
    national university of singapore, singapore
  Figure 1 Link: articels_figures_by_rev_year\2014\Order_Preserving_Sparse_Coding\figure_1.jpg
  Figure 1 caption: Application of order preserving sparse coding to human motion
    sequence classification (left panel) and still image classification (right panel).
    Here still images include landscape and street view images. For human motion sequence
    classification, we extract 3 D skeleton features for each frame, denoted by bf
    xt for dictionary sample and bf yt for testing sample. The frame-wise features
    within each sequence are ordered temporally. For still image classification, we
    first segment each image into regions. Then for each region, we extract a frame-based
    feature vector, e.g., color histogram or bag of SIFT features, denoted by bf xm
    for dictionary sample and bf ym for testing sample. These region-based features
    are then ordered according to the vertical position of the region centers in a
    sequence. Coding coefficients for different framesegment-wise features are color-coded
    on the right. The mean squared reconstruction residual is used as part of the
    classification criterion. Note that performing sparse coding for individual features
    independently makes input sequence bf Y have strong coding coefficients on dictionary
    sequence bf Xj , which leads to misclassification. In contrast, our order preserving
    coding will have weak response from because bf Xj has different ordering structure
    than bf Y (even though the atomic feature vectors from both bf Y and bf Xj are
    similar).
  Figure 10 Link: articels_figures_by_rev_year\2014\Order_Preserving_Sparse_Coding\figure_10.jpg
  Figure 10 caption: Recognition accuracy comparison for different classes using SC
    and MTO-SC methods on Scene-15 dataset.
  Figure 2 Link: articels_figures_by_rev_year\2014\Order_Preserving_Sparse_Coding\figure_2.jpg
  Figure 2 caption: Application of the regularizer (5) to human action sequence classification.
    The reconstruction coefficients that follow the temporal order of the input sequence
    are not penalized by the order-preserving regularizer (upper row). Those do not
    follow the ordering structure of the input sequence are penalized (middle row).
    In this case, the proposed regularizer will weaken the coding response on the
    dictionary sequence mathbf Xj which has a different ordering structure than the
    input sequence mathbf Y (bottom row).
  Figure 3 Link: articels_figures_by_rev_year\2014\Order_Preserving_Sparse_Coding\figure_3.jpg
  Figure 3 caption: Application of different types of regularizers. The input time
    sequence has three frames, i.e., t=3 . The reconstruction coefficients corresponding
    to the feature vectors from each input frame are color-coded.
  Figure 4 Link: articels_figures_by_rev_year\2014\Order_Preserving_Sparse_Coding\figure_4.jpg
  Figure 4 caption: 'Reconstruction coefficients of the input time series on the synthetic
    dataset using three algorithms: (a) sparse coding; (b) group sparse coding, and
    (c) Order-preserving sparse coding.'
  Figure 5 Link: articels_figures_by_rev_year\2014\Order_Preserving_Sparse_Coding\figure_5.jpg
  Figure 5 caption: 'Convergence curves of the MTO-SC algorithm on the AusLan1 dataset.
    From left to right: sample ID 10, 20, 30, and 40, respectively.'
  Figure 6 Link: articels_figures_by_rev_year\2014\Order_Preserving_Sparse_Coding\figure_6.jpg
  Figure 6 caption: Percentage of correctly ordered coding coefficient pairs from
    different coding algorithms on AusLan1 and ArabSpokenDigit datasets.
  Figure 7 Link: articels_figures_by_rev_year\2014\Order_Preserving_Sparse_Coding\figure_7.jpg
  Figure 7 caption: Classification accuracies for different algorithms on the AusLan1
    dataset under different noise conditions.
  Figure 8 Link: articels_figures_by_rev_year\2014\Order_Preserving_Sparse_Coding\figure_8.jpg
  Figure 8 caption: Classification accuracies of MTO-SC algorithm with different lambda
    1 and lambda 2 values on AusLan1 dataset.
  Figure 9 Link: articels_figures_by_rev_year\2014\Order_Preserving_Sparse_Coding\figure_9.jpg
  Figure 9 caption: 'Class confusion matrix for the proposed MTO-SC method on RGB-D
    human activity dataset. Acronyms for Action Categories: RM: rinsing mouth; BT:
    brushing teeth; CL: wearing contact lens; CS: cooking (stirring); WW: writing
    on white board; WC: working on computer; TP: talking on phone; RC: relaxing on
    a chair; OP: opening a pill container; DW: drinking water; CC: cooking (chopping);
    TC: talking on a chair; NA: neutral activity.'
  First author gender probability: 0.94
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Bingbing Ni
  Name of the last author: Shuicheng Yan
  Number of Figures: 11
  Number of Tables: 3
  Number of authors: 3
  Paper title: Order Preserving Sparse Coding
  Publication Date: 2014-10-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Classification Accuracy [Mean(Std. Dev.)] for Different Algorithms
      on Different Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Classification Accuracy (Percent) on RGB-D Human Activity
      Dataset
  Table 3 caption:
    table_text: TABLE 3 Classification Accuracy (Percent) Comparison on Scene-15 Dataset
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2362935
- Affiliation of the first author: institute of science and technology (ist), klosterneuburg,
    austria
  Affiliation of the last author: institute of science and technology (ist), klosterneuburg,
    austria
  Figure 1 Link: articels_figures_by_rev_year\2014\A_New_Look_at_Reweighted_Message_Passing\figure_1.jpg
  Figure 1 caption: Anisotropic MSD and MPLP updates for factors beta and alpha respectively.
    omega is a probability distribution over Iprime beta cup lbrace beta rbrace ,
    and rho is a probability distribution over Oprime alpha cup lbrace alpha rbrace
    . All updates should be done for all possible mboxboldmath xalpha , mboxboldmath
    xbeta with mboxboldmath xalpha sim mboxboldmath xbeta .
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\A_New_Look_at_Reweighted_Message_Passing\figure_2.jpg
  Figure 2 caption: 'Average lower bound and energy vs. time. X axis: SRMP iterations
    in the log scale. Notation 1 a b means that 1 iteration of SRMP takes the same
    time as a iterations of CMP, or b iterations of MPLP. Note that an SRMP iteration
    has two passes (forward and backward), while CMP and MPLP iterations have only
    forward passes. However in each pass SRMP updates only a subset of messages (half
    in the case of pairwise models). Y axis: lower boundenergy, normalized so that
    the initial lower bound (with zero messages) is -1 , and the best computed lower
    bound is 0 . Line Arightarrow B gives information about set J : ;; A=lbrace |alpha
    |:::(alpha ,beta )in Jrbrace , B=lbrace |beta |:::(alpha ,beta )in Jrbrace .'
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Vladimir Kolmogorov
  Name of the last author: Vladimir Kolmogorov
  Number of Figures: 2
  Number of Tables: 0
  Number of authors: 1
  Paper title: A New Look at Reweighted Message Passing
  Publication Date: 2014-10-16 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2363465
- Affiliation of the first author: department of mathematics and computer science,
    and the heidelberg collaboratory for image processing (hci), university of heidelberg,
    speyerer str. 6,, heidelberg, germany
  Affiliation of the last author: department of mathematics and computer science,
    and the heidelberg collaboratory for image processing (hci), university of heidelberg,
    speyerer str. 6,, heidelberg, germany
  Figure 1 Link: articels_figures_by_rev_year\2014\Beyond_the_Sum_of_Parts_Voting_with_Groups_of_Dependent_Entities\figure_1.jpg
  Figure 1 caption: Outline of the processing pipeline.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\Beyond_the_Sum_of_Parts_Voting_with_Groups_of_Dependent_Entities\figure_2.jpg
  Figure 2 caption: Figure a) shows the feature descriptor matching as in (3). Figure
    b) shows the geometric distortion term resulting out of transforming a group of
    parts from query image to training parts as in (5). Figure c) shows the scatter
    of the votes as indicated by the orange circle depicting (6).
  Figure 3 Link: articels_figures_by_rev_year\2014\Beyond_the_Sum_of_Parts_Voting_with_Groups_of_Dependent_Entities\figure_3.jpg
  Figure 3 caption: a) shows the probability density map (see Section 4.4 for details)
    generated for possible locations of relevant training parts of mugs in ETHZ images.
    Red indicates high probability and blue indicates low probability. Such maps are
    used to obtain weights for points sampled on various objects in panel b). High
    weight is associated to characteristic parts on the outline of the objects (neck
    and rear of the swan, sides of the bottle, tip and sides of the apple, handle
    of the mug, neck and upper parts of legs of the giraffe) whereas low weight is
    associated to the interiors and exteriors of the object.
  Figure 4 Link: articels_figures_by_rev_year\2014\Beyond_the_Sum_of_Parts_Voting_with_Groups_of_Dependent_Entities\figure_4.jpg
  Figure 4 caption: a) The orange curve shows the performance on applelogos without
    occlusion and the blue curve shows the performance on synthetic occlusions. See
    Section 5.4 for details. b) Detection rate at 0.3 fppi versus the hypotheses ratio
    for the category of giraffes. See Section 5.5 for details.
  Figure 5 Link: articels_figures_by_rev_year\2014\Beyond_the_Sum_of_Parts_Voting_with_Groups_of_Dependent_Entities\figure_5.jpg
  Figure 5 caption: All feature points that were assigned to the group that supports
    the object hypothesis are marked in green. All other points are in beige. Thus
    we obtain a foregroundbackground segregation and object shape is extracted.
  Figure 6 Link: articels_figures_by_rev_year\2014\Beyond_the_Sum_of_Parts_Voting_with_Groups_of_Dependent_Entities\figure_6.jpg
  Figure 6 caption: This figure shows the top five false hypotheses when searching
    for bottles in the test images of ETHZ shape data set. Results are shown for a)
    standard Hough voting b) our Hough voting with groups c) our full system. d) shows
    the edge image for the corresponding numbered figure from c). The numbers below
    each figure indicate the occurrence of the false hypothesis amongst all the hypotheses
    sorted according to their score and we notice a significant shift in this ranking
    from a) to b) to c).
  Figure 7 Link: articels_figures_by_rev_year\2014\Beyond_the_Sum_of_Parts_Voting_with_Groups_of_Dependent_Entities\figure_7.jpg
  Figure 7 caption: This figure shows the top five false hypotheses when searching
    for swans in the test images of ETHZ shape data set. Results are shown for a)
    standard Hough voting b) our Hough voting with groups c) our full system. d) shows
    the edge image for the corresponding numbered figure from c). The numbers below
    each figure indicate the occurrence of the false hypothesis amongst all the hypotheses
    sorted according to their score and we notice a significant shift in this ranking
    from a) to b) to c).
  Figure 8 Link: articels_figures_by_rev_year\2014\Beyond_the_Sum_of_Parts_Voting_with_Groups_of_Dependent_Entities\figure_8.jpg
  Figure 8 caption: a) the input image b) top five hypotheses obtained from standard
    hough voting c) top five hypotheses obtained from our voting d) top five hypotheses
    obtained from our full system. The scores of the top five hypotheses are plotted
    along with the scores for all the hypotheses in all the test images. Hypotheses
    with numbers marked in green indicate true hypotheses and the hypotheses with
    numbers marked in red indicate false hypotheses.
  Figure 9 Link: articels_figures_by_rev_year\2014\Beyond_the_Sum_of_Parts_Voting_with_Groups_of_Dependent_Entities\figure_9.jpg
  Figure 9 caption: a) The input image, b) top five hypotheses obtained from standard
    hough voting, c) top five hypotheses obtained from our voting, d) top five hypotheses
    obtained from our full system. The scores of the top five hypotheses are plotted
    along with the scores for all the hypotheses in all the test images. Hypotheses
    with numbers marked in green indicate true hypotheses and the hypotheses with
    numbers marked in red indicate false hypotheses.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Pradeep Yarlagadda
  Name of the last author: Pradeep Yarlagadda
  Number of Figures: 9
  Number of Tables: 4
  Number of authors: 1
  Paper title: 'Beyond the Sum of Parts: Voting with Groups of Dependent Entities'
  Publication Date: 2014-10-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparing Various Methods for Voting with Boundary Points
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance of H grp voting + verif on ETHZ Shape Classes
      over 5 Splits of Random Training and Test Images Is Reported in Terms of Mean
      Detection Rate and the Corresponding Standard Deviation at 0.3 and 0.4 fppi
  Table 3 caption:
    table_text: TABLE 3 Comparison of Various Hough Voting Approaches Which Vote with
      Regions for Detecting Objects in ETHZ Shape Classes
  Table 4 caption:
    table_text: TABLE 4 Comparison of Equal Error Rates (EER) for all the Categories
      of Shape-15 Data Set
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2363456
- Affiliation of the first author: department of electrical engineering, kaist, 291
    daehak-ro, yuseong-gu, daejeon, south korea
  Affiliation of the last author: department of electrical engineering, kaist, 291
    daehak-ro, yuseong-gu, daejeon, south korea
  Figure 1 Link: articels_figures_by_rev_year\2014\TimeofFlight_Sensor_Calibration_for_a_Color_and_Depth_Camera_Pair\figure_1.jpg
  Figure 1 caption: System setup and an overview of the proposed calibration method.
  Figure 10 Link: articels_figures_by_rev_year\2014\TimeofFlight_Sensor_Calibration_for_a_Color_and_Depth_Camera_Pair\figure_10.jpg
  Figure 10 caption: 3D rendering results of different scenes. (b) Manufacturer provided
    3D measurements, (c) ray corrected 3D measurements, and (d) ray and bias corrected
    3D measurements are aligned with the corresponding color values using the optimized
    pose of the ToF camera with respect to the color camera. The magnified views in
    the second row show that the range bias correction straightens the corner of the
    rendered pattern plane. The scanned lines in the bottom row show the distortion
    of the depth measurement of the wall on the corners and their correction.
  Figure 2 Link: articels_figures_by_rev_year\2014\TimeofFlight_Sensor_Calibration_for_a_Color_and_Depth_Camera_Pair\figure_2.jpg
  Figure 2 caption: "(a) The 2.5D pattern board in a 640\xD7480 color image and (b)\
    \ in a 176\xD7144 depth image. (c) The same depth image rescaled for better feature\
    \ detection and (d) radial distortion removed for better homography estimation.\
    \ (e) A black-and-white checkerboard in a color image and (f) in an amplitude\
    \ image of a ToF camera."
  Figure 3 Link: articels_figures_by_rev_year\2014\TimeofFlight_Sensor_Calibration_for_a_Color_and_Depth_Camera_Pair\figure_3.jpg
  Figure 3 caption: The distribution of R b according to range measurement (left)
    and distance of the pixel location from the image center (right).
  Figure 4 Link: articels_figures_by_rev_year\2014\TimeofFlight_Sensor_Calibration_for_a_Color_and_Depth_Camera_Pair\figure_4.jpg
  Figure 4 caption: (Top) Spatial distribution of the measurements. (Middle row) Range
    errors and the estimated B-spline functions. (Bottom) Range error after correction.
    (a) Range bias correction of all the measurement using a single B-spline function.
    (b-d) Three clusters (Cluster 1, 2, 3 in Table 2) of the measurements showing
    similar error profiles and their corrections.
  Figure 5 Link: articels_figures_by_rev_year\2014\TimeofFlight_Sensor_Calibration_for_a_Color_and_Depth_Camera_Pair\figure_5.jpg
  Figure 5 caption: Range error profiles obtained from six planar scenes. The lines
    show range bias estimation using a planar scene captured from six (black), 11
    (blue), 16 (green), and 21 different distances (magenta).
  Figure 6 Link: articels_figures_by_rev_year\2014\TimeofFlight_Sensor_Calibration_for_a_Color_and_Depth_Camera_Pair\figure_6.jpg
  Figure 6 caption: A color image of the cuboid and a manually constructed mask for
    its three faces.
  Figure 7 Link: articels_figures_by_rev_year\2014\TimeofFlight_Sensor_Calibration_for_a_Color_and_Depth_Camera_Pair\figure_7.jpg
  Figure 7 caption: Flat wall evaluation. The point clouds are the reconstructed walls
    captured from 3.5, 2.8, and 2.0 m. The deviation from planarity is reduced using
    ray and bias correction. The curvature on the boundaries are flattened.
  Figure 8 Link: articels_figures_by_rev_year\2014\TimeofFlight_Sensor_Calibration_for_a_Color_and_Depth_Camera_Pair\figure_8.jpg
  Figure 8 caption: Comparison with (b) structured light reconstruction method [39].
    The reconstructed meshes using (c) manufacturer provided transformation, (d) ray
    correction, and (e) ray and bias correction are aligned using iterative closest
    point (ICP) [40]. Green regions indicate the distance between the point cloud
    and the mesh is less than 10 mm.
  Figure 9 Link: articels_figures_by_rev_year\2014\TimeofFlight_Sensor_Calibration_for_a_Color_and_Depth_Camera_Pair\figure_9.jpg
  Figure 9 caption: Average depth error along the calibrated distance using different
    depth correction methods.
  First author gender probability: 0.98
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.88
  Name of the first author: Jiyoung Jung
  Name of the last author: In So Kweon
  Number of Figures: 12
  Number of Tables: 7
  Number of authors: 4
  Paper title: Time-of-Flight Sensor Calibration for a Color and Depth Camera Pair
  Publication Date: 2014-10-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average Projection Error of Homography Estimation [Pixel]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 RMS Error Before and After Range Bias Correction [mm]
  Table 3 caption:
    table_text: TABLE 3 Average Projection Errors on Color and Depth Images [Pixel]
      and Average Range Measurement Errors of the Pattern Plane [mm]
  Table 4 caption:
    table_text: TABLE 4 Percentage of the Range Measurements Having Errors Smaller
      than 5 10 20 mm [%]
  Table 5 caption:
    table_text: TABLE 5 Performance Difference of Bias Correction Using Different
      Number of Planar Scene Range Data
  Table 6 caption:
    table_text: TABLE 6 Angular Error between Reconstructed Planes in Degrees
  Table 7 caption:
    table_text: TABLE 7 Calibration Result of Projection Error (in Pixel) and Depth
      Error (in Millimeter)
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2363827
- Affiliation of the first author: "department of cybernetics, czech technical university,\
    \ karlovo n\xE1m\u011Bst\xED, praha, czech republic"
  Affiliation of the last author: "department of cybernetics, czech technical university,\
    \ karlovo n\xE1m\u011Bst\xED, praha, czech republic"
  Figure 1 Link: Not Available
  Figure 1 caption: Not Available
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "Tom\xE1\u0161 Werner"
  Name of the last author: "Tom\xE1\u0161 Werner"
  Number of Figures: Not Available
  Number of Tables: 0
  Number of authors: 1
  Paper title: 'Marginal Consistency: Upper-Bounding Partition Functions over Commutative
    Semirings'
  Publication Date: 2014-10-17 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2363833
- Affiliation of the first author: centre for intelligent sensing, school of electronic
    engineering and computer science, queen mary university of london, london e1 4ns,
    united kingdom
  Affiliation of the last author: centre for intelligent sensing, school of electronic
    engineering and computer science, queen mary university of london, london e1 4ns,
    united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2014\Automatic_Analysis_of_Facial_Affect_A_Survey_of_Registration_Representation_and_\figure_1.jpg
  Figure 1 caption: The proposed conceptual framework to be used for the analysis
    and comparison of facial affect recognition systems. The input is a single image
    ( It ) for spatial representations or a set of frames ( mathbf Itw ) within a
    temporal window w for spatio-temporal representations. The system output Yt is
    discrete if it is obtained through classification or continuous if obtained through
    regression. The recognition process can incorporate previous ( lbrace Yt-1,ldots
    ,Yt-nrbrace ) andor subsequent ( lbrace Yt+1,ldots ,Yt+mrbrace ) system output(s).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\Automatic_Analysis_of_Facial_Affect_A_Survey_of_Registration_Representation_and_\figure_2.jpg
  Figure 2 caption: Spatial representations. (a) Facial points; (b) LBP histograms;
    (c) LPQ histograms; (d) HoG; (e) Gabor-based representation; (f) dense BoW; (g)
    GP-NMF; (h) sparse coding; (i) part-based SIFT; (j) part-based NMF.
  Figure 3 Link: articels_figures_by_rev_year\2014\Automatic_Analysis_of_Facial_Affect_A_Survey_of_Registration_Representation_and_\figure_3.jpg
  Figure 3 caption: Spatio-temporal representations. (a) Geometric features from tracked
    feature points; (b) LBP-TOP, and the TOP paradigm; (c) LPQ-TOP; (d) spatio-temporal
    ICA filtering, the output on an exemplar spatio-temporal filter; (e) dynamic Haar
    representation; (f) similarity features representation; (g) free-form deformation
    representation, illustration of free-form deformation; (h) temporal BoW.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.82
  Name of the first author: Evangelos Sariyanidi
  Name of the last author: Andrea Cavallaro
  Number of Figures: 3
  Number of Tables: 3
  Number of authors: 3
  Paper title: 'Automatic Analysis of Facial Affect: A Survey of Registration, Representation,
    and Recognition'
  Publication Date: 2014-10-30 00:00:00
  Table 1 caption:
    table_text: ''
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: ''
  Table 3 caption:
    table_text: ''
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2366127
- Affiliation of the first author: department of maia, universitat de barcelona, barcelona,
    spain
  Affiliation of the last author: computer vision center, barcelona, spain
  Figure 1 Link: articels_figures_by_rev_year\2014\MetaParameter_Free_Unsupervised_Sparse_Feature_Learning\figure_1.jpg
  Figure 1 caption: Pipeline of our method (see text for details).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\MetaParameter_Free_Unsupervised_Sparse_Feature_Learning\figure_2.jpg
  Figure 2 caption: Random subset of bases learned by EPLS using the logistic activation
    function, a receptive field of 10 pixels and Nh = 1600 (better seen in color).
  Figure 3 Link: articels_figures_by_rev_year\2014\MetaParameter_Free_Unsupervised_Sparse_Feature_Learning\figure_3.jpg
  Figure 3 caption: Accuracy changing lifetime sparsity in a single layer network
    of Nh = 100 .
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Adriana Romero
  Name of the last author: Carlo Gatta
  Number of Figures: 3
  Number of Tables: 3
  Number of authors: 3
  Paper title: Meta-Parameter Free Unsupervised Sparse Feature Learning
  Publication Date: 2014-10-31 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Classification Accuracy on STL-10
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Classification Accuracy on CIFAR-10
  Table 3 caption:
    table_text: TABLE 3 Lifetime and Population Sparsity
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2366129
- Affiliation of the first author: department of information and computer science,
    aalto university, espoo, finland
  Affiliation of the last author: department of information and computer science,
    aalto university, espoo, finland
  Figure 1 Link: articels_figures_by_rev_year\2014\Learning_the_Information_Divergence\figure_1.jpg
  Figure 1 caption: beta selection using (from top to bottom) Tweedie likelihood,
    ED likelihood, negative SM objective of ED, EDA likelihood, and negative SM objective
    of EDA. Data were generated using Tweedie distribution with beta =-2,-1,0,1 (from
    left to right).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\Learning_the_Information_Divergence\figure_2.jpg
  Figure 2 caption: Log-likelihood of (a) Tweedie, (b) ED, and (c) EDA distributions
    for alpha -selection. In the Tweedie plot, blanks correspond to beta =1alpha -1
    values for which a Tweedie distribution pdf does not exist or cannot be evaluated,
    i.e., beta in (0,1)cup (1,infty ) . In (d), negative SM objective function values
    are plotted for EDA.
  Figure 3 Link: articels_figures_by_rev_year\2014\Learning_the_Information_Divergence\figure_3.jpg
  Figure 3 caption: (a, b) Log likelihood values for beta and alpha for the spectrogram
    of a short piano excerpt with F=513 , N=676 , K=6 . (c) Negative SM objective
    for beta .
  Figure 4 Link: articels_figures_by_rev_year\2014\Learning_the_Information_Divergence\figure_4.jpg
  Figure 4 caption: 'Top: the stock data. Bottom left: the EDA log-likelihood for
    beta in [-2,2] . Bottom right: negative SM objective function for beta in [-2,2]
    .'
  Figure 5 Link: articels_figures_by_rev_year\2014\Learning_the_Information_Divergence\figure_5.jpg
  Figure 5 caption: 'Swimmer dataset: (top) example images; (bottom) the best PNMF
    basis ( mathbf W ) selected by using (bottom left) MEDAL and (bottom right) score
    matching of EDA. The visualization reshapes each column of mathbf W to an image
    and displays it by the Matlab function imagesc.'
  Figure 6 Link: articels_figures_by_rev_year\2014\Learning_the_Information_Divergence\figure_6.jpg
  Figure 6 caption: 'Selecting the best gamma -divergence: (first row) for multinomial
    data, (second row) in PNMF for synthetic data, (third row) in PNMF for the swimmer
    dataset, and (fourth row) in s-SNE for the dolphins dataset; (left column) using
    MEDAL and (right column) using score matching of EDA. The red star highlights
    the peak and the small subfigures in each plot shows the zoom-in around the peak.
    The sub-figures in the third row zoom in the area near the peaks.'
  Figure 7 Link: articels_figures_by_rev_year\2014\Learning_the_Information_Divergence\figure_7.jpg
  Figure 7 caption: Visualization of the dolphins social network with the best gamma
    using (top) MEDAL and (bottom) score matching of EDA. Dolphins and their social
    connections are shown by circles and lines, respectively. The background illustrates
    the node density by the Parzen method [45].
  Figure 8 Link: articels_figures_by_rev_year\2014\Learning_the_Information_Divergence\figure_8.jpg
  Figure 8 caption: "Selecting the best \u03B3 by minimum cross-validation error for\
    \ the dolphins social network: (left) logorithm of cross-validation errors for\
    \ various \u03B3 values, (right) visualization of the dolphins social network\
    \ with \u03B3=40 ."
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Onur Dikmen
  Name of the last author: Erkki Oja
  Number of Figures: 8
  Number of Tables: 0
  Number of authors: 3
  Paper title: Learning the Information Divergence
  Publication Date: 2014-10-31 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2366144
