- Affiliation of the first author: institute for datability science, osaka university,
    osaka, japan
  Affiliation of the last author: faculty of information science and electrical engineering,
    kyushu university, fukuoka, japan
  Figure 1 Link: articels_figures_by_rev_year\2021\Surface_Normals_and_Light_Directions_From_Shading_and_Polarization\figure_1.jpg
  Figure 1 caption: "Normal vector n(p) is defined in the world coordinate system\
    \ by a zenith angle \u03B8(p) and an azimuth angle \u03D5(p) ."
  Figure 10 Link: articels_figures_by_rev_year\2021\Surface_Normals_and_Light_Directions_From_Shading_and_Polarization\figure_10.jpg
  Figure 10 caption: Arrangement of the polarization filter array on top of the SONY
    image sensor.
  Figure 2 Link: articels_figures_by_rev_year\2021\Surface_Normals_and_Light_Directions_From_Shading_and_Polarization\figure_2.jpg
  Figure 2 caption: Diffuse reflection at a surface point associated with pixel p
    and normal vector n(p) . All 3D vectors, such as l , n(p) , and o(p) , are normalized.
  Figure 3 Link: articels_figures_by_rev_year\2021\Surface_Normals_and_Light_Directions_From_Shading_and_Polarization\figure_3.jpg
  Figure 3 caption: Three polarization images taken at a zero polarizer angle for
    three light directions.
  Figure 4 Link: articels_figures_by_rev_year\2021\Surface_Normals_and_Light_Directions_From_Shading_and_Polarization\figure_4.jpg
  Figure 4 caption: 'Algorithm 1: Average error for all image pixels and different
    weights given to the two constraints.'
  Figure 5 Link: articels_figures_by_rev_year\2021\Surface_Normals_and_Light_Directions_From_Shading_and_Polarization\figure_5.jpg
  Figure 5 caption: 'Algorithm 1: Error map of image pixels for different weights
    given to the two constraints. A brighter pixel indicates larger error.'
  Figure 6 Link: articels_figures_by_rev_year\2021\Surface_Normals_and_Light_Directions_From_Shading_and_Polarization\figure_6.jpg
  Figure 6 caption: Variation of Gaussian noise deviation on the synthesized images.
    The maximum image intensity is normalized to 1.
  Figure 7 Link: articels_figures_by_rev_year\2021\Surface_Normals_and_Light_Directions_From_Shading_and_Polarization\figure_7.jpg
  Figure 7 caption: Comparison of results for normal (a) and refractive index (b).
  Figure 8 Link: articels_figures_by_rev_year\2021\Surface_Normals_and_Light_Directions_From_Shading_and_Polarization\figure_8.jpg
  Figure 8 caption: Porcelain doll in our experiment. Images shown have a zero polarizer
    angle for eight light directions.
  Figure 9 Link: articels_figures_by_rev_year\2021\Surface_Normals_and_Light_Directions_From_Shading_and_Polarization\figure_9.jpg
  Figure 9 caption: Experimental setup with a) Lumenera and b) SONY cameras.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.79
  Name of the first author: Trung Thanh Ngo
  Name of the last author: Rin-ichiro Taniguchi
  Number of Figures: 17
  Number of Tables: 4
  Number of authors: 3
  Paper title: Surface Normals and Light Directions From Shading and Polarization
  Publication Date: 2021-04-13 00:00:00
  Table 1 caption: 'TABLE 1 SONY Dataset: Mean Absolute Difference (in degree) of
    a Objects Normal in Comparison With its 3D Scanned Data'
  Table 10 caption: Not Available
  Table 2 caption: 'TABLE 2 SONY Dataset: Light Direction Estimation Error (degrees)'
  Table 3 caption: 'TABLE 3 Lumenera Dataset: Mean Absolute Difference (in degrees)
    of An Objects Normal in Comparison With the 3D Scanned Data'
  Table 4 caption: 'TABLE 4 Lumenera Dataset: Light Direction Estimation Error (degree)'
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3072656
- Affiliation of the first author: shanghai jiao tong university, shanghai, china
  Affiliation of the last author: shanghai jiao tong university, shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Understanding_PixelLevel_D_Image_Semantics_With_D_Keypoint_Knowledge_Engine\figure_1.jpg
  Figure 1 caption: Comparison between previous works and our method on 2D pixel-wise
    semantic understanding. Previous methods tend to find pixel-wise semantics by
    transferring from a reference image with 2D warping. Viewpoint and occlusion variations
    make them fail to align well with the input image. Our method directly estimates
    viewpoints and transfer dense semantics from an existing large-scale 3D model
    repository.
  Figure 10 Link: articels_figures_by_rev_year\2021\Understanding_PixelLevel_D_Image_Semantics_With_D_Keypoint_Knowledge_Engine\figure_10.jpg
  Figure 10 caption: 3D dense semantic embedding prediction. The database has a large
    collection of models (may not necessarily contain the model to be evaluated).
    We extract their keypoint embeddings using PointNet, whose embeddings are trained
    with contrastive loss. For an input model, its dense embedding is extracted with
    the same pretrained PointNet.
  Figure 2 Link: articels_figures_by_rev_year\2021\Understanding_PixelLevel_D_Image_Semantics_With_D_Keypoint_Knowledge_Engine\figure_2.jpg
  Figure 2 caption: Pixel-level semantic understanding pipeline. (a) 3D models are
    reconstructed from single RGB images. (b) Viewpoints are also estimated from RGB
    images. (c) We obtain point-wise embeddings by training on existing 3D keypoint
    database, with contrastive loss. (d) Given viewpoints, 3D models and transferred
    embeddings, we project them onto the original image plane. In this way, we can
    achieve pixel-level semantic understanding.
  Figure 3 Link: articels_figures_by_rev_year\2021\Understanding_PixelLevel_D_Image_Semantics_With_D_Keypoint_Knowledge_Engine\figure_3.jpg
  Figure 3 caption: Browser interface of the annotation tool.
  Figure 4 Link: articels_figures_by_rev_year\2021\Understanding_PixelLevel_D_Image_Semantics_With_D_Keypoint_Knowledge_Engine\figure_4.jpg
  Figure 4 caption: Dataset Visualization. Here we plot ground-truth keypoints for
    several categories. We can see that by utilizing our automatic aggregation method,
    keypoints of high fidelity are extracted.
  Figure 5 Link: articels_figures_by_rev_year\2021\Understanding_PixelLevel_D_Image_Semantics_With_D_Keypoint_Knowledge_Engine\figure_5.jpg
  Figure 5 caption: Keypoint aggregation pipeline. We first infer dense embeddings
    from human labeled raw annotations. Then fidelity error maps are calculated by
    summing embedding distances to human labeled keypoints. Non-minimum suppression
    is conducted to form a potential set of keypoints. These keypoints are then projected
    onto 2D subspace with t-SNE and verified by humans.
  Figure 6 Link: articels_figures_by_rev_year\2021\Understanding_PixelLevel_D_Image_Semantics_With_D_Keypoint_Knowledge_Engine\figure_6.jpg
  Figure 6 caption: Keypoint clustering results for airplanes with DBSCAN.
  Figure 7 Link: articels_figures_by_rev_year\2021\Understanding_PixelLevel_D_Image_Semantics_With_D_Keypoint_Knowledge_Engine\figure_7.jpg
  Figure 7 caption: Keypoint clustering results for cars with DBSCAN.
  Figure 8 Link: articels_figures_by_rev_year\2021\Understanding_PixelLevel_D_Image_Semantics_With_D_Keypoint_Knowledge_Engine\figure_8.jpg
  Figure 8 caption: Single view 3D model reconstruction. (a) First, 2.5D sketches
    including normals, silhouettes and depths are estimated. (b) Then, 3D transpose
    convolution is used to recover object voxels. (c) In addition, a shape naturalness
    score is proposed to ensure that generated shapes are not diverged from real shapes.
  Figure 9 Link: articels_figures_by_rev_year\2021\Understanding_PixelLevel_D_Image_Semantics_With_D_Keypoint_Knowledge_Engine\figure_9.jpg
  Figure 9 caption: Viewpoint Estimation Module. Viewpoint is estimated from 2D CNN
    followed by several fully connected layers. Azimuth and elevation are discretized
    into bins and optimized with cross-entropy loss.
  First author gender probability: 0.77
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Yang You
  Name of the last author: Cewu Lu
  Number of Figures: 15
  Number of Tables: 6
  Number of authors: 8
  Paper title: Understanding Pixel-Level 2D Image Semantics With 3D Keypoint Knowledge
    Engine
  Publication Date: 2021-04-13 00:00:00
  Table 1 caption: TABLE 1 Comparison of 3D Keypoint Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The Statistics of Number of Keypoints on Each Model, Collected
    on the Full Dataset
  Table 3 caption: TABLE 3 Keypoint Dataset Statistics
  Table 4 caption: TABLE 4 Comparison of Our Method With State-of-the-Arts
  Table 5 caption: TABLE 5 Ablation study
  Table 6 caption: TABLE 6 Detailed Analysis of Each Component With PCK Results Evaluated
    on Pix3D and ShapeNet Chairs
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3072659
- Affiliation of the first author: department of electrical and computer engineering,
    university of florida, gainesville, fl, usa
  Affiliation of the last author: department of electrical and computer engineering,
    university of florida, gainesville, fl, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\Fast_Foveating_Cameras_for_Dense_Adaptive_Resolution\figure_1.jpg
  Figure 1 caption: Moving mirror creates many virtual views.
  Figure 10 Link: articels_figures_by_rev_year\2021\Fast_Foveating_Cameras_for_Dense_Adaptive_Resolution\figure_10.jpg
  Figure 10 caption: Sample images from our dataset. Our foveating camera data is
    on the left and iPhone 6 smartphone is on the right. Some faces are off center
    due to this being a real experiment with the camera moving.
  Figure 2 Link: articels_figures_by_rev_year\2021\Fast_Foveating_Cameras_for_Dense_Adaptive_Resolution\figure_2.jpg
  Figure 2 caption: Foveating camera prototype.
  Figure 3 Link: articels_figures_by_rev_year\2021\Fast_Foveating_Cameras_for_Dense_Adaptive_Resolution\figure_3.jpg
  Figure 3 caption: The coverglass induces ghosting and double images. With a combination
    of using Brewsters angle, NIR notch filter and absorbing film, we eliminate the
    ghosting.
  Figure 4 Link: articels_figures_by_rev_year\2021\Fast_Foveating_Cameras_for_Dense_Adaptive_Resolution\figure_4.jpg
  Figure 4 caption: Timing description of our foveating camera. t mirror is the mirror
    settling time used to delay the camera trigger signal, t exposure is the camera
    exposure time and t system is the total time of one frame capture after all motion
    and processing delays.
  Figure 5 Link: articels_figures_by_rev_year\2021\Fast_Foveating_Cameras_for_Dense_Adaptive_Resolution\figure_5.jpg
  Figure 5 caption: Helper smartphone image.
  Figure 6 Link: articels_figures_by_rev_year\2021\Fast_Foveating_Cameras_for_Dense_Adaptive_Resolution\figure_6.jpg
  Figure 6 caption: Raw output of our foveating camera switching between two red objects
    based on point-to-point algorithm.
  Figure 7 Link: articels_figures_by_rev_year\2021\Fast_Foveating_Cameras_for_Dense_Adaptive_Resolution\figure_7.jpg
  Figure 7 caption: Heterogeneity.
  Figure 8 Link: articels_figures_by_rev_year\2021\Fast_Foveating_Cameras_for_Dense_Adaptive_Resolution\figure_8.jpg
  Figure 8 caption: 'Simulations of 1D slice optimization: In (I) and (II) we created
    simulations to test the iterative optimization in Algorithm 1. (I) is a free-form
    optimization, whereas (II) constrains the optimization along the proposed bounded
    region. Note that the percent error for (II) is slightly lower.'
  Figure 9 Link: articels_figures_by_rev_year\2021\Fast_Foveating_Cameras_for_Dense_Adaptive_Resolution\figure_9.jpg
  Figure 9 caption: Our eye tracking setup and gaze pattern used for our finetuning
    dataset.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Brevin Tilmon
  Name of the last author: Sanjeev Koppal
  Number of Figures: 15
  Number of Tables: 1
  Number of authors: 4
  Paper title: Fast Foveating Cameras for Dense Adaptive Resolution
  Publication Date: 2021-04-15 00:00:00
  Table 1 caption: TABLE 1 Random Initialization Fails (TrainVal error)
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3071588
- Affiliation of the first author: research school of electrical, energy and material
    engineering, australian national university, canberra, act, australia
  Affiliation of the last author: department of electrical and computer systems engineering,
    monash university, clayton, vic, australia
  Figure 1 Link: articels_figures_by_rev_year\2021\Attention_in_Attention_Networks_for_Person_Retrieval\figure_1.jpg
  Figure 1 caption: "The structure of Linear attention with AiA. \u03C6(\u22C5) ,\
    \ \u03D5(\u22C5) and \u03D6(\u22C5) are embedding functions. GAP indicates global\
    \ average pooling. \u2297 indicates element-wise multiplication."
  Figure 10 Link: articels_figures_by_rev_year\2021\Attention_in_Attention_Networks_for_Person_Retrieval\figure_10.jpg
  Figure 10 caption: Some failure cases on person re-ID datasets. In each ranking
    list, to the left is the query person and to the right is the corresponding ranked
    list in the gallery set. The correct and false matches are enclosed in green and
    red boxes. Best viewed in color.
  Figure 2 Link: articels_figures_by_rev_year\2021\Attention_in_Attention_Networks_for_Person_Retrieval\figure_2.jpg
  Figure 2 caption: Details of the Attention in Attention (AiA) mechanism.
  Figure 3 Link: articels_figures_by_rev_year\2021\Attention_in_Attention_Networks_for_Person_Retrieval\figure_3.jpg
  Figure 3 caption: The structure of Squeeze and Excitation block.
  Figure 4 Link: articels_figures_by_rev_year\2021\Attention_in_Attention_Networks_for_Person_Retrieval\figure_4.jpg
  Figure 4 caption: The structure of Linear attention without AiA.
  Figure 5 Link: articels_figures_by_rev_year\2021\Attention_in_Attention_Networks_for_Person_Retrieval\figure_5.jpg
  Figure 5 caption: 'The structure of AiA modules employing non-linear features in
    the feature map. (a): Second-order polynomial attention with AiA, (b): Gaussian
    attention with AiA, (c): Second-order polynomial attention without AiA, (d): Gaussian
    attention without AiA. mathrmSoP(cdot) indicates the bilinear pooling and second
    order feature rearrangement function. mathrmGau(cdot) indicates the random Fourier
    feature mapping function.'
  Figure 6 Link: articels_figures_by_rev_year\2021\Attention_in_Attention_Networks_for_Person_Retrieval\figure_6.jpg
  Figure 6 caption: Processing of bilinear pooling and second order feature rearrangement,
    denoted by mathrmSoP(cdot) . In this operation, we sample the elements in the
    upper triangle of boldsymbolY and vectorize those elements to a new feature vector
    tildeboldsymbolx .
  Figure 7 Link: articels_figures_by_rev_year\2021\Attention_in_Attention_Networks_for_Person_Retrieval\figure_7.jpg
  Figure 7 caption: The deep architecture of the proposed feature extractor. AiA-Net
    has two feature extractors, e.g., the person appearance feature extractor (i.e.,
    mathcal Fa ) and the part feature extractor (i.e., mathcal Fp ). fa and fp are
    concatenated to give the final person representation as f = [fatop , fatop ]top
    .
  Figure 8 Link: articels_figures_by_rev_year\2021\Attention_in_Attention_Networks_for_Person_Retrieval\figure_8.jpg
  Figure 8 caption: Comparison of the learned attention on CUHK03 (a) and Market-1501
    (b) datasets. In each dataset, we compare the the feature map from Lin-attention
    and its alternatives (e.g., SE block and NL block). In the heat map, the response
    increases from blue to red. Best viewed in color.
  Figure 9 Link: articels_figures_by_rev_year\2021\Attention_in_Attention_Networks_for_Person_Retrieval\figure_9.jpg
  Figure 9 caption: Visualization of the attention mechanism in person images, sampled
    from the CUHK03 dataset (a) and the Market dataset (b). In each dataset, from
    left to right, (1) the input person image, (2) the input feature map to attention
    and (3) the masked feature map. The heat maps are generated in AiA-Net with Gau-attention.
    In the heat map, the response increases from blue to red. Best viewed in color.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Pengfei Fang
  Name of the last author: Mehrtash Harandi
  Number of Figures: 11
  Number of Tables: 13
  Number of authors: 6
  Paper title: Attention in Attention Networks for Person Retrieval
  Publication Date: 2021-04-15 00:00:00
  Table 1 caption: TABLE 1 Summary of the Proposed Attention Modules
  Table 10 caption: TABLE 10 Evaluation on the CUHK03-New Dataset in Both Labeled
    and Detected Bounding Box
  Table 2 caption: TABLE 2 Result of Various Backbone Networks on the CUHK03 and Market-1501
    Datasets
  Table 3 caption: TABLE 3 Effect of the Attention in Attention Nechanism on the CUHK03
    and Market-1501 Datasets
  Table 4 caption: TABLE 4 Effect of the Learned Non-Linearity in Attention Mechanism
    on the CUHK03 and Market-1501 Datasets
  Table 5 caption: "TABLE 5 Effect of the Dimensionality Reduction Factor r r in the\
    \ Embedding Function \u03C6(\u22C5) \u03C6(\xB7) on the CUHK03 and Market-1501\
    \ Datasets"
  Table 6 caption: "TABLE 6 Effect of the Dimensionality c \u2032 c in Random Features\
    \ on the CUHK03 and Market-1501 Datasets"
  Table 7 caption: TABLE 7 Effect of the Position of the AiA Block on the CUHK03 and
    Market-1501 Datasets
  Table 8 caption: TABLE 8 Computational Complexity and Module Size of Proposed Attention
    Modules
  Table 9 caption: TABLE 9 Evaluation on the CUHK03-Vanilla Dataset in Both Labeled
    and Detected Bounding Box
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3073512
- Affiliation of the first author: mitsubishi electric research labs (merl), cambridge,
    ma, usa
  Affiliation of the last author: university of minnesota, minneapolis, mn, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\Learning_LogDeterminant_Divergences_for_Positive_Definite_Matrices\figure_1.jpg
  Figure 1 caption: An illustration of ABLD and its connections to popular divergences
    on SPD matrices.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Learning_LogDeterminant_Divergences_for_Positive_Definite_Matrices\figure_2.jpg
  Figure 2 caption: "A schematic illustration of our IDDL scheme. From an infinite\
    \ set of potential geometries, our goal is to learn multiple geometries (parameterized\
    \ by (\u03B1,\u03B2) ) and representative dictionary atoms for each geometry (represented\
    \ by B s), such that a given SPD data matrix X can be embedded into a similarity\
    \ vector V X , each dimension of which captures the divergence of X to the B s\
    \ using the respective measure. We use V X for classification."
  Figure 3 Link: articels_figures_by_rev_year\2021\Learning_LogDeterminant_Divergences_for_Positive_Definite_Matrices\figure_3.jpg
  Figure 3 caption: Comparisons between different variants of IDDL for increasing
    number of dictionary atoms.
  Figure 4 Link: articels_figures_by_rev_year\2021\Learning_LogDeterminant_Divergences_for_Positive_Definite_Matrices\figure_4.jpg
  Figure 4 caption: "4(a) and 4(b) show accuracy on KTH-TIPS2 and Virus datasets as\
    \ we change \u03B1 and \u03B2 , fixing the number of dictionary atoms. See the\
    \ algorithmic settings in Section 6.4.1). 4(c) and 4(d) show convergence of our\
    \ optimization scheme for IDDL (ridge regression) on KTH-TIPS2 and Virus datasets.\
    \ See the text for more details."
  Figure 5 Link: articels_figures_by_rev_year\2021\Learning_LogDeterminant_Divergences_for_Positive_Definite_Matrices\figure_5.jpg
  Figure 5 caption: Time taken for one gradient computation and two objective functions
    evaluations against an increasing number of matrix dimensions (left) and number
    of dictionary atoms (right).
  Figure 6 Link: articels_figures_by_rev_year\2021\Learning_LogDeterminant_Divergences_for_Positive_Definite_Matrices\figure_6.jpg
  Figure 6 caption: (a) Trajectory of alpha and beta over BCD iterations on the virus
    dataset, (b) shows the respective objective descent, and (c) shows the training
    set accuracy. We plot for various initializations of alpha and beta .
  Figure 7 Link: articels_figures_by_rev_year\2021\Learning_LogDeterminant_Divergences_for_Positive_Definite_Matrices\figure_7.jpg
  Figure 7 caption: Sensitivity of alpha beta -KMeans against 7(a) increasing dimensionality
    in the range [5, 100] . The blue and red lines correspond to alpha beta -KMeans
    with alpha =beta and alpha ne beta respectively, while the black line corresponds
    to LE-KMeans. 7(b) Time required for each iteration of updating parameters alpha
    beta (blue line) and centroids (green line). 7(c) and 7(d) show the same for increasing
    number of clusters.
  Figure 8 Link: articels_figures_by_rev_year\2021\Learning_LogDeterminant_Divergences_for_Positive_Definite_Matrices\figure_8.jpg
  Figure 8 caption: Convergence plot of the objective function 13 for the myometrium
    cancer dataset and alpha =beta . Cyan line segments correspond to iterations of
    updating the divergence parameters, blue segments correspond to updating the clustering
    assignment and magenta segments correspond to iterations of updating the centroids.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Anoop Cherian
  Name of the last author: Nikolaos Papanikolopoulos
  Number of Figures: 8
  Number of Tables: 7
  Number of authors: 6
  Paper title: Learning Log-Determinant Divergences for Positive Definite Matrices
  Publication Date: 2021-04-15 00:00:00
  Table 1 caption: TABLE 1 ABLD and Its Connections to Popular Divergences
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 Riemannian Tools to Perform RCG on S d ++ S++d. Here,\
    \ sym(X)= 1 2 (X+ X T ) sym (X)=12(X+XT), Exp(\u22C5) Exp(\xB7) Denotes the Matrix\
    \ Exponential and Z=(Y X \u22121 ) 1 2 Z=(YX-1)12"
  Table 3 caption: TABLE 3 Performance Evaluation of IDDL on a Single Split of the
    Virus Dataset When Fixing the Dictionary Atoms Against Fixing the Parameters,
    and Jointly Learning the Atoms and the Parameters
  Table 4 caption: TABLE 4 Comparisons Against 1-NN and SVM Classification
  Table 5 caption: TABLE 5 Comparisons Against State-of-the-Art
  Table 6 caption: TABLE 6 Comparisons Between IDDL Variants for Ridge Regression
    and Structured SVM Alternatives
  Table 7 caption: TABLE 7 F1-Score Based Comparisons Against Different KMeans Variants
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3073588
- Affiliation of the first author: chongqing institute of green and intelligent technology,
    chinese academy of sciences, chongqing, china
  Affiliation of the last author: department of computer science and information systems,
    birkbeck college, university of london, london, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2021\Robust_Face_Alignment_via_Deep_Progressive_Reinitialization_and_Adaptive_ErrorDr\figure_1.jpg
  Figure 1 caption: Training a system for face alignment is difficult because of the
    significant differences of results from face detectors (a) and the notable variations
    in ground-truth landmark annotations (b).
  Figure 10 Link: articels_figures_by_rev_year\2021\Robust_Face_Alignment_via_Deep_Progressive_Reinitialization_and_Adaptive_ErrorDr\figure_10.jpg
  Figure 10 caption: CED curves and NME (%) comparison of different landmark types
    predicted by the Small MobileNetV2 backbone learned with different loss functions
    on 300-W and WFLW.
  Figure 2 Link: articels_figures_by_rev_year\2021\Robust_Face_Alignment_via_Deep_Progressive_Reinitialization_and_Adaptive_ErrorDr\figure_2.jpg
  Figure 2 caption: The pipeline of the proposed deep regression architecture with
    two-stage reinitialization and error-driven learning. At the GSR stage (I), the
    coarse face (b) is first reinitialized to a normalized shape state (c), and then
    regresses a rough face shape (e). At the LSR stage (II), different face parts
    (f) are further separately reinitialized to their normalized shape states (g),
    followed by local regression sub-networks to get the final detection (h). The
    final shape is projected back to the initial coordinate (j). All landmarks in
    (e) and (h) are drawn by red circles with different sizes, representing their
    weight values of the landmark-weighted loss function regression in training progress.
  Figure 3 Link: articels_figures_by_rev_year\2021\Robust_Face_Alignment_via_Deep_Progressive_Reinitialization_and_Adaptive_ErrorDr\figure_3.jpg
  Figure 3 caption: Detailed configuration of the localization network designed for
    the global reinitialization module.
  Figure 4 Link: articels_figures_by_rev_year\2021\Robust_Face_Alignment_via_Deep_Progressive_Reinitialization_and_Adaptive_ErrorDr\figure_4.jpg
  Figure 4 caption: 'Results of the global reinitialization sub-network. Top row:
    the input initial face images with the original face boxes. Bottom row: the transformed
    face images output by the global reinitialization sub-network.'
  Figure 5 Link: articels_figures_by_rev_year\2021\Robust_Face_Alignment_via_Deep_Progressive_Reinitialization_and_Adaptive_ErrorDr\figure_5.jpg
  Figure 5 caption: Mean images of the original faces, the global and local reinitialization
    results.
  Figure 6 Link: articels_figures_by_rev_year\2021\Robust_Face_Alignment_via_Deep_Progressive_Reinitialization_and_Adaptive_ErrorDr\figure_6.jpg
  Figure 6 caption: Definitions of the four local patches on faces with different
    landmarks and output examples of the local reinitialization sub-network. The filled
    patterns with different colors on (a) and (b) represent different local patches.
  Figure 7 Link: articels_figures_by_rev_year\2021\Robust_Face_Alignment_via_Deep_Progressive_Reinitialization_and_Adaptive_ErrorDr\figure_7.jpg
  Figure 7 caption: The long tail phenomenon of prediction errors of different landmarks
    on 300-W. The green and red dots represent the ground-truth and predicted landmarks
    on face images, respectively.
  Figure 8 Link: articels_figures_by_rev_year\2021\Robust_Face_Alignment_via_Deep_Progressive_Reinitialization_and_Adaptive_ErrorDr\figure_8.jpg
  Figure 8 caption: Distributions of landmark-wise errors predicted by Small MobileNetV2
    using different loss functions on 300-W and WFLW. The origin of coordinates (black
    dot) represents the ground truth. Each dot denotes a landmark offset. For each
    plotted ellipse, its center, semi-major axes, and semi-minor axes denote the mean
    value, the X-axis, and the Y-axis standard deviation of all corresponding landmarks
    errors, respectively. The landmarks from left to right columns are selected from
    the subsets 1, 18, 37, 31, 49 and 1, 34, 61, 55, 77 of the full annotations of
    300-W (68-landmark) and WFLW (98-landmark), respectively.
  Figure 9 Link: articels_figures_by_rev_year\2021\Robust_Face_Alignment_via_Deep_Progressive_Reinitialization_and_Adaptive_ErrorDr\figure_9.jpg
  Figure 9 caption: Visual comparison of face alignment results predicted by different
    models on 300-W and WFLW.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xiaohu Shao
  Name of the last author: Steve Maybank
  Number of Figures: 11
  Number of Tables: 11
  Number of authors: 6
  Paper title: Robust Face Alignment via Deep Progressive Reinitialization and Adaptive
    Error-Driven Learning
  Publication Date: 2021-04-15 00:00:00
  Table 1 caption: TABLE 1 Comparisons of Alignment Performance on Different Face
    Parts Using the NME Metric
  Table 10 caption: TABLE 10 Comparison on AFLW Using NME (%)
  Table 2 caption: TABLE 2 Small MobileNetV2
  Table 3 caption: TABLE 3 Ablation Study of Four Different Baseline Models Using
    Four Kinds of Face Detectors on the 300-W Dataset Using NME
  Table 4 caption: TABLE 4 Comparison of NME(%) on 300-W Based on Different Kinds
    of Face Box Perturbations
  Table 5 caption: TABLE 5 Comparison of Our Proposed G&LSR G&LSR Model and Other
    Baselines on 300-W and WFLW Using NME(%)
  Table 6 caption: TABLE 6 Comparison of the Proposed Method Based on Different Backbones
    on 300-W
  Table 7 caption: TABLE 7 Comparison of Our Proposed Method Based on Different Networks
    in Terms of Model Size and Speed
  Table 8 caption: TABLE 8 Comparison on 300-W Using NME (%)
  Table 9 caption: TABLE 9 Comparison on 300-W Test and COFW Using NME (%), AUC and
    FR (%)
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3073593
- Affiliation of the first author: deparment of information engineering and mathematics,
    university of siena, siena, italy
  Affiliation of the last author: deparment of information engineering and mathematics,
    university of siena, siena, italy
  Figure 1 Link: articels_figures_by_rev_year\2021\Deep_ConstraintBased_Propagation_in_Graph_Neural_Networks\figure_1.jpg
  Figure 1 caption: The karate club dataset. This is a simple and well-known dataset
    exploited to perform a qualitative analysis of the behaviour of our model. Nodes
    have high intra-class connections and low inter-class connections. Each color
    is associated to a different class (four classes).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Deep_ConstraintBased_Propagation_in_Graph_Neural_Networks\figure_2.jpg
  Figure 2 caption: 'Node state embeddings. Evolution of the node state embeddings
    at different stages of the learning process: (a) beginning, (b) after 200 epochs
    and (c) at convergence. We are not exploiting any node-attached features from
    the available data, so that the plotted node representations are the outcome of
    the diffusion process only, which is capable of mapping the topology of the graph
    into meaningful latent representations. Each node is represented with the color
    of the given corresponding class (ground truth), while the four background colors
    are the predictions of the output function learned by our model. The model learns
    node state embeddings that are linearly separated with respect to the four classes.'
  Figure 3 Link: articels_figures_by_rev_year\2021\Deep_ConstraintBased_Propagation_in_Graph_Neural_Networks\figure_3.jpg
  Figure 3 caption: 'Left: an example of a subgraph matching problem, where the graph
    with the blue nodes is matched against the bigger graph. Right: an example of
    a graph containing a clique. The blue nodes represent a fully connected subgraph
    of dimension 4, whereas the red nodes do not belong to the clique.'
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Matteo Tiezzi
  Name of the last author: Marco Maggini
  Number of Figures: 3
  Number of Tables: 8
  Number of authors: 4
  Paper title: Deep Constraint-Based Propagation in Graph Neural Networks
  Publication Date: 2021-04-15 00:00:00
  Table 1 caption: TABLE 1 Common Implementations of the State Transition Function
    f a fa
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The Considered Variants of the G G Function
  Table 3 caption: TABLE 3 Accuracies on the Artificial Datasets, for the Proposed
    Model (Lagrangian Propagation GNN - LP-GNN) and the Standard GNN Model for Different
    Settings
  Table 4 caption: TABLE 4 Average and Standard Deviation of the Classification Accuracy
    on the Graph Classification Benchmarks, Evaluated on the Test Set, for Different
    GNN Models
  Table 5 caption: TABLE 5 Average and Standard Deviation of the Classification Accuracy
    on the CSL Dataset With Positional Embeddings, Evaluated on the Test Sets and,
    for Completeness, on the Training Sets, for Different GNNs (results of the competitors
    are taken from [19])
  Table 6 caption: TABLE 6 Average and Standard Deviation of the Classification Accuracy
    on the ENZYMES Benchmark, evaluated on the Test Set and, for Completeness, on
    the Training Set, for Different GNN Models (Results of the Competitors are Taken
    From [19])
  Table 7 caption: "TABLE 7 Absolute and Relative Test Accuracies in the IMDB-B Data\
    \ When the Number of GNN Layers Varies From 1 to 5 (i.e., K\u2208[1,5] K\u2208\
    [1,5])"
  Table 8 caption: TABLE 8 LP Diffusion in Other GNN Models
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3073504
- Affiliation of the first author: school of computing, acrv, data61-csiro, the australian
    national university, canberra, act, australia
  Affiliation of the last author: school of computing, the australian national university,
    canberra, act, australia
  Figure 1 Link: articels_figures_by_rev_year\2021\Uncertainty_Inspired_RGBD_Saliency_Detection\figure_1.jpg
  Figure 1 caption: "GT compared with our predicted saliency maps. For a simple context\
    \ image (first row), we can produce consistent predictions. When dealing with\
    \ complex scenarios where there exists uncertainties in salient regions (second\
    \ row), our model can produce diverse predictions (\u201COurCVAE Samples\u201D\
    \ and \u201COurABP Samples\u201D), where \u201COurCVAE\u201D and \u201COurABP\u201D\
    \ are our deterministic prediction after the saliency consensus module, which\
    \ will be introduced in Section 3.3."
  Figure 10 Link: articels_figures_by_rev_year\2021\Uncertainty_Inspired_RGBD_Saliency_Detection\figure_10.jpg
  Figure 10 caption: "Structured outputs generation, where \u201COurCVAE Samples\u201D\
    \ and \u201COurCVAE\u201D are samples and the deterministic prediction respectively."
  Figure 2 Link: articels_figures_by_rev_year\2021\Uncertainty_Inspired_RGBD_Saliency_Detection\figure_2.jpg
  Figure 2 caption: "Training and testing pipeline. During training, the inferred\
    \ latent variable z and input image X are fed to the \u201CGenerator Model\u201D\
    \ for stochastic saliency prediction. During testing, we sample from the prior\
    \ distribution of z to produce diverse predictions for each input image."
  Figure 3 Link: articels_figures_by_rev_year\2021\Uncertainty_Inspired_RGBD_Saliency_Detection\figure_3.jpg
  Figure 3 caption: "Details of the \u201CGenerator Model\u201D, which takes image\
    \ X and latent variable z as input, and produces stochastic saliency map S , where\
    \ \u201CS1-S4\u201D represent the four convolutional blocks of our backbone network.\
    \ \u201CDASPP\u201D is the DenseASPP module [71], \u201CPAM\u201D and \u201CCAM\u201D\
    \ are position attention and channel attention modules [29], \u201CRCA\u201D is\
    \ the Residual Channel Attention operation from [72]."
  Figure 4 Link: articels_figures_by_rev_year\2021\Uncertainty_Inspired_RGBD_Saliency_Detection\figure_4.jpg
  Figure 4 caption: "RGB-D saliency detection via a CVAE. The \u201CGenerator Model\u201D\
    \ is shown in Fig. 3. During training, we sample from both posterior net z\u223C\
    \ Q \u03D5 (z|X,Y) and prior net z\u223C P \u03B8 (z|X) to obtain predictions\
    \ S CVAE and S GSNN respectively. During testing, S GSNN is our prediction."
  Figure 5 Link: articels_figures_by_rev_year\2021\Uncertainty_Inspired_RGBD_Saliency_Detection\figure_5.jpg
  Figure 5 caption: "Detailed structure of the inference models, where K is dimension\
    \ of the latent space, \u201Cc14K\u201D represents a 1\xD71 convolutional layer\
    \ of output channel size 4\xD7K , \u201Cfc\u201D represents the fully connected\
    \ layer."
  Figure 6 Link: articels_figures_by_rev_year\2021\Uncertainty_Inspired_RGBD_Saliency_Detection\figure_6.jpg
  Figure 6 caption: Example showing how the saliency consensus module works.
  Figure 7 Link: articels_figures_by_rev_year\2021\Uncertainty_Inspired_RGBD_Saliency_Detection\figure_7.jpg
  Figure 7 caption: E-measure and F-measure curves on six testing datasets (NJU2K,
    SSB, DES, NLPR, LFSD, and SIP). Best viewed on screen.
  Figure 8 Link: articels_figures_by_rev_year\2021\Uncertainty_Inspired_RGBD_Saliency_Detection\figure_8.jpg
  Figure 8 caption: "Visual comparison of predictions of our methods and competing\
    \ methods. Note that, our final prediction is generated with the proposed \u201C\
    Saliency Consencus Module\u201D (see Section 3.3)."
  Figure 9 Link: articels_figures_by_rev_year\2021\Uncertainty_Inspired_RGBD_Saliency_Detection\figure_9.jpg
  Figure 9 caption: Precision-recall (PR) curves on six testing datasets, where the
    x -axis is the recall, and y -axis is the precision. Best viewed on screen.
  First author gender probability: 0.76
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jing Zhang
  Name of the last author: Nick Barnes
  Number of Figures: 14
  Number of Tables: 7
  Number of authors: 7
  Paper title: Uncertainty Inspired RGB-D Saliency Detection
  Publication Date: 2021-04-15 00:00:00
  Table 1 caption: TABLE 1 Summary of the Notations in Different Part of Our Framework
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Benchmarking Results of Ten Leading Handcrafted Feature-Based
    Models and Nine Deep Models on Six RGBD Saliency Datasets
  Table 3 caption: TABLE 3 Performance of Competing RGB Saliency Detection Models
    and Ours on RGBD Saliency Datasets, Where Depth Data is Not Used While Testing
    Using the RGB Saliency Models
  Table 4 caption: "TABLE 4 Distribution (Mean and Variance) of the Evaluation Metrics,\
    \ Where the Variance \u201C2.3\u201D Represents \u201C2.3e-06\u201D"
  Table 5 caption: TABLE 5 The Code Type and Inference Time of Existing Approaches
  Table 6 caption: TABLE 6 Evaluation of the Effect of Different Components in Our
    Models, and Alternative Structures
  Table 7 caption: TABLE 7 Comparison With the State-of-the-Art RGB Saliency Detection
    Models on Six Benchmark RGB Saliency Datasets
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3073564
- Affiliation of the first author: school of automation, northwestern polytechnical
    university, xian, shaanxi, china
  Affiliation of the last author: school of computer science and center for optical
    imagery analysis and learning (optimal), northwestern polytechnical university,
    xian, shaanxi, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Adaptive_Neighborhood_Metric_Learning\figure_1.jpg
  Figure 1 caption: The illustration of the geometric structures (inseparable regions)
    to cause the inseparable samples. In (a)-(c), the query is from class 1, the dissimilar
    samples are from class 2. There is no metric value to let dissimilar samples out
    of line segment (a), parallelogram (b), or polyhedron (c) under linear metric
    learning. In (d), there is no metric value to let ( x 2 , x 1 , x 4 ) and ( x
    4 , x 3 , x 2 ) be separable simultaneously.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Adaptive_Neighborhood_Metric_Learning\figure_2.jpg
  Figure 2 caption: "The illustration of the inseparable and separable samples of\
    \ the query x i in the projected space. \u2202 f \u03B8 \u2217 ( A i ) and \u2202\
    \ f \u03B8 \u2217 ( B i ) are the boundaries of the neighborhoods A i and B i\
    \ in projected space, respectively."
  Figure 3 Link: articels_figures_by_rev_year\2021\Adaptive_Neighborhood_Metric_Learning\figure_3.jpg
  Figure 3 caption: "The graphical illustration of values of the function r(S,\u03B8\
    ,x,K\u03B1) presented in Eq. (8). For better illustration, all the samples are\
    \ set on a line, and the distance set d \u03B8 (x, x j )| x j \u2208S forms a\
    \ arithmetic sequence. The blue circles are the samples in the set S . The white\
    \ circles are the positions estimated by r(S,\u03B8,x,K\u03B1) ."
  Figure 4 Link: articels_figures_by_rev_year\2021\Adaptive_Neighborhood_Metric_Learning\figure_4.jpg
  Figure 4 caption: "The illustration of the correspondence relationship between the\
    \ 1 K \u2211 K k=1 a k (or 1 K \u2211 K k=1 a n\u2212k+1 ) and b(\u03B3) . The\
    \ number series a i n i=1 is generalized randomly with n=10 and ranked with ascending\
    \ order. From left to right, x dots represent the values of 1 K \u2211 K k=1 a\
    \ n\u2212k+1 with K=2,3,\u2026,10 , respectively. Similarly, o dots represent\
    \ the values of 1 K \u2211 K k=1 a k with K=10,9,\u2026,2 , respectively."
  Figure 5 Link: articels_figures_by_rev_year\2021\Adaptive_Neighborhood_Metric_Learning\figure_5.jpg
  Figure 5 caption: The left figure shows the cases of selecting target neighbors
    in LMNN under the euclidean metric. The right figure shows the separable similar
    samples selected by LANML ( gamma 1 <0 ). Obviously, LANML ( gamma 1 <0 ) can
    select more separable samples than LMNN from the same similar sample set, and
    utilize more discriminant information to learn the metric.
  Figure 6 Link: articels_figures_by_rev_year\2021\Adaptive_Neighborhood_Metric_Learning\figure_6.jpg
  Figure 6 caption: The illustration of how the adjacency matrix affects the cluster
    number. In (a), the adjacency matrix has two diagonal blocks, and the samples
    lie within two clusters. In (b), the elements A34 and A43 are not zero and the
    whole adjacency matrix becomes a diagonal block. Thus, the samples lie within
    one cluster.
  Figure 7 Link: articels_figures_by_rev_year\2021\Adaptive_Neighborhood_Metric_Learning\figure_7.jpg
  Figure 7 caption: The curve line of g(gamma) = frac1gamma log(sum i=1negamma ai)
    . When gamma >0 ( gamma <0 ), g(gamma) is larger (smaller) than the maximal (minimal)
    value in the number series lbrace a1,a2,ldots,anrbrace .
  Figure 8 Link: articels_figures_by_rev_year\2021\Adaptive_Neighborhood_Metric_Learning\figure_8.jpg
  Figure 8 caption: The running times of different methods on different data sets.
    The LMNN-4, LMNN-5, LMNN-6, LMNN-7 represent LMNN with target neighbor number
    Kt = lbrace 4,5,6,7rbrace , respectively. LANML + and LANML - represent LANML
    with parameter gamma 1 >0 and gamma 2 <0 , respectively.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.79
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Kun Song
  Name of the last author: Feiping Nie
  Number of Figures: 8
  Number of Tables: 6
  Number of authors: 5
  Paper title: Adaptive Neighborhood Metric Learning
  Publication Date: 2021-04-15 00:00:00
  Table 1 caption: TABLE 1 The Details of Several Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison of Different Methods on 15 Data Sets
  Table 3 caption: TABLE 3 Performance on the CUB-200-2011 of the Three State-of-the-Art
    Methods and Their Improved Versions With 512 Dimension
  Table 4 caption: TABLE 4 RecallK(%) Performance on CUB-200-2011 Dataset and Cars-196
    Dataset
  Table 5 caption: TABLE 5 RecallK(%) Performance on In-Shop Dataset
  Table 6 caption: TABLE 6 RecallK(%) Performance on SOP Dataset
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3073587
- Affiliation of the first author: state key laboratory of synthetical automation
    for process industries, northeastern university, shenyang, china
  Affiliation of the last author: state key laboratory of synthetical automation for
    process industries, northeastern university, shenyang, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Revisiting_Light_Field_Rendering_With_Deep_AntiAliasing_Neural_Network\figure_1.jpg
  Figure 1 caption: Schematic diagram of the proposed Deep Anti-Aliasing Network (DA
    2 N) with embedding shearing, downscaling and prefiltering. In the result, we
    use input views indicated by the red lines for the EPI reconstruction, including
    interpolation and extrapolation.
  Figure 10 Link: articels_figures_by_rev_year\2021\Revisiting_Light_Field_Rendering_With_Deep_AntiAliasing_Neural_Network\figure_10.jpg
  Figure 10 caption: Comparison of the results on the LFs from camera array system
    ( times 3 upsamping). The results show one of the synthesized result, a reference
    image from the input LF, a portion of the EPI extracted from the location marked
    in the red line.
  Figure 2 Link: articels_figures_by_rev_year\2021\Revisiting_Light_Field_Rendering_With_Deep_AntiAliasing_Neural_Network\figure_2.jpg
  Figure 2 caption: "Fourier analysis of shear, downscaling (spatial) and prefilter\
    \ operations. The EPI for demonstration is composed of three Lambertian points\
    \ ( A,C and D ) and one non-Lambertian point ( B ) with different disparities.\
    \ (a) Original EPI (top), its Fourier transform after taking the logarithm (middle)\
    \ and diagram (down). The non-Lambertian point will introduce expansionary spectra\
    \ on each side of the \u03A9 s axis [6]; (b) Spectrum replicas appear when the\
    \ EPI is angularly down-sampled. The overlapped spectrum and its replicas will\
    \ lead to an aliasing effect. An ideal reconstruction filter (parallelogram) can\
    \ be obtained using the optimal rendering disparity (depth) shown by the dashed\
    \ line; (c) Shearing the EPI with a proper value forces the optimal rendering\
    \ disparity of the reconstruction filter to rotate towards the \u03A9 u axis;\
    \ (d) With a downscaling (spatial) operation, the original spectrum will shrink\
    \ towards the \u03A9 u axis. And the separated spectra substantially mitigates\
    \ the aliasing issue; (e) We show an EPI with shear, downscaling and prefilter\
    \ operations (top) and an EPI with directly up-sampling (middle), see the close-up\
    \ for the detail. We can efficiently decrease the shape parameter \u03C3 of the\
    \ prefilter by using shear and downscaling operations (bottom)."
  Figure 3 Link: articels_figures_by_rev_year\2021\Revisiting_Light_Field_Rendering_With_Deep_AntiAliasing_Neural_Network\figure_3.jpg
  Figure 3 caption: Architecture of the proposed deep anti-aliasing network (DA 2
    N) based on shearing, downscaling and prefiltering. The DA 2 N contains a reconstruction
    module for high angular resolution EPI and a fusion module for blending the EPIs
    with different shear amounts.
  Figure 4 Link: articels_figures_by_rev_year\2021\Revisiting_Light_Field_Rendering_With_Deep_AntiAliasing_Neural_Network\figure_4.jpg
  Figure 4 caption: Two-step strategy for reconstructing a 4D LF from 2D EPIs.
  Figure 5 Link: articels_figures_by_rev_year\2021\Revisiting_Light_Field_Rendering_With_Deep_AntiAliasing_Neural_Network\figure_5.jpg
  Figure 5 caption: The reconstructed LF (or EPI) benefits from the proposed shear
    layer as well as the downscaling structure. (a) Ground truth and input EPIs; (b)
    EPI without shear layer; (c) EPI without the learning-based Laplacian pyramid
    structure; (d) EPI with our proposed pipeline.
  Figure 6 Link: articels_figures_by_rev_year\2021\Revisiting_Light_Field_Rendering_With_Deep_AntiAliasing_Neural_Network\figure_6.jpg
  Figure 6 caption: Pseudo EPIs and regular EPIs in non-Lambertian scenes can be described
    by curves and variation of pixel intensity (color). These two EPIs show the same
    property in the Fourier domain as well.
  Figure 7 Link: articels_figures_by_rev_year\2021\Revisiting_Light_Field_Rendering_With_Deep_AntiAliasing_Neural_Network\figure_7.jpg
  Figure 7 caption: Comparison of the results ( times 16 upsampling) on the LFs from
    the ICME DSLF dataset [40]. The results show one of our reconstructed view, EPIs
    extracted from LFs reconstructed by each methods. The PSNR and SSIM values are
    averaged on the reconstructed views (input views are excluded).
  Figure 8 Link: articels_figures_by_rev_year\2021\Revisiting_Light_Field_Rendering_With_Deep_AntiAliasing_Neural_Network\figure_8.jpg
  Figure 8 caption: Comparison of the results ( times 16 upsampling) on the LFs from
    the MPI Light Field Archive [47]. The green lines in the ground truth EPI are
    input views. The PSNR and SSIM values are averaged on the reconstructed views
    (input views are excluded).
  Figure 9 Link: articels_figures_by_rev_year\2021\Revisiting_Light_Field_Rendering_With_Deep_AntiAliasing_Neural_Network\figure_9.jpg
  Figure 9 caption: Comparison of the results on the LFs from Lytro Illum ( times
    3 upsamping). The results show one of our reconstructed views, close-up versions
    of the image portions in the yellow and blue boxes, and the EPIs located at the
    red line shown in the left images. We also show a close-up of the portion of the
    EPIs in the red box. LFs are from the 30 Scenes set [10] and the Refractive and
    Reflective Surfaces category [49].
  First author gender probability: 0.75
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.89
  Name of the first author: Gaochang Wu
  Name of the last author: Tianyou Chai
  Number of Figures: 12
  Number of Tables: 7
  Number of authors: 4
  Paper title: Revisiting Light Field Rendering With Deep Anti-Aliasing Neural Network
  Publication Date: 2021-04-16 00:00:00
  Table 1 caption: TABLE 1 Main Symbol Used in the Paper
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 Architecture of the Reconstruction Net, Where k is the\
    \ Kernel Size (spatialangular), str is the Stride (spatialangular), chn is the\
    \ Number of Channels (inputoutput), and Input is the Input Layer With \u201C \u2296\
    \ \u2296\u201D Denoting Element-Wise Subtraction and \u201C;\u201D Denoting Concatenation\
    \ in the Channel Dimension"
  Table 3 caption: TABLE 3 Architecture of the Fusion Net
  Table 4 caption: TABLE 4 Quantitative Results (PSNRSSIM) of Reconstructed LFs on
    the LFs From the ICME DSLF Dataset [40]
  Table 5 caption: "TABLE 5 Quantitative Results (PSNRSSIM) of Reconstructed LFs (\
    \ \xD716 \xD716) on the LFs From the MPI Light Field Archive [47]"
  Table 6 caption: TABLE 6 Quantitative Results (PSNRSSIM) of Reconstructed Views
    on the LFs From Plenoptic Cameras
  Table 7 caption: TABLE 7 Comparison of Model Size (number of trainable parameters)
    and Running Time for the Reconstruction of LFs From Lytro Illum
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3073739
