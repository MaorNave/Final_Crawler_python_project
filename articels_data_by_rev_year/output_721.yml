- Affiliation of the first author: massachusetts institute of technology, cambridge,
    ma
  Affiliation of the last author: massachusetts institute of technology, cambridge,
    ma
  Figure 1 Link: articels_figures_by_rev_year\2016\Visual_Vibrometry_Estimating_Material_Properties_from_Small_Motions_in_Video\figure_1.jpg
  Figure 1 caption: We present a method for estimating material properties of an object
    by examining small motions in video. (A) We record video of different fabrics
    and clamped rods exposed to small forces such as sound or natural air currents
    in a room. (B) We show fabrics (top) color-coded and ordered by area weight, and
    rods (bottom) similarly ordered by their ratio of elastic modulus to density.
    (C) Local motion signals are extracted from captured videos and used to compute
    a temporal power spectrum for each object. These motion spectra contain information
    that is predictive of each object's material properties. For instance, observe
    the trends in the spectra for fabrics and rods as they increase in area weight
    and elasticitydensity, respectively (blue to red). By examining these spectra,
    we can make inferences about the material properties of objects.
  Figure 10 Link: articels_figures_by_rev_year\2016\Visual_Vibrometry_Estimating_Material_Properties_from_Small_Motions_in_Video\figure_10.jpg
  Figure 10 caption: Comparisons between ground truth and PLSR model predictions on
    material properties estimated from videos of fabric excited by ambient forces
    and acoustic waves. Each circle in the plots represents the estimated properties
    from a single video. Identical colors correspond to the same fabric. The Pearson
    product-moment correlation coefficient (R-value) averaged across video samples
    containing the same fabric is displayed.
  Figure 2 Link: articels_figures_by_rev_year\2016\Visual_Vibrometry_Estimating_Material_Properties_from_Small_Motions_in_Video\figure_2.jpg
  Figure 2 caption: Rods were clamped to a concrete block next to a loudspeaker (shown
    left) and filmed with a high-speed camera. By analyzing small motions in the recorded
    video, we are able to extract the locations of the rod's resonant frequencies
    and use these values to estimate the rod's material properties.
  Figure 3 Link: articels_figures_by_rev_year\2016\Visual_Vibrometry_Estimating_Material_Properties_from_Small_Motions_in_Video\figure_3.jpg
  Figure 3 caption: "Finding vibration modes of a clamped brass rod: (Left) We recover\
    \ a motion spectrum from 2,500 Hz video of a 22 inch clamped aluminum rod. Resonant\
    \ frequencies are labeled. To distinguish resonant frequencies from other spikes\
    \ in the spectrum, we look for energy at frequencies with ratios derived from\
    \ the known geometry of the rod. (Middle) A sample frame from the 80 \xD7 2,016\
    \ pixel input video. (Right) Visualizations of the first four recovered mode shapes\
    \ are shown next to the corresponding shapes predicted by theory."
  Figure 4 Link: articels_figures_by_rev_year\2016\Visual_Vibrometry_Estimating_Material_Properties_from_Small_Motions_in_Video\figure_4.jpg
  Figure 4 caption: Our damping selection interface, inspired by the standard procedure
    defined in [2] , presents users with a view of the recovered motion spectra around
    a predicted rod resonant frequency and asks them to click and drag over the spike
    region. A Lorentzian is then fit to the selected region and presented to the user
    for evaluation of accuracy.
  Figure 5 Link: articels_figures_by_rev_year\2016\Visual_Vibrometry_Estimating_Material_Properties_from_Small_Motions_in_Video\figure_5.jpg
  Figure 5 caption: 'Estimating the elastic modulus and length of clamped rods: (a)
    Young''s moduli (force per squared inch) reported by the manufacturer plotted
    against values estimated using our technique. Estimated values are close to those
    reported by the manufacturer, with the largest discrepancies happening in 15 inch
    rods made of aluminum and steel. (b) The length (inches) of each rod measured
    to the base of the clamp plotted against values estimated using our technique.'
  Figure 6 Link: articels_figures_by_rev_year\2016\Visual_Vibrometry_Estimating_Material_Properties_from_Small_Motions_in_Video\figure_6.jpg
  Figure 6 caption: The damping ratio estimated from the recovered motion spectra
    for each automatically identified resonant frequency. While reported damping ratios
    for different materials vary greatly, general trends are recognized. Our recovered
    rod damping ratios show recognized trends of higher damping in wood than in metals
    [12] , and higher damping in lower fundamental modes due to their high amplitude
    [3] .
  Figure 7 Link: articels_figures_by_rev_year\2016\Visual_Vibrometry_Estimating_Material_Properties_from_Small_Motions_in_Video\figure_7.jpg
  Figure 7 caption: Videos were recorded of the fabric moving from (c) a grayscale
    Point Grey camera (800 times 600 pixel resolution) at 60 fps and (d) an RGB SLR
    Camera (Canon 6D, 1,920 times 1,080 pixel resolution) at 30 fps. The experimental
    layout (a,b) consisted of the two cameras observing the fabric from different
    points of view.
  Figure 8 Link: articels_figures_by_rev_year\2016\Visual_Vibrometry_Estimating_Material_Properties_from_Small_Motions_in_Video\figure_8.jpg
  Figure 8 caption: Videos of fabric excited by two different types of force were
    recorded. Here we see space times time slices from minute long videos of a fabric
    responding ambient forces (b) and sound (c). The motion is especially subtle in
    (b), but still encodes predictive information about the fabric's material properties.
  Figure 9 Link: articels_figures_by_rev_year\2016\Visual_Vibrometry_Estimating_Material_Properties_from_Small_Motions_in_Video\figure_9.jpg
  Figure 9 caption: The Pearson product correlation value between predicted results
    and the ground truth measured properties when fitting a model with a varying number
    of components (dimensionality). The number of components, M , was selected for
    each model by choosing a value that resulted in good accuracy for both material
    properties (stiffness and area weight). These selected M values are specified
    above and are indicated on the plots as a vertical red line.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Abe Davis*
  Name of the last author: William T. Freeman
  Number of Figures: 16
  Number of Tables: 6
  Number of authors: 7
  Paper title: 'Visual Vibrometry: Estimating Material Properties from Small Motions
    in Video'
  Publication Date: 2016-11-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Percent Error in Estimating the Young's Modulus (Force per
      Squared Inch) for Each Rod
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Percent Error in Estimating the Length (Inches) for Each Rod
  Table 3 caption:
    table_text: TABLE 3 The Pearson Correlation R Value Obtained When Training and
      Testing a PLSR Model on Videos Captured under Different Excitation and Viewpoint
      Conditions
  Table 4 caption:
    table_text: TABLE 4 Performance in Estimating Fabric Properties
  Table 5 caption:
    table_text: TABLE 5 Comparison of Recovered Beam Mode Frequencies
  Table 6 caption:
    table_text: TABLE 6 Comparision of Recovered Damping Ratios
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2622271
- Affiliation of the first author: queensland brain institute and school of mathematics
    and physics, university of queensland, st lucia, qld, australia
  Affiliation of the last author: queensland brain institute and school of mathematics
    and physics, university of queensland, st lucia, qld, australia
  Figure 1 Link: articels_figures_by_rev_year\2016\Estimating_Cortical_Feature_Maps_with_Dependent_Gaussian_Processes\figure_1.jpg
  Figure 1 caption: Topographic features maps in primary visual cortex have stereotypical
    structures and spatial relationships. (a) Contours of an orientation preference
    (coloured lines) and an ocular dominance (black lines) map, demonstrating the
    tendency for their intersection at angles close to perpendicular, and for OP pinwheels
    to lie far from the OD contours. (b) The distribution of crossing angles of the
    two maps' contours for a particular pair of maps, quantifying the trend observed
    in panel a. (c) The relative location of OP pinwheels in the OD map, averaged
    over maps, quantifying the trend observed in panel a. Figures reproduced from
    [11].
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Estimating_Cortical_Feature_Maps_with_Dependent_Gaussian_Processes\figure_2.jpg
  Figure 2 caption: "Multiple output Gaussian process prior for multiple feature maps.\
    \ Schematic of our multiple output model using the filtering kernel method to\
    \ construct covariance functions. The independent Gaussian noise sources X 0 ,\
    \ X 1 , and X 2 are convolved ( \u2217 ) with difference-of-Gaussians filtering\
    \ kernels G and G \u2032 , and horizontal and vertical Gabor filtering kernels\
    \ H and V , to generate an ocular dominance map, m od , and the two components,\
    \ real( m \u20D7 op ) and imag( m \u20D7 op ) , of an orientation preference map\
    \ m \u20D7 op . The relative scaling of the two difference-of-Gaussians filtering\
    \ kernels controls the dependence of the maps through the proportion of the shared\
    \ noise source X 0 which influences the orientation preference map."
  Figure 3 Link: articels_figures_by_rev_year\2016\Estimating_Cortical_Feature_Maps_with_Dependent_Gaussian_Processes\figure_3.jpg
  Figure 3 caption: "Maps sampled from the prior have spatial relationships matching\
    \ empirical maps, and their level of dependence is parametrised. (a\u2013b) An\
    \ example pair of maps, orientation preference and ocular dominance, sampled from\
    \ the prior defined by the model given in Fig. 2. Parameters as in Table 1, \u03B1\
    \ \u2032 =1 . (c) Contour intersection angle distributions as a function of \u03B1\
    \ \u2032 (coloured as per legend), showing a preference for angles near perpendicular\
    \ for small values of the dependence parameter, and a shift towards the chance\
    \ distribution (black line) for increasing values of the parameter. The \u03B1\
    \ \u2032 =0 and 1 ( p=0.027 , \u03C7 2 =18.8 ), and \u03B1 \u2032 =0 and 2 ( p<5\xD7\
    \ 10 \u22127 , \u03C7 2 =46.8 ), distributions are significantly different (chi-square\
    \ tests). (d) Relative pinwheel location distributions as a function of \u03B1\
    \ \u2032 (coloured as per legend), showing a preference for locations near the\
    \ centre of ocular dominance domains for small values of the dependence parameter,\
    \ and a shift towards the chance distribution (black line) for increasing values\
    \ of the parameter. The \u03B1 \u2032 =0 and 1 ( p=0.016 , \u03C7 2 =14.0 ), and\
    \ \u03B1 \u2032 =0 and 2 ( p<3\xD7 10 \u22125 , \u03C7 2 =28.5 ), distributions\
    \ are significantly different (chi-square tests). Mean and standard error of the\
    \ mean shown for ten sampled pairs of maps."
  Figure 4 Link: articels_figures_by_rev_year\2016\Estimating_Cortical_Feature_Maps_with_Dependent_Gaussian_Processes\figure_4.jpg
  Figure 4 caption: "Estimating maps from noisy data. (a,e) A pair of maps sampled\
    \ from the joint prior with parameters as in Table 1 and \u03B1 \u2032 =1 . (b,f)\
    \ Noisy observations of the same pair of maps, corrupted with correlated Gaussian\
    \ noise ( \u03C3 2 noise =6 ). (c,g) Optimally filtered vector averaged maps constructed\
    \ from the noisy observations. Correlation with true maps: 0.95. (d,h) The posterior\
    \ mean estimates using the joint prior, constructed from the same noisy data as\
    \ the vector averaged maps. Correlation with true maps: 0.99. (i) The quality\
    \ of maps estimated with optimally filtered vector averaging (VA, blue) and our\
    \ joint prior Gaussian process model (GP, red), measured as the correlation with\
    \ the true maps, as a function of the level of added correlated noise \u03C3 2\
    \ noise . Results for both maps sampled from the joint prior (dotted) and maps\
    \ generated with the elastic net model (solid) are shown. The GP estimate is significantly\
    \ superior in all cases except the noiseless case on maps sampled from the prior\
    \ (two sample t -tests, p<0.001 ), and for all noise levels greater than \u03C3\
    \ 2 noise =4 on maps from the model (two sample t -tests, p<0.01 ). (j) The quality\
    \ of OP maps generated with the elastic net model, estimated with vector averaging\
    \ (VA), our joint prior model (GP), and an independent GP model that does not\
    \ take into account the OD map (iGP), as a function of the level of noise added.\
    \ The GP estimate is significantly better than the iGP estimate for \u03C3 2 noise\
    \ =0,2,4,6 (two sample t -tests, p<0.05 ). Mean and standard error of the mean\
    \ shown for ten different pairs of maps for each level of noise."
  Figure 5 Link: articels_figures_by_rev_year\2016\Estimating_Cortical_Feature_Maps_with_Dependent_Gaussian_Processes\figure_5.jpg
  Figure 5 caption: Estimating maps from experimental data. (a,d) A pair of maps estimated
    with vector averaging from previously published experimental data [27]. (b,e)
    Optimally filtered vector averaged maps, filtered to maximise the correlation
    with the joint prior estimate. (c,f) The posterior mean estimates using the joint
    prior. (g) The same region of cortex imaged with a different wavelength light
    to highlight the vasculature patterns present. The vector averaged maps are more
    strongly affected by artefacts induced by these patterns than the joint prior
    estimate.
  Figure 6 Link: articels_figures_by_rev_year\2016\Estimating_Cortical_Feature_Maps_with_Dependent_Gaussian_Processes\figure_6.jpg
  Figure 6 caption: "A joint prior for three visual cortical maps. (a) Schematic of\
    \ the model, in the same format as Fig. 2. A spatial frequency component has been\
    \ added, defined as the ocular dominance map filtered with a difference-of-Gaussians\
    \ kernel G sf with twice the wavelength of G . (b\u2013d) A triplet of maps, orientation\
    \ preference, ocular dominance, and spatial frequency, sampled from this prior\
    \ (parameters as in Table 1, \u03B1 \u2032 =0.25 ). The values and units of the\
    \ SF map are arbitrary, but follow a log-normal distribution, and when interpreted\
    \ as cycles per degree match well with experimental data [12]. (e) Crossing angles\
    \ between all three pairs of maps. The OD and OP maps (blue) intersect at angles\
    \ larger than expected by chance (black), while the OD and SF maps intersect at\
    \ more acute angles (green). Mean and standard error of the mean shown for ten\
    \ sampled triples of maps."
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Nicholas J. Hughes
  Name of the last author: Geoffrey J. Goodhill
  Number of Figures: 6
  Number of Tables: 1
  Number of authors: 2
  Paper title: Estimating Cortical Feature Maps with Dependent Gaussian Processes
  Publication Date: 2016-11-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Joint Prior Model Hyperparameters
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2624295
- Affiliation of the first author: agh university of science and technology, krakow,
    poland
  Affiliation of the last author: applied chest imaging laboratory (acil), harvard
    medical school, 1249 boylston st, boston, ma
  Figure 1 Link: articels_figures_by_rev_year\2016\NonStationary_Rician_Noise_Estimation_in_Parallel_MRI_Using_a_Single_Image_A_Var\figure_1.jpg
  Figure 1 caption: "(a) Optimized parameters \u03B8 1opt and \u03B8 2opt in terms\
    \ of SNR and their closed-form approximations, (b) comparison of standard deviations\
    \ between Rician distributed data and different stabilizing transforms, (c) standard\
    \ deviation of the variance-stabilizing parametric approach f stab (M|\u03C3,\
    \ \u0398 opt ) compared to Foi's stabilizers."
  Figure 10 Link: articels_figures_by_rev_year\2016\NonStationary_Rician_Noise_Estimation_in_Parallel_MRI_Using_a_Single_Image_A_Var\figure_10.jpg
  Figure 10 caption: Visual inspection of the methods for synthetic T1 -weighted MR
    brain data distorted by spatially variable noise (left figure) and corresponding
    relative errors of the estimators (right figure).
  Figure 2 Link: articels_figures_by_rev_year\2016\NonStationary_Rician_Noise_Estimation_in_Parallel_MRI_Using_a_Single_Image_A_Var\figure_2.jpg
  Figure 2 caption: General scheme of the proposed non-stationary Rician noise estimation
    algorithm. The red rectangles indicate interchangeable modules of the algorithm.
  Figure 3 Link: articels_figures_by_rev_year\2016\NonStationary_Rician_Noise_Estimation_in_Parallel_MRI_Using_a_Single_Image_A_Var\figure_3.jpg
  Figure 3 caption: 'Data sets used in the experiments: (a) synthetic noise-free T
    1 -, (b) T 2 - and (c) PD-weighted MR data, (d) synthetic noisy T 1 -weighted
    SENSE simulated image, (e) real noisy T 1 -weighted SENSE phantom and (f) in vivo
    T 2 -weighted FFE SENSE brain data.'
  Figure 4 Link: articels_figures_by_rev_year\2016\NonStationary_Rician_Noise_Estimation_in_Parallel_MRI_Using_a_Single_Image_A_Var\figure_4.jpg
  Figure 4 caption: Spatially variant noise patterns used in the experiments for synthetic
    MRI.
  Figure 5 Link: articels_figures_by_rev_year\2016\NonStationary_Rician_Noise_Estimation_in_Parallel_MRI_Using_a_Single_Image_A_Var\figure_5.jpg
  Figure 5 caption: Anderson-Darling test of Gaussianity for the noise component in
    variance-stabilized MR signal.
  Figure 6 Link: articels_figures_by_rev_year\2016\NonStationary_Rician_Noise_Estimation_in_Parallel_MRI_Using_a_Single_Image_A_Var\figure_6.jpg
  Figure 6 caption: 'Noise component extracted with different techniques: (a) local
    mean, (b) local EM algorithm, (c) bilateral filter, (d) HH subband of SWT, (e)
    VST + bilateral filter (proposed), (f) VST + HH subband of SWT (proposed).'
  Figure 7 Link: articels_figures_by_rev_year\2016\NonStationary_Rician_Noise_Estimation_in_Parallel_MRI_Using_a_Single_Image_A_Var\figure_7.jpg
  Figure 7 caption: Influence of SNR mismatch on standard deviation of variance-stabilized
    Rician data for boldsymbolThetamathrmopt . The red dashed line indicates the breakdown
    point of the parameter theta 2mathrmopt .
  Figure 8 Link: articels_figures_by_rev_year\2016\NonStationary_Rician_Noise_Estimation_in_Parallel_MRI_Using_a_Single_Image_A_Var\figure_8.jpg
  Figure 8 caption: Analysis of the error propagation in both the Quantile-Quantile
    transformation and the proposed methodology.
  Figure 9 Link: articels_figures_by_rev_year\2016\NonStationary_Rician_Noise_Estimation_in_Parallel_MRI_Using_a_Single_Image_A_Var\figure_9.jpg
  Figure 9 caption: 'Comparison of noise estimators for synthetic MR data contaminated
    by non-stationary Rician noise. First column: Spatially averaged relative error
    textRE(mathbf x) of the estimators; Second column: Spatially averaged variance
    textVAR(mathbf x) of the estimators; Third column: Zoomed textVAR(mathbf x) parameter
    to the range [0 - 0.01] from the second column. The first row corresponds to T1
    -, the second row to T2 - and the third one to PD-weighted MRI.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Tomasz Pieciak
  Name of the last author: "Gonzalo Vegas-S\xE1nchez-Ferrero"
  Number of Figures: 13
  Number of Tables: 2
  Number of authors: 3
  Paper title: 'Non-Stationary Rician Noise Estimation in Parallel MRI Using a Single
    Image: A Variance-Stabilizing Approach'
  Publication Date: 2016-11-07 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Comparison of Spatially Variable Noise Estimation Techniques\
      \ in MRI or Riciannc- \u03C7 Distributed Data"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Averaged Relative Errors of the Proposed Scheme Using Different\
      \ State-of-the-Art Techniques to Initialize \u03C3 0 (x)"
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2625789
- Affiliation of the first author: department of computer science, university of toronto,
    toronto, on, canada
  Affiliation of the last author: department of computer science, university of toronto,
    toronto, on, canada
  Figure 1 Link: articels_figures_by_rev_year\2016\Building_Proteins_in_a_Day_Efficient_D_Molecular_Structure_Estimation_with_Elect\figure_1.jpg
  Figure 1 caption: The goal is to reconstruct the 3D structure of a molecule (right),
    at sub-nanometer scales, from a large number of noisy, uncalibrated 2D projections
    obtained from cryogenically frozen samples in an electron microscope (left).
  Figure 10 Link: articels_figures_by_rev_year\2016\Building_Proteins_in_a_Day_Efficient_D_Molecular_Structure_Estimation_with_Elect\figure_10.jpg
  Figure 10 caption: A Winkel-Tripel projection of the importance distribution of
    view directions, mathbf qmathbf d , averaged over the thermus dataset at a typical
    iteration. Clearly visible is the equatorial belt of likely views, while axis
    aligned views (those on the top or bottom of the plot) are rarely seen.
  Figure 2 Link: articels_figures_by_rev_year\2016\Building_Proteins_in_a_Day_Efficient_D_Molecular_Structure_Estimation_with_Elect\figure_2.jpg
  Figure 2 caption: A generative image formation model in cryo-EM. The electron beam
    results in an orthographic integral projection of the electron density of the
    specimen. This projection is modulated by the contrast transfer function (CTF)
    and corrupted with noise. The images pictured here showcase the low SNR typical
    in cryo-EM. The zeros in the CTF (which completely destroy some spatial information)
    make estimation particularly challenging, however their locations vary as a function
    of microscope parameters. These are set differently across particle images in
    order to mitigate this problem. Particle images and density from [16].
  Figure 3 Link: articels_figures_by_rev_year\2016\Building_Proteins_in_a_Day_Efficient_D_Molecular_Structure_Estimation_with_Elect\figure_3.jpg
  Figure 3 caption: "The KL divergence between the values of \u03D5 d at the current\
    \ and previous epochs on the thermus dataset. As optimization progresses, the\
    \ coarse structure of the molecule is quickly determined, and the KL divergence\
    \ becomes very small by the third epoch. This indicates that the significantly\
    \ likely poses for each image have stabilized, and so importance sampling can\
    \ focus quadrature on these regions preferentially, providing significant speedups."
  Figure 4 Link: articels_figures_by_rev_year\2016\Building_Proteins_in_a_Day_Efficient_D_Molecular_Structure_Estimation_with_Elect\figure_4.jpg
  Figure 4 caption: Previously published structures for the datasets used in this
    paper.
  Figure 5 Link: articels_figures_by_rev_year\2016\Building_Proteins_in_a_Day_Efficient_D_Molecular_Structure_Estimation_with_Elect\figure_5.jpg
  Figure 5 caption: Sample particle images (left), an isosurface of the reconstructed
    3D density (middle) and slices through the 3D density with colour indicating relative
    density (right) for GroEL-GroES (top), thermus thermophilus ATPase (middle) and
    bovine mitochondrial ATPase (bottom). Reconstructions took a day or less on a
    single 16 core workstation.
  Figure 6 Link: articels_figures_by_rev_year\2016\Building_Proteins_in_a_Day_Efficient_D_Molecular_Structure_Estimation_with_Elect\figure_6.jpg
  Figure 6 caption: "Reconstruction progress at several times during a run of our\
    \ method. Top row thermus, middle row bovine, and bottom row GroEL-GroES datasets.\
    \ Initializations are generated randomly as a sum of spheres. Note that within\
    \ an hour of computation, the gross structure is already well determined, after\
    \ which fine details emerge gradually. Video sequences of reconstruction progress\
    \ can be found at http:www.cs.toronto.edu\u223Calipunjanipami16cryoem."
  Figure 7 Link: articels_figures_by_rev_year\2016\Building_Proteins_in_a_Day_Efficient_D_Molecular_Structure_Estimation_with_Elect\figure_7.jpg
  Figure 7 caption: "Fourier shell correlation between the estimated structure and\
    \ ground truth for GroELGroES. The estimated resolution of 9.1\xC5 is consistent\
    \ with the highest frequencies considered, i.e., the largest value of omega ast\
    \ (in cycles per Anstrom)."
  Figure 8 Link: articels_figures_by_rev_year\2016\Building_Proteins_in_a_Day_Efficient_D_Molecular_Structure_Estimation_with_Elect\figure_8.jpg
  Figure 8 caption: Relative error (blue, left axis) and fraction of total quadrature
    points (red, right axis) used in computing log p(hatmathcal I| theta, hatmathcal
    V) as a function of the ESS scaling factor, s0 (horizontal axis), on log-log axes.
    Error bars represent the variance over a population of 100 individual images.
  Figure 9 Link: articels_figures_by_rev_year\2016\Building_Proteins_in_a_Day_Efficient_D_Molecular_Structure_Estimation_with_Elect\figure_9.jpg
  Figure 9 caption: The fraction of naive quadrature points evaluated on average during
    optimization, when using importance sampling. As resolution increases, the speedup
    obtained increases significantly yielding more than a 100,000 fold speedup.
  First author gender probability: 0.94
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ali Punjani
  Name of the last author: David J. Fleet
  Number of Figures: 13
  Number of Tables: 0
  Number of authors: 3
  Paper title: 'Building Proteins in a Day: Efficient 3D Molecular Structure Estimation
    with Electron Cryomicroscopy'
  Publication Date: 2016-11-10 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2627573
- Affiliation of the first author: qualcomm research, amsterdam, xh, the netherlands
  Affiliation of the last author: quva lab, university of amsterdam, amsterdam, wx,
    the netherlands
  Figure 1 Link: articels_figures_by_rev_year\2016\Videovec_Embeddings_Recognize_Events_When_Examples_Are_Scarce\figure_1.jpg
  Figure 1 caption: Video exemplars (top) and the textual definition (bottom) of the
    event winning a race without a vehicle to illustrate one of the events studied
    in this paper. Following the NIST TRECVID evaluation guidelines [11], the textual
    definition is for zero-example recognition, and the ten provided video exemplars
    are for few-example event recognition.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Videovec_Embeddings_Recognize_Events_When_Examples_Are_Scarce\figure_2.jpg
  Figure 2 caption: 'Video2vec prediction: From the low-level video features the Video2vec
    representation and the term vector are predicted using the visual projection matrix
    W and the textual projection matrix A .'
  Figure 3 Link: articels_figures_by_rev_year\2016\Videovec_Embeddings_Recognize_Events_When_Examples_Are_Scarce\figure_3.jpg
  Figure 3 caption: Example videos and title captions from the VideoStory46K dataset
    [37], which we use for Video2vec representation learning.
  Figure 4 Link: articels_figures_by_rev_year\2016\Videovec_Embeddings_Recognize_Events_When_Examples_Are_Scarce\figure_4.jpg
  Figure 4 caption: Effect of embedding. The Video2vec embedding outperforms the term
    attribute and term attribute-f baselines, which are directly learned from the
    terms without embedding.
  Figure 5 Link: articels_figures_by_rev_year\2016\Videovec_Embeddings_Recognize_Events_When_Examples_Are_Scarce\figure_5.jpg
  Figure 5 caption: Video2vec versus other embeddings. Video2vec outperforms the CCA
    and the description embedding on all three test sets. The description embedding
    is the closest competitor, but it suffers from embedding correlated terms which
    are visually dissimilar. CCA, uses the same objective function to learn the visual
    and textual embeddings, which is suboptimal due to intrinsic differences between
    the visual and textual features.
  Figure 6 Link: articels_figures_by_rev_year\2016\Videovec_Embeddings_Recognize_Events_When_Examples_Are_Scarce\figure_6.jpg
  Figure 6 caption: From 1 to 100 examples. When the number of event exemplars is
    limited Video2vec outperforms the low-level representation. By increasing training
    examples the difference becomes more subtle.
  Figure 7 Link: articels_figures_by_rev_year\2016\Videovec_Embeddings_Recognize_Events_When_Examples_Are_Scarce\figure_7.jpg
  Figure 7 caption: "Effect of learning the embeddings jointly over all the modalities\
    \ by comparing Video2vec F with the Video2vec late fusion baseline. Each textual\
    \ projection matrix is visualized by plotting A\xD7 A \u22A4 , which reveals the\
    \ learned term combinations. The Video2vec F learn a more reasonable combination\
    \ of terms compared to the Video2vec late fusion, where the embeddings are learned\
    \ disjointedly per modality."
  Figure 8 Link: articels_figures_by_rev_year\2016\Videovec_Embeddings_Recognize_Events_When_Examples_Are_Scarce\figure_8.jpg
  Figure 8 caption: Unseen video examples and their term vectors predicted by the
    Video2vec (middle) and the Video2vec E embeddings (bottom). The size of each term
    indicates its prediction confidence score. The Video2vec E more effectively predicts
    the indicative terms of the event.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Amirhossein Habibian
  Name of the last author: Cees G. M. Snoek
  Number of Figures: 8
  Number of Tables: 10
  Number of authors: 3
  Paper title: Video2vec Embeddings Recognize Events When Examples Are Scarce
  Publication Date: 2016-11-10 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of the Core Notation Used for Video2vec
  Table 10 caption:
    table_text: TABLE 10 Zero-Example Event Recognition on MED 2013 by Querying with
      Event Names Instead of Event Definitions
  Table 2 caption:
    table_text: TABLE 2 Five Selected Dimensions of the Video2vec Embedding Trained
      on VideoStory46K
  Table 3 caption:
    table_text: TABLE 3 Video2vec versus Other Representations for Few-Example Event
      Recognition
  Table 4 caption:
    table_text: TABLE 4 Video2vec versus Other Semantic Representations for Zero-Example
      Event Recognition
  Table 5 caption:
    table_text: TABLE 5 Effect of Fusion
  Table 6 caption:
    table_text: TABLE 6 Comparison with Other Fusions
  Table 7 caption:
    table_text: TABLE 7 Effect of Learning Event Specific Video2vec for Zero-Example
      Event Recognition
  Table 8 caption:
    table_text: TABLE 8 Effect of Learning Event Specific Video2vec for Few-Example
      Event Recognition
  Table 9 caption:
    table_text: TABLE 9 Comparison with the State-of-the-Art
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2627563
- Affiliation of the first author: department of computer science, university of western
    ontario, london, on, canada
  Affiliation of the last author: department of electrical engineering, university
    of toronto, toronto, on, canada
  Figure 1 Link: articels_figures_by_rev_year\2016\Local_Submodularization_for_Binary_Pairwise_Energies\figure_1.jpg
  Figure 1 caption: "Standard linearization approaches for (1) and (2). Black dots\
    \ are integer points and \u2217 corresponds to the global optimum of (2). Colors\
    \ in (b) show iso-levels of the quadratic energy (1). This energy can be linearized\
    \ by introducing additional variables and linear constraints, see a schematic\
    \ polytope in (a) and [6]. Vector \u2207E is the gradient of the global linearization\
    \ of (1) in (a) and the gradient of the local linear approximation of (1) at point\
    \ S 0 in (b)."
  Figure 10 Link: articels_figures_by_rev_year\2016\Local_Submodularization_for_Binary_Pairwise_Energies\figure_10.jpg
  Figure 10 caption: "Compact Shape Prior Illustration: (a-top) the model in [33],\
    \ (a-bottom) our multilabel model, (b-top)\u2014an input silhouette that can be\
    \ modeled with our model but not with the model in [33] (see text for details),\
    \ (b-bottom) demonstrates how we split the image into five regions in our new\
    \ model, (c) schematic representation of the geometric exclusion constraints between\
    \ the layers of our graph for our model. (d) unary terms for each layer used in\
    \ our graph"
  Figure 2 Link: articels_figures_by_rev_year\2016\Local_Submodularization_for_Binary_Pairwise_Energies\figure_2.jpg
  Figure 2 caption: "Local linearization of supermodular pairwise potential f(x,y)=\u03B1\
    \u22C5xy for \u03B1>0 . This potential defines four costs f(0,0)=f(0,1)=f(1,0)=0\
    \ and f(1,1)=\u03B1 at four distinct configurations of binary variables x,y\u2208\
    0,1 . These costs can be plotted as four 3D points A , B , C , D in (a-c). We\
    \ need to approximate supermodular potential f with a linear function v\u22C5\
    x+w\u22C5y+const (plane or unary potentials). LSA-TR: one way to derive a local\
    \ linear approximation is to take Taylor expansion of f(x,y)=\u03B1\u22C5xy over\
    \ relaxed variables x,y\u2208[0,1] , see the continuous plot in (a). At first,\
    \ this idea may sound strange since there are infinitely many other continuous\
    \ functions that agree with A , B , C , D but have completely different derivatives,\
    \ e.g., g(x,y)=\u03B1\u22C5 x 2 y \u221A . However, Taylor expansions of bilinear\
    \ function f(x,y)=\u03B1\u22C5xy can be motivated geometrically. As shown in (b),\
    \ Taylor-based local linear approximation of f at any fixed integer configuration\
    \ (i,j) (e.g., blue plane at A , green at B , orange at C , and striped at D )\
    \ coincides with discrete pairwise potential f not only at point (i,j) but also\
    \ with two other closest integer configurations. Overall, each of those planes\
    \ passes exactly through three out of four points A , B , C , D . LSA-AUX: another\
    \ approach to justify a local linear approximation for non-submodular pairwise\
    \ potential f could be based on upper bounds passing through a current configuration.\
    \ For example, the green or orange planes in (b) are the tightest linear upper\
    \ bounds at configurations (0,1) and (1,0) , correspondingly. When current configuration\
    \ is either (0,0) or (1,1) then one can choose either orange or green plane in\
    \ (b), or anything in-between, e.g., the purple plane passing though A and D in\
    \ (c)."
  Figure 3 Link: articels_figures_by_rev_year\2016\Local_Submodularization_for_Binary_Pairwise_Energies\figure_3.jpg
  Figure 3 caption: "Binary deconvolution of an image created with a uniform 3\xD7\
    3 filter and additive Gaussian noise ( \u03C3\u22080.05,0.1,0.15,0.2 ). No length\
    \ regularization was used. We report mean energy (+-2std.) and time as a function\
    \ of noise level \u03C3 . TRWS, SRMP and LBP are run for 5,000 iterations."
  Figure 4 Link: articels_figures_by_rev_year\2016\Local_Submodularization_for_Binary_Pairwise_Energies\figure_4.jpg
  Figure 4 caption: "Segmentation with repulsion and attraction. We used \u03BC fg\
    \ =0.4, \u03BC bg =0.6, \u03C3 =0.2 for appearance, \u03BB reg =100 and c=0.06.\
    \ Repulsion potentials are shown in blue and attraction\u2014in red."
  Figure 5 Link: articels_figures_by_rev_year\2016\Local_Submodularization_for_Binary_Pairwise_Energies\figure_5.jpg
  Figure 5 caption: "Curvature regularizer [28] is more difficult to optimize when\
    \ regularizer weight is high. We show segmentation results for \u03BB curv =0.1\
    \ (top row), \u03BB curv =0.5 (middle row), \u03BB curv =2 (bottom row) as well\
    \ as energy plots. We used \u03BC fg = 1, \u03BC bg = 0, \u03BB app = 1."
  Figure 6 Link: articels_figures_by_rev_year\2016\Local_Submodularization_for_Binary_Pairwise_Energies\figure_6.jpg
  Figure 6 caption: 'Curvature regularizer [28]: We show segmentation results and
    energy plots for lambda curv =19 (left), lambda curv =21 (right).'
  Figure 7 Link: articels_figures_by_rev_year\2016\Local_Submodularization_for_Binary_Pairwise_Energies\figure_7.jpg
  Figure 7 caption: Examples of Chinese characters inpainting.
  Figure 8 Link: articels_figures_by_rev_year\2016\Local_Submodularization_for_Binary_Pairwise_Energies\figure_8.jpg
  Figure 8 caption: 'Multi-part object model for liver segmentation: (a) schematic
    representation of the liver containing four distinct and mutually excluding tumors.
    (b) each part of the object is represented with a separate binary layer in the
    graph. Each image pixel has a corresponding node in all five layers, resulting
    in a quintuple (FG, A, B, C, D). Interactions between corresponding nodes of different
    layers are shown with black solid lines for inclusion and blue dashed lines for
    exclusion. (c) summarizes six legal configurations for each pixel''s quintuple
    and the associated multilabel cost. All other configurations have an infinite
    cost due to inclusion or exclusion violations.'
  Figure 9 Link: articels_figures_by_rev_year\2016\Local_Submodularization_for_Binary_Pairwise_Energies\figure_9.jpg
  Figure 9 caption: "Multi-part object model for liver segmentation\u2014results and\
    \ comparison. Top-left: Input image containing one foreground object (liver) with\
    \ four distinct interior parts (tumors). Top-right: Appearance models for the\
    \ liver and the tumors are obtained from user scribbles. The liver is scribbled\
    \ with blue brush stroke and the tumors are scribbled with green, cyan, yellow\
    \ and magenta. Background is scribbled with red color. For each method we show\
    \ the resulting segmentation, color coded as in legend shown in top-right. (We\
    \ chose not to color the background pixels red for clarity). Dark gray pixels\
    \ in QPBO show pixels that were unlabeled at least in one of the layers."
  First author gender probability: 0.99
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Lena Gorelick
  Name of the last author: Andrew Delong
  Number of Figures: 14
  Number of Tables: 1
  Number of authors: 5
  Paper title: Local Submodularization for Binary Pairwise Energies
  Publication Date: 2016-11-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Chinese Characters In-Painting Database [9]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2630686
- Affiliation of the first author: department of electrical engineering, indian institute
    of technology madras, chennai, india
  Affiliation of the last author: department of electrical engineering, indian institute
    of technology madras, chennai, india
  Figure 1 Link: articels_figures_by_rev_year\2016\Image_Registration_and_Change_Detection_under_Rolling_Shutter_Motion_Blur\figure_1.jpg
  Figure 1 caption: Exposure mechanism of GS and RS cameras.
  Figure 10 Link: articels_figures_by_rev_year\2016\Image_Registration_and_Change_Detection_under_Rolling_Shutter_Motion_Blur\figure_10.jpg
  Figure 10 caption: Comparisons for the example in Fig. 9 [top].
  Figure 2 Link: articels_figures_by_rev_year\2016\Image_Registration_and_Change_Detection_under_Rolling_Shutter_Motion_Blur\figure_2.jpg
  Figure 2 caption: 'Distortions in RS cameras. Top: Three types of distortions based
    on the amount of row exposure. Bottom left: Plot of normalized cross-correlation
    between kernels from top and bottom image regions for different exposure times.
    Bottom right: Sample top and bottom blur kernels for different distortions.'
  Figure 3 Link: articels_figures_by_rev_year\2016\Image_Registration_and_Change_Detection_under_Rolling_Shutter_Motion_Blur\figure_3.jpg
  Figure 3 caption: Variation of relative depth and blur length versus building height
    for camera-to-ground distances of 600 and 1,000 m.
  Figure 4 Link: articels_figures_by_rev_year\2016\Image_Registration_and_Change_Detection_under_Rolling_Shutter_Motion_Blur\figure_4.jpg
  Figure 4 caption: 'Synthetic experiment: Change detection in a 3D scene. (a) Reference
    image, (b) Depth map, and (c) RSMB image.'
  Figure 5 Link: articels_figures_by_rev_year\2016\Image_Registration_and_Change_Detection_under_Rolling_Shutter_Motion_Blur\figure_5.jpg
  Figure 5 caption: 'Synthetic experiment: Change detection in a 3D scene. Row-1:
    Case-(i) One-object change, Row-2: Case-(ii) Two-object change, and Row-3: Case-(iii)
    No-object change. (a) Reference image, (b) RSMB image, (c) Background registered
    image, (d) Detected changes after background registration, (e) Extracted objects,
    (f) Detected changes, and (g) Estimated depth map (with detected changes in red).'
  Figure 6 Link: articels_figures_by_rev_year\2016\Image_Registration_and_Change_Detection_under_Rolling_Shutter_Motion_Blur\figure_6.jpg
  Figure 6 caption: RMSE versus dell for registration of bunny and block in Fig. 5.
  Figure 7 Link: articels_figures_by_rev_year\2016\Image_Registration_and_Change_Detection_under_Rolling_Shutter_Motion_Blur\figure_7.jpg
  Figure 7 caption: 'Real experiment: Change detection in a 2D scene. (a) Reference
    image, (b) RSMB image, (c) background registered image, (d) detected changes [7],
    (e) object detection, and (f) estimated depth map and detected changes (red).'
  Figure 8 Link: articels_figures_by_rev_year\2016\Image_Registration_and_Change_Detection_under_Rolling_Shutter_Motion_Blur\figure_8.jpg
  Figure 8 caption: Comparisons for the example in Fig. 7.
  Figure 9 Link: articels_figures_by_rev_year\2016\Image_Registration_and_Change_Detection_under_Rolling_Shutter_Motion_Blur\figure_9.jpg
  Figure 9 caption: 'Real experiment: Change detection in a 3D scene. (a) Reference
    image, (b) RSMB image, (c) background-registered image, (d) detected changes after
    background registration (output of [7]), (e) extracted objects, and (f) estimated
    depth map and detected changes (in red).'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.56
  Name of the first author: Vijay Rengarajan
  Name of the last author: Guna Seetharaman
  Number of Figures: 13
  Number of Tables: 3
  Number of authors: 4
  Paper title: Image Registration and Change Detection under Rolling Shutter Motion
    Blur
  Publication Date: 2016-11-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of RMSE Values Between RSRSMB and Registered Images
      for Different Methods in the Case of No Change
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Quantitative Metrics for Different Methods in
      Change Detection of Planar Scenes
  Table 3 caption:
    table_text: TABLE 3 Comparison of Quantitative Metrics for Different Methods in
      Change Detection of 3D Scenes
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2630687
- Affiliation of the first author: "biotechmed\u2014graz, graz, austria"
  Affiliation of the last author: department of computer science and engineering,
    university of washington, seattle, wa
  Figure 1 Link: articels_figures_by_rev_year\2016\On_the_Latent_Variable_Interpretation_in_SumProduct_Networks\figure_1.jpg
  Figure 1 caption: "Problems occurring when IVs of LVs are introduced. (a): Excerpt\
    \ of SPN containing a sum S , corresponding to LV Z . (b): Introducing IVs for\
    \ Z renders S \u2032 incomplete, assuming that S\u2209desc(N) . (c): Remedy by\
    \ extending SPN further, introducing twin sum node S \xAF ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\On_the_Latent_Variable_Interpretation_in_SumProduct_Networks\figure_2.jpg
  Figure 2 caption: Pseudo-code for augmentation of an SPN.
  Figure 3 Link: articels_figures_by_rev_year\2016\On_the_Latent_Variable_Interpretation_in_SumProduct_Networks\figure_3.jpg
  Figure 3 caption: "Augmentation of an SPN. (a): Example SPN over X= X 1 , X 2 ,\
    \ X 3 , containing sum nodes S 1 , S 2 , S 3 and S 4 . (b): Augmented SPN, containing\
    \ IVs corresponding to Z S 1 , Z S 2 , Z S 3 , Z S 4 , links and twin sum nodes\
    \ S \xAF 2 , S \xAF 3 , S \xAF 4 . For nodes introduced by augmentation, smaller\
    \ circles are used."
  Figure 4 Link: articels_figures_by_rev_year\2016\On_the_Latent_Variable_Interpretation_in_SumProduct_Networks\figure_4.jpg
  Figure 4 caption: Dependency structure of augmented SPN from Fig. 3, represented
    as BN.
  Figure 5 Link: articels_figures_by_rev_year\2016\On_the_Latent_Variable_Interpretation_in_SumProduct_Networks\figure_5.jpg
  Figure 5 caption: 'Explicitly introducing a switching parent Ymathsf S in an augmented
    SPN. (a): Part of an augmented SPN containing a sum node with three children and
    its twin. (b): Explicitly introduced switching parent Ymathsf S using IVs lambda
    Ymathsf S=ymathsf S and lambda Ymathsf S=ybarmathsf S .'
  Figure 6 Link: articels_figures_by_rev_year\2016\On_the_Latent_Variable_Interpretation_in_SumProduct_Networks\figure_6.jpg
  Figure 6 caption: Pseudo-code for EM algorithm in SPNs.
  Figure 7 Link: articels_figures_by_rev_year\2016\On_the_Latent_Variable_Interpretation_in_SumProduct_Networks\figure_7.jpg
  Figure 7 caption: Pseudo-code for MPE inference in selective SPNs.
  Figure 8 Link: articels_figures_by_rev_year\2016\On_the_Latent_Variable_Interpretation_in_SumProduct_Networks\figure_8.jpg
  Figure 8 caption: "Illustration of the low-depth bias using an SPN over RVs lbrace\
    \ X1,X2,X3rbrace . The structure introduced by augmentation is depicted by small\
    \ nodes and edges. When deterministic twin-weights are used, the state of Zmathsf\
    \ S1 corresponding to mathsf P1 is preferred over mathsf P2 and mathsf P3 , since\
    \ their probabilities are \u201Cdampened\u201D by the weights of mathsf S2 and\
    \ mathsf S3 , respectively."
  Figure 9 Link: articels_figures_by_rev_year\2016\On_the_Latent_Variable_Interpretation_in_SumProduct_Networks\figure_9.jpg
  Figure 9 caption: 'Normalized log-likelihood over EM-iterations, averaged over all
    103 datasets and three random initializations. (a): Training set. (b): Test set;
    Curves for V and WV are outside the displayed region, for better readability of
    the other curves. They start at approximately -8,000 nats and decreased to approximately
    -11,000 nats.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Robert Peharz
  Name of the last author: Pedro Domingos
  Number of Figures: 9
  Number of Tables: 5
  Number of authors: 4
  Paper title: On the Latent Variable Interpretation in Sum-Product Networks
  Publication Date: 2016-11-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Changes in Test Log-Likelihoods When Original Parameters Are
      Post-Trained Using EM
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Log-Likelihoods When Sum-Weights (W) Are Trained, Using Random
      Initialization
  Table 3 caption:
    table_text: TABLE 3 Differences of Log-Likelihood to the Ground-Truth MPE Solution
      Found by Exhaustive Enumeration, Averaged over 100 Independent Draws of Sum-Weights
  Table 4 caption:
    table_text: TABLE 4 Similar as in Table 3
  Table 5 caption:
    table_text: TABLE 5 Similar as in Table 3
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2618381
- Affiliation of the first author: "school of engineering, federal university of mato\
    \ grosso, v\xE1rzea grande, mt, brazil"
  Affiliation of the last author: institute of computing, university of campinas,
    campinas, sp, brazil
  Figure 1 Link: articels_figures_by_rev_year\2016\A_New_Framework_for_Quality_Assessment_of_HighResolution_Fingerprint_Images\figure_1.jpg
  Figure 1 caption: Example of spatial observation in R 2 . In this work, each point
    indicates, for example, the location of a pore in a fingerprint image. The window
    W from which the features are observed is represented by the silhouette of the
    captured fingerprint.
  Figure 10 Link: articels_figures_by_rev_year\2016\A_New_Framework_for_Quality_Assessment_of_HighResolution_Fingerprint_Images\figure_10.jpg
  Figure 10 caption: Histogram of Pearson's correlation between phases estimated from
    the original image blocks and phases reconstructed from the ground-truth features.
  Figure 2 Link: articels_figures_by_rev_year\2016\A_New_Framework_for_Quality_Assessment_of_HighResolution_Fingerprint_Images\figure_2.jpg
  Figure 2 caption: Comparison between real and modeled pores distribution. Black
    circles and red squares indicate, respectively, pores in the ground-truth and
    the centroid of their Voronoi regions.
  Figure 3 Link: articels_figures_by_rev_year\2016\A_New_Framework_for_Quality_Assessment_of_HighResolution_Fingerprint_Images\figure_3.jpg
  Figure 3 caption: Illustration of the global quality index Q 1 .
  Figure 4 Link: articels_figures_by_rev_year\2016\A_New_Framework_for_Quality_Assessment_of_HighResolution_Fingerprint_Images\figure_4.jpg
  Figure 4 caption: Illustration of the Voronoi-energy map.
  Figure 5 Link: articels_figures_by_rev_year\2016\A_New_Framework_for_Quality_Assessment_of_HighResolution_Fingerprint_Images\figure_5.jpg
  Figure 5 caption: Illustration of phase reconstruction for an ideal case.
  Figure 6 Link: articels_figures_by_rev_year\2016\A_New_Framework_for_Quality_Assessment_of_HighResolution_Fingerprint_Images\figure_6.jpg
  Figure 6 caption: Illustration of the structural quality algorithm.
  Figure 7 Link: articels_figures_by_rev_year\2016\A_New_Framework_for_Quality_Assessment_of_HighResolution_Fingerprint_Images\figure_7.jpg
  Figure 7 caption: Histogram of distances between location of pores and centroids
    of the Voronoi regions.
  Figure 8 Link: articels_figures_by_rev_year\2016\A_New_Framework_for_Quality_Assessment_of_HighResolution_Fingerprint_Images\figure_8.jpg
  Figure 8 caption: Histogram of smallest pairwise distances between centroids of
    the Voronoi regions.
  Figure 9 Link: articels_figures_by_rev_year\2016\A_New_Framework_for_Quality_Assessment_of_HighResolution_Fingerprint_Images\figure_9.jpg
  Figure 9 caption: 'Some reconstruction examples: (a-d) original images, and the
    corresponding reconstructed phases (e-h) by our approach and (i-l) by Level 2
    based approach [45].'
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.68
  Name of the first author: Raoni F. S. Teixeira
  Name of the last author: Neucimar J. Leite
  Number of Figures: 19
  Number of Tables: 1
  Number of authors: 2
  Paper title: A New Framework for Quality Assessment of High-Resolution Fingerprint
    Images
  Publication Date: 2016-11-22 00:00:00
  Table 1 caption:
    table_text: TABLE 1 A Summary of the Ground-Truth
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2631529
- Affiliation of the first author: institute of scientific and industrial research,
    osaka university, osaka, japan
  Affiliation of the last author: institute of scientific and industrial research,
    osaka university, osaka, japan
  Figure 1 Link: articels_figures_by_rev_year\2016\Recovering_Inner_Slices_of_Layered_Translucent_Objects_by_MultiFrequency_Illumin\figure_1.jpg
  Figure 1 caption: Illustration of the image formation model for translucent objects.
    (a) Recorded intensity is the summation of all layer's appearance. (b) Spatial
    spread of light varies with depth.
  Figure 10 Link: articels_figures_by_rev_year\2016\Recovering_Inner_Slices_of_Layered_Translucent_Objects_by_MultiFrequency_Illumin\figure_10.jpg
  Figure 10 caption: Selected PSFs and remaining high-frequency components for each
    layer. (a) Selected PSFs. There are two peaks in the plot of Eq. (17), hence two
    corresponding PSFs are selected to recover. (b) Remaining high-frequency components
    for each layer. Upper layer remains more high-frequency components than that of
    the inner layer.
  Figure 2 Link: articels_figures_by_rev_year\2016\Recovering_Inner_Slices_of_Layered_Translucent_Objects_by_MultiFrequency_Illumin\figure_2.jpg
  Figure 2 caption: "Illustration of RTE model described in [46]. (a) Solid arrow\
    \ represents a scattering light ray, whose intensity depends on d and \u03B8 .\
    \ d is the distance from the point light source I 0 to the point inside the scattering\
    \ medium, and \u03B8 is the radial direction of the light ray. (b) Depth-dependent\
    \ PSF h d can be expressed using RTE model. We consider the x -axis lies along\
    \ the tangent plane of the surface of translucent object and assume the direction\
    \ of light rays emitted from the surface becomes parallel to the depth axis."
  Figure 3 Link: articels_figures_by_rev_year\2016\Recovering_Inner_Slices_of_Layered_Translucent_Objects_by_MultiFrequency_Illumin\figure_3.jpg
  Figure 3 caption: Simulated PSF variations using the RTE model with varying optical
    thickness T.
  Figure 4 Link: articels_figures_by_rev_year\2016\Recovering_Inner_Slices_of_Layered_Translucent_Objects_by_MultiFrequency_Illumin\figure_4.jpg
  Figure 4 caption: Selection of informative PSFs (corresponding to certain depths).
    (a) Estimates hatRd obtained via optimization (Eq. (16)). Non-zero pixels indicate
    the informative regions. (b) We find the local maxima of the non-zero pixel counts
    (corresponding to l0 -norm of hatRd ) for all pixel coordinates across depth.
    (c) Selected PSFs corresponding to the local maximas.
  Figure 5 Link: articels_figures_by_rev_year\2016\Recovering_Inner_Slices_of_Layered_Translucent_Objects_by_MultiFrequency_Illumin\figure_5.jpg
  Figure 5 caption: Scene of inhomogeneous upper layer. PSFs and irradiance of inner
    layer vary depending on the upper layer's optical property.
  Figure 6 Link: articels_figures_by_rev_year\2016\Recovering_Inner_Slices_of_Layered_Translucent_Objects_by_MultiFrequency_Illumin\figure_6.jpg
  Figure 6 caption: Evaluation of the approximation in Eq. (11). We calculate the
    cross correlation between direct component slice Dd(p) and the ground truth slice
    Rd . It shows high correlation even for the worst case.
  Figure 7 Link: articels_figures_by_rev_year\2016\Recovering_Inner_Slices_of_Layered_Translucent_Objects_by_MultiFrequency_Illumin\figure_7.jpg
  Figure 7 caption: (a) Target scene (b) Result of three-layer recovery at d=1 , 5,
    and 15. The ground truth radiances slices (upper) and recovered slices (lower)
    are shown. ZNCC scores are 0.98, 0.83, and 0.51, respectively.
  Figure 8 Link: articels_figures_by_rev_year\2016\Recovering_Inner_Slices_of_Layered_Translucent_Objects_by_MultiFrequency_Illumin\figure_8.jpg
  Figure 8 caption: Measurement setup. The coaxial system allows us to maintain the
    correspondences between projector and camera pixels.
  Figure 9 Link: articels_figures_by_rev_year\2016\Recovering_Inner_Slices_of_Layered_Translucent_Objects_by_MultiFrequency_Illumin\figure_9.jpg
  Figure 9 caption: Experimental result of oil painting using the baseline method.
    (a) Target scene. We draw a colored round tree on top of the draft of spiny tree.
    (b) Inner layer (draft) of the painting. (c) Painted scene. Red rectangle region
    is measured. (d) Normal photo using infrared light. (e) Enhanced image. Intensity
    range and contrast of (d) are manually adjusted. (f, g) Layer separation results
    of Li and Brown [7]. Because their method separates sharp and blurred layers,
    it suffers from global components. (h, i) Results of our method. Layer of surface
    texture and hidden drawing, respectively. Range of the intensities is adjusted
    for visualization.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Kenichiro Tanaka
  Name of the last author: Yasushi Yagi
  Number of Figures: 15
  Number of Tables: 1
  Number of authors: 5
  Paper title: Recovering Inner Slices of Layered Translucent Objects by Multi-Frequency
    Illumination
  Publication Date: 2016-11-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Two Layers Recovery Result for 20 Sets of Scenes
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2631625
