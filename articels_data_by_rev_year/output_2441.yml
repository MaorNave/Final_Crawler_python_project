- Affiliation of the first author: department of computer science, university of hong
    kong, pokfulam, hong kong
  Affiliation of the last author: department of computer science, university of hong
    kong, pokfulam, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2021\Diagnose_Like_a_Radiologist_Hybrid_NeuroProbabilistic_Reasoning_for_AttributeBas\figure_1.jpg
  Figure 1 caption: 'Sample 2D slices of 3D pulmonary nodules from the LIDC-IDRI dataset,
    each with one of the following eight attributes: Subtlety, Internal Structure,
    Calcification, Sphericity, Margin, Lobulation, Spiculation, Texture. The annotated
    grade of each attribute is shown between the parentheses. For example, Subtlety
    (5) means its annotated grade is 5.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Diagnose_Like_a_Radiologist_Hybrid_NeuroProbabilistic_Reasoning_for_AttributeBas\figure_2.jpg
  Figure 2 caption: 'Our proposed framework consists of three stages: 1) Deep feature
    extraction using an CNN-FPN backbone; 2) hybrid attribute relational modeling
    and reasoning using a pair of coupled Bayesian network and deep graph network;
    3) causality reinforcement using a second Bayesian network. Note that BN-1 and
    BN-2 are distinct in both parameters and structures.'
  Figure 3 Link: articels_figures_by_rev_year\2021\Diagnose_Like_a_Radiologist_Hybrid_NeuroProbabilistic_Reasoning_for_AttributeBas\figure_3.jpg
  Figure 3 caption: 'Sample results of our method (O2) and four baseline methods (T1,
    T2, M41, M42) on the LIDC-IDRI dataset. Short names are defined as follows: SUB
    (Subtlety), IS (Internal Structure), CAL (Calcification), SP (Sphericity), MG
    (Margin), LB (Lobulation), SL (Spiculation), TE (Texture), and MA (Malignancy).
    The radar charts visualize the actual grades of attributes and disease, and there
    exist five possible grades for each attribute or disease. GT stands for the ground
    truth. The Histograms denote the numerical importance of different attributes
    for diagnosing malignancy (MA).'
  Figure 4 Link: articels_figures_by_rev_year\2021\Diagnose_Like_a_Radiologist_Hybrid_NeuroProbabilistic_Reasoning_for_AttributeBas\figure_4.jpg
  Figure 4 caption: 'The Bayesian network structure learned from the LIDC-IDRI dataset.
    Notations are defined as follows. SPI: spiculation; MAL: malignancy; SUB: subtlety;
    CAL: calcification; MAR: margin; LOB: lobulation.'
  Figure 5 Link: articels_figures_by_rev_year\2021\Diagnose_Like_a_Radiologist_Hybrid_NeuroProbabilistic_Reasoning_for_AttributeBas\figure_5.jpg
  Figure 5 caption: 'Sample chest X-ray images used for building the TB-Xatt dataset,
    each with one of the following seven attributes except the last one: Fibrotic
    Streaks, Pulmonary Consolidation, Diffuse Nodules, Pulmonary Cavitation, Atelectasis,
    Multiple Nodules, and Pleural Effusion. The last image shows a case of the disease,
    Pulmonary Tuberculosis.'
  Figure 6 Link: articels_figures_by_rev_year\2021\Diagnose_Like_a_Radiologist_Hybrid_NeuroProbabilistic_Reasoning_for_AttributeBas\figure_6.jpg
  Figure 6 caption: Construction of image samples in the TB-Xatt dataset. First, lung
    segmentation in chest X-ray images using a trained segmentation model. Second,
    resizing the left and right lung patches to the same resolution and placing them
    side by side.
  Figure 7 Link: articels_figures_by_rev_year\2021\Diagnose_Like_a_Radiologist_Hybrid_NeuroProbabilistic_Reasoning_for_AttributeBas\figure_7.jpg
  Figure 7 caption: "Sample results of our method (O2) and four baseline methods (T1,\
    \ T2, M41, M42) on the in-house TB-Xatt dataset. Short names are defined as follows:\
    \ FIB (Fibrotic Streaks), CON (Pulmonary Consolidation), DIF (Diffuse Nodules),\
    \ CAV (Pulmonary Cavitation), ATE (Atelectasis), MUL (Multiple Nodules), and PLE\
    \ (Pleural Effusion). The colored bars visualize attribute classification results\
    \ from individual methods as well as the ground truth. GT stands for the ground\
    \ truth. \u2713 and \xD7 visualize the diagnoses of pulmonary tuberculosis from\
    \ individual methods as well as the ground truth. The histograms under the X-ray\
    \ images visualize the importance of individual attributes for diagnosing pulmonary\
    \ tuberculosis."
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.67
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.76
  Name of the first author: Gangming Zhao
  Name of the last author: Yizhou Yu
  Number of Figures: 7
  Number of Tables: 10
  Number of authors: 5
  Paper title: 'Diagnose Like a Radiologist: Hybrid Neuro-Probabilistic Reasoning
    for Attribute-Based Medical Image Diagnosis'
  Publication Date: 2021-11-25 00:00:00
  Table 1 caption: TABLE 1 Performance Comparison of Lung Nodule Classification Models
    on the LIDC-IDRI Dataset
  Table 10 caption: TABLE 10 Classification Accuracy of Input and Output Signals of
    BN-1 and GCN on TB-Xatt Dataset
  Table 2 caption: TABLE 2 Comparison With Existing Lung Nodule Classification Models
    Under a Second Training and Testing Protocol
  Table 3 caption: TABLE 3 Distribution of Median Malignancy Levels (MML) in the LIDC-IDRI
    Dataset for Lung Nodule Classification
  Table 4 caption: TABLE 4 Ablation Study of Our Classification Algorithm on the LIDC-IDRI
    Dataset
  Table 5 caption: TABLE 5 Distribution of Disease and Radiological Abnormalities
    in the TB-Xatt Dataset for Tuberculosis Diagnosis
  Table 6 caption: TABLE 6 Performance Comparison of Tuberculosis Diagnosis Models
    on the TB-Xatt Dataset
  Table 7 caption: TABLE 7 Ablation Study of Our Classification Algorithm on the TB-Xatt
    Dataset
  Table 8 caption: TABLE 8 Performance Comparison of Our Tuberculosis Diagnosis Models
    Trained With Increasingly Smaller Datasets
  Table 9 caption: TABLE 9 Specification of Datasets Used for the Performance Comparison
    in Table 8
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3130759
- Affiliation of the first author: "jagiellonian university, krak\xF3w, poland"
  Affiliation of the last author: warsaw university of technology, warsaw, poland
  Figure 1 Link: articels_figures_by_rev_year\2021\General_Hypernetwork_Framework_for_Creating_D_Point_Clouds\figure_1.jpg
  Figure 1 caption: Mesh representations generated by our HyperCloud method. Contrary
    to the existing methods that return point cloud representations sparsely distributed
    in 3D space, our approach allows creating a continuous 3D object representation
    in the form of meshes.
  Figure 10 Link: articels_figures_by_rev_year\2021\General_Hypernetwork_Framework_for_Creating_D_Point_Clouds\figure_10.jpg
  Figure 10 caption: Comparison of training times and GPU memory used by PointFlow
    and HyperFlow. Our HyperFlow method offers over an order of magnitude decrease
    in both training time and memory.
  Figure 2 Link: articels_figures_by_rev_year\2021\General_Hypernetwork_Framework_for_Creating_D_Point_Clouds\figure_2.jpg
  Figure 2 caption: 'Top: The baseline approach for generating 3D point clouds returns
    a fixed number of points [10]. Bottom: Our HyperCloud method leverages a hypernetwork
    architecture that takes a 3D point cloud as an input while returning the parameters
    of the target network. Since the parameters of the target network are generated
    by a hypernetwork, the output dataset can be variable in size. We can sample any
    number of points from the uniform distribution on a 3D ball and transfer them
    to surface of an object. As a result, we obtain a continuous parametrization of
    the surface of the object and a more robust representation of its mesh.'
  Figure 3 Link: articels_figures_by_rev_year\2021\General_Hypernetwork_Framework_for_Creating_D_Point_Clouds\figure_3.jpg
  Figure 3 caption: Scheme of producing mesh representations with HyperCloud. When
    using 3D ball distribution, our method can generate 3D point clouds filled with
    data points, while when given 3D sphere distribution, it transforms samples from
    the sphere to surfaces of 3D objects - a feature highly desirable in the context
    of 3D mesh rendering.
  Figure 4 Link: articels_figures_by_rev_year\2021\General_Hypernetwork_Framework_for_Creating_D_Point_Clouds\figure_4.jpg
  Figure 4 caption: Interpolations between two 3D point clouds and its mesh representations.
  Figure 5 Link: articels_figures_by_rev_year\2021\General_Hypernetwork_Framework_for_Creating_D_Point_Clouds\figure_5.jpg
  Figure 5 caption: 3D point clouds and their mesh representations produced by HyperCloud.
  Figure 6 Link: articels_figures_by_rev_year\2021\General_Hypernetwork_Framework_for_Creating_D_Point_Clouds\figure_6.jpg
  Figure 6 caption: Example of interpolation of the 3D object representation space
    in the target network. Our hypernetwork architecture allows us to work with a
    single object, represented as a distribution of points on a single 3D point cloud,
    hence we can browse the space of potential 3D objects by interpolating representation
    space in the target network, instead of doing so in the latent auto-encoder space,
    as typically done.
  Figure 7 Link: articels_figures_by_rev_year\2021\General_Hypernetwork_Framework_for_Creating_D_Point_Clouds\figure_7.jpg
  Figure 7 caption: We compare how the prior density is modified for the model with
    a Gaussian prior (the upper two rows) and Spherical Log-Normal (the bottom two
    rows). In the first and the third row we show how the flow model transforms the
    original density into the target dataset. The second and the fourth row show the
    cross-sections along the plane depicted by red points. For the Gaussian distribution,
    target space points are not distributed evenly across the object (a central part
    of Gaussian distribution is transformed into the bottom of the plane, while its
    tails are used to model wingtips). For the Spherical Log-Normal, target space
    points are distributed evenly across the object, showcasing that our approach
    truly models the distribution of the points along object surfaces.
  Figure 8 Link: articels_figures_by_rev_year\2021\General_Hypernetwork_Framework_for_Creating_D_Point_Clouds\figure_8.jpg
  Figure 8 caption: "Level sets and samples from the Spherical Log-Normal distribution\
    \ with different parameters \u03C3 and \u03BC=0 . Since the Spherical Log-Normal\
    \ distribution does not have compact support, it can be used in flow-based architectures."
  Figure 9 Link: articels_figures_by_rev_year\2021\General_Hypernetwork_Framework_for_Creating_D_Point_Clouds\figure_9.jpg
  Figure 9 caption: Mesh representations generated by our HyperFlow method. Contrary
    to the existing methods that return point cloud representations sparsely distributed
    in 3D space, our approach allows creating a continuous 3D object representation
    in the form of meshes.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Przemyslaw Spurek
  Name of the last author: Tomasz Trzcinski
  Number of Figures: 11
  Number of Tables: 4
  Number of authors: 4
  Paper title: General Hypernetwork Framework for Creating 3D Point Clouds
  Publication Date: 2021-11-26 00:00:00
  Table 1 caption: TABLE 1 Generation Results
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Unsupervised Feature Learning
  Table 3 caption: TABLE 3 The Values of Quality Measures of 3D Representations Obtained
    by Sampling From a Sphere with a Given Radius R R for Airplane, Chair and Car
    Shapes
  Table 4 caption: TABLE 4 Reconstruction Results
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3131131
- Affiliation of the first author: college of information science and technologycollege
    of cyber security, jinan university, guangzhou, china
  Affiliation of the last author: college of information science and technologycollege
    of cyber security, jinan university, guangzhou, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Reversible_Data_Hiding_By_Using_CNN_Prediction_and_Adaptive_Embedding\figure_1.jpg
  Figure 1 caption: 'The proposed division method: (a) the cover image is first divided
    into the cross set and dot set, (b) the cover image is then divided into the black
    block and white block.'
  Figure 10 Link: articels_figures_by_rev_year\2021\Reversible_Data_Hiding_By_Using_CNN_Prediction_and_Adaptive_Embedding\figure_10.jpg
  Figure 10 caption: The histograms of prediction errors of the test set with the
    proposed predictor, CNNP [22], BIP [9], MEDP [6], GAP [7], and DP [5].
  Figure 2 Link: articels_figures_by_rev_year\2021\Reversible_Data_Hiding_By_Using_CNN_Prediction_and_Adaptive_Embedding\figure_2.jpg
  Figure 2 caption: 'Illustration on dividing the cover image into four independent
    parts: (a) black-cross set, (b) black-dot set, (c) white-cross set, and (d) white-dot
    set.'
  Figure 3 Link: articels_figures_by_rev_year\2021\Reversible_Data_Hiding_By_Using_CNN_Prediction_and_Adaptive_Embedding\figure_3.jpg
  Figure 3 caption: 'Architecture of the proposed predictor: (a) the structure of
    the proposed CNN-based predictor, (b) the structure of the FeatureBlock, and (c)
    the structure of the PredictBlock.'
  Figure 4 Link: articels_figures_by_rev_year\2021\Reversible_Data_Hiding_By_Using_CNN_Prediction_and_Adaptive_Embedding\figure_4.jpg
  Figure 4 caption: 'Two situations of the proposed background complexity calculation
    method: (a) the distribution of available neighboring pixels around the cross
    set pixel, (b) the distribution of available neighboring pixels around the dot
    set pixel.'
  Figure 5 Link: articels_figures_by_rev_year\2021\Reversible_Data_Hiding_By_Using_CNN_Prediction_and_Adaptive_Embedding\figure_5.jpg
  Figure 5 caption: 'Two-dimensional mapping: (a) the error pairs histogram of the
    image Lena generated by the proposed PEO strategy, and (b) the basic mapping rules
    of the 2D mapping, where the red points are not defined.'
  Figure 6 Link: articels_figures_by_rev_year\2021\Reversible_Data_Hiding_By_Using_CNN_Prediction_and_Adaptive_Embedding\figure_6.jpg
  Figure 6 caption: The flow chart of embedding the information and generating the
    data hidden image by using the proposed RDH method.
  Figure 7 Link: articels_figures_by_rev_year\2021\Reversible_Data_Hiding_By_Using_CNN_Prediction_and_Adaptive_Embedding\figure_7.jpg
  Figure 7 caption: The flow chart of extracting the information and recovering the
    cover image by using the proposed RDH method.
  Figure 8 Link: articels_figures_by_rev_year\2021\Reversible_Data_Hiding_By_Using_CNN_Prediction_and_Adaptive_Embedding\figure_8.jpg
  Figure 8 caption: An example to embed and extract bits in a divided segment with
    13 pixels by using the proposed RDH method.
  Figure 9 Link: articels_figures_by_rev_year\2021\Reversible_Data_Hiding_By_Using_CNN_Prediction_and_Adaptive_Embedding\figure_9.jpg
  Figure 9 caption: The MSE of the proposed CNN-based predictor on the test set and
    the time required for one training epoch under different training sets.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Runwen Hu
  Name of the last author: Shijun Xiang
  Number of Figures: 13
  Number of Tables: 3
  Number of authors: 2
  Paper title: Reversible Data Hiding By Using CNN Prediction and Adaptive Embedding
  Publication Date: 2021-11-30 00:00:00
  Table 1 caption: TABLE 1 The Mean, Variance, and MSE of the Absolute Prediction
    Errors in the Validation and Test Sets Under the Proposed One and Five Typical
    Predictors (CNNP [22], BIP [9], MEDP [6], GAP [7], and DP [5])
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 PSNR Values (in dB) of 8 Classical Images of the Test Set
    Generated by the Proposed RDH Method, LPVO [36], Pairwise IPVO [34], Pairwise
    PEE [23], Hybrid-PEE [24], and Skewed-HS [29] With Embedding Capacity of 10,000
    Bits and 20,000 Bits
  Table 3 caption: TABLE 3 PSNR Values (in dB) of the Tested Kodak Dataset Generated
    by the Proposed RDH Method, LPVO [36], Pairwise IPVO [34], Pairwise PEE [23],
    Hybrid-PEE [24], and Skewed-HS [29] With an Embedding Capacity of 10,000 Bits
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3131250
- Affiliation of the first author: guangdong university of technology, guangzhou,
    guangdong, china
  Affiliation of the last author: sun yat-sen university, guangzhou, guangdong, china
  Figure 1 Link: articels_figures_by_rev_year\2021\CrossDomain_Facial_Expression_Recognition_A_Unified_Evaluation_Benchmark_and_Adv\figure_1.jpg
  Figure 1 caption: The upper part presents two examples from CK+ [5] and RAF-DB [10],
    and the lower part presents the local mouth-corner and eye regions of the images
    from CK+ and RAF-DB.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\CrossDomain_Facial_Expression_Recognition_A_Unified_Evaluation_Benchmark_and_Adv\figure_2.jpg
  Figure 2 caption: Illustration of a feature distribution learned by the baseline
    adversarial learning [31] method that merely uses holistic features (left) and
    our proposed AGRA framework (right). It is obvious that the AGRA framework can
    better gather the samples of the same category and from different domains together
    than the baseline method, suggesting that our framework can learn more discriminative
    domain-invariant features for CD-FER.
  Figure 3 Link: articels_figures_by_rev_year\2021\CrossDomain_Facial_Expression_Recognition_A_Unified_Evaluation_Benchmark_and_Adv\figure_3.jpg
  Figure 3 caption: Visualization of samples from the CK+, JAFFE, SFEW2.0, FER2013,
    ExpW, and RAF-DB datasets. The examples from different datasets differ in appearance,
    color, and view point.
  Figure 4 Link: articels_figures_by_rev_year\2021\CrossDomain_Facial_Expression_Recognition_A_Unified_Evaluation_Benchmark_and_Adv\figure_4.jpg
  Figure 4 caption: (a) Accuracies of the cross-dataset evaluation using the ResNet-50
    baseline and (b) the accuracies of cross-dataset evaluation using the ResNet-50,
    ResNet-18, and MobileNet-v2 baselines.
  Figure 5 Link: articels_figures_by_rev_year\2021\CrossDomain_Facial_Expression_Recognition_A_Unified_Evaluation_Benchmark_and_Adv\figure_5.jpg
  Figure 5 caption: The MMD comparisons between features of the source RAF-DB and
    those of each target CK+, JAFFE, FER2013, SFEW2.0, ExpW for the baseline with
    holistic features (BH), baseline with local features (BL), and baseline with holistic-local
    features (BHL).
  Figure 6 Link: articels_figures_by_rev_year\2021\CrossDomain_Facial_Expression_Recognition_A_Unified_Evaluation_Benchmark_and_Adv\figure_6.jpg
  Figure 6 caption: An illustration of the proposed AGRA framework. The framework
    builds two graphs to correlate holistic-local features within each domain and
    across different domains, initializes the graph nodes with input image features
    of a certain domain and the learnable statistical distribution of the other domain,
    and introduces two stacked GCNs to propagate node information with each domain
    and transfer node messages across different domains for holistic-local feature
    co-adaptation. Note that the nodes in the intra-domain and inter-domain graphs
    are the same, and we arrange them in different layouts for a clearer illustration
    of the connections. The feature extractor F and domain discriminator D are also
    the same for the source and target domains, and we present two weight-sharing
    branches merely for simple illustration.
  Figure 7 Link: articels_figures_by_rev_year\2021\CrossDomain_Facial_Expression_Recognition_A_Unified_Evaluation_Benchmark_and_Adv\figure_7.jpg
  Figure 7 caption: The average accuracy comparisons using the RAF-DB and AFE datasets
    as the source dataset. We average the accuracies of all the methods using ResNet-50
    (left), ResNet-18 (middle), and MobileNet-v2 (right) as the backbone.
  Figure 8 Link: articels_figures_by_rev_year\2021\CrossDomain_Facial_Expression_Recognition_A_Unified_Evaluation_Benchmark_and_Adv\figure_8.jpg
  Figure 8 caption: The average accuracy comparisons using ResNet-50, ResNet-18, and
    MobileNet-v2 as the backbone. We average the accuracies of all the methods using
    the RAF-DB (left) and AFE (right) datasets as the source datasets.
  Figure 9 Link: articels_figures_by_rev_year\2021\CrossDomain_Facial_Expression_Recognition_A_Unified_Evaluation_Benchmark_and_Adv\figure_9.jpg
  Figure 9 caption: Illustration of the feature distributions learned by our proposed
    approach at epochs 0, 5, 10, 15, and 20 (from left to right) on the CK+ (upper)
    and SFEW2.0 (lower) datasets.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Tianshui Chen
  Name of the last author: Liang Lin
  Number of Figures: 9
  Number of Tables: 11
  Number of authors: 6
  Paper title: 'Cross-Domain Facial Expression Recognition: A Unified Evaluation Benchmark
    and Adversarial Graph Learning'
  Publication Date: 2021-11-30 00:00:00
  Table 1 caption: TABLE 1 Evaluation Settings and Accuracies of Current Leading CD-FER
    Methods on the CK+, JAFFE, SFEW2.0, FER2013, and ExpW Datasets
  Table 10 caption: TABLE 10 The Multiply-Accumulate Operations (MACs) and Running
    Time of the Proposed Framework with and without the GCNs (Ours W GCN and Ours
    WO GCN)
  Table 2 caption: TABLE 2 Accuracies of Our Proposed Framework with Current Leading
    Methods on the CK+, JAFFE, SFEW2.0, FER2013, and ExpW Datasets
  Table 3 caption: TABLE 3 Accuracies of Our Approach Using Holistic Features (HFs),
    Concatenating Holistic-Local Features (HLFs) and Ours for Adaptation on the CK+,
    JAFFE, SFEW2.0, FER2013, and ExpW Datasets
  Table 4 caption: TABLE 4 Accuracies of Our Approach Using Only the Intra-Domain
    GCN (Ours Intra-GCN), Using Only the Inter-Domain GCN (Ours Inter-GCN), Using
    Only One GCN (Ours Single GCN), and Our Original Approach (Ours) on the CK+, JAFFE,
    SFEW2.0, FER2013, and ExpW Datasets
  Table 5 caption: TABLE 5 Accuracies of Our Approach with the Mean Statistical Distribution
    (Ours Mean), Our Approach and Updating the Per-Class Statistical Distributions
    Every Iteration (Ours Iter), Our Approach and Updating the Per-Class Statistical
    Distributions Every Ten Epochs (Ours Epoch) and Our Original Approach (Ours) on
    the CK+, JAFFE, SFEW2.0, FER2013, and ExpW Datasets
  Table 6 caption: TABLE 6 Accuracies of Our Approach Where the Matrices are Initialized
    with Randomly Initialized Matrices (Ours RM), Our Approach Where the Matrices
    are Initialized with All-One Matrices (Ours OM), Our Approach Where the Matrices
    are Initialized with Fixed Matrices (Ours FM), and Our Original Approach (Ours)
    on the CK+, JAFFE, SFEW2.0, FER2013, and ExpW Datasets
  Table 7 caption: TABLE 7 Accuracies of Our Approach with Different Numbers of Iterations
    for the Intra-Domain GCN and Inter-Domain GCN on the CK+, JAFFE, SFEW2.0, FER2013,
    and ExpW Datasets
  Table 8 caption: TABLE 8 Accuracies of Our Approach That Uses GCN-Based (Ours GCN),
    Neural Tensor (Ours NT), and Transformer-Based (Ours TR) Fusion Algorithms on
    the CK+, JAFFE, SFEW2.0, FER2013, and ExpW Datasets
  Table 9 caption: TABLE 9 Accuracies of Our Approach with and without Adversarial
    Learning (Ours W AL and Ours WO AL, Respectively) on the CK+, JAFFE, SFEW2.0,
    FER2013, and ExpW Datasets
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3131222
- Affiliation of the first author: embedded deep learning and visual analysis group,
    school of information science and technology, fudan university, shanghai, china
  Affiliation of the last author: academy for engineering and technology, fudan university,
    shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Point_Cloud_Instance_Segmentation_With_SemiSupervised_BoundingBox_Mining\figure_1.jpg
  Figure 1 caption: Given the input point cloud, we propose the first weakly- and
    semi-supervised point cloud instance segmentation architecture with bounding box
    supervision. Bounding box proposals are generated first to group the input point
    cloud into some point subsets. Afterwards, we predict accurate instance masks
    with our proposed modules, including semantic propagation component, property
    consistency graph, and occupancy ratio guided refinement.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Point_Cloud_Instance_Segmentation_With_SemiSupervised_BoundingBox_Mining\figure_2.jpg
  Figure 2 caption: Illustration of the SPIB architecture. From an input point cloud
    P , we first generate bounding box proposals through the proposal generation network
    SPCR. We then conduct an in-box indexing to select a fixed K number of point subsets
    both in the original input point cloud and the learned semantic score map. Within
    these point subsets, we predict instance masks through our proposed semantic propagation
    component and property consistency graph, thus produce K instance candidates.
    Afterwards, category-based statistics of occupancy ratio are computed for C semantic
    classes, followed by class-specific occupancy ratio guided refinement for each
    instance candidate. Finally, we remove duplicate instances by 3D Non-Maximum Suppression
    (NMS) and output the final 3D instances.
  Figure 3 Link: articels_figures_by_rev_year\2021\Point_Cloud_Instance_Segmentation_With_SemiSupervised_BoundingBox_Mining\figure_3.jpg
  Figure 3 caption: "The network architecture of our proposed SPCR. Given the original\
    \ input point cloud (either labeled or unlabeled) and their transformed point\
    \ cloud, we first generate bounding box proposals through the Siamese network.\
    \ The predictions of the original point cloud are then transformed with the same\
    \ perturbation \u03A6 . Finally, these bounding box proposals are optimized by\
    \ the supervised loss and our proposed perturbation consistency regularization\
    \ mechanism."
  Figure 4 Link: articels_figures_by_rev_year\2021\Point_Cloud_Instance_Segmentation_With_SemiSupervised_BoundingBox_Mining\figure_4.jpg
  Figure 4 caption: Illustration of semantic propagation. Given the predicted discriminative
    points (red) of each object, we propagate its seed region by adding the highly
    similar points (purple) within the neighborhood balls of discriminative points.
    Points are enlarged for better visibility.
  Figure 5 Link: articels_figures_by_rev_year\2021\Point_Cloud_Instance_Segmentation_With_SemiSupervised_BoundingBox_Mining\figure_5.jpg
  Figure 5 caption: Examples of different classes instance predictions. The predicted
    instance points are denoted in green and non-instance points within the bounding
    box are shown in gray. It is obvious that the occupancy ratios of the same class
    are similar and different classes vary from each other.
  Figure 6 Link: articels_figures_by_rev_year\2021\Point_Cloud_Instance_Segmentation_With_SemiSupervised_BoundingBox_Mining\figure_6.jpg
  Figure 6 caption: Qualitative results on ScanNet v2 dataset. SP represents semantic
    propagation component, PC represents property consistency graph, and OR represents
    occupancy ratio guided refinement. Different colors represent separate instances.
    The color of ground truth and predicted instances are not necessary to be the
    same.
  Figure 7 Link: articels_figures_by_rev_year\2021\Point_Cloud_Instance_Segmentation_With_SemiSupervised_BoundingBox_Mining\figure_7.jpg
  Figure 7 caption: Occupancy ratio using ground truth bounding boxes versus the predicted
    bounding boxes.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.55
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.9
  Name of the first author: Yongbin Liao
  Name of the last author: Jiayuan Fan
  Number of Figures: 7
  Number of Tables: 6
  Number of authors: 6
  Paper title: Point Cloud Instance Segmentation With Semi-Supervised Bounding-Box
    Mining
  Publication Date: 2021-11-30 00:00:00
  Table 1 caption: TABLE 1 Comparison of Different Label Ratios With VoteNet and SPCR
    on ScanNet v2 Val Set
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison of Fully Supervised Methods on ScanNet v2
  Table 3 caption: TABLE 3 Comparison With Fully Supervised Methods
  Table 4 caption: TABLE 4 Comparison of Semi-Supervised Learning With VoteNet on
    ScanNet v2 Val Set
  Table 5 caption: TABLE 5 Comparison on the Data-Efficient Detection Benchmark of
    ScanNet
  Table 6 caption: TABLE 6 Ablation Study of Property Consistency Term on the ScanNet
    v2 Validation Set
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3131120
- Affiliation of the first author: meituan, beijing, china
  Affiliation of the last author: department of computer science and technology, tsinghua
    university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Syntax_Customized_Video_Captioning_by_Imitating_Exemplar_Sentences\figure_1.jpg
  Figure 1 caption: The syntax customized video captioning task by imitating different
    exemplar sentences. It can be observed that the generated syntax customized video
    captions by our model can not only precisely depict the video contents, but also
    highly resemble the syntactic structures of the given exemplar sentences.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Syntax_Customized_Video_Captioning_by_Imitating_Exemplar_Sentences\figure_2.jpg
  Figure 2 caption: Framework of our proposed model for the syntax customized video
    captioning task, which consists of three fully coupled components, i.e., a hierarchical
    sentence syntax encoder extracting the syntactic structure of the exemplar sentence,
    a video semantic encoder yielding the video semantic representation, and a syntax
    conditioned caption decoder relying on the encoded syntactic and semantic information
    to produce the syntax customized video caption. Best viewed in color.
  Figure 3 Link: articels_figures_by_rev_year\2021\Syntax_Customized_Video_Captioning_by_Imitating_Exemplar_Sentences\figure_3.jpg
  Figure 3 caption: Our proposed training strategy leverages (a) the pairwise video
    captioning data and (b) our collected auxiliary exemplar sentences to train our
    proposed model. The shadowed blocks are shared when training.
  Figure 4 Link: articels_figures_by_rev_year\2021\Syntax_Customized_Video_Captioning_by_Imitating_Exemplar_Sentences\figure_4.jpg
  Figure 4 caption: Qualitative results for the syntax customized video captioning.
    For each case, we provide the video, the groundtruth caption, and the caption
    generated by the Seq2Seq model. Two exemplar sentences are given to each video.
    The Template, GFN, and our predicted syntax customized video captions are also
    present correspondingly.
  Figure 5 Link: articels_figures_by_rev_year\2021\Syntax_Customized_Video_Captioning_by_Imitating_Exemplar_Sentences\figure_5.jpg
  Figure 5 caption: The illustration of correlation between the number of object (verb,
    noun) words in captions and the exemplar sentence lengths. In each scatter map,
    the x -axis indicates the lengths of exemplar sentences, and the y-axis indicate
    the number of noun, verb, and noun+verb words in our generated syntax customized
    video captions, respectively.
  Figure 6 Link: articels_figures_by_rev_year\2021\Syntax_Customized_Video_Captioning_by_Imitating_Exemplar_Sentences\figure_6.jpg
  Figure 6 caption: In this figure, we present the ablation studies of the proposed
    model by examining the contributions on (a) the loss components, (b) the sentence
    syntax encoding, and (c) the caption decoding.
  Figure 7 Link: articels_figures_by_rev_year\2021\Syntax_Customized_Video_Captioning_by_Imitating_Exemplar_Sentences\figure_7.jpg
  Figure 7 caption: More qualitative results on the MSRVTT dataset. For each video,
    we provide one groundtruth caption, one exemplar sentence, and the syntax customized
    video caption predicted by our proposed model.
  Figure 8 Link: articels_figures_by_rev_year\2021\Syntax_Customized_Video_Captioning_by_Imitating_Exemplar_Sentences\figure_8.jpg
  Figure 8 caption: More qualitative results on the ActivityNet Captions dataset.
    For each video, we provide one groundtruth caption, one exemplar sentence, and
    the syntax customized video caption predicted by our proposed model.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.77
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.92
  Name of the first author: Yitian Yuan
  Name of the last author: Wenwu Zhu
  Number of Figures: 8
  Number of Tables: 5
  Number of authors: 3
  Paper title: Syntax Customized Video Captioning by Imitating Exemplar Sentences
  Publication Date: 2021-11-30 00:00:00
  Table 1 caption: TABLE 1 Performance Comparisons on MSRVTT and ActivityNet Captions
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Human Evaluation of Captioning
  Table 3 caption: TABLE 3 Captioning Diversity Evaluation
  Table 4 caption: TABLE 4 Evaluation Results in the Conventional Captioning Metrics
    (%)
  Table 5 caption: TABLE 5 Model Performance With Different Word Replacement Probability
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3131618
- Affiliation of the first author: department of radiation oncology, university of
    california san francisco, san francisco, ca, usa
  Affiliation of the last author: division of biostatistics, university of california,
    berkeley, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\The_Conditional_Super_Learner\figure_1.jpg
  Figure 1 caption: 'This diagram shows an application of the CLS model. In this dataset
    we have 4 variables: number of bedrooms, bathrooms, latitude and longitude to
    predict house prices. The oracle was given two of the variables (longitude and
    latitude). The rectangular region shows how the oracle divides the latitude and
    longitude (normalized) in 3 regions. Each region has its own expert (using the
    all 4 variables), represented here by a diagram of a tree, to makes predictions.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\The_Conditional_Super_Learner\figure_2.jpg
  Figure 2 caption: One shot CSL versus cross validation. Partitioning the space in
    two regions with a depth = 1 decision tree and choosing a model for each result
    in an improvement, on average, over cross-validaiton.
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Gilmer Valdes
  Name of the last author: Mark Van der Laan
  Number of Figures: 2
  Number of Tables: 4
  Number of authors: 4
  Paper title: The Conditional Super Learner
  Publication Date: 2021-12-01 00:00:00
  Table 1 caption: TABLE 1 Description of the Number of Observations, Number of Covariates
    and number of Classes in the Classification Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Results Comparing CSL and Stacking on 19 Regression Datasets
  Table 3 caption: TABLE 3 Results Comparing CSL and Stacking on 5 Classification
    Datasets
  Table 4 caption: TABLE 4 Results on Running CSL on Synthetic Data
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3131976
- Affiliation of the first author: faculty of engineering, bar-ilan university, ramat
    gan, israel
  Affiliation of the last author: faculty of engineering, bar-ilan university, ramat
    gan, israel
  Figure 1 Link: articels_figures_by_rev_year\2021\Learning_to_Embed_Semantic_Similarity_for_Joint_ImageText_Retrieval\figure_1.jpg
  Figure 1 caption: Image caption retrieval (image to text), using the Flickr30K dataset
    [5]. The embedding of an image is compared with the embedding of all sentences
    in the dataset, and the closest ten captions are presented. The sentences marked
    in bold are the ground truth matches having identical semantic content, in contrast
    to the other sentences having similar content.
  Figure 10 Link: articels_figures_by_rev_year\2021\Learning_to_Embed_Semantic_Similarity_for_Joint_ImageText_Retrieval\figure_10.jpg
  Figure 10 caption: An example of failed image annotation (image to text retrieval)
    taken from the Flickr8K dataset. The top ten retrieval results bear partial semantic
    similarity to the image content.
  Figure 2 Link: articels_figures_by_rev_year\2021\Learning_to_Embed_Semantic_Similarity_for_Joint_ImageText_Retrieval\figure_2.jpg
  Figure 2 caption: "The proposed joint embedding scheme. The outputs of the image\
    \ and text branches are x and y \u02C6 , respectively. We used multiple different\
    \ backbones. The backbone shown follows [10], [13]. The subnetwork to the right\
    \ of x and y \u02C6 is common to all our architectures, and is the core of our\
    \ approach. x and y \u02C6 are connected to separate cross-entropy losses labeled\
    \ by their given set s n . A triplet hinge loss with adaptive margins, as in Section\
    \ 3.5, is used to merge the embeddings. The unquantized embedding is computed\
    \ by directly connecting x and y \u02C6 to the Center Loss as in Section 3.3,\
    \ to compute the semantic centers c n N 1 . For the quantized embedding as in\
    \ Section 3.4, x and y \u02C6 are connected to the \u201C N q FC\u201D and Softmax\
    \ layers to compute N q \u226AN embedding centers c \u02C6 n N q 1 ."
  Figure 3 Link: articels_figures_by_rev_year\2021\Learning_to_Embed_Semantic_Similarity_for_Joint_ImageText_Retrieval\figure_3.jpg
  Figure 3 caption: 'Image search (text to image retrieval) example taken from the
    Flickr30K dataset. The query caption appears on top of the images. The text embedding
    was compared to the embeddings of all of the images in the dataset. The ten top
    ranked images are presented. The retrieved images are depicted according to their
    similarity: left-to-right and then top-down. The most similar image in on the
    upper-left corner, and is the correct retrieval result.'
  Figure 4 Link: articels_figures_by_rev_year\2021\Learning_to_Embed_Semantic_Similarity_for_Joint_ImageText_Retrieval\figure_4.jpg
  Figure 4 caption: Image annotation (image to text retrieval) example taken from
    the Flickr30K dataset. The image embedding was compared to the embeddings of all
    captions in the dataset. The ten top ranked captions are presented, and the correctly
    retrieved ones are marked in bold.
  Figure 5 Link: articels_figures_by_rev_year\2021\Learning_to_Embed_Semantic_Similarity_for_Joint_ImageText_Retrieval\figure_5.jpg
  Figure 5 caption: An example of a failed image annotation (image to text retrieval)
    taken from the Flickr30K dataset. The top ten retrieval results bear partial semantic
    similarity to the image content.
  Figure 6 Link: articels_figures_by_rev_year\2021\Learning_to_Embed_Semantic_Similarity_for_Joint_ImageText_Retrieval\figure_6.jpg
  Figure 6 caption: Image annotation (image to text retrieval) example taken from
    the MS-COCO dataset. The image embedding was compared to the embeddings of all
    captions in the dataset. The ten top ranked captions are presented, and the correctly
    retrieved ones are marked in bold.
  Figure 7 Link: articels_figures_by_rev_year\2021\Learning_to_Embed_Semantic_Similarity_for_Joint_ImageText_Retrieval\figure_7.jpg
  Figure 7 caption: 'Image search (text to image retrieval) example taken from the
    MS-COCO dataset. The query caption appears on top of the images. The text embedding
    was compared to the embeddings of all of the images in the dataset. The ten top
    ranked images are presented. The retrieved images are depicted according to their
    similarity: left-to-right and then top-down. The most similar image in on the
    upper-left corner, and is the correct result.'
  Figure 8 Link: articels_figures_by_rev_year\2021\Learning_to_Embed_Semantic_Similarity_for_Joint_ImageText_Retrieval\figure_8.jpg
  Figure 8 caption: An example of failed image annotation (image to text retrieval)
    taken from the MS-COCO dataset. The top ten retrieval results bear partial semantic
    similarity to the image content.
  Figure 9 Link: articels_figures_by_rev_year\2021\Learning_to_Embed_Semantic_Similarity_for_Joint_ImageText_Retrieval\figure_9.jpg
  Figure 9 caption: Image annotation (image to text retrieval) example taken from
    the Flickr8K dataset. The image embedding was compared to the embeddings of all
    captions in the dataset. The ten top ranked captions are presented, and the correctly
    retrieved ones are marked in bold.
  First author gender probability: 0.95
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.67
  Name of the first author: Noam Malali
  Name of the last author: Yosi Keller
  Number of Figures: 11
  Number of Tables: 11
  Number of authors: 2
  Paper title: Learning to Embed Semantic Similarity for Joint Image-Text Retrieval
  Publication Date: 2021-12-02 00:00:00
  Table 1 caption: TABLE 1 The Layers of the Image Embedding Branch Corresponding
    to H i , Hi, as Depicted in Fig. 2
  Table 10 caption: TABLE 10 An Ablation Study of the Text Embedding
  Table 2 caption: TABLE 2 The Layers of the Text Embedding Branch Corresponding to
    H t , Ht, as Depicted in Fig. 2
  Table 3 caption: TABLE 3 The Accuracy of the Image Annotation and Search Tasks Evaluated
    Using the Flickr30K Dataset
  Table 4 caption: TABLE 4 The Accuracy of the Image Annotation and Search Tasks Evaluated
    Using the MS-COCO Dataset 1K Test
  Table 5 caption: TABLE 5 The Accuracy of the Image Annotation and Search Tasks Evaluated
    Using the MS-COCO Dataset 5K Test
  Table 6 caption: TABLE 6 The Accuracy of the Image Annotation and Search Tasks Evaluated
    Using the Flickr8K Dataset
  Table 7 caption: TABLE 7 Ablation Study of the Semantic Centers, and the Cross-Entropy
    (CE) Losses, Using the Flickr8K and Flickr30K Datasets
  Table 8 caption: TABLE 8 An Ablation Study of the Configurations of the Semantic
    Center Loss
  Table 9 caption: TABLE 9 Ablation Study Results for Number of Centers for the Quantization
    Phase
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3132163
- Affiliation of the first author: ningbo institute, and school of software, northwestern
    polytechnical university, xian, shaanxi, china
  Affiliation of the last author: university of adelaide, adelaide, sa, australia
  Figure 1 Link: articels_figures_by_rev_year\2021\Structured_Multimodal_Attentions_for_TextVQA\figure_1.jpg
  Figure 1 caption: "(Left) In Question Conditioned Graph Attention Module, we construct\
    \ a heterogeneous graph. We illustrate objects in yellow and OCR tokens in red.\
    \ While unbroken-line boxes represent nodes most relevant to the question, dashed-line\
    \ ones are other nodes. Understanding multiple relationships is crucial to answer\
    \ this question, e.g., \u201Cword above the number 12\u201D is a text-text relation.\
    \ (Right) Abstract entities and relationships. Red diamonds are OCR tokens and\
    \ yellow rectangles are objects. Blue arrows are relationships."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Structured_Multimodal_Attentions_for_TextVQA\figure_2.jpg
  Figure 2 caption: Three modules in our SMA model. Question Self-Attention Module
    decomposes the question into guiding signals that guide Graph Attention Module
    to reason over a graph, and a Global-Local Attentional Answering module to generate
    an answer.
  Figure 3 Link: articels_figures_by_rev_year\2021\Structured_Multimodal_Attentions_for_TextVQA\figure_3.jpg
  Figure 3 caption: 'An overview of Question Self-Attention Module. Input word sequence
    of a question, we get two kinds of attention weights: question self-attention
    weights which account for prior probability in the whole graph and fine-grained
    decomposed question features for corresponding nodes or edges.'
  Figure 4 Link: articels_figures_by_rev_year\2021\Structured_Multimodal_Attentions_for_TextVQA\figure_4.jpg
  Figure 4 caption: An overview of Question Conditioned Graph Attention Module. This
    module builds a heterogeneous graph whose mixed nodes are shown in different colors.
    Guiding signals help produce attention weights, fused which with node representations
    we get question-conditioned features.
  Figure 5 Link: articels_figures_by_rev_year\2021\Structured_Multimodal_Attentions_for_TextVQA\figure_5.jpg
  Figure 5 caption: Schematic diagram of four kinds of edge structures. Blue dots
    represent object nodes, and orange dots represent text nodes. Here we set k=3
    for simplicity, a node will only have edges with its k -nearest neighbours.
  Figure 6 Link: articels_figures_by_rev_year\2021\Structured_Multimodal_Attentions_for_TextVQA\figure_6.jpg
  Figure 6 caption: An overview of Global-Local Attentional Answering Module. The
    same transformer layers are split into three functionally different parts. Local
    Encoder updates local ocr embeddings. The results of Global Encoder are used to
    predict answer for the first timestep. General vocabulary and updated OCR consist
    answer candidates, from with we use a two-branch scoring function to select the
    answer in each timestep.
  Figure 7 Link: articels_figures_by_rev_year\2021\Structured_Multimodal_Attentions_for_TextVQA\figure_7.jpg
  Figure 7 caption: Edge attention and decomposed question attention visualization
    for SMA. Three representative examples that require relationship reasoning for
    question answering are presented, which demand different kinds of edge relations.
    For instance, to represents the relation whose former node is text and latter
    one is object. For each example we highlight nodes or edges with the highest attention
    weights, wherein nodes are represented by boxes and edges are displayed by arrows
    pointing from former node to latter one. For boxesnodes, yellow ones are for object
    and blue ones are for text. Solid ones are those with the highest attention weights
    whereas dashed ones are normal. For decomposed question attention, the darker
    highlighted text area has a higher attention weight. All of them are predicted
    by SMA with Ground-Truth OCR. It can be seen that question attention module successfully
    figures out desired relations.
  Figure 8 Link: articels_figures_by_rev_year\2021\Structured_Multimodal_Attentions_for_TextVQA\figure_8.jpg
  Figure 8 caption: Performance of LoRRA, M4C and SMA with different OCR systems on
    the TextVQA dataset. Hmean measures the ability of OCR models. The x-axis shows
    the Hmean value of four gradually increased OCRs (Rosetta-ml, Rosetta-en, SBD-Trans
    and Ground-Truth). The y-axis presents the val accuracy of reasoning models.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.95
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Chenyu Gao
  Name of the last author: Qi Wu
  Number of Figures: 8
  Number of Tables: 5
  Number of authors: 7
  Paper title: Structured Multimodal Attentions for TextVQA
  Publication Date: 2021-12-02 00:00:00
  Table 1 caption: TABLE 1 Ablation Study on Key Components of Question Conditioned
    Graph Attention Module on TextVQA Dataset
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 More ablation models and comparision to previous work
  Table 3 caption: TABLE 3 Evaluation With GT OCR on Three Models
  Table 4 caption: TABLE 4 An OCR Result is Considered as a Match if its Bounding
    Box Overlaps With Corresponding Ground-Truth One by Over 50% 50% of the Total
    Area and the Tokens Given are the Same
  Table 5 caption: TABLE 5 Evaluation on ST-VQA Dataset
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3132034
- Affiliation of the first author: school of computer science and engineering, nanjing
    university of science and technology, nanjing, china
  Affiliation of the last author: school of computer science and engineering, nanjing
    university of science and technology, nanjing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\CTNet_ContextBased_Tandem_Network_for_Semantic_Segmentation\figure_1.jpg
  Figure 1 caption: Semantic segmentation is to identify the category of each pixel.
    It is very challenging to parse pixels with similar appearance. It is prone to
    some regional segmentation errors without considering spatial context, while ignoring
    channel context is prone to incorrect category information. Areas in the black
    box are easily confused.
  Figure 10 Link: articels_figures_by_rev_year\2021\CTNet_ContextBased_Tandem_Network_for_Semantic_Segmentation\figure_10.jpg
  Figure 10 caption: Ablation study of CP-Loss in CTNet on PASCAL-Context. alpha denotes
    the weight of auxiliary loss, and beta denotes the weight of CP-Loss.
  Figure 2 Link: articels_figures_by_rev_year\2021\CTNet_ContextBased_Tandem_Network_for_Semantic_Segmentation\figure_2.jpg
  Figure 2 caption: The illustration of the proposed CTNet framework. It jointly explores
    the spatial dependency and the semantic dependency by levering the Spatial Contextual
    Module (SCM) and the Channel Contextual Module (CCM). With the extracted feature
    maps by the pre-trained backbone, CCM explores the semantic dependencies to learn
    the new feature map and the feature representation of each category. The learned
    features by CCM are utilized by SCM to update the feature map by considering the
    spatial context.
  Figure 3 Link: articels_figures_by_rev_year\2021\CTNet_ContextBased_Tandem_Network_for_Semantic_Segmentation\figure_3.jpg
  Figure 3 caption: "Comparison among different channel context extraction models.\
    \ (a) Squeeze Excitation (SE) block; (b) Global Context (GC) block; (c) Encoding\
    \ block; (d) The proposed Multi-local Channel Excitation (MCE) block. The shape\
    \ of the feature map X and X c is C\xD7H\xD7W , \u2297 represents the channel-wise\
    \ multiplication."
  Figure 4 Link: articels_figures_by_rev_year\2021\CTNet_ContextBased_Tandem_Network_for_Semantic_Segmentation\figure_4.jpg
  Figure 4 caption: The details of the channel context extraction model (CCM).
  Figure 5 Link: articels_figures_by_rev_year\2021\CTNet_ContextBased_Tandem_Network_for_Semantic_Segmentation\figure_5.jpg
  Figure 5 caption: The details of the spatial context extraction model (SCM).
  Figure 6 Link: articels_figures_by_rev_year\2021\CTNet_ContextBased_Tandem_Network_for_Semantic_Segmentation\figure_6.jpg
  Figure 6 caption: Ablation study of different scale local channel contexts in CTNet
    on PASCAL-Context. (The horizontal coordinates represent the kernel size of 1-D
    convolution.).
  Figure 7 Link: articels_figures_by_rev_year\2021\CTNet_ContextBased_Tandem_Network_for_Semantic_Segmentation\figure_7.jpg
  Figure 7 caption: Illustration of segmented examples of CTNet and OCCM.
  Figure 8 Link: articels_figures_by_rev_year\2021\CTNet_ContextBased_Tandem_Network_for_Semantic_Segmentation\figure_8.jpg
  Figure 8 caption: Illustration of segmented examples of CTNet and OSCM.
  Figure 9 Link: articels_figures_by_rev_year\2021\CTNet_ContextBased_Tandem_Network_for_Semantic_Segmentation\figure_9.jpg
  Figure 9 caption: Illustration of segmented examples of CTNet and PANet.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.93
  Name of the first author: Zechao Li
  Name of the last author: Jinhui Tang
  Number of Figures: 13
  Number of Tables: 13
  Number of authors: 4
  Paper title: 'CTNet: Context-Based Tandem Network for Semantic Segmentation'
  Publication Date: 2021-12-02 00:00:00
  Table 1 caption: TABLE 1 Ablation Study of the Proposed MCE Block on the PASCAL-Context
    Dataset
  Table 10 caption: TABLE 10 Comparison in Terms of mIoU on the PASCAL-Context Dataset
    With State-of-the-Arts
  Table 2 caption: TABLE 2 Ablation Study of the Global Pooling Strategy in CCM on
    PASCAL-Context
  Table 3 caption: TABLE 3 Comparison of Computational Cost on the Pascal-Context
    Dataset
  Table 4 caption: TABLE 4 Ablation Study of the Conv Layer to Produce C C and D D
    on the Pascal-Context Dataset
  Table 5 caption: TABLE 5 Ablation Study of the Temperature in Softmax on the Pascal-Context
    Dataset
  Table 6 caption: TABLE 6 Compared Results in Terms of mIoU on Pascal-Context and
    ADE20K
  Table 7 caption: TABLE 7 Ablation Study of CCM and SCM on PASCAL-Context
  Table 8 caption: TABLE 8 Ablation Study of Connection Mode in CTNet on PASCAL-Context
  Table 9 caption: TABLE 9 Ablation Study of the Generation Method of P gt Pgt on
    the Pascal-Context Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3132068
