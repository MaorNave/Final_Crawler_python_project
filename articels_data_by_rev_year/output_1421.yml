- Affiliation of the first author: beijing laboratory of intelligent information technology,
    school of computer science, beijing institute of technology, beijing, china
  Affiliation of the last author: markableai, new york, ny, 11201, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Revisiting_Video_Saliency_Prediction_in_the_Deep_Learning_Era\figure_1.jpg
  Figure 1 caption: 'Average attention maps of three benchmark datasets: (a) Hollywood-2
    [33], (b) UCF sports [33], and (c) DHF1K.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Revisiting_Video_Saliency_Prediction_in_the_Deep_Learning_Era\figure_2.jpg
  Figure 2 caption: Example frames from DHF1K with fixations and corresponding categories.
    Note that, for better visualization, we use enlarged red dots to represent the
    human eye fixations. This figure is best viewed in color (zoom in for details).
  Figure 3 Link: articels_figures_by_rev_year\2019\Revisiting_Video_Saliency_Prediction_in_the_Deep_Learning_Era\figure_3.jpg
  Figure 3 caption: Network architecture of the proposed video saliency model ACLNet.
    (a) Attentive CNN-LSTM architecture, (b) CNN layers with attention module are
    used for learning intra-frame static features, where the attention module is learned
    with the supervision from static saliency data, and (c) ConvLSTM used for learning
    sequential saliency representations.
  Figure 4 Link: articels_figures_by_rev_year\2019\Revisiting_Video_Saliency_Prediction_in_the_Deep_Learning_Era\figure_4.jpg
  Figure 4 caption: Performance of ACLNet with or without the attention module on
    the training and validation sets of DHF1K. The attention module significantly
    improves training efficiency and performance.
  Figure 5 Link: articels_figures_by_rev_year\2019\Revisiting_Video_Saliency_Prediction_in_the_Deep_Learning_Era\figure_5.jpg
  Figure 5 caption: Illustration of the attention maps predicted by our ACLNet and
    the attention module on two dynamic stimuli. Best viewed in color.
  Figure 6 Link: articels_figures_by_rev_year\2019\Revisiting_Video_Saliency_Prediction_in_the_Deep_Learning_Era\figure_6.jpg
  Figure 6 caption: Dynamic saliency prediction performance over time, evaluated on
    the DHF1K test set. The static (dynamic) saliency models are plotted as black
    (red) dots, and the deep learning based models are represented by black boxes.
    It can be observed a performance improvement starting in 2015, corresponding to
    the application of deep learning techniques to visual saliency detection. See
    Section 5.4 for details.
  Figure 7 Link: articels_figures_by_rev_year\2019\Revisiting_Video_Saliency_Prediction_in_the_Deep_Learning_Era\figure_7.jpg
  Figure 7 caption: 'Qualitative results of our ACLNet and four representative saliency
    models: ITTI [26] (non-deep static saliency model), DVA [29] (deep static saliency
    model), PQFT [10] (non-deep dynamic saliency model), and OM-CNN [24] (deep dynamic
    saliency model) on three video saliency datasets: UCF sports [33] (a, b), Hollywood-2
    [33] (c, d) and DHF1K (e, f). Best viewed in color. It can be observed that the
    proposed ACLNet is able to handle various challenging scenes well and produces
    more accurate video saliency results than other competitors. See Section 5.5 for
    details.'
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.94
  Name of the first author: Wenguan Wang
  Name of the last author: Ali Borji
  Number of Figures: 7
  Number of Tables: 14
  Number of authors: 6
  Paper title: Revisiting Video Saliency Prediction in the Deep Learning Era
  Publication Date: 2019-06-24 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Statistics of Typical Dynamic Eye-Tracking Datasets
  Table 10 caption:
    table_text: TABLE 10 Quantitative Results on UCF Sports [33]
  Table 2 caption:
    table_text: TABLE 2 Statistics for Video Categories in DHF1K Dataset
  Table 3 caption:
    table_text: TABLE 3 Statistics Regarding Motion Patterns
  Table 4 caption:
    table_text: TABLE 4 Statistics Regarding Number of Main Objects
  Table 5 caption:
    table_text: TABLE 5 Statistics Regarding Scene Illumination
  Table 6 caption:
    table_text: TABLE 6 Statistics Regarding Number of People
  Table 7 caption:
    table_text: TABLE 7 Statistics and Features of Saliency Prediction Algorithms
      Used in Our Evaluation
  Table 8 caption:
    table_text: TABLE 8 Quantitative Results on DHF1K
  Table 9 caption:
    table_text: TABLE 9 Quantitative Results on Hollywood-2 [33]
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2924417
- Affiliation of the first author: national university of singapore, singapore
  Affiliation of the last author: state university of new york at buffalo, buffalo,
    usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Asymmetric_Mapping_Quantization_for_Nearest_Neighbor_Search\figure_1.jpg
  Figure 1 caption: A randomly generated network with 10 nodes. Such a network can
    be modeled with an undirected and connected graph.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Asymmetric_Mapping_Quantization_for_Nearest_Neighbor_Search\figure_2.jpg
  Figure 2 caption: Convergence of our AMQ and DAMQ on SIFT1B dataset with 64-bit
    codes.
  Figure 3 Link: articels_figures_by_rev_year\2019\Asymmetric_Mapping_Quantization_for_Nearest_Neighbor_Search\figure_3.jpg
  Figure 3 caption: The vertical axis stands for the training time (minute), the horizontal
    axis for different code lengths.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.61
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.83
  Name of the first author: Weixiang Hong
  Name of the last author: Junsong Yuan
  Number of Figures: 3
  Number of Tables: 4
  Number of authors: 4
  Paper title: Asymmetric Mapping Quantization for Nearest Neighbor Search
  Publication Date: 2019-06-27 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Recall R R for Different Algorithms on SIFT1M and DEEP1M
      Datasets with 64-bit Codes
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Recall R R for Different Algorithms on SIFT1B Datasets
      with 64-bit and 128-bit Codes
  Table 3 caption:
    table_text: TABLE 3 The mAP on the Holidays Dataset with Distractors
  Table 4 caption:
    table_text: TABLE 4 The Results are Obtained on SIFT1B Dataset Using 64-bit Codes
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2925347
- Affiliation of the first author: tel-aviv university, tel aviv, israel
  Affiliation of the last author: tel-aviv university, tel aviv, israel
  Figure 1 Link: articels_figures_by_rev_year\2019\BorderPeeling_Clustering\figure_1.jpg
  Figure 1 caption: 'The Border-Peeling technique on two different datasets: Three
    consecutive peeling iterations are illustrated on the left, with the identified
    border points colored in red. Next, we illustrate the clustering of the highly-separable
    core points that remain after the peeling process. The rightmost figures illustrate
    our final clustering results (outliers are colored in black).'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\BorderPeeling_Clustering\figure_2.jpg
  Figure 2 caption: The border association process and calculation of l i during the
    peeling process. The figures illustrate the border points (shown in red) of the
    first two iterations and their association to non-border points, marked by the
    red arrows. The blue circles represent the association area induced by the spatially-variant
    threshold value l i .
  Figure 3 Link: articels_figures_by_rev_year\2019\BorderPeeling_Clustering\figure_3.jpg
  Figure 3 caption: Qualitative comparison of our Border-Peeling technique to parametric
    clustering techniques on three synthetic examples from the literature (labeled
    A-C). As the figure demonstrates, our technique successfully recovers the number
    of clusters automatically as well as identified the outliers (colored in black),
    while the two parametric methods fail to identify the correct clusters for datasets
    A and B.
  Figure 4 Link: articels_figures_by_rev_year\2019\BorderPeeling_Clustering\figure_4.jpg
  Figure 4 caption: Embeddings in 2d for selected samples of the MNIST features that
    were produced by a CNN. The embeddings were obtained by running PCA on the datasets.
  Figure 5 Link: articels_figures_by_rev_year\2019\BorderPeeling_Clustering\figure_5.jpg
  Figure 5 caption: The images corresponding to the bottom-10 (odd rows) and top-10
    values (even rows) of b (0) i for four different clusters which were obtained
    using Border-Peeling clustering on a random subset of the MNIST dataset.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Hadar Averbuch-Elor
  Name of the last author: Daniel Cohen-Or
  Number of Figures: 5
  Number of Tables: 3
  Number of authors: 3
  Paper title: Border-Peeling Clustering
  Publication Date: 2019-06-27 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Border-Peeling Clustering (BP) with Parametric
      (KM, SC) and Non-Parametric (HDB, DB, AP, MS, QCC, RCC) Clustering Techniques
      on the Common Synthetic Datasets A, B and C Which Are Illustrated in Fig. 3
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Border-Peeling Clustering (BP) with Parametric
      (KM, SC) and Non-Parametric (HDB, DB, AP, MS, RCC) Clustering Techniques on
      Large Datasets in an Unsupervised Setting
  Table 3 caption:
    table_text: TABLE 3 A comparison to Non-Parametric Clustering Techniques on Samples
      of the MNIST Dataset
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2924953
- Affiliation of the first author: computer science department, stanford university,
    stanford, ca, usa
  Affiliation of the last author: peking university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2019\A_General_Decoupled_Learning_Framework_for_Parameterized_Image_Operators\figure_1.jpg
  Figure 1 caption: "Our system consists of two networks: The above weight learning\
    \ network N weight is designed to learn the convolution weights for the bottom\
    \ base network N base . Given a parameterized image operator constraint by \u03B3\
    \ \u2192 , these two networks are jointly trained, and N weight will dynamically\
    \ update the weights of N base for different \u03B3 \u2192 in the inference stage."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\A_General_Decoupled_Learning_Framework_for_Parameterized_Image_Operators\figure_2.jpg
  Figure 2 caption: Visual examples produced by our framework trained on continuous
    parameter settings of six image filters independently. Note all the visual effects
    for one filter are generated by a single network.
  Figure 3 Link: articels_figures_by_rev_year\2019\A_General_Decoupled_Learning_Framework_for_Parameterized_Image_Operators\figure_3.jpg
  Figure 3 caption: Visual examples produced by our framework trained on continuous
    parameter settings of three image restoration tasks independently. Note all the
    clean images for one restoration task are generated by a single network.
  Figure 4 Link: articels_figures_by_rev_year\2019\A_General_Decoupled_Learning_Framework_for_Parameterized_Image_Operators\figure_4.jpg
  Figure 4 caption: "Effective receptive field of L 0 smoothing for different spatial\
    \ positions and parameter \u03BB . The top to bottom indicate the effective receptive\
    \ field of a non-edge point, a moderate edge point, and a strong edge point."
  Figure 5 Link: articels_figures_by_rev_year\2019\A_General_Decoupled_Learning_Framework_for_Parameterized_Image_Operators\figure_5.jpg
  Figure 5 caption: Equivalent analysis of the connection between the base network
    N base and the weight learning network N weight . One convolution layer whose
    weights are learnt by the fc layer is exactly equivalent to a multi-path convolution
    blocks.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.9
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.64
  Name of the first author: Qingnan Fan
  Name of the last author: Baoquan Chen
  Number of Figures: 5
  Number of Tables: 14
  Number of authors: 6
  Paper title: A General Decoupled Learning Framework for Parameterized Image Operators
  Publication Date: 2019-06-28 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Absolute Difference Between the Network Trained
      with a Single Parameter Value and Numerous Random Values for Each Image Operator
  Table 10 caption:
    table_text: TABLE 10 Quantitative Results (PSNR) of the Derain Task on the RAIN12
      Benchmark
  Table 2 caption:
    table_text: TABLE 2 Quantitative Absolute Difference in PSNR (Above) and SSIM
      (Bottom) Between the Network Trained on a Single Parameter Value and Numerous
      Random Values on the Three Image Restoration Tasks
  Table 3 caption:
    table_text: TABLE 3 Numerical Results of our Proposed Framework Jointly Trained
      over Different Number of Image Operators (Operators)
  Table 4 caption:
    table_text: TABLE 4 Quantitative Evaluation (PSNR) on the Higher Dimensional Parameter
      Space of the RTV Filter
  Table 5 caption:
    table_text: TABLE 5 Running Time Evaluation (Milliseconds) of Our Decouple Learning
      Framework and its Cheap Parameter Tuning Module on Different Image Resolutions,
      Along with the Baseline [18]
  Table 6 caption:
    table_text: TABLE 6 Quantitative Comparison with State-of-the-Art Approaches in
      Reproducing Image Operators
  Table 7 caption:
    table_text: TABLE 7 Quantitative Results (PSNRSSIM) of the JPEG Deblocking Task
      on the LIVE1 Benchmark
  Table 8 caption:
    table_text: TABLE 8 Quantitative Results (PSNRSSIM) of the Image Super Resolution
      Task on the BSD100 Benchmark
  Table 9 caption:
    table_text: TABLE 9 Quantitative Results (PSNR) of the Image Denoising Task on
      the BSD68 Benchmark
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2925793
- Affiliation of the first author: school of computing sciences, university of east
    anglia, norwich, united kingdom
  Affiliation of the last author: samsung semiconductor inc., san jose, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Providing_a_Single_GroundTruth_for_Illuminant_Estimation_for_the_ColorChecker_Da\figure_1.jpg
  Figure 1 caption: An image from the Macbeth ColorChecker dataset.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Providing_a_Single_GroundTruth_for_Illuminant_Estimation_for_the_ColorChecker_Da\figure_2.jpg
  Figure 2 caption: New RECommended ground-truth chromaticities are plotted as black
    asterisks. The SFUGt3 ground-truth is shown as green squares and Gt1 as red squares.
    The distributions convex hulls are plotted. 100 of the 568 chromaticities are
    shown.
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.55
  Name of the first author: Ghalia Hemrit
  Name of the last author: Lilong Shi
  Number of Figures: 2
  Number of Tables: 1
  Number of authors: 8
  Paper title: Providing a Single Ground-Truth for Illuminant Estimation for the ColorChecker
    Dataset
  Publication Date: 2019-07-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Performance of 6 Algorithms (see colorconstancy.com) are
      in Reverse Order in Terms of their Median Angular Error when the New REC vs
      the Legacy Gt1 Ground-Truth is Used
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2919824
- Affiliation of the first author: universal communication research institute, national
    institute of information and communications technology (nict), koganei, tokyo,
    japan
  Affiliation of the last author: national institute of information and communications
    technology (nict), universal communication research institute, soraku-gun, kyoto,
    japan
  Figure 1 Link: articels_figures_by_rev_year\2019\Community_Detection_Using_Restrained_RandomWalk_Similarity\figure_1.jpg
  Figure 1 caption: Graph consisting of two communities. Circles with arrows indicate
    random walks.
  Figure 10 Link: articels_figures_by_rev_year\2019\Community_Detection_Using_Restrained_RandomWalk_Similarity\figure_10.jpg
  Figure 10 caption: Example images in the CC2 communities of the match graph obtained
    from the Japanese templeshrine image dataset.
  Figure 2 Link: articels_figures_by_rev_year\2019\Community_Detection_Using_Restrained_RandomWalk_Similarity\figure_2.jpg
  Figure 2 caption: Relation between the number of steps of the random walker and
    the number of elements in the set of vertices passed by the walker when the random
    walk was executed on a network constructed using the Japanese templeshrine dataset.
  Figure 3 Link: articels_figures_by_rev_year\2019\Community_Detection_Using_Restrained_RandomWalk_Similarity\figure_3.jpg
  Figure 3 caption: Image of a match graph constructed to cluster images of tourist
    attractions by subject. The copyrights of the pictures are held by, from the left,
    Chang Tai Jyun, yannickdellafaille, rurinoshima, KimonBerlin, Jexweber.fotos,
    , timothybrennan, RachelH, ohsarahrose, Lora Sutyagina, RachelC, and nakashi.
  Figure 4 Link: articels_figures_by_rev_year\2019\Community_Detection_Using_Restrained_RandomWalk_Similarity\figure_4.jpg
  Figure 4 caption: Examples of matching SIFT key points [55] between images with
    different primary subjects.
  Figure 5 Link: articels_figures_by_rev_year\2019\Community_Detection_Using_Restrained_RandomWalk_Similarity\figure_5.jpg
  Figure 5 caption: Accuracy of the restrained random-walk similarity method with
    respect to the number of repeats of a random walk starting from each vertex (
    p in Algorithm 1) for CC1 in the match graph constructed with the Japanese templeshrine
    dataset (Section 4.1.3). Accuracies are shown in the boxplots [62] of the normalized
    mutual information (NMI).
  Figure 6 Link: articels_figures_by_rev_year\2019\Community_Detection_Using_Restrained_RandomWalk_Similarity\figure_6.jpg
  Figure 6 caption: "Accuracy of the community detection methods for the LFR benchmark\
    \ graphs (Section 4.1.1). SP: Spin-glass [8], LO: Louvain [23], IN: Infomap [41],\
    \ CD: CD-TRandwalk [6], CO: CONCLUDE [7], RW: Random-walk Similarity Method (Ours),\
    \ RR: Restrained Random-walk Similarity Method (Ours); NMI: Normalized Mutual\
    \ Information; N : Number of vertices, \u03BC : Mixing rate, s max : Maximal community\
    \ size."
  Figure 7 Link: articels_figures_by_rev_year\2019\Community_Detection_Using_Restrained_RandomWalk_Similarity\figure_7.jpg
  Figure 7 caption: 'Accuracy of the community detection methods for the SNAP datasets
    (Section 4.1.2). SP: Spin-glass [8], LO: Louvain [23], IN: Infomap [41], CD: CD-TRandwalk
    [6], CO: CONCLUDE [7], RW: Random-walk Similarity Method (Ours), RR: Restrained
    Random-walk Similarity Method (Ours); NMI: Normalized Mutual Information.'
  Figure 8 Link: articels_figures_by_rev_year\2019\Community_Detection_Using_Restrained_RandomWalk_Similarity\figure_8.jpg
  Figure 8 caption: 'Accuracy of the community detection methods for the match graph
    constructed from the images in the Japanese templeshrine image dataset (Section
    4.1.3). SP: Spin-glass [8], LO: Louvain [23], IN: Infomap [41], CD: CD-TRandwalk
    [6], CO: CONCLUDE [7], RW: Random-walk Similarity Method (Ours), RR: Restrained
    Random-walk Similarity Method (Ours); NMI: Normalized Mutual Information.'
  Figure 9 Link: articels_figures_by_rev_year\2019\Community_Detection_Using_Restrained_RandomWalk_Similarity\figure_9.jpg
  Figure 9 caption: 'Accuracy of the community detection methods for the match graph
    constructed from the images in the Paris 500k dataset (Section 4.1.4). SP: Spin-glass
    [8], LO: Louvain [23], IN: Infomap [41], CD: CD-TRandwalk [6], CO: CONCLUDE [7],
    RW: Random-walk Similarity Method (Ours), RR: Restrained Random-walk Similarity
    Method (Ours); NMI: Normalized Mutual Information.'
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Makoto Okuda
  Name of the last author: Yutaka Kidawara
  Number of Figures: 13
  Number of Tables: 5
  Number of authors: 4
  Paper title: Community Detection Using Restrained Random-Walk Similarity
  Publication Date: 2019-07-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 LFR Parameters
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Summary of the SNAP Graphs
  Table 3 caption:
    table_text: TABLE 3 Summary of the Connected Components of the Match Graph Constructed
      from the Japanese TempleShrine Image Dataset
  Table 4 caption:
    table_text: TABLE 4 Summary of the Connected Components of Match Graphs Constructed
      with the Paris 500k Dataset
  Table 5 caption:
    table_text: TABLE 5 Analysis of the Communities Obtained by Applying Each Community
      Detection Method to CC2 in the Match Graph Constructed from the Japanese TempleShrine
      Image Dataset
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2926033
- Affiliation of the first author: college of intelligence and computing, tianjin
    university, tianjin, china
  Affiliation of the last author: school of engineering, university of california,
    merced, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Simultaneous_Fidelity_and_Regularization_Learning_for_Image_Restoration\figure_1.jpg
  Figure 1 caption: Illustration of the SFARL model on three restoration tasks. (a)
    In image deconvolution with inaccurate blur kernels, The SFARL method is effective
    in relieving the ringing artifacts. (b) For deconvolution along with saturation,
    Gaussian noise and JPEG compression, the SFARL model can achieve visually plausible
    result with less noises than DCNN [15]. (c) For rain streak removal, the SFARL
    model can produce more clean image than DDNET [16].
  Figure 10 Link: articels_figures_by_rev_year\2019\Simultaneous_Fidelity_and_Regularization_Learning_for_Image_Restoration\figure_10.jpg
  Figure 10 caption: Visual quality of deraining results by transferring filters from
    SFARL models for deconvolution and denoising.
  Figure 2 Link: articels_figures_by_rev_year\2019\Simultaneous_Fidelity_and_Regularization_Learning_for_Image_Restoration\figure_2.jpg
  Figure 2 caption: Visual quality comparison on Levin et al.s dataset [57].
  Figure 3 Link: articels_figures_by_rev_year\2019\Simultaneous_Fidelity_and_Regularization_Learning_for_Image_Restoration\figure_3.jpg
  Figure 3 caption: Deblurring results on real blurry images, in which blur kernels
    are estimated by Xu and Jia [18].
  Figure 4 Link: articels_figures_by_rev_year\2019\Simultaneous_Fidelity_and_Regularization_Learning_for_Image_Restoration\figure_4.jpg
  Figure 4 caption: Visual quality comparison on deconvolution along with Gaussian
    noise, satature and JPEG compression.
  Figure 5 Link: articels_figures_by_rev_year\2019\Simultaneous_Fidelity_and_Regularization_Learning_for_Image_Restoration\figure_5.jpg
  Figure 5 caption: Rain streak removal results of five evaluated methods on a synthetic
    image in [24].
  Figure 6 Link: articels_figures_by_rev_year\2019\Simultaneous_Fidelity_and_Regularization_Learning_for_Image_Restoration\figure_6.jpg
  Figure 6 caption: Visual quality comparison for rain streak removal. Both SFARL
    and DDNET are trained for Rain1400 [16]. The first row is from Rain1400 [16],
    while the second row is from Rain100L [32] for evaluating generalization ability.
  Figure 7 Link: articels_figures_by_rev_year\2019\Simultaneous_Fidelity_and_Regularization_Learning_for_Image_Restoration\figure_7.jpg
  Figure 7 caption: Results on real rainy images. The rain in second image is very
    heavy and we first dehaze [64] it to make rain streaks more visible.
  Figure 8 Link: articels_figures_by_rev_year\2019\Simultaneous_Fidelity_and_Regularization_Learning_for_Image_Restoration\figure_8.jpg
  Figure 8 caption: Empirical convergence of a 5-stage SFARL for rain streak removal
    during greedy training and joint fine-tuning.
  Figure 9 Link: articels_figures_by_rev_year\2019\Simultaneous_Fidelity_and_Regularization_Learning_for_Image_Restoration\figure_9.jpg
  Figure 9 caption: Visual quality comparison of SFARL trained by MSE loss and negative
    SSIM loss.
  First author gender probability: 0.95
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Dongwei Ren
  Name of the last author: Ming-Hsuan Yang
  Number of Figures: 10
  Number of Tables: 6
  Number of authors: 5
  Paper title: Simultaneous Fidelity and Regularization Learning for Image Restoration
  Publication Date: 2019-07-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative SSIM Results on the Dataset by Levin et al. [57]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Comparison on Deconvolution with Multiple Degradations
      [15]
  Table 3 caption:
    table_text: TABLE 3 Deraining Results on Synthetic Rainy Images in [24] in Terms
      of SSIM
  Table 4 caption:
    table_text: TABLE 4 Average PSNRSSIM Comparison on Rain1400 [16] and Rain100L
      [32]
  Table 5 caption:
    table_text: TABLE 5 Average PSNR and SSIM on Testing Dataset [16] of SFARL Models
      for Rain Streak Removal Trained by MSE Loss and SSIM Loss
  Table 6 caption:
    table_text: TABLE 6 Quantitative Results on Rainy Rain12 Dataset [24] by Transferring
      Filters from SFARL Models for Deconvolution and Denoising
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2926357
- Affiliation of the first author: baidu research, beijing, china
  Affiliation of the last author: baidu research, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2019\The_ApolloScape_Open_Dataset_for_Autonomous_Driving_and_Its_Application\figure_1.jpg
  Figure 1 caption: A glance of ApolloScape with various properties. The images are
    cropped for better visualization.
  Figure 10 Link: articels_figures_by_rev_year\2019\The_ApolloScape_Open_Dataset_for_Autonomous_Driving_and_Its_Application\figure_10.jpg
  Figure 10 caption: DeLS-3D overview. The black arrows show the testing process,
    and red arrows indicate the rendering (projection) operation in training and inference.
    The yellow frustum shows the location of cameras inside the 3D map. The input
    of our system contains a sequence of images and corresponding GPSIMU signals.
    The outputs are the semantically segmented images, each with its refined camera
    pose.
  Figure 2 Link: articels_figures_by_rev_year\2019\The_ApolloScape_Open_Dataset_for_Autonomous_Driving_and_Its_Application\figure_2.jpg
  Figure 2 caption: Acquisition system consists of two laser scanners, up to six video
    cameras, and a combined IMUGNSS system.
  Figure 3 Link: articels_figures_by_rev_year\2019\The_ApolloScape_Open_Dataset_for_Autonomous_Driving_and_Its_Application\figure_3.jpg
  Figure 3 caption: Examples with challenging environments for object detection and
    segmentation (Images are center-cropped for better visualization). We highlight
    and zoom in the region of challenges in each image. (a) Objects with heavy occlusion
    and small scale. (b) Abnormal action by cyclist drivers. (c) High contrast and
    overexposure due to shadows and strong sunlight. (d) Mirror reflection on bus
    glasses.
  Figure 4 Link: articels_figures_by_rev_year\2019\The_ApolloScape_Open_Dataset_for_Autonomous_Driving_and_Its_Application\figure_4.jpg
  Figure 4 caption: 24 semantic classes and corresponding numbers of annotated pixels.
    Bar colors indicate different semantic groups.
  Figure 5 Link: articels_figures_by_rev_year\2019\The_ApolloScape_Open_Dataset_for_Autonomous_Driving_and_Its_Application\figure_5.jpg
  Figure 5 caption: "27 lane mark labels and corresponding numbers of annotated pixels.\
    \ Bar colors indicate 11 different lane mark usages. Here,\u201Cswd\u201D is short\
    \ for solid, white and dividing in Table 3 by combining the first letter of type,\
    \ color and usage respectively, and other classes are named accordingly."
  Figure 6 Link: articels_figures_by_rev_year\2019\The_ApolloScape_Open_Dataset_for_Autonomous_Driving_and_Its_Application\figure_6.jpg
  Figure 6 caption: Our 2D3D labeling pipeline that label static backgroundobjects
    and moving objects separately. We also adopt active strategies for accelerating
    the labelling process for scalability of the labelling process. For inputs, since
    GPSIMU still has some errors, we manually add control points to better align the
    point clouds and our image frames. In Section 3.3, we present the details of each
    components.
  Figure 7 Link: articels_figures_by_rev_year\2019\The_ApolloScape_Open_Dataset_for_Autonomous_Driving_and_Its_Application\figure_7.jpg
  Figure 7 caption: The user interface of our 3D labeling tool. At left-top, we show
    the pre-defined color code of different classes. At left-bottom, we show the labelling
    logs which can be used to revert the labelling when mistakes happen. At center
    part, labelled point cloud is shown indicating the labelling progress.
  Figure 8 Link: articels_figures_by_rev_year\2019\The_ApolloScape_Open_Dataset_for_Autonomous_Driving_and_Its_Application\figure_8.jpg
  Figure 8 caption: A labelled example of the labelling pipeline for semantic parsing,
    a subset of color coded labels are shown below. (a) Image. (b) Rendered label
    map with 3D point cloud projection, with an inaccurate moving object (rider) circled
    in blue. (c) Rendered label map with 3D point cloud projection after points with
    low temporal consistency being removed. (d) & (e) Rendered depth map of background
    and rendered label map after class dependent splatting in 3D point clouds (Section
    3.3). (f) Merged label map with missing region in-painted, moving objects and
    sky. (g) Another label map with very small traffic lights. Details are zoomed
    out highlighting the details of our rendered label maps. Other examples of our
    labeled videos is shown online [20].
  Figure 9 Link: articels_figures_by_rev_year\2019\The_ApolloScape_Open_Dataset_for_Autonomous_Driving_and_Its_Application\figure_9.jpg
  Figure 9 caption: Bird view of our projected road lane marks with labelling.
  First author gender probability: 0.98
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xinyu Huang
  Name of the last author: Ruigang Yang
  Number of Figures: 13
  Number of Tables: 9
  Number of authors: 6
  Paper title: The ApolloScape Open Dataset for Autonomous Driving and Its Application
  Publication Date: 2019-07-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison between Our Dataset and the Other Street-View Self-Driving
      Datasets Published
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Total and Average Number of Instances in KITTI [2], Cityscapes
      [3], BDD100K [29], and ApolloScape (Instance-Level)
  Table 3 caption:
    table_text: 'TABLE 3 Details of Lane Mark Labels in Our Dataset (y: yellow, w:white)'
  Table 4 caption:
    table_text: TABLE 4 Compare the Accuracy of Different Settings for Pose Estimation
      from the Two Datasets
  Table 5 caption:
    table_text: TABLE 5 Compare the Accuracy of Different Segment Networks Setting
      over Zpark (top) and Dlake (bottom) Dataset
  Table 6 caption:
    table_text: TABLE 6 Results of Image Parsing Based on ResNet-38 Network
  Table 7 caption:
    table_text: TABLE 7 The IoU Using One SOTA Semantic Segmentation Architecture,
      i.e., ResNet38 [76]
  Table 8 caption:
    table_text: TABLE 8 Results of Top Ranked Instance Segmentation Algorithms in
      ApolloScape and Cityscapes (Numbers Are Obtained at the Date of Submission)
  Table 9 caption:
    table_text: TABLE 9 Detailed Localization Accuracy of the Leading Results on Our
      Benchmark from [97]
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2926463
- Affiliation of the first author: institute of artificial intelligence and cognitive
    engineering, university of groningen, groningen, the netherlands
  Affiliation of the last author: "ieeta - instituto de engenharia electr\xF3nica\
    \ e telem\xE1tica de aveiro university of aveiro, aveiro, portugal"
  Figure 1 Link: articels_figures_by_rev_year\2019\LocalLDA_OpenEnded_Learning_of_Latent_Topics_for_D_Object_Recognition\figure_1.jpg
  Figure 1 caption: Proposed data processing layers being tested on a service robot.
  Figure 10 Link: articels_figures_by_rev_year\2019\LocalLDA_OpenEnded_Learning_of_Latent_Topics_for_D_Object_Recognition\figure_10.jpg
  Figure 10 caption: Four snapshots showing object recognition results on eight scenes
    of Washington scene dataset [49].
  Figure 2 Link: articels_figures_by_rev_year\2019\LocalLDA_OpenEnded_Learning_of_Latent_Topics_for_D_Object_Recognition\figure_2.jpg
  Figure 2 caption: 'Examples of common challenges in view-based object recognition:
    (left) mug observed from different viewpoints; (right) intra-class variation among
    mug instances.'
  Figure 3 Link: articels_figures_by_rev_year\2019\LocalLDA_OpenEnded_Learning_of_Latent_Topics_for_D_Object_Recognition\figure_3.jpg
  Figure 3 caption: 'Examples of the object detection in two different scenarios:
    (a and b) result of the table detection in isolated objects scenarios; Detected
    object candidates are shown by different bounding boxes and colors. The red, green
    and blue lines represent the local reference frame of the objects; (c) As shown
    in this figure, euclidean clustering algorithm is not enough for such a crowded
    scene. Therefore, the extracted region is dispatched to a hierarchical clustering
    procedure to detect multiple object candidates; (d) two highly crowded scenes
    and their corresponding segmentation results.'
  Figure 4 Link: articels_figures_by_rev_year\2019\LocalLDA_OpenEnded_Learning_of_Latent_Topics_for_D_Object_Recognition\figure_4.jpg
  Figure 4 caption: 'Object representation for a coffee cup : (a) point cloud of a
    coffee cup (b) key-point selection (blue points); spin-image descriptor is used
    to encode the surrounding shape in each key-point; (c) a schematic of how spin-image
    is computed for a keypoint p ; (d) the computed spin-image.'
  Figure 5 Link: articels_figures_by_rev_year\2019\LocalLDA_OpenEnded_Learning_of_Latent_Topics_for_D_Object_Recognition\figure_5.jpg
  Figure 5 caption: 'A sample set of visual words of a dictionary: spin-images compiled
    with IW = 4 bins and SL = 0.05m.'
  Figure 6 Link: articels_figures_by_rev_year\2019\LocalLDA_OpenEnded_Learning_of_Latent_Topics_for_D_Object_Recognition\figure_6.jpg
  Figure 6 caption: Visualization of five sample topics learned from the Washington
    RGB-D dataset [43].
  Figure 7 Link: articels_figures_by_rev_year\2019\LocalLDA_OpenEnded_Learning_of_Latent_Topics_for_D_Object_Recognition\figure_7.jpg
  Figure 7 caption: 'Examples of object-topic representation for eight categories
    of Restaurant Object dataset [36]: each object view is represented as a distribution
    over topics; each topic is shown with its top frequent visual words (i.e., visual
    words with more than 25 percent probability) using a unique color. It should be
    note, colours have different meaning in each category.'
  Figure 8 Link: articels_figures_by_rev_year\2019\LocalLDA_OpenEnded_Learning_of_Latent_Topics_for_D_Object_Recognition\figure_8.jpg
  Figure 8 caption: Summary of system performance in open-ended evaluations using
    Washington RGB-D object dataset [43].
  Figure 9 Link: articels_figures_by_rev_year\2019\LocalLDA_OpenEnded_Learning_of_Latent_Topics_for_D_Object_Recognition\figure_9.jpg
  Figure 9 caption: 'Real demonstration: (left) System setup; (right) two snapshots
    show the proposed system supports (top) classical learning from a batch of labelled
    training data and (bottom) open-ended learning from on-line experiences.'
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: S. Hamidreza Kasaei
  Name of the last author: "Ana Maria Tom\xE9"
  Number of Figures: 11
  Number of Tables: 6
  Number of authors: 3
  Paper title: 'Local-LDA: Open-Ended Learning of Latent Topics for 3D Object Recognition'
  Publication Date: 2019-07-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average Object Recognition Performance of Local-LDA in Batch
      Experiments for Different Parameter Values
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average Object Recognition Performance of LDA in Batch Experiments
      for Different Parameters Values
  Table 3 caption:
    table_text: TABLE 3 Average Object Recognition Performance of the RACE Approach
      in Batch Experiments for Different Parameter Values
  Table 4 caption:
    table_text: TABLE 4 Average Object Recognition Performance of the BoW Approach
      in Batch Experiments for Different Parameter Values
  Table 5 caption:
    table_text: TABLE 5 Object Recognition Performance in Batch Experiments for the
      Best Parameter Configuration in Each Approach
  Table 6 caption:
    table_text: TABLE 6 Comparison between the Open-Ended Experiments and Batch Experiments
      with the Same Number of Categories and Instances
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2926459
- Affiliation of the first author: school of information & gaoling school of artificial
    intelligence, renmin university of china, beijing, china
  Affiliation of the last author: department of electrical engineering, columbia university,
    new york, ny, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Variational_Context_Exploiting_Visual_and_Textual_Context_for_Grounding_Referrin\figure_1.jpg
  Figure 1 caption: The proposed Variational Context framework. Given an input referring
    expression and an image with region proposals, we localize the referent as output.
    We develop a grounding score function, with the variational lower-bound composed
    by three cue-specific multimodal modules, indicated by the description in the
    dashed color boxes.
  Figure 10 Link: articels_figures_by_rev_year\2019\Variational_Context_Exploiting_Visual_and_Textual_Context_for_Grounding_Referrin\figure_10.jpg
  Figure 10 caption: Example generation results using our full model (VC w Gen+PG)
    on three datasets. The ground-truthgenerated expression is linked with the described
    referent using the same color.
  Figure 2 Link: articels_figures_by_rev_year\2019\Variational_Context_Exploiting_Visual_and_Textual_Context_for_Grounding_Referrin\figure_2.jpg
  Figure 2 caption: "The architecture of the proposed Variational Context framework.\
    \ It consists of a region feature extraction module (Section 4.1), and a language\
    \ feature extraction module (Section 4.2), three grounding modules (Section 4.3),\
    \ and one generation module (Section 4.4). It can be trained in an end-to-end\
    \ fashion with the input of a set of image regions and a referring expression,\
    \ using the supervised loss (Eq. (10)) or the unsupervised loss (Eq. (11)). LSTMs:\
    \ Long short-term memory networks. fc: Fully-connected layer. concat: Vector concatenation.\
    \ L2Norm: L2 normalization layer. \u2299 : Element-wise vector multiplication.\
    \ \u2295 : Add."
  Figure 3 Link: articels_figures_by_rev_year\2019\Variational_Context_Exploiting_Visual_and_Textual_Context_for_Grounding_Referrin\figure_3.jpg
  Figure 3 caption: 'Two qualitative examples of the cue-specific language feature
    word weights. Darker color indicates higher weights. cr+12: Contextreferent-cue
    + singlepairwise.'
  Figure 4 Link: articels_figures_by_rev_year\2019\Variational_Context_Exploiting_Visual_and_Textual_Context_for_Grounding_Referrin\figure_4.jpg
  Figure 4 caption: 'Qualitative results on RefCOCOg (det) showing comparisons between
    correct (green tick) and wrong referent grounds (red cross) by VC and CMN using
    VGG features. The denotations of the bounding box colors are as follows. Solid
    red: Grounding referent; solid green: Ground truth; dashed yellow: Grounding context.
    We only display top 3 context objects with the context ground probability >0.1
    . We can observe that VC has more reasonable context localizations than CMN, even
    in cases when the referent ground of VC fails.'
  Figure 5 Link: articels_figures_by_rev_year\2019\Variational_Context_Exploiting_Visual_and_Textual_Context_for_Grounding_Referrin\figure_5.jpg
  Figure 5 caption: Performances of VC and CMN with different number of object bounding
    boxes on RefCOCO Test A &B, RefCOCO+ Test A & B, and RefCOCOg Val. Compared to
    CMN, we can see that VC is more effective in context modeling when the number
    of objects is large.
  Figure 6 Link: articels_figures_by_rev_year\2019\Variational_Context_Exploiting_Visual_and_Textual_Context_for_Grounding_Referrin\figure_6.jpg
  Figure 6 caption: 'Qualitative results of our full model (VC w Gen+PG) on RefCOCOg
    (det). The first column shows the grounding results. The second column shows the
    context estimation results. The third column shows the cue-specific language feature
    word weights. The denotations of the bounding box colors are as follows. Solid
    red: Grounding referent; solid green: Ground truth; solid blue: Grounding context
    with highest probability.'
  Figure 7 Link: articels_figures_by_rev_year\2019\Variational_Context_Exploiting_Visual_and_Textual_Context_for_Grounding_Referrin\figure_7.jpg
  Figure 7 caption: 'Common failure cases of our full model in supervised grounding
    on RefCOCOg. Each example shows grounding results, context estimation results,
    and cue-specific features from left to right. The denotations of the bounding
    box colors are as follows. Solid red: Grounding referent; solid green: Ground
    truth; solid blue: Grounding context with highest probability.'
  Figure 8 Link: articels_figures_by_rev_year\2019\Variational_Context_Exploiting_Visual_and_Textual_Context_for_Grounding_Referrin\figure_8.jpg
  Figure 8 caption: 'Common failure cases in unsupervised grounding with detected
    bounding boxes. From left to right: RefCOCO, RefCOCO+, and RefCOCOg. The failure
    is mainly to the challenging unsupervised relation modeling between referent and
    context.'
  Figure 9 Link: articels_figures_by_rev_year\2019\Variational_Context_Exploiting_Visual_and_Textual_Context_for_Grounding_Referrin\figure_9.jpg
  Figure 9 caption: "Word cloud visualizations of cue-specific word attention \u03B1\
    \ in Eq. (16) of context-cue (c2), referent-cue (r1), and generic-cue (g) using\
    \ supervised (top row) and unsupervised training (bottom row) on RefCOCOg. Without\
    \ supervision, it is difficult to discover meaningful language compositions."
  First author gender probability: 0.54
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yulei Niu
  Name of the last author: Shih-Fu Chang
  Number of Figures: 10
  Number of Tables: 7
  Number of authors: 4
  Paper title: 'Variational Context: Exploiting Visual and Textual Context for Grounding
    Referring Expressions'
  Publication Date: 2019-07-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Supervised Grounding Performances (Acc%) of Comparing Methods
      Using VGG Features on MSCOCO Ground-Truth Regions
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Supervised Grounding Performances (Acc%) of Comparing Methods
      Using VGG Features on MSCOCO Detected Regions
  Table 3 caption:
    table_text: TABLE 3 Performances (Acc%) of Supervised and Unsupervised Methods
      on RefCLEF
  Table 4 caption:
    table_text: TABLE 4 Supervised Grounding Performances (Acc%) of Ablation Study
      Using Generation Module or Better Visual Representation on MSCOCO Ground-Truth
      Regions
  Table 5 caption:
    table_text: TABLE 5 Unsupervised Grounding Performances (Acc%) of Comparing Methods
      Using VGG Features on RefCOCO, RefCOCO+, and RefCOCOg
  Table 6 caption:
    table_text: TABLE 6 Automatic Metrics on Referring Expression Generation
  Table 7 caption:
    table_text: TABLE 7 Human Evaluation (Acc%) on Referring Expression Generation
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2926266
