- Affiliation of the first author: school of electronic and computer engineering,
    shenzhen graduate school, peking university, shenzhen, china
  Affiliation of the last author: school of electronic and computer engineering, peking
    university shenzhen graduate school, shenzhen, china
  Figure 1 Link: articels_figures_by_rev_year\2022\TransCL_Transformer_Makes_Strong_and_Flexible_Compressive_Learning\figure_1.jpg
  Figure 1 caption: "Illustration of CS acquisition for a still image with a single-pixel\
    \ camera [2], which can be formulated as y=\u03A6x ."
  Figure 10 Link: articels_figures_by_rev_year\2022\TransCL_Transformer_Makes_Strong_and_Flexible_Compressive_Learning\figure_10.jpg
  Figure 10 caption: Visual comparison of semantic segmentation. We visualize the
    segmentation result of samples from PASCAL Context [69], Cityscapes [70], and
    ADE20K [68], which are presented in the first row, the second row, and the third
    row, respectively.
  Figure 2 Link: articels_figures_by_rev_year\2022\TransCL_Transformer_Makes_Strong_and_Flexible_Compressive_Learning\figure_2.jpg
  Figure 2 caption: Illustration of two ways of performing inference tasks (i.e.,
    image classification) on CS measurements. The blue dashed line presents the conventional
    two-stage operation (reconstruction first and then inference in the image domain),
    and the red dashed line represents the single-stage compressive learning (CL)
    direct in the measurement domain.
  Figure 3 Link: articels_figures_by_rev_year\2022\TransCL_Transformer_Makes_Strong_and_Flexible_Compressive_Learning\figure_3.jpg
  Figure 3 caption: Illustration of the self-attention (SA) mechanism [36], which
    is used to construct long-range correlations among the input sequence.
  Figure 4 Link: articels_figures_by_rev_year\2022\TransCL_Transformer_Makes_Strong_and_Flexible_Compressive_Learning\figure_4.jpg
  Figure 4 caption: An overview of our proposed transformer-based compressive learning
    (TransCL) framework, which is composed of compressed sensing module (CSM), Transformer-based
    Backbone (TB) and Task-oriented Head (TH).
  Figure 5 Link: articels_figures_by_rev_year\2022\TransCL_Transformer_Makes_Strong_and_Flexible_Compressive_Learning\figure_5.jpg
  Figure 5 caption: Detailed architectures of transformer-based backbone (TB) and
    task-oriented heads (TH) in our TransCL. (a) presents the details of our transformer-based
    backbone (TB). (b) is the illustration of task-oriented head for image classification
    (THIC). (c) presents the details of task-oriented head for semantic segmentation
    (THSS), which is a plain network to perform upsampling and predicting in a progressive
    manner.
  Figure 6 Link: articels_figures_by_rev_year\2022\TransCL_Transformer_Makes_Strong_and_Flexible_Compressive_Learning\figure_6.jpg
  Figure 6 caption: Illustration of compressed sensing module (CSM) in our TransCL.
  Figure 7 Link: articels_figures_by_rev_year\2022\TransCL_Transformer_Makes_Strong_and_Flexible_Compressive_Learning\figure_7.jpg
  Figure 7 caption: Illustration of generating the sampling matrix boldsymbolPhi Bgamma
    and the projection matrix mathbf WBgamma with arbitrary CS ratio gamma from their
    corresponding base matrices boldsymbolPhi and mathbf W , respectively.
  Figure 8 Link: articels_figures_by_rev_year\2022\TransCL_Transformer_Makes_Strong_and_Flexible_Compressive_Learning\figure_8.jpg
  Figure 8 caption: Visualization of privacy protection. We present the original image
    and reconstructed result generated by [61] without knowing the sampling matrix.
  Figure 9 Link: articels_figures_by_rev_year\2022\TransCL_Transformer_Makes_Strong_and_Flexible_Compressive_Learning\figure_9.jpg
  Figure 9 caption: Image classification performance of a single model (TransCL-32-arb)
    for handling input with arbitrary CS ratios. The first row presents the performance
    comparison in the case of input with a fixed CS ratio and arbitrary CS ratios.
    The second row is a partial enlargement of the region labeled with a green box
    in the first row, presenting the high robustness of our method in handling extremely
    low and arbitrary CS ratios.
  First author gender probability: 0.72
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Chong Mou
  Name of the last author: Jian Zhang
  Number of Figures: 18
  Number of Tables: 10
  Number of authors: 2
  Paper title: 'TransCL: Transformer Makes Strong and Flexible Compressive Learning'
  Publication Date: 2022-07-26 00:00:00
  Table 1 caption: "TABLE 1 Image Classification Performance on Validation Dataset\
    \ of ImageNet. \u201CTop-1 ACC\u201D Denotes the Top-1 ( % %) Accuracy. Performances\
    \ of Different Methods, Model Parameters, and the Number of Measurements for Each\
    \ Image are Reported"
  Table 10 caption: TABLE 10 Comparison of the Float-Point Sampling Matrix and the
    Binary Sampling Matrix. The Result Presents That Our TransCL has the Hardware-Friendly
    Property. The Image Classification Task is Conducted on TransCL-32, and the Semantic
    Segmentation Task is Conducted on TransCL-16
  Table 2 caption: TABLE 2 Image Classification Performance on Validation Datasets
    of CIFAR-10 and CIFAR-100 [33]. Top-1 ( % %) Accuracy is Presented Below
  Table 3 caption: TABLE 3 Quantitative Comparison of Semantic Segmentation Task on
    ADE20K [68] Dataset. Performances of Different Methods and the Number of Measurements
    for Each Image are Reported
  Table 4 caption: TABLE 4 Quantitative Comparison of Semantic Segmentation Task on
    Cityscapes [70] Dataset. Performances of Different Methods and the Number of Measurements
    for Each Image are Reported
  Table 5 caption: TABLE 5 Quantitative Comparison of Semantic Segmentation Task on
    Pascal Context [69] Dataset. Performances of Different Methods and the Number
    of Measurements for Each Image are Reported
  Table 6 caption: TABLE 6 Quantitative Comparison of Two-Stage Method and Our Single-Stage
    TransCL on Image Classification Task. We Present the Top-1 ( % %) Accuracy on
    ImageNet Validation Set in This Table
  Table 7 caption: "TABLE 7 Comparison of Complexity of Various Strategies to Infer\
    \ a 384\xD7384 384\xD7384 Image and the Top-1 ( % %) Accuracy on ImageNet [67]\
    \ Validation Set. The Additional Complexity Produced in Measurement Reconstruction\
    \ Stage is Highlighted in Red"
  Table 8 caption: TABLE 8 Quantitative Comparison of Our Compressive Learning Method
    With Traditional Data Compression Approaches on Image Classification Task. We
    Present the Top-1 ( % %) Accuracy on ImageNet Validation Set
  Table 9 caption: "TABLE 9 Complexity Comparison of Various CL Methods to Sample\
    \ a 384\xD7384 384\xD7384 Color Image, With the CS Ratio Being 1%"
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3194001
- Affiliation of the first author: college of electronic science and technology, nation
    university of defense technology (nudt), changsha, china
  Affiliation of the last author: college of electronic science and technology, nation
    university of defense technology (nudt), changsha, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Exploring_FineGrained_Sparsity_in_Convolutional_Neural_Networks_for_Efficient_In\figure_1.jpg
  Figure 1 caption: Connection between the sparsity in human brains (a) and the sparsity
    in convolutional neural networks (b) for image parsing.
  Figure 10 Link: articels_figures_by_rev_year\2022\Exploring_FineGrained_Sparsity_in_Convolutional_Neural_Networks_for_Efficient_In\figure_10.jpg
  Figure 10 caption: Comparison between our learning-based masks (red circles) and
    heuristic masks (yellow circles).
  Figure 2 Link: articels_figures_by_rev_year\2022\Exploring_FineGrained_Sparsity_in_Convolutional_Neural_Networks_for_Efficient_In\figure_2.jpg
  Figure 2 caption: Visualization of feature maps after the ReLU layer in PointNet++,
    RCAN, and PSMnet. Note that, sparsity is defined as the ratio of zeros in a channel.
  Figure 3 Link: articels_figures_by_rev_year\2022\Exploring_FineGrained_Sparsity_in_Convolutional_Neural_Networks_for_Efficient_In\figure_3.jpg
  Figure 3 caption: An illustration of sparse mask generation.
  Figure 4 Link: articels_figures_by_rev_year\2022\Exploring_FineGrained_Sparsity_in_Convolutional_Neural_Networks_for_Efficient_In\figure_4.jpg
  Figure 4 caption: An illustration of sparse mask convolution.
  Figure 5 Link: articels_figures_by_rev_year\2022\Exploring_FineGrained_Sparsity_in_Convolutional_Neural_Networks_for_Efficient_In\figure_5.jpg
  Figure 5 caption: An overview of our SMPointSeg network.
  Figure 6 Link: articels_figures_by_rev_year\2022\Exploring_FineGrained_Sparsity_in_Convolutional_Neural_Networks_for_Efficient_In\figure_6.jpg
  Figure 6 caption: Comparison of three different network acceleration techniques.
    A network pruning method (a) considers channels with redundant computation as
    redundant ones and prunes these channels for all data. An adaptive inference method
    (b) identifies data with redundant computation as redundant ones and skips all
    the computation on these data. In contrast, our sparse mask mechanism (c) takes
    the sparsity in both data and channel dimensions into consideration to prune redundant
    computation at a finer-grained level.
  Figure 7 Link: articels_figures_by_rev_year\2022\Exploring_FineGrained_Sparsity_in_Convolutional_Neural_Networks_for_Efficient_In\figure_7.jpg
  Figure 7 caption: An illustration of our sparse mask module (SMM) and geometry-aware
    point convolution (GAPconv).
  Figure 8 Link: articels_figures_by_rev_year\2022\Exploring_FineGrained_Sparsity_in_Convolutional_Neural_Networks_for_Efficient_In\figure_8.jpg
  Figure 8 caption: "Visualization of sparse masks learned in our SMM. Blue and green\
    \ regions in Mch represent channels with \u201Cdense\u201D and \u201Csparse\u201D\
    \ feature maps. Yellow and black points in Mpt refer to \u201Cimportant\u201D\
    \ and \u201Cunimportant\u201D locations."
  Figure 9 Link: articels_figures_by_rev_year\2022\Exploring_FineGrained_Sparsity_in_Convolutional_Neural_Networks_for_Efficient_In\figure_9.jpg
  Figure 9 caption: Comparison of sparsities achieved by different SMMs.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.73
  Name of the first author: Longguang Wang
  Name of the last author: Wei An
  Number of Figures: 18
  Number of Tables: 10
  Number of authors: 7
  Paper title: Exploring Fine-Grained Sparsity in Convolutional Neural Networks for
    Efficient Inference
  Publication Date: 2022-07-26 00:00:00
  Table 1 caption: TABLE 1 Comparative Results Achieved on the S3DIS Dataset (Area-5)
    by Our SMPointSeg With Different Settings
  Table 10 caption: TABLE 10 Comparative Results Achieved on SceneFlow
  Table 2 caption: TABLE 2 Comparative Results Achieved on the S3DIS Dataset (Area-5)
    by Our SMPointSeg With Different Sparsities
  Table 3 caption: TABLE 3 Comparative Results Achieved on the S3DIS Dataset (Area-5)
    by Our SMPointSeg With Different Settings
  Table 4 caption: TABLE 4 Comparative Results Achieved on the S3DIS Dataset (Area-5)
    by Our SMPointSeg With Different Kernel Sizes k k and Side Lengths l l
  Table 5 caption: TABLE 5 Results Achieved by Different Point-Based Methods on the
    S3DIS Dataset (6-Fold Cross-Validation)
  Table 6 caption: TABLE 6 Results Achieved by Different Methods on the SemanticKITTI
    Dataset
  Table 7 caption: "TABLE 7 Comparative Results Achieved for \xD7234 \xD7234 SR"
  Table 8 caption: "TABLE 8 Comparative Results Achieved on Set14 for \xD72 \xD72\
    \ SR"
  Table 9 caption: TABLE 9 Comparison to Existing Stereo Matching Methods on KITTI
    2015
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3193925
- Affiliation of the first author: australian artificial intelligence institute, university
    of technology sydney, ultimo, nsw, australia
  Affiliation of the last author: australian artificial intelligence institute, university
    of technology sydney, ultimo, nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2022\DSNet_Dynamic_Weight_Slicing_for_Efficient_Inference_in_CNNs_and_Vision_Transfor\figure_1.jpg
  Figure 1 caption: Illustration of dynamic networks on efficient inference. Input
    images are routed to use different architectures regarding their classification
    difficulty.
  Figure 10 Link: articels_figures_by_rev_year\2022\DSNet_Dynamic_Weight_Slicing_for_Efficient_Inference_in_CNNs_and_Vision_Transfor\figure_10.jpg
  Figure 10 caption: (a) Illustration of accuracy versus complexity of models in Tables
    12 and 13. (b) Gate distribution of DS-ResNet-M. The height of those colored blocks
    illustrate the partition of input samples that are routed to the sub-networks
    with respective routing signal boldsymbol phi .
  Figure 2 Link: articels_figures_by_rev_year\2022\DSNet_Dynamic_Weight_Slicing_for_Efficient_Inference_in_CNNs_and_Vision_Transfor\figure_2.jpg
  Figure 2 caption: Comparison of dynamic pruning methods [26], [27], [29], [30] and
    our dynamic weight slicing. Blocks in darker green represent more important weight
    elements (e.g., filters).
  Figure 3 Link: articels_figures_by_rev_year\2022\DSNet_Dynamic_Weight_Slicing_for_Efficient_Inference_in_CNNs_and_Vision_Transfor\figure_3.jpg
  Figure 3 caption: "Architecture of dynamic slice-able convolution neural networks\
    \ (DS-CNN++). The architecture of each supernet stage is adjusted adaptively by\
    \ the routing signal \u03D5 predicted by the gates. For simplicity, only channel\
    \ dimension is shown in this figure, see Fig. 4 for the illustration of dynamic\
    \ slicing in spatial dimension."
  Figure 4 Link: articels_figures_by_rev_year\2022\DSNet_Dynamic_Weight_Slicing_for_Efficient_Inference_in_CNNs_and_Vision_Transfor\figure_4.jpg
  Figure 4 caption: Illustrations of dynamic slicing in spatial dimensions.
  Figure 5 Link: articels_figures_by_rev_year\2022\DSNet_Dynamic_Weight_Slicing_for_Efficient_Inference_in_CNNs_and_Vision_Transfor\figure_5.jpg
  Figure 5 caption: Architecture of dynamic slice-able vision transformer (DS-ViT++).
    Embedding dimension, Q-K-V dimension, head number, MLP ratio and layer number
    are predictively adjusted by routing signal boldsymbol phi , with respect to different
    inputs.
  Figure 6 Link: articels_figures_by_rev_year\2022\DSNet_Dynamic_Weight_Slicing_for_Efficient_Inference_in_CNNs_and_Vision_Transfor\figure_6.jpg
  Figure 6 caption: Dynamic idle slicing applied on DS-ViT++.
  Figure 7 Link: articels_figures_by_rev_year\2022\DSNet_Dynamic_Weight_Slicing_for_Efficient_Inference_in_CNNs_and_Vision_Transfor\figure_7.jpg
  Figure 7 caption: Training process of dynamic supernet with in-place bootstrapping.
  Figure 8 Link: articels_figures_by_rev_year\2022\DSNet_Dynamic_Weight_Slicing_for_Efficient_Inference_in_CNNs_and_Vision_Transfor\figure_8.jpg
  Figure 8 caption: Accuracy versus complexity on ImageNet.
  Figure 9 Link: articels_figures_by_rev_year\2022\DSNet_Dynamic_Weight_Slicing_for_Efficient_Inference_in_CNNs_and_Vision_Transfor\figure_9.jpg
  Figure 9 caption: Evaluation accuracy of the smallest sub-network boldsymbol psi
    S during supernet training with three different schemes.
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.85
  Name of the first author: Changlin Li
  Name of the last author: Xiaojun Chang
  Number of Figures: 10
  Number of Tables: 13
  Number of authors: 6
  Paper title: 'DS-Net++: Dynamic Weight Slicing for Efficient Inference in CNNs and
    Vision Transformers'
  Publication Date: 2022-07-27 00:00:00
  Table 1 caption: TABLE 1 Latency Comparison of ResNet-50 With 25% Channels
  Table 10 caption: TABLE 10 Ablation Analysis of Proposed Disentangled Two-Stage
    Optimization Scheme for Dynamic Networks
  Table 2 caption: TABLE 2 Comparison of Efficient Inference Methods With MobileNet
    [43] on ImageNet
  Table 3 caption: TABLE 3 Comparison of Efficient Inference Methods with ResNet50
    [40] on ImageNet
  Table 4 caption: TABLE 4 Comparison of State-of-the-Art Models Based on ViT [44]
    on ImageNet
  Table 5 caption: TABLE 5 Practical Speed-up of DS-Net++ on Different Hardwares,
    Comparing With the Corresponding Static and Dynamic Pruning (Implemented as Masking)
    Baselines
  Table 6 caption: TABLE 6 Orthogonal Efficiency Improvement of DynViT [75] and DS-ViT
  Table 7 caption: TABLE 7 Comparison of Transfer Learning Performance on CIFAR-10
    and CIFAR-100
  Table 8 caption: TABLE 8 Performance Comparison of DS-MBNet and MobileNet With FSSD
    on VOC Object Detection Task
  Table 9 caption: TABLE 9 Ablation Analysis of Dynamic Architecture
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3194044
- Affiliation of the first author: moe key lab of artificial intelligence, ai institute,
    shanghai jiao tong university, shanghai, china
  Affiliation of the last author: moe key lab of artificial intelligence, ai institute,
    shanghai jiao tong university, shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2022\OutofDomain_Human_Mesh_Reconstruction_via_Dynamic_Bilevel_Online_Adaptation\figure_1.jpg
  Figure 1 caption: 'Top: Problem setup of learning to reconstruct human meshes from
    out-of-domain streaming videos. In terms of visual inputs, typical domain gaps
    include plaincrowded backgrounds, slightheavy occlusions, simplecomplex motions,
    etc. Bottom: Other domain gaps are reflected in the statistics of body skeleton
    and camera parameters. Bone length refers to the sum of the lengths between human
    joints, whose topology is shared across datasets.'
  Figure 10 Link: articels_figures_by_rev_year\2022\OutofDomain_Human_Mesh_Reconstruction_via_Dynamic_Bilevel_Online_Adaptation\figure_10.jpg
  Figure 10 caption: Correlations between the re-projection loss of 2D keypoints and
    the Mean Per Joint Position Error (MPJPE). Before computing the re-projection
    loss, 2D keypoints are normalized to [-1,1] based on the image size. Each point
    represents a target sample. We use the color intensity to indicate the point density.
    The points inside the dashed ellipses are typical samples suffering from 3D ambiguity
    (with low 2D re-projection errors but high 3D reconstruction errors).
  Figure 2 Link: articels_figures_by_rev_year\2022\OutofDomain_Human_Mesh_Reconstruction_via_Dynamic_Bilevel_Online_Adaptation\figure_2.jpg
  Figure 2 caption: "A brief introduction to bilevel online adaptation. The lower-level\
    \ probing step proposes a rational model \u03D5 \u2032 under the frame-wise loss\
    \ L F . The upper-level optimization step further considers the temporal loss\
    \ L T , and updates the parameters of \u03D5 using the gradients produced by \u03D5\
    \ \u2032 . Without upper-level optimization, the model is prone to produce incorrect\
    \ results due to depth ambiguity."
  Figure 3 Link: articels_figures_by_rev_year\2022\OutofDomain_Human_Mesh_Reconstruction_via_Dynamic_Bilevel_Online_Adaptation\figure_3.jpg
  Figure 3 caption: "An overview of the proposed approach of Dynamic Bilevel Online\
    \ Adaptation (DynaBOA), where the lower-level training step serves as a weight\
    \ probe to find a feasible response to the frame-wise pose constraints, and the\
    \ upper-level training step minimizes the overall multi-objectives in space-time\
    \ and updates the model with approximated second-order derivatives. The model\
    \ uses an adaptive number of optimization steps to adjust dynamically to address\
    \ the non-stationary distribution shift over time. It also retrieves source exemplars\
    \ as 3D guidance to compensate for the lack of target 3D supervision. Finally,\
    \ the model updated by DynaBOA infers the parameters \u0398 \u02C6 t of SMPL and\
    \ the camera configurations C t \u02C6 of the current frame."
  Figure 4 Link: articels_figures_by_rev_year\2022\OutofDomain_Human_Mesh_Reconstruction_via_Dynamic_Bilevel_Online_Adaptation\figure_4.jpg
  Figure 4 caption: The reconstruction errors of various single-level optimization
    schemes, indicating the incompatibility between the space-time multi-objectives.
    We use MPJPE as the evaluation metric.
  Figure 5 Link: articels_figures_by_rev_year\2022\OutofDomain_Human_Mesh_Reconstruction_via_Dynamic_Bilevel_Online_Adaptation\figure_5.jpg
  Figure 5 caption: "Illustration of the cross-domain retrieval for 3D example guidance.\
    \ Top: The source data is processed offline. Bottom: A comparison of the efficiency\
    \ between the baseline method of complete retrieval and the proposed retrieval\
    \ method. \u201Cindexing\u201D refers to finding the data with the same index\
    \ in the source domain."
  Figure 6 Link: articels_figures_by_rev_year\2022\OutofDomain_Human_Mesh_Reconstruction_via_Dynamic_Bilevel_Online_Adaptation\figure_6.jpg
  Figure 6 caption: Showcases of the query images from the target domain and the corresponding
    retrieved images from the source domain. The retrieved results have similar postures
    to the query images, even though the persons in the query images are severely
    occluded (highlighted in the red boxes in the two columns on the right).
  Figure 7 Link: articels_figures_by_rev_year\2022\OutofDomain_Human_Mesh_Reconstruction_via_Dynamic_Bilevel_Online_Adaptation\figure_7.jpg
  Figure 7 caption: Performance of BOA (Algorithm 1) and DynaBOA (Algorithm 2) on
    non-stationary streaming data, where the key-frames correspond to the hard cases
    that can drastically increase the error of human mesh reconstruction, e.g., sudden
    movements, severe occlusions, and scene changes. Thanks to the dynamic update
    strategy, DynaBOA adaptively fits the hard samples while avoiding overfitting
    regular frames.
  Figure 8 Link: articels_figures_by_rev_year\2022\OutofDomain_Human_Mesh_Reconstruction_via_Dynamic_Bilevel_Online_Adaptation\figure_8.jpg
  Figure 8 caption: Showcases of out-of-domain mesh reconstruction on 3DPW streaming
    data. Red circles highlight the examples of depth ambiguity.
  Figure 9 Link: articels_figures_by_rev_year\2022\OutofDomain_Human_Mesh_Reconstruction_via_Dynamic_Bilevel_Online_Adaptation\figure_9.jpg
  Figure 9 caption: Results of optimizing the model N steps on each frame, i.e., repeating
    the operations in Lines 4-9 in Algorithm 1 for N times. As N grows, BOA performs
    better than its single-level counterpart, showing the ability to prevent overfitting.
    Experiments are conducted under the PS protocol on 3DPW.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.89
  Name of the first author: Shanyan Guan
  Name of the last author: Xiaokang Yang
  Number of Figures: 16
  Number of Tables: 10
  Number of authors: 6
  Paper title: Out-of-Domain Human Mesh Reconstruction via Dynamic Bilevel Online
    Adaptation
  Publication Date: 2022-07-27 00:00:00
  Table 1 caption: TABLE 1 Technical Summary of DynaBOA and Our Preliminary work [23],
    Termed as the Original BOA
  Table 10 caption: TABLE 10 Ablation Studies on the New Contributions of DynaBOA
    (PS on 3DPW) Compared With the Original BOA Approach
  Table 2 caption: TABLE 2 Comparison of Different Retrieval Strategies
  Table 3 caption: TABLE 3 Typical Domain Gaps Between the Source Dataset (Human3.6M)
    and the Target Datasets (3DPW, 3DHP, SURREAL) Used in Our Experiments in Terms
    of Focal Length, Bone Length, Camera Distance, and Camera height [63]
  Table 4 caption: TABLE 4 Illustration of Different Protocols for Data Preprocessing
    on the 3DPW Dataset, Which Have Great Impacts on the Final Resuls.
  Table 5 caption: TABLE 5 Results on the 3DPW Test Set Using Three Data Preprocessing
    Protocols
  Table 6 caption: TABLE 6 Results on the Test Split of 3DHP
  Table 7 caption: TABLE 7 Results on the SURREAL Test Split ( D test Dtest)
  Table 8 caption: TABLE 8 Results of Using Different Online Optimization Schemes
    (PS on 3DPW)
  Table 9 caption: TABLE 9 Ablation Studies on Different Forms of Temporal Constraints
    Used in the Upper-Level BOA Step (PS on 3DPW)
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3194167
- Affiliation of the first author: bair in berkeley, berkeley, ca, usa
  Affiliation of the last author: computer science and artificial intelligence laboratory,
    massachusetts institute of technology, cambridge, ma, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\IncidentsM_A_LargeScale_Dataset_of_Images_With_Natural_Disasters_Damage_and_Inci\figure_1.jpg
  Figure 1 caption: Incidents1M Dataset images. These are some images from the Incidents1M
    Dataset, with their corresponding incident and place labels. 275,381 images have
    more than one incident labels. The place labels provide additional context for
    where the incident is occurring.
  Figure 10 Link: articels_figures_by_rev_year\2022\IncidentsM_A_LargeScale_Dataset_of_Images_With_Natural_Disasters_Damage_and_Inci\figure_10.jpg
  Figure 10 caption: StyleGAN2-generated images. Here are images generated from a
    places class-conditional StyleGAN2 model. The model is trained with the 764,124
    images that have at least one class-positive place label. We do not condition
    on incident labels, yet random seed sampling in the latent space yields diverse,
    realistic looking images.
  Figure 2 Link: articels_figures_by_rev_year\2022\IncidentsM_A_LargeScale_Dataset_of_Images_With_Natural_Disasters_Damage_and_Inci\figure_2.jpg
  Figure 2 caption: "Dataset statistics. (Left) The number of class-positive and class-negative\
    \ images for the incident and place categories. (Middle) The number of images\
    \ that contain both class-positive incidents, excluding the same-incident diagonal.\
    \ This highlights interesting incident correlations (e.g., \u201Cearthquake\u201D\
    \ and \u201Ccollapsed\u201D). (Right) The number of images with both a class-positive\
    \ incident and place label (e.g., \u201Ctraffic jam\u201D and \u201Chighway\u201D\
    )."
  Figure 3 Link: articels_figures_by_rev_year\2022\IncidentsM_A_LargeScale_Dataset_of_Images_With_Natural_Disasters_Damage_and_Inci\figure_3.jpg
  Figure 3 caption: Multi-label statistics. The Incidents1M Dataset has multiple labels
    per image, both class-positive and class-negative. Some images have up to 6 class-positive
    incident labels. (Left) We show the number of images that have the specified number
    of class-positive labels. (Right) We show the same information but for place labels.
    (Left and right) Notice that the 0 columns indicate the number of class-negative
    images, where no labeled incidents or places occur.
  Figure 4 Link: articels_figures_by_rev_year\2022\IncidentsM_A_LargeScale_Dataset_of_Images_With_Natural_Disasters_Damage_and_Inci\figure_4.jpg
  Figure 4 caption: "Class-negative usefulness. (Top left) We show images scored highly\
    \ for the corresponding incident when trained with only class-positive labels.\
    \ (Bottom left) We show highly ranked images when both class-positive and class-negative\
    \ labels are used. Notice that the model using class-negatives is more robust\
    \ to similarly looking visual features or noisy labels (e.g., incorrect labeling\
    \ of \u201Cblocked\u201D due to its polysemous usage in sports context). (Right)\
    \ We show the per-class AP improvement for both models, with and without class-negatives.\
    \ AP increases for almost all incidents."
  Figure 5 Link: articels_figures_by_rev_year\2022\IncidentsM_A_LargeScale_Dataset_of_Images_With_Natural_Disasters_Damage_and_Inci\figure_5.jpg
  Figure 5 caption: Objects in the Incidents1M Dataset. We run a pretrained Mask R-CNN
    model on images with class-positive incidents. (Top left) We show the number of
    objects found in the entire dataset for a particular incident category. (Top right)
    We show a 2D histogram of incident images that contain at least one detected object.
    We normalize each row. (Bottom) We show qualitative examples of incidents with
    detected objects overlaid.
  Figure 6 Link: articels_figures_by_rev_year\2022\IncidentsM_A_LargeScale_Dataset_of_Images_With_Natural_Disasters_Damage_and_Inci\figure_6.jpg
  Figure 6 caption: Geographical distribution. Here we report the approximate locations
    of images according to URLs. We perform geolocation with queries to https:geolocation-db.com.
    (Top) A word cloud and heatmap indicating where most of the images are located.
    (Bottom) A histogram of the 50 most common domain names with the corresponding
    number of images from the geolocalized server.
  Figure 7 Link: articels_figures_by_rev_year\2022\IncidentsM_A_LargeScale_Dataset_of_Images_With_Natural_Disasters_Damage_and_Inci\figure_7.jpg
  Figure 7 caption: Temporal monitoring on Twitter. (Top) Histogram of tweets obtained
    from Twitter using natural disaster keywords from 2017-2018. Black bars indicate
    periods of time when our data collection server was inactive. (Bottom) Number
    of tweets with earthquake images per day after filtering with at least 0.5 confidence.
    For significant earthquakes (above 6.5 magnitude), we notice an increase in earthquake
    images immediately after the event. Furthermore, we notice a spike on July 20,
    2018 not reported in the NOAA database. We manually checked the tweets and found
    images referring to a severe flood in Japan, indicating that the flood damage
    may resemble earthquake damage.
  Figure 8 Link: articels_figures_by_rev_year\2022\IncidentsM_A_LargeScale_Dataset_of_Images_With_Natural_Disasters_Damage_and_Inci\figure_8.jpg
  Figure 8 caption: Incident detection on Flickr images. Here we show incident detection
    results on 26M Flickr images. (Top) On the left we show a heatmap of where the
    images are located, and on the right we show some images. Red dots are earthquake
    epicenters or volcano locations. (Middle) we show locations of images filtered
    with high confidence for earthquakes. These correspond to the 0.9 threshold (blue
    lines) in the graphs. We filter at different thresholds and show AccuracyXKm (percent
    of images within X kilometers from a red dot) increasing. (Bottom) The same but
    for volcanic eruptions.
  Figure 9 Link: articels_figures_by_rev_year\2022\IncidentsM_A_LargeScale_Dataset_of_Images_With_Natural_Disasters_Damage_and_Inci\figure_9.jpg
  Figure 9 caption: Temporal monitoring on Twitter. (Top left) We report mRTI for
    global events and IoU for common US events. (Bottom left) We show a similar result
    as in Fig. 7 but for filtered volcanic eruptions images and ground truth events.
    (Right) For more frequent events in the United States, we filter tweets for flood,
    tornado, snowstorm, and wildfire images and compare with ground truth frequency
    events obtained from NOAA.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ethan Weber
  Name of the last author: Antonio Torralba
  Number of Figures: 11
  Number of Tables: 3
  Number of authors: 6
  Paper title: 'Incidents1M: A Large-Scale Dataset of Images With Natural Disasters,
    Damage, and Incidents'
  Publication Date: 2022-07-29 00:00:00
  Table 1 caption: TABLE 1 Class-Negative Loss and Dataset Size
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Geographical Model Bias
  Table 3 caption: TABLE 3 Twitter Incident Detection
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3191996
- Affiliation of the first author: university of wuppertal, wuppertal, germany
  Affiliation of the last author: "university of g\xF6ttingen, g\xF6ttingen, germany"
  Figure 1 Link: articels_figures_by_rev_year\2022\Do_the_Math_Making_Mathematics_in_Wikipedia_Computable\figure_1.jpg
  Figure 1 caption: Mathematical semantic annotation in Wikipedia.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Do_the_Math_Making_Mathematics_in_Wikipedia_Computable\figure_2.jpg
  Figure 2 caption: The workflow of our context sensitive translation from LaTeX to
    CAS syntax.
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.66
  Name of the first author: "Andr\xE9 Greiner-Petter"
  Name of the last author: Bela Gipp
  Number of Figures: 2
  Number of Tables: 3
  Number of authors: 6
  Paper title: 'Do the Math: Making Mathematics in Wikipedia Computable'
  Publication Date: 2022-08-01 00:00:00
  Table 1 caption: TABLE 1 Mappings and Likelihoods for the Semantic LaTeX Macro of
    the General Hypergeometric Function in the DLMF
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Performance of Description Extractions via MLP for Low
    (2a) and High (2b) Relevance and the Performance of Translatons from LaTeX to
    semantic LaTeX (2c)
  Table 3 caption: TABLE 3 The Symbolic and Numeric Evaluations (Left) and the Benchmark
    Evaluation for Translations to Mathematica (Right)
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3195261
- Affiliation of the first author: school of informatics, xiamen university, xiamen,
    fujian, china
  Affiliation of the last author: department of ece and college of cis, northeastern
    university, boston, ma, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\Lattice_Network_for_Lightweight_Image_Restoration\figure_1.jpg
  Figure 1 caption: 'Comparisons of the popular basic feature building blocks for
    IR task, from left to right: (a) residual block (RB) in SRResNet [21], (b) residual
    block (RB) in EDSR [13], which removes Batch Normalization (BN) layer, (c) memory
    block (MB) in MemNet [10], (d) dense block (DB) in SRDenseNet [22], (e) residual
    dense block (RDB) in RDN [23], (f) residual channel attention block (RCAB) in
    RCAN [14], (g) dual residual block (DuRB) in DuRN [12], (h) the proposed lattice
    block (LB).'
  Figure 10 Link: articels_figures_by_rev_year\2022\Lattice_Network_for_Lightweight_Image_Restoration\figure_10.jpg
  Figure 10 caption: "Qualitative results of the SOTA lightweight models and LatticeNet\
    \ on Urban100 dataset for 4\xD7 SR."
  Figure 2 Link: articels_figures_by_rev_year\2022\Lattice_Network_for_Lightweight_Image_Restoration\figure_2.jpg
  Figure 2 caption: The butterfly-style topological structure. (a) The basic structure
    of a standard 2-channel lattice filter. (b) The proposed lattice block (LB).
  Figure 3 Link: articels_figures_by_rev_year\2022\Lattice_Network_for_Lightweight_Image_Restoration\figure_3.jpg
  Figure 3 caption: Candidate structure examples of LB (the blue color represents
    RB). (a) pair-wise cascaded RBs. (b) pair-wise concurrent RBs with an RB followed.
    (c) an RB with pair-wise concurrent RBs followed. (d) pair-wise concurrent RBs
    with pair-wise concurrent RBs followed.
  Figure 4 Link: articels_figures_by_rev_year\2022\Lattice_Network_for_Lightweight_Image_Restoration\figure_4.jpg
  Figure 4 caption: The network structure of LatticeNet for image SR task, which consists
    of shallow feature extraction module, several lattice blocks (LBs) and backward
    fusion module (BFM).
  Figure 5 Link: articels_figures_by_rev_year\2022\Lattice_Network_for_Lightweight_Image_Restoration\figure_5.jpg
  Figure 5 caption: The learning procedure of combination coefficients. It contains
    two branches with mean and standard deviation to represent the statistics information
    of each channel.
  Figure 6 Link: articels_figures_by_rev_year\2022\Lattice_Network_for_Lightweight_Image_Restoration\figure_6.jpg
  Figure 6 caption: The network structure adopted for other IR tasks (including Gaussian
    denoising, real-world denoising, rain-streak removal, and raindrop removal) with
    the proposed lattice block (LB).
  Figure 7 Link: articels_figures_by_rev_year\2022\Lattice_Network_for_Lightweight_Image_Restoration\figure_7.jpg
  Figure 7 caption: "Visualized feature maps of V i\u22121 (z) in the first Lattice\
    \ block for image baboon on Set14. The values below each subfigure denote the\
    \ mean value, standard deviation and the corresponding coefficient, respectively."
  Figure 8 Link: articels_figures_by_rev_year\2022\Lattice_Network_for_Lightweight_Image_Restoration\figure_8.jpg
  Figure 8 caption: "Convergency analysis of various RB combination structures: 2RBs,\
    \ 2RBs+R1, 2RBs+R2, 4RBs, and LB on Set14 for 2\xD7 SR."
  Figure 9 Link: articels_figures_by_rev_year\2022\Lattice_Network_for_Lightweight_Image_Restoration\figure_9.jpg
  Figure 9 caption: "Qualitative results of the SOTA lightweight models and LatticeNet\
    \ on Set14 dataset for 3\xD7 and 4\xD7 image SR."
  First author gender probability: 0.67
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Xiaotong Luo
  Name of the last author: Yun Fu
  Number of Figures: 15
  Number of Tables: 13
  Number of authors: 6
  Paper title: Lattice Network for Lightweight Image Restoration
  Publication Date: 2022-08-01 00:00:00
  Table 1 caption: "TABLE 1 Quantitative Comparisons (PSNR) and Parameters (Params)\
    \ for 4\xD7 4\xD7 Image SR, Where SRResNet Means the Retrained Model With DIV2K\
    \ Dataset"
  Table 10 caption: TABLE 10 Quantitative Results (PSNR (dB)SSIM) With the State-of-the-Art
    Models for Rain-Steak Removal Task
  Table 2 caption: TABLE 2 Ablation Experiments About the Effectiveness of Lattice
    Block (LB) and Backward Fusion Module (BFM)
  Table 3 caption: TABLE 3 Experiments About the Performance of Various Combination
    Coefficient (CC) Setting
  Table 4 caption: "TABLE 4 Experiments About the Trade-Off of Network Parameters,\
    \ Accuracy and Inference Speed of LatticeNet for 4\xD7 4\xD7 Image SR"
  Table 5 caption: "TABLE 5 Quantitative Results for 2\xD7 2\xD7, 3\xD7 3\xD7, 4\xD7\
    \ 4\xD7 Image SR"
  Table 6 caption: "TABLE 6 Comparisons of Network Depth and Running Time (ms) With\
    \ the Popular SR Models for 4\xD7 4\xD7 Image SR"
  Table 7 caption: TABLE 7 Quantitative Results (PSNR(dB)SSIM) for Gaussian Denoising,
    Where DuRN is the Reproduced Results Under Our Experimental Settings
  Table 8 caption: TABLE 8 Comparison of Parameters and Muit-Adds Between DuRN and
    LB for Gaussian Denoising Task
  Table 9 caption: TABLE 9 Quantitative Results (PSNR(dB)SSIM) for Real-World Denoising
    Task, Where DuRN Indicates the Reproduction Results Under Our Experimental Settings
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3194090
- Affiliation of the first author: department of excellence in robotics, ai, scuola
    superiore santanna, pisa, italy
  Affiliation of the last author: department of excellence in robotics, ai, scuola
    superiore santanna, pisa, italy
  Figure 1 Link: articels_figures_by_rev_year\2022\On_the_Minimal_Adversarial_Perturbation_for_Deep_Neural_Networks_With_Provable_E\figure_1.jpg
  Figure 1 caption: "Illustration of the addressed problem. The blue points are DNN\
    \ inputs, while the black line f(x)=0 is the classification boundary that distinguishes\
    \ points belonging to the class \u22121 ( f(x)<0 ) and class 1 (f(x)>0) . The\
    \ dotted line starting from each point is the unknown optimal perturbation, which\
    \ is orthogonal to the classification boundary. The black arrows represent the\
    \ gradient directions. Observe that the gradients computed on the points whose\
    \ distance from the boundary is closer than \u03C3 provide a good approximation\
    \ to the minimal adversarial distance."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\On_the_Minimal_Adversarial_Perturbation_for_Deep_Neural_Networks_With_Provable_E\figure_2.jpg
  Figure 2 caption: "A graphical proof of Theorem 2. Lemma 3 ensures that in B(p,r)\
    \ the boundary belongs in the green area. Lemma 2 ensures that \u03BD(x) (in red)\
    \ lays in the brown area. In conclusion, there exists a solution T of RP, i.e.\
    \ an intersection between the boundary and the direction provided by the gradient."
  Figure 3 Link: articels_figures_by_rev_year\2022\On_the_Minimal_Adversarial_Perturbation_for_Deep_Neural_Networks_With_Provable_E\figure_3.jpg
  Figure 3 caption: "Comparison of the approximate distance t(x,l) , computed by the\
    \ Bisection CB strategy, and the ground-truth distance d(x,l) , for the four models\
    \ considered in Section 5.2. Each dot represents the pair (d(x,l),t(x,l)) where\
    \ x is a sample with label l . The region between the green line (slope 1) and\
    \ the other lines (slope 2 \u2013 \u221A , \u03C1 \u2217 ,2 ), highlights the\
    \ samples for which the Inequality 8 holds. Observe that, according to the theoretical\
    \ results, the closer the boundary (small d(x,l) ) the higher the number of dots\
    \ in the region of interest."
  Figure 4 Link: articels_figures_by_rev_year\2022\On_the_Minimal_Adversarial_Perturbation_for_Deep_Neural_Networks_With_Provable_E\figure_4.jpg
  Figure 4 caption: "Comparison of the distances computed by the Bisection CB strategy,\
    \ DeepFool, Decoupling Direction Norm (DDN), and Iterative Penalty with respect\
    \ to the ground-truth distance. For a clearer representation, the ground-truth\
    \ distance is partitioned into four intervals that contains a number of samples\
    \ summarized in Table 3. For each method, the lower and the upper side of each\
    \ box represent the first and the fourth quartile Q 1 and Q 2 , respectively;\
    \ the lower and the upper whisker represent the quantiles Q 1 \u22121.5\u22C5\
    \ I q and Q 3 +1.5\u22C5 I q , respectively, where I q is the interquartile range."
  Figure 5 Link: articels_figures_by_rev_year\2022\On_the_Minimal_Adversarial_Perturbation_for_Deep_Neural_Networks_With_Provable_E\figure_5.jpg
  Figure 5 caption: Attack success rate cumulative curve for attacks bounded in magnitude
    less than t(x)rho obtained with Bisection method and Closest Boundary strategy.
    The dashed red line represent hatsigma , which approximates sigma of Theorem 2.
    For MNIST and GTSRB, none of the samples with distance from the boundary less
    than hatsigma can be perturbed by the tested bounded attacks, in accordance with
    the theoretical results. For the FMNIST and the CIFAR10 dataset, instead, the
    estimation hatsigma results to be less accurate, failing in a tiny portions of
    the tested samples (3 and 5 samples overall respectively).
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Fabio Brau
  Name of the last author: Giorgio Buttazzo
  Number of Figures: 5
  Number of Tables: 5
  Number of authors: 4
  Paper title: On the Minimal Adversarial Perturbation for Deep Neural Networks With
    Provable Estimation Error
  Publication Date: 2022-08-01 00:00:00
  Table 1 caption: TABLE 1 Summary of the Most Frequent Symbols
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Average Distance From the Boundary and Average Number of
    Evaluations of the Models for the Four Datasets Obtained With Different Methods
  Table 3 caption: TABLE 3 Summary of the Number of Samples in Each Interval for Each
    Dataset
  Table 4 caption: "TABLE 4 Comparison of All the \u03C3 \u03C3 Estimated by the Different\
    \ Techniques"
  Table 5 caption: "TABLE 5 Comparison Between the Lower Bound t \u03C1 \u2217 t\u03C1\
    \ and \u03B2 L \u03B2L of CLEVER"
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3195616
- Affiliation of the first author: division of biostatistics, university of miami,
    miami, fl, usa
  Affiliation of the last author: center of proteomics and bioinformatics, case western
    reserve university, cleveland, oh, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\High_Dimensional_Mode_Hunting_Using_Pettiest_Components_Analysis\figure_1.jpg
  Figure 1 caption: The peeling procedure in PRIM.
  Figure 10 Link: articels_figures_by_rev_year\2022\High_Dimensional_Mode_Hunting_Using_Pettiest_Components_Analysis\figure_10.jpg
  Figure 10 caption: "MNIST modeling results 5\u20139 (PRIM)."
  Figure 2 Link: articels_figures_by_rev_year\2022\High_Dimensional_Mode_Hunting_Using_Pettiest_Components_Analysis\figure_2.jpg
  Figure 2 caption: Counterexample for a mixed multivariate distribution. The blue
    line is normal N(0,1) and the red line is Laplace L(0,1) .
  Figure 3 Link: articels_figures_by_rev_year\2022\High_Dimensional_Mode_Hunting_Using_Pettiest_Components_Analysis\figure_3.jpg
  Figure 3 caption: Region obtained by PRIM.
  Figure 4 Link: articels_figures_by_rev_year\2022\High_Dimensional_Mode_Hunting_Using_Pettiest_Components_Analysis\figure_4.jpg
  Figure 4 caption: Simulation results by method.
  Figure 5 Link: articels_figures_by_rev_year\2022\High_Dimensional_Mode_Hunting_Using_Pettiest_Components_Analysis\figure_5.jpg
  Figure 5 caption: Ranking changes by threshold.
  Figure 6 Link: articels_figures_by_rev_year\2022\High_Dimensional_Mode_Hunting_Using_Pettiest_Components_Analysis\figure_6.jpg
  Figure 6 caption: Cross-validation per digit.
  Figure 7 Link: articels_figures_by_rev_year\2022\High_Dimensional_Mode_Hunting_Using_Pettiest_Components_Analysis\figure_7.jpg
  Figure 7 caption: "MNIST modeling results 0\u20134 (fastPRIM)."
  Figure 8 Link: articels_figures_by_rev_year\2022\High_Dimensional_Mode_Hunting_Using_Pettiest_Components_Analysis\figure_8.jpg
  Figure 8 caption: "MNIST modeling results 5\u20139 (fastPRIM)."
  Figure 9 Link: articels_figures_by_rev_year\2022\High_Dimensional_Mode_Hunting_Using_Pettiest_Components_Analysis\figure_9.jpg
  Figure 9 caption: "MNIST modeling results 0\u20134 (PRIM)."
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: Tianhao Liu
  Name of the last author: Jean-Eudes Dazard
  Number of Figures: 14
  Number of Tables: 4
  Number of authors: 4
  Paper title: High Dimensional Mode Hunting Using Pettiest Components Analysis
  Publication Date: 2022-08-01 00:00:00
  Table 1 caption: TABLE 1 Density of Boxes Per Volume by Different Method
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Empirical Variance and Bias by Method
  Table 3 caption: TABLE 3 Final Region Size and Iterations Per Digit
  Table 4 caption: TABLE 4 Active Information by Method
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3195462
- Affiliation of the first author: school of mathematical and statistical science,
    southern illinois university carbondale, carbondale, il, usa
  Affiliation of the last author: school of mathematical and statistical science,
    southern illinois university carbondale, carbondale, il, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\A_New_Automatic_Hyperparameter_Recommendation_Approach_Under_LowRank_Tensor_Comp\figure_1.jpg
  Figure 1 caption: An example of performance data organization of RBF-SVM, which
    has two hyperparameters C and Gamma, in tensor format (left) and in vector format
    (right) on the UCI problem firm-teacher-clave-direction.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\A_New_Automatic_Hyperparameter_Recommendation_Approach_Under_LowRank_Tensor_Comp\figure_2.jpg
  Figure 2 caption: An example of 3D data organization. The first three frontal slices
    are the performance data of SVM on three historical problems while the fourth
    slice is the incomplete performance data on a new problem required to be completed.
  Figure 3 Link: articels_figures_by_rev_year\2022\A_New_Automatic_Hyperparameter_Recommendation_Approach_Under_LowRank_Tensor_Comp\figure_3.jpg
  Figure 3 caption: The automatic configuration recommendation framework based on
    our proposed approaches.
  Figure 4 Link: articels_figures_by_rev_year\2022\A_New_Automatic_Hyperparameter_Recommendation_Approach_Under_LowRank_Tensor_Comp\figure_4.jpg
  Figure 4 caption: The comparisons between RS, PSO, BO, and LRTC, where the reported
    performance is the average of ACA, ARA, and HR.
  Figure 5 Link: articels_figures_by_rev_year\2022\A_New_Automatic_Hyperparameter_Recommendation_Approach_Under_LowRank_Tensor_Comp\figure_5.jpg
  Figure 5 caption: The comparisons between CMF, LRTC, and their combinations on the
    average of ACA, ARA, and HR.
  Figure 6 Link: articels_figures_by_rev_year\2022\A_New_Automatic_Hyperparameter_Recommendation_Approach_Under_LowRank_Tensor_Comp\figure_6.jpg
  Figure 6 caption: The performance fluctuation of LRTC-RBF on SVM when the keeping
    ratio on the training tensor is changing from 10% to 100%.
  Figure 7 Link: articels_figures_by_rev_year\2022\A_New_Automatic_Hyperparameter_Recommendation_Approach_Under_LowRank_Tensor_Comp\figure_7.jpg
  Figure 7 caption: The comparisons between RS, PSO, BO and LRTC, where the reported
    performance is the average of ACA, ARA, and HR.
  Figure 8 Link: articels_figures_by_rev_year\2022\A_New_Automatic_Hyperparameter_Recommendation_Approach_Under_LowRank_Tensor_Comp\figure_8.jpg
  Figure 8 caption: The performance fluctuation of LRTC-RBF on ViT when the keeping
    ratio on the training tensor is changing from 10% to 100%.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.57
  Name of the first author: Liping Deng
  Name of the last author: Mingqing Xiao
  Number of Figures: 8
  Number of Tables: 6
  Number of authors: 2
  Paper title: A New Automatic Hyperparameter Recommendation Approach Under Low-Rank
    Tensor Completion e Framework
  Publication Date: 2022-08-01 00:00:00
  Table 1 caption: TABLE 1 The Comparisons of ACA (%), ARA (%), HR (%), and MRR Between
    LRTC and MFCF Under Various Keeping Ratios When Three Types of Kernels are Employed
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Wilcoxon Signed-Rank Test Results on CA Between LRTC and
    MFCF
  Table 3 caption: TABLE 3 The Comparisons of ACA (%), ARA (%), HR (%), and MRR Between
    KNN, KKNN, BPNN, MFCF, and CMF When Three Different Orders of Hyperparameters
    are Recommended
  Table 4 caption: TABLE 4 Wilcoxon Signed-Rank Test Results on CA Between KNN, KKNN,
    BPNN, MFCF, and CMF
  Table 5 caption: TABLE 5 The Comparisons of ACA (%), ARA (%), HR (%), and MRR Between
    LRTC and MFCF Under Various Keeping Ratios When Three Types of Kernels are Employed
  Table 6 caption: TABLE 6 Wilcoxon Signed-Rank Test Results on CA Between LRTC and
    MFCF
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3195658
