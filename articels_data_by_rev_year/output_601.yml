- Affiliation of the first author: institute of computing, university of campinas
    (unicamp), campinas, sp, brazil
  Affiliation of the last author: institute of computing, university of campinas (unicamp),
    campinas, sp, brazil
  Figure 1 Link: articels_figures_by_rev_year\2016\PSQP_Puzzle_Solving_by_Quadratic_Programming\figure_1.jpg
  Figure 1 caption: 'Different formulations of the jigsaw puzzle problem: (a) formulation
    considered in this work, in which pieces have linear boundaries, and (b) jigsaw
    puzzle formed by traditional puzzle pieces.'
  Figure 10 Link: articels_figures_by_rev_year\2016\PSQP_Puzzle_Solving_by_Quadratic_Programming\figure_10.jpg
  Figure 10 caption: Puzzles with 540 and 805 tiles.
  Figure 2 Link: articels_figures_by_rev_year\2016\PSQP_Puzzle_Solving_by_Quadratic_Programming\figure_2.jpg
  Figure 2 caption: "Examples of real puzzles. (a) Fragments from a mural found at\
    \ Huaca Bandera, 2010, photograph courtesy National Archaeological Museum of Br\xFC\
    ning, Peru. (b) Apictorial puzzle manufactured by Ravensburger."
  Figure 3 Link: articels_figures_by_rev_year\2016\PSQP_Puzzle_Solving_by_Quadratic_Programming\figure_3.jpg
  Figure 3 caption: Classification of relevant puzzle solving methods.
  Figure 4 Link: articels_figures_by_rev_year\2016\PSQP_Puzzle_Solving_by_Quadratic_Programming\figure_4.jpg
  Figure 4 caption: "Problem formulation. From top to bottom: each tile t i is assigned\
    \ to a location j . The result is represented by a permutation \u03C0 of the N=9\
    \ tiles; edge sets E H and E V representing horizontal and vertical neighboring\
    \ locations, respectively; and the final form of matrix A for this example."
  Figure 5 Link: articels_figures_by_rev_year\2016\PSQP_Puzzle_Solving_by_Quadratic_Programming\figure_5.jpg
  Figure 5 caption: "Simplified 2D gradient projection of the gradient \u2207f(p)\
    \ onto the space defined by the linear equality constraint, during a maximization\
    \ process, generating a constrained ascent direction s ."
  Figure 6 Link: articels_figures_by_rev_year\2016\PSQP_Puzzle_Solving_by_Quadratic_Programming\figure_6.jpg
  Figure 6 caption: Dissimilarities among tiles assigned to their correct location.
  Figure 7 Link: articels_figures_by_rev_year\2016\PSQP_Puzzle_Solving_by_Quadratic_Programming\figure_7.jpg
  Figure 7 caption: Example of the non-concave property of the global compatibility
    function. Note that the image contains several constant (white) tiles.
  Figure 8 Link: articels_figures_by_rev_year\2016\PSQP_Puzzle_Solving_by_Quadratic_Programming\figure_8.jpg
  Figure 8 caption: "Jigsaw puzzles with 432 tiles of size 28\xD728 pixels. In each\
    \ subfigure, we present the original image, the initial permutation for PSQP,\
    \ PSQP final permutation, and the result obtained by [9]."
  Figure 9 Link: articels_figures_by_rev_year\2016\PSQP_Puzzle_Solving_by_Quadratic_Programming\figure_9.jpg
  Figure 9 caption: "Jigsaw puzzles with 432 tiles of size 56\xD714 pixels. In each\
    \ subfigure, we present the initial permutation and the result."
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.83
  Name of the first author: "Fernanda A. Andal\xF3"
  Name of the last author: Siome Goldenstein
  Number of Figures: 15
  Number of Tables: 4
  Number of authors: 3
  Paper title: 'PSQP: Puzzle Solving by Quadratic Programming'
  Publication Date: 2016-03-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Sets of Parameters for PSQP, Depending on the Considered Dissimilarity
      Measure
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Metrics Computed for Each of the 20 Images Using Equation
      (10)
  Table 3 caption:
    table_text: TABLE 3 Accuracy Comparison
  Table 4 caption:
    table_text: TABLE 4 Accuracy Comparison
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2547394
- Affiliation of the first author: adobe research, san jose, ca
  Affiliation of the last author: school of engineering, university of california,
    merced, ca
  Figure 1 Link: articels_figures_by_rev_year\2016\TopDown_Visual_Saliency_via_Joint_CRF_and_Dictionary_Learning\figure_1.jpg
  Figure 1 caption: Proposed model. We construct a layered model on image patches
    for top-down visual saliency. In the bottom layer, we represent image patches
    X with the sparse codes S , using the dictionary D . In the top layer, the binary
    variables Y , which predict the target presence, form a Markov random field conditioned
    on sparse codes S , where the pairwise potentials impose the smoothness of label
    prediction. To learn the model, we develop a max-margin approach to deal with
    the partition function in the top layer such that the CRF parameters w and the
    dictionary D are learned jointly.
  Figure 10 Link: articels_figures_by_rev_year\2016\TopDown_Visual_Saliency_via_Joint_CRF_and_Dictionary_Learning\figure_10.jpg
  Figure 10 caption: Comparison of AUC scores of various state-of-the-art saliency
    maps (indicated by different colors) for fixation prediction on the PASCALS dataset.
    The dots on the curves show the best AUC scores are obtained using Gaussian blur
    kernels.
  Figure 2 Link: articels_figures_by_rev_year\2016\TopDown_Visual_Saliency_via_Joint_CRF_and_Dictionary_Learning\figure_2.jpg
  Figure 2 caption: Comparison of bottom-up and top-down saliency maps for human fixation
    prediction. Warmer color (from red to blue) indicates higher saliency value.
  Figure 3 Link: articels_figures_by_rev_year\2016\TopDown_Visual_Saliency_via_Joint_CRF_and_Dictionary_Learning\figure_3.jpg
  Figure 3 caption: Patch-based precision-recall curves on Graz-02 dataset.
  Figure 4 Link: articels_figures_by_rev_year\2016\TopDown_Visual_Saliency_via_Joint_CRF_and_Dictionary_Learning\figure_4.jpg
  Figure 4 caption: Top-down saliency maps generated by the proposed, DSD and SUN
    models.
  Figure 5 Link: articels_figures_by_rev_year\2016\TopDown_Visual_Saliency_via_Joint_CRF_and_Dictionary_Learning\figure_5.jpg
  Figure 5 caption: Saliency maps of bicycle, car and person categories from the Graz-02
    dataset generated by the proposed algorithm. In each panel, we present the original
    image and the saliency map, respectively. Overall, the proposed saliency maps
    are able to locate objects with large viewpoint changes, scale variations and
    heavy occlusions.
  Figure 6 Link: articels_figures_by_rev_year\2016\TopDown_Visual_Saliency_via_Joint_CRF_and_Dictionary_Learning\figure_6.jpg
  Figure 6 caption: "Performance gain with training cycles. The dictionary size k=256\
    \ and the sparsity regularization term \u03BB=0.15 ."
  Figure 7 Link: articels_figures_by_rev_year\2016\TopDown_Visual_Saliency_via_Joint_CRF_and_Dictionary_Learning\figure_7.jpg
  Figure 7 caption: Within-category saliency detection results. We present representative
    saliency maps from 20 categories in a 10 times 2 table and each cell includes
    two test cases where the original image is on the left while the saliency map
    is on the right. The lowest to highest saliency measures are shown in color from
    blue to red (the saliency maps are best viewed in color).
  Figure 8 Link: articels_figures_by_rev_year\2016\TopDown_Visual_Saliency_via_Joint_CRF_and_Dictionary_Learning\figure_8.jpg
  Figure 8 caption: Confusion matrix for cross-category saliency maps. The red dot
    denotes the class with high saliency precision while the blue dot denotes the
    class with low saliency precision.
  Figure 9 Link: articels_figures_by_rev_year\2016\TopDown_Visual_Saliency_via_Joint_CRF_and_Dictionary_Learning\figure_9.jpg
  Figure 9 caption: Affinity of 20 categories in a two-dimensional space by their
    Laplacian eigenmap. The red dot denotes the class with high saliency precision
    while the blue dot denotes the class with low saliency precision.
  First author gender probability: 0.66
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jimei Yang
  Name of the last author: Ming-Hsuan Yang
  Number of Figures: 12
  Number of Tables: 2
  Number of authors: 2
  Paper title: Top-Down Visual Saliency via Joint CRF and Dictionary Learning
  Publication Date: 2016-03-28 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Precision Rates (%) at EER on the Graz-02 Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 PASCAL VOC 2007 Localization Results
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2547384
- Affiliation of the first author: department of computer science, university of western
    ontario, london, on, canada
  Affiliation of the last author: international computer science institute, uc berkeley,
    ca
  Figure 1 Link: articels_figures_by_rev_year\2016\Convexity_Shape_Prior_for_Binary_Segmentation\figure_1.jpg
  Figure 1 caption: 'Segmentation with convexity shape prior: (a) input image, (b)
    user scribbles, (c) segmentation with contrast sensitive length regularization.
    We optimized the weight of length with respect to ground truth. (d) segmentation
    with convexity shape prior.'
  Figure 10 Link: articels_figures_by_rev_year\2016\Convexity_Shape_Prior_for_Binary_Segmentation\figure_10.jpg
  Figure 10 caption: Additional results comparing length and convexity regularizers.
    Except for the second row, all images demonstrate sensitivity to length weight
    omega .
  Figure 2 Link: articels_figures_by_rev_year\2016\Convexity_Shape_Prior_for_Binary_Segmentation\figure_2.jpg
  Figure 2 caption: "Left: Example of discretized orientations given by a 5\xD75 stencil.\
    \ One orientation d i is highlighted. Middle: Set L i of all discrete lines on\
    \ image grid that are parallel to d i . Right: Example of a triple clique ( p\
    \ s , p t , p v ) that violates convexity constraint."
  Figure 3 Link: articels_figures_by_rev_year\2016\Convexity_Shape_Prior_for_Binary_Segmentation\figure_3.jpg
  Figure 3 caption: "Evaluation of E convexity . The top row shows current configuration\
    \ x l of pixels on line l . The second and the third rows show the number of pixels\
    \ p s with x s =1 before and after each pixel p t , that is, functions C \u2212\
    \ (t) and C + (t) . The last row shows the number of violated constraints for\
    \ each p t with x t =0 , resulting in total of 12 violations on the line."
  Figure 4 Link: articels_figures_by_rev_year\2016\Convexity_Shape_Prior_for_Binary_Segmentation\figure_4.jpg
  Figure 4 caption: "First row shows synthetic images with added noise \u03C3 noise\
    \ =0.2 ; Second and third rows show contours and masks of segmentation, \u03C9\
    =0.1 . We used log-likelihood appearance terms, ( \u03BC fg =0, \u03C3 fg =0.1)\
    \ and ( \u03BC bg =1, \u03C3 bg =0.1) . The convexity prior removes noise, connects\
    \ components and fills holes while preserving sharp corners."
  Figure 5 Link: articels_figures_by_rev_year\2016\Convexity_Shape_Prior_for_Binary_Segmentation\figure_5.jpg
  Figure 5 caption: "First and second rows show unary approximation terms of E convexity\
    \ as in (11) during the first and second iterations of trust region for the examples\
    \ in Fig. 4. Red-yellow colors denote preference to background, and blue-cyan\
    \ colors\u2014preference to foreground. Unary terms encourage filling of holesconcavities\
    \ and removal of protrusionsdisconnected components."
  Figure 6 Link: articels_figures_by_rev_year\2016\Convexity_Shape_Prior_for_Binary_Segmentation\figure_6.jpg
  Figure 6 caption: "Illustration of robustness to parameter omega : results for length\
    \ regularization are shown with blue color and for convexity shape prior\u2014\
    with green. See text for details."
  Figure 7 Link: articels_figures_by_rev_year\2016\Convexity_Shape_Prior_for_Binary_Segmentation\figure_7.jpg
  Figure 7 caption: "Illustration of robustness to parameter omega : results for length\
    \ regularization are shown with blue color and for convexity shape prior\u2014\
    with green. See text for details."
  Figure 8 Link: articels_figures_by_rev_year\2016\Convexity_Shape_Prior_for_Binary_Segmentation\figure_8.jpg
  Figure 8 caption: "Robustness to parameter omega : results for length regularization\
    \ are shown with blue color and for convexity shape prior\u2014with green. See\
    \ text for details."
  Figure 9 Link: articels_figures_by_rev_year\2016\Convexity_Shape_Prior_for_Binary_Segmentation\figure_9.jpg
  Figure 9 caption: 'Robustness to scale: segmentation results obtained for original
    images and their 25 and 11 percent scaled versions. We used omega = 10 and 16
    bins per color channel in all experiments. User scribbles were scaled accordingly
    for each experiment.'
  First author gender probability: 0.99
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Lena Gorelick
  Name of the last author: Claudia Nieuwenhuis
  Number of Figures: 18
  Number of Tables: 1
  Number of authors: 4
  Paper title: Convexity Shape Prior for Binary Segmentation
  Publication Date: 2016-03-28 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Comparison of Direct and Gradual Non-Submodularization
      Approach in Terms of Mean Running Time, Mean Number of Misclassified Pixels
      with Respect to the Ground Truth
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2547399
- Affiliation of the first author: robotics institute, carnegie mellon university,
    pittsburgh, pa
  Affiliation of the last author: robotics institute, carnegie mellon university
  Figure 1 Link: articels_figures_by_rev_year\2016\Selective_Transfer_Machine_for_Personalized_Facial_Expression_Analysis\figure_1.jpg
  Figure 1 caption: 'An illustration of the proposed Selective Transfer Machine (STM):
    (a) 2D PCA projection of positive (squares) and negative (circles) samples for
    a given AU (in this case AU 12 or lip-corner raiser) for three subjects. An ideal
    classifier separates AU 12 nearly perfectly for each subject. (b) A generic classifier
    trained on all three subjects generalizes poorly to a new person (i.e., test subject)
    due to individual differences between the 3-subject training set and the new person.
    STM personalizes a generic classifier and reliably separates an AU for a new subject.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Selective_Transfer_Machine_for_Personalized_Facial_Expression_Analysis\figure_2.jpg
  Figure 2 caption: 'Visualization of samples from the RU-FACS dataset [4] in 3D eigenspace:
    colorsmarkers indicate different (a) positivenegative classes, and (b) subjects
    (best viewed in color).'
  Figure 3 Link: articels_figures_by_rev_year\2016\Selective_Transfer_Machine_for_Personalized_Facial_Expression_Analysis\figure_3.jpg
  Figure 3 caption: Fitting a line to a quadratic function using KMM and other re-weighting
    methods. The larger size (more red) of training data, the more weight KMM adopted.
    As can be observed, KMM puts higher weights in the training samples closer to
    the test ones. Compared to standard OLS or WOLS, KMM leads to better approximation
    for the test data.
  Figure 4 Link: articels_figures_by_rev_year\2016\Selective_Transfer_Machine_for_Personalized_Facial_Expression_Analysis\figure_4.jpg
  Figure 4 caption: 'Loss functions: (a) L 1 and L 2 loss, and (b) Huber loss.'
  Figure 5 Link: articels_figures_by_rev_year\2016\Selective_Transfer_Machine_for_Personalized_Facial_Expression_Analysis\figure_5.jpg
  Figure 5 caption: Comparisons of a generic SVM, a personalized STM, and an ideal
    classifier for synthetic data. The left-most figure shows the convergence curve
    of the objective value. Figures it1,4, 8,12 with trainingtest accuracy (Tr% and
    Te%) show the corresponding hyperplanes at each iteration. Grey (shaded) dots
    denote training samples, and white (unshaded) dots denote test samples. Circles
    and squares denote two different classes. Note that it1 indicates the results
    of KMM [31]. STM improves separation relative to the generic SVM as early as the
    1st iteration and converges toward the ideal hyperplane by the 12th iteration.
  Figure 6 Link: articels_figures_by_rev_year\2016\Selective_Transfer_Machine_for_Personalized_Facial_Expression_Analysis\figure_6.jpg
  Figure 6 caption: Comparison of different methods on the RU-FACS dataset. Light
    yellow (dark green) indicates AU 12 presense (absense) of Subject 12. The numbers
    in parentheses are F1 scores. Two misclassified frames of STM were chosen and
    fed into L-STM with correct labels.
  Figure 7 Link: articels_figures_by_rev_year\2016\Selective_Transfer_Machine_for_Personalized_Facial_Expression_Analysis\figure_7.jpg
  Figure 7 caption: Example images from (a) CK+ [44], (b) GEMEP-FERA [67], and (c)
    RU-FACS [4] datasets.
  Figure 8 Link: articels_figures_by_rev_year\2016\Selective_Transfer_Machine_for_Personalized_Facial_Expression_Analysis\figure_8.jpg
  Figure 8 caption: "Analysis experiments: (a)\u2013(b) Objective and variable differences\
    \ between iterations with initialization mathbf w0 (STM mathbf w ) and mathbf\
    \ s0 (STM mathbf s ), respectively. (c) Performance versus parameter choices.\
    \ (d) Per-subject F1 score versus training subjects. (e) Exemplar images of the\
    \ GFT dataset [57]."
  Figure 9 Link: articels_figures_by_rev_year\2016\Selective_Transfer_Machine_for_Personalized_Facial_Expression_Analysis\figure_9.jpg
  Figure 9 caption: 'Performance versus domain size: The averaged and standard deviation
    of F1 score on (a) RU-FACS. (b) and (c) show the F1 scores on the GFT dataset
    before and after removing the outlier subjects.'
  First author gender probability: 0.95
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Wen-Sheng Chu
  Name of the last author: Jeffrey F. Cohn
  Number of Figures: 9
  Number of Tables: 10
  Number of authors: 3
  Paper title: Selective Transfer Machine for Personalized Facial Expression Analysis
  Publication Date: 2016-03-28 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Representative Feature Extraction Methods
  Table 10 caption:
    table_text: TABLE 10 Expression Detection with AUC on (a) CK+ and (b) GEMEP-FERA
  Table 2 caption:
    table_text: TABLE 2 Representative Classifiers
  Table 3 caption:
    table_text: TABLE 3 Compare STM with Related Transductive Transfer Learning Methods
  Table 4 caption:
    table_text: TABLE 4 Detailed Content of Different Datasets
  Table 5 caption:
    table_text: TABLE 5 Within-Subject AU Detection with STM and PS Classifiers
  Table 6 caption:
    table_text: TABLE 6 Selection Percentage of STM for Different Subjects
  Table 7 caption:
    table_text: "TABLE 7 Cross-Subject AU Detection on RU-FACS Dataset. \u201CSA (NN|SVM)\u201D\
      \ Indicates SA with NN and SVM, Respectively"
  Table 8 caption:
    table_text: TABLE 8 Cross-Subject AU Detection on CK+ Dataset
  Table 9 caption:
    table_text: "TABLE 9 Cross-Dataset AU Detection: (a) RU-FACS \u2192 GEMEP-FERA,\
      \ and (b) GFT \u2192 RU-FACS (\u201CA \u2192 B\u201D Represents for Training\
      \ on Dataset A and Test on B)"
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2547397
- Affiliation of the first author: department of computer science, university of toronto,
    40 st. george st., bahen centre, toronto, ontario, canada
  Affiliation of the last author: department of computer science, university of toronto,
    40 st. george st., bahen centre, toronto, ontario, canada
  Figure 1 Link: articels_figures_by_rev_year\2016\D_Shape_and_Indirect_Appearance_by_Structured_Light_Transport\figure_1.jpg
  Figure 1 caption: 'Snapshots from raw live indirect video. Clockwise from top: (1)
    A hand; note the vein pattern and the inter-reflections between fingers. (2) Pouring
    water into a glass. (3) Caustics formed inside a mug from specular inter-reflections;
    note the secondary reflections to the board behind the mug and from the board
    onto the mug''s exterior surface. (4) Refractions and caustics from a beer glass.
    See Figs. 12 and 18 for more indirect-only images and [9] for videos.'
  Figure 10 Link: articels_figures_by_rev_year\2016\D_Shape_and_Indirect_Appearance_by_Structured_Light_Transport\figure_10.jpg
  Figure 10 caption: Photo of our low-speed, low-cost prototype. The projector can
    be detached to change the stereo baseline. The optical path is shown in red.
  Figure 2 Link: articels_figures_by_rev_year\2016\D_Shape_and_Indirect_Appearance_by_Structured_Light_Transport\figure_2.jpg
  Figure 2 caption: 'Light transport in a stereo projector-camera system. Light can
    reach pixel i on the image in one of three general ways: by indirect transport
    from an arbitrary pixel p on the corresponding epipolar line (green path); by
    indirect transport from a pixel q that is not on that line (red path); or by direct
    surface reflection, starting from projector pixel r on the epipolar line (black
    path).'
  Figure 3 Link: articels_figures_by_rev_year\2016\D_Shape_and_Indirect_Appearance_by_Structured_Light_Transport\figure_3.jpg
  Figure 3 caption: The light transport equation when patterns and images are vectorized
    so that consecutive pixels on corresponding epipolar lines form subvectors p e
    and i e , respectively. Under this vectorization scheme, block T ef of the transport
    matrix describes transport from epipolar line f on the pattern to epipolar line
    e on the image. Blocks T ee , shown in green, contain the epipolar elements.
  Figure 4 Link: articels_figures_by_rev_year\2016\D_Shape_and_Indirect_Appearance_by_Structured_Light_Transport\figure_4.jpg
  Figure 4 caption: "Structure of an epipolar block T ee . Element T ee [i,r] describes\
    \ transport from projector pixel p e [r] to image pixel i e [i] . This element\
    \ is direct if and only the scene point projecting to both pixels is the same,\
    \ i.e., the point's stereo disparity is i\u2212r . The set of direct elements\
    \ therefore represents the scene's instantaneous disparity map. Conventional stereo\
    \ algorithms attempt to localize this set while assuming that the transport matrix\
    \ is zero everywhere else\u2014both inside and outside its epipolar blocks."
  Figure 5 Link: articels_figures_by_rev_year\2016\D_Shape_and_Indirect_Appearance_by_Structured_Light_Transport\figure_5.jpg
  Figure 5 caption: 'Experimental validation of non-epipolar dominance for a scene
    containing diffuse, translucent, refractive and mirror-like objects. From left-to-right:
    (1) View under an all-white projection pattern. (2) View when just one white vertical
    stripe is projected onto the scene. The many bright regions in this image occur
    because the stripe illuminates the book''s pages in three different ways: directly
    from the projector, by diffuse inter-reflection from the opposite page, and by
    specular reflection via the mirror. Their existence makes the scene hard to reconstruct
    with conventional techniques such as laser-stripe 3D scanning [4]. A magnified
    view of these regions is shown in the inset on the right. (3) View for another
    vertical stripe, part of which falls on the candle. The stripe appears very broad
    and poorly localized there, because of strong sub-surface scattering. (4) View
    when just one projector pixel illuminates the scene. Camera pixels along the epipolar
    line receive light travelling along both direct and epipolar indirect paths; note
    that, unlike (2), these camera pixels receive no light travelling along non-epipolar
    indirect paths. (5) View when a single pixel illuminates a point on the candle.'
  Figure 6 Link: articels_figures_by_rev_year\2016\D_Shape_and_Indirect_Appearance_by_Structured_Light_Transport\figure_6.jpg
  Figure 6 caption: "Left: The epipolar block T ee for epipolar line e . We show T\
    \ ee using the conventions of Fig. 4, i.e., its r th column comes from an image\
    \ of the scene acquired with only projector pixel p e [r] turned on. Middle: To\
    \ assess the image contribution of non-epipolar transport, we acquire the block\
    \ sum \u2211 E f=1 T ef and compare it to block T ee \u2014observe that non-epipolar\
    \ contributions indeed far surpass the epipolar indirect ones. To acquire the\
    \ block sum, we capture images of the scene while sweeping a vertical stripe on\
    \ the projector plane (see [9] for a video of the captured image sequence). The\
    \ r th column of the block sum is given by the pixels on epipolar line e when\
    \ the stripe is at p e [r] . Right: Horizontal cross-section of T ee and \u2211\
    \ E f=1 T ef for two image pixels. Observe that T ee 's cross-section (blue) is\
    \ sharp and unimodal whereas the block sum's (red) is trimodal for one pixel and\
    \ very broad for the other."
  Figure 7 Link: articels_figures_by_rev_year\2016\D_Shape_and_Indirect_Appearance_by_Structured_Light_Transport\figure_7.jpg
  Figure 7 caption: The four basic probing matrices used in this paper. Their block
    structure mirrors the structure of mathbf T in Fig. 3.
  Figure 8 Link: articels_figures_by_rev_year\2016\D_Shape_and_Indirect_Appearance_by_Structured_Light_Transport\figure_8.jpg
  Figure 8 caption: Example layouts for color RGB, monochrome 6-pattern, and monochrome
    6-pattern indirect-invariant structured light imaging.
  Figure 9 Link: articels_figures_by_rev_year\2016\D_Shape_and_Indirect_Appearance_by_Structured_Light_Transport\figure_9.jpg
  Figure 9 caption: "Deriving random patternmask pairs for three cases of SLT imaging.\
    \ The derived patterns and masks are indicated with red and green borders, respectively.\
    \ Row 1: For indirect-only imaging, the patterns and masks are constant along\
    \ epipolar lines, with approximately half of them \u201Con.\u201D Row 2: Six of\
    \ the nine structured-light patterns we used. Rows 3-4: The masks for indirect-invariant\
    \ structured light are identical to those for indirect-only imaging but the projection\
    \ patterns differ. To generate them for a given grayscale structured-light pattern,\
    \ we first generate a random sequence of binary patterns (Row 3) and then use\
    \ that sequence, along with the sequence of masks, to compute the projection patterns.\
    \ Row 4 shows one such example. Row 5: We generate patternmask pairs for 6-shot\
    \ imaging as follows: (1) create 6 random binary images representing pixel membership\
    \ for each pattern; (2) generate a sequence of 132 indirect-invariant binary patternmask\
    \ pairs for each of 6 grayscale structured-light patterns, as outlined in Rows\
    \ 2-4; (3) use the 792 projection patterns as is, and (4) multiply the masks element-wise\
    \ with the associated pixel memberships. Row 5 shows one such calculation, for\
    \ grayscale pattern mathbf p(1) ."
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Matthew O'Toole
  Name of the last author: Kiriakos N. Kutulakos
  Number of Figures: 18
  Number of Tables: 1
  Number of authors: 3
  Paper title: 3D Shape and Indirect Appearance by Structured Light Transport
  Publication Date: 2016-03-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 List of Parts for the Low-Speed System Shown in Fig. 10 and
      the High-Speed System Shown in Fig. 11
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2545662
- Affiliation of the first author: calvin team at the university of edinburgh and
    the thoth team, ljk laboratory at inria grenoble
  Affiliation of the last author: thoth team, ljk laboratory at inria grenoble
  Figure 1 Link: articels_figures_by_rev_year\2016\Analysing_Domain_Shift_Factors_between_Videos_and_Images_for_Object_Detection\figure_1.jpg
  Figure 1 caption: Example YTO frames with ground-truth bounding-boxes.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Analysing_Domain_Shift_Factors_between_Videos_and_Images_for_Object_Detection\figure_2.jpg
  Figure 2 caption: Impact of the domain shift factors when training on VOC and YTO
    for two detectors DPM (top row (a) and (b)) and R-CNN (bottom row (c) and (d))
    and for two test sets VOC (left column) and YTO (right column).
  Figure 3 Link: articels_figures_by_rev_year\2016\Analysing_Domain_Shift_Factors_between_Videos_and_Images_for_Object_Detection\figure_3.jpg
  Figure 3 caption: Example bounding-boxes produced by PRE [21] (red), FVS [18] (blue),
    and ground-truth annotations (green).
  Figure 4 Link: articels_figures_by_rev_year\2016\Analysing_Domain_Shift_Factors_between_Videos_and_Images_for_Object_Detection\figure_4.jpg
  Figure 4 caption: '(top row) YTO dataset: Frames in the same shot that contain near
    identical samples of an object. (bottom row) VOC dataset: Example of near identical
    samples in the same image.'
  Figure 5 Link: articels_figures_by_rev_year\2016\Analysing_Domain_Shift_Factors_between_Videos_and_Images_for_Object_Detection\figure_5.jpg
  Figure 5 caption: Three example groups of near-identical samples in trainYTO. We
    display a subset of the frames for each group.
  Figure 6 Link: articels_figures_by_rev_year\2016\Analysing_Domain_Shift_Factors_between_Videos_and_Images_for_Object_Detection\figure_6.jpg
  Figure 6 caption: "(Left) 2D visualization of the trainYTO Unique Samples (red)\
    \ and trainVOC Motion Blurred Unique Samples (green) for the 'horse' class in\
    \ R-CNN feature space. Circles indicate the samples selected by our equalization\
    \ technique of Section 3.4: Equalization. (Right) evolution of d KL as more and\
    \ more sample pairs are added by our algorithm. The two distributions are very\
    \ similar at the beginning and start to diverge later. At the \u03F5=0.1 threshold,\
    \ 70 samples from each set are selected (left). This number is driven by \u03F5\
    \ and changes from class to class."
  Figure 7 Link: articels_figures_by_rev_year\2016\Analysing_Domain_Shift_Factors_between_Videos_and_Images_for_Object_Detection\figure_7.jpg
  Figure 7 caption: Video frame, VOC training image, Gaussian and motion blurred VOC
    training images.
  Figure 8 Link: articels_figures_by_rev_year\2016\Analysing_Domain_Shift_Factors_between_Videos_and_Images_for_Object_Detection\figure_8.jpg
  Figure 8 caption: 'Top row: Aspects common to both VOC (green) and YTO (red). Bottom
    row: Red: aspects occurring only in YTO. YouTube users often film their own pets
    doing funny things, such as jumping, rolling and going on a skateboard. Green:
    aspects occurring only in VOC. Chicken and birds flying are common in VOC, but
    do not appear in YTO.'
  Figure 9 Link: articels_figures_by_rev_year\2016\Analysing_Domain_Shift_Factors_between_Videos_and_Images_for_Object_Detection\figure_9.jpg
  Figure 9 caption: Impact of the domain shift factors when training on ILSVRC IMG
    and VID for the R-CNN detector.
  First author gender probability: 0.97
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Vicky Kalogeiton
  Name of the last author: Cordelia Schmid
  Number of Figures: 9
  Number of Tables: 2
  Number of authors: 3
  Paper title: Analysing Domain Shift Factors between Videos and Images for Object
    Detection
  Publication Date: 2016-04-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Number of Object Samples in the Training and Test Sets for
      Image (VOC) and Video (YTO) Domains
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Appearance Diversity Equalization
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2551239
- Affiliation of the first author: school of electrical and electronic engineering,
    nanyang technological university, 50 nanyang avenue, singapore
  Affiliation of the last author: school of electrical and electronic engineering,
    nanyang technological university, 50 nanyang avenue, singapore
  Figure 1 Link: articels_figures_by_rev_year\2016\Tracklet_Association_by_Online_TargetSpecific_Metric_Learning_and_Coherent_Dynam\figure_1.jpg
  Figure 1 caption: 'A difficult scenario of high appearance similarity among targets.
    (Frames from PETS dataset with pedestrian identities labeled by our method): Despite
    individuals 1 and 8 dressed in similarly colored clothes and severe occlusions
    and interactions between individuals 1, 8, and 9, their identities should remain
    unchanged.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Tracklet_Association_by_Online_TargetSpecific_Metric_Learning_and_Coherent_Dynam\figure_2.jpg
  Figure 2 caption: The proposed framework. In the cost-flow network, each node denotes
    a reliable tracklet, which is a tracklet with only one identity. The flow costs
    of edges are defined by negative log of the affinity scores, which are obtained
    through the online learning of target-specific metrics and motion dynamics with
    the off-line learned weights on segments of short-time sequences known as local
    segments.
  Figure 3 Link: articels_figures_by_rev_year\2016\Tracklet_Association_by_Online_TargetSpecific_Metric_Learning_and_Coherent_Dynam\figure_3.jpg
  Figure 3 caption: An example of the difficult situations.
  Figure 4 Link: articels_figures_by_rev_year\2016\Tracklet_Association_by_Online_TargetSpecific_Metric_Learning_and_Coherent_Dynam\figure_4.jpg
  Figure 4 caption: Influence of tracklet refinement process on tracking performance.
    Each plot presents the performance (measured by MOTA score) on a particular dataset
    w.r.t. the number of repetitions of tracklet refinement process.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Bing Wang
  Name of the last author: Li Wang
  Number of Figures: 4
  Number of Tables: 7
  Number of authors: 4
  Paper title: Tracklet Association by Online Target-Specific Metric Learning and
    Coherent Dynamics Estimation
  Publication Date: 2016-04-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Tracking Results with Network Flow Based Methods
      on TUD Crossing and ETH BAHNHOF (First 350 Frames) Sequences
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Tracking Results between State-of-the-Art Methods
      and Ours on PETS2009-S2L1
  Table 3 caption:
    table_text: TABLE 3 Comparison of Tracking Results between State-of-the-Art Methods
      and Ours on Town Centre Dataset
  Table 4 caption:
    table_text: TABLE 4 Comparison of Tracking Results between State-of-the-Art Methods
      and Ours on TUD Stadtmitte Dataset
  Table 5 caption:
    table_text: TABLE 5 Comparison of Tracking Results between State-of-the-Art Methods
      and Ours on ETH Dataset
  Table 6 caption:
    table_text: TABLE 6 Comparison of Tracking Results between State-of-the-Art Methods
      and Ours on MOTChallenge 2D Benchmark
  Table 7 caption:
    table_text: TABLE 7 Comparison of Tracking Results between State-of-the-Art Methods
      and Ours on PETS2009-S2L2
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2551245
- Affiliation of the first author: school of engineering, university of california,
    merced, ca
  Affiliation of the last author: school of engineering, university of california,
    merced, ca
  Figure 1 Link: articels_figures_by_rev_year\2016\LRegularized_Intensity_and_Gradient_Prior_for_Deblurring_Text_Images_and_Beyond\figure_1.jpg
  Figure 1 caption: A challenging blurred text image.
  Figure 10 Link: articels_figures_by_rev_year\2016\LRegularized_Intensity_and_Gradient_Prior_for_Deblurring_Text_Images_and_Beyond\figure_10.jpg
  Figure 10 caption: Quantitative comparison on the proposed text image dataset. The
    x -axis denotes the image index and the average PSNR values of all the images
    are shown on the rightmost column.
  Figure 2 Link: articels_figures_by_rev_year\2016\LRegularized_Intensity_and_Gradient_Prior_for_Deblurring_Text_Images_and_Beyond\figure_2.jpg
  Figure 2 caption: Intensity and gradient properties of text images. (a) A clear
    text image. (b) Pixel intensity distribution from (a). (c) Distribution of horizontal
    gradient from (a). (d) A blurred image. (e) Pixel intensity distribution from
    (d). (f) Distribution of horizontal gradient from (d).
  Figure 3 Link: articels_figures_by_rev_year\2016\LRegularized_Intensity_and_Gradient_Prior_for_Deblurring_Text_Images_and_Beyond\figure_3.jpg
  Figure 3 caption: "Effectiveness of Algorithm 1. (a) Blurred image and kernel. (b)-(c)\
    \ Results by only P t (x) and P t (\u2207x) . (d)-(e) Results by posing the estimation\
    \ of g in the outer and inner loop. The smooth regions enclosed in the red boxes\
    \ in (b) and (d) are not preserved well, while some characters in (c) are over\
    \ smoothed."
  Figure 4 Link: articels_figures_by_rev_year\2016\LRegularized_Intensity_and_Gradient_Prior_for_Deblurring_Text_Images_and_Beyond\figure_4.jpg
  Figure 4 caption: "Non-blind deconvolution examples. (a) Blurred images and the\
    \ estimated kernels. (b) Results by [35] with Laplacian prior. (c) Results by\
    \ setting \u03C3=0 in (5). (d) Ringing suppression results by [2]. (e) Our results."
  Figure 5 Link: articels_figures_by_rev_year\2016\LRegularized_Intensity_and_Gradient_Prior_for_Deblurring_Text_Images_and_Beyond\figure_5.jpg
  Figure 5 caption: "An example presented in [8]. (a) Blurred image and kernel; (b)\
    \ Results of [8]; (c) Our results without using P t (x) in the kernel estimation;\
    \ (d) Our final results; (e) Intermediate results of [8]; (f) Our intermediate\
    \ results (including x and u ); (g) Intermediate salient edges of [7]; (h) Intermediate\
    \ salient edges using only P t (\u2207x) ; (i) Intermediate results using only\
    \ P t (x) ; (j) Our intermediate salient edges, i.e., g in (12)."
  Figure 6 Link: articels_figures_by_rev_year\2016\LRegularized_Intensity_and_Gradient_Prior_for_Deblurring_Text_Images_and_Beyond\figure_6.jpg
  Figure 6 caption: Convergence properties of Algorithms 1 and 2. (a)-(b) show the
    convergence of Algorithm 1. (c)-(d) show the convergence of Algorithm 2 and the
    corresponding energies and kernel similarity values are computed from the finest
    level of Algorithm 2. The legend Intensity in (5) & Gradient in (6) in (c) and
    (d) indicates that the first terms in (5) and (6) use image intensity and image
    gradient, respectively.
  Figure 7 Link: articels_figures_by_rev_year\2016\LRegularized_Intensity_and_Gradient_Prior_for_Deblurring_Text_Images_and_Beyond\figure_7.jpg
  Figure 7 caption: Saturated images. (a) Clear images with saturated regions and
    ground-truth kernel. (b) Blurred images with saturated regions. (c) Binary images
    of (a). (d) Binary images of (b). (c) and (d) are obtained from (a) and (b) with
    the same threshold value.
  Figure 8 Link: articels_figures_by_rev_year\2016\LRegularized_Intensity_and_Gradient_Prior_for_Deblurring_Text_Images_and_Beyond\figure_8.jpg
  Figure 8 caption: Intermediate results with different priors in natural image deblurring.
    (a) Blurred image. (b) Intermediate result with Pt(nabla x) . (c) Deblurred result
    using [43] with fixed initial threshold 0.5 for fraclambda sigmabeta . (d) Deblurred
    result using the proposed algorithm with the adaptive initial threshold for fraclambda
    sigmabeta , estimated by Otsu algorithm [44]. (e) Intermediate results with Pt(nabla
    x) . (f) Intermediate results [43]. (g) Our intermediate results.
  Figure 9 Link: articels_figures_by_rev_year\2016\LRegularized_Intensity_and_Gradient_Prior_for_Deblurring_Text_Images_and_Beyond\figure_9.jpg
  Figure 9 caption: Effect of different priors used in intermediate latent image restoration.
    (a) Ground-truth clear image and the blur kernel. (b) Blurred image. (c)-(d) Non-blind
    deconvolution results with prior Pc(x) and P(x) . The red boxes in (c) enclose
    some artificial structures.
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jinshan Pan
  Name of the last author: Ming-Hsuan Yang
  Number of Figures: 20
  Number of Tables: 1
  Number of authors: 4
  Paper title: 'L

    0

    -Regularized Intensity and Gradient Prior for Deblurring Text Images and Beyond'
  Publication Date: 2016-04-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Comparison Using the Example Shown in Fig. 5a
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2551244
- Affiliation of the first author: department of electrical engineering and computer
    science, northwestern university, evanston
  Affiliation of the last author: viterbi faculty of electrical engineering, technion,
    israel institute of technology, haifa, israel
  Figure 1 Link: articels_figures_by_rev_year\2016\Triangulation_in_Random_Refractive_Distortions\figure_1.jpg
  Figure 1 caption: Images of a sample stereo pair taken by looking upward through
    a wavy water surface. By triangulation, using our method we obtain a likelihood
    function in 3D. The estimated location is the position of the maximum likelihood
    (ML).
  Figure 10 Link: articels_figures_by_rev_year\2016\Triangulation_in_Random_Refractive_Distortions\figure_10.jpg
  Figure 10 caption: Undetected tracking failure and its effect on localization error.
    The horizontal axis represents the frame at which tracking loss occurred in one
    of the views. The vertical axis is barvarepsilonmathbf a , using Nmathrmframes=16
    .
  Figure 2 Link: articels_figures_by_rev_year\2016\Triangulation_in_Random_Refractive_Distortions\figure_2.jpg
  Figure 2 caption: Relevant scenarios. (a) Birds fly by to hunt for a submerged fish.
    (b) The archer fish shoots water jets to shoot down airborne flies. (c) A submarine
    avoiding use of a physical periscope by upward vision through a wavy WAI.
  Figure 3 Link: articels_figures_by_rev_year\2016\Triangulation_in_Random_Refractive_Distortions\figure_3.jpg
  Figure 3 caption: Viewing geometry through a flat water surface. [Inset] Definition
    of additional length parameters (magenta).
  Figure 4 Link: articels_figures_by_rev_year\2016\Triangulation_in_Random_Refractive_Distortions\figure_4.jpg
  Figure 4 caption: Colors indicate matching. [Top] Correspondence produced by SIFT
    [32]. [Bottom] Correspondence produced by [19].
  Figure 5 Link: articels_figures_by_rev_year\2016\Triangulation_in_Random_Refractive_Distortions\figure_5.jpg
  Figure 5 caption: '[Left] All the pixels x corresponding to an object point over
    time. The mean location estimates x flat . [Middle] The distribution of x given
    x flat . The distribution of possible pixel positions is approximately normal
    around x flat . Red expresses high probability and blue low probability. [Right]
    The distribution of x flat given x .'
  Figure 6 Link: articels_figures_by_rev_year\2016\Triangulation_in_Random_Refractive_Distortions\figure_6.jpg
  Figure 6 caption: Position likelihood. Back-projecting all the pixels around x based
    on p( x flat |x) through a flat surface yields a 3D cone. This cone represents
    the position likelihood of the object imaged at x .
  Figure 7 Link: articels_figures_by_rev_year\2016\Triangulation_in_Random_Refractive_Distortions\figure_7.jpg
  Figure 7 caption: '[Left] A baseline that is significantly shorter than the typical
    correlation length of the WAI-slope, results in highly correlated distortions
    across views. [Right] A wide baseline results in uncorrelated distortions between
    the two views.'
  Figure 8 Link: articels_figures_by_rev_year\2016\Triangulation_in_Random_Refractive_Distortions\figure_8.jpg
  Figure 8 caption: (a) By projecting the uncertainties around x L and x R , the effective
    3D spatial support (orange region) of the overlap represents the 3D domain in
    which the airborne object point is likely to reside. (b) Additional temporal frames
    narrow this domain.
  Figure 9 Link: articels_figures_by_rev_year\2016\Triangulation_in_Random_Refractive_Distortions\figure_9.jpg
  Figure 9 caption: Airborne position estimation using multiple views. Red pixels
    are distorted projections of an object point. Green pixels are close to the distorted
    pixels, while their flat back-projections intersect in 3D at A .
  First author gender probability: 0.99
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Marina Alterman
  Name of the last author: Yohay Swirski
  Number of Figures: 20
  Number of Tables: 1
  Number of authors: 3
  Paper title: Triangulation in Random Refractive Distortions
  Publication Date: 2016-04-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of Experimental Results [cm]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2551740
- Affiliation of the first author: beijing key laboratory of big data management and
    analysis methods, school of information, renmin university of china, beijing,
    china
  Affiliation of the last author: computational bioscience research center (cbrc),
    cemse division, king abdullah university of science and technology (kaust), thuwal,
    saudi arabia
  Figure 1 Link: articels_figures_by_rev_year\2016\Learning_from_Weak_and_Noisy_Labels_for_Semantic_Segmentation\figure_1.jpg
  Figure 1 caption: The pipeline of our weakly supervised semantic segmentation model.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Learning_from_Weak_and_Noisy_Labels_for_Semantic_Segmentation\figure_2.jpg
  Figure 2 caption: Superpixel initial label assignment from image-level labels. The
    wrongly assigned labels are marked in red.
  Figure 3 Link: articels_figures_by_rev_year\2016\Learning_from_Weak_and_Noisy_Labels_for_Semantic_Segmentation\figure_3.jpg
  Figure 3 caption: "Typical examples of the soft-thresholding function z=softthr(x,y,\u03B3\
    ) . Here, \u03B3 is set to 1, 0.1, or 0.01."
  Figure 4 Link: articels_figures_by_rev_year\2016\Learning_from_Weak_and_Noisy_Labels_for_Semantic_Segmentation\figure_4.jpg
  Figure 4 caption: The architecture of our deep CNN appearance model. Our model contains
    eight layers, excluding the input layer.
  Figure 5 Link: articels_figures_by_rev_year\2016\Learning_from_Weak_and_Noisy_Labels_for_Semantic_Segmentation\figure_5.jpg
  Figure 5 caption: Comparison of different semantic segmentation methods under various
    noise settings. IOU is used as metrics for VOC, whilst total per-pixel accuracy
    is used for MSRC and SIFT-Flow.
  Figure 6 Link: articels_figures_by_rev_year\2016\Learning_from_Weak_and_Noisy_Labels_for_Semantic_Segmentation\figure_6.jpg
  Figure 6 caption: Comparison of different sparse learning models under various noisy
    settings.
  Figure 7 Link: articels_figures_by_rev_year\2016\Learning_from_Weak_and_Noisy_Labels_for_Semantic_Segmentation\figure_7.jpg
  Figure 7 caption: Qualitative results of semantic segmentation with clean and noisy
    tags on the SIFT-Flow dataset.
  Figure 8 Link: articels_figures_by_rev_year\2016\Learning_from_Weak_and_Noisy_Labels_for_Semantic_Segmentation\figure_8.jpg
  Figure 8 caption: Illustration of the effect of different parameters on our semantic
    segmentation algorithm for the MSRC dataset.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.95
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Zhiwu Lu
  Name of the last author: Xin Gao
  Number of Figures: 8
  Number of Tables: 8
  Number of authors: 6
  Paper title: Learning from Weak and Noisy Labels for Semantic Segmentation
  Publication Date: 2016-04-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Quality of the Tags of Both the Training and Test Images
      for the Two Versions of the Pascal VOC Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Our Method to [19] under the Same Noise Model
      on the SIFT-Flow Dataset
  Table 3 caption:
    table_text: TABLE 3 Results on the VOCT Dataset
  Table 4 caption:
    table_text: TABLE 4 Results on MSRC with Clean Tags
  Table 5 caption:
    table_text: TABLE 5 Results on SIFT-Flow with Clean Tags
  Table 6 caption:
    table_text: TABLE 6 Comparison of Our Algorithm to Alternative Noise Reduction
      Models on the MSRC Dataset
  Table 7 caption:
    table_text: TABLE 7 Comparison on Different Superpixel Representations and Appearance
      Models Used in Our Method on the SIFT-Flow Dataset
  Table 8 caption:
    table_text: TABLE 8 Comparison of Different Semantic Segmentation Methods in Terms
      of Runtime on the MSRC Dataset
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2552172
