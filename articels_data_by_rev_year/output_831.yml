- Affiliation of the first author: electrical engineering and computer science department,
    university of michigan, ann arbor, mi
  Affiliation of the last author: electrical engineering and computer science department,
    university of michigan, ann arbor, mi
  Figure 1 Link: articels_figures_by_rev_year\2017\Learning_Compositional_Sparse_Bimodal_Models\figure_1.jpg
  Figure 1 caption: Our overarching goal is to improve human-robotrobot-robot interaction
    across sensing modalities while aiming at generalization ability. Multi-modal
    compositional models are important for effective interactions.
  Figure 10 Link: articels_figures_by_rev_year\2017\Learning_Compositional_Sparse_Bimodal_Models\figure_10.jpg
  Figure 10 caption: Overall color and shape retrieval accuracy for both the paired
    and compositional models, by the number of dictionary elements used in learning.
  Figure 2 Link: articels_figures_by_rev_year\2017\Learning_Compositional_Sparse_Bimodal_Models\figure_2.jpg
  Figure 2 caption: Mapping the physical concepts from visual domain such as color,
    texture and shape to the spoken language domain.
  Figure 3 Link: articels_figures_by_rev_year\2017\Learning_Compositional_Sparse_Bimodal_Models\figure_3.jpg
  Figure 3 caption: Fourier representation of a triangular shape with 2 and 10 Fourier
    harmonics.
  Figure 4 Link: articels_figures_by_rev_year\2017\Learning_Compositional_Sparse_Bimodal_Models\figure_4.jpg
  Figure 4 caption: Approximate energy per frame for an audio recording of the word
    trapezium. Note the four peaks corresponding to the four syllables in the word.
    Red markers indicate 15 frames selected for use in the audio feature vector representing
    the sample.
  Figure 5 Link: articels_figures_by_rev_year\2017\Learning_Compositional_Sparse_Bimodal_Models\figure_5.jpg
  Figure 5 caption: Visualization of elements from both dictionaries. Each element
    is a concatenation of a visual feature and an audial feature. The visual features
    (the RGB color and the Fourier contour coefficients) are plotted for each sample
    element above, alongside a histogram indicating the number of original data samples
    that make use of the element in their sparse representations. The histogram is
    a visual substitute for the audial feature, which is difficult to visualize.
  Figure 6 Link: articels_figures_by_rev_year\2017\Learning_Compositional_Sparse_Bimodal_Models\figure_6.jpg
  Figure 6 caption: Word alignment accuracy over our dataset ( N=156 ) versus K ,
    the number of neighbors selected for use in the alignment procedure described
    in Section 2.3.1. Note that accuracy is 1.0 from K=16 to K=129 .
  Figure 7 Link: articels_figures_by_rev_year\2017\Learning_Compositional_Sparse_Bimodal_Models\figure_7.jpg
  Figure 7 caption: For the audial utterance blue halfcircle, (a) generated image
    by mapping from audial to visual domain. (b), (c) retrieval of color and shape
    neighbors by both models.
  Figure 8 Link: articels_figures_by_rev_year\2017\Learning_Compositional_Sparse_Bimodal_Models\figure_8.jpg
  Figure 8 caption: Comparison of correct retrievals by three different algorithms,
    multi-task learning (MTL), paired, and compositional. The dictionary learning
    methods use a dictionary size of 25 elements.
  Figure 9 Link: articels_figures_by_rev_year\2017\Learning_Compositional_Sparse_Bimodal_Models\figure_9.jpg
  Figure 9 caption: Noise elements of the compositional model's shape dictionary,
    for dictionary size of 50.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Suren Kumar
  Name of the last author: Jason J. Corso
  Number of Figures: 14
  Number of Tables: 2
  Number of authors: 4
  Paper title: Learning Compositional Sparse Bimodal Models
  Publication Date: 2017-04-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Shape and Color Exemplars in the Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 V-Measure Distance Matrix Between the Feature Representation
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2693987
- Affiliation of the first author: adobe research department, adobe systems inc, san
    jose, ca
  Affiliation of the last author: microsoft research, redmond, wa
  Figure 1 Link: articels_figures_by_rev_year\2017\Probabilistic_Elastic_Part_Model_A_PoseInvariant_Representation_for_RealWorld_Fa\figure_1.jpg
  Figure 1 caption: Matched image pairs present pose variations.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Probabilistic_Elastic_Part_Model_A_PoseInvariant_Representation_for_RealWorld_Fa\figure_2.jpg
  Figure 2 caption: Spatial-appearance feature extraction pipeline.
  Figure 3 Link: articels_figures_by_rev_year\2017\Probabilistic_Elastic_Part_Model_A_PoseInvariant_Representation_for_RealWorld_Fa\figure_3.jpg
  Figure 3 caption: Our pipeline of building the PEP-representations for face verification.
  Figure 4 Link: articels_figures_by_rev_year\2017\Probabilistic_Elastic_Part_Model_A_PoseInvariant_Representation_for_RealWorld_Fa\figure_4.jpg
  Figure 4 caption: 'Visualization of components in the PEP-model: each one shows
    the average appearance and spatial location of a Gaussian component. Each Gaussian
    component describes a facial part.'
  Figure 5 Link: articels_figures_by_rev_year\2017\Probabilistic_Elastic_Part_Model_A_PoseInvariant_Representation_for_RealWorld_Fa\figure_5.jpg
  Figure 5 caption: Examples for identifying the descriptors (shown as the rectangles
    on image) in building PEP-representations.
  Figure 6 Link: articels_figures_by_rev_year\2017\Probabilistic_Elastic_Part_Model_A_PoseInvariant_Representation_for_RealWorld_Fa\figure_6.jpg
  Figure 6 caption: 'Visualization of PEP-representations of face images: (a) the
    testing faces presents pose changes; (b) the PEP-representations alleviated pose
    changes while suffering from missing facial parts; (c) after adding the horizontally
    flipped image, the missing facial parts are complemented by its symmetrical counterpart;
    (d) without adding the horizontally flipped image, the PEP-representation evolves
    with 5, 10, 20, 50 frames (from left to right respectively) in a face track.'
  Figure 7 Link: articels_figures_by_rev_year\2017\Probabilistic_Elastic_Part_Model_A_PoseInvariant_Representation_for_RealWorld_Fa\figure_7.jpg
  Figure 7 caption: Each column shows descriptors (shown as image patches) identified
    by the same Gaussian component. In both figures, the row above shows partial PEP-representations
    from face A and the bottom ones are from face B in the Fig. 3.
  Figure 8 Link: articels_figures_by_rev_year\2017\Probabilistic_Elastic_Part_Model_A_PoseInvariant_Representation_for_RealWorld_Fa\figure_8.jpg
  Figure 8 caption: 'The effectiveness of spatial constraint: The highlighted patches
    locate the local descriptors induced the highest probabilities over the same Gaussian
    component. As shown in the top row, in lack of spatial constraint, the responses
    are more fuzzy and therefore the Gaussian component fails to locate two corresponded
    parts; while in the row below, with augmented spatial constraint, the enforced
    locality takes effects and results in a desired matching.'
  Figure 9 Link: articels_figures_by_rev_year\2017\Probabilistic_Elastic_Part_Model_A_PoseInvariant_Representation_for_RealWorld_Fa\figure_9.jpg
  Figure 9 caption: Spatial distribution of 10 selected Gaussian components in the
    PEP-model over a face. Each red ellipse (or circle) stands for a Gaussian component.
    The center and spanning show the mean and variance (standard deviation) of the
    spatial part of the Gaussian component.
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.95
  Name of the first author: Haoxiang Li
  Name of the last author: Gang Hua
  Number of Figures: 9
  Number of Tables: 10
  Number of authors: 2
  Paper title: 'Probabilistic Elastic Part Model: A Pose-Invariant Representation
    for Real-World Face Verification'
  Publication Date: 2017-04-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance Comparison on LFW Under the Restricted Without
      Outside Training Data Protocol
  Table 10 caption:
    table_text: TABLE 10 Performance Comparison on the MultiPIE Dataset
  Table 2 caption:
    table_text: TABLE 2 Performance Evaluation on the LFW with Respect to the Quality
      of Face Alignment
  Table 3 caption:
    table_text: TABLE 3 Performance on LFW with Respect to the Number of Gaussian
      Components
  Table 4 caption:
    table_text: TABLE 4 Performance on LFW with Respect to Spatial Constraint.
  Table 5 caption:
    table_text: TABLE 5 Performance on LFW with Respect to PCA and LDE Dimensionality
  Table 6 caption:
    table_text: TABLE 6 Performance Evaluation on the LFW for PEP-Representation,
      Fisher Kernel and VLAD
  Table 7 caption:
    table_text: TABLE 7 Performance Evaluation on the LFW Under the Unrestricted with
      Labeled Outside Data Protocol
  Table 8 caption:
    table_text: TABLE 8 Performance Comparison on YouTube Faces Under the Restricted
      without Outside Training Data Protocol
  Table 9 caption:
    table_text: TABLE 9 Performance Comparison on YouTube Faces Under the Unrestricted
      Protocol
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2695183
- Affiliation of the first author: nlpr, chinese academy of sciences, beijing, p.r.
    china
  Affiliation of the last author: mila lab, university of montreal, montreal, qc,
    canada
  Figure 1 Link: articels_figures_by_rev_year\2017\Drawing_and_Recognizing_Chinese_Characters_with_Recurrent_Neural_Network\figure_1.jpg
  Figure 1 caption: Illustration of three online handwritten Chinese characters. Each
    color represents a stroke and the numbers indicate the writing order. The purpose
    of this paper is to automatically recognize and draw (generate) real and cursive
    Chinese characters under a single framework based on recurrent neural networks.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Drawing_and_Recognizing_Chinese_Characters_with_Recurrent_Neural_Network\figure_2.jpg
  Figure 2 caption: (a) Removing of redundant points. (b) Coordinate normalization.
    (c) Character before preprocessing. (d) Character after preprocessing.
  Figure 3 Link: articels_figures_by_rev_year\2017\Drawing_and_Recognizing_Chinese_Characters_with_Recurrent_Neural_Network\figure_3.jpg
  Figure 3 caption: The stacked bidirectional RNN for end-to-end recognition.
  Figure 4 Link: articels_figures_by_rev_year\2017\Drawing_and_Recognizing_Chinese_Characters_with_Recurrent_Neural_Network\figure_4.jpg
  Figure 4 caption: Illustration of data augmentation by sequential dropout on the
    input sequence. The first column shows the original character, and the remaining
    columns are the characters after random dropout with probability 0.3.
  Figure 5 Link: articels_figures_by_rev_year\2017\Drawing_and_Recognizing_Chinese_Characters_with_Recurrent_Neural_Network\figure_5.jpg
  Figure 5 caption: 'For the time-step t in the generative RNN model: (a) illustration
    of the training process, and (b) illustration of the drawinggenerating process.'
  Figure 6 Link: articels_figures_by_rev_year\2017\Drawing_and_Recognizing_Chinese_Characters_with_Recurrent_Neural_Network\figure_6.jpg
  Figure 6 caption: Illustration of the generatingdrawing for one particular character
    in different epochs of the training process.
  Figure 7 Link: articels_figures_by_rev_year\2017\Drawing_and_Recognizing_Chinese_Characters_with_Recurrent_Neural_Network\figure_7.jpg
  Figure 7 caption: The character embedding matrix and the nearest neighbors (of some
    representative characters) calculated from the embedding matrix.
  Figure 8 Link: articels_figures_by_rev_year\2017\Drawing_and_Recognizing_Chinese_Characters_with_Recurrent_Neural_Network\figure_8.jpg
  Figure 8 caption: Illustration of the automatically generated characters for different
    classes. Each row represents a particular character class. To give a better illustration,
    each color (randomly selected) denotes one straight line (pen-down) as shown in
    Eq. (23).
  Figure 9 Link: articels_figures_by_rev_year\2017\Drawing_and_Recognizing_Chinese_Characters_with_Recurrent_Neural_Network\figure_9.jpg
  Figure 9 caption: "(a): The comparison of the classification accuracies for real\
    \ samples and generated samples on the 3,755 classes. (b): The generated characters\
    \ which have low recognition rates. (c): The generated characters which have high\
    \ recognition rates. For \u201CA rightarrow B\u201D, A means the accuracy for\
    \ real samples and B means the accuracy for generated samples."
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xu-Yao Zhang
  Name of the last author: Yoshua Bengio
  Number of Figures: 9
  Number of Tables: 3
  Number of authors: 5
  Paper title: Drawing and Recognizing Chinese Characters with Recurrent Neural Network
  Publication Date: 2017-04-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Different Network Architectures for Online Handwritten
      Chinese Character Recognition
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Test Accuracies (%) of Ensemble-Based Decisions from Sub-Sequences
      Generated by Random Dropout
  Table 3 caption:
    table_text: TABLE 3 Results on ICDAR-2013 Competition Database of Online Handwritten
      Chinese Character Recognition
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2695539
- Affiliation of the first author: ligm (umr 8049), umr, france
  Affiliation of the last author: "bccn, university of w\xFCrzburg, geschwister-scholl-platz,\
    \ tubingen, germany"
  Figure 1 Link: articels_figures_by_rev_year\2017\Efficient_D_and_D_Facade_Segmentation_Using_AutoContext\figure_1.jpg
  Figure 1 caption: Schematic of different components in our facade segmentation pipeline
    with a sample facade from ECP dataset [8]. 'Image' and 'Context' features correspond
    to features extracted on input image and previous-stage segmentation result respectively.
    'Stage- n ' refers to the nth stage auto-context classifier. The segmentation
    result is successively refined by the auto-context classifiers from their respective
    previous stage result.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Efficient_D_and_D_Facade_Segmentation_Using_AutoContext\figure_2.jpg
  Figure 2 caption: (a) A facade and (b) its window detection scores. Bright and dark
    regions correspond to high and low detection scores respectively.
  Figure 3 Link: articels_figures_by_rev_year\2017\Efficient_D_and_D_Facade_Segmentation_Using_AutoContext\figure_3.jpg
  Figure 3 caption: Sample segmentation visual results on different 2D facade datasets
    (best viewed in color and more in supplementary, which can be found on the Computer
    Society Digital Library at http:doi.ieeecomputersociety.org10.1109TPAMI.2017.2696526).
  Figure 4 Link: articels_figures_by_rev_year\2017\Efficient_D_and_D_Facade_Segmentation_Using_AutoContext\figure_4.jpg
  Figure 4 caption: Sample visual results for the point cloud and mesh labeling tasks
    on the RueMonge2014 dataset (more in supplementary, available online).
  Figure 5 Link: articels_figures_by_rev_year\2017\Efficient_D_and_D_Facade_Segmentation_Using_AutoContext\figure_5.jpg
  Figure 5 caption: (a) Sample facade image from ECP dataset; (b) Ground truth segmentation;
    and (c,d,e) Result of various classification stages of our auto-context method.
    Observe that the method removes isolated predictions and recovers the second lowest
    line of windows. (f) Potts model on top of ST3 result, and (g) parsed result obtained
    by applying reinforcement learning [4] using ST3 result.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Raghudeep Gadde
  Name of the last author: Peter V. Gehler
  Number of Figures: 5
  Number of Tables: 5
  Number of authors: 4
  Paper title: Efficient 2D and 3D Facade Segmentation Using Auto-Context
  Publication Date: 2017-04-24 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Segmentation Results of Various Methods on ECP Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Segmentation Results on Various 2D Datasets
  Table 3 caption:
    table_text: TABLE 3 Average Runtime for Various Methods on a Single Image of the
      ECP Dataset
  Table 4 caption:
    table_text: TABLE 4 Segmentation Results of Various Methods for the Tasks of (a)
      Image Labeling and (b) Point Cloud Labeling on the RueMonge2014 Dataset
  Table 5 caption:
    table_text: TABLE 5 Results for Mesh Labeling Task on the RueMonge2014 Dataset
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2696526
- Affiliation of the first author: department of electronic engineering, tsinghua
    university, beijing, china
  Affiliation of the last author: department of statistics, rutgers university, piscataway,
    nj
  Figure 1 Link: articels_figures_by_rev_year\2017\Learning_TransDimensional_Random_Fields_with_Applications_to_Language_Modeling\figure_1.jpg
  Figure 1 caption: Augmented stochastic approximation (AugSA).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Learning_TransDimensional_Random_Fields_with_Applications_to_Language_Modeling\figure_2.jpg
  Figure 2 caption: To compare the convergence of SA training using online estimated
    variances in [21] (green dash line) and using empirical variances in this paper
    (red solid line), and Adam method [22] (blue dot line), we plot an example of
    the scaled negative log-likelihoods on PTB training set along the iterations.
  Figure 3 Link: articels_figures_by_rev_year\2017\Learning_TransDimensional_Random_Fields_with_Applications_to_Language_Modeling\figure_3.jpg
  Figure 3 caption: "Word morphology experiment results. (a)(c) Plot the scaled negative\
    \ log-likelihoods (NLLs) on the training set during iterations or at the final\
    \ parameter estimates. (b) Shows the distribution of n j n and \u03C0 0 as discussed\
    \ in Section 5.3. (d) Plots the relative difference of the estimated log ratios\
    \ of normalizing constants \u03B6 (t) and the true ones \u03B6 \u2217(t) associated\
    \ with \u03BB (t) at t=100 and 1,000. The legends are interpreted as follows:"
  Figure 4 Link: articels_figures_by_rev_year\2017\Learning_TransDimensional_Random_Fields_with_Applications_to_Language_Modeling\figure_4.jpg
  Figure 4 caption: Log normalization constants estimated by different methods for
    one of the 10 TRFs, whose WER and PPL result are shown in Table 3. Dots represent
    the log weights from the 10 chains used to compute the AIS estimates (solid red
    line).
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Bin Wang
  Name of the last author: Zhiqiang Tan
  Number of Figures: 4
  Number of Tables: 5
  Number of authors: 3
  Paper title: Learning Trans-Dimensional Random Fields with Applications to Language
    Modeling
  Publication Date: 2017-04-24 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Trans-Dimensional Mixture Sampling (TransMS)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Feature Definition in TRF LMs
  Table 3 caption:
    table_text: TABLE 3 The WERs, NLLs and PPLs on the WSJ'92 Test Data
  Table 4 caption:
    table_text: TABLE 4 The Perplexities (PPL) of Various LMs with Different Sizes
      of Training Data (8, 16, 32 M) from Google 1-Billion Word Corpus
  Table 5 caption:
    table_text: TABLE 5 The WERs on WSJ'92 Test Set, Continuing from Table 4
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2696536
- Affiliation of the first author: department of information and electrical engineering
    and applied mathematics, university of salerno, fisciano, sa, italy
  Affiliation of the last author: department of information and electrical engineering
    and applied mathematics, university of salerno, fisciano, sa, italy
  Figure 1 Link: articels_figures_by_rev_year\2017\Challenging_the_Time_Complexity_of_Exact_Subgraph_Isomorphism_for_Huge_and_Dense\figure_1.jpg
  Figure 1 caption: "On the left the pattern and the target graph. On the right the\
    \ corresponding sets together with the functions for obtaining node and edge attributes.\
    \ As edges are unlabeled, the sets L e 1 and L e 2 are not present and the functions\
    \ \u03BB e 1 and \u03BB e 2 not defined. On the bottom right the mapping function\
    \ satisfying the subgraph isomorphism."
  Figure 10 Link: articels_figures_by_rev_year\2017\Challenging_the_Time_Complexity_of_Exact_Subgraph_Isomorphism_for_Huge_and_Dense\figure_10.jpg
  Figure 10 caption: The memory usage for eta =0.2 , 0.3 and 0.4 for unlabeled graphs
    and labeled with non uniform distribution.
  Figure 2 Link: articels_figures_by_rev_year\2017\Challenging_the_Time_Complexity_of_Exact_Subgraph_Isomorphism_for_Huge_and_Dense\figure_2.jpg
  Figure 2 caption: The state space (on the left) explored by VF3. The state s 6 is
    a goal state as it represents a coherent complete matching. In white, the inconsistent
    states s 3 , s 7 , s 10 , s 11 . They are not generated as they do not satisfy
    the feasibility rules (see Section 3.6). Details about the reason why s 3 , s
    7 , s 10 are inconsistent are given Figs. 6, 7 and 8.
  Figure 3 Link: articels_figures_by_rev_year\2017\Challenging_the_Time_Complexity_of_Exact_Subgraph_Isomorphism_for_Huge_and_Dense\figure_3.jpg
  Figure 3 caption: "The state s 2 of the SSR expansion (see Fig. 2); (a) unclassified\
    \ feasibility set; b) and c) classified feasibility sets obtained by using two\
    \ possible classification functions \u03C8 a and \u03C8 b defined on the graphs\
    \ G 1 and G 2 . The shading around the nodes identifies the class to which they\
    \ belong they belong."
  Figure 4 Link: articels_figures_by_rev_year\2017\Challenging_the_Time_Complexity_of_Exact_Subgraph_Isomorphism_for_Huge_and_Dense\figure_4.jpg
  Figure 4 caption: The state space generated with the sequence NG1 . At at each level,
    the nodes of G1 are statically identified by NG1 , while those of G2 have to be
    dynamically selected.
  Figure 5 Link: articels_figures_by_rev_year\2017\Challenging_the_Time_Complexity_of_Exact_Subgraph_Isomorphism_for_Huge_and_Dense\figure_5.jpg
  Figure 5 caption: The GetNextCandidate procedure applied to the state s2 shown in
    Fig. 2. On the left, u2 , the current node to be mapped for G1 and u3 , the parent
    of u2 . The class psi (u2) is c1 . On the right, the set R2(s2,c1) of nodes having
    the same class of u2 and the set R2mathcal S(s2,c1,v3) containing the neighbors
    of v3 that are candidates for u2 . v2 is selected, being the first node of R2mathcal
    S(s2,c1,v3) .
  Figure 6 Link: articels_figures_by_rev_year\2017\Challenging_the_Time_Complexity_of_Exact_Subgraph_Isomorphism_for_Huge_and_Dense\figure_6.jpg
  Figure 6 caption: Feasibility check Fc of s4 cup (u5,v12) (see Fig. 2). The couple
    does not satisfy the condition Fc . Indeed, u4 is predecessor of u5 , while v13
    , mapped with u4 , is the predecessor of v12 . Vice versa, u5 has u2 as predecessor
    but v12 has no other predecessor in widetildeM2(s4) .
  Figure 7 Link: articels_figures_by_rev_year\2017\Challenging_the_Time_Complexity_of_Exact_Subgraph_Isomorphism_for_Huge_and_Dense\figure_7.jpg
  Figure 7 caption: Feasibility check Fla1 of the addition of (u2,v8) to s9 for obtaining
    the state s10 (see Fig. 2). The couple does not satisfy the condition Fla1 . Indeed,
    the number of successors of u2 that are in widetildemathcal S1c3(s9) is greater
    than the number of successors of v8 that are in widetildemathcal S2c3(s9) . No
    edges connect v8 to one or more nodes in widetildemathcal S2c3(s9) = lbrace v6,v12rbrace
    . The empty feasibility sets are not shown in the figure. The shading around the
    nodes identifies the class to which they belong to.
  Figure 8 Link: articels_figures_by_rev_year\2017\Challenging_the_Time_Complexity_of_Exact_Subgraph_Isomorphism_for_Huge_and_Dense\figure_8.jpg
  Figure 8 caption: Feasibility check Fla2 of the addition of (u2,v2) to s2 for obtaining
    the state s3 (see Fig. 2). The couple does not satisfy the condition Fla2 . Indeed,
    the number of successors of u2 that are in widetildeV1c4(s2) is greater than the
    number of successors of v2 that are in widetildeV2c4(s2) . No edges connect v2
    to one or more nodes in widetildeV2c4(s2) = lbrace v5,v7,v11rbrace . The empty
    feasibility sets are not shown in the figure. The shading around the nodes identifies
    the class to which they belong to.
  Figure 9 Link: articels_figures_by_rev_year\2017\Challenging_the_Time_Complexity_of_Exact_Subgraph_Isomorphism_for_Huge_and_Dense\figure_9.jpg
  Figure 9 caption: The average (solid lines) and the maximum (dotted lines) matching
    times for eta =0.2 , eta =0.3 and eta =0.4 for unlabeled and non uniform labeled
    graphs.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Vincenzo Carletti
  Name of the last author: Mario Vento
  Number of Figures: 10
  Number of Tables: 7
  Number of authors: 4
  Paper title: Challenging the Time Complexity of Exact Subgraph Isomorphism for Huge
    and Dense Graphs with VF3
  Publication Date: 2017-04-24 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Three Main Approaches to Subgraph Isomorphism and the
      Relative Most Known Algorithms
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Probabilities P f of the Nodes of G 1 (see Fig. 1)
  Table 3 caption:
    table_text: TABLE 3 The Computation of the Node Ordering of G 1 (See Fig. 1),
      Using the Probabilities in Table 2
  Table 4 caption:
    table_text: TABLE 4 Core and Feasibility Sets of G 1 (See Fig. 1), Precomputed
      for Each Level of the Search. The Parent of u 2 and u 4 is u 3 , the Parent
      of u 5 is u 4 , the Parent of u 1 is u 2 .
  Table 5 caption:
    table_text: TABLE 5 Feature Comparison Among VF2, VF3 and RI
  Table 6 caption:
    table_text: "TABLE 6 The Proposed Dataset, Containing Unlabeled and Labeled Graphs\
      \ with Different Values of \u03B7"
  Table 7 caption:
    table_text: TABLE 7 Comparison Between VF2 and VF3 in Terms of Measured Computational
      Complexity
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2696940
- Affiliation of the first author: department of information engineering, via di s.
    marta, firenze, italy
  Affiliation of the last author: department of information engineering, via di s.
    marta, firenze, italy
  Figure 1 Link: articels_figures_by_rev_year\2017\Rethinking_the_sGLOH_Descriptor\figure_1.jpg
  Figure 1 caption: "Rotation of an image patch by a factor 2\u03C0 m with the superimposed\
    \ sGLOH grid (left), which corresponds to a cyclic shift of the block histogram\
    \ of each ring (right). In the example n=2 and m=4 , color labels on the patch\
    \ grid identify the corresponding block histograms on the descriptor (best viewed\
    \ in color)."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Rethinking_the_sGLOH_Descriptor\figure_2.jpg
  Figure 2 caption: Rotation of an image patch, and the corresponding cyclic shift
    of the BisGLOH histogram comparison tables. Color labels for each region R r,d
    in the image patch identify the corresponding table T t r,d , whose darker entries
    only are concatenated in the descriptor, due to the skew-symmetry table decimation.
    Similarly, the two color cell entries in the D r tables indicate the two regions
    whose gradient sums D r,d are compared. In the example n=2 and m=4 (best viewed
    in color and zoomed in).
  Figure 3 Link: articels_figures_by_rev_year\2017\Rethinking_the_sGLOH_Descriptor\figure_3.jpg
  Figure 3 caption: Corresponding image pairs from the oxford (a), viewpoint (b),
    approximated overlap (c), DTU (d), turntable (e), patch (f), ZuBuD (g) and Kentucky
    (h) datasets (best viewed in color).
  Figure 4 Link: articels_figures_by_rev_year\2017\Rethinking_the_sGLOH_Descriptor\figure_4.jpg
  Figure 4 caption: mAP (percent) on the rotation dataset for histogram-based (a)
    and binary (b) descriptors, SIFT is included in (b) as reference (best viewed
    in color and zoomed in).
  Figure 5 Link: articels_figures_by_rev_year\2017\Rethinking_the_sGLOH_Descriptor\figure_5.jpg
  Figure 5 caption: mAP for increasing viewpoint angle on the Turntable dataset (a)
    and overall ROC curves on the Patch dataset for DoG (b) and Harris (c) keypoint
    patches. Error rate at 95 percent recall for each descriptor is also shown in
    the legend (best viewed in color and zoomed in).
  Figure 6 Link: articels_figures_by_rev_year\2017\Rethinking_the_sGLOH_Descriptor\figure_6.jpg
  Figure 6 caption: Average cumulative histogram of the running times for each evaluated
    descriptor (best viewed in color and zoomed in).
  Figure 7 Link: articels_figures_by_rev_year\2017\Rethinking_the_sGLOH_Descriptor\figure_7.jpg
  Figure 7 caption: Descriptor total running time versus average number of keypoints
    (best viewed in color and zoomed in).
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Fabio Bellavia
  Name of the last author: Carlo Colombo
  Number of Figures: 7
  Number of Tables: 5
  Number of authors: 2
  Paper title: Rethinking the sGLOH Descriptor
  Publication Date: 2017-04-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Time Ratios with Respect to SIFT for the Different sGLOH-Based
      Exhaustive and Fast Matching Strategies, z=10 and m=8
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Descriptor Evaluation Setup Details
  Table 3 caption:
    table_text: TABLE 3 Results on the Oxford Dataset
  Table 4 caption:
    table_text: TABLE 4 Results on the Viewpoints, Approximated Overlap and DTU Datasets
  Table 5 caption:
    table_text: TABLE 5 Average Correctly Retrieved Queries (Percent) for the Object
      Retrieval Tests
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2697849
- Affiliation of the first author: national laboratory of pattern recognition, chinese
    academy of sciences, beijing, p.r. china
  Affiliation of the last author: ai institute, qihoo360 company, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2017\Towards_Robust_and_Accurate_MultiView_and_PartiallyOccluded_Face_Alignment\figure_1.jpg
  Figure 1 caption: A four-level hierarchy in the face partition sets for a face annotated
    with 68 landmarks.
  Figure 10 Link: articels_figures_by_rev_year\2017\Towards_Robust_and_Accurate_MultiView_and_PartiallyOccluded_Face_Alignment\figure_10.jpg
  Figure 10 caption: Evaluation and analyses of multi-view face alignment on our MVFW
    dataset.
  Figure 2 Link: articels_figures_by_rev_year\2017\Towards_Robust_and_Accurate_MultiView_and_PartiallyOccluded_Face_Alignment\figure_2.jpg
  Figure 2 caption: Definitions of the elemental occlusion patterns (EOPs) and the
    partial occlusion patterns (POPs). (a) A set of 16 EOPs defined for the 68 landmarks
    drawn on a mean face shape. (b) Three different POPs plotted by combining some
    EOPs.
  Figure 3 Link: articels_figures_by_rev_year\2017\Towards_Robust_and_Accurate_MultiView_and_PartiallyOccluded_Face_Alignment\figure_3.jpg
  Figure 3 caption: Illustration of extending the full occlusion dictionary D O to
    a block diagonal form for the partial occlusion dictionary D o .
  Figure 4 Link: articels_figures_by_rev_year\2017\Towards_Robust_and_Accurate_MultiView_and_PartiallyOccluded_Face_Alignment\figure_4.jpg
  Figure 4 caption: 'Different definitions of facial landmark annotations and their
    corresponding mean shapes estimated from a face detector: (a) 17 points, (b) 29
    points, (c) 51 points, and (d) 68 points.'
  Figure 5 Link: articels_figures_by_rev_year\2017\Towards_Robust_and_Accurate_MultiView_and_PartiallyOccluded_Face_Alignment\figure_5.jpg
  Figure 5 caption: The convergence properties of the SRD model during training phase
    (a) and testing phase (b), with comparisons to the SDM model.
  Figure 6 Link: articels_figures_by_rev_year\2017\Towards_Robust_and_Accurate_MultiView_and_PartiallyOccluded_Face_Alignment\figure_6.jpg
  Figure 6 caption: 'The effectiveness of different optimization procedures for learning
    the relational dictionary: (a) mean alignment errors in different training stages;
    (b) mean alignment errors in different testing stages.'
  Figure 7 Link: articels_figures_by_rev_year\2017\Towards_Robust_and_Accurate_MultiView_and_PartiallyOccluded_Face_Alignment\figure_7.jpg
  Figure 7 caption: Investigating the effects of the regularizer lambda (a) and dictionary
    size m (b) on the alignment errors.
  Figure 8 Link: articels_figures_by_rev_year\2017\Towards_Robust_and_Accurate_MultiView_and_PartiallyOccluded_Face_Alignment\figure_8.jpg
  Figure 8 caption: Cumulative error distribution curves on the BioID dataset.
  Figure 9 Link: articels_figures_by_rev_year\2017\Towards_Robust_and_Accurate_MultiView_and_PartiallyOccluded_Face_Alignment\figure_9.jpg
  Figure 9 caption: Face view distributions of (a) the BioID dataset, (b) the LFPW
    dataset, and (c) our MVFW dataset. MVFW has much larger view variations and is
    more well-proportioned.
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Junliang Xing
  Name of the last author: Shuicheng Yan
  Number of Figures: 13
  Number of Tables: 1
  Number of authors: 6
  Paper title: Towards Robust and Accurate Multi-View and Partially-Occluded Face
    Alignment
  Publication Date: 2017-04-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Mean Alignment Errors on the LFPW Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2697958
- Affiliation of the first author: google inc., mountain view, ca
  Affiliation of the last author: departments of cognitive science and computer science,
    johns hopkins university, baltimore, md
  Figure 1 Link: articels_figures_by_rev_year\2017\DeepLab_Semantic_Image_Segmentation_with_Deep_Convolutional_Nets_Atrous_Convolut\figure_1.jpg
  Figure 1 caption: Model illustration. A deep convolutional neural network such as
    VGG-16 or ResNet-101 is employed in a fully convolutional fashion, using atrous
    convolution to reduce the degree of signal downsampling (from 32x down 8x). A
    bilinear interpolation stage enlarges the feature maps to the original image resolution.
    A fully connected CRF is then applied to refine the segmentation result and better
    capture the object boundaries.
  Figure 10 Link: articels_figures_by_rev_year\2017\DeepLab_Semantic_Image_Segmentation_with_Deep_Convolutional_Nets_Atrous_Convolut\figure_10.jpg
  Figure 10 caption: '(a) Trimap examples (top-left: image. top-right: ground-truth.
    bottom-left: trimap of 2 pixels. bottom-right: trimap of 10 pixels). (b) Pixel
    mean IOU as a function of the band width around the object boundaries when employing
    VGG-16 or ResNet-101 before and after CRF.'
  Figure 2 Link: articels_figures_by_rev_year\2017\DeepLab_Semantic_Image_Segmentation_with_Deep_Convolutional_Nets_Atrous_Convolut\figure_2.jpg
  Figure 2 caption: Illustration of atrous convolution in 1-D. (a) Sparse feature
    extraction with standard convolution on a low resolution input feature map. (b)
    Dense feature extraction with atrous convolution with rate r=2 , applied on a
    high resolution input feature map.
  Figure 3 Link: articels_figures_by_rev_year\2017\DeepLab_Semantic_Image_Segmentation_with_Deep_Convolutional_Nets_Atrous_Convolut\figure_3.jpg
  Figure 3 caption: 'Illustration of atrous convolution in 2-D. Top row: sparse feature
    extraction with standard convolution on a low resolution input feature map. Bottom
    row: Dense feature extraction with atrous convolution with rate r=2 , applied
    on a high resolution input feature map.'
  Figure 4 Link: articels_figures_by_rev_year\2017\DeepLab_Semantic_Image_Segmentation_with_Deep_Convolutional_Nets_Atrous_Convolut\figure_4.jpg
  Figure 4 caption: Atrous Spatial Pyramid Pooling (ASPP). To classify the center
    pixel (orange), ASPP exploits multi-scale features by employing multiple parallel
    filters with different rates. The effective Field-Of-Views are shown in different
    colors.
  Figure 5 Link: articels_figures_by_rev_year\2017\DeepLab_Semantic_Image_Segmentation_with_Deep_Convolutional_Nets_Atrous_Convolut\figure_5.jpg
  Figure 5 caption: Score map (input before softmax function) and belief map (output
    of softmax function) for Aeroplane. We show the score (1st row) and belief (2nd
    row) maps after each mean field iteration. The output of last DCNN layer is used
    as input to the mean field inference.
  Figure 6 Link: articels_figures_by_rev_year\2017\DeepLab_Semantic_Image_Segmentation_with_Deep_Convolutional_Nets_Atrous_Convolut\figure_6.jpg
  Figure 6 caption: PASCAL VOC 2012 val results. Input image and our DeepLab results
    beforeafter CRF.
  Figure 7 Link: articels_figures_by_rev_year\2017\DeepLab_Semantic_Image_Segmentation_with_Deep_Convolutional_Nets_Atrous_Convolut\figure_7.jpg
  Figure 7 caption: DeepLab-ASPP employs multiple filters with different rates to
    capture objects and context at multiple scales.
  Figure 8 Link: articels_figures_by_rev_year\2017\DeepLab_Semantic_Image_Segmentation_with_Deep_Convolutional_Nets_Atrous_Convolut\figure_8.jpg
  Figure 8 caption: Qualitative segmentation results with ASPP compared to the baseline
    LargeFOV model. The ASPP-L model, employing multiple large FOVs can successfully
    capture objects as well as image context at multiple scales.
  Figure 9 Link: articels_figures_by_rev_year\2017\DeepLab_Semantic_Image_Segmentation_with_Deep_Convolutional_Nets_Atrous_Convolut\figure_9.jpg
  Figure 9 caption: DeepLab results based on VGG-16 net or ResNet-101 before and after
    CRF. The CRF is critical for accurate prediction along object boundaries with
    VGG-16, whereas ResNet-101 has acceptable performance even before CRF.
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Liang-Chieh Chen
  Name of the last author: Alan L. Yuille
  Number of Figures: 14
  Number of Tables: 9
  Number of authors: 5
  Paper title: 'DeepLab: Semantic Image Segmentation with Deep Convolutional Nets,
    Atrous Convolution, and Fully Connected CRFs'
  Publication Date: 2017-04-27 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Effect of Field-Of-View by Adjusting the Kernel Size and Atrous
      sampling Rate r at 'fc6' Layer
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 PASCAL VOC 2012 val Set Results (%) (before CRF) as Different
      Learning Hyper Parameters Vary
  Table 3 caption:
    table_text: TABLE 3 Effect of ASPP on PASCAL VOC 2012 val Set Performance (Mean
      IOU) for VGG-16 Based DeepLab Model
  Table 4 caption:
    table_text: TABLE 4 Employing ResNet-101 for DeepLab on PASCAL VOC 2012 val set
  Table 5 caption:
    table_text: TABLE 5 Performance on PASCAL VOC 2012 test Set
  Table 6 caption:
    table_text: TABLE 6 Comparison with Other State-of-Art Methods on PASCAL-Context
      Dataset
  Table 7 caption:
    table_text: TABLE 7 Comparison with Other State-of-Art Methods on PASCAL-Person-Part
      Dataset
  Table 8 caption:
    table_text: TABLE 8 Test Set Results on the Cityscapes Dataset, Comparing Our
      DeepLab System with Other State-of-Art Methods
  Table 9 caption:
    table_text: TABLE 9 Val Set Results on Cityscapes Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2699184
- Affiliation of the first author: university college london, bloomsbury, london,
    united kingdom
  Affiliation of the last author: "inria grenoble rh\xF4ne-alpes, montbonnot-saint-martin,\
    \ france"
  Figure 1 Link: articels_figures_by_rev_year\2017\Automatic_Camera_Calibration_Using_Multiple_Sets_of_Pairwise_Correspondences\figure_1.jpg
  Figure 1 caption: Correspondences extracted from SIFT features. Given the wide baseline
    between the views there is a single reliable triple correspondence (red) while
    there are many pairwise correspondences (blue and green).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Automatic_Camera_Calibration_Using_Multiple_Sets_of_Pairwise_Correspondences\figure_2.jpg
  Figure 2 caption: We consider the problem of fully calibrating the camera C , given
    pairwise correspondences with two calibrated cameras C A and C B .
  Figure 3 Link: articels_figures_by_rev_year\2017\Automatic_Camera_Calibration_Using_Multiple_Sets_of_Pairwise_Correspondences\figure_3.jpg
  Figure 3 caption: The space generated by two bundles of lines (the rays of 2 pinhole
    cameras) can be fully represented as the linear span of G 1 , G 2 , G 3 , G 4
    , G 5 .
  Figure 4 Link: articels_figures_by_rev_year\2017\Automatic_Camera_Calibration_Using_Multiple_Sets_of_Pairwise_Correspondences\figure_4.jpg
  Figure 4 caption: "Conic envelope \u03A9 establishes linear relations s T K K T\
    \ s=0 and r T K K T r=0 ."
  Figure 5 Link: articels_figures_by_rev_year\2017\Automatic_Camera_Calibration_Using_Multiple_Sets_of_Pairwise_Correspondences\figure_5.jpg
  Figure 5 caption: "Generation of candidate models mathsf TC . (a) In each RANSAC\
    \ iteration a subset mathcal S with s samples is selected from dataset mathcal\
    \ D . (b) In in each multi-RANSAC iteration there are two sampling steps: The\
    \ first step randomly selects M of the N datasets mathcal D1 ,\u2026, mathcal\
    \ DN ; in the second step sj samples are selected from each of the M selected\
    \ datasets hatmathcal Dj ."
  Figure 6 Link: articels_figures_by_rev_year\2017\Automatic_Camera_Calibration_Using_Multiple_Sets_of_Pairwise_Correspondences\figure_6.jpg
  Figure 6 caption: Comparison between multi-MAPSAC, multi-MLESAC and MLESAC using
    synthetic data. Each error distribution contains results from 50 calibration trials.
    The results with varying number of correspondences (a, c, e) contain 10 percent
    outliers in camera A and 40 percent outliers in camera B. The results with varying
    number of correspondences (b, d, f) contain 500 correspondences in camera A and
    100 correspondences in camera B.
  Figure 7 Link: articels_figures_by_rev_year\2017\Automatic_Camera_Calibration_Using_Multiple_Sets_of_Pairwise_Correspondences\figure_7.jpg
  Figure 7 caption: Addition of a new node to a camera network. In each trial we try
    to calibrate one of the cameras in (a) assuming that the remaining four are calibrated.
    (b), (c), (d), (e) show the comparative performance between 11-point (pairwise)
    and 6-point (triple) for 250 calibration trials.
  Figure 8 Link: articels_figures_by_rev_year\2017\Automatic_Camera_Calibration_Using_Multiple_Sets_of_Pairwise_Correspondences\figure_8.jpg
  Figure 8 caption: Online calibration of a hand-held camera. Sample images from the
    synchronized video sequence are depicted in (a). The trajectory of the hand-held
    camera was estimated with the 11-point (blue) and the 6-point (red) algorithms.
    In (b) the estimated trajectories are projected on a different calibrated view.
  Figure 9 Link: articels_figures_by_rev_year\2017\Automatic_Camera_Calibration_Using_Multiple_Sets_of_Pairwise_Correspondences\figure_9.jpg
  Figure 9 caption: VisualSfM was run on a set of 19 uncalibrated images and generated
    a 3D model with 13 views (blue and green). From the remaining 6, our 11-point
    calibration algorithm was able insert 3 more views into the SfM model (red).
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Francisco Vasconcelos
  Name of the last author: Edmond Boyer
  Number of Figures: 9
  Number of Tables: 0
  Number of authors: 3
  Paper title: Automatic Camera Calibration Using Multiple Sets of Pairwise Correspondences
  Publication Date: 2017-04-28 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2699648
