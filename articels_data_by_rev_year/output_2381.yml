- Affiliation of the first author: department of electrical engineering, indian institute
    of science, bengaluru, karnataka, india
  Affiliation of the last author: department of electrical engineering, indian institute
    of science, bengaluru, karnataka, india
  Figure 1 Link: articels_figures_by_rev_year\2021\Iteratively_Reweighted_MinimaxConcave_Penalty_Minimization_for_Accurate_Lowrank_\figure_1.jpg
  Figure 1 caption: "Visualization of various penalties: \u2113 1 , \u2113 p , MCP,\
    \ and their weighted counterparts (W \u2113 1 , W \u2113 p , and WMCP, respectively).\
    \ The weights chosen along x and y are 1.0 and 0.5, respectively."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Iteratively_Reweighted_MinimaxConcave_Penalty_Minimization_for_Accurate_Lowrank_\figure_2.jpg
  Figure 2 caption: Level-sets corresponding to the penalties shown in Fig. 1.
  Figure 3 Link: articels_figures_by_rev_year\2021\Iteratively_Reweighted_MinimaxConcave_Penalty_Minimization_for_Accurate_Lowrank_\figure_3.jpg
  Figure 3 caption: "Illustration of the proximal operators ((a) and (c)) and corresponding\
    \ level-sets ((b) and (d)) for: MCP ((a) and (b)) with \u03BB=1.5 ; vector EMCP\
    \ ((c) and (d)) with \u03BB 1 =1.5 and \u03BB 2 =0.5 ."
  Figure 4 Link: articels_figures_by_rev_year\2021\Iteratively_Reweighted_MinimaxConcave_Penalty_Minimization_for_Accurate_Lowrank_\figure_4.jpg
  Figure 4 caption: "Phase-transition diagrams illustrating low-rank recovery performance\
    \ in terms of the averaged log-scale relative reconstruction error (LRRE) across\
    \ 10 trials in comparison with the competing methods. The stopping criterion for\
    \ all the techniques is as follows: RRE\u2264 10 \u22127 or maximum number of\
    \ iterations = 1000, whichever happens earlier. The phase-transition behavior\
    \ is much smoother for the CoRe-LSD, AdaCoRe-LSD \u2013 Type-1 and Type-2 algorithms,\
    \ which indicates that these algorithms are more robust than the weighted nuclear\
    \ norm and weighted Schatten- p norm approaches. The dashed line indicates the\
    \ 45 \u2218 line. Performance comparison of the techniques along this line is\
    \ shown in Fig. 6."
  Figure 5 Link: articels_figures_by_rev_year\2021\Iteratively_Reweighted_MinimaxConcave_Penalty_Minimization_for_Accurate_Lowrank_\figure_5.jpg
  Figure 5 caption: Log relative reconstruction error (LRRE) versus iterations averaged
    over 10 trials for various choices of delta and s .
  Figure 6 Link: articels_figures_by_rev_year\2021\Iteratively_Reweighted_MinimaxConcave_Penalty_Minimization_for_Accurate_Lowrank_\figure_6.jpg
  Figure 6 caption: The log relative reconstruction error (LRRE) as a function of
    the rank parametersparsity factor along the 45circ line (i.e., delta = s ) shown
    in Fig. 4. The proposed techniques exhibit a smooth error behavior unlike the
    state-of-the-art techniques, which show an abrupt increase.
  Figure 7 Link: articels_figures_by_rev_year\2021\Iteratively_Reweighted_MinimaxConcave_Penalty_Minimization_for_Accurate_Lowrank_\figure_7.jpg
  Figure 7 caption: An illustration of foreground-background separation on video sequences
    from I2R dataset. The first column shows the ground-truth frame and ground-truth
    foreground mask for three sample frames. The columns corresponding to the techniques
    show the estimated low-rank (background) component and estimated sparse (foreground)
    component. The proposed techniques outperform the benchmark techniques. Among
    the proposed techniques, the Type-2 variant has the best performance.
  Figure 8 Link: articels_figures_by_rev_year\2021\Iteratively_Reweighted_MinimaxConcave_Penalty_Minimization_for_Accurate_Lowrank_\figure_8.jpg
  Figure 8 caption: An illustration of foreground-background separation on video sequences
    taken from I2R dataset. The first column shows the ground-truth frame and ground-truth
    foreground mask for three sample frames. The columns corresponding to the techniques
    show the estimated low-rank (background) component and estimated sparse (foreground)
    component. The proposed techniques outperform the benchmark techniques. Among
    the proposed techniques, the Type-2 variant has the best performance.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.97
  Name of the first author: Praveen Kumar Pokala
  Name of the last author: Chandra Sekhar Seelamantula
  Number of Figures: 8
  Number of Tables: 5
  Number of authors: 3
  Paper title: Iteratively Reweighted Minimax-Concave Penalty Minimization for Accurate
    Low-rank Plus Sparse Matrix Decomposition
  Publication Date: 2021-10-26 00:00:00
  Table 1 caption: TABLE 1 A Comparison of Low-Rank Recovery Performance of Various
    Techniques
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 A Comparison of Low-Rank Recovery Performance of Various
    Techniques
  Table 3 caption: 'TABLE 3 Foreground-Background Separation in Videos: IoUF-Measure
    Based Performance Comparison of the Proposed Techniques With the State-of-the-Art
    Techniques on Standard Test Video Sequences From I2R Dataset'
  Table 4 caption: 'TABLE 4 Foreground-Background Separation: IoUF-Measure Based Performance
    Comparison of the Proposed Techniques With the State-of-the-Art Techniques on
    Test Sequences Taken From CDnet 2012 and BMC 2012 Datasets'
  Table 5 caption: TABLE 5 Comparison of Run-Times (in Seconds) of Various Techniques
    for the Application of Foreground-Background Separation on Standard Test Video
    Sequences From I2R Dataset
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3122259
- Affiliation of the first author: school of electrical and electronic engineering,
    yonsei university, seoul, south korea
  Affiliation of the last author: school of electrical and electronic engineering,
    yonsei university, seoul, south korea
  Figure 1 Link: articels_figures_by_rev_year\2021\Disentangled_Representations_for_ShortTerm_and_LongTerm_Person_ReIdentification\figure_1.jpg
  Figure 1 caption: Visual comparison of identity-related and -unrelated features.
    We generate new person images by interpolating (a) identity-related features and
    (b) identity-unrelated ones between two images, while fixing the other ones. We
    can see that identity-related features encode, e.g., clothing and color, and identity-unrelated
    ones involve, e.g., human pose and background clutter. Note that we disentangle
    these features using identification labels only. (Best viewed in color.).
  Figure 10 Link: articels_figures_by_rev_year\2021\Disentangled_Representations_for_ShortTerm_and_LongTerm_Person_ReIdentification\figure_10.jpg
  Figure 10 caption: An example of generated images using a part-level identity shuffling
    technique. We generate person images by shuffling identity-related-unrelated features
    for lowerupper body parts between two images. We shuffle either identity-related
    or -unrelated features while fixing the other. (Best viewed in color.).
  Figure 2 Link: articels_figures_by_rev_year\2021\Disentangled_Representations_for_ShortTerm_and_LongTerm_Person_ReIdentification\figure_2.jpg
  Figure 2 caption: "Overview of IS-GAN. (a) IS-GAN disentangles identity-related-unrelated\
    \ features from input person images, i.e., anchor and positive images, denoted\
    \ by I a and I p , respectively. (b-c) To regularize the disentanglement process,\
    \ it learns to generate the same images as the inputs using disentangled features\
    \ (b) without and (c) with identity shuffling. The identity shuffling technique\
    \ encourages 1) the identity-related encoder E R to extract the shared information\
    \ between I a and I p , and 2) the identity-unrelated encoder E U to see other\
    \ factors in each image. We train all components of our model end-to-end, including\
    \ the encoders ( E R and E U ), the generator ( G ), and the discriminators (\
    \ D D and D C ). We denote by \u2295 an element-wise addition of features. See\
    \ text for details. (Best viewed in color.)."
  Figure 3 Link: articels_figures_by_rev_year\2021\Disentangled_Representations_for_ShortTerm_and_LongTerm_Person_ReIdentification\figure_3.jpg
  Figure 3 caption: Part-level shuffling. (a) We randomly swap local features between
    anchor and positive images. (b) Similar to Fig. 2c, we generate person images
    with identity-related features but shuffled in a part-level and identity-unrelated
    ones. The part-level shuffling technique makes our model see various combinations
    of identity-related features, and encourages feature consistencies between corresponding
    object parts. (Best viewed in color.).
  Figure 4 Link: articels_figures_by_rev_year\2021\Disentangled_Representations_for_ShortTerm_and_LongTerm_Person_ReIdentification\figure_4.jpg
  Figure 4 caption: Examples of image pairs, where shared attributes are not sufficient
    for determining an identity for reID, due to (a) severe occlusion and (b) viewpoint
    change.
  Figure 5 Link: articels_figures_by_rev_year\2021\Disentangled_Representations_for_ShortTerm_and_LongTerm_Person_ReIdentification\figure_5.jpg
  Figure 5 caption: Visualization of training loss curves on (a) the first, (b-f)
    the second, and (g-l) the third stages, and (m) rank-1(%) and (n) mAP(%).
  Figure 6 Link: articels_figures_by_rev_year\2021\Disentangled_Representations_for_ShortTerm_and_LongTerm_Person_ReIdentification\figure_6.jpg
  Figure 6 caption: Sensitivity analysis on hyperparameters of IS-GAN KL (orange)
    and IS-GAN DC (blue) for (a) identity-related, (b) identity-shuffling, (c) part-level
    shuffling, (d) domain, (e) class, and (f-g) identity-unrelated losses.
  Figure 7 Link: articels_figures_by_rev_year\2021\Disentangled_Representations_for_ShortTerm_and_LongTerm_Person_ReIdentification\figure_7.jpg
  Figure 7 caption: Visual comparison of retrieval results on (top) DukeMTMC-reID
    [30] and (bottom) Celeb-reID [48]. We compute the euclidean distances between
    identity-related features of query and gallery images, and visualize top-10 results
    sorted according to the distance. The results with green boxes have the same identity
    as the query, while those with red boxes do not. (Best viewed in color.).
  Figure 8 Link: articels_figures_by_rev_year\2021\Disentangled_Representations_for_ShortTerm_and_LongTerm_Person_ReIdentification\figure_8.jpg
  Figure 8 caption: t-SNE [83] visualization for (a) identity-related features and
    (b) identity-unrelated ones on Market-1501 [28]. We randomly sample 26 identities
    from the test split. The points with the same color indicate the same identity.
    (Best viewed in color.).
  Figure 9 Link: articels_figures_by_rev_year\2021\Disentangled_Representations_for_ShortTerm_and_LongTerm_Person_ReIdentification\figure_9.jpg
  Figure 9 caption: Visual comparison of reconstructed images using feature representations
    obtained from (a) IS-GAN KL , (b) IS-GAN DC , (c) IS-GAN DC (wo mathcal LmathrmU)
    , and (d) IS-GAN DC (w batch-wise mathcal LmathrmU) . For each model, a generator
    inputs an addition of identity-related and -unrelated features, identity-related
    ones, or identity-unrelated ones, respectively. (Best viewed in color.).
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Chanho Eom
  Name of the last author: Bumsub Ham
  Number of Figures: 14
  Number of Tables: 6
  Number of authors: 4
  Paper title: Disentangled Representations for Short-Term and Long-Term Person Re-Identification
  Publication Date: 2021-10-26 00:00:00
  Table 1 caption: TABLE 1 Quantitative Comparison With the State of the Art on Market-1501
    [28], CUHK03 [29], DukeMTMC-reID [30], and Celeb-reID [48] in Terms of Rank-1
    Accuracy(%) and mAP(%)
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Ablation Studies of IS-GAN KL KL and IS-GAN DC DC on Market-1501
    [28], CUHK03 [29], DukeMTMC-reID [30], and Celeb-reID [48] in Terms of Rank-1
    Accuracy(%) and mAP(%)
  Table 3 caption: TABLE 3 Quantitative Results of IS-GAN DC DC, IS-GAN KL KL, and
    Their Variants on Market-1501 [28], CUHK03 [29], DukeMTMC-reID [30], and Celeb-reID
    [48] in Terms of Rank-1 Accuracy(%) and mAP(%)
  Table 4 caption: TABLE 4 Quantitative Results of Our Models Using PCB [11] as a
    Backbone Network
  Table 5 caption: TABLE 5 Quantitative Results for Different Numbers of Body Parts
    on Market-1501 [28] in Terms of Rank-1 Accuracy(%) and mAP(%)
  Table 6 caption: TABLE 6 Quantitative Results for Classifying Person Attributes
    on the Gallery Set of Market-1501
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3122444
- Affiliation of the first author: computer science department, shantou university,
    shantou, china
  Affiliation of the last author: omni:us, berlin, germany
  Figure 1 Link: articels_figures_by_rev_year\2021\Content_and_Style_Aware_Generation_of_TextLine_Images_for_Handwriting_Recognitio\figure_1.jpg
  Figure 1 caption: "Examples of generated text-lines, each one using a different\
    \ handwriting style. The text corresponds to the first paragraph of the book \u201C\
    The Old Man and the Sea\u201D."
  Figure 10 Link: articels_figures_by_rev_year\2021\Content_and_Style_Aware_Generation_of_TextLine_Images_for_Handwriting_Recognitio\figure_10.jpg
  Figure 10 caption: Example of interpolations in the style latent space.
  Figure 2 Link: articels_figures_by_rev_year\2021\Content_and_Style_Aware_Generation_of_TextLine_Images_for_Handwriting_Recognitio\figure_2.jpg
  Figure 2 caption: "Architecture of the proposed handwriting synthesis model. It\
    \ consists of a Visual Appearance Encoder (cyan box), a Textual Content Encoder\
    \ (red box), a Generator (magenta box) and learning objectives (blue box). X i\
    \ and t are the images and text string input, respectively. The x \xAF is the\
    \ generated sample that shares the visual appearance with X i and contains the\
    \ textual information with t ."
  Figure 3 Link: articels_figures_by_rev_year\2021\Content_and_Style_Aware_Generation_of_TextLine_Images_for_Handwriting_Recognitio\figure_3.jpg
  Figure 3 caption: Periodic padding example. Given a real image, periodic padding
    to the right is applied several times until the maximum image width L is reached.
  Figure 4 Link: articels_figures_by_rev_year\2021\Content_and_Style_Aware_Generation_of_TextLine_Images_for_Handwriting_Recognitio\figure_4.jpg
  Figure 4 caption: Architecture of the textual content encoder. It consists of an
    Embedding Module (red box), a Character-wise Encoding (orange box) and a Global
    String Encoding (yellow box). In the sequence of character embeddings, each vector
    is represented by a specific color.
  Figure 5 Link: articels_figures_by_rev_year\2021\Content_and_Style_Aware_Generation_of_TextLine_Images_for_Handwriting_Recognitio\figure_5.jpg
  Figure 5 caption: Architecture of the Transformer-based handwritten text recognizer.
    The upper part is the Encoder (blue color) and the lower part is the Decoder (green
    color).
  Figure 6 Link: articels_figures_by_rev_year\2021\Content_and_Style_Aware_Generation_of_TextLine_Images_for_Handwriting_Recognitio\figure_6.jpg
  Figure 6 caption: (a) Inception module of FID with Average Pooling, (b) Updated
    Inception module of vFID with Temporal Pyramid Pooling.
  Figure 7 Link: articels_figures_by_rev_year\2021\Content_and_Style_Aware_Generation_of_TextLine_Images_for_Handwriting_Recognitio\figure_7.jpg
  Figure 7 caption: Histogram of FID (a) and vFID (b). The x -axis indicates the FIDvFID
    values, and the y -axis indicates the counts. The FIDvFID between subsets of samples
    in the same writer is shown in blue, and between different writers in red. The
    distribution of blue and red should be apart as far as possible. Both histograms
    are normalized to sum up to one.
  Figure 8 Link: articels_figures_by_rev_year\2021\Content_and_Style_Aware_Generation_of_TextLine_Images_for_Handwriting_Recognitio\figure_8.jpg
  Figure 8 caption: Examples of the IAM, Rimes and Spanish Numbers datasets are shown
    in (a), (b) and (c), respectively.
  Figure 9 Link: articels_figures_by_rev_year\2021\Content_and_Style_Aware_Generation_of_TextLine_Images_for_Handwriting_Recognitio\figure_9.jpg
  Figure 9 caption: "Comparison of the generated results for the same text string\
    \ \u201Cart in the ownership of both the state and the municipality of\u201D without\
    \ (upper) and with (lower) the periodic padding process."
  First author gender probability: 0.55
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Lei Kang
  Name of the last author: Mauricio Villegas
  Number of Figures: 14
  Number of Tables: 8
  Number of authors: 5
  Paper title: Content and Style Aware Generation of Text-Line Images for Handwriting
    Recognition
  Publication Date: 2021-10-26 00:00:00
  Table 1 caption: 'TABLE 1 Overview of the Datasets Used in Our HTR Experiments:
    Number of Text-Lines Used for Training, Validation and Test Sets, and Number of
    Writers'
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Three Categories of the IAM Offline Dataset, From Short
    to Long Text-Lines
  Table 3 caption: TABLE 3 Ablation Study for Convolutional Layers on the IAM Test
    Set
  Table 4 caption: TABLE 4 vFID Performance on Generating Different Length of Images
    for the Sequence-to-Sequence and Transformer-Based HTR Methods
  Table 5 caption: TABLE 5 Ablation Study on the Use of Character-Wise Encoding (Local
    Feature) and Global String Encoding (Global Feature)
  Table 6 caption: TABLE 6 Qualitative Comparison With Alonso et al. [15], Fogel et
    al. [17], and Davis et al. [18]
  Table 7 caption: TABLE 7 HTR Experiments
  Table 8 caption: TABLE 8 Transfer Learning Setting From IAM to Rimes
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3122572
- Affiliation of the first author: inception institute of artificial intelligence,
    abu dhabi, uae
  Affiliation of the last author: school of automation, northwestern polytechnical
    university, xian, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Learning_Selective_Mutual_Attention_and_Contrast_for_RGBD_Saliency_Detection\figure_1.jpg
  Figure 1 caption: "Comparison on the effectiveness of using self-attention (SA)\
    \ and mutual attention (MA). We first give the RGB image and the depth map of\
    \ an example image pair in (a) and (b). Then, we show the feature maps (FM) of\
    \ the two modalities in (c) and (d). In (e) and (f), we show the attention maps\
    \ (Att) of a query position (the white point) for the two modalities. Next, we\
    \ adopt the self-attention (SA) mechanism (shown as red paths), which uses \u201C\
    RGB Att\u201D to propagate context features on \u201CRGB FM\u201D, obtaining the\
    \ feature map \u201CSA FM\u201D and the final saliency map \u201CSA SM\u201D.\
    \ As contrast, we also adopt the proposed mutual-attention (MA) mechanism (shown\
    \ as green paths), which uses \u201CDepth Att\u201D to propagate context features\
    \ on \u201CRGB FM\u201D, obtaining \u201CMA FM\u201D and \u201CMA SM\u201D. We\
    \ observe that the mutual-attention mechanism can offer different guidance for\
    \ context propagation and obtain better SOD results."
  Figure 10 Link: articels_figures_by_rev_year\2021\Learning_Selective_Mutual_Attention_and_Contrast_for_RGBD_Saliency_Detection\figure_10.jpg
  Figure 10 caption: Example depth maps for the smallest (top row) and largest (bottom
    row) selective attention weights.
  Figure 2 Link: articels_figures_by_rev_year\2021\Learning_Selective_Mutual_Attention_and_Contrast_for_RGBD_Saliency_Detection\figure_2.jpg
  Figure 2 caption: Top 60% scene and object category distributions of our proposed
    ReDWeb-S dataset.
  Figure 3 Link: articels_figures_by_rev_year\2021\Learning_Selective_Mutual_Attention_and_Contrast_for_RGBD_Saliency_Detection\figure_3.jpg
  Figure 3 caption: Comparison of nine RGB-D SOD dataset in terms of the distributions
    of global contrast and interior contrast.
  Figure 4 Link: articels_figures_by_rev_year\2021\Learning_Selective_Mutual_Attention_and_Contrast_for_RGBD_Saliency_Detection\figure_4.jpg
  Figure 4 caption: Comparison of the average annotation maps for nine RGB-D SOD benchmark
    datasets. We also use a 2D Gaussian distribution to fit each map and mark the
    corresponding coordinates of the center point ( mu x and mu y ) and the standard
    deviations ( sigma x and sigma y ).
  Figure 5 Link: articels_figures_by_rev_year\2021\Learning_Selective_Mutual_Attention_and_Contrast_for_RGBD_Saliency_Detection\figure_5.jpg
  Figure 5 caption: Example images of our proposed ReDWeb-S dataset.
  Figure 6 Link: articels_figures_by_rev_year\2021\Learning_Selective_Mutual_Attention_and_Contrast_for_RGBD_Saliency_Detection\figure_6.jpg
  Figure 6 caption: Comparison of the distribution of object size (OS) for nine datasets.
  Figure 7 Link: articels_figures_by_rev_year\2021\Learning_Selective_Mutual_Attention_and_Contrast_for_RGBD_Saliency_Detection\figure_7.jpg
  Figure 7 caption: Network architecture of the proposed SMAC module. In the figure,
    otimes , oplus , ominus , and odot , indicate the matrix multiplication, addition,
    subtraction, and multiplication, respectively. bigcirc!!!!!!N and bigcirc!!!!!S
    are the Softmax normalization and the Sigmoid activation, respectively.
  Figure 8 Link: articels_figures_by_rev_year\2021\Learning_Selective_Mutual_Attention_and_Contrast_for_RGBD_Saliency_Detection\figure_8.jpg
  Figure 8 caption: "Architecture of our proposed RGB-D SOD network. (a) The main\
    \ two-stream network. The skip-connected VGG layers are marked in the first stream\
    \ by \u201CC\u201D and \u201Cfc\u201D. The channel numbers of the feature maps\
    \ are also marked in the second stream. \u201CNL\u201D is the Non-local module\
    \ [21]. (b) The structure of our DenseASPP module. Some key channel numbers are\
    \ also given. (c) The proposed decoder modules for the two streams. Here \u201C\
    UP\u201D means upsampling with bilinear interpolation. \u201CCross-Modal Fusion\u201D\
    \ can be either the proposed selective mutual attention module for the first three\
    \ decoders or a simple concatenation-based unidirectional fusion method for the\
    \ last two decoders. The dotted arrow means that the information fusion is not\
    \ used from the RGB branch to the depth branch when the simple concatenation-based\
    \ unidirectional fusion method is adopted. bigcirc!!!!!C and oplus represent concatenation\
    \ and element-wise summation, respectively."
  Figure 9 Link: articels_figures_by_rev_year\2021\Learning_Selective_Mutual_Attention_and_Contrast_for_RGBD_Saliency_Detection\figure_9.jpg
  Figure 9 caption: Visualization of some attention maps and feature maps. We show
    the feature maps ( boldsymbolX ), the attention maps ( A(boldsymbolX) ), the contrast
    attention maps ( mathcal C(boldsymbolX) ), and the output feature maps of the
    SMAC module ( boldsymbolZ ) for the RGB and depth modalities in two image pairs.
    In each image, the white point indicates the query position.
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: Nian Liu
  Name of the last author: Junwei Han
  Number of Figures: 13
  Number of Tables: 4
  Number of authors: 4
  Paper title: Learning Selective Mutual Attention and Contrast for RGB-D Saliency
    Detection
  Publication Date: 2021-10-26 00:00:00
  Table 1 caption: TABLE 1 Statistical Comparison of Different RGB-D SOD Benchmark
    Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Component Analysis on the Effectiveness of the Proposed
    SMAC RGB-D SOD Model
  Table 3 caption: TABLE 3 Quantitative Comparison of Our Proposed Model With Ten
    Other State-of-the-Art RGB-D SOD Methods
  Table 4 caption: TABLE 4 Comparison of the Average Performance Rank (APR) of Different
    Training Settings
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3122139
- Affiliation of the first author: department of data science and ai, faculty of information
    technology, monash university, melbourne, vic, australia
  Affiliation of the last author: school of computer science, the university of adelaide,
    adelaide, sa, australia
  Figure 1 Link: articels_figures_by_rev_year\2021\Learn_to_Predict_Sets_Using_FeedForward_Neural_Networks\figure_1.jpg
  Figure 1 caption: "Prediction of a set, e.g. an orderless set of bounding boxes\
    \ with unknown cardinality, using a feed-forward neural network. A backbone, e.g.\
    \ a CNN, MLP or Transformer, first encodes an input image to a feature vector\
    \ representation, which is then decoded by onefew outputs layers (referred also\
    \ as \u201Cdecoder\u201D in the text) such as MLP or transformer decoder, into\
    \ an output representation reflecting the states of the all set elements and their\
    \ distributions over cardinality and permutation."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Learn_to_Predict_Sets_Using_FeedForward_Neural_Networks\figure_2.jpg
  Figure 2 caption: "A schematic overview of our Deep Perm-Set Network (Scenario 2).\
    \ A tensor input, e.g. an RGB image, is passed through a backbone, e.g. CNN, and\
    \ output layers, e.g. series of fully connected layers with a collection of all\
    \ parameters denoted as w . The backbone can be either one of the standard convolutional\
    \ backbones [1], [14], [16], [48], a transformers encoder [49] or both and the\
    \ output layers can be a stack of fully connected layers or a transformers decoder\
    \ [49]. The final output layer consists of three parts shown by \u03B1 , O 1 and\
    \ O 2 , which respectively predict the cardinality, the states and the permutation\
    \ of the set elements. During training, \u03C0 \u2217 i,k representing a permutation\
    \ sample (attained by Eq. (9)), is used as the ground truth to update the loss\
    \ L perm ( \u03C0 \u2217 i,k , O 2 ) and to sort the elements of the ground truth\
    \ sets state in the L state ( Y m i \u03C0 \u2217 i,k , O 1 ) term in Eq. (10).\
    \ During inference, the optimal set Y \u2217 is only calculated using the cardinality\
    \ \u03B1 and the states O 1 outputs. \u03C0 \u2217 is an extra output for ordering\
    \ representation. For Scenarios 1 and 3, the permutation head, i.e. O 2 , is not\
    \ defined."
  Figure 3 Link: articels_figures_by_rev_year\2021\Learn_to_Predict_Sets_Using_FeedForward_Neural_Networks\figure_3.jpg
  Figure 3 caption: (a) Precisionrecall curves for the classification scores when
    the classifier is trained independently (red solid line) and when it is trained
    jointly with the cardinality term using our proposed joint approach (black solid
    line) on PASCAL VOC dataset. The circles represent the upper bound when ground
    truth cardinality is used for the evaluation of the corresponding classifiers.
    The ground truth prediction is shown by a blue triangle. (b) The value of F1 against
    the value of hyper-parameter U in log-scale (PASCAL VOC).
  Figure 4 Link: articels_figures_by_rev_year\2021\Learn_to_Predict_Sets_Using_FeedForward_Neural_Networks\figure_4.jpg
  Figure 4 caption: Qualitative comparison between our proposed set network with a
    shared backbone (JDS) and the deep set networks with softmax (DS (BCE-Sftmx))
    and Negative Binomial (DS (BCE-NB)) as the cardinality loss. For each image, the
    ground truth tags and the predictions for our JDS and the two baselines are denoted
    below. False positives are highlighted in red. Our JDS approach reduces both cardinality
    and classification error.
  Figure 5 Link: articels_figures_by_rev_year\2021\Learn_to_Predict_Sets_Using_FeedForward_Neural_Networks\figure_5.jpg
  Figure 5 caption: A qualitative comparison between the detection performance of
    (a) Faster R-CNN, (b) YOLO v3 and (c) our basic set detection baseline (i.e. ResNet101+L1-smooth
    loss) on heavily overlapping pedestrians from MOTChallenge benchmark. Both Faster
    R-CNN and YOLO v3 fail to properly detect heavily occluded pedestrians due to
    NMS.
  Figure 6 Link: articels_figures_by_rev_year\2021\Learn_to_Predict_Sets_Using_FeedForward_Neural_Networks\figure_6.jpg
  Figure 6 caption: Computation overhead of training (yellow line) and inference (green
    line) iteration in millisecond clock time over number of objects. We use our set
    prediction model with backbone ResNet101. We assume the number of ground truth
    objects is fixed and is equal to the maximum number of output instances (i.e.
    the worst case scenario). This computation overhead can increase significantly
    with more number of objects during training, due to the computation required for
    solving the assignment problem (blue line), while it does not affect inference
    time.
  Figure 7 Link: articels_figures_by_rev_year\2021\Learn_to_Predict_Sets_Using_FeedForward_Neural_Networks\figure_7.jpg
  Figure 7 caption: A qualitative comparison for our best set-based detection baselines
    (ResNet50+Transformers Ecoder-decoder) with (a) the cardinality term (b) without
    cardinality module (cf. [30]) on PASCAL VOC detection dataset. The baseline model
    trained with cardinality module can effectively reduce both false positives and
    false negatives.
  Figure 8 Link: articels_figures_by_rev_year\2021\Learn_to_Predict_Sets_Using_FeedForward_Neural_Networks\figure_8.jpg
  Figure 8 caption: A query digit (left) and a set of digits (right) for the proposed
    CAPTCHA test. The ground truth and our predicted solutions are shown by white
    and red boxes respectively.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Hamid Rezatofighi
  Name of the last author: Ian Reid
  Number of Figures: 8
  Number of Tables: 5
  Number of authors: 9
  Paper title: Learn to Predict Sets Using Feed-Forward Neural Networks
  Publication Date: 2021-10-27 00:00:00
  Table 1 caption: TABLE 1 All Mathematical Symbols and Notations Used Throughout
    the Paper
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Quantitative Results for Multi-Label Image Classification
    on (a) the PASCAL VOC and (b) the MS COCO Datasets
  Table 3 caption: TABLE 3 Detection Results on the Pedestrian Detection Dataset (Cropped
    MOTChallenge) Measured by mAP, F1 Score and MR Rate
  Table 4 caption: TABLE 4 Detection Results for the Set-Based Baselines Using Different
    Backbones and Decoders on (a) Pascal VOC and (b) MS COCO Measured by mAP, the
    Best F1 Scores and MR
  Table 5 caption: TABLE 5 Accuracy for Solving the CAPTCHA Test
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3122970
- Affiliation of the first author: school of electronic engineering, xidian university,
    xian, shaanxi, china
  Affiliation of the last author: jd explore academy, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Transferable_Coupled_Network_for_ZeroShot_SketchBased_Image_Retrieval\figure_1.jpg
  Figure 1 caption: Zero-shot sketch-based image retrieval aims at performing sketch-based
    image retrieval under the realistic scenario of zero-shot learning.
  Figure 10 Link: articels_figures_by_rev_year\2021\Transferable_Coupled_Network_for_ZeroShot_SketchBased_Image_Retrieval\figure_10.jpg
  Figure 10 caption: 'Left: t-SNE results of using 64 dimensional features on the
    random selected 8 testing categories of Sketchy. The dot and cross stand for natural
    image and sketch, respectively. Right: corresponding retrieval examples.'
  Figure 2 Link: articels_figures_by_rev_year\2021\Transferable_Coupled_Network_for_ZeroShot_SketchBased_Image_Retrieval\figure_2.jpg
  Figure 2 caption: Our framework consists of transferable coupled network (i.e.,
    coupled image and sketch encoders), feature embedding network, discrimination
    module and semantic metric module. Specifically, the first one includes soft weight-shared
    convolutional layers (i.e., constrained with L SWS ) and independent batch normalization
    layers. Retrieval features are obtained through feature embedding network, along
    with discrimination loss L DIS and semantic metric loss L SEM . At training stage,
    the guiding signals produced from teacher network as well as benchmark one-hot
    labels are provided to calculate L DIS . Similarly, word vectors and uniform noise
    are offered to compute L SEM .
  Figure 3 Link: articels_figures_by_rev_year\2021\Transferable_Coupled_Network_for_ZeroShot_SketchBased_Image_Retrieval\figure_3.jpg
  Figure 3 caption: Training loss and validation accuracy versus the number of iterations
    with 64 dimensional features on Sketchy.
  Figure 4 Link: articels_figures_by_rev_year\2021\Transferable_Coupled_Network_for_ZeroShot_SketchBased_Image_Retrieval\figure_4.jpg
  Figure 4 caption: Analysis of top-30 singular values (max-normalized in each modality)
    with 64 dimensional features on Sketchy.
  Figure 5 Link: articels_figures_by_rev_year\2021\Transferable_Coupled_Network_for_ZeroShot_SketchBased_Image_Retrieval\figure_5.jpg
  Figure 5 caption: The illustration of our proposed semantic metric within each batch.
    First, the semantic anchor (e.g., the yellow point with dotted circle) is generated
    by taking semantic representation and uniform noise as inputs. Then the hardest
    negative and positive sample are determined based on the new anchor. We believe
    that new anchor will be more central in its class center than other samples. Finally,
    we optimize the embedding by minimizing the positive distance (e.g., d pos ) and
    maximizing the negative one (e.g., d neg ).
  Figure 6 Link: articels_figures_by_rev_year\2021\Transferable_Coupled_Network_for_ZeroShot_SketchBased_Image_Retrieval\figure_6.jpg
  Figure 6 caption: The Qualitative comparison of Sketchy, TU-Berlin and QuickDraw
    datasets.
  Figure 7 Link: articels_figures_by_rev_year\2021\Transferable_Coupled_Network_for_ZeroShot_SketchBased_Image_Retrieval\figure_7.jpg
  Figure 7 caption: Top-13 retrieval results of testing samples on three large-scale
    datasets by using 64 dimensional real-valued features. The blue ticks stand for
    correctly retrieved candidates while the red crosses denote wrong retrievals.
  Figure 8 Link: articels_figures_by_rev_year\2021\Transferable_Coupled_Network_for_ZeroShot_SketchBased_Image_Retrieval\figure_8.jpg
  Figure 8 caption: Retrieval results of the random selected 8 categories on Sketchy
    dataset.
  Figure 9 Link: articels_figures_by_rev_year\2021\Transferable_Coupled_Network_for_ZeroShot_SketchBased_Image_Retrieval\figure_9.jpg
  Figure 9 caption: The results of top-30 singular values (max-normalized in each
    modality) with 64 dimensional features on Sketchy.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Hao Wang
  Name of the last author: Dacheng Tao
  Number of Figures: 13
  Number of Tables: 6
  Number of authors: 4
  Paper title: Transferable Coupled Network for Zero-Shot Sketch-Based Image Retrieval
  Publication Date: 2021-10-27 00:00:00
  Table 1 caption: TABLE 1 The Statistics of Three Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The Comparison of ZS-SBIR Performance Between TCN and Existing
    methods
  Table 3 caption: TABLE 3 Performance Comparison of Using Different Backbones in
    TCN with 64 Dimensional features
  Table 4 caption: TABLE 4 Ablation Studies of Individual Component in TCN With 64
    Dimensional Features
  Table 5 caption: TABLE 5 The Performance Improvement (Relative Increase) of the
    Random Selected 8 Categories at Testing Stage With 64 Dimensional Features on
    Sketchy
  Table 6 caption: TABLE 6 Classification Accuracy of Digit Images
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3123315
- Affiliation of the first author: school of cyber science and technology, shenzhen
    campus, sun yat-sen university, shenzhen, guangdong, china
  Affiliation of the last author: department of machine intelligence, key laboratory
    of machine perception (moe), peking university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Face_Restoration_via_PlugandPlay_D_Facial_Priors\figure_1.jpg
  Figure 1 caption: "Visual comparison with state-of-the-art face restoration methods.\
    \ Task I is face super-resolution: ( a ) 16\xD716 LR input. ( b ) 128\xD7128 HR\
    \ ground-truth. ( c ) 2D face parsing priors. ( d ) our 3D rendered priors. (\
    \ e ) Very Deep Residual Channel Attention Network (RCAN) [23]. ( f ) Progressive\
    \ Face Super-Resolution using the facial landmark (PSR-FAN) [19]. ( g ) End-to-End\
    \ Learning Face Super-Resolution with Facial Parsing Priors (FSRNet) [20]. ( h\
    \ ) Our proposed method by incorporating the 3D facial priors. Task II is face\
    \ deblurring: ( i ) Blurry input. ( j ) Ground truth. ( k ) 2D face parsing priors.\
    \ ( l ) our 3D rendered priors. ( m ) Nah et al. [24]. ( n ) Pan et al. [25].\
    \ ( o ) Shen et al. [22] using 2D parsing priors. ( p ) Our proposed method by\
    \ incorporating the 3D facial priors."
  Figure 10 Link: articels_figures_by_rev_year\2021\Face_Restoration_via_PlugandPlay_D_Facial_Priors\figure_10.jpg
  Figure 10 caption: "Visual comparison with state-of-the-art face super-resolution\
    \ method [59]. ( a ) 16\xD716 LR input. ( b ) Deep Iterative Collaboration [59].\
    \ ( c ) Our proposed method by incorporating the 3D facial priors. ( d ) 128\xD7\
    128 HR ground-truth."
  Figure 2 Link: articels_figures_by_rev_year\2021\Face_Restoration_via_PlugandPlay_D_Facial_Priors\figure_2.jpg
  Figure 2 caption: 'The proposed face restoration architecture. Our model consists
    of two branches: the top block is a ResNet-50 Network to extract the 3D facial
    coefficients and restore a sharp face rendered structure. The bottom block is
    dedicated to face restoration guided by the facial coefficients and rendered sharp
    face structures which are concatenated by the Spatial Feature Transform (SFT)
    layer. For face restoration branch, it comprises four parts: 1). Feature Extraction
    branch uses a series of convolutional layers to extract the features of the priors.
    2). Spatial Attention branch employs SFT layers to well incorporate the facial
    rendered priors. 3). Residual Channel Attention explores the knowledge and correlations
    between the channels. 4). HR branch is to reconstruct HR images. The detailed
    configuration of the proposed method is listed in the appendix.'
  Figure 3 Link: articels_figures_by_rev_year\2021\Face_Restoration_via_PlugandPlay_D_Facial_Priors\figure_3.jpg
  Figure 3 caption: The visual examples that compare our 3D priors against 2D parsing
    priors. As shown, it verifies the superiority of our 3D priors and the statement
    that the 2D parsing priors do not have corresponding position and shape of faces
    but our 3D priors provide clear spatial positions and skin color of the facial
    components, especially on large pose variation. The first row shows the facial
    priors from low-resolution inputs and the last row shows the facial priors from
    blurry ones. (a) low-resolution inputs. (b) 2D parsing priors obtained from low-resolution
    inputs. (c) our 3D rendered face structures obtained from low-resolution inputs.
    (d) ground-truths of the low-resolution inputs. (e) blurry inputs. (f) 2D parsing
    priors obtained from blurry inputs. (g) our 3D rendered face structures obtained
    from blurry inputs. (h) the Ground truth of the blurry inputs.
  Figure 4 Link: articels_figures_by_rev_year\2021\Face_Restoration_via_PlugandPlay_D_Facial_Priors\figure_4.jpg
  Figure 4 caption: The structure of the SFT layer. The rendered faces and feature
    vectors are regarded as the guidance for face restoration.
  Figure 5 Link: articels_figures_by_rev_year\2021\Face_Restoration_via_PlugandPlay_D_Facial_Priors\figure_5.jpg
  Figure 5 caption: 'Residual channel attention block (RCAB): an attention mechanism
    to explore the inter-dependency between inputs and facial priors.'
  Figure 6 Link: articels_figures_by_rev_year\2021\Face_Restoration_via_PlugandPlay_D_Facial_Priors\figure_6.jpg
  Figure 6 caption: "Face super-resolution comparison with state-of-the-art methods:\
    \ magnification factors \xD78 and the input resolution 16\xD716. Best viewed by\
    \ zooming in on the screen. Here we compare our algorithm with single image super-resolution\
    \ approaches (VDSR [27], RCAN [23], RDN [37], SRCNN) [28]), face super-resolution\
    \ methods (TDAE [52], Wavelet-SRNet [39]), and 2D facial prior-based methods (FSRNet\
    \ [20], FSRGAN [20], PSR-FAN [19])."
  Figure 7 Link: articels_figures_by_rev_year\2021\Face_Restoration_via_PlugandPlay_D_Facial_Priors\figure_7.jpg
  Figure 7 caption: "Face super-resolution visual comparison with state-of-the-art\
    \ methods (\xD78). The results by the proposed method have fewer artifacts on\
    \ face components (e.g., eyes, mouth, and nose). Here we compare our algorithm\
    \ with single image super-resolution approaches (VDSR [27], RCAN [23], RDN [37],\
    \ SRCNN) [28]), face super-resolution methods (TDAE [52], Wavelet-SRNet [39]),\
    \ and 2D facial prior-based methods (FSRNet [20], FSRGAN [20], PSR-FAN [19])."
  Figure 8 Link: articels_figures_by_rev_year\2021\Face_Restoration_via_PlugandPlay_D_Facial_Priors\figure_8.jpg
  Figure 8 caption: "Face super-resolution visual comparison with PULSE (\xD78)."
  Figure 9 Link: articels_figures_by_rev_year\2021\Face_Restoration_via_PlugandPlay_D_Facial_Priors\figure_9.jpg
  Figure 9 caption: "Face deblurring visual comparison with PULSE (\xD78)."
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Xiaobin Hu
  Name of the last author: Hongbin Zha
  Number of Figures: 18
  Number of Tables: 8
  Number of authors: 8
  Paper title: Face Restoration via Plug-and-Play 3D Facial Priors
  Publication Date: 2021-10-27 00:00:00
  Table 1 caption: TABLE 1 Face Super-Resolution Quantitative Results on the CelebA
    and Menpo Test Dataset With Different Large Facial Pose Variations (e.g., left,
    right, and semifrontal)
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Real-World Face Super-Resolution Quantitative Results (PIQE
    and NIQE) With Different Configurations
  Table 3 caption: TABLE 3 Face Deblurring Quantitative Comparison With State-of-the-Art
    Methods
  Table 4 caption: TABLE 4 Quantitative PSNR and SSIM Results of Face Video Deblurring
    on the Synthetic Datasets Using Different Deblurring Methods
  Table 5 caption: TABLE 5 Identity Similarity With Different Configurations on 9
    Synthetic Testing Videos Generated From the 300VW Dataset
  Table 6 caption: TABLE 6 Average Running Time on Deblurring Testing Video Frames
  Table 7 caption: TABLE 7 Ablation Study Results of Face Deblurring With Different
    Configurations on 9 Synthetic Testing Videos Generated From the 300VW Dataset
  Table 8 caption: TABLE 8 Position Deviation Analysis of Rendered 3D Priors in the
    Tasks of Face Super-Resolution and Deblurring
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3123085
- Affiliation of the first author: university of maryland, college park, md, usa
  Affiliation of the last author: university of maryland, college park, md, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\Leveraging_HandObject_Interactions_in_Assistive_Egocentric_Vision\figure_1.jpg
  Figure 1 caption: Examples of egocentric photos taken by people with visual impairments
    for object recognition in the crowdsourcing app, VizWiz [2], and a personalized
    fine-grained object recognition model in TEgO [3]. These examples illustrate the
    need for object-of-interest localization for better camera framing.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Leveraging_HandObject_Interactions_in_Assistive_Egocentric_Vision\figure_2.jpg
  Figure 2 caption: 'The architecture of our hand-primed object recognition model.
    It consists of two neural networks: one (above) for hand segmentation and another
    (below) for object localization and recognition. The hand segmentation network
    is pre-trained and frozen at the training time, and its output is infused into
    the other model for object localization and classification. A fully-connected
    layer for classification is appended to conv7 of the localization model. It allows
    object classification to share the features learned for object localization, and
    vice versa.'
  Figure 3 Link: articels_figures_by_rev_year\2021\Leveraging_HandObject_Interactions_in_Assistive_Egocentric_Vision\figure_3.jpg
  Figure 3 caption: 'Input data for training models: an original image, its hand mask,
    and its object center annotation.'
  Figure 4 Link: articels_figures_by_rev_year\2021\Leveraging_HandObject_Interactions_in_Assistive_Egocentric_Vision\figure_4.jpg
  Figure 4 caption: 'Input data for two different versions of Faster R-CNN: an original
    image, a whole object bounding box, and an object center bounding box.'
  Figure 5 Link: articels_figures_by_rev_year\2021\Leveraging_HandObject_Interactions_in_Assistive_Egocentric_Vision\figure_5.jpg
  Figure 5 caption: Object localization outputs of the comparison models and our approach.
    The object localization output is overlaid with the blue color on the testing
    examples; the threshold for the localization output was set to 0.5. In comparisons
    with NoHandData, our method (Ours) shows that the hand information helps localize
    the target object in general.
  Figure 6 Link: articels_figures_by_rev_year\2021\Leveraging_HandObject_Interactions_in_Assistive_Egocentric_Vision\figure_6.jpg
  Figure 6 caption: Failure cases of our method despite the appropriate hand segmentation
    outputs on these testing examples.
  Figure 7 Link: articels_figures_by_rev_year\2021\Leveraging_HandObject_Interactions_in_Assistive_Egocentric_Vision\figure_7.jpg
  Figure 7 caption: TEgO examples when object classification helps object localization.
    The previous models without classification fail to locate the target object, but
    become able to localize it after being trained for classification.
  Figure 8 Link: articels_figures_by_rev_year\2021\Leveraging_HandObject_Interactions_in_Assistive_Egocentric_Vision\figure_8.jpg
  Figure 8 caption: TEgO examples show that models sometimes become confused or unable
    to locate an object of interest when trained for both object localization and
    classification (With Classification).
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.83
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Kyungjun Lee
  Name of the last author: Hernisa Kacorri
  Number of Figures: 8
  Number of Tables: 3
  Number of authors: 3
  Paper title: Leveraging Hand-Object Interactions in Assistive Egocentric Vision
  Publication Date: 2021-10-27 00:00:00
  Table 1 caption: TABLE 1 Quantitative Analysis of the Object Localization Models
    Without Classification
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Object Classification Performances
  Table 3 caption: TABLE 3 Object Localization Performances of the Models on Two Subsets
    of TEgO
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3123303
- Affiliation of the first author: sony r&d center europe stuttgart laboratory 1,
    stuttgart, germany
  Affiliation of the last author: university of padova, padova, italy
  Figure 1 Link: articels_figures_by_rev_year\2021\Unsupervised_Domain_Adaptation_of_Deep_Networks_for_ToF_Depth_Refinement\figure_1.jpg
  Figure 1 caption: Representation of the ToF depth refinement network R . The upper
    part is the coarse branch. It is able to capture a wide receptive field at the
    cost of a lower output resolution. The lower part is the fine branch. This takes
    as input the raw ToF data and the output of the coarse branch in order to estimate
    an accurate depth map of the scene.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Unsupervised_Domain_Adaptation_of_Deep_Networks_for_ToF_Depth_Refinement\figure_2.jpg
  Figure 2 caption: "Flux of data during the training steps of the translation networks\
    \ T s\u21D2r and T s\u21D2r used inside the in-DA procedure."
  Figure 3 Link: articels_figures_by_rev_year\2021\Unsupervised_Domain_Adaptation_of_Deep_Networks_for_ToF_Depth_Refinement\figure_3.jpg
  Figure 3 caption: "When the two translation networks are trained, T s\u21D2r is\
    \ used to translate the whole synthetic training set I s in I \xAF f r . The latter\
    \ one is used to train R in order to adapt to real data without using real ground\
    \ truth."
  Figure 4 Link: articels_figures_by_rev_year\2021\Unsupervised_Domain_Adaptation_of_Deep_Networks_for_ToF_Depth_Refinement\figure_4.jpg
  Figure 4 caption: Flux of data during a training step of feat-DA method.
  Figure 5 Link: articels_figures_by_rev_year\2021\Unsupervised_Domain_Adaptation_of_Deep_Networks_for_ToF_Depth_Refinement\figure_5.jpg
  Figure 5 caption: Flux of data during a training step of out-DA method.
  Figure 6 Link: articels_figures_by_rev_year\2021\Unsupervised_Domain_Adaptation_of_Deep_Networks_for_ToF_Depth_Refinement\figure_6.jpg
  Figure 6 caption: 'Qualitative comparison: the images show the ToF depth error on
    some sample scenes. It compares the considered domain adaptation techniques (in-DA,
    feat-DA and out-DA) with the standard ToF acquisitions at 60 mathrmMHz and the
    output of the machine learning based depth refinement method CF-synth [2]. The
    error maps are computed as the estimated depth minus the ground truth depth. The
    first three rows depict scenes extracted from the dataset S4 , the last three
    are instead extracted from the dataset S5 . All the values are measured in meters.'
  Figure 7 Link: articels_figures_by_rev_year\2021\Unsupervised_Domain_Adaptation_of_Deep_Networks_for_ToF_Depth_Refinement\figure_7.jpg
  Figure 7 caption: Comparison on the reconstruction of a corner scene. The plot shows
    the output of the methods on the cross section highlighted with a red horizontal
    line on the corner depth map.
  Figure 8 Link: articels_figures_by_rev_year\2021\Unsupervised_Domain_Adaptation_of_Deep_Networks_for_ToF_Depth_Refinement\figure_8.jpg
  Figure 8 caption: Qualitative evaluation of the ToF depth translation operation
    from synthetic to fake real data using the network TsRightarrow r . The scenes
    are extracted from the synthetic dataset S1 . The values on the color bars are
    measured in meters.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Gianluca Agresti
  Name of the last author: Pietro Zanuttigh
  Number of Figures: 8
  Number of Tables: 2
  Number of authors: 5
  Paper title: Unsupervised Domain Adaptation of Deep Networks for ToF Depth Refinement
  Publication Date: 2021-10-29 00:00:00
  Table 1 caption: TABLE 1 Quantitative Evaluation on the Depth Refinement Performance
    of the Considered Domain Adaptation Techniques and of Other State of the Art Methods
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Quantitative Evaluation of Domain Adaptation Method Combination
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3123843
- Affiliation of the first author: department of mechanical engineering, visual intelligence
    lab, korea advanced institute of science and technology, daejeon, south korea
  Affiliation of the last author: department of mechanical engineering, visual intelligence
    lab, korea advanced institute of science and technology, daejeon, south korea
  Figure 1 Link: articels_figures_by_rev_year\2021\Deep_Learning_for_HDR_Imaging_StateoftheArt_and_Future_Trends\figure_1.jpg
  Figure 1 caption: Hierarchical and structural taxonomy of HDR imaging with deep
    learning.
  Figure 10 Link: articels_figures_by_rev_year\2021\Deep_Learning_for_HDR_Imaging_StateoftheArt_and_Future_Trends\figure_10.jpg
  Figure 10 caption: 'Visual results of IR-based methods. (a) HDR content for image
    reconstruction [24]. (b) HDR results from IR and LDR image fusion [144]. From
    left to right: IR, LDR, and HDR results.'
  Figure 2 Link: articels_figures_by_rev_year\2021\Deep_Learning_for_HDR_Imaging_StateoftheArt_and_Future_Trends\figure_2.jpg
  Figure 2 caption: DNN-based HDR imaging approaches. Learning to reconstruct an HDR
    image (a) by aligning, merging, and fusing multi-exposure LDR images; (b) by using
    an inverse one-mapping network from single exposure LDR images.
  Figure 3 Link: articels_figures_by_rev_year\2021\Deep_Learning_for_HDR_Imaging_StateoftheArt_and_Future_Trends\figure_3.jpg
  Figure 3 caption: Deep multi-exposure HDR imaging methods. (a) Optical flow-based
    (net) LDR image alignment. (b) Direct concatenation of features of LDR images
    to merge them and reconstruct the HDR image. (c) Correlation-guided LDR feature
    alignment, followed by merging and reconstruction. (d) Image translation-based
    LDR image alignment. (e) Reconstruction of the HDR image from static LDR image
    fusion.
  Figure 4 Link: articels_figures_by_rev_year\2021\Deep_Learning_for_HDR_Imaging_StateoftheArt_and_Future_Trends\figure_4.jpg
  Figure 4 caption: Deep single-exposure HDR imaging methods. (a) Directly learning
    a domain transformation autoencoder from a single LDR image. (b) Generating bracketed
    (up- and over-exposed) LDR image stacks and then reconstructing an HDR image.
    (c) Using a recurrent (efficient) learning network structure. (d) Learning camera-imaging
    pipeline with DNNs. (e) Learning camera sensing with DNNs (neural sensing encoder).
  Figure 5 Link: articels_figures_by_rev_year\2021\Deep_Learning_for_HDR_Imaging_StateoftheArt_and_Future_Trends\figure_5.jpg
  Figure 5 caption: Deep stereo-based HDR imaging method [100].
  Figure 6 Link: articels_figures_by_rev_year\2021\Deep_Learning_for_HDR_Imaging_StateoftheArt_and_Future_Trends\figure_6.jpg
  Figure 6 caption: Deep HDR imaging with SR methods. (a) SR is performed first and
    then the SR HDR image is reconstructed. (b) HDR image is reconstucted first and
    then SR is performed. (c) LDR SR, LR HDR imaging, and HDR SR are perfomed jointly.
    (d) LDR images are divided into base and detail layers and fusion of the two branches
    are learned. (d) SR HDR image is reconstructed from multi-exposure LDR images
    with optical flow and motion compensation.
  Figure 7 Link: articels_figures_by_rev_year\2021\Deep_Learning_for_HDR_Imaging_StateoftheArt_and_Future_Trends\figure_7.jpg
  Figure 7 caption: 'Examples of events in HDR image reconstruction. (a) HDR images
    from [104]. From left to right: Dark APS images, embedded LR events, reconstructed
    LR HDR images, restored LR HDR images, and SR HDR images. (b) HDR images of [122]
    (fourth column), where 2nd and 3rd columns show the reconstructed LR images of
    [23] and [123], respectively.'
  Figure 8 Link: articels_figures_by_rev_year\2021\Deep_Learning_for_HDR_Imaging_StateoftheArt_and_Future_Trends\figure_8.jpg
  Figure 8 caption: Event-based deep HDR methods. (a) Event-to-HDR image reconstruction.
    (b) Event-guided HDR image reconstruction [22].
  Figure 9 Link: articels_figures_by_rev_year\2021\Deep_Learning_for_HDR_Imaging_StateoftheArt_and_Future_Trends\figure_9.jpg
  Figure 9 caption: Thermal camera-based HDR methods. (a) Supervised color HDR reconstruction
    method (b) Unsupervised color HDR reconstruction methods with two steps [139].
    (c) IR and LDR image concatenation-based HDR reconstruction methods (d) IR and
    LDR feature concatenation-based HDR reconstruction method.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.56
  Name of the first author: Lin Wang
  Name of the last author: Kuk-Jin Yoon
  Number of Figures: 10
  Number of Tables: 3
  Number of authors: 2
  Paper title: 'Deep Learning for HDR Imaging: State-of-the-Art and Future Trends'
  Publication Date: 2021-10-29 00:00:00
  Table 1 caption: TABLE 1 Summary of Publicly Available Benchmark Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Deep Multi-Exposure HDR Reconstruction Employed by Some
    Representative Methods
  Table 3 caption: TABLE 3 Deep Single-Exposure HDR Imaging by Some Representative
    Methods
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3123686
