- Affiliation of the first author: school of data science, fudan university, shanghai,
    china
  Affiliation of the last author: department of computer science, university of british
    columbia, vancouver, bc, canada
  Figure 1 Link: articels_figures_by_rev_year\2019\VocabularyInformed_ZeroShot_and_OpenSet_Learning\figure_1.jpg
  Figure 1 caption: "Illustration of the semantic embeddings learned (left) using\
    \ support vector regression (SVR) and (right) using the proposed vocabulary-informed\
    \ learning (Deep WMM-Voc) approach. In both cases, t-SNE visualization is used\
    \ to illustrate samples from 4 sourceauxiliary classes (denoted by \xD7) and 2\
    \ targetzero-shot classed (denoted by \u2218 ) from the ImageNet dataset. Decision\
    \ boundaries, illustrated by dashed lines, are drawn by hand for visualization.\
    \ The large margin constraints, both among the sourcetarget classes and the external\
    \ vocabulary atoms, are denoted by arrows and words on the right. Note that the\
    \ WMM-Voc approach on the right leads to a better embedding with more compact\
    \ and separated classes (e.g., see truck and car or unicycle and tricycle)."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\VocabularyInformed_ZeroShot_and_OpenSet_Learning\figure_2.jpg
  Figure 2 caption: Illustration of margin distribution of prototypes in the semantic
    space.
  Figure 3 Link: articels_figures_by_rev_year\2019\VocabularyInformed_ZeroShot_and_OpenSet_Learning\figure_3.jpg
  Figure 3 caption: The ZSL results on AwA by different settings.
  Figure 4 Link: articels_figures_by_rev_year\2019\VocabularyInformed_ZeroShot_and_OpenSet_Learning\figure_4.jpg
  Figure 4 caption: Supervised learning results of AwA datasets.
  Figure 5 Link: articels_figures_by_rev_year\2019\VocabularyInformed_ZeroShot_and_OpenSet_Learning\figure_5.jpg
  Figure 5 caption: Openset results of AwA datasets. We use 1600 training instances
    equally sampled from all source classes to train the model.
  Figure 6 Link: articels_figures_by_rev_year\2019\VocabularyInformed_ZeroShot_and_OpenSet_Learning\figure_6.jpg
  Figure 6 caption: The supervised and zero-shot learning results on ImageNet 20122010
    dataset.
  Figure 7 Link: articels_figures_by_rev_year\2019\VocabularyInformed_ZeroShot_and_OpenSet_Learning\figure_7.jpg
  Figure 7 caption: "Open set recognition results on ImageNet 20122010 dataset: Openness\
    \ = 0.9839. Chance = 3.2e\u22124% . We use the synsets of each class\u2014 a set\
    \ of synonymous (word or prhase) terms as the ground truth names for each instance.\
    \ We use the model trained with 50,000 instances sampled equally from source classes."
  Figure 8 Link: articels_figures_by_rev_year\2019\VocabularyInformed_ZeroShot_and_OpenSet_Learning\figure_8.jpg
  Figure 8 caption: 'Visualization of the semantic space: We show the t-SNE visualization
    of the semantic space. The words in boxes are the mapping of training image in
    the semantic space, and close neighbors are shown. The neighborhoods extend the
    single training data to a space semantically meaningful.'
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yanwei Fu
  Name of the last author: Leonid Sigal
  Number of Figures: 8
  Number of Tables: 7
  Number of authors: 7
  Paper title: Vocabulary-Informed Zero-Shot and Open-Set Learning
  Publication Date: 2019-06-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Zero-Shot Comparison on AwA
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Classification Accuracy (Top-1 Top-5) on AwA Dataset for Supervised,General
      Zero-Shot and Zero-Shot Settings for 100-dim and 1000-dim word2vec Representation
      (200 Instances)
  Table 3 caption:
    table_text: TABLE 3 The G-ZSL Results (100-dim1000-dim) of AwA Dataset
  Table 4 caption:
    table_text: TABLE 4 The Classification Accuracy (Top-1 Top-5) of ImageNet 20122010
      Dataset on ZERO-SHOT and SUPERVISED Settings Using 3000 Source Training Instances
  Table 5 caption:
    table_text: TABLE 5 The G-ZSL Results (1000-dim) of ImageNet Datasets
  Table 6 caption:
    table_text: TABLE 6 Results of Few-Shot Target Training Instances on ImageNet
      Dataset
  Table 7 caption:
    table_text: 'TABLE 7 ImageNet Comparison to State-of-the-Art on ZSL: We Compare
      the Results of Using 3,000all Training Instances for All Methods; T-1 (top 1)
      and T-5 (top 5) Classification in (%) Is Reported'
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2922175
- Affiliation of the first author: national key laboratory for novel software technology,
    nanjing university, nanjing, jiangsu, china
  Affiliation of the last author: national key laboratory for novel software technology,
    nanjing university, nanjing, jiangsu, china
  Figure 1 Link: articels_figures_by_rev_year\2019\Towards_Safe_Weakly_Supervised_Learning\figure_1.jpg
  Figure 1 caption: In practice weakly supervised learning may be not safe, i.e.,
    it may degenerate the performance with the usage of weakly supervised data.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Towards_Safe_Weakly_Supervised_Learning\figure_2.jpg
  Figure 2 caption: "Intuition of our proposal via the projection viewpoint. Intuitively,\
    \ the proposal learns a projection of f 0 onto a convex feasible set \u03A9 ."
  Figure 3 Link: articels_figures_by_rev_year\2019\Towards_Safe_Weakly_Supervised_Learning\figure_3.jpg
  Figure 3 caption: Classification accuracy of compared methods with different numbers
    of noise ratio.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Yu-Feng Li
  Name of the last author: Zhi-Hua Zhou
  Number of Figures: 3
  Number of Tables: 5
  Number of authors: 3
  Paper title: Towards Safe Weakly Supervised Learning
  Publication Date: 2019-06-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of Notations Used in This Paper
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Commonly Used Loss Functions \u2113(p,q) \u2113(p,q) for\
      \ Classification and Regression Tasks"
  Table 3 caption:
    table_text: "TABLE 3 Mean Square Error (mean \xB1 \xB1std) for the Compared Methods\
      \ and SafeW Using 5 and 10 Labeled Instances"
  Table 4 caption:
    table_text: "TABLE 4 Classification Accuracy (mean \xB1 \xB1 std) of Domain Adaptation\
      \ Task for the Compared Methods and SafeW on 20newsgroup and Landmine Datasets"
  Table 5 caption:
    table_text: "TABLE 5 Accuracy (mean \xB1 \xB1 std) for Compared Methods and SafeW\
      \ on 7 Datasets"
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2922396
- Affiliation of the first author: "instituto de sistemas e rob\xF3tica, instituto\
    \ superior t\xE9cnico, lisboa, portugal"
  Affiliation of the last author: australian centre for visual technologies, university
    of adelaide, adelaide, sa, australia
  Figure 1 Link: articels_figures_by_rev_year\2019\One_Shot_Segmentation_Unifying_Rigid_Detection_and_NonRigid_Segmentation_Using_E\figure_1.jpg
  Figure 1 caption: Proposed segmentation approach for deformable visual objects (e.g.,
    lips) that merges the rigid and non-rigid segmentation tasks (a), compared to
    the more common 2-step segmentation (b).
  Figure 10 Link: articels_figures_by_rev_year\2019\One_Shot_Segmentation_Unifying_Rigid_Detection_and_NonRigid_Segmentation_Using_E\figure_10.jpg
  Figure 10 caption: 'Qualitative comparison for the test sequence. The segmentation
    of each tracker (and the expert annotation) is shown in different colors as follows:
    Green (medical ground-truth), pink (COM tracker), yellow (CAR tracker), cyan (MMDA
    tracker) and red (proposed approach).'
  Figure 2 Link: articels_figures_by_rev_year\2019\One_Shot_Segmentation_Unifying_Rigid_Detection_and_NonRigid_Segmentation_Using_E\figure_2.jpg
  Figure 2 caption: Generation of samples I(g(m)) that are used as an input for the
    proposed classifier p(m|I,D) in (4). For each example ((a), (b) and (c)) the top-left
    image shows the initial grid containing the initial shape (red) and the X-scale
    deformed shape (blue). The middle and top-right images show the grid obtained
    according to the imposed deformation (magenta and yellow). At the bottom ((a),
    (b) and (c)) it is shown the initial patch (bottom-left) and the resulting deformation
    patch obtained with the TPS (bottom-right).
  Figure 3 Link: articels_figures_by_rev_year\2019\One_Shot_Segmentation_Unifying_Rigid_Detection_and_NonRigid_Segmentation_Using_E\figure_3.jpg
  Figure 3 caption: The graph shows the input LV contours (described in Section 13.1,
    i.e., S after the PCA reduction, the first three components are shown for each
    contour (see blue dots). A total of 496 annotations are given. This is the input
    to the sparse low dimensional manifold learning (Section 7) The manifold learning
    algorithm estimates patch member points distributed in 13 patches. In red it is
    shown the landmarks estimated for these 13 patches.
  Figure 4 Link: articels_figures_by_rev_year\2019\One_Shot_Segmentation_Unifying_Rigid_Detection_and_NonRigid_Segmentation_Using_E\figure_4.jpg
  Figure 4 caption: Statistical distribution for the LV contour points obtained with
    the EM algorithm.
  Figure 5 Link: articels_figures_by_rev_year\2019\One_Shot_Segmentation_Unifying_Rigid_Detection_and_NonRigid_Segmentation_Using_E\figure_5.jpg
  Figure 5 caption: Training procedure in the manifold. It is displayed the patches
    (in green), at the bottom, the positive (blue) and negative (orange) regions are
    displayed in the tangent hyper-planes.
  Figure 6 Link: articels_figures_by_rev_year\2019\One_Shot_Segmentation_Unifying_Rigid_Detection_and_NonRigid_Segmentation_Using_E\figure_6.jpg
  Figure 6 caption: Segmentation procedure in the manifold. The patches are also shown
    (top) and the gradient ascent (GA) is performed a low dimensional space (bottom)
    with intrinsic dimension M .
  Figure 7 Link: articels_figures_by_rev_year\2019\One_Shot_Segmentation_Unifying_Rigid_Detection_and_NonRigid_Segmentation_Using_E\figure_7.jpg
  Figure 7 caption: 'LV segmentation using 12-fold cross validation. From left to
    right: Jaccard index (JCD), average (AV) mean absolute distance (MAD) and mean
    sum of squared distance (MSSD) as a function of the number of positives and negatives
    used during training. The top row shows the results using the proposed method
    with the landmarks and the bottom row is the baseline approach based on the patch
    member points.'
  Figure 8 Link: articels_figures_by_rev_year\2019\One_Shot_Segmentation_Unifying_Rigid_Detection_and_NonRigid_Segmentation_Using_E\figure_8.jpg
  Figure 8 caption: Several positive-negative data augmentation configuration, using
    Jaccard index (a) and Average metric (b). The proposed data augmentation (blue)
    using elastic deformation is shown in blue, while the traditional data augmentation
    (with rigid deformations) is shown in red.
  Figure 9 Link: articels_figures_by_rev_year\2019\One_Shot_Segmentation_Unifying_Rigid_Detection_and_NonRigid_Segmentation_Using_E\figure_9.jpg
  Figure 9 caption: Quantitative results using the test sequences T 1 (top) and T
    2 (bottom). Quantitative comparison using (from left to right) JCD, AV, MAD and
    MSSD, measures. In each graph (from left to right in the box-plots) we show the
    performance of the algorithms COM, CAR, MMDA and the proposed framework for the
    (10,100),(15,150),(20,200),(50,500) positive-negative configurations.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jacinto C. Nascimento
  Name of the last author: Gustavo Carneiro
  Number of Figures: 16
  Number of Tables: 7
  Number of authors: 2
  Paper title: 'One Shot Segmentation: Unifying Rigid Detection and Non-Rigid Segmentation
    Using Elastic Regularization'
  Publication Date: 2019-06-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Obtaining the Manifold M M and Sparsity
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Training
  Table 3 caption:
    table_text: TABLE 3 Segmentation
  Table 4 caption:
    table_text: TABLE 4 Comparison of Object Segmentation Complexity in Different
      Approaches
  Table 5 caption:
    table_text: TABLE 5 Wilcoxon Signed-Rank Test (WSR Test) between the Volumes Estimated
      with the Proposed Approach and with the CAR [1], COM [2], [5] and MMDA Approaches
      on the LV Test Sequences
  Table 6 caption:
    table_text: TABLE 6 Wilcoxon Signed-Rank Test (WSR Test) between the Areas Estimated
      with the Proposed Approach and with the CAR [1] on the Surprise and Happy Test
      Sequences
  Table 7 caption:
    table_text: TABLE 7 Running Time Figures for the LV and Lips Sequences, Comparison
      with [6]
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2922959
- Affiliation of the first author: national institute of advanced industrial science
    and technology (aist), tokyo, japan
  Affiliation of the last author: national institute of advanced industrial science
    and technology (aist), tokyo, japan
  Figure 1 Link: articels_figures_by_rev_year\2019\RotationNet_for_Joint_Object_Categorization_and_Unsupervised_Pose_Estimation_fro\figure_1.jpg
  Figure 1 caption: "Illustration of the proposed method RotationNet. RotationNet\
    \ takes a partial set ( \u22651 images) of the full multi-view images of an object\
    \ as input and predicts its object category by rotation, where the best pose is\
    \ selected to maximize the object category likelihood. Here, viewpoints from which\
    \ the images are observed are jointly estimated to predict the pose of the object."
  Figure 10 Link: articels_figures_by_rev_year\2019\RotationNet_for_Joint_Object_Categorization_and_Unsupervised_Pose_Estimation_fro\figure_10.jpg
  Figure 10 caption: Classification accuracy versus number of views used for prediction.
    From left to right are shown the results on ModelNet40, ModelNet10, and our new
    dataset MIRO. The results in case (i) are shown in top and those in case (ii)
    are shown in bottom. See Table 5 for an overall performance comparison to existing
    methods on ModelNet40 and ModelNet10.
  Figure 2 Link: articels_figures_by_rev_year\2019\RotationNet_for_Joint_Object_Categorization_and_Unsupervised_Pose_Estimation_fro\figure_2.jpg
  Figure 2 caption: Illustration of inter-class alignment of 3D models. Objects are
    placed in the direction of ever-increasing appearance. Inter-class alignment enables
    comparison among different categories in the pose where image appearance is similar
    to each other, which emphasizes differences in different categories.
  Figure 3 Link: articels_figures_by_rev_year\2019\RotationNet_for_Joint_Object_Categorization_and_Unsupervised_Pose_Estimation_fro\figure_3.jpg
  Figure 3 caption: "Illustration of the training process of RotationNet, where the\
    \ number of views M is 3 and the number of categories N is 2. A training sample\
    \ consists of M images of an unaligned object and its category label y . For each\
    \ input image, our CNN (RotationNet) outputs M histograms with N+1 bins whose\
    \ norm is 1. The last bin of each histogram represents the \u201Cincorrect view\u201D\
    \ class, which serves as a weight of how likely the histogram does not correspond\
    \ to each viewpoint variable. According to the histogram values, we decide which\
    \ image corresponds to views 1, 2, and 3. There are three candidates for view\
    \ rotation: (1, 2, 3), (2, 3, 1), and (3, 1, 2). For each candidate, we calculate\
    \ the score for the ground-truth category (\u201Ccar\u201D in this case) by multiplying\
    \ the histograms and selecting the best choice: (2, 3, 1) in this case. Finally,\
    \ we update the CNN parameters in a standard back-propagation manner with the\
    \ estimated viewpoint variables. Note that it is the same CNN that is being used."
  Figure 4 Link: articels_figures_by_rev_year\2019\RotationNet_for_Joint_Object_Categorization_and_Unsupervised_Pose_Estimation_fro\figure_4.jpg
  Figure 4 caption: Illustration of three viewpoint setups considered in this work.
    A target object is placed on the center of each circle.
  Figure 5 Link: articels_figures_by_rev_year\2019\RotationNet_for_Joint_Object_Categorization_and_Unsupervised_Pose_Estimation_fro\figure_5.jpg
  Figure 5 caption: Visualization of responses to each viewpoint. Images with the
    maximum responses to each viewpoint are shown right.
  Figure 6 Link: articels_figures_by_rev_year\2019\RotationNet_for_Joint_Object_Categorization_and_Unsupervised_Pose_Estimation_fro\figure_6.jpg
  Figure 6 caption: "Variance change of the average images generated by concatenating\
    \ multi-view images (case (i)) in order of their predicted viewpoint variables.\
    \ Average images of 40 classes and images of the \u201Cchair\u201D class with\
    \ the same predicted viewpoint variable in iterations 0, 100, 200, and 400 are\
    \ shown in red boxes and blue boxes, respectively."
  Figure 7 Link: articels_figures_by_rev_year\2019\RotationNet_for_Joint_Object_Categorization_and_Unsupervised_Pose_Estimation_fro\figure_7.jpg
  Figure 7 caption: "Variance change of the average images generated by concatenating\
    \ multi-view images (case (ii)) in order of their predicted viewpoint variables.\
    \ Average images of 40 classes and images of the \u201Cchair\u201D class with\
    \ the same predicted viewpoint variable in iterations 0, 100, 200, and 500 are\
    \ shown in red boxes and blue boxes, respectively."
  Figure 8 Link: articels_figures_by_rev_year\2019\RotationNet_for_Joint_Object_Categorization_and_Unsupervised_Pose_Estimation_fro\figure_8.jpg
  Figure 8 caption: Exemplar multi-view images of a chair captured in case (ii) with
    11 different camera system orientations. Numbers in the left indicate the camera
    system orientation IDs.
  Figure 9 Link: articels_figures_by_rev_year\2019\RotationNet_for_Joint_Object_Categorization_and_Unsupervised_Pose_Estimation_fro\figure_9.jpg
  Figure 9 caption: Class separation S for different camera system orientations on
    ModelNet10.
  First author gender probability: 0.99
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Asako Kanezaki
  Name of the last author: Yoshifumi Nishida
  Number of Figures: 16
  Number of Tables: 9
  Number of authors: 3
  Paper title: RotationNet for Joint Object Categorization and Unsupervised Pose Estimation
    from Multi-View Images
  Publication Date: 2019-06-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Classification Accuracy (%) with Different Camera
      System Orientations.
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Accuracy of Classification and Viewpoint Estimation (%) in
      Case (i) with RGBD
  Table 3 caption:
    table_text: TABLE 3 Accuracy of Classification and Viewpoint Estimation (%) in
      Case (i) with MIRO
  Table 4 caption:
    table_text: TABLE 4 Accuracy of Classification and Viewpoint Estimation (%) in
      Case (ii) with MIRO
  Table 5 caption:
    table_text: TABLE 5 Comparison of Classification Accuracy (%)
  Table 6 caption:
    table_text: TABLE 6 Comparison of the Number of Parameters, Memory Usage (During
      Training with a Batch Size 64), and Classification Accuracy on ModelNet40
  Table 7 caption:
    table_text: TABLE 7 Comparison on Object InstanceCategory Recognition and Pose
      Estimation on RGBD Dataset
  Table 8 caption:
    table_text: TABLE 8 Evaluation Results on the Test Set in SHREC17 Track 1 (Quoted
      from [15])
  Table 9 caption:
    table_text: TABLE 9 Evaluation Results on the Test Set in SHREC17 Track 3 (Quoted
      from [30])
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2922640
- Affiliation of the first author: school of interactive computing, georgia institute
    of technology, atlanta, usa
  Affiliation of the last author: school of information and computer sciences, university
    of california, irvine, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Clouds_of_Oriented_Gradients_for_D_Detection_of_Objects_Surfaces_and_Indoor_Scen\figure_1.jpg
  Figure 1 caption: Given input RGB and Depth images (left), we align oriented cuboids
    and transform observed data into a canonical coordinate frame. For each voxel,
    we then extract (from left to right) point cloud density features, 3D normal orientation
    histograms, and COG descriptors of back-projected image gradient orientations.
    COG bins (left) are colored to show the alignment between instances. The value
    of the point cloud density feature is proportional to the voxel intensity, each
    3D orientation histogram bin is assigned a distinct color, and COG features are
    proportional to the normalized energy in each orientation bin, similarly to HOG
    descriptors [25].
  Figure 10 Link: articels_figures_by_rev_year\2019\Clouds_of_Oriented_Gradients_for_D_Detection_of_Objects_Surfaces_and_Indoor_Scen\figure_10.jpg
  Figure 10 caption: To model contextual relationships between small objects and the
    large objects supporting them, we compute the 2D areas and overlaps between 3D
    bounding boxes (left) seen from a top-down view (right).
  Figure 2 Link: articels_figures_by_rev_year\2019\Clouds_of_Oriented_Gradients_for_D_Detection_of_Objects_Surfaces_and_Indoor_Scen\figure_2.jpg
  Figure 2 caption: For two corresponding voxels (red and green) on two chairs, we
    illustrate the orientation histograms that would be computed by a standard HOG
    descriptor [25] in 2D image coordinates, and our COG descriptor in which perspective
    geometry is used to align descriptor bins. Even though these object instances
    are very similar, their 3D pose leads to wildly different HOG descriptors.
  Figure 3 Link: articels_figures_by_rev_year\2019\Clouds_of_Oriented_Gradients_for_D_Detection_of_Objects_Surfaces_and_Indoor_Scen\figure_3.jpg
  Figure 3 caption: A false positive 3D detection for the nightstand category that
    occurs without a view-to-camera feature (top). The COG feature is similar to that
    of a correct detection (bottom) due to bilateral symmetry.
  Figure 4 Link: articels_figures_by_rev_year\2019\Clouds_of_Oriented_Gradients_for_D_Detection_of_Objects_Surfaces_and_Indoor_Scen\figure_4.jpg
  Figure 4 caption: Visualizing the learned weights for the COG (left) and expanded
    COG (right) features. Although chairs and toilets have similar geometric structures,
    the appearance of the 3D environment immediately surrounding them is different,
    producing local contextual cues captured by our expanded COG features.
  Figure 5 Link: articels_figures_by_rev_year\2019\Clouds_of_Oriented_Gradients_for_D_Detection_of_Objects_Surfaces_and_Indoor_Scen\figure_5.jpg
  Figure 5 caption: "Different surface heights for instances of the \u201Cdesk\u201D\
    \ category in SUN the RGB-D dataset [16] lead to inconsistent 3D COG representations."
  Figure 6 Link: articels_figures_by_rev_year\2019\Clouds_of_Oriented_Gradients_for_D_Detection_of_Objects_Surfaces_and_Indoor_Scen\figure_6.jpg
  Figure 6 caption: Visualization of 3D detection of beds and pillows using latent
    support surfaces. Given input RGB-D images, we use our learned COG descriptor
    to localize 3D objects and infer latent support surfaces (shaded) for 3D proposals
    of beds (red). Then we search for pillows (green) that lie on top of the inferred
    support surfaces.
  Figure 7 Link: articels_figures_by_rev_year\2019\Clouds_of_Oriented_Gradients_for_D_Detection_of_Objects_Surfaces_and_Indoor_Scen\figure_7.jpg
  Figure 7 caption: COG features for 3D cuboids and support surfaces. The surface
    feature is computed within a single slice of the cuboid, and concatenated with
    an indicator vector encoding the relative height. Expanded cuboid features are
    not visualized.
  Figure 8 Link: articels_figures_by_rev_year\2019\Clouds_of_Oriented_Gradients_for_D_Detection_of_Objects_Surfaces_and_Indoor_Scen\figure_8.jpg
  Figure 8 caption: 'Models for the 3D layout geometry of indoor scenes. Top: Ground
    truth annotation. Bottom: Top-down view of the scene and two voxel-based quantizations.
    We compare a regular voxel grid (left) to our Manhattan voxels (right; dashed
    red line is the layout hypothesis).'
  Figure 9 Link: articels_figures_by_rev_year\2019\Clouds_of_Oriented_Gradients_for_D_Detection_of_Objects_Surfaces_and_Indoor_Scen\figure_9.jpg
  Figure 9 caption: 'Cascaded classifiers capture contextual relationships among objects.
    From left to right: (i) A traditional undirected MRF representation of contextual
    relationships. Colored nodes represent object categories, and black nodes represent
    the room layout. (ii) A directed graphical representation of cascaded classification,
    where the first-stage detectors are hidden variables (dashed) that model contextual
    relationships among object and layout hypotheses (solid). Marginalizing the hidden
    nodes recovers the undirected MRF. (iii) First-stage detections independently
    computed for each category as in Section 3.4. (iv) Second-stage detections (Section
    6) efficiently computed using our directed representation of context, and capturing
    contextual relationships between objects and the overall scene layout.'
  First author gender probability: 0.95
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Zhile Ren
  Name of the last author: Erik B. Sudderth
  Number of Figures: 15
  Number of Tables: 2
  Number of authors: 2
  Paper title: Clouds of Oriented Gradients for 3D Detection of Objects, Surfaces,
    and Indoor Scene Layouts
  Publication Date: 2019-06-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average Precision Scores for Five Object Categories (Bed,
      Bathtub, Nightstand, Chair, Toilet) Given Various Sets of 3D Cuboid Features
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Experimental Results on the SUN RGB-D Dataset [16]
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2923201
- Affiliation of the first author: department of computer science, technical university
    of munich, munich, germany
  Affiliation of the last author: department of computer science, technical university
    of munich, munich, germany
  Figure 1 Link: articels_figures_by_rev_year\2019\Photometric_Depth_SuperResolution\figure_1.jpg
  Figure 1 caption: "Photometric depth super-resolution of a low-resolution depth\
    \ map z 0 to the higher resolution of the companion image I (first column, Rucksack\
    \ and Face 1 datasets were acquired using an Intel Realsense D415, and Tabletcase\
    \ using an Asus Xtion Pro Live). Second column: shape-from-shading (SfS) recovers\
    \ high-resolution albedo ( \u03C1 ) and depth ( z ) from a single RGB-D frame,\
    \ assuming piecewise-constant albedo. If this assumption is not satisfied (e.g.,\
    \ Face 1 and Tabletcase), shape estimation deteriorates. Third column: this can\
    \ be circumvented by learning reflectance, an approach which is efficient as long\
    \ as the target resembles the training data (here, training was carried out on\
    \ human faces). Fourth column: uncalibrated photometric stereo (UPS) requires\
    \ no training and handles arbitrary albedo, but it requires n\u22654 input frames\
    \ acquired under varying illumination. See Section 6 in the supplementary material,\
    \ which can be found on the Computer Society Digital Library at http:doi.ieeecomputersociety.org10.1109TPAMI.2019.2923621,\
    \ for additional comparisons."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Photometric_Depth_SuperResolution\figure_2.jpg
  Figure 2 caption: "Geometric setup. Depth measurements z 0 are available over a\
    \ low-resolution set \u03A9 LR , and color measurements I over a high-resolution\
    \ set \u03A9 HR . Photometric depth super-resolution consists in estimating a\
    \ high-resolution depth map z out of these geometric and photometric measurements,\
    \ which are connected through the surface normals n z,\u2207z , see Equations\
    \ (1), (2), and (3)."
  Figure 3 Link: articels_figures_by_rev_year\2019\Photometric_Depth_SuperResolution\figure_3.jpg
  Figure 3 caption: Qualitative results obtained using the proposed single-shot approach
    on three real-world datasets captured with an Intel Realsense D415 camera. Even
    when intensity is very low (second row), or when under- or over-segmentation of
    reflectance happens (third row), the minimal surface prior prevents artefacts
    from arising while still allowing the recovery of thin geometric structures.
  Figure 4 Link: articels_figures_by_rev_year\2019\Photometric_Depth_SuperResolution\figure_4.jpg
  Figure 4 caption: Examples of human faces rendered under varying viewing and lighting
    conditions (top), along with the corresponding albedo maps (bottom).
  Figure 5 Link: articels_figures_by_rev_year\2019\Photometric_Depth_SuperResolution\figure_5.jpg
  Figure 5 caption: Results of the proposed variational approach to photometric depth
    super-resolution, using deep learning to estimate reflectance. Data was captured
    with an Intel Realsense D415 camera.
  Figure 6 Link: articels_figures_by_rev_year\2019\Photometric_Depth_SuperResolution\figure_6.jpg
  Figure 6 caption: "Qualitative results of our uncalibrated photometric stereo-based\
    \ method, on real-world data captured using a RealSense D415 (\u201CHat\u201D\
    \ and \u201CFace 2\u201D) or an Xtion Pro Live (five other datasets)."
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Bjoern Haefner
  Name of the last author: Daniel Cremers
  Number of Figures: 6
  Number of Tables: 0
  Number of authors: 5
  Paper title: Photometric Depth Super-Resolution
  Publication Date: 2019-06-18 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2923621
- Affiliation of the first author: college of computer science and software engineering,
    shenzhen university, shenzhen, china
  Affiliation of the last author: college of computer science and software engineering,
    shenzhen university, shenzhen, china
  Figure 1 Link: articels_figures_by_rev_year\2019\ZigZag_Network_for_Semantic_Segmentation_of_RGBD_Images\figure_1.jpg
  Figure 1 caption: 'Correlation between depth and scene-scale: the near field (highlighted
    in blue rectangle) consists of a large scene-scale, while the far field (highlighted
    in red rectangle) has a small scene-scale.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\ZigZag_Network_for_Semantic_Segmentation_of_RGBD_Images\figure_2.jpg
  Figure 2 caption: Overview of our network. Given a color image, we use CNN to compute
    the convolutional feature maps. These are passed to the zig-zag architectures,
    which gradually recover their resolutions. Each zig-zag architecture has multiple
    branches. The discrete depth image is layered, where each layer represents a scene-scale
    and is used to match the image regions to corresponding network branches. Each
    branch has the context-aware receptive field (CARF), which produces context feature
    map to combine with the feature from an adjacent branch. The predictions of all
    branches are merged to achieve the eventual segmentation result. Please see Fig.
    3 for details of the CARF.
  Figure 3 Link: articels_figures_by_rev_year\2019\ZigZag_Network_for_Semantic_Segmentation_of_RGBD_Images\figure_3.jpg
  Figure 3 caption: 'Two-stage weighting scheme of CARF: (a) image partitioned into
    super-pixels with different sizes; (b) each neuron of the convolutional feature
    map is augmented by local weighting, which uses the information of neurons residing
    in the same super-pixel; (c) after local weighting, the neurons residing in each
    super-pixel are augmented; (d) each neuron is further augmented by high-order
    weighting, which uses the content of adjacent super-pixels, to form the context
    feature map. The two-stage weighting is repeatedly applied to the images partitioned
    by super-pixels of diverse sizes. Note that the feature map has smaller resolution
    than the image due to down-sampling of the network.'
  Figure 4 Link: articels_figures_by_rev_year\2019\ZigZag_Network_for_Semantic_Segmentation_of_RGBD_Images\figure_4.jpg
  Figure 4 caption: Sensitivities to the number of branches (a) and the scale of super-pixels
    (b). Performances are evaluated on the NYUDv2 validation set. Segmentation accuracy
    is reported in terms of IoU (%).
  Figure 5 Link: articels_figures_by_rev_year\2019\ZigZag_Network_for_Semantic_Segmentation_of_RGBD_Images\figure_5.jpg
  Figure 5 caption: Sensitivity to the number of two-stage weighting layers. Performances
    are evaluated on the NYUDv2 validation set. Segmentation accuracy is reported
    in terms of IoU (%).
  Figure 6 Link: articels_figures_by_rev_year\2019\ZigZag_Network_for_Semantic_Segmentation_of_RGBD_Images\figure_6.jpg
  Figure 6 caption: Sample of the comparison to state-of-the-art DPCNet [46] and ours.
    Scene images are taken from the PASCAL VOC 2012 [1] (the first four rows) and
    Cityscapes [4] (the last four rows) validation sets.
  Figure 7 Link: articels_figures_by_rev_year\2019\ZigZag_Network_for_Semantic_Segmentation_of_RGBD_Images\figure_7.jpg
  Figure 7 caption: The network can have (a) separate branches, (b) combined branches,
    (c) cascaded branches or (d) zig-zag branches. In each sub-figure, we illustrate
    the multiple branches of the backbone architecture and omit the decoder with similar
    structure. For clarity, we illustrate it with two branches only. Each network
    can be extended to have more branches.
  Figure 8 Link: articels_figures_by_rev_year\2019\ZigZag_Network_for_Semantic_Segmentation_of_RGBD_Images\figure_8.jpg
  Figure 8 caption: Sample of the comparison to state-of-the-art models [17], [20]
    and ours. Scene images are taken from the NYUDv2 dataset [30].
  Figure 9 Link: articels_figures_by_rev_year\2019\ZigZag_Network_for_Semantic_Segmentation_of_RGBD_Images\figure_9.jpg
  Figure 9 caption: Sample of the comparison to state-of-the-art models [17], [20]
    and ours. Scene images are taken from the SUN-RGBD dataset [31].
  First author gender probability: 0.64
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.96
  Name of the first author: Di Lin
  Name of the last author: Hui Huang
  Number of Figures: 9
  Number of Tables: 9
  Number of authors: 2
  Paper title: Zig-Zag Network for Semantic Segmentation of RGB-D Images
  Publication Date: 2019-06-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Ablation Experiments of Using Local and High-Order Weighting
      Schemes for Computing CARFs
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Strategies of Using the CARF, Evaluated on the NYUDv2 Validation
      Set
  Table 3 caption:
    table_text: TABLE 3 Improvement with CARF
  Table 4 caption:
    table_text: TABLE 4 Different Strategies of Propagating Context Information
  Table 5 caption:
    table_text: TABLE 5 Different Multi-Branch Networks, Evaluated on the NYUDv2 Validation
      Set
  Table 6 caption:
    table_text: TABLE 6 Comparisons with Other State-of-the-Art Methods on the NYUDv2
      Test Set
  Table 7 caption:
    table_text: TABLE 7 Class-Wise Semantic Segmentation Accuracy on the NYUDv2 Test
      Set
  Table 8 caption:
    table_text: TABLE 8 Comparisons with Other State-of-the-Art Methods on the SUN-RGBD
      Test Set
  Table 9 caption:
    table_text: TABLE 9 Class-Wise Semantic Segmentation Accuracy on the SUN-RGBD
      Test Set
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2923513
- Affiliation of the first author: department of computer science, hong kong baptist
    university, kowloon tong, hong kong
  Affiliation of the last author: department of computer science, institute of research
    and continuing education, hong kong baptist university, kowloon tong, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2019\Bayesian_LowTubalRank_Robust_Tensor_Factorization_with_MultiRank_Determination\figure_1.jpg
  Figure 1 caption: Graphical illustration of the BTRTF model.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Bayesian_LowTubalRank_Robust_Tensor_Factorization_with_MultiRank_Determination\figure_2.jpg
  Figure 2 caption: "Recovery results on the BSD500 dataset. (a) Original image; (b)\
    \ Corrupted image; (c)-(i) Recovered images by different robust PCA methods; (j)\
    \ Comparison of PSNR values on the above 8 images. Best viewed in \xD74 sized\
    \ color pdf file."
  Figure 3 Link: articels_figures_by_rev_year\2019\Bayesian_LowTubalRank_Robust_Tensor_Factorization_with_MultiRank_Determination\figure_3.jpg
  Figure 3 caption: Detected background and foreground masks on five videos from the
    I2R and CDnet datasets. (a) Curtain, (b) ShoppingMall, (c) WaterSurface, (d) Boats,
    (e) Fall. For each video, there are two rows corresponding to background and foreground
    masks. Blue and red regions in the learned masks indicate false positives and
    false negatives, respectively.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.71
  Name of the first author: Yang Zhou
  Name of the last author: Yiu-Ming Cheung
  Number of Figures: 3
  Number of Tables: 4
  Number of authors: 2
  Paper title: Bayesian Low-Tubal-Rank Robust Tensor Factorization with Multi-Rank
    Determination
  Publication Date: 2019-06-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Convention of Notations
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Recovery Results of BTRTF on the Synthetic Datasets
  Table 3 caption:
    table_text: "TABLE 3 Rank Determination Results on the Synthetic Datasets with\
      \ \u03C1=0% \u03C1=0%"
  Table 4 caption:
    table_text: TABLE 4 Summary of Precision, Recall, and F-Measure on the I2R and
      CDnet Datasets (Best; Second Best)
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2923240
- Affiliation of the first author: bnrist, department of computer science and technology,
    moe-key laboratory of pervasive computing, tsinghua university, beijing, china
  Affiliation of the last author: school of computer science and informatics, cardiff
    university, cardiff, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2019\RankingPreserving_CrossSource_Learning_for_Image_Retargeting_Quality_Assessment\figure_1.jpg
  Figure 1 caption: Subjective scores are only comparable for retargeting results
    of the same source image. In each row, two retargeting results are presented and
    their scores are shown in parentheses (the first numbers). These subjective scores
    provided in the RetargetMe benchmark [18] are numbers of votes that people cast
    when comparing this image against other images with the same source image. Higher
    scores mean better results. Although the scores of the two retargeting results
    3 and 4 are higher than the scores of results 1 and 2, we cannot conclude that
    the results 3 and 4 are better than the results 1 and 2; instead, the opposite
    appears to be true. The second numbers in parentheses are objective scores output
    from the method proposed in this paper. The scores not only preserve the ranking
    of retargeted images with the same source image, but also provide a reference
    to compare retargeted images from different sources. As a comparison, the third
    numbers in parentheses are objective scores predicted by [3], which cannot compare
    retargeted images from different sources.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\RankingPreserving_CrossSource_Learning_for_Image_Retargeting_Quality_Assessment\figure_2.jpg
  Figure 2 caption: 'Two image groups in the RetargetMe benchmark [18]: Each group
    has a source image and eight retargeted images. For each retargeted image, the
    numbers in parentheses are its subjective score (red), normalized subjective score
    (black) and the objective score computed by our method (blue). For each group,
    the difference between normalized subjective score and the objective score is
    a constant, and therefore, the objective scores predicted by our method preserve
    the ranking of subjective scores. Subjective scores are only comparable for retargeting
    results of the same source image. For example, although the subjective score of
    result 1-2 is higher than the subjective score of result 2-2, result 2-2 appears
    to be better than result 1-2. The objective scores computed by our method reveal
    this fact.'
  Figure 3 Link: articels_figures_by_rev_year\2019\RankingPreserving_CrossSource_Learning_for_Image_Retargeting_Quality_Assessment\figure_3.jpg
  Figure 3 caption: Three check point pairs. In (b) and (c), retargeted images on
    the left are obviously better than those on the right. In (d), the retargeted
    image on the right is obviously better than the one on the left.
  Figure 4 Link: articels_figures_by_rev_year\2019\RankingPreserving_CrossSource_Learning_for_Image_Retargeting_Quality_Assessment\figure_4.jpg
  Figure 4 caption: Two examples not in RetargetMe are illustrated, each of which
    has two retargeting results. Result 4 is obviously better than Result 2 because
    human observers are more sensitive to the change of human subjects. The scores
    predicted by our method provide a correct reference ( 0.87>0.65 ), while the scores
    predicted by L2Rank are in the wrong order due to cross source ( 0.0011<0.0023
    ).
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.55
  Name of the first author: Yong-Jin Liu
  Name of the last author: Yu-Kun Lai
  Number of Figures: 4
  Number of Tables: 2
  Number of authors: 4
  Paper title: Ranking-Preserving Cross-Source Learning for Image Retargeting Quality
    Assessment
  Publication Date: 2019-06-20 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Mean Kendall Correlation Coefficients of 37 Groups of
      Images in RetargetMe
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Mean Kendall Correlation Coefficients of 26 Groups of
      Images in Our Novel Dataset
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2923998
- Affiliation of the first author: rj research consulting, tarrytown, ny, usa
  Affiliation of the last author: hkust, clear water bay, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2019\A_Framework_of_Composite_Functional_Gradient_Methods_for_Generative_Adversarial_\figure_1.jpg
  Figure 1 caption: "Generator network automatically created by CFG or ICFG with T=3\
    \ . \u2295 indicates addition."
  Figure 10 Link: articels_figures_by_rev_year\2019\A_Framework_of_Composite_Functional_Gradient_Methods_for_Generative_Adversarial_\figure_10.jpg
  Figure 10 caption: 'Scores ( x -axis) and distances ( y -axis). 3 runs (with 3 random
    seeds) per method. Original GANs: GAN0 and GAN1. Improved GANs: GAN-sn, GAN-gp
    of two types, and WGANgp.'
  Figure 2 Link: articels_figures_by_rev_year\2019\A_Framework_of_Composite_Functional_Gradient_Methods_for_Generative_Adversarial_\figure_2.jpg
  Figure 2 caption: Image quality measured by the inception score in relation to training
    time. Convolutional networks. The legends are roughly sorted from the best to
    the worst.
  Figure 3 Link: articels_figures_by_rev_year\2019\A_Framework_of_Composite_Functional_Gradient_Methods_for_Generative_Adversarial_\figure_3.jpg
  Figure 3 caption: Image quality measured by the inception score in relation to training
    time. Convolutional networks without batch normalization anywhere. The legends
    are roughly sorted from the best to the worst.
  Figure 4 Link: articels_figures_by_rev_year\2019\A_Framework_of_Composite_Functional_Gradient_Methods_for_Generative_Adversarial_\figure_4.jpg
  Figure 4 caption: Image quality measured by the inception score in relation to training
    time. Fully-connected approximatorsgenerators.
  Figure 5 Link: articels_figures_by_rev_year\2019\A_Framework_of_Composite_Functional_Gradient_Methods_for_Generative_Adversarial_\figure_5.jpg
  Figure 5 caption: "Fr\xE9chet distance in relation to training time (left) and the\
    \ inception score (right) on the runs in Fig. 2. The arrows in the right graphs\
    \ show the direction of time flow. On both LSUN BR+LR (up) and T+B (down), GAN0\
    \ and GAN1 suffer from mode collapse or lack of diversity; their inception score\
    \ fluctuates (Fig. 2) and their Fr\xE9chet distance stays relatively high. xICFG\
    \ (and WGANgp) shows no sign of mode collapse (Figs. 13 and 14) and performs well\
    \ in both metrics."
  Figure 6 Link: articels_figures_by_rev_year\2019\A_Framework_of_Composite_Functional_Gradient_Methods_for_Generative_Adversarial_\figure_6.jpg
  Figure 6 caption: "\u201CRealistic\u201D and \u201Ccreative\u201D images generated\
    \ by xICFG. (a) Real Golden Gate Bridge images in the training set. (b) Images\
    \ generated by xICFG that look like Golden Gate Bridge though not perfect. (c)\
    \ Images generated by xICFG that look like modifications of Golden Gate Bridge."
  Figure 7 Link: articels_figures_by_rev_year\2019\A_Framework_of_Composite_Functional_Gradient_Methods_for_Generative_Adversarial_\figure_7.jpg
  Figure 7 caption: xICFG. LSUN BR+LR. 4-block ResNets. The setting almost equivalent
    to GANs performs poorly, similar to GANs. The performance improves as we improve
    the setting of T and the approximator update.
  Figure 8 Link: articels_figures_by_rev_year\2019\A_Framework_of_Composite_Functional_Gradient_Methods_for_Generative_Adversarial_\figure_8.jpg
  Figure 8 caption: "Relations between image quality and |D(real)\u2212D(gen)| (=\
    \ \u0394 D ). The arrows indicate the direction of time flow. A correlation is\
    \ observed both when training is succeeding (blue solid arrows) and failing (red\
    \ dotted arrows)."
  Figure 9 Link: articels_figures_by_rev_year\2019\A_Framework_of_Composite_Functional_Gradient_Methods_for_Generative_Adversarial_\figure_9.jpg
  Figure 9 caption: Larger values for U and N on MNIST. Convolutional.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: Rie Johnson
  Name of the last author: Tong Zhang
  Number of Figures: 15
  Number of Tables: 3
  Number of authors: 2
  Paper title: A Framework of Composite Functional Gradient Methods for Generative
    Adversarial Models
  Publication Date: 2019-06-24 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Examples of f f-Divergences: KL Divergence, Reverse KL Divergence,\
      \ JS Divergence \xD72, and Squared Hellinger Distance"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Meta-Parameters for xICFG
  Table 3 caption:
    table_text: "TABLE 3 Fr\xE9chet Distance Results"
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2924428
