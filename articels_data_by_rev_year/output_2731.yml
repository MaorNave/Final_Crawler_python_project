- Affiliation of the first author: cas key laboratory of technology in geo-spatial
    information processing and application system, university of science and technology
    of china, hefei, anhui, china
  Affiliation of the last author: cas key laboratory of technology in geo-spatial
    information processing and application system, university of science and technology
    of china, hefei, anhui, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Rectified_Wasserstein_Generative_Adversarial_Networks_for_Perceptual_Image_Resto\figure_1.jpg
  Figure 1 caption: "Original high-resolution image of Baboon (in Set14) and its 4\xD7\
    \ super-resolution results of different methods: the vanilla WGAN [7], WGAN-GP\
    \ [8], and our ReWaGAN. The three WGANs including ours have used the same network\
    \ structure for both generator and critic to ensure fair comparison. The low-resolution\
    \ image is not shown here."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Rectified_Wasserstein_Generative_Adversarial_Networks_for_Perceptual_Image_Resto\figure_2.jpg
  Figure 2 caption: "The ratio of the gradients in ReWaGAN, \u2207 \u03B8 ReWa \u02DC\
    \ (16), over the gradients in WGAN, \u2207 \u03B8 Wa \u02DC (7), with respect\
    \ to the value of f(m( g \u03B8 (z)))\u2212f( g \u03B8 (z)) . This figure is meant\
    \ to give the intuition for the naming \u201Crectified.\u201D"
  Figure 3 Link: articels_figures_by_rev_year\2022\Rectified_Wasserstein_Generative_Adversarial_Networks_for_Perceptual_Image_Resto\figure_3.jpg
  Figure 3 caption: "The used multi-scale critic in our experiments. The full critic\
    \ f() is the concatenation of four sub-critics, f 0 () , f 1 () , f 2 () , and\
    \ f 3 () . Note that the four sub-critics do not share network parameters. \u201C\
    \xD71(2, 3)\u201D stands for stacking 1(2, 3) blocks."
  Figure 4 Link: articels_figures_by_rev_year\2022\Rectified_Wasserstein_Generative_Adversarial_Networks_for_Perceptual_Image_Resto\figure_4.jpg
  Figure 4 caption: "Original high-resolution (HR) image and 4\xD7 SR results of different\
    \ methods: SRFlow [48], RankSRGAN [32], ESRGAN [14], ESRGAN+ [47], and our ReWaGAN."
  Figure 5 Link: articels_figures_by_rev_year\2022\Rectified_Wasserstein_Generative_Adversarial_Networks_for_Perceptual_Image_Resto\figure_5.jpg
  Figure 5 caption: "Original high-resolution image and 8\xD7 SR results of different\
    \ methods: SRFlow [48], ESRGAN [14], and our ReWaGAN. The original images come\
    \ from the DIV2K validation set."
  Figure 6 Link: articels_figures_by_rev_year\2022\Rectified_Wasserstein_Generative_Adversarial_Networks_for_Perceptual_Image_Resto\figure_6.jpg
  Figure 6 caption: "The percentage of the constraint f(m(x))>f(x) being satisfied\
    \ on the entire training set in the experiments of ReWaGAN for 4\xD7 image SR.\
    \ The four sub-critics ( f 0 , f 1 , f 2 , and f 3 ) are separately counted and\
    \ plotted."
  Figure 7 Link: articels_figures_by_rev_year\2022\Rectified_Wasserstein_Generative_Adversarial_Networks_for_Perceptual_Image_Resto\figure_7.jpg
  Figure 7 caption: "The effect of \u03BB in (26) on the visual quality of SR results."
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Haichuan Ma
  Name of the last author: Feng Wu
  Number of Figures: 7
  Number of Tables: 8
  Number of authors: 3
  Paper title: Rectified Wasserstein Generative Adversarial Networks for Perceptual
    Image Restoration
  Publication Date: 2022-06-22 00:00:00
  Table 1 caption: "TABLE 1 The Calculated Wasserstein Distances (See Section 5.2.1)\
    \ of Different 4\xD7 SR Methods on the DIV2K Validation Set"
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 Quantitative Results of 4\xD7 4\xD7 SR Using Different\
    \ Methods"
  Table 3 caption: "TABLE 3 Quantitative Results of 8\xD7 SR on the DIV2K Validation\
    \ Set Using Different Methods: RRDB [14], ESRGAN [14], SRFlow [48], and Our ReWaGAN"
  Table 4 caption: "TABLE 4 Favorable Ratio of Our ReWaGAN than Each Compared Method\
    \ for 4\xD7 SR in the Conducted Subjective Evaluation (See Section 5.3)"
  Table 5 caption: 'TABLE 5 Quantitative Results of Compression Artifact Reduction
    (CAR) Using Different Methods: Trained with MSE, ESRGAN-Like [42], and Our ReWaGAN'
  Table 6 caption: "TABLE 6 The Effect of \u039B \u039B in (26) on the FID and the\
    \ Calculated Wasserstein Distance (See Section 5.2.1) on the DIV2K Validation\
    \ Set"
  Table 7 caption: "TABLE 7 Quantitative Results of 4\xD7 SR on the DIV2K Validation\
    \ Set"
  Table 8 caption: "TABLE 8 Quantitative Results of 4\xD7 SR on Different Test Sets"
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3185316
- Affiliation of the first author: school of computer science and technology, university
    of chinese academy of sciences, beijing, china
  Affiliation of the last author: school of computer science and technology, university
    of chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Optimizing_TwoWay_Partial_AUC_With_an_EndtoEnd_Framework\figure_1.jpg
  Figure 1 caption: 'Comparisons of different AUC variants: (a) The entire area of
    ROC curve; (b) The One-way Partial AUC (OPAUC) which measures the area of a local
    region of ROC within an FPR range; (c) The Two-way Partial AUC (TPAUC).'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Optimizing_TwoWay_Partial_AUC_With_an_EndtoEnd_Framework\figure_2.jpg
  Figure 2 caption: 'TPR-dependent OPAUC versus TPAUC: Practically, OPAUC and TPAUC
    tend to exhibit different comparison results when two ROC curves intersect. For
    the two ROC curves in the figure, it is easy to see that TPAU C 1 = S 6 + S 7
    and TPAU C 2 = S 5 + S 7 . By calculating their tightest OPAUC upper bounds, we
    have OPAU C 1 = S 6 + S 7 + S 9 and OPAU C 2 = S 5 + S 7 + S 8 + S 9 . In this
    case, we have OPAU C 1 <OPAU C 2 but TPAU C 2 <TPAU C 1 whenever S 6 < S 5 + S
    8 and S 6 > S 5 . Moreover, RO C 1 tends to have better higher TPR than RO C 2
    in the most points within the restricted ROC region. TPAUC is more consistent
    with a better performance curve than TPR-dependent OPAUC.'
  Figure 3 Link: articels_figures_by_rev_year\2022\Optimizing_TwoWay_Partial_AUC_With_an_EndtoEnd_Framework\figure_3.jpg
  Figure 3 caption: Convex versus concave weighting functions. It is easy to achieve
    the upper bound of the original hard threshold weighting when the area of S 1
    is large and the area of S 2 is small. In this sense, concave functions are intuitively
    more suitable choices for the weighting functions.
  Figure 4 Link: articels_figures_by_rev_year\2022\Optimizing_TwoWay_Partial_AUC_With_an_EndtoEnd_Framework\figure_4.jpg
  Figure 4 caption: "Practical Validation of Proposition 1. (a) We construct a distribution\
    \ that f \u03B8 ( x + )\u223CN(0.5,0.08) , f \u03B8 ( x \u2212 )\u223CN(0.3,0.08)\
    \ , and sample 100 points for each class, and repeat it for 50 trials. For all\
    \ these trials, we can find p,q that satisfies the ineq. in Proposition 2 (a).\
    \ Thus the sufficient condition holds for these trials. (b) We plot the training\
    \ curve of the relaxed surrogate empirical risk R \u03B1,\u03B2 \u03C8 , the surrogate\
    \ empirical risk R \u03B1,\u03B2 \u2113 , and the 0\u22121 empirical risk R \u03B1\
    ,\u03B2 0\u22121 (1-TPAUC) for CIFAR-10-LT-Subset-1, with a concave polynomial\
    \ weighting function. As shown in the figure, the relaxed risk function R \u03B1\
    ,\u03B2 \u03C8 is always an upper bound of the other two risk functions."
  Figure 5 Link: articels_figures_by_rev_year\2022\Optimizing_TwoWay_Partial_AUC_With_an_EndtoEnd_Framework\figure_5.jpg
  Figure 5 caption: "Visualization of the Landscape of the pairwise weights \u03C8\
    \ \u03B3 (x)\u22C5 \u03C8 \u03B3 (y) . Here, (a) and (b) plot \u03C8 poly \u03B3\
    \ , while (c) and (d) plot \u03C8 exp \u03B3 ."
  Figure 6 Link: articels_figures_by_rev_year\2022\Optimizing_TwoWay_Partial_AUC_With_an_EndtoEnd_Framework\figure_6.jpg
  Figure 6 caption: "Empirical validations for Proposition 2 (a). R \u03B1,\u03B2\
    \ \u03A6 , R \u03B1,\u03B2 \u2113 , R \u03B1,\u03B2 0\u22121 in the training process\
    \ withPoly weighiting are plotted in each subfigure. Our loss R \u03B1,\u03B2\
    \ \u03A6 is always an upper bound of the original loss."
  Figure 7 Link: articels_figures_by_rev_year\2022\Optimizing_TwoWay_Partial_AUC_With_an_EndtoEnd_Framework\figure_7.jpg
  Figure 7 caption: Sensitivity analysis of Exp on gamma . Experiments are conducted
    on CIFAR-10-LT. For each box, gamma is fixed as the y-axis value, and the scattered
    points along the box show the variation of gamma .
  Figure 8 Link: articels_figures_by_rev_year\2022\Optimizing_TwoWay_Partial_AUC_With_an_EndtoEnd_Framework\figure_8.jpg
  Figure 8 caption: Sensitivity analysis of Poly on gamma . Experiments are conducted
    on CIFAR-10-LT. For each box, (gamma -1)-1 is fixed as the y-axis value, and the
    scattered points along the box show the variation of gamma .
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.89
  Name of the first author: Zhiyong Yang
  Name of the last author: Qingming Huang
  Number of Figures: 8
  Number of Tables: 4
  Number of authors: 6
  Paper title: Optimizing Two-Way Partial AUC With an End-to-End Framework
  Publication Date: 2022-06-22 00:00:00
  Table 1 caption: TABLE I Data Description
  Table 10 caption: Not Available
  Table 2 caption: TABLE II Performance Comparisons on CIFAR-10-LT With Different
    Metrics, Where (x,y) (x,y) Stands for TPAUC(x,y) TPAUC(x,y) in Short and the First
    and Second Best Results are Highlighted With Bold Text and underline, Respectively
  Table 3 caption: TABLE III Performance Comparisons on CIFAR-100-LT With Different
    Metrics, Where (x,y) (x,y) Stands for TPAUC(x,y) TPAUC(x,y) in Short and the First
    and Second Best Results are Highlighted With Bold Text and underline, Respectively
  Table 4 caption: TABLE IV Performance Comparisons on Tiny-ImageNet-200-LT With Different
    Metrics, Where (x,y) (x,y) Stands for TPAUC(x,y) TPAUC(x,y) in Short and the First
    and Second Best Results are Highlighted With Bold Text and underline, Respectively
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3185311
- Affiliation of the first author: national laboratory of pattern recognition, chinese
    academy of sciences, institution of automation, beijing, china
  Affiliation of the last author: department of computer science and information systems,
    birkbeck college, university of london, london, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2022\Learning_to_Explore_Distillability_and_Sparsability_A_Joint_Framework_for_Model_\figure_1.jpg
  Figure 1 caption: "Distillability and sparsability of a DNN. (a) Illustration of\
    \ distillability. The curves plot the cosine similarity between the KD loss and\
    \ the ground truth loss at different blocks, as the training epoch changes. Note\
    \ that \u201COur FC layer\u201D represents the results of the DNN trained by our\
    \ method. (b) Illustration of sparsability. The bar and line graph depict the\
    \ pruned ratios that the model can achieve with the same accuracy degradation\
    \ (i.e., 0.3%). Each bar represents the compressed model in which only the corresponding\
    \ layer is pruned. Note that the experiment is conducted on CIFAR10 using ResNet20."
  Figure 10 Link: articels_figures_by_rev_year\2022\Learning_to_Explore_Distillability_and_Sparsability_A_Joint_Framework_for_Model_\figure_10.jpg
  Figure 10 caption: Accuracy curves at different pruned ratios. The proposed method
    works best among different compression methods.
  Figure 2 Link: articels_figures_by_rev_year\2022\Learning_to_Explore_Distillability_and_Sparsability_A_Joint_Framework_for_Model_\figure_2.jpg
  Figure 2 caption: "The accuracy curves of the student network ResNet20, which is\
    \ trained under the guidance of teacher networks with different capacities. In\
    \ (a), the teachers vary in width. For example, \u201Cx1.0\u201D denotes the channel\
    \ number of this teacher is 1.0 time of ResNet20. In (b), the teachers vary in\
    \ depth, including ResNet8, ResNet14, ResNet20, ResNet32, ResNet44, ResNet56 and\
    \ ResNet110. Note that the orange dotted lines represent the trendline."
  Figure 3 Link: articels_figures_by_rev_year\2022\Learning_to_Explore_Distillability_and_Sparsability_A_Joint_Framework_for_Model_\figure_3.jpg
  Figure 3 caption: Pruned ratios of different models which have similar accuracy
    degradation and initial model sizes. The models are MobileNetV2, MobileNetV2-x0.35
    [43], MobileNetV3-Large, MobileNetV3-Small [44], MnasNet-A1, Mnas-Small [45] and
    Proxyless [46].
  Figure 4 Link: articels_figures_by_rev_year\2022\Learning_to_Explore_Distillability_and_Sparsability_A_Joint_Framework_for_Model_\figure_4.jpg
  Figure 4 caption: Cosine similarity between different losses. (a) Cosine similarity
    between knowledge distillation loss and ground truth loss. (b) Cosine similarity
    between sparsity loss and ground truth loss. Note that the student network in
    (a) and the initial network in (b) are both ResNet20.
  Figure 5 Link: articels_figures_by_rev_year\2022\Learning_to_Explore_Distillability_and_Sparsability_A_Joint_Framework_for_Model_\figure_5.jpg
  Figure 5 caption: "Test accuracy comparisons at different situations. (a) The test\
    \ accuracy improvement of ResNet20 on CIFAR10 and ResNet18 on ImageNet. ResNet20\
    \ and ResNet18 are distilled from different teacher networks. Note that \u201C\
    ResNet-KD-Full\u201D represents the models that use KD during the whole training\
    \ procedure, while \u201CResNet-KD-Partial\u201D represents the models that remove\
    \ KD at the end of training (i.e., after the 150-th epoch). (b) The test accuracy\
    \ of different pruned networks using different sparse regularizer coefficients.\
    \ A larger sparse regularizer coefficient corresponds to a higher sparsity ratio\
    \ in the pruned model. And \u201Cbaseline\u201D means the initial network trained\
    \ without sparsity constraint. Note that the student network in (a) and the initial\
    \ network in (b) are both ResNet20."
  Figure 6 Link: articels_figures_by_rev_year\2022\Learning_to_Explore_Distillability_and_Sparsability_A_Joint_Framework_for_Model_\figure_6.jpg
  Figure 6 caption: Radar chart of the student accuracy, at different coefficients
    of KD regularizer (i.e., 0.02, 0.04, 0.05, 0.06, 0.1, 0.5). Note that these full
    lines with different colors represent different teacher networks (i.e.ResNet8,
    ResNet14, ResNet 20, ResNet32). The student network is ResNet20.
  Figure 7 Link: articels_figures_by_rev_year\2022\Learning_to_Explore_Distillability_and_Sparsability_A_Joint_Framework_for_Model_\figure_7.jpg
  Figure 7 caption: "Test accuracy for different models training with different sequences\
    \ of supervision intensities. (a) training of ResNet20 on CIFAR10, utilizing KD\
    \ supervision. The teacher is ResNet20-2x. (b) Training of ResNet20 on CIFAR10\
    \ with sparse supervision. The pruned ratios of these models are all 50%. Note\
    \ that \u201CFull\u201D denotes that the supervision coefficient is static."
  Figure 8 Link: articels_figures_by_rev_year\2022\Learning_to_Explore_Distillability_and_Sparsability_A_Joint_Framework_for_Model_\figure_8.jpg
  Figure 8 caption: Test accuracy improvements of pruned models retrained with KD,
    at different parameter pruned ratios. Note that the initial network is ResNet20,
    which is regarded as the teacher in KD. The student networks are the pruned networks
    at different pruned ratios.
  Figure 9 Link: articels_figures_by_rev_year\2022\Learning_to_Explore_Distillability_and_Sparsability_A_Joint_Framework_for_Model_\figure_9.jpg
  Figure 9 caption: The overall framework of the proposed method, which contains teacher,
    student and dean.
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yufan Liu
  Name of the last author: Stephen Maybank
  Number of Figures: 15
  Number of Tables: 4
  Number of authors: 5
  Paper title: 'Learning to Explore Distillability and Sparsability: A Joint Framework
    for Model Compression'
  Publication Date: 2022-06-22 00:00:00
  Table 1 caption: TABLE 1 The Results of Pruning ResNet56 and ResNet110 on CIFAR-10
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The Results of Pruning ResNets and MobileNet V2 on ImageNet
  Table 3 caption: TABLE 3 Performance of Different Components in Our Method
  Table 4 caption: TABLE 4 Evaluation on other Visual Tasks, Including Object Detection,
    Instance Segmentation and Liveness Detection
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3185317
- Affiliation of the first author: "department of informatics, technical university\
    \ of munich, m\xFCnchen, germany"
  Affiliation of the last author: "department of informatics, technical university\
    \ of munich, m\xFCnchen, germany"
  Figure 1 Link: articels_figures_by_rev_year\2022\MetaReinforcement_Learning_in_NonStationary_and_Dynamic_Environments\figure_1.jpg
  Figure 1 caption: 'Components of our algorithm: The encoder learns task encoding
    for recent context from gradients (orange) of the decoder and provides it as input
    for SAC.'
  Figure 10 Link: articels_figures_by_rev_year\2022\MetaReinforcement_Learning_in_NonStationary_and_Dynamic_Environments\figure_10.jpg
  Figure 10 caption: 'Non-stationary, parametric environments: Responses of the agent
    to MDP changes.'
  Figure 2 Link: articels_figures_by_rev_year\2022\MetaReinforcement_Learning_in_NonStationary_and_Dynamic_Environments\figure_2.jpg
  Figure 2 caption: "Encoder: Based on the context of a transition c T t n the class\
    \ encoder q(y\u2223x) computes a probabilistic class encoding y , component-specific\
    \ Gaussian encoders q(z\u2223x,y=k) generate a probabilistic encoding z ."
  Figure 3 Link: articels_figures_by_rev_year\2022\MetaReinforcement_Learning_in_NonStationary_and_Dynamic_Environments\figure_3.jpg
  Figure 3 caption: 'Decoder: Based on the state s t , the action a t , and the task
    encoding z t produced by the encoder from the recent context, the decoder predicts
    the state transition and rewards for the timestep t .'
  Figure 4 Link: articels_figures_by_rev_year\2022\MetaReinforcement_Learning_in_NonStationary_and_Dynamic_Environments\figure_4.jpg
  Figure 4 caption: MuJoCo agents from Gym [18] used in experiments.
  Figure 5 Link: articels_figures_by_rev_year\2022\MetaReinforcement_Learning_in_NonStationary_and_Dynamic_Environments\figure_5.jpg
  Figure 5 caption: 'Stationary, parametric environments: meta-test performance over
    collected data during meta-training: CEMRL is more sample-efficient and stable
    than PEARL on cheetah-stationary-dir (5a) and outperforms PEARL in sample efficiency
    and asymptotic performance on cheetah-stationary-vel (5b). Note that the x -axis
    is in log scale.'
  Figure 6 Link: articels_figures_by_rev_year\2022\MetaReinforcement_Learning_in_NonStationary_and_Dynamic_Environments\figure_6.jpg
  Figure 6 caption: Task encodings for different training epochs on cheetah-stationary-vel.
  Figure 7 Link: articels_figures_by_rev_year\2022\MetaReinforcement_Learning_in_NonStationary_and_Dynamic_Environments\figure_7.jpg
  Figure 7 caption: 'Half-Cheetah with random, stationary goal velocity: Time responses
    for selected goal velocities from the task distribution with a well-trained policy.'
  Figure 8 Link: articels_figures_by_rev_year\2022\MetaReinforcement_Learning_in_NonStationary_and_Dynamic_Environments\figure_8.jpg
  Figure 8 caption: 'Non-stationary, parametric environments: meta-test performance
    over collected data during meta-training.'
  Figure 9 Link: articels_figures_by_rev_year\2022\MetaReinforcement_Learning_in_NonStationary_and_Dynamic_Environments\figure_9.jpg
  Figure 9 caption: 'Environment: cheetah-non-stationary-vel, encodings for different
    training epochs.'
  First author gender probability: 0.96
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: Zhenshan Bing
  Name of the last author: Alois Knoll
  Number of Figures: 16
  Number of Tables: 0
  Number of authors: 4
  Paper title: Meta-Reinforcement Learning in Non-Stationary and Dynamic Environments
  Publication Date: 2022-06-23 00:00:00
  Table 1 caption: Not Available
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3185549
- Affiliation of the first author: school of computer science and engineering, nanyang
    technological university, singapore
  Affiliation of the last author: school of artificial intelligent, sun yat-sen university,
    guangzhou, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Intrinsic_and_Isotropic_Resampling_for_D_Point_Clouds\figure_1.jpg
  Figure 1 caption: The pipeline of the proposed point cloud resampling method.
  Figure 10 Link: articels_figures_by_rev_year\2022\Intrinsic_and_Isotropic_Resampling_for_D_Point_Clouds\figure_10.jpg
  Figure 10 caption: Some instances of adaptively-isotropic resampling for curvature
    sensitive keeping. A:( t=5,lq in lbrace text2.0~l,1.5~l,l,0.8~l,0.6lrbrace );
    B:( t=5,lq in lbrace text1.5~l,1.3~l,l,0.9~l,0.8lrbrace ); C:( t=5,lq in lbrace
    text1.2~l,1.1~l,l,0.95~l,0.9lrbrace ); D:( t=3,lq in lbrace text1.5~l,l,0.6lrbrace
    ); E:( t=3,lq in lbrace text1.2~l,l,0.8lrbrace ).
  Figure 2 Link: articels_figures_by_rev_year\2022\Intrinsic_and_Isotropic_Resampling_for_D_Point_Clouds\figure_2.jpg
  Figure 2 caption: An instance of neighbor searching error of outlier. The red point
    is the outlier, blue points represent neighbor points of the outlier by k-nearest
    searching. It is clear that the blue points can not be used to represent a continuous
    local region and the outlier can not be mapped on the MLS surface.
  Figure 3 Link: articels_figures_by_rev_year\2022\Intrinsic_and_Isotropic_Resampling_for_D_Point_Clouds\figure_3.jpg
  Figure 3 caption: An instance of denoising result. The red point cloud is the input
    point cloud with noise; the green point cloud is the denoising result.
  Figure 4 Link: articels_figures_by_rev_year\2022\Intrinsic_and_Isotropic_Resampling_for_D_Point_Clouds\figure_4.jpg
  Figure 4 caption: An instance of density adjustment. The red point cloud is the
    input point cloud with non-uniform density. The green point cloud is processed
    by octree with uniform density.
  Figure 5 Link: articels_figures_by_rev_year\2022\Intrinsic_and_Isotropic_Resampling_for_D_Point_Clouds\figure_5.jpg
  Figure 5 caption: An instance of the geodesic coordinate system construction. Three
    pictures in first row represents the satellite detection. Two pictures in second
    row represents the mapping from a point cloud in euclidean space into the geodesic
    coordinate system.
  Figure 6 Link: articels_figures_by_rev_year\2022\Intrinsic_and_Isotropic_Resampling_for_D_Point_Clouds\figure_6.jpg
  Figure 6 caption: An instance of local region detection with efficient intrinsic
    control. The geodesic coordinate system from a 2D curve can be regarded as a 1D
    distance field. It provides the efficient intrinsic control to detect the neighbor
    points (blue points) for the red point. It is clear that the efficient intrinsic
    control is useful for accurate neighbor points judgment.
  Figure 7 Link: articels_figures_by_rev_year\2022\Intrinsic_and_Isotropic_Resampling_for_D_Point_Clouds\figure_7.jpg
  Figure 7 caption: Some instances of point cloud representation in the geodesic coordinate
    system. The color maps represent geodesic distance fields constructed by satellites
    (red).
  Figure 8 Link: articels_figures_by_rev_year\2022\Intrinsic_and_Isotropic_Resampling_for_D_Point_Clouds\figure_8.jpg
  Figure 8 caption: An instance of isotropic resampling with intermediate outputs.
  Figure 9 Link: articels_figures_by_rev_year\2022\Intrinsic_and_Isotropic_Resampling_for_D_Point_Clouds\figure_9.jpg
  Figure 9 caption: Comparing the resampling results from the adaptively-isotropic
    remeshing and our adaptively-isotropic resampling. It is clear that the resampling
    result of adaptively-isotropic remeshing is discontinuous with abnormal distribution.
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Chenlei Lv
  Name of the last author: Baoquan Zhao
  Number of Figures: 27
  Number of Tables: 10
  Number of authors: 3
  Paper title: Intrinsic and Isotropic Resampling for 3D Point Clouds
  Publication Date: 2022-06-23 00:00:00
  Table 1 caption: TABLE 1 Geometric Average Error-Based Evaluation for Different
    Denoising Methods
  Table 10 caption: TABLE 10 Time Cost Report of Our Resampling Method in Extreme
    Cases
  Table 2 caption: TABLE 2 Time Report for Different Denoising Methods (Average Time
    in Seconds for Different Kinds of Noisy Point Clouds)
  Table 3 caption: TABLE 3 Geometric Error Analysis of Different Simplification Methods
  Table 4 caption: TABLE 4 Geometric Error Analysis of Different Simplification Methods
  Table 5 caption: TABLE 5 Triangle Quality Measurement by Different Reconstruction
    Methods in SHREC (10,000 Points) and RGB-D Scene (20,000 Points) Models
  Table 6 caption: TABLE 6 Hausdorff Distances by Different Reconstruction Methods
    in SHREC (10,000 Points) and RGB-D Scene (20,000 Points) Models
  Table 7 caption: TABLE 7 Mean Squared Errors (MSE) of Point Distance by Different
    Registration Methods in RGB-D Scene Models
  Table 8 caption: TABLE 8 Mean Squared Errors (MSE) of Point Distance by Different
    Registration Methods in SHREC Models
  Table 9 caption: TABLE 9 Time Cost Report of Our Resampling Method (Previous Steps)
    in Extreme Cases
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3185644
- Affiliation of the first author: department of computer science and engineering,
    the chinese university of hong kong, hong kong
  Affiliation of the last author: department of computer science and engineering,
    the chinese university of hong kong, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2022\Instance_Shadow_Detection_With_a_SingleStage_Detector\figure_1.jpg
  Figure 1 caption: Given (a) an input photo, the goal of instance shadow detection
    is to detect (c) individual shadow instances, (d) individual object instances,
    and (e) shadow-object associations. (b) shows the overall result produced by our
    method from (a).
  Figure 10 Link: articels_figures_by_rev_year\2022\Instance_Shadow_Detection_With_a_SingleStage_Detector\figure_10.jpg
  Figure 10 caption: Visual comparison between instance shadow detection results produced
    by various methods (b)-(d) on the SOBA-challenge set.
  Figure 2 Link: articels_figures_by_rev_year\2022\Instance_Shadow_Detection_With_a_SingleStage_Detector\figure_2.jpg
  Figure 2 caption: Example images with the mask labels in our SOBA data set. Please
    zoom in for a better visualization.
  Figure 3 Link: articels_figures_by_rev_year\2022\Instance_Shadow_Detection_With_a_SingleStage_Detector\figure_3.jpg
  Figure 3 caption: Statistical properties of the SOBA dataset.
  Figure 4 Link: articels_figures_by_rev_year\2022\Instance_Shadow_Detection_With_a_SingleStage_Detector\figure_4.jpg
  Figure 4 caption: The schematic illustration of our single-stage instance shadow
    detection network (SSISv2). The mask feature and outputs of the box tower and
    class tower are used to formulate the bidirectional relation learning module;
    see Fig. 5. The mask feature and output instance masks are sent to the deformable
    MaskIoU head for mask refinement; see Section 4.3. Note that each head has its
    own box head and class head, and the filter parameters among these heads are shared.
  Figure 5 Link: articels_figures_by_rev_year\2022\Instance_Shadow_Detection_With_a_SingleStage_Detector\figure_5.jpg
  Figure 5 caption: "The schematic illustration of the bidirectional relation learning\
    \ module in our network. The left part (Object \u2192 Shadow) shows how to find\
    \ the associated shadow instance from the location of the paired object instance,\
    \ whereas the right part (Shadow \u2192 Object) shows how to find the associated\
    \ object instance from the location of the paired shadow instance."
  Figure 6 Link: articels_figures_by_rev_year\2022\Instance_Shadow_Detection_With_a_SingleStage_Detector\figure_6.jpg
  Figure 6 caption: Confidence scores versus mask IoUs before and after applying the
    MaskIoU head or Deformable MaskIoU Head. Each point in the figure denotes a predicted
    instance mask. The thick lines in the plots indicate Confidence score equals mask
    IoU. As shown in (c), with the Deformable MaskIoU head, we can dramatically avoid
    more masks with high confidence scores but low IoUs.
  Figure 7 Link: articels_figures_by_rev_year\2022\Instance_Shadow_Detection_With_a_SingleStage_Detector\figure_7.jpg
  Figure 7 caption: Shadow-aware copy-and-paste augmentation. (b) & (c) show example
    copying-and-pasting results on different objects.
  Figure 8 Link: articels_figures_by_rev_year\2022\Instance_Shadow_Detection_With_a_SingleStage_Detector\figure_8.jpg
  Figure 8 caption: The mask head (top right) simultaneously predicts a thick boundary
    map and an instance mask. We then pass the instance mask to the Laplacian filter
    to produce the thin boundary map.
  Figure 9 Link: articels_figures_by_rev_year\2022\Instance_Shadow_Detection_With_a_SingleStage_Detector\figure_9.jpg
  Figure 9 caption: Visual comparison between instance shadow detection results produced
    by various methods (b)-(d) on images (a) in the SOBA-testing set; (e) shows the
    learned locations for pairing shadow and object instances in our method.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Tianyu Wang
  Name of the last author: Chi-Wing Fu
  Number of Figures: 15
  Number of Tables: 7
  Number of authors: 4
  Paper title: Instance Shadow Detection With a Single-Stage Detector
  Publication Date: 2022-06-23 00:00:00
  Table 1 caption: TABLE 1 Comparison With the Previous State-of-the-Art Methods for
    Instance Shadow Detection on the SOBA-Testing Set
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison With the Previous State-of-The-Art Methods for
    Instance Shadow Detection on the SOBA-Challenge Set
  Table 3 caption: "TABLE 3 Component Analysis on the SOBA-Testing Set; \u201CData\
    \ Augm\u201D Denotes Shadow-Aware Copy-and-Paste (See Section 4.4.1)"
  Table 4 caption: TABLE 4 Evaluation on the Bidirectional Learning Strategy
  Table 5 caption: TABLE 5 Evaluation on the MaskIoU Head Strategy
  Table 6 caption: TABLE 6 Evaluation on the Boundary Loss Strategy
  Table 7 caption: TABLE 7 Evaluation on Shadow-Aware Copy-and-Paste Augm
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3185628
- Affiliation of the first author: biomedical computer vision group, bioquant, ipmb,
    heidelberg university, heidelberg, germany
  Affiliation of the last author: biomedical computer vision group, bioquant, ipmb,
    heidelberg university, heidelberg, germany
  Figure 1 Link: articels_figures_by_rev_year\2022\Superadditivity_and_Convex_Optimization_for_Globally_Optimal_Cell_Segmentation_U\figure_1.jpg
  Figure 1 caption: Overview of our SuperDSM method for cell nuclei segmentation.
  Figure 10 Link: articels_figures_by_rev_year\2022\Superadditivity_and_Convex_Optimization_for_Globally_Optimal_Cell_Segmentation_U\figure_10.jpg
  Figure 10 caption: (a) Original H&E-stained histopathology image. (b) Ground truth.
    (c) Segmentation result (green contours) of SuperDSM.
  Figure 2 Link: articels_figures_by_rev_year\2022\Superadditivity_and_Convex_Optimization_for_Globally_Optimal_Cell_Segmentation_U\figure_2.jpg
  Figure 2 caption: "Example segmentation results for different values of the regularization\
    \ parameter \u03B1 . Left to right: Original image section, segmentation results\
    \ using \u03B1=0.001 , \u03B1=0.002 , \u03B1=0.003 , \u03B1=0.004 , \u03B1=0.005\
    \ ."
  Figure 3 Link: articels_figures_by_rev_year\2022\Superadditivity_and_Convex_Optimization_for_Globally_Optimal_Cell_Segmentation_U\figure_3.jpg
  Figure 3 caption: "Relation of the energy inf \u03B8,\u03BE \u03C8 \u03C9 (\u03B8\
    ,\u03BE) and the scale \u03C3 . Top: Example image regions of different cell types,\
    \ from left to right: U2OS, NIH3T3, GOWT1, Fibroblast, HeLa. Bottom: Corresponding\
    \ energy inf \u03B8,\u03BE \u03C8 \u03C9 (\u03B8,\u03BE) as a function of the\
    \ scale."
  Figure 4 Link: articels_figures_by_rev_year\2022\Superadditivity_and_Convex_Optimization_for_Globally_Optimal_Cell_Segmentation_U\figure_4.jpg
  Figure 4 caption: "A posteriori assessment of global optimality of Algorithm 2 for\
    \ different numbers of iterations ( maxiter ) and values of \u03B3 based on all\
    \ instances of MSC (S) in Algorithm 1 in our experiments. Each curve shows a lower\
    \ bound (indicated by the shading) of the ratio of de-facto exact solutions."
  Figure 5 Link: articels_figures_by_rev_year\2022\Superadditivity_and_Convex_Optimization_for_Globally_Optimal_Cell_Segmentation_U\figure_5.jpg
  Figure 5 caption: Example segmentation results (green contours) for the NIH3T3 dataset.
    (a) Original image. (b) Ground truth. (c) Result of RFOVE. (d) Result of SuperDSM.
  Figure 6 Link: articels_figures_by_rev_year\2022\Superadditivity_and_Convex_Optimization_for_Globally_Optimal_Cell_Segmentation_U\figure_6.jpg
  Figure 6 caption: Example segmentation results (green contours) for the U2OS dataset.
    (a) Original image (contrast-enhanced). (b) Ground truth. (c) Result of SEG-SELF.
    (d) Result of SuperDSM.
  Figure 7 Link: articels_figures_by_rev_year\2022\Superadditivity_and_Convex_Optimization_for_Globally_Optimal_Cell_Segmentation_U\figure_7.jpg
  Figure 7 caption: Example segmentation results (green contours) for GOWT1 dataset
    1 (left column) and GOWT1 dataset 2 (right column). (a) Original images (contrast-enhanced).
    (b) Ground truth. (c) Result of RFOVE. (d) Result of SuperDSM.
  Figure 8 Link: articels_figures_by_rev_year\2022\Superadditivity_and_Convex_Optimization_for_Globally_Optimal_Cell_Segmentation_U\figure_8.jpg
  Figure 8 caption: Example segmentation results (green contours) for the Fibroblast
    dataset. (a) Original image (contrast-enhanced). (b) Result of GOCELL. (c) Result
    of Cellpose. (d) Result of SuperDSM.
  Figure 9 Link: articels_figures_by_rev_year\2022\Superadditivity_and_Convex_Optimization_for_Globally_Optimal_Cell_Segmentation_U\figure_9.jpg
  Figure 9 caption: Run time performance of our approach for the six datasets (darker
    color shades correspond to SuperDSM, brighter color shades correspond to SuperDSM).
    (a) Histogram of the set exclusion rate SER (relative frequencies). (b) Computation
    time of the individual processing steps of our approach (mean and standard deviation).
    (c) Histogram of the total run time per image (absolute frequencies). The labels
    on the horizontal axis denote intervals (e.g., 0 to 1 minutes, 1 to 2 minutes).
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Leonid Kostrykin
  Name of the last author: Karl Rohr
  Number of Figures: 10
  Number of Tables: 1
  Number of authors: 2
  Paper title: Superadditivity and Convex Optimization for Globally Optimal Cell Segmentation
    Using Deformable Shape Models
  Publication Date: 2022-06-23 00:00:00
  Table 1 caption: TABLE 1 Segmentation Performance of Different Approaches.
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3185583
- Affiliation of the first author: department of economics, american university, washington,
    dc, usa
  Affiliation of the last author: new school for social research, new york, ny, usa
  Figure 1 Link: Not Available
  Figure 1 caption: Not Available
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Amos Golan
  Name of the last author: Duncan K. Foley
  Number of Figures: 0
  Number of Tables: 0
  Number of authors: 2
  Paper title: Understanding the Constraints in Maximum Entropy Methods for Modeling
    and Inference
  Publication Date: 2022-06-23 00:00:00
  Table 1 caption: Not Available
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3185394
- Affiliation of the first author: "universit\xE9 de paris, inria, centre de recherche\
    \ des cordeliers, inserm, sorbonne universit\xE9, paris, france"
  Affiliation of the last author: "universit\xE9 de paris, inria, centre de recherche\
    \ des cordeliers, inserm, sorbonne universit\xE9, paris, france"
  Figure 1 Link: articels_figures_by_rev_year\2022\Data_Augmentation_in_High_Dimensional_Low_Sample_Size_Setting_Using_a_GeometryBa\figure_1.jpg
  Figure 1 caption: Geometry-aware VAE framework. Neural networks are highlighted
    with the colored arrows and H Riemannian are the normalizing flows using Riemannian
    Hamiltonian equations.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Data_Augmentation_in_High_Dimensional_Low_Sample_Size_Setting_Using_a_GeometryBa\figure_2.jpg
  Figure 2 caption: 'Geodesic interpolations under the learned metric in two different
    latent spaces. Top: Latent spaces with the log metric volume element presented
    in gray scale. Second row: The resulting interpolations under the euclidean metric
    or the Riemannian metric. Third row: The learned manifolds and corresponding decoded
    samples. Bottom: Decoded samples all along the interpolation curves.'
  Figure 3 Link: articels_figures_by_rev_year\2022\Data_Augmentation_in_High_Dimensional_Low_Sample_Size_Setting_Using_a_GeometryBa\figure_3.jpg
  Figure 3 caption: "VAE sampling comparison. Top: The learned latent space along\
    \ with the means \u03BC \u03D5 ( x i ) of the latent code distributions (colored\
    \ dots and crosses) and 100 latent space samples (blue dots) using either the\
    \ prior distribution or the proposed scheme. For the geometry-aware VAEs, the\
    \ log metric volume element is presented in gray scale in the background. Bottom:\
    \ The 100 corresponding decoded samples in the data space."
  Figure 4 Link: articels_figures_by_rev_year\2022\Data_Augmentation_in_High_Dimensional_Low_Sample_Size_Setting_Using_a_GeometryBa\figure_4.jpg
  Figure 4 caption: Overview of the data augmentation procedure. The input data set
    is divided into a train set (the baseline), a validation set and a test set. The
    train set is augmented using the VAE framework and generated data are then added
    to the baseline to train a benchmark classifier.
  Figure 5 Link: articels_figures_by_rev_year\2022\Data_Augmentation_in_High_Dimensional_Low_Sample_Size_Setting_Using_a_GeometryBa\figure_5.jpg
  Figure 5 caption: Evolution of the accuracy of four benchmark classifiers on reduced
    balanced MNIST (left) and reduced unbalanced MNIST data sets (right). Stochastic
    classifiers are trained with five independent runs and we report the mean accuracy
    and standard deviation on the test set.
  Figure 6 Link: articels_figures_by_rev_year\2022\Data_Augmentation_in_High_Dimensional_Low_Sample_Size_Setting_Using_a_GeometryBa\figure_6.jpg
  Figure 6 caption: Evolution of the accuracy of a benchmark DenseNet classifier according
    to the number of samples in the train set (i.e., the baseline) (left), the number
    of parameters of the Densenet (middle) and the latent space dimension of the VAE
    (right) on MNIST. Curves show the mean accuracy and standard deviation across
    5 runs on the original test set for the baseline (blue), the augmented data (orange)
    and the synthetic ones (green).
  Figure 7 Link: articels_figures_by_rev_year\2022\Data_Augmentation_in_High_Dimensional_Low_Sample_Size_Setting_Using_a_GeometryBa\figure_7.jpg
  Figure 7 caption: Example of two true patients compared to two generated by our
    method. Can you find the intruders ? Answers in Appendix I, available in the online
    supplemental material.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: "Cl\xE9ment Chadebec"
  Name of the last author: "St\xE9phanie Allassonni\xE8re"
  Number of Figures: 7
  Number of Tables: 7
  Number of authors: 4
  Paper title: Data Augmentation in High Dimensional Low Sample Size Setting Using
    a Geometry-Based Variational Autoencoder
  Publication Date: 2022-06-24 00:00:00
  Table 1 caption: TABLE 1 Effect of Geometrical Considerations on the Estimated Log-Likelihood
    and ELBO on MNIST Test Set
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 GAN-Train (the Higher the Better) and GAN-Test (the Closer
    to the Baseline the Better) Scores
  Table 3 caption: TABLE 3 Data Augmentation With a DenseNet Model as Benchmark
  Table 4 caption: TABLE 4 Accuracy Obtained by Studies Performing AD versus CN Classification
    With CNNs Applied on T1w MRI and Using Data Augmentation
  Table 5 caption: TABLE 5 Summary of Participant Demographics, Mini-Mental State
    Examination (MMSE) and Global Clinical Dementia Rating (CDR) Scores at Baseline
  Table 6 caption: TABLE 6 Mean Test Performance of Each Series of 20 Runs Trained
    With the baseline Hyperparameters
  Table 7 caption: TABLE 7 Mean Test Performance of Each Series of 20 Runs Trained
    With the optimized Hyperparameters
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3185773
- Affiliation of the first author: key laboratory of intelligent information processing
    of chinese academy of sciences (cas), institute of computing technology, chinese
    academy of sciences, beijing, china
  Affiliation of the last author: school of computer and control engineering, university
    of chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\EntityEnhanced_Adaptive_Reconstruction_Network_for_Weakly_Supervised_Referring_E\figure_1.jpg
  Figure 1 caption: 'Some examples of referring expression grounding (REG). REG aims
    to find the particular target in an image described by the language expression.
    We adaptively learn the subject, location and context because the discriminative
    information varies in different expressions. Different columns show the dominant
    cue (subject, location, context) for grounding. Red box: subject; Green box: object.'
  Figure 10 Link: articels_figures_by_rev_year\2022\EntityEnhanced_Adaptive_Reconstruction_Network_for_Weakly_Supervised_Referring_E\figure_10.jpg
  Figure 10 caption: 'Some failure cases on RefCOCO, RefCOCO+ and RefCOCOg datasets.
    The denotations of the bounding box colors are as follows. Solid red: ground truth;
    dashed blue: predicted proposal.'
  Figure 2 Link: articels_figures_by_rev_year\2022\EntityEnhanced_Adaptive_Reconstruction_Network_for_Weakly_Supervised_Referring_E\figure_2.jpg
  Figure 2 caption: The pipeline of our method. (a) Entity enhancement for proposal
    selection. (b) Adaptive grounding. (c) Collaborative reconstruction.
  Figure 3 Link: articels_figures_by_rev_year\2022\EntityEnhanced_Adaptive_Reconstruction_Network_for_Weakly_Supervised_Referring_E\figure_3.jpg
  Figure 3 caption: The proposed entity-enhanced adaptive reconstruction network (EARN).
    Given a query and an image with region proposals, EARN localizes the referential
    object through adaptive grounding and collaborative reconstruction.
  Figure 4 Link: articels_figures_by_rev_year\2022\EntityEnhanced_Adaptive_Reconstruction_Network_for_Weakly_Supervised_Referring_E\figure_4.jpg
  Figure 4 caption: "The structure of adaptive grounding (Section 3.3) and collaborative\
    \ reconstruction (Section 3.4). The reconstruction process contains attribute\
    \ classification loss, adaptive reconstruction loss (including adaptive language\
    \ and visual reconstruction loss) and language reconstruction loss. ATT: attention\
    \ layer. \u2295 : plus operation. \u2299 : element-wise vector multiplication.\
    \ C: vector concatenation."
  Figure 5 Link: articels_figures_by_rev_year\2022\EntityEnhanced_Adaptive_Reconstruction_Network_for_Weakly_Supervised_Referring_E\figure_5.jpg
  Figure 5 caption: The sketch map of (a) Adaptive reconstruction and (b) Language
    reconstruction.
  Figure 6 Link: articels_figures_by_rev_year\2022\EntityEnhanced_Adaptive_Reconstruction_Network_for_Weakly_Supervised_Referring_E\figure_6.jpg
  Figure 6 caption: 'Qualitative results on referring expression with complex relationship.
    The denotations of the bounding box colors are as follows. Solid red: Ground truth;
    dashed blue: Predicted proposal with soft context pooling; dashed green: Predicted
    proposal with max context pooling.'
  Figure 7 Link: articels_figures_by_rev_year\2022\EntityEnhanced_Adaptive_Reconstruction_Network_for_Weakly_Supervised_Referring_E\figure_7.jpg
  Figure 7 caption: Accuracy (IoU) of different threshold for hard subject filter
    on RefCOCO, RefCOCO+, RefCOCOg, and RefCLEF.
  Figure 8 Link: articels_figures_by_rev_year\2022\EntityEnhanced_Adaptive_Reconstruction_Network_for_Weakly_Supervised_Referring_E\figure_8.jpg
  Figure 8 caption: 'Qualitative results on RefCOCO, RefCOCO+, and RefCOCOg datasets.
    The denotations of the bounding box colors are as follows. Solid white: ground
    truth; dashed blue: predicted proposal; dashed yellow: context ground.'
  Figure 9 Link: articels_figures_by_rev_year\2022\EntityEnhanced_Adaptive_Reconstruction_Network_for_Weakly_Supervised_Referring_E\figure_9.jpg
  Figure 9 caption: 'More detailed qualitative results on RefCOCO, RefCOCO+ and RefCOCOg
    datasets. The denotations of the bounding box colors are as follows. Solid red:
    ground truth; dashed blue: predicted proposal. The saturation of the blue boxes
    denotes the attention score of each proposals.'
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.89
  Name of the first author: Xuejing Liu
  Name of the last author: Qingming Huang
  Number of Figures: 10
  Number of Tables: 13
  Number of authors: 7
  Paper title: Entity-Enhanced Adaptive Reconstruction Network for Weakly Supervised
    Referring Expression Grounding
  Publication Date: 2022-06-27 00:00:00
  Table 1 caption: TABLE 1 Accuracy (IoU > > 0.5) of Comparing Methods on RefCOCO,
    RefCOCO+, and RefCOCOg Datasets
  Table 10 caption: TABLE 10 Accuracy of Soft Context Pooling on Complex Relationship
  Table 2 caption: TABLE 2 Accuracy (IoU > > 0.5) of Comparing Methods on RefCLEF
    and Flickr30k Dataset
  Table 3 caption: TABLE 3 Accuracy (IoU > > 0.5) of EARN With Different Network Modules
    on RefCOCO, RefCOCO+, and RefCOCOg Dataset
  Table 4 caption: TABLE 4 Accuracy (IoU > > 0.5) Comparison Between EARN and the
    Variant of Supervised Method (MattNet [7]+GroundeR) [19]
  Table 5 caption: TABLE 5 Accuracy (IoU > > 0.5) of EARN With Different Network Modules
    on RefCLEF Dataset
  Table 6 caption: TABLE 6 The Ablation Study of Features and Entity Enhancement on
    RefCOCO, RefCOCO+, and RefCOCOg Datasets
  Table 7 caption: TABLE 7 Ablation Study of Different Loss Proportion on RefCLEF
    Dataset
  Table 8 caption: TABLE 8 Comparison Between Parsing-Based and Attention-Learned
    Language Features
  Table 9 caption: TABLE 9 Comparison of Different Context Pooling
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3186410
