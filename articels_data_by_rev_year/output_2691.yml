- Affiliation of the first author: moe key lab of artificial intelligence, ai institute,
    shanghai jiao tong university, shanghai, china
  Affiliation of the last author: department of computer science, city university
    of hong kong, kowloon, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2022\Continual_Learning_for_Blind_Image_Quality_Assessment\figure_1.jpg
  Figure 1 caption: Illustration of the continual learning paradigm for BIQA. Subpopulation
    shift exists across distortion types and scenarios. In Setting I, a BIQA model
    continually evolves from one distortion type to another within the same distortion
    scenario. In Setting II, a BIQA model continually evolves with varying distortion
    scenarios.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Continual_Learning_for_Blind_Image_Quality_Assessment\figure_2.jpg
  Figure 2 caption: Images sampled from the deraining quality assessment dataset [63].
    A larger MOS in the dataset denotes lighter rain density. It is not hard to observe
    that rain density is not monotonically correlated with perceived image quality.
    Therefore, the dataset violates Desideratum I and should be excluded from the
    task sequence for BIQA. Images are cropped for improved visibility.
  Figure 3 Link: articels_figures_by_rev_year\2022\Continual_Learning_for_Blind_Image_Quality_Assessment\figure_3.jpg
  Figure 3 caption: System diagram of the proposed continual learning method. Black
    and grey arrows correspond to the training and testing phases, respectively.
  Figure 4 Link: articels_figures_by_rev_year\2022\Continual_Learning_for_Blind_Image_Quality_Assessment\figure_4.jpg
  Figure 4 caption: The pairwise CORAL distances between the six IQA datasets with
    different distortion scenarios. A larger CORAL value indicates a higher level
    of dissimilarity between two datasets.
  Figure 5 Link: articels_figures_by_rev_year\2022\Continual_Learning_for_Blind_Image_Quality_Assessment\figure_5.jpg
  Figure 5 caption: PSI t as a function of the task index t .
  Figure 6 Link: articels_figures_by_rev_year\2022\Continual_Learning_for_Blind_Image_Quality_Assessment\figure_6.jpg
  Figure 6 caption: Perceptual scaling of images sampled from the six IQA datasets.
    The bar charts of weights and quality predictions of all heads are also presented
    alongside each image. The final quality prediction hatq(x) is shown in the subcaption.
    Zoom in for better distortion visibility.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.97
  Name of the first author: Weixia Zhang
  Name of the last author: Kede Ma
  Number of Figures: 6
  Number of Tables: 10
  Number of authors: 6
  Paper title: Continual Learning for Blind Image Quality Assessment
  Publication Date: 2022-05-30 00:00:00
  Table 1 caption: TABLE 1 Summary of IQA Datasets Used in Our Experiments
  Table 10 caption: TABLE 10 Performance Comparison in Terms of mSRCC mSRCC, mPI mPI,
    mSI mSI, and mPSI mPSI of Different Experience Replay Methods with Varying Memory
    Budgets
  Table 2 caption: TABLE 2 The Proposed Two-Stream Network, Consisting of a VGG-like
    CNN [33] and a Variant of ResNet-18 [76], for a T T-Length Task Sequence
  Table 3 caption: TABLE 3 Performance Comparison in Terms of mSRCC mSRCC, mPI mPI,
    mSI mSI , and mPSI mPSI
  Table 4 caption: TABLE 4 Performance Comparison in Terms of SRCC Between the Proposed
    Method and Its Variants
  Table 5 caption: TABLE 5 Performance Comparison of Different BIQA Models
  Table 6 caption: TABLE 6 Performance Comparison for Different Task Orders
  Table 7 caption: TABLE 7 Performance Comparison in Terms of mSRCC mSRCC and mPSI
    mPSI of Different Gating Strategies
  Table 8 caption: TABLE 8 Confusion Matrices of the KG Mechanism With Different Feature
    Representations
  Table 9 caption: TABLE 9 Performance Comparison of Different Feature Representations
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3178874
- Affiliation of the first author: department of information engineering and computer
    science, university of trento, trento, italy
  Affiliation of the last author: department of information engineering and computer
    science, university of trento, trento, italy
  Figure 1 Link: articels_figures_by_rev_year\2022\On_the_Eigenvalues_of_Global_Covariance_Pooling_for_FineGrained_Visual_Recogniti\figure_1.jpg
  Figure 1 caption: (Left) Validation accuracy on Aircrafts versus the number of truncated
    eigenvalues. When the small eigenvalues are truncated, the performance drops drastically.
    After 30 eigenvalues are truncated, the model cannot converge. (Middle and Right)
    Validationtraining accuracy and loss on Aircrafts versus training epochs when
    the last 50 eigenvalues are truncated. The model fails to converge on the dataset.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\On_the_Eigenvalues_of_Global_Covariance_Pooling_for_FineGrained_Visual_Recogniti\figure_2.jpg
  Figure 2 caption: '(Left) Input responses of backward gradients to specific eigenvalues.
    Small eigenvalues (i.e., last 50 out of 256) highlight the salient class-discriminative
    regions, and they are coherent with the cases which use all eigenvalues. In contrast,
    the large eigenvalues (i.e., top 206) correspond to the background region. (Right)
    Visualization of learned feature patterns that maximally activate the specific
    eigenvalues: small eigenvalues correspond to the class-specific features (e.g.,
    feather, beak, and head), while the features associated with the large eigenvalues
    are not obviously class-relevant and human-interpretable. Zoom in for a better
    view.'
  Figure 3 Link: articels_figures_by_rev_year\2022\On_the_Eigenvalues_of_Global_Covariance_Pooling_for_FineGrained_Visual_Recogniti\figure_3.jpg
  Figure 3 caption: Overview of the standard global covariance pooling procedure (i.e.,
    MPN-COV [7] and iSQRT-COV [8]) and the proposed SEB component (indicated in pink).
    Our proposed SEB augments the representation power of matrix square root by magnifying
    the importance of small eigenvalues. Without introducing any additional parameters,
    the performances are significantly boosted.
  Figure 4 Link: articels_figures_by_rev_year\2022\On_the_Eigenvalues_of_Global_Covariance_Pooling_for_FineGrained_Visual_Recogniti\figure_4.jpg
  Figure 4 caption: Different eigenvalue normalization schemes. Our proposed matrix
    exponential inverse can effectively narrow the eigenvalue range and amplify the
    importance of the small ones.
  Figure 5 Link: articels_figures_by_rev_year\2022\On_the_Eigenvalues_of_Global_Covariance_Pooling_for_FineGrained_Visual_Recogniti\figure_5.jpg
  Figure 5 caption: Validation accuracy curves versus training epochs of ResNet-50
    on three fine-grained benchmarks. Our proposed SEB consistently outperforms the
    original GCP methods by a large margin. The lines are smoothed by a moving average
    filter for a better view.
  Figure 6 Link: articels_figures_by_rev_year\2022\On_the_Eigenvalues_of_Global_Covariance_Pooling_for_FineGrained_Visual_Recogniti\figure_6.jpg
  Figure 6 caption: "(Top) The histogram of eigenvalue distribution of the covariance\
    \ P of MPN-COV [7] and our SEB on three fine-grained benchmarks. The proposed\
    \ SEB effectively narrows the eigenvalue range and magnify the small ones. (Bottom)\
    \ The condition number \u03BA(P) of MPN-COV [7] and our proposed SEB on the fine-grained\
    \ benchmarks. Our covariance matrices are consistently better-conditioned than\
    \ MPN-COV [7] and the relative importance of small eigenvalues are amplified."
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.67
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Yue Song
  Name of the last author: Wei Wang
  Number of Figures: 6
  Number of Tables: 8
  Number of authors: 3
  Paper title: On the Eigenvalues of Global Covariance Pooling for Fine-Grained Visual
    Recognition
  Publication Date: 2022-05-30 00:00:00
  Table 1 caption: TABLE 1 The Average Correlation Coefficient and MAE Between the
    Input Activation of the Specific Eigenvalues and the Responses of All the Eigenvalues
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Classification Accuracy (%) Using Only Subsets of Eigenvalues
    During the Inference Stage
  Table 3 caption: TABLE 3 Comparison With the State-of-the-Art GCP Approaches
  Table 4 caption: TABLE 4 Comparison With Other State-of-The-Arts That are Achieved
    by Transformer-Based Methods on Larger Datasets
  Table 5 caption: TABLE 5 Performance on Other Popular Deep Architectures
  Table 6 caption: TABLE 6 Impact of Using Only SEB Path
  Table 7 caption: TABLE 7 Impact of Different Scaling Factors
  Table 8 caption: TABLE 8 Time and Memory Consumption of GCP Methods With ResNet-50
    Model on Cars Dataset for Each Mini-Batch.
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3178802
- Affiliation of the first author: school of electrical and computer engineering,
    georgia institute of technology, atlanta, ga, usa
  Affiliation of the last author: school of electrical and computer engineering, georgia
    institute of technology, atlanta, ga, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\RadarBased_Shape_and_Reflectivity_Reconstruction_Using_Active_Surfaces_and_the_L\figure_1.jpg
  Figure 1 caption: 'Concept of image-radar data fusion: images convey lateral information
    while radar conveys depth. a) Example of cloud points as it would be produced
    by LIDAR. b) RGB-like image. c) Example of ToF-like depth-map image. d) 2D images
    preserve a one-to-one correspondence with the probed objects points. e) Each sample
    of a radar time serie collapses information from surface points with the same
    range.'
  Figure 10 Link: articels_figures_by_rev_year\2022\RadarBased_Shape_and_Reflectivity_Reconstruction_Using_Active_Surfaces_and_the_L\figure_10.jpg
  Figure 10 caption: a) Energy minimization (black line) and value of the regularizing
    term. Plots are normalized for better comparison. b) estimated squared reflectivity
    G as a function of iterations. Labels (b), (c), and (d) within the two plots correspond
    to Figs. 11(b), 11(c), and 11(d), respectively.
  Figure 2 Link: articels_figures_by_rev_year\2022\RadarBased_Shape_and_Reflectivity_Reconstruction_Using_Active_Surfaces_and_the_L\figure_2.jpg
  Figure 2 caption: a) Generative model for multiview stereo reconstruction. b) Iterative
    work flow underlying the active surface based multiview stereo reconstruction
    approach.
  Figure 3 Link: articels_figures_by_rev_year\2022\RadarBased_Shape_and_Reflectivity_Reconstruction_Using_Active_Surfaces_and_the_L\figure_3.jpg
  Figure 3 caption: 'Image-based multiview stereo reconstruction using active surfaces.
    An initially ellipsoidal 3D active surface is evolved to fit the silhouettes of
    several toy horse images. a) Top row: input images. Bottom row: projection of
    the converged surface (in yellow) onto the input images. b) Evolution of the active
    surface at selected iterations. c) Full 3D rendering of the converged surface.'
  Figure 4 Link: articels_figures_by_rev_year\2022\RadarBased_Shape_and_Reflectivity_Reconstruction_Using_Active_Surfaces_and_the_L\figure_4.jpg
  Figure 4 caption: a) Iterative algorithm for radar (analogous to Fig. 2b). b) Example
    of how each sample of our observable u 0 (the quantity derived from radar pulses
    to be inverted), collapses information from a ribbon of visible surface falling
    within the same time window. Consecutive windows identify slightly different surface
    ribbons.
  Figure 5 Link: articels_figures_by_rev_year\2022\RadarBased_Shape_and_Reflectivity_Reconstruction_Using_Active_Surfaces_and_the_L\figure_5.jpg
  Figure 5 caption: Experimental setup. a), b) and c) show antenna locations (on the
    left) and the reference shape used to generate synthetic signals (on the right),
    for the inversion examples described in Sections 4.1, 4.2, and 4.3, respectively.
    All dimensions are in meters.
  Figure 6 Link: articels_figures_by_rev_year\2022\RadarBased_Shape_and_Reflectivity_Reconstruction_Using_Active_Surfaces_and_the_L\figure_6.jpg
  Figure 6 caption: Example of preprocessed radar signals (5) from selected antennas
    [left side of Fig. 5(a)] are shown in black. The radar response computed by our
    forward model during inversion, for both the initial shape and for the converged
    shape are shown in blue and red, respectively. For sake of comparison, all signals
    are shown normalized with respect to the input data maximum amplitude. In addition,
    to better highlight the different behavior of radar data from image-based approaches,
    on the top-right corner of each figure we provide the active surface depth captured
    by a z-buffer algorithm we used for the computation of the scene visibility. Note
    that, in Fig. a, the z-buffer depth reveals the two separate spheres, while the
    radar signal exhibit only one peak.
  Figure 7 Link: articels_figures_by_rev_year\2022\RadarBased_Shape_and_Reflectivity_Reconstruction_Using_Active_Surfaces_and_the_L\figure_7.jpg
  Figure 7 caption: a) Energy minimization (black line) and value of the regularizing
    term. Plots are normalized for better comparison. b) Estimated squared reflectivity
    G as a function of iterations.
  Figure 8 Link: articels_figures_by_rev_year\2022\RadarBased_Shape_and_Reflectivity_Reconstruction_Using_Active_Surfaces_and_the_L\figure_8.jpg
  Figure 8 caption: "Radar-base shape reconstruction. The active surface (shown in\
    \ red) is iteratively evolved seeking the shape which best reproduces the input\
    \ preprocessed radar echoes (e.g. black plots in Fig. 6), acquired at the 63 locations\
    \ shown in Fig. 5(a). Input data were generated using the \u201Ctrue model\u201D\
    \ shape on the right of Fig. 5(a), which is shown in gray. a) The active surface\
    \ is initialized to a sphere. b) Subsequently it morphs into an elongated shape.\
    \ c) The surface thins in the middle while invading the location at which the\
    \ two spheres should be. d) Converged geometry."
  Figure 9 Link: articels_figures_by_rev_year\2022\RadarBased_Shape_and_Reflectivity_Reconstruction_Using_Active_Surfaces_and_the_L\figure_9.jpg
  Figure 9 caption: Example of preprocessed radar signals (5) from selected antennas
    (left side of Fig. 5(b)) are shown in black. The radar response computed by our
    forward model during inversion, for both the initial shape and for the converged
    shape are shown in blue and red, respectively. For sake of comparison, all signals
    are shown normalized with respect to the input data maximum amplitude.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Samuel Bignardi
  Name of the last author: Anthony Yezzi
  Number of Figures: 14
  Number of Tables: 0
  Number of authors: 3
  Paper title: Radar-Based Shape and Reflectivity Reconstruction Using Active Surfaces
    and the Level Set Method
  Publication Date: 2022-05-30 00:00:00
  Table 1 caption: Not Available
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3178969
- Affiliation of the first author: school of automation, northwestern polytechnical
    university, xian, china
  Affiliation of the last author: school of automation, northwestern polytechnical
    university, xian, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Equivalent_Classification_Mapping_for_Weakly_Supervised_Temporal_Action_Localiza\figure_1.jpg
  Figure 1 caption: Illustration of two action localization pipelines. (a) Pre-classification
    pipeline first performs classification at each temporal point then aggregates
    scores. (b) Post-classification pipeline first aggregates features then predicts
    the classification score. Both pipelines are driven by the video-level classification
    loss L cls .
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Equivalent_Classification_Mapping_for_Weakly_Supervised_Temporal_Action_Localiza\figure_2.jpg
  Figure 2 caption: Framework of the proposed equivalent classification mapping (ECM)
    method. We extract video feature and send it to the pre-classification stream
    (shown on the top part) and the post-classification stream (shown on the bottom
    part). The pre-classification stream performs classification at each temporal
    point, while the post-classification stream performs classification at the aggregated
    video-level features. The classifier is shared between the pre-classification
    stream and the post-classification stream, and is learned by equivalence mechanism.
    The complete ECM is driven by two classification losses L cls and two equivalence-based
    losses ( L c2c and L a2c ).
  Figure 3 Link: articels_figures_by_rev_year\2022\Equivalent_Classification_Mapping_for_Weakly_Supervised_Temporal_Action_Localiza\figure_3.jpg
  Figure 3 caption: "Illustration of the detailed operations for feature aggregation\
    \ in the post-classification stream. In order to share the classifiers between\
    \ pre-classification stream and post-classification stream, we split features\
    \ and weights into three groups. Please refer to Subsection \u201CThe body network\u201D\
    \ for details."
  Figure 4 Link: articels_figures_by_rev_year\2022\Equivalent_Classification_Mapping_for_Weakly_Supervised_Temporal_Action_Localiza\figure_4.jpg
  Figure 4 caption: "Equivalent weight-transition module. We predict the foreground\
    \ and background aggregation weights from the class activation sequence. \xA9\
    \ indicates temporal convolution, \u24C8 indicates sigmoid activation."
  Figure 5 Link: articels_figures_by_rev_year\2022\Equivalent_Classification_Mapping_for_Weakly_Supervised_Temporal_Action_Localiza\figure_5.jpg
  Figure 5 caption: Qualitative results of the class activation sequences for pre-classification
    stream (Pre-Cls), post-classification stream (Post-Cls) and the proposed ECM.
  Figure 6 Link: articels_figures_by_rev_year\2022\Equivalent_Classification_Mapping_for_Weakly_Supervised_Temporal_Action_Localiza\figure_6.jpg
  Figure 6 caption: "Qualitative results of failure cases for the proposed ECM. There\
    \ are three typical failure cases. First, as shown in the red dotted box, some\
    \ frames exhibit a slow playback for an action instance, whose duration is quite\
    \ different from the normal instances. In these cases, the algorithm would produce\
    \ miss-localization results. Second, for the instances in the green dotted box,\
    \ our proposed ECM predicts a longer action duration than the ground truth. This\
    \ is because similar context frames would confuse the algorithm to find the accurate\
    \ action-background boundary. Third, the blue dotted box exhibits the false alarm\
    \ within background segments. This is an unusual case that the person does the\
    \ sub-action \u201Cjump\u201D of Diving on the diving board for a while but cancels\
    \ the action at the end."
  Figure 7 Link: articels_figures_by_rev_year\2022\Equivalent_Classification_Mapping_for_Weakly_Supervised_Temporal_Action_Localiza\figure_7.jpg
  Figure 7 caption: Mainfold distribution of point features and aggregated features.
    Background features, action features and the aggregated feature are shown in gray,
    green and red, respectively. Best viewed in zoom.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.87
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.67
  Name of the first author: Tao Zhao
  Name of the last author: Dingwen Zhang
  Number of Figures: 7
  Number of Tables: 7
  Number of authors: 4
  Paper title: Equivalent Classification Mapping for Weakly Supervised Temporal Action
    Localization
  Publication Date: 2022-05-30 00:00:00
  Table 1 caption: TABLE 1 Comparisons Between ECM and State-of-the-Art Methods on
    ActivityNet v1.2 Dataset
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison Experiments on ActivityNet v1.3 Dataset
  Table 3 caption: TABLE 3 Comparisons Between ECM and Recent State-of-the-art Methods
    on THUMOS14 Dataset
  Table 4 caption: TABLE 4 Ablation Studies About Network Architecture and Loss Functions
    on THUMOS14 Dataset, Measured by mAP0.5
  Table 5 caption: TABLE 5 Ablation Studies About Fusion Localization Results From
    the Pre-Classification Stream and the Post-Classification Stream
  Table 6 caption: TABLE 6 Ablation Studies About Frame Accuracy on THUMOS14 Dataset
  Table 7 caption: TABLE 7 Performance of Employing the Proposed Equivalent Classification
    Mapping (ECM) Mechanism to the Stronger Pre-Classification Baseline Model Pre-Cls
    Model+ and Post-Classification Baseline Model Post-Cls Model+ on THUMOS14, Measured
    by mAP Under Different IoU Thresholds
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3178957
- Affiliation of the first author: laboratory for information and decision systems
    (lids), massachusetts institute of technology, cambridge, ma, usa
  Affiliation of the last author: laboratory for information and decision systems
    (lids), massachusetts institute of technology, cambridge, ma, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\Certifiably_Optimal_OutlierRobust_Geometric_Perception_Semidefinite_Relaxations_\figure_1.jpg
  Figure 1 caption: Estimation using common robust cost functions can be equivalently
    reformulated as polynomial optimization problems (cf. Proposition 4).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Certifiably_Optimal_OutlierRobust_Geometric_Perception_Semidefinite_Relaxations_\figure_2.jpg
  Figure 2 caption: Perception examples considered in this paper that can be modeled
    as polynomial optimization problems (cf. Examples 1-6).
  Figure 3 Link: articels_figures_by_rev_year\2022\Certifiably_Optimal_OutlierRobust_Geometric_Perception_Semidefinite_Relaxations_\figure_3.jpg
  Figure 3 caption: Comparison of SDP sizes when applying (LAS) and (SSR) to Example
    1 ( d=9 ) with N increased from 20 to 200.
  Figure 4 Link: articels_figures_by_rev_year\2022\Certifiably_Optimal_OutlierRobust_Geometric_Perception_Semidefinite_Relaxations_\figure_4.jpg
  Figure 4 caption: Single rotation averaging (Example 1).
  Figure 5 Link: articels_figures_by_rev_year\2022\Certifiably_Optimal_OutlierRobust_Geometric_Perception_Semidefinite_Relaxations_\figure_5.jpg
  Figure 5 caption: 2D Multiple rotation averaging (Example 2).
  Figure 6 Link: articels_figures_by_rev_year\2022\Certifiably_Optimal_OutlierRobust_Geometric_Perception_Semidefinite_Relaxations_\figure_6.jpg
  Figure 6 caption: Point cloud registration (Example 3).
  Figure 7 Link: articels_figures_by_rev_year\2022\Certifiably_Optimal_OutlierRobust_Geometric_Perception_Semidefinite_Relaxations_\figure_7.jpg
  Figure 7 caption: Absolute pose estimation (Example 5).
  Figure 8 Link: articels_figures_by_rev_year\2022\Certifiably_Optimal_OutlierRobust_Geometric_Perception_Semidefinite_Relaxations_\figure_8.jpg
  Figure 8 caption: Category-level object perception (Example 6).
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: Heng Yang
  Name of the last author: Luca Carlone
  Number of Figures: 8
  Number of Tables: 2
  Number of authors: 2
  Paper title: 'Certifiably Optimal Outlier-Robust Geometric Perception: Semidefinite
    Relaxations and Scalable Global Optimization'
  Publication Date: 2022-05-31 00:00:00
  Table 1 caption: TABLE 1 Comparison of the Sizes of two Semidefinite Relaxations
    Applied to Problem (TLS)
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Average Timing of STRIDE STRIDE and MOSEK MOSEK (in Seconds)
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3179463
- Affiliation of the first author: school of computer science and engineering, sun
    yat-sen university, guangzhou, china
  Affiliation of the last author: school of computer science and engineering, sun
    yat-sen university, guangzhou, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Online_Metro_OriginDestination_Prediction_via_Heterogeneous_Information_Aggregat\figure_1.jpg
  Figure 1 caption: Illustration of the difference between station-level ridership
    and origin-destination ridership. (a) is a metro system with four stations. (b)
    and (c) are the inflowoutflow of each station, respectively. (d) is an Origin-Destination
    (OD) matrix that represents the destination distribution of incoming passengers.
    (e) is a Destination-Origin (DO) matrix that represents the origin distribution
    of outgoing passengers.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Online_Metro_OriginDestination_Prediction_via_Heterogeneous_Information_Aggregat\figure_2.jpg
  Figure 2 caption: Illustration of the incomplete OD matrix in online metro systems.
    Suppose there were 228 passengers that entered station s in the past 15 minutes
    and 136 people have arrived at their destinations up to now. Unfortunately, the
    destinations of the remaining people are unknowable. In this case, we can only
    construct an incomplete OD matrix from finished trip transactions.
  Figure 3 Link: articels_figures_by_rev_year\2022\Online_Metro_OriginDestination_Prediction_via_Heterogeneous_Information_Aggregat\figure_3.jpg
  Figure 3 caption: The architecture of the proposed Heterogeneous Information Aggregation
    Machine. This module is composed of two parallel branches respectively for OD
    and DO modeling, and a Dual Information Transformer for OD-DO interaction modeling.
  Figure 4 Link: articels_figures_by_rev_year\2022\Online_Metro_OriginDestination_Prediction_via_Heterogeneous_Information_Aggregat\figure_4.jpg
  Figure 4 caption: Illustration of the graph construction for non-euclidean metro
    systems. (a) is the physical topology of a metro system with five stations. Matrix
    (b) recodes the connectivity of edges in the constructed graph, while matrix (c)
    is the normalized weights of edges.
  Figure 5 Link: articels_figures_by_rev_year\2022\Online_Metro_OriginDestination_Prediction_via_Heterogeneous_Information_Aggregat\figure_5.jpg
  Figure 5 caption: Illustration of the potential destination matrices estimated for
    unfinished orders. Specifically, we first measure two long short-term destination
    distributions of historical unfinished orders and then estimate the potential
    destinations of ongoing passengers.
  Figure 6 Link: articels_figures_by_rev_year\2022\Online_Metro_OriginDestination_Prediction_via_Heterogeneous_Information_Aggregat\figure_6.jpg
  Figure 6 caption: The architecture of the proposed Dual Information Transformer.
    In this module, we perform information prorogation among OD branch and DO branch
    to jointly model the OD and DO distribution. The enhanced OD and DO features are
    more informative for ridership prediction.
  Figure 7 Link: articels_figures_by_rev_year\2022\Online_Metro_OriginDestination_Prediction_via_Heterogeneous_Information_Aggregat\figure_7.jpg
  Figure 7 caption: The architecture of our online metro origin-destination prediction
    framework. The framework is developed with a Seq2Seq architecture, whose encoder
    and decoder are based on the proposed module, i.e., Heterogeneous Information
    Aggregation Machine (HIAM).
  Figure 8 Link: articels_figures_by_rev_year\2022\Online_Metro_OriginDestination_Prediction_via_Heterogeneous_Information_Aggregat\figure_8.jpg
  Figure 8 caption: The Cumulative Density Function of the commuting time of metro
    passengers in Shanghai and Hangzhou. We can observe that the commuting time of
    most passengers is within one hour.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Lingbo Liu
  Name of the last author: Liang Lin
  Number of Figures: 8
  Number of Tables: 12
  Number of authors: 6
  Paper title: Online Metro Origin-Destination Prediction via Heterogeneous Information
    Aggregation
  Publication Date: 2022-05-31 00:00:00
  Table 1 caption: TABLE 1 Some Notations for Online Metro Origin-Destination Ridership
    Prediction
  Table 10 caption: TABLE 10 Performance of Different Lengths of the Input Sequence
    on the HZMOD Dataset
  Table 2 caption: TABLE 2 Details of SHMOD and HZMOD Datasets
  Table 3 caption: TABLE 3 Performance of OD Prediction and DO Prediction on the SHMOD
    Dataset
  Table 4 caption: TABLE 4 Performance of OD Prediction and DO Prediction on the HZMOD
    Dataset
  Table 5 caption: TABLE 5 Performance of Different Input Information for OD Prediction
  Table 6 caption: TABLE 6 The Influence of OD-to-DO Causality on DO Prediction Performance
  Table 7 caption: TABLE 7 The Influence of DO-to-OD Correlation on OD Prediction
    Performance
  Table 8 caption: TABLE 8 Performance of Different OD-DO Interaction Methods on the
    SHMOD and HZMOD Datasets
  Table 9 caption: TABLE 9 Performance of Different Lengths of the Input Sequence
    on the SHMOD Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3178184
- Affiliation of the first author: department of computer and information technology,
    beijing jiaotong university, beijing, china
  Affiliation of the last author: the chinese university of hong kong, hong kong sar,
    china
  Figure 1 Link: articels_figures_by_rev_year\2022\Deep_Generative_Mixture_Model_for_Robust_Imbalance_Classification\figure_1.jpg
  Figure 1 caption: tSNE analysis of (a) input data ( X ) and (b) latent codes ( Z
    ) obtained by the proposed method on Fashion-MNIST dataset.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Deep_Generative_Mixture_Model_for_Robust_Imbalance_Classification\figure_2.jpg
  Figure 2 caption: Two strategies (data perturbation and model perturbation) for
    deep imbalanced learning. Data perturbation strategy balances the training data
    via generating the minority class instances. Model perturbation method is to introduce
    uncertainty in the training process.
  Figure 3 Link: articels_figures_by_rev_year\2022\Deep_Generative_Mixture_Model_for_Robust_Imbalance_Classification\figure_3.jpg
  Figure 3 caption: The architecture of graphical models for a) the traditional generative
    classifier and b) deep generative classifier(DGC). c) the proposed generative
    classifier based on GMM. The nodes denote observed or latent variables. The solid
    lines represent generative model.
  Figure 4 Link: articels_figures_by_rev_year\2022\Deep_Generative_Mixture_Model_for_Robust_Imbalance_Classification\figure_4.jpg
  Figure 4 caption: "Neural network version of the proposed DGCMM. \u03D5 1 , \u03D5\
    \ 2 and \u03B8 1 , \u03B8 2 indicate the network parameters of the recognition\
    \ model and the generative model respectively. DGCMM generates s k latent codes\
    \ for the minority instances in the k th class."
  Figure 5 Link: articels_figures_by_rev_year\2022\Deep_Generative_Mixture_Model_for_Robust_Imbalance_Classification\figure_5.jpg
  Figure 5 caption: tSNE analysis of predicted probabilities of testing instances
    of DGC (left) and DGCMM (right) on MNIST, SVHN and CIFAR10.
  Figure 6 Link: articels_figures_by_rev_year\2022\Deep_Generative_Mixture_Model_for_Robust_Imbalance_Classification\figure_6.jpg
  Figure 6 caption: "Convergence of (a) \u03C3 values and (b) \u03BC values. (only\
    \ the first latent feature is demonstrated for eight instances randomly selected\
    \ from the largest and smallest two classes)."
  Figure 7 Link: articels_figures_by_rev_year\2022\Deep_Generative_Mixture_Model_for_Robust_Imbalance_Classification\figure_7.jpg
  Figure 7 caption: tSNE analysis of latent codes obtained by DGCMM along with training
    processes.
  Figure 8 Link: articels_figures_by_rev_year\2022\Deep_Generative_Mixture_Model_for_Robust_Imbalance_Classification\figure_8.jpg
  Figure 8 caption: tSNE analysis of latent codes obtained by DFBS, GAMO, DGC (the
    right line) on MNIST (top) and CIFAR10 (bottom) dataset.
  Figure 9 Link: articels_figures_by_rev_year\2022\Deep_Generative_Mixture_Model_for_Robust_Imbalance_Classification\figure_9.jpg
  Figure 9 caption: Generated instances on MNIST (the first line) and CIFAR10 (the
    second line) for BAGAN, DGC, and DGCMM (with FID score in brackets, lower is better).
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xinyue Wang
  Name of the last author: Tieyong Zeng
  Number of Figures: 9
  Number of Tables: 7
  Number of authors: 8
  Paper title: Deep Generative Mixture Model for Robust Imbalance Classification
  Publication Date: 2022-06-01 00:00:00
  Table 1 caption: TABLE 1 Summarization of the Experimental Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparing Overall Classification Performance on Single-Channel
    Experimental Datasets
  Table 3 caption: TABLE 3 Comparing Overall Classification Performance on Three-Channel
    Experimental Datasets
  Table 4 caption: TABLE 4 Comparing Prediction Performance on the Smallest Class
    ( R MI RMI) and Largest Class ( P MA PMA)
  Table 5 caption: TABLE 5 Ablation Test With ResNet50 on Natural and Long-Tailed
    Datasets
  Table 6 caption: TABLE 6 Comparing Overall Classification Performance on Natural
    and Long-Tailed Datasets
  Table 7 caption: TABLE 7 Comparing Overall Classification Performance on Different
    Testing Datasets
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3178914
- Affiliation of the first author: state key laboratory for novel software technology,
    nanjing university, nanjing, china
  Affiliation of the last author: state key laboratory for novel software technology,
    nanjing university, nanjing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Revisiting_Unsupervised_MetaLearning_via_the_Characteristics_of_FewShot_Tasks\figure_1.jpg
  Figure 1 caption: Supervised meta-learning (a) and Unsupervised Meta-Learning (UML)
    (b) for few-shot image classification. The learned meta-model should be applied
    to novel classes few-shot tasks after meta-training on base classes. Although
    given unlabeled base classes, UML still facilitates the few-shot model construction
    on novel classes.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Revisiting_Unsupervised_MetaLearning_via_the_Characteristics_of_FewShot_Tasks\figure_2.jpg
  Figure 2 caption: "The change of the embedding norm \u2225\u03D5(x)\u2225 (mean\
    \ as well as std. over all instances) during the meta-training progress when learning\
    \ with cosine similarity and SNS on MiniImageNet. We set \u03D5 as a four-layer\
    \ ConvNet. The detailed setup is the same as that described in Section 4.2."
  Figure 3 Link: articels_figures_by_rev_year\2022\Revisiting_Unsupervised_MetaLearning_via_the_Characteristics_of_FewShot_Tasks\figure_3.jpg
  Figure 3 caption: 'Empirical study to show the importance of sufficient sampling
    and semi-normalized similarity in UML on MiniImageNet with a four-layer ConvNet
    over 100 epochs. Upper: the meta-training loss and 5-way 1-shot meta-validation
    accuracy along epochs. Sufficient sampling with multiple tasks demonstrates fast
    convergence speed and high generalization ability. Lower left: By comparing different
    similarity measures, we find SNS always performs the best when tested with 5-way
    1,5,20,50 -shot tasks. Lower right: The gradient norms w.r.t. embeddings averaged
    over all instances in the mini-batch along with the meta-training progress. Different
    similarities have diverse changes in their gradient norms.'
  Figure 4 Link: articels_figures_by_rev_year\2022\Revisiting_Unsupervised_MetaLearning_via_the_Characteristics_of_FewShot_Tasks\figure_4.jpg
  Figure 4 caption: Comparison between SNS and cosine on MiniImageNet with ConvNet.
    We carefully tune the temperature for cosine with 16 values in [0.005,1] , and
    show mean as well as std. over all temperatures.
  Figure 5 Link: articels_figures_by_rev_year\2022\Revisiting_Unsupervised_MetaLearning_via_the_Characteristics_of_FewShot_Tasks\figure_5.jpg
  Figure 5 caption: "Comparisons among our augmentation-based UML baseline, clustering-based\
    \ UML method CACTUs, and representative self-supervised learning methods (i.e.,\
    \ MoCo and SimCLR). All methods are trained with a four-layer ConvNet on MiniImageNet.\
    \ The meta-learned embeddings with UML show better discriminative ability on \u201C\
    downstream\u201D novel class 5-way lbrace 1,5,20,50rbrace -shot tasks."
  Figure 6 Link: articels_figures_by_rev_year\2022\Revisiting_Unsupervised_MetaLearning_via_the_Characteristics_of_FewShot_Tasks\figure_6.jpg
  Figure 6 caption: Illustrations of our proposed UML methods. (a) Based on a sampled
    mini-batch, we apply image augmentations to construct pseudo-classes. Multiple
    tasks are re-sampled from a mini-batch with Sufficient Episodic Sampling (SES).
    (b) Given a query embedding, we mix it up with the most similar instances (with
    mixup strength coefficient s ). The mixed embeddings are then added as extra supports
    for this query, which constructs a query-specific difficult task. (c) To relieve
    the variety of task distributions, we transform task agnostic embeddings to task-specific
    ones with an auxiliary task-specific projection head (implemented with Transformer
    [85]) during meta-training. This component is discarded during meta-test.
  Figure 7 Link: articels_figures_by_rev_year\2022\Revisiting_Unsupervised_MetaLearning_via_the_Characteristics_of_FewShot_Tasks\figure_7.jpg
  Figure 7 caption: The change of 5-way 1-shot classification accuracy on MiniImageNet
    with ResNet when the label ratio in the base class set increases. The dotted line
    is the performance of the ProtoNet learned with a fully labeled meta-training
    set.
  Figure 8 Link: articels_figures_by_rev_year\2022\Revisiting_Unsupervised_MetaLearning_via_the_Characteristics_of_FewShot_Tasks\figure_8.jpg
  Figure 8 caption: Linear evaluation comparisons. The meta-test set is split into
    ten folds. A logistic regression is trained on frozen embeddings of the nine folds,
    and the remaining fold is used to evaluate accuracy. All methods are learned on
    the meta-training set of MiniImageNet with ResNet.
  Figure 9 Link: articels_figures_by_rev_year\2022\Revisiting_Unsupervised_MetaLearning_via_the_Characteristics_of_FewShot_Tasks\figure_9.jpg
  Figure 9 caption: "T-SNE visualization of learned embeddings on the MiniImageNet.\
    \ Four plots display results of ProtoNet [10] (a), UML baseline (b), HMS (c),\
    \ and TSP-Head (d). We use circle and cross to represent \u201Cbase\u201D and\
    \ \u201Cnovel\u201D classes, respectively. ProtoNet is trained in a supervised\
    \ manner, and the other three are trained without using any base class labels."
  First author gender probability: 0.54
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.91
  Name of the first author: Han-Jia Ye
  Name of the last author: De-Chuan Zhan
  Number of Figures: 9
  Number of Tables: 11
  Number of authors: 3
  Paper title: Revisiting Unsupervised Meta-Learning via the Characteristics of Few-Shot
    Tasks
  Publication Date: 2022-06-01 00:00:00
  Table 1 caption: TABLE 1 Comparison of Different Augmentations on MiniImageNet With
    ConvNet Backbone
  Table 10 caption: TABLE 10 Mean Classification Accuracy Between Task-Agnostic Projection
    Head Task-Specific Projection Head on Different N N-Way K K-Shot Tasks on MiniImageNet
  Table 2 caption: TABLE 2 UML Comparison on MiniImageNet With ConvNet Backbone
  Table 3 caption: TABLE 3 UML Comparisons on MiniImageNet, CIFAR-FS, and FC-100 With
    ResNet
  Table 4 caption: TABLE 4 The Mean Classification Accuracy on Tiered ImageNet With
    ResNet Backbone
  Table 5 caption: TABLE 5 Mean Classification Accuracy of Methods Learned From the
    Base Class Set of MiniImageNet and Evaluated on N N-Way K K-Shot Novel Class Tasks
    of CUB
  Table 6 caption: TABLE 6 Mean Classification Accuracy Evaluated on Different N N-Way
    K K-Shot Tasks on MiniImageNet
  Table 7 caption: TABLE 7 Comparison Between HMS and Other Mixup-Based Methods
  Table 8 caption: TABLE 8 The Influence of Mixup Strength in HMS
  Table 9 caption: TABLE 9 We Evaluate the Pre-Adapted (wo Head) and Post-Adapted
    (w Head) Embeddings on Two Kinds of Tasks Based on the Learned TSP-Head
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3179368
- Affiliation of the first author: beijing key laboratory of multimedia and intelligent
    software technology, beijing institute of artificial intelligence, faculty of
    information technology, beijing university of technology, beijing, china
  Affiliation of the last author: beijing key laboratory of multimedia and intelligent
    software technology, beijing institute of artificial intelligence, faculty of
    information technology, beijing university of technology, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Logarithmic_Schatten_p_p_Norm_Minimization_for_Tensorial_MultiView_Subspace_Clus\figure_1.jpg
  Figure 1 caption: Comparison of the rank, the convex tensor nuclear norm, and the
    proposed tensor logarithmic Schatten- p norm on the scalar case.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Logarithmic_Schatten_p_p_Norm_Minimization_for_Tensorial_MultiView_Subspace_Clus\figure_2.jpg
  Figure 2 caption: Weight w (k) i varies with the change of p and S (k) f (i,i) .
    Please note that the x -axis represents p , y -axis represents the S (k) f (i,i)
    , z -axis represents the weight w (k) i .
  Figure 3 Link: articels_figures_by_rev_year\2022\Logarithmic_Schatten_p_p_Norm_Minimization_for_Tensorial_MultiView_Subspace_Clus\figure_3.jpg
  Figure 3 caption: "Visual comparison of TNNM and TLS p NM on the scalar case, i.e.,\
    \ x \u2217 = argmin x\u22650 \u03B2f(x)+ 1 2 (x\u2212y ) 2 , where TNNM: f(x)=x\
    \ ; TLS p NM: f(x)=log(1+ x p ) with p=1,23,12 ."
  Figure 4 Link: articels_figures_by_rev_year\2022\Logarithmic_Schatten_p_p_Norm_Minimization_for_Tensorial_MultiView_Subspace_Clus\figure_4.jpg
  Figure 4 caption: Performance of textTLSp NM-MSC with different p on the COIL20,
    Caltech101, Scene-15 and Notting-Hill databases.
  Figure 5 Link: articels_figures_by_rev_year\2022\Logarithmic_Schatten_p_p_Norm_Minimization_for_Tensorial_MultiView_Subspace_Clus\figure_5.jpg
  Figure 5 caption: 'Singular value distribution of fused representations learned
    by t-SVD-MSC and the proposed textTLSp NM-MSC with p=1, 23, 12 . Remark: the singular
    values are sorted from large to small, and the y -axis represents the cumulative
    sum ratio of the singular values.'
  Figure 6 Link: articels_figures_by_rev_year\2022\Logarithmic_Schatten_p_p_Norm_Minimization_for_Tensorial_MultiView_Subspace_Clus\figure_6.jpg
  Figure 6 caption: Comparisons of the confusion matrices between t-SVD-MSC method
    and the proposed model textTLSp NM-MSC with p=12 on Scene-15 database.
  Figure 7 Link: articels_figures_by_rev_year\2022\Logarithmic_Schatten_p_p_Norm_Minimization_for_Tensorial_MultiView_Subspace_Clus\figure_7.jpg
  Figure 7 caption: Parameter tuning with respect to lambda on COIL20, MITIndoor,
    UCI-Digits and Notting-Hill databases.
  Figure 8 Link: articels_figures_by_rev_year\2022\Logarithmic_Schatten_p_p_Norm_Minimization_for_Tensorial_MultiView_Subspace_Clus\figure_8.jpg
  Figure 8 caption: Convergence associated with stopping criteria (i.e., RE and ME)
    versus iterations on COIL20, MITIndoor, UCI-Digits and Notting-Hill databases.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jipeng Guo
  Name of the last author: Baocai Yin
  Number of Figures: 8
  Number of Tables: 4
  Number of authors: 5
  Paper title: Logarithmic Schatten- p p Norm Minimization for Tensorial Multi-View
    Subspace Clustering
  Publication Date: 2022-06-01 00:00:00
  Table 1 caption: TABLE 1 Statistical Information for the Benchmark Databases
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Clustering Results on the MSRC-v1, BBC4view, Reuters, COIL20,
    Caltech101, Scene-15, and MITIndoor Databases
  Table 3 caption: TABLE 3 Clustering Results on the UCI-Digits and Notting-Hill Databases
  Table 4 caption: TABLE 4 Clustering Results Compared With Deep-Learning-Based Methods
    on BBC4view, UCI-Digits, Reuters, COIL20, and MITIndoor Databases
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3179556
- Affiliation of the first author: "autonomous vision group, university of t\xFCbingen\
    \ and max planck institute for intelligent systems, t\xFCbingen, germany"
  Affiliation of the last author: "autonomous vision group, university of t\xFCbingen\
    \ and max planck institute for intelligent systems, t\xFCbingen, germany"
  Figure 1 Link: articels_figures_by_rev_year\2022\KITTI_A_Novel_Dataset_and_Benchmarks_for_Urban_Scene_Understanding_in_D_and_D\figure_1.jpg
  Figure 1 caption: "KITTI-360. Our dataset contains rich sensor modalities, including\
    \ a perspective stereo camera, a pair of fisheye cameras, a Velodyne and a SICK\
    \ laser scanning unit which together enable 360 \u2218 scene perception. We release\
    \ comprehensive annotations including consistent semantic and instance labels\
    \ for every 2D image pixel and 3D point."
  Figure 10 Link: articels_figures_by_rev_year\2022\KITTI_A_Novel_Dataset_and_Benchmarks_for_Urban_Scene_Understanding_in_D_and_D\figure_10.jpg
  Figure 10 caption: Qualitative results for semantic mapping on test sequence 3 colored
    based on semantic class labels.
  Figure 2 Link: articels_figures_by_rev_year\2022\KITTI_A_Novel_Dataset_and_Benchmarks_for_Urban_Scene_Understanding_in_D_and_D\figure_2.jpg
  Figure 2 caption: Georegistered poses overlaid on OpenStreetMap.
  Figure 3 Link: articels_figures_by_rev_year\2022\KITTI_A_Novel_Dataset_and_Benchmarks_for_Urban_Scene_Understanding_in_D_and_D\figure_3.jpg
  Figure 3 caption: 'Annotation interface. Our interface consists of three main components:
    scene view (perspective views and 3D view), semantic label panel, and controllers.'
  Figure 4 Link: articels_figures_by_rev_year\2022\KITTI_A_Novel_Dataset_and_Benchmarks_for_Urban_Scene_Understanding_in_D_and_D\figure_4.jpg
  Figure 4 caption: 3D-to-2D label transfer. (a) We illustrate the 3D-to-2D projection
    of static and dynamic object annotations. A static 3D primitive is projected to
    multiple frames while a dynamic 3D object is projected only into the corresponding
    frame. (b) Factor graph representation of our model. Note that the CRF model is
    defined over all pixels and visible 3D points at a single timestamp. (c) We show
    the semantic inference result at timestamp t .
  Figure 5 Link: articels_figures_by_rev_year\2022\KITTI_A_Novel_Dataset_and_Benchmarks_for_Urban_Scene_Understanding_in_D_and_D\figure_5.jpg
  Figure 5 caption: 'Qualitative results on semantic instance segmentation transfer.
    Each subfigure shows from top-to-bottom: the input image with the projected 3D
    points and inferred semantic segmentation boundaries, the inferred semantic instance
    segmentation, as well as the confidence map of the inferred label with bright
    and dark colors indicating high and low confidence, respectively. See supplementary
    material and text for details. The first scene (1st column) contains only static
    objects while the others (2nd and 3rd columns) also contain dynamic objects.'
  Figure 6 Link: articels_figures_by_rev_year\2022\KITTI_A_Novel_Dataset_and_Benchmarks_for_Urban_Scene_Understanding_in_D_and_D\figure_6.jpg
  Figure 6 caption: "Qualitative results for 2D instance segmentation. The first row\
    \ shows inference results of Mask-RCNN with a ResNet-50 backbone while the second\
    \ row uses a ResNet-101 backbone. The ResNet-101 backbone leads to better results,\
    \ e.g., with the ResNet-50 backbone, the car occluded by the person is split into\
    \ two instances (left) and the motorcycle is not detected (middle). Note that\
    \ both variants are able to predict \u201CBuilding\u201D instances after being\
    \ trained on KITTI-360."
  Figure 7 Link: articels_figures_by_rev_year\2022\KITTI_A_Novel_Dataset_and_Benchmarks_for_Urban_Scene_Understanding_in_D_and_D\figure_7.jpg
  Figure 7 caption: Qualitative results for 3D scene perception. We establish benchmarks
    for 3D semanticinstance segmentation and 3D bounding box detection. This figure
    shows the ground truth and the prediction of a baseline method for each sub-task.
  Figure 8 Link: articels_figures_by_rev_year\2022\KITTI_A_Novel_Dataset_and_Benchmarks_for_Urban_Scene_Understanding_in_D_and_D\figure_8.jpg
  Figure 8 caption: Qualitative results for semantic scene completion evaluated at
    a distance threshold of 20cm. Green denotes completeaccurate, red denotes incompleteinaccurate
    and blue denotes points in unobserved region.
  Figure 9 Link: articels_figures_by_rev_year\2022\KITTI_A_Novel_Dataset_and_Benchmarks_for_Urban_Scene_Understanding_in_D_and_D\figure_9.jpg
  Figure 9 caption: Qualitative results for novel view appearance & semantic synthesis.
    The first row shows the GT image and novel view appearance synthesis results.
    The second row shows the corresponding semantic segmentation using PSPNet [116].
  First author gender probability: 0.89
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yiyi Liao
  Name of the last author: Andreas Geiger
  Number of Figures: 10
  Number of Tables: 6
  Number of authors: 3
  Paper title: 'KITTI-360: A Novel Dataset and Benchmarks for Urban Scene Understanding
    in 2D and 3D'
  Publication Date: 2022-06-01 00:00:00
  Table 1 caption: TABLE 1 Overview of Publicly Available Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison to Label Transfer Baselines on Semantic Segmentation
    Transfer
  Table 3 caption: TABLE 3 Semantic Instance Segmentation Transfer Ablation Evaluated
    on Static Objects
  Table 4 caption: TABLE 4 Quantitative Results for 2D & 3D Scene Understanding on
    Various Different Tasks
  Table 5 caption: TABLE 5 Quantitative Results for Novel View Appearance & Novel
    View Semantic Synthesis Using a 50% 50% Drop Rate
  Table 6 caption: TABLE 6 Quantitative Results for Semantic SLAM
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3179507
