- Affiliation of the first author: department of computing, the hong kong polytechnic
    university, hung hom, hong kong
  Affiliation of the last author: department of computing, the hong kong polytechnic
    university, hung hom, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2019\Contactless_Biometric_Identification_Using_D_Finger_Knuckle_Patterns\figure_1.jpg
  Figure 1 caption: Block diagram of 3D finger knuckle recognition system.
  Figure 10 Link: articels_figures_by_rev_year\2019\Contactless_Biometric_Identification_Using_D_Finger_Knuckle_Patterns\figure_10.jpg
  Figure 10 caption: Schematic representation of the gradient derivative features.
  Figure 2 Link: articels_figures_by_rev_year\2019\Contactless_Biometric_Identification_Using_D_Finger_Knuckle_Patterns\figure_2.jpg
  Figure 2 caption: Sample raw images acquired from different subjects.
  Figure 3 Link: articels_figures_by_rev_year\2019\Contactless_Biometric_Identification_Using_D_Finger_Knuckle_Patterns\figure_3.jpg
  Figure 3 caption: Sample segmented images acquired from different subjects.
  Figure 4 Link: articels_figures_by_rev_year\2019\Contactless_Biometric_Identification_Using_D_Finger_Knuckle_Patterns\figure_4.jpg
  Figure 4 caption: 'Sample images of surface gradient: (a) with respect to horizontal
    direction; (b) with respect to vertical direction; (c) Surface normal vectors.'
  Figure 5 Link: articels_figures_by_rev_year\2019\Contactless_Biometric_Identification_Using_D_Finger_Knuckle_Patterns\figure_5.jpg
  Figure 5 caption: '3D reconstructed images using: (a) Frankot Chellappa; (b) Poisson
    Solver.'
  Figure 6 Link: articels_figures_by_rev_year\2019\Contactless_Biometric_Identification_Using_D_Finger_Knuckle_Patterns\figure_6.jpg
  Figure 6 caption: Comparisons between different reconstruction methods.
  Figure 7 Link: articels_figures_by_rev_year\2019\Contactless_Biometric_Identification_Using_D_Finger_Knuckle_Patterns\figure_7.jpg
  Figure 7 caption: Illustration of the derivatives of gradient p.
  Figure 8 Link: articels_figures_by_rev_year\2019\Contactless_Biometric_Identification_Using_D_Finger_Knuckle_Patterns\figure_8.jpg
  Figure 8 caption: Sample binary feature images of Surface Code [30], Binary Shape
    [31] and proposed Surface Gradient Derivatives. Images in a row are from the same
    subject.
  Figure 9 Link: articels_figures_by_rev_year\2019\Contactless_Biometric_Identification_Using_D_Finger_Knuckle_Patterns\figure_9.jpg
  Figure 9 caption: Comparisons of using various matching schemes (a) 30 subjects;
    (b) 105 subjects.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Kevin H. M. Cheng
  Name of the last author: Ajay Kumar
  Number of Figures: 18
  Number of Tables: 4
  Number of authors: 2
  Paper title: Contactless Biometric Identification Using 3D Finger Knuckle Patterns
  Publication Date: 2019-03-10 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of Key Differences Between 2D and 3D Finger Knuckle
      Identification
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Pixelwise Surface Gradient Derivative Features Mapping Function
  Table 3 caption:
    table_text: TABLE 3 Comparative Computational Time (in Milliseconds)
  Table 4 caption:
    table_text: TABLE 4 Dissimilarity Scores from the Spoof Experiments
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2904232
- Affiliation of the first author: department of biomedical engineering, mathematical
    institute for data science (minds), johns hopkins university, baltimore, usa
  Affiliation of the last author: "department of electrical engineering and of computer\
    \ science, technion \u2013 israel institute of technology, haifa, israel"
  Figure 1 Link: articels_figures_by_rev_year\2019\On_MultiLayer_Basis_Pursuit_Efficient_Algorithms_and_Convolutional_Neural_Networ\figure_1.jpg
  Figure 1 caption: "Recovery error for \u03B3 1 and \u03B3 2 employing BP ( \u03BB\
    \ 1 =0 ) and Multi-Layer BP ( \u03BB 1 >0 )."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\On_MultiLayer_Basis_Pursuit_Efficient_Algorithms_and_Convolutional_Neural_Networ\figure_2.jpg
  Figure 2 caption: Comparison of different solvers for Multi-Layer Basis Pursuit
    in terms of objective value (left) and distance to optimal solution (right).
  Figure 3 Link: articels_figures_by_rev_year\2019\On_MultiLayer_Basis_Pursuit_Efficient_Algorithms_and_Convolutional_Neural_Networ\figure_3.jpg
  Figure 3 caption: "ML-ISTA (top) and ML-FISTA (bottom) evaluation for different\
    \ values of the parameter \u03BC ."
  Figure 4 Link: articels_figures_by_rev_year\2019\On_MultiLayer_Basis_Pursuit_Efficient_Algorithms_and_Convolutional_Neural_Networ\figure_4.jpg
  Figure 4 caption: ML-ISTA graph interpretation for a two-layer model as a recurrent
    neural network (top), and its unfolded version for 2 iterations (bottom).
  Figure 5 Link: articels_figures_by_rev_year\2019\On_MultiLayer_Basis_Pursuit_Efficient_Algorithms_and_Convolutional_Neural_Networ\figure_5.jpg
  Figure 5 caption: Training ML-ISTA for different number of unfoldings, on CIFAR10.
    The case of 0 unfoldings corresponds to the traditional feed-forward convolutional
    network. All networks have the same number of parameters.
  Figure 6 Link: articels_figures_by_rev_year\2019\On_MultiLayer_Basis_Pursuit_Efficient_Algorithms_and_Convolutional_Neural_Networ\figure_6.jpg
  Figure 6 caption: "Comparison of different architectures on MNIST, SVHN and CIFAR10\
    \ datasets, with a feed-forward CNN as baseline. All networks (except ML-LISTA\
    \ and \u201CAll Free\u201D) have the same number of parameters."
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jeremias Sulam
  Name of the last author: Michael Elad
  Number of Figures: 6
  Number of Tables: 1
  Number of authors: 4
  Paper title: On Multi-Layer Basis Pursuit, Efficient Algorithms and Convolutional
    Neural Networks
  Publication Date: 2019-03-10 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Classification Results for MNIST, SVHN and CIFAR10
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2904255
- Affiliation of the first author: "technicolor cesson-s\xE9vign\xE9, bretagne, france"
  Affiliation of the last author: valeo.ai, paris, france
  Figure 1 Link: articels_figures_by_rev_year\2019\ROAM_A_Rich_Object_Appearance_Model_with_Application_to_Rotoscoping\figure_1.jpg
  Figure 1 caption: 'Operations in the video post-production pipeline. First row:
    An image sequence from a video matting benchmark [17]. Second row: Roto-curves
    drawn by an artist. Third row: Annotated trimaps along the sequence. Fourth row:
    Alpha-mattes computed by [13]. Fifth row: The foreground is composited with a
    pattern background.'
  Figure 10 Link: articels_figures_by_rev_year\2019\ROAM_A_Rich_Object_Appearance_Model_with_Application_to_Rotoscoping\figure_10.jpg
  Figure 10 caption: 'Trimap results on CPC and Video Matting datasets. We present
    two non-consecutive frames of a single sequence in each row. Yellow: Roams roto-curve
    (a); Trimap (b). Alpha-mattes computed by [11] from our trimaps (c).'
  Figure 2 Link: articels_figures_by_rev_year\2019\ROAM_A_Rich_Object_Appearance_Model_with_Application_to_Rotoscoping\figure_2.jpg
  Figure 2 caption: Graphical model of Roam. Joint conditional graphical model is
    defined by energy E(X,Y,T;I) in (1). Contour node variables (white squares) form
    a closed 1-st order chain conditioned on image data (grey box). Landmark variables
    (white circles) form a shallow tree and are conditioned on node variables and
    image data. Trimap internal and external offset variables (white triangles) are
    conditioned on contour node variables and image data.
  Figure 3 Link: articels_figures_by_rev_year\2019\ROAM_A_Rich_Object_Appearance_Model_with_Application_to_Rotoscoping\figure_3.jpg
  Figure 3 caption: "Structure and notation of proposed model. (Left) A simple closed\
    \ curve X outlines the object region R(X) in the image grid \u03A9 . Several landmarks,\
    \ forming a star-shaped graphical model, are defined in this region. (Middle)\
    \ Each edge e n of the closed polyline defines a region R n that straddles R(X)\
    \ ; each node x n of the polyline is possibly connected to one or several landmarks.\
    \ (Right) Automatically generated and propagated trimap with adaptive band-width.\
    \ Observe that the fuzzy hair of the stuffed toy generates the widest region,\
    \ while regions of sharper edges are thinner."
  Figure 4 Link: articels_figures_by_rev_year\2019\ROAM_A_Rich_Object_Appearance_Model_with_Application_to_Rotoscoping\figure_4.jpg
  Figure 4 caption: 'Qualitative results on the Davis dataset: Comparisons on blackswan,
    car-roundabout, scooter-gray, and car-shadow sequences, between (from top to bottom
    for each sequence): JumpCut, Rotobrush, Roto++ and Roam.'
  Figure 5 Link: articels_figures_by_rev_year\2019\ROAM_A_Rich_Object_Appearance_Model_with_Application_to_Rotoscoping\figure_5.jpg
  Figure 5 caption: Assessing first part of the model. (a) Edge strength only; (b)
    Global colour model; (c) Edge strength combined with global colour model; (d)
    Full cost function E C , including local colour modeling, on frame 13 from surfer
    sequence.
  Figure 6 Link: articels_figures_by_rev_year\2019\ROAM_A_Rich_Object_Appearance_Model_with_Application_to_Rotoscoping\figure_6.jpg
  Figure 6 caption: Evolution of IoU for different sequences of the Davis dataset.
    For our method, the blue shadow indicates influence of varying the label space
    size for each node (set of possible moves in dynamic programming inference).
  Figure 7 Link: articels_figures_by_rev_year\2019\ROAM_A_Rich_Object_Appearance_Model_with_Application_to_Rotoscoping\figure_7.jpg
  Figure 7 caption: 'Trimaps on the Davis dataset: Our trimaps on blackswan, car-roundabout,
    scooter-gray and car-shadow.'
  Figure 8 Link: articels_figures_by_rev_year\2019\ROAM_A_Rich_Object_Appearance_Model_with_Application_to_Rotoscoping\figure_8.jpg
  Figure 8 caption: Benefit of landmarks-based modeling. Automatically detected landmarks
    (orange boxes) are accurately tracked over the sequence. This further improves
    the control of the boundary (bottom), compared to no landmarks (top).
  Figure 9 Link: articels_figures_by_rev_year\2019\ROAM_A_Rich_Object_Appearance_Model_with_Application_to_Rotoscoping\figure_9.jpg
  Figure 9 caption: Energy versus number of iterations on three sequences from the
    experimental datasets.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "Juan-Manuel P\xE9rez-R\xFAa"
  Name of the last author: "Patrick P\xE9rez"
  Number of Figures: 14
  Number of Tables: 6
  Number of authors: 6
  Paper title: 'ROAM: A Rich Object Appearance Model with Application to Rotoscoping'
  Publication Date: 2019-03-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Comparisons on Davis Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Quantitative Evaluation on Cpc Dataset ( \u2217 : Partial\
      \ Evaluation Only, See Text)"
  Table 3 caption:
    table_text: TABLE 3 Different Types of Contour Warping for Handling Long Displacements
      on a Subset of Sequences of the Davis Dataset
  Table 4 caption:
    table_text: TABLE 4 Quantitative Evaluation of Automatically Extracted Trimaps
      on [17]
  Table 5 caption:
    table_text: TABLE 5 Quantitative Evaluation of Automatically Extracted Alpha-Mattes
      from Our Trimaps on [17]
  Table 6 caption:
    table_text: TABLE 6 Timing Details for Full Configuration of Roam in Seconds-Per-Frame
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2904963
- Affiliation of the first author: beijing laboratory of intelligent information technology,
    school of computer science, beijing institute of technology, beijing, china
  Affiliation of the last author: university of kentucky, lexington, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Inferring_Salient_Objects_from_Human_Fixations\figure_1.jpg
  Figure 1 caption: Given complex scenes like (a), what are the salient objects? We
    propose the Attentive Saliency Network (ASNet) that infers the object saliency
    (b) from predicted fixation maps (c), which is consistent with human visual attention
    mechanisms.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Inferring_Salient_Objects_from_Human_Fixations\figure_2.jpg
  Figure 2 caption: '(a)-(c) Typical network architectures used in previous FP or
    SOD models (from left to right): single-stream network, multi-stream network,
    and skip-layer network. (d) Branched network adopted in [41], where FP and SOD
    are achieved via two branches sharing several bottom layers. (e) The adopted ASNet
    captures fixation map from upper layers, which is indicative of the inference
    of object saliency from lower layers. Stack of convLSTMs are adopted for iteratively
    optimizing features, while preserving spatial information. See Section 2.3 for
    a more detailed discussion.'
  Figure 3 Link: articels_figures_by_rev_year\2019\Inferring_Salient_Objects_from_Human_Fixations\figure_3.jpg
  Figure 3 caption: Architecture of the proposed ASNet. The fixation map is learned
    from the upper layers and is used by ASNet to locate the salient objects. Then,
    the fine-grained object-level saliency is gradually inferred from lower layers
    and is successively optimized via the recurrent architecture of convLSTM. Zoom-in
    for details.
  Figure 4 Link: articels_figures_by_rev_year\2019\Inferring_Salient_Objects_from_Human_Fixations\figure_4.jpg
  Figure 4 caption: 'Illustration of utilizing the recurrent mechanism of convLSTM
    for iteratively optimizing saliency representation. The figures in columns (b)-(d)
    correspond to the learned saliency features H 1 , H 2 , H 3 in different time
    steps: t=1,2,3 , and (e) shows the final salient object prediction maps. We observe
    that the convLSTM is able to gradually improve the learned saliency representations
    via suppressing the responses from the background and enhancing the foreground
    features step by step. Please see Section 3.2 for more details.'
  Figure 5 Link: articels_figures_by_rev_year\2019\Inferring_Salient_Objects_from_Human_Fixations\figure_5.jpg
  Figure 5 caption: Illustration of our convLSTM based object-level saliency optimization,
    where (b) shows detailed architecture of our convLSTM optimization module in (a).
    Zoom-in for details.
  Figure 6 Link: articels_figures_by_rev_year\2019\Inferring_Salient_Objects_from_Human_Fixations\figure_6.jpg
  Figure 6 caption: Illustration of visual attention driven, coarse-to-fine object
    saliency inference process. It can be observed that the visual attention prior
    (b) predicted by the top layer of ASNet is able to guide fine-grained object-level
    saliency estimation in lower layers in a top-down fashion. With incorporating
    finer features from different conv blocks of VGG-16 base network, the SOD results
    (c)-(g) can be gradually optimized in a coarse-to-fine manner (best viewed in
    color). Please see Section 3.2 for more details.
  Figure 7 Link: articels_figures_by_rev_year\2019\Inferring_Salient_Objects_from_Human_Fixations\figure_7.jpg
  Figure 7 caption: 'Illustration of the consistency between FP and SOD annotations
    of the training datasets. From top to bottom: image, FP annotation and SOD groundtruth.'
  Figure 8 Link: articels_figures_by_rev_year\2019\Inferring_Salient_Objects_from_Human_Fixations\figure_8.jpg
  Figure 8 caption: 'SOD results with PR-curve over three widely used benchmarks:
    ECCSD [49], HKU-IS [55] and PASCAL-S [12], where the scores from non-deep learning
    models are indicated by dashed lines. Best viewed in color.'
  Figure 9 Link: articels_figures_by_rev_year\2019\Inferring_Salient_Objects_from_Human_Fixations\figure_9.jpg
  Figure 9 caption: Qualitative results of the proposed ASNet and other 7 representative
    SOD models (MDF [55], DCL [57], ELD [58], HEDS [66], DLS [62], UCF [70], and FSN
    [64]) on sample images. For each example image, we highlight the main challenges
    and features. For our ASNet, we show both FP and SOD results. It can be observed
    that ASNet is able to infer object-level saliency maps with the guidance of visual
    attention predictions. Please see Section 4.2.2 for more details.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Wenguan Wang
  Name of the last author: Ruigang Yang
  Number of Figures: 9
  Number of Tables: 6
  Number of authors: 5
  Paper title: Inferring Salient Objects from Human Fixations
  Publication Date: 2019-03-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Statistics of the Datasets Used for Training and Testing ASNet
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Details of the Evaluation Metrics Used for the FP Task
  Table 3 caption:
    table_text: TABLE 3 Quantitative Comparison of Different FP Models on the MIT1003
      [29] Dataset (See Section 4.2.1)
  Table 4 caption:
    table_text: TABLE 4 Quantitative Comparison of Different FP Models on the PASCAL-S
      [12] Dataset (See Section 4.2.1)
  Table 5 caption:
    table_text: 'TABLE 5 The F-Measure and MAE Scores of SOD on Four Popular Datasets:
      ECCSD [49], HKU-IS [55], PASCAL-S [12], and SOD [77]'
  Table 6 caption:
    table_text: TABLE 6 Ablation Study of ASNet
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2905607
- Affiliation of the first author: department of computer engineering, sharif university
    of technology, tehran, iran
  Affiliation of the last author: department of computer engineering, sharif university
    of technology, tehran, iran
  Figure 1 Link: articels_figures_by_rev_year\2019\Learning_of_Gaussian_Processes_in_Distributed_and_Communication_Limited_Systems\figure_1.jpg
  Figure 1 caption: "The curve of \u0394\u03C3 as a function of bit rate R for standard\
    \ normal distribution. (a) Linear scale, (b) logarithmic scale."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Learning_of_Gaussian_Processes_in_Distributed_and_Communication_Limited_Systems\figure_2.jpg
  Figure 2 caption: The distortion curve of the optimal (lower bound), per-symbol
    compression and the dimension reduction methods for a 20-dimensional Gaussian
    distribution.
  Figure 3 Link: articels_figures_by_rev_year\2019\Learning_of_Gaussian_Processes_in_Distributed_and_Communication_Limited_Systems\figure_3.jpg
  Figure 3 caption: Comparison of distortions for PCA and the proposed dimension reduction.
    (a) Gaussian data with different covariance matrices in each machine. (b) Gaussian
    data with identical covariance matrices in the machines. (c) Images of MNIST for
    digit 6 in the first machine and digit 7 in the other. (d) Distributing images
    of MNIST for digits 6 and 7 randomly between two machines.
  Figure 4 Link: articels_figures_by_rev_year\2019\Learning_of_Gaussian_Processes_in_Distributed_and_Communication_Limited_Systems\figure_4.jpg
  Figure 4 caption: "Training GP regression on a 1-dimensional dataset with different\
    \ bit rates: R=1,\u2026,8 . Quantized inputs are shown by red + markers."
  Figure 5 Link: articels_figures_by_rev_year\2019\Learning_of_Gaussian_Processes_in_Distributed_and_Communication_Limited_Systems\figure_5.jpg
  Figure 5 caption: Error of regression as a function of the bit rate on Flights,
    Sarcos and Abalone datasets for GP model with the linear kernel function in (4).
  Figure 6 Link: articels_figures_by_rev_year\2019\Learning_of_Gaussian_Processes_in_Distributed_and_Communication_Limited_Systems\figure_6.jpg
  Figure 6 caption: Error of regression as a function of the bit rate on Flights,
    Sarcos, Kin40k, and Abalone datasets for GP with the squared exponential kernel
    function (63).
  Figure 7 Link: articels_figures_by_rev_year\2019\Learning_of_Gaussian_Processes_in_Distributed_and_Communication_Limited_Systems\figure_7.jpg
  Figure 7 caption: Running time per iteration on Flights dataset.
  Figure 8 Link: articels_figures_by_rev_year\2019\Learning_of_Gaussian_Processes_in_Distributed_and_Communication_Limited_Systems\figure_8.jpg
  Figure 8 caption: Error of regression on Flights and Sarcos datasets with quantization
    of both input and output.
  Figure 9 Link: articels_figures_by_rev_year\2019\Learning_of_Gaussian_Processes_in_Distributed_and_Communication_Limited_Systems\figure_9.jpg
  Figure 9 caption: Error of regression for sparse GP with quantized inducing (variational)
    variables on Kin40k dataset.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Mostafa Tavassolipour
  Name of the last author: Mohammad Taghi Manzuri Shalmani
  Number of Figures: 9
  Number of Tables: 0
  Number of authors: 3
  Paper title: Learning of Gaussian Processes in Distributed and Communication Limited
    Systems
  Publication Date: 2019-03-19 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2906207
- Affiliation of the first author: department of computer science, university of western
    ontario, london, canada
  Affiliation of the last author: department of computer science, university of western
    ontario, london, canada
  Figure 1 Link: articels_figures_by_rev_year\2019\Efficient_Graph_Cut_Optimization_for_Full_CRFs_with_Quantized_Edges\figure_1.jpg
  Figure 1 caption: Illustrates edge weights w pq . Input image is in (a), superpixels
    computed with [21] are in (b). In (c) we illustrate the weight strength between
    pixels inside the same superpixel. Brighter intensities correspond to stronger
    edge weights. In (d) we illustrate the strength of the edges that connect a pixel
    inside the superpixel highlighted with blue and the pixels inside other superpixels.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Efficient_Graph_Cut_Optimization_for_Full_CRFs_with_Quantized_Edges\figure_2.jpg
  Figure 2 caption: "Comparison of Gaussian edge model [1] with our quantized edge\
    \ model. On the y axis we plot percent average relative difference of our energy\
    \ from energy of the Gaussian edge CRF model. Increasing number of superpixels\
    \ and increasing \u03B2 parameters in Eqs. (2) and (3) result in smaller relative\
    \ percentage difference."
  Figure 3 Link: articels_figures_by_rev_year\2019\Efficient_Graph_Cut_Optimization_for_Full_CRFs_with_Quantized_Edges\figure_3.jpg
  Figure 3 caption: (a,b,c,d) show the same superpixel under four different binary
    labelings. These labelings have different regularization cost under length regularizer
    [23], but the same cost under our model.
  Figure 4 Link: articels_figures_by_rev_year\2019\Efficient_Graph_Cut_Optimization_for_Full_CRFs_with_Quantized_Edges\figure_4.jpg
  Figure 4 caption: "The transformation from the binary energy in the pixel domain\
    \ to the multi-label energy in the superpixel domain. The input image is partitioned\
    \ into four superpixels, each containing 4, 10, 8 and 11 pixels, respectively.\
    \ Superpixel 1 can be assigned labels from the set 0,1,\u2026,4 , and similarly\
    \ for the other three superpixels. We vertically stack the pixels in each superpixel\
    \ in order of their preference to label 1 in the original binary problem. Those\
    \ that prefer label 1 the most are on the bottom. Superpixel 1 is assigned to\
    \ state 2. This means that the 2 of its pixels counting from the bottom are assigned\
    \ to label 1, and the rest to label 0 in the original binary problem. Pixels assigned\
    \ label 1 in the original binary problem are shown with darker shade in each superpixel.\
    \ Similarly for the other superpixels. All pixels in superpixel 4 are assigned\
    \ to label 1, which corresponds to the largest label, namely label 11 that superpixel\
    \ 4 can be assigned to."
  Figure 5 Link: articels_figures_by_rev_year\2019\Efficient_Graph_Cut_Optimization_for_Full_CRFs_with_Quantized_Edges\figure_5.jpg
  Figure 5 caption: 'Illustrates formation of new superpixels: (a) for original superpixels
    shown with different colors. The pixels inside these superpixels have different
    labels, shown with greek letters; (b) shows the new superpixels formed by breaking
    the original four superpixels in (a) according to the current labeling.'
  Figure 6 Link: articels_figures_by_rev_year\2019\Efficient_Graph_Cut_Optimization_for_Full_CRFs_with_Quantized_Edges\figure_6.jpg
  Figure 6 caption: 'Comparison of our method, superpixel ICM, and mean field. Left:
    For binary Full-CRF; right: Multilabel Full-CRF.'
  Figure 7 Link: articels_figures_by_rev_year\2019\Efficient_Graph_Cut_Optimization_for_Full_CRFs_with_Quantized_Edges\figure_7.jpg
  Figure 7 caption: A sample of results.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.99
  Name of the first author: Olga Veksler
  Name of the last author: Olga Veksler
  Number of Figures: 7
  Number of Tables: 1
  Number of authors: 1
  Paper title: Efficient Graph Cut Optimization for Full CRFs with Quantized Edges
  Publication Date: 2019-03-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Results on PASCAL VOC 2012 Test Data, Using the IOU Measure
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2906204
- Affiliation of the first author: computer science department, boston university,
    boston, usa
  Affiliation of the last author: school of electrical and electronic engineering,
    nanyang technological university, singapore
  Figure 1 Link: articels_figures_by_rev_year\2019\MotionGuided_Cascaded_Refinement_Network_for_Video_Object_Segmentation\figure_1.jpg
  Figure 1 caption: Examples of our method. (a) Input frame and the initial active
    contour. (b) Optical flow. (c) Segmentation by evolving the active contour on
    (b). (d) Final results with (c) as guidance.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\MotionGuided_Cascaded_Refinement_Network_for_Video_Object_Segmentation\figure_2.jpg
  Figure 2 caption: An overview of the proposed method for single-instance VOS. For
    each frame, optical flow is first estimated. Then we use the mask of the last
    frame to initialize an active contour (shown as the blue curve in Active Contour
    Model) on the optical flow, and evolve it N steps to minimize an energy function
    to coarsely segment the object. Subsequently, the coarse object mask is used as
    guidance to help the Cascaded Refinement Network to accurately segment the target
    object instance. To begin the process, user annotation is used to initialize frame
    0 in semi-supervised VOS, and a predefined rectangle is used to initialize frame
    0 in unsupervised VOS.
  Figure 3 Link: articels_figures_by_rev_year\2019\MotionGuided_Cascaded_Refinement_Network_for_Video_Object_Segmentation\figure_3.jpg
  Figure 3 caption: "Network overview. (a) An overview of the Cascaded Refinement\
    \ Network (CRN). The \u201C0-tensor\u201D below RM 5 is a 0-padding tensor used\
    \ as an input for RM 5. (b) Details of the Refining Module RM 3. All Refining\
    \ Modules share the same structure. (c) Details of the Single-channel Residual\
    \ Attention Module(SRAM) used in RM."
  Figure 4 Link: articels_figures_by_rev_year\2019\MotionGuided_Cascaded_Refinement_Network_for_Video_Object_Segmentation\figure_4.jpg
  Figure 4 caption: "An overview of the method for multi-instance VOS. For a current\
    \ frame t , we first segment all the instances as foreground using the method\
    \ in the previous section. Pixels within the foreground are then assigned with\
    \ an instance label according to their feature similarity with instances segmented\
    \ in the previous frame t\u22121 ."
  Figure 5 Link: articels_figures_by_rev_year\2019\MotionGuided_Cascaded_Refinement_Network_for_Video_Object_Segmentation\figure_5.jpg
  Figure 5 caption: Qualitative results. (a) and (b) are results for single-instance
    VOS. (c) are results for semi-supervised multi-instance VOS. The first row in
    (a) and (c) are user annotations, and the first row of (b) are the predefined
    rectangle (the yellow rectangle) which is used as initial contour for the first
    frame.
  Figure 6 Link: articels_figures_by_rev_year\2019\MotionGuided_Cascaded_Refinement_Network_for_Video_Object_Segmentation\figure_6.jpg
  Figure 6 caption: "mIoU on DAVIS2016 for different \u03BB 1 and \u03BB 2 in the\
    \ active contour model."
  Figure 7 Link: articels_figures_by_rev_year\2019\MotionGuided_Cascaded_Refinement_Network_for_Video_Object_Segmentation\figure_7.jpg
  Figure 7 caption: Performance for different feature dimensions in spatio-temporal
    instance embedding.
  Figure 8 Link: articels_figures_by_rev_year\2019\MotionGuided_Cascaded_Refinement_Network_for_Video_Object_Segmentation\figure_8.jpg
  Figure 8 caption: Some failures of our method for multi-instance VOS.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.68
  Name of the first author: Ping Hu
  Name of the last author: Yap-Peng Tan
  Number of Figures: 8
  Number of Tables: 10
  Number of authors: 5
  Paper title: Motion-Guided Cascaded Refinement Network for Video Object Segmentation
  Publication Date: 2019-03-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance on the Val Val Split of DAVIS2016.
  Table 10 caption:
    table_text: TABLE 10 Effectiveness of Different Components of the Loss
  Table 2 caption:
    table_text: TABLE 2 Comparisons on YoutubeObjects
  Table 3 caption:
    table_text: TABLE 3 mIoU for Different Iterations Number N N in the Active Contour
      Model
  Table 4 caption:
    table_text: TABLE 4 Performance for Different Training Phase of CRN and the Baseline
  Table 5 caption:
    table_text: TABLE 5 Performance with CRN of Different Iterations in the Online
      Training
  Table 6 caption:
    table_text: TABLE 6 mIoU on DAVIS2016 for Introducing the Coarse Masks as Guidance
      for Different Refining Modules
  Table 7 caption:
    table_text: TABLE 7 Performance for Mask Propagation via Warping the Optical Flow
  Table 8 caption:
    table_text: TABLE 8 Multi-Instance VOS Performance on the Val Val Split of DAVIS17
  Table 9 caption:
    table_text: TABLE 9 Effectiveness of Different Components in the Proposed Method
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2906175
- Affiliation of the first author: school of information and communication engineering,
    dalian university of technology, dalian, p.r. china
  Affiliation of the last author: school of information and communication engineering,
    dalian university of technology, dalian, p.r. china
  Figure 1 Link: articels_figures_by_rev_year\2019\Defocus_Blur_Detection_via_MultiStream_BottomTopBottom_Network\figure_1.jpg
  Figure 1 caption: 'A challenging example for defocus blur detection (DBD). (a)-(f):
    source image, magnified rectangular regions (MRRs), ground truth (GT), DBDF [14],
    DHCF [12], and our DBD map.'
  Figure 10 Link: articels_figures_by_rev_year\2019\Defocus_Blur_Detection_via_MultiStream_BottomTopBottom_Network\figure_10.jpg
  Figure 10 caption: Representative images for generating simulated defocus images.
  Figure 2 Link: articels_figures_by_rev_year\2019\Defocus_Blur_Detection_via_MultiStream_BottomTopBottom_Network\figure_2.jpg
  Figure 2 caption: An example of multi-scale blur perception. This figure illustrates
    that the blur confidence is highly related to scales. A clear image patch (e.g.,
    scale 1) can be regarded as blurry depending on the size of the patch (e.g., scale
    3), and vice versa.
  Figure 3 Link: articels_figures_by_rev_year\2019\Defocus_Blur_Detection_via_MultiStream_BottomTopBottom_Network\figure_3.jpg
  Figure 3 caption: The pipeline of our DBD algorithm. Each colorful box is considered
    as a feature block. The arrows between blocks indicate the information stream.
    Given an input image, its multi-scale versions generated by the resize operation
    are first encoded in the bottom-top stream by a modified VGG16 model [28], respectively.
    Then, the integration of bottom-top and top-bottom streams is performed by feedback
    and forward information combination modules (FFICs). After that, CRLNet is designed
    to gradually refine the preceding blur detection maps from the small scale to
    the original scale through learning the residual.
  Figure 4 Link: articels_figures_by_rev_year\2019\Defocus_Blur_Detection_via_MultiStream_BottomTopBottom_Network\figure_4.jpg
  Figure 4 caption: "The architecture of BTBNet. Information of an input image I n\
    \ passes from the bottom layer to the top layer with a series of feature blocks,\
    \ thereby resulting in high-level semantic information. Then, high-level semantic\
    \ information passes from the top layers down and is integrated with high-resolution\
    \ low-level cues by the FFIC module, ultimately producing the DBD map M n through\
    \ multi-step DBD maps integration. FFIC is composed of three convolutional layers\
    \ and a element-wise addition. \u2295 stands for the element-wise addition."
  Figure 5 Link: articels_figures_by_rev_year\2019\Defocus_Blur_Detection_via_MultiStream_BottomTopBottom_Network\figure_5.jpg
  Figure 5 caption: The architecture of FNet and RRNet models.
  Figure 6 Link: articels_figures_by_rev_year\2019\Defocus_Blur_Detection_via_MultiStream_BottomTopBottom_Network\figure_6.jpg
  Figure 6 caption: 'Comparison of multi-stream DBD maps-based refinement results.
    (a)-(g): Source image, soft weight-based network (SWNet) [39], FRRNet, MSCRLNet,
    SCCRLNet, CRLNet, and ground truth (GT). It can be seen from that CRLNet produces
    the most sharpest edge and cleanest background.'
  Figure 7 Link: articels_figures_by_rev_year\2019\Defocus_Blur_Detection_via_MultiStream_BottomTopBottom_Network\figure_7.jpg
  Figure 7 caption: "The architecture of CRLNet. The DBD map and image with scale\
    \ n (n=1,2,\u2026,N) are first concatenated into a single 4-channel feature map.\
    \ Then, this map is fed to a series of Conv and ReLU layers. The output DBD map\
    \ and the residue of the current RLNet are integrated with the tail and middle\
    \ of the next RLNet by element-wise addition operation, making the next step learn\
    \ the residual. Interpolation is used for resolution matching. For step 1, the\
    \ RLNet refines the full DBD map. Then, the other steps learn the residual map\
    \ with shared parameters."
  Figure 8 Link: articels_figures_by_rev_year\2019\Defocus_Blur_Detection_via_MultiStream_BottomTopBottom_Network\figure_8.jpg
  Figure 8 caption: The architecture of SCCRLNet and MSCRLNet models. The RLNet in
    blue rectangle box has the same structure with the RLNet in orange rectangle box
    except for its 3-channel concatenated layer.
  Figure 9 Link: articels_figures_by_rev_year\2019\Defocus_Blur_Detection_via_MultiStream_BottomTopBottom_Network\figure_9.jpg
  Figure 9 caption: Sampled images with labeled ground truths in our dataset.
  First author gender probability: 0.96
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Wenda Zhao
  Name of the last author: Huchuan Lu
  Number of Figures: 16
  Number of Tables: 6
  Number of authors: 4
  Paper title: Defocus Blur Detection via Multi-Stream Bottom-Top-Bottom Network
  Publication Date: 2019-03-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Comparison of F-Measure and MAE Scores
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Effectiveness Analysis of BTBNet Using F-Measure and MAE Values
  Table 3 caption:
    table_text: TABLE 3 Effectiveness Analysis of CRLNet Using F-Measure and MAE Values
  Table 4 caption:
    table_text: TABLE 4 Comparison of Quantitative Results Including F-Measure and
      MAE Values on Shis Dataset
  Table 5 caption:
    table_text: TABLE 5 Comparison of Quantitative Results Including F-Measure and
      MAE Values on Our Dataset
  Table 6 caption:
    table_text: TABLE 6 Ablation Analysis for New Datasets Using F-Measure and MAE
      Values
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2906588
- Affiliation of the first author: department of computer science and engineering,
    ohio state university, columbus, usa
  Affiliation of the last author: department of computer science and engineering,
    ohio state university, columbus, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Back_to_the_Future_Radial_Basis_Function_Network_Revisited\figure_1.jpg
  Figure 1 caption: "Contours and classification boundaries for f 1 (left) and f 2\
    \ (right). Two labeled points x + and x \u2212 , grey unlabeled points are sampled\
    \ from p . Note that \u2225 f 1 \u2225 H =\u2225 f 2 \u2225 H , however \u2225\
    \ f 1 \u2225 H W \u226B\u2225 f 2 \u2225 H W ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Back_to_the_Future_Radial_Basis_Function_Network_Revisited\figure_2.jpg
  Figure 2 caption: 'Left: k=2 ; Middle: k=10 ; Right: k=100 .'
  Figure 3 Link: articels_figures_by_rev_year\2019\Back_to_the_Future_Radial_Basis_Function_Network_Revisited\figure_3.jpg
  Figure 3 caption: MNIST, MNIST-rand, MNIST-img.
  Figure 4 Link: articels_figures_by_rev_year\2019\Back_to_the_Future_Radial_Basis_Function_Network_Revisited\figure_4.jpg
  Figure 4 caption: k -means centers represented as images for MNIST (left); MNIST-rand
    (center); MNIST-img (right).
  Figure 5 Link: articels_figures_by_rev_year\2019\Back_to_the_Future_Radial_Basis_Function_Network_Revisited\figure_5.jpg
  Figure 5 caption: The classification error of supervised, semi-supervised, k -means
    RBF networks, with different number of labeled points.
  Figure 6 Link: articels_figures_by_rev_year\2019\Back_to_the_Future_Radial_Basis_Function_Network_Revisited\figure_6.jpg
  Figure 6 caption: The regularization effect of k -means, based on the classification
    error of RBFs network on MNIST-rand.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.86
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Qichao Que
  Name of the last author: Mikhail Belkin
  Number of Figures: 6
  Number of Tables: 3
  Number of authors: 2
  Paper title: 'Back to the Future: Radial Basis Function Network Revisited'
  Publication Date: 2019-03-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Classification Errors (%) for Supervised Learning with Whole
      Training Data Using K-SVM, K-RLSC, RBF Network with Hinge Loss and Least-Square
      Loss
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Classification Errors (%) for Semi-Supervised Learning, with
      100 Labeled Points, Using K-SVM, K-RLSC, Ridge RBF Network with Hinge Loss and
      Least-Square Loss
  Table 3 caption:
    table_text: TABLE 3 Classification Errors (%) for K-RLSC, RBF Network with 1000
      Randomly Selected Points as Centers, k k-Means Centers, Unweighted and Weighted,
      k=1000 k=1000
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2906594
- Affiliation of the first author: school of computer science and engineering, nanjing
    university of science and technology, nanjing, china
  Affiliation of the last author: noahs ark lab, huawei, shenzhen, china
  Figure 1 Link: articels_figures_by_rev_year\2019\Social_AnchorUnit_Graph_Regularized_Tensor_Completion_for_LargeScale_Image_Retag\figure_1.jpg
  Figure 1 caption: Conventional methods refine tags of images by exploring the inter-association
    among all data. This comes at a price of increased computational cost when faced
    with a larger number of images. The proposed method refines tag on a smaller-scale
    anchor-unit set, and then assigns tags to all non-anchor images in an efficient
    way, regardless of the data scale.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Social_AnchorUnit_Graph_Regularized_Tensor_Completion_for_LargeScale_Image_Retag\figure_2.jpg
  Figure 2 caption: 'The whole framework of SUGAR-TC for social image retagging. (a)
    Social anchor-unit graph construction: Obtain anchor units and construct anchor-unit
    graph with the inter- and intra- adjacency edges. (b) SUGAR tensor completion:
    refine tags of anchor images. (c) Anchor-aware tag assignment: assign tags to
    non-anchor images via the inter-association between non-anchor units and anchor
    units.'
  Figure 3 Link: articels_figures_by_rev_year\2019\Social_AnchorUnit_Graph_Regularized_Tensor_Completion_for_LargeScale_Image_Retag\figure_3.jpg
  Figure 3 caption: The comparisons of detailed F-scores of different methods. For
    those concepts denoted with the black bounding box, the proposed SUGAR-TC shows
    remarkable improvements. Best view in color.
  Figure 4 Link: articels_figures_by_rev_year\2019\Social_AnchorUnit_Graph_Regularized_Tensor_Completion_for_LargeScale_Image_Retag\figure_4.jpg
  Figure 4 caption: Some results of image retagging obtained by SUGAR-TC.
  Figure 5 Link: articels_figures_by_rev_year\2019\Social_AnchorUnit_Graph_Regularized_Tensor_Completion_for_LargeScale_Image_Retag\figure_5.jpg
  Figure 5 caption: The curves of F-scores obtained by varying one parameter with
    the other parameters fixed to be the default values.
  Figure 6 Link: articels_figures_by_rev_year\2019\Social_AnchorUnit_Graph_Regularized_Tensor_Completion_for_LargeScale_Image_Retag\figure_6.jpg
  Figure 6 caption: MAP of different methods for tag-based image retrieval.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.93
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Jinhui Tang
  Name of the last author: Qi Tian
  Number of Figures: 6
  Number of Tables: 3
  Number of authors: 5
  Paper title: Social Anchor-Unit Graph Regularized Tensor Completion for Large-Scale
    Image Retagging
  Publication Date: 2019-03-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Notations and Definitions
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average F-Scores, Precisions, and Recalls Obtained by Different
      Methods for Image Retagging
  Table 3 caption:
    table_text: TABLE 3 Detailed Comparisons of Computational Time (Hour) and Memory
      (GB) on NUS-WIDE-128
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2906603
