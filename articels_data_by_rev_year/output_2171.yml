- Affiliation of the first author: school of computer science and engineering, and
    key laboratory of computer network and information integration (southeast university),
    ministry of education, southeast university, nanjing, china
  Affiliation of the last author: school of computer science and engineering, and
    key laboratory of computer network and information integration (southeast university),
    ministry of education, southeast university, nanjing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\ReWeighting_Large_Margin_Label_Distribution_Learning_for_Classification\figure_1.jpg
  Figure 1 caption: "Illustration of label ambiguity and label distribution. Fig.\
    \ 1a shows a multi-label scene image with little \u201CSun\u201D and mostly \u201C\
    Water\u201D. Fig. 1b shows an image from the JAFFE database [3] with a ground-truth\
    \ single-label \u201CANG.\u201D. The image is a mixture of emotions by different\
    \ relevance. The label distribution of Fig. 1a is got from [6], and the label\
    \ distribution of Fig. 1b is from the mean ratings from 60 annotators [5]."
  Figure 10 Link: articels_figures_by_rev_year\2021\ReWeighting_Large_Margin_Label_Distribution_Learning_for_Classification\figure_10.jpg
  Figure 10 caption: Convergence of RWLM-LDL on Alpha in SLL and MLL cases.
  Figure 2 Link: articels_figures_by_rev_year\2021\ReWeighting_Large_Margin_Label_Distribution_Learning_for_Classification\figure_2.jpg
  Figure 2 caption: Illustration of multi-label classification in the framework of
    LDL, where d is the given label distribution function, and g is an LDL model.
  Figure 3 Link: articels_figures_by_rev_year\2021\ReWeighting_Large_Margin_Label_Distribution_Learning_for_Classification\figure_3.jpg
  Figure 3 caption: "Examples to illustrate the inconsistency, where \u2113 1 denotes\
    \ the L 1 -norm loss. The red bar represents the ground-truth label distribution,\
    \ and the blue bar denotes the learned label distribution. Figs. 3a and 3b demonstrate\
    \ the case of SLL, and Figs. 3c and 3d manifest the case of MLL. The cases of\
    \ Figs. 3a and 3c have small classification losses, but the cases of Figs. 3b\
    \ and 3d have small L 1 -norm losses."
  Figure 4 Link: articels_figures_by_rev_year\2021\ReWeighting_Large_Margin_Label_Distribution_Learning_for_Classification\figure_4.jpg
  Figure 4 caption: Illustration of the re-weighting LDL, where the sizes of the markers
    indicate the scales of the weights. Note that the instances with evenly-distributed
    label distributions are assigned with higher weights and have smaller losses in
    the re-weighting LDL.
  Figure 5 Link: articels_figures_by_rev_year\2021\ReWeighting_Large_Margin_Label_Distribution_Learning_for_Classification\figure_5.jpg
  Figure 5 caption: Visualization of the advantages of re-weighting and large margin
    classifier in the case of binary classification. The solid lines and the dash
    lines are separating hyper-planes and marginal hyper-planes, respectively. Fig.
    5a illustrates a large margin classifier with two misclassified points highlighted
    in red. Fig. 5b showcases the advantage of re-weighting. The sizes of the markers
    indicate the scales of weights. Fig. 5c illuminates the advantage of large margin.
    The points outside the marginal hyper-planes have already been correctly classified.
  Figure 6 Link: articels_figures_by_rev_year\2021\ReWeighting_Large_Margin_Label_Distribution_Learning_for_Classification\figure_6.jpg
  Figure 6 caption: CD diagrams of the Bonferroni-Dunn tests, where RWLM-LDL is set
    as the control algorithm. The algorithms not connected with RWLM-LDL are considered
    to have significantly different performance from RWLM-LDL.
  Figure 7 Link: articels_figures_by_rev_year\2021\ReWeighting_Large_Margin_Label_Distribution_Learning_for_Classification\figure_7.jpg
  Figure 7 caption: Performance comparison among LDL- ell 1 , LDL- ell 2 , LDL-J,
    and LDL-KL in terms of 01 loss and ranking loss.
  Figure 8 Link: articels_figures_by_rev_year\2021\ReWeighting_Large_Margin_Label_Distribution_Learning_for_Classification\figure_8.jpg
  Figure 8 caption: Influence of rho in terms of 01 loss and five MLL metrics.
  Figure 9 Link: articels_figures_by_rev_year\2021\ReWeighting_Large_Margin_Label_Distribution_Learning_for_Classification\figure_9.jpg
  Figure 9 caption: Influence of lambda 1 and lambda 2 in terms of each metric on
    SJAFFE.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.96
  Name of the first author: Jing Wang
  Name of the last author: Hui Xue
  Number of Figures: 11
  Number of Tables: 7
  Number of authors: 3
  Paper title: Re-Weighting Large Margin Label Distribution Learning for Classification
  Publication Date: 2021-05-21 00:00:00
  Table 1 caption: TABLE 1 Summary of the Mainly Used Notations
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Statistics of the Experimental Datasets
  Table 3 caption: "TABLE 3 SLL Predictive Results (mean \xB1 \xB1std) on 16 Datasets,\
    \ Where \u2022 \u2218 \u2218 Indicates Whether RWLM-LDL is Statistically SuperiorInferior\
    \ to the Comparing Algorithms (Pairwise t t-Test at 0.05 Significance Level),\
    \ and the wintielose (w.t.l.) Counts are Summarized in the Last Row"
  Table 4 caption: "TABLE 4 MLL Predictive Performance (mean \xB1 \xB1std.(Rank))\
    \ of Each Comparing Algorithm on 12 Selected Datasets in Terms of Five MLL Metrics"
  Table 5 caption: TABLE 5 Results of the Friedman Test
  Table 6 caption: TABLE 6 Results (wintielose[ p p-value]) of the Wilcoxon Signed-Rank
    Test in Terms of 01 Loss and Five MLL Metrics (at 0.05 Confidence Level)
  Table 7 caption: "TABLE 7 Performance (mean \xB1 \xB1 std) Comparison for the Algorithms\
    \ Learning the Label Distribution and Multi-Label on RAFML"
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3082623
- Affiliation of the first author: college of mathematics and computer science, fuzhou
    university, fuzhou, china
  Affiliation of the last author: key laboratory of machine perception (moe), school
    of eecs, peking university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Learning_Deep_Sparse_Regularizers_With_Applications_to_MultiView_Clustering_and_\figure_1.jpg
  Figure 1 caption: "Illustration of some popular hand-crafted sparse regularizers\
    \ (For \u2113 p -norm, p=0.5 . For all penalties, \u03BB=1.0 , \u03B3=0.5 ). All\
    \ these sparse regularizers share some common properties: nonconvex and non-decreasing\
    \ on (0,\u221E) ."
  Figure 10 Link: articels_figures_by_rev_year\2021\Learning_Deep_Sparse_Regularizers_With_Applications_to_MultiView_Clustering_and_\figure_10.jpg
  Figure 10 caption: Relationship between clustering performance (ACC, ARI, and NMI)
    and block number in lbrace 2, 4, ldots, 24rbrace of the proposed DSRL method.
  Figure 2 Link: articels_figures_by_rev_year\2021\Learning_Deep_Sparse_Regularizers_With_Applications_to_MultiView_Clustering_and_\figure_2.jpg
  Figure 2 caption: Framework of the proposed DSRL method. It consists of multiple
    unfolded blocks in which a basic block is made up of several differentiable units,
    as demonstrated in the blue shapes.
  Figure 3 Link: articels_figures_by_rev_year\2021\Learning_Deep_Sparse_Regularizers_With_Applications_to_MultiView_Clustering_and_\figure_3.jpg
  Figure 3 caption: Several sample images from the test image datasets.
  Figure 4 Link: articels_figures_by_rev_year\2021\Learning_Deep_Sparse_Regularizers_With_Applications_to_MultiView_Clustering_and_\figure_4.jpg
  Figure 4 caption: "The learned sparse regularizer g(x)= \u222B x 0 ( \u03BE \u2212\
    1 ( \u03B8 1 , \u03B8 2 ) (y)\u2212y)dy on test datasets for multi-view clustering.\
    \ All learned parameters \u03BE ( \u03B8 1 , \u03B8 2 ) (x) of the activation\
    \ functions are listed under each subfigure, with the points x=\xB1 w 1 ( b 2\
    \ \u2212 b 1 ) marked in red."
  Figure 5 Link: articels_figures_by_rev_year\2021\Learning_Deep_Sparse_Regularizers_With_Applications_to_MultiView_Clustering_and_\figure_5.jpg
  Figure 5 caption: Visualization for multi-view clustering results in dataset MNIST.
    Here, the high-dimensional input data are projected onto a two-dimensional subspace
    using t-SNE, then the corresponding data points are marked in varying colors according
    to their predictive labels.
  Figure 6 Link: articels_figures_by_rev_year\2021\Learning_Deep_Sparse_Regularizers_With_Applications_to_MultiView_Clustering_and_\figure_6.jpg
  Figure 6 caption: "The learned sparse regularizer g(x)= \u222B x 0 ( \u03BE \u2212\
    1 ( \u03B8 1 , \u03B8 2 ) (y)\u2212y)dy on test datasets for multi-view semi-supervised\
    \ classification. All learned parameters \u03BE ( \u03B8 1 , \u03B8 2 ) (x) of\
    \ the activation functions are listed under each subfigure, with the points x=\xB1\
    \ w 1 ( b 2 \u2212 b 1 ) marked in red."
  Figure 7 Link: articels_figures_by_rev_year\2021\Learning_Deep_Sparse_Regularizers_With_Applications_to_MultiView_Clustering_and_\figure_7.jpg
  Figure 7 caption: "Performance of all compared methods in semi-supervised classification\
    \ tasks as the ratio of labeled data ranges in 0.05,0.10,\u2026,0.80 ."
  Figure 8 Link: articels_figures_by_rev_year\2021\Learning_Deep_Sparse_Regularizers_With_Applications_to_MultiView_Clustering_and_\figure_8.jpg
  Figure 8 caption: Runtime comparison for all sparse surrogates and DSRL in (a) clustering
    and (b) semi-supervised classification.
  Figure 9 Link: articels_figures_by_rev_year\2021\Learning_Deep_Sparse_Regularizers_With_Applications_to_MultiView_Clustering_and_\figure_9.jpg
  Figure 9 caption: The convergence curves of the proposed DSRL method on all test
    datasets.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.64
  Name of the first author: Shiping Wang
  Name of the last author: Zhouchen Lin
  Number of Figures: 11
  Number of Tables: 6
  Number of authors: 4
  Paper title: Learning Deep Sparse Regularizers With Applications to Multi-View Clustering
    and Semi-Supervised Classification
  Publication Date: 2021-05-21 00:00:00
  Table 1 caption: "TABLE 1 Several Specified Definitions of g(\u22C5) g(\xB7) for\
    \ Sparse Surrogates"
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 A Brief Description of the Test Datasets
  Table 3 caption: TABLE 3 Clustering Accuracy (Mean% and Standard Deviation%) and
    Sparsity (Proportion of Near Zero Outputs) of the Proposed DSRL Method and Hand-Crafted
    Sparse Surrogates g(x) g(x) Defined in Table 1, Where the Best Performance is
    Highlighted in Bold
  Table 4 caption: TABLE 4 Clustering Accuracy (Mean% and Standard Deviation%) of
    All Compared Multi-View Clustering Methods, Where the Best Performance is Highlighted
    in Bold and the Second Best is Underlined
  Table 5 caption: TABLE 5 Classification Accuracy (Mean% and Standard Deviation%)
    and Sparsity (Proportion of Near Zero Outputs) of the Proposed Method DSRL and
    Compared Hand-Crafted Sparse Surrogates g(x) g(x) Defined in Table 1, Where the
    Best Performance is Highlighted in Bold
  Table 6 caption: TABLE 6 Classification Accuracy (Mean% and Standard Deviation%)
    of All Compared Semi-Supervised Classification Methods, Where the Best Performance
    is Highlighted in Bold and the Second Best is Underlined
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3082632
- Affiliation of the first author: shanghai jiaotong university, shanghai, china
  Affiliation of the last author: seattle research center, oppo research usa, bellevue,
    wa, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\K_Shot_Contrastive_Learning_of_Visual_Features_With_Multiple_Instance_Augmentati\figure_1.jpg
  Figure 1 caption: "The pipline of the proposed K -Shot Contrastive Learning (KSCL).\
    \ For each instance x n , an instance subspace S n is spanned by the \u2113 2\
    \ -normalized embeddings v k n of K -shot augmentations x k n on a unit hyper-sphere.\
    \ A given query embedding v of unit length is projected onto the subspace of each\
    \ instance, resulting in the projection length \u2225 \u03A0 n (v) \u2225 2 to\
    \ measure the probability q(n|v) of the query belonging to the associated instance\
    \ class. The projection length also gives the cosine similarity of the acute angle\
    \ \u03B8 vn between the query vector v and the instance subspace S n ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\K_Shot_Contrastive_Learning_of_Visual_Features_With_Multiple_Instance_Augmentati\figure_2.jpg
  Figure 2 caption: The learned basis in an instance subspace. Each of the first five
    columns is an augmented image from an instance, and the last column is the basis
    images each of which is synthesized as a linear combination of the five augmented
    images weighted by the inner product with the corresponding eigenvector in the
    embedding space.
  Figure 3 Link: articels_figures_by_rev_year\2021\K_Shot_Contrastive_Learning_of_Visual_Features_With_Multiple_Instance_Augmentati\figure_3.jpg
  Figure 3 caption: Stability of temperature.
  Figure 4 Link: articels_figures_by_rev_year\2021\K_Shot_Contrastive_Learning_of_Visual_Features_With_Multiple_Instance_Augmentati\figure_4.jpg
  Figure 4 caption: The instances augmented by different types of augmentations. The
    top row five images are augmented with the same spatial augmentations and different
    random color augmentations. The bottom row five images are augmented with the
    same color augmentations and different random spatial augmentations.
  Figure 5 Link: articels_figures_by_rev_year\2021\K_Shot_Contrastive_Learning_of_Visual_Features_With_Multiple_Instance_Augmentati\figure_5.jpg
  Figure 5 caption: The top-1 accuracy by applying different types of augmentations.
    The first bar and the last bar correspond to KSCL and MoCo v2 baseline. The second
    and third bar correspond to KSCL with five fixed color augmentations and five
    fixed spatial augmentations, respectively.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Haohang Xu
  Name of the last author: Guo-Jun Qi
  Number of Figures: 5
  Number of Tables: 5
  Number of authors: 3
  Paper title: K -Shot Contrastive Learning of Visual Features With Multiple Instance
    Augmentations
  Publication Date: 2021-05-21 00:00:00
  Table 1 caption: TABLE 1 The Top-1 Accuracy of Different Models on ImageNet
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The Top-1 Accuracy of Different Models on ImageNet
  Table 3 caption: "TABLE 3 The Top-1 Accuracy of the Proposed KSCL With Varying K\
    \ Ks and \u03C1 \u03C1s Under 200 Epochs of Pretraining on ImageNet"
  Table 4 caption: "TABLE 4 The Comparison Between the Proposed KSCL ( K=5 K=5 and\
    \ \u03C1=40% \u03C1=40%) and the MoCo Models"
  Table 5 caption: TABLE 5 Transfer Learning Results on COCO Instance Segmentation
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3082567
- Affiliation of the first author: research school of engineering, australian national
    university, canberra, act, australia
  Affiliation of the last author: school of computing, australian national university,
    canberra, act, australia
  Figure 1 Link: articels_figures_by_rev_year\2021\Cost_Volume_Pyramid_Based_Depth_Inference_for_MultiView_Stereo\figure_1.jpg
  Figure 1 caption: Network structure. Reference and source images are first downsampled
    to form an image pyramid. We apply feature extraction network to all levels of
    images to extract feature maps. We then build the cost volume pyramid in a coarse-to-fine
    manner. Specifically, we start with the construction of a cost volume corresponding
    to coarsest image resolution followed by building partial cost volumes iteratively
    for depth residual estimation in order to achieve depth map D= D 0 for I . Please
    refer to Fig. 2 for details about re-projection, feature fetching, building cost
    volume and Figs. 4 and 5 for supervision.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Cost_Volume_Pyramid_Based_Depth_Inference_for_MultiView_Stereo\figure_2.jpg
  Figure 2 caption: 'Reprojection, feature fetching and building cost volume. Left:
    We define M depth hypotheses for each pixel ( u,v ) in the reference view. By
    projecting them to each source view, we can fetch M corresponding features. Right:
    For each depth hypothesis, the matching cost is the variance of fetched features
    across source views and the reference view. The cost volume C l+1 defines matching
    costs for all depth hypotheses of all pixels in the reference view.'
  Figure 3 Link: articels_figures_by_rev_year\2021\Cost_Volume_Pyramid_Based_Depth_Inference_for_MultiView_Stereo\figure_3.jpg
  Figure 3 caption: Interpolation of two sampling points from four feature points
    in source view. (a) Densely sampled depth will result in very close ( <0.5 pixel)
    locations which have similar feature. (b) Points projected using appropriate depth
    sampling carry distinguishable information.
  Figure 4 Link: articels_figures_by_rev_year\2021\Cost_Volume_Pyramid_Based_Depth_Inference_for_MultiView_Stereo\figure_4.jpg
  Figure 4 caption: Supervised training. The network shown in Fig. 1 is simplified
    here. For each level of the pyramid, we directly utilize corresponding downsampled
    ground-truth depth for supervision (Highlighted orange part). The loss is the
    smoothed l1 distance between estimated depth map and ground-truth depth map.
  Figure 5 Link: articels_figures_by_rev_year\2021\Cost_Volume_Pyramid_Based_Depth_Inference_for_MultiView_Stereo\figure_5.jpg
  Figure 5 caption: Unsupervised training. The network shown in Fig. 1 is simplified
    here. We utilize the synthesized image and smoothness of the estimated depth for
    unsupervised training. For each level of the network and each source image, we
    generate an image intensity volume for probability based image synthesis. The
    image synthesis loss is computed between the synthesized reference image and the
    reference image. The depth smoothness loss is computed on the estimated depth
    map.
  Figure 6 Link: articels_figures_by_rev_year\2021\Cost_Volume_Pyramid_Based_Depth_Inference_for_MultiView_Stereo\figure_6.jpg
  Figure 6 caption: DTU dataset. Qualitative results of scan 9. The upper row shows
    the point clouds and the bottom row shows the normal map corresponding to the
    orange rectangle. As highlighted in the blue rectangle, the completeness of our
    results is better than those provided by Point-MVSNet[2]. The normal map (orange
    rectangle) further shows that our results are smoother on surfaces while maintaining
    more high-frequency details.
  Figure 7 Link: articels_figures_by_rev_year\2021\Cost_Volume_Pyramid_Based_Depth_Inference_for_MultiView_Stereo\figure_7.jpg
  Figure 7 caption: DTU dataset. Additional results from DTU dataset. Best viewed
    on screen.
  Figure 8 Link: articels_figures_by_rev_year\2021\Cost_Volume_Pyramid_Based_Depth_Inference_for_MultiView_Stereo\figure_8.jpg
  Figure 8 caption: Tanks and Temples. Point cloud reconstruction of Tanks and Temples
    dataset [36]. Best viewed on screen.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.79
  Name of the first author: Jiayu Yang
  Name of the last author: Miaomiao Liu
  Number of Figures: 8
  Number of Tables: 8
  Number of authors: 4
  Paper title: Cost Volume Pyramid Based Depth Inference for Multi-View Stereo
  Publication Date: 2021-05-21 00:00:00
  Table 1 caption: TABLE 1 DTU Dataset
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Tanks and Temples
  Table 3 caption: TABLE 3 DTU Dataset
  Table 4 caption: TABLE 4 ETH3D
  Table 5 caption: TABLE 5 DTU Dataset
  Table 6 caption: TABLE 6 DTU Dataset
  Table 7 caption: TABLE 7 DTU Dataset
  Table 8 caption: TABLE 8 DTU Dataset
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3082562
- Affiliation of the first author: department of computer science, city university
    of hong kong, hong kong, china
  Affiliation of the last author: wangxuan institute of computer technology, peking
    university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Recurrent_MultiFrame_Deraining_Combining_Physics_Guidance_and_Adversarial_Learni\figure_1.jpg
  Figure 1 caption: Visibility degradation caused by rain. (a) Rain streaks. (b) Rain
    accumulation. (c) Rain accumulation flow. The atmosphere flow makes veiling layers
    densities at the same pixel of two frames different. (d) Rain occlusion. There
    is an identical intensity in the occlusion regions.
  Figure 10 Link: articels_figures_by_rev_year\2021\Recurrent_MultiFrame_Deraining_Combining_Physics_Guidance_and_Adversarial_Learni\figure_10.jpg
  Figure 10 caption: Results of rain streak removal by different methods on a real
    video frame. The results of JCAS, UGSM, MS-CSC, SE, and FastDeRain have obvious
    remaining rain streaks. For SpacCNN, there are small rain streaks in the regions
    denoted by blue boxes. Comparatively, our method can well handle the rain streaks.
  Figure 2 Link: articels_figures_by_rev_year\2021\Recurrent_MultiFrame_Deraining_Combining_Physics_Guidance_and_Adversarial_Learni\figure_2.jpg
  Figure 2 caption: Several example data based on our synthesis data produced by Eq.
    (5).
  Figure 3 Link: articels_figures_by_rev_year\2021\Recurrent_MultiFrame_Deraining_Combining_Physics_Guidance_and_Adversarial_Learni\figure_3.jpg
  Figure 3 caption: "Our two-stage progressive network framework for video rain removal.\
    \ In the first stage, Initial-DerainNet uses successive rain frames O t\u2212\
    2 , O t\u22121 ,\u2026, O t+2 as its input and outputs the estimation of the rain-related\
    \ variables of the frame t . Physics recovery module translates these predicted\
    \ rain-related variables into the initially estimated background frame B i t with\
    \ the guidance of the inverse recovery in Eq. (3) and Eq. (5). In the second stage,\
    \ Refined-DerainNet takes the existing clean frames ( B \u02C6 f t\u22122 , B\
    \ \u02C6 f t\u22121 and the initially estimated clean frame B \u02C6 i t from\
    \ the first stage) as well as their corresponding rainy frames O t+1 , O t+2 as\
    \ the networks input to directly predict the rain-free frames. We train the whole\
    \ model in an end-to-end manner with the loss functions for variable estimation\
    \ L f Var , and background frame refinement (reconstruction constraint L f Rect\
    \ , adversarial learning L f Dis , and dark channel prior constraint L f Dark\
    \ , L f DarkTV )."
  Figure 4 Link: articels_figures_by_rev_year\2021\Recurrent_MultiFrame_Deraining_Combining_Physics_Guidance_and_Adversarial_Learni\figure_4.jpg
  Figure 4 caption: (a) The architecture of our encoder in Fig. 3. (b) The sub-encoder
    architecture that constitutes the encoder in (a).
  Figure 5 Link: articels_figures_by_rev_year\2021\Recurrent_MultiFrame_Deraining_Combining_Physics_Guidance_and_Adversarial_Learni\figure_5.jpg
  Figure 5 caption: Results of rain removal methods on a video frame from the synthesized
    dataset RainSynComplex25.
  Figure 6 Link: articels_figures_by_rev_year\2021\Recurrent_MultiFrame_Deraining_Combining_Physics_Guidance_and_Adversarial_Learni\figure_6.jpg
  Figure 6 caption: Results of rain removal methods on a video frame from the synthesized
    dataset RainSynAll100. Except for our method, other methods apply ST-MRF as post-processing.
  Figure 7 Link: articels_figures_by_rev_year\2021\Recurrent_MultiFrame_Deraining_Combining_Physics_Guidance_and_Adversarial_Learni\figure_7.jpg
  Figure 7 caption: Results of rain removal methods on a video frame from the synthesized
    dataset RainSynAll100. Except for our method, other methods apply EVD-Net as pre-processing.
  Figure 8 Link: articels_figures_by_rev_year\2021\Recurrent_MultiFrame_Deraining_Combining_Physics_Guidance_and_Adversarial_Learni\figure_8.jpg
  Figure 8 caption: Results of rain streak removal by different methods on a real
    video frame. The results of SE, PreNet, FastDeRain, UMRL, and MS-CSC have remaining
    rain streaks, as denoted in blue boxes. Meanwhile, SE, PreNet and FastDeRain also
    falsely remove some details, as denoted by red boxes. Comparatively, our method
    can well handle the rain streaks.
  Figure 9 Link: articels_figures_by_rev_year\2021\Recurrent_MultiFrame_Deraining_Combining_Physics_Guidance_and_Adversarial_Learni\figure_9.jpg
  Figure 9 caption: Results of rain streak removal by different methods on a real
    video frame. The results of SE, DIP, FastDeRain, UMRL, and MS-CSC have obvious
    remaining rain streaks. For SpacCNN, there are small rain streaks in the regions
    denoted by blue boxes and details are falsely removed as denoted in the red box.
    Comparatively, our method can well handle the rain streaks.
  First author gender probability: 0.81
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.94
  Name of the first author: Wenhan Yang
  Name of the last author: Jiaying Liu
  Number of Figures: 18
  Number of Tables: 7
  Number of authors: 6
  Paper title: 'Recurrent Multi-Frame Deraining: Combining Physics Guidance and Adversarial
    Learning'
  Publication Date: 2021-05-24 00:00:00
  Table 1 caption: TABLE 1 Summary of Rain Synthetic Models in the Literature
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Quantitative Evaluation of Different Rain Streak Removal
    Methods on RainSynLight25, RainSynComplex25, and NTURain
  Table 3 caption: TABLE 3 Quantitative Evaluation on RainSynAll100
  Table 4 caption: TABLE 4 Summary of Code Links for All Methods
  Table 5 caption: "TABLE 5 Running Time Comparison (in Section) of Different Rain\
    \ Removal Methods on a Video with the Spatial Resolution 832\xD7512 832\xD7512"
  Table 6 caption: TABLE 6 Parameter Comparison of Different Deep-Learning Based Rain
    Removal Methods
  Table 7 caption: TABLE 7 Ablation Analysis for Network Architecture
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3083076
- Affiliation of the first author: pattern recognition and intelligent systems lab.,
    beijing university of posts and telecommunications, beijing, china
  Affiliation of the last author: pattern recognition and intelligent systems lab.,
    beijing university of posts and telecommunications, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Advanced_Dropout_A_ModelFree_Methodology_for_Bayesian_Dropout_Optimization\figure_1.jpg
  Figure 1 caption: Development of the distributions applied in various dropout techniques.
  Figure 10 Link: articels_figures_by_rev_year\2021\Advanced_Dropout_A_ModelFree_Methodology_for_Bayesian_Dropout_Optimization\figure_10.jpg
  Figure 10 caption: Training and test accuracy comparisons of the last epoch with
    VGG16 and ResNet18 models on miniImageNet and Caltech-256 datasets, respectively.
    The vertical lines in red are for the proposed advanced dropout, the advanced
    dropout wo prior, and the advanced dropout wo optimization, respectively. It can
    be observed that all the essential components in the proposed advanced dropout
    contribute to overfitting prevention. The other solid lines (with different colors)
    are for the referred methods.
  Figure 2 Link: articels_figures_by_rev_year\2021\Advanced_Dropout_A_ModelFree_Methodology_for_Bayesian_Dropout_Optimization\figure_2.jpg
  Figure 2 caption: "Probabilistic graphical model of the advanced dropout technique.\
    \ Two key components in the architecture are 1) the model-free distribution g(\
    \ m (l) | \u03BC l , \u03C3 l ) and 2) the parametric prior p( \u03BC l , \u03C3\
    \ l | h (l) 1 ,\u2026, h (l) N ) \u220F N i=1 p( h (l) i | x (l) i ) ."
  Figure 3 Link: articels_figures_by_rev_year\2021\Advanced_Dropout_A_ModelFree_Methodology_for_Bayesian_Dropout_Optimization\figure_3.jpg
  Figure 3 caption: "PDFs of the model-free distribution g applied in experiments\
    \ with different parameter settings. The parameter settings of g with different\
    \ shapes are \u03BC=0,\u03C3=3 for the \u201CU\u201D shape, \u03BC=0,\u03C3=1.6\
    \ for the approximated uniform shape, \u03BC=0,\u03C3=150 for the approximated\
    \ Bernoulli shape, \u03BC=0,\u03C3=1 for the bell shape, and \u03BC=\u22121,\u03C3\
    =1 for the skewed bell shape. Note that we can approximate the Gaussian distribution\
    \ by the bell-shaped g and the log-normal distribution by the skewed bell-shaped\
    \ g , and the beta distribution by the U-, bell-, skewed bell-shaped g , respectively."
  Figure 4 Link: articels_figures_by_rev_year\2021\Advanced_Dropout_A_ModelFree_Methodology_for_Bayesian_Dropout_Optimization\figure_4.jpg
  Figure 4 caption: "PDFs of the softplus-gaussian distribution (in blue) and log-normal\
    \ distribution [42] (in red) approximating the inverse gamma distribution (in\
    \ black) with different parameters: (a) k=5,\u03B8=0.1 ; (b) k=8,\u03B8=0.1 ;\
    \ (c) k=2,\u03B8=0.5 ; (d) k=0.5,\u03B8=3 ."
  Figure 5 Link: articels_figures_by_rev_year\2021\Advanced_Dropout_A_ModelFree_Methodology_for_Bayesian_Dropout_Optimization\figure_5.jpg
  Figure 5 caption: Normalized effectiveness ratios of the advanced dropout technique
    and various dropout techniques for quantitative comparison of accuracy improvement
    over training time. We show the results with the two metrics in (a) and (b), respectively.
    The red squares are the proposed advanced dropout technique, while other circles
    represent the referred dropout techniques. Note that for the ImageNet- 32!times
    !32 dataset, top-1 accuracies are used only.
  Figure 6 Link: articels_figures_by_rev_year\2021\Advanced_Dropout_A_ModelFree_Methodology_for_Bayesian_Dropout_Optimization\figure_6.jpg
  Figure 6 caption: Illustration of the adaptive dropout rates in the advanced dropout
    technique on the MNIST dataset. The subfigures illustrate the dropout rate curves
    of the advanced dropout of (a) the input features, (b) the first hidden layer,
    and (c) the second hidden layer. Note that differently initialized dropout rates
    rho (see legends) are set for initialization of the parameters mu l and sigma
    l . Please refer to (27) for the calculation formula of the dropout rate. Corresponding
    test accuracies (acc.) with different initializations are also reported in (d).
  Figure 7 Link: articels_figures_by_rev_year\2021\Advanced_Dropout_A_ModelFree_Methodology_for_Bayesian_Dropout_Optimization\figure_7.jpg
  Figure 7 caption: Illustration of the adaptive dropout rates in the advanced dropout
    technique on the CIFAR-10 dataset. The subfigures mean the dropout rate curves
    of the advanced dropout of (a) the convolutional features of the last convolutional
    layer, (b) the first hidden layer, and (c) the second hidden layer. Differently
    initialized dropout rates rho (see legends) are set for initialization of the
    parameters mu l and sigma l . Corresponding test accuracies (acc.) with different
    initializations are also reported in (d).
  Figure 8 Link: articels_figures_by_rev_year\2021\Advanced_Dropout_A_ModelFree_Methodology_for_Bayesian_Dropout_Optimization\figure_8.jpg
  Figure 8 caption: Learned distributions of the dropout masks with the VGG16 model
    on the CIFAR-10 dataset. Learned distributions of eight selected epochs are shown
    in (a). Note that for other datasets and base models, the distributions are similar
    with the shown ones. To further discuss the relationship between the convergence
    of the model and the distributions, we illustrate the curve of test accuracies
    in (b) and KL divergences from the distributions of different epochs to that of
    the last epoch in (c).
  Figure 9 Link: articels_figures_by_rev_year\2021\Advanced_Dropout_A_ModelFree_Methodology_for_Bayesian_Dropout_Optimization\figure_9.jpg
  Figure 9 caption: Test accuracy curves with VGG16 on the CIFAR-10 dataset as an
    example. We only illustrate the last 100 epochs for better comparisons of the
    accuracies of different dropout variants and the proposed advanced dropout technique.
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Jiyang Xie
  Name of the last author: Jun Guo
  Number of Figures: 13
  Number of Tables: 13
  Number of authors: 7
  Paper title: 'Advanced Dropout: A Model-Free Methodology for Bayesian Dropout Optimization'
  Publication Date: 2021-05-24 00:00:00
  Table 1 caption: TABLE 1 Test Accuracies ( % %) on MNIST, CIFAR-10, CIFAR-100, miniImageNet,
    and Caltech-256 Datasets
  Table 10 caption: TABLE 10 Test Accuracies ( % %) of Fully Connected (FC) Neural
    Networks With Different Depth on MNIST Dataset
  Table 2 caption: TABLE 2 The p-Values of Students t-Tests Between the Accuracies
    of the Advanced Dropout Technique and All the Referred Techniques With Different
    Base Models on MNIST, CIFAR-10, CIFAR-100, miniImageNet, and Caltech-256 Datasets,
    Respectively
  Table 3 caption: "TABLE 3 Test Accuracies ( % %) and the p p-Values of Students\
    \ t-Tests Between the Accuracies of the Advanced Dropout Technique and All the\
    \ Referred Techniques on the ImageNet- 32\xD732 32\xD732 Dataset"
  Table 4 caption: TABLE 4 Test Accuracies ( % %) and the p p-Values of Students t-Tests
    Between the Accuracies of the Advanced Dropout Technique and All the Referred
    Techniques on the ImageNet Dataset
  Table 5 caption: "TABLE 5 Comparison of Training Time Per Epoch (in SecondEpoch)\
    \ on MNIST, CIFAR-10, CIFAR-100, MiniImageNet, Caltech-256, and ImageNet- 32\xD7\
    32 32\xD732 Datasets by Using One NVIDIA GTX1080Ti"
  Table 6 caption: TABLE 6 Test Accuracies ( % %) and p p-Values of Students t-Tests
    Between the Accuracies of the Advanced Dropout Technique and All the Referred
    Techniques With Other Base Models on CIFAR-10 and -100 Datasets, Respectively
  Table 7 caption: TABLE 7 Test Accuracies ( % %) of Ablation Studies on MNIST, CIFAR-10,
    CIFAR-100, miniImageNet, and Caltech-256 Datasets
  Table 8 caption: TABLE 8 Comparison of the Advanced Dropout and a Dropout Variant
    That Directly Computes the Dropout Rate by the MLP
  Table 9 caption: TABLE 9 Top-1 and Top-5 Test Accuracies (Acc., % %) With Different
    Training Set Sizes on Caltech-256 Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3083089
- Affiliation of the first author: "viterbi faculty of electrical engineering, biomedical\
    \ engineering department, lorry i. lokey center for life sciences and engineering,\
    \ technion\u2013israel institute of technology, haifa, israel"
  Affiliation of the last author: "biomedical engineering department and the lorry\
    \ i. lokey center for life sciences and engineering, russel berrie nanotechnology\
    \ institute, technion\u2013israel institute of technology, haifa, israel"
  Figure 1 Link: articels_figures_by_rev_year\2021\Learning_Optimal_Wavefront_Shaping_for_MultiChannel_Imaging\figure_1.jpg
  Figure 1 caption: End-to-end learned phase masks ( M 1 , M 2 ) and localization
    network ( H W ) enable precise sub-cellular 3D tracking in live cells, at extremely
    high labelling densities. The spatiotemporal trajectories are recovered from a
    time lapse of snapshot image-pairs ( I 1 ( t i ), I 2 ( t i ) ), encoding 3D information
    jointly in their PSFs.
  Figure 10 Link: articels_figures_by_rev_year\2021\Learning_Optimal_Wavefront_Shaping_for_MultiChannel_Imaging\figure_10.jpg
  Figure 10 caption: Dense-particle tracking of labelled telomeres in live U2OS cells
    with the Nebulae PSFs. (a) A single time point showing the two PSF-modulated images.
    (b)-(c) 3D spatiotemporal trajectories for telomeres (b) and (c), exhibiting drastically
    different diffusion behaviors, in different regions of the nucleus. (d) 3D rendered
    cell with all the accumulated tracks showing the motion tracking of telomeres
    in 3D. Most telomeres were localized in all frames ( leq 5% missing localizations).
    (e) Ensemble mean squared displacement (MSD) of all the estimated tracks, obscures
    the dynamics of individual particles, such as tracks (b) and (c). Please see Supp.
    Videos 4-6 for localization validation and online tracks accumulation.
  Figure 2 Link: articels_figures_by_rev_year\2021\Learning_Optimal_Wavefront_Shaping_for_MultiChannel_Imaging\figure_2.jpg
  Figure 2 caption: The multi-PSF optical system. (a) A standard inverted microscope
    with laser illumination. (b-c) Two imaging paths, split by polarization, employing
    two LC-SLMs placed in conjugate back focal planes to the objective lens. Each
    optical path can be modulated with a different phase mask ( M 1 & M 2 ). Please
    see supplementary Fig. S3 for an image of the experimental setup, which can be
    found on the Computer Society Digital Library at http:doi.ieeecomputersociety.org10.1109TPAMI.2021.3076873.
  Figure 3 Link: articels_figures_by_rev_year\2021\Learning_Optimal_Wavefront_Shaping_for_MultiChannel_Imaging\figure_3.jpg
  Figure 3 caption: "A small-footprint EDOF mask. (a) The evolution of the EDOF phase\
    \ mask optimization over 400 iterations. (b) Comparison between the standard PSF\
    \ (top) and the final EDOF PSF (bottom). (c) The 4 \u03BCm Tetrapod PSF [17].\
    \ (d) The XZ cross-sections of the standard (left) and EDOF (right) PSFs, respectively.\
    \ The colorscale is normalized to the maximum intensity of the in-focus, unmodulated\
    \ PSF. Please see Supp. Video 1 for optimization steps."
  Figure 4 Link: articels_figures_by_rev_year\2021\Learning_Optimal_Wavefront_Shaping_for_MultiChannel_Imaging\figure_4.jpg
  Figure 4 caption: "Snapshot, dense-emitter, 3D localizations in fluorescently labelled\
    \ cells. (a) A single frame recorded with a single-channel, 4 \u03BCm Tetrapod\
    \ PSF (left) and the split-channel, Tetrapod-EDOF approach (right). (b-c) Localizations\
    \ (Tetrapod (top), Tetrapod-EDOF (bottom)) are plotted with the ground truth measured\
    \ by axially scanning the sample with the unmodulated PSF."
  Figure 5 Link: articels_figures_by_rev_year\2021\Learning_Optimal_Wavefront_Shaping_for_MultiChannel_Imaging\figure_5.jpg
  Figure 5 caption: Experimental false negatives for the Tetrapod-EDOF pair. (a) Experimental
    snapshot with the Tetrapod PSF utilizing 50 percent of the photons (Fig. 4a right).
    (b) Reconstructed image by rendering the positions recovered by the net with the
    Tetrapod PSF. Asterisks mark true positives (green) and false negatives (blue).
    (c) Paired experimental EDOF snapshot. (d) Zoom-ins on undetected emitters (false
    positives).
  Figure 6 Link: articels_figures_by_rev_year\2021\Learning_Optimal_Wavefront_Shaping_for_MultiChannel_Imaging\figure_6.jpg
  Figure 6 caption: "CRLB-optimized PSF pair. (a) The two phase masks generated by\
    \ CRLB optimization, and the resulting lateral (upper) and axial (lower) bound\
    \ on precision as a function of emitter depth of each PSF separately (red and\
    \ cyan), and after combining both channels (green), as well as the single-channel\
    \ Tetrapod PSF (blue). (b) The PSFs corresponding to the phase masks in (a). Interestingly,\
    \ each channel encodes a complementary part of the axial range. These PSFs have\
    \ a smaller lateral footprint than the similar z-range 4 \u03BCm Tetrapod. The\
    \ colorbar is normalized to the in-focus unmodulated PSF of the system. Please\
    \ see Supp. Video 2 for optimization steps."
  Figure 7 Link: articels_figures_by_rev_year\2021\Learning_Optimal_Wavefront_Shaping_for_MultiChannel_Imaging\figure_7.jpg
  Figure 7 caption: End-to-end learning of the dual-channel optical system. (a) Simulated
    3D positions of emitters cup i ri are fed into two physical layers, which differ
    only in the applied phase mask mathcal M , to simulate the acquired image pairs
    with the modulated PSFs - mathcal I1 & mathcal I2 . Next, both images are fed
    through a convolutional neural network to recover the 3D positions in the simulation
    cup i hatri . Afterwards, these reconstructed positions are compared to the ground
    truth with our loss function mathcal LtextNet , and the gradients (red lines)
    are back propagated through the layers to jointly optimize the encoding masks
    mathcal M1 & mathcal M2 , and the localization CNN parameters mathcal W . (b)
    Nebulae PSFs, which are the result of the end-to-end learning for a 4 mu m axial
    range. The colorbar is normalized compared to the in-focus unmodulated PSF of
    the system. Please see Supp. Video 3 for optimization steps.
  Figure 8 Link: articels_figures_by_rev_year\2021\Learning_Optimal_Wavefront_Shaping_for_MultiChannel_Imaging\figure_8.jpg
  Figure 8 caption: Simulation results. (a) Performance comparison of a Tetrapod DOE
    (red), End-to-end DOE (orange, adapted from [23]), Tetrapod-EDOF pair (blue, Method
    1), CRLB optimized pair (black, Method 2), and the end-to-end learned Nebulae
    PSFs (cyan, Method 3). The Nebulae PSFs perform best in detectability (Jaccard
    index) and in precision (lateralaxial RMSE). Emitters were simulated with approx
    15K signal photons per emitter and approx 500 background photons per pixel. Matching
    of points was computed with a threshold distance of 100 nm. Each data point is
    an average of n = 100 simulated images. Average standard deviation in Jaccard
    index was approx 5 percent and in precision was approx 3 nm. (b) Example of a
    simulated frame of density approx 0.5 left[fracemittersmu m2right] with the Nebulae
    PSFs alongside 3D comparison of the recovered (red) and the ground truth (blue)
    positions. The CNN was able to recover all 90 emitters with a lateralaxial RMSE
    of 1721 nm respectively.
  Figure 9 Link: articels_figures_by_rev_year\2021\Learning_Optimal_Wavefront_Shaping_for_MultiChannel_Imaging\figure_9.jpg
  Figure 9 caption: Experimental measurement of fixed U2OS cells with fluorescently
    labelled telomeres. (a) Tetrapod-EDOF pair (left, Method 1) and the 3D localizations
    plotted with the approximate ground truth achieved via axial scanning (right).
    (b) Nebulae PSFs snapshot pair (left, Method 3) and the 3D localizations plotted
    with approximate ground truth. Additional cell is shown in supplementary Fig.
    S9, available online.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Elias Nehme
  Name of the last author: Yoav Shechtman
  Number of Figures: 11
  Number of Tables: 0
  Number of authors: 7
  Paper title: Learning Optimal Wavefront Shaping for Multi-Channel Imaging
  Publication Date: 2021-05-24 00:00:00
  Table 1 caption: Not Available
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3076873
- Affiliation of the first author: department of electrical and computer engineering,
    worcester polytechnic institute, worcester, ma, usa
  Affiliation of the last author: department of electrical and computer engineering,
    worcester polytechnic institute, worcester, ma, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\Revisiting_D_Convolutional_Neural_Networks_for_GraphBased_Applications\figure_1.jpg
  Figure 1 caption: Illustration of differences between (a) a graph, (b) a graph-preserving
    grid layout (GPGL) of the graph, and (c) an image. The black color in (b) denotes
    no correspondence to any vertex in (a), and other colors denote non-zero features
    on the grid vertices.
  Figure 10 Link: articels_figures_by_rev_year\2021\Revisiting_D_Convolutional_Neural_Networks_for_GraphBased_Applications\figure_10.jpg
  Figure 10 caption: Illustration of two-level H-GPGL algorithm on a 2048-node graph,
    leading to a 256times 256 grid layout.
  Figure 2 Link: articels_figures_by_rev_year\2021\Revisiting_D_Convolutional_Neural_Networks_for_GraphBased_Applications\figure_2.jpg
  Figure 2 caption: Illustration of layout comparison between the KK algorithm and
    our proposed penalized KK algorithm before and after rounding based on a fully-connected
    graph with 32 vertices.
  Figure 3 Link: articels_figures_by_rev_year\2021\Revisiting_D_Convolutional_Neural_Networks_for_GraphBased_Applications\figure_3.jpg
  Figure 3 caption: Illustration of the augmented representations.
  Figure 4 Link: articels_figures_by_rev_year\2021\Revisiting_D_Convolutional_Neural_Networks_for_GraphBased_Applications\figure_4.jpg
  Figure 4 caption: "Illustration of effects of different combinations of \u03B1,\u03BB\
    \ on grid layout generation (IMDB-B)."
  Figure 5 Link: articels_figures_by_rev_year\2021\Revisiting_D_Convolutional_Neural_Networks_for_GraphBased_Applications\figure_5.jpg
  Figure 5 caption: 'Illustration of vertex loss on different data sets: In each subfigure,
    (left) before rounding and (right) after rounding.'
  Figure 6 Link: articels_figures_by_rev_year\2021\Revisiting_D_Convolutional_Neural_Networks_for_GraphBased_Applications\figure_6.jpg
  Figure 6 caption: Multi-scale maxout convolution (MSM-Conv).
  Figure 7 Link: articels_figures_by_rev_year\2021\Revisiting_D_Convolutional_Neural_Networks_for_GraphBased_Applications\figure_7.jpg
  Figure 7 caption: Running time at inference for GPGL and MSM-CNN.
  Figure 8 Link: articels_figures_by_rev_year\2021\Revisiting_D_Convolutional_Neural_Networks_for_GraphBased_Applications\figure_8.jpg
  Figure 8 caption: Illustration of data augmentation on classification.
  Figure 9 Link: articels_figures_by_rev_year\2021\Revisiting_D_Convolutional_Neural_Networks_for_GraphBased_Applications\figure_9.jpg
  Figure 9 caption: State-of-the-art result comparison. Numbers are cited from the
    leaderboard at https:paperswithcode.comtaskgraph-classification.
  First author gender probability: 0.65
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.88
  Name of the first author: Yecheng Lyu
  Name of the last author: Ziming Zhang
  Number of Figures: 14
  Number of Tables: 8
  Number of authors: 3
  Paper title: Revisiting 2D Convolutional Neural Networks for Graph-Based Applications
  Publication Date: 2021-05-25 00:00:00
  Table 1 caption: TABLE 1 Statistics of Benchmark Data Sets for Graph Classification
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 Mean Accuracy (%) Using Different Combinations of \u03B1\
    ,\u03BB \u03B1,\u03BB"
  Table 3 caption: TABLE 3 Vertex Loss Ratio (%) on Each Data Set Using Different
    Initialization Methods
  Table 4 caption: TABLE 4 Ratios (%) Between Vertex Loss and Misclassification
  Table 5 caption: TABLE 5 Mean Accuracy (%) Using Different CNN Classifiers
  Table 6 caption: TABLE 6 Running Time (ms) Comparison on a 2048 Point Cloud With
    5-NN and Delaunay Triangulation, Followed by Our H-GPGL
  Table 7 caption: TABLE 7 Results of ShapeNet Point Cloud Segmentation Using Different
    CNN Models
  Table 8 caption: TABLE 8 Result Comparison on ShapeNet Part Segmentation IoU (%)
    of Each Object Class
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3083614
- Affiliation of the first author: school of information science and engineering,
    the key lab of intelligent computing and information security at universities
    of shandong, shandong provincial key laboratory for novel distributed computer
    software technology, shandong key laboratory of medical physics and image processing,
    and the institute of biomedical sciences, shandong normal university, jinan, china
  Affiliation of the last author: department of electronic engineering, chinese university
    of hong kong, shatin, n.t., hong kong
  Figure 1 Link: articels_figures_by_rev_year\2021\SymRegGAN_Symmetric_Image_Registration_With_Generative_Adversarial_Networks\figure_1.jpg
  Figure 1 caption: "Symmetric image registration by SymReg-GAN. For easy understanding,\
    \ a T1-weighted MRI image I and T2 image J are replaced by a picture from \u201C\
    black Sedan\u201D and \u201Cwhite SUV\u201D, respectively and the registration\
    \ task aims to build a spatial mapping between the horizontal \u201Cblack Sedan\u201D\
    \ and the rotated \u201Cwhite SUV\u201D. To do it, SymReg-GAN trains bipartite\
    \ networks to provide an estimation of spatial mapping M \u2192 from I to J and\
    \ M \u2190 from J to I , respectively. The network for M \u2192 translates I into\
    \ a \u201Cwhite Sedan\u201D I \u2032 = H \u2192 (I) with a modality-translator\
    \ H \u2192 and then transforms it to the space of J with M \u2192 (composed of\
    \ an affine transformation A \u2192 followed by a nonlinear deformation N \u2192\
    \ ) to form a \u201Ctransformed white Sedan\u201D I \u2032\u2032 = M \u2192 (\
    \ I \u2032 ) . Symmetry condition is then expressed as \u201Ccycle consistency\u201D\
    \ [13] for registration: I= M \u2190 ( H \u2190 ( M \u2192 ( H \u2192 (I)))) and\
    \ J= M \u2192 ( H \u2192 ( M \u2190 ( H \u2190 (J)))) , meaning that the circle\
    \ composed of M \u2192 and M \u2190 should bring an image back."
  Figure 10 Link: articels_figures_by_rev_year\2021\SymRegGAN_Symmetric_Image_Registration_With_Generative_Adversarial_Networks\figure_10.jpg
  Figure 10 caption: Triplanar view (axial, sagittal and coronal, from left to right)
    of an exampling pair of pre-aligned CT and MRI, the deformed MRI obtained by deforming
    the original MRI with an artificial deformation, the transformed MRI images by
    ANTs-SyN, Tanner-GAN and SymReg-GAN which register the deformed MRI image to the
    CT image. We also show the triplanar view of the difference map between the original
    MRI image and the corresponding transformed MRI image. A cleaner map means a better
    registration result.
  Figure 2 Link: articels_figures_by_rev_year\2021\SymRegGAN_Symmetric_Image_Registration_With_Generative_Adversarial_Networks\figure_2.jpg
  Figure 2 caption: "Left: the bipartite networks in SymReg-GAN, each of which is\
    \ trained to estimate one directional transformation. For example, the one for\
    \ M \u2192 (estimation of the I\u2192J transformation) is trained via an adversarial\
    \ game between a generator G \u2192 and a discriminator D \u2192 . Right: transformation\
    \ symmetry in SymReg-GAN, which connects the bipartite networks."
  Figure 3 Link: articels_figures_by_rev_year\2021\SymRegGAN_Symmetric_Image_Registration_With_Generative_Adversarial_Networks\figure_3.jpg
  Figure 3 caption: "Generator G \u2192 in SymReg-GAN contains a modality translator\
    \ H \u2192 , an affine-transformation regressor A \u2192 and a nonlinear-deformation\
    \ regressor N \u2192 . Image transformation is realized by a same spatial transformer\
    \ S with the corresponding transformation estimation from the two regressors."
  Figure 4 Link: articels_figures_by_rev_year\2021\SymRegGAN_Symmetric_Image_Registration_With_Generative_Adversarial_Networks\figure_4.jpg
  Figure 4 caption: "Discriminator D \u2192 contains a network Z \u2192 which distinguishes\
    \ transformations estimation M \u2192 from its ground-truth M \u2217 \u2192 by\
    \ taking the transformed I (via spatial transformer S ) and the original image\
    \ J as inputs."
  Figure 5 Link: articels_figures_by_rev_year\2021\SymRegGAN_Symmetric_Image_Registration_With_Generative_Adversarial_Networks\figure_5.jpg
  Figure 5 caption: The semi-supervised strategy in SymReg-GAN updates the generator
    using two registration losses L dis and L smt , plus a symmetry loss L sym and
    an adversarial loss L G adv from the discriminator. A supervision loss L spv is
    also applied for labeled images (solid red arrow) but not for unlabeled images
    (dashed red arrow).
  Figure 6 Link: articels_figures_by_rev_year\2021\SymRegGAN_Symmetric_Image_Registration_With_Generative_Adversarial_Networks\figure_6.jpg
  Figure 6 caption: Mean absolute error (MAE) of SymReg-GAN when varying the number
    of labeled image pairs and unlabeled image pairs. (a) Errors when training SymReg-GAN
    only with labeled image pairs only, for which we vary the number of training pairs
    and observe that accuracy improves little when the training pairs number increases
    but exceeds 80. (b) Errors when training SymReg-GAN first with labeled image pairs
    then with unlabeled image pairs, for which we choose the 80 pairs specified in
    the experiment of (a) as the labeled images and perform the training, then treat
    the left 70 pairs from the 150 pairs as the training pairs and carry out the training
    by varying the number of training pairs. (c) Errors when training SymReg-GAN with
    both labeled image pairs and unlabeled image pais, for which we always use a total
    of 150 labeledunlabeled image pairs but varying the number of labeled image pairs.
  Figure 7 Link: articels_figures_by_rev_year\2021\SymRegGAN_Symmetric_Image_Registration_With_Generative_Adversarial_Networks\figure_7.jpg
  Figure 7 caption: Bar plots of MAE results generated on the BraTS 2018 images without
    (left) and with (right) white Gaussian noise added by ANTs-SyN, Tanner-GAN, SymReg-U,
    SymReg-L, Reg-GAN and SymReg-GAN, respectively. Each gray bracket connects a pair
    of techniques for which the difference of results is statistically significant
    ( p<0.01 ).
  Figure 8 Link: articels_figures_by_rev_year\2021\SymRegGAN_Symmetric_Image_Registration_With_Generative_Adversarial_Networks\figure_8.jpg
  Figure 8 caption: 'Registration results on a pair of T1-T2 images from the BraTS
    2018 dataset, in which white Gaussian noise is added. The 1st image of 3rd row
    and the 2nd image of 1st row are the pre-aligned T1 and T2 MRI images, respectively.
    The 1st image of the 1st row is an image created by transforming the T1 image
    with an artificial geometric deformation and the 1st image of the 5th row is the
    brain mask of the T1 image. Left to right and top to down for the left images
    in the 1-2 rows: transformed images from the transformed T1 image to the T2 image
    by ANTS-SyN, Tanner-GAN, SymReg-U, SymReg-L, Reg-GAN and SymReg-GAN, respectively.
    The left images of 3-4 rows are the ground-truth and the corresponding algorithms
    deformed meshes with the MAE numbers, respectively. The left images of 5-6 rows
    are the ground-truth and corresponding algorithms Jacobian determinants, respectively.'
  Figure 9 Link: articels_figures_by_rev_year\2021\SymRegGAN_Symmetric_Image_Registration_With_Generative_Adversarial_Networks\figure_9.jpg
  Figure 9 caption: Dice Similarity Coefficient boxplot for 16 anatomical structures
    from ALBERTs database of 6 multimodal registration techniques including ANTs-SyN,
    Tanner-GAN, SymReg-U, SymReg-L, Reg-GAN and SymReg-GAN. The orange arrows points
    at the results that SymReg-GAN outperforms in a statistically significant fashion
    ( p<0.01 ).
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Yuanjie Zheng
  Name of the last author: Hongsheng Li
  Number of Figures: 11
  Number of Tables: 2
  Number of authors: 7
  Paper title: 'SymReg-GAN: Symmetric Image Registration With Generative Adversarial
    Networks'
  Publication Date: 2021-05-25 00:00:00
  Table 1 caption: TABLE 1 Mean (Standard Deviation) of the Dice Similarity Coefficient
    for White Matter (WM), Grey Matter (GM) and Cerebrospinal Fluid (CSF), by Various
    Unimodal Image Registration Techniques Including ANTs-SyN, D.Demons, MRF-LP, DL,
    Multiscale-GAN, SymReg-L, SymReg-U, Reg-GAN and SymReg-GAN
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Mean Running Time (in Seconds) of ANTs-SyN, Tanner-GAN,
    SymReg-GAN, D.Demons, MRF-LP, DL and Multiscale-GAN for Registering a Pair of
    MultimodalUnimodal Images
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3083543
- Affiliation of the first author: department of computer science, university of oxford,
    oxford, u.k.
  Affiliation of the last author: department of computer science, university of oxford,
    oxford, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2021\Learning_Semantic_Segmentation_of_LargeScale_Point_Clouds_With_Random_Sampling\figure_1.jpg
  Figure 1 caption: "Semantic segmentation results of PointNet++ [1], SPG [2] and\
    \ our approach on SemanticKITTI [3]. Our RandLA-Net takes only 0.04s to directly\
    \ process a large point cloud with 81920 points over 150 \xD7130\xD7 10 m 3 in\
    \ 3D space, which is up to 200\xD7 faster than SPG. Red circles highlight the\
    \ superior segmentation accuracy of our approach."
  Figure 10 Link: articels_figures_by_rev_year\2021\Learning_Semantic_Segmentation_of_LargeScale_Point_Clouds_With_Random_Sampling\figure_10.jpg
  Figure 10 caption: 'The results of our RandLA-Net with different neighbour searching
    methods. Left: The mIoU scores for different choices of K in KNN. Right: The mIoU
    scores for different choices of the radius R in spherical neighbours.'
  Figure 2 Link: articels_figures_by_rev_year\2021\Learning_Semantic_Segmentation_of_LargeScale_Point_Clouds_With_Random_Sampling\figure_2.jpg
  Figure 2 caption: In each layer of RandLA-Net, the large-scale point cloud is significantly
    downsampled, yet is capable of retaining features necessary for accurate segmentation.
  Figure 3 Link: articels_figures_by_rev_year\2021\Learning_Semantic_Segmentation_of_LargeScale_Point_Clouds_With_Random_Sampling\figure_3.jpg
  Figure 3 caption: The proposed local feature aggregation module. The top panel shows
    the local spatial encoding block that extracts features, and the attentive pooling
    mechanism that weights the important neighbouring features, based on the local
    context and geometry. The bottom panel shows how two of these components are chained
    together, to increase the receptive field size, within a residual block.
  Figure 4 Link: articels_figures_by_rev_year\2021\Learning_Semantic_Segmentation_of_LargeScale_Point_Clouds_With_Random_Sampling\figure_4.jpg
  Figure 4 caption: "The detailed architecture of our RandLA-Net. ( N \u02DC ,D) represents\
    \ the number of points and feature dimension respectively. FC: Fully Connected\
    \ layer, LFA: Local Feature Aggregation, RS: Random Sampling, MLP: shared Multi-Layer\
    \ Perceptron, US: Up-sampling, DP: Dropout."
  Figure 5 Link: articels_figures_by_rev_year\2021\Learning_Semantic_Segmentation_of_LargeScale_Point_Clouds_With_Random_Sampling\figure_5.jpg
  Figure 5 caption: 'Illustration of the dilated residual block which significantly
    increases the receptive field (dotted circle) of each point, colored points represent
    the aggregated features. L: Local spatial encoding, A: Attentive pooling.'
  Figure 6 Link: articels_figures_by_rev_year\2021\Learning_Semantic_Segmentation_of_LargeScale_Point_Clouds_With_Random_Sampling\figure_6.jpg
  Figure 6 caption: Time and memory consumption of different sampling approaches.
    The dashed lines represent estimated values due to the limited GPU memory. Note
    that, the curves of GPU memory consumption for RPS, PDS, FPS, and IDIS overlap
    together, because these sampling methods mainly run on the CPU.
  Figure 7 Link: articels_figures_by_rev_year\2021\Learning_Semantic_Segmentation_of_LargeScale_Point_Clouds_With_Random_Sampling\figure_7.jpg
  Figure 7 caption: 'Qualitative results of RandLA-Net on the reduced-8 split of Semantic3D.
    From left to right: full RGB colored point clouds, predicted semantic labels of
    full point clouds, detailed view of colored point clouds, detailed view of predicted
    semantic labels. Note that the ground truth of the test set is not publicly available.'
  Figure 8 Link: articels_figures_by_rev_year\2021\Learning_Semantic_Segmentation_of_LargeScale_Point_Clouds_With_Random_Sampling\figure_8.jpg
  Figure 8 caption: Qualitative results of RandLA-Net on the validation set of SemanticKITTI
    [3]. Red boxes show the failure cases.
  Figure 9 Link: articels_figures_by_rev_year\2021\Learning_Semantic_Segmentation_of_LargeScale_Point_Clouds_With_Random_Sampling\figure_9.jpg
  Figure 9 caption: Semantic segmentation results of KPConv [23] and our approach
    on Toronto-3D [37]. Our method clearly performs better on the category of road
    marking.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Qingyong Hu
  Name of the last author: Andrew Markham
  Number of Figures: 11
  Number of Tables: 12
  Number of authors: 8
  Paper title: Learning Semantic Segmentation of Large-Scale Point Clouds With Random
    Sampling
  Publication Date: 2021-05-25 00:00:00
  Table 1 caption: TABLE 1 Comparison of Representative Sampling Methods for Processing
    Point Clouds
  Table 10 caption: TABLE 10 The Mean IoU Scores of Our RandLA-Net With Different
    Designs of LocSE
  Table 2 caption: "TABLE 2 The Runtime Decomposition and Peak Memory of Different\
    \ Approaches for Semantic Segmentation of Input Point Clouds With 12\xD781920\
    \ ( \u223C 10 6 \u223C106) Points, Which is Sampled From the LiDAR Point Clouds\
    \ on Sequence 08 (Around 80K-120K Points Per Frame) of the SemanticKITTI [3] Dataset"
  Table 3 caption: TABLE 3 Quantitative Results of Different Approaches on Semantic3D
    (Reduced-8) [36]
  Table 4 caption: TABLE 4 Quantitative Results of Different Approaches on Semantic3D
    (Semantic-8) [36]
  Table 5 caption: TABLE 5 Quantitative Results of Different Approaches on SemanticKITTI
    [3]
  Table 6 caption: TABLE 6 Quantitative Results of Different Approaches on Toronto3D
    [37]
  Table 7 caption: TABLE 7 Quantitative Results of Different Approaches on NPM3D [73]
  Table 8 caption: TABLE 8 Quantitative Results of Different Approaches on S3DIS [72]
    (6-Fold Cross-Validation)
  Table 9 caption: TABLE 9 The Mean IoU Scores of All Ablated Networks Based on Our
    Full RandLA-Net
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3083288
