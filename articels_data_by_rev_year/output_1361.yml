- Affiliation of the first author: school of computer science and engineering, beihang
    university, beijing, china
  Affiliation of the last author: qihoo 360 ai institute, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2019\FineGrained_HumanCentric_Tracklet_Segmentation_with_Single_Frame_Supervision\figure_1.jpg
  Figure 1 caption: Examples of the training data from DAVIS 2017 [35], Youtube-VOS
    [43] and our newly collected Outdoor dataset. Best viewed in color.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\FineGrained_HumanCentric_Tracklet_Segmentation_with_Single_Frame_Supervision\figure_2.jpg
  Figure 2 caption: "During testing, a segmentation window is slided along the video.\
    \ The parsing result of frame I t is determined by current frame I t and two historical\
    \ frames I t\u2212l and I t\u2212s ."
  Figure 3 Link: articels_figures_by_rev_year\2019\FineGrained_HumanCentric_Tracklet_Segmentation_with_Single_Frame_Supervision\figure_3.jpg
  Figure 3 caption: The architecture of the proposed Temporal Context segmentation
    Network.
  Figure 4 Link: articels_figures_by_rev_year\2019\FineGrained_HumanCentric_Tracklet_Segmentation_with_Single_Frame_Supervision\figure_4.jpg
  Figure 4 caption: "Details of pixel-level fusion and frame-level fusion. \u2299\
    ,+ respectively denote element-wise product and element-wise add."
  Figure 5 Link: articels_figures_by_rev_year\2019\FineGrained_HumanCentric_Tracklet_Segmentation_with_Single_Frame_Supervision\figure_5.jpg
  Figure 5 caption: Images in the four datasets.
  Figure 6 Link: articels_figures_by_rev_year\2019\FineGrained_HumanCentric_Tracklet_Segmentation_with_Single_Frame_Supervision\figure_6.jpg
  Figure 6 caption: Comparisons of optical flow with and without refinement. The optical
    flow warps Image B to Image A. The warped image using the refined optical flow
    are more similar with Image A.
  Figure 7 Link: articels_figures_by_rev_year\2019\FineGrained_HumanCentric_Tracklet_Segmentation_with_Single_Frame_Supervision\figure_7.jpg
  Figure 7 caption: "The fusion weights for \u201CFace\u201D and \u201CL-arm\u201D\
    \ in the Indoor dataset. The 13-dim weights for I t\u2212l , I t\u2212s and I\
    \ t are shown sequentially, separated by vertical dotted lines. For each frame,\
    \ the category with the maximum fusion weight is denoted by the black dot."
  Figure 8 Link: articels_figures_by_rev_year\2019\FineGrained_HumanCentric_Tracklet_Segmentation_with_Single_Frame_Supervision\figure_8.jpg
  Figure 8 caption: 'Step by step illustration of the effectiveness of the pixel-level
    context fusion. 1 sim 4 columns: the long-range frame, its parsing result, the
    warped parsing result and the confidence map. 5 sim 8 columns: the short-range
    frame, its parsing result, the warped parsing result and the confidence map. 9
    sim 12 columns: test image, the rough parsing result, refined parsing result and
    ground truth.'
  Figure 9 Link: articels_figures_by_rev_year\2019\FineGrained_HumanCentric_Tracklet_Segmentation_with_Single_Frame_Supervision\figure_9.jpg
  Figure 9 caption: Qualitative results of the EM-Adapt, NetWarp-DIS, and NetWarp-Flownet
    and several variants of TCNet.
  First author gender probability: 0.72
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Si Liu
  Name of the last author: Shuicheng Yan
  Number of Figures: 9
  Number of Tables: 9
  Number of authors: 7
  Paper title: Fine-Grained Human-Centric Tracklet Segmentation with Single Frame
    Supervision
  Publication Date: 2019-04-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Number of Labeled Images of the Four Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison with State-of-the-Arts and Several Variants of
      TCNet in Indoor Dataset ( % %)
  Table 3 caption:
    table_text: TABLE 3 Per-Class Comparison of F-1 Scores with State-of-the-Arts
      and Several Architectural Variants of TCNet in Indoor Dataset ( % %)
  Table 4 caption:
    table_text: TABLE 4 Comparison with State-of-the-Arts and Several Variants of
      TCNet in Outdoor Dataset ( % %)
  Table 5 caption:
    table_text: TABLE 5 Per-Class Comparison of F-1 Scores with State-of-the-Arts
      and Several Architectural Variants of TCNet in Outdoor Dataset ( % %)
  Table 6 caption:
    table_text: TABLE 6 Comparison with State-of-the-Arts and Several Variants of
      TCNet in iLIDS-Parsing Dataset ( % %)
  Table 7 caption:
    table_text: TABLE 7 Per-Class Comparison of F-1 Scores with State-of-the-Arts
      and Several Variants of TCNet in iLIDS-Parsing Dataset ( % %)
  Table 8 caption:
    table_text: TABLE 8 Comparison with State-of-the-Arts and Several Variants of
      TCNet in Daily Dataset ( % %)
  Table 9 caption:
    table_text: TABLE 9 Per-Class Comparison of F-1 Scores with State-of-the-Arts
      and Some Variants of TCNet in Daily Dataset ( % %)
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2911936
- Affiliation of the first author: school of computer science and technology, university
    of science and technology of china, hefei, china
  Affiliation of the last author: department of electrical, computer systems engineering
    rensselaer polytechnic institute, troy, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\A_Novel_Dynamic_Model_Capturing_Spatial_and_Temporal_Patterns_for_Facial_Express\figure_1.jpg
  Figure 1 caption: Sample images demonstrating spatial patterns inherent in expressions.
  Figure 10 Link: articels_figures_by_rev_year\2019\A_Novel_Dynamic_Model_Capturing_Spatial_and_Temporal_Patterns_for_Facial_Express\figure_10.jpg
  Figure 10 caption: Graphical depiction of the selected event pairs in the MMI database.
  Figure 2 Link: articels_figures_by_rev_year\2019\A_Novel_Dynamic_Model_Capturing_Spatial_and_Temporal_Patterns_for_Facial_Express\figure_2.jpg
  Figure 2 caption: Image sequences demonstrating temporal patterns inherent in expressions.
    The x -axis is the frame number.
  Figure 3 Link: articels_figures_by_rev_year\2019\A_Novel_Dynamic_Model_Capturing_Spatial_and_Temporal_Patterns_for_Facial_Express\figure_3.jpg
  Figure 3 caption: Outline of the recognition system.
  Figure 4 Link: articels_figures_by_rev_year\2019\A_Novel_Dynamic_Model_Capturing_Spatial_and_Temporal_Patterns_for_Facial_Express\figure_4.jpg
  Figure 4 caption: (a) Facial muscle movement as captured by the movement of facial
    points. (b) Duration for events V 1 and V 2 and their temporal relations. (c)
    Example movement states of a primitive facial event.
  Figure 5 Link: articels_figures_by_rev_year\2019\A_Novel_Dynamic_Model_Capturing_Spatial_and_Temporal_Patterns_for_Facial_Express\figure_5.jpg
  Figure 5 caption: An example of IT-RBM model.
  Figure 6 Link: articels_figures_by_rev_year\2019\A_Novel_Dynamic_Model_Capturing_Spatial_and_Temporal_Patterns_for_Facial_Express\figure_6.jpg
  Figure 6 caption: 'Facial feature points. Left: SPOS, CK+, MMI; right: DISFA+.'
  Figure 7 Link: articels_figures_by_rev_year\2019\A_Novel_Dynamic_Model_Capturing_Spatial_and_Temporal_Patterns_for_Facial_Express\figure_7.jpg
  Figure 7 caption: (a) Graphical depiction of temporal relations selected in DISFA+.
    (b) Examples of relation between point 20 and 29. (c) Frequencies of thirteen
    relations between point 20 and point 29 with respect to posed and genuine expressions.
    x -axis represents the index of relationships.
  Figure 8 Link: articels_figures_by_rev_year\2019\A_Novel_Dynamic_Model_Capturing_Spatial_and_Temporal_Patterns_for_Facial_Express\figure_8.jpg
  Figure 8 caption: The mean weight of PSgender models at different move models on
    all facial points and hidden nodes. x -axis represents K move models, y -axis
    represents the mean value of W j ik at every k=K .
  Figure 9 Link: articels_figures_by_rev_year\2019\A_Novel_Dynamic_Model_Capturing_Spatial_and_Temporal_Patterns_for_Facial_Express\figure_9.jpg
  Figure 9 caption: On the DISFA+ database, the mean weight of PSexp models at every
    selected facial points on a certain movement state with all hidden nodes from
    trained IT-RBMs. z -axis represents the mean value of W k ij when k=K at every
    facial points.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: Shangfei Wang
  Name of the last author: Qiang Ji
  Number of Figures: 11
  Number of Tables: 7
  Number of authors: 5
  Paper title: A Novel Dynamic Model Capturing Spatial and Temporal Patterns for Facial
    Expression Analysis
  Publication Date: 2019-04-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 TR and Interval Relation Mapping Table
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Data Distribution of SPOS and DISFA+
  Table 3 caption:
    table_text: TABLE 3 Results of Posed and Spontaneous Distinction Experiments
  Table 4 caption:
    table_text: TABLE 4 Posed and Spontaneous Distinction
  Table 5 caption:
    table_text: TABLE 5 Comparison with Related Work of Posed and Spontaneous Expressions
      Distinction on SPOS
  Table 6 caption:
    table_text: TABLE 6 Data Distribution of CK+ and MMI
  Table 7 caption:
    table_text: TABLE 7 Results of Expression Categories Recognition Experiments
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2911937
- Affiliation of the first author: "institute of mathematics, \xE9cole polytechnique\
    \ f\xE9d\xE9rale de lausanne, 1015 lausanne, switzerland"
  Affiliation of the last author: theoretical statistics and mathematics unit, indian
    statistical institute, kolkata, india
  Figure 1 Link: articels_figures_by_rev_year\2019\On_Perfect_Clustering_of_High_Dimension_Low_Sample_Size_Data\figure_1.jpg
  Figure 1 caption: (a) Central regions of the two normal populations in Example A
    and (b) Supports of the three populations in Example B (for d=2 ).
  Figure 10 Link: articels_figures_by_rev_year\2019\On_Perfect_Clustering_of_High_Dimension_Low_Sample_Size_Data\figure_10.jpg
  Figure 10 caption: Six classes in the Control Chart data set.
  Figure 2 Link: articels_figures_by_rev_year\2019\On_Perfect_Clustering_of_High_Dimension_Low_Sample_Size_Data\figure_2.jpg
  Figure 2 caption: Average Rand indices for different clustering algorithms in Examples
    A and B.
  Figure 3 Link: articels_figures_by_rev_year\2019\On_Perfect_Clustering_of_High_Dimension_Low_Sample_Size_Data\figure_3.jpg
  Figure 3 caption: Supports of different populations in Examples 3 and 4 for d=2
    .
  Figure 4 Link: articels_figures_by_rev_year\2019\On_Perfect_Clustering_of_High_Dimension_Low_Sample_Size_Data\figure_4.jpg
  Figure 4 caption: Frequency distributions of number of clusters estimated by different
    methods in Examples 1 and 2.
  Figure 5 Link: articels_figures_by_rev_year\2019\On_Perfect_Clustering_of_High_Dimension_Low_Sample_Size_Data\figure_5.jpg
  Figure 5 caption: A clustering algorithm which is POP at 4.
  Figure 6 Link: articels_figures_by_rev_year\2019\On_Perfect_Clustering_of_High_Dimension_Low_Sample_Size_Data\figure_6.jpg
  Figure 6 caption: Gene expression for DLBCL(), FL() and CLL() samples.
  Figure 7 Link: articels_figures_by_rev_year\2019\On_Perfect_Clustering_of_High_Dimension_Low_Sample_Size_Data\figure_7.jpg
  Figure 7 caption: Compositions of (a) two and (b) three estimated clusters for Lymphoma
    data. Each bar corresponds to a single cluster consisting of DLBCL(), FL() and
    CLL() samples.
  Figure 8 Link: articels_figures_by_rev_year\2019\On_Perfect_Clustering_of_High_Dimension_Low_Sample_Size_Data\figure_8.jpg
  Figure 8 caption: Four classes in the Trace data set.
  Figure 9 Link: articels_figures_by_rev_year\2019\On_Perfect_Clustering_of_High_Dimension_Low_Sample_Size_Data\figure_9.jpg
  Figure 9 caption: Compositions of (a) two and (b) four estimated clusters for Trace
    data. Each bar corresponds to a single cluster consisting of observations from
    class-1(), class-2(), class-3() and class-4().
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Soham Sarkar
  Name of the last author: Anil K. Ghosh
  Number of Figures: 12
  Number of Tables: 11
  Number of authors: 2
  Paper title: On Perfect Clustering of High Dimension, Low Sample Size Data
  Publication Date: 2019-04-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average Rand Indices of Different Clustering Algorithms in
      Examples 1, 2, 3, 4, 5, and 6
  Table 10 caption:
    table_text: TABLE 10 Number of Clusters Estimated by Different Methods in the
      Trace Data Set
  Table 2 caption:
    table_text: TABLE 2 Average Rand Indices of Different Clustering Algorithms in
      Examples 7 and 8
  Table 3 caption:
    table_text: TABLE 3 Average Rand Indices of Different Clustering Algorithms in
      Examples 7 and 8
  Table 4 caption:
    table_text: TABLE 4 Average Rand Indices of Different Clustering Algorithms in
      Examples 9 and 10
  Table 5 caption:
    table_text: TABLE 5 Computing Times (in seconds) for Different Clustering Algorithms
  Table 6 caption:
    table_text: TABLE 6 Frequency Distribution for the Estimated Number of Clusters
      in Examples 3, 4, 5, and 6
  Table 7 caption:
    table_text: TABLE 7 Frequency Distribution for the Estimated Number of Clusters
      in Examples 7 and 8
  Table 8 caption:
    table_text: TABLE 8 Frequency Distribution for the Estimated Number of Clusters
      in Examples 9 and 10
  Table 9 caption:
    table_text: TABLE 9 Number of Clusters Estimated by Different Methods in the Lymphoma
      Data Set
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2912599
- Affiliation of the first author: department of electrical and computer engineering,
    university of delaware, newark, usa
  Affiliation of the last author: department of electrical and computer engineering,
    university of delaware, newark, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Snapshot_Compressive_ToFSpectral_Imaging_via_Optimized_ColorCoded_Apertures\figure_1.jpg
  Figure 1 caption: Sketch of the proposed compressive snapshot MS+D imaging system.
    A pulsed NIR LED board shines the ambient-illuminated scene, and the reflected
    light is imaged by the front lens onto the color-coded aperture (CCA), which lies
    in the image plane of the front lens. The relay lens transmits the coded light
    through the dispersive element and onto the ToF sensor, which lies in a plane
    optically conjugate to the CCA.
  Figure 10 Link: articels_figures_by_rev_year\2019\Snapshot_Compressive_ToFSpectral_Imaging_via_Optimized_ColorCoded_Apertures\figure_10.jpg
  Figure 10 caption: Depth characterization setup. A box is attached to a linear stage
    with micro-positioning accuracy, placed 80 cm away from the sensor, and move it
    back and forth, along the axial coordinate. In particular, the linear stage is
    moved from 0 to 30 centimeters, in steps of 0.1 centimeter, and the depth maps
    are measured at 3, 6, 12 and 24 MHz.
  Figure 2 Link: articels_figures_by_rev_year\2019\Snapshot_Compressive_ToFSpectral_Imaging_via_Optimized_ColorCoded_Apertures\figure_2.jpg
  Figure 2 caption: CCA realizations with (a) 2 filters and (b) 8 filters. The filters
    come in pairs (a high-pass and a low-pass), which are designed to allow the passing
    of the NIR-LED pulsed light at 800 nm.
  Figure 3 Link: articels_figures_by_rev_year\2019\Snapshot_Compressive_ToFSpectral_Imaging_via_Optimized_ColorCoded_Apertures\figure_3.jpg
  Figure 3 caption: "Quadrature sensing. Each of the 4 DCS frames has a different\
    \ phase relation between modulation (ledmod) and demodulation (mga, mgb) signals\
    \ which makes phase-to-distance calculation possible. (a) For DCS0, ledmod is\
    \ phase-shifted by 0 rad and \u03C0 rad with respect to mga and mgb. (b) For DCS1,\
    \ ledmod is phase-shifted by \u03C02 rad and 3\u03C02 rad. (c) For DCS2, the phase\
    \ shifts are \u03C0 rad and 0 rad. (d) For DCS3, the phase shifts are 3\u03C0\
    2 rad and \u03C02 rad. Note that for DCS2 and DCS3, the demodulation signals are\
    \ simply swapped with respect to DCS0 and DCS1. (e) A snapshot of the proposed\
    \ system requires the acquisition of 5 frames: grayscale, DCS0, DCS1, DCS2 and\
    \ DCS3."
  Figure 4 Link: articels_figures_by_rev_year\2019\Snapshot_Compressive_ToFSpectral_Imaging_via_Optimized_ColorCoded_Apertures\figure_4.jpg
  Figure 4 caption: "Blue noise CCA optimization algorithm analysis. (a) RAPSD over\
    \ 100 iterations. (b) Averaged objection function error ( E ) versus algorithm\
    \ iterations. (c) Averaged number of iterations required until convergence. (d)\
    \ (Left to right) Evolution of the cross-correlation error function \u0393 h ~\
    \ e ~ along the first 4 iterations. (e) (Left to right) Evolution of a halftone\
    \ along the first 6 iterations of the algorithm. (f, g, h) Input random CCAs are\
    \ shown in the first row, and the optimized CCAs are shown in the second row of\
    \ each subfigure, when 2, 4 and 8 filters are employed, respectively. Note the\
    \ uniform spread of the optical filters at each optimized layer."
  Figure 5 Link: articels_figures_by_rev_year\2019\Snapshot_Compressive_ToFSpectral_Imaging_via_Optimized_ColorCoded_Apertures\figure_5.jpg
  Figure 5 caption: Datasets for simulations. Each subfigure shows the actual scene
    captured with an external high-resolution color camera, along with the RGB-mapped
    version of the spectral bands as seen by the ToF camera, the ground-truth depth
    map (using the DCS components at 24 MHz), and the lateral view of the scene with
    the distances between objects in centimeters. All target scenes are placed 80
    centimeters away from the camera.
  Figure 6 Link: articels_figures_by_rev_year\2019\Snapshot_Compressive_ToFSpectral_Imaging_via_Optimized_ColorCoded_Apertures\figure_6.jpg
  Figure 6 caption: 'Summary of the simulation results. Four data sets are used (DB1:
    Fruit, DB2: Mistletoe, DB3: Orchid, DB4: UDGlass) to evaluate the performance
    of three reconstruction algorithms (TwIST, GPSR and C-SALSA) in terms of PSNR
    and SAM, when random (Rand) and optimized (Opt.) CCAs are employed.'
  Figure 7 Link: articels_figures_by_rev_year\2019\Snapshot_Compressive_ToFSpectral_Imaging_via_Optimized_ColorCoded_Apertures\figure_7.jpg
  Figure 7 caption: Depth map estimations at modulation frequencies of 3, 6, 12 and
    24 MHz (along the columns), for the four data sets, Fruit, Mistletoe, Orchid and
    UDGlass (along the rows). Note that higher frequencies entail better precision
    but shorter ranges.
  Figure 8 Link: articels_figures_by_rev_year\2019\Snapshot_Compressive_ToFSpectral_Imaging_via_Optimized_ColorCoded_Apertures\figure_8.jpg
  Figure 8 caption: Snapshot compressive ToF+spectral testbed and testing target scene.
    (a) Optical testbed build in our lab. (b)-(c) Frontal and lateral view of the
    target scene. A mix of large and small colorful objects are spatially arranged
    as in (b), and placed at 5 different depth planes as shown in (c). The target
    scene is placed around 150 cm away from the testbed.
  Figure 9 Link: articels_figures_by_rev_year\2019\Snapshot_Compressive_ToFSpectral_Imaging_via_Optimized_ColorCoded_Apertures\figure_9.jpg
  Figure 9 caption: Fabricated CCAs. (First row) Random realization. (Second row)
    Optimized realization. From left to right, different zoomed versions are included
    for details on their distribution, sloped edges and size.
  First author gender probability: 0.93
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Hoover Rueda-Chacon
  Name of the last author: Gonzalo R. Arce
  Number of Figures: 18
  Number of Tables: 0
  Number of authors: 4
  Paper title: Snapshot Compressive ToF+Spectral Imaging via Optimized Color-Coded
    Apertures
  Publication Date: 2019-04-23 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2912961
- Affiliation of the first author: "institute of telecommunications, technische universit\xE4\
    t wien, vienna, austria"
  Affiliation of the last author: department of mathematics and computer science (imada),
    university of southern denmark (sdu), odense, denmark
  Figure 1 Link: articels_figures_by_rev_year\2019\Absolute_Cluster_Validity\figure_1.jpg
  Figure 1 caption: Different algorithms might resolve differently a problem. Algorithm
    A discovers clusters A1,A2,A3 and A4 surrounded by outliers, whereas algorithm
    B finds two big clusters B1 and B2 and no outliers. The establishment of the best
    solution depends on the application.
  Figure 10 Link: articels_figures_by_rev_year\2019\Absolute_Cluster_Validity\figure_10.jpg
  Figure 10 caption: Example 1. a) Input space, b) solution with k=15 , c) solution
    with k=20 , d) solution with k=25 . Clusters are drawn with core and extended
    volume boundaries.
  Figure 2 Link: articels_figures_by_rev_year\2019\Absolute_Cluster_Validity\figure_2.jpg
  Figure 2 caption: Example of a dataset where samples are arranged in a non-chaotic
    shape, but with difficulty accepts any meaningful cluster-like representation.
  Figure 3 Link: articels_figures_by_rev_year\2019\Absolute_Cluster_Validity\figure_3.jpg
  Figure 3 caption: "Example of o i rex and o i str for cluster 1, where centroid\
    \ c2 is the nearest centroid to c1 (based on the oi metric). Provided that c1\
    \ is also the nearest centroid to c2 we have: \u0393 1 = \u0393 2 , o i rex,1\
    \ =o i rex,2 and o i str,1 =o i str,2 . The continuous circles mark core volume\
    \ boundaries, the dashed ones mark extended volume boundaries. Objects in cluster\
    \ 1 follow a log-normal distribution, objects in cluster 2 follow a triangular\
    \ distribution."
  Figure 4 Link: articels_figures_by_rev_year\2019\Absolute_Cluster_Validity\figure_4.jpg
  Figure 4 caption: "\u03C1 \xAF as a function of N for the conducted experiments.\
    \ \u03C1 \xAF is calculated considering all clusters in 225 different datasets.\
    \ Standard deviations are shown by the shaded error bars. The observed linear\
    \ correlation between N and \u03C1 is Corr=\u22120.37 ; the Mutual Information\
    \ is I=0.05 , if normalized to [0...1] according to the method defined in [26],\
    \ it remains I n =0.18 ."
  Figure 5 Link: articels_figures_by_rev_year\2019\Absolute_Cluster_Validity\figure_5.jpg
  Figure 5 caption: Cluster 9 embraces two well-differentiated group of objects. The
    univariate analysis of feature distributions by means of kernel density estimations
    reveals this fact by detecting multimodality in x .
  Figure 6 Link: articels_figures_by_rev_year\2019\Absolute_Cluster_Validity\figure_6.jpg
  Figure 6 caption: Representation of the input space as a set of massive hyperspheres.
    Only magnitudes for calculating Gmathrmstr are shown. Note that oimathrmstr,1
    and oimathrmstr,2 are negative, whereas oimathrmstr,3 is positive.
  Figure 7 Link: articels_figures_by_rev_year\2019\Absolute_Cluster_Validity\figure_7.jpg
  Figure 7 caption: 'Fuzzy-clustering example: a) input space; b) core and extended
    volumes; c) centroids and membership isolines.'
  Figure 8 Link: articels_figures_by_rev_year\2019\Absolute_Cluster_Validity\figure_8.jpg
  Figure 8 caption: 'Cluster kinship types: a) unrelated, b) friends, c) relatives,
    d) parent and child.'
  Figure 9 Link: articels_figures_by_rev_year\2019\Absolute_Cluster_Validity\figure_9.jpg
  Figure 9 caption: Example of a noisy dataset. a) Solution with k=3 ; b) solution
    with k=5 ; c) solution with k=4 ; d) solution with k=4 and outlier detection (outliers
    are drawn bigger and gray instead of black).
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "F\xE9lix Iglesias"
  Name of the last author: Arthur Zimek
  Number of Figures: 14
  Number of Tables: 8
  Number of authors: 3
  Paper title: Absolute Cluster Validity
  Publication Date: 2019-04-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Notation
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Results of the Multimodalityc Test (10000 Datasets: 1984
      without mm, 8016 with mm)'
  Table 3 caption:
    table_text: TABLE 3 Context Interpretation by G G Indices
  Table 4 caption:
    table_text: 'TABLE 4 Example 1: Validity Indices for Different Values of k k'
  Table 5 caption:
    table_text: 'TABLE 5 Example 2: Validity Indices for Different Values of k k'
  Table 6 caption:
    table_text: 'TABLE 6 Example 4: Validity Indices for Different Algorithms'
  Table 7 caption:
    table_text: 'TABLE 7 Example 5: Validity Indices for Different Values of k k'
  Table 8 caption:
    table_text: TABLE 8 Scores of Validity Tests, N>2 N>2 Datasets
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2912970
- Affiliation of the first author: department of systems science, graduate school
    of informatics, kyoto university, kyoto, japan
  Affiliation of the last author: department of systems science, graduate school of
    informatics, kyoto university, kyoto, japan
  Figure 1 Link: articels_figures_by_rev_year\2019\Properties_of_Mean_Shift\figure_1.jpg
  Figure 1 caption: "Comparison of improvementascentLipschitz balls for the experiment\
    \ in Example 1. The KDE is plotted as contour lines: the KDE is higher for red\
    \ and is lower for blue. The black points represent mode estimate sequence y t\
    \ t=0,\u2026,10 . The blueredgreen circles represent the improvementascentLipschitz\
    \ balls, respectively."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Properties_of_Mean_Shift\figure_2.jpg
  Figure 2 caption: "KDE and modedensity estimate paths for the experiment in Example\
    \ 2. The red and blue dotted lines represent the locations of the modes near x=0.5\
    \ and \u22120.5 , respectively, and the density estimates at these modes. (a)\
    \ shows the KDE. (b-1), \u2026 ,(f-1) show the mode estimate path M(\u03C4) in\
    \ a solid line and the mode estimate sequence y t t=0,\u2026,10 as points. (b-2),\
    \ \u2026 ,(f-2) show the density estimate path D(\u03C4) in a solid line and the\
    \ density estimate sequence p ( y t ) t=0,\u2026,10 as points. (b-1),(b-2), \u2026\
    \ ,(f-1),(f-2) are the results with \u03F5=0.5,1,1.1,1.9,2.1 , respectively."
  Figure 3 Link: articels_figures_by_rev_year\2019\Properties_of_Mean_Shift\figure_3.jpg
  Figure 3 caption: "KDE and modedensity estimate paths for the experiment in Example\
    \ 3. The red and blue dotted lines represent the locations of the modes near x=0.5\
    \ and \u22120.5 , respectively, and the density estimates at these modes. (a)\
    \ shows the KDE. (b-1) and (c-1) show the mode estimate paths M(\u03C4) in solid\
    \ lines and the mode estimate sequences y t t=0,\u2026,50 as points. (b-2) and\
    \ (c-2) show the density estimate paths D(\u03C4) in solid lines and the density\
    \ estimate sequences p ( y t ) t=0,\u2026,50 as points. (b-1),(b-2) and (c-1),(c-2)\
    \ are the results with \u03F5=1,1.9 , respectively."
  Figure 4 Link: articels_figures_by_rev_year\2019\Properties_of_Mean_Shift\figure_4.jpg
  Figure 4 caption: "KDE and modedensity estimate paths for the experiment in Example\
    \ 5. The red and blue dotted lines represent the locations of the modes near x=0.5\
    \ and \u22120.5 , respectively, and the density estimates at these modes. (a)\
    \ shows the KDE. (b-1) and (c-1) show the mode estimate paths M(\u03C4) in solid\
    \ lines and the mode estimate sequences y t t=0,\u2026,5 as points. (b-2) and\
    \ (c-2) show the density estimate paths D(\u03C4) in solid lines and the density\
    \ estimate sequences p ( y t ) t=0,\u2026,5 as points. (b-1),(b-2) and (c-1),(c-2)\
    \ are the results with y 0 =\u22121.5,\u22125 , respectively."
  Figure 5 Link: articels_figures_by_rev_year\2019\Properties_of_Mean_Shift\figure_5.jpg
  Figure 5 caption: KDE and modedensity estimate paths for the experiment in Example
    6. The red and blue dotted lines represent the locations of the modes near x=0.5
    and -0.5 , respectively, and the density estimates at these modes. (a) shows the
    KDE. (b-1) and (c-1) show the mode estimate paths M(tau) in solid lines and the
    mode estimate sequences lbrace ytrbrace t=0,ldots, 5 as points. (b-2) and (c-2)
    show the density estimate paths D(tau) in solid lines and the density estimate
    sequences lbrace hatp(yt)rbrace t=0,ldots, 5 as points. (b-1),(b-2) and (c-1),(c-2)
    are the results with y0=-1.5, -5 , respectively.
  Figure 6 Link: articels_figures_by_rev_year\2019\Properties_of_Mean_Shift\figure_6.jpg
  Figure 6 caption: 'KDE and mode estimate sequence for the experiment in Example
    7. The KDE is plotted as contour lines: the KDE is higher for red and is lower
    for blue. The mode estimate sequence shown in red (resp. blue) is obtained via
    the iteration boldsymbol yt+1=boldsymbol yt+boldsymbol m(boldsymbol yt) (resp.
    boldsymbol yt+1=boldsymbol yt+0.1boldsymbol m(boldsymbol yt) ) from boldsymbol
    y0 , and converges to the point tildeboldsymbol yapprox (1.69,0.00)T (resp. boldsymbol
    yapprox (-1.46,-0.50)T ).'
  Figure 7 Link: articels_figures_by_rev_year\2019\Properties_of_Mean_Shift\figure_7.jpg
  Figure 7 caption: Sample points and conditional mode estimates obtained by Algorithm
    1 with l=30 for the numerical experiment in Section 4.2. The black dots represent
    the sample points and red circles represent the mode estimates of the conditional
    PDF.
  Figure 8 Link: articels_figures_by_rev_year\2019\Properties_of_Mean_Shift\figure_8.jpg
  Figure 8 caption: The number of the overlooked conditional modes (per 386) and the
    total execution time (in second) by Algorithms 1, 2, and 3 for the numerical experiment
    in Section 4.2. The vertical axis is log-scaled and the horizontal axis is scaled
    by the so-called log-plus-one transformation log (cdot +1) .
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ryoya Yamasaki
  Name of the last author: Toshiyuki Tanaka
  Number of Figures: 8
  Number of Tables: 1
  Number of authors: 2
  Paper title: Properties of Mean Shift
  Publication Date: 2019-04-26 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Classification of the Main Results According to Problem Dimension
      and Kernel Function
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2913640
- Affiliation of the first author: state key laboratory of computer science, institute
    of software, chinese academy of sciences, beijing, china
  Affiliation of the last author: state key laboratory of computer science, institute
    of software, chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2019\SqueezeandExcitation_Networks\figure_1.jpg
  Figure 1 caption: A squeeze-and-excitation block.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\SqueezeandExcitation_Networks\figure_2.jpg
  Figure 2 caption: The schema of the original Inception module (left) and the SE-Inception
    module (right).
  Figure 3 Link: articels_figures_by_rev_year\2019\SqueezeandExcitation_Networks\figure_3.jpg
  Figure 3 caption: The schema of the original Residual module (left) and the SE-ResNet
    module (right).
  Figure 4 Link: articels_figures_by_rev_year\2019\SqueezeandExcitation_Networks\figure_4.jpg
  Figure 4 caption: Training baseline architectures and their SENet counterparts on
    ImageNet. SENets exhibit improved optimisation characteristics and produce consistent
    gains in performance which are sustained throughout the training process.
  Figure 5 Link: articels_figures_by_rev_year\2019\SqueezeandExcitation_Networks\figure_5.jpg
  Figure 5 caption: SE block integration designs explored in the ablation study.
  Figure 6 Link: articels_figures_by_rev_year\2019\SqueezeandExcitation_Networks\figure_6.jpg
  Figure 6 caption: 'Activations induced by the Excitation operator at different depths
    in the SE-ResNet-50 on ImageNet. Each set of activations is named according to
    the following scheme: SEstageIDblockID. With the exception of the unusual behaviour
    at SE52, the activations become increasingly class-specific with increasing depth.'
  Figure 7 Link: articels_figures_by_rev_year\2019\SqueezeandExcitation_Networks\figure_7.jpg
  Figure 7 caption: "Activations induced by Excitation in the different modules of\
    \ SE-ResNet-50 on image samples from the goldfish and plane classes of ImageNet.\
    \ The module is named \u201CSEstageIDblockID\u201D."
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.75
  Name of the first author: Jie Hu
  Name of the last author: Enhua Wu
  Number of Figures: 7
  Number of Tables: 16
  Number of authors: 5
  Paper title: Squeeze-and-Excitation Networks
  Publication Date: 2019-04-29 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 (Left) ResNet-50 [13]. (Middle) SE-ResNet-50. (Right) SE-ResNeXt-50\
      \ with a 32 \xD7 4d template"
  Table 10 caption:
    table_text: TABLE 10 Single-Crop Error Rates (%) on ImageNet and Parameter Sizes
      for SE-ResNet-50 at Different Reduction Ratios
  Table 2 caption:
    table_text: TABLE 2 Single-Crop Error Rates (%) on the ImageNet Validation Set
      and Complexity Comparisons
  Table 3 caption:
    table_text: TABLE 3 Single-Crop Error Rates (%) on the ImageNet Validation Set
      and Complexity Comparisons
  Table 4 caption:
    table_text: TABLE 4 Classification Error (%) on CIFAR-10
  Table 5 caption:
    table_text: TABLE 5 Classification Error (%) on CIFAR-100
  Table 6 caption:
    table_text: TABLE 6 Single-Crop Error Rates (%) on Places365 Validation Set
  Table 7 caption:
    table_text: TABLE 7 Faster R-CNN Object Detection Results (%) on COCO minival
      Set
  Table 8 caption:
    table_text: "TABLE 8 Single-Crop Error Rates (%) of State-of-the-Art CNNs on ImageNet\
      \ Validation Set with Crop Sizes 224\xD7224 224\xD7224 and 320\xD7320 320\xD7\
      320 299\xD7299 299\xD7299"
  Table 9 caption:
    table_text: TABLE 9 Comparison (%) with State-of-the-art CNNs on ImageNet Validation
      Set Using Larger Crop SizesAdditional Training Data
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2913372
- Affiliation of the first author: computer science and artificial intelligence lab
    (csail), massachusetts institute of technology (mit), cambridge, usa
  Affiliation of the last author: list, cea, palaiseau, france
  Figure 1 Link: articels_figures_by_rev_year\2019\Learning_More_Universal_Representations_for_TransferLearning\figure_1.jpg
  Figure 1 caption: "A universal representation is a set of features learned on a\
    \ source-problem (SP), that exhibits good performances when it is transferred\
    \ on many target-problems (TPs). A representation R A is obtained by a method\
    \ \u24B6 that consists of training a network N on a SP \u24D0. When it is transferred\
    \ on a large set of TPs \u24D4, it leads to an aggregated performance P A . We\
    \ propose to build a more universal representation R B by: (i) applying a source-problem\
    \ variation (SPV) \u24D2 to transform an initial SP \u24D0 into a new one \u24D1\
    \ (ii) leaning features on (b) with the same method (yellow N ) and combining\
    \ the resulting features with those of \u24B6 (iii) re-training both networks\
    \ on their initial SP through \u24D3, a method named Focused ReTraining (FoRT).\
    \ From the evaluation on the multiple TPs: P B > P A , thus the resulting representation\
    \ R B is more universal than R A ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Learning_More_Universal_Representations_for_TransferLearning\figure_2.jpg
  Figure 2 caption: "Focused ReTraining (FoRT). An initial network (gray branch) denoted\
    \ initNet (first layers weights denoted \u03B8 a 1 , last ones \u03B8 a 2 and\
    \ the classifier \u03A8 a ) is trained on a source-database, by minimizing the\
    \ loss-function L . Once initNet trained, another network (black branch) denoted\
    \ FoRT (first layers weights denoted \u03B8 b 1 , last ones \u03B8 b 2 and the\
    \ classifier \u03A8 b ) is trained on the same source-database and solves the\
    \ same problem (minimize same loss L ). The weights \u03B8 b 1 (black block filled\
    \ in gray) of the first layers of FoRT are initialized with the firsts of initNet\
    \ ( \u03B8 a 1 ) and weakly updated through training. The weights of the last\
    \ layers \u03B8 b 2 (black blocks filled in white) are randomly initialized and\
    \ fully trained."
  Figure 3 Link: articels_figures_by_rev_year\2019\Learning_More_Universal_Representations_for_TransferLearning\figure_3.jpg
  Figure 3 caption: "An initial source problem (SP) D S 0 contains a set of images\
    \ and their associated labels ( x 0 i , y 0 i ) i\u2208[[1,N]] . MulDiP-Net has\
    \ three steps: (i) variation ( \u03D1 ) of the initial SP ( D 0 ) into new ones\
    \ ( D k ); (ii) training networks ( \u03D5 and \u03A8 ) on the whole set of SPs\
    \ ( D 0 , D k ); and (iii) normalization ( Z ) followed by combination ( F ) of\
    \ the features extracted from each trained network in order to form a more universal\
    \ representation. Phase (i) is a source problem variation (SPV) ( \u03D1 k 0 )\
    \ applied on the initial SP ( D S 0 ), which outputs a new SP ( D S k,k>0 ). After\
    \ applying K SPV functions, we get a set of K +1 SPs containing the new SPs and\
    \ the initial one (only one variation illustrated here, thus K=1 ). Phase (ii)\
    \ consists of learning one network for each of the K +1 SPs, resulting in a set\
    \ of K +1 trained networks: N k k=[[0,K]] . Each network N k (that is a composition\
    \ of a features-extractor \u03D5 k and a classifier \u03A8 k ) is trained by minimizing\
    \ the loss function L k computed using the output of the predictor \u03A8 k and\
    \ the ground-truth of the SP D S k . Phase (iii) consists of extracting the more\
    \ universal representations R i,\u03C4 of the images I \u03C4 i of a target task\
    \ \u03C4 , obtained through an independent normalization ( Z ) and merging ( F\
    \ ) of the features computed by the extractor \u03D5 k ."
  Figure 4 Link: articels_figures_by_rev_year\2019\Learning_More_Universal_Representations_for_TransferLearning\figure_4.jpg
  Figure 4 caption: "Illustration of three types of SPV: Splitting, Adding and Grouping.\
    \ An initial source problem (SP) D S 0 is illustrated in \u24B6. \u24B7 is the\
    \ output of the splitting SPV ( \u03D1 S ) which results in two smaller sets of\
    \ training data. In contrast, in \u24B8, an adding SPV ( \u03D1 A ) results in\
    \ a larger set of training data (more images and categories). The grouping SPV\
    \ ( \u03D1 G ) in \u24B9 results in the same amount of training-data (same images\
    \ but labeled according less categories)."
  Figure 5 Link: articels_figures_by_rev_year\2019\Learning_More_Universal_Representations_for_TransferLearning\figure_5.jpg
  Figure 5 caption: "Illustration of our grouping SPV. Given a set of specific categories\
    \ (leaf nodes of the hierarchy H in white diamonds) and a set of generic categories\
    \ (here c 1 i ) at a certain level (here categorical denoted B cat L ), our grouping\
    \ SPV \u03D1 G consists of three steps: Computation of all descendants of c 1\
    \ i according to the H (1); computation of the descendants that belong to the\
    \ initial set of categories (2), producing a group G 0 j ; and re-labeling of\
    \ the categories (as well as their images) of the later group (3) into the categories\
    \ of B cat L . The first two points correspond to Eq. (3), while the last one\
    \ corresponds to Eq. (4)."
  Figure 6 Link: articels_figures_by_rev_year\2019\Learning_More_Universal_Representations_for_TransferLearning\figure_6.jpg
  Figure 6 caption: Illustration of the different baseline methods.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Youssef Tamaazousti
  Name of the last author: Mohamed Tamaazousti
  Number of Figures: 6
  Number of Tables: 6
  Number of authors: 5
  Paper title: Learning More Universal Representations for Transfer-Learning
  Publication Date: 2019-04-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Universality Evaluation-Metrics
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Source-Datasets (Top) and Target Datasets (Bottom) Used in
      This Paper
  Table 3 caption:
    table_text: TABLE 3 Comparison of Our Methods (Bottom) to State-of-the-Art Universalizing-Methods
      (Top)
  Table 4 caption:
    table_text: TABLE 4 Comparison of Our Universalizing-Methods to Several Baselines,
      in Terms of mNRG (Last Column)
  Table 5 caption:
    table_text: TABLE 5 MulDiP-Net Performances with Different Network Architectures
      and More Training Data
  Table 6 caption:
    table_text: 'TABLE 6 Performances of MulDip Compared to Net-S in Three Cases:
      Standard Transfer Learning (Top), with an Additive Fine-Tuning (Middle) or Residual
      Adapters (Bottom)'
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2913857
- Affiliation of the first author: college of computer science, nankai university,
    tianjin, china
  Affiliation of the last author: school of engineering, university of california,
    merced, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Subspace_Clustering_via_Good_Neighbors\figure_1.jpg
  Figure 1 caption: "Main steps of the proposed FGNSC algorithm. We first organize\
    \ the data matrix X from input image dataset I . Then, we compute the initial\
    \ coefficient matrix Z from X according to (1). Using Algorithm 1 with Z , we\
    \ obtain the matrix of good neighbors N . Then, Z \u2217 is computed by assigning\
    \ new coefficients to the good neighbors and eliminating the other values. Here,\
    \ Z \u2217 maintains both the sparsity and connectivity, as it preserves few non-zero\
    \ elements with strong connections. Lastly, we apply the classic spectral clustering\
    \ for the final segmentation result L . (a) Calculating (1). (b) Finding good\
    \ neighbors, as shown in Algorithm 1. (c) Generating Z \u2217 by computing (7).\
    \ (d) Conducting spectral clustering."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Subspace_Clustering_via_Good_Neighbors\figure_2.jpg
  Figure 2 caption: "Clustering performance with different \u03B3 on the extended\
    \ Yale B dataset of 8 classes."
  Figure 3 Link: articels_figures_by_rev_year\2019\Subspace_Clustering_via_Good_Neighbors\figure_3.jpg
  Figure 3 caption: "Affinity matrix W derived by six methods on the first three digits\
    \ of the MNIST dataset. (a) Affinity matrix by LSR [13] with an accuracy of 64.89\
    \ percent. (b) Affinity matrix by SMR [14] with an accuracy of 83.11 percent.\
    \ (c) Affinity matrix by SSC-OMP [15], which is sufficiently sparse enough but\
    \ does not have the block-diagonal property. (d) Affinity matrix by ORGEN [10]\
    \ with an accuracy of 53.22 percent and an NMI of 0.4912 . (e) A variant of (b)\
    \ that simply preserves the top \u03B3 entries for each column, resulting in an\
    \ accuracy of 81.33 percent and an NMI of 0.4835 . (f) Affinity matrix by FGNSC\
    \ with an accuracy of 98.11 percent and an NMI of 0.9140 ."
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jufeng Yang
  Name of the last author: Ming-Hsuan Yang
  Number of Figures: 3
  Number of Tables: 4
  Number of authors: 5
  Paper title: Subspace Clustering via Good Neighbors
  Publication Date: 2019-04-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Clustering Results by the SSC-OMP, iPursuit and FGNSC Methods
      on Subsets of the Extended Yale B Dataset Using N e Ne and ACC
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Run-Time in Seconds (s) on the Subsets of the Extended Yale
      B Database
  Table 3 caption:
    table_text: 'TABLE 3 Clustering Results of the Comparative Methods on Five Datasets:
      Extended Yale B [49], COIL-20 [50], MNIST [51], USPS [52] and AR [53], Where
      Each Result Is an Average of Fifty Trails'
  Table 4 caption:
    table_text: TABLE 4 More Results from Combining the Proposed FGN with Other Self-Representation
      Schemes on the Extended Yale B Dataset (e.g., FGN-LSR is the Combination of
      the FGN in Algorithm 1 and the Linear Self-Representation Module in LSR [13])
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2913863
- Affiliation of the first author: college of computer science and technology, zhejiang
    university, hangzhou, china
  Affiliation of the last author: center for future media and school of computer science
    and engineering, university of electronic science and technology of china, chengdu,
    china
  Figure 1 Link: articels_figures_by_rev_year\2019\The_Gap_of_Semantic_Parsing_A_Survey_on_Automatic_Math_Word_Problem_Solvers\figure_1.jpg
  Figure 1 caption: Technology evolving trend in solving MWPs.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\The_Gap_of_Semantic_Parsing_A_Survey_on_Automatic_Math_Word_Problem_Solvers\figure_2.jpg
  Figure 2 caption: An example of arithmetic word problem.
  Figure 3 Link: articels_figures_by_rev_year\2019\The_Gap_of_Semantic_Parsing_A_Survey_on_Automatic_Math_Word_Problem_Solvers\figure_3.jpg
  Figure 3 caption: Examples of expression tree and equation tree for Fig. 2.
  Figure 4 Link: articels_figures_by_rev_year\2019\The_Gap_of_Semantic_Parsing_A_Survey_on_Automatic_Math_Word_Problem_Solvers\figure_4.jpg
  Figure 4 caption: An example of equation set problem.
  Figure 5 Link: articels_figures_by_rev_year\2019\The_Gap_of_Semantic_Parsing_A_Survey_on_Automatic_Math_Word_Problem_Solvers\figure_5.jpg
  Figure 5 caption: An example of geometric problem.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Dongxiang Zhang
  Name of the last author: Heng Tao Shen
  Number of Figures: 5
  Number of Tables: 5
  Number of authors: 5
  Paper title: 'The Gap of Semantic Parsing: A Survey on Automatic Math Word Problem
    Solvers'
  Publication Date: 2019-04-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Statistics of Arithmetic Word Problem Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Accuracy of Statistic-Based and Tree-Based Methods in Solving
      Arithmetic Problems
  Table 3 caption:
    table_text: TABLE 3 Statistics of Datasets for Equation Set Problems
  Table 4 caption:
    table_text: TABLE 4 Accuracies of Equation Set Problem Solvers on Existing Datasets
  Table 5 caption:
    table_text: TABLE 5 Common Features
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2914054
