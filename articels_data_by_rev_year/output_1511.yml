- Affiliation of the first author: department of electronic engineering, school of
    engineering, sogang university, mapo-gu, seoul, korea
  Affiliation of the last author: department of electronic engineering, school of
    engineering, sogang university, mapo-gu, seoul, korea
  Figure 1 Link: articels_figures_by_rev_year\2019\ComplexValued_Disparity_Unified_Depth_Model_of_Depth_from_Stereo_Depth_from_Focu\figure_1.jpg
  Figure 1 caption: Signal processing viewpoint of the local DFLF methods with respect
    to the sign function. (a) Relationship between the Dirac delta function and the
    cost-based methods. (b) Relationship between the sign function and the FBS-based
    methods. (c) Relationship among the absolute linear function, the proposed depth
    model, and the depth model-based methods.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\ComplexValued_Disparity_Unified_Depth_Model_of_Depth_from_Stereo_Depth_from_Focu\figure_2.jpg
  Figure 2 caption: "Examples of disparity responses of the proposed depth model.\
    \ (a) Reference view of MonasRoom image in dataset [38] (circle: foreground, square:\
    \ in-focus plane, and diamond: background). (b) Five samples of GT disparity maps\
    \ by moving zero disparity and those of the real response (average gradient approach)\
    \ by the LF re-parameterization (number in parentheses represents the disparity\
    \ index). (c)\u2013(f) Disparity responses of the average gradient and LS gradient\
    \ approaches at three dots: real, imaginary, magnitude, and angle responses, respectively."
  Figure 3 Link: articels_figures_by_rev_year\2019\ComplexValued_Disparity_Unified_Depth_Model_of_Depth_from_Stereo_Depth_from_Focu\figure_3.jpg
  Figure 3 caption: Relationship between the spatial and angular gradients of LF.
    (a) LF gradients in the spatial and angular coordinates. (b) LF gradients in the
    horizontal and vertical EPIs. (c) Projection of LF gradients onto the spatial
    coordinates.
  Figure 4 Link: articels_figures_by_rev_year\2019\ComplexValued_Disparity_Unified_Depth_Model_of_Depth_from_Stereo_Depth_from_Focu\figure_4.jpg
  Figure 4 caption: Relationship among the DFS, DFF, and DFD methods.
  Figure 5 Link: articels_figures_by_rev_year\2019\ComplexValued_Disparity_Unified_Depth_Model_of_Depth_from_Stereo_Depth_from_Focu\figure_5.jpg
  Figure 5 caption: 'Comparison of local disparity maps of the proposed depth model
    and the existing methods on datasets [8], [38], and [24] (from top to bottom:
    AvgDFS, AvgDFF, AvgDFD, AvgRe, LSDFS, LSDFF, LSDFD, LSRe, FBS, LF, CCDDFS, CCDDFF,
    CAE, CAD, AE, AD, SPOh, SPOv, EPIh, and EPIv). (a) Disparity maps of Cotton and
    Dino in category Training of dataset [8]. (b) Disparity maps of Buddha and Papillon
    in category Blender of dataset [38]. (c) Disparity maps of Dots in dataset [8].
    (d) Disparity maps of two sample images in the real dataset [24].'
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.83
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.94
  Name of the first author: Jae Young Lee
  Name of the last author: Rae-Hong Park
  Number of Figures: 5
  Number of Tables: 2
  Number of authors: 2
  Paper title: 'Complex-Valued Disparity: Unified Depth Model of Depth from Stereo,
    Depth from Focus, and Depth from Defocus Based on the Light Field Gradient'
  Publication Date: 2019-10-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of Existing Local DFLF Methods
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Evaluation in Terms of the Average of the BP,
      MSE, Q50, Occ, and NOcc
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2946159
- Affiliation of the first author: department of automation, institute for brain and
    cognitive sciences, beijing key laboratory of multi-dimension & multi-scale computational
    photography (mmcp), tsinghua university, beijing, china
  Affiliation of the last author: department of automation, institute for brain and
    cognitive sciences, beijing key laboratory of multi-dimension & multi-scale computational
    photography (mmcp), tsinghua university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2019\Sinusoidal_Sampling_Enhanced_Compressive_Camera_for_High_Speed_Imaging\figure_1.jpg
  Figure 1 caption: "The scheme of the proposed combinational compressive video acquisition\
    \ approach. In spatial domain, the scheme adopts superimposition of random and\
    \ sinusoidal codes for modulation: the former compressively encodes m -frame groups\
    \ and then the latter further encodes each group with a specific frequency. At\
    \ the camera sensor, these m\xD7n frames are integrated into a snapshot. Due to\
    \ the impulse-shaped frequency distribution, the introduced sinusoidal modulation\
    \ maps the compressively encoded measurements of n groups to different positions\
    \ in the Fourier domain. The reconstruction of m\xD7n frames is conducted via\
    \ Fourier transformation, decoupling of n groups, inverse Fourier transformation,\
    \ and compressive sensing based reconstruction, successively."
  Figure 10 Link: articels_figures_by_rev_year\2019\Sinusoidal_Sampling_Enhanced_Compressive_Camera_for_High_Speed_Imaging\figure_10.jpg
  Figure 10 caption: 'The PSNR, RMSE, SSIM, and running time comparison with state-of-the-art
    high speed imaging methods. (a) The averaged image quality and running time of
    different methods on a 100-sequence data set, with L ranging from 10 to 130 (the
    corresponding compression ratio of S2EC2 is ranging from 0.118 to 0.009). (b)
    The performance comparison of an example at L=128 (compression ratio of S2EC2
    is 0.009). First row: Ground truth of one exemplar frame and the coded measurement
    of our encoding scheme. Second and third rows: Reconstruction of three recently
    proposed method and our approach: the frame using GAP under conventional compressive
    sensing (GAP), TwIST under conventional compressive sensing (TwIST), pixel-wise
    coded exposure based on over-complete dictionary (PCE), and result of our approach
    (S2EC2), respectively.'
  Figure 2 Link: articels_figures_by_rev_year\2019\Sinusoidal_Sampling_Enhanced_Compressive_Camera_for_High_Speed_Imaging\figure_2.jpg
  Figure 2 caption: "Coding scheme of the proposed S2EC2 architecture: During the\
    \ exposure time T of the low-frame-rate camera, SLM plays n\xD7m binary codes,\
    \ which is an exhaustive combination of n sinusoidal and m random codes. Specifically,\
    \ n sinusoidal codes (period being T 1 =Tn ) and m different random codes (period\
    \ being T 2 = T 1 m ) are superimposed in dot-product manner within elapse of\
    \ each sinusoidal code."
  Figure 3 Link: articels_figures_by_rev_year\2019\Sinusoidal_Sampling_Enhanced_Compressive_Camera_for_High_Speed_Imaging\figure_3.jpg
  Figure 3 caption: "The illustration of the encoding and decoding by the combinational\
    \ codes. Here, we take m=16 , n=8 as an example. For each of the eight groups,\
    \ random codes r 1 \u22EF r 16 encode a short sequence compressively and generate\
    \ a coded measurement with concentratively distributed frequencies. The eight\
    \ sinusoidal codes s 1 \u22EF s 8 shift the frequency components of these coded\
    \ measurements by different offsets and thus their dominant frequencies are staggered\
    \ in the Fourier domain. For each group, the shifted frequency has three replicas:\
    \ one at the centroid and two symmetric conjugates located aside (shown by the\
    \ solid and dashed rectangle with the same color). During decoding, we separate\
    \ the eight compressed measurements by cropping out corresponding frequencies\
    \ framed by the solid rectangles (or averaging the conjugate pair) and subsequent\
    \ optimization based algorithmic decoupling."
  Figure 4 Link: articels_figures_by_rev_year\2019\Sinusoidal_Sampling_Enhanced_Compressive_Camera_for_High_Speed_Imaging\figure_4.jpg
  Figure 4 caption: The fast gray scale modulation using a DMD. (a) The target gray
    scale combinational code, and its representation accuracy via direct binarization
    and FFS modulation. (b) The optical design of the FFS system. The DMD is decomposed
    into two regions, and optically two regions locate at the spatial and Fourier
    plane of a 4f system [39]. The left region displays binary codes and the right
    conducts Fourier-domain filtering to generate gray scale counterpart. Both spatial
    patterning and Fourier domain filtering are binary, and thus the DMD works at
    high frame rate.
  Figure 5 Link: articels_figures_by_rev_year\2019\Sinusoidal_Sampling_Enhanced_Compressive_Camera_for_High_Speed_Imaging\figure_5.jpg
  Figure 5 caption: The light path (a) and implementation of our prototype (b). The
    light from the fast dynamic scene first goes through the primary camera lens,
    and then relayed to the DMD plane for modulation. The Frequency Filtering System
    (FFS) module comprised of DMD, concave mirror, and Fourier lens, is designed for
    limiting the bandwidth of the coded measurements for each group and mitigating
    the system error in high speed coding. The outgoing light of the FFS module is
    finally captured by the low frame rate camera. DMD and camera are synchronized
    at high precision via DMDs triggering signal.
  Figure 6 Link: articels_figures_by_rev_year\2019\Sinusoidal_Sampling_Enhanced_Compressive_Camera_for_High_Speed_Imaging\figure_6.jpg
  Figure 6 caption: "The performance at varying blur kernel sizes of within-group\
    \ random codes and Fourier domain cropping sizes for group decoupling. We test\
    \ the fidelity of the decoupling by comparing the reconstructed compressively\
    \ sensed measurement with the corresponding ground truth. (a) The averaged RMSE\
    \ scores (log scale) of different sizes of Gaussian blur kernel and Fourier cropping\
    \ window based on the data set of 100 128-frame sequences. The optimum value of\
    \ RMSE is achieved when kernel size is five pixels and Fourier domain cropping\
    \ size is 136 pixels. (b) The visual comparison on the performances at varying\
    \ blur kernel sizes ( 0\u223C7 , marked in the top row), with respective best\
    \ cropping size (marked in the bottom row). The data is from 128 frames of the\
    \ \u201Drunning dog\u201D video, with imaging size being 1024 \xD7 1024 pixels.\
    \ BR: blurry random code; MS: the modulated snapshot of one group; FS: Fourier\
    \ spectrum of the snapshot MS; CFS: combined Fourier spectrum after modulated\
    \ by eight sinusoidal codes; RRes: reconstruction residue of the coded measurement\
    \ MS by applying Fourier domain decoupling and zero padding to CFS. MFeq: missing\
    \ frequency of reconstruction with respect to MS. We show the reconstruction residue\
    \ to measure the fidelity of group decoupling."
  Figure 7 Link: articels_figures_by_rev_year\2019\Sinusoidal_Sampling_Enhanced_Compressive_Camera_for_High_Speed_Imaging\figure_7.jpg
  Figure 7 caption: The performance comparison between random codes and their blurry
    counterparts on an isolated group evaluated in PSNR and RMSE.
  Figure 8 Link: articels_figures_by_rev_year\2019\Sinusoidal_Sampling_Enhanced_Compressive_Camera_for_High_Speed_Imaging\figure_8.jpg
  Figure 8 caption: The average resolution loss of sinusoidal coding using 100 8-frame
    video sequences.
  Figure 9 Link: articels_figures_by_rev_year\2019\Sinusoidal_Sampling_Enhanced_Compressive_Camera_for_High_Speed_Imaging\figure_9.jpg
  Figure 9 caption: "Quantitative analysis on the performances at different m and\
    \ n , in terms of PSNR (a), RMSE (b) and SSIM (c). The black dashed curves specify\
    \ the performances at two fixed frame rate enhancement\u2014 m\xD7n=24 and m\xD7\
    n=36 , respectively. The three dashed lines highlight the performance when applying\
    \ varying number of sinusoid codes at a fixed compressive sensing ratio."
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.76
  Name of the first author: Chao Deng
  Name of the last author: Qionghai Dai
  Number of Figures: 14
  Number of Tables: 0
  Number of authors: 7
  Paper title: Sinusoidal Sampling Enhanced Compressive Camera for High Speed Imaging
  Publication Date: 2019-10-11 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2946567
- Affiliation of the first author: ncmis, cems, rcsds, academy of mathematics and
    systems science, chinese academy of sciences, beijing, china
  Affiliation of the last author: ncmis, cems, rcsds, academy of mathematics and systems
    science, chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2019\Bayesian_Joint_Matrix_Decomposition_for_Data_Integration_with_Heterogeneous_Nois\figure_1.jpg
  Figure 1 caption: Graphical models of (a) BJMD and (b) the reformulated BJMD.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Bayesian_Joint_Matrix_Decomposition_for_Data_Integration_with_Heterogeneous_Nois\figure_2.jpg
  Figure 2 caption: "Illustration of the convergence behavior of VI-BJMD using a small-scale\
    \ synthetic dataset with \u03C3 3 =4.0. (a) The ELBO versus the iteration numbers,\
    \ (b) the relative change of the coefficient matrices in percentage, and (c) the\
    \ AUCs versus the iteration numbers for each source."
  Figure 3 Link: articels_figures_by_rev_year\2019\Bayesian_Joint_Matrix_Decomposition_for_Data_Integration_with_Heterogeneous_Nois\figure_3.jpg
  Figure 3 caption: True noise histograms and the estimated PDF (shown with curves)
    by JMD (top), VI-BJMD (middle), and MAP-BJMD (bottom), respectively. The true
    standard deviation of source 1, source 2, and source 3 are 1.0, 2.5, and 4.0,
    respectively.
  Figure 4 Link: articels_figures_by_rev_year\2019\Bayesian_Joint_Matrix_Decomposition_for_Data_Integration_with_Heterogeneous_Nois\figure_4.jpg
  Figure 4 caption: "The objective values versus the iteration numbers for methods\
    \ on a small-scale synthetic dataset with \u03C3 3 =4.0."
  Figure 5 Link: articels_figures_by_rev_year\2019\Bayesian_Joint_Matrix_Decomposition_for_Data_Integration_with_Heterogeneous_Nois\figure_5.jpg
  Figure 5 caption: The objective values versus the iteration numbers for methods
    on a large-scale synthetic dataset with sigma 3 =4.0.
  Figure 6 Link: articels_figures_by_rev_year\2019\Bayesian_Joint_Matrix_Decomposition_for_Data_Integration_with_Heterogeneous_Nois\figure_6.jpg
  Figure 6 caption: Performance comparison of JMD, VI-BJMD, and MAP-BJMD and their
    respective version ignoring heterogeneous noise on the small-scale synthetic datasets.
    The standard deviation of the Gaussian noise is fixed in Source 1 and Source 2.
    The noise levels of Source 3 range from 1.5 to 5.5.
  Figure 7 Link: articels_figures_by_rev_year\2019\Bayesian_Joint_Matrix_Decomposition_for_Data_Integration_with_Heterogeneous_Nois\figure_7.jpg
  Figure 7 caption: Performance comparison of VI-BJMD and MAP-BJMD (as well as VI-catBJMD,
    MAP-catBJMD) on the large-scale synthetic datasets. The settings are the same
    as those in Fig. 6.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.63
  Name of the first author: Chihao Zhang
  Name of the last author: Shihua Zhang
  Number of Figures: 7
  Number of Tables: 8
  Number of authors: 2
  Paper title: Bayesian Joint Matrix Decomposition for Data Integration with Heterogeneous
    Noise
  Publication Date: 2019-10-11 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Performance Comparison on a Small-Scale Synthetic Dataset\
      \ with \u03C3 3 \u03C33=4.0"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Performance Comparison on a Large-Scale Synthetic Dataset\
      \ with \u03C3 3 \u03C33=4.0"
  Table 3 caption:
    table_text: TABLE 3 Summary of the Three Data Sets
  Table 4 caption:
    table_text: TABLE 4 The Noise Levels Estimated in Each Dataset
  Table 5 caption:
    table_text: TABLE 5 Performance Comparison of Different Methods on the 3Sources
      Dataset
  Table 6 caption:
    table_text: TABLE 6 Performance Comparison of Different Methods on the Extended
      Yale Face B Dataset
  Table 7 caption:
    table_text: TABLE 7 Performance Comparison of Different Methods on the METABRIC
      Dataset
  Table 8 caption:
    table_text: TABLE 8 Performance Comparison of Different Methods on the Filtered
      METABRIC Dataset
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2946370
- Affiliation of the first author: shanghai jiao tong university, shanghai, china
  Affiliation of the last author: shanghai jiao tong university, shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2019\FineGrained_Video_Captioning_via_Graphbased_MultiGranularity_Interaction_Learnin\figure_1.jpg
  Figure 1 caption: Fine-grained video captioning task versus conventional video captioning.
    Fine-grained video caption sentences have fine-grained action details (marked
    in red) and multi-granular interactions (marked in blue). Best viewed in colors.
  Figure 10 Link: articels_figures_by_rev_year\2019\FineGrained_Video_Captioning_via_Graphbased_MultiGranularity_Interaction_Learnin\figure_10.jpg
  Figure 10 caption: Visualizations of the attention weights of some frequently appeared
    verbs. The per team attention weights (A and B) and global interaction weights
    (A+B) are visualized.
  Figure 2 Link: articels_figures_by_rev_year\2019\FineGrained_Video_Captioning_via_Graphbased_MultiGranularity_Interaction_Learnin\figure_2.jpg
  Figure 2 caption: The illustration of our fine-grained video caption model. The
    Individual Action Feature Extraction modeling sub-network encodes the motion details
    of every subject with extracted optical flows and skeleton-related information.
    The extracted features are further clustered based on three different interaction
    patterns with the Multi-granular Interaction Encoding module. After that, the
    spatial and temporal association is exploited by the Multi-granular Attention
    module. Finally, video captions are generated with the guidance of the latent
    state of a single layer LSTM and Multi-granular Attention modules outputs.
  Figure 3 Link: articels_figures_by_rev_year\2019\FineGrained_Video_Captioning_via_Graphbased_MultiGranularity_Interaction_Learnin\figure_3.jpg
  Figure 3 caption: The illustration of the proposed framework of individual action
    modeling. For every frame, the skeletons, segmented regions and optical flows
    of players are extracted. A ten-dimensional descriptor is assigned to every joint
    of skeletons by merging the information of these features. After a step of affine
    transformation, features are converted to point cloud form.
  Figure 4 Link: articels_figures_by_rev_year\2019\FineGrained_Video_Captioning_via_Graphbased_MultiGranularity_Interaction_Learnin\figure_4.jpg
  Figure 4 caption: "The illustration of multi-granular interaction encoding model.\
    \ The PointCloud data of three different interactions serve as the input. In the\
    \ model, the KNN graphs G A , G B , and G A\u222AB are constructed. In the Graph\
    \ CNN, the extracted features are from local to global as layers get deeper. In\
    \ the final layer, three interactions encodings f A , f B , and f A\u222AB are\
    \ outputted."
  Figure 5 Link: articels_figures_by_rev_year\2019\FineGrained_Video_Captioning_via_Graphbased_MultiGranularity_Interaction_Learnin\figure_5.jpg
  Figure 5 caption: "The illustration of the proposed attention model. First, three\
    \ different granular point cloud features ( O 1 , O 2 , O 3 ) are extracted from\
    \ the input frames, with three interaction patterns ( C (i,p) ) included in every\
    \ kind of features. After that, spatial-temporal correlation is excavated by the\
    \ spatial-temporal attention module with weight \u03B1 t . Multi-granular information\
    \ is then combined by temporal attention module with weight \u03B2 t . Each fused\
    \ dense multi-granular feature vector is assigned with corresponding attention\
    \ weight. Given the temporal attention modules output ( B t ) and the latent state\
    \ h t of LSTM, the caption word y t of step t is generated."
  Figure 6 Link: articels_figures_by_rev_year\2019\FineGrained_Video_Captioning_via_Graphbased_MultiGranularity_Interaction_Learnin\figure_6.jpg
  Figure 6 caption: Sample sequences from our dataset. Each player and the ball has
    accurate pixel-level annotation, the figures belowabove the line indicate the
    duration of each captioning event, with caption highlighted in the same color.
    Best viewed in colors.
  Figure 7 Link: articels_figures_by_rev_year\2019\FineGrained_Video_Captioning_via_Graphbased_MultiGranularity_Interaction_Learnin\figure_7.jpg
  Figure 7 caption: The part-of-speech distribution of SVN dataset compared with ActivityNet
    Captions dataset. All the values in this figure are the differences between these
    two datasets in the percentage form. There are more verbs in SVN dataset, as this
    is a fine-grained captioning dataset focusing more on detailed actions.
  Figure 8 Link: articels_figures_by_rev_year\2019\FineGrained_Video_Captioning_via_Graphbased_MultiGranularity_Interaction_Learnin\figure_8.jpg
  Figure 8 caption: Illustration of the relationship between metric score and the
    number of mismatched verbs. 100 pairs of candidate and reference sentences are
    chosen to average the measurement under each case.
  Figure 9 Link: articels_figures_by_rev_year\2019\FineGrained_Video_Captioning_via_Graphbased_MultiGranularity_Interaction_Learnin\figure_9.jpg
  Figure 9 caption: Comparison of paragraphs generated by our full model with its
    downgraded versions (e.g., without optical flow, without segmentation, without
    skeleton, without attention, and without MIE).
  First author gender probability: 0.76
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.69
  Name of the first author: Yichao Yan
  Name of the last author: Wenjun Zhang
  Number of Figures: 11
  Number of Tables: 4
  Number of authors: 12
  Paper title: Fine-Grained Video Captioning via Graph-based Multi-Granularity Interaction
    Learning
  Publication Date: 2019-10-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Good Caption and Bad Caption With the Same METEOR Scores Get
      Different FCE Scores
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparisons of Different Video Caption Datasets
  Table 3 caption:
    table_text: TABLE 3 Comparative Results With Respect to CIDEr-D (C), METEOR (M),
      Bleu (B), Rouge-L (R) and FCE (F) Scores
  Table 4 caption:
    table_text: TABLE 4 The Table Illustrates the CIDEr-D (C), FCE (F) Scores With
      the Multiple-Granular of Quantities and Spatial Temporal Granularities Features
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2946823
- Affiliation of the first author: department of information and communication engineering,
    daegu gyeongbuk institute of science and technology, daegu, republic of korea
  Affiliation of the last author: school of electrical engineering, korea advanced
    institute of science and technology, daejeon, republic of korea
  Figure 1 Link: articels_figures_by_rev_year\2019\Deep_Depth_from_Uncalibrated_Small_Motion_Clip\figure_1.jpg
  Figure 1 caption: A small motion clip is our sole input. Our uncalibrated bundle
    adjustment produces camera intrinsic and extrinsic parameters as well as sparse
    3D points. From this, our deep plane sweep network regresses a dense depth map.
    Please note that our method does not require camera calibration.
  Figure 10 Link: articels_figures_by_rev_year\2019\Deep_Depth_from_Uncalibrated_Small_Motion_Clip\figure_10.jpg
  Figure 10 caption: Depth map results on four-view image sequences from the ETH3D
    dataset.
  Figure 2 Link: articels_figures_by_rev_year\2019\Deep_Depth_from_Uncalibrated_Small_Motion_Clip\figure_2.jpg
  Figure 2 caption: Small motion geometry used in our bundle adjustment for an uncalibrated
    camera (Section 3). We adopt the distorted-to-undistorted mapping function F to
    utilize the inverse depth representation in an analytic form.
  Figure 3 Link: articels_figures_by_rev_year\2019\Deep_Depth_from_Uncalibrated_Small_Motion_Clip\figure_3.jpg
  Figure 3 caption: Comparing reconstructed 3D point clouds. Our approach recovers
    reliable 3D point clouds like the result of Yu and Gallup [8], but our approach
    is additionally capable of camera self-calibration. We use calibrated camera parameters
    to apply [8].
  Figure 4 Link: articels_figures_by_rev_year\2019\Deep_Depth_from_Uncalibrated_Small_Motion_Clip\figure_4.jpg
  Figure 4 caption: Overview of the DPSNet pipeline.
  Figure 5 Link: articels_figures_by_rev_year\2019\Deep_Depth_from_Uncalibrated_Small_Motion_Clip\figure_5.jpg
  Figure 5 caption: Illustration of context-aware cost aggregation.
  Figure 6 Link: articels_figures_by_rev_year\2019\Deep_Depth_from_Uncalibrated_Small_Motion_Clip\figure_6.jpg
  Figure 6 caption: Outputs of the proposed pipeline. (a) Reference image from the
    clip. (b) Our depth result from Section 4. (c) and (d) Estimated 3D points & camera
    poses from Section 3.
  Figure 7 Link: articels_figures_by_rev_year\2019\Deep_Depth_from_Uncalibrated_Small_Motion_Clip\figure_7.jpg
  Figure 7 caption: "Synthetic dataset generation. (a) Blender 3D model with the reference\
    \ camera and its coordinates. (b) Ground truth camera poses (Blue colored) and\
    \ estimated camera poses (Red colored) and 3D points from the proposed bundle\
    \ adjustment ( b=\u22122 )."
  Figure 8 Link: articels_figures_by_rev_year\2019\Deep_Depth_from_Uncalibrated_Small_Motion_Clip\figure_8.jpg
  Figure 8 caption: 'Quantitative evaluation on bundle adjustment with respect to
    the magnitude of the baseline ( x -axis: b= log 10 ( Baseline Min.depth ) ).'
  Figure 9 Link: articels_figures_by_rev_year\2019\Deep_Depth_from_Uncalibrated_Small_Motion_Clip\figure_9.jpg
  Figure 9 caption: Comparison of depth map results on MVS, SUN3D, RGBD, and Scenes11
    (top to bottom).
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.88
  Name of the first author: Sunghoon Im
  Name of the last author: In So Kweon
  Number of Figures: 18
  Number of Tables: 5
  Number of authors: 5
  Paper title: Deep Depth from Uncalibrated Small Motion Clip
  Publication Date: 2019-10-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Evaluation on the Estimated Intrinsic Camera Parameters (i.e.
      Focal Length and Radial Distortion)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison Results
  Table 3 caption:
    table_text: TABLE 3 Comparison Results
  Table 4 caption:
    table_text: TABLE 4 Ablation Experiment
  Table 5 caption:
    table_text: TABLE 5 Confidence Measures [50] on Cost Volumes
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2946806
- Affiliation of the first author: "department of computer science, eth zurich, z\xFC\
    rich, switzerland"
  Affiliation of the last author: "department of computer science, eth zurich, z\xFC\
    rich, switzerland"
  Figure 1 Link: articels_figures_by_rev_year\2019\SurfelMeshing_Online_SurfelBased_Mesh_Reconstruction\figure_1.jpg
  Figure 1 caption: Triangle mesh reconstructed with our method with loop closures
    handled during reconstruction, colored (left), and shaded.
  Figure 10 Link: articels_figures_by_rev_year\2019\SurfelMeshing_Online_SurfelBased_Mesh_Reconstruction\figure_10.jpg
  Figure 10 caption: 'Performance of our incremental meshing (sum of synchronization,
    remeshing and meshing times per frame) on the dataset from Fig. 1 compared to:
    1) batch-meshing the surfel reconstruction at each frame from scratch, and 2)
    [27]s meshing performance.'
  Figure 2 Link: articels_figures_by_rev_year\2019\SurfelMeshing_Online_SurfelBased_Mesh_Reconstruction\figure_2.jpg
  Figure 2 caption: 'Left to right: Voxel-based meshes by InfiniTAM [29] and FastFusion
    [27] (each with default voxel size settings), surfel splat rendering by ElasticFusion
    [17], our surfel-based mesh. The same camera trajectory is used with all approaches.
    Notice the sharper colors produced by our method.'
  Figure 3 Link: articels_figures_by_rev_year\2019\SurfelMeshing_Online_SurfelBased_Mesh_Reconstruction\figure_3.jpg
  Figure 3 caption: Data flow overview for our algorithm.
  Figure 4 Link: articels_figures_by_rev_year\2019\SurfelMeshing_Online_SurfelBased_Mesh_Reconstruction\figure_4.jpg
  Figure 4 caption: 'Illustration of surfel reconstruction: (a) Data association.
    New measurements (gray squares) are associated with existing surfels as follows:
    Surfels outside the camera view are not considered (black dots). Green surfels
    are supported by the nearby measurements. The red surfel conflicts with the measurement
    behind it. The blue surfels are occluded by the measurements. (b) After integrating
    the measurements from (a), the conflicting surfel has been replaced, the supported
    surfels have been averaged with the measurements, and new surfels (gray) have
    been created for measurements without a supported or conflicting surfel. (c) Loops
    are closed with a non-rigid deformation, aiming to align new surface parts (green)
    with corresponding old surfaces (blue).'
  Figure 5 Link: articels_figures_by_rev_year\2019\SurfelMeshing_Online_SurfelBased_Mesh_Reconstruction\figure_5.jpg
  Figure 5 caption: Illustration of raw depth map integration, (a) - (c), versus observation
    boundary blending, (d) - (f). (a) The camera observes an existing surface (black)
    in a different position (gray), e.g., due to slight pose drift. (b) After integrating
    one new observation of the gray surface, discontinuities (red) are created. (c)
    After integrating many new observations, there is still a discontinuity, causing
    a hole in the mesh. (d) Observation hallucinated by Algorithm 1 based on (a).
    (e) After integrating one new observation of the gray surface, no discontinuities
    are created. (f) After integrating many new observations, blending still avoids
    discontinuities. In contrast, using a lower integration weight for boundary regions
    would lead to situation (c).
  Figure 6 Link: articels_figures_by_rev_year\2019\SurfelMeshing_Online_SurfelBased_Mesh_Reconstruction\figure_6.jpg
  Figure 6 caption: Overview of triangulation for the center surfel (following [22]).
    (a) Neighbor search and projection onto the tangent plane of the surfel, shown
    in (b) with visibility in the plane. (c) After initial triangle creation. (d)
    Updated visibility and neighbor ordering. (e) Gap and narrow triangle classification.
    (f) Result after gap and narrow triangle removal.
  Figure 7 Link: articels_figures_by_rev_year\2019\SurfelMeshing_Online_SurfelBased_Mesh_Reconstruction\figure_7.jpg
  Figure 7 caption: Overview of the remeshing process. (a) Invalid triangles (red)
    are identified (c.f. Section 3.4). (b) All triangles connected to surfels (red)
    within the neighbor search radii (yellow) of the triangles corner vertices are
    deleted. Affected surfels (red and blue) are scheduled for remeshing. (c) Holes
    are filled by the meshing algorithm (Section 3.3 Fig. 6).
  Figure 8 Link: articels_figures_by_rev_year\2019\SurfelMeshing_Online_SurfelBased_Mesh_Reconstruction\figure_8.jpg
  Figure 8 caption: Comparison between no smoothing (top left), observation boundary
    blending only (top right), regularization only (bottom left), and both (bottom
    right). Both types of smoothing contribute to a hole-free triangulation and improve
    the surface quality.
  Figure 9 Link: articels_figures_by_rev_year\2019\SurfelMeshing_Online_SurfelBased_Mesh_Reconstruction\figure_9.jpg
  Figure 9 caption: 'Comparison between our denoising (with regularization, shown
    in the fully converged state) and standard Laplacian Smoothing. Left block: no
    smoothing (top left), observation boundary blending only (top right), regularization
    only (bottom left), and both (bottom right). Right block: top row: one, two, three
    iterations of Laplacian smoothing only. Bottom row: four, five, twenty iterations
    of Laplacian smoothing only. Out of all smoothing methods, only boundary blending
    is able to remove the horizontal scanning artefact caused by small pose errors
    (marked in red in the top left image). Laplacian smoothing shrinks the object
    and continues to smooth it with every iteration, while ours converges to a good
    solution due to our data term.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "Thomas Sch\xF6ps"
  Name of the last author: Marc Pollefeys
  Number of Figures: 19
  Number of Tables: 2
  Number of authors: 3
  Paper title: 'SurfelMeshing: Online Surfel-Based Mesh Reconstruction'
  Publication Date: 2019-10-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Mesh Quality of our Method (With and Without Regularization
      (reg), Blending (bld), and Incremental Remeshing (remesh)) and of Volume-Based
      Methods
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Accuracy [%], Completeness [%], and Mean Curvature [ 0.01
      m 0.01m] Results for ICL-NUIM [45] Sequences
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2947048
- Affiliation of the first author: department of information engineering and computer
    science (disi), university of trento, trento, italy
  Affiliation of the last author: department of information engineering and computer
    science (disi), university of trento, trento, italy
  Figure 1 Link: articels_figures_by_rev_year\2019\Appearance_and_PoseConditioned_Human_Image_Generation_Using_Deformable_GANs\figure_1.jpg
  Figure 1 caption: "(a) An example of a \u201Crigid\u201D scene generation task,\
    \ where the conditioning and the output image local structures are well aligned.\
    \ (b) In a deformable-object generation task, the input and output images are\
    \ not spatially aligned."
  Figure 10 Link: articels_figures_by_rev_year\2019\Appearance_and_PoseConditioned_Human_Image_Generation_Using_Deformable_GANs\figure_10.jpg
  Figure 10 caption: Examples of badly generated images on the DeepFashion dataset.
  Figure 2 Link: articels_figures_by_rev_year\2019\Appearance_and_PoseConditioned_Human_Image_Generation_Using_Deformable_GANs\figure_2.jpg
  Figure 2 caption: "A schematic representation of our network architectures. For\
    \ the sake of clarity, in this figure we depict P(\u22C5) as a skeleton and each\
    \ tensor H as the average of its component matrices H j ( 1\u2264j\u2264k ). The\
    \ white rectangles in the decoder represent the feature maps directly obtained\
    \ using up-convolutional filters applied to the previous-layer maps. The reddish\
    \ rectangles represent the feature maps \u201Cshuttled\u201D by the skip connections\
    \ in the target stream. Finally, blueish rectangles represent the deformed tensors\
    \ ( d(F) ) \u201Cshuttled\u201D by the deformable skip connections in the source\
    \ stream."
  Figure 3 Link: articels_figures_by_rev_year\2019\Appearance_and_PoseConditioned_Human_Image_Generation_Using_Deformable_GANs\figure_3.jpg
  Figure 3 caption: "For each specific body part, an affine transformation f h is\
    \ computed. This transformation is used to \u201Cmove\u201D the feature-map content\
    \ corresponding to that body part."
  Figure 4 Link: articels_figures_by_rev_year\2019\Appearance_and_PoseConditioned_Human_Image_Generation_Using_Deformable_GANs\figure_4.jpg
  Figure 4 caption: A qualitative comparison on the Market-1501 dataset between our
    approach and [4] and [1]. Columns 1 and 2 show the (testing) conditioning appearance
    and pose image, respectively, which are used as reference by all methods. Columns
    3, 4 and 5 respectively show the images generated by our full-pipeline and by
    [4] and [1].
  Figure 5 Link: articels_figures_by_rev_year\2019\Appearance_and_PoseConditioned_Human_Image_Generation_Using_Deformable_GANs\figure_5.jpg
  Figure 5 caption: A qualitative comparison on the DeepFashion dataset between our
    approach and the results obtained by [4] and [1].
  Figure 6 Link: articels_figures_by_rev_year\2019\Appearance_and_PoseConditioned_Human_Image_Generation_Using_Deformable_GANs\figure_6.jpg
  Figure 6 caption: Qualitative results on the PRW dataset when conditioning on the
    background. We use three different pairs of conditioning appearance image x a
    and target pose P( x b ) . For each pair, we use five different target background
    images x B b extracted from the PRW dataset and 2 background images that are visually
    really different from the PRW dataset backgrounds (last two columns).
  Figure 7 Link: articels_figures_by_rev_year\2019\Appearance_and_PoseConditioned_Human_Image_Generation_Using_Deformable_GANs\figure_7.jpg
  Figure 7 caption: "Qualitative results on the Market-1501 dataset. Columns 1, 2\
    \ and 3 represent the input of our model. We plot P(\u22C5) as a skeleton for\
    \ the sake of clarity, but actually no joint-connectivity relation is exploited\
    \ in our approach. Column 4 corresponds to the ground truth. The last four columns\
    \ show the output of our approach with respect to different variants of our method."
  Figure 8 Link: articels_figures_by_rev_year\2019\Appearance_and_PoseConditioned_Human_Image_Generation_Using_Deformable_GANs\figure_8.jpg
  Figure 8 caption: Qualitative results on the DeepFashion dataset with respect to
    different variants of our method. Some images have been cropped to improve the
    visualization.
  Figure 9 Link: articels_figures_by_rev_year\2019\Appearance_and_PoseConditioned_Human_Image_Generation_Using_Deformable_GANs\figure_9.jpg
  Figure 9 caption: Examples of badly generated images on the Market-1501 dataset.
    See the text for more details.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Aliaksandr Siarohin
  Name of the last author: Nicu Sebe
  Number of Figures: 12
  Number of Tables: 11
  Number of authors: 4
  Paper title: Appearance and Pose-Conditioned Human Image Generation Using Deformable
    GANs
  Publication Date: 2019-10-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison with the State-of-the-Art
  Table 10 caption:
    table_text: 'TABLE 10 Quantitative Ablation Study on the Market-1501: Neighborhood
      Size'
  Table 2 caption:
    table_text: TABLE 2 User Study ( % %)
  Table 3 caption:
    table_text: 'TABLE 3 User Study Based on Direct Comparisons: We Report User Preference
      in % %'
  Table 4 caption:
    table_text: TABLE 4 Influence of Person-Generation Based Data Augmentation on
      the Accuracy of Different Re-ID Methods on the Market-1501 Test Set (Rank 1
      mAP in % %)
  Table 5 caption:
    table_text: TABLE 5 Quantitative Ablation Study on the Market-1501 and the DeepFashion
      Dataset
  Table 6 caption:
    table_text: 'TABLE 6 Combining the Affine Transformations: Quantitative Ablation
      Study on the Market-1501'
  Table 7 caption:
    table_text: 'TABLE 7 Choice of the g g Function: Quantitative Ablation Study on
      the Market-1501'
  Table 8 caption:
    table_text: "TABLE 8 Quantitative Ablation Study on the Market-1501: Sensitivity\
      \ to the \u03BB \u03BB Parameter"
  Table 9 caption:
    table_text: 'TABLE 9 Quantitative Ablation Study on the Market-1501: Exploiting
      Symmetry'
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2947427
- Affiliation of the first author: baidu research, baidu inc., beijing, china
  Affiliation of the last author: baidu research, baidu inc., beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2019\Learning_Depth_with_Convolutional_Spatial_Propagation_Network\figure_1.jpg
  Figure 1 caption: "Two depth estimation tasks focused in this paper: (a) depth completion,\
    \ the output from various methods and color means the redder the further. \u201C\
    Network output\u201D is from Ma et al. [10]; \u201Cw\u201D is short for with.\
    \ \u201Cw SPN\u201D means after refinement with SPN [25]. The root mean square\
    \ error (RMSE) is put at the left-top of each predicted depth map. (b) stereo\
    \ depth estimation, the output from PSMNet [26] and our prediction. Color means\
    \ the bluer the further. The D1 error rate is put at left-top of the predictions.\
    \ A significantly improved region is highlighted with dash box, and corresponding\
    \ error map are shown below (the bluer the lower of error). In both cases, our\
    \ outputs are significantly better."
  Figure 10 Link: articels_figures_by_rev_year\2019\Learning_Depth_with_Convolutional_Spatial_Propagation_Network\figure_10.jpg
  Figure 10 caption: Qualitative comparisons on NYU v2 dataset. (a) Input image. (b)
    Sparse depth samples(500). (c) Ma et al. [10]. (d) Mirror connection (MC)+SPN
    [62]. (e) MC+CSPN(Ours). (f) MC+CSPN+CSPF (Ours). (g) Ground Truth. Most significantly
    improved regions are highlighted with dash boxes (best view in color).
  Figure 2 Link: articels_figures_by_rev_year\2019\Learning_Depth_with_Convolutional_Spatial_Propagation_Network\figure_2.jpg
  Figure 2 caption: "Comparison between the propagation process in (a) SPN [25], (b)\
    \ 2D CPSN and (c) 3D CSPN in this work. Notice for 3D CSPN, the dashed volume\
    \ means one slice of the feature channel in a 4D volume with size of d\xD7h\xD7\
    w\xD7c (Detailed in Section 3.4)."
  Figure 3 Link: articels_figures_by_rev_year\2019\Learning_Depth_with_Convolutional_Spatial_Propagation_Network\figure_3.jpg
  Figure 3 caption: Different structures of context pyramid module. (a) spatial pyramid
    pooling (SPP) module applied by PSMNet [91] (b) Our convolutional SPP (CSPP) module
    using 2D CSPN with different kernel size and stride. (c) Our convolutional feature
    fusion (CFF) using 3D CSPN. (d) Our final combined SPP module, namely convolutional
    spatial pyramid fusion (CSPF). (Details in Section 3.2).
  Figure 4 Link: articels_figures_by_rev_year\2019\Learning_Depth_with_Convolutional_Spatial_Propagation_Network\figure_4.jpg
  Figure 4 caption: Architecture of our networks with mirror connections for depth
    completion with CSPN (best view in color). Sparse depth map is embedded into the
    CSPN process to guide the depth refinement. The light blue blocks are the identity
    copy of blue blocks before. (Details in Section 3.3.2).
  Figure 5 Link: articels_figures_by_rev_year\2019\Learning_Depth_with_Convolutional_Spatial_Propagation_Network\figure_5.jpg
  Figure 5 caption: (a) Histogram of RMSE with depth maps from Ma et al. [10] at given
    sparse depth points. (b) Comparison of gradient error between depth maps with
    sparse depth replacement (blue bars) and with ours CSPN (green bars), where ours
    is much smaller. Check Fig. 6 for an example. Vertical axis shows the count of
    pixels.
  Figure 6 Link: articels_figures_by_rev_year\2019\Learning_Depth_with_Convolutional_Spatial_Propagation_Network\figure_6.jpg
  Figure 6 caption: 'Comparison of depth map [10] with sparse depth replacement and
    with our CSPN w.r.t. smoothness of depth gradient at sparse depth points. (a)
    Input image. (b) Sparse depth points. (c) Depth map with sparse depth replacement.
    Left: Depth map. Right: Sobel gradient in the x-axis direction (d) Depth map with
    our CSPN with sparse depth points. We highlight the differences in the red box.'
  Figure 7 Link: articels_figures_by_rev_year\2019\Learning_Depth_with_Convolutional_Spatial_Propagation_Network\figure_7.jpg
  Figure 7 caption: Architecture of our networks for stereo depth estimation via transformation
    kernel prediction with 3D CSPN (best view in color).
  Figure 8 Link: articels_figures_by_rev_year\2019\Learning_Depth_with_Convolutional_Spatial_Propagation_Network\figure_8.jpg
  Figure 8 caption: "Details of our 3D Module (best view in color). Downsample rate\
    \ w.r.t. image size is shown at the right top corner of each block, e.g., 4x means\
    \ the size of the feature map is h 4 \xD7 w 4 where h\xD7w is image size. The\
    \ red, green and blue arrows are skip connections, indicating feature concatenation\
    \ at particular position, which are the same with PSMNet [91]."
  Figure 9 Link: articels_figures_by_rev_year\2019\Learning_Depth_with_Convolutional_Spatial_Propagation_Network\figure_9.jpg
  Figure 9 caption: "Ablation study.(a) RMSE (left axis, lower the better) and \u03B4\
    <1.02 (right axis, higher the better) of CSPN w.r.t. number of iterations. Horizontal\
    \ lines show the corresponding results from SPN [25]. (b) RMSE and \u03B4<1.02\
    \ of CSPN w.r.t. kernel size. (c) Testing times w.r.t. input image size."
  First author gender probability: 0.75
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xinjing Cheng
  Name of the last author: Ruigang Yang
  Number of Figures: 12
  Number of Tables: 6
  Number of authors: 3
  Paper title: Learning Depth with Convolutional Spatial Propagation Network
  Publication Date: 2019-10-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison Results on NYU v2 Dataset [14] between Different
      Variants of CSPN and other State-of-the-Art Strategies
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Average Memory Cost, Training and Inference Time on NYU v2\
      \ Dataset of Various Models with Batch Size as 8 and Image Size as 304\xD7 228\
      \ (4 Iterations in CSPN)"
  Table 3 caption:
    table_text: TABLE 3 Comparison Results on KITTI Dataset [17]
  Table 4 caption:
    table_text: TABLE 4 Ablation Studies for 3D Module on the Scene Flow Dataset [34]
  Table 5 caption:
    table_text: TABLE 5 Ablation sTudies for Various Spatial Pyramid Module on Scene
      Flow Dataset [34]
  Table 6 caption:
    table_text: TABLE 6 Results on Scene Flow Dataset and KITTI Benchmarks
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2947374
- Affiliation of the first author: university of pittsburgh, pittsburgh, pa, usa
  Affiliation of the last author: university of pittsburgh, pittsburgh, pa, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Interpreting_the_Rhetoric_of_Visual_Advertisements\figure_1.jpg
  Figure 1 caption: Example advertisements from our dataset that require challenging
    visual recognition and reasoning. Despite the potential applications of understanding
    the messages of ads, this problem has not been tackled in computer vision.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Interpreting_the_Rhetoric_of_Visual_Advertisements\figure_2.jpg
  Figure 2 caption: 'The core idea of our method: We rely on spatial or temporal attention
    (symbols, climax), and two modaliities (visual and textaudio), to understand the
    messages of image and video ads. At the top, we show symbols (rocket-speed, blood-injury,
    cap-education) and slogans embedded in the ads. At the bottom, we show frames
    from a video ad, along with predicted scene types, and the audio amplitude; we
    use these to infer the most important temporal region, or the climax, of a video.'
  Figure 3 Link: articels_figures_by_rev_year\2019\Interpreting_the_Rhetoric_of_Visual_Advertisements\figure_3.jpg
  Figure 3 caption: Examples of ads grouped by strategy or visual understanding required
    for decoding the ad.
  Figure 4 Link: articels_figures_by_rev_year\2019\Interpreting_the_Rhetoric_of_Visual_Advertisements\figure_4.jpg
  Figure 4 caption: Statistics about topics and sentiments.
  Figure 5 Link: articels_figures_by_rev_year\2019\Interpreting_the_Rhetoric_of_Visual_Advertisements\figure_5.jpg
  Figure 5 caption: 'Atypical object transformations in ads: (a-c, e) atypical textures,
    (d, f) parts of objects combined, (g) parts missing, (h) one object in another,
    and (i) object in new context.'
  Figure 6 Link: articels_figures_by_rev_year\2019\Interpreting_the_Rhetoric_of_Visual_Advertisements\figure_6.jpg
  Figure 6 caption: Our image embedding model. In the image branch (1), multiple image
    symbolic anchors are proposed. Attention weighting is applied, and the image is
    represented as a weighted combination of the regions. The knowledge branch (3)
    predicts the existence of symbols and maps these to the 200-D embedding. For both
    the slogan (2) and visual objects captions (4) branches, we use LSTM to model
    the phrases. Pointwise addition is applied to fuse the features from four different
    modalities. We then perform triplet training to learn such an embedding space
    that keeps images close to their matching action-reason statements.
  Figure 7 Link: articels_figures_by_rev_year\2019\Interpreting_the_Rhetoric_of_Visual_Advertisements\figure_7.jpg
  Figure 7 caption: Example slogans from the image ads dataset. Both images require
    reasoning which makes the task challenging even for a human. However, given the
    slogan text information, understanding the message of the ads becomes easier.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Keren Ye
  Name of the last author: Adriana Kovashka
  Number of Figures: 7
  Number of Tables: 12
  Number of authors: 6
  Paper title: Interpreting the Rhetoric of Visual Advertisements
  Publication Date: 2019-10-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Annotations Collected for our Image Dataset
  Table 10 caption:
    table_text: TABLE 10 Ranking Action-Reason Statements for Video Ads
  Table 2 caption:
    table_text: TABLE 2 The Annotations for our Video Ads
  Table 3 caption:
    table_text: TABLE 3 A Sample From our List of Topics and Sentiments
  Table 4 caption:
    table_text: TABLE 4 Examples of Collected Action-Reason Pairs
  Table 5 caption:
    table_text: TABLE 5 Common Words in Responses to Action and Reason Questions for
      Selected Topics, From the Image Dataset
  Table 6 caption:
    table_text: TABLE 6 Annotation Statistics for Atypical Objects
  Table 7 caption:
    table_text: TABLE 7 Action-Reason Statement Ranking Results; High Accuracy and
      Low Min Rank is Desired
  Table 8 caption:
    table_text: TABLE 8 Ranking Action and Reason Statements Separately, Versus Action-Reason
      Together
  Table 9 caption:
    table_text: TABLE 9 Effectiveness Prediction Using the Most Promising Frame-Level
      Climax Features (Last Two) and Video-Level Features
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2947440
- Affiliation of the first author: department of computer science, stony brook university,
    stony brook, ny, usa
  Affiliation of the last author: department of computer science, stony brook university,
    stony brook, ny, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Large_Scale_Shadow_Annotation_and_Detection_Using_Lazy_Annotation_and_Stacked_CN\figure_1.jpg
  Figure 1 caption: Lazy annotation pipeline for efficient labeling of shadow images.
    a) An annotator is asked to draw some strokes on some (not all) shadow areas (white
    strokes) and non-shadow areas (red strokes). b) Automatically segmented shadow
    regions. c) Obtained shadow mask, mostly good with a few exceptions where some
    shadow regions are mis-labeled as non-shadow. Subsequently, the noisy labels are
    corrected using the label recovery method proposed in this paper.
  Figure 10 Link: articels_figures_by_rev_year\2019\Large_Scale_Shadow_Annotation_and_Detection_Using_Lazy_Annotation_and_Stacked_CN\figure_10.jpg
  Figure 10 caption: 'Recovery from noisy annotations. Example of shadow region label
    recovery. a) Original shadow annotation depicted with red boundaries. b) Recovered
    shadows depicted with blue boundaries. c) Resulting cleaned-up shadow annotation:
    Shadow boundaries depicted in red.'
  Figure 2 Link: articels_figures_by_rev_year\2019\Large_Scale_Shadow_Annotation_and_Detection_Using_Lazy_Annotation_and_Stacked_CN\figure_2.jpg
  Figure 2 caption: Lazy annotation pipeline. a) Input image. b) Annotators shadow
    strokes in white. c) Annotators non-shadow strokes in red. d) Initial shadow segmentation
    in green (outer side) and red (inner side). e) Refined shadow segmentation with
    a final shadow stroke in the lower center of the image. f) Resulting binary mask.
  Figure 3 Link: articels_figures_by_rev_year\2019\Large_Scale_Shadow_Annotation_and_Detection_Using_Lazy_Annotation_and_Stacked_CN\figure_3.jpg
  Figure 3 caption: From lazy shadow mask to region labels. a) Initial SLIC superpixels.
    b) Regions obtained by merging superpixels. c) Lazy mask overlaid on regions.
    d) Final region ground-truth.
  Figure 4 Link: articels_figures_by_rev_year\2019\Large_Scale_Shadow_Annotation_and_Detection_Using_Lazy_Annotation_and_Stacked_CN\figure_4.jpg
  Figure 4 caption: Examples of clusters of similar shadow images. Clusters of images
    resulting from running modified PGP on SBU training set.
  Figure 5 Link: articels_figures_by_rev_year\2019\Large_Scale_Shadow_Annotation_and_Detection_Using_Lazy_Annotation_and_Stacked_CN\figure_5.jpg
  Figure 5 caption: The proposed pipeline for shadow segmentation. A conventional
    semantic image segmentation CNN [27], [28], [29], [30] takes an RGB image and
    outputs an image level shadow prior map. Then a patch level CNN examines local
    texture and color via a sliding window approach, taking an RGBP (P is the image
    level shadow Prior channel) image patch and outputs a local shadow prediction
    map. The probability of each pixel being a shadow pixel is computed by averaging
    results from different patches.
  Figure 6 Link: articels_figures_by_rev_year\2019\Large_Scale_Shadow_Annotation_and_Detection_Using_Lazy_Annotation_and_Stacked_CN\figure_6.jpg
  Figure 6 caption: Shadow segmentation examples. Qualitative results using Patch-CNN
    on RGB images, and on RGBP (P is the image level shadow prior) images (Stacked-DeconvNet).
    The Stacked-DeconvNet achieves the best results by incorporating both semantic
    and subtle local texture and color information. For example, in the first image,
    although the color and texture of the tree resemble a shadow, we can exclude the
    tree pixels thanks to the DeconvNet generated shadow prior.
  Figure 7 Link: articels_figures_by_rev_year\2019\Large_Scale_Shadow_Annotation_and_Detection_Using_Lazy_Annotation_and_Stacked_CN\figure_7.jpg
  Figure 7 caption: "Patch-CNN with structured output. The input is a 32\xD732 RGBP\
    \ (RGB + image level shadow Prior) image, the output is a 32\xD732 shadow probability\
    \ map."
  Figure 8 Link: articels_figures_by_rev_year\2019\Large_Scale_Shadow_Annotation_and_Detection_Using_Lazy_Annotation_and_Stacked_CN\figure_8.jpg
  Figure 8 caption: Shadow detection comparison between models trained with lazy labels
    and recovered labels on UIUC. a) Input image. b) Detection results from model
    trained on lazy labels overlaid in yellow. c) Detection results from model trained
    on recovered lazy labels overlaid in yellow.
  Figure 9 Link: articels_figures_by_rev_year\2019\Large_Scale_Shadow_Annotation_and_Detection_Using_Lazy_Annotation_and_Stacked_CN\figure_9.jpg
  Figure 9 caption: Example of label recovery. a) Input image. b) Lazy annotation
    shadow mask overlaid in blue, outer contour in green, and inner contour in red.
    c) Recovered regions with flipped shadow label are shown with yellow contours.
  First author gender probability: 0.79
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Le Hou
  Name of the last author: Dimitris Samaras
  Number of Figures: 14
  Number of Tables: 8
  Number of authors: 4
  Paper title: Large Scale Shadow Annotation and Detection Using Lazy Annotation and
    Stacked CNNs
  Publication Date: 2019-10-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Classification Performance on UIUC and UCF Test Sets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparing to Interactive Shadow Detection Methods on the UCF
      Test Set [9]
  Table 3 caption:
    table_text: TABLE 3 Label Recovery on Noisy UCF Data
  Table 4 caption:
    table_text: TABLE 4 Classification Accuracy of Our Method and Several Others on
      Noisy UCI Datasets
  Table 5 caption:
    table_text: TABLE 5 Label Recovery Influence on CNNs
  Table 6 caption:
    table_text: TABLE 6 Evaluation of Shadow Detection on UCF [9]
  Table 7 caption:
    table_text: TABLE 7 Results on the Proposed SBU Dataset and Across UCF-SBU Datasets
  Table 8 caption:
    table_text: TABLE 8 Image-Level Shadow Predictor Network (ILN) Comparison
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2948011
