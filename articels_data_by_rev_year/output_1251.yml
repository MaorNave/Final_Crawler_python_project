- Affiliation of the first author: national key laboratory of fundamental science
    on synthetic vision, college of computer science, sichuan university, chengdu,
    p. r. china
  Affiliation of the last author: national key laboratory of fundamental science on
    synthetic vision, college of computer science, sichuan university, chengdu, p.
    r. china
  Figure 1 Link: articels_figures_by_rev_year\2018\Joint_Face_Alignment_and_D_Face_Reconstruction_with_Application_to_Face_Recognit\figure_1.jpg
  Figure 1 caption: We view 2D landmarks are generated from a 3D face through 3D expression
    ( f E ) and pose ( f P ) deformation, and camera projection ( f C ). While conventional
    face alignment and 3D face reconstruction are two separated tasks and the latter
    requires the former as input, this paper performs these two tasks jointly, i.e.,
    reconstructing a 3D face and estimating visibleinvisible landmarks (greenred points)
    from a 2D face image with arbitrary poses and expressions.
  Figure 10 Link: articels_figures_by_rev_year\2018\Joint_Face_Alignment_and_D_Face_Reconstruction_with_Application_to_Face_Recognit\figure_10.jpg
  Figure 10 caption: PEN 3D face reconstruction accuracy (MAE) of the proposed method,
    [8] and [42] under different expressions.
  Figure 2 Link: articels_figures_by_rev_year\2018\Joint_Face_Alignment_and_D_Face_Reconstruction_with_Application_to_Face_Recognit\figure_2.jpg
  Figure 2 caption: "A 3D face shape of a subject ( S ) is represented as summation\
    \ of the mean pose-and-expression-normalized (PEN) 3D face shape ( S \xAF ), the\
    \ difference between the subjects PEN 3D shape and the mean PEN 3D shape ( \u0394\
    \ S Id ), and the expression deformation ( \u0394 S Exp )."
  Figure 3 Link: articels_figures_by_rev_year\2018\Joint_Face_Alignment_and_D_Face_Reconstruction_with_Application_to_Face_Recognit\figure_3.jpg
  Figure 3 caption: Flowchart of the proposed joint face alignment and 3D face reconstruction
    method.
  Figure 4 Link: articels_figures_by_rev_year\2018\Joint_Face_Alignment_and_D_Face_Reconstruction_with_Application_to_Face_Recognit\figure_4.jpg
  Figure 4 caption: 'Example images with annotated landmarks (1st, 4th rows), their
    3D faces (2nd, 5th rows) and expression shapes (3rd, 6th rows) from the BU3DFE
    database. Seven expressions are shown: Angry (AN), disgust (DI), fear (FE), happy
    (HA), neutral (NE), sad (SA), and surprise (SU). The 3D shapes corresponding to
    the neutral expression are their PEN 3D face shapes, which are highlighted in
    blue boxes.'
  Figure 5 Link: articels_figures_by_rev_year\2018\Joint_Face_Alignment_and_D_Face_Reconstruction_with_Application_to_Face_Recognit\figure_5.jpg
  Figure 5 caption: 'Four subjects in 300W-LP. From left to right: Images with annotated
    landmarks, PEN 3D face shapes, and expression shapes.'
  Figure 6 Link: articels_figures_by_rev_year\2018\Joint_Face_Alignment_and_D_Face_Reconstruction_with_Application_to_Face_Recognit\figure_6.jpg
  Figure 6 caption: Diagram of the proposed method implemented with nonlinear regressors.
    Deep Alignment Network (DAN) denotes the DCNN-based landmark regressors and Deep
    Reconstruction Network (DRN) denotes the MLP-based 3D shape regressors. Note that
    the landmark heatmap is not used at the initial stage.
  Figure 7 Link: articels_figures_by_rev_year\2018\Joint_Face_Alignment_and_D_Face_Reconstruction_with_Application_to_Face_Recognit\figure_7.jpg
  Figure 7 caption: Block diagram of the proposed 3D-enhanced face recognition.
  Figure 8 Link: articels_figures_by_rev_year\2018\Joint_Face_Alignment_and_D_Face_Reconstruction_with_Application_to_Face_Recognit\figure_8.jpg
  Figure 8 caption: Reconstruction results for a BU3DFE subject at nine poses. The
    even rows show the reconstructed 3D faces by [3], [8], [41], [42] and the proposed
    method. Except the first row, the odd rows show their corresponding NPDE maps.
    The colormap goes from dark blue to dark red (corresponding to errors between
    0 and 5). The numbers under each error map represent the mean and standard deviation
    (in % ).
  Figure 9 Link: articels_figures_by_rev_year\2018\Joint_Face_Alignment_and_D_Face_Reconstruction_with_Application_to_Face_Recognit\figure_9.jpg
  Figure 9 caption: '3D face reconstruction accuracy (MAE) of the proposed method,
    [41] and [3] under different expressions: Angry (AN), disgust (DI), fear (FE),
    happy (HA), neutral (NE), sad (SA) and surprise (SU).'
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Feng Liu
  Name of the last author: Dan Zeng
  Number of Figures: 15
  Number of Tables: 8
  Number of authors: 4
  Paper title: Joint Face Alignment and 3D Face Reconstruction with Application to
    Face Recognition
  Publication Date: 2018-12-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 3D Face Reconstruction Accuracy (MAE) of the Proposed Method
      and State-of-the-Art Methods at Different Yaw Poses on the BU3DFE Database
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Number and Percentage of Subjects of Different Genders and
      Races in the FRGC v2.0 Database
  Table 3 caption:
    table_text: TABLE 3 The Face Alignment Accuracy (NME) of the Proposed Method and
      State-of-the-Art Methods on AFLW and AFLW2000-3D Databases
  Table 4 caption:
    table_text: "TABLE 4 Recognition Accuracy (%) in the First Experiment on Multi-PIE\
      \ by the Four State-of-the-Art DL-Based Face Matchers Before (Indicated by Suffix\
      \ \u201C2D\u201D) and After (Indicated by Suffix \u201CFusion\u201D) Our 3D\
      \ Enhancement"
  Table 5 caption:
    table_text: TABLE 5 Recognition Accuracy (%) of the CenterLoss Matcher in the
      Second Experiment on Multi-PIE
  Table 6 caption:
    table_text: "TABLE 6 Verification Accuracy on CFP by the CenterLoss Face Matchers\
      \ Before (Indicated by Suffix \u201C2D\u201D) and After (Indicated by Suffix\
      \ \u201CFusion\u201D) the Enhancement by Our Proposed Method"
  Table 7 caption:
    table_text: TABLE 7 The Time Efficiency (in Milliseconds or ms ms) of the Proposed
      Method
  Table 8 caption:
    table_text: TABLE 8 Efficiency Comparison of Different Reconstruction Methods
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2885995
- Affiliation of the first author: school of computing science, simon fraser university,
    burnaby, canada
  Affiliation of the last author: school of computing science, simon fraser university,
    burnaby, canada
  Figure 1 Link: articels_figures_by_rev_year\2018\Deep_Neural_Network_Compression_by_InParallel_PruningQuantization\figure_1.jpg
  Figure 1 caption: An overview of our deep network compression approach. CLIP-Q combines
    weight pruning and quantization in a single learning framework, and performs pruning
    and quantization in parallel with fine-tuning. The joint pruning-quantization
    adapts over time with the changing network.
  Figure 10 Link: articels_figures_by_rev_year\2018\Deep_Neural_Network_Compression_by_InParallel_PruningQuantization\figure_10.jpg
  Figure 10 caption: Network size-accuracy tradeoff for MobileNet and ShuffleNet.
    The original networks as well as the other compression results reported in this
    paper are included for reference.
  Figure 2 Link: articels_figures_by_rev_year\2018\Deep_Neural_Network_Compression_by_InParallel_PruningQuantization\figure_2.jpg
  Figure 2 caption: An example illustrating the three steps of the pruning-quantization
    operation for a layer with 16 weights, p=0.25 and b=2 . Pruning-quantization is
    performed in parallel with fine-tuning the networks full-precision weights, and
    updates the pruning statuses, quantization levels, and assignments of weights
    to quantization levels after each training minibatch.
  Figure 3 Link: articels_figures_by_rev_year\2018\Deep_Neural_Network_Compression_by_InParallel_PruningQuantization\figure_3.jpg
  Figure 3 caption: Pruning-quantization hyperparameter prediction for AlexNet on
    ImageNet.
  Figure 4 Link: articels_figures_by_rev_year\2018\Deep_Neural_Network_Compression_by_InParallel_PruningQuantization\figure_4.jpg
  Figure 4 caption: "Layerwise compression statistics for GoogLeNet on ImageNet. Overall\
    \ compression from 28.0 MB to 2.8 MB (10\xD7 compression). Original top-1 accuracy:\
    \ 68.9 percent. Compressed top-1 accuracy: 68.9 percent."
  Figure 5 Link: articels_figures_by_rev_year\2018\Deep_Neural_Network_Compression_by_InParallel_PruningQuantization\figure_5.jpg
  Figure 5 caption: Pruning-quantization hyperparameter prediction for a sample of
    GoogleNet layers.
  Figure 6 Link: articels_figures_by_rev_year\2018\Deep_Neural_Network_Compression_by_InParallel_PruningQuantization\figure_6.jpg
  Figure 6 caption: "Layerwise compression statistics for ResNet-50 on ImageNet. Overall\
    \ compression from 102.5 MB to 6.7 MB (15\xD7 compression). Original top-1 accuracy:\
    \ 73.1 percent. Compressed top-1 accuracy: 73.7 percent."
  Figure 7 Link: articels_figures_by_rev_year\2018\Deep_Neural_Network_Compression_by_InParallel_PruningQuantization\figure_7.jpg
  Figure 7 caption: "Layerwise compression statistics for MobileNet on ImageNet. Overall\
    \ compression from 17.0 MB to 2.2 MB (7.6\xD7 compression). Original top-1 accuracy:\
    \ 70.3 percent. Compressed top-1 accuracy: 70.3 percent."
  Figure 8 Link: articels_figures_by_rev_year\2018\Deep_Neural_Network_Compression_by_InParallel_PruningQuantization\figure_8.jpg
  Figure 8 caption: "Layerwise compression statistics for ShuffleNet on ImageNet.\
    \ Overall compression from 7.4 MB to 1.1 MB (6.6\xD7 compression). Original top-1\
    \ accuracy: 65.3 percent. Compressed top-1 accuracy: 65.4 percent."
  Figure 9 Link: articels_figures_by_rev_year\2018\Deep_Neural_Network_Compression_by_InParallel_PruningQuantization\figure_9.jpg
  Figure 9 caption: Pruning-quantization hyperparameter prediction for a sample of
    MobileNet layers.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Frederick Tung
  Name of the last author: Greg Mori
  Number of Figures: 12
  Number of Tables: 4
  Number of authors: 2
  Paper title: Deep Neural Network Compression by In-Parallel Pruning-Quantization
  Publication Date: 2018-12-12 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Layerwise Compression Statistics for AlexNet on ImageNet
      ( p p: Pruning Rate, b b: Bits Per Weight)'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Layerwise Compression Statistics for VGG-16 on ImageNet (
      p p: Pruning Rate, b b: Bits per Weight)'
  Table 3 caption:
    table_text: TABLE 3 Network Compression Performance Compared with State-of-the-Art
      Algorithms for AlexNet, VGG-16, GoogLeNet, and ResNet-50 on ImageNet
  Table 4 caption:
    table_text: TABLE 4 Network Compression Performance Compared with State-of-the-Art
      Algorithms for MobileNet and ShuffleNet on ImageNet
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2886192
- Affiliation of the first author: national laboratory of pattern recognition, institute
    of automation, chinese academy of sciences, beijing, p.r. china
  Affiliation of the last author: northwestern university, evanston, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\Baselines_Extraction_from_Curved_Document_Images_via_Slope_Fields_Recovery\figure_1.jpg
  Figure 1 caption: The overview of the proposed method. The method consists of four
    main steps, i.e., projection cube computation, projected rulings estimation, robust
    spherical curve fitting and slope field solving.
  Figure 10 Link: articels_figures_by_rev_year\2018\Baselines_Extraction_from_Curved_Document_Images_via_Slope_Fields_Recovery\figure_10.jpg
  Figure 10 caption: Solving for the curved baselines by the Euler method. The Euler
    method constructs polygonal lines to approximate the curved baselines.
  Figure 2 Link: articels_figures_by_rev_year\2018\Baselines_Extraction_from_Curved_Document_Images_via_Slope_Fields_Recovery\figure_2.jpg
  Figure 2 caption: The baselines across a ruling have the same tangent vector. Therefore,
    after perspective projection, these tangents will converge at a point on the image.
  Figure 3 Link: articels_figures_by_rev_year\2018\Baselines_Extraction_from_Curved_Document_Images_via_Slope_Fields_Recovery\figure_3.jpg
  Figure 3 caption: "A point (x,y) on the image is mapped onto a 3D point (t,\u03C1\
    ,\u03B8) in the projection cube given a line \u2113 passing through the origin\
    \ O of the image coordinate system with an angle \u03B8 ."
  Figure 4 Link: articels_figures_by_rev_year\2018\Baselines_Extraction_from_Curved_Document_Images_via_Slope_Fields_Recovery\figure_4.jpg
  Figure 4 caption: Examples of local orientation estimation of baselines by using
    the function of local projections.
  Figure 5 Link: articels_figures_by_rev_year\2018\Baselines_Extraction_from_Curved_Document_Images_via_Slope_Fields_Recovery\figure_5.jpg
  Figure 5 caption: The stereographic projection maps a point on the plane to a unique
    point on the sphere and represents it by two angles.
  Figure 6 Link: articels_figures_by_rev_year\2018\Baselines_Extraction_from_Curved_Document_Images_via_Slope_Fields_Recovery\figure_6.jpg
  Figure 6 caption: "The estimation of slope field along a projected ruling. (a) A\
    \ given projected ruling \u2113 k , (b) the function of local projections along\
    \ \u2113 k , (c) accumulating the weighted votes in the parameter space for each\
    \ point p\u2208 \u2113 k to compute M k (\u03B1,\u03B2; \u2113 k ,I) , (d) the\
    \ estimated local orientation of baselines for each point p\u2208 \u2113 k , (e)\
    \ the recovered slope field along \u2113 k ."
  Figure 7 Link: articels_figures_by_rev_year\2018\Baselines_Extraction_from_Curved_Document_Images_via_Slope_Fields_Recovery\figure_7.jpg
  Figure 7 caption: "The estimation of projected rulings. (a) The edge maps, (b) U\
    \ 0 (\u22C5,\u22C5) , (c) U 1 (\u22C5,\u22C5) , (d) the estimated \u03D5 k (k=1,2,\u2026\
    ,n) of the projected rulings. We further interpolate them to yield a smooth curve\
    \ \u03D5=\u03D5(x) . (e) The estimated projected rulings on the image (red lines).\
    \ The estimated slope field along each ruling is also plotted (see the green line\
    \ segments). (f) The close-up patches."
  Figure 8 Link: articels_figures_by_rev_year\2018\Baselines_Extraction_from_Curved_Document_Images_via_Slope_Fields_Recovery\figure_8.jpg
  Figure 8 caption: A 2D illustration of angle flipping transform. (a) The convergent
    points of two adjacent rulings locate on the same hemisphere, (b) the convergent
    points of two adjacent rulings locate on the different hemispheres. In this case,
    we have to flip the vector from P to ( u k , v k ) over 180 degree when measuring
    the correct included angle between the adjacent tangents.
  Figure 9 Link: articels_figures_by_rev_year\2018\Baselines_Extraction_from_Curved_Document_Images_via_Slope_Fields_Recovery\figure_9.jpg
  Figure 9 caption: "An example of angle flipping transform and robust parametric\
    \ curve fitting on the sphere. (a) The \u03B1 k before angle flipping, (b) the\
    \ \u03B2 k before angle flipping, (c) the \u03B1 ~ k after angle flipping and\
    \ the fitted \u03B1 ~ (t) , (d) the \u03B2 ~ k after angle flipping and the fitted\
    \ \u03B2 ~ (t) ."
  First author gender probability: 0.66
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.86
  Name of the first author: Gaofeng Meng
  Name of the last author: Ying Wu
  Number of Figures: 18
  Number of Tables: 5
  Number of authors: 4
  Paper title: Baselines Extraction from Curved Document Images via Slope Fields Recovery
  Publication Date: 2018-12-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Parameter Settings of Our Method
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Errors of Baselines Extraction (Unit: Degree) from Synthetic
      Document Images with Different Shapes'
  Table 3 caption:
    table_text: 'TABLE 3 Errors of Baselines Extraction (Unit: Degree) from Synthetic
      Document Images with Different Viewing Angles of Camera'
  Table 4 caption:
    table_text: 'TABLE 4 Errors of Baselines Extraction (Unit: Degree) from Synthetic
      Document Images with Different Levels of Gaussian Blur'
  Table 5 caption:
    table_text: TABLE 5 The Comparisons of the OCR Accuracy (%) of the Six State-of-the-Art
      Methods on the DFKI Dataset
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2886900
- Affiliation of the first author: school of data and computer science, sun yat-sen
    university, guangzhou, china
  Affiliation of the last author: school of data and computer science, sun yat-sen
    university, guangzhou, china
  Figure 1 Link: articels_figures_by_rev_year\2018\Unsupervised_Person_ReIdentification_by_Deep_Asymmetric_Metric_Embedding\figure_1.jpg
  Figure 1 caption: Cause and effect of the view-specific bias. (a) The images from
    different camera views suffer from dramatic viewing condition variations across
    camera views. Images in the same colored box belong to the same person. (b) The
    specific viewing conditions lead to the view-specific feature distortionsbias,
    making unsupervised Re-ID more challenging. A visualization of the view-specific
    bias can be found in Fig. 3a. For example, given a probe image (in blue box),
    the correct match falls to the third rank due to the view-specific bias. (c) If
    we can tackle the view-specific feature distortion (i.e., alleviate the view-specific
    bias), we may reach a better cross-view matching performance. Further visualization
    and discussion can be found in Figs. 3 and 4. (Best viewed in color).
  Figure 10 Link: articels_figures_by_rev_year\2018\Unsupervised_Person_ReIdentification_by_Deep_Asymmetric_Metric_Embedding\figure_10.jpg
  Figure 10 caption: Performances of DECAMEL VC in the view-extendable setting in
    the MSMT17 dataset. We fix J=10 .
  Figure 2 Link: articels_figures_by_rev_year\2018\Unsupervised_Person_ReIdentification_by_Deep_Asymmetric_Metric_Embedding\figure_2.jpg
  Figure 2 caption: 'Illustration of our framework DECAMEL. We follow the brown arrows
    to inspect it. We extract features for person images by a deep network. Due to
    view-specific conditions, the initial feature space has severe view-specific bias:
    the red triangles (data points from Camera 1) and blue circles (from Camera 2)
    are far apart. We perform CAMEL to learn an initial asymmetric metric. In the
    shared space produced by the asymmetric metric, the view-specific bias is alleviated.
    By optimizing the proposed unsupervised loss, DECAMEL jointly learns the feature
    representation and asymmetric metric in an end-to-end manner. During testing,
    pairwise distance can be computed by Eq. (2). (Best viewed in color).'
  Figure 3 Link: articels_figures_by_rev_year\2018\Unsupervised_Person_ReIdentification_by_Deep_Asymmetric_Metric_Embedding\figure_3.jpg
  Figure 3 caption: Illustration of asymmetric metric alleviating view-specific bias.
    The data is randomly sampled from the SYSU dataset [28]. We performed PCA for
    visualization. Blue circles and red triangles represent data points from two camera
    views. (a) Cross-view data distribution in original feature representation space.
    View-specific bias is severe here, since one can easily draw a boundary to roughly
    separate the circles and triangles. (b) Distribution in the shared space after
    projected by the learned view-generic transformation (symmetric metric). The bias
    is not alleviated. (c) Distribution in the shared space after projected by the
    learned view-specific transformations (asymmetric metric). The bias is largely
    alleviated. (Best viewed in color).
  Figure 4 Link: articels_figures_by_rev_year\2018\Unsupervised_Person_ReIdentification_by_Deep_Asymmetric_Metric_Embedding\figure_4.jpg
  Figure 4 caption: Illustration of DECAMEL learning the better cross-view cluster
    structure via jointly learning the feature representation and the asymmetric metric.
    We perform PCA for visualization. Images of an identity are indicated by a specific
    color (e.g., all red triangles and circles are images of the first identity in
    the feature space). The numbers in the upper left of each figure indicates different
    stages, from initial to convergence. The two figures in each column are synchronous
    and corresponding to each other. Data points are only a subset from those in Fig.
    3 for clarity. Specifically, the initial stages (the leftmost column) are subsets
    of Fig. 3a and 3c, respectively. (Best viewed in color and please refer to the
    text in Section 4.2 for more analysis. Please zoom in for better visual quality).
  Figure 5 Link: articels_figures_by_rev_year\2018\Unsupervised_Person_ReIdentification_by_Deep_Asymmetric_Metric_Embedding\figure_5.jpg
  Figure 5 caption: Samples of the datasets. Every two images in a column are from
    one identity across two disjoint camera views. (a) VIPeR (b) CUHK01 (c) CUHK03
    (d) SYSU (e) Market (f) ExMarket (g) MSMT17.
  Figure 6 Link: articels_figures_by_rev_year\2018\Unsupervised_Person_ReIdentification_by_Deep_Asymmetric_Metric_Embedding\figure_6.jpg
  Figure 6 caption: CMC curves for comparisons with related unsupervised models. In
    each legend, the figure beside the model name is the rank-1 matching rate. For
    clarity, we omit VIPeR and show the single-shot results for CUHK01, CUHK03 and
    SYSU.
  Figure 7 Link: articels_figures_by_rev_year\2018\Unsupervised_Person_ReIdentification_by_Deep_Asymmetric_Metric_Embedding\figure_7.jpg
  Figure 7 caption: "(a)-(c) Matching rate versus \u03BB on the three large-scale\
    \ datasets. Similar observations can be made on other datasets. (d) The cross-view\
    \ distribution without cross-view consistency regularization. Data is from the\
    \ SYSU dataset, and we performed PCA for visualization as in Fig. 3."
  Figure 8 Link: articels_figures_by_rev_year\2018\Unsupervised_Person_ReIdentification_by_Deep_Asymmetric_Metric_Embedding\figure_8.jpg
  Figure 8 caption: Matching rate as a function of K on CUHK01. K (blue parts) is
    linearly spaced from 100 to 1000. We show two extreme cases (red parts) when K=1
    and K=1940 , where 1940 is the number of total training samples. Similar observations
    can be made on other datasets.
  Figure 9 Link: articels_figures_by_rev_year\2018\Unsupervised_Person_ReIdentification_by_Deep_Asymmetric_Metric_Embedding\figure_9.jpg
  Figure 9 caption: Number of clusters containing more than one person at the initial
    stage (red solid line) and at the convergence stage (blue dashed line) when K
    varies on CUHK01. Similar observations can be made on other datasets.
  First author gender probability: 0.53
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Hong-Xing Yu
  Name of the last author: Wei-Shi Zheng
  Number of Figures: 10
  Number of Tables: 10
  Number of authors: 3
  Paper title: Unsupervised Person Re-Identification by Deep Asymmetric Metric Embedding
  Publication Date: 2018-12-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Overview of Dataset Scales
  Table 10 caption:
    table_text: TABLE 10 Comparative Results in the View-Extendable Setting
  Table 2 caption:
    table_text: "TABLE 2 Comparison with Related Unsupervised Models: Single-Shot\
      \ (\u201CSingle\u201D) and Multi-Shot (\u201CMulti\u201D) Rank-1 Matching Rate\
      \ and MAP in Percentage"
  Table 3 caption:
    table_text: TABLE 3 Comparison with the State-of-the-Art Unsupervised Re-ID Models
      Reported in Literature
  Table 4 caption:
    table_text: TABLE 4 Evaluation of the Asymmetric Modelling in Our Framework
  Table 5 caption:
    table_text: TABLE 5 Component-Wise Evaluation. Feat init init Denotes the Initialized
      Feature
  Table 6 caption:
    table_text: "TABLE 6 Evaluation of Different Initialization Strategies: Single-Shot\
      \ (\u201CSingle\u201D) and Multi-Shot (\u201CMulti\u201D) Rank-1 Matching Rate\
      \ and MAP in Percentage"
  Table 7 caption:
    table_text: TABLE 7 Evaluation When Given Label Information to a Small Proportion
      of Training Samples on Market
  Table 8 caption:
    table_text: TABLE 8 Evaluation When the Training Samples Size Grows on the Largest
      Dataset ExMarket
  Table 9 caption:
    table_text: TABLE 9 Running Time on the Market-1501 Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2886878
- Affiliation of the first author: "computer vision group, friedrich-schiller-universit\xE4\
    t jena, jena, germany"
  Affiliation of the last author: "computer vision group, friedrich-schiller-universit\xE4\
    t jena, jena, germany"
  Figure 1 Link: articels_figures_by_rev_year\2018\The_Whole_Is_More_Than_Its_Parts_From_Explicit_to_Implicit_Pose_Normalization\figure_1.jpg
  Figure 1 caption: Outline of the activation flow calculation. We first perform a
    regular forward pass through the network and determine the highest scoring class.
    Afterward, we trace back the most contributing elements from previous layers.
    For each identified element, we project its location on the feature map back to
    the input image for visualization purposes.
  Figure 10 Link: articels_figures_by_rev_year\2018\The_Whole_Is_More_Than_Its_Parts_From_Explicit_to_Implicit_Pose_Normalization\figure_10.jpg
  Figure 10 caption: "Influence of \u03B1 using VGG16 without fine-tuning. \u03B1\
    =1 corresponds to average pooling and \u03B1=2 to bilinear pooling. \u03B1 is\
    \ manually set in this experiment."
  Figure 2 Link: articels_figures_by_rev_year\2018\The_Whole_Is_More_Than_Its_Parts_From_Explicit_to_Implicit_Pose_Normalization\figure_2.jpg
  Figure 2 caption: Details about the visualization. We trace back the two highest
    contribution elements for each element selected before and project all elements
    as a tree to the input image. Earlier layers are marked with brighter colors.
  Figure 3 Link: articels_figures_by_rev_year\2018\The_Whole_Is_More_Than_Its_Parts_From_Explicit_to_Implicit_Pose_Normalization\figure_3.jpg
  Figure 3 caption: Activation flow for VGG-VG on CUB200-2011 birds. The top row depicts
    common poses, which were correctly classified by the network, and the bottom row
    failure cases with rare poses.
  Figure 4 Link: articels_figures_by_rev_year\2018\The_Whole_Is_More_Than_Its_Parts_From_Explicit_to_Implicit_Pose_Normalization\figure_4.jpg
  Figure 4 caption: The percentage of elements of the activation flow located on the
    object (coverage) versus pose rarity.
  Figure 5 Link: articels_figures_by_rev_year\2018\The_Whole_Is_More_Than_Its_Parts_From_Explicit_to_Implicit_Pose_Normalization\figure_5.jpg
  Figure 5 caption: The framework of our explicit pose normalization approach.
  Figure 6 Link: articels_figures_by_rev_year\2018\The_Whole_Is_More_Than_Its_Parts_From_Explicit_to_Implicit_Pose_Normalization\figure_6.jpg
  Figure 6 caption: 'Correspondence of functions defined in our model definition and
    computational blocks in common CNN architectures. Architectures are simplified
    for visualization purposes. Notation: conv - convolutional block, FC - linear
    transformation block, GAP - global average pooling, 2AvgP - bilinear encoding.'
  Figure 7 Link: articels_figures_by_rev_year\2018\The_Whole_Is_More_Than_Its_Parts_From_Explicit_to_Implicit_Pose_Normalization\figure_7.jpg
  Figure 7 caption: Visualization of local regions which have the largest influence
    on the linear kernel. The brighter and thicker the line, the larger is the corresponding
    inner product between these local features.
  Figure 8 Link: articels_figures_by_rev_year\2018\The_Whole_Is_More_Than_Its_Parts_From_Explicit_to_Implicit_Pose_Normalization\figure_8.jpg
  Figure 8 caption: Visualization on how to express explicit as implicit models.
  Figure 9 Link: articels_figures_by_rev_year\2018\The_Whole_Is_More_Than_Its_Parts_From_Explicit_to_Implicit_Pose_Normalization\figure_9.jpg
  Figure 9 caption: Influence of the number of selected parts on the bird classification
    accuracy on CUB200-2011 with VGG-VD. Compared to the other experiments, only one
    patch was extracted per part proposal.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Marcel Simon
  Name of the last author: Joachim Denzler
  Number of Figures: 12
  Number of Tables: 3
  Number of authors: 4
  Paper title: The Whole Is More Than Its Parts? From Explicit to Implicit Pose Normalization
  Publication Date: 2018-12-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Mean Coverage and Classification Accuracy of Different Network
      Architectures Using CUB200-2011
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Our Explicit Pose Normalization Approaches on
      the CUB200-2011 Birds Dataset
  Table 3 caption:
    table_text: TABLE 3 Comparison of the Presented Pose Normalization Approaches
      on the Oxford Flowers 102, Oxford-IIIT Pets, and Stanford 40 Actions Datasets
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2885764
- Affiliation of the first author: cas center for excellence in brain science and
    intelligence technology, national laboratory of pattern recognition, institute
    of automation, chinese academy of sciences, beijing, p. r. china
  Affiliation of the last author: department of computer science and information systems,
    birkbeck college, london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2018\TrackingbyFusion_via_Gaussian_Process_Regression_Extended_to_Transfer_Learning\figure_1.jpg
  Figure 1 caption: "The overview and system flowchart of the proposed formulation.\
    \ The iteration loop labeled as steps \u2460, \u2461 and \u2462 shows the interactions\
    \ between the CFs and the transfer learning extension of GPs."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\TrackingbyFusion_via_Gaussian_Process_Regression_Extended_to_Transfer_Learning\figure_2.jpg
  Figure 2 caption: An illustration of the un-normalized auxiliary frame weights for
    training CFs at the exemplary frame instance 762 on the example tracking sequence
    Panda in the OTB-2015 benchmark. The weights are obtained from the re-weighting
    of our GPs-based transfer learning formulation. The image patches in the auxiliary
    domain are shown for example frames, which are obtained from the auxiliary frames
    by padding the corresponding tracking results scales (blue box) to include the
    context regions. The aim of our tracking is to estimate the location and scale
    of the panda at frame 762 in the target domain. It can be easily found that the
    auxiliary frames mostly related to the current tracking task (frames 618 and 624
    ) are considerably up-weighted for training CFs, while the others are down-weighted
    or even useless (frames 564 and 600 ).
  Figure 3 Link: articels_figures_by_rev_year\2018\TrackingbyFusion_via_Gaussian_Process_Regression_Extended_to_Transfer_Learning\figure_3.jpg
  Figure 3 caption: Success plots showing an ablation study comparison of our proposed
    formulation with two kinds of variants and some baselines in terms of OPE on the
    OTB-2015 benchmark. The legend contains the AUC scores for each method. Best viewed
    in color.
  Figure 4 Link: articels_figures_by_rev_year\2018\TrackingbyFusion_via_Gaussian_Process_Regression_Extended_to_Transfer_Learning\figure_4.jpg
  Figure 4 caption: Success plots showing the performance of our formulation (complete
    version) compared to some representative tracking algorithms provided with OTB-2015
    and some latest state-of-the-art trackers in terms of OPE on the OTB-2015 and
    Temple-Color benchmarks. The legends of the success plots contain the AUC scores
    for each method. Only some top-performing trackers are displayed in the legend
    for clarity. Best viewed in color.
  Figure 5 Link: articels_figures_by_rev_year\2018\TrackingbyFusion_via_Gaussian_Process_Regression_Extended_to_Transfer_Learning\figure_5.jpg
  Figure 5 caption: Success plots showing the performance of our formulation (complete
    version) compared to some representative tracking algorithms provided with OTB-2015
    and some latest state-of-the-art trackers in terms of TRE and SRE on the OTB-2015
    benchmark. The legends of the success plots contain the AUC scores for each method.
    Only some top-performing trackers are displayed in the legend for clarity. Best
    viewed in color.
  Figure 6 Link: articels_figures_by_rev_year\2018\TrackingbyFusion_via_Gaussian_Process_Regression_Extended_to_Transfer_Learning\figure_6.jpg
  Figure 6 caption: The AR-rank plot (left) and AR-raw plot (middle) using the Atext-RS
    pairs generated by sequence pooling on VOT2015, and the corresponding expected
    average overlap graph (right) with trackers ranked from right to left. Best viewed
    in color.
  Figure 7 Link: articels_figures_by_rev_year\2018\TrackingbyFusion_via_Gaussian_Process_Regression_Extended_to_Transfer_Learning\figure_7.jpg
  Figure 7 caption: The AR-rank plot (left) and AR-raw plot (middle) using the Atext-RS
    pairs generated by sequence pooling on VOT2016, and the corresponding expected
    average overlap graph (right) with trackers ranked from right to left. Best viewed
    in color.
  Figure 8 Link: articels_figures_by_rev_year\2018\TrackingbyFusion_via_Gaussian_Process_Regression_Extended_to_Transfer_Learning\figure_8.jpg
  Figure 8 caption: Expected average overlap curves on VOT2015 (left) and VOT2016
    (middle), and the OPE no-reset plot for AO curves on VOT2016 (right). Best viewed
    in color.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jin Gao
  Name of the last author: Stephen Maybank
  Number of Figures: 8
  Number of Tables: 3
  Number of authors: 6
  Paper title: Tracking-by-Fusion via Gaussian Process Regression Extended to Transfer
    Learning
  Publication Date: 2018-12-21 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Ablation Study of the Proposed Tracking-by-Fusion Formulation;
      We Conduct the Experiments in Terms of OPE on the OTB-2015 Benchmark; the Results
      Are Reported as Mean OP OP (%) Mean DP DP (%) Scores at Thresholds of 0.5 20
      Pixels Respectively; We Select SRDCF and Our Initial Work TGPR with HOG Settings
      as Baselines Since Our Formulation Is Implemented by Fusing SRDCF with TGPR;
      Two Kinds of Simplified Variants Are Compared: i) TGPRfSRDCFD without (wo) Distribution
      Adaptation and ii) TGPRfSRDCFW wo Using the Re-Weighted Knowledge; Five Different
      Learning Rate Values for TGPRfSRDCFW Are Tested, i.e., 0.015, 0.02, 0.025, 0.03,
      0.035'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Our Proposed Formulation (Complete Version)
      with Some Participating Algorithms in the OTB-2015 Benchmark and Other Latest
      State-of-the-Art Trackers; We Conduct the Experiments in Terms of OPE, TRE and
      SRE on OTB-2015, and Only OPE on Temple-Color; the Results Are Reported as AUC
      AUC (%) Mean OP OP (%, at 0.5) Mean DP DP (%, at 20 Pixels) Scores
  Table 3 caption:
    table_text: TABLE 3 Comparison of Our Proposed Formulation (Complete Version)
      with Some Participating Algorithms on the VOT20152016 Benchmarks; the Results
      Are Reported as EAO EAO, A A, R fr Rfr or R S RS ( S=100 S=100), No-Reset AO
      AO, and VOT Tracking Speed in EFO EFO
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2889070
- Affiliation of the first author: university of oxford, oxford, united kingdom
  Affiliation of the last author: university of oxford, oxford, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2018\Deep_AudioVisual_Speech_Recognition\figure_1.jpg
  Figure 1 caption: Outline of the audio-visual speech recognition pipeline.
  Figure 10 Link: articels_figures_by_rev_year\2018\Deep_AudioVisual_Speech_Recognition\figure_10.jpg
  Figure 10 caption: "Visualization of the effect of additive noise on the attention\
    \ masks of the different TM-seq2seq models. We show the attentions on (a) the\
    \ clean audio utterance, and (b) on the noisy utterance which we obtain by adding\
    \ babble noise to the 25 central audio frames. Comparing (c) with (d), the attention\
    \ of the audio-only models appears to be more spread around the area where the\
    \ noise is applied, while the last frames are not attended upon. Similarly for\
    \ the audio-visual model, the audio attention is more focused when the audio is\
    \ clean (f) compared to when it is noisy (g). The ground truth transcription of\
    \ the sentence is \u201Cone of the articles there is about the queen elizabeth\u201D\
    . Observing the transcriptions, we see that the audio-only model (d) does not\
    \ predict the central words correctly when noise is added, however the audio-visual\
    \ model (g & h) successfully transcribes the sentence, by leveraging the visual\
    \ cues. Interestingly, in this particular example, the transcription that the\
    \ video-only model outputs (e) is completely wrong; the combination of both modalities\
    \ however yields a correct prediction. Finally, the attention mask of the AV model\
    \ on the video input (f) has a clear monotonic trend and is similar to the one\
    \ of the video-only model (e); this also verifies that the model indeed learns\
    \ to use the video modality even when audio is present."
  Figure 2 Link: articels_figures_by_rev_year\2018\Deep_AudioVisual_Speech_Recognition\figure_2.jpg
  Figure 2 caption: 'Audio-visual speech recognition models. (a) Common encoder: The
    visual image sequence is processed by a spatio-temporal ResNet, while the audio
    features are the spectrograms obtained by applying Short Time Fourier Transform
    (STFT) to the audio signal. Each modality is then encoded by a separate Transformer
    encoder. (b) TM-seq2seq: A Transformer model. On every decoder layer, the video
    (V) and audio (A) encodings are attended to separately by independent multi-head
    attention modules. The context vectors produced for the two modalities, Vc and
    Ac respectively, are concatenated channel-wise and fed to the feed forward layers.
    K, V and Q denote the Key, Value and Query tensors for the multi-head attention
    blocks. For the self-attention layers it is always Q=K=V , while for the encoder-decoder
    attentions, K = V are the encodings (V or A), while Q is the previous layers output
    (or, for the first layer, the prediction of the network at the previous decoding
    step). (c) TM-CTC: Transformer CTC, a model composed of stacks of self-attention
    and feed forward layers, producing CTC posterior probabilities for every input
    frame. For full details on the multi-head attention and feed forward blocks refer
    to Appendix B, available online.'
  Figure 3 Link: articels_figures_by_rev_year\2018\Deep_AudioVisual_Speech_Recognition\figure_3.jpg
  Figure 3 caption: 'Top: Original still images from videos used in the making of
    the LRS2-BBC dataset. Bottom: The mouth motions from two different speakers. The
    network sees the areas inside the red squares.'
  Figure 4 Link: articels_figures_by_rev_year\2018\Deep_AudioVisual_Speech_Recognition\figure_4.jpg
  Figure 4 caption: Pipeline to generate the dataset.
  Figure 5 Link: articels_figures_by_rev_year\2018\Deep_AudioVisual_Speech_Recognition\figure_5.jpg
  Figure 5 caption: Word error rate per number of words in the sentence for the video-only
    models, evaluated on the test set of LRS2-BBC. We exclude sentence sizes represented
    by less than 5 samples in the set (i.e., 15, 16 and 19 words). The dashed lines
    show the average WER over all the sentences. For both models, the WER is relatively
    uniform for different sentence sizes. However samples with very few words (3)
    appear to be more difficult, presumably because they provide less context.
  Figure 6 Link: articels_figures_by_rev_year\2018\Deep_AudioVisual_Speech_Recognition\figure_6.jpg
  Figure 6 caption: Per word F1, Precision and Recall rates, on the 30 most common
    words in the LRS2-BBC test set, for the video-only models. The measures are calculated
    via the minimum edit-distance operations (details in Appendix E, available online).
    For all words and both models, precision is higher than recall.
  Figure 7 Link: articels_figures_by_rev_year\2018\Deep_AudioVisual_Speech_Recognition\figure_7.jpg
  Figure 7 caption: The effect of beam width on Word Error Rate for the video-only
    TM-seq2seq model, when evaluating on LRS2-BBC.
  Figure 8 Link: articels_figures_by_rev_year\2018\Deep_AudioVisual_Speech_Recognition\figure_8.jpg
  Figure 8 caption: WER scored by the audio-visual models on LRS2-BBC when the video
    frames are artificially shifted by a number of frames compared to audio. The TM-seq2seq
    model is only fine-tuned for one epoch, while CTC for 4 epochs on the train-val
    set.
  Figure 9 Link: articels_figures_by_rev_year\2018\Deep_AudioVisual_Speech_Recognition\figure_9.jpg
  Figure 9 caption: Alignment between the video frames and the character output with
    TM-seq2seq. The alignment is produced by averaging all the encoder-decoder attention
    heads over all the decoder layers in the log domain.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Triantafyllos Afouras
  Name of the last author: Andrew Zisserman
  Number of Figures: 10
  Number of Tables: 4
  Number of authors: 5
  Paper title: Deep Audio-Visual Speech Recognition
  Publication Date: 2018-12-21 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Statistics on the Lip Reading Sentences (LRS) Audio-Visual
      Datasets, and Other Existing Large-Scale Lip Reading Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Word Error Rates (WER) on the LRS2-BBC and LRS3-TED Datasets
  Table 3 caption:
    table_text: TABLE 3 Examples of Unseen Sentences that TM-seq2seq Correctly Predicts
      (Video Only)
  Table 4 caption:
    table_text: TABLE 4 Examples of AVSR Results
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2889052
- Affiliation of the first author: graduate school of informatics, kyoto university,
    kyoto, japan
  Affiliation of the last author: academic center for computing and media studies,
    kyoto university, kyoto, japan
  Figure 1 Link: articels_figures_by_rev_year\2018\Photometric_Stereo_in_Participating_Media_Using_an_Analytical_Solution_for_Shape\figure_1.jpg
  Figure 1 caption: Images captured in (a) pure water and (b) diluted milk. In participating
    media, the quality of the captured image is degraded by light scattering and attenuation.
  Figure 10 Link: articels_figures_by_rev_year\2018\Photometric_Stereo_in_Participating_Media_Using_an_Analytical_Solution_for_Shape\figure_10.jpg
  Figure 10 caption: 'Examples of the synthesized images: (a) synthesized image without
    a participating medium, (b) reflected component mathbf Ls , and (c) backscatter
    subtracted image mathbf Lprime .'
  Figure 2 Link: articels_figures_by_rev_year\2018\Photometric_Stereo_in_Participating_Media_Using_an_Analytical_Solution_for_Shape\figure_2.jpg
  Figure 2 caption: In participating media, the observed irradiance at a camera includes
    a direct component reflected on a surface, and both backscatter and forward scatter
    components.
  Figure 3 Link: articels_figures_by_rev_year\2018\Photometric_Stereo_in_Participating_Media_Using_an_Analytical_Solution_for_Shape\figure_3.jpg
  Figure 3 caption: The reflected component L s (p) consists of a direct component
    L s,d (p) (yellow arrow) and a source-surface forward scatter component L s,f
    (p) (red arrow). The direct component reaches the surface directly from the source.
    The source-surface forward scatter component is a reflected costituent whose incident
    light reaches the surface via forward scatter.
  Figure 4 Link: articels_figures_by_rev_year\2018\Photometric_Stereo_in_Participating_Media_Using_an_Analytical_Solution_for_Shape\figure_4.jpg
  Figure 4 caption: Backscatter component is the sum of scattered light on the viewline
    without reaching the surface.
  Figure 5 Link: articels_figures_by_rev_year\2018\Photometric_Stereo_in_Participating_Media_Using_an_Analytical_Solution_for_Shape\figure_5.jpg
  Figure 5 caption: Surface-camera forward scatter component. When we observe surface
    point p in a participating medium, the light reflected on point q is scattered
    on the viewline, and the scattered light is also observed.
  Figure 6 Link: articels_figures_by_rev_year\2018\Photometric_Stereo_in_Participating_Media_Using_an_Analytical_Solution_for_Shape\figure_6.jpg
  Figure 6 caption: Example of the backscatter removal.
  Figure 7 Link: articels_figures_by_rev_year\2018\Photometric_Stereo_in_Participating_Media_Using_an_Analytical_Solution_for_Shape\figure_7.jpg
  Figure 7 caption: Comparison between the model of Murez et al. [6] and of ours.
    Murez et al. [6] assumed that the scene can be approximated as a plane. Under
    orthogonal projection, this assumption yields a spatially-invariant point spread
    function. In contrast, we compute the forward scatter considering the objects
    shape under perspective projection. Thus, the kernel is spatially-variant.
  Figure 8 Link: articels_figures_by_rev_year\2018\Photometric_Stereo_in_Participating_Media_Using_an_Analytical_Solution_for_Shape\figure_8.jpg
  Figure 8 caption: (a) 2D visualization of a row of K when we observe a plane; (b)
    profile of the blue line in (a).
  Figure 9 Link: articels_figures_by_rev_year\2018\Photometric_Stereo_in_Participating_Media_Using_an_Analytical_Solution_for_Shape\figure_9.jpg
  Figure 9 caption: "G( T sp , n \u22A4 p l sp ) (blue line) and G( T sp ,1)( n \u22A4\
    \ p l sp ) (green line) when (a) T sp =0.6 and (b) T sp =2 . Although the error\
    \ increases as arccos( n T p l sp ) increases, these graphs validate the approximation\
    \ G( T sp , n \u22A4 p l sp )\u2248G( T sp ,1)( n \u22A4 p l sp ) ."
  First author gender probability: 0.61
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yuki Fujimura
  Name of the last author: Michihiko Minoh
  Number of Figures: 19
  Number of Tables: 3
  Number of authors: 4
  Paper title: Photometric Stereo in Participating Media Using an Analytical Solution
    for Shape-Dependent Forward Scatter
  Publication Date: 2018-12-21 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Mean Angular Error of the Output of the Each Iteration with
      Synthesized Data
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Medium Parameters Used in Our Experiments
  Table 3 caption:
    table_text: TABLE 3 Mean Angular Error of Sphere
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2889088
- Affiliation of the first author: institute of computer science iii, university of
    bonn, bonn, germany
  Affiliation of the last author: institute of computer science iii, university of
    bonn, bonn, germany
  Figure 1 Link: articels_figures_by_rev_year\2018\A_Hybrid_RNNHMM_Approach_for_Weakly_Supervised_Temporal_Action_Segmentation\figure_1.jpg
  Figure 1 caption: RNN using gated recurrent units with framewise video features
    as input. At each frame, the network outputs a probability for each possible subaction
    while considering the temporal context of the video by the preceding frames.
  Figure 10 Link: articels_figures_by_rev_year\2018\A_Hybrid_RNNHMM_Approach_for_Weakly_Supervised_Temporal_Action_Segmentation\figure_10.jpg
  Figure 10 caption: Results of temporal action segmentation with semi supervised
    training on the Breakfast dataset for 10 iterations. Solid lines show the results
    until the proposed stop criterion is reached, dashed lines show the results after
    the stop criterion. It shows that even small fractions of annotated frames can
    significantly improve the overall performance of the system.
  Figure 2 Link: articels_figures_by_rev_year\2018\A_Hybrid_RNNHMM_Approach_for_Weakly_Supervised_Temporal_Action_Segmentation\figure_2.jpg
  Figure 2 caption: Example for the extractor function A . During inference, a frame-to-subaction
    alignment s t is found. To compute the respective unique action sequence. The
    extractor function maps the subactions back to its respective action classes.
  Figure 3 Link: articels_figures_by_rev_year\2018\A_Hybrid_RNNHMM_Approach_for_Weakly_Supervised_Temporal_Action_Segmentation\figure_3.jpg
  Figure 3 caption: "Boundary adjustment for semi-supervised training with sparse\
    \ frame annotation. If the annotated frames are not consistent with the result\
    \ after Viterbi decoding, the segmentation needs to be adjusted to fit the annotated\
    \ frames. This also includes the association of the annotated frames to the respective\
    \ segments. In this example, a( \u03C4 2 ) and a( \u03C4 3 ) both belong to the\
    \ same class and could be associated to segment a 2 and a 4 . Using a dynamic\
    \ warping approach, boundary shifts are chosen to be as small as possible."
  Figure 4 Link: articels_figures_by_rev_year\2018\A_Hybrid_RNNHMM_Approach_for_Weakly_Supervised_Temporal_Action_Segmentation\figure_4.jpg
  Figure 4 caption: Example of state alignment for two instances of the same action
    as they are usually produced by the system without length prior and of an instance
    showing the intended state alignment. In the first two cases the HMM does not
    model the temporal progression, but rather uses the subaction states to distinguish
    between different action appearances.
  Figure 5 Link: articels_figures_by_rev_year\2018\A_Hybrid_RNNHMM_Approach_for_Weakly_Supervised_Temporal_Action_Segmentation\figure_5.jpg
  Figure 5 caption: Training process of our model. Initially, each action is modeled
    with the same number of subactions and the video is linearly aligned to these
    subactions. Based on this alignment, the RNN is trained and used in combination
    with the HMMs to realign the video frames to the subactions. Eventually, the number
    of subactions per action is reestimated and the process is iterated until convergence.
  Figure 6 Link: articels_figures_by_rev_year\2018\A_Hybrid_RNNHMM_Approach_for_Weakly_Supervised_Temporal_Action_Segmentation\figure_6.jpg
  Figure 6 caption: "Example of temporal action segmentation for two samples from\
    \ the Breakfast dataset showing the segmentation result for \u201Cpreparing cereals\u201D\
    \ and \u201Cpreparing friedegg\u201D. Although the actions are not always correctly\
    \ detected, there is still a reasonable alignment of detected actions and ground\
    \ truth boundaries."
  Figure 7 Link: articels_figures_by_rev_year\2018\A_Hybrid_RNNHMM_Approach_for_Weakly_Supervised_Temporal_Action_Segmentation\figure_7.jpg
  Figure 7 caption: Evolution of number of states for the model with state reestimation.
    The number of states increases in the first five iterations and converges after
    ten iterations.
  Figure 8 Link: articels_figures_by_rev_year\2018\A_Hybrid_RNNHMM_Approach_for_Weakly_Supervised_Temporal_Action_Segmentation\figure_8.jpg
  Figure 8 caption: Overview of evaluated length models showing a simple box function,
    a linear decay function, a half Poisson decay and a half Gaussian function for
    a subaction with a mean length of 10 frames. See Appendix for formulas of the
    functions.
  Figure 9 Link: articels_figures_by_rev_year\2018\A_Hybrid_RNNHMM_Approach_for_Weakly_Supervised_Temporal_Action_Segmentation\figure_9.jpg
  Figure 9 caption: Results for temporal segmentation with different length models
    on the Breakfast dataset over 15 iterations. Solid lines show the results until
    the proposed stop criterion is reached. Dashed lines show the results after the
    stop criterion.
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Hilde Kuehne
  Name of the last author: Juergen Gall
  Number of Figures: 10
  Number of Tables: 7
  Number of authors: 3
  Paper title: A Hybrid RNN-HMM Approach for Weakly Supervised Temporal Action Segmentation
  Publication Date: 2018-12-21 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Results for Temporal Action Segmentation with GRU-Based Model
      Compared to MLP-Based Model and GMM over Five Iterations
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results for Temporal Action Segmentation on the Breakfast
      Dataset Comparing Accuracy of the Proposed System (GRU + Reestimation) to the
      Accuracy of the Same Architecture without Subactions (GRU No Subactions) and
      to the Architecture with Subclasses but without Reestimation
  Table 3 caption:
    table_text: TABLE 3 Results of Temporal Action Segmentation for Different Length
      Prior Functions on the Breakfast and the Hollywood Extended Dataset
  Table 4 caption:
    table_text: TABLE 4 Results for Temporal Action Segmentation with Semi Supervised
      Learning on the Breakfast and the Hollywood Extended Dataset with a Half Gaussian
      Length Prior
  Table 5 caption:
    table_text: TABLE 5 Comparison of Temporal Action Segmentation Performance for
      GRU Based Weak Learning with Other Approaches
  Table 6 caption:
    table_text: TABLE 6 Results for Temporal Action Alignment on the Test Set of the
      Breakfast and the Hollywood Extended Dataset Reported as Jaccard Index of Intersection
      over Detection (IoD)(Results Obtained from the Authors)
  Table 7 caption:
    table_text: TABLE 7 Results for Fully Supervised Temporal Action Segmentation
      on the Breakfast Dataset (MoF)
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2884469
- Affiliation of the first author: department of radiology and bric, university of
    north carolina at chapel hill, chapel hill, usa
  Affiliation of the last author: department of radiology and bric, university of
    north carolina at chapel hill, chapel hill, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\Hierarchical_Fully_Convolutional_Network_for_Joint_Atrophy_Localization_and_Alzh\figure_1.jpg
  Figure 1 caption: 'Illustration of our hierarchical fully convolutional network
    (H-FCN), which includes four components: 1) location proposals, 2) patch-level
    sub-networks, 3) region-level sub-networks, and 4) subject-level sub-network.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Hierarchical_Fully_Convolutional_Network_for_Joint_Atrophy_Localization_and_Alzh\figure_2.jpg
  Figure 2 caption: Comparison between no-prior locations proposals (i.e., nH-FCN)
    and with-prior location proposals (i.e., wH-FCN). (a) and (b) show the classification
    results for AD diagnosis and MCI conversion prediction, respectively.
  Figure 3 Link: articels_figures_by_rev_year\2018\Hierarchical_Fully_Convolutional_Network_for_Joint_Atrophy_Localization_and_Alzh\figure_3.jpg
  Figure 3 caption: Results of AD classification produced by our wH-FCN method with
    and without the network pruning strategy, respectively. For each case, the average
    classification performance of the sub-networks defined at different scales are
    presented.
  Figure 4 Link: articels_figures_by_rev_year\2018\Hierarchical_Fully_Convolutional_Network_for_Joint_Atrophy_Localization_and_Alzh\figure_4.jpg
  Figure 4 caption: Comparison between our wH-FCN models trained without and with
    transferred knowledge, respectively, for MCI conversion prediction. In the latter
    case, the parameters of the network for AD classification were transferred to
    initialize the training of the network for MCI conversion prediction.
  Figure 5 Link: articels_figures_by_rev_year\2018\Hierarchical_Fully_Convolutional_Network_for_Joint_Atrophy_Localization_and_Alzh\figure_5.jpg
  Figure 5 caption: "Results of AD classification obtained by our wH-FCN method in\
    \ terms of different numbers of input image patches (i.e., P=40,60,\u2026,120\
    \ )."
  Figure 6 Link: articels_figures_by_rev_year\2018\Hierarchical_Fully_Convolutional_Network_for_Joint_Atrophy_Localization_and_Alzh\figure_6.jpg
  Figure 6 caption: Results of AD classification obtained by our wH-FCN method in
    terms of different sizes of input image patches (i.e., 15times 15times 15 , 25times
    25times 25 , 35times 35times 35 , and 45times 45times 45 ).
  Figure 7 Link: articels_figures_by_rev_year\2018\Hierarchical_Fully_Convolutional_Network_for_Joint_Atrophy_Localization_and_Alzh\figure_7.jpg
  Figure 7 caption: Discriminative locations automatically identified by our proposed
    method at the patch-level (i.e., the left panel) and region-level (i.e., the right
    panel). The first to third rows correspond, respectively, to our proposed wH-FCN
    model trained for AD classification, our proposed nH-FCN model trained for AD
    classification, and our proposed wH-FCN model trained from scratch for MCI conversion
    prediction.
  Figure 8 Link: articels_figures_by_rev_year\2018\Hierarchical_Fully_Convolutional_Network_for_Joint_Atrophy_Localization_and_Alzh\figure_8.jpg
  Figure 8 caption: Voxel-level AD heatmaps for the discriminative patches automatically-identified
    by our H-FCN method in six different subjects. The heatmaps and the image patches
    have the same spatial resolution (i.e., 25times 25times 25 ). Note that voxels
    with warmer (or more yellow) colors in these heatmaps have higher discrminative
    capacities.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.64
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Chunfeng Lian
  Name of the last author: Dinggang Shen
  Number of Figures: 8
  Number of Tables: 4
  Number of authors: 4
  Paper title: Hierarchical Fully Convolutional Network for Joint Atrophy Localization
    and Alzheimer's Disease Diagnosis Using Structural MRI
  Publication Date: 2018-12-21 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Demographic Information of the Subjects Included in the Studied
      Datasets (i.e., the Baseline ADNI-1 and ADNI-2)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results for AD Classification (i.e., AD versus NC) and MCI
      Conversion Prediction (i.e., pMCI versus sMCI)
  Table 3 caption:
    table_text: TABLE 3 Results for AD Classification (i.e., AD versus NC) on the
      Baseline ADNI-1, Using the Baseline ADNI-2 as the Training Set
  Table 4 caption:
    table_text: TABLE 4 A Brief Description of the State-of-the-art Studies Using
      Baseline sMRI Data of ADNI-1 for AD Classification (i.e., AD versus NC) and
      MCI Conversion Prediction (i.e., pMCI versus sMCI)
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2889096
