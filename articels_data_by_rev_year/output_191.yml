- Affiliation of the first author: school of artificial intelligence, optics and electronics
    (iopen), northwestern polytechnical university, xi'an, shaanxi, china
  Affiliation of the last author: school of artificial intelligence, optics and electronics
    (iopen), northwestern polytechnical university, xi'an, shaanxi, china
  Figure 1 Link: articels_figures_by_rev_year\2023\LargeScale_Clustering_With_Structured_Optimal_Bipartite_Graph\figure_1.jpg
  Figure 1 caption: Illustration of the structured optimal bipartite graph. When a
    rank constraint is imposed on the Laplacian matrix of bipartite graph at left,
    the suboptimal graph is optimized to be structured optimal with the specific connected
    components ( c=2 here) in right.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\LargeScale_Clustering_With_Structured_Optimal_Bipartite_Graph\figure_2.jpg
  Figure 2 caption: Illustration of the anchors learning with a graph (ALG). Colorful
    blocks in the graph represent different similarity values. The gray blocks are
    the scores of samples, ranging from 0 to 1. Red star marks the anchor index in
    each iteration. I & N is the initialization and normalization of scores, while
    R & N is the recalculation and normalization of scores.
  Figure 3 Link: articels_figures_by_rev_year\2023\LargeScale_Clustering_With_Structured_Optimal_Bipartite_Graph\figure_3.jpg
  Figure 3 caption: "Illustration to the iterative anchor selecting process in a two-moon\
    \ data by using ALG, where plus \u201C+\u201D denotes the sample selected as anchor\
    \ in each iteration, and samples which are ever selected are marked by circles\
    \ \u201Co\u201D."
  Figure 4 Link: articels_figures_by_rev_year\2023\LargeScale_Clustering_With_Structured_Optimal_Bipartite_Graph\figure_4.jpg
  Figure 4 caption: Clustering results of three graph-based methods by using PKN with
    different number of pre-defined neighbors.
  Figure 5 Link: articels_figures_by_rev_year\2023\LargeScale_Clustering_With_Structured_Optimal_Bipartite_Graph\figure_5.jpg
  Figure 5 caption: Clustering results of three graph-based methods by using different
    graph construction strategies on three-ring data.
  Figure 6 Link: articels_figures_by_rev_year\2023\LargeScale_Clustering_With_Structured_Optimal_Bipartite_Graph\figure_6.jpg
  Figure 6 caption: Structured optimal bipartite graph learning on balanced data (a,
    c) and unbalanced data (e, g). The input graphs are the three-block diagonal matrix
    with different noise, and the output graphs are the corresponding graphs learned
    by LCSOG.
  Figure 7 Link: articels_figures_by_rev_year\2023\LargeScale_Clustering_With_Structured_Optimal_Bipartite_Graph\figure_7.jpg
  Figure 7 caption: The number of zero eigenvalues of the Laplacian matrix tildemathbf
    Ls after each iteration in Algorithm 1.
  Figure 8 Link: articels_figures_by_rev_year\2023\LargeScale_Clustering_With_Structured_Optimal_Bipartite_Graph\figure_8.jpg
  Figure 8 caption: Clustering results with the varying number of anchors. The x -axis
    represents the number of anchor points, while y -axis is the clustering accuracy
    (ACC) and normalized mutual information (NMI) in (a)(b)(c) and (d)(e)(f), respectively.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.66
  Name of the first author: Han Zhang
  Name of the last author: Xuelong Li
  Number of Figures: 8
  Number of Tables: 4
  Number of authors: 3
  Paper title: Large-Scale Clustering With Structured Optimal Bipartite Graph
  Publication Date: 2023-05-18 00:00:00
  Table 1 caption:
    table_text: TABLE I The Detailed Introduction to Benchmark Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Clustering Results Obtained by Using the Connectivity of
      the Input Graph B B (IN) and the Output Graph P P (OUT) in Algorithm 1 Under
      Different Number of Neighbors k k
  Table 3 caption:
    table_text: "TABLE III Comparison of the Proposed LCSOG Over Clustering Performance\
      \ (%, average \xB1 deviation) and CPU Time (second) Against K-Means Based Methods\
      \ (Non-Graph Based Methods) and Graph Based Methods on Three Large-Scale Datasets"
  Table 4 caption:
    table_text: "TABLE IV Clustering Performance of LCSOG Compared to a Baseline of\
      \ the Accelerated Algorithm (Nystr\xF6m), Three Regular Graph Based Algorithms\
      \ (CLR-L2, CLR-L1 and Ncut) and the Baseline of K-Means on Three Normal-Scale\
      \ Datasets"
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3277532
- Affiliation of the first author: computer science and artificial intelligence laboratory,
    mit, cambridge, ma, usa
  Affiliation of the last author: computer science and artificial intelligence laboratory,
    mit, cambridge, ma, usa
  Figure 1 Link: articels_figures_by_rev_year\2013\Bayesian_Nonparametric_Methods_for_PartiallyObservable_Reinforcement_Learning\figure_1.jpg
  Figure 1 caption: Reinforcement learning framework. At each time-step, the agent
    sends an action to the environment and receives an observation and reward.
  Figure 10 Link: articels_figures_by_rev_year\2013\Bayesian_Nonparametric_Methods_for_PartiallyObservable_Reinforcement_Learning\figure_10.jpg
  Figure 10 caption: Action selection comparison on tiger.
  Figure 2 Link: articels_figures_by_rev_year\2013\Bayesian_Nonparametric_Methods_for_PartiallyObservable_Reinforcement_Learning\figure_2.jpg
  Figure 2 caption: Graphical model showing a time-slice of a POMDP. The world-state
    st is a hidden variable.
  Figure 3 Link: articels_figures_by_rev_year\2013\Bayesian_Nonparametric_Methods_for_PartiallyObservable_Reinforcement_Learning\figure_3.jpg
  Figure 3 caption: Graphical model showing a time-slice of the DMM. Note that variables
    are observed at all times.
  Figure 4 Link: articels_figures_by_rev_year\2013\Bayesian_Nonparametric_Methods_for_PartiallyObservable_Reinforcement_Learning\figure_4.jpg
  Figure 4 caption: 'Graphical models showing the generative process for the iPOMDP
    and the iDMM: (a) iPOMDP graphical model. (b) Asymmetric iDMM graphical model.
    (c) Symmetric iDMM graphical model.'
  Figure 5 Link: articels_figures_by_rev_year\2013\Bayesian_Nonparametric_Methods_for_PartiallyObservable_Reinforcement_Learning\figure_5.jpg
  Figure 5 caption: 'Both lineworld and loopworld have hallways with a start (shaded),
    goal (double line), and identical middle states (a). The plots in (b) show the
    number of states inferred by the iPOMDP against the number of times the agent
    has traversed the hallways over 50 repeated trials: the black line shows the mean
    number of inferred iPOMDP states, and boxplots show the medians, quartiles, and
    outliers at each episode. Of note is that loopworld infers only necessary states,
    ignoring the more complex (but irrelevant) structure: (a) Cartoon of models. (b)
    Evolution of number of states.'
  Figure 6 Link: articels_figures_by_rev_year\2013\Bayesian_Nonparametric_Methods_for_PartiallyObservable_Reinforcement_Learning\figure_6.jpg
  Figure 6 caption: Evolution of reward from tiger-3. The agent's performance dips
    slightly after the third door is introduced, but then it adapts its representation
    to the new environment. The number of instantiated states also grows to accommodate
    this new possibility.
  Figure 7 Link: articels_figures_by_rev_year\2013\Bayesian_Nonparametric_Methods_for_PartiallyObservable_Reinforcement_Learning\figure_7.jpg
  Figure 7 caption: Learning rates for various algorithms in the gridworld domain.
    The iDMM models outperform U-Tree, a simpler history-based learning method, but
    do not outperform the hidden-variable methods. Among the hidden-variable methods,
    the Bayesian methods (iPOMDP and FFBS) outperform EM by avoiding local optima.
  Figure 8 Link: articels_figures_by_rev_year\2013\Bayesian_Nonparametric_Methods_for_PartiallyObservable_Reinforcement_Learning\figure_8.jpg
  Figure 8 caption: Performance of various algorithms on Benchmark Problems. The Bayesian
    hidden-variable approaches perform best overall, and iPOMDP matches or bests the
    performance of FFBS with less information about the state space.
  Figure 9 Link: articels_figures_by_rev_year\2013\Bayesian_Nonparametric_Methods_for_PartiallyObservable_Reinforcement_Learning\figure_9.jpg
  Figure 9 caption: Wall-clock running time of various algorithms on Benchmark Problems.
    All approaches were run on a computer with a 2.6GHz CPU. The iPOMDP, FFBS, and
    EM approaches were all coded in highly-optimized Matlab and shared subcomputations.
    The U-Tree approach was also coded in highly-optimized Matlab. The iDMM inference
    was coded in Java. Each circle in the top figure shows the running time of each
    comparison algorithm compared to the iPOMDP for a particular domain. A value greater
    than 1 means that the comparison algorithm was slower than the iPOMDP. While simple
    algorithms, such as EM and U-Tree, generally run faster than iPOMDP, they had
    significantly worse performance. FFBS-Big appears faster than FFBS because it
    could only be run on the six smallest domains.
  First author gender probability: 0.55
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Finale Doshi-Velez
  Name of the last author: Nicholas Roy
  Number of Figures: 11
  Number of Tables: 1
  Number of authors: 4
  Paper title: Bayesian Nonparametric Methods for Partially-Observable Reinforcement
    Learning
  Publication Date: 2013-10-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of iPOMDP Benchmarks
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2013.191
- Affiliation of the first author: preston m. green department of electrical and system
    engineering, washington university in st. louis, 14049 agusta dr., chesterfield,
    mo
  Affiliation of the last author: department of statistics, columbia university, room
    1026 ssw, mc 4690, 1255 amsterdam ave, new york, ny
  Figure 1 Link: articels_figures_by_rev_year\2013\Scaling_Multidimensional_Inference_for_Structured_Gaussian_Processes\figure_1.jpg
  Figure 1 caption: Graphical model for projected additive GP regression. In general,
    P ne D . We present a greedy algorithm to select P , and jointly optimize bf W
    and thetapp=1P .
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2013\Scaling_Multidimensional_Inference_for_Structured_Gaussian_Processes\figure_2.jpg
  Figure 2 caption: A comparison of runtimes for efficient bayesian additive GP regression,
    with D=8 . The algorithms ran on a Linux server, in a multicore parallel scheme
    using eight processors (solid lines). For comparison, we also added the runtime
    of PPGPR-Greedy using a single thread (dash lines). At rm N=7,168 , we added an
    overlay of the runtime results for the pumadyn8-nm data set (described in section
    3.2.1), showing that this figure is representative of runtime in real data sets.
  Figure 3 Link: articels_figures_by_rev_year\2013\Scaling_Multidimensional_Inference_for_Structured_Gaussian_Processes\figure_3.jpg
  Figure 3 caption: 'This figure shows the runtime of the classification algorithms
    for the synthetic data set with D=8 . For the learning stage we used 50 iterations,
    and we did prediction on 1,000 points. The log-log slopes of the algorithms are:
    Full-GP = 2.75 , Additive-LA = 1.07 , FIC = 1.53 , IVM = 0.80 , SVM = 2.16 .'
  Figure 4 Link: articels_figures_by_rev_year\2013\Scaling_Multidimensional_Inference_for_Structured_Gaussian_Processes\figure_4.jpg
  Figure 4 caption: Runtime complexity of naive full-GP, GP-grid, and GP-grid spherical.
    The runtime illustrated is for a single calculation of the negative log marginal
    likelihood and its derivatives (dnlml). The ratio of input size to the complete
    grid size ( nN ) is 0.7. The slope for the naive full-gp is 2.9, for GP-grid is
    1.1, and for gp-grid spherical is 1.0 (based on the last eight points). This empirically
    verifies the improvement in scaling.
  Figure 5 Link: articels_figures_by_rev_year\2013\Scaling_Multidimensional_Inference_for_Structured_Gaussian_Processes\figure_5.jpg
  Figure 5 caption: These figures offer a comparison between the different gp methods
    discussed in the text, taking into account both speedup and accuracy. For comparison
    we used several known data sets from the literature and ran the algorithms on
    a multicore (eight-core) computer. The top figure illustrates the speedup of the
    approximation algorithms runtimes with respect to the full-gp (exact inference)
    runtime. The bottom two figures show two metrics for calculating regression accuracy.
  Figure 6 Link: articels_figures_by_rev_year\2013\Scaling_Multidimensional_Inference_for_Structured_Gaussian_Processes\figure_6.jpg
  Figure 6 caption: The two fundamental desiderata of our algorithms are accuracy
    and speed. Here we plot error versus runtime to quantify the tradeoff between
    these two objectives using the notion of pareto efficiency. Every algorithm is
    represented using a unique marker and with a color scheme chosen according to
    the data sets. For each data set, the pareto efficient frontier is shown as a
    color line passing through the efficient algorithms for that data set.
  Figure 7 Link: articels_figures_by_rev_year\2013\Scaling_Multidimensional_Inference_for_Structured_Gaussian_Processes\figure_7.jpg
  Figure 7 caption: As in Fig. 6, here we plot error (MNLL) versus runtime to quantify
    the tradeoff between these two objectives using pareto efficiency.
  Figure 8 Link: articels_figures_by_rev_year\2013\Scaling_Multidimensional_Inference_for_Structured_Gaussian_Processes\figure_8.jpg
  Figure 8 caption: An example of the face image separated to an object segment (Fig.
    8a) and a background segment (Fig. 8b), which are used for interpolation comparison
    along with their empirical noise versus intensity model (Fig. 8c). The red line
    corresponds to the camera specific linear noise model in eq. (42).
  Figure 9 Link: articels_figures_by_rev_year\2013\Scaling_Multidimensional_Inference_for_Structured_Gaussian_Processes\figure_9.jpg
  Figure 9 caption: Average monthly land surface temperatures in north america in
    1950. The left column presents the real measurements. Small images (nine months)
    were used as a training set, and april, august, and december were used as a held-out
    test set. The middle and right columns show the corresponding gp-grid posterior
    mean and 95 percent confidence intervals for april, august, and december.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Elad Gilboa
  Name of the last author: John P. Cunningham
  Number of Figures: 9
  Number of Tables: 2
  Number of authors: 3
  Paper title: Scaling Multidimensional Inference for Structured Gaussian Processes
  Publication Date: 2013-10-01 00:00:00
  Table 1 caption:
    table_text: Table 1 Performance comparison of efficient additive GP classification
      algorithms with commonly-used classification techniques on larger data sets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Table 2 Comparison of standardized MSE interpolation results for images
      with additive variable gaussian noise we tested each image when taken as a whole
      (W), object segment (O), and background segment (B).
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2013.192
- Affiliation of the first author: school of computer science and technology, university
    of electronic science and technology of china, chengdu, china
  Affiliation of the last author: department of computer science and department of
    statistics, purdue university, west lafayette, usa
  Figure 1 Link: articels_figures_by_rev_year\2013\Bayesian_Nonparametric_Models_for_Multiway_Data_Analysis\figure_1.jpg
  Figure 1 caption: Illustration of Tucker decomposition on a 3-mode tensor mathcal
    Y . The core tensor mathcal W is multiplied by matrices mathbf U1 , mathbf U2
    , and mathbf U3 on different dimensions to obtain mathcal Y .
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2013\Bayesian_Nonparametric_Models_for_Multiway_Data_Analysis\figure_2.jpg
  Figure 2 caption: Prediction accuracies on three tensor datasets. Both average AUC
    values and their stand errors are reported. (a) Enron. (b) digg1. (c) digg2.
  Figure 3 Link: articels_figures_by_rev_year\2013\Bayesian_Nonparametric_Models_for_Multiway_Data_Analysis\figure_3.jpg
  Figure 3 caption: "Synthetic network and the estimated networks. (a) Shows the noisy\
    \ network data containing groups. (b)\u2013(d) Show the posterior means of mathbf\
    \ Y estimated by LEM, MMSB, and SMGB, respectively."
  Figure 4 Link: articels_figures_by_rev_year\2013\Bayesian_Nonparametric_Models_for_Multiway_Data_Analysis\figure_4.jpg
  Figure 4 caption: AUC on the synthetic data. SMGB outperforms LEM and MMSB in terms
    of prediction accuracy.
  Figure 5 Link: articels_figures_by_rev_year\2013\Bayesian_Nonparametric_Models_for_Multiway_Data_Analysis\figure_5.jpg
  Figure 5 caption: Visualization of the true memberships mathbf U0 and the membership
    matrices mathbf U estimated by each method.
  Figure 6 Link: articels_figures_by_rev_year\2013\Bayesian_Nonparametric_Models_for_Multiway_Data_Analysis\figure_6.jpg
  Figure 6 caption: 'AUC values of SMGB, MMSB, and LEM on Coauthor and Friend networks.
    The dimension of the latent membership vector varies: d = 3,5,7. (a) Coauthor.
    (b) Friend.'
  Figure 7 Link: articels_figures_by_rev_year\2013\Bayesian_Nonparametric_Models_for_Multiway_Data_Analysis\figure_7.jpg
  Figure 7 caption: 'Estimated membership matrices of SMGB, MMSB, and LEM on Coauthor
    and Friend networks. The dimension of the latent membership vector varies: d =
    3,5,7. (a) d = 3, Coauthor. (b) d = 5, Coauthor. (c) d = 7, Coauthor. (d) d =
    3, Friend. (e) d = 5, Friend. (f) d = 7, Friend.'
  Figure 8 Link: articels_figures_by_rev_year\2013\Bayesian_Nonparametric_Models_for_Multiway_Data_Analysis\figure_8.jpg
  Figure 8 caption: Log of the variational lower bound (after removing constants)
    and the prediction accuracy. (a) Variational lower bound. (b) AUC.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Zenglin Xu
  Name of the last author: Yuan Qi
  Number of Figures: 8
  Number of Tables: 2
  Number of authors: 3
  Paper title: Bayesian Nonparametric Models for Multiway Data Analysis
  Publication Date: 2013-10-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Mean Square Errors (MSE) with Standard Errors for Predictions
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Keyword Groups Identified by InfTucker gp
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2013.201
- Affiliation of the first author: department of information, risk, and operations
    management, mccombs school of business, university of texas at austin, austin,
    tx, usa
  Affiliation of the last author: department of electrical and computer engineering,
    duke university, durham, nc, usa
  Figure 1 Link: articels_figures_by_rev_year\2013\Negative_Binomial_Process_Count_and_Mixture_Modeling\figure_1.jpg
  Figure 1 caption: Poisson-logarithmic bivariate distribution models the total numbers
    of customers and tables as random variables. As shown in Theorem 1, it has two
    equivalent representations, which connect the Poisson, logarithmic, and negative
    binomial distributions and the Chinese restaurant process.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2013\Negative_Binomial_Process_Count_and_Mixture_Modeling\figure_2.jpg
  Figure 2 caption: Graphical models of the gamma-negative binomial process under
    the gamma-gamma-Poisson (left), gamma-compound Poisson (center), and gamma-negative
    binomial-Chinese restaurant table constructions (right). The center and right
    constructions are equivalent in distribution.
  Figure 3 Link: articels_figures_by_rev_year\2013\Negative_Binomial_Process_Count_and_Mixture_Modeling\figure_3.jpg
  Figure 3 caption: Distinct sharing mechanisms and model properties are evident between
    various NB process topic models, by comparing their inferred NB dispersion parameters
    ( rk or rj ) and probability parameters ( pk or pj ). Note that the transition
    between active and non-active topics is very sharp when pk is used and much smoother
    when rk is used. Both the documents and topics are ordered in a decreasing order
    based on the associated number of words. These results are based on the last MCMC
    iteration, on the Psychological Review corpus with 80% of the words in each document
    used as training. The values along the vertical axis are shown in either linear
    or log scales for convenient visualization. Document-specific and topic-specific
    parameters are shown in blue and red colors, respectively.
  Figure 4 Link: articels_figures_by_rev_year\2013\Negative_Binomial_Process_Count_and_Mixture_Modeling\figure_4.jpg
  Figure 4 caption: Comparison of per-word perplexity on held out words between various
    algorithms listed in Table 1 on the Psychological Review corpus. LDA-Optimal-
    alpha refers to an LDA algorithm whose topic proportion Dirichlet concentration
    parameter alpha is optimized based on the results of the CRF-HDP on the same dataset.
    (a) With 60% of the words in each document used for training, the performance
    varies as a function of K in both LDA and NB-LDA, which are parametric models,
    whereas the NB, Beta-Geometric, NB-HDP, NB-FTM, Beta-NB, CRF-HDP, Gamma-NB and
    Marked-Beta-NB all infer the number of active topics, which are 225, 28, 127,
    201, 107, 161, 177 and 130, respectively, according to the last Gibbs sampling
    iteration. (b) Per-word perplexities of various algorithms as a function of the
    percentage of words in each document used for training. The results of LDA and
    NB-LDA are shown with the best settings of K under each trainingtesting partition.
    Nonparametric Bayesian algorithms listed in Table 1 are ranked in the legend from
    top to bottom according to their overall performance.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.69
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Mingyuan Zhou
  Name of the last author: Lawrence Carin
  Number of Figures: 4
  Number of Tables: 1
  Number of authors: 2
  Paper title: Negative Binomial Process Count and Mixture Modeling
  Publication Date: 2013-10-18 00:00:00
  Table 1 caption:
    table_text: "TABLE I Variety of Negative Binomial Processes are Constructed with\
      \ Distinct Sharing Mechanisms, Reflected with Which Parameters from r k , r\
      \ j , p k , p j and \u03C0 k ( b jk ) are Inferred (Indicated by a Check-Mark\
      \ \u2713 ), and the Implied Variance-Mean-Ratio (VMR) and Overdispersion Level\
      \ (ODL) for Counts n jk j,k"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2013.211
- Affiliation of the first author: statistics department, university of washington,
    seattle, wa, usa
  Affiliation of the last author: department of information, university of texas at
    austin, austin, tx, usa
  Figure 1 Link: articels_figures_by_rev_year\2013\A_Survey_of_NonExchangeable_Priors_for_Bayesian_Nonparametric_Models\figure_1.jpg
  Figure 1 caption: 'Left: Atoms following a stochastic process. Right: Resulting
    random measures at three covariate values.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2013\A_Survey_of_NonExchangeable_Priors_for_Bayesian_Nonparametric_Models\figure_2.jpg
  Figure 2 caption: 'Left: Stick lengths following stochastic processes. The length
    of a stick at a given covariate value is the width of the colored region at the
    covariate. Middle: Masses of the random measure by combining the covariate-dependent
    sticks according to the stick-breaking construction. Right: Resulting random measures
    at three covariate values.'
  Figure 3 Link: articels_figures_by_rev_year\2013\A_Survey_of_NonExchangeable_Priors_for_Bayesian_Nonparametric_Models\figure_3.jpg
  Figure 3 caption: Markov chain of gamma processes. The current gamma process Gn
    is first sub-sampled according to mathcal S(cdot, q) where the dashed masses are
    removed and new atoms from the gamma process Gn are added resulting in the gamma
    process Gn+1 .
  Figure 4 Link: articels_figures_by_rev_year\2013\A_Survey_of_NonExchangeable_Priors_for_Bayesian_Nonparametric_Models\figure_4.jpg
  Figure 4 caption: 'Left: Masses of CRM modulated by smooth functions. Right: Resulting
    random measures at three covariate values.'
  Figure 5 Link: articels_figures_by_rev_year\2013\A_Survey_of_NonExchangeable_Priors_for_Bayesian_Nonparametric_Models\figure_5.jpg
  Figure 5 caption: 'Left: Masses of CRM modulated by piecewise-constant functions.
    Right: Resulting random measures at three covariate values.'
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Nicholas J. Foti
  Name of the last author: Sinead A. Williamson
  Number of Figures: 5
  Number of Tables: 0
  Number of authors: 2
  Paper title: A Survey of Non-Exchangeable Priors for Bayesian Nonparametric Models
  Publication Date: 2013-11-04 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2013.224
- Affiliation of the first author: department of statistics, the ohio state university,
    404 cockins hall, 1958 neil ave., columbus, oh
  Affiliation of the last author: department of statistics, the ohio state university,
    404 cockins hall, 1958 neil ave., columbus, oh
  Figure 1 Link: articels_figures_by_rev_year\2013\Modeling_NonGaussian_Time_Series_with_Nonparametric_Bayesian_Model\figure_1.jpg
  Figure 1 caption: Serial plots of the three simulation data sets. Each data set
    consists of 500 observations.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2013\Modeling_NonGaussian_Time_Series_with_Nonparametric_Bayesian_Model\figure_2.jpg
  Figure 2 caption: The relationship between the observed series Xt and the transformed
    series Wt .
  Figure 3 Link: articels_figures_by_rev_year\2013\Modeling_NonGaussian_Time_Series_with_Nonparametric_Bayesian_Model\figure_3.jpg
  Figure 3 caption: The one-, five- and 10-step ahead predictive densities based on
    the bimodal innovation model, the ctar(1) model, and the AR(1) model with gaussian
    innovations.
  Figure 4 Link: articels_figures_by_rev_year\2013\Modeling_NonGaussian_Time_Series_with_Nonparametric_Bayesian_Model\figure_4.jpg
  Figure 4 caption: Total variation distances between estimated marginal distributions
    and true marginal distributions.
  Figure 5 Link: articels_figures_by_rev_year\2013\Modeling_NonGaussian_Time_Series_with_Nonparametric_Bayesian_Model\figure_5.jpg
  Figure 5 caption: Comparison of total variation distances of estimated marginal
    distributions versus true marginal distributions.
  Figure 6 Link: articels_figures_by_rev_year\2013\Modeling_NonGaussian_Time_Series_with_Nonparametric_Bayesian_Model\figure_6.jpg
  Figure 6 caption: The relationship between the observed series Xt and the transformed
    series Wt for stock indexes.
  Figure 7 Link: articels_figures_by_rev_year\2013\Modeling_NonGaussian_Time_Series_with_Nonparametric_Bayesian_Model\figure_7.jpg
  Figure 7 caption: Differences in log predictive distributions based on ctar(1)-garch(1,
    1) model and based on AR(1)-garch(1, 1) model with gaussian innovations.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Zhiguang Xu
  Name of the last author: Xinyi Xu
  Number of Figures: 7
  Number of Tables: 4
  Number of authors: 3
  Paper title: Modeling Non-Gaussian Time Series with Nonparametric Bayesian Model
  Publication Date: 2013-11-04 00:00:00
  Table 1 caption:
    table_text: Table 1 Posterior estimates of under the copula model
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Table 2 Total variation distances between estimated predictivedistributions
      and true predictive distributions
  Table 3 caption:
    table_text: Table 3 Posterior estimates under the bayesian copula(CTAR(1)-GARCH(1,
      1)) model for stock indexes
  Table 4 caption:
    table_text: Table 4 Posterior estimates for the differences in log predictivelikelihoods
      based on CTAR(1)-GARCH(1,1) modeland based on AR(1)-GARCH(1,1) model
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2013.222
- Affiliation of the first author: department of computing, imperial college london,
    london, united kingdom
  Affiliation of the last author: department of engineering, university of cambridge,
    trumpington street, cambridge cb21pz, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2013\Gaussian_Processes_for_DataEfficient_Learning_in_Robotics_and_Control\figure_1.jpg
  Figure 1 caption: 'Effect of model errors. Left: small data set of observed transitions
    from an idealized one-dimensional representations of states and actions (xt,ut)
    to the next state. Center: multiple plausible deterministic models. Right: probabilistic
    model. The probabilistic model describes the uncertainty about the latent function
    by a probability distribution on the set of all plausible transition functions.
    Predictions with deterministic models are claimed with full confidence, while
    the probabilistic model expresses its predictive uncertainty by a probability
    distribution.'
  Figure 10 Link: articels_figures_by_rev_year\2013\Gaussian_Processes_for_DataEfficient_Learning_in_Robotics_and_Control\figure_10.jpg
  Figure 10 caption: Robotic unicycle system and simulation results. The state space
    is rm Ikern-2pt R12 , the control space rm Ikern-2pt R2 .
  Figure 2 Link: articels_figures_by_rev_year\2013\Gaussian_Processes_for_DataEfficient_Learning_in_Robotics_and_Control\figure_2.jpg
  Figure 2 caption: Gp prediction at an uncertain input. The input distribution p(schmi
    xt,schmi ut) is assumed Gaussian (lower left panel). When propagating it through
    the GP model (upper left panel), we obtain the shaded distribution p(bm Delta
    t) , upper right panel. We approximate p(bm Delta t) by a gaussian (upper right
    panel), which is computed by means of either moment matching (blue) or linearization
    of the posterior gp mean (red). Using linearization for approximate inference
    can lead to predictive distributions that are too tight.
  Figure 3 Link: articels_figures_by_rev_year\2013\Gaussian_Processes_for_DataEfficient_Learning_in_Robotics_and_Control\figure_3.jpg
  Figure 3 caption: Constraining the control signal. Panel (a) shows an example of
    an unconstrained preliminary policy tildepi as a function of the state x . Panel
    (b) shows the constrained policy pi (x) = sigma (tildepi (x)) as a function of
    the state x .
  Figure 4 Link: articels_figures_by_rev_year\2013\Gaussian_Processes_for_DataEfficient_Learning_in_Robotics_and_Control\figure_4.jpg
  Figure 4 caption: Automatic exploration and exploitation with the saturating cost
    function (blue, solid). The x -axes describe the state space. The target state
    is the origin.
  Figure 5 Link: articels_figures_by_rev_year\2013\Gaussian_Processes_for_DataEfficient_Learning_in_Robotics_and_Control\figure_5.jpg
  Figure 5 caption: Double pendulum with two actuators applying torques u1 and u2
    . The cost function penalizes the distance d to the target.
  Figure 6 Link: articels_figures_by_rev_year\2013\Gaussian_Processes_for_DataEfficient_Learning_in_Robotics_and_Control\figure_6.jpg
  Figure 6 caption: 'Empirical computational demand for approximate inference and
    derivative computation with GPS for a single time step, shown on a log scale.
    (a): linearization of the posterior GP mean. (b): exact moment matching.'
  Figure 7 Link: articels_figures_by_rev_year\2013\Gaussian_Processes_for_DataEfficient_Learning_in_Robotics_and_Control\figure_7.jpg
  Figure 7 caption: Results for the cart-pole swing-up task. (a) learning curves for
    moment matching and linearization (simulation task), (b) required interaction
    time for solving the cart-pole swing-up task compared with other algorithms.
  Figure 8 Link: articels_figures_by_rev_year\2013\Gaussian_Processes_for_DataEfficient_Learning_in_Robotics_and_Control\figure_8.jpg
  Figure 8 caption: Average success as a function of the total data used for learning
    (double pendulum swing-up). The blue error bars show the 95 percent confidence
    bounds of the standard error for the moment matching approximation, the red area
    represents the corresponding confidence bounds of success when using approximate
    inference by means of linearizing the posterior gp mean (lin).
  Figure 9 Link: articels_figures_by_rev_year\2013\Gaussian_Processes_for_DataEfficient_Learning_in_Robotics_and_Control\figure_9.jpg
  Figure 9 caption: Long-term predictive (gaussian) distributions during planning
    (shaded) and sample rollouts (red). (a) in the early stages of learning, the gaussian
    approximation is a suboptimal choice. (b) pilco learned a controller such that
    the gaussian approximations of the predictive states are good. Note the different
    scales in (a) and (b).
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Marc Peter Deisenroth
  Name of the last author: Carl Edward Rasmussen
  Number of Figures: 13
  Number of Tables: 1
  Number of authors: 3
  Paper title: Gaussian Processes for Data-Efficient Learning in Robotics and Control
  Publication Date: 2013-11-04 00:00:00
  Table 1 caption:
    table_text: Table 1 Average learning success with learned nonparametric(NP) transition
      models (cart-pole swing-up)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2013.218
- Affiliation of the first author: department of economics and statistics, university
    of torino, torino, italy
  Affiliation of the last author: department of economics and statistics, university
    of torino, torino, italy
  Figure 1 Link: articels_figures_by_rev_year\2013\Are_GibbsType_Priors_the_Most_Natural_Generalization_of_the_Dirichlet_Process\figure_1.jpg
  Figure 1 caption: Prior distributions on the number of groups corresponding to the
    NGG process with n=50, beta =1 and sigma =0.2,0.3,ldots, 0.7 and sigma =0.8 .
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2013\Are_GibbsType_Priors_the_Most_Natural_Generalization_of_the_Dirichlet_Process\figure_2.jpg
  Figure 2 caption: "Prior distributions on the number of clusters corresponding to\
    \ the Dirichlet (DP), the Pitman\u2013Yor (PY) and the normalized generalized\
    \ gamma (NGG) processes. The values of the parameters are set in such a way that\
    \ mathrm E(K50)=25 ."
  Figure 3 Link: articels_figures_by_rev_year\2013\Are_GibbsType_Priors_the_Most_Natural_Generalization_of_the_Dirichlet_Process\figure_3.jpg
  Figure 3 caption: "Posterior distributions on the number of components corresponding\
    \ to mixtures of the Dirichlet (DP), the Pitman\u2013Yor (PY) and the normalized\
    \ generalized gamma (NGG) processes with n=50 and parameters set so that mathrm\
    \ E(K50)=25 ."
  Figure 4 Link: articels_figures_by_rev_year\2013\Are_GibbsType_Priors_the_Most_Natural_Generalization_of_the_Dirichlet_Process\figure_4.jpg
  Figure 4 caption: Density estimates corresponding to the 5 mixture models that have
    been considered.
  Figure 5 Link: articels_figures_by_rev_year\2013\Are_GibbsType_Priors_the_Most_Natural_Generalization_of_the_Dirichlet_Process\figure_5.jpg
  Figure 5 caption: "EST data from Naegleria gruberi aerobic and anaerobic cDNA libraries\
    \ with basic sample n cong 950 : Good-Toulmin (GT) and Pitman\u2013Yor process\
    \ (PY) estimators of the probability of discovering a new gene at the (n+m+1)\
    \ \u2013th sampling step for m=1, ldots, 2000 ."
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Pierpaolo De Blasi
  Name of the last author: Matteo Ruggiero
  Number of Figures: 5
  Number of Tables: 2
  Number of authors: 6
  Paper title: Are Gibbs-Type Priors the Most Natural Generalization of the Dirichlet
    Process?
  Publication Date: 2013-11-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Posterior Distributions of K n
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 ESTs from Two Naegleria Gruberi Libraries
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2013.217
- Affiliation of the first author: university of queensland, st. lucia, brisbane,
    qld, australia
  Affiliation of the last author: robotics institute, carnegie mellon university,
    pittsburgh, pa
  Figure 1 Link: articels_figures_by_rev_year\2013\Convolutional_Sparse_Coding_for_Trajectory_Reconstruction\figure_1.jpg
  Figure 1 caption: The top row (a) depicts a single point x-coordinate trajectory
    of a 3d moving canine. The second row (b) depicts the set of fixed size DCT basis
    vectors that are required to reconstruct the trajectory. The third row (c) depicts
    the set of fixed size sparse coding basis vectors that are required to reconstruct
    the trajectory. The fourth row (d) depicts the set of basis vectors generated
    from the set of convolutional sparse filters. The final row (e) depicts the individual
    convolutional sparse filters. The sparse coding bases and convolutional sparse
    coding filters are learned independently on the CMU motion capture data set including
    many different subjects of human motion (better viewed in color).
  Figure 10 Link: articels_figures_by_rev_year\2013\Convolutional_Sparse_Coding_for_Trajectory_Reconstruction\figure_10.jpg
  Figure 10 caption: The visualization of the reconstructed structure at camera speed
    0.25pi sec. Visualizations of (a) the DCT, (b) sparse coding, and (c) convolutional
    sparse coding basis all employing the ell1 objective (Equation (24)) (better viewed
    in color).
  Figure 2 Link: articels_figures_by_rev_year\2013\Convolutional_Sparse_Coding_for_Trajectory_Reconstruction\figure_2.jpg
  Figure 2 caption: Comparison of different trajectory basis in terms of normalized
    root mean square (rms) error (err) as a function of the number of active ( K(cal
    S) ) basis vectors. Reconstruction error was calculated on the actual 3d trajectories
    (not 2d projections). The sequence length is 150. Results show that a sparse coded
    basis can encode unseen 3d trajectory observations far more sparsely than conventional
    trajectory bases (dct) (better viewed in color).
  Figure 3 Link: articels_figures_by_rev_year\2013\Convolutional_Sparse_Coding_for_Trajectory_Reconstruction\figure_3.jpg
  Figure 3 caption: Reconstruction (normalized RMS error) versus different camera
    angle speeds for the canine sequences. 2d projections were generated using a synthetic
    orthographic camera with the y-axis pointed to the center of the object with the
    angle of rotation speed being varied (better viewed in color).
  Figure 4 Link: articels_figures_by_rev_year\2013\Convolutional_Sparse_Coding_for_Trajectory_Reconstruction\figure_4.jpg
  Figure 4 caption: The condition of matrix bf Qperp T bf Mcal S bf Qperp versus different
    camera angle speeds (better viewed in color).
  Figure 5 Link: articels_figures_by_rev_year\2013\Convolutional_Sparse_Coding_for_Trajectory_Reconstruction\figure_5.jpg
  Figure 5 caption: Reconstruction (normalized RMS error) versus different length
    trajectories by sparse coding basis and convolutional sparse coding basis at camera
    speed 0.25 pi sec. The size of convolutional filter is 30 frames (better viewed
    in color).
  Figure 6 Link: articels_figures_by_rev_year\2013\Convolutional_Sparse_Coding_for_Trajectory_Reconstruction\figure_6.jpg
  Figure 6 caption: Basis ratio (i.e., K3F ) versus sequence size ( F ) for the sparse
    coding and convolutional sparse coding bases. The size of convolutional filters
    is 30 frames (better viewed in color).
  Figure 7 Link: articels_figures_by_rev_year\2013\Convolutional_Sparse_Coding_for_Trajectory_Reconstruction\figure_7.jpg
  Figure 7 caption: The reconstruction (normalized RMS error) versus the ratio of
    missing data (better viewed in color).
  Figure 8 Link: articels_figures_by_rev_year\2013\Convolutional_Sparse_Coding_for_Trajectory_Reconstruction\figure_8.jpg
  Figure 8 caption: Reconstruction (normalized RMS error) versus the noise magnitude
    (better viewed in color).
  Figure 9 Link: articels_figures_by_rev_year\2013\Convolutional_Sparse_Coding_for_Trajectory_Reconstruction\figure_9.jpg
  Figure 9 caption: 'The reconstructed trajectories versus the ground truth trajectories
    at camera speed 0.25 pi sec for the: (a) DCT, (b) sparse coding, and (c) convolutional
    sparse coding bases using the ell1 (Equation (24)) objective. As expected the
    convolutional sparse coding basis obtained superior performance (better viewed
    in color).'
  First author gender probability: 0.61
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yingying Zhu
  Name of the last author: Simon Lucey
  Number of Figures: 12
  Number of Tables: 0
  Number of authors: 2
  Paper title: Convolutional Sparse Coding for Trajectory Reconstruction
  Publication Date: 2013-12-18 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2013.2295311
