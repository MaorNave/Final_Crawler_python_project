- Affiliation of the first author: key laboratory of machine perception (moe), school
    of electronics engineering and computer science, peking university, beijing, p.r.
    china
  Affiliation of the last author: key laboratory of machine perception (moe), school
    of electronics engineering and computer science, peking university, beijing, p.r.
    china
  Figure 1 Link: articels_figures_by_rev_year\2020\Training_Neural_Networks_by_Lifted_Proximal_Operator_Machines\figure_1.jpg
  Figure 1 caption: Comparison of LPOM and three gradient-based methods on the MNIST
    and the CIFAR-10 datasets. The inset figures in (a), (b), and (d) show the accuracies
    for the final 30 epochs in detail. Images in this paper are best viewed in color.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Training_Neural_Networks_by_Lifted_Proximal_Operator_Machines\figure_2.jpg
  Figure 2 caption: Comparison of LPOM and three gradient-based methods on the ImageNet32x32
    dataset.
  Figure 3 Link: articels_figures_by_rev_year\2020\Training_Neural_Networks_by_Lifted_Proximal_Operator_Machines\figure_3.jpg
  Figure 3 caption: "Comparison of LPOM with various activation functions. The penalty\
    \ parameters \u03BC i s in (19) are all fixed at 5."
  Figure 4 Link: articels_figures_by_rev_year\2020\Training_Neural_Networks_by_Lifted_Proximal_Operator_Machines\figure_4.jpg
  Figure 4 caption: Results of the speedup experiments. (a) and (b) are the training
    time and the speedup ratio with a variety of layers, respectively. (c) and (d)
    are the training time and the speedup ratio with a variety of CPU cores, respectively.
    (e) are curves of the training loss versus the running time.
  Figure 5 Link: articels_figures_by_rev_year\2020\Training_Neural_Networks_by_Lifted_Proximal_Operator_Machines\figure_5.jpg
  Figure 5 caption: Comparison of Adam, AMSGrad, SGD, and sync-LPOM with single-layer,
    multi-layer, and deep autoencoders. The first row is the training losses, while
    the second row is the test losses. The architectures are given in Table 7. The
    numbers in boldface denote the real speedup ratios of sync-LPOM over SGD with
    respective autoencoders.
  Figure 6 Link: articels_figures_by_rev_year\2020\Training_Neural_Networks_by_Lifted_Proximal_Operator_Machines\figure_6.jpg
  Figure 6 caption: Reconstructions using multi-layer autoencoders trained on the
    MNIST dataset. The top row is original images. The following rows are the results
    of autoencoders trained by Adam, AMSGrad, SGD, and sync-LPOM, respectively.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.64
  Name of the first author: Jia Li
  Name of the last author: Zhouchen Lin
  Number of Figures: 6
  Number of Tables: 7
  Number of authors: 6
  Paper title: Training Neural Networks by Lifted Proximal Operator Machines
  Publication Date: 2020-12-31 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Property Summary of LPOM as Compared to SGD
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Summary of the Main Notations Used in This Paper
  Table 3 caption:
    table_text: TABLE 3 The f(x) f(x) and g(x) g(x) of Several Commonly Used Activation
      Functions
  Table 4 caption:
    table_text: TABLE 4 Comparison of Test Accuracies of LPOM, [23], and [24] on the
      MNIST Dataset Using Different Networks
  Table 5 caption:
    table_text: TABLE 5 Comparison of Test Accuracies of LPOM, SGD, [17], [23], and
      [24] on the SVHN Dataset
  Table 6 caption:
    table_text: "TABLE 6 The Training and the Test Accuracies of LPOM on the MNIST\
      \ Dataset With Varying \u03BC \u03BC and Activation Function Used"
  Table 7 caption:
    table_text: TABLE 7 Experimental Settings of Autoencoder Training
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3048430
- Affiliation of the first author: department of computer science, city university
    of hong kong, kowloon, hong kong
  Affiliation of the last author: department of computer science, city university
    of hong kong, kowloon, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2021\PRIMALGMM_PaRametrIc_MAnifold_Learning_of_Gaussian_Mixture_Models\figure_1.jpg
  Figure 1 caption: "Learning a parametric manifold for GMMs. (left) Given a set of\
    \ GMM distributions \u03D5 i N i=1 , parametrized by component priors, means,\
    \ and covariances \u03D5 i = \u03C0 ik , \u03BC ik , \u03A3 ik K b k=1 , the goal\
    \ is to learn a low-dimensional manifold spanning the GMM distributions. (middle)\
    \ On the manifold, the GMM parameters \u03D5 i = \u03C0 ir , \u03BC ir , \u03A3\
    \ ir K r k=1 are generated from a latent space (w,z,y) by their corresponding\
    \ generative functions ( f \u22121 \u03C0 r ( w i ), f \u22121 \u03BC r ( z i\
    \ ), f \u22121 \u03A3 r ( y i )) . The parameters of the generative functions\
    \ are obtained by minimizing the reconstruction loss (KL divergence) between the\
    \ given GMMs \u03D5 i and their reconstructions \u03D5 i . (right) We further\
    \ assume that the latent space (w,z,y) is generated from a hierarchical latent\
    \ space (HLS) v by parametric functions h \u22121 (v) to further reduce the dimension,\
    \ which models dependencies among the component priors, means, and covariances."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\PRIMALGMM_PaRametrIc_MAnifold_Learning_of_Gaussian_Mixture_Models\figure_2.jpg
  Figure 2 caption: Experiment on synthetic data. The positions of HLS variables are
    kept the same as that of ground truth. PRIMAL-L learns a smooth latent structure
    and achieve the best reconstruction.
  Figure 3 Link: articels_figures_by_rev_year\2021\PRIMALGMM_PaRametrIc_MAnifold_Learning_of_Gaussian_Mixture_Models\figure_3.jpg
  Figure 3 caption: Experiment on synthetic data. PRIMAL-NL learns a smooth latent
    structure and achieves a good reconstruction.
  Figure 4 Link: articels_figures_by_rev_year\2021\PRIMALGMM_PaRametrIc_MAnifold_Learning_of_Gaussian_Mixture_Models\figure_4.jpg
  Figure 4 caption: 'HLS of PRIMAL-L under different hyperparameter settings ( c wyz
    , c v ) : (1,0.2), (0.1,0.1), (0.01,0.01), (0.001,0.001); from left to right,
    then top to bottom.'
  Figure 5 Link: articels_figures_by_rev_year\2021\PRIMALGMM_PaRametrIc_MAnifold_Learning_of_Gaussian_Mixture_Models\figure_5.jpg
  Figure 5 caption: Hiearchical latent space visualization. (1) Eye-fixation data;
    (2) Flow Cytometry data (FC); (3) Topic models. The 2D view of the 3D axis is
    selected to best show the class separation in the HLS.
  Figure 6 Link: articels_figures_by_rev_year\2021\PRIMALGMM_PaRametrIc_MAnifold_Learning_of_Gaussian_Mixture_Models\figure_6.jpg
  Figure 6 caption: Manifold visualization of eye-fixation data of PRIMAL.
  Figure 7 Link: articels_figures_by_rev_year\2021\PRIMALGMM_PaRametrIc_MAnifold_Learning_of_Gaussian_Mixture_Models\figure_7.jpg
  Figure 7 caption: Manifold visualization of flow cytometry data of PRIMAL.
  Figure 8 Link: articels_figures_by_rev_year\2021\PRIMALGMM_PaRametrIc_MAnifold_Learning_of_Gaussian_Mixture_Models\figure_8.jpg
  Figure 8 caption: Manifold visualization of topic model HLS learned by PRIMAL-L.
    Colors denote topics while markers denote different words. In (b), word samples
    of each topic keep the same and from GMM order 0 to 40 denotes from start to end
    of the visualization line.
  Figure 9 Link: articels_figures_by_rev_year\2021\PRIMALGMM_PaRametrIc_MAnifold_Learning_of_Gaussian_Mixture_Models\figure_9.jpg
  Figure 9 caption: LDA classification accuracy versus no. of nearest neighbors.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Ziquan Liu
  Name of the last author: Antoni B. Chan
  Number of Figures: 9
  Number of Tables: 5
  Number of authors: 4
  Paper title: 'PRIMAL-GMM: PaRametrIc MAnifold Learning of Gaussian Mixture Models'
  Publication Date: 2021-01-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 KL Reconstruction Loss for Held-Out Test GMMs and LDA Classification
      Accuracy in the Latent Space
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Eye Fixation Data: Correlation Between the HLS and Age Using
      Multivariate Linear Regression Analysis'
  Table 3 caption:
    table_text: TABLE 3 First Sentences of Documents Along the Directions in Linear
      HLS
  Table 4 caption:
    table_text: TABLE 4 Top-10 Words of 5 Topics From PRIMAL and RecoverKL on BBC
      News
  Table 5 caption:
    table_text: TABLE 5 Comparisons of PRIMAL With Topic Models on Pointwise Mutual
      Information (PMI) and LDA Accuracy
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3048727
- Affiliation of the first author: eth zurich, zurich, switzerland
  Affiliation of the last author: tsinghua university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Cascaded_Parsing_of_HumanObject_Interaction_Recognition\figure_1.jpg
  Figure 1 caption: "Our proposed CP-HOI is able to handle both object-level relation\
    \ detection and pixel-wise relation segmentation. Given an input image, CP-HOI\
    \ performs coarse-to-fine inference over instance detection ( D 1 \u223C D 3 )\
    \ and structured interaction recognition ( P 1 \u223C P 3 ). The interaction recognition\
    \ is achieved by a graph parsing neural network (GPNN), which alternatively infers\
    \ meaningful HOI structures and propagates information over the structures. The\
    \ final parse graph explains the given scene with the connectivity between nodes\
    \ (e.g., the strong edge between the person and laptop) and the edge labels (e.g.,\
    \ read)."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Cascaded_Parsing_of_HumanObject_Interaction_Recognition\figure_2.jpg
  Figure 2 caption: "(a) Illustration of our cascaded parsing network (CP-HOI), which\
    \ identifies a triplet of \u27E8person,read,laptop\u27E9 from an input image.\
    \ (b) Illustration of our instance detection module and the pipeline for constructing\
    \ a complete HOI graph from detected instances (Section 3.2). Here we showcase\
    \ how to extract initial embeddings for nodes: v (person) and w (laptop), and\
    \ an edge (v,w) ."
  Figure 3 Link: articels_figures_by_rev_year\2021\Cascaded_Parsing_of_HumanObject_Interaction_Recognition\figure_3.jpg
  Figure 3 caption: Illustration of our GPNN-based structured relation reasoning (Section
    3.3). GPNN takes node and edge features as inputs, and outputs a parse graph in
    a message passing fashion. The structure of the parse graph is given by a soft
    adjacency matrix, computed by the link function. The darker the color in the adjacency
    matrix, the stronger the connectivity is. Then message functions compute incoming
    messages for each nodeedge as a weighted sum of the messages from other edgesnodes.
    Thicker edges indicate larger information flows. The update functions update the
    internal states of each nodeedge. This process is repeated for several steps,
    iteratively and jointly learning the computation of HOI graph structures and message
    passing. Finally, for each node, the readout functions output HOI action labels
    from the hidden edge states.
  Figure 4 Link: articels_figures_by_rev_year\2021\Cascaded_Parsing_of_HumanObject_Interaction_Recognition\figure_4.jpg
  Figure 4 caption: Visual results for relation detection, on HOIW test set in PIC
    19 Challenge.
  Figure 5 Link: articels_figures_by_rev_year\2021\Cascaded_Parsing_of_HumanObject_Interaction_Recognition\figure_5.jpg
  Figure 5 caption: "Visual results for relation segmentation, on PIC test set in\
    \ PIC 19 Challenge. First column: Instance segmentation results. Last five columns:\
    \ Top ranked \u27E8human,verb,object\u27E9 triplets. For each triplet, the human\
    \ and object are shown in red and green."
  Figure 6 Link: articels_figures_by_rev_year\2021\Cascaded_Parsing_of_HumanObject_Interaction_Recognition\figure_6.jpg
  Figure 6 caption: Visual results for relation segmentation, on V-COCO test set [19].
  Figure 7 Link: articels_figures_by_rev_year\2021\Cascaded_Parsing_of_HumanObject_Interaction_Recognition\figure_7.jpg
  Figure 7 caption: Visual results for relation detection, on HICO-DET test set [20].
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.69
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.61
  Name of the first author: Tianfei Zhou
  Name of the last author: Song-Chun Zhu
  Number of Figures: 7
  Number of Tables: 6
  Number of authors: 5
  Paper title: Cascaded Parsing of Human-Object Interaction Recognition
  Publication Date: 2021-01-05 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Relation Detection Results on HOIW test and val Sets in PIC
      19 19 Challenge (Section 4.2)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Relation Segmentation Results on PIC test and val Sets in
      PIC 19 19 Challenge
  Table 3 caption:
    table_text: TABLE 3 Performance Comparison on V-COCO test [19] in Terms of mAP
      role mAProle (Section 4.3)
  Table 4 caption:
    table_text: TABLE 4 Performance Comparison on HICO-DET test [20] in Terms of mAP
      role mAProle
  Table 5 caption:
    table_text: TABLE 5 Ablation Study of Our CP-HOI Model in Terms of mAP role mAProle
  Table 6 caption:
    table_text: TABLE 6 Comparison Between Mask and Bbox Representations (Section
      4.5)
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3049156
- Affiliation of the first author: school of computer and information, hefei university
    of technology, hefei, anhui, china
  Affiliation of the last author: school of computer science and engineering, texas
    a&m university, tx, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\Differentiated_Explanation_of_Deep_Neural_Networks_With_Skewed_Distributions\figure_1.jpg
  Figure 1 caption: 'Comparison of different saliency maps: (a) The proposed DRE,
    (b) Vanilla Gradient [20], (c) Grad-CAM++ [2], (d) Mask Generator [4], and (e)
    Extreme Perturbation [7].'
  Figure 10 Link: articels_figures_by_rev_year\2021\Differentiated_Explanation_of_Deep_Neural_Networks_With_Skewed_Distributions\figure_10.jpg
  Figure 10 caption: The illustrative examples for Section 4.4, which are built upon
    ResNet50 with ImageNet. (a) The effects of post hoc tuning tricks on saliency
    maps. (b) The effects of different pre-configured distributions (the 1st column)
    on saliency maps (the 2nd-7th columns). Each row of masks corresponds to a pre-configured
    distribution. (c) The effects of the ad hoc constraints on DRE and MGnet [4].
  Figure 2 Link: articels_figures_by_rev_year\2021\Differentiated_Explanation_of_Deep_Neural_Networks_With_Skewed_Distributions\figure_2.jpg
  Figure 2 caption: The saliency maps obtained from the right-skewed distribution
    controller to the left-skewed distribution controller, illustrating the benefit
    of the right-skewed distribution for human-friendly explanations.
  Figure 3 Link: articels_figures_by_rev_year\2021\Differentiated_Explanation_of_Deep_Neural_Networks_With_Skewed_Distributions\figure_3.jpg
  Figure 3 caption: "The framework of our differentiated relevance estimator, where\
    \ a distribution controller C is introduced right after the mask generator G .\
    \ For each instance I , G takes the feature maps of the neural network E(I) as\
    \ inputs and feeds the obtained mask X into the controller C . The detailed process\
    \ flow inside the mask generator G can be found in Section 3.2.2. Then C guides\
    \ the relevance scores towards the right-skewed distribution for a differentiated\
    \ mask through X\u2192Y\u2192Z . The final mask M with the original size is obtained\
    \ via upsampling. In addition, we annotate the spatial sizes of feature maps and\
    \ display the expected distributions of the scores within the controller."
  Figure 4 Link: articels_figures_by_rev_year\2021\Differentiated_Explanation_of_Deep_Neural_Networks_With_Skewed_Distributions\figure_4.jpg
  Figure 4 caption: The examples of the masks obtained with non-monotonic mappings,
    where higher scores can not guarantee larger contributions.
  Figure 5 Link: articels_figures_by_rev_year\2021\Differentiated_Explanation_of_Deep_Neural_Networks_With_Skewed_Distributions\figure_5.jpg
  Figure 5 caption: The PDFs with the different settings of hyper-parameters.
  Figure 6 Link: articels_figures_by_rev_year\2021\Differentiated_Explanation_of_Deep_Neural_Networks_With_Skewed_Distributions\figure_6.jpg
  Figure 6 caption: The effects of the controller on the synthetic data with different
    distributions. The 1st and the 2nd rows show original distributions and their
    transformed distributions, respectively. For a quantitative comparison, we also
    list their Pearsons Coefficients of Skewness (CoS) [5].
  Figure 7 Link: articels_figures_by_rev_year\2021\Differentiated_Explanation_of_Deep_Neural_Networks_With_Skewed_Distributions\figure_7.jpg
  Figure 7 caption: The saliency maps of different explanation methods for the CNNs
    trained on ImageNet, in which these sampled images obtain correct classifications.
    We also display the heatmap of their normalized M F values.
  Figure 8 Link: articels_figures_by_rev_year\2021\Differentiated_Explanation_of_Deep_Neural_Networks_With_Skewed_Distributions\figure_8.jpg
  Figure 8 caption: The saliency maps of different explanation methods for the CNNs
    trained on Birds-200-2011, in which these sampled images obtain correct classifications.
    We also display the heatmap of their normalized M F values.
  Figure 9 Link: articels_figures_by_rev_year\2021\Differentiated_Explanation_of_Deep_Neural_Networks_With_Skewed_Distributions\figure_9.jpg
  Figure 9 caption: The saliency maps of different explanation methods for the ResNet50
    trained on Places365, in which these sampled images obtain correct classifications.
    We also display the heatmap of their normalized M F values.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Weijie Fu
  Name of the last author: Xia Hu
  Number of Figures: 11
  Number of Tables: 5
  Number of authors: 6
  Paper title: Differentiated Explanation of Deep Neural Networks With Skewed Distributions
  Publication Date: 2021-01-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Notations and Definitions
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Characteristics of Compared Methods
  Table 3 caption:
    table_text: TABLE 3 Ranking-Based Quantitative Evaluation on Faithfulness M F
      MF
  Table 4 caption:
    table_text: TABLE 4 Ranking-Based Quantitative Evaluation on Explainability M
      E ME
  Table 5 caption:
    table_text: TABLE 5 Ranking-Based Quantitative Evaluation on Faithfulness M F
      MF
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3049784
- Affiliation of the first author: department of computer science, stony brook university,
    stony brook, ny, usa
  Affiliation of the last author: department of computer science, stony brook university,
    stony brook, ny, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\EndtoEnd_Full_Projector_Compensation\figure_1.jpg
  Figure 1 caption: 'Full projector geometric correction and photometric compensation:
    (a) system setup with a nonplanar and textured surface (b), (c) projection result
    without compensation, (d) fully compensated projector input image by our method,
    (e) camera-captured compensated projection result [i.e., (d) projected onto (b)],
    and (f) desired visual effect. Comparing (c) and (e) we see clearly improved geometry,
    color and texture details.'
  Figure 10 Link: articels_figures_by_rev_year\2021\EndtoEnd_Full_Projector_Compensation\figure_10.jpg
  Figure 10 caption: Output of CompenNeSt when sequentially enabling the three surface
    skip connections. We start with a trained CompenNeSt and an uncompensated camera-captured
    sampling image and disable all the skip connections between the surface branch
    and the backbone network (i.e., s1-s3 in Fig. 9). Then, we sequentially enabled
    s1 to s3 as shown in columns 4-7. Note that after we disabled a surface skip connection,
    we subtract its feature mean, e.g., Compensated w m means that we disabled s1-s3
    but subtracted their corresponding surface feature means from the backbone network.
    Compared with subtracting the actual feature map, subtracting feature mean only
    performs a global color and brightness adjustment (see the difference between
    the 3rd and the 4th columns). Then, when we enabled a surface skip connection,
    the feature variancetexture information can be better visualized. E.g., comparing
    the 4th with the 5th6th columns, we see that s1 and s2 carry low-level surface
    texture features, subtracting them significantly removes the surface pattern.
    Comparing the sixth and the seventh columns, we see that s3 carries global color
    information.
  Figure 2 Link: articels_figures_by_rev_year\2021\EndtoEnd_Full_Projector_Compensation\figure_2.jpg
  Figure 2 caption: "Architecture of the proposed CompenNeSt++ and its training in\
    \ two major steps. (a) Project and capture a surface image and a set of sampling\
    \ images. (b) CompenNeSt++, i.e., \u03C0 \u2020 \u03B8 , is trained using the\
    \ data prepared in (a). The projector input images are outlined in green, the\
    \ camera-captured images are outlined in purple, the intermediate results (e.g.,\
    \ warped images) are outlined in blue and the network output images are outlined\
    \ in red. The dashed lines indicate network input training data. WarpingNet (\
    \ T \u22121 , yellow block) warps the camera-captured images s ~ and x ~ to the\
    \ projector canonical frontal view using a cascaded coarse-to-fine structure,\
    \ where the pink modules are learnable parameters. Operator \u2297 denotes a bilinear\
    \ interpolator, i.e., \u03D5(\u22C5;\u22C5) . The grid refinement network W \u03B8\
    \ r consists of a UNet-like [48] structure, it generates a refined sampling grid\
    \ that is used to sample (warp) the input images. CompenNeSt ( F \u2020 , light\
    \ blue block) consists of a siamese encoder (orange modules share weights) and\
    \ a decoder (blue modules). Best viewed in color."
  Figure 3 Link: articels_figures_by_rev_year\2021\EndtoEnd_Full_Projector_Compensation\figure_3.jpg
  Figure 3 caption: "We train CompenNeSt++ using camera-captured and projector input\
    \ image pairs like ( x ~ ,x) instead of desired viewer-perceived and compensation\
    \ image pairs like ( x \u2032 , x \u2217 ) , obviating the need for the ground\
    \ truth compensation image x \u2217 , because learning the backward mapping from\
    \ the camera-captured uncompensated image to the projector input image (left:\
    \ x ~ \u21A6x ) [equation (7)] is the same as learning the backward mapping from\
    \ the desired viewer-perceived image to the projector compensation image (right:\
    \ x \u2032 \u21A6 x \u2217 ) [equation (6)]."
  Figure 4 Link: articels_figures_by_rev_year\2021\EndtoEnd_Full_Projector_Compensation\figure_4.jpg
  Figure 4 caption: "CompenNeSt++ in testinginference phase. Due to the novel structure\
    \ and sampling strategy, WarpingNet T \u22121 can be simplified to a single sampling\
    \ grid \u03A9 r and an image interpolator \u2297 . Moreover, CompenNeSt surface\
    \ features can be integrated into the backbone as biases. Both techniques improve\
    \ computational and memory efficiency during testinginference with no performance\
    \ drop. Finally, the model inferred compensation image z \u2217 is both geometrically\
    \ and photometrically compensated, such that after projection it cancels the geometric\
    \ and photometric distortions and produces an image that is close to z \u2032\
    \ , i.e., Fig. 1e. More camera-captured compensation results are shown in Fig.\
    \ 7 and supplementary material, available online."
  Figure 5 Link: articels_figures_by_rev_year\2021\EndtoEnd_Full_Projector_Compensation\figure_5.jpg
  Figure 5 caption: Projector FOV mask, bounding rectangle (green) and optimal displayable
    area (red). The optimal displayable area is defined as the maximum inscribed rectangle
    (keep aspect ratio) [44]. The affine matrix A is estimated given the displayable
    area and the projector input image size.
  Figure 6 Link: articels_figures_by_rev_year\2021\EndtoEnd_Full_Projector_Compensation\figure_6.jpg
  Figure 6 caption: Blender rendered images (purple boxes). Two different surfaces
    (blue boxes) and three different projector input images (green boxes) are shown.
  Figure 7 Link: articels_figures_by_rev_year\2021\EndtoEnd_Full_Projector_Compensation\figure_7.jpg
  Figure 7 caption: Qualitative comparison of TPS [15] w SL, TPS textured w SL, Pix2pix
    [27] w SL, our CompenNeSt w SL, our pre-trained CompenNeSt++ fine-tuned using
    only eight sampling images, i.e., CompenNeSt++ pre and our CompenNeSt++ on two
    different surfaces. All models were trained using 500 sampling images (except
    for CompenNeSt++ pre). The first to third columns are camera-captured projection
    surface, desired viewer-perceived image and camera-captured uncompensated projection,
    respectively. The rest columns are compensation results of different methods.
    Each image is provided with two zoomed-in patches for detailed comparison. See
    supplementary material, available online, for more results.
  Figure 8 Link: articels_figures_by_rev_year\2021\EndtoEnd_Full_Projector_Compensation\figure_8.jpg
  Figure 8 caption: "Qualitative comparison of CompenNeSt++ trained with \u2113 1\
    \ loss, \u2113 2 loss, SSIM loss and \u2113 1 + SSIM loss. Clearly, \u2113 1 and\
    \ \u2113 2 losses are unable to successfully compensate the surface patterns (see\
    \ the dog head). \u2113 1 + SSIM and the SSIM losses produce similar results,\
    \ but the cloud in the red zoomed-in patch of SSIM is grayer than \u2113 1 + SSIM\
    \ and the ground truth."
  Figure 9 Link: articels_figures_by_rev_year\2021\EndtoEnd_Full_Projector_Compensation\figure_9.jpg
  Figure 9 caption: 'Visualization of CompenNeSt photometric compensation mechanism.
    Left: a trained CompenNeSt takes two warped images as input and we investigate
    the feature maps by enabling the input and the corresponding skip connections
    once at a time. Right: the top and the bottom rows show the network outputs when
    input the surface image or the sampling image, respectively; and each column shows
    the output when a specific skip connection and the corresponding layers are enabled.
    We use gray color to indicate disabled inputs, modules and connections. As shown
    in the first two columns, the feature maps of the first two layers carry low-level
    texture information and greenred components. In the third column, we see that
    the feature maps of the fourth layer carry high-level global information and blue
    and yellow components.'
  First author gender probability: 0.61
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.95
  Name of the first author: Bingyao Huang
  Name of the last author: Haibin Ling
  Number of Figures: 11
  Number of Tables: 5
  Number of authors: 3
  Paper title: End-to-End Full Projector Compensation
  Publication Date: 2021-01-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Comparison of Full Compensation Algorithms
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Comparison Between CompenNeSt wSL and CompenNeSt++
  Table 3 caption:
    table_text: "TABLE 3 Quantitative Comparison of the Proposed CompenNeSt With CompenNet\
      \ [20] and Three Degraded Versions That are (1) Without the Surface Image, (2)\
      \ With CompenNet-Like 2\xD72 Transposed Convolutional Filters, and (3) Additionally\
      \ With CompenNet-Like Degraded Skip Convolutional Layers"
  Table 4 caption:
    table_text: TABLE 4 Quantitative Comparison of Different Loss Functions for the
      Proposed CompenNeSt++ on the Full Compensation Dataset Using 500 Images and
      1,500 Iterations and the Results are Averaged Over K=20 K=20 Setups
  Table 5 caption:
    table_text: TABLE 5 Quantitative Comparison Between the Pre-Trained CompenNeSt++
      and the Default CompenNeSt++ on the Full Compensation Benchmark.
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3050124
- Affiliation of the first author: department of automation, beijing national research
    center for information science and technology (bnrist), tsinghua university, beijing,
    china
  Affiliation of the last author: department of automation, beijing national research
    center for information science and technology (bnrist), tsinghua university, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2021\Learning_Efficient_Binarized_Object_Detectors_With_Information_Compression\figure_1.jpg
  Figure 1 caption: An example of the predicted objects with the binarized SSD detector
    with VGG16 backbone on PASCAL VOC. (a) and (b) demonstrate the detection results
    via Xnor-Net and the proposed BiDet respectively, where the false positives are
    significantly reduced in our method. (c) and (d) reveal the information plane
    dynamics for the training and test sets respectively. The horizontal axis means
    the mutual information between the high-level feature map and input, and the vertical
    axis represents the mutual information between object detection and the feature
    map. Compared with Xnor-Net, our method removes the redundant information and
    fully utilizes the network capacity to achieve higher performance.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Learning_Efficient_Binarized_Object_Detectors_With_Information_Compression\figure_2.jpg
  Figure 2 caption: "The pipeline of the information bottleneck based detectors, which\
    \ consists of the backbone part and the detection part. The solid line represents\
    \ the forward propagation in the network, while the dashed line means sampling\
    \ from a parameterized distribution \u03A6 . The high-level feature map F is sampled\
    \ from the distribution parameterized by the backbone network. The one-stage and\
    \ two-stage detector framework can be both employed in the detection part of our\
    \ BiDet. For one-stage detectors, the head network parameterizes the posterior\
    \ distribution of object classes and location. For two-stage detectors, Region\
    \ Proposal Networks (RPN) parameterize the prior distribution of location and\
    \ the posteriors are parameterized by the refining networks."
  Figure 3 Link: articels_figures_by_rev_year\2021\Learning_Efficient_Binarized_Object_Detectors_With_Information_Compression\figure_3.jpg
  Figure 3 caption: The detected objects and the foreground confidence score (a) before
    and (b) after optimizing (6). The contrast of foreground confidence score among
    different detected objects is significantly enlarged by minimizing alternate objective.
    As the NMS eliminates the positives with foreground confidence score lower than
    the threshold, the sparse object priors are acquired and the posteriors are enforced
    to be concentrated on informative prediction.
  Figure 4 Link: articels_figures_by_rev_year\2021\Learning_Efficient_Binarized_Object_Detectors_With_Information_Compression\figure_4.jpg
  Figure 4 caption: "The motivation of the proposed AutoBiDet. (a) and (b) illustrate\
    \ the images in low and high complexity respectively. (c) and (d) depict the information\
    \ plane for the representations of (a) and (b). The information plane is divided\
    \ into the Impossible region and Possible region by the Pareto frontier, where\
    \ the learned representations can only be in the Possible region. The mutual information\
    \ between the high-level feature maps and the input is limited by the network\
    \ capacity so that the high-level feature maps are restricted in the region of\
    \ Possible (I). When the constant network capacity is fully utilized, the high-level\
    \ feature maps of images in high complexity carry less information regarding to\
    \ the input due to the more significant information loss and vice versa. O A and\
    \ O B mean the optimal representations for image (a) and (b) in the information\
    \ plane, and the optimal hyperparameter \u03B2 controlling the IB trade-off is\
    \ denoted as \u03B2 A and \u03B2 B respectively. As we apply the fixed IB trade-off\
    \ in BiDet which is represented by O with the hyperparameter \u03B2 O , the resulted\
    \ representations for image (a) and (b) denoted as A and B are far from O A and\
    \ O B . For images in low complexity, the fixed IB trade-off causes excessive\
    \ compression and network capacity is not fully utilized. For images in high complexity,\
    \ the insufficient redundancy removal obstacles the acquisition of representations\
    \ in the Pareto frontier. When the network capacity is occupied by redundancy,\
    \ the achievable frontier for representations is degraded significantly compared\
    \ with the Pareto frontier."
  Figure 5 Link: articels_figures_by_rev_year\2021\Learning_Efficient_Binarized_Object_Detectors_With_Information_Compression\figure_5.jpg
  Figure 5 caption: The pipeline of AutoBiDet. The black solid line, the red solid
    line and the black dashed line represent the forward propagation, the backward
    propagation and the IB trade-off transition in AutoBiDet respectively. The backbone
    part learns the high-level feature maps to represent the input image, and the
    detection part predicts the class and location of the objects in the input image
    according to the high-level feature map. The complexity estimator reconstructs
    the input image via the generator, and the discriminator outputs the probability
    that the recovered image comes from the true sample set. The probability from
    the discriminator is used to acquire the optimal IB trade-off via the transformation
    functions, and the dynamic IB trade-off is employed to train the backbone and
    detection networks. As a result, the network capacity is fully utilized and the
    redundancy is effectively removed for all input.
  Figure 6 Link: articels_figures_by_rev_year\2021\Learning_Efficient_Binarized_Object_Detectors_With_Information_Compression\figure_6.jpg
  Figure 6 caption: (a) The detection prediction of a sampled image, where boxes in
    different colors illustrate the object predictions for various classes. False
    positives with low foreground confidence score are erased due to severe occlusion.
    (b), (c), (d) and (e) demonstrate the foreground confidence score of all predicted
    positives for the classes of bottles, chairs, TV monitors and persons respectively,
    where blue and orange bars mean the true and false positives respectively. It
    is obvious that the predicted positives for the TV monitor are the fewest among
    the four classes, which leads to the minimum false positives among all classes.
    On the contrary, the class of persons obtains the most predicted positives and
    the false positives of persons are also the maximum.
  Figure 7 Link: articels_figures_by_rev_year\2021\Learning_Efficient_Binarized_Object_Detectors_With_Information_Compression\figure_7.jpg
  Figure 7 caption: Ablation study w.r.t. hyperparameters beta and gamma , where the
    variety of (a) mAP, (b) the mutual information between high-level feature maps
    and the object detection I(F;L,C) , (c) the number of false positives and (d)
    the number of false negatives are demonstrated.
  Figure 8 Link: articels_figures_by_rev_year\2021\Learning_Efficient_Binarized_Object_Detectors_With_Information_Compression\figure_8.jpg
  Figure 8 caption: Qualitative results on PASCAL VOC. Images from the top to the
    bottom row show the groundtruth objects, the objects predicted by Xnor-Net, BiDet
    and AutoBiDet respectively. The proposed BiDet removes the false positives significantly
    compared with Xnor-Net. Moreover, our AutoBiDet eliminates the false positives
    more thoroughly for all classes and enhances the recall especially for small objects.
    The arrows in the figures represent objects missed by BiDet while detected by
    AutoBiDet.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.59
  Name of the first author: Ziwei Wang
  Name of the last author: Jie Zhou
  Number of Figures: 8
  Number of Tables: 4
  Number of authors: 4
  Paper title: Learning Efficient Binarized Object Detectors With Information Compression
  Publication Date: 2021-01-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 mAP (%) on PASCAL VOC w.r.t. Different Transformation Functions
      in Automatic Information Compression (AIC) and Different Object Priors
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of the Parameter Size, FLOPs and mAP (%) with the
      State-of-the-Art Binary Neural Networks in Both One-Stage and Two-Stage Detection
      Frameworks on PASCAL VOC
  Table 3 caption:
    table_text: TABLE 3 Comparison With the State-of-the-Art Binarized Object Detectors
      on COCO, Where mAP [.5,.95] .5,.95 (%), AP With Different IOU Threshold and
      AP for Objects in Various Sizes are dezmonstrated
  Table 4 caption:
    table_text: TABLE 4 Extension of Techniques in BiDet and AutoBiDet to Different
      Model Compression Methods
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3050464
- Affiliation of the first author: "eth zurich, z\xFCrich, switzerland"
  Affiliation of the last author: university of kentucky, lexington, ky, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\Salient_Object_Detection_in_the_Deep_Learning_Era_An_InDepth_Survey\figure_1.jpg
  Figure 1 caption: A brief chronology of SOD. The very first SOD models date back
    to the work of Liu et al. [30] and Achanta et al. [31]. The first incorporation
    of deep learning techniques into SOD models was in 2015. Listed methods are milestones,
    which are typically highly cited. See Section 1.1 for more details.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Salient_Object_Detection_in_the_Deep_Learning_Era_An_InDepth_Survey\figure_2.jpg
  Figure 2 caption: Categorization of previous deep SOD models according to the adopted
    network architecture. (a) MLP-based methods. (b)-(f) FCN-based methods, mainly
    using (b) single-stream network, (c) multi-stream network, (d) side-out fusion
    network, (e) bottom-uptop-down network, and (f) branch network architectures.
    (g) Hybrid network-based methods. (h) Capsule-based methods. See Section 2.1 for
    more detailed descriptions.
  Figure 3 Link: articels_figures_by_rev_year\2021\Salient_Object_Detection_in_the_Deep_Learning_Era_An_InDepth_Survey\figure_3.jpg
  Figure 3 caption: Annotation distributions of SOD datasets (see Section 3 for details).
  Figure 4 Link: articels_figures_by_rev_year\2021\Salient_Object_Detection_in_the_Deep_Learning_Era_An_InDepth_Survey\figure_4.jpg
  Figure 4 caption: Sample images from the hybrid benchmark consisting of images randomly
    selected from 6 SOD datasets. Salient regions are uniformly highlighted. Corresponding
    attributes are listed. See Section 5.3 for more detailed descriptions.
  Figure 5 Link: articels_figures_by_rev_year\2021\Salient_Object_Detection_in_the_Deep_Learning_Era_An_InDepth_Survey\figure_5.jpg
  Figure 5 caption: Examples of saliency prediction under various input perturbations.
    The max F values are denoted in red color. See Section 5.4 for more details.
  Figure 6 Link: articels_figures_by_rev_year\2021\Salient_Object_Detection_in_the_Deep_Learning_Era_An_InDepth_Survey\figure_6.jpg
  Figure 6 caption: Examples of SOD prediction under adversarial perturbations of
    different target networks. The perturbations are magnified by 10 for better visualization.
    Red for max F. See Section 5.5 for details.
  Figure 7 Link: articels_figures_by_rev_year\2021\Salient_Object_Detection_in_the_Deep_Learning_Era_An_InDepth_Survey\figure_7.jpg
  Figure 7 caption: Network architecture of the SOD model used in cross-dataset generalization
    evaluation. See Section 5.6 for more detailed descriptions.
  Figure 8 Link: articels_figures_by_rev_year\2021\Salient_Object_Detection_in_the_Deep_Learning_Era_An_InDepth_Survey\figure_8.jpg
  Figure 8 caption: Examples for annotation inconsistency. Each row shows two exemplar
    image pairs. See Section 6.2 for more detailed descriptions.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Wenguan Wang
  Name of the last author: Ruigang Yang
  Number of Figures: 8
  Number of Tables: 11
  Number of authors: 6
  Paper title: 'Salient Object Detection in the Deep Learning Era: An In-Depth Survey'
  Publication Date: 2021-01-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of Previous Reviews
  Table 10 caption:
    table_text: TABLE 10 Results for Adversarial Attack Experiments
  Table 2 caption:
    table_text: TABLE 2 Taxonomies and Representative Publications of Deep SOD Methods
  Table 3 caption:
    table_text: TABLE 3 Summary of Essential Characteristics for Popular SOD Methods
  Table 4 caption:
    table_text: TABLE 4 Statistics of Popular SOD Datasets, Including the Number of
      Images, Number of Salient Objects Per Image, Area Ratio of the Salient Objects
      in Images, Annotation Type, Image Resolution, and Existence of Fixation Data
  Table 5 caption:
    table_text: TABLE 5 Benchmarking Results of 44 State-of-the-Art Deep SOD Models
      and Three Top-Performing Classic SOD Methods on Six Famous Datasets (Section
      5.2)
  Table 6 caption:
    table_text: TABLE 6 Descriptions of Attributes That Often Bring Difficulties to
      SOD (see Section 5.3)
  Table 7 caption:
    table_text: TABLE 7 Attribute-Based Study w.r.t. Salient Object Categories, Challenges,
      and Scene Categories
  Table 8 caption:
    table_text: TABLE 8 Attribute Statistics of Top and Bottom 100 Images Based on
      F-Measure
  Table 9 caption:
    table_text: TABLE 9 Input Perturbation Study on the hybrid benchmark
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3051099
- Affiliation of the first author: smartmore corporation, ltd., shenzhen, guangdong,
    china
  Affiliation of the last author: smartmore corporation, ltd., shenzhen, guangdong,
    china
  Figure 1 Link: articels_figures_by_rev_year\2021\HEMlets_PoSh_Learning_PartCentric_Heatmap_Triplets_for_D_Human_Pose_and_Shape_Es\figure_1.jpg
  Figure 1 caption: "Overview of the HEMlets-based 3D pose estimation. (a) Input RGB\
    \ image. Our algorithm encodes (b) the 2D locations for the joints p and c , but\
    \ also (c) their relative depth relationship for each skeletal part pc \u2192\
    \ into HEMlets. (d) Output 3D human pose."
  Figure 10 Link: articels_figures_by_rev_year\2021\HEMlets_PoSh_Learning_PartCentric_Heatmap_Triplets_for_D_Human_Pose_and_Shape_Es\figure_10.jpg
  Figure 10 caption: The qualitative results for some examples of MPI-INF-3DHP [25],
    using different additional datasets. For each example, we present the input RGB
    image, the 3D human pose predicted by three different models. The groundtruth
    pose is shown in dashed line.
  Figure 2 Link: articels_figures_by_rev_year\2021\HEMlets_PoSh_Learning_PartCentric_Heatmap_Triplets_for_D_Human_Pose_and_Shape_Es\figure_2.jpg
  Figure 2 caption: "Part-centric heatmap triplets T \u22121 k , T 0 k , T +1 k where\
    \ p and c are the parent joint and the child joint. (a, b) Joints and skeletal\
    \ parts. We locate the parent joint p of the k th skeletal part B k at the zero\
    \ polarity heatmap T 0 k (c-e). The child joint c is located, according to relative\
    \ depth of p and c , in the positive (c), zero (d), and negative polarity heatmap\
    \ (e), respectively."
  Figure 3 Link: articels_figures_by_rev_year\2021\HEMlets_PoSh_Learning_PartCentric_Heatmap_Triplets_for_D_Human_Pose_and_Shape_Es\figure_3.jpg
  Figure 3 caption: "The network architecture of our proposed approach. It consists\
    \ of four major modules: (a) A ResNet-50 backbone for image feature extraction.\
    \ (b) A ConvNet for image feature upsampling. (c) Another ConvNet for HEMlets\
    \ learning and 2D joint detection. (d) A 3D pose regression module adopting a\
    \ soft-argmax operation for 3D human pose estimation. (e) Details of the HEMlets\
    \ learning module. \u201CFeature concatenate\u201D denotes concatenating the feature\
    \ maps from the HEMlets learning branch and the upsampling branch together."
  Figure 4 Link: articels_figures_by_rev_year\2021\HEMlets_PoSh_Learning_PartCentric_Heatmap_Triplets_for_D_Human_Pose_and_Shape_Es\figure_4.jpg
  Figure 4 caption: A unified body joint definition adopted in our method by merging
    the joints defined by the Human3.6M and MPII datasets.
  Figure 5 Link: articels_figures_by_rev_year\2021\HEMlets_PoSh_Learning_PartCentric_Heatmap_Triplets_for_D_Human_Pose_and_Shape_Es\figure_5.jpg
  Figure 5 caption: "HEMlets-based parametric 3D human body regression from a single\
    \ color image. We append a shallow yet effective SMPL body mesh regression network\
    \ to the preceding HEMlets pose estimation network, which is trained end-to-end\
    \ to regress the SMPL shape and pose parameters \u03B2,\u03B8 ."
  Figure 6 Link: articels_figures_by_rev_year\2021\HEMlets_PoSh_Learning_PartCentric_Heatmap_Triplets_for_D_Human_Pose_and_Shape_Es\figure_6.jpg
  Figure 6 caption: "User annotation interface for obtaining the weakly-annotated\
    \ FBI dataset. An annotator is asked to assign a label of either \u201CBackward\u201D\
    , \u201CForward\u201D or \u201CUnknown\u201D to a given skeletal part."
  Figure 7 Link: articels_figures_by_rev_year\2021\HEMlets_PoSh_Learning_PartCentric_Heatmap_Triplets_for_D_Human_Pose_and_Shape_Es\figure_7.jpg
  Figure 7 caption: A simple illustration of the difference between the FBI and Ordinal
    annotation schemes. (a) Global relative depth ordering between disconnected joints
    (e.g., (q0,q1) in the top-left image, (q0,q1) and (q0,q2) in the bottom-left image)
    need to be annotated in the Ordinal scheme, which are however challenging to annotate
    correctly. (b) In contrast, only local relative depth ordering between connected
    joints (e.g., (pi,ci) and (pj,cj) in the right-side images) need to be annotated
    in the FBI scheme.
  Figure 8 Link: articels_figures_by_rev_year\2021\HEMlets_PoSh_Learning_PartCentric_Heatmap_Triplets_for_D_Human_Pose_and_Shape_Es\figure_8.jpg
  Figure 8 caption: 'An example image with the detected joints overlaid and shown
    from a novel view, using different methods: (a) mathcal Lmathrm 3Dlambda + mathcal
    Lmathrm 2D (2D error: 15.2; 3D joint error: 81.3mm). (b) mathcal Lmathrm 3Dlambda
    + mathcal Lmathrm 2D + mathcal Lmathrm HEM (2D error: 13.0; 3D error: 41.2mm).
    (c) Ground-truth. HEMlets learning helps fixing local part errors, see the blue
    skeletal part in (a) versus the red skeletal part in (b).'
  Figure 9 Link: articels_figures_by_rev_year\2021\HEMlets_PoSh_Learning_PartCentric_Heatmap_Triplets_for_D_Human_Pose_and_Shape_Es\figure_9.jpg
  Figure 9 caption: The MPJPE of the validation set of 5s-HEM, 2s-HEM and HEMlets,
    respectively. All are trained with the Human3.6M dataset.
  First author gender probability: 0.79
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Kun Zhou
  Name of the last author: Jiangbo Lu
  Number of Figures: 14
  Number of Tables: 9
  Number of authors: 5
  Paper title: 'HEMlets PoSh: Learning Part-Centric Heatmap Triplets for 3D Human
    Pose and Shape Estimation'
  Publication Date: 2021-01-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Comparisons of the Mean Per-Joint Position Error
      (MPJPE) on Human3.6M [14] Under Protocol 1 and Protocol 2, As Well As Using
      PA MPJPE As the Evaluation Metric
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Detailed Results on the Validation Set of HumanEva-I [25]
  Table 3 caption:
    table_text: TABLE 3 Detailed Results on the Test Set of MPI-INF-3DHP [25]
  Table 4 caption:
    table_text: TABLE 4 Ablative Study on the Effects of Alternative Intermediate
      Supervision Evaluated on Human3.6M Using Protocol 1
  Table 5 caption:
    table_text: TABLE 5 Evaluation of 3DPCK Scores by Adding Different Augmenting
      Datasets That Provide Relative Depth Ordering Annotations
  Table 6 caption:
    table_text: TABLE 6 Quantitative Comparisons of Fully Body Model Recovery Results
      Over Different Datasets
  Table 7 caption:
    table_text: TABLE 7 Quantitative Comparisons Between Our Method and Existing Ones
      on Foreground and Part Segmentation of the Recovered Full Body Mesh on the UP-3D
      Dataset
  Table 8 caption:
    table_text: TABLE 8 Evaluating the Impact of Each Branch in Fig. 5 on Human Body
      Estimation
  Table 9 caption:
    table_text: "TABLE 9 Evaluation of the Impact of Learning \u03B8 \u03B8 and \u03B2\
      \ \u03B2 on Human Body Estimation"
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3051173
- Affiliation of the first author: school of computer science and engineering, nanjing
    university of science and technology, nanjing, jiangsu, china
  Affiliation of the last author: school of computer science and engineering, nanjing
    university of science and technology, nanjing, jiangsu, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Spatiotemporal_CoAttention_Recurrent_Neural_Networks_for_HumanSkeleton_Motion_Pr\figure_1.jpg
  Figure 1 caption: The idea of Skeleton-joint Co-Attention (SCA). Traditional attention
    methods only learn the skeleton-attention feature vectors in temporal space, where
    all elements in a vector are assigned with the same attention factor. Our SCA
    dynamically learns a skeleton-joint co-attention feature map of human joints and
    skeletons in spatiotemporal space, where all elements in such feature map are
    assigned with different attention factors. SCA will be embedded into a variant
    of RNN, dubbed Spatiotemporal Co-Attention RNN (SC-RNN) in this paper.
  Figure 10 Link: articels_figures_by_rev_year\2021\Spatiotemporal_CoAttention_Recurrent_Neural_Networks_for_HumanSkeleton_Motion_Pr\figure_10.jpg
  Figure 10 caption: MAEs of different methods with varying inference time for predicting
    interaction motions.
  Figure 2 Link: articels_figures_by_rev_year\2021\Spatiotemporal_CoAttention_Recurrent_Neural_Networks_for_HumanSkeleton_Motion_Pr\figure_2.jpg
  Figure 2 caption: The framework of the proposed SC-RNN in modeling human motions.
    At each time step, a co-attention feature map of all the observed motions is learned
    by Skeleton-joint Co-Attention (SCA) in spatiotemporal space. And then, SC-RNN
    models human-skeleton motions and human-joint motions in spatiotemporal space
    by regarding the skeleton-joint co-attention feature map as a new motion context.
    A new weighted gram-matrix loss is presented to learn SC-RNN in the training phase.
  Figure 3 Link: articels_figures_by_rev_year\2021\Spatiotemporal_CoAttention_Recurrent_Neural_Networks_for_HumanSkeleton_Motion_Pr\figure_3.jpg
  Figure 3 caption: "The architecture of SC-GRU (a GRU version of the proposed SC-RNN)\
    \ at time step t \u2032 . SC-RNN mainly consists of the skeleton-attention network\
    \ and joint-attention network, which simultaneously model the skeleton motion\
    \ and joint motion."
  Figure 4 Link: articels_figures_by_rev_year\2021\Spatiotemporal_CoAttention_Recurrent_Neural_Networks_for_HumanSkeleton_Motion_Pr\figure_4.jpg
  Figure 4 caption: Toy examples for two types of joint sequences. (a) and (b) represent
    the traveling-based and surrounding-based sequences, respectively.
  Figure 5 Link: articels_figures_by_rev_year\2021\Spatiotemporal_CoAttention_Recurrent_Neural_Networks_for_HumanSkeleton_Motion_Pr\figure_5.jpg
  Figure 5 caption: Some visualized results of the motion prediction for all 15 actions
    on H3.6M. In each group, the first and the second rows are the observed motions
    and the ground-truth human motions, respectively; the 3rd-7th rows are the human
    motions predicted by ERD [20], LSTM-3LR [20], Res-GRU [1], MHU [22], and the proposed
    SC-RNN, respectively. For better view, please see times 3 original color PDF.
    This figure is followed by Fig. 6.
  Figure 6 Link: articels_figures_by_rev_year\2021\Spatiotemporal_CoAttention_Recurrent_Neural_Networks_for_HumanSkeleton_Motion_Pr\figure_6.jpg
  Figure 6 caption: Some visualized results of the motion prediction for all 15 actions
    on H3.6M. In each group, the first and the second rows are the observed motions
    and the ground-truth human motions, respectively; the 3rd-7th rows are the human
    motions predicted by ERD [20], LSTM-3LR [20], Res-GRU [1], MHU [22], and the proposed
    SC-RNN, respectively. For better view, please see times 3 original color PDF.
    This figure follows Fig. 5.
  Figure 7 Link: articels_figures_by_rev_year\2021\Spatiotemporal_CoAttention_Recurrent_Neural_Networks_for_HumanSkeleton_Motion_Pr\figure_7.jpg
  Figure 7 caption: Some visualized results of the motion prediction on the CMU Mocap
    dataset. In each group, the first and the second rows are the observed motions
    and the ground-truth human motions, respectively; the 3rd-8th rows are the human
    motions predicted by ERD [20], LSTM-3LR [20], Res-GRU [1], MHU [21], CSSM [30]
    and the proposed SC-RNN, respectively. For better view, please see times 3 original
    color PDF.
  Figure 8 Link: articels_figures_by_rev_year\2021\Spatiotemporal_CoAttention_Recurrent_Neural_Networks_for_HumanSkeleton_Motion_Pr\figure_8.jpg
  Figure 8 caption: "Some fail results of the motion prediction obtained by the proposed\
    \ SC-RNN on the H3.6M and CMU Mocap datasets. The \u201Csitting\u201D and \u201C\
    pose\u201D actions are from H3.6M, while the \u201Cwash windows\u201D and \u201C\
    soccer\u201D actions are from CMU Mocap. In each group, the first, second, and\
    \ third rows are the observed motions, the ground-truth motions, and the predicted\
    \ motions by SC-RNN, respectively. For better view, please see times 3 original\
    \ color PDF."
  Figure 9 Link: articels_figures_by_rev_year\2021\Spatiotemporal_CoAttention_Recurrent_Neural_Networks_for_HumanSkeleton_Motion_Pr\figure_9.jpg
  Figure 9 caption: Some visualized results of the interaction motion prediction.
    In each group, the 1st-2nd rows are the observed motions and the ground-truth
    human motions, respectively; the third-fourth rows are the human motions predicted
    by MHU [22] and SC-RNN, respectively.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.93
  Name of the first author: Xiangbo Shu
  Name of the last author: Jinhui Tang
  Number of Figures: 11
  Number of Tables: 6
  Number of authors: 5
  Paper title: Spatiotemporal Co-Attention Recurrent Neural Networks for Human-Skeleton
    Motion Prediction
  Publication Date: 2021-01-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Notations and Definitions
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Mean Angle Error Obtained by SC-RNN With Two Types of Joint
      Sequences
  Table 3 caption:
    table_text: TABLE 3 Performance (MAE) Comparison Among Different Methods in Terms
      of Short-Term and Long-Term Human Motion Prediction for All 15 Actions on H3.6M
  Table 4 caption:
    table_text: TABLE 4 Performance (MAE) Comparison Among Different Methods for Each
      Human Action on H3.6M
  Table 5 caption:
    table_text: TABLE 5 Performance (MAE) Comparison Among Different Methods in Terms
      of Short-Term and Long-Term Human Motion Prediction for Eight Actions on the
      CMU Mocap Dataset
  Table 6 caption:
    table_text: TABLE 6 Recognition Accuracies (%) of Different Methods on H3.6M
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3050918
- Affiliation of the first author: institute for brain and cognitive sciences, tsinghua
    university, beijing, china
  Affiliation of the last author: institute for brain and cognitive sciences, tsinghua
    university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\PaMIR_Parametric_ModelConditioned_Implicit_Representation_for_ImageBased_Human_R\figure_1.jpg
  Figure 1 caption: Example results reconstructed by our method. Our method is able
    to infer the underlying body shape, the 3D surface geometry and its texture given
    a single RGB image.
  Figure 10 Link: articels_figures_by_rev_year\2021\PaMIR_Parametric_ModelConditioned_Implicit_Representation_for_ImageBased_Human_R\figure_10.jpg
  Figure 10 caption: Comparison between the single-image reconstruction results and
    the multi-image results from input images with moderate pose deviations. In these
    cases, our method is still able to integrate information from different frames.
  Figure 2 Link: articels_figures_by_rev_year\2021\PaMIR_Parametric_ModelConditioned_Implicit_Representation_for_ImageBased_Human_R\figure_2.jpg
  Figure 2 caption: Overview of the PaMIR representation and the corresponding network
    architecture. Given an input image, we first estimate a corresponding SMPL model
    in the SMPL estimation step. In the following step, the image and the SMPL model
    are converted into a feature map and a feature volume respectively. Conditioned
    on the feature vectors sampled in the feature sampling step, the implicit function
    value for each 3D point can be obtained in the final step.
  Figure 3 Link: articels_figures_by_rev_year\2021\PaMIR_Parametric_ModelConditioned_Implicit_Representation_for_ImageBased_Human_R\figure_3.jpg
  Figure 3 caption: Comparison of the traditional reconstruction loss and our proposed
    depth-ambiguity-aware reconstruction loss. Unlike the traditional reconstruction
    loss, we shift the point samples according to the depth difference between the
    ground-truth SMPL and the predicted one. In this way the network is able to learn
    to infer an implicit field registered with the predicted 3D pose.
  Figure 4 Link: articels_figures_by_rev_year\2021\PaMIR_Parametric_ModelConditioned_Implicit_Representation_for_ImageBased_Human_R\figure_4.jpg
  Figure 4 caption: Illustration of body reference optimization. The estimated SMPL
    parameters are first decoded into a SMPL mesh and then the occupancy values of
    its vertices are infered by our PaMIR network. Through minimizing the body fitting
    loss, the SMPL parameters are optimized iteratively.
  Figure 5 Link: articels_figures_by_rev_year\2021\PaMIR_Parametric_ModelConditioned_Implicit_Representation_for_ImageBased_Human_R\figure_5.jpg
  Figure 5 caption: 'Visualization of the body optimization process. The leftmost
    column: the input image. The 2nd to 4th columns: the reconstruction results before
    reference body optimization, the intermediate status of optimization and the results
    after optimization. We present the SMPL models in the top row and the reconstructed
    models in the bottom row. The median row shows the occupancy probability of SMPL
    vertices: blue color represents F(C(v))=1.0 , while red means F(C(v))=0.0 .'
  Figure 6 Link: articels_figures_by_rev_year\2021\PaMIR_Parametric_ModelConditioned_Implicit_Representation_for_ImageBased_Human_R\figure_6.jpg
  Figure 6 caption: Illustration of our texture inference process. The texture network
    first recovers the color on the whole surface as well as an alpha channel which
    is used to blend the predicted color with the observation. In this way, the input
    image is maximally utilized and more texture details are recovered (Please zoom
    in to compare the texture on the shirt).
  Figure 7 Link: articels_figures_by_rev_year\2021\PaMIR_Parametric_ModelConditioned_Implicit_Representation_for_ImageBased_Human_R\figure_7.jpg
  Figure 7 caption: Comparison between the single-image result and the multi-image
    result. By adding four more frames (without calibration and synchronization infomation)
    as input, our method is able to recover the surface details on the back. In contrast,
    the back area is over-smoothed in the single-image setting. Zoom in for better
    view.
  Figure 8 Link: articels_figures_by_rev_year\2021\PaMIR_Parametric_ModelConditioned_Implicit_Representation_for_ImageBased_Human_R\figure_8.jpg
  Figure 8 caption: 'Our results on natural images. From left to right: the 1st is
    the input images, the second and third column show the SMPL models estimated by
    our method (network inference + optimization), the fourth to sixth demonstrates
    our geometry reconstruction results, and the last three column demonstrates the
    texture inference results. The results demonstrate the ability of our method to
    reconstruct high-quality models and its robust performance to tackle various human
    poses and clothing styles.'
  Figure 9 Link: articels_figures_by_rev_year\2021\PaMIR_Parametric_ModelConditioned_Implicit_Representation_for_ImageBased_Human_R\figure_9.jpg
  Figure 9 caption: 'Multi-image human model reconstruction on VideoAvatar dataset:
    (a) the first column is the first images of the input sequences, (b) the reconstructed
    models by our method, (c) results by [60], (d) Results by [61]. Our method can
    reconstruct full-body models with high-resolution details, proving the capability
    of our PaMIR-based reconstruction method.'
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.76
  Name of the first author: Zerong Zheng
  Name of the last author: Qionghai Dai
  Number of Figures: 16
  Number of Tables: 5
  Number of authors: 4
  Paper title: 'PaMIR: Parametric Model-Conditioned Implicit Representation for Image-Based
    Human Reconstruction'
  Publication Date: 2021-01-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Number of Parameters and Execution Time of Each Network
      Module
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Numerical Comparison Results
  Table 3 caption:
    table_text: TABLE 3 Numerical Ablation Study
  Table 4 caption:
    table_text: 'TABLE 4 Mean Per Joint Position Error (MPJPE, Unit: cm) BeforeAfter
      Body Reference Optimization'
  Table 5 caption:
    table_text: TABLE 5 Normal Reprojection Error With Inputs Images From Various
      Numbers of View Points
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3050505
