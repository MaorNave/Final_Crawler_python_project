- Affiliation of the first author: key laboratory of complex systems modeling and
    simulation, school of computer science and technology, hangzhou dianzi university,
    hangzhou, china
  Affiliation of the last author: ubtech sydney artificial intelligence centre, school
    of computerscience, faculty of engineering, university of sydney, darlington,
    nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2019\Hierarchical_Deep_Click_Feature_Prediction_for_FineGrained_Image_Recognition\figure_1.jpg
  Figure 1 caption: Pipeline of our image recognition with predicted hierarchical
    click features. With the help of a dataset with click information, a coarse-to-fine
    click feature model is devised. A hierarchical deep word embedding model is learned
    to generate the coarse-to-fine click feature predictor. Also, a feature selection
    module (shown in Fig. 2) is introduced to deal with heavy noise and high dimensionality
    in the click features.
  Figure 10 Link: articels_figures_by_rev_year\2019\Hierarchical_Deep_Click_Feature_Prediction_for_FineGrained_Image_Recognition\figure_10.jpg
  Figure 10 caption: Unseen category recognition ability of HDWE compared with others.
    Both the out-of-class and cross-object scenarios are tested.
  Figure 2 Link: articels_figures_by_rev_year\2019\Hierarchical_Deep_Click_Feature_Prediction_for_FineGrained_Image_Recognition\figure_2.jpg
  Figure 2 caption: "The feature selection module designed by introducing a weight\
    \ vector with sparse penalty and \u201CABS-RELU\u201D operator. The output feature\
    \ is obtained by element-wise product of weight vector and input feature."
  Figure 3 Link: articels_figures_by_rev_year\2019\Hierarchical_Deep_Click_Feature_Prediction_for_FineGrained_Image_Recognition\figure_3.jpg
  Figure 3 caption: "Recognition accuracies with varied \u03B1 and \u03B2 . \u201C\
    Baseline\u201D denotes the traditional single-level click prediction with single\
    \ word embedding."
  Figure 4 Link: articels_figures_by_rev_year\2019\Hierarchical_Deep_Click_Feature_Prediction_for_FineGrained_Image_Recognition\figure_4.jpg
  Figure 4 caption: Comparison of hierarchical and single-layer word embedding model
    with decreased word vocabularies.
  Figure 5 Link: articels_figures_by_rev_year\2019\Hierarchical_Deep_Click_Feature_Prediction_for_FineGrained_Image_Recognition\figure_5.jpg
  Figure 5 caption: Weight distribution (minimum value, variance, and margin) by varying
    a 3 . Both dog (top row) and bird (bottom row) dataset are tested.
  Figure 6 Link: articels_figures_by_rev_year\2019\Hierarchical_Deep_Click_Feature_Prediction_for_FineGrained_Image_Recognition\figure_6.jpg
  Figure 6 caption: "Recognition accuracies (%) and number of nonzero (NNZ) items\
    \ under different T \u2032 controlling pruning ratio. The dog dataset is tested."
  Figure 7 Link: articels_figures_by_rev_year\2019\Hierarchical_Deep_Click_Feature_Prediction_for_FineGrained_Image_Recognition\figure_7.jpg
  Figure 7 caption: Comparisons between HDWE and HDWE-P in recognition accuracy (top
    row) and pruning ratio (bottom row) under different thresholds.
  Figure 8 Link: articels_figures_by_rev_year\2019\Hierarchical_Deep_Click_Feature_Prediction_for_FineGrained_Image_Recognition\figure_8.jpg
  Figure 8 caption: Comparison between HDWE(R-) and other methods in scalability to
    word vocabulary.
  Figure 9 Link: articels_figures_by_rev_year\2019\Hierarchical_Deep_Click_Feature_Prediction_for_FineGrained_Image_Recognition\figure_9.jpg
  Figure 9 caption: Comparison of one-shot learning ability between HDWE and other
    methods by recognition accuracies with decreased source data.
  First author gender probability: 0.9
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jun Yu
  Name of the last author: Dacheng Tao
  Number of Figures: 14
  Number of Tables: 17
  Number of authors: 5
  Paper title: Hierarchical Deep Click Feature Prediction for Fine-Grained Image Recognition
  Publication Date: 2019-07-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Major Notations and Definitions
  Table 10 caption:
    table_text: TABLE 10 Comparison of Recognition Accuracies (%) with Different Initialization
      Methods for Feature Weight w w
  Table 2 caption:
    table_text: TABLE 2 Statistics of Used Dog and Bird Datasets
  Table 3 caption:
    table_text: "TABLE 3 Comparison of Models with Count-Based Loss ( \u03C4=0 \u03C4\
      =0), Position-Sensitive Loss ( \u03C4=1 \u03C4=1), and Their Combination ( 0<\u03C4\
      <1 0<\u03C4<1)"
  Table 4 caption:
    table_text: TABLE 4 Word Embedding Compared with Traditional Regression-Based
      Model in Click Prediction
  Table 5 caption:
    table_text: TABLE 5 Impact of Semantic Level Number in Word Hierarchies
  Table 6 caption:
    table_text: TABLE 6 Comparison of Recognition Accuracy (%) by Hierarchical Word
      Embedding and Two Single-Layer Word Embedding Models
  Table 7 caption:
    table_text: "TABLE 7 Recognition Accuracies with Varying \u2113 1 \u21131 Decay\
      \ a 3 a3 with ( a 2 =0.0005 a2=0.0005) or without ( a 2 =0 a2=0) the Traditional\
      \ \u2113 2 \u21132 Decay under Fixed \u03B4 \u2032 =1 \u03B4=1"
  Table 8 caption:
    table_text: "TABLE 8 Impact of \u03B4 \u2032 \u03B4 Controlling the Shrinking\
      \ Speed of \u2113 1 \u21131 Weight Decay"
  Table 9 caption:
    table_text: "TABLE 9 Recognition Accuracies (%) under Different Combinations of\
      \ \u03B3 0 \u03B30 and \u03B3 1 \u03B31"
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2932058
- Affiliation of the first author: department of computer science and engineering,
    jadavpur university, kolkata, india
  Affiliation of the last author: department of information technology, rcc institute
    of information technology, beliaghata, kolkata, india
  Figure 1 Link: articels_figures_by_rev_year\2019\Pattern_of_Local_Gravitational_Force_PLGF_A_Novel_Local_Image_Descriptor\figure_1.jpg
  Figure 1 caption: An overview of the proposed PLGF descriptor representation algorithm
    for describing a given input image.
  Figure 10 Link: articels_figures_by_rev_year\2019\Pattern_of_Local_Gravitational_Force_PLGF_A_Novel_Local_Image_Descriptor\figure_10.jpg
  Figure 10 caption: 'Application of PLGF descriptor on one sample photo (1st row)
    and sketch (2nd row) of CUFSF Database. (left to right): original image, M 1 x
    filtered image, M 1 y filtered image, F mag image, PLGFM image, F ang image, and
    PLGFA image.'
  Figure 2 Link: articels_figures_by_rev_year\2019\Pattern_of_Local_Gravitational_Force_PLGF_A_Novel_Local_Image_Descriptor\figure_2.jpg
  Figure 2 caption: The gravitational force between two bodies and its X and Y components.
  Figure 3 Link: articels_figures_by_rev_year\2019\Pattern_of_Local_Gravitational_Force_PLGF_A_Novel_Local_Image_Descriptor\figure_3.jpg
  Figure 3 caption: Different illumination variation images from Ex Yale B database
    and effects of proposed illumination-invariant magnitude. (a) The original images
    with illumination variations. (b) Canny edge detection images. (c) Normal gravitational
    force magnitude calculation. (d) Proposed illumination-invariant magnitude using
    Theorem 1.
  Figure 4 Link: articels_figures_by_rev_year\2019\Pattern_of_Local_Gravitational_Force_PLGF_A_Novel_Local_Image_Descriptor\figure_4.jpg
  Figure 4 caption: "The distribution of pixels with respect to central pixel ( c\
    \ ) at different values of ( P,w ) i.e., (8,1) and (16,2), different positional\
    \ angles ( \u03B8,\u03B2 ) and distances ( r 1 , r 2 , r 3 , r 4 , r 5 ) of neighboring\
    \ pixels."
  Figure 5 Link: articels_figures_by_rev_year\2019\Pattern_of_Local_Gravitational_Force_PLGF_A_Novel_Local_Image_Descriptor\figure_5.jpg
  Figure 5 caption: (a) Responses of the M x and M y filters in the space domain.
    (b) Responses of the M x and M y filters in the frequency domain.
  Figure 6 Link: articels_figures_by_rev_year\2019\Pattern_of_Local_Gravitational_Force_PLGF_A_Novel_Local_Image_Descriptor\figure_6.jpg
  Figure 6 caption: (a) Original gray image, (b) Filtered image after applying filter
    M 1 x , (c) Filtered image after applying filter M 1 y , (d) Image of the normalized
    force magnitude obtained using Theorem 1 and (8,1) filter, (e) Image of the direction
    of force using (8,1) filter.
  Figure 7 Link: articels_figures_by_rev_year\2019\Pattern_of_Local_Gravitational_Force_PLGF_A_Novel_Local_Image_Descriptor\figure_7.jpg
  Figure 7 caption: (a) Best threshold (th) selection for PLGFA. (b) Best block size
    selection for PLGF. (c) Rank 1 recognition results on different rotation angles.
    (d) Recognition results on different percentage of pixel corruption on CUFSF database.
    (e) Recognition results on different percentage of pixel corruption on Outex (TC10)
    database. (f) Recognition results on different percentage of pixel corruption
    on OASIS database. (g) Comparison in terms of ARP of proposed method with other
    existing methods as a function of number of top matches on OASIS database with
    Normal query images, (h) with images corrupted with noise.
  Figure 8 Link: articels_figures_by_rev_year\2019\Pattern_of_Local_Gravitational_Force_PLGF_A_Novel_Local_Image_Descriptor\figure_8.jpg
  Figure 8 caption: 'Application of PLGF descriptor on one normal sample (1st row)
    and one illumination variation sample (2nd row) of Ex Yale B Face Database. (left
    to right): original image, M 1 x filtered image, M 1 y filtered image, F mag image,
    PLGFM image, F ang image, and PLGFA image.'
  Figure 9 Link: articels_figures_by_rev_year\2019\Pattern_of_Local_Gravitational_Force_PLGF_A_Novel_Local_Image_Descriptor\figure_9.jpg
  Figure 9 caption: 'Application of PLGF descriptor on one sample image of LFW Face
    Database. (left to right): original image, Mx1 filtered image, My1 filtered image,
    Fmag image, PLGFM image, Fang image, and PLGFA image.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Debotosh Bhattacharjee
  Name of the last author: Hiranmoy Roy
  Number of Figures: 13
  Number of Tables: 3
  Number of authors: 2
  Paper title: 'Pattern of Local Gravitational Force (PLGF): A Novel Local Image Descriptor'
  Publication Date: 2019-08-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Rank 1 Recognition Comparison of Different Components of PLGF
      and Its Improvement on Different Databases
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Rank 1 Recognition Rates (%) for Different Methods of Different
      Variations of Faces on Extended Yale B, LFW, and CUFSF Databases
  Table 3 caption:
    table_text: TABLE 3 Classification Scores(%) on Outex Database
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2930192
- Affiliation of the first author: broad institute of harvard and mit, cambridge,
    ma, usa
  Affiliation of the last author: courant institute of mathematical sciences, new
    york university, new york, ny, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\SafePredict_A_MetaAlgorithm_for_Machine_Learning_That_Uses_Refusals_to_Guarantee\figure_1.jpg
  Figure 1 caption: "The meta-algorithm, represented by M , makes a prediction equivalent\
    \ to the recommendation of the base predictor P or refuses to do so for data point\
    \ t while guaranteeing a target rate \u03F5 ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\SafePredict_A_MetaAlgorithm_for_Machine_Learning_That_Uses_Refusals_to_Guarantee\figure_2.jpg
  Figure 2 caption: "Efficiency experiments on synthetic data: The efficiency ( T\
    \ \u2217 T ) of SafePredict with respect to increasing choices of \u03B1 . (top)\
    \ If the base predictor has a constant error rate which is higher than the target,\
    \ SafePredict almost always refuses. The number of predictions, in this case,\
    \ increases with \u03B1 . (bottom) On the other hand, when the error rate of the\
    \ base predictor fluctuates around the target, the efficiency of SafePredict increases\
    \ as \u03B1 increases and achieves nearly the same efficiency as the oracle, which\
    \ predicts if only if \u03F5 t \u2264\u03F5 . So, in all cases, asymptotic validity\
    \ is preserved."
  Figure 3 Link: articels_figures_by_rev_year\2019\SafePredict_A_MetaAlgorithm_for_Machine_Learning_That_Uses_Refusals_to_Guarantee\figure_3.jpg
  Figure 3 caption: "Synthetic data, evolution of efficiency: Note \u03B1=0 corresponds\
    \ to the original SafePredict (Algorithm 3) and has no adaptivity. For \u03B1\
    >0 , SafePredict can track the change points and boost efficiency. Larger \u03B1\
    \ implies better tracking. As the number of change points increases, SafePredict\
    \ does a poorer job tracking the performance of the base predictor (relative to\
    \ the oracle that knows the error rate), thus the efficiency drops. All the predictors\
    \ in the figures are valid."
  Figure 4 Link: articels_figures_by_rev_year\2019\SafePredict_A_MetaAlgorithm_for_Machine_Learning_That_Uses_Refusals_to_Guarantee\figure_4.jpg
  Figure 4 caption: "MNIST dataset: Efficiency is 1.0 for the base predictor but lower\
    \ for the various refusing meta-algorithms. However, the base predictor has a\
    \ poor error rate (way over \u03F5 ). All the SafePredict variants rapidly approach\
    \ an error rate value below the target error rate 0.05 as they make predictions.\
    \ The confidence-based meta-algorithm cannot guarantee asymptotic validity due\
    \ to the changes in the underlying distribution (marked by vertical dashed lines).\
    \ Two forms of adaptivity help reduce the number of refusals: weight-shifting\
    \ especially with a high \u03B1 value and amnesic adaptivity. Combining both leads\
    \ to the highest efficiency while preserving validity."
  Figure 5 Link: articels_figures_by_rev_year\2019\SafePredict_A_MetaAlgorithm_for_Machine_Learning_That_Uses_Refusals_to_Guarantee\figure_5.jpg
  Figure 5 caption: 'IMDB dataset: Similar to Fig. 4, except in this case, the experiment
    consists of a randomly chosen 10000 data points used from the IMDB sentiment analysis
    dataset [43] instead of MNIST.'
  Figure 6 Link: articels_figures_by_rev_year\2019\SafePredict_A_MetaAlgorithm_for_Machine_Learning_That_Uses_Refusals_to_Guarantee\figure_6.jpg
  Figure 6 caption: "Reuters dataset: Similar to Fig. 4, except randomly chosen 10000\
    \ data points used from Reuters dataset [44] instead of MNIST. Note the error\
    \ rate for SafePredict (SP) is way above the target error rate for the first 10,000\
    \ points. Ideally, SafePredict should refuse always. In fact, the expected number\
    \ of predictions ( T \u2217 ) is only 16.44."
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Mustafa A. Kocak
  Name of the last author: Dennis E. Shasha
  Number of Figures: 6
  Number of Tables: 1
  Number of authors: 4
  Paper title: 'SafePredict: A Meta-Algorithm for Machine Learning That Uses Refusals
    to Guarantee Correctness'
  Publication Date: 2019-08-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of the Notation
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2932415
- Affiliation of the first author: department of electrical and computer engineering,
    university of florida, gainesville, fl, usa
  Affiliation of the last author: department of electrical and computer engineering,
    university of florida, gainesville, fl, usa
  Figure 1 Link: "articels_figures_by_rev_year\\2019\\Multivariate_Extension_of_MatrixBased_R\xE9\
    nyis\u03B1\u03B1Order_Entropy_Functional\\figure_1.jpg"
  Figure 1 caption: Validation accuracy or Leave-one-out (LOO) results on synthetic
    and real datasets. The number of samples and the feature dimensionality for each
    dataset are listed in the title. The value beside each method in the legend indicates
    the average rank in that dataset. Our method performs favorably in all datasets
    regardless of data characteristics.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: "articels_figures_by_rev_year\\2019\\Multivariate_Extension_of_MatrixBased_R\xE9\
    nyis\u03B1\u03B1Order_Entropy_Functional\\figure_2.jpg"
  Figure 2 caption: "Critical difference (CD) [31] generated using Nemenyis post-hoc\
    \ test [30] with significance level 0.05. The groups are identified using the\
    \ mean rank of a model \xB1 the CD (marked with a horizontal blue line). There\
    \ is no evidence of significant differences for models in the same group (joined\
    \ by the dashed green lines). Only our method and CMIM are significantly different\
    \ from the baseline method MIM."
  Figure 3 Link: "articels_figures_by_rev_year\\2019\\Multivariate_Extension_of_MatrixBased_R\xE9\
    nyis\u03B1\u03B1Order_Entropy_Functional\\figure_3.jpg"
  Figure 3 caption: "Validation accuracy of our method on waveform and semeion datasets\
    \ with respect to different \u03C3 and feature numbers ( \u03B1=1.01 ). The value\
    \ in the parenthesis is the average rank of our method with respect to other competing\
    \ methods, when using the corresponding \u03C3 . Our method works well in a large\
    \ range of \u03C3 , and the 10 to 20 percent of the total (median) range is a\
    \ more reliable choice to \u03C3 (marked with red)."
  Figure 4 Link: "articels_figures_by_rev_year\\2019\\Multivariate_Extension_of_MatrixBased_R\xE9\
    nyis\u03B1\u03B1Order_Entropy_Functional\\figure_4.jpg"
  Figure 4 caption: Overall accuracy (OA) and average accuracy (AA) of competing methods
    in three different gallery sizes (1, 5 and 10 percent gallery samples per class)
    on Indian Pine dataset. MIFS, FOU and MIM perform poorly in this example, and
    thus omitted. Our method is consistently better than other in the scenario of
    1 and 5 percent gallery sizes, but inferior to JMI using 10 percent gallery samples
    per class.
  Figure 5 Link: "articels_figures_by_rev_year\\2019\\Multivariate_Extension_of_MatrixBased_R\xE9\
    nyis\u03B1\u03B1Order_Entropy_Functional\\figure_5.jpg"
  Figure 5 caption: Reflectance of 16 land-cover categories on the Indian Pine data.
    Only spectral bands between 1 and 103 are displayed, since the 10 most significant
    bands selected by JMI and our method all lie in this region. In (a) and (b), the
    black dashed lines represent the bands selected by JMI and our method respectively
    over 10 independent runs, whereas the red solid line highlight the bands that
    have been selected at least 7 times in the 10 runs. (c) shows the land-cover legend.
    Our method is stable with respect to training data perturbations.
  Figure 6 Link: "articels_figures_by_rev_year\\2019\\Multivariate_Extension_of_MatrixBased_R\xE9\
    nyis\u03B1\u03B1Order_Entropy_Functional\\figure_6.jpg"
  Figure 6 caption: Classification maps on the Indian Pine data. 5 percent samples
    from each class care selected to constitute gallery set. The legend is same to
    Fig. 5. Our method improves the uniformity of regions marked with white rectangles.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.53
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Shujian Yu
  Name of the last author: "Jos\xE9 C. Pr\xEDncipe"
  Number of Figures: 6
  Number of Tables: 2
  Number of authors: 4
  Paper title: "Multivariate Extension of Matrix-Based R\xE9nyi's\n\u03B1\n\u03B1\
    -Order Entropy Functional"
  Publication Date: 2019-08-05 00:00:00
  Table 1 caption:
    table_text: TABLE 1 A Summarization of Different Information-Theoretic Feature
      Selection Methods and Their Average Ranks over Different Number of Features
      in Each Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Summary of Computational and Memory Cost
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2932976
- Affiliation of the first author: department of computer science and engineering,
    university of notre dame, notre dame, in, usa
  Affiliation of the last author: department of computer science and engineering,
    university of notre dame, notre dame, in, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\DACSDC_Low_Power_Object_Detection_Challenge_for_UAV_Applications\figure_1.jpg
  Figure 1 caption: Overview of the dataset provided by DJI. There are 12 categories,
    each of which includes several sub-categories as indicated in the bracket, and
    there are totally 95 sub-categories. Note that there is only one object in each
    image.
  Figure 10 Link: articels_figures_by_rev_year\2019\DACSDC_Low_Power_Object_Detection_Challenge_for_UAV_Applications\figure_10.jpg
  Figure 10 caption: Statistical significance analysis using bootstrapping with 99.9
    percent confidence intervals for GPU entries. Note that the entries are ranked
    from high to low in the vertical axis.
  Figure 2 Link: articels_figures_by_rev_year\2019\DACSDC_Low_Power_Object_Detection_Challenge_for_UAV_Applications\figure_2.jpg
  Figure 2 caption: Distributions of the training and testing datasets with respect
    to image categories.
  Figure 3 Link: articels_figures_by_rev_year\2019\DACSDC_Low_Power_Object_Detection_Challenge_for_UAV_Applications\figure_3.jpg
  Figure 3 caption: Distributions of the training and testing datasets with respect
    to object size ratio.
  Figure 4 Link: articels_figures_by_rev_year\2019\DACSDC_Low_Power_Object_Detection_Challenge_for_UAV_Applications\figure_4.jpg
  Figure 4 caption: Distributions of the training and testing datasets with respect
    to image brightness.
  Figure 5 Link: articels_figures_by_rev_year\2019\DACSDC_Low_Power_Object_Detection_Challenge_for_UAV_Applications\figure_5.jpg
  Figure 5 caption: Distributions of the training and testing datasets with respect
    to amount of information.
  Figure 6 Link: articels_figures_by_rev_year\2019\DACSDC_Low_Power_Object_Detection_Challenge_for_UAV_Applications\figure_6.jpg
  Figure 6 caption: Number of entries using (a) neural network models and (b) deep
    learning frameworks.
  Figure 7 Link: articels_figures_by_rev_year\2019\DACSDC_Low_Power_Object_Detection_Challenge_for_UAV_Applications\figure_7.jpg
  Figure 7 caption: 'Neural network structure of the top-3 GPU entries: (a) ICT-CAS,
    (b) DeepZ, and (c) SDU-Legend.'
  Figure 8 Link: articels_figures_by_rev_year\2019\DACSDC_Low_Power_Object_Detection_Challenge_for_UAV_Applications\figure_8.jpg
  Figure 8 caption: 'Neural network structure of the top-3 FPGA entries: (a) TGIIF,
    (b) SystemETHZ, and (c) iSmart2.'
  Figure 9 Link: articels_figures_by_rev_year\2019\DACSDC_Low_Power_Object_Detection_Challenge_for_UAV_Applications\figure_9.jpg
  Figure 9 caption: Overall results of GPU entries. Note that the entries are ranked
    from high to low in the horizontal axis. The details of the scores can be found
    on the website of the challenge.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.55
  Name of the first author: Xiaowei Xu
  Name of the last author: Yiyu Shi
  Number of Figures: 15
  Number of Tables: 1
  Number of authors: 7
  Paper title: DAC-SDC Low Power Object Detection Challenge for UAV Applications
  Publication Date: 2019-08-05 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Resource Utilization of FPGA Entries
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2932429
- Affiliation of the first author: cristal laboratory, cnrs umr, imt lille douai,
    villeneuve-dascq, france
  Affiliation of the last author: inception institute of artificial intelligence (iiai),
    abu dhabi, uae
  Figure 1 Link: articels_figures_by_rev_year\2019\Sparse_Coding_of_Shape_Trajectories_for_Facial_Expression_and_Action_Recognition\figure_1.jpg
  Figure 1 caption: Overview of the proposed approaches. Sequences of 2D3D landmark
    configurations are first represented as trajectories in the Kendalls shape space.
    A Riemannian dictionary is learned from training samples before it is used to
    code trajectories. This yields euclidean sparse time-series that are temporally
    modeled and classified in vector space. 2D facial expression trajectories are
    coded using extrinsic SCDL while 3D action trajectories are coded with intrinsic
    SCDL.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Sparse_Coding_of_Shape_Trajectories_for_Facial_Expression_and_Action_Recognition\figure_2.jpg
  Figure 2 caption: Illustration of intrinsic sparse coding in the Kendalls shape
    space. Given a dictionary D= d i N i=1 , a skeletal trajectory is coded as a smoothly-varying
    sparse time-series. Using D , the original trajectory can be reconstructed with
    the weighted Karcher mean algorithm.
  Figure 3 Link: articels_figures_by_rev_year\2019\Sparse_Coding_of_Shape_Trajectories_for_Facial_Expression_and_Action_Recognition\figure_3.jpg
  Figure 3 caption: Illustration of the proposed clustering approach. 2D facial shapes
    (respectively 3D skeletons) are mapped from the 2D (respectively 3D) Kendalls
    space to RKHS by computing the inner product matrix from the data. Bayesian clustering
    is then applied on this matrix to construct the final clusters whose number is
    automatically inferred.
  Figure 4 Link: articels_figures_by_rev_year\2019\Sparse_Coding_of_Shape_Trajectories_for_Facial_Expression_and_Action_Recognition\figure_4.jpg
  Figure 4 caption: Recognition accuracy achieved for each emotion class in the CK+
    (left) and the CASIA (rigth) datasets, and comparison between extrinsic and intrinsic
    SCDL approaches.
  Figure 5 Link: articels_figures_by_rev_year\2019\Sparse_Coding_of_Shape_Trajectories_for_Facial_Expression_and_Action_Recognition\figure_5.jpg
  Figure 5 caption: 'Visualization of 2-dimensional features of the NTU-RGB+D dataset.
    Left: original data. Right: the corresponding SCDL features. Each class is represented
    by a different color. This figure is better seen in colors.'
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Amor Ben Tanfous
  Name of the last author: Boulbaba Ben Amor
  Number of Figures: 5
  Number of Tables: 8
  Number of authors: 3
  Paper title: Sparse Coding of Shape Trajectories for Facial Expression and Action
    Recognition
  Publication Date: 2019-08-05 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison with State-of-the-Art on CK+ and Oulu-CASIA Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Confusion Matrix on the Oulu-Casia Dataset
  Table 3 caption:
    table_text: TABLE 3 Recognition Accuracy on CASME II Dataset and Comparison with
      State-of-the-Art Methods
  Table 4 caption:
    table_text: TABLE 4 Overall Recognition Accuracy (%) on MSR-Action 3D, Florence
      Action 3D, and UTKinect 3D Datasets
  Table 5 caption:
    table_text: TABLE 5 Confusion Matrix on the Florence Action 3D Dataset
  Table 6 caption:
    table_text: TABLE 6 Overall Recognition Accuracy (%) on NTU-RGB+D Following the
      X-Sub and X-View Protocols
  Table 7 caption:
    table_text: TABLE 7 Evaluation of the Kendalls Shape Space Representation
  Table 8 caption:
    table_text: TABLE 8 Classification Performances When Using Different Landmark
      Detectors
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2932979
- Affiliation of the first author: school of mathematical sciences, university of
    science and technology of china, hefei, anhui, china
  Affiliation of the last author: school of computer science and engineering, nanyang
    technological university, singapore
  Figure 1 Link: articels_figures_by_rev_year\2019\Parallel_and_Scalable_Heat_Methods_for_Geodesic_Distance_Computation\figure_1.jpg
  Figure 1 caption: Geodesic distance fields computed using our parallel and scalable
    heat method (Algorithm 1) on different high-resolution models, visualized using
    their level sets. A comparison with other methods on computational time, peak
    memory consumption and accuracy is provided in Table 1.
  Figure 10 Link: articels_figures_by_rev_year\2019\Parallel_and_Scalable_Heat_Methods_for_Geodesic_Distance_Computation\figure_10.jpg
  Figure 10 caption: We repeatedly subdivide some meshes (bottom row) to obtain very
    high-resolution models (top row). On such models, even our face-based method runs
    out of memory, while our edge-based method can still work on a PC with 128 GB
    RAM and an octa-core CPU at 3.6 GHz. The numbers on the top row show the performance
    and accuracy of our edge-based method on each model. The bottom row shows the
    VTP results on the original models for comparison.
  Figure 2 Link: articels_figures_by_rev_year\2019\Parallel_and_Scalable_Heat_Methods_for_Geodesic_Distance_Computation\figure_2.jpg
  Figure 2 caption: Our Gauss-Seidel heat diffusion quickly decreases the mean error
    of the result, computed based on Eq. (8). As a result, it produces a solution
    good enough for subsequent steps with significantly less computational time than
    a direct solver or a Krylov subspace method.
  Figure 3 Link: articels_figures_by_rev_year\2019\Parallel_and_Scalable_Heat_Methods_for_Geodesic_Distance_Computation\figure_3.jpg
  Figure 3 caption: For a gradient field g i to be integrable, the gradients g e 1
    , g e 2 on a pair of adjacent faces must have the same projection on their common
    edge e , resulting in the compatibility condition (11).
  Figure 4 Link: articels_figures_by_rev_year\2019\Parallel_and_Scalable_Heat_Methods_for_Geodesic_Distance_Computation\figure_4.jpg
  Figure 4 caption: The ADMM solver quickly decreases the primal and dual residuals
    in the initial iterations, as shown here on the same model as in Fig. 2.
  Figure 5 Link: articels_figures_by_rev_year\2019\Parallel_and_Scalable_Heat_Methods_for_Geodesic_Distance_Computation\figure_5.jpg
  Figure 5 caption: Starting from the unit gradients resulting from heat diffusion,
    our ADMM solver can produce a geodesic distance gradient field of good accuracy
    within a small number of iterations. Here we run the ADMM solver for a prescribed
    number of iterations, and integrate the resulting gradients according to Section
    3.3. The recovered geodesic distance field is visualized with color coding and
    level sets. The caption of each image shows the number of ADMM iterations and
    the mean relative error of the resulting geodesic distance, computed according
    to Eq. (29).
  Figure 6 Link: articels_figures_by_rev_year\2019\Parallel_and_Scalable_Heat_Methods_for_Geodesic_Distance_Computation\figure_6.jpg
  Figure 6 caption: "The integrability condition for the edge variables within a triangle\
    \ depends on the direction of their orientation halfedges, shown using arrows\
    \ in this figure. The dashed arcs show the orientation for each triangle. For\
    \ this particular example, the integrability conditions are x e 1 + x e 2 + x\
    \ e 3 =0 and \u2212 x e 1 + x e 4 \u2212 x e 5 =0 , respectively."
  Figure 7 Link: articels_figures_by_rev_year\2019\Parallel_and_Scalable_Heat_Methods_for_Geodesic_Distance_Computation\figure_7.jpg
  Figure 7 caption: "Geodesic distance on a surface of genus eight, computed using\
    \ our face-based method with and without the constraint (23), and its mean relative\
    \ error \u03B5 compared to the ground truth as defined in Eq. (29)."
  Figure 8 Link: articels_figures_by_rev_year\2019\Parallel_and_Scalable_Heat_Methods_for_Geodesic_Distance_Computation\figure_8.jpg
  Figure 8 caption: "Geodesic distance from multiple sources, computed using our face-based\
    \ method with and without the constraint (25), and its mean relative error \u03B5\
    \ compared to the ground truth as defined in Eq. (29)."
  Figure 9 Link: articels_figures_by_rev_year\2019\Parallel_and_Scalable_Heat_Methods_for_Geodesic_Distance_Computation\figure_9.jpg
  Figure 9 caption: "Given three input meshes with the same underlying geometry and\
    \ different triangulation quality, using our edge-based method together with the\
    \ intrinsic Delaunay triangulation helps to retain the accuracy of the computed\
    \ geodesic distance even if the input mesh is poorly triangulated. Here the triangulation\
    \ quality \u03C4 is defined in Eq. (27), and the mean relative error \u03B5 compared\
    \ to the ground truth as defined in Eq. (29)."
  First author gender probability: 0.87
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.86
  Name of the first author: Jiong Tao
  Name of the last author: Ying He
  Number of Figures: 13
  Number of Tables: 1
  Number of authors: 6
  Paper title: Parallel and Scalable Heat Methods for Geodesic Distance Computation
  Publication Date: 2019-08-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Computational Time (in seconds), Peak Memory
      Consumption (in MB), and Accuracy between VTP [21], Heat Method [14], Variational
      Heat Method [27], and Our Methods (Face-Based and Edge-Based), on a PC with
      an Octa-Core CPU and 128 GB Memory
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2933209
- Affiliation of the first author: school of automation, northwestern polytechnical
    university, xian, shaanxi, china
  Affiliation of the last author: school of electrical and information engineering,
    university of sydney, camperdown, nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2019\PCNN_PartBased_Convolutional_Neural_Networks_for_FineGrained_Visual_Categorizati\figure_1.jpg
  Figure 1 caption: 'Illustration of challenges in fine-grained visual classification:
    the first row shows the large variances in the same subcategory and the second
    row shows the small variances among different subcategories.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\PCNN_PartBased_Convolutional_Neural_Networks_for_FineGrained_Visual_Categorizati\figure_2.jpg
  Figure 2 caption: Illustration of our proposed P-CNN based FGVC framework. It is
    composed of three modules. The first module is a SE block used to recalibrate
    channel-wise feature responses. The second module is a part localization network
    used to discover distinctive object parts. The third module is a part classification
    network that classifies each individual part into image-level categories and also
    combines part-level local features and object-level global feature into a joint
    feature for the final classification. These three modules are merged into a unified
    network for an end-to-end training by sharing their full-image convolutional features
    between the tasks of part localization and part classification.
  Figure 3 Link: articels_figures_by_rev_year\2019\PCNN_PartBased_Convolutional_Neural_Networks_for_FineGrained_Visual_Categorizati\figure_3.jpg
  Figure 3 caption: Illustration of our proposed part localization network used to
    discover distinctive object parts. It aims to learn a set of discriminative part
    detectors in an unsupervised learning fashion to discover informative object parts.
  Figure 4 Link: articels_figures_by_rev_year\2019\PCNN_PartBased_Convolutional_Neural_Networks_for_FineGrained_Visual_Categorizati\figure_4.jpg
  Figure 4 caption: Part localization results on the CUB-200-2011 bird dataset, the
    FGVC-aircraft dataset and the stanford cars dataset. The part detectors can localize
    consistently discriminative parts on all these three datasets.
  Figure 5 Link: articels_figures_by_rev_year\2019\PCNN_PartBased_Convolutional_Neural_Networks_for_FineGrained_Visual_Categorizati\figure_5.jpg
  Figure 5 caption: 'Part localization results: (a) the initial parts by channel clustering
    without SE block, (b) the initial parts by channel clustering with SE block, (c)
    the intermediate parts by training PLN, (d) the final parts by jointly training
    PLN and PCN, and (e) the final parts by using MA-CNN method [43].'
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.96
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Junwei Han
  Name of the last author: Dong Xu
  Number of Figures: 5
  Number of Tables: 3
  Number of authors: 5
  Paper title: 'P-CNN: Part-Based Convolutional Neural Networks for Fine-Grained Visual
    Categorization'
  Publication Date: 2019-08-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Accuracies (%) of Our Proposed Method and 12 State-of-the-Art
      Methods on the CUB-200-2011 Dataset, the FGVC Aircraft Dataset, and the Stanford
      Cars Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparisons of Different Methods for Part Localization in
      Terms of Classification Accuracy (%) on the CUB-200-2011 Dataset, the FGVC-Aircraft
      Dataset and the Stanford Cars Dataset
  Table 3 caption:
    table_text: TABLE 3 Ablation Studies of the P-CNN on the CUB-200-2011 Dataset,
      the FGVC-Aircraft Dataset and the Stanford Cars Dataset
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2933510
- Affiliation of the first author: school of electrical and computer engineering,
    ulsan national institute of science and technology, ulsan, korea
  Affiliation of the last author: school of electrical and computer engineering, ulsan
    national institute of science and technology, ulsan, korea
  Figure 1 Link: articels_figures_by_rev_year\2019\Virtual_Point_Removal_for_LargeScale_D_Point_Clouds_with_Multiple_Glass_Planes\figure_1.jpg
  Figure 1 caption: Reflection artifact in LS3DPC. (a) Reflection artifact in 2D image.
    (b) The principle of reflection in 3D point clouds captured by a LiDAR scanner.
    (c) A LS3DPC model where the virtual points caused by glass reflection are detected
    in red and the associated glass plane is shown in cyan.
  Figure 10 Link: articels_figures_by_rev_year\2019\Virtual_Point_Removal_for_LargeScale_D_Point_Clouds_with_Multiple_Glass_Planes\figure_10.jpg
  Figure 10 caption: "Fast Point Feature Histogram which describes the three angular\
    \ variations (\u03B1,\u03D5,\u03B8) associated with the two points p i and p j\
    \ ."
  Figure 2 Link: articels_figures_by_rev_year\2019\Virtual_Point_Removal_for_LargeScale_D_Point_Clouds_with_Multiple_Glass_Planes\figure_2.jpg
  Figure 2 caption: Partitioning of the unit sphere into local surface patches.
  Figure 3 Link: articels_figures_by_rev_year\2019\Virtual_Point_Removal_for_LargeScale_D_Point_Clouds_with_Multiple_Glass_Planes\figure_3.jpg
  Figure 3 caption: Distribution of the number of projected points in International
    Hall model. (a) A panoramic image for target scene. (b) The distribution of the
    number of projected points g i . (c) The histogram of g i s. (d) The distribution
    of the posterior probability p 1 ( g i ) .
  Figure 4 Link: articels_figures_by_rev_year\2019\Virtual_Point_Removal_for_LargeScale_D_Point_Clouds_with_Multiple_Glass_Planes\figure_4.jpg
  Figure 4 caption: Glass plane estimation. (a) The rendered 3D point cloud where
    the estimated three glass planes are depicted in purple, yellow, and cyan, respectively.
    (b) A color image for target scene captured at a similar viewpoint to (a).
  Figure 5 Link: articels_figures_by_rev_year\2019\Virtual_Point_Removal_for_LargeScale_D_Point_Clouds_with_Multiple_Glass_Planes\figure_5.jpg
  Figure 5 caption: Glass region refinement. (a) Absence of sampled points within
    the minimum acquisition distance from the scanner. (b) An image of target scene.
    (c) The initial reliability map. (d) The refined reliability map.
  Figure 6 Link: articels_figures_by_rev_year\2019\Virtual_Point_Removal_for_LargeScale_D_Point_Clouds_with_Multiple_Glass_Planes\figure_6.jpg
  Figure 6 caption: "Estimation results of multiple glass regions. (a) An input panoramic\
    \ image where the three glass planes \u03A0 0 , \u03A0 1 , and \u03A0 2 , and\
    \ the marble floor \u03A0 3 are depicted in purple, cyan, yellow and green, respectively.\
    \ The resulting reliability maps of (b) \u03A8 0 , (c) \u03A8 1 , (d) \u03A8 2\
    \ , and (e) \u03A8 3 ."
  Figure 7 Link: articels_figures_by_rev_year\2019\Virtual_Point_Removal_for_LargeScale_D_Point_Clouds_with_Multiple_Glass_Planes\figure_7.jpg
  Figure 7 caption: Virtual point generation with two parallel glass planes.
  Figure 8 Link: articels_figures_by_rev_year\2019\Virtual_Point_Removal_for_LargeScale_D_Point_Clouds_with_Multiple_Glass_Planes\figure_8.jpg
  Figure 8 caption: Trajectory estimation with multiple glass planes.
  Figure 9 Link: articels_figures_by_rev_year\2019\Virtual_Point_Removal_for_LargeScale_D_Point_Clouds_with_Multiple_Glass_Planes\figure_9.jpg
  Figure 9 caption: Symmetry relation of reflection between a pair of real point and
    virtual point.
  First author gender probability: 0.55
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Jae-Seong Yun
  Name of the last author: Jae-Young Sim
  Number of Figures: 17
  Number of Tables: 2
  Number of authors: 2
  Paper title: Virtual Point Removal for Large-Scale 3D Point Clouds with Multiple
    Glass Planes
  Publication Date: 2019-08-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Quantitative Performance
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Processing Times in Seconds
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2933818
- Affiliation of the first author: department of electrical and computer engineering,
    national university of singapore, singapore
  Affiliation of the last author: department of electrical and computer engineering,
    national university of singapore, singapore
  Figure 1 Link: articels_figures_by_rev_year\2019\Faster_FirstOrder_Methods_for_Stochastic_NonConvex_Optimization_on_Riemannian_Ma\figure_1.jpg
  Figure 1 caption: Comparison between R-SPIDER using exponential mapping and parallel
    transport (R-SPIDER-Exp for short) and R-SPIDER using polar retraction and vector
    transport (R-SPIDER-Rec) on the low-rank matrix completion problem. R-SPIDER-Rec
    and its learning-rate adaptive version, R-SPIDER-A-Rec, respectively achieve very
    similar first order oracle (IFO, i.e. stochastic gradient evaluation number, see
    Definition 3) complexity as R-SPIDER-Exp and R-SPIDER-A-Exp, but run much faster
    than R-SPIDER-Exp and R-SPIDER-A-Exp in terms of the algorithm execution time.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Faster_FirstOrder_Methods_for_Stochastic_NonConvex_Optimization_on_Riemannian_Ma\figure_2.jpg
  Figure 2 caption: Comparison among Riemannian stochastic gradient algorithms on
    the k -PCA problem.
  Figure 3 Link: articels_figures_by_rev_year\2019\Faster_FirstOrder_Methods_for_Stochastic_NonConvex_Optimization_on_Riemannian_Ma\figure_3.jpg
  Figure 3 caption: Comparison between R-SPIDER and R-SRG with adaptive learning rates
    on the k -PCA problem.
  Figure 4 Link: articels_figures_by_rev_year\2019\Faster_FirstOrder_Methods_for_Stochastic_NonConvex_Optimization_on_Riemannian_Ma\figure_4.jpg
  Figure 4 caption: Comparison among Riemannian stochastic gradient algorithms on
    low-rank matrix completion problem.
  Figure 5 Link: articels_figures_by_rev_year\2019\Faster_FirstOrder_Methods_for_Stochastic_NonConvex_Optimization_on_Riemannian_Ma\figure_5.jpg
  Figure 5 caption: Comparison among Riemannian stochastic gradient algorithms on
    low-rank matrix completion problem.
  Figure 6 Link: articels_figures_by_rev_year\2019\Faster_FirstOrder_Methods_for_Stochastic_NonConvex_Optimization_on_Riemannian_Ma\figure_6.jpg
  Figure 6 caption: Comparison between (1) R-SPIDER equipped with exponential mapping
    and parallel transport (R-SPIDER-Exp) and (2) R-SPIDER equipped with polar retraction
    and vector transport (R-SPIDER-Rec) on the LRMC problem.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Pan Zhou
  Name of the last author: Jiashi Feng
  Number of Figures: 6
  Number of Tables: 2
  Number of authors: 4
  Paper title: Faster First-Order Methods for Stochastic Non-Convex Optimization on
    Riemannian Manifolds
  Publication Date: 2019-08-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of IFO Complexity for Different Riemannian First-Order
      Stochastic Optimization Algorithms on the Noncovnex Problem (1) under Finite-Sum
      and Online Settings
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Descriptions of the Ten Testing Datasets
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2933841
