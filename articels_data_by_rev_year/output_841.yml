- Affiliation of the first author: microsoft research, beijing, p.r. china
  Affiliation of the last author: school of computer science and engineering, university
    of electronic science and technology of china, chengdu, china
  Figure 1 Link: articels_figures_by_rev_year\2017\A_Survey_on_Learning_to_Hash\figure_1.jpg
  Figure 1 caption: 'Illustrating the search strategies. (a) Multi table lookup: the
    list corresponding to the hash code of the query in each table is retrieved. (b)
    Single table lookup: the lists corresponding to and near to the hash code of the
    query are retrieved. (c) Hash code ranking: compare the query with each reference
    item in the coding space. (d) Non-exhaustive search: hash table lookup (or other
    inverted index structure) retrieves the candidates, followed by hash code ranking
    over the candidates.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\A_Survey_on_Learning_to_Hash\figure_2.jpg
  Figure 2 caption: 2D toy examples illustrating the quantization algorithms. The
    space partitioning results are generated by (a) product quantization, (b) Cartesian
    k -means, and (c) composite quantization. The space partition from composition
    quantization is more flexible.
  Figure 3 Link: articels_figures_by_rev_year\2017\A_Survey_on_Learning_to_Hash\figure_3.jpg
  Figure 3 caption: 2D toy examples illustrating the comparison between binary code
    hashing and quantization. (a) shows the Hamming distances from clusters B and
    D to cluster A , usually adopted in the binary code hashing algorithms, are the
    same while the Euclidean distances, used in the quantization algorithms, are different.
    (b) the binary code hashing algorithms need 6 hash bits (red lines show the corresponding
    hash functions) to differentiate the 16 uniformly-distributed clusters while the
    quantization algorithms only require 4 ( =log16 ) bits (green lines show the partition
    line).
  Figure 4 Link: articels_figures_by_rev_year\2017\A_Survey_on_Learning_to_Hash\figure_4.jpg
  Figure 4 caption: (a) and (b) show the performance in terms of recall R over SIFT
    1M and GloVe 1.2M for the representative hashing and quantization algorithms.
    (c) and (d) show the performance over the ILSVRC 2012 ImageNet dataset under the
    Euclidean distance in terms of recall R and under the semantic similarity in terms
    of mAP versus bits. BRE = binary reconstructive embedding [66], MLH = minimal
    loss hashing [107], LSH = locality sensitive hashing [14], ITQ = iterative quantization
    [35], [36], SH = spectral hashing [156], AGH-2 = two-layer hashing with graphs
    [86], USPLH = unsupervised sequential projection learning hashing [143], PQ =
    product quantization [50] , CKM = Cartesian k -means [108], CQ = composite quantization
    [171], SCQ = sparse composite quantization [172] whose dictionary is the same
    sparse with PQ. CCA-ITQ = iterative quantization with canonical correlation analysis
    [36], SSH = semi-supervised hashing [143], KSH = supervised hashing with kernels
    [85], FastHash = fash supervised hashing [76], SDH = supervised discrete hashing
    with kernels [125], SDH-linear = supervised discrete hashing without using kernel
    representations [125], SQ = supervised quantization [154], Euclidean = linear
    scan with the Euclidean distance.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Jingdong Wang
  Name of the last author: Heng Tao Shen
  Number of Figures: 4
  Number of Tables: 3
  Number of authors: 5
  Paper title: A Survey on Learning to Hash
  Publication Date: 2017-05-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 A Summary of Representative Hashing Algorithms with Respect
      to Similarity Preserving Functions, Code Balance, Hash Function Similarity in
      the Coding Space, and the Manner to Handle the sgn Function
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 A Summary of Evaluation Datasets
  Table 3 caption:
    table_text: TABLE 3 A Summary of Query Performance Comparison for Approximate
      Nearest Neighbor Search Under the Euclidean Distance
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2699960
- Affiliation of the first author: center of computational imaging & simulation technologies
    in biomedicine, university of sheffield, sheffield, united kingdom
  Affiliation of the last author: center of computational imaging & simulation technologies
    in biomedicine, university of sheffield, sheffield, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2017\Mixture_of_Probabilistic_Principal_Component_Analyzers_for_Shapes_from_Point_Set\figure_1.jpg
  Figure 1 caption: "Conceptual representation of the proposed generative model with\
    \ J=2 PPCA clusters with L=1 principal modes each. Non-linear shape variation\
    \ (along the green line) is captured in a piece-wise form linear around the local\
    \ means ( \u03BC \xAF j ) using the principal modes ( W j ). The projection of\
    \ a point set X k on each space, \u03BC jk , is a linear combination of \u03BC\
    \ \xAF j (with M model points) and loaded W j 's ( j=1,2 , in this example)."
  Figure 10 Link: articels_figures_by_rev_year\2017\Mixture_of_Probabilistic_Principal_Component_Analyzers_for_Shapes_from_Point_Set\figure_10.jpg
  Figure 10 caption: 'Model evidence versus VB iterations showing convergence and
    maximizing the lower bound for mixtures of: (a) Normal-PH, (b) Noraml-HCM, and
    (c) vertebra point sets.'
  Figure 2 Link: articels_figures_by_rev_year\2017\Mixture_of_Probabilistic_Principal_Component_Analyzers_for_Shapes_from_Point_Set\figure_2.jpg
  Figure 2 caption: The graphical representation of the proposed model; shaded and
    hollow circles represent observed and latent variables, respectively, arrows imply
    the dependencies and plates indicate the number of instances.
  Figure 3 Link: articels_figures_by_rev_year\2017\Mixture_of_Probabilistic_Principal_Component_Analyzers_for_Shapes_from_Point_Set\figure_3.jpg
  Figure 3 caption: 'Clustering and mode estimation of synthetic point sets, color
    coded by their types. (a) Left: Overlay of K=80 point sets generated using M =50
    model points, J =2 clusters, and L =1 variation mode, Right: Overlay of K=120
    point sets sampled from M =50 model points, J =3 clusters, and L =2 variation
    modes; (b) Overlay of the estimated corresponding clustering and variation modes.
    The match of the colors and major structures between (a) and (b), shows a good
    clustering and estimation of principal variation modes.'
  Figure 4 Link: articels_figures_by_rev_year\2017\Mixture_of_Probabilistic_Principal_Component_Analyzers_for_Shapes_from_Point_Set\figure_4.jpg
  Figure 4 caption: 'Short axis MR images from normal (a), PH (b), and HCM patients
    (c). Compared to normal hearts: the RV in the PH patients tend to be larger, and
    LV in the HCM patients appears thicker.'
  Figure 5 Link: articels_figures_by_rev_year\2017\Mixture_of_Probabilistic_Principal_Component_Analyzers_for_Shapes_from_Point_Set\figure_5.jpg
  Figure 5 caption: "Short axis CT images from lumbar for three sample patients. Columns\
    \ (a)-(e) correspond to L1-L5 vertebrae, respectively. Compared to L1-L4 vertebrae,\
    \ the body of the L5 vertebra in column (e) seem more flattered around the pedicles,\
    \ forming \u201Cbat\u201D like structures."
  Figure 6 Link: articels_figures_by_rev_year\2017\Mixture_of_Probabilistic_Principal_Component_Analyzers_for_Shapes_from_Point_Set\figure_6.jpg
  Figure 6 caption: 'Model evidence versus M for different numbers of point sets generated
    in Fig. 3a: (a) K=10 , (b) K=40 , and (c) K=80 samples from mixture of two clusters
    each having one mode of variation; (d) K=30 , (e) K=60 , and (f) K=120 samples
    from mixture of three clusters with two modes of variations. As the number of
    training samples ( K ) increases maximal evidences are attained at M= M and correct
    models are selected.'
  Figure 7 Link: articels_figures_by_rev_year\2017\Mixture_of_Probabilistic_Principal_Component_Analyzers_for_Shapes_from_Point_Set\figure_7.jpg
  Figure 7 caption: 'Model evidence versus M for mixtures of: (a) 55 Normal-PH, (b)
    53 Normal-HCM, and (c) 100 vertebra point sets.'
  Figure 8 Link: articels_figures_by_rev_year\2017\Mixture_of_Probabilistic_Principal_Component_Analyzers_for_Shapes_from_Point_Set\figure_8.jpg
  Figure 8 caption: 'Model evidence versus L and J parameters, evaluated through ten
    fold cross-validations. At each fold, a model was fit to the training data, thus
    obtaining 10 different L values for various mixtures of: (a) Synthetic samples
    obtained with ground truth parameters of L =3 and J =3 , (b) Normal-PH, (c) Normal-HCM,
    and (d) vertebra point sets. The upper charts in (b)-(d) correspond to the lower
    counterparts in a finer scale. Comparing the means (denoted by crosses), it can
    be noticed that maximal model evidences ( L ) happen in the correct underlying
    model in (a), and clinically plausible models in (b)-(d), indicating presence
    of two clusters.'
  Figure 9 Link: articels_figures_by_rev_year\2017\Mixture_of_Probabilistic_Principal_Component_Analyzers_for_Shapes_from_Point_Set\figure_9.jpg
  Figure 9 caption: "Generalization and specificity errors (in [mm] ) of models trained\
    \ through 10-fold cross-validations using point sets from mixtures of: (a) Synthetic\
    \ samples from a model having the ground truth parameters of L , J =3 , (b) Normal-PH,\
    \ (c) Normal-HCM, and (d) vertebrae. In each panel, d , d \u2217 , and d H distances\
    \ are quantified from left to right. For each model, markers and dotted lines\
    \ indicate the corresponding average and rough variability of the errors, respectively.\
    \ The models having maximal evidences (in Fig. 8) are placed competitively close\
    \ to the lower-left corner in each graph, indicating concurrent small generalization\
    \ and specificity errors."
  First author gender probability: 0.94
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ali Gooya
  Name of the last author: Alejandro F. Frangi
  Number of Figures: 13
  Number of Tables: 1
  Number of authors: 4
  Paper title: Mixture of Probabilistic Principal Component Analyzers for Shapes from
    Point Sets
  Publication Date: 2017-05-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Generalization and Specificity Errors (in mm ) for the Methods
      Proposed in [31],[34], and the Selected Models with L=1 , J=2 (Significant Differences
      Are in Bold ( p-Value<0.001 ))
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2700276
- Affiliation of the first author: school of electronic and information engineering,
    south china university of technology, tianhe district, guangzhou, p.r. china
  Affiliation of the last author: ubtech sydney artificial intelligence institute
    and the school of information technologies in the faculty of engineering and information
    technologies, the university of sydney, darlington, nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2017\TrunkBranch_Ensemble_Convolutional_Neural_Networks_for_VideoBased_Face_Recogniti\figure_1.jpg
  Figure 1 caption: Video frames captured by surveillance or mobile devices suffer
    from severe image blur, dramatic pose variations, and occlusion. (a) Image blur
    caused by the motion of the subject, camera shake (for mobile devices), and out-of-focus
    capture. (b) Faces in videos usually exhibit occlusion and a large range of pose
    variations.
  Figure 10 Link: articels_figures_by_rev_year\2017\TrunkBranch_Ensemble_Convolutional_Neural_Networks_for_VideoBased_Face_Recogniti\figure_10.jpg
  Figure 10 caption: ROC curves of the trunk network and TBE-CNN on the handheld set
    of PaSC. (a) Without BN layers; (b) with BN layers.
  Figure 2 Link: articels_figures_by_rev_year\2017\TrunkBranch_Ensemble_Convolutional_Neural_Networks_for_VideoBased_Face_Recogniti\figure_2.jpg
  Figure 2 caption: Examples of the original still face images and simulated video
    frames. (a) Original still images; (b) simulated video frames by applying artificial
    out-of-focus blur (the two figures on the left) and motion blur (the two figures
    on the right).
  Figure 3 Link: articels_figures_by_rev_year\2017\TrunkBranch_Ensemble_Convolutional_Neural_Networks_for_VideoBased_Face_Recogniti\figure_3.jpg
  Figure 3 caption: Model architecture for Trunk-Branch Ensemble CNN (TBE-CNN). Note
    that a max pooling layer is omitted for simplicity following each convolution
    module, e.g., Conv1 and Inception 3. TBE-CNN is composed of one trunk network
    that learns representations for holistic face images and two branch networks that
    learn representations for image patches cropped around facial components. The
    trunk network and the branch networks share the same low- and middle-level layers,
    and they have individual high-level layers. The output feature maps of the trunk
    network and branch networks are fused by concatenation. The output of the last
    fully connected layer is utilized as the final face representation of one video
    frame.
  Figure 4 Link: articels_figures_by_rev_year\2017\TrunkBranch_Ensemble_Convolutional_Neural_Networks_for_VideoBased_Face_Recogniti\figure_4.jpg
  Figure 4 caption: The principle of Mean Distance Regularized Triplet Loss (MDR-TL).
    (a) Triplets sampled in the training batch satisfy the triplet constraint (Eq.
    (4)). However, due to the non-uniform intra- and inter-class distance distributions,
    it is hard to select an ideal threshold for face verification. (b) MDR-TL regularizes
    the triplet loss by setting a margin for the distance between subject mean representations.
  Figure 5 Link: articels_figures_by_rev_year\2017\TrunkBranch_Ensemble_Convolutional_Neural_Networks_for_VideoBased_Face_Recogniti\figure_5.jpg
  Figure 5 caption: Illustration of TBE-CNN training with MDR-TL. MDR-TL is employed
    to further enhance the discriminative power of learnt face representations.
  Figure 6 Link: articels_figures_by_rev_year\2017\TrunkBranch_Ensemble_Convolutional_Neural_Networks_for_VideoBased_Face_Recogniti\figure_6.jpg
  Figure 6 caption: 'Sample video frames after normalization: PaSC (first row), COX
    face (second row), and YouTube faces (third row). For each database, the four
    frames on the left are sampled from a video recorded under relatively good conditions,
    and the four frames on the right are selected from low-quality video.'
  Figure 7 Link: articels_figures_by_rev_year\2017\TrunkBranch_Ensemble_Convolutional_Neural_Networks_for_VideoBased_Face_Recogniti\figure_7.jpg
  Figure 7 caption: ROC curves of the trunk network trained with different types of
    training data on the PaSC database. (a) Control set; (b) handheld set.
  Figure 8 Link: articels_figures_by_rev_year\2017\TrunkBranch_Ensemble_Convolutional_Neural_Networks_for_VideoBased_Face_Recogniti\figure_8.jpg
  Figure 8 caption: ROC curves of MDR-TL and the triplet loss functions on the handheld
    set of PaSC. (a) SI training data; (b) TS training data.
  Figure 9 Link: articels_figures_by_rev_year\2017\TrunkBranch_Ensemble_Convolutional_Neural_Networks_for_VideoBased_Face_Recogniti\figure_9.jpg
  Figure 9 caption: Verification rates (%) at 1 percent FAR by the trunk network and
    TBE-CNN. Comparison is based on the softmax loss. (a) Performance comparison without
    BN layers; (b) performance comparison with BN layers.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Changxing Ding
  Name of the last author: Dacheng Tao
  Number of Figures: 12
  Number of Tables: 9
  Number of authors: 2
  Paper title: Trunk-Branch Ensemble Convolutional Neural Networks for Video-Based
    Face Recognition
  Publication Date: 2017-05-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Trunk Network Parameters (GoogLeNet)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Verification Rates (%) at 1% FAR on PaSC with Different Types
      of Training Data
  Table 3 caption:
    table_text: TABLE 3 Verification Rates (%) at 1% FAR with Different Loss Functions
      on PaSC (Semi-Hard Negative Mining)
  Table 4 caption:
    table_text: TABLE 4 Verification Rates (%) at 1% FAR with Different Loss Functions
      on PaSC (Hard Negative Mining)
  Table 5 caption:
    table_text: TABLE 5 Verification Rates (%) at 1% FAR with Different Loss Functions
      on PaSC (Hardest Negative Mining)
  Table 6 caption:
    table_text: TABLE 6 Verification Rates (%) at 1% FAR of Different Methods on PaSC
  Table 7 caption:
    table_text: TABLE 7 Rank-1 Identification Rates (%) Under the V2SS2V Settings
      for Different Methods on the COX Face Database
  Table 8 caption:
    table_text: TABLE 8 Rank-1 Identification Rates (%) Under the V2V Setting for
      Different Methods on the COX Face Database
  Table 9 caption:
    table_text: TABLE 9 Mean Verification Accuracy on the YouTube Faces Database (Restricted
      Protocol)
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2700390
- Affiliation of the first author: school of computer science and engineering, nanjing
    university of science and technology, nanjing, china
  Affiliation of the last author: "max planck institute for informatics, saarbr\xFC\
    cken, germany"
  Figure 1 Link: articels_figures_by_rev_year\2017\Towards_Reaching_Human_Performance_in_Pedestrian_Detection\figure_1.jpg
  Figure 1 caption: "Overview of the top results on the Caltech-USA pedestrian benchmark.\
    \ At \u223C95 percent recall, state-of-the-art detectors make ten times more errors\
    \ than the human baseline."
  Figure 10 Link: articels_figures_by_rev_year\2017\Towards_Reaching_Human_Performance_in_Pedestrian_Detection\figure_10.jpg
  Figure 10 caption: Oracle cases evaluation over Caltech test set. Both localisation
    and background-versus-foreground show important room for improvement.
  Figure 2 Link: articels_figures_by_rev_year\2017\Towards_Reaching_Human_Performance_in_Pedestrian_Detection\figure_2.jpg
  Figure 2 caption: Comparison of filters between some filtered channels detector
    variants.
  Figure 3 Link: articels_figures_by_rev_year\2017\Towards_Reaching_Human_Performance_in_Pedestrian_Detection\figure_3.jpg
  Figure 3 caption: Illustration of bounding box generation for human baseline. The
    annotator only needs to draw a line from the top of the head to the central point
    between both feet, a tight bounding box is then automatically generated.
  Figure 4 Link: articels_figures_by_rev_year\2017\Towards_Reaching_Human_Performance_in_Pedestrian_Detection\figure_4.jpg
  Figure 4 caption: Detection quality (log-average miss rate) for different test set
    subsets. Each group shows the human baseline, the Checkerboards [53] and RotatedFilters
    detectors, as well as the next top three (unspecified) methods (different for
    each setting).
  Figure 5 Link: articels_figures_by_rev_year\2017\Towards_Reaching_Human_Performance_in_Pedestrian_Detection\figure_5.jpg
  Figure 5 caption: Error sources of Checkerboards [53] on the Caltech test set.
  Figure 6 Link: articels_figures_by_rev_year\2017\Towards_Reaching_Human_Performance_in_Pedestrian_Detection\figure_6.jpg
  Figure 6 caption: Example false positives (Checkerboards) from different sources.
    False positives in red, original annotations in blue, ignore annotations in dashed
    blue and true positives in green.
  Figure 7 Link: articels_figures_by_rev_year\2017\Towards_Reaching_Human_Performance_in_Pedestrian_Detection\figure_7.jpg
  Figure 7 caption: Examples for images with different levels of contrastblur. The
    number on top of each image indicates the contrastblur measure.
  Figure 8 Link: articels_figures_by_rev_year\2017\Towards_Reaching_Human_Performance_in_Pedestrian_Detection\figure_8.jpg
  Figure 8 caption: Correlation between sizecontrastblur and score.
  Figure 9 Link: articels_figures_by_rev_year\2017\Towards_Reaching_Human_Performance_in_Pedestrian_Detection\figure_9.jpg
  Figure 9 caption: False positive sources for different datasets and detectors.
  First author gender probability: 0.95
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Shanshan Zhang
  Name of the last author: Bernt Schiele
  Number of Figures: 19
  Number of Tables: 10
  Number of authors: 5
  Paper title: Towards Reaching Human Performance in Pedestrian Detection
  Publication Date: 2017-05-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Convnet Detectors (Sorted from Low to High Detection
      Quality on the Caltech-USA Benchmark)
  Table 10 caption:
    table_text: TABLE 10 Comparison of Runtime versus Performance for Different Detectors
      on the Caltech-USA Benchmark
  Table 2 caption:
    table_text: TABLE 2 Comparison of Pedestrian Benchmarks (Numbers are on Test Set
      Only)
  Table 3 caption:
    table_text: TABLE 3 The Filter Type Determines the ICF Methods Quality
  Table 4 caption:
    table_text: TABLE 4 Detection Quality Gain of Adding Context [32] and Optical
      Flow [34], as Function of the Base Detector
  Table 5 caption:
    table_text: TABLE 5 Median IoU of True Positives for Detectors Trained on Different
      Data, Evaluated on Original and New Caltech Test
  Table 6 caption:
    table_text: "TABLE 6 Effects of Different Training Annotations on Detection Quality\
      \ on Validation Set ( 1\xD7 Training Set)"
  Table 7 caption:
    table_text: TABLE 7 Detection Quality of RotatedFilters on Test Set When Using
      Different Aligned Training Sets
  Table 8 caption:
    table_text: TABLE 8 Detection Quality of Convnets with Different Proposals
  Table 9 caption:
    table_text: TABLE 9 Step by Step Improvements from Previous Best Method Checkerboards
      to RotatedFilters-New10x+VGG
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2700460
- Affiliation of the first author: department of computer science and engineering,
    seoul national university, seoul, korea
  Affiliation of the last author: department of computer science and engineering,
    seoul national university, seoul, korea
  Figure 1 Link: articels_figures_by_rev_year\2017\Retrieval_of_Sentence_Sequences_for_an_Image_Stream_via_Coherence_Recurrent_Conv\figure_1.jpg
  Figure 1 caption: An intuition of our problem statement with an Australia example.
    We aim to express an image stream with a sequence of natural sentences. (a) We
    leverage natural blog posts to learn the relation between image streams and sentence
    sequences. (b) We propose coherence recurrent convolutional networks (CRCN) that
    integrate convolutional networks, bidirectional recurrent networks, and the entity-based
    coherence model. Once we learn the CRCN model with blog posts, the model retrieves
    fluent and coherent sequences of sentences for a novel image stream.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Retrieval_of_Sentence_Sequences_for_an_Image_Stream_via_Coherence_Recurrent_Conv\figure_2.jpg
  Figure 2 caption: Illustration of (a) pre-processing steps of blog posts, and (b)
    the proposed CRCN architecture. (a) For each blog post, we perform text segmentation
    and summarization to associate a single sentence to each image ( Section 3.2).
    We then extract the paragraph vector p t and a parse tree Z t ( Section 3.3).
    (b) The BLSTM model captures the flow of text content from sentence descriptors
    p t ( Section 4.1). The local coherence model represents the coherence of a passage
    from parse trees Z t (Section 4.2). The output of the BLSTM and the coherence
    models are mixed via two fully-connected layers (Section 4.3). Finally, a compatibility
    score between a pair of image and sentence sequences is computed ( Section 4.4).
  Figure 3 Link: articels_figures_by_rev_year\2017\Retrieval_of_Sentence_Sequences_for_an_Image_Stream_via_Coherence_Recurrent_Conv\figure_3.jpg
  Figure 3 caption: 'An illustrative example of how the entity-based coherence representation
    is extracted from the sentences associated with a photo stream. (a) A photo stream
    and summarization of each text segment per photo into a key sentence using LSA-based
    summarization method. (b) Extraction of a parse tree for each sentence. (c) The
    entity grid with the sentences. Each grid cell indicates a grammatical role: subjects
    (S), objects (O), or neither (X). (d) The final coherence feature with a dimension
    of 64(= 4 3 ) , where there are four entity categories and three transition lengths.'
  Figure 4 Link: articels_figures_by_rev_year\2017\Retrieval_of_Sentence_Sequences_for_an_Image_Stream_via_Coherence_Recurrent_Conv\figure_4.jpg
  Figure 4 caption: 'Comparison of training losses (left) and validation losses (right)
    between the variants of our model CRCN+ over the first 45 epochs on the NYC dataset.
    We plot the sum of losses within a batch for every five epochs. The variants of
    our model are as follows. BRNN: The original CRCN model with BRNN proposed in
    our previous paper [16]. BRNN+L2: The original CRCN with L2 regularization. BRNN+ReLU+L2:
    The original CRCN with L2 regularization and ReLU activation in the FC layers.
    BLSTM+L2: The CRCN with the BLSTM instead of the BRNN, and L2 regularization.
    BLSTM+ReLU+L2: The CRCN with the BLSTM, L2 regularization, and ReLU activation
    in the FC layers.'
  Figure 5 Link: articels_figures_by_rev_year\2017\Retrieval_of_Sentence_Sequences_for_an_Image_Stream_via_Coherence_Recurrent_Conv\figure_5.jpg
  Figure 5 caption: "Examples of sentence sequence retrieval for the topics of (a\u2013\
    b) NYC and (c\u2013d) Disneyland. In each set, we present a part of a query image\
    \ stream (i.e., five uniformly sampled images), and its corresponding text output\
    \ by our method and baselines."
  Figure 6 Link: articels_figures_by_rev_year\2017\Retrieval_of_Sentence_Sequences_for_an_Image_Stream_via_Coherence_Recurrent_Conv\figure_6.jpg
  Figure 6 caption: "Examples of sentence sequence retrieval for the topics of (a\u2013\
    b) Australia and (c\u2013d) Hawaii. In each set, we present a part of a query\
    \ image stream (i.e., five uniformly sampled images), and its corresponding text\
    \ output by our method and baselines."
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.75
  Name of the first author: Cesc Chunseong Park
  Name of the last author: Gunhee Kim
  Number of Figures: 6
  Number of Tables: 4
  Number of authors: 3
  Paper title: Retrieval of Sentence Sequences for an Image Stream via Coherence Recurrent
    Convolutional Networks
  Publication Date: 2017-05-02 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Statistics of Our Blog Datasets: The Numbers of Unique Posts
      and Associated Images for Four Tourism Topics'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Evaluation of Sentence Generation with Language Similarity
      Metrics (BLEU, CIDEr, METEOR) and Retrieval Metrics (RK, Median Rank) for NYC,
      Disneyland, Australia, and Hawaii
  Table 3 caption:
    table_text: TABLE 3 The Results of AMT Pairwise Preference Tests for the Two Datasets
      of NYC and Disneyland
  Table 4 caption:
    table_text: TABLE 4 The Results of AMT Pairwise Preference Tests for the Two Datasets
      of NYC and Disneyland
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2700381
- Affiliation of the first author: "computer vision laboratory, ethz, z\xFCrich, switzerland"
  Affiliation of the last author: "computer vision laboratory, ethz, z\xFCrich, switzerland"
  Figure 1 Link: articels_figures_by_rev_year\2017\Convolutional_Oriented_Boundaries_From_Image_Segmentation_to_HighLevel_Tasks\figure_1.jpg
  Figure 1 caption: 'Overview of COB: From a single pass of a base CNN, we obtain
    multiscale oriented contours. We combine them to build Ultrametric Contour Maps
    (UCMs) at different scales and fuse them into a single hierarchical segmentation
    structure.'
  Figure 10 Link: articels_figures_by_rev_year\2017\Convolutional_Oriented_Boundaries_From_Image_Segmentation_to_HighLevel_Tasks\figure_10.jpg
  Figure 10 caption: 'Qualitative results for object boundaries. Row 1: Original images,
    Row 2: Generic image segmentation results, Row 3: Object boundary results.'
  Figure 2 Link: articels_figures_by_rev_year\2017\Convolutional_Oriented_Boundaries_From_Image_Segmentation_to_HighLevel_Tasks\figure_2.jpg
  Figure 2 caption: Our deep learning architecture (best viewed in color). The connections
    show the different stages that are used to generate the multiscale contours. Orientations
    further require additional convolutional layers in multiple stages of the network.
  Figure 3 Link: articels_figures_by_rev_year\2017\Convolutional_Oriented_Boundaries_From_Image_Segmentation_to_HighLevel_Tasks\figure_3.jpg
  Figure 3 caption: 'Illustration of contour orientation learning. Row 1 shows the
    responses B k for 4 out of the 8 orientation bins. Row 2, from left to right:
    Original image, contour strength, learned orientation map into 8 orientations,
    and hierarchical boundaries.'
  Figure 4 Link: articels_figures_by_rev_year\2017\Convolutional_Oriented_Boundaries_From_Image_Segmentation_to_HighLevel_Tasks\figure_4.jpg
  Figure 4 caption: 'Image Partition Representation: (a) Pixel labeling, each pixel
    gets assigned a region label. (b) Boundary grid, markers of the boundary positions.
    (c) Sparse boundaries, lists of boundary coordinates between neighboring regions.'
  Figure 5 Link: articels_figures_by_rev_year\2017\Convolutional_Oriented_Boundaries_From_Image_Segmentation_to_HighLevel_Tasks\figure_5.jpg
  Figure 5 caption: 'Polygon simplification: From all boundary points (left) to simplified
    polygons (right), which are used to compute the ground-truth orientation robustly.'
  Figure 6 Link: articels_figures_by_rev_year\2017\Convolutional_Oriented_Boundaries_From_Image_Segmentation_to_HighLevel_Tasks\figure_6.jpg
  Figure 6 caption: 'Contour orientation: Classification accuracy into 8 bins.'
  Figure 7 Link: articels_figures_by_rev_year\2017\Convolutional_Oriented_Boundaries_From_Image_Segmentation_to_HighLevel_Tasks\figure_7.jpg
  Figure 7 caption: 'PASCAL context VOC test evaluation: Precision-recall curves for
    evaluation of boundaries ( Fb [25]), and regions ( Fop [45]). Open contour methods
    in dashed lines and closed boundaries (from segmentation) in solid lines. ODS,
    OIS, and AP summary measures. Markers indicate the optimal operating point, where
    Fb and Fop are maximized.'
  Figure 8 Link: articels_figures_by_rev_year\2017\Convolutional_Oriented_Boundaries_From_Image_Segmentation_to_HighLevel_Tasks\figure_8.jpg
  Figure 8 caption: 'BSDS500 test evaluation: Precision-recall curves for evaluation
    of boundaries ( Fb [25]), and regions ( Fop [45]).'
  Figure 9 Link: articels_figures_by_rev_year\2017\Convolutional_Oriented_Boundaries_From_Image_Segmentation_to_HighLevel_Tasks\figure_9.jpg
  Figure 9 caption: 'Qualitative results on PASCAL - hierarchical regions. Row 1:
    Original images, Row 2: Ground-truth boundaries, Row 3: Hierarchical regions with
    COB.'
  First author gender probability: 0
  Gender of the first author: Not Available
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Kevis-Kokitsi Maninis
  Name of the last author: Luc Van Gool
  Number of Figures: 16
  Number of Tables: 7
  Number of authors: 4
  Paper title: 'Convolutional Oriented Boundaries: From Image Segmentation to High-Level
    Tasks'
  Publication Date: 2017-05-02 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Datasets and Parameters: The List of Databases Used to Evaluate
      Our Approach on Various Low-Level and High-Level Tasks'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Ablation Analysis on VOC val: Comparison of Different Ablated
      and Baseline Versions of Our System'
  Table 3 caption:
    table_text: 'TABLE 3 Timing Experiments: Comparing Our Approach to Different Baselines'
  Table 4 caption:
    table_text: 'TABLE 4 SBD val Evaluation: Semantic Contours Results: Maximal Fb
      per Class and Mean Maximal F b Is Reported for All Methods'
  Table 5 caption:
    table_text: 'TABLE 5 SBD val Evaluation: Semantic Contours Results: Average Precision
      (AP) per Class and Mean AP (mAP) Is Reported for All Methods'
  Table 6 caption:
    table_text: 'TABLE 6 PASCAL VOC Segmentation val Evaluation: Effect of COB on
      Semantic Segmentation'
  Table 7 caption:
    table_text: 'TABLE 7 VOC 2007 Test Evaluation: Object Detection Performance (mAP)
      of Fast-RCNN [70], Using Object Proposals from [60] (Original) or COB'
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2700300
- Affiliation of the first author: visual geometry and modelling (vgm) lab, istituto
    italiano di tecnologia, genova, italy
  Affiliation of the last author: visual geometry and modelling (vgm) lab, istituto
    italiano di tecnologia, genova, italy
  Figure 1 Link: articels_figures_by_rev_year\2017\D_Object_Localisation_from_MultiView_Image_Detections\figure_1.jpg
  Figure 1 caption: Example of a set of images of a given 3D rigid scene taken from
    a camera at different viewpoints. The problem consists in recovering the 3D occupancy
    of each object given the 2D bounding boxes detected at each image frame.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\D_Object_Localisation_from_MultiView_Image_Detections\figure_2.jpg
  Figure 2 caption: Example of bounding boxes (yellow) and corresponding fitted ellipses
    (red) for a set of objects.
  Figure 3 Link: articels_figures_by_rev_year\2017\D_Object_Localisation_from_MultiView_Image_Detections\figure_3.jpg
  Figure 3 caption: Example of a set of conics textstyle tt C if , textstyle tt C
    i(f+1) and textstyle tt C i(f+2) which ?> represents the outlines in three frames
    of a given quadric textstyle tt Q i .
  Figure 4 Link: articels_figures_by_rev_year\2017\D_Object_Localisation_from_MultiView_Image_Detections\figure_4.jpg
  Figure 4 caption: "Results for the synthetic tests versus different types of errors.\
    \ Average accuracy given by LfD or LfD+NL for RE\u2014rotation error (a), SE\u2014\
    size error (b), TE\u2014translation error (c), measured by O scriptstyle tt 3D\
    \ metric."
  Figure 5 Link: articels_figures_by_rev_year\2017\D_Object_Localisation_from_MultiView_Image_Detections\figure_5.jpg
  Figure 5 caption: Results for the ACCV (first row) and TUW (second row) datasets.
    The first three columns show a close up of the views with the output of a generic
    object detector (yellow bounding box) and projections of GT and estimated ellipsoids
    (blue and green ellipses respectively). The last column shows the cloud points
    of the object (red), GT ellipsoid (blue) and estimated ellipsoid (green).
  Figure 6 Link: articels_figures_by_rev_year\2017\D_Object_Localisation_from_MultiView_Image_Detections\figure_6.jpg
  Figure 6 caption: Results for the ACCV and TUW sequences using the CP [35] (top
    images) and the IA [36] (bottom images) methods. In the figures we show the GT
    point clouds of the objects (red) and the estimated polyhedrons for the different
    objects. It can be noticed a tendency of both the approaches to overestimate the
    occupancy of the objects. This is more evident in the TUW dataset, where the IA
    fails the estimation.
  Figure 7 Link: articels_figures_by_rev_year\2017\D_Object_Localisation_from_MultiView_Image_Detections\figure_7.jpg
  Figure 7 caption: Results for the five sequences of KITTI dataset. The left and
    centre images show a close up of the views with the output of a generic object
    detector (yellow bounding box) and projections of GT and estimated ellipsoids
    (blue and green ellipses respectively). The right images display the GT ellipsoids
    (blue) and estimated ellipsoids (green) in 3D.
  Figure 8 Link: articels_figures_by_rev_year\2017\D_Object_Localisation_from_MultiView_Image_Detections\figure_8.jpg
  Figure 8 caption: Results for the indoor (a) and outdoor (b) sequences of the TANGO
    dataset. In (a) and (b) the top images show two frames with the output of a generic
    object detector (yellow bounding box) and projections the estimated ellipsoids
    (green ellipses). The bottom images display the estimated ellipsoids (green),
    the 3D mesh vertexes provided by Tango and the registered 3D CADs of the objects.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Cosimo Rubino
  Name of the last author: Alessio Del Bue
  Number of Figures: 8
  Number of Tables: 10
  Number of authors: 3
  Paper title: 3D Object Localisation from Multi-View Image Detections
  Publication Date: 2017-05-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 O 3D for Sequences from ACCV Dataset for LfD, LfD c and Baseline
  Table 10 caption:
    table_text: TABLE 10 O 3D for the Sequences from the TANGO Dataset
  Table 2 caption:
    table_text: TABLE 2 O 3D for the Sequences from TUW Dataset
  Table 3 caption:
    table_text: TABLE 3 O 3D for the Sequences from the KITTI Dataset
  Table 4 caption:
    table_text: TABLE 4 Number of Objects Detected from Each Algorithm and Each Sequence
  Table 5 caption:
    table_text: TABLE 5 Percentages of Estimated Centroids within 1 m or 2 m w.r.t.
      GT Centroids for the Six Sequences of the KITTI Dataset
  Table 6 caption:
    table_text: "TABLE 6 \u03B8 err for the Sequences from the KITTI Dataset (in Radiants)"
  Table 7 caption:
    table_text: TABLE 7 Computation Time (in Seconds) for Each Algorithm in Case of
      the Sequence 9
  Table 8 caption:
    table_text: TABLE 8 Average O 3D in Case of Minimal Configurations for the Sequences
      from ACCV, TUW and KITTI Dataset
  Table 9 caption:
    table_text: TABLE 9 Percentages of Estimated Centroids within 1 m or 2 m w.r.t.
      GT Centroids for the Five Sequences of the KITTI Dataset in Case of Minimal
      Configurations
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2701373
- Affiliation of the first author: center for research on intelligent perception and
    computing (cripac), university of chinese academy of sciences (ucas), huairou,
    beijing, china
  Affiliation of the last author: center for excellence in brain science and intelligence
    technology (cebsit), chinese academy of sciences (casia), beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2017\Video_SuperResolution_via_Bidirectional_Recurrent_Convolutional_Networks\figure_1.jpg
  Figure 1 caption: The proposed bidirectional recurrent convolutional network (BRCN).
  Figure 10 Link: articels_figures_by_rev_year\2017\Video_SuperResolution_via_Bidirectional_Recurrent_Convolutional_Networks\figure_10.jpg
  Figure 10 caption: Visualization of learned filters in the first hidden and output
    layers of the proposed BRCN, with an upscaling factor of 2.
  Figure 2 Link: articels_figures_by_rev_year\2017\Video_SuperResolution_via_Bidirectional_Recurrent_Convolutional_Networks\figure_2.jpg
  Figure 2 caption: The details of 3D feedforward and recurrent convolutions (best
    viewed in colors).
  Figure 3 Link: articels_figures_by_rev_year\2017\Video_SuperResolution_via_Bidirectional_Recurrent_Convolutional_Networks\figure_3.jpg
  Figure 3 caption: "3D feedforward convolution as local full connection (filter size:\
    \ 1 \xD71\xD7 3)."
  Figure 4 Link: articels_figures_by_rev_year\2017\Video_SuperResolution_via_Bidirectional_Recurrent_Convolutional_Networks\figure_4.jpg
  Figure 4 caption: Comparison between TRBM and the proposed BRCN. For both models,
    we just show details at two timesteps for clear illustration.
  Figure 5 Link: articels_figures_by_rev_year\2017\Video_SuperResolution_via_Bidirectional_Recurrent_Convolutional_Networks\figure_5.jpg
  Figure 5 caption: Closeup comparison among original frames and super resolved results
    by Bicubic, ANR, SR-CNN and BRCN, respectively, on the Set1 dataset with an upscaling
    factor of 4.
  Figure 6 Link: articels_figures_by_rev_year\2017\Video_SuperResolution_via_Bidirectional_Recurrent_Convolutional_Networks\figure_6.jpg
  Figure 6 caption: Closeup comparison among original frames and super resolved results
    by Bicubic, ANR, SR-CNN and BRCN, respectively, on the Set2 dataset with an upscaling
    factor of 2.
  Figure 7 Link: articels_figures_by_rev_year\2017\Video_SuperResolution_via_Bidirectional_Recurrent_Convolutional_Networks\figure_7.jpg
  Figure 7 caption: Time versus PSNR for all the methods.
  Figure 8 Link: articels_figures_by_rev_year\2017\Video_SuperResolution_via_Bidirectional_Recurrent_Convolutional_Networks\figure_8.jpg
  Figure 8 caption: Visual comparison among input low-resolution frames and super
    resolved results by Bicubic, FUS, Enhancer, DeepSR and BRCN, respectively, on
    the Set2 dataset with an upscaling factor of 4.
  Figure 9 Link: articels_figures_by_rev_year\2017\Video_SuperResolution_via_Bidirectional_Recurrent_Convolutional_Networks\figure_9.jpg
  Figure 9 caption: Visualization of learned filters in the first hidden and output
    layers of the proposed BRCN, with an upscaling factor of 4.
  First author gender probability: 0.76
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Yan Huang
  Name of the last author: Liang Wang
  Number of Figures: 11
  Number of Tables: 5
  Number of authors: 3
  Paper title: Video Super-Resolution via Bidirectional Recurrent Convolutional Networks
  Publication Date: 2017-05-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Results of PSNR (dB) and Time (sec) by the Proposed Model
      with Different Filter Sizes of Recurrent Convolution (Filter Size) and Temporal
      Lengths of Training Volume (Length)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Results of PSNR (dB) and Time (sec) by the Proposed Model
      with Different Directions of Network (Direction), Model Sizes (Size), and Temporal
      Steps of 3D Feedforward Convolution (Step)
  Table 3 caption:
    table_text: TABLE 3 The Results of PSNR, SSIM and Time by All the Methods on the
      Set1 Dataset with Two Upscaling Factors of 2 and 4
  Table 4 caption:
    table_text: TABLE 4 The Results of PSNR, SSIM and Time by All the Methods on the
      Set2 Dataset with Two Upscaling Factors of 2 and 4
  Table 5 caption:
    table_text: TABLE 5 The Results of PSNR and SSIM for Color Video SR on the Set2
      Dataset with an Upscaling Factor of 4
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2701380
- Affiliation of the first author: faculty of engineering and information technology,
    university of technology sydney, ultimo, nsw, australia
  Affiliation of the last author: ubtech sydney artificial intelligence institute
    and the school of information technologies in the faculty of engineering and information
    technologies at the university of sydney, darlington, nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2017\Shakeout_A_New_Approach_to_Regularized_Deep_Neural_Network_Training\figure_1.jpg
  Figure 1 caption: "Comparison between shakeout and dropout operations. This figure\
    \ shows how shakeout and dropout are applied to the weights in a linear module.\
    \ In the original linear module, the output is the summation of the inputs x weighted\
    \ by w , while for dropout and shakeout, the weights w are first randomly modified.\
    \ In detail, a random switch r controls how each w is modified. The manipulation\
    \ of w is illustrated within the amplifier icons (the red curves, best seen with\
    \ colors). The coefficients are \u03B1=1(1\u2212\u03C4) and \u03B2(w)=cs(w) ,\
    \ where s(w) extracts the sign of w and c>0 , \u03C4\u2208[0,1] . Note the sign\
    \ of \u03B2(w) is always the same as that of w . The magnitudes of coefficients\
    \ \u03B1 and \u03B2(w) are determined by the shakeout hyper-parameters \u03C4\
    \ and c . Dropout can be viewed as a special case of shakeout when c=0 because\
    \ \u03B2(w) is zero at this circumstance."
  Figure 10 Link: articels_figures_by_rev_year\2017\Shakeout_A_New_Approach_to_Regularized_Deep_Neural_Network_Training\figure_10.jpg
  Figure 10 caption: The value of -V(D,G) as a function of iteration for the training
    process of DCGAN. DCGANs are trained using standard BP, dropout and shakeout for
    comparison. Dropout or shakeout is applied on the discriminator of GAN.
  Figure 2 Link: articels_figures_by_rev_year\2017\Shakeout_A_New_Approach_to_Regularized_Deep_Neural_Network_Training\figure_2.jpg
  Figure 2 caption: Regularization effect as a function of a single weight when other
    weights are fixed to zeros for logistic regression model. The corresponding feature
    x is fixed at 1.
  Figure 3 Link: articels_figures_by_rev_year\2017\Shakeout_A_New_Approach_to_Regularized_Deep_Neural_Network_Training\figure_3.jpg
  Figure 3 caption: The contour plots of the regularization effect induced by Shakeout
    in 2D weight space with input x=[1,1 ] T . Note that Dropout is a special case
    of Shakeout with c=0 .
  Figure 4 Link: articels_figures_by_rev_year\2017\Shakeout_A_New_Approach_to_Regularized_Deep_Neural_Network_Training\figure_4.jpg
  Figure 4 caption: Distributions of the weights of the autoencoder models learned
    by different training approaches. Each curve in the figure shows the frequencies
    of the weights of an autoencoder taking particular values, i.e., the empirical
    population densities of the weights. The five curves correspond to five autoencoders
    learned by standard back-propagation, dropout ( tau =0.5 ), gaussian dropout (
    sigma 2=1 ) and shakeout ( tau =0.5 , c=lbrace 1,10rbrace ). The sparsity of the
    weights obtained via shakeout can be seen by comparing the curves.
  Figure 5 Link: articels_figures_by_rev_year\2017\Shakeout_A_New_Approach_to_Regularized_Deep_Neural_Network_Training\figure_5.jpg
  Figure 5 caption: Features captured by the hidden units of the autoencoder models
    learned by different training methods. The features captured by a hidden unit
    are represented by a group of weights that connect the image pixels with this
    corresponding hidden unit. One image patch in a sub-graph corresponds to the features
    captured by one hidden unit.
  Figure 6 Link: articels_figures_by_rev_year\2017\Shakeout_A_New_Approach_to_Regularized_Deep_Neural_Network_Training\figure_6.jpg
  Figure 6 caption: Classification of two kinds of neural networks on MNIST using
    training sets of different sizes. The curves show the performances of the models
    trained by standard BP, and those by dropout and shakeout applied on the hidden
    units of the fully-connected layer.
  Figure 7 Link: articels_figures_by_rev_year\2017\Shakeout_A_New_Approach_to_Regularized_Deep_Neural_Network_Training\figure_7.jpg
  Figure 7 caption: Classification on CIFAR-10 using training sets of different sizes.
    The curves show the performances of the models trained by standard BP, and those
    by dropout and shakeout applied on the hidden units of the fully-connected layer.
  Figure 8 Link: articels_figures_by_rev_year\2017\Shakeout_A_New_Approach_to_Regularized_Deep_Neural_Network_Training\figure_8.jpg
  Figure 8 caption: Comparison of the distributions of the magnitude of weights trained
    by dropout and shakeout. The experiments are conducted using AlexNet on ImageNet-2012
    dataset. Shakeout or dropout is applied on the last two fully-connected layers,
    i.e. FC7 layer and FC8 layer.
  Figure 9 Link: articels_figures_by_rev_year\2017\Shakeout_A_New_Approach_to_Regularized_Deep_Neural_Network_Training\figure_9.jpg
  Figure 9 caption: Distributions of the maximum magnitude of the weights connected
    to the same input unit of a layer. The maximum magnitude of the weights connected
    to one input unit can be regarded as a metric of the importance of that unit.
    The experiments are conducted using AlexNet on ImageNet-2012 dataset. For shakeout,
    the units can be approximately separated into two groups and the one around zero
    is less important than the other, whereas for dropout, the units are more concentrated.
  First author gender probability: 0.83
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Guoliang Kang
  Name of the last author: Dacheng Tao
  Number of Figures: 13
  Number of Tables: 5
  Number of authors: 3
  Paper title: 'Shakeout: A New Approach to Regularized Deep Neural Network Training'
  Publication Date: 2017-05-05 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Architecture of Convolutional Neural Network Adopted for
      MNIST Classification Experiment
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Classification on MNIST Using Training Sets of Different
      Sizes: Fully-Connected Neural Network'
  Table 3 caption:
    table_text: 'TABLE 3 Classification on MNIST Using Training Sets of Different
      Sizes: Convolutional Neural Network'
  Table 4 caption:
    table_text: 'TABLE 4 Classification on CIFAR-10 Using Training Sets of Different
      Sizes: AlexFastNet'
  Table 5 caption:
    table_text: 'TABLE 5 Classification on CIFAR-10 Using Training Sets of Different
      Sizes: WRN-16-4'
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2701831
- Affiliation of the first author: department of brain and cognitive engineering,
    korea university, anam-dong, seongbuk-ku, seoul, korea
  Affiliation of the last author: department of brain and cognitive engineering, korea
    university, anam-dong, seongbuk-ku, seoul, korea
  Figure 1 Link: articels_figures_by_rev_year\2017\A_Novel_LineletBased_Representation_for_Line_Segment_Detection\figure_1.jpg
  Figure 1 caption: Illustration of how a line segment is represented in a digital
    image. At first glance, it is merely a set of connected pixels (a). However, when
    we look in more details, there are several intrinsic properties such as a stair-like
    shape of horizontal chunks (b), gradient magnitude pattern (c), and gradient orientation
    pattern (d) (see Section 2.2 for more details). Note that subfigures (b), (c),
    and (d) are magnified from the green rectangle of (a) (best seen in color).
  Figure 10 Link: articels_figures_by_rev_year\2017\A_Novel_LineletBased_Representation_for_Line_Segment_Detection\figure_10.jpg
  Figure 10 caption: Screenshots of our annotation system. Annotators can use a zoom
    in or out function when line segments are densely located (top) or ambiguous from
    the original resolution due to blur effect or small gradient changes (bottom)
    (best seen in color).
  Figure 2 Link: articels_figures_by_rev_year\2017\A_Novel_LineletBased_Representation_for_Line_Segment_Detection\figure_2.jpg
  Figure 2 caption: Line segment detection comparison on a challenging situation where
    a facade and tree exist together. Edge-based methods (HT and PPHT) generate a
    number of false positives on the tree which has a complex edge response. They
    also fail to detect lines on the facade where edge response is not significant.
    In the meanwhile, local gradient-based methods (LSD and EDLines) successfully
    reject false positives on the tree by exploiting the Helmholtz principle for a
    validation strategy, which also leads to rejecting true positives on the facade.
    The proposed method (Linelet), however, gives a better result compared to others;
    it detects most of lines on the facade and only a few number of false positives
    on the tree.
  Figure 3 Link: articels_figures_by_rev_year\2017\A_Novel_LineletBased_Representation_for_Line_Segment_Detection\figure_3.jpg
  Figure 3 caption: Illustration of the proposed framework. First, linelets are detected
    horizontally and vertically ( Section 4.1). Next, neighboring linelets are grouped
    using their properties ( Section 2.2). The angle is estimated using a probabilistic
    model for lines ( Section 4.2), then detections are validated to judge whether
    they are true positives, target lines in the image, using a model which uses cues
    from the gradient magnitude and orientation ( Section 3). Finally, an aggregation
    process is conducted to further improve the results by connecting line segments
    that are likely from the same line segment ( Section 4.3) (best seen in color).
  Figure 4 Link: articels_figures_by_rev_year\2017\A_Novel_LineletBased_Representation_for_Line_Segment_Detection\figure_4.jpg
  Figure 4 caption: Illustration of four lines with different angle (slope). The lengths,
    of connected pixels, are of size 6, 3, 2, and 1 respectively, according to the
    angle (1).
  Figure 5 Link: articels_figures_by_rev_year\2017\A_Novel_LineletBased_Representation_for_Line_Segment_Detection\figure_5.jpg
  Figure 5 caption: "Illustration of two subsets of lines considered in this paper.\
    \ For convenience of notation, we assume that the range of angle \u03B8 in image\
    \ plane is from \u2212 \u03C0 4 to 3\u03C0 4 . Additionally, according to the\
    \ angle \u03B8 , lines are divided into two subsets: horizontal lines for \u03B8\
    \u2208[\u2212 \u03C0 4 , \u03C0 4 ) and vertical lines for \u03B8\u2208[ \u03C0\
    \ 4 , 3\u03C0 4 ) ."
  Figure 6 Link: articels_figures_by_rev_year\2017\A_Novel_LineletBased_Representation_for_Line_Segment_Detection\figure_6.jpg
  Figure 6 caption: Illustration of the graphical model (b) and its probability distribution
    (c) of the detected line segment (a).
  Figure 7 Link: articels_figures_by_rev_year\2017\A_Novel_LineletBased_Representation_for_Line_Segment_Detection\figure_7.jpg
  Figure 7 caption: Illustration of validation results on the image 22. For each detected
    line segment, the probability of being true positive is colored, from zero (dark
    red) to one (white). Note that false positives detected from the tree at the right
    side (B) have low ( < 0.5 ) probability, and then are rejected. In the meanwhile,
    line segments detected from true lines (A) have high ( geq 0.5 ) probability (best
    seen in color).
  Figure 8 Link: articels_figures_by_rev_year\2017\A_Novel_LineletBased_Representation_for_Line_Segment_Detection\figure_8.jpg
  Figure 8 caption: Illustration of the linelet detection process. (From left to right)
    an input image, the gradient magnitude, non-maximal suppression ( NMS(Gmag, primeprimetextrm
    verticalprime) ) result, and linelets detected (and color coded). Note that, in
    the gradient magnitude, dark color means low value and bright color high value
    (best seen in color).
  Figure 9 Link: articels_figures_by_rev_year\2017\A_Novel_LineletBased_Representation_for_Line_Segment_Detection\figure_9.jpg
  Figure 9 caption: An example of aggregation of Li and Lj (left) and the probability
    distribution (right). The aggregated line segments is more plausible, thus has
    a peakier probability distribution than before (see text for more details, best
    seen in color).
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.83
  Name of the first author: Nam-Gyu Cho
  Name of the last author: Seong-Whan Lee
  Number of Figures: 17
  Number of Tables: 3
  Number of authors: 3
  Paper title: A Novel Linelet-Based Representation for Line Segment Detection
  Publication Date: 2017-05-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance of Methods on the YorkUrban-LineSegment Dataset
      with Regard to Two Conditions, Con 1 and Con 2
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance of Methods on the Original YorkUrban Dataset with
      Regard to Two Conditions, Con 1 and Con 2
  Table 3 caption:
    table_text: TABLE 3 Performance of the Proposed Method on the YorkUrban-LineSegment
      Dataset with Regard to the Influence of the Validation and Aggregation Methods
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2703841
