- Affiliation of the first author: school of data and computer science, sun yat-sen
    university, china
  Affiliation of the last author: department of electrical and computer engineering,
    national university of singapore, singapore
  Figure 1 Link: articels_figures_by_rev_year\2016\Human_Parsing_with_Contextualized_Convolutional_Neural_Network\figure_1.jpg
  Figure 1 caption: "Co-CNN integrates the cross-layer context, global image-level\
    \ context, semantic edge context and local super-pixel contexts into a unified\
    \ network. It consists of cross-layer combination, global image-level label prediction,\
    \ within-super-pixel smoothing, cross-super-pixel neighborhood voting and semantic\
    \ edge prediction. First, given an input 150\xD7100 image, we extract the feature\
    \ maps for four resolutions (i.e., 150\xD7100 , 75\xD750 , 37\xD725 and 18\xD7\
    12 ). Then we gradually up-sample the feature maps and combine the corresponding\
    \ early, fine layers (blue dash line) and deep, coarse layers (blue circle with\
    \ plus) under the same resolutions to capture the cross-layer context. Second,\
    \ an auxiliary objective (shown as \u201CSquared loss on image-level labels\u201D\
    ) is appended after the down-sampling stream to predict global image-level labels.\
    \ These predicted probabilities are then aggregated into the subsequent layers\
    \ after the up-sampling (green line) and used to re-weight pixel-wise prediction\
    \ (green circle with plus). Third, the multi-scale prediction streams are appended\
    \ to predict semantic edges, and then the predicted edge maps are utilized to\
    \ guide the feature learning in subsequent convolutional layers. Finally, the\
    \ within-super-pixel smoothing and cross-super-pixel neighborhood voting are performed\
    \ based on the predicted confidence maps (orange planes) and the generated super-pixel\
    \ over-segmentation map to produce the final parsing result. Only down-sampling,\
    \ up-sampling, and prediction layers are shown; intermediate convolution layers\
    \ are omitted. For better viewing of all figures in this paper, please see original\
    \ zoomed-in color pdf file."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Human_Parsing_with_Contextualized_Convolutional_Neural_Network\figure_2.jpg
  Figure 2 caption: Comparison of label confidence maps between Co-CNN and that without
    using global labels.
  Figure 3 Link: articels_figures_by_rev_year\2016\Human_Parsing_with_Contextualized_Convolutional_Neural_Network\figure_3.jpg
  Figure 3 caption: The comparison of exemplar parsing results between the version
    of Co-CNN without using global labels and our complete Co-CNN.
  Figure 4 Link: articels_figures_by_rev_year\2016\Human_Parsing_with_Contextualized_Convolutional_Neural_Network\figure_4.jpg
  Figure 4 caption: "Comparison of example results of incorporating semantic edge\
    \ contextual information into Co-CNN. For each image, we show the results from\
    \ \u201CCo-CNN wo semantic edge\u201D (i.e., no semantic edge information is used),\
    \ \u201CPredicted Semantic Edge\u201D and \u201CCo-CNN\u201D sequentially."
  Figure 5 Link: articels_figures_by_rev_year\2016\Human_Parsing_with_Contextualized_Convolutional_Neural_Network\figure_5.jpg
  Figure 5 caption: "Exemplar images of our \u201CChictopia10k\u201D dataset."
  Figure 6 Link: articels_figures_by_rev_year\2016\Human_Parsing_with_Contextualized_Convolutional_Neural_Network\figure_6.jpg
  Figure 6 caption: Result comparison of our Co-CNN and two state-of-the-art methods.
    For each image, we show the parsing results by PaperDoll [4], ATR [1] and our
    Co-CNN sequentially.
  Figure 7 Link: articels_figures_by_rev_year\2016\Human_Parsing_with_Contextualized_Convolutional_Neural_Network\figure_7.jpg
  Figure 7 caption: Some failure cases generated by our Co-CNN.
  Figure 8 Link: articels_figures_by_rev_year\2016\Human_Parsing_with_Contextualized_Convolutional_Neural_Network\figure_8.jpg
  Figure 8 caption: Some example parsing results and semantic edge prediction results
    of our Co-CNN.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xiaodan Liang
  Name of the last author: Shuicheng Yan
  Number of Figures: 8
  Number of Tables: 6
  Number of authors: 7
  Paper title: Human Parsing with Contextualized Convolutional Neural Network
  Publication Date: 2016-03-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Detailed Configuration of Our Co-CNN
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Human Parsing Performances with Several Architectural
      Variants of Our Model and Four State-of-the-Arts When Evaluating on ATR [1]
  Table 3 caption:
    table_text: TABLE 3 Human Parsing Performances on the 1,000 Testing Images from
      Chictopia10k
  Table 4 caption:
    table_text: TABLE 4 Per-Class Comparison of F-1 Scores with Several Variants of
      Our Versions and Four State-of-the-Art Methods on ATR [1]
  Table 5 caption:
    table_text: TABLE 5 Comparison of Parsing Performance with Three State-of-the-Arts
      on the Test Images of Fashionista [8]
  Table 6 caption:
    table_text: TABLE 6 Performance Analysis on Three Main Influential Factors Affecting
      Human Parsing, i.e., Diverse Poses, Background Clutters and Viewpoints
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2537339
- Affiliation of the first author: johns hopkins university, baltimore, md
  Affiliation of the last author: virginia tech
  Figure 1 Link: articels_figures_by_rev_year\2016\Empirical_Minimum_Bayes_Risk_Prediction\figure_1.jpg
  Figure 1 caption: 'Classical Min Bayes Risk (MBR) versus Empirical Min Bayes Risk
    (EMBR): Probabilistic reasoning involves (a) learning the parameters of our model
    from training data, and (b) making predictions or decisions by optimizing Bayes
    Risk or expected loss. We present a meta-algorithm (EMBR) that is motivated by
    MBR but is instead based on Empirical Risk Minimization principle.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Empirical_Minimum_Bayes_Risk_Prediction\figure_2.jpg
  Figure 2 caption: 'Quantitative results using DivMBest: We show the performance
    of different methods versus M on three different problems. We observe that EMBR
    consistently and convincingly outperforms the natural baseline of MAP, and in
    the case of pose estimation, achieves state-of-art results.'
  Figure 3 Link: articels_figures_by_rev_year\2016\Empirical_Minimum_Bayes_Risk_Prediction\figure_3.jpg
  Figure 3 caption: Illustration of the effect of two of the three parameters in our
    model. The colored dots represent the chosen candidates mathbfYM , and the height
    of the curve at these points illustrates the mathrmtildeP(mathbfy) value assigned
    to each candidate. These parameters are learned via grid search, then predictions
    are made by using these mathrmtildeP(mathbfy) values within a Minimum Bayes Risk
    predictor.
  Figure 4 Link: articels_figures_by_rev_year\2016\Empirical_Minimum_Bayes_Risk_Prediction\figure_4.jpg
  Figure 4 caption: 'Quantitative results using Perturb-and-MAP: We show the performance
    of different method versus M on the interactive binary segmentation and the pose
    estimation problems. We observe that EMBR-PnM does improve upon the natural baseline
    of MAP (though the improvement is not as much as in the DivMBest case.).'
  Figure 5 Link: articels_figures_by_rev_year\2016\Empirical_Minimum_Bayes_Risk_Prediction\figure_5.jpg
  Figure 5 caption: 'Qualitative Results: Within each row, the first column corresponds
    to MAP, the middle columns show the diverse solutions, and the last column shows
    the EMBR prediction. The top two rows show examples where EMBR selects a better
    pose than MAP, while the bottom row shows an example where MAP produces a better
    result. Notice the right hand, and the separation of the legs in the EMBR solution
    in the first row. In the second row, notice the right hand being correctly detected
    by EMBR.'
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.96
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Vittal Premachandran
  Name of the last author: Dhruv Batra
  Number of Figures: 5
  Number of Tables: 1
  Number of authors: 4
  Paper title: Empirical Minimum Bayes Risk Prediction
  Publication Date: 2016-03-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 This Table Shows the Performance of the EMBR Predictor When
      the Three Parameters Are Tuned on Both the Corpus-Level Loss and the Instance-Level
      Loss
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2537807
- Affiliation of the first author: mit and google
  Affiliation of the last author: johns hopkins university
  Figure 1 Link: Not Available
  Figure 1 caption: Not Available
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: William T. Freeman
  Name of the last author: Gregory D. Hager
  Number of Figures: Not Available
  Number of Tables: 0
  Number of authors: 3
  Paper title: 'Guest Editorial: Special Section on CVPR 2013'
  Publication Date: 2016-03-03 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2529898
- Affiliation of the first author: school of computers, guangdong university of technology,
    guangzhou, china
  Affiliation of the last author: school of computers, guangdong university of technology,
    guangzhou, china
  Figure 1 Link: articels_figures_by_rev_year\2016\A_SphereDescriptionBased_Approach_for_MultipleInstance_Learning\figure_1.jpg
  Figure 1 caption: "An illustrative toy example. \u201C \u25EF \u201D signs represent\
    \ the instances from positive bags and \u201C \u25A1 \u201D signs stand for the\
    \ instances from negative bags. The number inside the \u201C \u25EF \u201D or\
    \ \u201C \u25A1 \u201D sign indicates the bag ID."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\A_SphereDescriptionBased_Approach_for_MultipleInstance_Learning\figure_2.jpg
  Figure 2 caption: Illustration of MIOptimalBall (base classifier) on toy MIL training
    and testing sets.
  Figure 3 Link: articels_figures_by_rev_year\2016\A_SphereDescriptionBased_Approach_for_MultipleInstance_Learning\figure_3.jpg
  Figure 3 caption: "Positive and negative images randomly sampled from the training\
    \ set and the testing set. (1) The images in (a) and (b) are relevant to the targeted\
    \ topic - \u201Cdog\u201D and thus are treated as positive. We can observe that\
    \ it is relatively easy to describe the topic shared in the positive images of\
    \ the training set (in (a)) and those of the testing set (in (b)), since both\
    \ of them are relevant to the targeted topic - \u201Cdog\u201D. (2) The images\
    \ in (c) and (d) are irrelevant to \u201Cdog\u201D and thus are considered as\
    \ negative. It is seen that there are some topics which appear in the testing\
    \ set (in (d)), but not in the training set (in (c)). For example, the first two\
    \ images in the testing set (in (d)) are related to \u201Ccup\u201D and \u201C\
    ship\u201D, respectively. However, these two topics are not represented in the\
    \ negative images of the training set (in (c)). In the Internet, there are hundreds\
    \ of thousands of image topics which are irrelevant to \u201Cdog\u201D. However,\
    \ the number of negative images in the training set is usually limited and can\
    \ represent only a part of negative topics irrelevant to \u201Cdog\u201D. As a\
    \ result, there may be some negative topics which appear in the testing set, but\
    \ not in the training set. When the negative training data cannot sufficiently\
    \ represent the topics of negative data in the testing set, it is more appropriate\
    \ to build up the classifier by enclosing the positive data, whose topic is relatively\
    \ explicit and easy to be described, in an optimal sphere, and excluding the negative\
    \ data outside the sphere."
  Figure 4 Link: articels_figures_by_rev_year\2016\A_SphereDescriptionBased_Approach_for_MultipleInstance_Learning\figure_4.jpg
  Figure 4 caption: Performance variation with different numbers of CCCP iterations.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.57
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Yanshan Xiao
  Name of the last author: Zhifeng Hao
  Number of Figures: 4
  Number of Tables: 7
  Number of authors: 3
  Paper title: A Sphere-Description-Based Approach for Multiple-Instance Learning
  Publication Date: 2016-03-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Categories Contained in the Training and Testing Sets
  Table 3 caption:
    table_text: TABLE 3 The F -Measure Values on the Benchmark Datasets
  Table 4 caption:
    table_text: TABLE 4 The Precision and Recall Values on the Benchmark Datasets
  Table 5 caption:
    table_text: TABLE 5 Classification Results on the Real-World MIL Dataset
  Table 6 caption:
    table_text: TABLE 6 The Confusion Matrix on the Real-World Dataset (Numbers in
      Bold Indicate the Classification Accuracy for Each Class)
  Table 7 caption:
    table_text: TABLE 7 Confusion Matrix on the Real-World Dataset (cons.)
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2539952
- Affiliation of the first author: metamind inc., palo alto, ca
  Affiliation of the last author: department of electrical engineering and computer
    science, university of michigan, ann arbor, mi
  Figure 1 Link: articels_figures_by_rev_year\2016\Active_Clustering_with_ModelBased_Uncertainty_Reduction\figure_1.jpg
  Figure 1 caption: 'Sample images from three image datasets: (a) Leaves [8]; (b)
    Faces [9]; (c) Dogs [10]. Best viewed in color.'
  Figure 10 Link: articels_figures_by_rev_year\2016\Active_Clustering_with_ModelBased_Uncertainty_Reduction\figure_10.jpg
  Figure 10 caption: Results of our experiments on large-scale image datasets. Note
    that Active-HACC could not be run on this data due to memory limitations.
  Figure 2 Link: articels_figures_by_rev_year\2016\Active_Clustering_with_ModelBased_Uncertainty_Reduction\figure_2.jpg
  Figure 2 caption: Pipeline of our active clustering framework as applied to image
    clustering. We iteratively choose a maximally informative image, then select and
    query new pairwise constraints based on the chosen image, update the certain image
    sets, and refine the clustering results before returning to select a new most
    informative image.
  Figure 3 Link: articels_figures_by_rev_year\2016\Active_Clustering_with_ModelBased_Uncertainty_Reduction\figure_3.jpg
  Figure 3 caption: A simple example of our uncertainty reducing active clustering
    method on toy data. The initial clustering result is poor, and the first certain
    sample P1 is chosen randomly. After this, however, the algorithm quickly identifies
    informative samples and queries the oracle to determine the ground truth relationships
    between each chosen point. Within three iterations, our method has explored the
    space and determined the correct borders of the two clusters.
  Figure 4 Link: articels_figures_by_rev_year\2016\Active_Clustering_with_ModelBased_Uncertainty_Reduction\figure_4.jpg
  Figure 4 caption: Comparison of variants of our methods against the random baseline.
  Figure 5 Link: articels_figures_by_rev_year\2016\Active_Clustering_with_ModelBased_Uncertainty_Reduction\figure_5.jpg
  Figure 5 caption: Comparison of our methods against sample-based active learning
    methods. Since QUIRE is a binary-only method, there is no result for QUIRE [34]
    on the multi-cluster datasets.
  Figure 6 Link: articels_figures_by_rev_year\2016\Active_Clustering_with_ModelBased_Uncertainty_Reduction\figure_6.jpg
  Figure 6 caption: Comparison to state-of-the-art active clustering methods. y-axis
    is Jaccard Coefficient score. Best viewed in color.
  Figure 7 Link: articels_figures_by_rev_year\2016\Active_Clustering_with_ModelBased_Uncertainty_Reduction\figure_7.jpg
  Figure 7 caption: Comparison of our methods against other active clustering methods
    on two image datasets with a 2 percent simulated error rate on the oracle queries.
    Best viewed in color.
  Figure 8 Link: articels_figures_by_rev_year\2016\Active_Clustering_with_ModelBased_Uncertainty_Reduction\figure_8.jpg
  Figure 8 caption: Comparison of our methods against other active clustering methods
    on the dog-100 image dataset when using real human input to acquire the pairwise
    constraints. Best viewed in color.
  Figure 9 Link: articels_figures_by_rev_year\2016\Active_Clustering_with_ModelBased_Uncertainty_Reduction\figure_9.jpg
  Figure 9 caption: Comparison of URASC+N clustering results with known and (initially)
    unknown numbers of clusters. Best viewed in color.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Caiming Xiong
  Name of the last author: Jason J. Corso
  Number of Figures: 10
  Number of Tables: 1
  Number of authors: 3
  Paper title: Active Clustering with Model-Based Uncertainty Reduction
  Publication Date: 2016-03-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 UCI Machine Learning and Gene Data Sets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2539965
- Affiliation of the first author: institute of artificial intelligence and robotics,
    xi'an jiaotong university, xi'an, shaanxi, china
  Affiliation of the last author: institute of artificial intelligence and robotics,
    xi'an jiaotong university, xi'an, shaanxi, china
  Figure 1 Link: articels_figures_by_rev_year\2016\MultiTimescale_Collaborative_Tracking\figure_1.jpg
  Figure 1 caption: The overview of our Multi-timescale Collaborative Tracking. We
    crop the long-term target samples, medium-term target-background samples and short-term
    contextual samples into sets S L , S M and S S . The target samples are in red,
    while the samples from background are in orange. After collaborative learning,
    the samples with non-zeros dual variables are sent to the sets V L , V M and V
    S . Samples in V L and V M contribute to the appearance model while samples in
    V S contribute to the motion model. They determine the probability of target state
    together.
  Figure 10 Link: articels_figures_by_rev_year\2016\MultiTimescale_Collaborative_Tracking\figure_10.jpg
  Figure 10 caption: Influence of changing maximum scale change per frame on 10 additional
    video sequences and the 50 video benchmark [7].
  Figure 2 Link: articels_figures_by_rev_year\2016\MultiTimescale_Collaborative_Tracking\figure_2.jpg
  Figure 2 caption: 'Illustration of the representation: (a) shows the generation
    process of feature for one samples; and (b) shows the spatial configuration of
    training samples in one frame.'
  Figure 3 Link: articels_figures_by_rev_year\2016\MultiTimescale_Collaborative_Tracking\figure_3.jpg
  Figure 3 caption: 'The motion model: (a) we learn the shot-term component in the
    last frame; (b) we predict the target position via auxiliary samples.'
  Figure 4 Link: articels_figures_by_rev_year\2016\MultiTimescale_Collaborative_Tracking\figure_4.jpg
  Figure 4 caption: Search strategy. We utilize SMC Sampler to coarsely localize the
    target, then exhaustively search in a small region considering scales.
  Figure 5 Link: articels_figures_by_rev_year\2016\MultiTimescale_Collaborative_Tracking\figure_5.jpg
  Figure 5 caption: Representation analysis. (a), (b) are the success plots and precision
    plots for different types of descriptors with the overall scores in the legend.
    (c) demonstrates the AUC scores over attributes.
  Figure 6 Link: articels_figures_by_rev_year\2016\MultiTimescale_Collaborative_Tracking\figure_6.jpg
  Figure 6 caption: Component analysis. (a), (b) are the success plots and precision
    plots for different component combination with the overall scores in the legend.
    (c) demonstrates the AUC scores over attributes.
  Figure 7 Link: articels_figures_by_rev_year\2016\MultiTimescale_Collaborative_Tracking\figure_7.jpg
  Figure 7 caption: "Qualitative analysis of components. We compare MCT with L \u2217\
    \ -T in lemming and tiger1 in the top row, compare MCT with M \u2217 -T in skating1\
    \ and liquor in the middle row, compare MCT with S \u2217 -T in soccer and bolt\
    \ in the bottom row."
  Figure 8 Link: articels_figures_by_rev_year\2016\MultiTimescale_Collaborative_Tracking\figure_8.jpg
  Figure 8 caption: Component visualization. We visualize the three components, where
    the descriptive, discriminative and regressive samples with non-zero dual variables
    are shown in the yellow, the green and blue boxes beside the corresponding frames.
    As discriminative support vectors include both positive samples and negative samples,
    we arrange the positive samples before the negative samples, and separate them
    by a red line.
  Figure 9 Link: articels_figures_by_rev_year\2016\MultiTimescale_Collaborative_Tracking\figure_9.jpg
  Figure 9 caption: Parameter analysis. The default parameters are set according to
    Section 5.1. We demonstrate the performance change of average AUC score w.r.t.
    (a) parameter n1 , (b) parameter C1 , (c) parameter C2 , (d) parameter C3 and
    (e) parameter sigma t .
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Dapeng Chen
  Name of the last author: Nanning Zheng
  Number of Figures: 18
  Number of Tables: 2
  Number of authors: 5
  Paper title: Multi-Timescale Collaborative Tracking
  Publication Date: 2016-03-09 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 The Three Basic Blocks for Selecting the Dual Variable Pairs\
      \ from \u03B1 and \u03B2"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison over Different Attributes
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2539956
- Affiliation of the first author: department of statistics and biostatistics, rutgers
    university, piscataway, nj
  Affiliation of the last author: department of statistics and biostatistics, rutgers
    university, piscataway, nj
  Figure 1 Link: articels_figures_by_rev_year\2016\Blessing_of_Dimensionality_Recovering_Mixture_Data_via_Dictionary_Pursuit\figure_1.jpg
  Figure 1 caption: 'Illustrating that Problem 1.1 is ill-posed in general case. Left:
    A collection of 2-dimensional authentic samples that exactly lie on the union
    of 2 subspaces. Right: The points in the observed data matrix X , 10 percent entries
    of which are corrupted. It can be seen that there exist some data points that
    are mixed up with each other such that their subspace membership is unidentifiable.
    Thus, in such circumstances, it is theoretically impossible to exactly recover
    L 0 from the given X .'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Blessing_of_Dimensionality_Recovering_Mixture_Data_via_Dictionary_Pursuit\figure_2.jpg
  Figure 2 caption: "Exploring the influence of the subspace number, using randomly\
    \ generated matrices. (a) The column-coherence parameter \u03BC 1 ( L 0 ) vs subspace\
    \ number. (b) \u03BC 2 ( L 0 ) vs subspace number. (c) Recovery error (produced\
    \ by RPCA) vs subspace number. In these experiments, the ambient data dimension\
    \ and the number of data points are m=n=500 , the subspace number k is varying\
    \ from 1 to 50, the dimension of each subspace is set as 100k such that the rank\
    \ of L 0 is always 100, and S 0 is fixed as a sparse matrix with 13 percent nonzero\
    \ entries. The recovery error is computed as \u2225 L 0 \u2212 L 0 \u2225 F \u2225\
    \ L 0 \u2225 F , where L 0 is an estimate to L 0 . The numbers shown above figures\
    \ are averaged from 10 random trials."
  Figure 3 Link: articels_figures_by_rev_year\2016\Blessing_of_Dimensionality_Recovering_Mixture_Data_via_Dictionary_Pursuit\figure_3.jpg
  Figure 3 caption: "Exemplifying that LRR can completely avoid \u03BC 2 . In this\
    \ experiment, L 0 is a 200\xD7200 rank-1 matrix with one column being 1 (i.e.,\
    \ a vector of all ones) and everything else being zero. Thus, \u03BC 1 ( L 0 )=1\
    \ and \u03BC 2 ( L 0 )=n=200 . The sparse matrix S 0 is with Bernoulli 0,1 values,\
    \ and its support fraction is 5 percent. The dictionary is set as A=[1,W] , where\
    \ W is a 200\xD7p random Gaussian matrix ( p is varying). As long as r A =p+1\u2264\
    40 , LRR with \u03BB=0.1 can exactly recover L 0 from the given X ."
  Figure 4 Link: articels_figures_by_rev_year\2016\Blessing_of_Dimensionality_Recovering_Mixture_Data_via_Dictionary_Pursuit\figure_4.jpg
  Figure 4 caption: "Investigating the properties of \u03BC A 3 ( L 0 ) , using randomly\
    \ generated matrices. While the matrix size m is varying, we set n=m , d=m , r\
    \ A =0.5m and r 0 =0.2mlog(m) . The numbers shown above are averaged from 10 random\
    \ trials."
  Figure 5 Link: articels_figures_by_rev_year\2016\Blessing_of_Dimensionality_Recovering_Mixture_Data_via_Dictionary_Pursuit\figure_5.jpg
  Figure 5 caption: "Comparing Algorithm 1 with RPCA, in terms of recovering randomly\
    \ generated matrices. The numbers plotted on the above figures are the success\
    \ rates within 20 random trials. The white and black areas mean \u201Csucceed\u201D\
    \ and \u201Cfail\u201D, respectively. Here, the success is in a sense that Vert\
    \ hatL0-L0Vert F<0.05Vert L0Vert F , where hatL0 denotes an estimate to L0 ."
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.66
  Name of the first author: Guangcan Liu
  Name of the last author: Ping Li
  Number of Figures: 5
  Number of Tables: 2
  Number of authors: 3
  Paper title: 'Blessing of Dimensionality: Recovering Mixture Data via Dictionary
    Pursuit'
  Publication Date: 2016-03-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 List of Notations
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Clustering Error Rates (Percent) on Hopkins155 Extended
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2539946
- Affiliation of the first author: cas center for excellence in brain science and
    intelligence technology, chinese academy of sciences, beijing, china
  Affiliation of the last author: department of computer science and information systems,
    birkbeck college, malet street, london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2016\SemiSupervised_TensorBased_Graph_Embedding_Learning_and_Its_Application_to_Visua\figure_1.jpg
  Figure 1 caption: The high level framework for our tracker.
  Figure 10 Link: articels_figures_by_rev_year\2016\SemiSupervised_TensorBased_Graph_Embedding_Learning_and_Its_Application_to_Visua\figure_10.jpg
  Figure 10 caption: The success plots of the top 10 ranked trackers among TrSSI-TDT
    and the 29 competing trackers for different attributes.
  Figure 2 Link: articels_figures_by_rev_year\2016\SemiSupervised_TensorBased_Graph_Embedding_Learning_and_Its_Application_to_Visua\figure_2.jpg
  Figure 2 caption: An overview of the proposed tracker.
  Figure 3 Link: articels_figures_by_rev_year\2016\SemiSupervised_TensorBased_Graph_Embedding_Learning_and_Its_Application_to_Visua\figure_3.jpg
  Figure 3 caption: Tracking results of TrSSI-TDT, TrSSI-2DLDA, TrSSI-VDT, TDT, MI-TDT,
    SSI-TDT, and TrSSI-RSF.
  Figure 4 Link: articels_figures_by_rev_year\2016\SemiSupervised_TensorBased_Graph_Embedding_Learning_and_Its_Application_to_Visua\figure_4.jpg
  Figure 4 caption: 'The precision plots and the success plots of TrSSI-TDT, TrSSI-2DLDA,
    TrSSI-VDT, TDT, MI-TDT, SSI-TDT, and TrSSI-RSF on all the sequences: each decimal
    number in the legend is the representative precision or the representative success
    rate of the corresponding tracker: (a) the precision plots and (b) the success
    plots.'
  Figure 5 Link: articels_figures_by_rev_year\2016\SemiSupervised_TensorBased_Graph_Embedding_Learning_and_Its_Application_to_Visua\figure_5.jpg
  Figure 5 caption: The precision plots of TrSSI-TDT, TrSSI-2DLDA, TrSSI-VDT, TDT,
    MI-TDT, SSI-TDT, and TrSSI-RSF for different attributes.
  Figure 6 Link: articels_figures_by_rev_year\2016\SemiSupervised_TensorBased_Graph_Embedding_Learning_and_Its_Application_to_Visua\figure_6.jpg
  Figure 6 caption: The success plots of TrSSI-TDT, TrSSI-2DLDA, TrSSI-VDT, TDT, MI-TDT,
    SSI-TDT, and TrSSI-RSF for different attributes.
  Figure 7 Link: articels_figures_by_rev_year\2016\SemiSupervised_TensorBased_Graph_Embedding_Learning_and_Its_Application_to_Visua\figure_7.jpg
  Figure 7 caption: Tracking results of TrSSI-TDT and the top 10 ranked competing
    trackers.
  Figure 8 Link: articels_figures_by_rev_year\2016\SemiSupervised_TensorBased_Graph_Embedding_Learning_and_Its_Application_to_Visua\figure_8.jpg
  Figure 8 caption: 'The precision plots and the success plots of the top 10 ranked
    trackers among TrSSI-TDT and the 29 competing trackers on all the sequences: (a)
    the precision plots; (b) the success plots.'
  Figure 9 Link: articels_figures_by_rev_year\2016\SemiSupervised_TensorBased_Graph_Embedding_Learning_and_Its_Application_to_Visua\figure_9.jpg
  Figure 9 caption: The precision plots of the top 10 ranked trackers among TrSSI-TDT
    and the 29 competing trackers for different attributes.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Weiming Hu
  Name of the last author: Stephen Maybank
  Number of Figures: 10
  Number of Tables: 0
  Number of authors: 5
  Paper title: Semi-Supervised Tensor-Based Graph Embedding Learning and Its Application
    to Visual Discriminant Tracking
  Publication Date: 2016-03-09 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2539944
- Affiliation of the first author: university of adelaide, adelaide, australia
  Affiliation of the last author: university of oxford, oxford, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2016\LargeScale_Binary_Quadratic_Optimization_Using_Semidefinite_Relaxation_and_Appli\figure_1.jpg
  Figure 1 caption: "Results for 2 -demensional points bisection. The resulting two\
    \ classes of points are shown in red '+' and blue ' \u2218 ' respectively. SDCut-QN\
    \ succeeds in clustering the points as desired, while both RatioCut and NCut failed\
    \ in these two cases."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\LargeScale_Binary_Quadratic_Optimization_Using_Semidefinite_Relaxation_and_Appli\figure_2.jpg
  Figure 2 caption: "Results for graph bisection with different values of the parameter\
    \ \u03B3 . The illustrated results are averaged over 10 random graphs. Upper-bounds\
    \ and lower-bounds achieved by SDCut-QN are shown in this figure (those of SDCut-SN\
    \ is very similar and thus omitted). The relaxation becomes tighter (that is,\
    \ upper-bounds and lower-bounds are closer) for larger \u03B3 . The number of\
    \ iterations for both SDCut-SN and SDCut-QN grows with the increase of \u03B3\
    \ ."
  Figure 3 Link: articels_figures_by_rev_year\2016\LargeScale_Binary_Quadratic_Optimization_Using_Semidefinite_Relaxation_and_Appli\figure_3.jpg
  Figure 3 caption: Image segmentation with partial grouping constraints. The top
    row shows the original images with 10 labelled foreground (red markers) and 10
    background (blue markers) pixels. SDCut-QN achieves significantly better results
    than BNCut. The results of SeDuMi and SDPT3 are omitted, as they are similar to
    those of SDCut-QN.
  Figure 4 Link: articels_figures_by_rev_year\2016\LargeScale_Binary_Quadratic_Optimization_Using_Semidefinite_Relaxation_and_Appli\figure_4.jpg
  Figure 4 caption: 'Image segmentation with histogram constraints (coarse over-segmentation).
    The number of superpixels is around 726 . From top to bottom are: original images,
    ground-truth (GT), superpixels, unary terms for graph-cuts, results of graph-cuts,
    SMQC and SDCut-QN. Results for other SDP based methods are similar to that of
    SDCut-QN and thus omitted. Graph cuts tends to mix together the foreground and
    background with similar color. SDCut-QN achieves the best segmentation results.'
  Figure 5 Link: articels_figures_by_rev_year\2016\LargeScale_Binary_Quadratic_Optimization_Using_Semidefinite_Relaxation_and_Appli\figure_5.jpg
  Figure 5 caption: Image co-segmentation on Weizman horses and MSRC datasets. The
    original images, the results (score vectors) of LowRank and SDCut-QN are illustrated
    from top to bottom. Other methods produce similar segmentation results.
  Figure 6 Link: articels_figures_by_rev_year\2016\LargeScale_Binary_Quadratic_Optimization_Using_Semidefinite_Relaxation_and_Appli\figure_6.jpg
  Figure 6 caption: Image deconvolution. QPBO cannot label most of pixels (grey pixels
    denote unlabelled pixels), as the MRF models are highly non-submodular. SDCut-SN
    and MOSEK have similar segmentation results. TRWS and MPLP achieve worse segmentation
    results than our methods.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Peng Wang
  Name of the last author: Philip H. S. Torr
  Number of Figures: 6
  Number of Tables: 11
  Number of authors: 4
  Paper title: Large-Scale Binary Quadratic Optimization Using Semidefinite Relaxation
    and Applications
  Publication Date: 2016-03-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Comparison of Our Algorithms and Interior-Point Algorithms
      on Convergence Rate, Computational Complexity and Memory Requirement
  Table 10 caption:
    table_text: TABLE 10 Image Deconvolution ( n=2,250 , m=2,250 )
  Table 2 caption:
    table_text: TABLE 2 BQP Formulations for Different Applications Considered in
      This Paper
  Table 3 caption:
    table_text: TABLE 3 Numerical Results for Graph Bisection with Dense Affinity
      Matrices
  Table 4 caption:
    table_text: TABLE 4 Numerical Results for Graph Bisection with Sparse Affinity
      Matrices
  Table 5 caption:
    table_text: TABLE 5 Graph Bisection on Large Dense Graphs ( n=10,000,m=10,001
      ).
  Table 6 caption:
    table_text: TABLE 6 Numerical Results for Image Segmentation with Partial Grouping
      Constraints
  Table 7 caption:
    table_text: TABLE 7 Image Segmentation with Histogram Constraints
  Table 8 caption:
    table_text: TABLE 8 Numerical Results for Image Co-Segmentation
  Table 9 caption:
    table_text: TABLE 9 Numerical Results for Graph Matching, which Are the Mean over
      10 Random Graphs
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2541146
- Affiliation of the first author: school of electrical engineering, kaist, daejeon,
    republic of korea
  Affiliation of the last author: school of electrical engineering, kaist, daejeon,
    republic of korea
  Figure 1 Link: articels_figures_by_rev_year\2016\Geometric_Calibration_of_MicroLensBased_Light_Field_Cameras_Using_Line_Features\figure_1.jpg
  Figure 1 caption: A magnified raw image of a checkerboard pattern captured by Lytro.
    It is difficult to extract the precise locations of checkerboard corners from
    raw images. However, lines are relatively visible in small micro-lens images.
  Figure 10 Link: articels_figures_by_rev_year\2016\Geometric_Calibration_of_MicroLensBased_Light_Field_Cameras_Using_Line_Features\figure_10.jpg
  Figure 10 caption: Examples of line features extracted from raw images. Red dots
    and green lines indicate micro-lens centers and line features, respectively. No
    line features are extracted from corner regions.
  Figure 2 Link: articels_figures_by_rev_year\2016\Geometric_Calibration_of_MicroLensBased_Light_Field_Cameras_Using_Line_Features\figure_2.jpg
  Figure 2 caption: Vignetting effect of micro-lenses. The top rows (a, b) and bottom
    rows (c, d) are examples of raw images with vignetting and without vignetting,
    respectively. Raw images are divided by white-plane images to remove the vignetting
    effect from them. Checkerboard images without vignetting (d) are better than those
    with vignetting (b) for line feature extraction.
  Figure 3 Link: articels_figures_by_rev_year\2016\Geometric_Calibration_of_MicroLensBased_Light_Field_Cameras_Using_Line_Features\figure_3.jpg
  Figure 3 caption: "Template generation for line feature extraction. The line equation\
    \ is determined by the angle \u03B8 and translation t ."
  Figure 4 Link: articels_figures_by_rev_year\2016\Geometric_Calibration_of_MicroLensBased_Light_Field_Cameras_Using_Line_Features\figure_4.jpg
  Figure 4 caption: 'Upper-limit distance estimation of a line segment: (a) ''Feature
    distance'' refers to the distance between the micro-lens center (red dot) and
    the line feature (green line) in a raw image. (b) ''Segment distance'' is the
    distance between the micro-lens center (red dot) and the line segment (green line)
    connecting two adjacent corners in a sub-aperture image. (c) The relationship
    between the feature distance (vertical axis) and the segment distance (horizontal
    axis) is estimated via line fitting.'
  Figure 5 Link: articels_figures_by_rev_year\2016\Geometric_Calibration_of_MicroLensBased_Light_Field_Cameras_Using_Line_Features\figure_5.jpg
  Figure 5 caption: "(a) Line features exist only near lines, not homogeneous regions\
    \ or near corners. For each line segment (red line with an upper-limit distance\
    \ of d ), there are four neighboring line segments (orange lines with upper-limit\
    \ distances of d 1 \u223C d 4 ) perpendicular to it. The blue regions near the\
    \ perpendicular segments within their upper-limit distances multiplied by s are\
    \ classified as 'corner regions'. The remaining region (green region) near the\
    \ line segment within its upper-limit distance is classified as a 'line region'.\
    \ We extract line features from only micro-lenses whose centers are in the line\
    \ regions of the sub-aperture image. (b) An example of line regions (green boxes)\
    \ in a checkerboard pattern."
  Figure 6 Link: articels_figures_by_rev_year\2016\Geometric_Calibration_of_MicroLensBased_Light_Field_Cameras_Using_Line_Features\figure_6.jpg
  Figure 6 caption: Projection model of a micro-lens-based light field camera. The
    thin-lens model and the pinhole model are used for the main lens and the micro-lenses,
    respectively. All rays from an arbitrary point pass through the image of the point.
    For each micro-lens, a ray from the image passes through its center and meets
    the CCD array.
  Figure 7 Link: articels_figures_by_rev_year\2016\Geometric_Calibration_of_MicroLensBased_Light_Field_Cameras_Using_Line_Features\figure_7.jpg
  Figure 7 caption: An example of distorted and undistorted images. The structure
    of micro-lens centers (red dots) which is near-regular in the distorted image
    becomes irregular after undistortion. On the other hand, the displacement (difference
    between blue dot and red dot) for generating a sub-aperture image is regular in
    the undistorted image, but is irregular after distortion (i.e., the inverse of
    (36)).
  Figure 8 Link: articels_figures_by_rev_year\2016\Geometric_Calibration_of_MicroLensBased_Light_Field_Cameras_Using_Line_Features\figure_8.jpg
  Figure 8 caption: "(a) An example of triangular meshes (blue line) generated by\
    \ connecting pixels at (i,j)=(2,\u22121) from adjacent micro-lens centers (red\
    \ dots). (b) The meshes are transformed into a sub-aperture image to compute the\
    \ intensity value of each pixel (green dots) in the sub-aperture image. Grey lines\
    \ indicate the pixel boundaries of the sub-aperture image. (c) Notation for triangle\
    \ interpolation. The intensity value y at pixel (u,v) is computed by a weighted\
    \ sum of three vertices of a mesh that contains the pixel. Weights of the vertices\
    \ are proportional to the area of the triangles at their opposite side."
  Figure 9 Link: articels_figures_by_rev_year\2016\Geometric_Calibration_of_MicroLensBased_Light_Field_Cameras_Using_Line_Features\figure_9.jpg
  Figure 9 caption: "(a) The Lytro camera used in this paper. The size of its raw\
    \ image and the radius of its micro-lens image are 3280\xD73280 pixels and 5 pixels,\
    \ respectively. (b) An example of raw images captured by the Lytro camera (unit\
    \ square size of checkerboard: 10 mm)."
  First author gender probability: 0.77
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.88
  Name of the first author: Yunsu Bok
  Name of the last author: In So Kweon
  Number of Figures: 17
  Number of Tables: 6
  Number of authors: 3
  Paper title: Geometric Calibration of Micro-Lens-Based Light Field Cameras Using
    Line Features
  Publication Date: 2016-03-11 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Projection Errors of Sub-Aperture Images (Unit: Pixel)'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Ray Re-Projection Error of Sub-Aperture Images (Unit: mm)'
  Table 3 caption:
    table_text: 'TABLE 3 Estimated Distance between Adjacent Planes in Fig. 14 (Unit:
      mm)'
  Table 4 caption:
    table_text: TABLE 4 Intrinsic Parameters According to the Proposed Method
  Table 5 caption:
    table_text: 'TABLE 5 Projection Error of Sub-Aperture Images Generated by Lytro
      Illum (Unit: Pixel)'
  Table 6 caption:
    table_text: 'TABLE 6 Ray Re-Projection Error of Sub-Aperture Images (Unit: mm)'
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2541145
