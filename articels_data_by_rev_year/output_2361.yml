- Affiliation of the first author: electrical engineering department, california institute
    of technology, pasadena, ca, usa
  Affiliation of the last author: electrical engineering department, california institute
    of technology, pasadena, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\How_to_Query_an_Oracle_Efficient_Strategies_to_Label_Data\figure_1.jpg
  Figure 1 caption: System model.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\How_to_Query_an_Oracle_Efficient_Strategies_to_Label_Data\figure_2.jpg
  Figure 2 caption: A 3IC query. Circles represent the samples to be labeled. An edge
    shows that two samples are from the same class. For N=2 , the first row shows
    the possible valid responses and the second row the invalid responses. For N>2
    , the first element of the second row is also a valid response.
  Figure 3 Link: articels_figures_by_rev_year\2021\How_to_Query_an_Oracle_Efficient_Strategies_to_Label_Data\figure_3.jpg
  Figure 3 caption: A basic example of three queries with Algorithm 3 and 3IC. The
    blue circles show the class representatives. The squares with round corners show
    temporary bins. The green circles are samples to be labeled.
  Figure 4 Link: articels_figures_by_rev_year\2021\How_to_Query_an_Oracle_Efficient_Strategies_to_Label_Data\figure_4.jpg
  Figure 4 caption: Query rate of the proposed algorithms for labeling of a dataset
    with uniform class distribution (a) as a function of number of classes N for 2IC
    and 3IC query schemes, (b) as a function of number of samples k in a k IC query
    for N=5, 10 .
  Figure 5 Link: articels_figures_by_rev_year\2021\How_to_Query_an_Oracle_Efficient_Strategies_to_Label_Data\figure_5.jpg
  Figure 5 caption: Performance of Algorithm 2 with batch query processing for L=100000
    as a function of algorithm rounds. (a) 3IC with nonuniform class distribution
    boldsymbolpi n(10)=lbrace 0.9;; 0.0250.025;; 0.025;; 0.025rbrace ; (b) 3IC with
    uniform class distribution.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Farshad Lahouti
  Name of the last author: Babak Hassibi
  Number of Figures: 5
  Number of Tables: 1
  Number of authors: 3
  Paper title: How to Query an Oracle? Efficient Strategies to Label Data
  Publication Date: 2021-10-07 00:00:00
  Table 1 caption: "TABLE 1 Summary of Query Rate Analyses Results for k kIC With\
    \ Uniform Class Distribution \u03C0 u \u03C0u and Nonuniform Class Distribution\
    \ \u03C0 n ( 1 \u03F5 \u03C0n(1\u03B5)"
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3118644
- Affiliation of the first author: "department of informatics, technical university\
    \ of munich, m\xFCnchen, germany"
  Affiliation of the last author: "department of informatics, technical university\
    \ of munich, m\xFCnchen, germany"
  Figure 1 Link: articels_figures_by_rev_year\2021\DPODv_Dense_CorrespondenceBased__DoF_Pose_Estimation\figure_1.jpg
  Figure 1 caption: 'Synthetic toy example illustrating three-stage correspondence-based
    6 DoF pose estimation: A full RGB image is fed to a 2D object detector for bounding
    box estimation. Resulting bounding boxes are then used to generate crops on the
    available data, which are subsequently fed into the correspondence network. If
    only RGB images are provided, then the pose is estimated from correspondences
    using 2D-3D PnP. In case registered depth data is also in place, we project the
    estimated correspondences into 3D and use the 3D-3D Kabsch algorithm.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\DPODv_Dense_CorrespondenceBased__DoF_Pose_Estimation\figure_2.jpg
  Figure 2 caption: 'Multi-view pose optimization: The algorithm takes the output
    of DPODv2 from several views with known relative camera transformations as input.
    An initial pose hypothesis is iteratively refined until it converges to a pose,
    which is consistent with predicted correspondences in all frames. The proposed
    loss function penalizes pixel-wise distance between a predicted correspondence
    and a correspondence corresponding to the current pose hypothesis. The loss function
    is implemented using a differentiable renderer.'
  Figure 3 Link: articels_figures_by_rev_year\2021\DPODv_Dense_CorrespondenceBased__DoF_Pose_Estimation\figure_3.jpg
  Figure 3 caption: Pose recalculation for symmetric objects during training. Poses
    are disambiguated during training to produce consistent NOCS maps. In case of
    a continuous symmetry around Z axis (Example a), a rotation around Z is added
    to the initial pose to ensure that the camera is always located on the same arch
    in the object coordinate system. In case of discrete symmetries (Example b), all
    symmetric poses are mapped to the one base pose by rotation around the symmetry
    axis.
  Figure 4 Link: articels_figures_by_rev_year\2021\DPODv_Dense_CorrespondenceBased__DoF_Pose_Estimation\figure_4.jpg
  Figure 4 caption: Visual comparison of predicted segmentation maps of the visible
    object parts and color-coded NOCS for the same patch depending on whether RGB
    or Depth CENet is used.
  Figure 5 Link: articels_figures_by_rev_year\2021\DPODv_Dense_CorrespondenceBased__DoF_Pose_Estimation\figure_5.jpg
  Figure 5 caption: Dependence of quality of pose estimation on objects visibility.
    Pose quality is reported in terms of the Average Recall [14] on objects 2 and
    21 from the TLESS dataset. The plots show that correspondence prediction works
    from RGB works considerably more reliably than from depth in case of large occlusions.
    It also shows that synthetic training data allows for more reliable pose estimation
    of occluded objects.
  Figure 6 Link: articels_figures_by_rev_year\2021\DPODv_Dense_CorrespondenceBased__DoF_Pose_Estimation\figure_6.jpg
  Figure 6 caption: Success cases and failure cases. Top row provides an example of
    successfully estimated segmentation mask and NOCS correspondences even in case
    of occlusions and similar objects present in the image patch. The bottom row illustrates
    a failure case, where the CENet is confused by occlusions and similar objects
    belonging to other object classes, which leads to an incorrect estimated pose.
    The green cuboid represents the ground truth pose (up to a symmetry transformation),
    whereas the blue cuboid represents the estimated pose.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Ivan Shugurov
  Name of the last author: Slobodan Ilic
  Number of Figures: 6
  Number of Tables: 12
  Number of authors: 3
  Paper title: 'DPODv2: Dense Correspondence-Based 6 DoF Pose Estimation'
  Publication Date: 2021-10-08 00:00:00
  Table 1 caption: TABLE 1 Data Modalities and Types of Train Data Used in the Experiments
  Table 10 caption: 'TABLE 10 Pose Estimation Performance on the Occlusion Dataset
    on Different Data Modalities and Type of Train Data: The Table Reports in Terms
    of the AR Score [14]'
  Table 2 caption: 'TABLE 2 Pose Estimation Performance on Linemod on RGB Images of
    Methods Trained on Synthetic Data: The Table Reports the Percentages of Correctly
    Estimated Poses w.r.t. the ADD Score'
  Table 3 caption: 'TABLE 3 Pose Estimation Performance on Linemod on RGB Images of
    Methods Trained on Real Data: The Table Reports the Percentages of Correctly Estimated
    Poses w.r.t. the ADD Score'
  Table 4 caption: 'TABLE 4 Pose Estimation Performance on Linemod on Depth and RGBD
    Images: The Table Reports the Percentages of Correctly Estimated Poses w.r.t.
    the ADD Score'
  Table 5 caption: 'TABLE 5 Pose Estimation Performance on Linemod on Depth and RGBD
    Images: The Table Reports the Percentages of Correctly Estimated Poses w.r.t.
    the ADD Score'
  Table 6 caption: 'TABLE 6 Pose Estimation Performance Comparison on the Occlusion
    Dataset: Results are Reported in Terms of the Average Recall Score'
  Table 7 caption: 'TABLE 7 Pose Estimation Performance Comparison on the Homebrewed
    Dataset: Results are Reported in Terms of the Average Recall Score [14]'
  Table 8 caption: 'TABLE 8 Pose Estimation Performance Comparison on the BOP Images
    of the TLESS Dataset: Results are Reported in Terms of the Average Recall Score
    [14]'
  Table 9 caption: 'TABLE 9 Pose Estimation Performance on Linemod on Different Data
    Modalities and Types of Train Data: The Table Reports the Percentages of Correctly
    Estimated Poses w.r.t. the ADD Score'
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3118833
- Affiliation of the first author: "instituto de investigaci\xF3n en ingenier\xED\
    a de arag\xF3n, universidad de zaragoza, zaragoza, spain"
  Affiliation of the last author: "instituto de investigaci\xF3n en ingenier\xEDa\
    \ de arag\xF3n, universidad de zaragoza, zaragoza, spain"
  Figure 1 Link: articels_figures_by_rev_year\2021\MORPHDSLAM_Model_Order_Reduction_for_PhysicsBased_Deformable_SLAM\figure_1.jpg
  Figure 1 caption: Levels in the Spatial AI approach. Image inspired in the CVPR
    presentation of A. Davison [2].
  Figure 10 Link: articels_figures_by_rev_year\2021\MORPHDSLAM_Model_Order_Reduction_for_PhysicsBased_Deformable_SLAM\figure_10.jpg
  Figure 10 caption: Parameters to optimize in the video sequence.
  Figure 2 Link: articels_figures_by_rev_year\2021\MORPHDSLAM_Model_Order_Reduction_for_PhysicsBased_Deformable_SLAM\figure_2.jpg
  Figure 2 caption: a) Rigid SfM allowing a reconstruction of the world from different
    views. b) Non-rigid SfM implies that both the camera and the scene may take different
    positions and shapes (time-dependent).
  Figure 3 Link: articels_figures_by_rev_year\2021\MORPHDSLAM_Model_Order_Reduction_for_PhysicsBased_Deformable_SLAM\figure_3.jpg
  Figure 3 caption: Kinematic modeling of the proposed formulation.
  Figure 4 Link: articels_figures_by_rev_year\2021\MORPHDSLAM_Model_Order_Reduction_for_PhysicsBased_Deformable_SLAM\figure_4.jpg
  Figure 4 caption: In general, we consider applications where an important part of
    the scene is static (blue points), while some regions (red points) are non-rigid
    or deformable.
  Figure 5 Link: articels_figures_by_rev_year\2021\MORPHDSLAM_Model_Order_Reduction_for_PhysicsBased_Deformable_SLAM\figure_5.jpg
  Figure 5 caption: Cantilever beam problem. A moving load F is parameterized by its
    position coordinate s .
  Figure 6 Link: articels_figures_by_rev_year\2021\MORPHDSLAM_Model_Order_Reduction_for_PhysicsBased_Deformable_SLAM\figure_6.jpg
  Figure 6 caption: Separated solution of the vertical displacement. (a) First five
    modes depending on space. (b) First five modes depending on the position of the
    load. (c) Built solution using the sum of the product of modes. Note how the result
    of the PGD approximation provides with a sort of response surface, without the
    need of any parameter space sampling strategy. Finally, a slice of the surface
    with the prescribed load position s 0 builds the solution.
  Figure 7 Link: articels_figures_by_rev_year\2021\MORPHDSLAM_Model_Order_Reduction_for_PhysicsBased_Deformable_SLAM\figure_7.jpg
  Figure 7 caption: Deformable object tracking and augmentation procedure based in
    ORB features matching.
  Figure 8 Link: articels_figures_by_rev_year\2021\MORPHDSLAM_Model_Order_Reduction_for_PhysicsBased_Deformable_SLAM\figure_8.jpg
  Figure 8 caption: Transformations and projection of the precomputed sensitivities
    from the original system of coordinates to the image reference.
  Figure 9 Link: articels_figures_by_rev_year\2021\MORPHDSLAM_Model_Order_Reduction_for_PhysicsBased_Deformable_SLAM\figure_9.jpg
  Figure 9 caption: Cartesian mean 3D error in displacements due to the projection
    of the solution in the reduced space respect to the number of modes.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Alberto Badias
  Name of the last author: "El\xEDas Cueto"
  Number of Figures: 15
  Number of Tables: 0
  Number of authors: 5
  Paper title: 'MORPH-DSLAM: Model Order Reduction for Physics-Based Deformable SLAM'
  Publication Date: 2021-10-08 00:00:00
  Table 1 caption: Not Available
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3118802
- Affiliation of the first author: universitas indonesia, kota depok, jb, indonesia
  Affiliation of the last author: department of computer science, graduate school
    at shengzhen, tsinghua university, shenzhen, china
  Figure 1 Link: articels_figures_by_rev_year\2021\GCP_Graph_Encoder_With_ContentPlanning_for_Sentence_Generation_From_Knowledge_Ba\figure_1.jpg
  Figure 1 caption: Data-to-text sentence generation from knowledge base triples based
    on an encoder-decoder architecture.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\GCP_Graph_Encoder_With_ContentPlanning_for_Sentence_Generation_From_Knowledge_Ba\figure_2.jpg
  Figure 2 caption: The construction of a word-entity graph.
  Figure 3 Link: articels_figures_by_rev_year\2021\GCP_Graph_Encoder_With_ContentPlanning_for_Sentence_Generation_From_Knowledge_Ba\figure_3.jpg
  Figure 3 caption: A small knowledge graph formed by a set of triples.
  Figure 4 Link: articels_figures_by_rev_year\2021\GCP_Graph_Encoder_With_ContentPlanning_for_Sentence_Generation_From_Knowledge_Ba\figure_4.jpg
  Figure 4 caption: The proposed GCP model.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Bayu Distiawan Trisedya
  Name of the last author: Rui Zhang
  Number of Figures: 4
  Number of Tables: 5
  Number of authors: 4
  Paper title: 'GCP: Graph Encoder With Content-Planning for Sentence Generation From
    Knowledge Bases'
  Publication Date: 2021-10-08 00:00:00
  Table 1 caption: TABLE 1 Data-to-Text Generation From Knowledge Base Triples
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison of Model Performance
  Table 3 caption: TABLE 3 Sample Output of the System
  Table 4 caption: TABLE 4 Human Evaluation Results
  Table 5 caption: TABLE 5 The Results of Different Content-Planning Mechanism
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3118703
- Affiliation of the first author: department of electrical and computer engineering,
    shanghai jiao tong university, shanghai, china
  Affiliation of the last author: qing yuan research institute, moe key lab of artificial
    intelligence, ai institute, shanghai jiao tong university, shanghai qi zhi institute,
    shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Learning_SingleMultiAttribute_of_Object_With_Symmetry_and_Group\figure_1.jpg
  Figure 1 caption: "Except for the compositionality and contextuality, attribute-object\
    \ compositions also have the symmetry property. For instance, a peeled-apple should\
    \ not change after \u201Cadding\u201D the peeled attribute. Similarly, an apple\
    \ should keep the same after \u201Cremoving\u201D the peeled attribute because\
    \ it does not have it."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Learning_SingleMultiAttribute_of_Object_With_Symmetry_and_Group\figure_2.jpg
  Figure 2 caption: "Overview of our proposed method. We construct a \u201Cgroup\u201D\
    \ to learn the symmetry and operate the composition learning. The attribute transformations\
    \ are implemented as coupling and decoupling networks and constrained by symmetry\
    \ and group axiom objectives. Then relative moving distance based paradigm is\
    \ applied in attribute classification."
  Figure 3 Link: articels_figures_by_rev_year\2021\Learning_SingleMultiAttribute_of_Object_With_Symmetry_and_Group\figure_3.jpg
  Figure 3 caption: The structure of CoN and DecoN. They take the attribute embedding
    to assign a specific attribute a j . f i o , f ij o are the object embeddings
    extracted from ResNet [41]. The attribute embeddings are converted to attentions
    and applied on object embeddings, then compressed by MLPs to output transformed
    representations.
  Figure 4 Link: articels_figures_by_rev_year\2021\Learning_SingleMultiAttribute_of_Object_With_Symmetry_and_Group\figure_4.jpg
  Figure 4 caption: "Comparison between typical method and relative moving distance\
    \ (RMD) based recognition. Previous methods mainly try to adjust the decision\
    \ boundary. RMD based approach moves the embedding with T + and T \u2212 and classifies\
    \ by comparing their moving distances."
  Figure 5 Link: articels_figures_by_rev_year\2021\Learning_SingleMultiAttribute_of_Object_With_Symmetry_and_Group\figure_5.jpg
  Figure 5 caption: Attribute correlation matrices of aPY [6] and SUN [7]. The correlation
    can be positive or negative ([-1,1], green to red). We calculate the mean and
    variance of the absolute correlation values, which show that aPY [6] contains
    stronger but more variable correlations.
  Figure 6 Link: articels_figures_by_rev_year\2021\Learning_SingleMultiAttribute_of_Object_With_Symmetry_and_Group\figure_6.jpg
  Figure 6 caption: "Under multi-attributelabel setting, RMD need to consider more\
    \ constraints since the existence of attribute correlation. (Left) For example,\
    \ for a banana with existing attributes X , since X are closely related to rotten\
    \ but have weak connection with green, the results differ when \u201Cremoving\u201D\
    \ attribute rotten and green respectively. In single-attribute RMD, the results\
    \ should be comparably close to the original banana, since this banana has neither\
    \ attribute rotten nor green. But in multi-attribute RMD, the moving distance\
    \ depends on the similarity of X and the operated attribute. In general, removing\
    \ an attribute which is more correlated to X will make a larger difference, and\
    \ the less correlated removing would make a minor difference. (Right) Meanwhile,\
    \ for operated attributes, correlation would also affect the generated attentions:\
    \ more correlated attributes should generate more similar attentions."
  Figure 7 Link: articels_figures_by_rev_year\2021\Learning_SingleMultiAttribute_of_Object_With_Symmetry_and_Group\figure_7.jpg
  Figure 7 caption: The moving distances after attribute removal on multi-attribute
    benchmarks. The distance is always positive, and the correlation can be positive
    ( >0 ) or negative ( <0 ). According to our setting, larger positive correlation
    would generate larger moving distance in removal, e.g., removing attribute rotten
    in Fig. 6 (left). Oppositely, the smaller negative correlation could make the
    moving distance smaller, e.g., removing attribute green in Fig. 6 (left). The
    bold dots indicate the centroids of the dots with top-10% and down-10% correlation
    values. Thus, the line linked to two bold points should have a smaller slope ideally.
    The model is trained with single (left, red) or multiple (right, blue) RMD settings.
    We can find that the multi-attribute RMD generate more reasonable distances, i.e.,
    has a smaller slope (blue lines).
  Figure 8 Link: articels_figures_by_rev_year\2021\Learning_SingleMultiAttribute_of_Object_With_Symmetry_and_Group\figure_8.jpg
  Figure 8 caption: Visualization of symmetry, group axioms and attention by t-SNE
    [46]. In (a)-(d), points with colors in the same dotted box should be close according
    to the corresponding learning principles. Especially, (e) and (f) shows the RMD
    property in the multi-attribute setting where the red dot is the original object
    embedding and the other dots are the embeddings after attribute removals. For
    example, if attribute a i has a larger correlation to the existing attributes
    of this object, as corr( a i ,X) is large, then the corresponding dot is in a
    darker color and has a larger distance after removal. On the contrary, the dot
    of removing attribute a j with smaller corr( a j ,X) is represented in a lighter
    color and closer to the origin. However, without the L sym trip , the distribution
    of attribute removal is much noisier. (g) and (f) are the distributions of attention
    representation. Attentions of more correlated attributes are closer in latent
    space, e.g., leaves and flowers, but this property cannot keep without L corr
    tri .
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Yong-Lu Li
  Name of the last author: Cewu Lu
  Number of Figures: 8
  Number of Tables: 7
  Number of authors: 5
  Paper title: Learning SingleMulti-Attribute of Object With Symmetry and Group
  Publication Date: 2021-10-12 00:00:00
  Table 1 caption: TABLE 1 Attribute Learning Results (Accuracy, %) on Single-Attribute
    Benchmarks
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Attribute Learning Results (mAUC, %) on Multi-Attribute
    Benchmarks
  Table 3 caption: TABLE 3 CZSL Results (Top-k Accuracy, %) on MIT-States [10] and
    UT-Zappos [11]
  Table 4 caption: TABLE 4 Results of Generalized CZSL on MIT-States [10] Following
    [42]
  Table 5 caption: TABLE 5 Results of Generalized CZSL on UT-Zappos [11] Following
    [45]
  Table 6 caption: TABLE 6 Results of Few-Shot Recognition on CUB-200-2011[9]
  Table 7 caption: TABLE 7 Ablation Studies on CZSL and Multi-Attribute Learning
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3119406
- Affiliation of the first author: school of computer science, wuhan university, wuhan,
    china
  Affiliation of the last author: centre for artificial intelligence, feit, university
    of technology sydney, ultimo, nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2021\The_Emerging_Trends_of_MultiLabel_Learning\figure_1.jpg
  Figure 1 caption: The structure of this paper.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\The_Emerging_Trends_of_MultiLabel_Learning\figure_2.jpg
  Figure 2 caption: Illustration of some MLC settings with different types of supervision.
    (a) instances with full supervision; (b) instances with explicitly missing labels;
    (c) instances with implicitly missing labels; (d) instances with a set of candidate
    relevant labels. Here (b) and (c) are two different settings of MLML problems.
    Semi-supervised MLC is a special case of (b) where some instances miss all the
    labels.
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Weiwei Liu
  Name of the last author: Ivor W. Tsang
  Number of Figures: 2
  Number of Tables: 1
  Number of authors: 4
  Paper title: The Emerging Trends of Multi-Label Learning
  Publication Date: 2021-10-12 00:00:00
  Table 1 caption: "TABLE 1 The Training and Testing Time Complexity of XMLC Methods\
    \ ( nnz(X) nnz(X) Denotes the Number of Non-Zeros in X X, C C is a Constant, O(\u03B6\
    ) O(\u03B6) Denotes the Computational Complexity of \u03D6 \u03D6-bit Hamming\
    \ Distance Calculation. T T is the Number of Trees. h h is the Number of Levels\
    \ in the Tree. c c is the Number of Top-Scoring Items Being Reranked by the Base-Classifiers.\
    \ k\u226AL k\u226AL is a Small Constant)"
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3119334
- Affiliation of the first author: school of electrical engineering, korea advanced
    institute of science and technology, daejeon, republic of korea
  Affiliation of the last author: school of electrical engineering, korea advanced
    institute of science and technology, daejeon, republic of korea
  Figure 1 Link: articels_figures_by_rev_year\2021\Dense_Relational_Image_Captioning_via_MultiTask_TripleStream_Networks\figure_1.jpg
  Figure 1 caption: Difference of our proposed relational captioning from existing
    image understanding frameworks. Compared to traditional frameworks, our work is
    advantageous in both interaction understanding and high-level expressive interpretation.
  Figure 10 Link: articels_figures_by_rev_year\2021\Dense_Relational_Image_Captioning_via_MultiTask_TripleStream_Networks\figure_10.jpg
  Figure 10 caption: Visualization of POS importance transition. Y -axis represents
    respective representative hidden values of Subject-Predicate-Object LSTMs, and
    X -axis represents words of each caption in order. subj-pred-obj are color-coded
    by red, green, and blue colors according to the output of the POS predictor, respectively.
    Each word in the captions comes from the corresponding LSTM.
  Figure 2 Link: articels_figures_by_rev_year\2021\Dense_Relational_Image_Captioning_via_MultiTask_TripleStream_Networks\figure_2.jpg
  Figure 2 caption: Overall architecture of the proposed multi-task triple-stream
    networks (MTTSNet). Three region features (Union, Subject, Object) come from the
    same shared branch (Region Proposal Network), and for subject and object features,
    the first intermediate FC layer share the weights. Relational Embedding Module
    (REM) is introduced as an extension, which takes into account early dependency
    between subject and object.
  Figure 3 Link: articels_figures_by_rev_year\2021\Dense_Relational_Image_Captioning_via_MultiTask_TripleStream_Networks\figure_3.jpg
  Figure 3 caption: 'An illustration of the unrolled triple-stream LSTM. Our model
    consists of two major parts: triple-stream LSTM and a multi-task module. The multi-task
    module jointly predicts a caption word and its POS class (subj-pred-obj, illustrated
    as three cells colored according to the POS class), as well as the input vector
    for the next time step.'
  Figure 4 Link: articels_figures_by_rev_year\2021\Dense_Relational_Image_Captioning_via_MultiTask_TripleStream_Networks\figure_4.jpg
  Figure 4 caption: "Architecture of the relational embedding module (REM). \u2A02\
    \ denotes the matrix multiplication, and \u2A01 the element-wise sum. The softmax\
    \ operation is applied row-wise. The blue boxes with the FC label denote FC layers."
  Figure 5 Link: articels_figures_by_rev_year\2021\Dense_Relational_Image_Captioning_via_MultiTask_TripleStream_Networks\figure_5.jpg
  Figure 5 caption: Example captions and region generated by the proposed model on
    Visual Genome test images. The region detection and caption results are obtained
    by the proposed model from Visual Genome test images. We compare our result with
    the image captioner [65] and the dense captioner [25] in order to contrast the
    amount of information and diversity.
  Figure 6 Link: articels_figures_by_rev_year\2021\Dense_Relational_Image_Captioning_via_MultiTask_TripleStream_Networks\figure_6.jpg
  Figure 6 caption: "Results of generating \u201Ccaption graph\u201D from our relational\
    \ captioniner. In order to compare the diversity of the outputs, we also show\
    \ the result of the scene graph generator, Neural Motifs [80]."
  Figure 7 Link: articels_figures_by_rev_year\2021\Dense_Relational_Image_Captioning_via_MultiTask_TripleStream_Networks\figure_7.jpg
  Figure 7 caption: Examples of different captions predicted from relational captioning
    by (a) changing objects, (b) changing subjects, and (c) switching the subject
    and object. Our model shows different predictions from different subject and object
    pairs.
  Figure 8 Link: articels_figures_by_rev_year\2021\Dense_Relational_Image_Captioning_via_MultiTask_TripleStream_Networks\figure_8.jpg
  Figure 8 caption: Sentence based image and region-pair retrieval results on Visual
    Genome test images. The retrieved results are shown in the ranked order.
  Figure 9 Link: articels_figures_by_rev_year\2021\Dense_Relational_Image_Captioning_via_MultiTask_TripleStream_Networks\figure_9.jpg
  Figure 9 caption: Qualitative comparison with visual relationship detection model
    [45]. The proposed relational captioning model is able to provide more detailed
    information than the traditional relationship detection model.
  First author gender probability: 0.7
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.88
  Name of the first author: Dong-Jin Kim
  Name of the last author: In So Kweon
  Number of Figures: 11
  Number of Tables: 7
  Number of authors: 4
  Paper title: Dense Relational Image Captioning via Multi-Task Triple-Stream Networks
  Publication Date: 2021-10-14 00:00:00
  Table 1 caption: TABLE 1 Comparison of Model Configurations
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Ablation Study for the Relational Dense Captioning Task
    on the Relational Captioning Dataset
  Table 3 caption: TABLE 3 Comparisons of the Holistic Level Image Captioning
  Table 4 caption: TABLE 4 Sentence Based Image Retrieval Performance Comparison Across
    Different Representations
  Table 5 caption: TABLE 5 Ablation Study on the Relational Dense Captioning Task
    With the VRD Dataset
  Table 6 caption: TABLE 6 Comparison of Our MTTSNet With VRD Models on the VRD Metrics
    on the VRD Dataset
  Table 7 caption: TABLE 7 Diversity Comparison Between Image Captioning, Scene Graph
    Generation, Dense Captioning, and Relational Captioning Frameworks
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3119754
- Affiliation of the first author: computer science, university of illinois at urbana
    champaign, champaign, il, usa
  Affiliation of the last author: computer science, university of illinois at urbana
    champaign, champaign, il, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\Intrinsic_Image_Decomposition_Using_Paradigms\figure_1.jpg
  Figure 1 caption: Samples from our paradigms, without curation. The first five samples
    on the Alb row are from the kd-tree model; the next five from the generalized
    Mondrian model. The Alb-S row contains samples from the simplified albedo paradigm,
    where parameters are chosen to produce very few albedo cells per sample. This
    is used to investigate CGI effects. Sha-h, Sha-m and Sha-l differ by the brightness
    of the darkest pixel (darkest to lightest; the brightest pixel in each image is
    one) and so in contrast (highest to lowest). Parameter details in Section 3.1.
    Two qualitative differences between our models and CGI images (from [1]) are apparent.
    First, the CGI albedos have relatively few edges, and many are low contrast (textures
    in col. 1, 2, 6 for example). Second, there is significant spatial noise in some
    shading images.
  Figure 10 Link: articels_figures_by_rev_year\2021\Intrinsic_Image_Decomposition_Using_Paradigms\figure_10.jpg
  Figure 10 caption: Varying the scale of the discriminator has an important effect
    on performance. SD the discriminator sees 10times 10 patches; BI our chosen scale,
    22times 22 ; ID 29 times 29 ; MD 48 times 48 ; and BD 128times 128 . The scale
    of ID was chosen by interpolating oracle WHDR for the others, then choosing the
    scale that produced the best predicted WHDR. The red boxes show the scale of the
    discriminator patches with respect to the tile (black boxes) for each model. Graphical
    conventions as in Fig. 9. Best viewed in color.
  Figure 2 Link: articels_figures_by_rev_year\2021\Intrinsic_Image_Decomposition_Using_Paradigms\figure_2.jpg
  Figure 2 caption: Our network decomposes an image tile into albedo and shading using
    a U-net with one encoder and two decoders, and skip connections (A). Each training
    batch for the network contains two kinds of data. A synthetic image (B) is obtained
    by multiplying a shading paradigm by an albedo paradigm. In this case, the network
    must minimize a residual loss (B, top row) and one of a loss comparing the shading
    to the shading paradigm (C) and a loss comparing the albedo to the albedo paradigm
    (D). The choice of loss is made randomly, per example, at training time. For a
    real image (E), neither albedo nor shading are known. In this case, the network
    must minimize a residual loss and an adversarial loss (F). The adversarial loss
    tries to force the decomposition results to match albedo and shading paradigms
    at a short spatial scale (G). The images are decomposed by computing albedo and
    shading tiles at various scales and translations within the image, then averaging
    (Section 3.4).
  Figure 3 Link: articels_figures_by_rev_year\2021\Intrinsic_Image_Decomposition_Using_Paradigms\figure_3.jpg
  Figure 3 caption: 'Albedo and shading estimates for a subset of IIW images, curated
    for qualitative effects. Note: strong suppression of shading on material folds
    (a, b); strong suppression of smooth shadows and glint (c, d, e, f); suppression
    of reflected smooth shadows (d - in mirror); error at sharp shadow boundaries
    (f, g - ceiling); apparent flattening (c, e, h). Shading fields are mostly smooth,
    but have some higher contrast edges.'
  Figure 4 Link: articels_figures_by_rev_year\2021\Intrinsic_Image_Decomposition_Using_Paradigms\figure_4.jpg
  Figure 4 caption: 'Albedo and shading estimates for a subset of IIW images, curated
    for qualitative effects. Note: strong suppression of fairly sharp shadows (a,
    j); strong suppression of smooth shadows and glint (b, c, d, e, f, j); error at
    sharp shadow boundaries (d, e, g, i); incorrect handling of strong glint (c);
    apparent flattening (all). Shading fields are mostly smooth, but have some higher
    contrast edges.'
  Figure 5 Link: articels_figures_by_rev_year\2021\Intrinsic_Image_Decomposition_Using_Paradigms\figure_5.jpg
  Figure 5 caption: "Qualitative comparison to [18], [19], [50], [51] and [52], using\
    \ parts of Fig. 1 of [18]. As [18] remark, the methods of [19] and [50] are trained\
    \ on rendered data alone, and face difficulties due to the difference between\
    \ rendered data and real images; furthermore, the methods of [50] and [51] face\
    \ difficulties due to the deep shadows in the scene. The albedo produced by our\
    \ method does not show the \u201Ccolored paper\u201D effect seen in other methods\
    \ and does not produce odd colors; this is an advantage (text). Our method reports\
    \ albedo and shading up to image boundaries, that of [18] appears not to (the\
    \ crop of the figures is as in the original paper; for our method, we show the\
    \ whole image). Bi et al. [18] report the per-image WHDR for this image; our per-image\
    \ WHDR (at 16.1 for threshold 0.1; 7 for threshold 0.165) is in each case weaker\
    \ than that of Bi et al."
  Figure 6 Link: articels_figures_by_rev_year\2021\Intrinsic_Image_Decomposition_Using_Paradigms\figure_6.jpg
  Figure 6 caption: "Qualitative comparison to [11], [16] and [44], using in part\
    \ Fig. 14 of [44]. The method of [44] is more successful than others at suppressing\
    \ this complex mixed shadow, but produces \u201Ccolored paper\u201D effects in\
    \ the albedo. The method of [11] does not handle the shadows well; the method\
    \ of [16] is better, but washes out the albedo. By comparison, our method is moderately\
    \ successful on this challenging image. Best viewed in color."
  Figure 7 Link: articels_figures_by_rev_year\2021\Intrinsic_Image_Decomposition_Using_Paradigms\figure_7.jpg
  Figure 7 caption: A naive application of our adversarial smoothing procedure works
    poorly. For the image shown we compare albedo reconstructions from a reference
    model (Best, our best) with others. Model 1 and Model 0 are different checkpoints,
    separated by approximately 10,000 training images; notice how there are significant
    long scale differences, caused by the fact that the adversarial smoothing does
    not identify a unique best model. An exponential moving average resolves this
    effect (Section 3.3).
  Figure 8 Link: articels_figures_by_rev_year\2021\Intrinsic_Image_Decomposition_Using_Paradigms\figure_8.jpg
  Figure 8 caption: Equivariance failures are severe and punishing. For the image
    shown we compare albedo reconstructions from a reference model (Best, our best)
    with others. BR and TL show the results of cutting the image into two overlapping
    tiles, and passing each through the network; comparing these shows a severe failure
    of translation equivariance. Rescale shows the (rescaled) albedo for an image
    that was rescaled down by 1.4, then decomposed. Comparing this to the result of
    model 0 of Fig. 7 shows a severe failure of scale equivariance. Similarly, Flip
    shows the (reflected) albedo for an image that was reflected in both axes, then
    passed through model 0; comparing this to the result of model 0 shows a severe
    failure of rotation equivariance. The symptom of these equivariance failures is
    error at a long spatial scale; this causes poor WHDR scores. Our strong WHDR performance
    shows that the tile averaging of Section 3.4 controls these effects (Section 3.5).
  Figure 9 Link: articels_figures_by_rev_year\2021\Intrinsic_Image_Decomposition_Using_Paradigms\figure_9.jpg
  Figure 9 caption: "Smoothing, averaging and postprocessing are important. A method\
    \ that sees no adversarial smoothing (NS) is about as strong as Retinex. NP sees\
    \ paradigm images in training only via the adversary, and is surprisingly well\
    \ behaved. Averaging makes a very significant difference (compare blueblack bars\
    \ and purplegreen bars). There is very little difference between oracle and held-out\
    \ threshold performance, implying that WHDR changes very slowly with threshold.\
    \ Key: Fixed thresholds: shown in boxplots of WHDR values for 50 simulated test\
    \ sets for the two fixed thresholds, and green bars are the value for the standard\
    \ test set. Oracle thresholds: heavy black bar. Held out threshold: heavy red\
    \ bar. Oracle threshold without smoothing: heavy blue dashed bar. Fixed threshold\
    \ without smoothing: heavy purple bar. Boxplots: horizontal bar = median; notch\
    \ = fraction of interquartile range outside which a difference in medians is significant;\
    \ bottom and top of the box = 25 and 75 percentiles resp.; whiskers extend to\
    \ the most extreme data points that are not outliers; outliers \u2013 greater\
    \ than 1.5 times the interquartile range outside top and bottom \u2013 are +.\
    \ Best viewed in color."
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: David Forsyth
  Name of the last author: Jason J. Rock
  Number of Figures: 13
  Number of Tables: 3
  Number of authors: 2
  Paper title: Intrinsic Image Decomposition Using Paradigms
  Publication Date: 2021-10-14 00:00:00
  Table 1 caption: TABLE 1 Summary Comparison to Recent High Performing Supervised
    (Above) and Unsupervised (Below) Methods, all Evaluated on the Standard IIW Test
    Set; Sources Indicated
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Key to Models Used in Initial Experiments
  Table 3 caption: TABLE 3 Key to Refined Models
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3119551
- Affiliation of the first author: college of intelligence and computing, tianjin
    university, tianjin, china
  Affiliation of the last author: department of computer science, stony brook university,
    stony brook, ny, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\Detection_and_Tracking_Meet_Drones_Challenge\figure_1.jpg
  Figure 1 caption: Annotated example images in the proposed datasets. The dashed
    bounding box indicates the object is occluded. Different colors indicate different
    classes of objects. For better visualization, only a few attributes are displayed.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Detection_and_Tracking_Meet_Drones_Challenge\figure_2.jpg
  Figure 2 caption: The number of objects with different occlusion degrees of different
    object categories in the subsets of the DET track.
  Figure 3 Link: articels_figures_by_rev_year\2021\Detection_and_Tracking_Meet_Drones_Challenge\figure_3.jpg
  Figure 3 caption: Descriptions of the challenging issues in the image object detection
    task.
  Figure 4 Link: articels_figures_by_rev_year\2021\Detection_and_Tracking_Meet_Drones_Challenge\figure_4.jpg
  Figure 4 caption: The number of object trajectories in different categories in the
    subsets of the VID and MOT tracks.
  Figure 5 Link: articels_figures_by_rev_year\2021\Detection_and_Tracking_Meet_Drones_Challenge\figure_5.jpg
  Figure 5 caption: (a) The number of frames versus the aspect ratio (height divided
    by width) change rate with respect to the first frame, (b) the number of frames
    versus the area variation rate with respect to the first frame, and (c) the distributions
    of the number of frames of video clips, in the subsets for the SOT track.
  Figure 6 Link: articels_figures_by_rev_year\2021\Detection_and_Tracking_Meet_Drones_Challenge\figure_6.jpg
  Figure 6 caption: The success versus precision scores of (a) the top 10 trackers
    in VisDrone-SOT2018 (red marks), VisDrone-SOT2019 (blue marks) and VisDrone-SOT2020
    (purple marks) on the test-challenge 2018 set, (b) the top 10 trackers in VisDrone-SOT2019
    (blue marks) and VisDrone-SOT2020 (purple marks) on the test-challenge 2019 set,
    (c) 21 state-of-the-art trackers on the test-dev set.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.95
  Name of the first author: Pengfei Zhu
  Name of the last author: Haibin Ling
  Number of Figures: 6
  Number of Tables: 5
  Number of authors: 7
  Paper title: Detection and Tracking Meet Drones Challenge
  Publication Date: 2021-10-14 00:00:00
  Table 1 caption: TABLE 1 Comparison of the Existing Benchmarks and Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison Results (i.e., Percentage of AP Scores) of the
    Algorithms on the VisDrone-DET Dataset
  Table 3 caption: TABLE 3 Comparison Results (i.e., Percentage of AP Scores) of the
    Algorithms on the VisDrone-VID Dataset
  Table 4 caption: TABLE 4 Comparisons Results of the Algorithms on the VisDrone-MOT
    Dataset Using the Evaluation Protocol in [131]
  Table 5 caption: TABLE 5 Comparisons Results of the Algorithms on the VisDrone-MOT
    Dataset Using the CLEAR-MOT Evaluation Protocol [24]
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3119563
- Affiliation of the first author: school of computer science and engineering, southeast
    university, nanjing, china
  Affiliation of the last author: college of computer and information science, southwest
    university, chongqing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Adaptive_Graph_Guided_Disambiguation_for_Partial_Label_Learning\figure_1.jpg
  Figure 1 caption: 'Label disambiguation for partially labeled examples. Left: The
    instances are assigned with multiple candidate labels. Right: The ground-truth
    labeling confidences are expected to be recovered after label disambiguation.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Adaptive_Graph_Guided_Disambiguation_for_Partial_Label_Learning\figure_2.jpg
  Figure 2 caption: There real-world partial label learning scenarios. (a) In Part-of-speech
    (POS) tagging, the predictive model will be learned given a dictionary of words
    with their possible POS tags; (b) In automatic face naming, the predictive model
    will be learned with the imagecaption pair collected from websites; (c) In crowdsourcing-based
    classification, the predictive model will be learned from inaccurate labeling
    information.
  Figure 3 Link: articels_figures_by_rev_year\2021\Adaptive_Graph_Guided_Disambiguation_for_Partial_Label_Learning\figure_3.jpg
  Figure 3 caption: The optimization procedure of our PL-AGGD approach. From this
    illustration we can see in our approach, the similarity weights between examples
    are dynamically updated with the model training and label disambiguation.
  Figure 4 Link: articels_figures_by_rev_year\2021\Adaptive_Graph_Guided_Disambiguation_for_Partial_Label_Learning\figure_4.jpg
  Figure 4 caption: "The first two rows illustrate the classification accuracy of\
    \ each comparing algorithm with varying \u03F5 (co-occurring probability of the\
    \ coupling label) and fixed r and p ( r=1,p=1 ).The last two rows illustrate the\
    \ classification accuracy of each comparing algorithm with varying p (the proportion\
    \ of examples which are partially labeled) and fixed r and \u03F5 ( r=2 for vehicle\
    \ and sensor, r=5 for others, \u03F5= 1 r )."
  Figure 5 Link: articels_figures_by_rev_year\2021\Adaptive_Graph_Guided_Disambiguation_for_Partial_Label_Learning\figure_5.jpg
  Figure 5 caption: "Parameter sensitivity analysis for PL-AGGD. (a) Classification\
    \ accuracy of PL-AGGD on Lost by varying \u03BB and \u03BC ; (b) Classification\
    \ accuracy of PL-AGGD on BirdSong by varying \u03BB and \u03BC ; (c) Classification\
    \ accuracy of PL-AGGD on MSRCv2 by varying \u03BB and \u03BC ; (d) Classification\
    \ accuracy of PL-AGGD on Lost, BirdSong and MSRCv2 by varying \u03B3 ; (e) Classification\
    \ accuracy of PL-AGGD on Lost, BirdSong and MSRCv2 by varying k ; (f) Convergence\
    \ curves of the labeling scores on Lost, BirdSong and MSRCv2."
  Figure 6 Link: articels_figures_by_rev_year\2021\Adaptive_Graph_Guided_Disambiguation_for_Partial_Label_Learning\figure_6.jpg
  Figure 6 caption: Active learning performance of each comparing query strategy.
    We compare 3 active query strategies, Margin-based Uncertainty query strategy,
    Entropy-based Uncertainty query strategy and Random query strategy, as well as
    vanilla PL-AGGD method without active query.
  Figure 7 Link: articels_figures_by_rev_year\2021\Adaptive_Graph_Guided_Disambiguation_for_Partial_Label_Learning\figure_7.jpg
  Figure 7 caption: 'The accuracy curves of our method AdaKerI and FixKerI in active
    learning setting with varied population of selectively queried instances. Top:
    Random query strategy. Middle: Entropy-Based Uncertainty query strategy. Bottom:
    Margin-based Uncertainty query strategy.'
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.66
  Name of the first author: Deng-Bao Wang
  Name of the last author: Li Li
  Number of Figures: 7
  Number of Tables: 12
  Number of authors: 3
  Paper title: Adaptive Graph Guided Disambiguation for Partial Label Learning
  Publication Date: 2021-10-14 00:00:00
  Table 1 caption: TABLE 1 Summary of Major Mathematical Notations
  Table 10 caption: "TABLE 10 Transductive Accuracy (Mean \xB1 \xB1std) of Each Comparing\
    \ Algorithm on the Real-World Partial Label Data Sets"
  Table 2 caption: TABLE 2 The Pseudo-Code of PL-AGGD
  Table 3 caption: TABLE 3 The Pseudo-Code of PL-AGGD + +
  Table 4 caption: TABLE 4 Characteristics of the UCI Data Sets
  Table 5 caption: "TABLE 5 Classification Accuracy (mean \xB1 \xB1std) of Each Comparing\
    \ Algorithm on the Controlled UCI Data Sets With Varying r r (False Positive Candidate\
    \ Label)"
  Table 6 caption: TABLE 6 WintieLoss Counts on the Classification Performance of
    PL-AGGD Against Each Comparing Algorithm on Controlled UCI Data Sets
  Table 7 caption: TABLE 7 Characteristics of the Real-World Data Sets
  Table 8 caption: "TABLE 8 Classification Accuracy (Mean \xB1 \xB1std) of Each Comparing\
    \ Algorithm on the Real-World Partial Label Data Sets"
  Table 9 caption: TABLE 9 WinTieLoss Counts on the Transductive Performance of PL-AGGD
    Against Each Comparing Algorithm on Controlled UCI Data Sets
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3120012
