- Affiliation of the first author: viterbi faculty of electrical engineering, technion
    - israel institute of technology, haifa, israel
  Affiliation of the last author: department of intelligence science and technology,
    kyoto university, kyoto, japan
  Figure 1 Link: Not Available
  Figure 1 caption: Not Available
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.8
  Name of the first author: Yoav Y. Schechner
  Name of the last author: Ko Nishino
  Number of Figures: 0
  Number of Tables: 0
  Number of authors: 5
  Paper title: 'Guest Editorial: Introduction to the Special Section on Computational
    Photography'
  Publication Date: 2021-06-08 00:00:00
  Table 1 caption: Not Available
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3078707
- Affiliation of the first author: salesforce, sea ai lab of sea group, singapore,
    singapore
  Affiliation of the last author: salesforce research, singapore, singapore
  Figure 1 Link: articels_figures_by_rev_year\2021\A_Hybrid_StochasticDeterministic_Minibatch_Proximal_Gradient_Method_for_Efficien\figure_1.jpg
  Figure 1 caption: "Investigation of \u03BD and h : stochastic gradient algorithms\
    \ process data multiple pass on quadratic problems ( letter and protein ) and\
    \ logistic regression problems ( ijcnn and w8a ) where their regularization constant\
    \ \u03BC is \u03BC= 10 \u22124 . We define \u039B 1 = 1 n \u2211 n i=1 \u2225\
    \ H \u221212 (\u2207F(\u03B8)\u2212\u2207 \u2113 i (\u03B8)\u2212\u03C4\u03BC\u03B8\
    ) \u2225 2 2 , \u039B 2 = 1 n \u2211 n i=1 \u2225\u2207F(\u03B8)\u2212\u2207 \u2113\
    \ i (\u03B8)\u2212\u03C4\u03BC\u03B8 \u2225 2 2 , the smallest eigenvalue \u03BB\
    \ min and average eigenvalue \u03BB avg of the Hessian H at each iteration. One\
    \ can observe that \u03BD 2 (=max \u039B 1 ) is often of the order O(1 \u03BB\
    \ avg ) and is much smaller than O(1 \u03BB min ) and h 2 is often at the same\
    \ order as \u03BD 2 . Best viewed in \xD72 sized color pdf file."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\A_Hybrid_StochasticDeterministic_Minibatch_Proximal_Gradient_Method_for_Efficien\figure_2.jpg
  Figure 2 caption: 'Single-epoch processing: stochastic gradient algorithms process
    data a single pass on quadratic problems.'
  Figure 3 Link: articels_figures_by_rev_year\2021\A_Hybrid_StochasticDeterministic_Minibatch_Proximal_Gradient_Method_for_Efficien\figure_3.jpg
  Figure 3 caption: 'Multi-epoch processing: stochastic gradient algorithms process
    data multiple pass on quadratic problems.'
  Figure 4 Link: articels_figures_by_rev_year\2021\A_Hybrid_StochasticDeterministic_Minibatch_Proximal_Gradient_Method_for_Efficien\figure_4.jpg
  Figure 4 caption: "Investigation of the effects of the regularization constant \u03B7\
    \ in the Bregman divergence in Eqn. (4) to the performance of HSDMPG. The test\
    \ problems are quadratic problems with with regularization constant \u03BC= 10\
    \ \u22124 on letter and satimage ."
  Figure 5 Link: articels_figures_by_rev_year\2021\A_Hybrid_StochasticDeterministic_Minibatch_Proximal_Gradient_Method_for_Efficien\figure_5.jpg
  Figure 5 caption: 'Multi-epoch processing (about 8 epochs): stochastic gradient
    algorithms process data multiple pass on logistic regression problems ( ijcnn
    and w08 ) and softmax regression problems ( protein and letter ).'
  Figure 6 Link: articels_figures_by_rev_year\2021\A_Hybrid_StochasticDeterministic_Minibatch_Proximal_Gradient_Method_for_Efficien\figure_6.jpg
  Figure 6 caption: 'Evaluation under online setting: stochastic gradient algorithms
    process data a single pass on quadratic problems ( SUSY ) and logistic regression
    problems ( HIGGS ).'
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Pan Zhou
  Name of the last author: Steven C.H. Hoi
  Number of Figures: 6
  Number of Tables: 2
  Number of authors: 4
  Paper title: A Hybrid Stochastic-Deterministic Minibatch Proximal Gradient Method
    for Efficient Optimization and Generalization
  Publication Date: 2021-06-08 00:00:00
  Table 1 caption: "TABLE 1 Comparison of IFO Complexity for First-Order Stochastic\
    \ Algorithms on the \u03BC \u03BC-Strongly-Convex Finite-Sum Problem (1) With\
    \ Linear Prediction Structure"
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Descriptions of the Twelve Testing Datasets
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3087328
- Affiliation of the first author: school of electronic and information engineering,
    south china university of technology, guangzhou, china
  Affiliation of the last author: school of mathematics and computer science, fuzhou
    university, fuzhou, china
  Figure 1 Link: articels_figures_by_rev_year\2021\SynSigVec_ForgeryFree_Learning_of_Dynamic_Signature_Representations_by_Sigma_Log\figure_1.jpg
  Figure 1 caption: "Our SynSig2Vec framework for learning dynamic signature representations.\
    \ SynSig2Vec consists of a novel learning-by-synthesis method for training and\
    \ the 1D CNN-based Sig2Vec model. Specifically, given a template signature, we\
    \ extract its \u03A3\u039B parameters and introduce perturbations of two strength\
    \ levels; two groups of distorted synthetic signatures can thus be reconstructed\
    \ from the perturbed parameters (green and blue colors indicate the dataflow of\
    \ high and low perturbation synthesis respectively), and the Sig2Vec model learns\
    \ to rank these synthesized samples with supervision information from the perturbation\
    \ process. Different colors in the SigVec model simply denote different feature\
    \ channels. The ranking branch optimizes the average precision of ranking, while\
    \ the classification branch performs ordinary classification of signature contents."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\SynSigVec_ForgeryFree_Learning_of_Dynamic_Signature_Representations_by_Sigma_Log\figure_2.jpg
  Figure 2 caption: The velocity profile of a typical human handwriting component
    consists of lognormal impulse responses, called strokes. Based on these strokes
    parameters, the velocity and trajectory can be recovered.
  Figure 3 Link: articels_figures_by_rev_year\2021\SynSigVec_ForgeryFree_Learning_of_Dynamic_Signature_Representations_by_Sigma_Log\figure_3.jpg
  Figure 3 caption: Visualization of synthesized signatures from three template signatures.
    Inside each box, the first signature is the template signature, while signatures
    in the second and third rows are from G 1 and G 2 , respectively. The signature
    images are kindly permitted by the writers (u1010, u1013 and u1028 in DeepSignDB)
    to be used here.
  Figure 4 Link: articels_figures_by_rev_year\2021\SynSigVec_ForgeryFree_Learning_of_Dynamic_Signature_Representations_by_Sigma_Log\figure_4.jpg
  Figure 4 caption: The architecture of the proposed Sig2Vec model as well as the
    SP module. Each convolutional layer is followed by the SELU activation function
    [37]. The feature sequence from the sixth convolutional layer is up-sampled and
    added to that of the fourth layer by a feature fusion layer, following [36]. Two
    SP modules are applied to the sixth convolutional layer and the feature fusion
    layer, respectively, leading to a 1024-dimensional signature representation. Inside
    each SP module, multi-head attention [24] with learnable queries is proposed to
    extract fixed-length vectors from feature sequences of arbitrary lengths.
  Figure 5 Link: articels_figures_by_rev_year\2021\SynSigVec_ForgeryFree_Learning_of_Dynamic_Signature_Representations_by_Sigma_Log\figure_5.jpg
  Figure 5 caption: Median DDRs of the learned representations. R i denotes the median
    DDR for the i th head, and R full for the full representation.
  Figure 6 Link: articels_figures_by_rev_year\2021\SynSigVec_ForgeryFree_Learning_of_Dynamic_Signature_Representations_by_Sigma_Log\figure_6.jpg
  Figure 6 caption: "Visualization of attention weights of the second SP module. We\
    \ simply up-sample the attention weights 4\xD7 to align with the signature trajectory,\
    \ and positions with larger attention weights are marked with darker dots. We\
    \ can observe that, the first query detects low-curvature bottom loops, the second\
    \ query detects high-curvature corners, while the third query detects low-curvature\
    \ upper loops. The signature images are kindly permitted by the writer (u1028\
    \ in DeepSignDB) to be used here."
  Figure 7 Link: articels_figures_by_rev_year\2021\SynSigVec_ForgeryFree_Learning_of_Dynamic_Signature_Representations_by_Sigma_Log\figure_7.jpg
  Figure 7 caption: Template signatures with shorter durations lead to verification
    errors more frequently.
  Figure 8 Link: articels_figures_by_rev_year\2021\SynSigVec_ForgeryFree_Learning_of_Dynamic_Signature_Representations_by_Sigma_Log\figure_8.jpg
  Figure 8 caption: System ROC curves on the DeepSignDB database.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.55
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.64
  Name of the first author: Songxuan Lai
  Name of the last author: Luojun Lin
  Number of Figures: 8
  Number of Tables: 8
  Number of authors: 5
  Paper title: 'SynSig2Vec: Forgery-Free Learning of Dynamic Signature Representations
    by Sigma Lognormal-Based Synthesis and 1D CNN'
  Publication Date: 2021-06-08 00:00:00
  Table 1 caption: TABLE 1 Configurations of the Random Variables That Decide the
    Signature Distortion Levels
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Signature Verification EERs (%), Using Real Handwritten
    or Synthetic Signatures for Sig2Vec Training With the AP Loss
  Table 3 caption: TABLE 3 Signature Verification EERs (%), Using Real Handwritten
    or Synthetic Signatures for Sig2Vec Training With the AP Loss
  Table 4 caption: TABLE 4 Comparison of the AP Loss, BCE and the Triplet Loss on
    Sig2Vec Training
  Table 5 caption: TABLE 5 Comparison of the Selective Pooling Module and Traditional
    Average Pooling on the DeepSignDB Database (EER, %)
  Table 6 caption: TABLE 6 Comparison of DTW, TA-RNN and Our Proposed SynSig2Vec on
    the DeepSignDB Database (EER, %)
  Table 7 caption: TABLE 7 Comparison of DTW, TA-RNN, and Our Proposed SynSig2Vec
    on the DeepSignDB Database (EER, %)
  Table 8 caption: TABLE 8 Comparison of EERs (%) With Published State-of-the-Art
    Methods on the MCYT-100 and the SVC-Task2 Databases
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3087619
- Affiliation of the first author: department of applied mathematics, beijing jiaotong
    university, beijing, china
  Affiliation of the last author: department of applied mathematics, beijing jiaotong
    university, beijing, china
  Figure 1 Link: "articels_figures_by_rev_year\\2021\\\u2113__Norm_Quantile_Regression_Screening_Rule_via_the_Dual_Circumscribed_Sphere\\\
    figure_1.jpg"
  Figure 1 caption: "Cases 1: b 1 \u22650 and b 2 \u22650 ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: "articels_figures_by_rev_year\\2021\\\u2113__Norm_Quantile_Regression_Screening_Rule_via_the_Dual_Circumscribed_Sphere\\\
    figure_2.jpg"
  Figure 2 caption: "Cases 2: b 1 \u22650 and b 2 <0 ."
  Figure 3 Link: "articels_figures_by_rev_year\\2021\\\u2113__Norm_Quantile_Regression_Screening_Rule_via_the_Dual_Circumscribed_Sphere\\\
    figure_3.jpg"
  Figure 3 caption: 'Cases 3: b 1 <0 and b 2 <0 .'
  Figure 4 Link: "articels_figures_by_rev_year\\2021\\\u2113__Norm_Quantile_Regression_Screening_Rule_via_the_Dual_Circumscribed_Sphere\\\
    figure_4.jpg"
  Figure 4 caption: "The rejection ratio on \u03A3 1 with different p and \u03C4 .\
    \ There are slight differences between \u03A3 1 and \u03A3 2 on the rejection\
    \ ratio, so we omit these results on \u03A3 2 ."
  Figure 5 Link: "articels_figures_by_rev_year\\2021\\\u2113__Norm_Quantile_Regression_Screening_Rule_via_the_Dual_Circumscribed_Sphere\\\
    figure_5.jpg"
  Figure 5 caption: The rejection ratio under different data sets.
  Figure 6 Link: "articels_figures_by_rev_year\\2021\\\u2113__Norm_Quantile_Regression_Screening_Rule_via_the_Dual_Circumscribed_Sphere\\\
    figure_6.jpg"
  Figure 6 caption: The rejection ratio under different data sets.
  Figure 7 Link: "articels_figures_by_rev_year\\2021\\\u2113__Norm_Quantile_Regression_Screening_Rule_via_the_Dual_Circumscribed_Sphere\\\
    figure_7.jpg"
  Figure 7 caption: The speedup of different datasets under different tau .
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Pan Shang
  Name of the last author: Lingchen Kong
  Number of Figures: 7
  Number of Tables: 1
  Number of authors: 2
  Paper title: "\u2113 1 -Norm Quantile Regression Screening Rule via the Dual Circumscribed\
    \ Sphere"
  Publication Date: 2021-06-08 00:00:00
  Table 1 caption: "TABLE 1 In this Table, We Set the Sample Size n=100 n=100 and\
    \ Report the Speedup of Different Datasets Under Different p p, \u03A3 \u03A3\
    , and \u03C4 \u03C4. These Reported Speedup are the Means of 10 Simulation Results.\
    \ Here, We also Report the Standard Deviations (Sd) of Speedup Values."
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3087160
- Affiliation of the first author: school of electronic and information engineering,
    south china university of technology, guangzhou, guangdong, china
  Affiliation of the last author: school of electronic and information engineering,
    south china university of technology, guangzhou, guangdong, china
  Figure 1 Link: articels_figures_by_rev_year\2021\SkeletonNet_A_TopologyPreserving_Solution_for_Learning_Mesh_Reconstruction_of_Ob\figure_1.jpg
  Figure 1 caption: Given an input RGB image of an object instance, we aim to recover
    a surface mesh of the object and expect its topology to be correct. To tackle
    this challenging task, we propose SkeletonNet, an end-to-end model that is able
    to efficiently generate a high-quality skeletal shape representation whose topology
    is the same as that of the underlying surface. We use the generated skeleton either
    as a bridge to explicitly recover a surface mesh by deforming the extracted base
    mesh, or as a constraint to regularize the learning of an implicit field using
    the learned skeleton features. We obtain state-of-the-art results using both the
    explicit and implicit approaches.
  Figure 10 Link: articels_figures_by_rev_year\2021\SkeletonNet_A_TopologyPreserving_Solution_for_Learning_Mesh_Reconstruction_of_Ob\figure_10.jpg
  Figure 10 caption: Example results of skeleton interpolation in the forms of skeletal
    point set (a) and skeletal volume (b).
  Figure 2 Link: articels_figures_by_rev_year\2021\SkeletonNet_A_TopologyPreserving_Solution_for_Learning_Mesh_Reconstruction_of_Ob\figure_2.jpg
  Figure 2 caption: The pipeline of our proposed SkeletonNet. The module in the red
    dotted box (left) stacks the parallel CurSkeDecoder and SurSkeDecoder on top of
    a CNN based image encoder; given an input image I , the module is trained to regress
    a skeletal point set K . The module in the blue dotted box (right) consists of
    two parallel streams of 3D CNNs; the top one is for a global, low-resolution volume
    synthesis, which guides the synthesis of a high-resolution skeletal volume V via
    a sliding subvolume fashion in the bottom stream. Connecting the two modules is
    a differentiable Point2Voxel layer that converts K as the initial volume and makes
    the proposed SkeletonNet an end-to-end trainable network.
  Figure 3 Link: articels_figures_by_rev_year\2021\SkeletonNet_A_TopologyPreserving_Solution_for_Learning_Mesh_Reconstruction_of_Ob\figure_3.jpg
  Figure 3 caption: The pipeline of our proposed Skeleton-Based Graph CNN (SkeGCNN)
    for explicit mesh recovery from an input image.
  Figure 4 Link: articels_figures_by_rev_year\2021\SkeletonNet_A_TopologyPreserving_Solution_for_Learning_Mesh_Reconstruction_of_Ob\figure_4.jpg
  Figure 4 caption: An illustration of our proposed Skeleton-Regularized Deep Implicit
    Surface Network (SkeDISN).
  Figure 5 Link: articels_figures_by_rev_year\2021\SkeletonNet_A_TopologyPreserving_Solution_for_Learning_Mesh_Reconstruction_of_Ob\figure_5.jpg
  Figure 5 caption: Qualitative comparisons between SkeletonNet and its variants.
    These variants replace the module of SkeletonNet for producing the intermediate
    results of skeletal point set from input RGB images. (a) Input image; (b) Point-wise
    fitting; (c) Line-wise fitting (with Laplacian regularization); (d) Square-wise
    fitting (with Laplacian regularization); (e) Shared line-and-square fitting (with
    Laplacian regularization); (f) SkeletonNet without Laplacian regularization; (g)
    SkeletonNet without end-to-end training; (h) SkeletonNet; (i) Ground truth. Please
    refer to the main text for specific settings of these variants.
  Figure 6 Link: articels_figures_by_rev_year\2021\SkeletonNet_A_TopologyPreserving_Solution_for_Learning_Mesh_Reconstruction_of_Ob\figure_6.jpg
  Figure 6 caption: Qualitative comparisons between the SkeletonNet module of volumetric
    refinement and its variants. (a) Input image; (b) The intermediate result of skeletal
    point set; (c) Quantization and morphological dilation; (d) Subvolume synthesis
    alone; (e) Our used module of globally guided subvolume synthesis; (f) Ground
    truth.
  Figure 7 Link: articels_figures_by_rev_year\2021\SkeletonNet_A_TopologyPreserving_Solution_for_Learning_Mesh_Reconstruction_of_Ob\figure_7.jpg
  Figure 7 caption: Visualization results of our proposed SkeletonNet and its intermediate
    predictions of skeletal point set on ShapeNet [40] dataset. (a) Input images;
    (b) The produced skeletal points; (c) The ground-truth skeletal points; (d) The
    refined skeletal volumes; (e) The ground-truth skeletal volumes.
  Figure 8 Link: articels_figures_by_rev_year\2021\SkeletonNet_A_TopologyPreserving_Solution_for_Learning_Mesh_Reconstruction_of_Ob\figure_8.jpg
  Figure 8 caption: (a) Input Images; (b) OGN; (c) IMNet; (d) OccNet; (e) DISN; (f)
    SkeDISN; (g) Pix2Mesh; (h) AtlasNet; (i) TMNet; (j) SkeGCNN; (k) Ground truths.
  Figure 9 Link: articels_figures_by_rev_year\2021\SkeletonNet_A_TopologyPreserving_Solution_for_Learning_Mesh_Reconstruction_of_Ob\figure_9.jpg
  Figure 9 caption: Example results of the ablation studies on the effectiveness of
    SkeletonNet for the downstream tasks of explicit and implicit mesh recoveries.
    (a) Input image; (b) OGN + SkeGCNN; (c) SkeletonNet + SkeGCNN; (d) OGN + SkeDISN;
    (e) SkeletonNet + SkeDISN; (f) Ground truth.
  First author gender probability: 0.93
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.65
  Name of the first author: Jiapeng Tang
  Name of the last author: Kui Jia
  Number of Figures: 15
  Number of Tables: 7
  Number of authors: 5
  Paper title: 'SkeletonNet: A Topology-Preserving Solution for Learning Mesh Reconstruction
    of Object Surfaces From RGB Images'
  Publication Date: 2021-06-08 00:00:00
  Table 1 caption: TABLE 1 Quantitative Comparisons Between the Module of SkeletonNet
    and its Variants for Producing the Intermediate Results of Skeletal Point Set
    From Input RGB Images
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Quantitative Comparisons Between the SkeletonNet Module
    of Volumetric Refinement and its Variants
  Table 3 caption: TABLE 3 Quantitative Results of Our SkeletonNet and its Intermediate
    Predictions of Skeletal Point Set on all of 13 Categories in ShapeNet[40]
  Table 4 caption: "TABLE 4 Quantitative Comparisons (Chamfer Distance \xD70.001 \xD7\
    0.001) of Our Method Against State-of-the-Arts on ShapeNet [40] dataset"
  Table 5 caption: "TABLE 5 Quantitative Comparisons (Intersection Over Union \xD7\
    100 \xD7100) of Our Method Against State-of-the-Arts on ShapeNet [40] Dataset"
  Table 6 caption: TABLE 6 Ablation Studies on the Effectiveness of SkeletonNet for
    the Downstream Tasks of Explicit and Implicit Mesh Recoveries
  Table 7 caption: TABLE 7 Quantitative Results on the Tasks of Explicit and Implicit
    human Mesh Recoveries
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3087358
- Affiliation of the first author: department of computer science, city university
    of hong kong, hong kong
  Affiliation of the last author: school of electrical and electronics engineering,
    nanyang technological university, singapore, singapore
  Figure 1 Link: articels_figures_by_rev_year\2021\Deep_SpatialAngular_Regularization_for_Light_Field_Imaging_Denoising_and_SuperRe\figure_1.jpg
  Figure 1 caption: The pipeline of our deep learning-based compressive LF reconstruction
    over coded apertures. Our method elegantly incorporates the observation model
    of coded measurements into deep learning framework. The left side illustrates
    the acquisition of coded measurements by learning apertures, and the right side
    shows the reconstruction phase. More details of the reconstruction module are
    shown in Fig. 2.
  Figure 10 Link: articels_figures_by_rev_year\2021\Deep_SpatialAngular_Regularization_for_Light_Field_Imaging_Denoising_and_SuperRe\figure_10.jpg
  Figure 10 caption: Visual comparisons of denoised LFs from different methods.
  Figure 2 Link: articels_figures_by_rev_year\2021\Deep_SpatialAngular_Regularization_for_Light_Field_Imaging_Denoising_and_SuperRe\figure_2.jpg
  Figure 2 caption: The architectures of the proposed iterative framework for reconstructing
    LFs from coded measurements and the deep spatial-angular regularization sub-network.
  Figure 3 Link: articels_figures_by_rev_year\2021\Deep_SpatialAngular_Regularization_for_Light_Field_Imaging_Denoising_and_SuperRe\figure_3.jpg
  Figure 3 caption: "The quantitative comparison of our method against LF coded aperture\
    \ methods [16], [21], [35] and LF view synthesis methods[4], [5] on tasks: 1\u2192\
    49 , 2\u219249 , and 4\u219249 . Here the PSNR and SSIM values refer to the average\
    \ of all 30 LFs contained in the test set from Kalantari Lytro[4]."
  Figure 4 Link: articels_figures_by_rev_year\2021\Deep_SpatialAngular_Regularization_for_Light_Field_Imaging_Denoising_and_SuperRe\figure_4.jpg
  Figure 4 caption: 'The average PSNR at each angular position of reconstructed LFs
    from different compressive LF reconstruction methods. Here the PSNR values refer
    to the average of all 30 LFs contained in the test set from Kalantari Lytro[4].
    From left to right: (a) Inagaki et al. [16] (2), (b) Vadathya et al. [21] (2),
    (c)Guo et al. [35] (2) (d) Ours (2), (e) Inagaki et al. [16] (4), (f) Vadathya
    et al. [21] (4), (g)Guo et al. [35] (4) (h) Ours (4). The digits in brackets are
    the numbers of input measurements for each method.'
  Figure 5 Link: articels_figures_by_rev_year\2021\Deep_SpatialAngular_Regularization_for_Light_Field_Imaging_Denoising_and_SuperRe\figure_5.jpg
  Figure 5 caption: "Visual comparisons of all methods over real LF images under various\
    \ reconstruction tasks: 1\u219249 , 2\u219249 and 4\u219249 . The selected regions\
    \ in SAIs have been zoomed in. The error maps are calculated in gray-scale space."
  Figure 6 Link: articels_figures_by_rev_year\2021\Deep_SpatialAngular_Regularization_for_Light_Field_Imaging_Denoising_and_SuperRe\figure_6.jpg
  Figure 6 caption: "Visual comparisons of our method against Inagaki[16], Vadathya[21],\
    \ and Guo[35] over synthetic LF data under the task 2\u219225 ."
  Figure 7 Link: articels_figures_by_rev_year\2021\Deep_SpatialAngular_Regularization_for_Light_Field_Imaging_Denoising_and_SuperRe\figure_7.jpg
  Figure 7 caption: "Comparisons of depth maps estimated from reconstructed LFs by\
    \ different methods under tasks 2\u219249 and 4\u219249 . The selected regions\
    \ in SAIs has been zoomed in."
  Figure 8 Link: articels_figures_by_rev_year\2021\Deep_SpatialAngular_Regularization_for_Light_Field_Imaging_Denoising_and_SuperRe\figure_8.jpg
  Figure 8 caption: Comparisons of (a)different numbers of iterative stages, (b)different
    numbers of SAS convolutional layers in deep spatial-angular regularization, and
    (c)different levels of noise of compressive LF reconstruction.
  Figure 9 Link: articels_figures_by_rev_year\2021\Deep_SpatialAngular_Regularization_for_Light_Field_Imaging_Denoising_and_SuperRe\figure_9.jpg
  Figure 9 caption: Performance comparisons of our LF denoising method with (a) different
    numbers of iterative stages and (b) different numbers of SAS convolutional layers
    in deep spatial-angular regularization.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.52
  Name of the first author: Mantang Guo
  Name of the last author: Lap-Pui Chau
  Number of Figures: 15
  Number of Tables: 4
  Number of authors: 5
  Paper title: Deep Spatial-Angular Regularization for Light Field Imaging, Denoising,
    and Super-Resolution
  Publication Date: 2021-06-08 00:00:00
  Table 1 caption: "TABLE 1 The Quantitative Comparison of Different Compressive LF\
    \ Reconstruction Methods on Synthetic LFs Under the Task 2\u219225 2\u219225"
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Running Time (in Second) of Different Reconstruction MethodsAverage
    EPI-SSIM of Reconstructed LFs by Different Methods
  Table 3 caption: TABLE 3 Comparisons of Different LF Denoising Methods in Terms
    of Average PSNR, SSIM, and EPI-SSIM
  Table 4 caption: TABLE 4 Comparisons of Different LF Spatial SR Methods in Terms
    of Average PSNR and SSIM of SAIs and Average EPI-SSIM of EPIs
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3087485
- Affiliation of the first author: school of computer science and technology, university
    of chinese academy of sciences, beijing, china
  Affiliation of the last author: key laboratory of intelligent information processing,
    institute of computing technology, chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Poisoning_Attack_Against_Estimating_From_Pairwise_Comparisons\figure_1.jpg
  Figure 1 caption: The amount of changed pairwise comparisons by the poisoning attack
    with static game. The x -axis is the index of pairwise comparisons and the y -axis
    is the amount of change. Note that the ranges of y -axis in each sub-figure are
    different.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Poisoning_Attack_Against_Estimating_From_Pairwise_Comparisons\figure_2.jpg
  Figure 2 caption: The number of correct pairwise comparisons and comparisons which
    conflict with the ground-truth ranking in the poisoned training set by Static
    method.
  Figure 3 Link: articels_figures_by_rev_year\2021\Poisoning_Attack_Against_Estimating_From_Pairwise_Comparisons\figure_3.jpg
  Figure 3 caption: The ranking generated from the original data (Original), random
    attack data (Random), static poisoning attack data (Static) and dynamic poisoning
    attack data (Dynamic).
  Figure 4 Link: articels_figures_by_rev_year\2021\Poisoning_Attack_Against_Estimating_From_Pairwise_Comparisons\figure_4.jpg
  Figure 4 caption: "The ranking generated from the original data (Original), random\
    \ perturbation data (Random), poisoned data (Static and Dynamic) on Human Age\
    \ dataset. When the Kendall- \u03C4 is smaller than 0 ( \u03B1\u2265 10 \u2212\
    4 ), we observe that the aggregated results would put the younger people at the\
    \ top of the lists. Moreover, the same phenomenons in the simulation are still\
    \ observed. The training data with more than 50% outliers could generate an arbitrarily\
    \ ordered list. If it happens, the Kendall- \u03C4 could not monotonically decrease\
    \ when we increase the uncertainty budget continuously for the static attack strategies."
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.89
  Name of the first author: Ke Ma
  Name of the last author: Qingming Huang
  Number of Figures: 4
  Number of Tables: 6
  Number of authors: 5
  Paper title: Poisoning Attack Against Estimating From Pairwise Comparisons
  Publication Date: 2021-06-08 00:00:00
  Table 1 caption: TABLE 1 Comparative Results of Different Attack Methods on Simulated
    Data
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparative Results of Different Attack Methods on Human
    Age Data
  Table 3 caption: TABLE 3 Comparative Results of Different Attack Methods on Dublin
    Election Data
  Table 4 caption: TABLE 4 Comparative Results of Different Attack Methods on Sushi
    Election Data
  Table 5 caption: TABLE 5 Computational Complexity (ms) Comparisons on the Synthetic
    Dataset
  Table 6 caption: TABLE 6 Computational Complexity (ms) Comparisons on the Real-World
    Datasets
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3087514
- Affiliation of the first author: department of computing science, university of
    alberta, edmonton, ab, canada
  Affiliation of the last author: "fg intelligent autonomous systems, technische universit\xE4\
    t darmstadt, darmstadt, germany"
  Figure 1 Link: articels_figures_by_rev_year\2021\Batch_Reinforcement_Learning_With_a_Nonparametric_OffPolicy_Policy_Gradient\figure_1.jpg
  Figure 1 caption: In the off-policy reinforcement learning scheme, the policy can
    be optimized using an off-policy dataset. This allows for safer interaction with
    the system and for better sample efficiency.
  Figure 10 Link: articels_figures_by_rev_year\2021\Batch_Reinforcement_Learning_With_a_Nonparametric_OffPolicy_Policy_Gradient\figure_10.jpg
  Figure 10 caption: 'Comparison of NOPG in its deterministic and stochastic versions
    to state-of-the-art offline algorithms on continuous control tasks: Swing-Up Pendulum
    with random agent (left), the Cart-Pole stabilization (center) and U-Maze with
    D4RL dataset (right). The figures depict the mean and 95 percent confidence interval
    over 10 trials. NOPG is competitive with the sample efficiency of the considered
    baselines. Note the log-scale along the x -axis.'
  Figure 2 Link: articels_figures_by_rev_year\2021\Batch_Reinforcement_Learning_With_a_Nonparametric_OffPolicy_Policy_Gradient\figure_2.jpg
  Figure 2 caption: The classic effect (known as boundary-bias) of the Nadaraya-Watson
    regression predicting a constant function in low-density regions is beneficial
    in our case, as it prevents the policy from moving in those areas as the gradient
    gets close to zero.
  Figure 3 Link: articels_figures_by_rev_year\2021\Batch_Reinforcement_Learning_With_a_Nonparametric_OffPolicy_Policy_Gradient\figure_3.jpg
  Figure 3 caption: "Some of the benchmarking tasks. The returns landscape (a) of\
    \ the LQG problem. In the gradient analysis, we obtain the gradient of the policy\
    \ with parameters \u03B8 1 , \u03B8 2 by sampling from a policy interpolated with\
    \ the parameters \u03B8 \u2032 1 , \u03B8 \u2032 2 . Sub-figures (b) and (d) depicts\
    \ the OpenAI environment used. The real system in (c) has been used to evaluate\
    \ the policy learned to stabilize the cart-pole task."
  Figure 4 Link: articels_figures_by_rev_year\2021\Batch_Reinforcement_Learning_With_a_Nonparametric_OffPolicy_Policy_Gradient\figure_4.jpg
  Figure 4 caption: "Evaluated in the initial state, the optimization policy having\
    \ parameters \u03B8 1 , \u03B8 2 and the behavioral policy having parameters \u03B8\
    \ \u2032 1 , \u03B8 \u2032 2 exhibit a fair distance in probability space."
  Figure 5 Link: articels_figures_by_rev_year\2021\Batch_Reinforcement_Learning_With_a_Nonparametric_OffPolicy_Policy_Gradient\figure_5.jpg
  Figure 5 caption: "(a) The dataset used for the experiment in Section 5.3.4. The\
    \ blue plane represent a policy with constant action. By setting different values\
    \ of \u03B8 3 we can obtain different policies. For \u03B8 3 \u22480 , the policy\
    \ is most fitting with the data. (b) When the policy has low log-likelihood, the\
    \ gradient quickly approaches the zero (i.e., \u2225 \u2207 \u03B8 J \u03C0 \u2225\
    \u21920 ). (c) The magnitude of the gradient decreases in low density regions,\
    \ where the prediction is most uncertain."
  Figure 6 Link: articels_figures_by_rev_year\2021\Batch_Reinforcement_Learning_With_a_Nonparametric_OffPolicy_Policy_Gradient\figure_6.jpg
  Figure 6 caption: Bias, variance, MSE and gradient direction analysis. The MSE plots
    are equipped with a 95 percent interval using bootstrapping techniques. The direction
    analysis plots describe the distribution of angle between the estimates and the
    ground truth gradient. NOPG exhibits favorable bias, variance and gradient direction
    compared to PWIS and semi-gradient.
  Figure 7 Link: articels_figures_by_rev_year\2021\Batch_Reinforcement_Learning_With_a_Nonparametric_OffPolicy_Policy_Gradient\figure_7.jpg
  Figure 7 caption: A phase portrait of the state distribution tildemu pi and value
    function tildeVpi estimated in the swing-up pendulum task with NOPG-D. Green corresponds
    to higher values. The two leftmost figures show the estimates before any policy
    improvement, while the two rightmost show them after 300 offline updates of NOPG-D.
    Notice that the algorithm finds a very good approximation of the optimal value
    function and is able to predict that the system will reach the goal state ( (omega,
    dotomega ) = (0, 0) ).
  Figure 8 Link: articels_figures_by_rev_year\2021\Batch_Reinforcement_Learning_With_a_Nonparametric_OffPolicy_Policy_Gradient\figure_8.jpg
  Figure 8 caption: A lower bandwidth corresponds to higher variance, while higher
    bandwidth increases the bias up to a plateau.
  Figure 9 Link: articels_figures_by_rev_year\2021\Batch_Reinforcement_Learning_With_a_Nonparametric_OffPolicy_Policy_Gradient\figure_9.jpg
  Figure 9 caption: 'Comparison of NOPG in its deterministic and stochastic versions
    to state-of-the-art online algorithms on continuous control tasks: Swing-Up Pendulum
    with uniform grid sampling (left), Swing-Up Pendulum with the random agent (center)
    and the Cart-Pole stabilization (right). The figures depict the mean and 95 percent
    confidence interval over 10 trials. NOPG outperforms the baselines w.r.t the sample
    complexity. Note the log-scale along the x -axis.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.91
  Name of the first author: Samuele Tosatto
  Name of the last author: Jan Peters
  Number of Figures: 13
  Number of Tables: 1
  Number of authors: 3
  Paper title: Batch Reinforcement Learning With a Nonparametric Off-Policy Policy
    Gradient
  Publication Date: 2021-06-09 00:00:00
  Table 1 caption: 'TABLE 1 Acronyms Used in the Paper to Refer to Practical Implementation
    of the Algorithms (SG: Semi-Gradient, PWIS: Path-Wise Importance Sampling, MB:
    Model Based)'
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3088063
- Affiliation of the first author: school of electronic and information engineering,
    south china university of technology, guangzhou, china
  Affiliation of the last author: school of computer science and engineering, south
    china university of technology, guangzhou, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Towards_Uncovering_the_Intrinsic_Data_Structures_for_Unsupervised_Domain_Adaptat\figure_1.jpg
  Figure 1 caption: "A schematic illustration of our proposed hybrid model of Structurally\
    \ Regularized Deep Clustering (H-SRDC). It is formulated as a constrained clustering\
    \ framework with two key components: (a) structurally regularized discriminative\
    \ clustering, which uncovers the intrinsic discrimination of unlabeled target\
    \ data with structural regularization from the labeled source data (cf. Section\
    \ 3.1), and (b) structurally regularized generative clustering, which modulates\
    \ and potentially enhances the learning in the feature space by generative learning\
    \ of cluster centroids using self-attentive interactions of instance features\
    \ (cf. Section 3.2). Once trained, the classification model f\u2218\u03C6 is deployed\
    \ for the UDA task. In this figure, orange and green arrows represent the data\
    \ flows from the source and target domains, respectively."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Towards_Uncovering_the_Intrinsic_Data_Structures_for_Unsupervised_Domain_Adaptat\figure_2.jpg
  Figure 2 caption: An illustration of our used subnetwork for self-attentive learning
    of cluster centroids. One may refer to the main text for definitions of the used
    terms and math notations.
  Figure 3 Link: articels_figures_by_rev_year\2021\Towards_Uncovering_the_Intrinsic_Data_Structures_for_Unsupervised_Domain_Adaptat\figure_3.jpg
  Figure 3 caption: "A schematic illustration of extending H-SRDC for image semantic\
    \ segmentation in an unsupervised domain adaptation setting. A task-related notion\
    \ of layout-wise consistency is introduced, which inspires the design of additional\
    \ loss terms for adaptation of segmentation maps in an adversarial training manner.\
    \ Once trained, the segmentation model f\u2218\u03C6 is deployed for the UDA task.\
    \ In this figure, orange and green arrows represent the data flows from the source\
    \ and target domains, respectively."
  Figure 4 Link: articels_figures_by_rev_year\2021\Towards_Uncovering_the_Intrinsic_Data_Structures_for_Unsupervised_Domain_Adaptat\figure_4.jpg
  Figure 4 caption: An illustration of our used weighted combination scheme in the
    loss term (17) for learning cluster centroids in the feature maps of reduced resolution,
    which receive supervision signals from the labeled source segmentation maps of
    original resolution.
  Figure 5 Link: articels_figures_by_rev_year\2021\Towards_Uncovering_the_Intrinsic_Data_Structures_for_Unsupervised_Domain_Adaptat\figure_5.jpg
  Figure 5 caption: Learning diagnosis on the effect of the SRGenC objective (12)
    used in H-SRDC. Three types of distances for the source and target data are plotted
    against the training epochs. Comparisons between our H-SRDC and MCD [6] are made
    in the two figures of last column. The experiments are conducted on the adaptation
    tasks of A to D and D to A on the Office-31 benchmark [22]. Refer to the main
    text for how these distances are defined and computed.
  Figure 6 Link: articels_figures_by_rev_year\2021\Towards_Uncovering_the_Intrinsic_Data_Structures_for_Unsupervised_Domain_Adaptat\figure_6.jpg
  Figure 6 caption: "Analysis of convergence and generalization. \u201CTraining\u201D\
    \ and \u201CTest\u201D refer to results on training and held-out test sets on\
    \ the target domain, respectively. Experiments in the inductive UDA setting are\
    \ conducted on adaptation tasks of A to D and D to A on the Office-31 benchmark\
    \ [22], and Ar to Rw and Rw to Ar on the Office-Home benchmark [24]."
  Figure 7 Link: articels_figures_by_rev_year\2021\Towards_Uncovering_the_Intrinsic_Data_Structures_for_Unsupervised_Domain_Adaptat\figure_7.jpg
  Figure 7 caption: The t-SNE visualization of feature distributions from MCD [6]
    and H-SRDC, using the test data on the target domain (Amazon in Office-31). Classes
    are color coded.
  Figure 8 Link: articels_figures_by_rev_year\2021\Towards_Uncovering_the_Intrinsic_Data_Structures_for_Unsupervised_Domain_Adaptat\figure_8.jpg
  Figure 8 caption: Qualitative results on the inductive UDA task of GTA5 rightarrow
    Cityscapes.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.96
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Hui Tang
  Name of the last author: C. L. Philip Chen
  Number of Figures: 8
  Number of Tables: 9
  Number of authors: 5
  Paper title: Towards Uncovering the Intrinsic Data Structures for Unsupervised Domain
    Adaptation Using Structurally Regularized Deep Clustering
  Publication Date: 2021-06-09 00:00:00
  Table 1 caption: TABLE 1 Fine-Grained Ablation Studies on the Four Key Components
    of Our Proposed H-SRDC
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparative Results (%) in the Inductive Setting on the
    Office-31 Benchmark [22]
  Table 3 caption: TABLE 3 Comparative Results (%) in the Inductive Setting on the
    ImageCLEF-DA Benchmark [23]
  Table 4 caption: TABLE 4 Comparative Results (%) in the Inductive Setting on the
    Office-Home Benchmark [24]
  Table 5 caption: TABLE 5 Comparative Results (%) in the Inductive Setting on the
    VisDA-2017 Benchmark [25]
  Table 6 caption: TABLE 6 Comparative Results (%) in the Inductive Setting on the
    Digits Benchmark
  Table 7 caption: "TABLE 7 Comparative Results (%) in the Inductive Setting on the\
    \ Benchmark of GTA5 \u2192 \u2192Cityscapes"
  Table 8 caption: "TABLE 8 Comparative Results (%) in the Inductive Setting on the\
    \ Benchmark of SYNTHIA \u2192 \u2192Cityscapes"
  Table 9 caption: TABLE 9 Comparative Results (%) in the Transductive Setting on
    the Benchmarks of Office-31 [22], ImageCLEF-DA [23], Office-Home [24], and VisDA-2017
    [25]
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3087830
- Affiliation of the first author: university of science and technology of china (ustc),
    hefei, china
  Affiliation of the last author: university of science and technology of china (ustc),
    hefei, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Segment_as_Points_for_Efficient_and_Effective_Online_MultiObject_Tracking_and_Se\figure_1.jpg
  Figure 1 caption: Comparison between our method and the state-of-the-art methods
    on sMOTSA (left) and id switches (right). The hollow symbols and the filled symbols
    denote the results for pedestrians and for cars respectively. On the right subfigure,
    four methods track on the same segmentation result, which takes 3.66s.
  Figure 10 Link: articels_figures_by_rev_year\2021\Segment_as_Points_for_Efficient_and_Effective_Online_MultiObject_Tracking_and_Se\figure_10.jpg
  Figure 10 caption: Visualizations of learned instance embeddings.
  Figure 2 Link: articels_figures_by_rev_year\2021\Segment_as_Points_for_Efficient_and_Effective_Online_MultiObject_Tracking_and_Se\figure_2.jpg
  Figure 2 caption: Overview of PointTrackV2. For an input image, PointTrackV2 obtains
    instance segments by the segmentation network. Then, for each instance, PointTrackV2
    regards the segment and its surrounding ENV as two 2D point clouds and learns
    instance embeddings in a point cloud processing manner. MLP stands for the multi-layer
    perceptron with Leaky ReLU.
  Figure 3 Link: articels_figures_by_rev_year\2021\Segment_as_Points_for_Efficient_and_Effective_Online_MultiObject_Tracking_and_Se\figure_3.jpg
  Figure 3 caption: Segmentation network of PointTrackV2. The segmentation network
    generates instance masks in one shot by jointly considering the instance-agnostic
    semantic segmentation and the instance grouping parameters. After that, we apply
    the aforementioned embedding network to extract instance embeddings for online
    tracking.
  Figure 4 Link: articels_figures_by_rev_year\2021\Segment_as_Points_for_Efficient_and_Effective_Online_MultiObject_Tracking_and_Se\figure_4.jpg
  Figure 4 caption: Comparison of car density. The density of APOLLO MOTS is significantly
    higher than KITTI MOTS.
  Figure 5 Link: articels_figures_by_rev_year\2021\Segment_as_Points_for_Efficient_and_Effective_Online_MultiObject_Tracking_and_Se\figure_5.jpg
  Figure 5 caption: Sample images from APOLLO MOTS. White areas are DontCare areas.
    Demo videos are available in the supplementary material, available online.
  Figure 6 Link: articels_figures_by_rev_year\2021\Segment_as_Points_for_Efficient_and_Effective_Online_MultiObject_Tracking_and_Se\figure_6.jpg
  Figure 6 caption: The proposed Copy-and-Paste strategy. We copy pedestrians from
    other images to the current image to actively increase the quality of the training
    samples.
  Figure 7 Link: articels_figures_by_rev_year\2021\Segment_as_Points_for_Efficient_and_Effective_Online_MultiObject_Tracking_and_Se\figure_7.jpg
  Figure 7 caption: Quantitative results on KITTI MOTS. Instances belonging to the
    same track id are plotted in the same color.
  Figure 8 Link: articels_figures_by_rev_year\2021\Segment_as_Points_for_Efficient_and_Effective_Online_MultiObject_Tracking_and_Se\figure_8.jpg
  Figure 8 caption: Inconsistency of ground truth between our predictions (in red)
    and the ground truth of KITTI MOT (in green).
  Figure 9 Link: articels_figures_by_rev_year\2021\Segment_as_Points_for_Efficient_and_Effective_Online_MultiObject_Tracking_and_Se\figure_9.jpg
  Figure 9 caption: Visualizations of critical points for both PointTrack and PointTrackV2.
    Red points and yellow points represent the critical FG points and the critical
    ENV points, respectively.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.61
  Name of the first author: Zhenbo Xu
  Name of the last author: Liusheng Huang
  Number of Figures: 10
  Number of Tables: 12
  Number of authors: 6
  Paper title: Segment as Points for Efficient and Effective Online Multi-Object Tracking
    and Segmentation
  Publication Date: 2021-06-09 00:00:00
  Table 1 caption: TABLE 1 Explanations of Hyper-Parameters
  Table 10 caption: TABLE 10 Impact of Modifications on the Segmentation Network on
    the KITTI MOTS Validation
  Table 2 caption: TABLE 2 Comparison Between APOLLO MOTS and KITTI MOTS on Their
    Respective TrainValidation Sets
  Table 3 caption: TABLE 3 Results on the KITTI MOTS Validation
  Table 4 caption: TABLE 4 Results on the KITTI MOTS Test Set
  Table 5 caption: TABLE 5 Results on APOLLO MOTS Validation
  Table 6 caption: TABLE 6 Comparison With Methods Using Private Segmentation Results
    on MOTSChallenge Test Set
  Table 7 caption: TABLE 7 Comparisons of IDS on the Same Segmentation Results Across
    Three Datasets
  Table 8 caption: 'TABLE 8 Impact of Different Data Modalities on the KITTI MOTS
    Validation: Color (Col.), Offset (Off.), Category (Cat.), Position (Pos.)'
  Table 9 caption: TABLE 9 Impact of RandLA, the Point Weighting Layer, M F MF, M
    E ME, the Multi-Stage Training (MT), and the CNN Baseline on the KITTI MOTS Validation
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3087898
