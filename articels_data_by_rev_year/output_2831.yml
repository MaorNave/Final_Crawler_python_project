- Affiliation of the first author: school of computer science and engineering, central
    south univerisity, changsha, china
  Affiliation of the last author: school of computer science and engineering, central
    south univerisity, changsha, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Parameterized_Hamiltonian_Learning_With_Quantum_Circuit\figure_1.jpg
  Figure 1 caption: The quantum circuit of quantum Hamiltonian learning. The state
    of the untrusted simulators is swapped and the trusted simulator attempts to invert
    the evolution [5].
  Figure 10 Link: articels_figures_by_rev_year\2022\Parameterized_Hamiltonian_Learning_With_Quantum_Circuit\figure_10.jpg
  Figure 10 caption: The loss function of experiment (01), experiment (02), and experiment
    (03).
  Figure 2 Link: articels_figures_by_rev_year\2022\Parameterized_Hamiltonian_Learning_With_Quantum_Circuit\figure_2.jpg
  Figure 2 caption: The framework of Hamiltonian learning algorithm.
  Figure 3 Link: articels_figures_by_rev_year\2022\Parameterized_Hamiltonian_Learning_With_Quantum_Circuit\figure_3.jpg
  Figure 3 caption: "The parameterized Hamiltonian learning model. We construct an\
    \ initial quantum system | \u03C8 in \u27E9 with Hamiltonian of H b and hope that\
    \ the system will evolve through the parameterized quantum circuits based on the\
    \ R y (\u03B1) gate and the CNOT - R z (\u03B2) gate to generate the final quantum\
    \ system | \u03C8 out \u27E9 with the Hamiltonian H p ."
  Figure 4 Link: articels_figures_by_rev_year\2022\Parameterized_Hamiltonian_Learning_With_Quantum_Circuit\figure_4.jpg
  Figure 4 caption: The example of the quantum circuit in PHL. In this example, we
    define the step of evolution M=1 and use five qubits N=5 to build the circuit.
  Figure 5 Link: articels_figures_by_rev_year\2022\Parameterized_Hamiltonian_Learning_With_Quantum_Circuit\figure_5.jpg
  Figure 5 caption: The framework of the PHLA. We design a parameterized quantum Hamiltonian
    learning algorithm based on PHL, where the gap between the intermediate state
    and final state is calculated using the method of comparing the expectation of
    Hamiltonian in the quantum system. The ultimate state is with the highest expectation
    value.
  Figure 6 Link: articels_figures_by_rev_year\2022\Parameterized_Hamiltonian_Learning_With_Quantum_Circuit\figure_6.jpg
  Figure 6 caption: 'The Process of Quantum Image Segmentation. The figure is divided
    into five parts: Fig. (a) is the original image, Fig. (b) is the segmentation
    result, Fig. (c) is the qubit position for pixel, Fig. (d) is the undirected weighted
    network for graph G , Fig. (e) is the result of boundary after the process of
    PHLA. The segmentation process from Fig. (a) to Fig. (b) can be represented by
    Fig. (d) to Fig. (e) and then fig. (b) .'
  Figure 7 Link: articels_figures_by_rev_year\2022\Parameterized_Hamiltonian_Learning_With_Quantum_Circuit\figure_7.jpg
  Figure 7 caption: Results in experiment (01).
  Figure 8 Link: articels_figures_by_rev_year\2022\Parameterized_Hamiltonian_Learning_With_Quantum_Circuit\figure_8.jpg
  Figure 8 caption: Results in experiment (02).
  Figure 9 Link: articels_figures_by_rev_year\2022\Parameterized_Hamiltonian_Learning_With_Quantum_Circuit\figure_9.jpg
  Figure 9 caption: Results in experiment (03).
  First author gender probability: 0.55
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.63
  Name of the first author: Jinjing Shi
  Name of the last author: Shichao Zhang
  Number of Figures: 10
  Number of Tables: 6
  Number of authors: 4
  Paper title: Parameterized Hamiltonian Learning With Quantum Circuit
  Publication Date: 2022-08-31 00:00:00
  Table 1 caption: TABLE 1 Notations
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Results of Qubits in Experiment (01)
  Table 3 caption: TABLE 3 Results of Qubits in Experiment (02)
  Table 4 caption: TABLE 4 Results of Qubits in Experiment (03)
  Table 5 caption: TABLE 5 Hamiltonian Learning Model
  Table 6 caption: TABLE 6 Hamiltonian Learning Algorithm
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3203157
- Affiliation of the first author: centre for artificial intelligence and robotics,
    hong kong institute of science & innovation, chinese academy of sciences (hkisicas),
    hongkong, china
  Affiliation of the last author: centre for artificial intelligence and robotics,
    hong kong institute of science & innovation, chinese academy of sciences (hkisicas),
    hongkong, china
  Figure 1 Link: articels_figures_by_rev_year\2022\MemoryBased_CrossImage_Contexts_for_Weakly_Supervised_Semantic_Segmentation\figure_1.jpg
  Figure 1 caption: (a) Methods that only take single image into consideration; (b)
    Our approach mines cross-image contexts from related images to assist the pseudo-mask
    generation.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\MemoryBased_CrossImage_Contexts_for_Weakly_Supervised_Semantic_Segmentation\figure_2.jpg
  Figure 2 caption: 'The framework of our approach. Left: The overall framework of
    our approach. The classification module learns from the classification task and
    produces initial CAM pseudo-masks. The segmentation module is responsible for
    final semantic segmentation. The cross-image module applies the memory-bank mechanism
    to explore cross-image contexts, which is the core of our approach. Right: The
    framework of the cross-image module. Please see Section 3 for details.'
  Figure 3 Link: articels_figures_by_rev_year\2022\MemoryBased_CrossImage_Contexts_for_Weakly_Supervised_Semantic_Segmentation\figure_3.jpg
  Figure 3 caption: The cross-image context propagation process. Each query feature
    is equipped with some reference features that obtained from the memory bank. The
    obtained cross-image message is added back to the query feature as the final cross-image
    feature. Please see Section 3.2 for details.
  Figure 4 Link: articels_figures_by_rev_year\2022\MemoryBased_CrossImage_Contexts_for_Weakly_Supervised_Semantic_Segmentation\figure_4.jpg
  Figure 4 caption: Illustration of the process for obtaining image-level contexts.
    Colored regions in the confidence score maps are of high values. For simplicity,
    only three classes are illustrated as a demo.
  Figure 5 Link: articels_figures_by_rev_year\2022\MemoryBased_CrossImage_Contexts_for_Weakly_Supervised_Semantic_Segmentation\figure_5.jpg
  Figure 5 caption: The stop-gradient strategy to train the cross-image module. Please
    see Section 3.5 for details. Best viewed in color.
  Figure 6 Link: articels_figures_by_rev_year\2022\MemoryBased_CrossImage_Contexts_for_Weakly_Supervised_Semantic_Segmentation\figure_6.jpg
  Figure 6 caption: Examples of the weights for merging features as cross-image contexts
    with different thresholds. The three rows correspond to the three different classes.
    Please see Section 4.5.1 for details. Best viewed in color.
  Figure 7 Link: articels_figures_by_rev_year\2022\MemoryBased_CrossImage_Contexts_for_Weakly_Supervised_Semantic_Segmentation\figure_7.jpg
  Figure 7 caption: Qualitative results on the Pascal VOC 2012 val set. Best viewed
    in color.
  Figure 8 Link: articels_figures_by_rev_year\2022\MemoryBased_CrossImage_Contexts_for_Weakly_Supervised_Semantic_Segmentation\figure_8.jpg
  Figure 8 caption: Qualitative results on the COCO val set. Best viewed in color.
  Figure 9 Link: articels_figures_by_rev_year\2022\MemoryBased_CrossImage_Contexts_for_Weakly_Supervised_Semantic_Segmentation\figure_9.jpg
  Figure 9 caption: The quality of the pseudo-masks along the training process. Best
    viewed in color.
  First author gender probability: 0.9
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Junsong Fan
  Name of the last author: Zhaoxiang Zhang
  Number of Figures: 9
  Number of Tables: 11
  Number of authors: 2
  Paper title: Memory-Based Cross-Image Contexts for Weakly Supervised Semantic Segmentation
  Publication Date: 2022-09-01 00:00:00
  Table 1 caption: TABLE 1 Ablation Study of the Proposed Approach
  Table 10 caption: TABLE 10 Results on the COCO Dataset With Different Capacities,
    Obtained by the VGG16 Backbone on the COCO val set
  Table 2 caption: TABLE 2 Influence of the Capacity of the Memory Bank for Each Class
  Table 3 caption: TABLE 3 Effect of Utilizing the EMA Model for Encoding the Memory
    Features
  Table 4 caption: TABLE 4 The Influence of the Threshold for the Fusion Features
  Table 5 caption: TABLE 5 Comparison of the Pixel-Level Contexts (pix.) and Image-Level
    Contexts (img.)
  Table 6 caption: TABLE 6 The Influence of Different Methods to Provide the Cross-Image
    Context
  Table 7 caption: TABLE 7 The Influence of the Stop-Gradient Strategy for the Proposed
    Cross-Image Module
  Table 8 caption: TABLE 8 Comparison With Related Works
  Table 9 caption: TABLE 9 Comparison With Related Works
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3203402
- Affiliation of the first author: department of computer science, purdue university,
    west lafayette, in, usa
  Affiliation of the last author: futurewei seattle cloud lab, seattle, wa, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\Contrastive_Learning_With_Stronger_Augmentations\figure_1.jpg
  Figure 1 caption: Contrastive instance learning framework.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Contrastive_Learning_With_Stronger_Augmentations\figure_2.jpg
  Figure 2 caption: Comparison of the strongly weakly augmented images. The left is
    the original image, the middle is the weakly augmented image, and the right is
    the strongly augmented one with over-contrastive details.
  Figure 3 Link: articels_figures_by_rev_year\2022\Contrastive_Learning_With_Stronger_Augmentations\figure_3.jpg
  Figure 3 caption: The comparison of the distribution of positive pairs probabilities
    and variance of negative pairs probabilities given weakly augmented query and
    strongly augmented query. A. The distribution with a randomly initialized network.
    B. The distribution with a pre-trained network by contrastive methods.
  Figure 4 Link: articels_figures_by_rev_year\2022\Contrastive_Learning_With_Stronger_Augmentations\figure_4.jpg
  Figure 4 caption: Diagram of distributional divergence minimization. Here the representation
    bank consists of K stored features z k of previous batches and online features
    z i from the key encoder. They will be used to calculate the conditional probability
    of current weakly and strongly augmented query images.
  Figure 5 Link: articels_figures_by_rev_year\2022\Contrastive_Learning_With_Stronger_Augmentations\figure_5.jpg
  Figure 5 caption: Comparison of KNN accuracy of MoCo V2 [4] and CLSA. Both results
    are compared based on pretrained model with 200800 epochs with single crop. The
    comparison used the representation of weaklystrongly augmented images, respectively.
    The neighbor K for KNN here is set to 20.
  Figure 6 Link: articels_figures_by_rev_year\2022\Contrastive_Learning_With_Stronger_Augmentations\figure_6.jpg
  Figure 6 caption: The comparison of the distribution of positive pairs probabilities
    and variance of negative pairs probabilities with a pre-trained network by CLSA.
    A. The distribution comparison of positive pairs probabilities. B. The distribution
    comparison of variance of negative pairs probabilities.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Xiao Wang
  Name of the last author: Guo-Jun Qi
  Number of Figures: 6
  Number of Tables: 8
  Number of authors: 2
  Paper title: Contrastive Learning With Stronger Augmentations
  Publication Date: 2022-09-01 00:00:00
  Table 1 caption: TABLE 1 Various Augmentations we Applied in Experiments to Strongly
    Augment Training Images
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Top-1 Accuracy Under the Linear Evaluation on ImageNet
    With the ResNet-50 Backbone With 200 Epochs Training
  Table 3 caption: TABLE 3 Top-1 Accuracy Under the Linear Evaluation on ImageNet
    With the ResNet-50 Backbone With Various Numbers of Epochs
  Table 4 caption: TABLE 4 Transfer Learning Results on Various Downstream Tasks
  Table 5 caption: TABLE 5 Ablation Study of the CLSA on ImageNet With 200 Epochs
    of Pre-Training
  Table 6 caption: TABLE 6 Training Time Comparison of CLSA
  Table 7 caption: TABLE 7 Comparison of CLSA Under Different Strong Augmentations
  Table 8 caption: TABLE 8 Generalization Experiments of CLSA
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3203630
- Affiliation of the first author: department of electrical engineering, national
    tsing hua university, hsinchu, taiwan
  Affiliation of the last author: department of electrical engineering, national tsing
    hua university, hsinchu, taiwan
  Figure 1 Link: articels_figures_by_rev_year\2022\BiFuse_SelfSupervised_and_Efficient_BiProjection_Fusion_for__Depth_Estimation\figure_1.jpg
  Figure 1 caption: "Our BiFuse++ is a self-supervised framework of monocular 360\
    \ \u2218 depth estimation. The depth estimation network (DepthNet) is a bi-projection\
    \ architecture consisting of two encoders and a shared decoder. The inputs of\
    \ DepthNet are equirectangular and cubemap projections of reference panorama I\
    \ t . Between each adjacent layer of encoders, the feature maps of two projections\
    \ are fused by our proposed fusion module (green arrows). To achieve self-supervised\
    \ learning, an additional network (PoseNet) takes three adjacent panoramas ( I\
    \ t\u22121 , I t , and I t+1 ) in a video sequence as inputs and infers the corresponding\
    \ camera motions ( P t\u22121 and P t+1 ). We then compute the photo consistency\
    \ error based on the predicted depth map and camera motions to jointly train the\
    \ two networks."
  Figure 10 Link: articels_figures_by_rev_year\2022\BiFuse_SelfSupervised_and_Efficient_BiProjection_Fusion_for__Depth_Estimation\figure_10.jpg
  Figure 10 caption: The effect of Contrast-Aware Photometric Loss (CAPL). The Spherical
    Photometric Loss (SPL) cannot deal with the low-texture area and thus produces
    unstable depth maps (red indicates a large depth value). Note that we mask out
    the photographer at the bottom left and right region.
  Figure 2 Link: articels_figures_by_rev_year\2022\BiFuse_SelfSupervised_and_Efficient_BiProjection_Fusion_for__Depth_Estimation\figure_2.jpg
  Figure 2 caption: 360-SelfNet [5] trained on real-world images. The spherical photometric
    loss cannot deal with the low-texture area and thus produces unstable depth maps
    (red indicates a large depth value). Note that we mask out the photographer at
    the bottom left and right region.
  Figure 3 Link: articels_figures_by_rev_year\2022\BiFuse_SelfSupervised_and_Efficient_BiProjection_Fusion_for__Depth_Estimation\figure_3.jpg
  Figure 3 caption: The overview of our DepthNet. Our DepthNet consists of two encoders
    ( B e and B c ) based on ResNet-34 and a single shared decoder that unifies the
    feature maps from the two encoders. The inputs are the equirectangular and cubemap
    projections converted from a single panorama, and the output is the corresponding
    equirectangular depth map. During the encoding procedure, the feature maps of
    B e and B c are fused by our proposed fusion module (green). Unlike [3] and [7],
    our fusion module refines the original feature maps and the refined ones are then
    passed into next layers of B e and B c . To preserve complete details in the final
    predicted depth maps, we add three skip-connections by concatenating the fused
    feature maps ( f 1 fuse , f 2 fuse , f 3 fuse ) from fusion modules with decoded
    feature maps. Then, we extract multi-scale depth maps ( d 1 , d 2 , d 3 , and
    d 4 ) from these concatenated feature maps by 1x1 convolutional layers.
  Figure 4 Link: articels_figures_by_rev_year\2022\BiFuse_SelfSupervised_and_Efficient_BiProjection_Fusion_for__Depth_Estimation\figure_4.jpg
  Figure 4 caption: "The architecture of our fusion module. The feature maps from\
    \ equirectangular and cubemap branches are first concatenated and passed into\
    \ three convolutional blocks. Then, we add a skip connection to the original feature\
    \ maps and obtain the fused feature maps f \u2032 equi and f \u2032 cube , which\
    \ are the inputs of the next convolutional layers. In addition, the other fused\
    \ feature map f fuse are concatenated in the decoding process later."
  Figure 5 Link: articels_figures_by_rev_year\2022\BiFuse_SelfSupervised_and_Efficient_BiProjection_Fusion_for__Depth_Estimation\figure_5.jpg
  Figure 5 caption: The overview of our PoseNet. Our PoseNet is based on ResNet-18
    and the inputs are three sequential panoramas ( It-1, It, It+1 ) in a video and
    PoseNet infers the corresponding camera motion Pt-1 and Pt+1 . To suppress the
    ambiguity of photo consistency error in occluded areas and stabilize the training,
    PoseNet estimates four occlusion masks Xs to find the occluded areas.
  Figure 6 Link: articels_figures_by_rev_year\2022\BiFuse_SelfSupervised_and_Efficient_BiProjection_Fusion_for__Depth_Estimation\figure_6.jpg
  Figure 6 caption: The qualitative results on Matterport3D, Stanford2D3D, and PanoSUNCG
    under the supervised scenario (every two rows show qualitative results of each
    dataset). Note that the dark blue and red colors indicate close and far distance,
    and we use red circles to highlight the inconsistent predictions of all approaches.
  Figure 7 Link: articels_figures_by_rev_year\2022\BiFuse_SelfSupervised_and_Efficient_BiProjection_Fusion_for__Depth_Estimation\figure_7.jpg
  Figure 7 caption: The distortion introduced by equirectangular projection. When
    the pitch of the camera is 0 circ , the structure of the room is clear. As the
    pitch becomes larger, the effect of equirectangular distortion is more obvious.
    The distortion affects the training stability when applying existing approaches
    designed for perspective cameras to panoramas.
  Figure 8 Link: articels_figures_by_rev_year\2022\BiFuse_SelfSupervised_and_Efficient_BiProjection_Fusion_for__Depth_Estimation\figure_8.jpg
  Figure 8 caption: The 3D reconstruction comparison of BiFuse++ with other baselines.
    We note that the red circles indicate the incorrect depth prediction. Our BiFuse++
    is able to preserve the corner details, while the other approaches predict inconsistent
    results.
  Figure 9 Link: articels_figures_by_rev_year\2022\BiFuse_SelfSupervised_and_Efficient_BiProjection_Fusion_for__Depth_Estimation\figure_9.jpg
  Figure 9 caption: The qualitative results on PanoSUNCG under self-supervised scenario.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.58
  Name of the first author: Fu-En Wang
  Name of the last author: Min Sun
  Number of Figures: 11
  Number of Tables: 8
  Number of authors: 5
  Paper title: "BiFuse++: Self-Supervised and Efficient Bi-Projection Fusion for 360\xB0\
    \ Depth Estimation"
  Publication Date: 2022-09-01 00:00:00
  Table 1 caption: TABLE 1 The Quantitative Results on Matterport3D [8]
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The Quantitative Results on Stanford2D3D [9]
  Table 3 caption: TABLE 3 The Quantitative Results on PanoSUNCG [5]
  Table 4 caption: TABLE 4 The Quantitative Results After Applying Rotation Noise
    on Matterport3D [8]
  Table 5 caption: TABLE 5 The Quantitative Results After Applying Rotation Noise
    on Stanford2D3D [9]
  Table 6 caption: TABLE 6 The Number of Parameters of Different Fusion Modules (we
    set the channels to 512)
  Table 7 caption: TABLE 7 The Computational Comparison of Fusion Approaches
  Table 8 caption: TABLE 8 The Quantitative Results on PanoSUNCG [5] Under the Self-Supervised
    Scenario
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3203516
- Affiliation of the first author: hong kong university of science and technology
    (guangzhou), guangzhou, guangdong, china
  Affiliation of the last author: tencent ai lab, bellevue, wa, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\SemiSupervised_Hierarchical_Graph_Classification\figure_1.jpg
  Figure 1 caption: A hierarchical graph example with four graph instances A,B,C,D
    , each of which corresponds to a user group in a social network. This figure is
    adapted from [25].
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\SemiSupervised_Hierarchical_Graph_Classification\figure_2.jpg
  Figure 2 caption: "Overview of the proposed hierarchical graph mutual information\
    \ computation. The orange part shows the mutual information computation between\
    \ the input G and graph instance representations E ; the green part shows the\
    \ mutual information computation between graph instance representations E and\
    \ final hierarchical representations \u0393 ."
  Figure 3 Link: articels_figures_by_rev_year\2022\SemiSupervised_Hierarchical_Graph_Classification\figure_3.jpg
  Figure 3 caption: 'Schematic diagram of the learning framework SEAL-CI. There are
    two subroutines: graph instance representation (in the orange box) and hierarchical
    graph representation (in the green box).'
  Figure 4 Link: articels_figures_by_rev_year\2022\SemiSupervised_Hierarchical_Graph_Classification\figure_4.jpg
  Figure 4 caption: An illustration of the hierarchical graph of arXiv papers data.
  Figure 5 Link: articels_figures_by_rev_year\2022\SemiSupervised_Hierarchical_Graph_Classification\figure_5.jpg
  Figure 5 caption: Accuracy with different number of labeled training instances on
    text data for semi-supervised graph classification.
  Figure 6 Link: articels_figures_by_rev_year\2022\SemiSupervised_Hierarchical_Graph_Classification\figure_6.jpg
  Figure 6 caption: The false prediction rate of IC with lambda in SEAL-CI.
  Figure 7 Link: articels_figures_by_rev_year\2022\SemiSupervised_Hierarchical_Graph_Classification\figure_7.jpg
  Figure 7 caption: "The ego network of a \u201Cgame\u201D group. The left side is\
    \ the ego network, in which \u201Cgame\u201D groups are in red and \u201Cnon-game\u201D\
    \ groups are in blue. The right side is the internal structure of the ego \u201C\
    game\u201D group, in which a bigger node indicates a larger importance, and a\
    \ darker color implies a larger node degree."
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.53
  Name of the first author: Jia Li
  Name of the last author: Yu Rong
  Number of Figures: 7
  Number of Tables: 6
  Number of authors: 4
  Paper title: Semi-Supervised Hierarchical Graph Classification
  Publication Date: 2022-09-02 00:00:00
  Table 1 caption: TABLE 1 Statistics of Generated Graph Instances
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison of Different Methods on the Synthetic Data Set
    for Semi-Supervised Graph Classification
  Table 3 caption: TABLE 3 Statistics of arXiv Paper Data
  Table 4 caption: TABLE 4 Comparison of Different Methods on arXiv Paper Data for
    Semi-Supervised Graph Classification
  Table 5 caption: TABLE 5 Statistics of Collected Tencent QQ Groups
  Table 6 caption: TABLE 6 Comparison of Different Methods on Tencent QQ Group Data
    for Semi-Supervised Graph Classification
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3203703
- Affiliation of the first author: school of computer science and engineering, key
    lab of computer network and information integration (ministry of education), southeast
    university, nanjing, china
  Affiliation of the last author: school of computer science and engineering, key
    lab of computer network and information integration (ministry of education), southeast
    university, nanjing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Variational_Label_Enhancement\figure_1.jpg
  Figure 1 caption: An example of the relative importance among relevant and irrelevant
    labels.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Variational_Label_Enhancement\figure_2.jpg
  Figure 2 caption: The framework of the proposed methods. Levi-mlp and Levi-gcn are
    two LE approaches, where the inference model w is respectively instantiated by
    MLP and GCN to recover the label distributions from the training data. Then, the
    MLL training set D could be transformed into the label distribution training set
    E , which induces the regression model to deal with multi-label learning. The
    black solid lines denote the forward process, and the black dotted lines mark
    the gradient flow.
  Figure 3 Link: articels_figures_by_rev_year\2022\Variational_Label_Enhancement\figure_3.jpg
  Figure 3 caption: The visualization of the ground-truth and the recovered label
    distributions (RGB colors) on the artificial dataset.
  Figure 4 Link: articels_figures_by_rev_year\2022\Variational_Label_Enhancement\figure_4.jpg
  Figure 4 caption: "Parameter sensitivity analysis for Levi-mlp and Levi-gcn on Yeast-spoem,\
    \ SBD3DFE and Movie. (a) and (b): Performance of Levi-mlp changes in terms of\
    \ two evaluation metrics as the parameter \u03BB increases from 0.4 to 1.6. (c)\
    \ and (d): Performance of Levi-gcn changes in terms of two evaluation metrics\
    \ as the parameter \u03BB increases from 0.4 to 1.6."
  Figure 5 Link: articels_figures_by_rev_year\2022\Variational_Label_Enhancement\figure_5.jpg
  Figure 5 caption: Comparison of Levi-mlp against other comparing approaches with
    the Bonferroni-Dunn test. The approaches not connected with Levi-mlp are considered
    to be significantly different from Levi-mlp (CD=2.4905 at 0.05 significance level).
  Figure 6 Link: articels_figures_by_rev_year\2022\Variational_Label_Enhancement\figure_6.jpg
  Figure 6 caption: Comparison of Levi-gcn against other comparing approaches with
    the Bonferroni-Dunn test. The approaches not connected with Levi-gcn are considered
    to be significantly different from Levi-gcn (CD=2.4905 at 0.05 significance level).
  Figure 7 Link: articels_figures_by_rev_year\2022\Variational_Label_Enhancement\figure_7.jpg
  Figure 7 caption: 'Parameter sensitivity analysis for LEVI on enron, slashdot and
    yeast. (a) and (b): Performance changes in terms of Ranking loss and Average precision
    as the parameter beta increases from 0.4 to 1.6 ( gamma =0.01 ). (c) and (d):
    Performance changes in terms of Ranking loss and Average precision as the parameter
    gamma increases from 0.01 to 10 ( beta = 1 ).'
  Figure 8 Link: articels_figures_by_rev_year\2022\Variational_Label_Enhancement\figure_8.jpg
  Figure 8 caption: Performance comparison among Levi-gcn, Levi-mlp and Levi-non in
    terms of Ranking loss and Average precision.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.63
  Name of the first author: Ning Xu
  Name of the last author: Min-Ling Zhang
  Number of Figures: 8
  Number of Tables: 7
  Number of authors: 6
  Paper title: Variational Label Enhancement
  Publication Date: 2022-09-02 00:00:00
  Table 1 caption: TABLE 1 Statistics of the 14 Datasets Adopted in the Label Distribution
    Recovery Experiment
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Recovery Results Evaluated by Six Label Distribution Evaluation
    Metrics
  Table 3 caption: TABLE 3 Statistics of the 14 Datasets Utilized in Multi-Label Prediction
    Experiment
  Table 4 caption: "TABLE 4 Predictive Performance of Each Approach (mean \xB1 std)\
    \ Measured by Ranking Loss \u2193 \u2193"
  Table 5 caption: "TABLE 5 Predictive Performance of Each Approach (mean \xB1 std)\
    \ Measured by Hamming Loss \u2193 \u2193"
  Table 6 caption: "TABLE 6 Predictive Performance of Each Approach (mean \xB1 std)\
    \ Measured by Average Precision \u2191 \u2191"
  Table 7 caption: TABLE 7 Wilcoxon Signed-Ranks Test for Levi Against its Variant
    Levi-non in Terms of Each Evaluation Metric (at 0.05 Significance Level) and p
    p-Values are Shown in the Brackets
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3203678
- Affiliation of the first author: school of mathematics and statistics and ministry
    of education key lab of intelligent networks and network security, xian jiaotong
    university, xian, shaanxi, china
  Affiliation of the last author: school of mathematics and statistics and ministry
    of education key lab of intelligent networks and network security, xian jiaotong
    university, xian, shaanxi, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Exact_Decomposition_of_Joint_Low_Rankness_and_Local_Smoothness_Plus_Sparse_Matri\figure_1.jpg
  Figure 1 caption: From top to bottom, a frame of natural, video, hyperspectral,
    and multispectral image (a), the gradient maps of all images in vertical dimension
    and their statistical histograms (b) and the gradient maps of all images in horizontal
    dimension and their statistical histograms (c).
  Figure 10 Link: articels_figures_by_rev_year\2022\Exact_Decomposition_of_Joint_Low_Rankness_and_Local_Smoothness_Plus_Sparse_Matri\figure_10.jpg
  Figure 10 caption: Recovered images of all competing methods with bands 6-104-36
    as R-G-B. (a) The original urban part image. (b-l) Restoration results obtained
    by 11 comparison methods, with a demarcated zoomed in three times for easy observation.
  Figure 2 Link: articels_figures_by_rev_year\2022\Exact_Decomposition_of_Joint_Low_Rankness_and_Local_Smoothness_Plus_Sparse_Matri\figure_2.jpg
  Figure 2 caption: The illustration of correlation and sparsity of gradient maps.
    The left and right figures show the illustration of an HSI and streaming video
    data separately. The G i ,i=1,2,3 is the gradient tensor map by conducting differential
    operator, and the G i ,i=1,2,3 is the gradient matrix map by unfolding the gradient
    tensor map along the spectral direction. Columns (a-5) and (b-5) show the sparsity
    of each gradient map. Columns (a-6) and (b-6) show the singular vector curve of
    gradient maps.
  Figure 3 Link: articels_figures_by_rev_year\2022\Exact_Decomposition_of_Joint_Low_Rankness_and_Local_Smoothness_Plus_Sparse_Matri\figure_3.jpg
  Figure 3 caption: The simulated data generated mechanism of joint low rank and local
    smoothness data X 0 .
  Figure 4 Link: articels_figures_by_rev_year\2022\Exact_Decomposition_of_Joint_Low_Rankness_and_Local_Smoothness_Plus_Sparse_Matri\figure_4.jpg
  Figure 4 caption: Convergence curves of 3DCTV-RPCA model (9).
  Figure 5 Link: articels_figures_by_rev_year\2022\Exact_Decomposition_of_Joint_Low_Rankness_and_Local_Smoothness_Plus_Sparse_Matri\figure_5.jpg
  Figure 5 caption: "Comparison of logarithmic relative error (left) and the constant\
    \ value \u03BC of incoherence conditions (right) with fixed rank rn=0.05 and varying\
    \ sparsity \u03C1 s ."
  Figure 6 Link: articels_figures_by_rev_year\2022\Exact_Decomposition_of_Joint_Low_Rankness_and_Local_Smoothness_Plus_Sparse_Matri\figure_6.jpg
  Figure 6 caption: "Comparison of logarithmic relative error (left) and the constant\
    \ value \u03BC of incoherence conditions (right) with fixed sparsity \u03C1 s\
    \ =0.05 and varying rank r ."
  Figure 7 Link: articels_figures_by_rev_year\2022\Exact_Decomposition_of_Joint_Low_Rankness_and_Local_Smoothness_Plus_Sparse_Matri\figure_7.jpg
  Figure 7 caption: Fraction of correct recoveries across 30 trials, as a function
    of sparsity of mathbf S0 ( x -axis) and of rank( mathbf X0 ) ( y -axis). The phase
    transition diagram of PCP model (a) and 3DCTV-RPCA model (b).
  Figure 8 Link: articels_figures_by_rev_year\2022\Exact_Decomposition_of_Joint_Low_Rankness_and_Local_Smoothness_Plus_Sparse_Matri\figure_8.jpg
  Figure 8 caption: Recovered images of all competing methods with bands 58-27-17
    as R-G-B. (a) The simulated DC mall image. (b) The noise image with Gaussian noise
    variance is 0.4. (c-i) The results were obtained by all comparison methods, with
    a demarcated zoomed in three times for easy observation.
  Figure 9 Link: articels_figures_by_rev_year\2022\Exact_Decomposition_of_Joint_Low_Rankness_and_Local_Smoothness_Plus_Sparse_Matri\figure_9.jpg
  Figure 9 caption: Recovered images of all competing methods with bands 58-27-17
    as R-G-B. (a) The simulated DC mall image. (b) The noise image with Gaussian noise
    variance is 0.05 and the sparse noise variance is 0.2. (c-i) Restoration results
    obtained by all comparison methods, with a demarcated zoomed in 3 times for easy
    observation.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.68
  Name of the first author: Jiangjun Peng
  Name of the last author: Deyu Meng
  Number of Figures: 14
  Number of Tables: 4
  Number of authors: 5
  Paper title: Exact Decomposition of Joint Low Rankness and Local Smoothness Plus
    Sparse Matrices
  Publication Date: 2022-09-05 00:00:00
  Table 1 caption: TABLE 1 Comparison of Computational Complexities and Encoded Priors
    of 3DCTV-RPCA, RPCA and LRTV Methods
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The Quantitative Comparison of all Competing Methods Under
    Different Levels of Noise
  Table 3 caption: TABLE 3 The Quantitative Comparison of all Competing Methods Under
    Different Noise Levels on 32 Scenes in the CAVE Database
  Table 4 caption: TABLE 4 AUC Comparison of all Competing Methods on all Video Sequences
    in the Li Dataset
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3204203
- Affiliation of the first author: department of computer science and engineering,
    texas a&m university, college station, tx, usa
  Affiliation of the last author: department of computer science and engineering,
    texas a&m university, college station, tx, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\Explainability_in_Graph_Neural_Networks_A_Taxonomic_Survey\figure_1.jpg
  Figure 1 caption: 'An overview of our proposed taxonomy. We categorize existing
    GNN explanation approaches into two branches: instance-level methods and model-level
    methods. For the instance-level methods, the gradientsfeatures-based methods include
    SA [54], Guided BP [54], CAM [55], and Grad-CAM [55]; the perturbation-based methods
    are GNNExplainer [46], PGExplainer [47], ZORRO [56], GraphMask [57], Causal Screening
    [58], and SubgraphX [48]; the decomposition methods contains LRP [54], [59], Excitation
    BP [55] and GNN-LRP [60]; the surrogate methods include GraphLime [61], RelEx
    [62], and PGM-Explainer [63]. For the model-level methods, the only existing approach
    is XGNN [45].'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Explainability_in_Graph_Neural_Networks_A_Taxonomic_Survey\figure_2.jpg
  Figure 2 caption: The general pipeline of the perturbation-based methods. They employ
    different mask generation algorithms to obtain different types of masks. Note
    that the mask can correspond to nodes, edges, or node features. In this example,
    we show a soft mask for node features, a discrete mask for edges, and an approximated
    discrete mask for nodes. Then the mask is combined with the input graph to capture
    important input information. Finally, the trained GNNs evaluate whether the new
    prediction is similar to the original prediction and can provide guidance for
    improving the mask generation algorithms.
  Figure 3 Link: articels_figures_by_rev_year\2022\Explainability_in_Graph_Neural_Networks_A_Taxonomic_Survey\figure_3.jpg
  Figure 3 caption: The general pipeline of the surrogate methods. Given an input
    graph and its prediction, they first sample a local dataset to represent the relationships
    around the target data. Then different surrogate methods are applied to fit the
    local dataset. Note that surrogate models are generally simple and interpretable
    ML models. Finally, the explanations from the surrogate model can be regarded
    as the explanations of the original prediction.
  Figure 4 Link: articels_figures_by_rev_year\2022\Explainability_in_Graph_Neural_Networks_A_Taxonomic_Survey\figure_4.jpg
  Figure 4 caption: The general pipeline of the decomposition methods. These methods
    distribute the prediction score to input space to indicate the input importance.
    The target score is decomposed layer by layer in a back-propagation manner. Note
    that the main difference among these methods is the employed score decomposition
    rule.
  Figure 5 Link: articels_figures_by_rev_year\2022\Explainability_in_Graph_Neural_Networks_A_Taxonomic_Survey\figure_5.jpg
  Figure 5 caption: A data example of our graph sentiment datasets. Each node represents
    a word while edges indicates the relationship between the words. Note that edges
    are directed but their labels are ignored.
  Figure 6 Link: articels_figures_by_rev_year\2022\Explainability_in_Graph_Neural_Networks_A_Taxonomic_Survey\figure_6.jpg
  Figure 6 caption: The Fidelity+ comparisons between different GNN explanation techniques
    under different Sparsity levels.
  Figure 7 Link: articels_figures_by_rev_year\2022\Explainability_in_Graph_Neural_Networks_A_Taxonomic_Survey\figure_7.jpg
  Figure 7 caption: The Fidelity- comparisons between different GNN explanation techniques
    under different Sparsity levels.
  Figure 8 Link: articels_figures_by_rev_year\2022\Explainability_in_Graph_Neural_Networks_A_Taxonomic_Survey\figure_8.jpg
  Figure 8 caption: Visualizations of explanations on the BBBP and Graph-SST2 datasets.
  Figure 9 Link: articels_figures_by_rev_year\2022\Explainability_in_Graph_Neural_Networks_A_Taxonomic_Survey\figure_9.jpg
  Figure 9 caption: A overview of our open-source library, which includes unified
    implementations of multiple common baseline methods, commonly used datasets, and
    three evaluation metrics.
  First author gender probability: 0.9
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.78
  Name of the first author: Hao Yuan
  Name of the last author: Shuiwang Ji
  Number of Figures: 9
  Number of Tables: 5
  Number of authors: 4
  Paper title: 'Explainability in Graph Neural Networks: A Taxonomic Survey'
  Publication Date: 2022-09-05 00:00:00
  Table 1 caption: TABLE 1 A Comprehensive Analysis of Different Explanation Methods
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Statistics and Properties of Our Sentiment Graph Datasets
  Table 3 caption: TABLE 3 The Fidelity+ Comparisons Between Different GNN Explanation
    Techniques and the Random Designation Baseline
  Table 4 caption: TABLE 4 The Fidelity- Comparisons Between Different GNN Explanation
    Techniques and the Random Designation Baseline
  Table 5 caption: TABLE 5 The Accuracy and Stability Comparisons Between Different
    GNN Explanation Techniques
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3204236
- Affiliation of the first author: information technology university (itu) of punjab,
    lahore, pakistan
  Affiliation of the last author: qatar university, doha, qatar
  Figure 1 Link: articels_figures_by_rev_year\2022\Untrained_Neural_Network_Priors_for_Inverse_Imaging_Problems_A_Survey\figure_1.jpg
  Figure 1 caption: "Pipeline to solve IIPs. In the forward model, the transformation\
    \ A(.) (i.e., sensing model) is applied to an input image x 0 to acquire measurements.\
    \ The inverse problem aims to obtain an estimate of x 0 from the observation via\
    \ a reconstruction algorithm that leverages the prior knowledge about the true\
    \ (target) image x 0 and A(.) . Normal noise \u03B7 is added to the measurements\
    \ which are fed to the reconstruction algorithm."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Untrained_Neural_Network_Priors_for_Inverse_Imaging_Problems_A_Survey\figure_2.jpg
  Figure 2 caption: 'Examples of inverse imaging problems (IIPs). Individual images
    adapted from: Left: [18]; Right (top to bottom): first 1 ; second [24]; third
    [64]; fourth [65].'
  Figure 3 Link: articels_figures_by_rev_year\2022\Untrained_Neural_Network_Priors_for_Inverse_Imaging_Problems_A_Survey\figure_3.jpg
  Figure 3 caption: Denoising input image x 0 over different iterations of UNNP, z
    is the fixed random noise which is given as an input to UNNP (Figure adapted from
    [6]).
  Figure 4 Link: articels_figures_by_rev_year\2022\Untrained_Neural_Network_Priors_for_Inverse_Imaging_Problems_A_Survey\figure_4.jpg
  Figure 4 caption: 'Different UNNP architectures proposed in the literature. Relevant
    papers: (a) [6] ; (b) [12] ; (c) [7], [25], [49] ; (d) [37], [68], [69] ; (e)
    [28], [70] ; (f) [67] ; (g) [39].'
  Figure 5 Link: articels_figures_by_rev_year\2022\Untrained_Neural_Network_Priors_for_Inverse_Imaging_Problems_A_Survey\figure_5.jpg
  Figure 5 caption: As the optimization process of UNNP progresses, the recovery of
    the image is progressively improved while the degradation (e.g., holes, noise)
    reduces gradually. Figures reproduced using the publicly available source code.
    2
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Adnan Qayyum
  Name of the last author: Junaid Qadir
  Number of Figures: 5
  Number of Tables: 4
  Number of authors: 6
  Paper title: 'Untrained Neural Network Priors for Inverse Imaging Problems: A Survey'
  Publication Date: 2022-09-05 00:00:00
  Table 1 caption: TABLE 1 Summary of Different Alternative Terms Used to Describe
    Untrained Neural Networks-Based Priors Over the Past Few Years, They All Rely
    on the Same Underlying Principle
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 A Summary of Different Inverse Problems and Associated
    Forward Operators (Adapted From [4])
  Table 3 caption: TABLE 3 Summary of Various Applications of Untrained Neural Network
    Priors (UNNPs) for IIPs
  Table 4 caption: TABLE 4 Summary of Medical Imaging Applications of Untrained Neural
    Network Priors (UNNPs)
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3204527
- Affiliation of the first author: digital medical research center, school of basic
    medical science, fudan university, shanghai, china
  Affiliation of the last author: digital medical research center, school of basic
    medical science, fudan university, shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Robust_Point_Cloud_Registration_Framework_Based_on_Deep_Graph_Matching\figure_1.jpg
  Figure 1 caption: The idea of point cloud registration based on graph matching.
    Dashed lines represent correspondences. Point features and graph features are
    the features extracted directly through points and the features extracted based
    on graphs, respectively. The two points x i and y j have similar point features
    because they have similar local geometries, but they have different graph features
    because the graph topologies around them are different, so they are not mismatched
    when graph-based matching is used.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Robust_Point_Cloud_Registration_Framework_Based_on_Deep_Graph_Matching\figure_2.jpg
  Figure 2 caption: "The pipeline of the proposed 3D rigid point cloud registration\
    \ framework, RGM, where \u2A01 represents concatenate features and \u2A02 represents\
    \ matrix multiplication. The solid lines are the data flow of both training and\
    \ testing, and the dashed lines are the data flow that exists only in testing."
  Figure 3 Link: articels_figures_by_rev_year\2022\Robust_Point_Cloud_Registration_Framework_Based_on_Deep_Graph_Matching\figure_3.jpg
  Figure 3 caption: Qualitative registration results on ModelNet40, (a) clean, (b)
    noise, (c) partial-to-partial, and (d) unseen categories, (e) cross dataset, (f)full-range
    rotation.
  Figure 4 Link: articels_figures_by_rev_year\2022\Robust_Point_Cloud_Registration_Framework_Based_on_Deep_Graph_Matching\figure_4.jpg
  Figure 4 caption: "Qualitative registration results of RGM \u22C6 on 3DMatch."
  Figure 5 Link: articels_figures_by_rev_year\2022\Robust_Point_Cloud_Registration_Framework_Based_on_Deep_Graph_Matching\figure_5.jpg
  Figure 5 caption: Visualization of graphs obtained by different edge generation
    methods. For our graph, down-sampled points and edges with large weights are illustrated.
    Please note that most edges of our graph are in the overlapping region and the
    edges of the two graphs roughly correspond to each other. These two properties
    are expected for better registration.
  Figure 6 Link: articels_figures_by_rev_year\2022\Robust_Point_Cloud_Registration_Framework_Based_on_Deep_Graph_Matching\figure_6.jpg
  Figure 6 caption: An illustrative case of the ground-truth correspondences and the
    correspondences generated by RGM and its variants. Much more correct correspondences
    are generated by RGM.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Kexue Fu
  Name of the last author: Manning Wang
  Number of Figures: 6
  Number of Tables: 8
  Number of authors: 6
  Paper title: Robust Point Cloud Registration Framework Based on Deep Graph Matching
  Publication Date: 2022-09-06 00:00:00
  Table 1 caption: TABLE 1 Performance on Clean Point Clouds
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Performance on Point Clouds With Gaussian Noise
  Table 3 caption: TABLE 3 Performance on Partial-to-Partial Point Clouds
  Table 4 caption: TABLE 4 Performance on Unseen Categories Point Clouds
  Table 5 caption: TABLE 5 Performance on Full-Range Rotation Point Clouds
  Table 6 caption: TABLE 6 Performance of Cross-Dataset Generalization (Learning-Based
    Methods are Trained on ModelNet40 and Tested on ShapeNet)
  Table 7 caption: TABLE 7 Performance on Indoor Datasets 3DMatch and 3DLoMatch
  Table 8 caption: TABLE 8 Ablation Studies
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3204713
