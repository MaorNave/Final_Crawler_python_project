- Affiliation of the first author: department of computer science, university of york,
    york, united kingdom
  Affiliation of the last author: department of psychology, university of york, york,
    united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2019\Defining_Image_Memorability_Using_the_Visual_Memory_Schema\figure_1.jpg
  Figure 1 caption: Visual memory schemas (VMSs) corresponding to correct (b), false
    (c) and both correct and false retrievals (d) of the image shown in (a). In this
    paper, visual memory schemas correspond to human-annotated regions which are pooled
    across observers, who are asked during a memory experiment to indicate the regions
    of the image that made them remember that image.
  Figure 10 Link: articels_figures_by_rev_year\2019\Defining_Image_Memorability_Using_the_Visual_Memory_Schema\figure_10.jpg
  Figure 10 caption: The least memorable, moderately memorable and the most memorable
    image, are shown together with true and false VMSs, where HR and FAR scores are
    indicated as well.
  Figure 2 Link: articels_figures_by_rev_year\2019\Defining_Image_Memorability_Using_the_Visual_Memory_Schema\figure_2.jpg
  Figure 2 caption: The memory experiment has two stages. During the first stage,
    the participants in the experiment, are shown 400 images, each for 3 seconds.
    During the second stage they are shown another 400 images, including 200 that
    are repetitions from the first stage. Participants are also asked to rate how
    well they think they remember the image they see and select rectangular regions
    from the image that made them remember it.
  Figure 3 Link: articels_figures_by_rev_year\2019\Defining_Image_Memorability_Using_the_Visual_Memory_Schema\figure_3.jpg
  Figure 3 caption: "Spearmans rank correlation between the hit rates and the false\
    \ alarm rates as a function of the participant rating score threshold. At the\
    \ selected threshold 40, \u03C1 value is 0.0036."
  Figure 4 Link: articels_figures_by_rev_year\2019\Defining_Image_Memorability_Using_the_Visual_Memory_Schema\figure_4.jpg
  Figure 4 caption: "Receiver operating characteristics (ROC) for the overall experiment\
    \ and for each participant separately. The thick-dashed line is the estimated\
    \ fourth-order B\xE9zier curve of the red dots, which together represent the ROC\
    \ curve of the overall experiment. The thin dashed lines of various colours stand\
    \ for the ROC curve results for each participant separately."
  Figure 5 Link: articels_figures_by_rev_year\2019\Defining_Image_Memorability_Using_the_Visual_Memory_Schema\figure_5.jpg
  Figure 5 caption: 'Categories of images in a hierarchical structure used for the
    experiments and average hit rates obtained from the three memory experiments:
    proposed framework indicated by circles and AMT1 [6] indicated by crosses and
    AMT2 [6] indicated by plus sign. The number of images used for each FIGRIMSUN
    category is indicated next to the category labels on x -axis. Results are indicated
    for each category separately.'
  Figure 6 Link: articels_figures_by_rev_year\2019\Defining_Image_Memorability_Using_the_Visual_Memory_Schema\figure_6.jpg
  Figure 6 caption: "Histograms of the average correlation ( \u03C1 2D ) and MI, between\
    \ the VMSs corresponding to two equally sized groups of participants. The green\
    \ histograms represent the correlation or MI for the true VMSs, while the red\
    \ histograms correspond to those of the false VMSs."
  Figure 7 Link: articels_figures_by_rev_year\2019\Defining_Image_Memorability_Using_the_Visual_Memory_Schema\figure_7.jpg
  Figure 7 caption: "Correlation ( \u03C1 2D ) and MI histograms between the VMSs\
    \ and eye fixations maps are depicted separately in green for the true VMS, in\
    \ red for the false VMS and the combined VMSs in black."
  Figure 8 Link: articels_figures_by_rev_year\2019\Defining_Image_Memorability_Using_the_Visual_Memory_Schema\figure_8.jpg
  Figure 8 caption: Correlation ( rho 2D ) and MI histograms between VMS and saliency
    maps are depicted separately for true VMS, shown in green, false VMS, shown in
    red, and combined VMSs, shown in black.
  Figure 9 Link: articels_figures_by_rev_year\2019\Defining_Image_Memorability_Using_the_Visual_Memory_Schema\figure_9.jpg
  Figure 9 caption: Plotting predicted against empirically obtained memorability scores
    for each image category. The circles are the average predictions results for VMS-weighted
    features, as shown in Table 5), whereas the plus signs are the average prediction
    results when No VMS is used, as shown in Table 2). The ellipses, drawn around
    the small central circles, indicate the error spreads of the VMS-weighted features
    with widths corresponding to three standard deviations in the direction of each
    eigen-vector.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Erdem Akagunduz
  Name of the last author: Karla K. Evans
  Number of Figures: 12
  Number of Tables: 6
  Number of authors: 3
  Paper title: Defining Image Memorability Using the Visual Memory Schema
  Publication Date: 2019-05-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average HR and FAR Values with Their Corresponding Standard
      Deviations Compared with the Results from the FIGRIM Experiments [6]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Performance of Computer Vision Features on Predicting
      Image Memorability and Human Consistency
  Table 3 caption:
    table_text: TABLE 3 The Performance of Computer Vision Features on Predicting
      Image Memorability When Spatially Pooled and Weighted with Saliency Maps
  Table 4 caption:
    table_text: TABLE 4 The Performance of Computer Vision Features on Predicting
      Image Memorability When Spatially Pooled and Weighted with Eye-Fixation Maps
  Table 5 caption:
    table_text: TABLE 5 The Performance of Computer Vision Features on Predicting
      Image Memorability When Spatially Pooled and Weighted with VMS Selections
  Table 6 caption:
    table_text: TABLE 6 The Generic Structure of the CNNs Used in the Experiments
      Is Given
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2914392
- Affiliation of the first author: research school of engineering, australian national
    university, canberra, australia
  Affiliation of the last author: research school of engineering, australian national
    university, canberra, australia
  Figure 1 Link: articels_figures_by_rev_year\2019\Can_We_See_More_Joint_Frontalization_and_Hallucination_of_Unaligned_Tiny_Faces\figure_1.jpg
  Figure 1 caption: "Comparison with the combination of face hallucination [2] and\
    \ frontalization [1] methods. (a) 16\xD716 LR non-frontal input image. (b) 128\xD7\
    128 HR original frontal image (not available in training). (c) The best possible\
    \ match to the given LR image in the dataset after compensating for in-plane rotations\
    \ by STN 0 [3]. (d) Detected landmarks by [4] after bicubic upsampling. (e) Result\
    \ obtained by applying [1] first and then [2]. In [2], the first decoder and encoder\
    \ are used to reduce image noise. Hereby, we only use the second decoder of [2]\
    \ for super-resolving LR faces. (f) Result obtained by applying [2] first and\
    \ then [1]. (g) Image generated by [2], which is retrained with LR non-frontal\
    \ and HR frontal face images. (h) Our result."
  Figure 10 Link: articels_figures_by_rev_year\2019\Can_We_See_More_Joint_Frontalization_and_Hallucination_of_Unaligned_Tiny_Faces\figure_10.jpg
  Figure 10 caption: "Illustrations of super-resolving and frontalizing face images\
    \ in different resolutions by our method. First row: Ground-truth frontal HR face\
    \ images. Second row: Input LR faces (left) and our results with a magnification\
    \ factor 8\xD7 (right). Third row: Input LR faces and our results with a magnification\
    \ factor 4\xD7. Fourth row: input LR faces and our results with a magnification\
    \ factor 2\xD7."
  Figure 2 Link: articels_figures_by_rev_year\2019\Can_We_See_More_Joint_Frontalization_and_Hallucination_of_Unaligned_Tiny_Faces\figure_2.jpg
  Figure 2 caption: 'TANN consists of two parts: A transformative upsampling network
    (red box) and a discriminative network (blue box). In our transformer subnetwork,
    we also employ skip connections between our encoding layers and decoding layers,
    indicated by the purple line. For simplicity, we only draw the first skip connections.'
  Figure 3 Link: articels_figures_by_rev_year\2019\Can_We_See_More_Joint_Frontalization_and_Hallucination_of_Unaligned_Tiny_Faces\figure_3.jpg
  Figure 3 caption: "Artifacts caused by the state-of-the-art face frontalization\
    \ and hallucination methods. (a) The input 16\xD716 LR image. (b) The original\
    \ 128\xD7128 HR frontal image. (c) The aligned upright version of (a) by STN 0\
    \ . (d) Frontalized result of (c) using [1]. Note that, we first upsample (c)\
    \ by bicubic interpolation, then apply [1], and downsample the frontalized result.\
    \ (e) HR image after applying [51] to (d). (f) HR image after applying [51] to\
    \ (c) directly. (g) The frontalized version of (f) by [1]. (h) The result of applying\
    \ [2] to (a). (i) The result of TANN without the transformer subnetwork, which\
    \ is similar to the upsampling network [2], retrained with LR non-frontal and\
    \ HR frontal faces. (j) The aligned and frontalized LR face by our transformer\
    \ subnetwork. Note that, in our end-to-end trained TANN, the output of the transformer\
    \ network is a set of feature maps not an image. (k) The hallucinated result of\
    \ (j) by our upsampling subnetwork (here, we retrained the upsampling network).\
    \ (l) Our final result."
  Figure 4 Link: articels_figures_by_rev_year\2019\Can_We_See_More_Joint_Frontalization_and_Hallucination_of_Unaligned_Tiny_Faces\figure_4.jpg
  Figure 4 caption: "Illustrations of influence of different losses. (a) The input\
    \ 16\xD716 LR images. (b) The original 128\xD7128 HR frontal images. (c) The downsampled\
    \ version of (b). (d) The frontalized LR faces by our transformer subnetwork.\
    \ (e) The upsampling results only using pixel-wise loss. (f) The upsampling results\
    \ using the pixel-wise and perceptual losses. (g) The upsampling results without\
    \ using the triplet loss. (h) Our final results."
  Figure 5 Link: articels_figures_by_rev_year\2019\Can_We_See_More_Joint_Frontalization_and_Hallucination_of_Unaligned_Tiny_Faces\figure_5.jpg
  Figure 5 caption: Illustration of the synthesized dataset. (a) Original frontal
    HR face image. (b) The generated views of (a). (c) Spatially transformed and downsampled
    version of (b).
  Figure 6 Link: articels_figures_by_rev_year\2019\Can_We_See_More_Joint_Frontalization_and_Hallucination_of_Unaligned_Tiny_Faces\figure_6.jpg
  Figure 6 caption: "Results of the state-of-the-art methods for frontalization followed\
    \ by hallucination. The input faces are first frontalized by [1] and then hallucinated\
    \ by different algorithms. Rows: +75 \u2218 , +40 \u2218 , 0 \u2218 , \u221240\
    \ \u2218 , and \u221275 \u2218 . Columns: (a) Unaligned non-frontal LR inputs.\
    \ (b) Original frontal HR images. (c) [1] + bicubic interpolation. (d) [1] + [59].\
    \ (e) [1] + [60]. (f) [1] + [17]. (g) [1] + [51]. (h) [1] + [2]. (i) Our method.\
    \ Notice that, TANN does not need or use [1]."
  Figure 7 Link: articels_figures_by_rev_year\2019\Can_We_See_More_Joint_Frontalization_and_Hallucination_of_Unaligned_Tiny_Faces\figure_7.jpg
  Figure 7 caption: 'Results of the state-of-the-art methods for hallucination followed
    by frontalization by [1]. Columns: (a) Unaligned non-frontal LR inputs. (b) Original
    frontal HR images. (c) Bicubic interpolation + [1]. (d) [59] + [1]. (e) [60] +
    [1]. (f) [17] + [1]. (g) [51] + [1]. (h) [2] + [1]. (i) Our method.'
  Figure 8 Link: articels_figures_by_rev_year\2019\Can_We_See_More_Joint_Frontalization_and_Hallucination_of_Unaligned_Tiny_Faces\figure_8.jpg
  Figure 8 caption: 'Results of the state-of-the-art methods for frontalization followed
    by hallucination. Columns: (a) Unaligned non-frontal LR inputs. (b) Original frontal
    HR images. (c) [1] + bicubic interpolation. (d) [1] + [59]. (e) [1] + [60]. (f)
    [1] + [17]. (g) [1] + [51]. (h) [1] + [2]. (i) Our method.'
  Figure 9 Link: articels_figures_by_rev_year\2019\Can_We_See_More_Joint_Frontalization_and_Hallucination_of_Unaligned_Tiny_Faces\figure_9.jpg
  Figure 9 caption: 'Results of the state-of-the-art methods for hallucination followed
    by frontalization by [1]. Columns: (a) Unaligned non-frontal LR inputs. (b) Original
    frontal HR images. (c) Bicubic interpolation + [1]. (d) [59] + [1]. (e) [60] +
    [1]. (f) [17] + [1]. (g) [51] + [1]. (h) [2] + [1]. (i) Our method.'
  First author gender probability: 0.6
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Xin Yu
  Name of the last author: Fatih Porikli
  Number of Figures: 15
  Number of Tables: 7
  Number of authors: 4
  Paper title: Can We See More? Joint Frontalization and Hallucination of Unaligned
    Tiny Faces
  Publication Date: 2019-05-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Evaluations on the Entire Test Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Evaluations on Different Out-of-Plane Rotation
      Degrees
  Table 3 caption:
    table_text: TABLE 3 Quantitative Evaluations on the Frontal View
  Table 4 caption:
    table_text: TABLE 4 Results of Different Face Recognition Networks Trained on
      Different Source Images
  Table 5 caption:
    table_text: TABLE 5 Face Recognition Results for Different Methods
  Table 6 caption:
    table_text: TABLE 6 Face Retrieval Results for Different Methods
  Table 7 caption:
    table_text: TABLE 7 Quantitative Evaluations on the Influence of Different Losses
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2914039
- Affiliation of the first author: faculty of information science and electrical engineering,
    kyushu university, fukuoka, japan
  Affiliation of the last author: institute of industrial science, the university
    of tokyo, tokyo, japan
  Figure 1 Link: articels_figures_by_rev_year\2019\Hierarchical_Gaussian_Descriptors_with_Application_to_Person_ReIdentification\figure_1.jpg
  Figure 1 caption: 'Importance of hierarchal distribution: (a) Regions that have
    the same distribution (meancovariance) of pixel features (each color indicates
    the same feature vector). (b) Local patches with a different pixel feature distribution
    inside the regions. (c) Regions can be distinguished via distributions of patch
    level distributions.'
  Figure 10 Link: articels_figures_by_rev_year\2019\Hierarchical_Gaussian_Descriptors_with_Application_to_Person_ReIdentification\figure_10.jpg
  Figure 10 caption: 'Histogram of decomposed logarithmic eigenvalues ( alpha and
    beta ) on the VIPeR dataset: (a)(b) Non-scaled patchregion Gaussian matrices.
    In (b), (alpha , beta ) (alpha, beta) represent the values in the case withoutwith
    applying MSN to the patch Gaussians. The relation |alpha | gg |beta | implies
    that a large diagonal bias occurs in the matrix.'
  Figure 2 Link: articels_figures_by_rev_year\2019\Hierarchical_Gaussian_Descriptors_with_Application_to_Person_ReIdentification\figure_2.jpg
  Figure 2 caption: 'Importance of mean: (a) Original images. (b) Mean RGB values
    of local patches. (c) Mean removed images. Determining the same persons from (b)
    is easy, whereas difficult from (c).'
  Figure 3 Link: articels_figures_by_rev_year\2019\Hierarchical_Gaussian_Descriptors_with_Application_to_Person_ReIdentification\figure_3.jpg
  Figure 3 caption: 'Hierarchical Gaussian Descriptors (HGDs): (a) The two descriptors
    we extract by changing the Gaussian embeddings of patchregion levels. (b) Common
    pipeline for each descriptor: (i) Densely extract local patches located inside
    each region. (ii) Describe each of these local patches via a Gaussian distribution
    of pixel features which we refer to as a patch Gaussian. (iii) Flatten and vectorize
    each of the patch Gaussians by considering their underlying Riemannian geometry.
    (iv) Summarize the patch Gaussians inside a region into a region Gaussian. (v)
    Flatten the region Gaussian and create a feature vector. (vi) Concatenate the
    feature vectors extracted from all regions into one vector.'
  Figure 4 Link: articels_figures_by_rev_year\2019\Hierarchical_Gaussian_Descriptors_with_Application_to_Person_ReIdentification\figure_4.jpg
  Figure 4 caption: Two counter-bias methods applied before feature norm normalization.
    (a) Extrinsic and (b) Intrinsic bias removals. Note that the sample points here
    are the region Gaussian matrices of different training images.
  Figure 5 Link: articels_figures_by_rev_year\2019\Hierarchical_Gaussian_Descriptors_with_Application_to_Person_ReIdentification\figure_5.jpg
  Figure 5 caption: Example images from the person re-id datasets. For each dataset,
    images in the same column represent the same person.
  Figure 6 Link: articels_figures_by_rev_year\2019\Hierarchical_Gaussian_Descriptors_with_Application_to_Person_ReIdentification\figure_6.jpg
  Figure 6 caption: Feature embedding analysis on the VIPeR dataset. All methods use
    the E-L2 normalization without MSN and PN. The figures on the CMC curves indicate
    the rank-1 rates.
  Figure 7 Link: articels_figures_by_rev_year\2019\Hierarchical_Gaussian_Descriptors_with_Application_to_Person_ReIdentification\figure_7.jpg
  Figure 7 caption: 'Analysis of regularization parameters: (a) PUR scores of the
    GOG descriptor in cases withoutwith applying MSN. (b) Eigenvalue distribution
    of patchregion covariance matrices (wo MSN). To show a typical example, we used
    patch Gaussian matrices of epsilon = 10-4 for boldsymbol Sigmamathcal G .'
  Figure 8 Link: articels_figures_by_rev_year\2019\Hierarchical_Gaussian_Descriptors_with_Application_to_Person_ReIdentification\figure_8.jpg
  Figure 8 caption: Analysis of patchregion sizes on the VIPeR dataset.
  Figure 9 Link: articels_figures_by_rev_year\2019\Hierarchical_Gaussian_Descriptors_with_Application_to_Person_ReIdentification\figure_9.jpg
  Figure 9 caption: 'Visualization of patch Gaussian matrices extracted from randomly
    sampled patches on the VIPeR dataset: (a) Image patches ( 7 times 7 pixels). (b)
    Patch Gaussian matrices ( 9 times 9 dims). (c)(d) After obtaining the principal
    matrix logarithm of (b) withoutwith applying MSN.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Tetsu Matsukawa
  Name of the last author: Yoichi Sato
  Number of Figures: 12
  Number of Tables: 4
  Number of authors: 4
  Paper title: Hierarchical Gaussian Descriptors with Application to Person Re-Identification
  Publication Date: 2019-05-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Impact of the Hierarchical Gaussian Embedding Evaluated
      with MSN
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Evaluating the Impact of MSN, PN, and Norm Normalizations
  Table 3 caption:
    table_text: 'TABLE 3 Comparison of State-of-the-Art Descriptors: (a) Meta-Descriptors;
      (b) Hand-Crafted Re-Id Descriptors; (c) Re-Id Features Using CNN'
  Table 4 caption:
    table_text: TABLE 4 State-of-the-Art Results (CMCrank-rmAP):(a) Feature Extraction
      + Metric Learning Approaches; (b) Deep Learning Approaches
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2914686
- Affiliation of the first author: firstfuel software
  Affiliation of the last author: department of computer science, boston university,
    boston, ma, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Hashing_with_Mutual_Information\figure_1.jpg
  Figure 1 caption: "Overview of the proposed hashing method. We use a deep neural\
    \ network to compute b -bit binary codes for: a (1) query image x , (2) its neighbors\
    \ in \u2295 x , and (3) its non-neighbors in \u2296 x . The binary codes are obtained\
    \ via thresholding the activations in the last layer of the neural network. Computing\
    \ hamming distances between the binary code of the query and the binary codes\
    \ of neighbors and non-neighbors yields two distributions of Hamming distances.\
    \ The information-theoretic quantity, Mutual Information, can be used to capture\
    \ the separability between these two distributions, which gives a good quality\
    \ indicator and learning objective."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Hashing_with_Mutual_Information\figure_2.jpg
  Figure 2 caption: (Left) We plot the training objective value (Equation (11)) and
    the mAP score from the 32-bit cifar-1 experiment. Notice that both the mutual
    information objective and the mAP value show similar behavior, i.e., exhibit strong
    correlation. (Middle) We apply min-max normalization in order to scale both measures
    to the same range. (Right) We conduct an additional set of experiments in which
    100 instances are selected as the query set, and the rest is used to populate
    the hash table. The hash mapping parameters are randomly sampled from a Gaussian,
    similar to LSH [15]. Each experiment is conducted 50 times. There exists strong
    correlation between MI and mAP as validated by the Pearson Correlation Coefficient
    score of 0.98.
  Figure 3 Link: articels_figures_by_rev_year\2019\Hashing_with_Mutual_Information\figure_3.jpg
  Figure 3 caption: "We plot the distributions p + D and p \u2212 D , averaged on\
    \ the CIFAR-10 test set, before and after learning MIHash with a single-layer\
    \ model and 20K training examples. Optimizing the mutual information objective\
    \ substantially reduces the overlap between them, resulting in high mAP."
  Figure 4 Link: articels_figures_by_rev_year\2019\Hashing_with_Mutual_Information\figure_4.jpg
  Figure 4 caption: t-SNE [70] visualization of the 48-bit binary codes produced by
    MIHash and HashNet on ImageNet100, for a random subset of 10 different color-coded
    classes in the test set. MIHash yields well-separated codes with distinct structures,
    opposed to HashNet, in which the binary codes have a higher overlap.
  Figure 5 Link: articels_figures_by_rev_year\2019\Hashing_with_Mutual_Information\figure_5.jpg
  Figure 5 caption: 'We show sample retrieval results from the ImageNet100 dataset.
    Left: query images, right: top 10 retrieved images from MIHash (top row) and from
    HashNet (bottom row). Retrieved images marked with a green border belong to the
    same class as the query image, while ones marked with a red border do not belong
    to the same class as the query image.'
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Fatih Cakir
  Name of the last author: Stan Sclaroff
  Number of Figures: 5
  Number of Tables: 5
  Number of authors: 4
  Paper title: Hashing with Mutual Information
  Publication Date: 2019-05-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Results on CIFAR-10 and NUSWIDE Datasets with cifar-1 and
      nus-1 Partitionings
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results on CIFAR-10 and NUSWIDE Datasets with cifar-2 and
      nus-2 Partitionings (with VGG-F Architecture)
  Table 3 caption:
    table_text: TABLE 3 mAP1K mAP1K Values on ImageNet100 ImageNet100 Using AlexNet
  Table 4 caption:
    table_text: TABLE 4 22K LabelMe 22KLabelMe Results with GIST Features
  Table 5 caption:
    table_text: "TABLE 5 Ablation Study for the Steepness Parameter \u03B3 \u03B3\
      \ and Mini-Batch size M M on the cifar-1 Benchmark with 32 bit Codes"
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2914897
- Affiliation of the first author: department of statistical science, university college
    london, london, united kingdom
  Affiliation of the last author: department of statistical science, university college
    london, london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2019\Learning_Local_Metrics_and_Influential_Regions_for_Classification\figure_1.jpg
  Figure 1 caption: "An example of calculating the distance between two points x i\
    \ and x j . A 1 and A 2 are different influential regions with metrics M( A 1\
    \ ) and M( A 2 ) , and B is the background region with metric M(B) . The distance\
    \ between x i and x j equals to the sum of three line segments local distances,\
    \ i.e., l( x i x j \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \u2229 A\
    \ 1 ;M( A 1 )) , l( x i x j \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF\
    \ \u2229 A 2 ;M( A 2 )) and l( x i x j \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF\
    \ \xAF \xAF \u2229B;M(B)) ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Learning_Local_Metrics_and_Influential_Regions_for_Classification\figure_2.jpg
  Figure 2 caption: "An illustration of learning local influential regions. The distance\
    \ between the adjacent verticalhorizontal grids is one unit. The location and\
    \ radius of a local area could be learned and a suitable local metric could help\
    \ to enhance the separability of the data, such as increasing l( N 1 P 1 \xAF\
    \ \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF ) and l( N 2 P 3 \xAF \xAF\
    \ \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF ) while decreasing l( P 1 P 2 \xAF\
    \ \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF ) and l( P 3 P 4 \xAF \xAF\
    \ \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF ) ."
  Figure 3 Link: articels_figures_by_rev_year\2019\Learning_Local_Metrics_and_Influential_Regions_for_Classification\figure_3.jpg
  Figure 3 caption: "The positions of u,v (intersection points between line x i x\
    \ j and the influential region A ) and p,q (intersection points between line segment\
    \ x i x j \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF and A ) under different\
    \ situations. h is the middle point of line segment pq \xAF \xAF \xAF \xAF \xAF\
    \ \xAF ."
  Figure 4 Link: articels_figures_by_rev_year\2019\Learning_Local_Metrics_and_Influential_Regions_for_Classification\figure_4.jpg
  Figure 4 caption: Illustration of parameter learning using a toy data set. This
    figure is best viewed in color.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.72
  Name of the first author: Mingzhi Dong
  Name of the last author: Jing-Hao Xue
  Number of Figures: 4
  Number of Tables: 3
  Number of authors: 4
  Paper title: Learning Local Metrics and Influential Regions for Classification
  Publication Date: 2019-05-03 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Relationship between \u03BB u , \u03BB v \u03BBu,\u03BBv\
      \ and \u03BB p , \u03BB q \u03BBp,\u03BBq for Different Positions of x i x j\
      \ \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF xixj\xAF"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Partial Gradients of \u2202\u03B3 \u2202o \u2202\u03B3\u2202\
      o and \u2202\u03B3 \u2202r \u2202\u03B3\u2202r in Different Cases"
  Table 3 caption:
    table_text: 'TABLE 3 Metric Learning Algorithm Results: Mean Accuracy and Standard
      Deviation are Reported with the Best Ones in Bold; AVERAGE Denotes the Average
      Accuracy of All Data Sets; of BEST Denotes the Number of Data Sets that an Algorithm
      Performs the Best; NAN Indicates the Algorithm Cannot Return a Classification
      Result for the Data Set'
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2914899
- Affiliation of the first author: department of mathematics & statistics, boston
    university, boston, ma, usa
  Affiliation of the last author: department of mathematics, university of massachusetts,
    amherst, ma, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Matched_Filters_for_Noisy_Induced_Subgraph_Detection\figure_1.jpg
  Figure 1 caption: "Results for homogeneous Erd\u0151s-R\xE9nyi experiments."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Matched_Filters_for_Noisy_Induced_Subgraph_Detection\figure_2.jpg
  Figure 2 caption: Number of correctly matched vertices as a function of the objective
    function rank for the planted partition model described in Section 4.2. Each line
    corresponds to the density of the induced subgraph, ranging from q=0.25 to 0.5.
  Figure 3 Link: articels_figures_by_rev_year\2019\Matched_Filters_for_Noisy_Induced_Subgraph_Detection\figure_3.jpg
  Figure 3 caption: A representative set of latent positions for the random dot product
    graph model. The red circles correspond to the max-angle sampling and the points
    in blue correspond to a possible random sampling (these do not overlap for the
    purposes of visualization).
  Figure 4 Link: articels_figures_by_rev_year\2019\Matched_Filters_for_Noisy_Induced_Subgraph_Detection\figure_4.jpg
  Figure 4 caption: Random dot product graph matching results.
  Figure 5 Link: articels_figures_by_rev_year\2019\Matched_Filters_for_Noisy_Induced_Subgraph_Detection\figure_5.jpg
  Figure 5 caption: Connectomes and matching analysis of the Drosophila connectomes.
  Figure 6 Link: articels_figures_by_rev_year\2019\Matched_Filters_for_Noisy_Induced_Subgraph_Detection\figure_6.jpg
  Figure 6 caption: These panels show the accuracy for detecting the 100 K-cells in
    the left hemisphere using the K-cells subgraph for the right hemisphere. The four
    columns correspond to the 4 different centering options and each curve corresponds
    to the centering that was used for the second matching. Kernel smoothing with
    bandwidth 0.02 was used to smooth the average accuracy across the normalized objective
    function ranks (for the first match). The two rows correspond to using 0 and 10
    seeds.
  Figure 7 Link: articels_figures_by_rev_year\2019\Matched_Filters_for_Noisy_Induced_Subgraph_Detection\figure_7.jpg
  Figure 7 caption: Example matches from human DTMRI graphs.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Daniel L. Sussman
  Name of the last author: Vince Lyzinski
  Number of Figures: 7
  Number of Tables: 3
  Number of authors: 4
  Paper title: Matched Filters for Noisy Induced Subgraph Detection
  Publication Date: 2019-05-03 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Runtimes in Seconds per Restart for Each Setting in Terms\
      \ of the Number of Seeds and the Correlation \u03C1 \u03C1 in the Homogeneous\
      \ Erd\u0151s-R\xE9nyi Setting"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Number of K-Cells in the Left Hemishpere Detected by the
      Match with the Best Objective Function after the First Match
  Table 3 caption:
    table_text: TABLE 3 Performance Comparisons for the Large Scale DTMRI Examples
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2914651
- Affiliation of the first author: fiveai ltd, oxford, united kingdom
  Affiliation of the last author: university of oxford, oxford, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2019\RealTime_RGBD_Camera_Pose_Estimation_in_Novel_Scenes_Using_a_Relocalisation_Casc\figure_1.jpg
  Figure 1 caption: Overview of our approach (without the cascade). First, we train
    a regression forest offline to predict 2D-to-3D correspondences for a generic
    scene. To adapt this forest to a new scene, we remove the scene-specific information
    in the forests leaves while retaining the branching structure (with learned split
    parameters) of the trees; we then refill the leaves online using training examples
    from the new scene. The adapted forest can be deployed to predict correspondences
    for the new scene, triples of which are then fed to the Kabsch [20] algorithm
    to generate a large number of pose hypotheses. We then reduce these hypotheses
    to a much smaller number of refined hypotheses using RANSAC [21] (in this paper,
    we modify our RANSAC module from [37] to return multiple hypotheses rather than
    just a single one). Finally, we score and rank the final few hypotheses using
    a model-based approach, yielding a single resulting output pose.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\RealTime_RGBD_Camera_Pose_Estimation_in_Novel_Scenes_Using_a_Relocalisation_Casc\figure_2.jpg
  Figure 2 caption: 'An example of the effect that online adaptation has on a pre-trained
    forest: (Left) the modal clusters present in a small number of randomly selected
    leaves of a forest pre-trained on the Chess scene from the 7-Scenes dataset [25]
    (the colour of each mode indicates its containing leaf); (Right) the modal clusters
    that are added to the same leaves during the process of adapting the forest to
    the Kitchen scene.'
  Figure 3 Link: articels_figures_by_rev_year\2019\RealTime_RGBD_Camera_Pose_Estimation_in_Novel_Scenes_Using_a_Relocalisation_Casc\figure_3.jpg
  Figure 3 caption: Our model-based approach to ranking the camera pose hypotheses
    that survive the RANSAC stage (see Section 3.2.4). For each hypothesis, we first
    refine the pose by performing ICP [67] with respect to the 3D scene model. If
    this succeeds, we score the hypothesis by comparing a synthetic depth raycast
    from the refined pose to the live depth image from the camera. Once all hypotheses
    have been scored, we rank them and return the one with the lowest score.
  Figure 4 Link: articels_figures_by_rev_year\2019\RealTime_RGBD_Camera_Pose_Estimation_in_Novel_Scenes_Using_a_Relocalisation_Casc\figure_4.jpg
  Figure 4 caption: 'Our relocalisation cascade (see Section 3.2.5): we instantiate
    multiple instances of our relocaliser, backed by the same regression forest, but
    with different hypothesis generation and RANSAC parameters, and run them one at
    a time on the camera input until one of them produces an acceptable pose (or we
    reach the end of the cascade). The idea is to gradually fall back from fast but
    less effective relocalisers to slower but more effective ones, with the aim of
    yielding an effective overall relocaliser that is fast on average.'
  Figure 5 Link: articels_figures_by_rev_year\2019\RealTime_RGBD_Camera_Pose_Estimation_in_Novel_Scenes_Using_a_Relocalisation_Casc\figure_5.jpg
  Figure 5 caption: Our approachs performance for tracking loss recovery (Section
    4.2). Filling the leaves of a forest pre-trained on Office frame-by-frame directly
    from the testing sequence, we are able to start relocalising almost immediately
    in new scenes.
  Figure 6 Link: articels_figures_by_rev_year\2019\RealTime_RGBD_Camera_Pose_Estimation_in_Novel_Scenes_Using_a_Relocalisation_Casc\figure_6.jpg
  Figure 6 caption: Evaluating how well our approach generalises to novel poses in
    comparison to a keyframe-based random fern relocaliser based on [17]. The performance
    decay experienced as test poses get further from the training trajectory is much
    less severe with our approach than with ferns.
  Figure 7 Link: articels_figures_by_rev_year\2019\RealTime_RGBD_Camera_Pose_Estimation_in_Novel_Scenes_Using_a_Relocalisation_Casc\figure_7.jpg
  Figure 7 caption: "Novel poses from which we are able to relocalise to within 5\
    \ cm5 \u2218 on the Fire sequence from 7-Scenes [25]. Pose novelty measures the\
    \ distance of a test pose from a nearby pose (blue) on the training trajectory\
    \ (yellow). We can relocalise from both easy poses (up to 35 cm35 \u2218 from\
    \ the training trajectory, green) and hard poses ( > 35 cm35 \u2218 , red). The\
    \ images below the main figure show views of the scene from the training poses\
    \ and testing poses indicated."
  Figure 8 Link: articels_figures_by_rev_year\2019\RealTime_RGBD_Camera_Pose_Estimation_in_Novel_Scenes_Using_a_Relocalisation_Casc\figure_8.jpg
  Figure 8 caption: 'Visualising the leaves (from the first tree) and world-space
    points that the forest predicts for three images of Fire from 7-Scenes [25]. Top-to-bottom:
    input images, predicted leaves (randomly colourised), predicted world-space points
    (mapped to an RGB cube), ground truth world-space points.'
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Tommaso Cavallari
  Name of the last author: Philip H. S. Torr
  Number of Figures: 8
  Number of Tables: 2
  Number of authors: 7
  Paper title: Real-Time RGB-D Camera Pose Estimation in Novel Scenes Using a Relocalisation
    Cascade
  Publication Date: 2019-05-06 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Comparing Our Adaptive Approach to State-of-the-Art Offline\
      \ Methods on the 7-Scenes Dataset [25] (the Percentages Denote Proportions of\
      \ Test Frames with \u22645 \u22645 cm Translation Error and \u2264 5 \u2218\
      \ \u22645\u2218 Angular Error; Red and Blue Colours Denote Respectively the\
      \ Best and Second-Best Results in Each Column)"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparing Our Adaptive Approach to State-of-the-Art Offline
      Methods on the Stanford 4 Scenes Dataset [23]
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2915068
- Affiliation of the first author: tsinghua-berkeley shenzhen institute, tsinghua
    university, beijing, china
  Affiliation of the last author: tsinghua-berkeley shenzhen institute, tsinghua university,
    beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2019\UnstructuredFusion_Realtime_D_Geometry_and_Texture_Reconstruction_Using_Commerci\figure_1.jpg
  Figure 1 caption: Illustration of the system setup and the real-time reconstruction
    results of our UnstructuredFusion. The red circles indicate the unstructured sparse
    multi-view RGBD cameras.
  Figure 10 Link: articels_figures_by_rev_year\2019\UnstructuredFusion_Realtime_D_Geometry_and_Texture_Reconstruction_Using_Commerci\figure_10.jpg
  Figure 10 caption: Evaluation of atlas blending in terms of the number of input
    views. (a) The textured and relighting results with atlas blending using the single-view
    sequence from DoubleFusion.[6] (b) The results using the three-views sequence
    captured by UnstructuredFusion.
  Figure 2 Link: articels_figures_by_rev_year\2019\UnstructuredFusion_Realtime_D_Geometry_and_Texture_Reconstruction_Using_Commerci\figure_2.jpg
  Figure 2 caption: 'The system pipeline of UnstructuredFusion. We first initialize
    our system at the first frame by performing the online multi-camera calibration
    (Section 4.2). Then for each frame, we sequentially perform the next 3 steps:
    skeleton warping based non-rigid tracking (Section 4.3), geometric fusion and
    atlas texturing based on temporal blending (Section 4.4). Finally, live textured
    meshes with geometry details are obtained.'
  Figure 3 Link: articels_figures_by_rev_year\2019\UnstructuredFusion_Realtime_D_Geometry_and_Texture_Reconstruction_Using_Commerci\figure_3.jpg
  Figure 3 caption: 'Illustration of the online calibration. (a,b,c) are the depth
    inputs with the initial camera poses, the camera poses after solving Eqn. (7),
    and the one after solving Eqn. (4). (d) The final output of online calibration.
    From left to right: the aligned color frames, the aligned depth frames and the
    embedded SMPL model.'
  Figure 4 Link: articels_figures_by_rev_year\2019\UnstructuredFusion_Realtime_D_Geometry_and_Texture_Reconstruction_Using_Commerci\figure_4.jpg
  Figure 4 caption: Illustration of our projective atlas scheme. (a) The input RGBD
    image examples. (b) The corresponding partial atlas a i . (c) The global blended
    atlas A i . (d) The textured mesh output in the live frame.
  Figure 5 Link: articels_figures_by_rev_year\2019\UnstructuredFusion_Realtime_D_Geometry_and_Texture_Reconstruction_Using_Commerci\figure_5.jpg
  Figure 5 caption: Our atlas texturing scheme (right) compared to per-vertex color
    scheme (left). Per-vertex colors suffer from block artifacts while our method
    provides sharper textures with more dynamic facial expression.
  Figure 6 Link: articels_figures_by_rev_year\2019\UnstructuredFusion_Realtime_D_Geometry_and_Texture_Reconstruction_Using_Commerci\figure_6.jpg
  Figure 6 caption: Several examples that demonstrate the quality and fidelity of
    the reconstructed 4D geometry and texture results of the proposed UnstructuredFusion
    system.
  Figure 7 Link: articels_figures_by_rev_year\2019\UnstructuredFusion_Realtime_D_Geometry_and_Texture_Reconstruction_Using_Commerci\figure_7.jpg
  Figure 7 caption: Evaluation of skeleton warping. (a) Geometric and textured results
    without skeleton warping. (b) The corresponding results with skeleton warping.
  Figure 8 Link: articels_figures_by_rev_year\2019\UnstructuredFusion_Realtime_D_Geometry_and_Texture_Reconstruction_Using_Commerci\figure_8.jpg
  Figure 8 caption: 'Evaluation of our method using moving cameras. (a, b, c) The
    three example sequences in which the three cameras move in roughly fixed positions,
    move forward in a roughly straight line and circle around the captured target,
    respectively. From left to right: the captured scene; the reconstruction results
    without skeleton-warping and our results.'
  Figure 9 Link: articels_figures_by_rev_year\2019\UnstructuredFusion_Realtime_D_Geometry_and_Texture_Reconstruction_Using_Commerci\figure_9.jpg
  Figure 9 caption: Evaluation of the atlas blending. (a) Input depth and color image.
    (b) The reconstructed result of our atlas texturing with grid-based warping, where
    the blue map indicates the color-coded residual compared with the input color
    image. (c) The reconstructed result of our atlas texturing without grid-based
    warping. (d) The reconstructed result using per-vertex scheme. (e) The corresponding
    quantitative error curves.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Lan Xu
  Name of the last author: Lu Fang
  Number of Figures: 15
  Number of Tables: 2
  Number of authors: 6
  Paper title: 'UnstructuredFusion: Realtime 4D Geometry and Texture Reconstruction
    Using Commercial RGBD Cameras'
  Publication Date: 2019-05-07 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Average Projective Numerical Errors of All the Captured Sequences
      for the Concerned Methods: DoubleFusion, [6] Multi-DoubleFusion [6] and Our
      UnstructuredFusion'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Average and Maximal Numerical Errors on the Entire Sequence
      Compared to the Ground Truth Observation from the OptiTrack System, for These
      Three Methods: DoubleFusion, [6] Multi-DoubleFusion and Our Method, Respectively'
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2915229
- Affiliation of the first author: school of data and computer science, sun yat-sen
    university, guangzhou, china
  Affiliation of the last author: school of data and computer science, sun yat-sen
    university, guangzhou, china
  Figure 1 Link: articels_figures_by_rev_year\2019\Face_Hallucination_by_Attentive_Sequence_Optimization_with_Reinforcement_Learnin\figure_1.jpg
  Figure 1 caption: 'Pipeline of Attention-FH. The proposed framework contains two
    modules: the policy network and the local enhancement network. In each step, the
    attention agent glimpses the whole image and provides actions. Each action indicates
    the center position of the next attended rectangular region and the size of the
    bounding box. The attended patch is further fed as input to a local enhancement
    network for super-resolution. We use the red solid bounding boxes and the blue
    dashed bounding boxes to indicate the attended patch and enhanced patch, respectively.
    A global reward is given at the end of the sequence to enforce the policy network
    to learn the optimal restoring route. With the two components, we can perform
    a coarse-to-fine face hallucination paradigm.'
  Figure 10 Link: articels_figures_by_rev_year\2019\Face_Hallucination_by_Attentive_Sequence_Optimization_with_Reinforcement_Learnin\figure_10.jpg
  Figure 10 caption: Qualitative comparisons on the SCface [40] dataset with a scaling
    factor of 8. The testing faces are captured by surveillance cameras, similarly
    to practical scenarios. This figure is best viewed by zooming in on the electronic
    version.
  Figure 2 Link: articels_figures_by_rev_year\2019\Face_Hallucination_by_Attentive_Sequence_Optimization_with_Reinforcement_Learnin\figure_2.jpg
  Figure 2 caption: "Network architecture of our Attention-FH. At each time step t\
    \ , the recurrent policy network outputs the actions by observing the original\
    \ corrupted face image I 0 , the current enhanced face image I t\u22121 and the\
    \ historical actions v l . Meanwhile, v l is encoded by latent variables (e.g.,\
    \ 64 hidden states) of GRU. I 0 and I t\u22121 are first represented as high-level\
    \ features from an output feature vector of a fully connected layer and are then\
    \ concatenated to form a vector of 128 dimensions. The above three corresponding\
    \ pieces of information constitute the state s . The GRU layer learns to infer\
    \ the action probabilities by considering s . The output probabilities are formulated\
    \ by a fully connected linear layer (128 neurons) for all candidate actions. Given\
    \ the actions, we can obtain the local patch I l t t\u22121 . Then, I l t t\u2212\
    1 is sent to the local enhancement network for hallucinating, which results in\
    \ the enhanced patch I l t t . Finally, a new hallucinated face is generated by\
    \ replacing I l t t\u22121 with I l t t ."
  Figure 3 Link: articels_figures_by_rev_year\2019\Face_Hallucination_by_Attentive_Sequence_Optimization_with_Reinforcement_Learnin\figure_3.jpg
  Figure 3 caption: Qualitative comparison on the PubFig [38] dataset with a scaling
    factor of 8. The image is best viewed by zooming in on the electronic version.
  Figure 4 Link: articels_figures_by_rev_year\2019\Face_Hallucination_by_Attentive_Sequence_Optimization_with_Reinforcement_Learnin\figure_4.jpg
  Figure 4 caption: Qualitative comparison on the PubFig [38] dataset with a scaling
    factor of 4. The image is best viewed by zooming in on the electronic version.
  Figure 5 Link: articels_figures_by_rev_year\2019\Face_Hallucination_by_Attentive_Sequence_Optimization_with_Reinforcement_Learnin\figure_5.jpg
  Figure 5 caption: Qualitative comparison on the PubFig [38] dataset with a scaling
    factor of 16. The image is best viewed by zooming in on the electronic version.
  Figure 6 Link: articels_figures_by_rev_year\2019\Face_Hallucination_by_Attentive_Sequence_Optimization_with_Reinforcement_Learnin\figure_6.jpg
  Figure 6 caption: PSNR comparison on the variants of our Attention-FH using different
    numbers of steps for sequentially enhancing facial parts on the LFW dataset. T=18
    achieves a balance between accuracy and efficiency.
  Figure 7 Link: articels_figures_by_rev_year\2019\Face_Hallucination_by_Attentive_Sequence_Optimization_with_Reinforcement_Learnin\figure_7.jpg
  Figure 7 caption: We enhance the reward function by improving its stability. We
    redefine the previous reward function from our preliminary conference version
    by means of a more accurate baseline and achieve superior performance.
  Figure 8 Link: articels_figures_by_rev_year\2019\Face_Hallucination_by_Attentive_Sequence_Optimization_with_Reinforcement_Learnin\figure_8.jpg
  Figure 8 caption: A demonstration of size-free attention. As the facial part has
    different sizes with particular identities, our method locates the region with
    a flexible attention mechanism.
  Figure 9 Link: articels_figures_by_rev_year\2019\Face_Hallucination_by_Attentive_Sequence_Optimization_with_Reinforcement_Learnin\figure_9.jpg
  Figure 9 caption: Detailed training procedure of our Attention-FH. The local enhancement
    network is optimized via the L 2 loss between the restored patch and the corresponding
    HR ground truth. After performing the last step, we calculate the global reward
    and pass the policy gradient to optimize the recurrent policy network.
  First author gender probability: 0.95
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Yukai Shi
  Name of the last author: Liang Lin
  Number of Figures: 11
  Number of Tables: 11
  Number of authors: 5
  Paper title: Face Hallucination by Attentive Sequence Optimization with Reinforcement
    Learning
  Publication Date: 2019-05-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Detailed Setting of Each Component in the Feature Extractor
  Table 10 caption:
    table_text: TABLE 10 Experimental Study of the Trade-off between Efficiency and
      Accuracy on Different Local Enhancement Network Architectures
  Table 2 caption:
    table_text: TABLE 2 Detailed Setting of Each Component in the Local Enhancement
      Network
  Table 3 caption:
    table_text: TABLE 3 Quantitative Comparison of Our Model and the Competing Methods
      in Terms of the PSNR Index
  Table 4 caption:
    table_text: TABLE 4 Quantitative Comparison of Our Model and the Competing Methods
      in Terms of the SSIM Index
  Table 5 caption:
    table_text: TABLE 5 Quantitative Comparison of Our Model and the Competing Methods
      in Terms of the FSIM Index
  Table 6 caption:
    table_text: TABLE 6 Comparison on Multi-PIE [42] and Extended Yale-B [43]
  Table 7 caption:
    table_text: TABLE 7 Comparison of Our Proposed Model with Deeper Inferences
  Table 8 caption:
    table_text: TABLE 8 Comparison on General Image Super-Resolution
  Table 9 caption:
    table_text: TABLE 9 Comparison of Our Proposed Model Under Different Settings
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2915301
- Affiliation of the first author: robotics institute, carnegie mellon university,
    pittsburgh, pa, usa
  Affiliation of the last author: department of information engineering, the chinese
    university of hong kong, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2019\Deep_Imbalanced_Learning_for_Face_Recognition_and_Attribute_Prediction\figure_1.jpg
  Figure 1 caption: "Example of class imbalance for the binary face attribute \u201C\
    wear hat\u201D. Our method aims to separate the cluster distributions both within\
    \ and between classes. This effectively reduces the class imbalance in local neighborhoods\
    \ and forms balanced local class boundaries that are insensitive to the imbalanced\
    \ size of remaining class samples."
  Figure 10 Link: articels_figures_by_rev_year\2019\Deep_Imbalanced_Learning_for_Face_Recognition_and_Attribute_Prediction\figure_10.jpg
  Figure 10 caption: Most imbalanced face attributes (from Table 5) that are correctly
    predicted by our method.
  Figure 2 Link: articels_figures_by_rev_year\2019\Deep_Imbalanced_Learning_for_Face_Recognition_and_Attribute_Prediction\figure_2.jpg
  Figure 2 caption: The 2-D feature space of triplet loss [33], A-Softmax loss [39],
    Large Margin Local Embedding (LMLE) [32] and the proposed Cluster-based Large
    Margin Local Embedding (CLMLE). Class imbalance is exemplified in a binary-class
    case. The triplet and A-Softmax losses enforce euclidean and angular margins respectively
    at class-level, assuming each class can be captured by a single mode. Such unimodal
    discrimination imposes too strong of a requirement, and may fail to collapse the
    majority class with larger variation and lead to class overlap. The LMLE enforces
    euclidean margins among quintuple examples sampled from the intra- and inter-class
    local clusters. Such constraint preserves discrimination in local neighborhood
    and helps form local class boundaries that are insensitive to the imbalanced class
    size. The proposed CLMLE samples the entire cluster distributions instead to address
    the inefficiency and inconsistency issues with quintuplet sampling in LMLE. CLMLE
    also naturally facilitates the derivation of angular margins between cluster distributions
    on the unit hypersphere.
  Figure 3 Link: articels_figures_by_rev_year\2019\Deep_Imbalanced_Learning_for_Face_Recognition_and_Attribute_Prediction\figure_3.jpg
  Figure 3 caption: Speed and performance analyses in face recognition (on LFW [1])
    and face attribute (on CelebA [53]) tasks. We compare convergence speed by the
    number of seen training data, which is fair and independent of differences in
    GPU, batch size used and other factors. We also report training times for the
    face attribute task. The compared methods are triplet loss [33], Center loss [44],
    A-Softmax [39], LMCL [40], Large Margin Local Embedding (LMLE) [32] and our Cluster-based
    Large Margin Local Embedding (CLMLE). CLMLE strikes a good performance-speed trade-off.
  Figure 4 Link: articels_figures_by_rev_year\2019\Deep_Imbalanced_Learning_for_Face_Recognition_and_Attribute_Prediction\figure_4.jpg
  Figure 4 caption: Extreme 2D cases for deriving the upper bounds of inter-cluster
    angular margins between class ( a max 1 ) and within class ( a max 2 ).
  Figure 5 Link: articels_figures_by_rev_year\2019\Deep_Imbalanced_Learning_for_Face_Recognition_and_Attribute_Prediction\figure_5.jpg
  Figure 5 caption: The 2-D feature space using t-SNE [59] and pairwise feature similarity
    for one binary face attribute from the CelebA dataset [53]. We only show 2 Positive
    Clusters (PC) and 5 Negative Clusters (NC) to represent the class imbalance. The
    embedding of a pre-trained model, our CLMLE, and triplet embedding are compared.
    We can see that between-class clusters (with different colors) are well separated
    in CLMLE, but they are overlapped in triplet embedding, leading to overlapping
    binary score distributions.
  Figure 6 Link: articels_figures_by_rev_year\2019\Deep_Imbalanced_Learning_for_Face_Recognition_and_Attribute_Prediction\figure_6.jpg
  Figure 6 caption: Imbalanced data distribution for face recognition and face attribute
    prediction. (a) Long-tailed distribution of image number per class on CASIA-WebFace
    dataset [43]. (b) 40 binary face attributes on CelebA dataset [53], each with
    imbalanced positive and negative samples.
  Figure 7 Link: articels_figures_by_rev_year\2019\Deep_Imbalanced_Learning_for_Face_Recognition_and_Attribute_Prediction\figure_7.jpg
  Figure 7 caption: CMC and ROC curves of top performing methods with 1M distractors
    on MegaFace Challenge 1 [2]. Note the FaceNet v8 follows the large training set
    protocol, while other methods follow the small one.
  Figure 8 Link: articels_figures_by_rev_year\2019\Deep_Imbalanced_Learning_for_Face_Recognition_and_Attribute_Prediction\figure_8.jpg
  Figure 8 caption: 'Challenging pairs (green: positive pair; red: negative pair)
    that are correctly recognized by our method.'
  Figure 9 Link: articels_figures_by_rev_year\2019\Deep_Imbalanced_Learning_for_Face_Recognition_and_Attribute_Prediction\figure_9.jpg
  Figure 9 caption: Relative gains over the state-of-the-art methods without imbalance
    handling mechanism across the 40 imbalanced attributes on CelebA [53].
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.93
  Name of the first author: Chen Huang
  Name of the last author: Xiaoou Tang
  Number of Figures: 10
  Number of Tables: 8
  Number of authors: 4
  Paper title: Deep Imbalanced Learning for Face Recognition and Attribute Prediction
  Publication Date: 2019-05-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The CNN Architecture and Prior Features for Initial Clustering
      in Our Considered Imbalanced Tasks
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Face Verification Accuracy (%) on LFW [1] and YTF [64] Datasets
  Table 3 caption:
    table_text: TABLE 3 Face Recognition on MegaFace Challenge 1 [2] Under the Protocols
      of Small and Large Training Set
  Table 4 caption:
    table_text: TABLE 4 Face Recognition on MegaFace Challenge 2 [68] Under the Large
      Training Set Protocol
  Table 5 caption:
    table_text: TABLE 5 Mean Per-Class Accuracy (%) and Class Imbalance Level ( =|
      =| positive class rate-50 | |%) of Each of the 40 Binary Attributes on CelebA
      Dataset [53]
  Table 6 caption:
    table_text: TABLE 6 Average Balanced Accuracy and Classification Accuracy (%)
      for the 40 Binary Attributes on CelebA Dataset [53]
  Table 7 caption:
    table_text: TABLE 7 Ablation Study in Terms of the Face Verification Accuracy
      (%) on LFW and Average Balanced Accuracy (%) for Attribute Prediction on CelebA
  Table 8 caption:
    table_text: "TABLE 8 Mean Class-Balanced Accuracy (%) on CIFAR-100 [71] and MNIST-Rot-Back-Image\
      \ [72] Datasets with Simulated Class Imbalance (with Parameter \u03B3 \u03B3\
      \ - the Smaller, the More Imbalanced)"
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2914680
