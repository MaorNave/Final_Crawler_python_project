- Affiliation of the first author: department of computer science, city university
    of hong kong, hong kong
  Affiliation of the last author: department of computer science, city university
    of hong kong, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2014\A_Scalable_and_Accurate_Descriptor_for_Dynamic_Textures_Using_Bag_of_System_Tree\figure_1.jpg
  Figure 1 caption: (a) A BoS Tree is built from a collection of videos cal Xc by
    forming a hierarchy of codewords. (b) The tree structure of the BoS Tree enables
    efficient indexing of codewords.
  Figure 10 Link: articels_figures_by_rev_year\2014\A_Scalable_and_Accurate_Descriptor_for_Dynamic_Textures_Using_Bag_of_System_Tree\figure_10.jpg
  Figure 10 caption: Percentage of patches that are assigned to different codewords
    in BoS and BoST histograms for UCLA-39. Videos are sorted by increasing assignment
    of patches.
  Figure 2 Link: articels_figures_by_rev_year\2014\A_Scalable_and_Accurate_Descriptor_for_Dynamic_Textures_Using_Bag_of_System_Tree\figure_2.jpg
  Figure 2 caption: 'Comparison of SML-DTM framework and BoS Tree framework: (a) Learning
    DT annotation models using HEM-DTM algorithm. (b) Building a BoS Tree with three
    levels.'
  Figure 3 Link: articels_figures_by_rev_year\2014\A_Scalable_and_Accurate_Descriptor_for_Dynamic_Textures_Using_Bag_of_System_Tree\figure_3.jpg
  Figure 3 caption: (a) Average precisionrecall plot; (b) F-measure plot, showing
    all annotation levels, using BoSTree and SML-DTM on DynTex.
  Figure 4 Link: articels_figures_by_rev_year\2014\A_Scalable_and_Accurate_Descriptor_for_Dynamic_Textures_Using_Bag_of_System_Tree\figure_4.jpg
  Figure 4 caption: Examples from YUPENN dynamic scenes data set.
  Figure 5 Link: articels_figures_by_rev_year\2014\A_Scalable_and_Accurate_Descriptor_for_Dynamic_Textures_Using_Bag_of_System_Tree\figure_5.jpg
  Figure 5 caption: Speedaccuracy tradeoff for BoS and BoS Trees.
  Figure 6 Link: articels_figures_by_rev_year\2014\A_Scalable_and_Accurate_Descriptor_for_Dynamic_Textures_Using_Bag_of_System_Tree\figure_6.jpg
  Figure 6 caption: Intersection distance matrix between (a) BoS histograms, (b) BoST
    histograms, (c) BoS and BoST histograms, for all videos in UCLA-39. Videos are
    grouped by class, delineated by black lines.
  Figure 7 Link: articels_figures_by_rev_year\2014\A_Scalable_and_Accurate_Descriptor_for_Dynamic_Textures_Using_Bag_of_System_Tree\figure_7.jpg
  Figure 7 caption: Scatter plots between the distance matrices from Fig. 6.
  Figure 8 Link: articels_figures_by_rev_year\2014\A_Scalable_and_Accurate_Descriptor_for_Dynamic_Textures_Using_Bag_of_System_Tree\figure_8.jpg
  Figure 8 caption: Average set intersection between top-K nearest neighbors using
    BoS or BoST histograms, for all videos in UCLA39.
  Figure 9 Link: articels_figures_by_rev_year\2014\A_Scalable_and_Accurate_Descriptor_for_Dynamic_Textures_Using_Bag_of_System_Tree\figure_9.jpg
  Figure 9 caption: Distances between BoS and BoST histograms for each video in UCLA-39.
    Videos are sorted according to the distance. The horizontal line is the average
    distance.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Adeel Mumtaz
  Name of the last author: Antoni B. Chan
  Number of Figures: 11
  Number of Tables: 7
  Number of authors: 4
  Paper title: A Scalable and Accurate Descriptor for Dynamic Textures Using Bag of
    System Trees
  Publication Date: 2014-09-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Annotation Results on the DynTex Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Per-tag Performance of BoSTree and SML-DTM Annotation on DynTex
  Table 3 caption:
    table_text: TABLE 3 Distances and Kernels Used for Classification
  Table 4 caption:
    table_text: TABLE 4 Comparison of BoS Tree Classification Rates with TSVQ [8]
      and Various Spatial and Temporal Methods Reported in [24]
  Table 5 caption:
    table_text: "TABLE 5 Confusion Matrix for YUPENN Dataset using (a) a Three-Level\
      \ BoS Tree and (b) SOE ( 4\xD74\xD71 )"
  Table 6 caption:
    table_text: TABLE 6 Video Classification Results on UCLA-8, UCLA-39 and DynTex-35
      Using a Large Codebook, Reduced Codebooks, and BoS Trees
  Table 7 caption:
    table_text: TABLE 7 Music Annotation Results on CAL500, Using a Large Codebook,
      Reduced Codebooks, and BoS Trees
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2359432
- Affiliation of the first author: department of statistics, university of california
    at los angeles, los angeles, ca
  Affiliation of the last author: department of statistics and computer science, university
    of california at los angeles, los angeles, ca
  Figure 1 Link: articels_figures_by_rev_year\2014\Learning_NearOptimal_CostSensitive_Decision_Policy_for_Object_Detection\figure_1.jpg
  Figure 1 caption: Illustration of two decision policies learned for a human face
    AdaBoost classifier (The legend is only shown in the right plot for clarity).
    The horizontal axis is the 1,146 boosted weak classifiers divided in 20 stages,
    and the vertical axis is the cumulative scores. At each stage the solid histograms
    show the original positive (red) and negative (green) populations in training
    dataset, and the dotted histograms show the changed populations after early decisions
    by the policy.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\Learning_NearOptimal_CostSensitive_Decision_Policy_for_Object_Detection\figure_2.jpg
  Figure 2 caption: Graph interpretation of minimizing the empirical risk. The two-sided
    thresholds at a stage i generate the three numbers ( mboxFNi , mboxFPi , Si ).
    All the thresholds are fully dependent in the sense that the positive and negative
    sub-populations observed at a stage i are affected by the thresholds at all previous
    stages.
  Figure 3 Link: articels_figures_by_rev_year\2014\Learning_NearOptimal_CostSensitive_Decision_Policy_for_Object_Detection\figure_3.jpg
  Figure 3 caption: Twenty trajectories of positive (red dashed lines) and negative
    (blue) training examples in the Ktimes N matrix. The examples are from the human
    face AdaBoost classifier. between successive steps in the trajectories.
  Figure 4 Link: articels_figures_by_rev_year\2014\Learning_NearOptimal_CostSensitive_Decision_Policy_for_Object_Detection\figure_4.jpg
  Figure 4 caption: Illustrating the notations of the four types of statistics projected
    from trajectories of all training examples (only D+ is used for clarity). Note
    that notations like underline65 and overline75 are used for this specific illustration.
  Figure 5 Link: articels_figures_by_rev_year\2014\Learning_NearOptimal_CostSensitive_Decision_Policy_for_Object_Detection\figure_5.jpg
  Figure 5 caption: Illustration of the pixel-based computational costs compared between
    our policy (b), the hard cascade (c), and the soft cascade (d) on a testing image
    in MIT+CMU face detection dataset. All the three methods can detect all faces
    in the testing image (a). (e) and (f) show the computing cost difference between
    our policy and the hard cascade and the soft cascade respectively. To record the
    work load at each image pixel location, starting from a zero image, every sliding
    window tested by the detector contributes an amount of intensity proportional
    the number of Haar features (as indicated by the color bar on the right of sub-figures
    from (b) to (f)) applied to that window. For the smallest detection windows (i.e.,
    the windows in the bottom of the testing image pyramid, i.e., the original image),
    this intensity is concentrated in a single pixel at the center of the window;
    for larger windows (i.e., windows in the higher levels of the testing pyramid),
    it is spread out over a proportionally larger area.
  Figure 6 Link: articels_figures_by_rev_year\2014\Learning_NearOptimal_CostSensitive_Decision_Policy_for_Object_Detection\figure_6.jpg
  Figure 6 caption: Comparisons of the stage computing costs between our policy (top)
    and the hard cascade (bottom) on human face classification testing dataset (red
    bars for positives and blue ones for negatives where for illustration we use the
    minus computing costs of negatives in the plots). The stage computing cost of
    the i th stage is computed as the product of the number of examples exited (i.e.,
    Si in Eq. (17)) and the cumulative computational cost of the current stage (i.e.,
    sum j=1i c(Gj) in Eqn. (14)).
  Figure 7 Link: articels_figures_by_rev_year\2014\Learning_NearOptimal_CostSensitive_Decision_Policy_for_Object_Detection\figure_7.jpg
  Figure 7 caption: 'Efficiency of the policy (red curves) over the population ratios
    compared with cascade (blue curves). Left: The computing costs per example over
    different population ratios. Right: The comparisons on FPR by mean(horizontal
    curve)-std (vertical bar) plots.'
  Figure 8 Link: articels_figures_by_rev_year\2014\Learning_NearOptimal_CostSensitive_Decision_Policy_for_Object_Detection\figure_8.jpg
  Figure 8 caption: Comparison of robustness to outliers between the naive decision
    policy and the learned decision policy of human face AdaBoost classifier on the
    classification testing data. Blue curves are for the baseline decision policy
    and red and magenta curves are for two of our learned decision policies (which
    prefer relatively smaller FPR and FNR respectively). The naive policy is more
    sensitive to the outliers in terms of the computational efficiency.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.75
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.61
  Name of the first author: Tianfu Wu
  Name of the last author: Song-Chun Zhu
  Number of Figures: 8
  Number of Tables: 3
  Number of authors: 2
  Paper title: Learning Near-Optimal Cost-Sensitive Decision Policy for Object Detection
  Publication Date: 2014-09-22 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Comparison between the Cascade and Our Decision Policy (\
      \ \u03A0 ) on Human Face AdaBoost Classifier"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Comparison between PAA [27] and Our Policy \u03A0 of the\
      \ SVM Classifier on the INRIA Person Dataset [3]"
  Table 3 caption:
    table_text: "TABLE 3 Comparison between PAA [27], and Our Policy \u03A0 of the\
      \ DPMs Trained and Tested on PASCAL VOC2007 Dataset [41]"
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2359653
- Affiliation of the first author: department of statistical science, university college
    london, london, wc1e 6bt, united kingdom
  Affiliation of the last author: department of mathematics and statistics, the university
    of melbourne, vic, australia
  Figure 1 Link: articels_figures_by_rev_year\2014\Why_Does_Rebalancing_ClassUnbalanced_Data_Improve_AUC_for_Linear_Discriminant_An\figure_1.jpg
  Figure 1 caption: 'AUC for data (with n=1,000 ) arising from two Gaussian classes
    with unequal covariance matrices. Left: Scatter plot of AUC for LDA trained with
    fully rebalanced data (i.e. hatpi 1=50% ) vs. AUC for LDA trained with original
    unbalanced data (with hatpi 1=20% ). Right: Boxplots of AUC for LDA trained with
    the original unbalanced data and rebalanced data, in which, from left to right,
    the leftmost boxplot corresponds to the original data, the next four boxplots
    to the rebalancing scenarios ''O30''-''O60 percent'' and the rightmost four boxplots
    to ''U30''-''U60 percent'', where ''O50 percent'' (''U50 percent'') denotes class
    rebalancing through random oversampling (undersampling) the minority (majority)
    class such that hatpi 1=50% , for example. The dashed horizontal line across the
    boxplots indicates the median AUC for the original data.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\Why_Does_Rebalancing_ClassUnbalanced_Data_Improve_AUC_for_Linear_Discriminant_An\figure_2.jpg
  Figure 2 caption: AUC for data arising from two Gaussian classes with equal covariance
    matrices. The rest of the caption is as in Fig. 1.
  Figure 3 Link: articels_figures_by_rev_year\2014\Why_Does_Rebalancing_ClassUnbalanced_Data_Improve_AUC_for_Linear_Discriminant_An\figure_3.jpg
  Figure 3 caption: 'Left: AUC for data with n=50 . The setting and rest of the caption
    are as in Fig. 1. Right: The percentage, for which full rebalancing (i.e. ''O50
    percent'' or ''U50 percent'') achieves the largest improvement in our experiments,
    versus the training sample size n , with n=125, 500, 2,000 and 8,000 .'
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.72
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jing-Hao Xue
  Name of the last author: Peter Hall
  Number of Figures: 3
  Number of Tables: 0
  Number of authors: 2
  Paper title: Why Does Rebalancing Class-Unbalanced Data Improve AUC for Linear Discriminant
    Analysis?
  Publication Date: 2014-09-22 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2359660
- Affiliation of the first author: sysu-cmu shunde international joint research institute,
    shunde, china
  Affiliation of the last author: sysu-cmu shunde international joint research institute,
    shunde, china
  Figure 1 Link: articels_figures_by_rev_year\2014\Discriminatively_Trained_AndOr_Graph_Models_for_Object_Shape_Detection\figure_1.jpg
  Figure 1 caption: 'An example of our And-Or graph model. It comprises four layers
    from bottom to top: the leaf-nodes (denoted by the solid circles) at the bottom
    for localizing local contour fragments, the or-nodes (denoted by the dashed blue
    circles) over the bottom specifying the activations of their child leaf-nodes,
    the and-nodes (denoted by the solid squares) encoding the holistic (view-based)
    variances, and the root-node (denoted by the dashed blue squares) on the top to
    switch the selection of its child and-nodes. The horizontal links incorporate
    contextual interactions among parts. Note that the leaf-nodes inherit the links
    that are defined between the layer of or-nodes. The nodes and links in red indicate
    the activation of leaf-nodes during the detection.'
  Figure 10 Link: articels_figures_by_rev_year\2014\Discriminatively_Trained_AndOr_Graph_Models_for_Object_Shape_Detection\figure_10.jpg
  Figure 10 caption: A few typical object shape detections generated by our approach
    on the SYSU-Shape dataset. The localized contours are highlighted in black, and
    the green boxes and red boxes indicate detected shapes and their parts, respectively.
  Figure 2 Link: articels_figures_by_rev_year\2014\Discriminatively_Trained_AndOr_Graph_Models_for_Object_Shape_Detection\figure_2.jpg
  Figure 2 caption: Illustration of the proposed contour descriptor. This feature
    combines the Shape Context descriptor in (a) and the triangle-based descriptor
    in (b) to characterize a local contour fragment.
  Figure 3 Link: articels_figures_by_rev_year\2014\Discriminatively_Trained_AndOr_Graph_Models_for_Object_Shape_Detection\figure_3.jpg
  Figure 3 caption: The spatial contextual features defined for the collaborative
    edges.
  Figure 4 Link: articels_figures_by_rev_year\2014\Discriminatively_Trained_AndOr_Graph_Models_for_Object_Shape_Detection\figure_4.jpg
  Figure 4 caption: Mapping the latent And-Or graph with the discriminative function
    defined in Equation (6). Different layers of nodes in our model are associated
    with certain bins in the feature vector phi (X,H) (at the bottom). The activated
    leaf-nodes are highlighted in red, and the feature bins are set to zeros for the
    other inactivated nodes. The embedded latent variables H = (P, V) make our model
    reconfigurable during detection.
  Figure 5 Link: articels_figures_by_rev_year\2014\Discriminatively_Trained_AndOr_Graph_Models_for_Object_Shape_Detection\figure_5.jpg
  Figure 5 caption: Illustration of the inference procedure. (a) shows local testing
    for detecting contour fragments within the edge map; the blue dashed boxes represent
    perturbed blocks associated with the leaf-nodes. (b) shows a hypothesis of detection
    including candidates (indicated by the red boxes) proposed by all or-nodes, in
    which the collaborative edges are imposed. (c) shows the global verification,
    in which the ensemble of contours are measured as a whole.
  Figure 6 Link: articels_figures_by_rev_year\2014\Discriminatively_Trained_AndOr_Graph_Models_for_Object_Shape_Detection\figure_6.jpg
  Figure 6 caption: Illustration of the structure reconfiguration. Parts of the model,
    two or-nodes ( U1,U6 ), are visualized in three intermediate steps. (a) The initial
    structure, i.e. the regular layout of an object. Two new structures are dynamically
    generated during the iterations. (b) A leaf-node associated with U1 is removed.
    (c) A new leaf-node is created and assigned to U6 .
  Figure 7 Link: articels_figures_by_rev_year\2014\Discriminatively_Trained_AndOr_Graph_Models_for_Object_Shape_Detection\figure_7.jpg
  Figure 7 caption: Geometric illustration of the CCCP procedure. The target energy
    is decomposed into two functions, f(omega ) and g(omega ) . At each step of iteration,
    a hyperplane (represented by the red line) is calculated as the upper-bound at
    omega t for optimizing omega t+1 .
  Figure 8 Link: articels_figures_by_rev_year\2014\Discriminatively_Trained_AndOr_Graph_Models_for_Object_Shape_Detection\figure_8.jpg
  Figure 8 caption: A toy example for structure reconfiguration. We consider 4 samples,
    X1, ldots , X4 , for training the structure of Ui (or Ar ). (a) shows the feature
    vectors phi of the samples associated with Ui (or Ar ), and the intensity of the
    feature bin indicates the feature value. (b) illustrates the clustering performed
    with phi prime . The vector langle phi 5, cdots , phi 8, rangle of X2 is grouped
    from cluster 2 to cluster 1. (c) shows the adjusted feature vectors according
    to the clustering. Note that the model structure reconfiguration is realized by
    the rearrange of feature vectors, as we discuss in the text. This figure should
    be viewed in electronic form.
  Figure 9 Link: articels_figures_by_rev_year\2014\Discriminatively_Trained_AndOr_Graph_Models_for_Object_Shape_Detection\figure_9.jpg
  Figure 9 caption: Precision-Recall (PR) curves on the SYSU-Shape dataset.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.55
  Name of the first author: Liang Lin
  Name of the last author: Jian-Huang Lai
  Number of Figures: 14
  Number of Tables: 4
  Number of authors: 4
  Paper title: Discriminatively Trained And-Or Graph Models for Object Shape Detection
  Publication Date: 2014-09-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Notation Summary of this Work
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Numbers of Nodes in the And-Or Graph Models for Different
      Databases
  Table 3 caption:
    table_text: TABLE 3 Detection Accuracies on the SYSU-Shape Dataset
  Table 4 caption:
    table_text: TABLE 4 Comparisons of Detection Accuracies on the UIUC-People Dataset
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2359888
- Affiliation of the first author: yamanashi testing center, chemitox, inc., hokuto,
    yamanashi, japan
  Affiliation of the last author: department of statistics, the ohio state university,
    columbus, oh
  Figure 1 Link: articels_figures_by_rev_year\2014\Statistical_Optimality_in_Multipartite_Ranking_and_Ordinal_Regression\figure_1.jpg
  Figure 1 caption: Theoretical ranking function (dotted line) and estimated ranking
    function (solid line) for pairwise ranking risk minimization with exponential
    loss, ORBoost, proportional odds model and SVOR with implicit constraints.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\Statistical_Optimality_in_Multipartite_Ranking_and_Ordinal_Regression\figure_2.jpg
  Figure 2 caption: Contours of the underlying ranking functions, fracp2(x) + p3(x)p1(x)+
    p2(x) for proportional odds model and SVOR with implicit constraints (left), fracp2(x)
    + 2 p3(x)2 p1(x)+ p2(x) for expected relevance (middle) and frac1- 2p1(x)p2(x)
    for Shashua and Levin's SVOR (right). The change in color from green to brown
    indicates the change in score from high to low.
  Figure 3 Link: articels_figures_by_rev_year\2014\Statistical_Optimality_in_Multipartite_Ranking_and_Ordinal_Regression\figure_3.jpg
  Figure 3 caption: Scatter plots of ranking scores from ORBoost, regression, proportional
    odds model, and SVOR against pairwise ranking scores with matching cost c13 for
    MovieLens data with three categories. The solid lines indicate theoretical relation
    between ranking scores.
  Figure 4 Link: articels_figures_by_rev_year\2014\Statistical_Optimality_in_Multipartite_Ranking_and_Ordinal_Regression\figure_4.jpg
  Figure 4 caption: Scatter plots of pairwise ranking scores (centered to zero) with
    different ranking cost c13 for MovieLens data when c12=c23=1 .
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Kazuki Uematsu
  Name of the last author: Yoonkyung Lee
  Number of Figures: 4
  Number of Tables: 3
  Number of authors: 2
  Paper title: Statistical Optimality in Multipartite Ranking and Ordinal Regression
  Publication Date: 2014-09-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Ranking Measures for the Bipartite Case
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Implicit Cost Schemes for ORBoost in the Balanced (Left) and
      Unbalanced (Right) Cases
  Table 3 caption:
    table_text: TABLE 3 Mean Ranking Error Rates for a Pair of Ratings and Their Standard
      Errors in Parentheses from 50 Replicates of Training Samples of Size 2,500
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2360397
- Affiliation of the first author: department of electrical, computer, and systems
    engineering, rensselaer polytechnic institute, troy, ny
  Affiliation of the last author: department of electrical, computer, and systems
    engineering, rensselaer polytechnic institute, troy, ny
  Figure 1 Link: articels_figures_by_rev_year\2014\Viewpoint_Invariant_Human_ReIdentification_in_Camera_Networks_Using_Pose_Priors_\figure_1.jpg
  Figure 1 caption: (a) Sample images from the VIPeR dataset [20]. (b) Sample images
    from the iLids dataset [38].
  Figure 10 Link: articels_figures_by_rev_year\2014\Viewpoint_Invariant_Human_ReIdentification_in_Camera_Networks_Using_Pose_Priors_\figure_10.jpg
  Figure 10 caption: Illustration of learning discriminative features.
  Figure 2 Link: articels_figures_by_rev_year\2014\Viewpoint_Invariant_Human_ReIdentification_in_Camera_Networks_Using_Pose_Priors_\figure_2.jpg
  Figure 2 caption: Improvements to the re-id process with the contributions proposed
    in this paper. Blocks in blue are steps in a traditional re-id process while blocks
    in red are the new steps proposed in this paper.
  Figure 3 Link: articels_figures_by_rev_year\2014\Viewpoint_Invariant_Human_ReIdentification_in_Camera_Networks_Using_Pose_Priors_\figure_3.jpg
  Figure 3 caption: Illustration of sub-image rectification.
  Figure 4 Link: articels_figures_by_rev_year\2014\Viewpoint_Invariant_Human_ReIdentification_in_Camera_Networks_Using_Pose_Priors_\figure_4.jpg
  Figure 4 caption: Example image pairs that may cause serious matching errors if
    the same descriptor is used without regard to pose. The yellow rectangles show
    the most difficult parts.
  Figure 5 Link: articels_figures_by_rev_year\2014\Viewpoint_Invariant_Human_ReIdentification_in_Camera_Networks_Using_Pose_Priors_\figure_5.jpg
  Figure 5 caption: Example images used in learning the pose prior.
  Figure 6 Link: articels_figures_by_rev_year\2014\Viewpoint_Invariant_Human_ReIdentification_in_Camera_Networks_Using_Pose_Priors_\figure_6.jpg
  Figure 6 caption: Learning the pose prior.
  Figure 7 Link: articels_figures_by_rev_year\2014\Viewpoint_Invariant_Human_ReIdentification_in_Camera_Networks_Using_Pose_Priors_\figure_7.jpg
  Figure 7 caption: Complete pose prior generated by projections of pose priors calculated
    from multiple viewpoints.
  Figure 8 Link: articels_figures_by_rev_year\2014\Viewpoint_Invariant_Human_ReIdentification_in_Camera_Networks_Using_Pose_Priors_\figure_8.jpg
  Figure 8 caption: Examples of the trained pose prior. It can be seen that the results
    match our assumptions. For example, at 90 degrees, the weights on the right side
    are much heavier than on the left side. At 135 and 180 degrees, the weights in
    the central area (i.e., the back of the person) are significantly lower than in
    the neighboring areas. We found this phenomenon to be generally caused by people
    carrying backpacks.
  Figure 9 Link: articels_figures_by_rev_year\2014\Viewpoint_Invariant_Human_ReIdentification_in_Camera_Networks_Using_Pose_Priors_\figure_9.jpg
  Figure 9 caption: The distance between descriptors of the same person from different
    viewpoints is reduced by applying the pose prior. Without the pose prior, the
    distance between the two descriptors is 1.4709, which has been reduced to 0.9738
    after applying the pose prior. Without the pose prior, the mean distance of all
    negative pairs containing this person is 5.2815, which increased to 5.6369 with
    the pose prior.
  First author gender probability: 0.75
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ziyan Wu
  Name of the last author: Richard J. Radke
  Number of Figures: 20
  Number of Tables: 4
  Number of authors: 3
  Paper title: Viewpoint Invariant Human Re-Identification in Camera Networks Using
    Pose Priors and Subject-Discriminative Features
  Publication Date: 2014-09-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Result Comparisons between the Proposed Algorithms and Competitive
      Algorithms
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Re-Identification Results on the SAIVT-SoftBio Dataset
  Table 3 caption:
    table_text: TABLE 3 Rank Matching Rate Comparison on VIPeR Dataset, with Training
      Size = 316, Testing Size = 316
  Table 4 caption:
    table_text: TABLE 4 Re-Identification Results on the Airport Dataset
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2360373
- Affiliation of the first author: department of electrical engineering, king abdullah
    university of science and technology (kaust), thuwal, saudi arabia
  Affiliation of the last author: department of electrical engineering and the department
    of applied mathematics and computational science, king abdullah university of
    science and technology (kaust), thuwal, saudi arabia
  Figure 1 Link: articels_figures_by_rev_year\2014\Shape_Tracking_with_Occlusions_via_CoarsetoFine_RegionBased_Sobolev_Descent\figure_1.jpg
  Figure 1 caption: "Diagram illustrating our dynamic model. Left: template ( R t\
    \ , a t ) (non-gray), right: I t+1 . Self-occlusions O t , dis-occlusions D t+1\
    \ and its radiance a t+1 d , the region at frame t+1 is R t+1 (inside the green\
    \ contour), and the warp is w t , which is defined in R t \u2216 O t . The curved\
    \ black line is a self-occlusion since the arm moves towards the left."
  Figure 10 Link: articels_figures_by_rev_year\2014\Shape_Tracking_with_Occlusions_via_CoarsetoFine_RegionBased_Sobolev_Descent\figure_10.jpg
  Figure 10 caption: 'Distinctive foregroundbackground global statistics. [Top]: Scribbles,
    [Middle]: AAE, [Bottom]: proposed method. When forebackground global statistics
    are separable, Scribbles, and AAE, for minor occlusions, performs well.'
  Figure 2 Link: articels_figures_by_rev_year\2014\Shape_Tracking_with_Occlusions_via_CoarsetoFine_RegionBased_Sobolev_Descent\figure_2.jpg
  Figure 2 caption: "Illustration of frame processing in our algorithm. (a): Estimate\
    \ at frame t of the shape and radiance ( a t , R t ) , and the next image I t+1\
    \ . (b): Simultaneous non-rigid warping and occlusion estimation is performed\
    \ (first image: warped template a t \u2218 w t , second: boundary of warped template\
    \ in I t+1 , third: warped occlusion w t ( O t ) determined, fourth: warped template\
    \ with warped occlusion removed w t ( R t \u2216 O t ) , fifth: boundary of w\
    \ t ( R t \u2216 O t ) ). (c): Dis-Occlusion D t+1 in I t+1 determined from input\
    \ w t ( R t \u2216 O t ) . (d): Final shape and radiance ( a t+1 , R t+1 ) in\
    \ frame t+1 (adding dis-occlusion D t+1 to w t ( R t \u2216 O t ) ). Shaded gray\
    \ regions indicates not defined."
  Figure 3 Link: articels_figures_by_rev_year\2014\Shape_Tracking_with_Occlusions_via_CoarsetoFine_RegionBased_Sobolev_Descent\figure_3.jpg
  Figure 3 caption: "Diagram of quantities used in the likelihood p(x) of a dis-occluded\
    \ pixel. The dark gray region is the dis-occlusion to be determined. Light gray\
    \ region is R \u2032 , region before the dis-occlusion is determined. A pixel\
    \ x within the band 0< d R \u2032 \u2264\u03B5 is depicted, and its closest pixel\
    \ to R \u2032 , cl(x) . The green (blue) region is where the foreground (background)\
    \ distribution p cl,f (x) ( p cl,b (x) ) is determined."
  Figure 4 Link: articels_figures_by_rev_year\2014\Shape_Tracking_with_Occlusions_via_CoarsetoFine_RegionBased_Sobolev_Descent\figure_4.jpg
  Figure 4 caption: Images I1 (left) and I2 (middle) used in the experiment in Fig.
    5, and an overlay of I1 on I2 to show the motiondeformation between frames, which
    is non-rigid and contains both coarse and fine motiondeformations.
  Figure 5 Link: articels_figures_by_rev_year\2014\Shape_Tracking_with_Occlusions_via_CoarsetoFine_RegionBased_Sobolev_Descent\figure_5.jpg
  Figure 5 caption: 'Coarse-to-fine behavior of region-based sobolev descent. Matching
    a template (obtained from I1 ) to I2 from Fig. 4 using regularization of the velocity
    field in the energy, and Sobolev descent. In each row, the evolution (until convergence)
    is shown. [First four images]: partial Rtau on I2 for various snapshots tau .
    [Last three images]: displacement of object between adjacent snapshots (in optical
    flow color code). Small gamma favors fine deformations and is sensitive to intermediate
    structures, whereas large gamma favors only coarse deformations and cannot capture
    regions with fine-scale deformations, e.g., legs. Sobolev descent captures all
    scales of deformation without being sensitive to intermediate structures.'
  Figure 6 Link: articels_figures_by_rev_year\2014\Shape_Tracking_with_Occlusions_via_CoarsetoFine_RegionBased_Sobolev_Descent\figure_6.jpg
  Figure 6 caption: 'Zoom of converged results of experiment of Fig. 5. Boundary of
    converged region on I2 . [Top-left]: energy regularization gamma =2.5times 105
    , [Top-right]: energy regularization gamma =50 times 105 , [Bottom-left]: energy
    regularization gamma =1,000 times 105 , [Bottom-right]: region-based Sobolev.
    Notice that small gamma misses regions of coarse motion, larger gamma obtains
    regions of coarse motion, but misses regions where finer deformation occurs. Sobolev
    obtains both coarse and fine deformations.'
  Figure 7 Link: articels_figures_by_rev_year\2014\Shape_Tracking_with_Occlusions_via_CoarsetoFine_RegionBased_Sobolev_Descent\figure_7.jpg
  Figure 7 caption: "Occlusion estimation and warping. [Top to bottom]: Beginning\
    \ ( \u03C4=0 ), intermediate, and final stages of evolution. [first column]: radiance\
    \ a \u03C4 , [second]: target image I and boundary of R \u03C4 , [third]: velocity\
    \ \u2212 G \u03C4 , [fourth]: occlusion estimation Res at time \u03C4 , [fifth]:\
    \ optical flow color code. The final occluded region is shown in Fig. 2b."
  Figure 8 Link: articels_figures_by_rev_year\2014\Shape_Tracking_with_Occlusions_via_CoarsetoFine_RegionBased_Sobolev_Descent\figure_8.jpg
  Figure 8 caption: "Illustration of disocclusion detection.[first]: warped un-occluded\
    \ radiance defined on R \u2032 (after occlusion and deformation computation),\
    \ [second]: target image I , [third]: likelihood of dis-occlusion map p (defined\
    \ in B R \u2032 (\u03B5)) , [fourth]: computed dis-occlusion D (white), and [fifth]:\
    \ final radiance. Boundary of final region super-imposed on I is in Fig. 2d."
  Figure 9 Link: articels_figures_by_rev_year\2014\Shape_Tracking_with_Occlusions_via_CoarsetoFine_RegionBased_Sobolev_Descent\figure_9.jpg
  Figure 9 caption: 'Modeling occlusionsdis-occlusions is necessary. [first row]:
    occlusiondis-occlusion detection are turned off in our method. [second]: occlusion
    modeling done, but not dis-occlusions in our method. [third]: dis-occlusions detected
    but not occlusions. [fourth]: result of Scribbles. [fifth]: result of AAE. [sixth]:
    accurate tracking when both occlusion and dis-occlusion modeling is performed
    (our final result).'
  First author gender probability: 0.66
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yanchao Yang
  Name of the last author: Ganesh Sundaramoorthi
  Number of Figures: 12
  Number of Tables: 1
  Number of authors: 2
  Paper title: Shape Tracking with Occlusions via Coarse-to-Fine Region-Based Sobolev
    Descent
  Publication Date: 2014-09-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Performance Analysis
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2360380
- Affiliation of the first author: cea leti, grenoble, france
  Affiliation of the last author: eads astrium, toulouse, france
  Figure 1 Link: articels_figures_by_rev_year\2014\Estimation_of_an_Observation_Satellites_Attitude_Using_Multimodal_Pushbroom_Came\figure_1.jpg
  Figure 1 caption: 'Pushbroom acquisition principle: the camera is moving straight
    along the y axis and recording 1D images over time denoted by t . x is the camera
    axis and z the orthogonal axis to the image plane. The attitude of the camera
    is defined by the yaw (rotation about z ), the roll (rotation about y ), and the
    pitch (rotation about x ).'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\Estimation_of_an_Observation_Satellites_Attitude_Using_Multimodal_Pushbroom_Came\figure_2.jpg
  Figure 2 caption: Example of warps in a regular checkerboard when the pushbroom
    camera is tilting around its three rotation axes.
  Figure 3 Link: articels_figures_by_rev_year\2014\Estimation_of_an_Observation_Satellites_Attitude_Using_Multimodal_Pushbroom_Came\figure_3.jpg
  Figure 3 caption: 'Standard focal plane geometry of an observation satellite with
    four pushbroom cameras: panchromatic, blue, green, and red (respectively enumerated
    as 1 , 2 , 3 , and 4 ). What is seen by camera 2 at time t will be seen by camera
    1 at time t+delta .'
  Figure 4 Link: articels_figures_by_rev_year\2014\Estimation_of_an_Observation_Satellites_Attitude_Using_Multimodal_Pushbroom_Came\figure_4.jpg
  Figure 4 caption: Power spectrum of kdelta for several values of delta .
  Figure 5 Link: articels_figures_by_rev_year\2014\Estimation_of_an_Observation_Satellites_Attitude_Using_Multimodal_Pushbroom_Came\figure_5.jpg
  Figure 5 caption: 'Results for datasets D3 (a) and D4 (b) using method M4; on the
    top is the warped image acquired by the satellite (panchromatic in both cases).
    The error images are the pixels intensity substraction between the undistorted
    panchromatic image (the one the satellite should have recorded if it was not oscillating;
    it is the groundtruth) and respectively: the acquired panchromatic image (top),
    and the rectified panchromatic image (bottom) after attitude estimation. Bottom
    plots display estimated roll and pitch in black curves with the error with respect
    to the real attitude in gray curves; the thin white strip delimits the size of
    a pixel.'
  Figure 6 Link: articels_figures_by_rev_year\2014\Estimation_of_an_Observation_Satellites_Attitude_Using_Multimodal_Pushbroom_Came\figure_6.jpg
  Figure 6 caption: Color image patch from dataset D1 before and after the registration
    process. Several bleeding artifacts occur on the edges of the left image due to
    the attitude variations. Those are corrected after the registration process on
    the right image.
  Figure 7 Link: articels_figures_by_rev_year\2014\Estimation_of_an_Observation_Satellites_Attitude_Using_Multimodal_Pushbroom_Came\figure_7.jpg
  Figure 7 caption: Comparison between two image patches mathbf i and mathbf jtheta
    , of size (100 times 100) pixels from the panchromatic and the red channels of
    D1, after the registration process of method M3. The error image |mathbf a+ mathbf
    bmathbf i- mathbf jtheta | is much closer to a random Gaussian noise than |mathbf
    i- mathbf jtheta | .
  Figure 8 Link: articels_figures_by_rev_year\2014\Estimation_of_an_Observation_Satellites_Attitude_Using_Multimodal_Pushbroom_Came\figure_8.jpg
  Figure 8 caption: Performances of the methods with respect to the number of horizontal
    pixels nl in the images. Results were averaged over 100 runs on randomly chosen
    image patches from dataset it D1 at a given size of (512 times nl) pixels.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "R\xE9gis Perrier"
  Name of the last author: Mathias Ortner
  Number of Figures: 8
  Number of Tables: 2
  Number of authors: 4
  Paper title: "Estimation of an Observation Satellite\u2019s Attitude Using Multimodal\
    \ Pushbroom Cameras"
  Publication Date: 2014-09-26 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Estimated Hyperparameters for the Different Dataset with the
      Evidence Procedure of Section 5.1
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Attitude Estimation Results on Datasets for Each Method with
      Respect to Criterion (41)
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2360394
- Affiliation of the first author: amazon
  Affiliation of the last author: department of computer science, university of texas
    at austin, 1 university station, tx, austin
  Figure 1 Link: articels_figures_by_rev_year\2014\Boundary_Preserving_Dense_Local_Regions\figure_1.jpg
  Figure 1 caption: Boundary-preserving local regions capture local object shape with
    dense spatial coverage. (We densely extract BPLRs across the image, but for visualization
    purposes this figure displays only a subset per image.)
  Figure 10 Link: articels_figures_by_rev_year\2014\Boundary_Preserving_Dense_Local_Regions\figure_10.jpg
  Figure 10 caption: "Localization accuracy on ETHZ objects. Plots compare our approach\
    \ (BPLR) to three alternative region detectors: MSER, dense sampling, and segments.\
    \ Quality is measured by the bounding box overlap score\u2014recall (BBOS-Recall),\
    \ which captures the layout of the feature matches. Curves that are higher in\
    \ the y-axis (better object overlap) and longer along the x-axis (higher recall)\
    \ are better. Maximum F-numbers in the legend are defined as the harmonic mean\
    \ of BBOS and Recall along the curves."
  Figure 2 Link: articels_figures_by_rev_year\2014\Boundary_Preserving_Dense_Local_Regions\figure_2.jpg
  Figure 2 caption: "Illustration of BPLR's key contrasts with representative existing\
    \ detectors. (a) The proposed BPLR features are reliably repeated across different\
    \ object instances in spite of large intra-class variation in pose and appearance.\
    \ They respect object boundaries while maintaining good spatial coverage per region.\
    \ (Note, we display only a sample for different foreground object parts; our complete\
    \ extraction is dense and covers entire image.) (b) Regions from a segmentation\
    \ algorithm (here, obtained with [1], and pruned to the best foreground-overlapping\
    \ regions) typically produce some high quality segments, but the shape and localization\
    \ often lacks repeatability across instances. Further, even if a good segment\
    \ encompasses the entire object, it won't match other instances with deformation.\
    \ (c) Superpixels (obtained here with Normalized Cuts) are also local and dense,\
    \ but typically lose informative shape cues and lack repeatability (compare shapes\
    \ of superpixels on the two giraffe instances). (d) Local interest regions (obtained\
    \ with MSER [2]) are highly repeatable for multiple views of the same instance,\
    \ but do not respect object boundaries and fire very differently across different\
    \ instances of the same object class. (e) Dense patches offer good coverage and\
    \ \u201Cbrute force\u201D repeatability, but many features straddle object boundaries,\
    \ and shape is mostly not preserved."
  Figure 3 Link: articels_figures_by_rev_year\2014\Boundary_Preserving_Dense_Local_Regions\figure_3.jpg
  Figure 3 caption: Overview of our method. Best viewed in color. Given multiple segmentations
    as input, we first sample local elements from the segments. The sampled elements
    are then linked across the image via a minimum spanning tree. Finally, we group
    neighbor elements in the spanning tree to form BPLRs. For legibility, we show
    only a subset of the extracted BPLRs in the last image.
  Figure 4 Link: articels_figures_by_rev_year\2014\Boundary_Preserving_Dense_Local_Regions\figure_4.jpg
  Figure 4 caption: Sampling elements. For each initial segment, we sample local elements
    densely in a grid according to its distance transform. Each circle denotes the
    sampled element and its scale.
  Figure 5 Link: articels_figures_by_rev_year\2014\Boundary_Preserving_Dense_Local_Regions\figure_5.jpg
  Figure 5 caption: Linking elements. Elements are linked across the image via a minimum
    spanning tree to create a single structure that reflects the main shapes and segment
    layout. Green lines in right image indicate linked elements.
  Figure 6 Link: articels_figures_by_rev_year\2014\Boundary_Preserving_Dense_Local_Regions\figure_6.jpg
  Figure 6 caption: Grouping neighboring elements relative to a reference element.
    Green lines in the first three images denote the spanning tree links that connect
    elements. Given a reference element whose scale is r (marked by a dotted circle
    in the first image), its euclidean neighbors are the elements within F times the
    scale of the reference (second image). Topological neighbors are the elements
    up to N hops from the reference element in the graph (third image). The intersection
    of the euclidean and topological neighbors forms one BPLR for the reference element
    (fourth image). Finally, the BPLR is mapped to some descriptor (we use HOG+gPb).
    Note, we repeat this procedure for every element in the graph, generating dense
    BPLR extractions across the entire image. Best viewed in color.
  Figure 7 Link: articels_figures_by_rev_year\2014\Boundary_Preserving_Dense_Local_Regions\figure_7.jpg
  Figure 7 caption: Illustration of the BBHR-FPR metric. A bounding box hit is declared
    when at least k true matches are found. We set k = 5 following the original author's
    choice [14]. BBHR-FPR records the average hit rate and corresponding false positive
    rate for all test images.
  Figure 8 Link: articels_figures_by_rev_year\2014\Boundary_Preserving_Dense_Local_Regions\figure_8.jpg
  Figure 8 caption: 'Repeatability on ETHZ objects. Plots compare our approach (BPLR)
    to three alternative region detectors: MSER, dense sampling, and segments. Quality
    is measured by the bounding box hit rate-false positive rate tradeoff . Curves
    that are lower on the y-axis (fewer false positives) and longer along the x-axis
    (higher hit rate) are better. Maximum F-numbers in the legend are defined as the
    maximum harmonic mean of BBHR and 1-FPR along the curves, meaning the best combination
    of two scores along the curve; higher F values are better.'
  Figure 9 Link: articels_figures_by_rev_year\2014\Boundary_Preserving_Dense_Local_Regions\figure_9.jpg
  Figure 9 caption: Repeatability on ETH+TUD objects. Plots compare our approach (BPLR)
    to two state-of-the-art semi-local feature methods [14], [16]. Lower and longer
    curves are better. ([16] does not report results on the Giraffe class.)
  First author gender probability: 0.7
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Jaechul Kim
  Name of the last author: Kristen Grauman
  Number of Figures: 16
  Number of Tables: 6
  Number of authors: 2
  Paper title: Boundary Preserving Dense Local Regions
  Publication Date: 2014-09-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Bounding Box Detection Rate on ETHZ Objects
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Bounding Box Detection Rate from Two Different Seed Segmentations
      on ETHZ Objects
  Table 3 caption:
    table_text: TABLE 3 Quality of Features Using the Same Underlying Gradient Image
      on ETHZ Objects
  Table 4 caption:
    table_text: TABLE 4 Foreground Discovery Results, Compared to Several State-of-the-Art
      Methods
  Table 5 caption:
    table_text: TABLE 5 Direct Comparison of BPLR to Other Feature Detectors on the
      Caltech-101
  Table 6 caption:
    table_text: TABLE 6 Comparison to Existing Results on the Caltech-101 That Use
      Nearest Neighbor-Based Classifiers and a Single Descriptor Type
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2360689
- Affiliation of the first author: moe key lab of bioinformatics, bioinformatics division
    and center for synthetic & systems biology, center for brain inspired computing
    research (cbicr), tnlist, department of computer science and technology, state
    key lab of intelligent technology and systems, tsinghua university, beijing, china
  Affiliation of the last author: moe key lab of bioinformatics, bioinformatics division
    and center for synthetic & systems biology, center for brain inspired computing
    research (cbicr), tnlist, department of computer science and technology, state
    key lab of intelligent technology and systems, tsinghua university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2014\Discriminative_Relational_Topic_Models\figure_1.jpg
  Figure 1 caption: "A plate graph of Bayesian gRTMs, where \u03BB ij is an augmented\
    \ variable used for sampling; \u03B1 , \u03B2 , and \u03BD are hyper-parameters."
  Figure 10 Link: articels_figures_by_rev_year\2014\Discriminative_Relational_Topic_Models\figure_10.jpg
  Figure 10 caption: Performance of Gibbs-MMRTM with different c values on the Citeseer
    dataset.
  Figure 2 Link: articels_figures_by_rev_year\2014\Discriminative_Relational_Topic_Models\figure_2.jpg
  Figure 2 caption: Results of various models with different numbers of topics on
    the Cora citation dataset.
  Figure 3 Link: articels_figures_by_rev_year\2014\Discriminative_Relational_Topic_Models\figure_3.jpg
  Figure 3 caption: Results of various models with different numbers of topics on
    the WebKB dataset.
  Figure 4 Link: articels_figures_by_rev_year\2014\Discriminative_Relational_Topic_Models\figure_4.jpg
  Figure 4 caption: Results of various models with different numbers of topics on
    the Citeseer dataset.
  Figure 5 Link: articels_figures_by_rev_year\2014\Discriminative_Relational_Topic_Models\figure_5.jpg
  Figure 5 caption: Results of various models with different numbers of topics on
    the Cora dataset.
  Figure 6 Link: articels_figures_by_rev_year\2014\Discriminative_Relational_Topic_Models\figure_6.jpg
  Figure 6 caption: Results of various models with different numbers of topics on
    the Citeseer dataset.
  Figure 7 Link: articels_figures_by_rev_year\2014\Discriminative_Relational_Topic_Models\figure_7.jpg
  Figure 7 caption: "Time complexity of drawing \u03BB and \u03B7 on the Citeseer\
    \ dataset."
  Figure 8 Link: articels_figures_by_rev_year\2014\Discriminative_Relational_Topic_Models\figure_8.jpg
  Figure 8 caption: Performance of Gibbs-RTM with different c values on the Cora dataset.
  Figure 9 Link: articels_figures_by_rev_year\2014\Discriminative_Relational_Topic_Models\figure_9.jpg
  Figure 9 caption: Performance of Gibbs-gRTM with different c values on the Cora
    dataset.
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.83
  Name of the first author: Ning Chen
  Name of the last author: Bo Zhang
  Number of Figures: 15
  Number of Tables: 4
  Number of authors: 4
  Paper title: Discriminative Relational Topic Models
  Publication Date: 2014-10-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Learned Diagonal Weight Matrix of 10-Topic RTM and Representative
      Words Corresponding with Topics
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Learned Weight Matrix of 10-Topic gRTM and Representative
      Words Corresponding with Topics
  Table 3 caption:
    table_text: TABLE 3 Split of Training Time on Cora Dataset
  Table 4 caption:
    table_text: TABLE 4 Top 8 Link Predictions Made by Gibbs-gRTM and Var-RTM on the
      Cora Dataset
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2361129
