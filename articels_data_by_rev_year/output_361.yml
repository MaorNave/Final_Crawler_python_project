- Affiliation of the first author: department of information engineering, university
    of padova, italy
  Affiliation of the last author: department of information engineering, university
    of padova, italy
  Figure 1 Link: articels_figures_by_rev_year\2015\Probabilistic_ToF_and_Stereo_Data_Fusion_Based_on_Mixed_Pixels_Measurement_Model\figure_1.jpg
  Figure 1 caption: "Considered acquisition system made by a ToF camera T and a stereo\
    \ vision system S\u225CL,R ."
  Figure 10 Link: articels_figures_by_rev_year\2015\Probabilistic_ToF_and_Stereo_Data_Fusion_Based_on_Mixed_Pixels_Measurement_Model\figure_10.jpg
  Figure 10 caption: Example of the structure of the messages exchanged between pi
    in Lambda Z and pj in mathcal N(pi) with Ni =3 range (depth) samples for pi and
    Nj = 2 for pj . The messages relative to zj1 are in purple and the messages relative
    to zj2 in green. Notice how each range value zini receives exactly Nj=2 messages.
  Figure 2 Link: articels_figures_by_rev_year\2015\Probabilistic_ToF_and_Stereo_Data_Fusion_Based_on_Mixed_Pixels_Measurement_Model\figure_2.jpg
  Figure 2 caption: CCSs (3D and 2D) associated to the various sensors of the acquisition
    system.
  Figure 3 Link: articels_figures_by_rev_year\2015\Probabilistic_ToF_and_Stereo_Data_Fusion_Based_on_Mixed_Pixels_Measurement_Model\figure_3.jpg
  Figure 3 caption: 'Data acquired by T: A T (left), B T (center) and Z T (right).
    (Images A T and B T were processed to increase printing visibility.)'
  Figure 4 Link: articels_figures_by_rev_year\2015\Probabilistic_ToF_and_Stereo_Data_Fusion_Based_on_Mixed_Pixels_Measurement_Model\figure_4.jpg
  Figure 4 caption: "Formation of a finite size sensor pixel p i \u2208 \u039B T relative\
    \ to a finite size scene area."
  Figure 5 Link: articels_figures_by_rev_year\2015\Probabilistic_ToF_and_Stereo_Data_Fusion_Based_on_Mixed_Pixels_Measurement_Model\figure_5.jpg
  Figure 5 caption: 'Discontinuity types: a) disconnected discontinuity; b) connected
    discontinuity; c) the discontinuity between R C and R F crosses the area associated
    to p i , p i 4 , p i 6 : points p i 1 , p i 2 , p i 5 , p i 7 pertain to scene
    region R F while points p i 3 , p i 8 to scene region R C .'
  Figure 6 Link: articels_figures_by_rev_year\2015\Probabilistic_ToF_and_Stereo_Data_Fusion_Based_on_Mixed_Pixels_Measurement_Model\figure_6.jpg
  Figure 6 caption: The concept of useful interval allows for a reduction of the number
    of operations, as it will be shown in Section 9.
  Figure 7 Link: articels_figures_by_rev_year\2015\Probabilistic_ToF_and_Stereo_Data_Fusion_Based_on_Mixed_Pixels_Measurement_Model\figure_7.jpg
  Figure 7 caption: 'Stereo likelihood computation: a) The 3D points sampled from
    the useful interval are re-projected onto the two stereo images; b) the stereo
    likelihood is computed by matching the windows centered on conjugate pairs.'
  Figure 8 Link: articels_figures_by_rev_year\2015\Probabilistic_ToF_and_Stereo_Data_Fusion_Based_on_Mixed_Pixels_Measurement_Model\figure_8.jpg
  Figure 8 caption: Flowchart of the proposed ML and MAP-MRF fusion frameworks.
  Figure 9 Link: articels_figures_by_rev_year\2015\Probabilistic_ToF_and_Stereo_Data_Fusion_Based_on_Mixed_Pixels_Measurement_Model\figure_9.jpg
  Figure 9 caption: a) In all the previous approaches each pixel of Lambda Z is associated
    to the same set of possible depth values; b) In the proposed approach each pixel
    of Lambda Z is associated to its own set of depth values different from the ones
    of the other pixels.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Carlo Dal Mutto
  Name of the last author: Guido Maria Cortelazzo
  Number of Figures: 17
  Number of Tables: 3
  Number of authors: 3
  Paper title: Probabilistic ToF and Stereo Data Fusion Based on Mixed Pixels Measurement
    Models
  Publication Date: 2015-03-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Accuracy (in mm) of Depth Information Acquired by ToF, Stereo,
      ML Fusion and MAP Fusion
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of the Depth Estimation Accuracy in mm Obtained
      with Various Approaches
  Table 3 caption:
    table_text: TABLE 3 Comparison of the Depth Estimation Accuracy in mm Obtained
      with Various Approaches in Areas Close to Depth Discontinuities
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2408361
- Affiliation of the first author: department of computer science, university of tsukuba,
    tsukuba, japan
  Affiliation of the last author: school of computer science and communication, kth
    royal institute of technology, stockholm, sweden
  Figure 1 Link: articels_figures_by_rev_year\2015\Difference_Subspace_and_Its_Generalization_for_SubspaceBased_Methods\figure_1.jpg
  Figure 1 caption: "Conceptual diagram of the constrained mutual subspace method.\
    \ The canonical angles \u03B8 C between the two projected subspaces P C and Q\
    \ C onto the GDS are measured. Since it is not possible to depict the subspaces\
    \ in a high-dimensional space, we show these schematically."
  Figure 10 Link: articels_figures_by_rev_year\2015\Difference_Subspace_and_Its_Generalization_for_SubspaceBased_Methods\figure_10.jpg
  Figure 10 caption: PCS and DS of two real cow reliefs.
  Figure 2 Link: articels_figures_by_rev_year\2015\Difference_Subspace_and_Its_Generalization_for_SubspaceBased_Methods\figure_2.jpg
  Figure 2 caption: "Conceptual diagram of difference subspace: (a) difference vector\
    \ d between u and v ; (b) difference subspace D ~ 2 between N p -dimensional subspaces\
    \ P and N q -dimensional Q is defined with the canonical vectors u i \u2208P and\
    \ v i \u2208Q forming the i th canonical angle \u03B8 i between them. The orthogonal\
    \ bases d i \xAF N p i=1 of D ~ 2 are obtained by normalizing the difference vectors\
    \ d i between u i and v i ."
  Figure 3 Link: articels_figures_by_rev_year\2015\Difference_Subspace_and_Its_Generalization_for_SubspaceBased_Methods\figure_3.jpg
  Figure 3 caption: Direct sum decomposition of sum subspace W 2 of P and Q into M
    ~ 2 and D ~ 2 .
  Figure 4 Link: articels_figures_by_rev_year\2015\Difference_Subspace_and_Its_Generalization_for_SubspaceBased_Methods\figure_4.jpg
  Figure 4 caption: Examples of M ~ 2 and D ~ 2 between two 3-dimensional illumination
    subspaces P and Q .
  Figure 5 Link: articels_figures_by_rev_year\2015\Difference_Subspace_and_Its_Generalization_for_SubspaceBased_Methods\figure_5.jpg
  Figure 5 caption: Conceptual diagram of GDS D C for C subspaces P k C k=1 , obtained
    by removing the PCS M C from the sum subspace of the subspaces.
  Figure 6 Link: articels_figures_by_rev_year\2015\Difference_Subspace_and_Its_Generalization_for_SubspaceBased_Methods\figure_6.jpg
  Figure 6 caption: PCS M 3 and GDS D 3 of three three-dimensional illumination subspaces
    of different synthetic objects.
  Figure 7 Link: articels_figures_by_rev_year\2015\Difference_Subspace_and_Its_Generalization_for_SubspaceBased_Methods\figure_7.jpg
  Figure 7 caption: Orthogonal basis vectors of M 128 in the upper region (a) with
    45 dimensions, and D 128 in the remaining region (b) with 211 dimensions.
  Figure 8 Link: articels_figures_by_rev_year\2015\Difference_Subspace_and_Its_Generalization_for_SubspaceBased_Methods\figure_8.jpg
  Figure 8 caption: Changes in the orthogonal degree between the projected subspaces
    and three performance indexes against the dimension of the GDS.
  Figure 9 Link: articels_figures_by_rev_year\2015\Difference_Subspace_and_Its_Generalization_for_SubspaceBased_Methods\figure_9.jpg
  Figure 9 caption: The flow of KCMSM in hand shape classification.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Kazuhiro Fukui
  Name of the last author: Atsuto Maki
  Number of Figures: 16
  Number of Tables: 3
  Number of authors: 2
  Paper title: Difference Subspace and Its Generalization for Subspace-Based Methods
  Publication Date: 2015-03-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 EER(%) of the CSM-9PL and the SM-9PL
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Performance of Different Methods on the CMU Database
  Table 3 caption:
    table_text: TABLE 3 Experimental Results of Hand Shape Classification
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2408358
- Affiliation of the first author: school of information science and technology, sun
    yat-sen university
  Affiliation of the last author: department of electrical and computer engineering,
    national university of singapore
  Figure 1 Link: articels_figures_by_rev_year\2015\Deep_Human_Parsing_with_Active_Template_Regression\figure_1.jpg
  Figure 1 caption: Exemplar parsing results by our Active Template Regression (ATR)
    model. For better viewing of all figures in this paper, please see original zoomed-in
    color pdf file.
  Figure 10 Link: articels_figures_by_rev_year\2015\Deep_Human_Parsing_with_Active_Template_Regression\figure_10.jpg
  Figure 10 caption: Visualization of our template dictionaries of eight semantic
    labels, including pants, dress, hair, left-arm, right-leg, bag, hat and dress
    that are displayed sequentially. For each label, we display 21 learned templates
    by the NMF method. Brighter pixels represent the most important parts for distinguishing
    different label masks.
  Figure 2 Link: articels_figures_by_rev_year\2015\Deep_Human_Parsing_with_Active_Template_Regression\figure_2.jpg
  Figure 2 caption: Predicted label masks by our model. We directly predict each label
    mask and then morph them into the absolute image coordinates. Different colors
    indicate different labels.
  Figure 3 Link: articels_figures_by_rev_year\2015\Deep_Human_Parsing_with_Active_Template_Regression\figure_3.jpg
  Figure 3 caption: Framework of our active template regression model. Given a test
    image, we first locate the bounding box for human body and then feed it into two
    separate networks. The template coefficients from the active template network
    are used to reconstruct the normalized mask. The masks of all labels are then
    fused together to generate the label confidence maps and the background confidence
    map by morphing with the active shape parameters (i.e., position, scale and visibility).
    The super-pixel smoothing is finally used to refine the parsing result.
  Figure 4 Link: articels_figures_by_rev_year\2015\Deep_Human_Parsing_with_Active_Template_Regression\figure_4.jpg
  Figure 4 caption: "Our active template network. A 227\xD7227\xD73 image is taken\
    \ as the input. We convolve it with 96 different first layer filters (red), each\
    \ of which with the size 7\xD77 , using a stride of 2. The obtained feature maps\
    \ are then: (1) passed through a rectified linear function (not shown), (2) max\
    \ pooled (within 3\xD73 filter, using stride 2) and (3) contrast normalized. Similar\
    \ operations are repeated in the second, third, fourth, fifth layers. The last\
    \ two layers are fully-connected, taking features from the top convolutional layer.\
    \ The output layer with 850=17\xD750 units is a regression function with \u2113\
    \ 2 -norm for K=17 labels and each with M=50 coefficients."
  Figure 5 Link: articels_figures_by_rev_year\2015\Deep_Human_Parsing_with_Active_Template_Regression\figure_5.jpg
  Figure 5 caption: "Architecture of our active shape network. We take a 227\xD7227\xD7\
    3 image as the input and convolve it with 48 different frist layer filters (red),\
    \ each of which with the size 7\xD77 , using a stride of 2 in both dimensions.\
    \ The obtained feature maps are then passed through a rectified linear function\
    \ (not shown) to get 48 different 111\xD7111 feature maps. Similar operations\
    \ are repeated in second, third, fourth, fifth layers. The last two layers are\
    \ fully-connected with 2,048 units and 1,024 units, respectively. The output layer\
    \ with 85=17\xD75 units is a regression function with \u2113 2 -norm for K=17\
    \ semantic labels and each with five dimensions, including positions, scales and\
    \ visibility flag."
  Figure 6 Link: articels_figures_by_rev_year\2015\Deep_Human_Parsing_with_Active_Template_Regression\figure_6.jpg
  Figure 6 caption: 'Our structure output combination. The confidence maps of all
    foreground labels are predicted by fusing two types of structure outputs. Then
    we can produce the background confidence map: we first generate the foreground
    (blue pixels) and background seeds (pink pixels) and then predict the background
    confidence map (the red colored pixels have the highest probability for background).
    Finally the superpixel smoothing is used to refine the parsing result.'
  Figure 7 Link: articels_figures_by_rev_year\2015\Deep_Human_Parsing_with_Active_Template_Regression\figure_7.jpg
  Figure 7 caption: Exemplar images in the combined dataset.
  Figure 8 Link: articels_figures_by_rev_year\2015\Deep_Human_Parsing_with_Active_Template_Regression\figure_8.jpg
  Figure 8 caption: Visualization of our predicted label masks with the active template
    network. We take the six semantic labels as the examples, such as hat, hair, scarf,
    upper-clothes, skirt and bag. The pixel with brighter color indicates that it
    is more likely to be assigned as the specific label.
  Figure 9 Link: articels_figures_by_rev_year\2015\Deep_Human_Parsing_with_Active_Template_Regression\figure_9.jpg
  Figure 9 caption: "Comparison of parsing results with the state-of-the-art method\
    \ and our two versions. For each image, we show the parsing results by PaperDoll\
    \ [28], our \u201CATR (noSPR)\u201D with no super-pixel smoothing and our full\
    \ method \u201CATR\u201D sequentially."
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xiaodan Liang
  Name of the last author: Shuicheng Yan
  Number of Figures: 10
  Number of Tables: 3
  Number of authors: 8
  Paper title: Deep Human Parsing with Active Template Regression
  Publication Date: 2015-03-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Parsing Performances with Several Architectural
      Variants of Our Model and Two State-of-the-Arts
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 F-1 Scores of Foreground Semantic Labels
  Table 3 caption:
    table_text: TABLE 3 Detailed Experimental Settings by Varying the Model Architectures
      of Our Networks
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2408360
- Affiliation of the first author: "max planck institute for informatics, 66123 saarbr\xFC\
    cken, germany"
  Affiliation of the last author: "max planck institute for informatics, 66123 saarbr\xFC\
    cken, germany"
  Figure 1 Link: articels_figures_by_rev_year\2015\MultiView_and_D_Deformable_Part_Models\figure_1.jpg
  Figure 1 caption: 3D 2 PM model visualization. Learned part 3D displacement distributions
    along with the continuous appearance model.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\MultiView_and_D_Deformable_Part_Models\figure_2.jpg
  Figure 2 caption: Graphical models depicting (a) general part-based model as a CRF
    over the parts o i conditioned on the data X . (b) The 2D DPM, conditioned on
    an image I . With shaded nodes, we denote the observed variables.
  Figure 3 Link: articels_figures_by_rev_year\2015\MultiView_and_D_Deformable_Part_Models\figure_3.jpg
  Figure 3 caption: "Comparison of the different presented models. In the first row\
    \ from left to right the graphical models of (a) DPM-VOC+VP, (b) DPM-3D-Constraints,\
    \ and (c) 3D 2 PM, are shown. In the second row, the part parameterization is\
    \ illustrated. The third row shows a possible layout of the part configuration.\
    \ The last row visualizes the covariances of the placement distributions. The\
    \ variables \u03B2 i,v , of the 3D 2 PM are implicitly defined via projection,\
    \ see Section 3.5. Both DPM-3D-Constraints and 3D 2 PM define parts in a 3D reference\
    \ frame, therefore it is possible to establish part-correspondences across different\
    \ viewpoints."
  Figure 4 Link: articels_figures_by_rev_year\2015\MultiView_and_D_Deformable_Part_Models\figure_4.jpg
  Figure 4 caption: 3D part parametrization for an example 3D CAD model (center).
    Corresponding projected part positions in two different views (left, right).
  Figure 5 Link: articels_figures_by_rev_year\2015\MultiView_and_D_Deformable_Part_Models\figure_5.jpg
  Figure 5 caption: 2D bounding box localization (left) and viewpoint estimation (right)
    results on nine 3D Object classes [3].
  Figure 6 Link: articels_figures_by_rev_year\2015\MultiView_and_D_Deformable_Part_Models\figure_6.jpg
  Figure 6 caption: Qualitative results on KITTI and 3D object classes. Corresponding
    part detections (for a given class) are color coded. 3D 2 PM (first row), DPM-3D-Constraints
    (second row) and DPM-VOC+VP (third row).
  Figure 7 Link: articels_figures_by_rev_year\2015\MultiView_and_D_Deformable_Part_Models\figure_7.jpg
  Figure 7 caption: Fine viewpoint estimation performance (in MAE) using linear (left)
    and exponential interpolation (right).
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Bojan Pepik
  Name of the last author: Bernt Schiele
  Number of Figures: 7
  Number of Tables: 10
  Number of authors: 4
  Paper title: Multi-View and 3D Deformable Part Models
  Publication Date: 2015-03-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Different Models in Terms of Part Parameterization,
      Appearance Model, Component Initialization and Training Loss
  Table 10 caption:
    table_text: TABLE 10 Ultra-Wide Baseline Matching Performance, Measured by the
      Fraction of Correctly Estimated Fundamental Matrices
  Table 2 caption:
    table_text: TABLE 2 2D BB Localization Performance on Pascal VOC 2007 [5], Comparing
      our DPM-VOC to DPM-Hinge and [63]
  Table 3 caption:
    table_text: TABLE 3 The Results of DPM-Hinge, VDPM and DPM-VOC+VP Are Shown
  Table 4 caption:
    table_text: TABLE 4 Comparison to State-of-the-Art in 2D BB Localization and Viewpoint
      Estimation on 3D Object Classes [3]
  Table 5 caption:
    table_text: TABLE 5 2D BB Localization and Viewpoint Estimation on KITTI Testing
      [2]
  Table 6 caption:
    table_text: TABLE 6 2D BB Localization and Viewpoint Estimation on KITTI [2]
  Table 7 caption:
    table_text: TABLE 7 Fine Viewpoint Estimation on EPFL [6]
  Table 8 caption:
    table_text: TABLE 8 2D BB Localization (AP) and Viewpoint Estimation (MPPE [29])
      on EPFL [6]
  Table 9 caption:
    table_text: TABLE 9 Detection (AP) and vp. Estimation (MAE); Full vs. Coarse-to-Fine
      Inference
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2408347
- Affiliation of the first author: school of computer science and the centre for intelligent
    machines, mcgill university, montreal, qc, canada
  Affiliation of the last author: school of computer science and the centre for intelligent
    machines, mcgill university, montreal, qc, canada
  Figure 1 Link: articels_figures_by_rev_year\2015\MaurerCartan_Forms_for_Fields_on_Surfaces_Application_to_Heart_Fiber_Geometry\figure_1.jpg
  Figure 1 caption: Geometry characterized by connection forms.
  Figure 10 Link: articels_figures_by_rev_year\2015\MaurerCartan_Forms_for_Fields_on_Surfaces_Application_to_Heart_Fiber_Geometry\figure_10.jpg
  Figure 10 caption: Volume histograms of cijk connections (radiansvoxel) and fitting
    errors (radians) for Omega 3 by fusing all rats in the dataset using optimized
    parameter computations (shown in red) together with the min (red) and max (blue)
    dataset envelope for each value.
  Figure 2 Link: articels_figures_by_rev_year\2015\MaurerCartan_Forms_for_Fields_on_Surfaces_Application_to_Heart_Fiber_Geometry\figure_2.jpg
  Figure 2 caption: "(From left to right) parameters after 200 Nelder-Mead iterations\
    \ (in radiansvoxel for c ijk and radians for \u03F5 i ), convergence plot for\
    \ c 123 and \u03F5 1 , and computational timings, for different estimation techniques."
  Figure 3 Link: articels_figures_by_rev_year\2015\MaurerCartan_Forms_for_Fields_on_Surfaces_Application_to_Heart_Fiber_Geometry\figure_3.jpg
  Figure 3 caption: "Solution to (29) and (30) for three neighboring spheres \u03C1\
    =1,1.15,1.3 (yellow, blue, red). K 1 models the turning towards the tangent vector\
    \ f 1 . Each column represents a distinct tuple of ( K 2 , K 3 ), which expresses\
    \ the turning of the tangent vector towards f 2 when moving along the f 2 (spreading)\
    \ and f 3 (turning) directions."
  Figure 4 Link: articels_figures_by_rev_year\2015\MaurerCartan_Forms_for_Fields_on_Surfaces_Application_to_Heart_Fiber_Geometry\figure_4.jpg
  Figure 4 caption: Solutions to (29) and (30) on the unit sphere, with initial frames
    shown in red, green, and blue. Column 1 shows a single flow line (yellow) with
    the yellow arrow pointing to the origin. Columns 2-4 shows neighbouring flow lines
    from an initial starting point (red) for various (K1,K2) tuples.
  Figure 5 Link: articels_figures_by_rev_year\2015\MaurerCartan_Forms_for_Fields_on_Surfaces_Application_to_Heart_Fiber_Geometry\figure_5.jpg
  Figure 5 caption: Examples of ellipsoidal solutions to (29) and (32) integrated
    from a flat patch (first row), a spherical patch (second row), and a cylindrical
    patch (third row) for various values of c 131 , c 132 , c 231 , c 232 .
  Figure 6 Link: articels_figures_by_rev_year\2015\MaurerCartan_Forms_for_Fields_on_Surfaces_Application_to_Heart_Fiber_Geometry\figure_6.jpg
  Figure 6 caption: The helix angle alpha H is defined as the angle between the short
    axis of the heart and the projected myofiber direction in a local normal plane.
  Figure 7 Link: articels_figures_by_rev_year\2015\MaurerCartan_Forms_for_Fields_on_Surfaces_Application_to_Heart_Fiber_Geometry\figure_7.jpg
  Figure 7 caption: Principal fiber direction and cardiac frame field boldsymbolf1
    (red), boldsymbolf2 (green), and boldsymbolf3 (blue) for a rat heart, obtained
    from dMRI. The base is located upwards and the apex downwards. Color indicates
    the helix angle of the fiber direction, from -90 degree (green) to +90 degree
    (red).
  Figure 8 Link: articels_figures_by_rev_year\2015\MaurerCartan_Forms_for_Fields_on_Surfaces_Application_to_Heart_Fiber_Geometry\figure_8.jpg
  Figure 8 caption: Endocardium (left) and epicardium (right) euclidean distance,
    ranging from zero (black) to 20 voxels (red), and distance gradient vectors (blue).
  Figure 9 Link: articels_figures_by_rev_year\2015\MaurerCartan_Forms_for_Fields_on_Surfaces_Application_to_Heart_Fiber_Geometry\figure_9.jpg
  Figure 9 caption: Flow lines in the myocardium, found by tracking the first eigenvector
    of diffusion originating in the red boxes. A frame indicates the axial plane of
    the heart (red axis), the transmural direction (green axis), and the direction
    from the base to the apex (blue axis). The fiber geometry expressed in this coordinate
    system corresponds to the connection forms c123 (a), c232 (b) and c131 (c), using
    Fig. 1 for comparison.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Emmanuel Piuze
  Name of the last author: Kaleem Siddiqi
  Number of Figures: 18
  Number of Tables: 2
  Number of authors: 3
  Paper title: 'Maurer-Cartan Forms for Fields on Surfaces: Application to Heart Fiber
    Geometry'
  Publication Date: 2015-03-03 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Effect of Neighborhood Size on Optimized Connection Forms\
      \ (RadiansVoxel) and on Fitting Errors (Radians) for the Rat Dataset as Mean\
      \ \xB1 Standard Deviation"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Effect of Iterative Gaussian Smoothing Applied to f 1 on\
      \ Extrapolation Errors and Connection Forms for the Direct and Optimized Parameter\
      \ Computations in \u03A9 3"
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2408352
- Affiliation of the first author: disney research, pittsburgh, pa
  Affiliation of the last author: school of electronic engineering and computer science,
    queen mary university of london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2015\Transductive_MultiView_ZeroShot_Learning\figure_1.jpg
  Figure 1 caption: An illustration of the projection domain shift problem. Zero-shot
    prototypes are shown as red stars and predicted semantic attribute projections
    (defined in Section 3.2) shown in blue.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Transductive_MultiView_ZeroShot_Learning\figure_2.jpg
  Figure 2 caption: The pipeline of our framework illustrated on the task of classifying
    unlabelled target data into two classes.
  Figure 3 Link: articels_figures_by_rev_year\2015\Transductive_MultiView_ZeroShot_Learning\figure_3.jpg
  Figure 3 caption: "An example of constructing heterogeneous hypergraphs. Suppose\
    \ in the embedding space, we have 14 nodes belonging to seven data points A ,\
    \ B , C , D , E , F and G of two views\u2014view i (rectangle) and view j (circle).\
    \ Data points A , B , C and D , E , F , G belong to two different classes\u2014\
    red and green respectively. The multi-view semantic embedding maximises the correlations\
    \ (connected by black dash lines) between the two views of the same node. Two\
    \ hypergraphs are shown ( G ij at the left and G ji at the right) with the heterogeneous\
    \ hyperedges drawn with redgreen dash ovals for the nodes of redgreen classes.\
    \ Each hyperedge consists of two most similar nodes to the query node."
  Figure 4 Link: articels_figures_by_rev_year\2015\Transductive_MultiView_ZeroShot_Learning\figure_4.jpg
  Figure 4 caption: "(a) Comparing soft and hard dimension weighting of CCA for AwA.\
    \ (b) Contributions of CCA and label propagation on AwA. \u03A8 A and \u03A8 V\
    \ indicate the subspaces of target data from view A and V in \u0393 respectively;\
    \ and LP is conducted on G A and G V respectively. Hand-crafted features are used\
    \ in both experiments."
  Figure 5 Link: articels_figures_by_rev_year\2015\Transductive_MultiView_ZeroShot_Learning\figure_5.jpg
  Figure 5 caption: "Effectiveness of transductive multi-view embedding. (a) zero-shot\
    \ learning on AwA using only hand-crafted features; (b) zero-shot learning on\
    \ AwA using hand-crafted and deep features together; (c) zero-shot learning on\
    \ USAA. [V,A] indicates the concatenation of semantic word and attribute space\
    \ vectors. \u0393(X+V) and \u0393(X+A) mean using low-level+semantic word spaces\
    \ and low-level+attribute spaces respectively to learn the embedding. \u0393(X+V+A)\
    \ indicates using all three views to learn the embedding."
  Figure 6 Link: articels_figures_by_rev_year\2015\Transductive_MultiView_ZeroShot_Learning\figure_6.jpg
  Figure 6 caption: t-SNE Visualisation of (a) OverFeat view ( mathcal Xmathcal O
    ), (b) attribute view ( mathcal Amathcal O ), (c) word vector view ( mathcal Vmathcal
    O ), and (d) transition probability of pairwise nodes computed by Eq. (9) of TMV-HLP
    in ( Gamma (mathcal X+mathcal A+mathcal V)mathcal O,mathcal D ). The unlabelled
    target classes are much more separable in (d).
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.8
  Name of the first author: Yanwei Fu
  Name of the last author: Shaogang Gong
  Number of Figures: 6
  Number of Tables: 3
  Number of authors: 4
  Paper title: Transductive Multi-View Zero-Shot Learning
  Publication Date: 2015-03-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison with the State-of-the-Art on Zero-Shot Learning
      on AwA, USAA and CUB
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Zero-Shot Description of 10 AwA Target Classes
  Table 3 caption:
    table_text: TABLE 3 Zero Prototype Learning on USAA
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2408354
- Affiliation of the first author: "department of computer science, pontificia universidad\
    \ cat\xF3lica de chile, santiago, chile"
  Affiliation of the last author: "department of computer science, pontificia universidad\
    \ cat\xF3lica de chile, santiago, chile"
  Figure 1 Link: articels_figures_by_rev_year\2015\Learning_Shared_Discriminative_and_Compact_Representations_for_Visual_Recognitio\figure_1.jpg
  Figure 1 caption: "Contribution of word k to the categorization of class y= MITcoast\
    \ as measured by C(y,k) in (13). When \u03B1=0 , the \u2113 2 regularizer encourages\
    \ most of the words to be used. On the contrary, when \u03B1=0.1 , the group sparse\
    \ term encourages the use only a few words. This trend repeats for every category\
    \ in every analyzed dataset."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Learning_Shared_Discriminative_and_Compact_Representations_for_Visual_Recognitio\figure_2.jpg
  Figure 2 caption: "Number of classes of dataset 15 scene categories in which word\
    \ k participates. When \u03B1=0 (blue), most words are used by many classes. When\
    \ \u03B1=0.1 (red), each word is used by very few classes."
  Figure 3 Link: articels_figures_by_rev_year\2015\Learning_Shared_Discriminative_and_Compact_Representations_for_Visual_Recognitio\figure_3.jpg
  Figure 3 caption: Activations of the word are marked with red circles, with the
    radii being proportional to the activation score. Using the initial dictionary,
    activations tend to appear on the clouds, but also on some trees. After the training
    process, the same word shows higher activation scores and appears only on the
    clouds.
  Figure 4 Link: articels_figures_by_rev_year\2015\Learning_Shared_Discriminative_and_Compact_Representations_for_Visual_Recognitio\figure_4.jpg
  Figure 4 caption: Activations of a word specialized in the bedroom category. Figs.
    4a and 4b show activations for category bedroom before and after training, respectively.
    Figs. 4c and 4 d show activations for category MITcoast before and after training,
    respectively. Red circles indicate a positive response, while blue circles indicate
    a negative response.
  Figure 5 Link: articels_figures_by_rev_year\2015\Learning_Shared_Discriminative_and_Compact_Representations_for_Visual_Recognitio\figure_5.jpg
  Figure 5 caption: Activations of a word specialized in multiple categories. This
    word seems associated to horizontal edge patterns such as the horizon, a pattern
    shared by categories MITcoast (5 a, 5b) and MITopencountry (5 c, 5d).
  Figure 6 Link: articels_figures_by_rev_year\2015\Learning_Shared_Discriminative_and_Compact_Representations_for_Visual_Recognitio\figure_6.jpg
  Figure 6 caption: A word with low activation for all classes. The strongest activations
    of the word show no clear pattern.
  Figure 7 Link: articels_figures_by_rev_year\2015\Learning_Shared_Discriminative_and_Compact_Representations_for_Visual_Recognitio\figure_7.jpg
  Figure 7 caption: "Performance of the proposed approach as a function of dictionary\
    \ size for different datasets. Near 1,000\u22121,100 words, our method achieves\
    \ a stability point, where performance remains almost constant and sees no benefit\
    \ from adding more words."
  Figure 8 Link: articels_figures_by_rev_year\2015\Learning_Shared_Discriminative_and_Compact_Representations_for_Visual_Recognitio\figure_8.jpg
  Figure 8 caption: "Performance of the proposed approach ( \u03B1=0.1 ), the method\
    \ of [15] ( \u03B1=0 ) and a baseline method with no joint optimization ( \u03B1\
    =0.1 , C \u0398 =0 ), as a function of the number of categories of the MIT67 dataset."
  Figure 9 Link: articels_figures_by_rev_year\2015\Learning_Shared_Discriminative_and_Compact_Representations_for_Visual_Recognitio\figure_9.jpg
  Figure 9 caption: Class performance (hit rate) as a function of the effective number
    of words used. Classes with lower complexity (higher hit rate), tend to use less
    visual words than classes with higher complexity (lower hit rate).
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Hans Lobel
  Name of the last author: Alvaro Soto
  Number of Figures: 9
  Number of Tables: 3
  Number of authors: 3
  Paper title: Learning Shared, Discriminative, and Compact Representations for Visual
    Recognition
  Publication Date: 2015-03-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Effective Number of Visual Words K ~ , Minimum and Maximum
      Number of Words Used per Class y , and Number of Discarded Words as a Function
      of the Dictionary Size K for Four Different Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Categorization Performance of Methods that Use Small Patches
      on Four Different Datasets
  Table 3 caption:
    table_text: TABLE 3 Categorization Performance of Methods that Use Large Patches
      on Four Different Datasets (p.c. = per Class)
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2408349
- Affiliation of the first author: department of automation, tsinghua university,
    beijing, china
  Affiliation of the last author: department of automation, tsinghua university, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2015\Learning_Compact_Binary_Face_Descriptor_for_Face_Recognition\figure_1.jpg
  Figure 1 caption: The pipeline of our proposed feature learning-based face representation
    approach. For each training face image, we first extract PDVs and learn a feature
    mapping using CBFD to project each PDV into low-dimensional binary codes. Then,
    these binary codes are clustered to learn a codebook. For each test image, the
    PDVs are first extracted and encoded into binary codes using the learned feature
    mapping. Lastly, these binary codes are pooled as a histogram feature descriptor
    with the learned codebook.
  Figure 10 Link: articels_figures_by_rev_year\2015\Learning_Compact_Binary_Face_Descriptor_for_Face_Recognition\figure_10.jpg
  Figure 10 caption: "ROC curves of different methods on LFW with the unsupervised\
    \ setting. CBFD (a), CBFD (b), and CBFD (c) are face representations obtained\
    \ by extracting CBFD in 150 \xD7 130 with contour, 150 \xD7 130 without contour,\
    \ and 128 \xD7 128 without contour face images, respectively. CBFD mean (a, b,\
    \ c) is the combination of these three representations."
  Figure 2 Link: articels_figures_by_rev_year\2015\Learning_Compact_Binary_Face_Descriptor_for_Face_Recognition\figure_2.jpg
  Figure 2 caption: The bin distributions of the (a) LBP and (b) our CBFD methods.
    We computed the bin distributions in the LBP histogram and our method in the FERET
    training set, which consists of 1,002 images from 429 subjects. For a fair comparison,
    both of them adopted the same number of bins for feature representation, which
    was set to 59 in this figure. We clearly see from this figure that the histogram
    distribution is uneven for LBP and is more uniform for our CBFD method.
  Figure 3 Link: articels_figures_by_rev_year\2015\Learning_Compact_Binary_Face_Descriptor_for_Face_Recognition\figure_3.jpg
  Figure 3 caption: "One example to show how to extract a pixel difference vectors\
    \ from the original face image. For any pixel in the image, we first identify\
    \ its neighbors in a (2R+1)\xD7(2R+1) space, where R is a parameter to define\
    \ the neighborhood size and it is selected as 1 in this figure for easy illustration.\
    \ Then, the difference between the center point and neighboring pixels is computed\
    \ as the PDV."
  Figure 4 Link: articels_figures_by_rev_year\2015\Learning_Compact_Binary_Face_Descriptor_for_Face_Recognition\figure_4.jpg
  Figure 4 caption: The flow-chart of the CBFD-based face representation and recognition
    method. For each training face, we first divide it into several non-overlapped
    regions and learn the feature filter and dictionary for each region, individually.
    Then, we apply the learned filter and dictionary to extract histogram feature
    for each block and concatenate them into a longer feature vector for face representation.
    Finally, the nearest neighbor classifier is used to measure the sample similarity.
  Figure 5 Link: articels_figures_by_rev_year\2015\Learning_Compact_Binary_Face_Descriptor_for_Face_Recognition\figure_5.jpg
  Figure 5 caption: One example to show how to extract coupled-PDVs from a face pair
    captured in two different modalities. There are two face images captured by the
    web camera and near-infrared camera, respectively, and they are aligned at the
    pixel level so that each pixel at the same position in these two images is aligned.
    Given any position, we extract two PDVs x 1 n and x 2 n to form a coupled-PDV
    for feature learning.
  Figure 6 Link: articels_figures_by_rev_year\2015\Learning_Compact_Binary_Face_Descriptor_for_Face_Recognition\figure_6.jpg
  Figure 6 caption: Several aligned and cropped face examples from the FERET dataset.
  Figure 7 Link: articels_figures_by_rev_year\2015\Learning_Compact_Binary_Face_Descriptor_for_Face_Recognition\figure_7.jpg
  Figure 7 caption: "(a) Rank-one recognition rate of CBFD on FERET versus different\
    \ values of \u03BB 1 and \u03BB 2 . (b) Objective function value of CBFD versus\
    \ different number of iterations on FERET. (c) Rank-one recognition rate of CBFD\
    \ on FERET versus different values of the binary codes length."
  Figure 8 Link: articels_figures_by_rev_year\2015\Learning_Compact_Binary_Face_Descriptor_for_Face_Recognition\figure_8.jpg
  Figure 8 caption: Several aligned and cropped face examples from the CAS-PEAL-R1
    dataset.
  Figure 9 Link: articels_figures_by_rev_year\2015\Learning_Compact_Binary_Face_Descriptor_for_Face_Recognition\figure_9.jpg
  Figure 9 caption: "Several aligned and cropped face examples with different similarity\
    \ transformations from the deep funneled LFW dataset. (a) 150 \xD7 130 with contour,\
    \ (b) 150 \xD7 130 without contour, and (c) 128 \xD7 128 without contour."
  First author gender probability: 0.95
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.59
  Name of the first author: Jiwen Lu
  Name of the last author: Jie Zhou
  Number of Figures: 17
  Number of Tables: 13
  Number of authors: 4
  Paper title: Learning Compact Binary Face Descriptor for Face Recognition
  Publication Date: 2015-03-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Rank-One Recognition Rates (Percent) Comparison with State-of-the-Art
      Feature Descriptors with the Standard FERET Evaluation Protocol
  Table 10 caption:
    table_text: TABLE 10 Comparisons of the Mean Verification Rate and Standard Error
      (Percent) with the State-of-the-Art Results on LFW under the Image Restricted
      Setting with Label-Free Outside Data
  Table 2 caption:
    table_text: TABLE 2 Rank-One Recognition Rates (Percent) of Our CBFD and Three
      Other Facial Feature Descriptors When PCA and WPCA Are Applied with the Standard
      FERET Evaluation Protocol
  Table 3 caption:
    table_text: TABLE 3 Rank-One Recognition Rates (Percent) of Our CBFD Method and
      Other Alternative Baselines with the Standard FERET Evaluation Protocol
  Table 4 caption:
    table_text: TABLE 4 Rank-One Recognition Rates (Percent) of Our CBFD Method and
      Other Real-Valued Codes Feature Learning Methods with the Standard FERET Evaluation
      Protocol
  Table 5 caption:
    table_text: TABLE 5 Rank-One Recognition Rates (Percent) of Our CBFD Method and
      Four Existing Binary Codes Learning Methods with the Standard FERET Evaluation
      Protocol
  Table 6 caption:
    table_text: TABLE 6 Computational Time (ms) Comparison of Different Face Feature
      Representation Methods
  Table 7 caption:
    table_text: TABLE 7 Rank-One Recognition Rates (Percent) Comparison with the State-of-the-Art
      Facial Descriptors Tested with the Standard CAS-PEAL-R1 Evaluation Protocol
  Table 8 caption:
    table_text: TABLE 8 AUC (Percent) Comparisons with the State-of-the-Art Methods
      on LFW with the Unsupervised Setting
  Table 9 caption:
    table_text: TABLE 9 AUC (Percent) Comparisons with the Existing Face Feature Descriptors
      on LFW with the Unsupervised Setting
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2408359
- Affiliation of the first author: institute of software technology and interactive
    systems, vienna university of technology, vienna, austria
  Affiliation of the last author: institute of software technology and interactive
    systems, vienna university of technology, vienna, austria
  Figure 1 Link: articels_figures_by_rev_year\2015\An_Efficient_Algorithm_for_Calculating_the_Exact_Hausdorff_Distance\figure_1.jpg
  Figure 1 caption: "x (i\u22121) is the point already minimized in the previous iteration\
    \ where cmax was found to be the current maximum (temporary HD). Point x i is\
    \ being currently minimized by calculating its distances to B . Points y 1 \u2026\
    \ y 8 \u2208B are numbered according to their distance to x i . An iteration order\
    \ beginning with y 1 , y 2 or y 3 is good because it will cause an immediate break\
    \ whereas an iteration order beginning with other points is worse because the\
    \ scan will continue."
  Figure 10 Link: articels_figures_by_rev_year\2015\An_Efficient_Algorithm_for_Calculating_the_Exact_Hausdorff_Distance\figure_10.jpg
  Figure 10 caption: The performance of the proposed algorithm in comparing volumes
    with grid size increased to 350 times 350 times 350 voxels. No runtime plot is
    shown for the ITK algorithm because it failed in all cases with a memory allocation
    error.
  Figure 2 Link: articels_figures_by_rev_year\2015\An_Efficient_Algorithm_for_Calculating_the_Exact_Hausdorff_Distance\figure_2.jpg
  Figure 2 caption: The probability density function of a geometrical distribution.
  Figure 3 Link: articels_figures_by_rev_year\2015\An_Efficient_Algorithm_for_Calculating_the_Exact_Hausdorff_Distance\figure_3.jpg
  Figure 3 caption: "Distribution of pairwise distances assuming a normal distribution\
    \ for illustration. (A) Position of the Hausdorff distance h relative to the distribution\
    \ affects p because cmax\u2264h . (B) h is large and cmax can reach large values\
    \ thereby increasing p . (C) h is small and cmax remains small thereby decreasing\
    \ p ."
  Figure 4 Link: articels_figures_by_rev_year\2015\An_Efficient_Algorithm_for_Calculating_the_Exact_Hausdorff_Distance\figure_4.jpg
  Figure 4 caption: Progress of cmax in the first 10 thousand iterations (outer loop)
    when comparing two real brain tumor segmentations. Note that only 10 thousand
    of 15.6 million iterations in total are shown and thus the curve does not reach
    the HD.
  Figure 5 Link: articels_figures_by_rev_year\2015\An_Efficient_Algorithm_for_Calculating_the_Exact_Hausdorff_Distance\figure_5.jpg
  Figure 5 caption: The average number of iterations in the inner loop until the early
    break at each iteration of the outer loop. Values are recorded from measuring
    the HD between 1,000 pairs of trajectories generated from the road network of
    Oldenburg. Each trajectory contains 2,000 points. Iterations of the outer loop
    are on the x-axis and the number of iterations in the inner loop averaged over
    all pairs on the y-axis.
  Figure 6 Link: articels_figures_by_rev_year\2015\An_Efficient_Algorithm_for_Calculating_the_Exact_Hausdorff_Distance\figure_6.jpg
  Figure 6 caption: Convergence behaviour of the temporary HD ( cmax ) along the iterations
    of the outer loop. Iterations of the outer loop are on the x-axis and the frequencies
    of cmax values exceeding 90 and 99 percent of the corresponding HD at each iteration
    are on the y-axis.
  Figure 7 Link: articels_figures_by_rev_year\2015\An_Efficient_Algorithm_for_Calculating_the_Exact_Hausdorff_Distance\figure_7.jpg
  Figure 7 caption: Comparison between the performance of the proposed algorithm and
    the ITK algorithm in validating 240 real brain tumor segmentations against the
    corresponding ground truth. The set size in kilo voxels is on the horizontal axis
    and the run time in seconds is on the vertical axis. The grid size varies from
    125 times 125 times 125 to 250 times 250 times 250 voxels.
  Figure 8 Link: articels_figures_by_rev_year\2015\An_Efficient_Algorithm_for_Calculating_the_Exact_Hausdorff_Distance\figure_8.jpg
  Figure 8 caption: Comparison between the performance of the proposed algorithm and
    the ITK algorithm in comparing 300 pairs of volumes selected randomly so that
    the overlap between volumes in each pair is zero. The set size in voxels is on
    the horizontal axis and the runtime in seconds on the vertical axis. All volumes
    have a unified grid size of 250 times 250 times 250 voxels.
  Figure 9 Link: articels_figures_by_rev_year\2015\An_Efficient_Algorithm_for_Calculating_the_Exact_Hausdorff_Distance\figure_9.jpg
  Figure 9 caption: Performance comparison between the proposed algorithm and the
    ITK algorithm in comparing enlarged volumes. The set size in kilo voxels is on
    the horizontal axis and the runtime in seconds on the vertical axis. All volumes
    have a unified grid size of 250 times 250 times 250 voxels.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Abdel Aziz Taha
  Name of the last author: Allan Hanbury
  Number of Figures: 13
  Number of Tables: 1
  Number of authors: 2
  Paper title: An Efficient Algorithm for Calculating the Exact Hausdorff Distance
  Publication Date: 2015-03-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Result Summary for Experiments on Medical Images of Varying
      Sizes and Characteristics Where n1...n2 Is the Size Range of the Compared Point
      Sets, L, B, H Are the Grid Dimensions for Medical Volumes and the Time Values
      Are the Average Execution Time for Calculating the HD
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2408351
- Affiliation of the first author: department of mechanical and control engineering,
    graduate school of science and engineering, tokyo institute of technology, tokyo,
    japan
  Affiliation of the last author: center for machine perception, department of cybernetics,
    faculty of electrical engineering, czech technical university in prague
  Figure 1 Link: articels_figures_by_rev_year\2015\Visual_Place_Recognition_with_Repetitive_Structures\figure_1.jpg
  Figure 1 caption: 'Overview of visual place recognition with repetitive structures.
    Left: We detect groups of repeated local features (overlaid in colors). Middle:
    Repetitive features (shown in red) implicitly provide soft-assignment to multiple
    visual words (here A and C). Right: Truncating large weights (shown in red) in
    the bag-of-visual-word vectors prevents repetitions from dominating the matching
    score.'
  Figure 10 Link: articels_figures_by_rev_year\2015\Visual_Place_Recognition_with_Repetitive_Structures\figure_10.jpg
  Figure 10 caption: Sensitivity to different parameters on the Pittsburgh dataset.
    The fraction of correctly recognized queries (Recall, y-axis) versus the number
    of top N retrieved database images (x-axis) for different parameter setup.
  Figure 2 Link: articels_figures_by_rev_year\2015\Visual_Place_Recognition_with_Repetitive_Structures\figure_2.jpg
  Figure 2 caption: "Examples of detected repetitive patterns of local invariant features\
    \ (\u201Crepttiles\u201D). The different repetitive patterns detected in each\
    \ image are shown in different colors. The detection is robust against local deformation\
    \ of the repeated element and makes only weak assumptions on the spatial structure\
    \ of the repetition."
  Figure 3 Link: articels_figures_by_rev_year\2015\Visual_Place_Recognition_with_Repetitive_Structures\figure_3.jpg
  Figure 3 caption: "Examples of detected repetitive patterns of local invariant features\
    \ (\u201Crepttiles\u201D) in images from the INRIA Holidays dataset [8]. The different\
    \ repetitive patterns detected in each image are shown in different colors. The\
    \ color indicates the number of features in each group (red indicates large and\
    \ blue indicates small groups). Note the variety of detected repetitive structures\
    \ such as different building facades, trees, indoor objects, window tiles or floor\
    \ patterns."
  Figure 4 Link: articels_figures_by_rev_year\2015\Visual_Place_Recognition_with_Repetitive_Structures\figure_4.jpg
  Figure 4 caption: "Examples of adaptive soft-assignment with \u201Crepttile\u201D\
    \ detection in images from the Pittsburgh dataset. (Top) The repttiles composed\
    \ from more than 20 image features are shown in different colors (red indicates\
    \ large and blue indicates small groups, similarly to Fig. 3). (Bottom) The number\
    \ of visual word assignments of each feature is adaptively defined by the number\
    \ of features in the repttile as in Equation (7). The color indicates the number\
    \ of multiple assignments, red =1 , green =2 and blue =3 . Features belonging\
    \ to larger repttiles are assigned to fewer visual words (red) but discriminative\
    \ features (blue and green) are assigned to multiple visual words (up to 3)."
  Figure 5 Link: articels_figures_by_rev_year\2015\Visual_Place_Recognition_with_Repetitive_Structures\figure_5.jpg
  Figure 5 caption: "Examples of adaptive soft-assignment with \u201Crepttile\u201D\
    \ detection in images from the San Francisco dataset. See the caption of Fig.\
    \ 4 for details."
  Figure 6 Link: articels_figures_by_rev_year\2015\Visual_Place_Recognition_with_Repetitive_Structures\figure_6.jpg
  Figure 6 caption: Evaluation on the Pittsburgh dataset. (a) Locations of query (blue
    dots) and database (gray dots) images. (b-c) The fraction of correctly recognized
    queries (Recall, y-axis) versus the number of top N retrieved database images
    ( x-axis) for the proposed method (AA thr-idf) compared to several other methods.
  Figure 7 Link: articels_figures_by_rev_year\2015\Visual_Place_Recognition_with_Repetitive_Structures\figure_7.jpg
  Figure 7 caption: Evaluation on the San Francisco dataset. (a) Locations of query
    (blue dots) and database (gray dots) images. (b-c) The fraction of correctly recognized
    queries (Recall, y-axis) versus the number of top N retrieved database images
    ( x-axis) for the proposed method (AA thr-idf) compared to several other methods.
  Figure 8 Link: articels_figures_by_rev_year\2015\Visual_Place_Recognition_with_Repetitive_Structures\figure_8.jpg
  Figure 8 caption: Examples of place recognition results on the Pittsburgh dataset.
    Each figure shows the query image (left column) and the three best matching database
    images (second to fourth column) using the baseline burstiness method [8] (top
    row) and the proposed repttile detection and adaptive assignment (bottom row).
    The green borders indicate correctly recognized images. The orange dots show the
    visual word matches between the query image (first column) and the best matching
    database image (second column). The visual word matches are also displayed on
    the second and third best matching images. Please note that for improved clarity,
    the matching visual words in the query are not shown for the second and third
    match.
  Figure 9 Link: articels_figures_by_rev_year\2015\Visual_Place_Recognition_with_Repetitive_Structures\figure_9.jpg
  Figure 9 caption: Examples of place recognition results on the San Francisco dataset.
    See the caption of Fig. 8 for details.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Akihiko Torii
  Name of the last author: Tomas Pajdla
  Number of Figures: 14
  Number of Tables: 5
  Number of authors: 4
  Paper title: Visual Place Recognition with Repetitive Structures
  Publication Date: 2015-03-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Scalability and Place Recognition Performance on the Pittsburgh
      Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Scalability and Place Recognition Performance on the San Francisco
      Dataset
  Table 3 caption:
    table_text: TABLE 3 Re-Ranking with Geometric Verification on the Pittsburgh Dataset
  Table 4 caption:
    table_text: TABLE 4 Re-Ranking with Geometric Verification on the San Francisco
      Dataset
  Table 5 caption:
    table_text: TABLE 5 mAP on INRIA Holidays and Oxford Building Datasets
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2409868
