- Affiliation of the first author: department of electrical engineering, sharif university
    of technology, tehran, iran
  Affiliation of the last author: department of electrical engineering, sharif university
    of technology, tehran, iran
  Figure 1 Link: articels_figures_by_rev_year\2022\Berrut_Approximated_Coded_Computing_Straggler_Resistance_Beyond_Polynomial_Compu\figure_1.jpg
  Figure 1 caption: "The value of Lebesgue function versus x\u2208[\u22121,1] in Berruts\
    \ rational interpolant with different parameters value N and s ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Berrut_Approximated_Coded_Computing_Straggler_Resistance_Beyond_Polynomial_Compu\figure_2.jpg
  Figure 2 caption: "Expectation of the relative error of the 100 polynomial functions\
    \ f of degree degf using the proposed scheme. Note that in many cases, the number\
    \ of non-straggling worker nodes is less than (K\u22121)degf+1 , and still the\
    \ error is reasonable."
  Figure 3 Link: articels_figures_by_rev_year\2022\Berrut_Approximated_Coded_Computing_Straggler_Resistance_Beyond_Polynomial_Compu\figure_3.jpg
  Figure 3 caption: Comparison between the impact of using the proposed scheme and
    using the equidistant points in the expectation of the relative error of a set
    of 100 polynomial functions f of degree degf=25 , where N=500 and K=20 .
  Figure 4 Link: articels_figures_by_rev_year\2022\Berrut_Approximated_Coded_Computing_Straggler_Resistance_Beyond_Polynomial_Compu\figure_4.jpg
  Figure 4 caption: Approximation of the function f=xsinx in the input data set using
    BACC, where K=20 , N=60 and s=20 .
  Figure 5 Link: articels_figures_by_rev_year\2022\Berrut_Approximated_Coded_Computing_Straggler_Resistance_Beyond_Polynomial_Compu\figure_5.jpg
  Figure 5 caption: "Expectation of the relative error of the approximation of the\
    \ function f=xsinx for x\u2208[\u22121,1] using the proposed scheme, for different\
    \ values of N and K ."
  Figure 6 Link: articels_figures_by_rev_year\2022\Berrut_Approximated_Coded_Computing_Straggler_Resistance_Beyond_Polynomial_Compu\figure_6.jpg
  Figure 6 caption: A simple comparison among three distributed learning settings
    (a) Berrut Coded Computing, (b) the scheme without data redundancy, and (c) the
    data replication scheme, all with N=3 , K=3 , s=1 where each tildexi is a particular
    linear combination of xj,jin [1:3] as mentioned in the proposed scheme.
  Figure 7 Link: articels_figures_by_rev_year\2022\Berrut_Approximated_Coded_Computing_Straggler_Resistance_Beyond_Polynomial_Compu\figure_7.jpg
  Figure 7 caption: Comparison of the test accuracy of BACC, data replication scheme,
    and the scheme without data redundancy on (a) MNIST, (b) Fashion-MNIST, and (c)
    Cifar-10 on LeNet architecture, in a distributed system with (I) N=3 , s=1 and
    (II) N=5 , s=2 .
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Tayyebeh Jahani-Nezhad
  Name of the last author: Mohammad Ali Maddah-Ali
  Number of Figures: 7
  Number of Tables: 0
  Number of authors: 2
  Paper title: 'Berrut Approximated Coded Computing: Straggler Resistance Beyond Polynomial
    Computing'
  Publication Date: 2022-02-14 00:00:00
  Table 1 caption: Not Available
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3151434
- Affiliation of the first author: department of electrical and computer engineering,
    and ingenuity labs research institute, queens university, kingston, on, canada
  Affiliation of the last author: department of electrical and computer engineering,
    and ingenuity labs research institute, queens university, kingston, on, canada
  Figure 1 Link: articels_figures_by_rev_year\2022\Deep_Gait_Recognition_A_Survey\figure_1.jpg
  Figure 1 caption: The number of gait recognition papers published after 2015 using
    non-deep (orange) and deep (blue) gait recognition methods. These papers have
    been published in top-tier journals and conferences in the field. Journal publications
    include IEEE Transactions ( 19% ) including T-PAMI, T-IP, T-IFS, T-MM, T-CSVT,
    and T-Biom, as well as other top journals ( 24% ) such as Pattern Recognition
    and Pattern Recognition Letter. Conference publications include highly ranked
    computer vision and machine learning conferences ( 22% ) including CVPR, AAAI,
    ICCV, ECCV, ACCV, BMVC, as well as other top relevant conferences ( 35% ) such
    as ICASSP, ICIP, ICPR, ICME, ACM Multimedia, and IJCB. The figure shows clear
    opposing trends between the two approaches, indicating that, unsurprisingly, deep
    learning methods have become the dominant approach is recent years.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Deep_Gait_Recognition_A_Survey\figure_2.jpg
  Figure 2 caption: The evolution of deep gait recognition methods.
  Figure 3 Link: articels_figures_by_rev_year\2022\Deep_Gait_Recognition_A_Survey\figure_3.jpg
  Figure 3 caption: An overview of test protocols is presented. These protocols can
    be categorized into subject-dependent or subject-independent according to whether
    test subjects appear in the training set or not.
  Figure 4 Link: articels_figures_by_rev_year\2022\Deep_Gait_Recognition_A_Survey\figure_4.jpg
  Figure 4 caption: 'Our taxonomy consisting of 4 dimensions: body representation,
    temporal representation, feature representation, and neural architecture.'
  Figure 5 Link: articels_figures_by_rev_year\2022\Deep_Gait_Recognition_A_Survey\figure_5.jpg
  Figure 5 caption: 'Overview of temporal representations. Generating templates in:
    (a) the initial layer of a deep network; (b) an intermediate layer of the network
    after several convolution and pooling layers. Illustration of (c) GEI [85], (d)
    CGI [86], (e) FDEI [87], (f) GEnI [88], and (g) PEI [89] temporal gait templates.'
  Figure 6 Link: articels_figures_by_rev_year\2022\Deep_Gait_Recognition_A_Survey\figure_6.jpg
  Figure 6 caption: 'Three different approaches for using RNNs in the context of deep
    gait recognition systems: (a) RNNs directly learn from the movement of joint positions;
    (b) RNNs are combined with CNNs; and (c) RNNs recurrently learn the relationships
    between partial representations in gait templates.'
  Figure 7 Link: articels_figures_by_rev_year\2022\Deep_Gait_Recognition_A_Survey\figure_7.jpg
  Figure 7 caption: (a) Visualization of deep gait recognition methods, according
    to three levels of our taxonomy and publication date; (b) The frequency of different
    neural architectures, loss functions, and gait datasets used in the literature.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.94
  Name of the first author: Alireza Sepas-Moghaddam
  Name of the last author: Ali Etemad
  Number of Figures: 7
  Number of Tables: 6
  Number of authors: 2
  Paper title: 'Deep Gait Recognition: A Survey'
  Publication Date: 2022-02-15 00:00:00
  Table 1 caption: TABLE 1 Summary of Well-Known Gait Datasets Used in the Literature
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Overview of Recent CNN Architectures Adopted for Deep Gait
    Recognition
  Table 3 caption: TABLE 3 Classification of Deep Gait Recognition Methods Based on
    Our Proposed Taxonomy
  Table 4 caption: TABLE 4 State-of-the-Art Results on CASIA-B Dataset [32]
  Table 5 caption: TABLE 5 State-of-the-Art Results on OU-ISIR [65] Dataset
  Table 6 caption: TABLE 6 State-of-the-Art Results on OU-MVLP [68] Dataset
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3151865
- Affiliation of the first author: huawei noahs ark lab, beijing, china
  Affiliation of the last author: school of computer science, faculty of engineering,
    university of sydney, darlington, nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2022\A_Survey_on_Vision_Transformer\figure_1.jpg
  Figure 1 caption: Key milestones in the development of transformer. The vision transformer
    models are marked in red.
  Figure 10 Link: articels_figures_by_rev_year\2022\A_Survey_on_Vision_Transformer\figure_10.jpg
  Figure 10 caption: Diagram of IPT architecture (image from [19]).
  Figure 2 Link: articels_figures_by_rev_year\2022\A_Survey_on_Vision_Transformer\figure_2.jpg
  Figure 2 caption: Structure of the original transformer (image from [9]).
  Figure 3 Link: articels_figures_by_rev_year\2022\A_Survey_on_Vision_Transformer\figure_3.jpg
  Figure 3 caption: (Left) Self-attention process. (Right) Multi-head attention. The
    image is from [9].
  Figure 4 Link: articels_figures_by_rev_year\2022\A_Survey_on_Vision_Transformer\figure_4.jpg
  Figure 4 caption: A taxonomy of backbone using convolution and attention.
  Figure 5 Link: articels_figures_by_rev_year\2022\A_Survey_on_Vision_Transformer\figure_5.jpg
  Figure 5 caption: The framework of ViT (image from [15]).
  Figure 6 Link: articels_figures_by_rev_year\2022\A_Survey_on_Vision_Transformer\figure_6.jpg
  Figure 6 caption: FLOPs and throughput comparison of representative CNN and vision
    transformer models.
  Figure 7 Link: articels_figures_by_rev_year\2022\A_Survey_on_Vision_Transformer\figure_7.jpg
  Figure 7 caption: General framework of transformer-based object detection.
  Figure 8 Link: articels_figures_by_rev_year\2022\A_Survey_on_Vision_Transformer\figure_8.jpg
  Figure 8 caption: The overall architecture of DETR (image from [16]).
  Figure 9 Link: articels_figures_by_rev_year\2022\A_Survey_on_Vision_Transformer\figure_9.jpg
  Figure 9 caption: A generic framework for transformer in image generation.
  First author gender probability: 0.93
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Kai Han
  Name of the last author: Dacheng Tao
  Number of Figures: 13
  Number of Tables: 4
  Number of authors: 13
  Paper title: A Survey on Vision Transformer
  Publication Date: 2022-02-18 00:00:00
  Table 1 caption: TABLE 1 Representative Works of Vision Transformers
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 ImageNet Result Comparison of Representative CNN and Vision
    Transformer Models
  Table 3 caption: TABLE 3 Comparison of Different Transformer-Based Object Detectors
    on COCO 2017 Val Set
  Table 4 caption: TABLE 4 List of Representative Compressed Transformer-Based Models
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3152247
- Affiliation of the first author: college of electronic science and technology, national
    university of defense technology, changsha, china
  Affiliation of the last author: college of electronic science and technology, national
    university of defense technology, changsha, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Disentangling_Light_Fields_for_SuperResolution_and_Disparity_Estimation\figure_1.jpg
  Figure 1 caption: The SAI and EPI representation of 4D LFs.
  Figure 10 Link: articels_figures_by_rev_year\2022\Disentangling_Light_Fields_for_SuperResolution_and_Disparity_Estimation\figure_10.jpg
  Figure 10 caption: "An overview of our DistgDisp. The input 9\xD79 SAIs are first\
    \ re-organized into a MacPI and fed to 8 spatial residual blocks for spatial information\
    \ incorporation. Then, a series of disparity-selective angular feature extractors\
    \ are introduced to disentangle the disparity information from the MacPI features\
    \ to generate cost volumes. The generated cost volumes are further aggregated\
    \ via a 3D hourglass module to regress the final disparity."
  Figure 2 Link: articels_figures_by_rev_year\2022\Disentangling_Light_Fields_for_SuperResolution_and_Disparity_Estimation\figure_2.jpg
  Figure 2 caption: "An illustration of the relationship between SAI and MacPI representations.\
    \ To convert an SAI array into a MacPI, pixels at the same spatial locations of\
    \ each SAI need to be organized according to their angular coordinates to generate\
    \ a macro-pixel (the 5\xD75 patch on the top-right corner of (b)). Then, the generated\
    \ macro-pixels need to be organized according to their spatial coordinates. See\
    \ details in the Appendix, which can be found on the Computer Society Digital\
    \ Library at http:doi.ieeecomputersociety.org10.1109TPAMI.2022.3152488."
  Figure 3 Link: articels_figures_by_rev_year\2022\Disentangling_Light_Fields_for_SuperResolution_and_Disparity_Estimation\figure_3.jpg
  Figure 3 caption: An illustration of the spatial, angular, and EPI feature extractors.
    Here, an LF of size U = V = 3 (i.e., A = 3), H = 3, W = 4 is used as a toy example.
    For better visualization of the MacPI, different macro-pixels are paint with different
    background colors while pixels from different views are denoted with different
    characters. The proposed feature extractors can disentangle LFs into different
    subspaces, i.e., an SFE convolves pixels from the same views, an AFE convolves
    pixels from the same macro-pixel, and an EFE convolves pixels on EPIs.
  Figure 4 Link: articels_figures_by_rev_year\2022\Disentangling_Light_Fields_for_SuperResolution_and_Disparity_Estimation\figure_4.jpg
  Figure 4 caption: An overview of our DistgSSR network.
  Figure 5 Link: articels_figures_by_rev_year\2022\Disentangling_Light_Fields_for_SuperResolution_and_Disparity_Estimation\figure_5.jpg
  Figure 5 caption: "Visual comparisons for 4\xD7SR. The super-resolved center view\
    \ images and horizontal EPIs are shown. The PSNR and SSIM scores achieved by different\
    \ methods on the presented scenes are reported below the zoom-in regions."
  Figure 6 Link: articels_figures_by_rev_year\2022\Disentangling_Light_Fields_for_SuperResolution_and_Disparity_Estimation\figure_6.jpg
  Figure 6 caption: "Depth estimation results achieved by SPO [62] using 4\xD7SR LF\
    \ images produced by different SR methods. The mean square error multiplied with\
    \ 100 (i.e., MSE\xD7100) was used as the quantitative metric. Note that, the accuracy\
    \ is improved by using the LF images produced by our DistgSSR."
  Figure 7 Link: articels_figures_by_rev_year\2022\Disentangling_Light_Fields_for_SuperResolution_and_Disparity_Estimation\figure_7.jpg
  Figure 7 caption: "An overview of our DistgASR network. Here, we take the 2 \xD7\
    2\u21927\xD7 7 angular SR as an example to illustrate the network structure."
  Figure 8 Link: articels_figures_by_rev_year\2022\Disentangling_Light_Fields_for_SuperResolution_and_Disparity_Estimation\figure_8.jpg
  Figure 8 caption: "Visual results achieved by different methods on scenes monasRoon\
    \ (top) [76] and IMG1555 (bottom) [47] for 2 \xD72\u21927\xD7 7 angular SR. Here,\
    \ we show the error maps of the reconstructed center view images, along with two\
    \ zoom-in regions and horizontal vertical EPIs."
  Figure 9 Link: articels_figures_by_rev_year\2022\Disentangling_Light_Fields_for_SuperResolution_and_Disparity_Estimation\figure_9.jpg
  Figure 9 caption: "Disparity estimation results achieved by SPO [62] using LF images\
    \ produced by different angular SR methods. The mean square error multiplied with\
    \ 100 (i.e., MSE\xD7100) was used as the quantitative metric. The accuracy is\
    \ improved by using the LF images produced by our DistgASR."
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.91
  Name of the first author: Yingqian Wang
  Name of the last author: Yulan Guo
  Number of Figures: 15
  Number of Tables: 8
  Number of authors: 7
  Paper title: Disentangling Light Fields for Super-Resolution and Disparity Estimation
  Publication Date: 2022-02-18 00:00:00
  Table 1 caption: "TABLE 1 PSNR Results Achieved by Several Variants of our DistgSSR\
    \ for 2\xD7 2\xD7SR"
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 PSNR Results Achieved by our DistgSSR With Different Angular\
    \ Resolutions for 2\xD7 2\xD7SR"
  Table 3 caption: "TABLE 3 PSNR SSIM Values Achieved by Different Methods for 2\xD7\
    \ 2\xD7 and 4\xD7 4\xD7SR"
  Table 4 caption: "TABLE 4 Comparisons of the Number of Parameters (Param.) and FLOPs\
    \ for 2\xD7 and 4\xD7SR"
  Table 5 caption: "TABLE 5 Comparative PSNR Results Achieved by Several Variants\
    \ of our DistgASR for 2\xD72\u21927\xD77 2\xD72\u21927\xD77 ASR"
  Table 6 caption: "TABLE 6 PSNR Values Achieved by Different Methods for 2\xD72\u2192\
    7\xD77 2\xD72\u21927\xD77 Angular SR"
  Table 7 caption: TABLE 7 Comparative Results Achieved by Different LF Disparity
    Estimation Methods on the HCI Benchmark
  Table 8 caption: "TABLE 8 Running Time Achieved by Using the \u201CShift-and-Concat\u201D\
    \ Approach and the Proposed DS-AFE for Cost Volume Construction"
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3152488
- Affiliation of the first author: school of electronic engineering, xidian university,
    xian, china
  Affiliation of the last author: department of computer science and engineering,
    texas a&m university, college station, tx, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\Towards_Improved_and_Interpretable_Deep_Metric_Learning_via_Attentive_Grouping\figure_1.jpg
  Figure 1 caption: "An illustration of our proposed deep metric learning framework.\
    \ Our A-grouping module takes the feature maps generated by CNNs as the input\
    \ and outputs multi-group features. In this example, the number of groups is set\
    \ to three. The key and value tensors (blue cube) are first computed by two independent\
    \ linear transformations of the feature maps, which is implemented by the 1\xD7\
    1 convolution. Then the position-wise weight maps are generated for each group\
    \ by attending the corresponding query vector to the group-shared key. Finally,\
    \ we compute the weighted summation over the value tensor with group-specific\
    \ weight maps to obtain the output feature vectors for different groups. We provide\
    \ an example in the dashed box to explain the computation procedure for group\
    \ 3. To train the model, the metric loss is applied to each group separately,\
    \ whereas the diversity loss is computed across every two groups."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Towards_Improved_and_Interpretable_Deep_Metric_Learning_via_Attentive_Grouping\figure_2.jpg
  Figure 2 caption: Examples showing the translation invariance property of our method.
    Each image pair contains the raw image (left) and the attention weight map visualization
    (right). Our method can capture the key object components in different locations.
  Figure 3 Link: articels_figures_by_rev_year\2022\Towards_Improved_and_Interpretable_Deep_Metric_Learning_via_Attentive_Grouping\figure_3.jpg
  Figure 3 caption: Visualization results of the CUB-200-2011 dataset. Each image
    pair composes of the raw image on the left side and the attentive map visualization
    on the right side. The highlighted regions indicate the spatial locations that
    different groups are detecting. The four groups in our A-grouping focus on different
    patterns of the bird images regardless of the spatial locations of these patterns.
    These patterns are (a) the body, (b) the head, (c) the neck, and (d) the background.
  Figure 4 Link: articels_figures_by_rev_year\2022\Towards_Improved_and_Interpretable_Deep_Metric_Learning_via_Attentive_Grouping\figure_4.jpg
  Figure 4 caption: Plots of the Recall 1 metric from epoch 0 to epoch 150 on CUB-200-2011.
    Different grouping modules are shown in different colors while solid and dashed
    lines represent the curves for training and testing respectively. It is obvious
    that our proposed method has the smallest gap between the training curve and the
    testing curve.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.78
  Name of the first author: Xinyi Xu
  Name of the last author: Shuiwang Ji
  Number of Figures: 4
  Number of Tables: 5
  Number of authors: 5
  Paper title: Towards Improved and Interpretable Deep Metric Learning via Attentive
    Grouping
  Publication Date: 2022-02-18 00:00:00
  Table 1 caption: TABLE 1 Three Loss Functions Used in the Experiments and Their
    Corresponding Hyperparameters
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparisons With the State-of-the-Art Methods on the CUB-200-2011,
    Cars-196, and Stanford Online Products (SOP) Datasets
  Table 3 caption: TABLE 3 Comparisons Between Our A-Grouping Method, the M-Grouping,
    G-Grouping, and C-Grouping Methods on the CUB-200-2011 and Cars-196 Datasets
  Table 4 caption: TABLE 4 Comparisons Between Our A-Grouping Method, the M-Grouping,
    G-Grouping, and C-Grouping Methods on the Stanford Online Products Dataset
  Table 5 caption: TABLE 5 The Comparison of Numerical Correlation Between Different
    Groups of M-Grouping, C-Grouping, G-Grouping, and A-Grouping
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3152495
- Affiliation of the first author: wangxuan institute of computer technology, peking
    university, beijing, china
  Affiliation of the last author: wangxuan institute of computer technology, peking
    university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Unsupervised_Face_Detection_in_the_Dark\figure_1.jpg
  Figure 1 caption: Qualitative results and our adaptation paradigm for face detection
    in the dark. In comparison to DSFD [4] on original low light and the LIME [9]-enhanced
    versions, our model recognizes faces more accurately. Here, the color of the bounding
    boxes represents the confidence of recognition, with yellow indicating higher
    confidence.
  Figure 10 Link: articels_figures_by_rev_year\2022\Unsupervised_Face_Detection_in_the_Dark\figure_10.jpg
  Figure 10 caption: The color distribution of different domains. Each spot represents
    the average pixel value of an image patch. We use blue to represent E(L) samples,
    and red to represent H and D(H) samples.
  Figure 2 Link: articels_figures_by_rev_year\2022\Unsupervised_Face_Detection_in_the_Dark\figure_2.jpg
  Figure 2 caption: "Principles of different adaptive low-light detection methods.\
    \ L : low-light data. H : normal light data. Compared with existing methods (b)\u2013\
    (d), our method narrows low-level and high-level gaps by comprehensive learning\
    \ constraints and therefore is more effective and powerful."
  Figure 3 Link: articels_figures_by_rev_year\2022\Unsupervised_Face_Detection_in_the_Dark\figure_3.jpg
  Figure 3 caption: Representative samples from WIDER FACE and DARK FACE. We enhance
    DARK FACE for better visibility.
  Figure 4 Link: articels_figures_by_rev_year\2022\Unsupervised_Face_Detection_in_the_Dark\figure_4.jpg
  Figure 4 caption: "Curves under different parameters. For each function, we use\
    \ a variety of parameters to show its representation range. Among them, tangent\
    \ has the risk of becoming a horizontal line; quadratic, cubic, circle, sine,\
    \ and arcsine cannot cover the entire [0,1]\xD7[0,1] space."
  Figure 5 Link: articels_figures_by_rev_year\2022\Unsupervised_Face_Detection_in_the_Dark\figure_5.jpg
  Figure 5 caption: "Enhancement results by different curves. (b) Tangent curve suffers\
    \ from severe distortion. (c)\u2013(g) Quadratic, cubic, circle, sine, and arcsine\
    \ curves fail to lighten the image. (h)\u2013(l) Logarithmic, reciprocal, exponential,\
    \ power, and arctangent curves successfully brighten the low-light image."
  Figure 6 Link: articels_figures_by_rev_year\2022\Unsupervised_Face_Detection_in_the_Dark\figure_6.jpg
  Figure 6 caption: Our joint high-low adaptation (HLA) for face detection under low-light
    conditions. We bidirectionally fill the low-level gap by building intermediate
    states and fill the high-level gap comprehensively through multitask cross-domain
    self-supervised learning.
  Figure 7 Link: articels_figures_by_rev_year\2022\Unsupervised_Face_Detection_in_the_Dark\figure_7.jpg
  Figure 7 caption: Results of transferring from WIDER FACE to DARK FACE. The partial
    regions are enhanced for better visibility.
  Figure 8 Link: articels_figures_by_rev_year\2022\Unsupervised_Face_Detection_in_the_Dark\figure_8.jpg
  Figure 8 caption: Results of our bidirectionally brightening DARK FACE images and
    degrading WIDER FACE images.
  Figure 9 Link: articels_figures_by_rev_year\2022\Unsupervised_Face_Detection_in_the_Dark\figure_9.jpg
  Figure 9 caption: Comparison of different strategies for adding noise. (b) Our previous
    publication [13]. (c) Our new strategy.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.94
  Name of the first author: Wenjing Wang
  Name of the last author: Jiaying Liu
  Number of Figures: 20
  Number of Tables: 9
  Number of authors: 4
  Paper title: Unsupervised Face Detection in the Dark
  Publication Date: 2022-02-18 00:00:00
  Table 1 caption: TABLE 1 The Forms of Our Low-Light Enhancement Curves
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Low-Light Face Detection Comparison Results
  Table 3 caption: TABLE 3 Benchmarking the Combination Performance of Different Face
    Detection and Low-Light Enhancement Methods
  Table 4 caption: TABLE 4 Ablation Study Experimental Results
  Table 5 caption: TABLE 5 Comparison of Different Curve Forms
  Table 6 caption: TABLE 6 Comparison of Full and Light Version Enhancement Curves
  Table 7 caption: TABLE 7 Comparison of Deep-Based Enhancement Methods on Detection
    Performance (mAP), Computational Complexity (FLOPs), Network Parameters, and Running
    Time Analysis
  Table 8 caption: TABLE 8 Normal Light Face Detection Comparison Results on WIDER
    FACE
  Table 9 caption: TABLE 9 Comparison on Generic Object Detection, Classification,
    and Semantic Segmentation
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3152562
- Affiliation of the first author: school of artificial intelligence, optics and electronics
    (iopen), northwestern polytechnical university, xian, p.r. china
  Affiliation of the last author: school of artificial intelligence, optics and electronics
    (iopen), northwestern polytechnical university, xian, p.r. china
  Figure 1 Link: articels_figures_by_rev_year\2022\LSVLP_LargeScale_VideoBased_License_Plate_Detection_and_Recognition\figure_1.jpg
  Figure 1 caption: The display of the proposed LSV-LP dataset with annotations. The
    first column of the video sequence is move vs. static, the second column is move
    vs. move, and the last column is the type of static vs. move.
  Figure 10 Link: articels_figures_by_rev_year\2022\LSVLP_LargeScale_VideoBased_License_Plate_Detection_and_Recognition\figure_10.jpg
  Figure 10 caption: The results of the detection stage under different volumes of
    the training data on the validation dataset. a,b,c are the three subsets of the
    LSV-LP dataset and d is the average results of the whole dataset.
  Figure 2 Link: articels_figures_by_rev_year\2022\LSVLP_LargeScale_VideoBased_License_Plate_Detection_and_Recognition\figure_2.jpg
  Figure 2 caption: Typical examples of the proposed LSV-LP dataset. The first line
    of the video sequence is move vs. static, the second line is static vs. move,
    and the third line is move vs. move. The actual frame interval between two adjacent
    frames in the figure is 50.
  Figure 3 Link: articels_figures_by_rev_year\2022\LSVLP_LargeScale_VideoBased_License_Plate_Detection_and_Recognition\figure_3.jpg
  Figure 3 caption: Differences in the position distribution of LP in the three subsets
    of images. (a)-(d) for the move vs. move subset, (e)-(h) for the move vs. static
    subset, and (i)-(l) for the static vs. move subset. (a), (e), and (i) represent
    the statistical maps of the LP distribution. (b), (f), and (j) mean the probability
    density maps obtained by 2-dimensional Parzen-window estimation with Gaussian
    kernel. (c), (g), and (k) are the density maps on X-aixs. (d), (h), and (l) are
    the density maps on Y-aixs.
  Figure 4 Link: articels_figures_by_rev_year\2022\LSVLP_LargeScale_VideoBased_License_Plate_Detection_and_Recognition\figure_4.jpg
  Figure 4 caption: The display of the proportional distribution curve of LP areas
    to the examined frames. The abscissa represents the proportion, and the ordinate
    represents the total number of LPs under the current proportion.
  Figure 5 Link: articels_figures_by_rev_year\2022\LSVLP_LargeScale_VideoBased_License_Plate_Detection_and_Recognition\figure_5.jpg
  Figure 5 caption: The display of the proportional distribution curve of LP areas
    to the examined frames. The abscissa represents the proportion, and the ordinate
    represents the total number of LPs under the current proportion.
  Figure 6 Link: articels_figures_by_rev_year\2022\LSVLP_LargeScale_VideoBased_License_Plate_Detection_and_Recognition\figure_6.jpg
  Figure 6 caption: The display of the distribution curve of LP areas. The abscissa
    represents the area, and the ordinate represents the total number of LPs. The
    abscissa is the result of dividing the number of pixels by 1000.
  Figure 7 Link: articels_figures_by_rev_year\2022\LSVLP_LargeScale_VideoBased_License_Plate_Detection_and_Recognition\figure_7.jpg
  Figure 7 caption: The overall structure of the proposed MFLPR-Net. It consists of
    Detection Module, Optical Flow Module and Recognition Module. The input of the
    entire network is the continuous multi-frame images. These frames generate continuous
    feature fi through backbone and four convolution layers. The optical flow module
    is responsible for assigning weights to these features and aggregating them. fi
    is updated as new features and sent to the latter part of the detection module
    and the LP position is output. The recognition module corrects the rotating region
    of the LP according to the detected coordinates, then extracts the features, and
    finally outputs the LP characters in frame i .
  Figure 8 Link: articels_figures_by_rev_year\2022\LSVLP_LargeScale_VideoBased_License_Plate_Detection_and_Recognition\figure_8.jpg
  Figure 8 caption: The overall structure of the detection module without optical
    flow module is a U-shaped network structure. First, the size is reduced by pooling
    and then the size is expanded by upsampling.
  Figure 9 Link: articels_figures_by_rev_year\2022\LSVLP_LargeScale_VideoBased_License_Plate_Detection_and_Recognition\figure_9.jpg
  Figure 9 caption: The display of the overall structure of the Recognition Module
    in MFLPR-Net. The feature extraction module is used to obtain the feature map,
    and the feature alignment module is used to generate the attention diagram and
    redistribute the weight and then produce the output of the network.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.61
  Name of the first author: Qi Wang
  Name of the last author: Yuan Yuan
  Number of Figures: 11
  Number of Tables: 7
  Number of authors: 4
  Paper title: 'LSV-LP: Large-Scale Video-Based License Plate Detection and Recognition'
  Publication Date: 2022-02-23 00:00:00
  Table 1 caption: TABLE 1 A Comparison of Publicly Available Datasets for LPDR and
    Our Proposed Dataset LSV-LP. Var Denotes Varations. From This Table, the Advantages
    of LSV-LP in Complex Scenarios Can Be Demonstrated
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The Number of Videos, Frames, LPs and the Average Resolution
    of the Subsets in the LSV-LP
  Table 3 caption: TABLE 3 Comparison Results of Tilt Angles of AOLP, CCPD, CLPD and
    LSV-LP
  Table 4 caption: TABLE 4 The Detection Results of Various Indicators on the LSV-LP
    Dataset
  Table 5 caption: TABLE 5 The Recognition Results of Various Indicators on the LSV-LP
    Dataset
  Table 6 caption: TABLE 6 The Results of the Optical Flow Module on the Detection
    Stage
  Table 7 caption: TABLE 7 The Results of the Affine Transformation on the Recognition
    Stage
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3153691
- Affiliation of the first author: college of computer science and technology, national
    university of defense technology, changsha, hunan, china
  Affiliation of the last author: institute of neuroinformatics, uzh-eth, zurich,
    switzerland
  Figure 1 Link: articels_figures_by_rev_year\2022\Low_Cost_and_Latency_Event_Camera_Background_Activity_Denoising\figure_1.jpg
  Figure 1 caption: "Example of denoising event camera output. A: Slot car race track\
    \ setup [1]. B: Sample DVS data from moving car corrupted by background activity\
    \ noise events, including from \u201Chot\u201D pixels that fire at abnormally\
    \ high rates all the time (Section 2.1.2). C: DVS data after denoising (using\
    \ the STCF noise filter from Section 4.2 with k=4 and \u03C4=10ms )."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Low_Cost_and_Latency_Event_Camera_Background_Activity_Denoising\figure_2.jpg
  Figure 2 caption: 'A: DVS principle of detecting brightness change events [4]. B:
    Simplified DVS pixel architecture. The most important noise and mismatch variability
    sources (which cause hot pixels) are shown in red (see Section 2.1.2).'
  Figure 3 Link: articels_figures_by_rev_year\2022\Low_Cost_and_Latency_Event_Camera_Background_Activity_Denoising\figure_3.jpg
  Figure 3 caption: "Observations of DVS BA leak and shot noise. A and B both show\
    \ a collection of histograms of Inter Spike Interval (ISI) between DVS events\
    \ measured for pixels in a Region of Interest (ROI). The background image is a\
    \ 20ms frame of accumulated ON (white) and OFF (black) events. A: BA noise under\
    \ brighter illumination conditions ( > 100 lx scene illumination) is dominated\
    \ by leak noise events that occur with fixed but noisy and highly variable rates\
    \ (note logarithmic time scale of ISI histograms). B: BA noise under lower illumination\
    \ conditions ( < 10 lx scene illumination) is dominated by shot noise events that\
    \ have approximately Poisson statistics (note linear time scale of ISI histograms\
    \ and exponential decay of waiting times). The insets shows that the average of\
    \ ISI across all pixels are log-normal with a 1 \u03C3 Coefficient Of Variation\
    \ (COV) of about 1 decade and a significant tail of hot pixels with higher firing\
    \ rates (see Section 2.1.2)."
  Figure 4 Link: articels_figures_by_rev_year\2022\Low_Cost_and_Latency_Event_Camera_Background_Activity_Denoising\figure_4.jpg
  Figure 4 caption: Overview of filters compared in this paper. Green pixels means
    current event. Yellow pixel means past correlated event. Red pixel means past
    uncorrelated noise event.
  Figure 5 Link: articels_figures_by_rev_year\2022\Low_Cost_and_Latency_Event_Camera_Background_Activity_Denoising\figure_5.jpg
  Figure 5 caption: FWF filter noise rate (false positive) theory of Eq. 1 compared
    with measurement using data of Fig. 3 A.
  Figure 6 Link: articels_figures_by_rev_year\2022\Low_Cost_and_Latency_Event_Camera_Background_Activity_Denoising\figure_6.jpg
  Figure 6 caption: STCF filter false positive rate of (2) compared with measurements
    using real and simulated shot noise.
  Figure 7 Link: articels_figures_by_rev_year\2022\Low_Cost_and_Latency_Event_Camera_Background_Activity_Denoising\figure_7.jpg
  Figure 7 caption: Samples from the two datasets.
  Figure 8 Link: articels_figures_by_rev_year\2022\Low_Cost_and_Latency_Event_Camera_Background_Activity_Denoising\figure_8.jpg
  Figure 8 caption: 'ROC curves and summarized AUC denoising accuracy for both datasets.
    A: hotel-bar ROC curves. B: driving ROC curves. C: AUC for both datasets as a
    function of BA shot noise rate.'
  Figure 9 Link: articels_figures_by_rev_year\2022\Low_Cost_and_Latency_Event_Camera_Background_Activity_Denoising\figure_9.jpg
  Figure 9 caption: Denoising comparisons. The small blue overlay statistics text
    are generated by NoiseTesterFilter. Hotel-bar shows 20ms frame at time 1606.903s.
    Driving shows 20ms frame at time 4.355s. Both use full scale 3 ON and OFF event
    rendering. Yellow crosses show ROC TPR and FPR point as in Figs. 8 A and 8 B.
    Panels K and L label classifications from STCF.
  First author gender probability: 0.95
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: Shasha Guo
  Name of the last author: Tobi Delbruck
  Number of Figures: 9
  Number of Tables: 2
  Number of authors: 2
  Paper title: Low Cost and Latency Event Camera Background Activity Denoising
  Publication Date: 2022-02-23 00:00:00
  Table 1 caption: TABLE 1 DVS Event Camera Denoising Algorithms
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Denoising Accuracy Comparison
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3152999
- Affiliation of the first author: gaoling school of artificial intelligence, renmin
    university of china, beijing, china
  Affiliation of the last author: provost, king abdullah university of science & technology
    (kaust), thuwal, saudi arabia
  Figure 1 Link: articels_figures_by_rev_year\2022\Representing_Graphs_via_GromovWasserstein_Factorization\figure_1.jpg
  Figure 1 caption: An illustration of our Gromov-Wasserstein factorization model.
    The surface represents a collection of data and each star on the surface indicates
    a graph. For each graph, the black curves show its edges, and the dots with different
    colors are its nodes with different attributes.
  Figure 10 Link: articels_figures_by_rev_year\2022\Representing_Graphs_via_GromovWasserstein_Factorization\figure_10.jpg
  Figure 10 caption: Comparison of alternating optimization v.s. joint optimization.
    The datasets are AIDS, PROTEINS, PROTEINS-F, and IMDB-B. In (b), because each
    of the datasets has two clusters, given the ground truth labels boldsymbolyin
    lbrace 0,1rbrace N and the estimated labels hatboldsymbolyin lbrace 0, 1rbrace
    N , we calculate the clustering accuracy via (1-frac1Nmin (Vert boldsymboly-hatboldsymbolyVert
    1, Vert boldsymboly-boldsymbol1+hatboldsymbolyVert 1))times 100% .
  Figure 2 Link: articels_figures_by_rev_year\2022\Representing_Graphs_via_GromovWasserstein_Factorization\figure_2.jpg
  Figure 2 caption: (a) An illustration of the GW discrepancy between two measure
    metric spaces. The mm-space on the left is a two-dimensional Euclidean space associated
    with a probability measure defined by a Gaussian mixture model (GMM) with three
    modalities. The mm-space on the right is a three-dimensional Euclidean space associated
    with a GMM probability measure. The GW distance minimizes the expectation of the
    relational distance (the black arrow) between arbitrary two pairs of samples (the
    lines connecting two stars) from the two spaces. (b) An illustration of the GW
    discrepancy between two graphs, where each is associated with an empirical distribution
    of nodes. The relational distance corresponds to the difference between the elements
    of their adjacency matrices. The GW discrepancy minimizes the expectation of the
    relational distance and leads to an optimal transport matrix indicating the joint
    distribution of the graphs nodes that corresponds to the minimum expectation.
  Figure 3 Link: articels_figures_by_rev_year\2022\Representing_Graphs_via_GromovWasserstein_Factorization\figure_3.jpg
  Figure 3 caption: An illustration of the computation of the GW barycenter. Each
    matrix represents an optimal transport between a graph (in orange) and the barycenter
    (in blue). The red arrows indicate that we need to update the optimal transports
    and the barycenter iteratively by alternating optimization. For each graph, the
    colors of nodes indicate their node attributes. We can consider the node attributes
    by extending the GW discrepancy to the fused GW discrepancy [41], which will be
    discussed in the following content.
  Figure 4 Link: articels_figures_by_rev_year\2022\Representing_Graphs_via_GromovWasserstein_Factorization\figure_4.jpg
  Figure 4 caption: An illustration of our inductive GWF-GNN model. Both the graph
    neural network and the graph factors (the orange graphs) are learnable parameters.
  Figure 5 Link: articels_figures_by_rev_year\2022\Representing_Graphs_via_GromovWasserstein_Factorization\figure_5.jpg
  Figure 5 caption: (a) The feedforward computation of the GWF model. The loss module
    corresponds to the dtextlossq in (16). (b) One step of the GW barycenter module.
    The red arrows represent the paths used for both feedforward computation and backpropagation,
    while the black ones are just for feedforward computation. The GWD corresponds
    to Algorithms 1 or 2, and the GWB corresponds to Algorithm 3.
  Figure 6 Link: articels_figures_by_rev_year\2022\Representing_Graphs_via_GromovWasserstein_Factorization\figure_6.jpg
  Figure 6 caption: Comparisons for our PPA-based method [44], our BADMM-based method
    [46], and the entropic regularization-based method (Entropy) [12] on four kinds
    of graphs. For each method, we set the maximum number of iterations to be 1000,
    and the stop criterion is that the change of GW discrepancy is smaller than 10-16
    . In each subfigure, we record the mean and the standard deviation of the GW discrepancy
    concerning the runtime of each method. In each curve, the dot corresponds to the
    GW discrepancy and the runtime after ten iterations.
  Figure 7 Link: articels_figures_by_rev_year\2022\Representing_Graphs_via_GromovWasserstein_Factorization\figure_7.jpg
  Figure 7 caption: The comparison on runtime for various loss functions.
  Figure 8 Link: articels_figures_by_rev_year\2022\Representing_Graphs_via_GromovWasserstein_Factorization\figure_8.jpg
  Figure 8 caption: (a-d) Visualizations of boldsymbolz1:I based on t-SNE. In each
    subfigure, the left image corresponds to the transductive GWF model and the right
    image corresponds to the inductive GWF-GNN model.
  Figure 9 Link: articels_figures_by_rev_year\2022\Representing_Graphs_via_GromovWasserstein_Factorization\figure_9.jpg
  Figure 9 caption: Visualizations of typical graphs and their adjacency matrices
    of AIDS dataset and some learned graph factors of our GWF-GNN model. The graphs
    and the graph factors are sorted according to their sizes.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Hongteng Xu
  Name of the last author: Lawrence Carin
  Number of Figures: 11
  Number of Tables: 4
  Number of authors: 4
  Paper title: Representing Graphs via Gromov-Wasserstein Factorization
  Publication Date: 2022-02-23 00:00:00
  Table 1 caption: TABLE 1 Comparisons on Runtime and Memory Cost
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The Statistics of Various Datasets
  Table 3 caption: TABLE 3 Comparisons on Rand-Index (%)
  Table 4 caption: TABLE 4 Comparison on Classification Accuracy ( % %).
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3153126
- Affiliation of the first author: school of information and communication engineering,
    dalian university of technology, dalian, ganjingzi, china
  Affiliation of the last author: school of information and communication engineering,
    dalian university of technology, dalian, ganjingzi, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Effective_Local_and_Global_Search_for_Fast_LongTerm_Tracking\figure_1.jpg
  Figure 1 caption: Overall pipeline of the proposed tracking framework. Better viewed
    in color with zoom-in.
  Figure 10 Link: articels_figures_by_rev_year\2022\Effective_Local_and_Global_Search_for_Fast_LongTerm_Tracking\figure_10.jpg
  Figure 10 caption: Qualitative comparison results of several top trackers on VOT2018LT.
  Figure 2 Link: articels_figures_by_rev_year\2022\Effective_Local_and_Global_Search_for_Fast_LongTerm_Tracking\figure_2.jpg
  Figure 2 caption: 'Frameworks of global re-detection modules: (a) skimming module;
    and (b) objectness-guided module.'
  Figure 3 Link: articels_figures_by_rev_year\2022\Effective_Local_and_Global_Search_for_Fast_LongTerm_Tracking\figure_3.jpg
  Figure 3 caption: Effectiveness of the proposed re-detection module. The abbreviations
    w skim and w obj denote the trackers with the skimming module and objectness-guided
    module, respectively. wo denote the tracker without any re-detection module.
  Figure 4 Link: articels_figures_by_rev_year\2022\Effective_Local_and_Global_Search_for_Fast_LongTerm_Tracking\figure_4.jpg
  Figure 4 caption: Designed long short-term updated verifier in our tracker.
  Figure 5 Link: articels_figures_by_rev_year\2022\Effective_Local_and_Global_Search_for_Fast_LongTerm_Tracking\figure_5.jpg
  Figure 5 caption: Positive samples and hard negative samples.
  Figure 6 Link: articels_figures_by_rev_year\2022\Effective_Local_and_Global_Search_for_Fast_LongTerm_Tracking\figure_6.jpg
  Figure 6 caption: Feature map visualization of the verification model.
  Figure 7 Link: articels_figures_by_rev_year\2022\Effective_Local_and_Global_Search_for_Fast_LongTerm_Tracking\figure_7.jpg
  Figure 7 caption: The green box is the reference box generated by the proposed re-detection
    module. The red box is the annotated groundtruth.
  Figure 8 Link: articels_figures_by_rev_year\2022\Effective_Local_and_Global_Search_for_Fast_LongTerm_Tracking\figure_8.jpg
  Figure 8 caption: 'Quantitative analysis with respect to different attributes. Visual
    attributes: (O) Full occlusion, (V) Out-of-view, (P) Partial occlusion, (C) Camera
    motion, (F) Fast motion, (S) Scale change, (A) Aspect ratio change, (W) Viewpoint
    change, (I) Similar objects, and (D) Deformable object.'
  Figure 9 Link: articels_figures_by_rev_year\2022\Effective_Local_and_Global_Search_for_Fast_LongTerm_Tracking\figure_9.jpg
  Figure 9 caption: Average recovery length plots of several competing trackers on
    the VOT2018-LT dataset.
  First author gender probability: 0.67
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Haojie Zhao
  Name of the last author: Huchuan Lu
  Number of Figures: 13
  Number of Tables: 12
  Number of authors: 6
  Paper title: Effective Local and Global Search for Fast Long-Term Tracking
  Publication Date: 2022-02-23 00:00:00
  Table 1 caption: TABLE 1 Comparison of our Tracker and Competing Algorithms on the
    VOT18-LT [13] Datasets
  Table 10 caption: TABLE 10 Evaluation Results of Several Trackers on the VOT2018-LT
    Dataset
  Table 2 caption: TABLE 2 Comparison of our Tracker and Competing Algorithms on the
    VOT19-LT [49] Datasets
  Table 3 caption: TABLE 3 Comparison of our Trackers and 15 Competing Algorithms
    on the OxUvA Dataset [14]
  Table 4 caption: TABLE 4 Comparison of our Trackers and Eight Competing Algorithms
    on TLP in Terms of Success Rate (SR, Under Overlap Threshold 0.5), Success Score,
    and Precision Score
  Table 5 caption: TABLE 5 Evaluation Results on VOT-2018 and VOT-2019
  Table 6 caption: TABLE 6 Effectiveness of Different Components for our Tracker
  Table 7 caption: TABLE 7 Effectiveness of Different Templates in the Proposed Verifier
    for our Tracker
  Table 8 caption: TABLE 8 Comparison of our Verifier and RT-MDNet on VOT2018-LT [13]
  Table 9 caption: TABLE 9 Effectiveness of Different Re-Detection Methods
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3153645
