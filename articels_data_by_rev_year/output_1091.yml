- Affiliation of the first author: microsoft research, beijing, p.r. china
  Affiliation of the last author: microsoft research, beijing, p.r. china
  Figure 1 Link: articels_figures_by_rev_year\2018\Composite_Quantization\figure_1.jpg
  Figure 1 caption: Illustrating the relation between recall and quantization error.
    Red, green, and blue lines correspond to recall at positions 1, 10, and 100, respectively.
    For each color, the line segments from left to right correspond to 128, 64, and
    32 bits, where the number of dictionaries M are 16, 8, and 4, respectively, and
    each dictionary contains K =256 elements. The mark on each line represents NOCQ,
    CKM, and PQ from left to right respectively. For (R,B) in the legend, R indicates
    the rank position to evaluate the recall, and B indicates the number of bits.
  Figure 10 Link: articels_figures_by_rev_year\2018\Composite_Quantization\figure_10.jpg
  Figure 10 caption: The inner product search performance for different algorithms
    on (a) 1M SIFT, (b) 1M GIST and (c) 1B SIFT.
  Figure 2 Link: articels_figures_by_rev_year\2018\Composite_Quantization\figure_2.jpg
  Figure 2 caption: The results of directly minimizing the upper bound, denoted as
    UB, NOCQ, and composite quantization with encoding the third term, denoted as
    CQ-e, on 1M SIFT and 1M GIST. The horizontal axis labelled as (R,B) indicates
    the recall R performance when encoded with B bits.
  Figure 3 Link: articels_figures_by_rev_year\2018\Composite_Quantization\figure_3.jpg
  Figure 3 caption: The convergence curve of our algorithm. The vertical axis represents
    the objective function value of Equation (14) and the horizontal axis corresponds
    to the number of iterations. The curve is obtained from the result over a representative
    dataset 1M SIFT with 64 bits.
  Figure 4 Link: articels_figures_by_rev_year\2018\Composite_Quantization\figure_4.jpg
  Figure 4 caption: Average query time on 1M SIFT and 1M GIST.
  Figure 5 Link: articels_figures_by_rev_year\2018\Composite_Quantization\figure_5.jpg
  Figure 5 caption: The performance for different algorithms on (a) MNIST and (b)
    LabelMe 22K for searching various numbers of ground truth nearest neighbors (
    T=1,10 ).
  Figure 6 Link: articels_figures_by_rev_year\2018\Composite_Quantization\figure_6.jpg
  Figure 6 caption: The performance for different algorithms on (a) 1M SIFT and (b)
    1M GIST for searching various numbers of ground truth nearest neighbors ( T=1,
    10, 50 ).
  Figure 7 Link: articels_figures_by_rev_year\2018\Composite_Quantization\figure_7.jpg
  Figure 7 caption: The performance for different algorithms on 1M CNN with 64 bits
    and 128 bits for searching various numbers of ground truth nearest neighbors (
    T=1, 10, 50 ).
  Figure 8 Link: articels_figures_by_rev_year\2018\Composite_Quantization\figure_8.jpg
  Figure 8 caption: The performance for different algorithms on 1B SIFT with (a) 64
    bits and (b) 128 bits for searching various numbers of ground truth nearest neighbors
    ( T=1, 10, 50 ).
  Figure 9 Link: articels_figures_by_rev_year\2018\Composite_Quantization\figure_9.jpg
  Figure 9 caption: The performance in terms of MAP versus code length for different
    algorithms on (a) MNIST, (b) LabelMe 22K . (c) 1M SIFT, (d) 1M GIST, (e) 1M CNN
    and (f) 1B SIFT.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.67
  Name of the first author: Jingdong Wang
  Name of the last author: Ting Zhang
  Number of Figures: 12
  Number of Tables: 7
  Number of authors: 2
  Paper title: Composite Quantization
  Publication Date: 2018-05-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Search Accuracy Comparison Between Our Approach (NOCQ)
      and Orthogonal Composite Quantization (OCQ)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Description of the Datasets
  Table 3 caption:
    table_text: TABLE 3 Comparison of Multi-D-ADC Systems with Different Quantization
      Algorithms in Terms of recall R R with R R Being 1,10,100, Time Cost (in Millisecond)
      with Database Vector Reconstruction ( T1 T1), Time Cost (in Millisecond) without
      Database Vector Reconstruction ( T2 T2)
  Table 4 caption:
    table_text: 'TABLE 4 Comparing the Performances of Alternative Schemes without
      the Near-Orthogonality Constraint: CQ with Extra Bytes to Storing the Third
      Term (CQ), CQ with Discarding the Third Term in the Search Stage (CQ-d), CQ
      with One Byte to Quantize the Third Term (CQ-e), and Our Approach (NOCQ)'
  Table 5 caption:
    table_text: TABLE 5 Comparing the Approach of Inner Product Search on Augmented
      Vector Quantization (AVCQ) and Our Approach NOCQ
  Table 6 caption:
    table_text: TABLE 6 The Search Accuracy Comparison for Our Approach, NOCQ, and
      AdditiveComposite Quantization (AQ) [1], and Optimized Tree Quantization (OTQ)
      [2] on 1M 1MSIFT. AQ-n Means the Scheme Using 1 Byte to Encode the Norm of the
      Reconstructed Vector
  Table 7 caption:
    table_text: TABLE 7 Results of Local Search Quantization (LSQ) [24] That Adopts
      Iterative Local Search for Encoding, Our Approach NOCQ, and NOCQ Using Iterative
      Local Search for Encoding (NOCQ-ILS)
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2835468
- Affiliation of the first author: vision, learning & control, university of southampton
    faculty of physical sciences and engineering, southampton, united kingdom
  Affiliation of the last author: school of electronics and comptuer science, university
    of southampton, southampton, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2018\SuperFine_Attributes_with_Crowd_Prototyping\figure_1.jpg
  Figure 1 caption: Conventional ground-truth (blue) versus Super-fine (red) labels
    on the PETA dataset. Conventional categories are coarse-grained and can be inconsistent
    andor irrelevant. In contrast, super-fine annotations improve automatic subject
    retrieval with more precise and relevant descriptions, represented as multi-dimensional
    coordinates (red numbers).
  Figure 10 Link: articels_figures_by_rev_year\2018\SuperFine_Attributes_with_Crowd_Prototyping\figure_10.jpg
  Figure 10 caption: Ranked retrieval ROC curves. Dotted lines indicate maximum performance
    assuming perfect label estimation. (TPR) True Positive Rate. (FPR) False Positive
    Rate. Shaded areas represent standard deviation.
  Figure 2 Link: articels_figures_by_rev_year\2018\SuperFine_Attributes_with_Crowd_Prototyping\figure_2.jpg
  Figure 2 caption: Approach overview, Contributions (green). a) Crowd prototyping
    previews the crowd's perception of an image subset to discover a perceptual space
    and salient visual prototypes (Section 4). b) Super-fine annotation efficiently
    matches unlabelled images to pre-discovered visual prototypes, generating super-fine
    coordinate labels (Section 5). In contrast, a conventional approach matches images
    to pre-defined categorical text labels. c) Image labels are estimated by fine-tuning
    the ResNet-152 CNN, classifying conventional binary attributes and regressing
    super-fine attributes (Section 6). d) Approaches are evaluated comparing ranked
    retrieval performance in multi-shot and zero-shot scenarios (Section 7).
  Figure 3 Link: articels_figures_by_rev_year\2018\SuperFine_Attributes_with_Crowd_Prototyping\figure_3.jpg
  Figure 3 caption: Pairwise similarity task for crowd prototyping.
  Figure 4 Link: articels_figures_by_rev_year\2018\SuperFine_Attributes_with_Crowd_Prototyping\figure_4.jpg
  Figure 4 caption: "MDS embedding strategy evaluation of g 0 , g 1 and g 2 , reporting\
    \ the normed embedding Stress-1 error \u03C3 1 (upper), Spearman's rank correlation\
    \ coefficient \u03C1 between \u03B4 ij and d ij (mid) and the silhouette score\
    \ coefficient s of the original categories protected in embedded space (lower)."
  Figure 5 Link: articels_figures_by_rev_year\2018\SuperFine_Attributes_with_Crowd_Prototyping\figure_5.jpg
  Figure 5 caption: Comparison of new prototype clusters (upper) against original
    ground-truth categories (lower) projected in embedded conceptual space. Each point
    represents one of the N=95 images annotated with similarity. Two examples of conflicts
    between original and new gender prototypes are highlight.
  Figure 6 Link: articels_figures_by_rev_year\2018\SuperFine_Attributes_with_Crowd_Prototyping\figure_6.jpg
  Figure 6 caption: Discovered crowd prototypes, depicting distinct trait concepts
    from the crowd's perceptual consensus. Prototypes are formed by clustering conceptual
    spaces, selecting up to 8 images from each region's centroid. Semantic text descriptions
    are added manually for discussion.
  Figure 7 Link: articels_figures_by_rev_year\2018\SuperFine_Attributes_with_Crowd_Prototyping\figure_7.jpg
  Figure 7 caption: Large-scale visual prototype matching task.
  Figure 8 Link: articels_figures_by_rev_year\2018\SuperFine_Attributes_with_Crowd_Prototyping\figure_8.jpg
  Figure 8 caption: Visualisation of large-scale super-fine annotations (subset of
    all 19K instances). Images are located at their annotated conceptual coordinates
    and head cropped for clarity. Border colours relate to median super-fine attribute
    annotation.
  Figure 9 Link: articels_figures_by_rev_year\2018\SuperFine_Attributes_with_Crowd_Prototyping\figure_9.jpg
  Figure 9 caption: Label distributions and confusion between subject-level super-fine
    attributes and original binary categories.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Daniel Martinho-Corbishley
  Name of the last author: John N. Carter
  Number of Figures: 10
  Number of Tables: 3
  Number of authors: 3
  Paper title: Super-Fine Attributes with Crowd Prototyping
  Publication Date: 2018-05-15 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Pairwise Annotations and Associated Proximity p ij pij and
      Uncertainty u ij uij Interval Measures Between Subjects i i and j j
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Mean Recognition Accuracy (mA %) of Binary Attributes
      Across Previous PETA Studies and Our Work
  Table 3 caption:
    table_text: TABLE 3 Ranked Retrieval Results
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2836900
- Affiliation of the first author: department of computing, south kensington campus,
    london, united kingdom
  Affiliation of the last author: department of computing, south kensington campus,
    london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2018\D_Reconstruction_of_IntheWild_Faces_in_Images_and_Videos\figure_1.jpg
  Figure 1 caption: "Results of our 3DMM image fitting method ITW(Basel) on \u201C\
    in-the-wild\u201D images from the 300W dataset [15]. We note that our proposed\
    \ technique is able to handle extremely challenging pose, illumination, and expression\
    \ variations, returning plausible 3D facial shapes in all the above cases."
  Figure 10 Link: articels_figures_by_rev_year\2018\D_Reconstruction_of_IntheWild_Faces_in_Images_and_Videos\figure_10.jpg
  Figure 10 caption: 'Facial shape estimation on 3dMDLab-real: Quantitative comparison
    of our image fitting method ITW(Basel) with other methods. The results are presented
    as CEDs of the normalised dense vertex error.'
  Figure 2 Link: articels_figures_by_rev_year\2018\D_Reconstruction_of_IntheWild_Faces_in_Images_and_Videos\figure_2.jpg
  Figure 2 caption: "Left: The mean and first four shape and SIFT texture principal\
    \ components of our \u201Cin-the-wild\u201D SIFT texture model. Right: To aid\
    \ in interpretation we also show the equivalent RGB basis."
  Figure 3 Link: articels_figures_by_rev_year\2018\D_Reconstruction_of_IntheWild_Faces_in_Images_and_Videos\figure_3.jpg
  Figure 3 caption: Building an ITW texture model. The red coloured region denotes
    the occlusion mask obtained by fitting the 3D shape model on the sparse 2D landmarks
    of the original image.
  Figure 4 Link: articels_figures_by_rev_year\2018\D_Reconstruction_of_IntheWild_Faces_in_Images_and_Videos\figure_4.jpg
  Figure 4 caption: '3dMDLab benchmark: (a,b) Examples of 2 out of 8 images of 3dMDLab-real
    set. We introduce this benchmark to evaluate image fitting methods under ideal
    conditions. (c,d) Examples of 2 out of 8 images of 3dMDLab-synthetic set. We introduce
    this benchmark to evaluate image fitting methods under synthetic strong illumination
    conditions.'
  Figure 5 Link: articels_figures_by_rev_year\2018\D_Reconstruction_of_IntheWild_Faces_in_Images_and_Videos\figure_5.jpg
  Figure 5 caption: '4DMaja-synthetic benchmark video: 4 out of 440 frames of a synthetic
    video created using high-resolution 4D face scans and rendering using a synthetic
    camera under varying 3D pose. Since this is a rendered video, it is accompanied
    by 4D ground truth mesh information.'
  Figure 6 Link: articels_figures_by_rev_year\2018\D_Reconstruction_of_IntheWild_Faces_in_Images_and_Videos\figure_6.jpg
  Figure 6 caption: "4DMaja-real benchmark video: (a-c) 3 out of 387 frames of a real\
    \ video under \u201Cin-the-wild\u201D conditions. (d) Ground truth mesh representing\
    \ the shape identity component of the 3D facial shape of the captured subject."
  Figure 7 Link: articels_figures_by_rev_year\2018\D_Reconstruction_of_IntheWild_Faces_in_Images_and_Videos\figure_7.jpg
  Figure 7 caption: '3D face reconstruction of challenging face images: qualitative
    comparison of our method (ITW(LSFM)) with other methods: (a) MoFA, (b) Jackson
    et al. 2017, (c) 3DMMedges, (d) Classic.'
  Figure 8 Link: articels_figures_by_rev_year\2018\D_Reconstruction_of_IntheWild_Faces_in_Images_and_Videos\figure_8.jpg
  Figure 8 caption: Accuracy results for facial shape estimation on the KF-ITW database.
    The results are presented as CEDs of the normalized dense vertex error. Table
    1 reports additional measures.
  Figure 9 Link: articels_figures_by_rev_year\2018\D_Reconstruction_of_IntheWild_Faces_in_Images_and_Videos\figure_9.jpg
  Figure 9 caption: Accuracy results for facial surface normal estimation on 100 subjects
    from the Photoface database [72]. The results are presented as CEDs of mean angular
    error.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: James Booth
  Name of the last author: Stefanos Zafeiriou
  Number of Figures: 14
  Number of Tables: 3
  Number of authors: 7
  Paper title: "3D Reconstruction of \u201CIn-the-Wild\u201D Faces in Images and Videos"
  Publication Date: 2018-05-15 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Accuracy Results for Facial Shape Estimation on the KF-ITW
      Database
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Facial Shape Estimation on 3dMDLab-Synthetic: Comparison
      of ITW (Our Fitting Method), RGB-MM (a Simplified Version of Our Method Where
      We Have Replaced the ITW Texture Model with an RGB Texture Model) and Classic
      3DMM Fitting [43], Showing Area Under the Curve (AUC) and Failure Rate of the
      CED for Each Method'
  Table 3 caption:
    table_text: 'TABLE 3 3D Identity Shape Estimation on 4DMaja-real Video: Self-Evaluation
      of Our Fitting Framework, Comparing Our Video Fitting Method (ITW-V) with the
      Initialisation of Our Video Fitting Method from Sparse Landmarks as Described
      in Section 4.2.2 (ITW-V, init)'
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2832138
- Affiliation of the first author: intelligent image processing research center, korea
    electronics technology institute (keti), seongnam-si, gyeonggi-do, south korea
  Affiliation of the last author: department of mechanical engineering, korea advanced
    institute of science and technology (kaist), daejeon, south korea
  Figure 1 Link: articels_figures_by_rev_year\2018\Learning_and_Selecting_Confidence_Measures_for_Robust_Stereo_Matching\figure_1.jpg
  Figure 1 caption: Stereo matching results in a challenging environment. The predicted
    confidence map (b) indicates that white pixels are more likely to be matched correctly.
    (c) and (d) show colored disparity maps overlaid on the input image.
  Figure 10 Link: articels_figures_by_rev_year\2018\Learning_and_Selecting_Confidence_Measures_for_Robust_Stereo_Matching\figure_10.jpg
  Figure 10 caption: Comparison of disparity maps without and with the confidence-based
    cost modulation procedure for a Middlebury 2014 dataset, Adirondack. We colored
    the mismatched pixels in red, and the difference between the two algorithms is
    the matching cost modulation part.
  Figure 2 Link: articels_figures_by_rev_year\2018\Learning_and_Selecting_Confidence_Measures_for_Robust_Stereo_Matching\figure_2.jpg
  Figure 2 caption: The overall framework with the proposed method.
  Figure 3 Link: articels_figures_by_rev_year\2018\Learning_and_Selecting_Confidence_Measures_for_Robust_Stereo_Matching\figure_3.jpg
  Figure 3 caption: Comparison of predicted error maps using various confidence measures.
    Error maps in (c)-(i) are computed by each confidence measure for the disparity
    map (b). The red color indicates that the pixel is predicted as mismatched and
    the white color indicates the correctly matched pixels. To show the characteristics
    of the confidence measures, we manually set the threshold values for different
    confidence measures, though it is very difficult to determine an optimal threshold
    value in practice. Here, LRC and MDD measures show strong correlations with the
    real error map (c), compared to others.
  Figure 4 Link: articels_figures_by_rev_year\2018\Learning_and_Selecting_Confidence_Measures_for_Robust_Stereo_Matching\figure_4.jpg
  Figure 4 caption: Random permutation changes the designated element of an OOB sample
    with the value of a randomly selected OOB sample.
  Figure 5 Link: articels_figures_by_rev_year\2018\Learning_and_Selecting_Confidence_Measures_for_Robust_Stereo_Matching\figure_5.jpg
  Figure 5 caption: Illustration of matching cost modulation. As the confidence value
    decreases, the matching costs will be close to a constant value. Therefore, the
    reliably matched pixels affect the ambiguous pixels strongly and effectively.
  Figure 6 Link: articels_figures_by_rev_year\2018\Learning_and_Selecting_Confidence_Measures_for_Robust_Stereo_Matching\figure_6.jpg
  Figure 6 caption: Stereo matching with confidence-based cost modulation. Even though
    the disparity maps in (e), (f) employ the same parameters and algorithms, except
    the proposed modulation scheme, our result gives a smooth disparity map for a
    noisy input image (a). We did not apply any post-processing algorithm to clearly
    show the difference of the disparity maps.
  Figure 7 Link: articels_figures_by_rev_year\2018\Learning_and_Selecting_Confidence_Measures_for_Robust_Stereo_Matching\figure_7.jpg
  Figure 7 caption: Comparison of permutation importance with randomly selected training
    images. The numbers along the x -axis indicate the confidence measures and those
    along the y -axis indicate the permutation importance of each measure.
  Figure 8 Link: articels_figures_by_rev_year\2018\Learning_and_Selecting_Confidence_Measures_for_Robust_Stereo_Matching\figure_8.jpg
  Figure 8 caption: Comparison of AUC values in ascending order, per to the optimal
    AUC values for the KITTI 2012 dataset. We compared the proposed methods, Prop.
    8 and Prop. 50, with widely used confidence measures such as PRKN [5], LRC, and
    MDD [4] measures, and the learning-based approaches such as Ensemble [4], GCP
    [3], LevStereo [9] with 22 and 8 confidence measures, and O(1) confidence measures
    [24]. Note that Prop. 8 indicates different sets of confidence measures, selected
    by the sequential selection scheme from the training data. We magnified two subregions
    for a clear comparison among the different methods. Here, the error rate of disparity
    maps in Fig. 8a ranges from 10 to 53 percent whereas the average bad pixel rate
    of disparity maps in Fig. 8a was less than 4 percent.
  Figure 9 Link: articels_figures_by_rev_year\2018\Learning_and_Selecting_Confidence_Measures_for_Robust_Stereo_Matching\figure_9.jpg
  Figure 9 caption: Comparison of disparity maps with and without the confidence-based
    cost modulation procedure. Images in (a) are selected from the KITTI 2012 and
    2015 datasets that showed the highest improvement. In most cases, the proposed
    confidence-based modulation scheme reduced the noisy disparity maps significantly.
    Thus, the disparity values of the reflected surfaces and highly ambiguous regions
    were improved significantly (about 5 percent for the selected images). Here, we
    did not apply any post-processing techniques to clearly demonstrate the influence
    of the proposed technique.
  First author gender probability: 0.94
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.56
  Name of the first author: Min-Gyu Park
  Name of the last author: Kuk-Jin Yoon
  Number of Figures: 11
  Number of Tables: 4
  Number of authors: 2
  Paper title: Learning and Selecting Confidence Measures for Robust Stereo Matching
  Publication Date: 2018-05-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Selected Confidence Measures Based on Raw Matching
      Costs
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Selected Confidence Measures Based on Optimized
      Matching Costs via SGM [11]
  Table 3 caption:
    table_text: TABLE 3 Comparison of the Proposed Methods with Ensemble [4], GCP
      [3], O(1) [24], and LevStereo [9] in Terms of Average AUC Values
  Table 4 caption:
    table_text: TABLE 4 Performance Improvement Using the Proposed Cost Modulation
      Scheme
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2837760
- Affiliation of the first author: school of mathematical sciences, university of
    science and technology of china, hefei, china
  Affiliation of the last author: school of computer science and engineering, nanyang
    technological university, singapore
  Figure 1 Link: articels_figures_by_rev_year\2018\CNNBased_RealTime_Dense_Face_Reconstruction_with_InverseRendered_PhotoRealistic_\figure_1.jpg
  Figure 1 caption: The pipeline of our proposed learning based dense 3D face reconstruction
    and tracking framework. The first frame of the input video is initially reconstructed
    by a single-image CoarseNet for coarse face geometry reconstruction, followed
    by using FineNet for detailed face geometry recovery. Each of the subsequent frames
    is processed by a tracking CoarseNet followed by FineNet.
  Figure 10 Link: articels_figures_by_rev_year\2018\CNNBased_RealTime_Dense_Face_Reconstruction_with_InverseRendered_PhotoRealistic_\figure_10.jpg
  Figure 10 caption: 'Comparisons of our inverse rendering and our learning based
    dense face tracking solution. From top to bottom: input face video frame and groundtruth
    mesh in dataset [55], results of the inverse rendering approach, results of our
    learning based dense face tracking solution.'
  Figure 2 Link: articels_figures_by_rev_year\2018\CNNBased_RealTime_Dense_Face_Reconstruction_with_InverseRendered_PhotoRealistic_\figure_2.jpg
  Figure 2 caption: 'The pipeline of our proposed inverse rendering method. Given
    an input face image (left), our inverse rendering consists of three stages: Model
    fitting (second column), geometry refinement (third column) and albedo blending
    (last column). At each stage, the top to bottom rows are the corresponding recovered
    lighting, geometry and albedo, and the rendered face image is shown on the right.
    The arrows indicate which component is updated.'
  Figure 3 Link: articels_figures_by_rev_year\2018\CNNBased_RealTime_Dense_Face_Reconstruction_with_InverseRendered_PhotoRealistic_\figure_3.jpg
  Figure 3 caption: "Training data synthesis for Single-image CoarseNet. Given a real\
    \ face image, we first do the inverse rendering to estimate lighting, albedo and\
    \ geometry. Then, by changing the expression parameter \u03B1 exp and the pose\
    \ parameters pitch,yaw,roll , the face geometry is augmented. In the final, a\
    \ set of new face images is obtained by rendering the newly changed face geometry."
  Figure 4 Link: articels_figures_by_rev_year\2018\CNNBased_RealTime_Dense_Face_Reconstruction_with_InverseRendered_PhotoRealistic_\figure_4.jpg
  Figure 4 caption: Examples of adjacent frame simulations. For each pair, the left
    is the PNCC image generated by simulating the previous frame, and the right is
    the current face frame.
  Figure 5 Link: articels_figures_by_rev_year\2018\CNNBased_RealTime_Dense_Face_Reconstruction_with_InverseRendered_PhotoRealistic_\figure_5.jpg
  Figure 5 caption: Synthetic data generation for training FineNet. Given a target
    face image without many geometry details (top left) and a source face image (bottom
    left) that is rich of wrinkles, we first apply our developed inverse rendering
    on both images to obtain the projected geometry for target face (top second) and
    a displacement map for the source face (bottom right). Then we transfer the displacement
    map of the source face to the geometry of the target face. Finally we render the
    updated geometry to get a new face image (top right) which contains the same type
    of wrinkles as the source face.
  Figure 6 Link: articels_figures_by_rev_year\2018\CNNBased_RealTime_Dense_Face_Reconstruction_with_InverseRendered_PhotoRealistic_\figure_6.jpg
  Figure 6 caption: 'Results of our two-stage CNN based face tracking. Left: four
    frames of a video. Middle: results of Tracking CoarseNet (projected mesh and rendered
    face). Right: results of FineNet.'
  Figure 7 Link: articels_figures_by_rev_year\2018\CNNBased_RealTime_Dense_Face_Reconstruction_with_InverseRendered_PhotoRealistic_\figure_7.jpg
  Figure 7 caption: 'Comparisons with image-based dense face tracking. Top row: four
    continuous frames from a video. Middle row: results of using our Single-image
    CoarseNet on each frame. Bottom row: results of our Tracking CoarseNet. It can
    be observed that Tracking CoarseNet achieves more robust tracking.'
  Figure 8 Link: articels_figures_by_rev_year\2018\CNNBased_RealTime_Dense_Face_Reconstruction_with_InverseRendered_PhotoRealistic_\figure_8.jpg
  Figure 8 caption: Comparisons with the state-of-art dense face tracking methods
    [18], [22], [48]. The average computation time for each frame is given in the
    bracket. [48] does not report the running time of their method, while it should
    take much longer time than ours since it iteratively solves several complex optimization
    problems.
  Figure 9 Link: articels_figures_by_rev_year\2018\CNNBased_RealTime_Dense_Face_Reconstruction_with_InverseRendered_PhotoRealistic_\figure_9.jpg
  Figure 9 caption: 'Reconstruction results for faces with large poses and extreme
    expressions. Top row: several frames from one input video. Bottom row: the reconstructed
    face shapes with geometry details. See the complete sequence in the accompanying
    video or via the link: https:youtu.bedghlMXxD-rk.'
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Yudong Guo
  Name of the last author: Jianmin Zheng
  Number of Figures: 16
  Number of Tables: 3
  Number of authors: 5
  Paper title: CNN-Based Real-Time Dense Face Reconstruction with Inverse-Rendered
    Photo-Realistic Face Images
  Publication Date: 2018-05-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Results of Dense Face Reconstruction from Monocular
      Video
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparisons of Testing Errors Under Different Metrics
  Table 3 caption:
    table_text: TABLE 3 Quantitative Comparison
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2837742
- Affiliation of the first author: department of mathematics, national university
    of singapore, singapore
  Affiliation of the last author: department of mathematics, national university of
    singapore, singapore
  Figure 1 Link: articels_figures_by_rev_year\2018\A_Fast_Frequent_Directions_Algorithm_for_Low_Rank_Approximation\figure_1.jpg
  Figure 1 caption: Illustration of FD.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\A_Fast_Frequent_Directions_Algorithm_for_Low_Rank_Approximation\figure_2.jpg
  Figure 2 caption: Illustration of SpFD.
  Figure 3 Link: articels_figures_by_rev_year\2018\A_Fast_Frequent_Directions_Algorithm_for_Low_Rank_Approximation\figure_3.jpg
  Figure 3 caption: 'Results on synthetic datasets and real world datasets: w8a and
    Birds.'
  Figure 4 Link: articels_figures_by_rev_year\2018\A_Fast_Frequent_Directions_Algorithm_for_Low_Rank_Approximation\figure_4.jpg
  Figure 4 caption: 'Results on real world datasets: Protein, MNIST-all, amazon7-small
    and rcv1-small.'
  Figure 5 Link: articels_figures_by_rev_year\2018\A_Fast_Frequent_Directions_Algorithm_for_Low_Rank_Approximation\figure_5.jpg
  Figure 5 caption: Results of comparison with SFD on w8a, Birds and rcv1-small.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.95
  Name of the first author: Dan Teng
  Name of the last author: Delin Chu
  Number of Figures: 5
  Number of Tables: 3
  Number of authors: 2
  Paper title: A Fast Frequent Directions Algorithm for Low Rank Approximation
  Publication Date: 2018-05-22 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Flop Counts with Sketch Size \u2113 \u2113"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Data Statistics
  Table 3 caption:
    table_text: TABLE 3 The Top 10 Hubs, Top 10 Authorities and Running Time Used
      to Derive the Hub and Authority Scores for Networks Computational Complexity
      and Death Penalty
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2839198
- Affiliation of the first author: department of information engineering and computer
    science, university of trento, trento, italy
  Affiliation of the last author: department of information engineering and computer
    science, university of trento, trento, italy
  Figure 1 Link: articels_figures_by_rev_year\2018\Monocular_Depth_Estimation_Using_MultiScale_Continuous_CRFs_as_Sequential_Deep_N\figure_1.jpg
  Figure 1 caption: Monocular depth estimation results on three different benchmark
    datasets, i.e., NYUD-V2 (the 1st row), Make3D (the 2nd row) and Kitti (the 3rd
    row), using the proposed multi-scale CRF model with a pre-trained CNN (e.g., VGG
    Convolution-Deconvolution [9]). From left to right, each column is original RGB
    images, the recovered depth maps and the groundtruth, respectively.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Monocular_Depth_Estimation_Using_MultiScale_Continuous_CRFs_as_Sequential_Deep_N\figure_2.jpg
  Figure 2 caption: 'Overview of the proposed deep architecture. Our model is composed
    of two main components: A front-end CNN and a fusion module. The fusion module
    uses continuous CRFs to integrate multiple side output maps of the front-end CNN.
    We consider two different CRFs-based multi-scale models and implement them as
    sequential deep networks by stacking several elementary blocks, the C-MF blocks.'
  Figure 3 Link: articels_figures_by_rev_year\2018\Monocular_Depth_Estimation_Using_MultiScale_Continuous_CRFs_as_Sequential_Deep_N\figure_3.jpg
  Figure 3 caption: Illustration of different multi-scale message passing structures
    for the integration of the multi-scale predictions s 1 to s 5 produced from the
    front-end convolutional network. The arrows represent the direction of the message
    passing, and the numbers in circles represent the order. The dashed line box in
    Fig. 2 shows a bottom-up passing structure.
  Figure 4 Link: articels_figures_by_rev_year\2018\Monocular_Depth_Estimation_Using_MultiScale_Continuous_CRFs_as_Sequential_Deep_N\figure_4.jpg
  Figure 4 caption: "Detailed computing flow graph of the proposed C-MF block. J represents\
    \ a W\xD7H matrix with all elements equal to one. The symbols \u2295 , \u2296\
    \ , \u2205 and \u2297 indicate element-wise addition, subtraction, division and\
    \ Gaussian convolution operation, respectively. G1 and G2 represent two gate functions\
    \ for controlling the computing flow."
  Figure 5 Link: articels_figures_by_rev_year\2018\Monocular_Depth_Estimation_Using_MultiScale_Continuous_CRFs_as_Sequential_Deep_N\figure_5.jpg
  Figure 5 caption: Description of the proposed two CRF models as sequential deep
    networks. The blue and yellow boxes indicate the estimated variables and observations,
    respectively. The parameters beta m are used for mean-field updates. As in the
    cascade model parameters are not shared among different CRFs, we use the notation
    beta 1l, beta 2l to denote parameters associated to the l th scale.
  Figure 6 Link: articels_figures_by_rev_year\2018\Monocular_Depth_Estimation_Using_MultiScale_Continuous_CRFs_as_Sequential_Deep_N\figure_6.jpg
  Figure 6 caption: Examples of qualitative depth prediction results of different
    methods on the NYU v2 test dataset. Different front-end deep network architectures
    are investigated. Ours (VGG-CD) and Ours (ResNet-50) represent the the proposed
    multi-scale continuous CRF model plugged on the front-end CNN network of VGG-CD
    and ResNet-50, respectively.
  Figure 7 Link: articels_figures_by_rev_year\2018\Monocular_Depth_Estimation_Using_MultiScale_Continuous_CRFs_as_Sequential_Deep_N\figure_7.jpg
  Figure 7 caption: Examples of depth prediction results on the Make3D dataset. The
    four rows from up to bottom are the input test RGB images, the results produced
    from Laina et al. [30], the results of our ResNet50-MSCRF model and the groundtruth
    depth maps, respectively.
  Figure 8 Link: articels_figures_by_rev_year\2018\Monocular_Depth_Estimation_Using_MultiScale_Continuous_CRFs_as_Sequential_Deep_N\figure_8.jpg
  Figure 8 caption: Examples of depth prediction results on the KITTI raw dataset.
    Qualitative comparison with other depth estimation methods on this dataset is
    presented. The sparse ground-truth depth maps are interpolated for better visualization.
  Figure 9 Link: articels_figures_by_rev_year\2018\Monocular_Depth_Estimation_Using_MultiScale_Continuous_CRFs_as_Sequential_Deep_N\figure_9.jpg
  Figure 9 caption: Examples of depth prediction results on the KITTI raw dataset.
    The middle column and the right column show the pretrained and the fine-tuned
    estimation results respectively.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: dan xu
  Name of the last author: Nicu Sebe
  Number of Figures: 9
  Number of Tables: 7
  Number of authors: 5
  Paper title: Monocular Depth Estimation Using Multi-Scale Continuous CRFs as Sequential
    Deep Networks
  Publication Date: 2018-05-22 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Parameter Details of the Sub-Network for Generating the
      Side Output from the Last-Scale Convolutional Block of ResNet-50
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Performance Comparison of Different Front-End
      Deep Network Architectures and the Proposed Two Multi-Scale CRF Models Associated
      with the Pretrained Front-End Networks on the NYU Depth V2 Dataset
  Table 3 caption:
    table_text: TABLE 3 Quantitative Baseline Comparison with Different Multi-Scale
      Fusion Schemes, and with the Continuous CRF as a Post-Processing Module on the
      NYU Depth V2 Dataset
  Table 4 caption:
    table_text: TABLE 4 Quantitative Performance Evaluation of Different Message Passing
      Structures for the Cascade CRF Model via Building the Sequential Deep Network
      with the Proposed C-MF Block on the NYU Depth V2 Dataset
  Table 5 caption:
    table_text: TABLE 5 Overall Performance Comparison with State of the Art Methods
      on the NYU Depth V2 Dataset
  Table 6 caption:
    table_text: TABLE 6 Overall Performance Comparison with State of the Art Methods
      on the Make3D Dataset
  Table 7 caption:
    table_text: TABLE 7 Overall Performance Comparison with State of the Art Methods
      on the KITTI Raw Dataset
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2839602
- Affiliation of the first author: "ethz, z\xFCrich, switzerland"
  Affiliation of the last author: "ethz, z\xFCrich, switzerland"
  Figure 1 Link: articels_figures_by_rev_year\2018\Video_Object_Segmentation_without_Temporal_Information\figure_1.jpg
  Figure 1 caption: 'Example result of our technique: The segmentation of the first
    frame (red) is used to learn the model of the specific object to track, which
    is segmented in the rest of the frames independently (green). One every 10 frames
    shown of 90 in total.'
  Figure 10 Link: articels_figures_by_rev_year\2018\Video_Object_Segmentation_without_Temporal_Information\figure_10.jpg
  Figure 10 caption: 'Error analysis of our method: Errors divided into False Positives
    (FP-Close and FP-Far) and False Negatives (FN). Values are percentage (%) of FP-Close,
    FP-Far or FN pixels in a sequence.'
  Figure 2 Link: articels_figures_by_rev_year\2018\Video_Object_Segmentation_without_Temporal_Information\figure_2.jpg
  Figure 2 caption: 'Overview of OSVOS: (1) We start with a pre-trained base CNN for
    image labeling on ImageNet; its results in terms of segmentation, although conform
    with some image features, are not useful. (2) We then train a parent network on
    the training set of DAVIS 2016; the segmentation results improve but are not focused
    on an specific object yet. (3) By fine-tuning on a segmentation example for the
    specific target object in a single frame, the network rapidly focuses on that
    target.'
  Figure 3 Link: articels_figures_by_rev_year\2018\Video_Object_Segmentation_without_Temporal_Information\figure_3.jpg
  Figure 3 caption: 'Two-stream FCN architecture: The main foreground branch (1) is
    complemented by a contour branch (2) which improves the localization of the boundaries
    (3).'
  Figure 4 Link: articels_figures_by_rev_year\2018\Video_Object_Segmentation_without_Temporal_Information\figure_4.jpg
  Figure 4 caption: 'Qualitative evolution of the fine tuning: Results at 10 seconds
    and 1 minute per sequence.'
  Figure 5 Link: articels_figures_by_rev_year\2018\Video_Object_Segmentation_without_Temporal_Information\figure_5.jpg
  Figure 5 caption: 'Network architecture overview: Our network is composed of three
    major components: a base network as the feature extractor, and three classifiers
    built on top with shared features: a first-round foreground estimator to produce
    the semantic prior, and two conditional classifiers to model the appearance likelihood.'
  Figure 6 Link: articels_figures_by_rev_year\2018\Video_Object_Segmentation_without_Temporal_Information\figure_6.jpg
  Figure 6 caption: 'Semantic selection and propagation: Illustrative example of the
    estimation of the semantics of the object from the first frame (semantic selection)
    and its propagation to the following frames (semantic propagation).'
  Figure 7 Link: articels_figures_by_rev_year\2018\Video_Object_Segmentation_without_Temporal_Information\figure_7.jpg
  Figure 7 caption: 'Forward pass of the conditional classifier layer: Red denotes
    foreground probability, and blue background probability. The output is the weighted
    sum of the two conditional classifier.'
  Figure 8 Link: articels_figures_by_rev_year\2018\Video_Object_Segmentation_without_Temporal_Information\figure_8.jpg
  Figure 8 caption: 'Semantic selection evaluation: Semantic instances selected by
    the semantic selection step, with its category overlaid. We observe that in some
    cases either the semantic labels (h-i) or the number of instances (j) is incorrect.
    The final results, however, are robust to such mistakes.'
  Figure 9 Link: articels_figures_by_rev_year\2018\Video_Object_Segmentation_without_Temporal_Information\figure_9.jpg
  Figure 9 caption: 'DAVIS 2016 Validation: Per-sequence results of mean region similarity
    and contour accuracy ( mathcal J&mathcal F ).'
  First author gender probability: 0
  Gender of the first author: Not Available
  Gender of the last author: male
  Last author gender probability: 0.67
  Name of the first author: K.-K. Maninis
  Name of the last author: L. Van Gool
  Number of Figures: 14
  Number of Tables: 8
  Number of authors: 7
  Paper title: Video Object Segmentation without Temporal Information
  Publication Date: 2018-05-23 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Ablation Study on DAVIS 2016: From a Network Pretrained on
      ImageNet, All Improvement Steps to the Proposed OSVOS S S (Right-Most Column)'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Semantic Propagation: Comparing the Automatic Selection of
      Instances Against an Oracle and Our Final Result'
  Table 3 caption:
    table_text: 'TABLE 3 DAVIS 2016 Validation: OSVOS S S versus the State of the
      Art (Both Semi- and Un-Supervised, and a Practical Bound)'
  Table 4 caption:
    table_text: 'TABLE 4 Attribute-Based Performance ( J&F J & F ): Impact of the
      Attributes of the Sequences on the Results'
  Table 5 caption:
    table_text: 'TABLE 5 Amount of Training Data: Region Similarity ( J J ) as a Function
      of the Number of Training Images for the Parent Network of OSVOS S S'
  Table 6 caption:
    table_text: 'TABLE 6 Youtube-Objects Evaluation: Per-Category and Overall Mean
      Intersection Over Union ( J J )'
  Table 7 caption:
    table_text: 'TABLE 7 DAVIS 2017 Evaluation: Performance of OSVOS S S Compared
      to the DAVIS 2017 Challenge Winners, on the Test-Dev Set'
  Table 8 caption:
    table_text: 'TABLE 8 Performance versus Instance Segmentation Quality: Evaluation
      with Respect to the Instance Segmentation Algorithm'
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2838670
- Affiliation of the first author: department of computer science, the university
    of hong kong, hong kong
  Affiliation of the last author: department of computer science, the university of
    hong kong, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2018\Piecewise_Flat_Embedding_for_Image_Segmentation\figure_1.jpg
  Figure 1 caption: Embedding and segmentation results of piecewise flat embedding
    (PFE). Given an input image (top left), PFE transforms it into a set of embedding
    channels (bottom), each of which focuses on a subset of characteristics of the
    original image. A main highlight of our embedding is that the embedding channels
    tend to be piecewise flat. Two frameworks are tested to segment the image using
    embedding results, including clustering based segmentation [8] (top middle) and
    contour driven hierarchical segmentation (top right) [9].
  Figure 10 Link: articels_figures_by_rev_year\2018\Piecewise_Flat_Embedding_for_Image_Segmentation\figure_10.jpg
  Figure 10 caption: Initialization. The 16 probability density maps (Middle) are
    defined with Gaussian models of 16 pixel clusters formed via spectral clustering.
    Each channel in the initialization (Right) represents the mixed density of 8 Gaussian
    models. The i th channel (ordered from left to right and top to bottom) mixes
    8 density maps of lbrace (25-ij+k)% 16;|;j=1,ldots, 2i-1;;k=1,ldots, 24-irbrace
    .
  Figure 2 Link: articels_figures_by_rev_year\2018\Piecewise_Flat_Embedding_for_Image_Segmentation\figure_2.jpg
  Figure 2 caption: "Sparsity of edges crossing region boundaries in a locally connected\
    \ graph. The ratio r b of boundary length against whole image area in the top-left\
    \ image is 2.409 percent. The N 4 -connected graph of a 8\xD710 patch is visualized\
    \ (top-right). It is divided by a boundary shown in red. Only 9 among 142 edges\
    \ cross the boundary. Actually the percentage of such edges r e depends on the\
    \ length and shape of boundaries. Statical observations in all 2697 groundtruth\
    \ segmentation maps of BSDS500 are provided in bottom row: approximately linear\
    \ relation between r e and r b (left); distribution of r e among these segmentation\
    \ maps (right). Here every pixel is connected to others in a 11\xD711 neighborhood.\
    \ Average of r b and r e is 2.413 and 5.113 percent respectively."
  Figure 3 Link: articels_figures_by_rev_year\2018\Piecewise_Flat_Embedding_for_Image_Segmentation\figure_3.jpg
  Figure 3 caption: Laplacian Eigenmaps (top row) and PFE (third row) embedding results,
    and histograms of each embedding channel. PFE channels are almost piecewise constant
    while LE channels are piecewise smooth. Because of the adopted L 1,p norm, the
    histograms of our results are much sparser than those of Laplacian Eigenmaps.
  Figure 4 Link: articels_figures_by_rev_year\2018\Piecewise_Flat_Embedding_for_Image_Segmentation\figure_4.jpg
  Figure 4 caption: "Energy curves. The black curve shows how the energy decreases\
    \ with regular single-stage Bregman iterations with \u03BB=1000,r=100 . The dashed\
    \ curves show how energy evolves with our 2-stage algorithm. The first stage is\
    \ the same as the black curve. The second stage starts from iteration 50. The\
    \ three dashed curves show varying convergence rates using different parameter\
    \ values ( r=100,10,1 respectively for the red, blue and pink curves.)."
  Figure 5 Link: articels_figures_by_rev_year\2018\Piecewise_Flat_Embedding_for_Image_Segmentation\figure_5.jpg
  Figure 5 caption: Comparison of segmentations using piecewise flat embedding without
    (middle) and with (right) residual-based channel weighting. Left are input images.
  Figure 6 Link: articels_figures_by_rev_year\2018\Piecewise_Flat_Embedding_for_Image_Segmentation\figure_6.jpg
  Figure 6 caption: "Comparison between PFE and 1-Laplacian [13]. Top row from left\
    \ to right: input image; detected boundary map using [30] for affinity matrix\
    \ computation; initialization based on weighted spectral clustering as in Section\
    \ 4.3. Bottom row from left to right: curves of energy value f (PFE) and the second\
    \ smallest eigenvalue \u03BB 2 (1-Laplacian); result of 1-Laplacian; result of\
    \ PFE. 4 channels are computed in practice but only 3 embedding channels are visualized\
    \ as color channels (best viewed in close-up and color image)."
  Figure 7 Link: articels_figures_by_rev_year\2018\Piecewise_Flat_Embedding_for_Image_Segmentation\figure_7.jpg
  Figure 7 caption: Comparison of PFE-based segmentations with L1,1 (middle) and L1,0.8
    (right) regularization. Left are input images.
  Figure 8 Link: articels_figures_by_rev_year\2018\Piecewise_Flat_Embedding_for_Image_Segmentation\figure_8.jpg
  Figure 8 caption: Examples of our piecewise flat embedding. Input images are located
    in the first and third rows. Images in the second and fourth rows are generated
    by visualizing 3 embedding channels as color channels. Note that the embedding
    results clearly exhibit piecewise flatness.
  Figure 9 Link: articels_figures_by_rev_year\2018\Piecewise_Flat_Embedding_for_Image_Segmentation\figure_9.jpg
  Figure 9 caption: Image segmentation pipeline. Given an input image, our method
    generates a piecewise flat embedding from an affinity matrix of the image in a
    two-stage optimization. The channels in the embedding are used for image segmentation
    either in a clustering-based method, or a contour-driven hierarchical method.
  First author gender probability: 0.72
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.76
  Name of the first author: Chaowei Fang
  Name of the last author: Yizhou Yu
  Number of Figures: 15
  Number of Tables: 7
  Number of authors: 3
  Paper title: Piecewise Flat Embedding for Image Segmentation
  Publication Date: 2018-05-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of swPFE 0.8 0.8, Normalized Cut (NCut), Spectral
      Clustering (SC) and Its Weighted Version (WSC) on BSDS500 Using Affinity in
      [11]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of swPFE 0.8 0.8, Spectral Clustering (SC) and
      Its Weighted Version (WSC) on BSDS500 Using Affinity Computed with DB as in
      [9]
  Table 3 caption:
    table_text: TABLE 3 Hierarchical Segmentation Performance on BSDS500
  Table 4 caption:
    table_text: TABLE 4 Hierarchical Segmentation Performance on SBD Dataset
  Table 5 caption:
    table_text: TABLE 5 Hierarchical Segmentation Performance on MSRC Dataset
  Table 6 caption:
    table_text: TABLE 6 Hierarchical Segmentation Performance on PASCAL Context Dataset
  Table 7 caption:
    table_text: TABLE 7 Experiment with Various Options in the Contour-Driven Hierarchical
      Segmentation Task on BSDS500
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2839733
- Affiliation of the first author: division of computer science and engineering, louisiana
    state university, baton rouge, la
  Affiliation of the last author: university of delaware, newark, de
  Figure 1 Link: articels_figures_by_rev_year\2018\Content_Aware_Image_PreCompensation\figure_1.jpg
  Figure 1 caption: 'Effect of simple linear pre-compensation. The PSF can be from
    a typical projector, with the input image being of LDR (low dynamic range). Top
    row: With no pre-compensation, the step edge image is blurred. Bottow row: The
    pre-compensated image of a step edge under an invertible kernel incurs a significant
    increase in dynamic range (HDR, or high dynamic range). Linear tone mapping produces
    a ringing-free sharp result but significantly reduces the scale (contrast) of
    the step.'
  Figure 10 Link: articels_figures_by_rev_year\2018\Content_Aware_Image_PreCompensation\figure_10.jpg
  Figure 10 caption: 'Emulation of myopia by displaying blurred text. Left to right:
    Blurred text using the myopia kernel, result using pre-compensated image under
    linear tone mapping, result using our approach.'
  Figure 2 Link: articels_figures_by_rev_year\2018\Content_Aware_Image_PreCompensation\figure_2.jpg
  Figure 2 caption: Histograms of pre-compensated natural images. (a) Histograms of
    five natural images from BSDS500 [34] pre-compensated by one invertible kernel.
    (b) Histograms of one natural image pre-compensated using six different kernels.
    Notice that the histogram changes more dramatically over different kernels than
    over different images.
  Figure 3 Link: articels_figures_by_rev_year\2018\Content_Aware_Image_PreCompensation\figure_3.jpg
  Figure 3 caption: "Measuring contrast and ringing under non-linear tone mapping\
    \ f . (a) The sharp reference image I and kernel K ; (b) The baseline pre-compensated\
    \ image J l and its histogram H( J l ) ; (c) We compute the tangent line f m of\
    \ f at the mode of H( J l ) ; (d) Linear tone mapping using m produces ringing\
    \ free result I RF whereas f produces ringing-corrupted result I f ; (e) The contrast\
    \ of I f is nearly the same as the contrast of I RF ; (f) | I RF \u2212 I f |\
    \ approximates the ringing in I f ."
  Figure 4 Link: articels_figures_by_rev_year\2018\Content_Aware_Image_PreCompensation\figure_4.jpg
  Figure 4 caption: 'Left: The construction of an s function. Right: Tone mapping
    the pre-compensated result using different s functions: (a) The original image
    and its defocused projection without image pre-compensation; (b) & (c) are tone
    mapped and final perceived results by applying different s functions.'
  Figure 5 Link: articels_figures_by_rev_year\2018\Content_Aware_Image_PreCompensation\figure_5.jpg
  Figure 5 caption: Balance between contrast and ringing. The top two rows show the
    reference image and the blurred result if we do not pre-compensate the input.
    The bottom three rows show the results under different tone mapping functions.
    Smaller alpha leads to more contrast loss but incurs minimal ringing. Larger alpha
    enhances the contrast but incurs more ringing.
  Figure 6 Link: articels_figures_by_rev_year\2018\Content_Aware_Image_PreCompensation\figure_6.jpg
  Figure 6 caption: Experimental setup on projector defocus compensation.
  Figure 7 Link: articels_figures_by_rev_year\2018\Content_Aware_Image_PreCompensation\figure_7.jpg
  Figure 7 caption: 'Projector defocus compensation results. From left to right: reference
    sharp images; blurred images (without pre-compensation) captured by a camera;
    pre-compensation results by Zhang and Nayar''s algorithm [10], which preserves
    the contrast but exhibits strong ringing; pre-compensation results by linear tone
    mapping, which avoids ringing but loses contrast; our pre-compensation results
    enhance the contrast with only slight ringing.'
  Figure 8 Link: articels_figures_by_rev_year\2018\Content_Aware_Image_PreCompensation\figure_8.jpg
  Figure 8 caption: Comparison with simple gamma correction. (a) Linear tone mapping
    result; (b) our pre-compensation result; (c) gamma correction result with gamma
    = 12.2 ; (d) gamma correction result with gamma = 1.8 .
  Figure 9 Link: articels_figures_by_rev_year\2018\Content_Aware_Image_PreCompensation\figure_9.jpg
  Figure 9 caption: 'Effect of PSF noise. Left: original sharp images and the ground
    truth PSF. Right: Pre-compensation images (with close-ups) and their corresponding
    PSFs with noise (which increases from left to right, namely, 1 percent, 5 percent,
    and 10 percent). Notice that while the quality degrades with noise, the visual
    quality is still reasonable despite the noticeable ringing.'
  First author gender probability: 0.88
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.99
  Name of the first author: Jinwei Ye
  Name of the last author: Jingyi Yu
  Number of Figures: 15
  Number of Tables: 2
  Number of authors: 5
  Paper title: Content Aware Image Pre-Compensation
  Publication Date: 2018-05-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Table of Notations
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Effect of PSF Noise on SSIM Quality Metric
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2839115
