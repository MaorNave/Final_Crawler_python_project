- Affiliation of the first author: national laboratory of pattern recognition, institute
    of automation, chinese academy of sciences, beijing, china
  Affiliation of the last author: national laboratory of pattern recognition, institute
    of automation, chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Integrating_MultiLabel_Contrastive_Learning_With_Dual_Adversarial_Graph_Neural_N\figure_1.jpg
  Figure 1 caption: "The left picture illustrates the co-occurrence of \u201CWindow\u201D\
    \ and \u201CBuildings\u201D in the NUS-WIDE dataset. The right image shows the\
    \ co-occurrence of \u201CWater\u201D and \u201CLake\u201D in MIRFlickr dataset."
  Figure 10 Link: articels_figures_by_rev_year\2022\Integrating_MultiLabel_Contrastive_Learning_With_Dual_Adversarial_Graph_Neural_N\figure_10.jpg
  Figure 10 caption: Parameter Sensitivity of gamma on NUS-WIDE and MIRFlickr.
  Figure 2 Link: articels_figures_by_rev_year\2022\Integrating_MultiLabel_Contrastive_Learning_With_Dual_Adversarial_Graph_Neural_N\figure_2.jpg
  Figure 2 caption: An example of incomplete-modal training samples.
  Figure 3 Link: articels_figures_by_rev_year\2022\Integrating_MultiLabel_Contrastive_Learning_With_Dual_Adversarial_Graph_Neural_N\figure_3.jpg
  Figure 3 caption: 'The overall architecture of our proposed P-GNN-CON model: (1)
    In the image and text encoding network, an image CNN and a text MLP inputs images
    and texts respectively to obtain high-level semantic representations; (2) Two
    MLPs further convert high-level representations of various modalities to common
    representations; (3) In the dual generative adversarial networks, common representations
    are subsequently reconstructed to the other modality via two generators; (4) Modality
    discriminators try to distinguish between different modalities based on original
    high-level representations and reconstructed representations; (5) In the P-GNN-CON
    model, label-level word embeddings are inputted with the probabilistic correlation
    graph to learn the classifiers; (6) The label prediction is conducted by the learned
    classifiers and the common representations; (7) The Soft Multi-label Contrastive
    Loss is applied to common representations.'
  Figure 4 Link: articels_figures_by_rev_year\2022\Integrating_MultiLabel_Contrastive_Learning_With_Dual_Adversarial_Graph_Neural_N\figure_4.jpg
  Figure 4 caption: An example of a 4-layer multi-hop graph neural network.
  Figure 5 Link: articels_figures_by_rev_year\2022\Integrating_MultiLabel_Contrastive_Learning_With_Dual_Adversarial_Graph_Neural_N\figure_5.jpg
  Figure 5 caption: The overall architecture of our proposed I-GNN-CON model. Different
    from the P-GNN-CON model, an iterative learning method is introduced to update
    the label correlation graph to adaptively obtain a specific graph for the multi-label
    cross-modal retrieval task.
  Figure 6 Link: articels_figures_by_rev_year\2022\Integrating_MultiLabel_Contrastive_Learning_With_Dual_Adversarial_Graph_Neural_N\figure_6.jpg
  Figure 6 caption: "An example of biased positive samples on MIRFlickr dataset: The\
    \ positive sample x1+ contains the category \u201Cwater\u201D which is not contained\
    \ by x and the positive sample x4+ contains the categories \u201Cwater,\u201D\
    \ \u201Csky,\u201D \u201Cclouds,\u201D and \u201Dsnow\u201D which are not contained\
    \ by x ."
  Figure 7 Link: articels_figures_by_rev_year\2022\Integrating_MultiLabel_Contrastive_Learning_With_Dual_Adversarial_Graph_Neural_N\figure_7.jpg
  Figure 7 caption: Effects of various GNN models on NUS-WIDE and MIRFlickr.
  Figure 8 Link: articels_figures_by_rev_year\2022\Integrating_MultiLabel_Contrastive_Learning_With_Dual_Adversarial_Graph_Neural_N\figure_8.jpg
  Figure 8 caption: Effects of various word embeddings on NUS-WIDE and MIRFlickr dataset.
  Figure 9 Link: articels_figures_by_rev_year\2022\Integrating_MultiLabel_Contrastive_Learning_With_Dual_Adversarial_Graph_Neural_N\figure_9.jpg
  Figure 9 caption: Parameter Sensitivity of tau on NUS-WIDE and MIRFlickr.
  First author gender probability: 0.95
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Shengsheng Qian
  Name of the last author: Changsheng Xu
  Number of Figures: 13
  Number of Tables: 3
  Number of authors: 4
  Paper title: Integrating Multi-Label Contrastive Learning With Dual Adversarial
    Graph Neural Networks for Cross-Modal Retrieval
  Publication Date: 2022-07-05 00:00:00
  Table 1 caption: TABLE 1 The mAP Results on NUS-WIDE, MIRFlikcr, and MS-COCO Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The mAP Results of I-GNN-CON Variants on NUS-WIDE and MIRFlikcr
  Table 3 caption: TABLE 3 The mAP Results on NUS-WIDE, MIRFlikcr, and MS-COCO Datasets
    With Incomplete-Modal Setting (20%, 40%, 40%)
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3188547
- Affiliation of the first author: school of information, renmin university of china,
    beijing, china
  Affiliation of the last author: school of information, renmin university of china,
    beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Contrastive_Active_Learning_Under_Class_Distribution_Mismatch\figure_1.jpg
  Figure 1 caption: Illustration of class distribution mismatch. Unlabeled data contains
    some instances out of the class distribution of labeled data (i.e., unknown categories).
  Figure 10 Link: articels_figures_by_rev_year\2022\Contrastive_Active_Learning_Under_Class_Distribution_Mismatch\figure_10.jpg
  Figure 10 caption: "Distribution of queried instances based on ConAL, random sampling,\
    \ and Entropy, respectively. Specifically, the points with \u201Cblue\u201D and\
    \ \u201Cred\u201D indicate the queried instances with target and unknown categories,\
    \ respectively. The unlabeled instances with target and unknown categories are\
    \ represented by \u201Cyellow\u201D and \u201Cgray\u201D points."
  Figure 2 Link: articels_figures_by_rev_year\2022\Contrastive_Active_Learning_Under_Class_Distribution_Mismatch\figure_2.jpg
  Figure 2 caption: An AL cycle of our framework ConAL. ConAL uses the combination
    of the semantic score S sem and the distinctive score S dis to actively query
    the most informative instances in target class distribution in each AL cycle.
  Figure 3 Link: articels_figures_by_rev_year\2022\Contrastive_Active_Learning_Under_Class_Distribution_Mismatch\figure_3.jpg
  Figure 3 caption: 'Illustrative figure visualize the relation among data sets: D
    L , D U , S , Q and T .'
  Figure 4 Link: articels_figures_by_rev_year\2022\Contrastive_Active_Learning_Under_Class_Distribution_Mismatch\figure_4.jpg
  Figure 4 caption: Illustration of ConAL query strategy.
  Figure 5 Link: articels_figures_by_rev_year\2022\Contrastive_Active_Learning_Under_Class_Distribution_Mismatch\figure_5.jpg
  Figure 5 caption: Classification accuracies of ConAL and compared AL algorithms
    on CIFAR10 under different I-class mismatch proportions.
  Figure 6 Link: articels_figures_by_rev_year\2022\Contrastive_Active_Learning_Under_Class_Distribution_Mismatch\figure_6.jpg
  Figure 6 caption: Classification accuracies of ConAL and compared AL algorithms
    on CIFAR100 under different I-class mismatch proportions.
  Figure 7 Link: articels_figures_by_rev_year\2022\Contrastive_Active_Learning_Under_Class_Distribution_Mismatch\figure_7.jpg
  Figure 7 caption: Classification accuracy of ConAL and compared AL algorithms on
    a cross-dataset under different I-class mismatch proportions.
  Figure 8 Link: articels_figures_by_rev_year\2022\Contrastive_Active_Learning_Under_Class_Distribution_Mismatch\figure_8.jpg
  Figure 8 caption: Accuracy curves of ablation study.
  Figure 9 Link: articels_figures_by_rev_year\2022\Contrastive_Active_Learning_Under_Class_Distribution_Mismatch\figure_9.jpg
  Figure 9 caption: The classification accuracy of ConAL with different k and t .
  First author gender probability: 0.96
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.66
  Name of the first author: Pan Du
  Name of the last author: Cuiping Li
  Number of Figures: 14
  Number of Tables: 1
  Number of authors: 6
  Paper title: Contrastive Active Learning Under Class Distribution Mismatch
  Publication Date: 2022-07-06 00:00:00
  Table 1 caption: 'TABLE 1 The Classification Accuracies of ConAL and SSL Methods:
    DS 3 L DS3L and USAD'
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3188807
- Affiliation of the first author: "computer science laboratory, \xE9cole polytechnique,\
    \ palaiseau, france"
  Affiliation of the last author: "computer science laboratory, \xE9cole polytechnique,\
    \ palaiseau, france"
  Figure 1 Link: articels_figures_by_rev_year\2022\Permute_Me_Softly_Learning_Soft_Permutations_for_Graph_Representations\figure_1.jpg
  Figure 1 caption: Mean Squared Error and Pearson Correlation of the Frobenius distances
    with respect to the number of latent nodes.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Permute_Me_Softly_Learning_Soft_Permutations_for_Graph_Representations\figure_2.jpg
  Figure 2 caption: A heatmap of distances produced by the function of Equation (1)
    and by the proposed model.
  Figure 3 Link: articels_figures_by_rev_year\2022\Permute_Me_Softly_Learning_Soft_Permutations_for_Graph_Representations\figure_3.jpg
  Figure 3 caption: Average running time per epoch with respect to the number of vertices
    of the input graphs n (top), and to the number of latent vertices p (bottom).
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Giannis Nikolentzos
  Name of the last author: Michalis Vazirgiannis
  Number of Figures: 3
  Number of Tables: 6
  Number of authors: 3
  Paper title: 'Permute Me Softly: Learning Soft Permutations for Graph Representations'
  Publication Date: 2022-07-06 00:00:00
  Table 1 caption: TABLE 1 Summary of the Synthetic Dataset That We Used in Our Experiments
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Summary of the Ten Datasets That Were Used in Our Experiments
  Table 3 caption: "TABLE 3 Classification Accuracy (\xB1 Standard Deviation) of the\
    \ Proposed Model and the Baselines on the Ten Benchmark Datasets"
  Table 4 caption: TABLE 4 Performance on the Ogbg-Molhiv and Ogbg-Molpcba Datasets
  Table 5 caption: TABLE 5 Mean Absolute Errors of the Proposed Model and the Baselines
    on the QM9 Dataset
  Table 6 caption: "TABLE 6 Classification Accuracy (\xB1 Standard Deviation) of the\
    \ Proposed Model and the Baselines on the Ten Benchmark Datasets"
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3188911
- Affiliation of the first author: college of computer science and technology, national
    university of defense technology (nudt), changsha, china
  Affiliation of the last author: school of computer science and technology, harbin
    institue of technology (shenzhen), harbin, heilongjiang, china
  Figure 1 Link: articels_figures_by_rev_year\2022\E__Outlier_a_SelfSupervised_Framework_for_Unsupervised_Deep_Outlier_Detection\figure_1.jpg
  Figure 1 caption: An example of deep outlier image removal task.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\E__Outlier_a_SelfSupervised_Framework_for_Unsupervised_Deep_Outlier_Detection\figure_2.jpg
  Figure 2 caption: 'Overview of the proposed discriminative E 3 Outlier for deep
    OD.: Given unlabeled image data polluted by outliers, three operation sets are
    first imposed on images to create multiple pseudo classes and provide self-supervision.
    Then, a discriminative DNN is trained to perform the self-supervised learning,
    i.e., learning to classify those created pseudo classes. Next, the outlierness
    of each image is measured by the proposed network uncertainty based outlier score.
    Finally, the joint score refinement with re-weighting and ensemble strategy can
    be used to further boost the OD performance of E 3 Outlier.'
  Figure 3 Link: articels_figures_by_rev_year\2022\E__Outlier_a_SelfSupervised_Framework_for_Unsupervised_Deep_Outlier_Detection\figure_3.jpg
  Figure 3 caption: Comparison of learned image representations.
  Figure 4 Link: articels_figures_by_rev_year\2022\E__Outlier_a_SelfSupervised_Framework_for_Unsupervised_Deep_Outlier_Detection\figure_4.jpg
  Figure 4 caption: An illustration of de facto update and the average de facto update
    of inliersoutliers during the network training. The class used as inliers is in
    brackets.
  Figure 5 Link: articels_figures_by_rev_year\2022\E__Outlier_a_SelfSupervised_Framework_for_Unsupervised_Deep_Outlier_Detection\figure_5.jpg
  Figure 5 caption: Normalized histograms of inliersoutliers S gtp (x) . The class
    used as inliers is in brackets.
  Figure 6 Link: articels_figures_by_rev_year\2022\E__Outlier_a_SelfSupervised_Framework_for_Unsupervised_Deep_Outlier_Detection\figure_6.jpg
  Figure 6 caption: The uncertainty of a regression network.
  Figure 7 Link: articels_figures_by_rev_year\2022\E__Outlier_a_SelfSupervised_Framework_for_Unsupervised_Deep_Outlier_Detection\figure_7.jpg
  Figure 7 caption: AUROC comparison of OD methods under different outlier ratios.
  Figure 8 Link: articels_figures_by_rev_year\2022\E__Outlier_a_SelfSupervised_Framework_for_Unsupervised_Deep_Outlier_Detection\figure_8.jpg
  Figure 8 caption: Different factors influence on E 3 Outliers performance under
    rho =10% .
  Figure 9 Link: articels_figures_by_rev_year\2022\E__Outlier_a_SelfSupervised_Framework_for_Unsupervised_Deep_Outlier_Detection\figure_9.jpg
  Figure 9 caption: Examples of abnormal events on UCSDped1, UCSDped2, and Avenue
    datasets (walking pedestrians are normal).
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.96
  Name of the first author: Siqi Wang
  Name of the last author: Qing Liao
  Number of Figures: 9
  Number of Tables: 6
  Number of authors: 10
  Paper title: 'E 3 3Outlier: a Self-Supervised Framework for Unsupervised Deep Outlier
    Detection'
  Publication Date: 2022-07-06 00:00:00
  Table 1 caption: "TABLE 1 OD Performance Comparison (In %) in Terms of AUROC (Area\
    \ Under ROC Curve, Shorted as ROC), AUPR-In (Area Under PR Curve With Inliers\
    \ to Be the Positive Class, Shorted as PR-I) and AUPR-Out (Area Under PR Curve\
    \ With Outliers to Be the Positive Class, Shorted as PR-O). Each Benchmark Shows\
    \ the Case Where \u03C1=10% \u03C1=10% and \u03C1=20% \u03C1=20%. Note That Contrastive\
    \ E 3 3Outlier is Only Used for Benchmark Datasets With Colored Images (CIFAR10SVHNCIFAR100),\
    \ and the Raw Performance Without Score Refinement is Compared for Fairness. The\
    \ Best Performer is Shown in Bold Font"
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Performance of Discriminative E 3 3Outlier (In %) Before
    and After Joint Score Refinement (JSR) in Terms of Area Under ROC Curve, PR Curve
    With Inliers to Be the Positive Class (PR-I) and PR Curve With Outliers to Be
    the Positive Class (PR-O)
  Table 3 caption: TABLE 3 Comparison of Score Refinement Strategies (In %)
  Table 4 caption: TABLE 4 Performance Comparison (In %) of Discriminative E 3 3Outlier
    With Single-Label (SL) and Multi-Label (ML) Learning
  Table 5 caption: TABLE 5 Performance Comparison (In %) of Different DNN Models for
    Generative E 3 3Outlier
  Table 6 caption: "TABLE 6 Performance Comparison of State-of-The-Art UVAD Methods\
    \ With Our E 3 3Outlier Based UVAD Solution in Terms of Frame-Level AUC (\u201C\
    -\u201D Indicates Unreported Performance)"
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3188763
- Affiliation of the first author: institute of artificial intelligence and robotics,
    xian jiaotong university, xian, shaanxi, china
  Affiliation of the last author: wormpex ai research, bellevue, wa, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\Adaptive_TwoStream_Consensus_Network_for_WeaklySupervised_Temporal_Action_Locali\figure_1.jpg
  Figure 1 caption: Visualization of two-stream outputs and their late-fusion result.
    The five rows show the input video, the ground truth action instances and attention
    sequences (scaled from 0 to 1) predicted by the RGB stream, the flow stream and
    their weighted sum (i.e., the fusion result), respectively. The horizontal and
    vertical axes denote the time and the intensity of attention values, respectively.
    The green boxes denote the localization results generated by thresholding the
    attention at the value of 0.5. By properly combining the two different attention
    distributions predicted by the RGB and flow streams, the late-fusion result achieves
    better localization performance.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Adaptive_TwoStream_Consensus_Network_for_WeaklySupervised_Temporal_Action_Locali\figure_2.jpg
  Figure 2 caption: An overview of the proposed adaptive two-stream consensus network,
    which consists of two parts. (1) Two-stream base models, where RGB and optical
    flow snippet-level features are first extracted with pre-trained models, then
    action recognition is performed on the two modalities with two-stream base models,
    respectively. (2) Pseudo ground truth learning, where a frame-level pseudo ground
    truth is generated from the two-stream late-fusion attention sequence, along with
    video-level and snippet-level uncertainty estimators computing the confidence
    of the generated pseudo ground truth. The pseudo ground truth in turn provides
    frame-level supervision to two-stream base models.
  Figure 3 Link: articels_figures_by_rev_year\2022\Adaptive_TwoStream_Consensus_Network_for_WeaklySupervised_Temporal_Action_Locali\figure_3.jpg
  Figure 3 caption: "Comparison between models trained with different pseudo ground\
    \ truth at different refinement iterations on the THUMOS14 testing set. \u201C\
    Hard\u201D denotes models trained with hard pseudo ground truth, and \u201CSoft\u201D\
    \ denotes models trained with soft pseudo ground truth."
  Figure 4 Link: articels_figures_by_rev_year\2022\Adaptive_TwoStream_Consensus_Network_for_WeaklySupervised_Temporal_Action_Locali\figure_4.jpg
  Figure 4 caption: Per category precision-recall (PR) curves on the THUMOS14 testing
    set. The PR curve is plotted under IoU threshold 0.3. The area enclosed by the
    PR curve, x axis and y axis is average precision (AP) of each category.
  Figure 5 Link: articels_figures_by_rev_year\2022\Adaptive_TwoStream_Consensus_Network_for_WeaklySupervised_Temporal_Action_Locali\figure_5.jpg
  Figure 5 caption: Comparison between models trained with hard pseudo ground truth
    under different thresholding theta values.
  Figure 6 Link: articels_figures_by_rev_year\2022\Adaptive_TwoStream_Consensus_Network_for_WeaklySupervised_Temporal_Action_Locali\figure_6.jpg
  Figure 6 caption: Comparison between models under different fusion parameter lambda
    on the THUMOS14 testing set.
  Figure 7 Link: articels_figures_by_rev_year\2022\Adaptive_TwoStream_Consensus_Network_for_WeaklySupervised_Temporal_Action_Locali\figure_7.jpg
  Figure 7 caption: Qualitative results on the THUMOS14 testing set. The eight rows
    in each example are input video, ground truth action instance, RGB stream, flow
    stream, and fusion attention sequences from the model trained with only video-level
    labels and frame-level pseudo ground truth, respectively. Green box denotes area
    whose attention activation is higher than 0.5. The horizontal and vertical axes
    are time and intensity of attention, respectively.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.53
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.95
  Name of the first author: Yuanhao Zhai
  Name of the last author: Gang Hua
  Number of Figures: 7
  Number of Tables: 9
  Number of authors: 8
  Paper title: Adaptive Two-Stream Consensus Network for Weakly-Supervised Temporal
    Action Localization
  Publication Date: 2022-07-11 00:00:00
  Table 1 caption: TABLE 1 Comparison of Our Method With State-of-the-Art TAL Methods
    on the THUMOS14 Testing Set
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison of Our Method With State-of-the-Art TAL Methods
    on the ActivityNet v1.2 Validation Set
  Table 3 caption: TABLE 3 Comparison of Our Method With State-of-the-Art W-TAL Methods
    on the ActivityNet v1.3 Validation Set
  Table 4 caption: TABLE 4 Comparison of Our Method With State-of-the-Art TAL Methods
    on the HACS Validation Set
  Table 5 caption: TABLE 5 Ablation Study on the Adaptive Attention Normalization
    Loss L a-norm La-norm
  Table 6 caption: TABLE 6 Comparison Between the Models Trained With Only Video-Level
    labels and the Model Trained With Hard Pseudo Ground Truth on the THUMOS14 Testing
    Set
  Table 7 caption: TABLE 7 Ablation Study on Video-Level and Snippet-Level Uncertainty
    Estimators
  Table 8 caption: TABLE 8 Results of the Proposed Method in the Early-Fusion Framework
  Table 9 caption: TABLE 9 Hyperparameter Sensitivity Analysis
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3189662
- Affiliation of the first author: anu, australian national university, canberra,
    act, australia
  Affiliation of the last author: anu, australian national university, canberra, act,
    australia
  Figure 1 Link: articels_figures_by_rev_year\2022\Accurate_DoF_Camera_GeoLocalization_via_GroundtoSatellite_Image_Matching\figure_1.jpg
  Figure 1 caption: (a) Given a query ground image, we first retrieve its most similar
    satellite image from the database. (b) Then, we use a set of candidate locations
    in the satellite image as projection centers. The fine-grained location of the
    query ground image is then achieved from the projected satellite image that is
    most similar to the query image. The estimated orientation is obtained by comparing
    the selected projected image and the query image. (c) The black box represents
    a large satellite map covering the whole region, from which the small satellite
    images in the database (shown in (a)) are cropped for coarse camera localization.
    The blue dots denote the centers of those cropped images. The red boxes indicate
    the regions selected for fine-grained camera localization, which cover nearly
    the entire satellite map.
  Figure 10 Link: articels_figures_by_rev_year\2022\Accurate_DoF_Camera_GeoLocalization_via_GroundtoSatellite_Image_Matching\figure_10.jpg
  Figure 10 caption: 'Visualization of localization results attained by our method
    on the CVACTtest set. From left to right: ground-level query image and the top
    1-5 retrieved satellite candidates. Green and red borders indicate correctly and
    incorrectly retrieved results, respectively.'
  Figure 2 Link: articels_figures_by_rev_year\2022\Accurate_DoF_Camera_GeoLocalization_via_GroundtoSatellite_Image_Matching\figure_2.jpg
  Figure 2 caption: Given a satellite image (a), we explore two transforms, i.e.,
    polar transform (b) and the projective transform (c), to align it to its corresponding
    ground-view panorama (d).
  Figure 3 Link: articels_figures_by_rev_year\2022\Accurate_DoF_Camera_GeoLocalization_via_GroundtoSatellite_Image_Matching\figure_3.jpg
  Figure 3 caption: 'The challenges of cross-view image matching: the orientation
    of the query ground image is unknown, and its FoV is restricted. The scene content
    in panoramas captured at the same location but with different azimuth angles is
    offset. The image content in an image with a restricted FoV can be entirely different
    from another image captured from the same location, indicated by different boxes
    in (b). The polar-transformed satellite image (c) is an approximation to the ground
    panorama, which preserves all information from the original satellite image. The
    projective-transformed satellite image (d) loses some information, but preserves
    the ground-level geometry.'
  Figure 4 Link: articels_figures_by_rev_year\2022\Accurate_DoF_Camera_GeoLocalization_via_GroundtoSatellite_Image_Matching\figure_4.jpg
  Figure 4 caption: The overall framework of the proposed method. For the satellite
    image, we use a two-branch network that first applies a polar transform and a
    projective transform before extracting features with a CNN. For the ground image,
    we also use a two-branch network that takes the bottom half of the image corresponding
    to the projective-transformed satellite image and the whole image corresponding
    to the polar-transformed image before extracting features. Given the concatenated
    feature tensors, the correlation between the two streams is used for estimation
    of the orientation of the ground image with respect to the satellite image. Next,
    the satellite features are shifted and cropped to obtain the section that (potentially)
    corresponds to the ground features. The similarity of the resulting features is
    then used for the retrieval of location.
  Figure 5 Link: articels_figures_by_rev_year\2022\Accurate_DoF_Camera_GeoLocalization_via_GroundtoSatellite_Image_Matching\figure_5.jpg
  Figure 5 caption: Illustration of latent geometric correspondences between a satellite
    image and a ground-level panorama for pixels on the ground plane.
  Figure 6 Link: articels_figures_by_rev_year\2022\Accurate_DoF_Camera_GeoLocalization_via_GroundtoSatellite_Image_Matching\figure_6.jpg
  Figure 6 caption: Visualization of the source (satellite) and target (ground-level
    panorama) coordinate correspondences by (a) the polar transform and (b) the projective
    transform. The spatial positions corresponds to the satellite image pixels. Different
    colors in the two images indicate the row number in the target coordinates.
  Figure 7 Link: articels_figures_by_rev_year\2022\Accurate_DoF_Camera_GeoLocalization_via_GroundtoSatellite_Image_Matching\figure_7.jpg
  Figure 7 caption: Qualitative illustration of fine-grained 3-DoF camera localization
    for query images with unknown orientation and varying FoVs. Given query images
    (the last column), we first retrieve their most similar satellite images (the
    first column) from the database. The projective-transformed satellite images according
    to the query camera GPS locations are presented in the second column. The ground
    structure of those images (the second column) is significantly different from
    the query images (the last column), indicating that the GPS locations are not
    accurate. According to our fine-grained camera geo-localization method, we exhaustively
    project the retrieved satellite image to their ground panorama coordinates at
    points in a central square region of the retrieved satellite image. Among the
    projected images, the most similar ones to the query ground images are presented
    in the third column. The fourth column shows the shifted and cropped projective-transformed
    satellite images that align with query images.
  Figure 8 Link: articels_figures_by_rev_year\2022\Accurate_DoF_Camera_GeoLocalization_via_GroundtoSatellite_Image_Matching\figure_8.jpg
  Figure 8 caption: Cross-view image pairs from the CVUSA (top two rows) and CVACT
    (bottom two rows) datasets. The satellite images are on the left and the ground
    panoramas are on the right.
  Figure 9 Link: articels_figures_by_rev_year\2022\Accurate_DoF_Camera_GeoLocalization_via_GroundtoSatellite_Image_Matching\figure_9.jpg
  Figure 9 caption: Evaluations of recall at different values of K on the CVUSA, CVACTval
    and CVACTtest datasets.
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.67
  Name of the first author: Yujiao Shi
  Name of the last author: Hongdong Li
  Number of Figures: 16
  Number of Tables: 8
  Number of authors: 6
  Paper title: Accurate 3-DoF Camera Geo-Localization via Ground-to-Satellite Image
    Matching
  Publication Date: 2022-07-11 00:00:00
  Table 1 caption: TABLE 1 Comparison of Our Approach With Existing Methods on the
    CVUSA [2] Dataset
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison of Our Approach With Existing Methods on the
    CVACTval [5] Dataset by Re-Training Existing Networks
  Table 3 caption: TABLE 3 Comparison of Recall Rates for Localizing Ground Images
    With Unknown Orientations and Varying FoVs (Models Trained With Random Orientation
    Augmentation)
  Table 4 caption: TABLE 4 The Overall Performance of 3-DoF Coarse Camera Localization
  Table 5 caption: TABLE 5 Comparison of Recall Rates When the Query Image is Orientation-Aligned
    to Every Element in the Database (Standard Case) or Just the Matching Image
  Table 6 caption: TABLE 6 Comparison of Recall Rates for Localizing Ground Images
    With Unknown Orientation and Varying FoVs
  Table 7 caption: TABLE 7 Comparison of Localization Performance When Ablating the
    Transforms for Orientation Estimation
  Table 8 caption: TABLE 8 The Performance of Our Method for Fine-Grained Camera Localization
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3189702
- Affiliation of the first author: department of biomedical engineering, mathematical
    institute for data science, johns hopkins university, baltimore, md, usa
  Affiliation of the last author: department of biomedical engineering, mathematical
    institute for data science, johns hopkins university, baltimore, md, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\Fast_Hierarchical_Games_for_Image_Explanations\figure_1.jpg
  Figure 1 caption: "Expected number of visited nodes as a function of \u03C1 when\
    \ n=64,\u03B3=2,s=1 ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Fast_Hierarchical_Games_for_Image_Explanations\figure_2.jpg
  Figure 2 caption: A few saliency maps for the three settings studied in this work,
    where blue pixels have negative, white pixels have negligible, and red pixels
    have positive Shapley coefficients. The color mapping is adapted to each saliency
    map and centered around 0. For h-Shap, we show the saliency map before the normalization
    step.
  Figure 3 Link: articels_figures_by_rev_year\2022\Fast_Hierarchical_Games_for_Image_Explanations\figure_3.jpg
  Figure 3 caption: Ablation examples for all explanation methods removing all important
    pixels from the original image 3a. The model is trained to predict if a given
    image does contain a cross or not.
  Figure 4 Link: articels_figures_by_rev_year\2022\Fast_Hierarchical_Games_for_Image_Explanations\figure_4.jpg
  Figure 4 caption: "f 1 scores as a function of runtime for all explanation methods\
    \ in all three experiments. To account for noise in the explanations, we threshold\
    \ saliency maps at 1\xD7 10 \u22126 and compute f 1 scores on the resulting binary\
    \ masks. For PartitionExplainer, m indicates the maximal number of model evaluations."
  Figure 5 Link: articels_figures_by_rev_year\2022\Fast_Hierarchical_Games_for_Image_Explanations\figure_5.jpg
  Figure 5 caption: Degradation of h-Shaps maps as the minimal feature size s becomes
    smaller than the target concept.
  Figure 6 Link: articels_figures_by_rev_year\2022\Fast_Hierarchical_Games_for_Image_Explanations\figure_6.jpg
  Figure 6 caption: Example saliency maps for different labels in a multiclass setting.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jacopo Teneggi
  Name of the last author: Jeremias Sulam
  Number of Figures: 6
  Number of Tables: 0
  Number of authors: 3
  Paper title: Fast Hierarchical Games for Image Explanations
  Publication Date: 2022-07-11 00:00:00
  Table 1 caption: Not Available
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3189849
- Affiliation of the first author: department of electrical and computer engineering,
    johns hopkins university, baltimore, md, usa
  Affiliation of the last author: department of electrical and computer engineering,
    johns hopkins university, baltimore, md, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\Adversarially_Robust_OneClass_Novelty_Detection\figure_1.jpg
  Figure 1 caption: Overview of the proposed adversarially robust one-class novelty
    detection idea (PrincipaLS). The vanilla Auto-Encoder (AE) and AE+PrincipaLS are
    trained with the known class defined as digit 8. AE+PrincipaLS reconstructs every
    adversarial data into the known class (digit 8) and thus produces preferred reconstruction
    errors for novelty detection, even under attacks.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Adversarially_Robust_OneClass_Novelty_Detection\figure_2.jpg
  Figure 2 caption: 'Overview of the proposed PrincipaLS. f V : forward Vector-PCA,
    f S : forward Spatial-PCA, g S : inverse Spatial-PCA, g V : inverse Vector-PCA,
    h V and h S are the mappings for computing principal components.'
  Figure 3 Link: articels_figures_by_rev_year\2022\Adversarially_Robust_OneClass_Novelty_Detection\figure_3.jpg
  Figure 3 caption: The mAUROC of PrincipaLS under PrincipaLS-knowledgeable attacks
    with varied trade-off parameters. (a) Knowledgeable A. (b) Knowledgeable B.
  Figure 4 Link: articels_figures_by_rev_year\2022\Adversarially_Robust_OneClass_Novelty_Detection\figure_4.jpg
  Figure 4 caption: The mAUROC of models under PGD attack with varied numbers of attack
    iterations t max .
  Figure 5 Link: articels_figures_by_rev_year\2022\Adversarially_Robust_OneClass_Novelty_Detection\figure_5.jpg
  Figure 5 caption: "The mAUROC of models under PGD attack with varied perturbation\
    \ sizes \u03F5 ."
  Figure 6 Link: articels_figures_by_rev_year\2022\Adversarially_Robust_OneClass_Novelty_Detection\figure_6.jpg
  Figure 6 caption: Mean L2 -norm between the latent space of PGD adversarial examples
    and that of their clean counterpart on different defenses. The values are the
    mean over an entire dataset.
  Figure 7 Link: articels_figures_by_rev_year\2022\Adversarially_Robust_OneClass_Novelty_Detection\figure_7.jpg
  Figure 7 caption: Histograms of reconstruction errors. (a) No Defense under clean
    data. (b) No Defense under PGD attack. (c) PGD-AT under PGD attack. (d) PrincipaLS
    under PGD attack. Digit 0 of MNIST is set to normal data, and the other digits
    are anomalous.
  Figure 8 Link: articels_figures_by_rev_year\2022\Adversarially_Robust_OneClass_Novelty_Detection\figure_8.jpg
  Figure 8 caption: Reconstructions under (a) PGD attack with epsilon = 76255 and
    (b) AF attack with framing with = 1 , epsilon = 255255 . Digit 2 is set to normal
    data, and the other digits are anomalous.
  Figure 9 Link: articels_figures_by_rev_year\2022\Adversarially_Robust_OneClass_Novelty_Detection\figure_9.jpg
  Figure 9 caption: Reconstructions under PGD attack with epsilon = 25255 . Digit
    0 and digit 2 are set to normal data, and the other digits are anomalous.
  First author gender probability: 0.78
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Shao-Yuan Lo
  Name of the last author: Vishal M. Patel
  Number of Figures: 9
  Number of Tables: 9
  Number of authors: 3
  Paper title: Adversarially Robust One-Class Novelty Detection
  Publication Date: 2022-07-11 00:00:00
  Table 1 caption: TABLE 1 The mAUROC of Models Under Various Adversarial Attacks
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The mAUROC of Models Under PGD Attack
  Table 3 caption: TABLE 3 The mAUROC of Models Under Clean Data
  Table 4 caption: TABLE 4 The Inference Speed of Each Defense
  Table 5 caption: TABLE 5 The mAUROC of Different PrincipaLS Variants Under PGD Attack
  Table 6 caption: TABLE 6 The Trade-Off Analysis of PrincipaLSs k V kV and k S kS
    Values on MNIST Dataset
  Table 7 caption: TABLE 7 The mAUROC of Models Under PGD, PGD-Normal, PGD-Latent,
    PGD-Clean, and PGD-Anomalous Attacks
  Table 8 caption: TABLE 8 Image Classification Accuracy (%) on CIFAR-10
  Table 9 caption: TABLE 9 The AUROC of Multiple Class Novelty Detection on MNIST
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3189638
- Affiliation of the first author: centre for perceptual and interactive intelligence,
    hong kong sar, china
  Affiliation of the last author: university of california, los angeles, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\MetaDrive_Composing_Diverse_Driving_Scenarios_for_Generalizable_Reinforcement_Le\figure_1.jpg
  Figure 1 caption: A. MetaDrive supports importing maps and traffic flow from real-world
    dataset. B. A road map procedurally generated from elementary road blocks. C.
    Multi-modal observations provided by MetaDrive, including Lidar-like point clouds,
    RGB depth camera, bird-view semantic map, and scalar sensory data. D. MetaDrive
    supports the control and demonstration from human subject.
  Figure 10 Link: articels_figures_by_rev_year\2022\MetaDrive_Composing_Diverse_Driving_Scenarios_for_Generalizable_Reinforcement_Le\figure_10.jpg
  Figure 10 caption: Since the training time safety is critical to Safe RL, we show
    the learning progress of different Safe RL methods. Though achieves superior sample
    efficiency, the reward shaping version of SAC induces a peak in the training cost,
    while the Lagrangian SAC improves the policy while satisfying the safety constraint.
  Figure 2 Link: articels_figures_by_rev_year\2022\MetaDrive_Composing_Diverse_Driving_Scenarios_for_Generalizable_Reinforcement_Le\figure_2.jpg
  Figure 2 caption: MetaDrive can compose new scenarios by combining various managers.
    The first column shows two original cases from PG and Waymo datasets. In the second
    column, by replacing the Single-agent Manager with the Multi-agent Manager, we
    compose the multi-agent navigation tasks in two maps. In the third column, IDM
    Traffic Manager is added to both maps, enabling responsive traffic vehicles following
    IDM policies. The last column indicates that it also supports composing safety-critical
    environments on both PG map and real map by adding the Object Manager.
  Figure 3 Link: articels_figures_by_rev_year\2022\MetaDrive_Composing_Diverse_Driving_Scenarios_for_Generalizable_Reinforcement_Le\figure_3.jpg
  Figure 3 caption: First row shows the basic road blocks and the other rows plot
    the generated maps with different block numbers.
  Figure 4 Link: articels_figures_by_rev_year\2022\MetaDrive_Composing_Diverse_Driving_Scenarios_for_Generalizable_Reinforcement_Le\figure_4.jpg
  Figure 4 caption: MetaDrive can derive diverse scenarios with different config in
    the input config.
  Figure 5 Link: articels_figures_by_rev_year\2022\MetaDrive_Composing_Diverse_Driving_Scenarios_for_Generalizable_Reinforcement_Le\figure_5.jpg
  Figure 5 caption: MetaDrive can load real scenarios from Argoverse dataset [5] and
    Waymo dataset [51].
  Figure 6 Link: articels_figures_by_rev_year\2022\MetaDrive_Composing_Diverse_Driving_Scenarios_for_Generalizable_Reinforcement_Le\figure_6.jpg
  Figure 6 caption: A simulation scenario is replicated from the real traffic data
    of Argoverse dataset [5]. The traffic vehicles are actuated by Replay Traffic
    Manager.
  Figure 7 Link: articels_figures_by_rev_year\2022\MetaDrive_Composing_Diverse_Driving_Scenarios_for_Generalizable_Reinforcement_Le\figure_7.jpg
  Figure 7 caption: A. Safe driving environments where obstacles such as broken down
    vehicles and traffic cones are randomly scattered on the map. B. Multi-agent environments
    where all agents need to coordinate their driving behaviors to achieve the population
    efficiency.
  Figure 8 Link: articels_figures_by_rev_year\2022\MetaDrive_Composing_Diverse_Driving_Scenarios_for_Generalizable_Reinforcement_Le\figure_8.jpg
  Figure 8 caption: The generalization result of the agents trained with off-policy
    RL algorithm Soft Actor-critic (SAC) [14] and on-policy RL algorithm PPO [45].
    Increasing the number of training scenarios leads to a higher test success rate
    and lower traffic rule violation and crash probability, which indicates the agents
    generalization is significantly improved. Compared to PPO, the SAC algorithm brings
    a more stable training performance. The shadow of the curves indicates the standard
    deviation.
  Figure 9 Link: articels_figures_by_rev_year\2022\MetaDrive_Composing_Diverse_Driving_Scenarios_for_Generalizable_Reinforcement_Le\figure_9.jpg
  Figure 9 caption: A. Waymo generalization experiment with a changing number of scenarios
    contained in training set. B. The test performance of PG-map-trained policies
    in real-world scenarios. C. Result of the policies trained on 5 training sets
    consisting of different mixing ratio of real and synthetic data.
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.75
  Name of the first author: Quanyi Li
  Name of the last author: Bolei Zhou
  Number of Figures: 11
  Number of Tables: 4
  Number of authors: 6
  Paper title: 'MetaDrive: Composing Diverse Driving Scenarios for Generalizable Reinforcement
    Learning'
  Publication Date: 2022-07-13 00:00:00
  Table 1 caption: TABLE 1 Comparison of Representative Driving Simulators
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Hyper-Parameters of Benchmarked Methods
  Table 3 caption: TABLE 3 The Test Performance of Different Approaches in Safe Exploration
  Table 4 caption: TABLE 4 Success Rate (%) of Different Approaches in Multi-Agent
    RL Benchmarks
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3190471
- Affiliation of the first author: school of mathematics, sun yat-sen university,
    guangzhou, china
  Affiliation of the last author: school of mathematics, sun yat-sen university, guangzhou,
    china
  Figure 1 Link: articels_figures_by_rev_year\2022\BuresNet_Conditional_Bures_Metric_for_Transferable_Representation_Learning\figure_1.jpg
  Figure 1 caption: "(a): Illustration of the conditional distribution shift problem,\
    \ where \u2022 and \u25A0 represent different classes. (b): Many existing metrics\
    \ that only consider the marginal distribution discrepancy may lead to the misclassified\
    \ samples, i.e., the red points. (c): The cluster structure alignment or predictor\
    \ alignment is achieved by exploiting the conditional distribution embedding metric.\
    \ Best viewed in color."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\BuresNet_Conditional_Bures_Metric_for_Transferable_Representation_Learning\figure_2.jpg
  Figure 2 caption: Illustration of BuresNet for UDA problems. The features are learned
    by deep networks and then mapped into the RKHS, where the conditional distributions
    of the domains are represented by their conditional covariance operators. Then
    the conditional distribution discrepancy is estimated by the CKB metric, and the
    adaptation model is optimized according to the discrepancy feedback. Best viewed
    in color.
  Figure 3 Link: articels_figures_by_rev_year\2022\BuresNet_Conditional_Bures_Metric_for_Transferable_Representation_Learning\figure_3.jpg
  Figure 3 caption: 'Illustration of BuresNet for FSL problems. (a): The dataset shift
    may exists in the embedding space, and the misclassification occurs in Q , i.e.,
    the red points. (b): The posterior distribution regularization via CKB will minimize
    the distance between the optimal predictors on S and Q . Then the minimal risk
    will be achieved on Q during the testing stage. Best viewed in color.'
  Figure 4 Link: articels_figures_by_rev_year\2022\BuresNet_Conditional_Bures_Metric_for_Transferable_Representation_Learning\figure_4.jpg
  Figure 4 caption: "Experiments of kernel parameters on Image-CLEF-DA P \u2192 I\
    \ task and Office-Home Ar \u2192 Pr task. (a)-(b): The curves and 95% confidence\
    \ intervals of on-the-fly kernel parameter \u03C3 in Eq. (21). (c)-(d): Classification\
    \ results of different \u03C3 values. \u201COTF\u201D denotes the on-the-fly setting\
    \ for \u03C3 . \u201CMD\u201D and the remainders imply \u03C3 is fixed as the\
    \ initialized mean distance or other predefined values."
  Figure 5 Link: articels_figures_by_rev_year\2022\BuresNet_Conditional_Bures_Metric_for_Transferable_Representation_Learning\figure_5.jpg
  Figure 5 caption: "(a)-(b): Analysis of hyper-parameters \u03BB Ent and \u03BB CKB\
    \ on Image-CLEF-DA. The classification accuracies of BuresNet are robust to changes\
    \ in hyper-parameters. Besides, the conditional distribution alignment is crucial\
    \ for knowledge transfer since better accuracies are achieved when the weight\
    \ of CKB term (i.e., \u03BB CKB ) is larger. (c)-(d): Ablation analysis on Image-CLEF-DA.\
    \ Compared with the entropy term and other alignment metrics, the CKB term improves\
    \ the classification accuracy significantly. Best viewed in color. (e)-(h): Feature\
    \ visualization of source-only model and BuresNet on Image-CLEF-DA C \u2192 I\
    \ task. +: source domain, \u2022: target domain, where features are colored by\
    \ domains in (e)-(f) and classes in (g)-(h), respectively. Best viewed in color."
  Figure 6 Link: articels_figures_by_rev_year\2022\BuresNet_Conditional_Bures_Metric_for_Transferable_Representation_Learning\figure_6.jpg
  Figure 6 caption: "Feature visualization of the baseline models and their extensions\
    \ with BuresNet on FS-Office-Home Cl \u2192 Pr. \u25A0 : prototypes in support\
    \ set (source domain), \u2022: samples in query set (target domain). The background\
    \ color represents the decision boundary of NPC. Best viewed in color."
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.95
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.76
  Name of the first author: Chuan-Xian Ren
  Name of the last author: Dao-Qing Dai
  Number of Figures: 6
  Number of Tables: 7
  Number of authors: 3
  Paper title: 'BuresNet: Conditional Bures Metric for Transferable Representation
    Learning'
  Publication Date: 2022-07-13 00:00:00
  Table 1 caption: TABLE 1 Mean Accuracies (%) on Office-Home (ResNet-50), Office-10
    (AlexNet), and VisDA-2017 (ResNet-101)
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Mean Accuracies (%) on Image-CLEF-DA (ResNet-50)
  Table 3 caption: TABLE 3 Mean Accuracies (%) on Digits (LeNet)
  Table 4 caption: TABLE 4 Mean Accuracy (%) and 95% Confidence Interval of Five-Way
    Cross-Domain Few-Shot Classification on FS-Office-Home
  Table 5 caption: TABLE 5 Mean Accuracy (%) of Five-Way Few-Shot Classification on
    Mini-ImageNet
  Table 6 caption: TABLE 6 Mean Accuracy (%) of Five-Way Few-Shot Classification on
    Tiered-ImageNet
  Table 7 caption: "TABLE 7 Mean Domain Discrepancy and 95% Confidence Interval of\
    \ Five-Way Few-Shot Classification on FS-Office-Home Cl \u2192 \u2192 Pr"
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3190645
