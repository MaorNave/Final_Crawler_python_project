- Affiliation of the first author: snapchat research, venice, 90291
  Affiliation of the last author: department of information engineering and computer
    science, university of trento, trento, tn, italy
  Figure 1 Link: articels_figures_by_rev_year\2017\ViewpointConsistent_D_Face_Alignment\figure_1.jpg
  Figure 1 caption: Viewpoint-consistent 3D landmarks (a) represent the same underlying
    3D structure for every frame (b).
  Figure 10 Link: articels_figures_by_rev_year\2017\ViewpointConsistent_D_Face_Alignment\figure_10.jpg
  Figure 10 caption: Cumulative error distributions for CVGTC and GTC for different
    methods.
  Figure 2 Link: articels_figures_by_rev_year\2017\ViewpointConsistent_D_Face_Alignment\figure_2.jpg
  Figure 2 caption: 'Top: Conventional viewpoint-inconsistent landmarks. Bottom: 3D
    landmarks annotated in a viewpoint-consistent manner. Right: Landmarks from different
    viewpoints plotted in the common coordinate system. The third dimension of the
    inconsistent landmarks was estimated by fitting a 3D deformable model to the estimated
    2D landmarks ( Section 4). Bringing the landmarks to the common coordinate system
    is discussed in Section 5.3.'
  Figure 3 Link: articels_figures_by_rev_year\2017\ViewpointConsistent_D_Face_Alignment\figure_3.jpg
  Figure 3 caption: An example of the actual landmark positions. Left image shows
    an annotated mesh with several landmarks occluded. Central image shows the landmarks
    on the frontal face. Right image shows the projections of the actual landmarks
    onto the image plane.
  Figure 4 Link: articels_figures_by_rev_year\2017\ViewpointConsistent_D_Face_Alignment\figure_4.jpg
  Figure 4 caption: Examples of face bases estimated using S wR (green) and S c (pink).
    Note that the bases estimated using shapes in the world coordinate system (green)
    are more consistent with the head rotation. The detected points are plotted in
    green. The background was removed for visualization purposes after detection.
  Figure 5 Link: articels_figures_by_rev_year\2017\ViewpointConsistent_D_Face_Alignment\figure_5.jpg
  Figure 5 caption: Comparisons of viewpoint-consistent 3D landmarks (blue) and inconsistent
    2D landmarks (red). In yellow we highlight several major differences located on
    the jaw-line, eyebrows, nose. The viewpoint-consistent landmarks were obtained
    using the method discussed in this paper. The inconsistent landmarks are taken
    from [58].
  Figure 6 Link: articels_figures_by_rev_year\2017\ViewpointConsistent_D_Face_Alignment\figure_6.jpg
  Figure 6 caption: "Top: Cumulative error distribution rates for head pose estimation\
    \ for yaw and pitch angles. We do not report results of [43] for pitch since the\
    \ method provides only yaw estimates. To initialize our method we used the face\
    \ detector in [64]. The benchmark systems use their internal face detectors to\
    \ find faces. Bottom: The distribution of the fraction of correctly recognized\
    \ images within \xB1 15 \u2218 error tolerance over the yaw angle."
  Figure 7 Link: articels_figures_by_rev_year\2017\ViewpointConsistent_D_Face_Alignment\figure_7.jpg
  Figure 7 caption: Selected examples from the benchmark datasets. Selected views
    from the MultiPIE-VC (a), automatic 3D annotation performed using ZFace tracker
    (b). Rendered images of the BU-4DFE dataset with 3D ground-truth.
  Figure 8 Link: articels_figures_by_rev_year\2017\ViewpointConsistent_D_Face_Alignment\figure_8.jpg
  Figure 8 caption: 'Pairwise view consistencies for every method are reported. Top:
    CVPC. Bottom: CVGTC. Cross-view transformation parameters were estimated using
    Eq. (6).'
  Figure 9 Link: articels_figures_by_rev_year\2017\ViewpointConsistent_D_Face_Alignment\figure_9.jpg
  Figure 9 caption: Selected images from the 300-W [58] challenge are plotted. Each
    example contains the original image with the plotted 66 landmarks and the landmarks
    in 3D space.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Sergey Tulyakov
  Name of the last author: Nicu Sebe
  Number of Figures: 10
  Number of Tables: 8
  Number of authors: 4
  Paper title: Viewpoint-Consistent 3D Face Alignment
  Publication Date: 2017-09-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Available Databases That Were Used for Face
      Alignment in the Literature
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Landmark Localization Errors
  Table 3 caption:
    table_text: TABLE 3 Head Pose Estimation Results
  Table 4 caption:
    table_text: TABLE 4 Head Pose Estimation Results Obtained on the Boston University
      Head Tracking Dataset, Presented in the Mean Absolute Angular Error in Degrees
  Table 5 caption:
    table_text: TABLE 5 Head Pose Estimation Results Obtained on the HPDB Database,
      Present in the Mean Absolute Angular Error in Degrees
  Table 6 caption:
    table_text: TABLE 6 Viewpoint-Consistent Landmarks Detection Accuracy Among Different
      Methods on MultiPIE-VC Database (Best Performance in Bold)
  Table 7 caption:
    table_text: TABLE 7 Distribution of the Different Sets
  Table 8 caption:
    table_text: TABLE 8 Prediction Consistency Error (CVGTC) and Ground Truth Consistency
      (GTC) of the Different Methods on the Test Set
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2750687
- Affiliation of the first author: school of computer science, the university of adelaide,
    adelaide, sa, australia
  Affiliation of the last author: school of computer science, the university of adelaide,
    adelaide, sa, australia
  Figure 1 Link: articels_figures_by_rev_year\2017\Coresets_for_Triangulation\figure_1.jpg
  Figure 1 caption: "Triangulating a point x observed in 10 views. The red '+' is\
    \ the \u2113 \u221E solution x \u2217 . Observe that there are four viewsmeasurements\
    \ with the same residual at x \u2217 . The index of the support set is thus B=7,8,9,10\
    \ ."
  Figure 10 Link: articels_figures_by_rev_year\2017\Coresets_for_Triangulation\figure_10.jpg
  Figure 10 caption: Results for Vercingetorix Statue. (top left) Histogram of problem
    sizes. (top right) Approximation error ratio versus error ratio bound. (bottom
    left) Runtime of coreset versus batch, for bisection solver. (bottom right) Runtime
    of coreset versus batch, for Dinkelbach solver.
  Figure 2 Link: articels_figures_by_rev_year\2017\Coresets_for_Triangulation\figure_2.jpg
  Figure 2 caption: "A sample run of Algorithm 1 on the data displayed in Fig. 1.\
    \ (a) Four image measurementscamera viewpoints (in red) were selected to form\
    \ the initial coreset C 1 . The current solution x 1 is shown as a red cross.\
    \ (b)\u2013(d) Algorithm 1 progressively inserts new data into the coreset. Data\
    \ in the current coreset is shown in black, and the newly inserted datum (chosen\
    \ according to Step 6) is shown in red. Similarly, the previous solutions x s\
    \ are shown in black, and the current estimate is shown in red. If terminated\
    \ at t=\u23082\u03F5\u2309 , the estimate is a \u03F5 -approximation of the true\
    \ optimum. Iterated until convergence, the global optimum is achieved. For anytime\
    \ behaviour, the error bound can be backtracked (see Section 3.5) to obtain the\
    \ approximation error of the last estimate at termination."
  Figure 3 Link: articels_figures_by_rev_year\2017\Coresets_for_Triangulation\figure_3.jpg
  Figure 3 caption: "Definition of several geometrical quantities for \u2113 \u221E\
    \ triangulation."
  Figure 4 Link: articels_figures_by_rev_year\2017\Coresets_for_Triangulation\figure_4.jpg
  Figure 4 caption: 'The four types of synthetically generated triangulation instances.
    Type A: Camera positions are on a straight line; Type B: Camera positions are
    randomly distributed; Type C: Camera positions are on a circle; Type D: Stereo
    cameras with fixed baselines and positions randomly distributed. In all cases,
    the cameras are roughly oriented towards the 3D point to respect cheirality.'
  Figure 5 Link: articels_figures_by_rev_year\2017\Coresets_for_Triangulation\figure_5.jpg
  Figure 5 caption: Approximation error ratios (39) across counter t (red curves)
    for 200 synthetically generated triangulation instances. The blue curve is the
    upper bound on the error ratio ( 1+2t ) as predicted by Theorem 2.
  Figure 6 Link: articels_figures_by_rev_year\2017\Coresets_for_Triangulation\figure_6.jpg
  Figure 6 caption: (a) Approximation error ratio (39) plotted against coreset size
    for one of the synthetic data instances. In this instance, Condition 2 occurred
    as the coreset was increased from size 8 to 9, thus the bound (40) does not decrease
    between these two iterations. (b) Approximation error ratio plotted against coreset
    size for all the synthetic data instances. Since Condition 2 occurred at different
    iterations for the respective problem instances, the bounding curve is not plotted.
  Figure 7 Link: articels_figures_by_rev_year\2017\Coresets_for_Triangulation\figure_7.jpg
  Figure 7 caption: Average alpha (probability of occurrence of Condition 2) as the
    problem size N increases, separated according to the type of camera pose distribution
    (see Fig. 4).
  Figure 8 Link: articels_figures_by_rev_year\2017\Coresets_for_Triangulation\figure_8.jpg
  Figure 8 caption: Average size of coreset plotted against decreasing epsilon , separated
    according to the four types of camera distribution as in Fig. 4. Panel (a),(b),(c),
    and (d) are the results respectively for N = 100 , 500 , 1000 , and 5000.
  Figure 9 Link: articels_figures_by_rev_year\2017\Coresets_for_Triangulation\figure_9.jpg
  Figure 9 caption: Comparing sim and Hartley's outlier removal scheme [20] with an
    exact solver (Dinkelbach's method) and with Algorithm 1. (a) Average runtime;
    (b) Number of inliers remaining in the final inlier set (the dashed green line
    to indicate the number of inliers in original input).
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.94
  Name of the first author: Qianggong Zhang
  Name of the last author: Tat-Jun Chin
  Number of Figures: 15
  Number of Tables: 2
  Number of authors: 2
  Paper title: Coresets for Triangulation
  Publication Date: 2017-09-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparisons Between Coreset and Batch in Terms of Total Runtime
      in Seconds
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Comparisons Between Coreset and Batch in Terms of Total Runtime\
      \ in Seconds, Under the \u2113 1 and \u2113 \u221E Reprojection Error (41)"
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2750672
- Affiliation of the first author: computational sciences engineering division, oak
    ridge national laboratory, oak ridge, tn
  Affiliation of the last author: computational sciences engineering division, oak
    ridge national laboratory, oak ridge, tn
  Figure 1 Link: articels_figures_by_rev_year\2017\Learning_Building_Extraction_in_Aerial_Scenes_with_Convolutional_Networks\figure_1.jpg
  Figure 1 caption: A ConvNet integrating multi-stage feature maps. Solid arrows represent
    convolutional operations, and dotted arrows upsampling.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Learning_Building_Extraction_in_Aerial_Scenes_with_Convolutional_Networks\figure_2.jpg
  Figure 2 caption: Output representation. (a) Original image. (b) Boundary map. (c)
    Region map. (d) Signed distance transform. Gray values are proportional to distance
    values.
  Figure 3 Link: articels_figures_by_rev_year\2017\Learning_Building_Extraction_in_Aerial_Scenes_with_Convolutional_Networks\figure_3.jpg
  Figure 3 caption: 'Reducing misalignment between maps and images. Left: building
    footprints (red) directly overlaid with the image. Right: after alignment adjustment.'
  Figure 4 Link: articels_figures_by_rev_year\2017\Learning_Building_Extraction_in_Aerial_Scenes_with_Convolutional_Networks\figure_4.jpg
  Figure 4 caption: D.C. building footprint map. Training and test areas are divided
    roughly along Constitution Avenue.
  Figure 5 Link: articels_figures_by_rev_year\2017\Learning_Building_Extraction_in_Aerial_Scenes_with_Convolutional_Networks\figure_5.jpg
  Figure 5 caption: Example results of two images. Transparent red regions indicate
    buildings extracted by our method, and blue pixels show detected building boundaries.
    For each image, a zoomed-in view is provided with building boundaries marked in
    blue.
  Figure 6 Link: articels_figures_by_rev_year\2017\Learning_Building_Extraction_in_Aerial_Scenes_with_Convolutional_Networks\figure_6.jpg
  Figure 6 caption: Example results from three different networks. The left column
    shows original images, the middle left the FCN output, the middle right the output
    from the network trained with region maps, and the right the output from our complete
    method. Extracted buildings are marked in transparent red.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.61
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.61
  Name of the first author: Jiangye Yuan
  Name of the last author: Jiangye Yuan
  Number of Figures: 6
  Number of Tables: 2
  Number of authors: 1
  Paper title: Learning Building Extraction in Aerial Scenes with Convolutional Networks
  Publication Date: 2017-09-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Using Different Networks
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison on Training with Different Sample Sizes
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2750680
- Affiliation of the first author: "computer vision and multimodal computing group,\
    \ max planck institute for informatics, saarbr\xFCcken, saarland, germany"
  Affiliation of the last author: "computer vision and multimodal computing group,\
    \ max planck institute for informatics, saarbr\xFCcken, saarland, germany"
  Figure 1 Link: articels_figures_by_rev_year\2017\Analysis_and_Optimization_of_Loss_Functions_for_Multiclass_Topk_and_Multilabel_C\figure_1.jpg
  Figure 1 caption: 'Class ambiguity with a single label on Places 205 [2]. Labels:
    Valley, Pasture, Mountain; Ski resort, Chalet, Sky. Note that multiple labels
    apply to each image and k guesses may be required to guess the ground truth label
    correctly.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Analysis_and_Optimization_of_Loss_Functions_for_Multiclass_Topk_and_Multilabel_C\figure_2.jpg
  Figure 2 caption: "(a) Scaling of the projection algorithms used in SDCA optimization.\
    \ (b-c) SDCA convergence of the L R Multi , SV M Multi , and smooth SV M Multi\
    \ \u03B3 methods on the ImageNet 2012 dataset. (d) SDCA versus FISTA as implemented\
    \ in the SPAMS toolbox."
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Maksim Lapin
  Name of the last author: Bernt Schiele
  Number of Figures: 2
  Number of Tables: 10
  Number of authors: 3
  Paper title: Analysis and Optimization of Loss Functions for Multiclass, Top-k,
    and Multilabel Classification
  Publication Date: 2017-09-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Overview of the Methods Considered in This Work and Our Contributions
  Table 10 caption:
    table_text: TABLE 10 MS COCO Multilabel Classification Results
  Table 2 caption:
    table_text: "TABLE 2 Runtime (in Seconds) for Solving 1000 Projection Problems\
      \ onto B(\u03C1) with \u03C1=10 and m=n=d2 , see Eq. (17)"
  Table 3 caption:
    table_text: 'TABLE 3 Statistics of Multiclass Classification Benchmarks ( m :
      Classes, n : Training Examples, d : Features)'
  Table 4 caption:
    table_text: TABLE 4 Top- k Accuracy (%) on Various Datasets
  Table 5 caption:
    table_text: TABLE 5 Top- k Accuracy (%), as Reported by Caffe [104], on Large
      Scale Datasets After Fine-Tuning (FT) for Approximately One Epoch on Places
      and 3 Epochs on ImageNet
  Table 6 caption:
    table_text: 'TABLE 6 Statistics of Multilabel Benchmarks ( m : Classes, n : Training
      Examples, d : Features, l c : Label Cardinality)'
  Table 7 caption:
    table_text: TABLE 7 Multilabel Classification
  Table 8 caption:
    table_text: TABLE 8 Pascal VOC 2007 Classification Results
  Table 9 caption:
    table_text: TABLE 9 Pascal VOC 2007 Multilabel Classification Results
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2751607
- Affiliation of the first author: "institut de rob\xF2tica i inform\xE0tica industrial\
    \ (csic-upc), barcelona, spain"
  Affiliation of the last author: "institut de rob\xF2tica i inform\xE0tica industrial\
    \ (csic-upc), barcelona, spain"
  Figure 1 Link: articels_figures_by_rev_year\2017\ForceBased_Representation_for_NonRigid_Shape_and_Elastic_Model_Estimation\figure_1.jpg
  Figure 1 caption: "Force-based NRSfM. The proposed algorithm allows to simultaneously\
    \ recover the 3D non-rigid shape, camera motion and the full elastic model, from\
    \ a sequence of 2D point trajectories acquired with a monocular camera. Self-occlusions\
    \ (see the track annotated as \u201Dnon-visible\u201D) can also be naturally handled\
    \ with our formulation."
  Figure 10 Link: articels_figures_by_rev_year\2017\ForceBased_Representation_for_NonRigid_Shape_and_Elastic_Model_Estimation\figure_10.jpg
  Figure 10 caption: 'Estimating forces that produce a specific deformation for the
    ASL1 sequence. Once the compliance matrix is learned, we can estimate the forces
    that define any shape in the full physical space. First column: Four target shapes.
    Second column: Compliance matrix C estimated without assuming boundary conditions.
    f 1 , f 2 , f 3 , f 4 , are the forces necessary to deform the mean shape into
    the target shapes. Third column: Random symmetric matrix C r , and the corresponding
    forces f r 1 , f r 2 , f r 3 , f r 4 to produce the target shapes. Fourth column:
    Compliance matrix C bc estimated after enforcing B=14 boundary-condition constraints,
    and the corresponding forces to produce the target shapes. The figure is best
    viewed in color.'
  Figure 2 Link: articels_figures_by_rev_year\2017\ForceBased_Representation_for_NonRigid_Shape_and_Elastic_Model_Estimation\figure_2.jpg
  Figure 2 caption: Intuition behind our approach. The non-rigid shape with N vertexes
    can be encoded in terms of its underlying elastic model C (compliance matrix)
    and the force field f acting on it. In turn, the full force field can be approximated
    by a low-rank basis F ~ . In this work, we simultaneously learn both the elastic
    model and the low-rank force space, while recovering shape and camera motion.
    The figure shows the full force-space and its corresponding shapes in red together
    with force vectors. The low-rank force and the corresponding shapes are shown
    in blue; and a tentative shape subspace S ~ in green.
  Figure 3 Link: articels_figures_by_rev_year\2017\ForceBased_Representation_for_NonRigid_Shape_and_Elastic_Model_Estimation\figure_3.jpg
  Figure 3 caption: "Equivalence between Shape, Trajectory and Force low-rank models.\
    \ The matrix S of temporal shapes shown on the left is approximated using four\
    \ low-rank spaces: Force, Shape, Trajectory and Shape-Trajectory (from top to\
    \ bottom). The dotted lines, represent the arrangement of the shapes and bases\
    \ within the matrices. The low-rank force model (top row), incorporates the compliance\
    \ matrix C to encode the full elastic physical model, which by direct comparison\
    \ with the other three statistical sub-spaces, let us to give them also physical\
    \ interpretation. Note that for the shape-trajectory model (bottom row), Q\u2260\
    R as the number of trajectory vectors is usually larger than the number of shape\
    \ bases."
  Figure 4 Link: articels_figures_by_rev_year\2017\ForceBased_Representation_for_NonRigid_Shape_and_Elastic_Model_Estimation\figure_4.jpg
  Figure 4 caption: 3D Reconstruction error as a function of the rank Q of the force
    subspace. Results on the four Mocap sequences. For the Jacky and Flag sequences
    the rank Q has no influence on the reconstruction errors. The other two sequences
    (Face and Walking are more sensitive, but the error always remains within reasonable
    bounds.
  Figure 5 Link: articels_figures_by_rev_year\2017\ForceBased_Representation_for_NonRigid_Shape_and_Elastic_Model_Estimation\figure_5.jpg
  Figure 5 caption: 'Beating heart sequence. Top: 2D tracking data and reconstructed
    3D shape reprojected onto several images with green circles and red dots, respectively.
    Middle: Reconstructed 3D shape, color code such that reddish areas indicate larger
    displacements. Bottom: Reconstructed 3D shape, using the original texture. Best
    viewed in color.'
  Figure 6 Link: articels_figures_by_rev_year\2017\ForceBased_Representation_for_NonRigid_Shape_and_Elastic_Model_Estimation\figure_6.jpg
  Figure 6 caption: 'Actress sequence. Top: 2D tracking data (green circles) and reprojection
    (red dots) of the reconstructed 3D shape. Middle: Camera and side-views of the
    reconstructed shapes obtained by our approach. Bottom: Same views using EM-PND
    [31] .'
  Figure 7 Link: articels_figures_by_rev_year\2017\ForceBased_Representation_for_NonRigid_Shape_and_Elastic_Model_Estimation\figure_7.jpg
  Figure 7 caption: 'Back sequence. Top: 2D tracking data and reconstructed 3D shape
    reprojected into several images with green circles and red dots, respectively.
    Bottom: Side view of the reconstructed shape.'
  Figure 8 Link: articels_figures_by_rev_year\2017\ForceBased_Representation_for_NonRigid_Shape_and_Elastic_Model_Estimation\figure_8.jpg
  Figure 8 caption: 'ASL1 and ASL2 sequences. The same information is shown for the
    two experiments. Top: 2D tracking data (green circles) and reconstructed 3D shape
    (red dots) reprojected onto several images. Blue circles correspond to reconstructed
    missing points. Bottom: Camera frame and side-views of the reconstructed 3D shape
    when considering boundary conditions with B=14 rigid points. These points are
    represented by squares. The 3D reconstruction without assuming these priors is
    very similar, but computationally more expensive. Best viewed in color.'
  Figure 9 Link: articels_figures_by_rev_year\2017\ForceBased_Representation_for_NonRigid_Shape_and_Elastic_Model_Estimation\figure_9.jpg
  Figure 9 caption: 'Comparison of low-rank spaces for the Actress sequence. Equivalence
    between the force, shape and trajectory spaces, for rank Q=5 without boundary
    conditions and with B=17 fixed points (represented by squares). First and second
    row: Modes in the force space withoutwith boundary conditions, respectively. Third
    row: Modes in the shape space. Fourth row: Modes in the trajectory space.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Antonio Agudo
  Name of the last author: Francesc Moreno-Noguer
  Number of Figures: 11
  Number of Tables: 3
  Number of authors: 2
  Paper title: Force-Based Representation for Non-Rigid Shape and Elastic Model Estimation
  Publication Date: 2017-09-15 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Total Number of Unknowns That Need to Be Estimated When Considering
      the Full Model, or the Low-Rank Models in Shape, Trajectory, Shape-Trajectory
      or Force Space
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Total Number of Unknowns That Need to Be Estimated When Considering
      the Full and the Low-Rank Models, for the Combination of Parameters N , Q and
      T We Consider in the Experimental Section
  Table 3 caption:
    table_text: TABLE 3 Reconstruction Error of All Methods for Noise-Free, Noisy
      and Missing Observations
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2752710
- Affiliation of the first author: "centre de visi\xF3 per computador (cvc), universitat\
    \ aut\xF2noma de barcelona, barcelona, spain"
  Affiliation of the last author: "centre de visi\xF3 per computador (cvc), universitat\
    \ aut\xF2noma de barcelona, barcelona, spain"
  Figure 1 Link: articels_figures_by_rev_year\2017\Colour_Constancy_Beyond_the_Classical_Receptive_Field\figure_1.jpg
  Figure 1 caption: The flowchart of our model. The input image is convolved by a
    centre-surround contrast-dependent asymmetric difference-of-Gaussian envelope
    (inspired by V1 neurons whose receptive fields are large at low contrast and suppressed
    by high contrast surround). The output of V1 is pooled by V4 neurons according
    to the sparse-coding principle considering global contrast of image.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Colour_Constancy_Beyond_the_Classical_Receptive_Field\figure_2.jpg
  Figure 2 caption: Size tuning curve of a cell in macaque V1, adapted from [48].
    Black and grey curves show responses to a grating of high and low contrast, respectively.
    The dual-role area is suppressive for high contrast stimuli, whereas it acts as
    a facilitator in the case of low contrast. The schematic on the right represents
    the RFs of a V1 neuron. Arrow heads point to radii that determine sR F high (0.26)
    degree and sR F low (0.54 degree).
  Figure 3 Link: articels_figures_by_rev_year\2017\Colour_Constancy_Beyond_the_Classical_Receptive_Field\figure_3.jpg
  Figure 3 caption: The influence of surround on the centre, adapted from [48]. Response
    of a V1 cell in an anaesthetised macaque as a function of the inner radius of
    the surround annular grating. The triangles are responses to centre-only stimulation.
    The square indicates response to a surround stimulus of the smallest inner radius
    presented alone.
  Figure 4 Link: articels_figures_by_rev_year\2017\Colour_Constancy_Beyond_the_Classical_Receptive_Field\figure_4.jpg
  Figure 4 caption: "V4 \u201Cwinner-takes-all\u201D mechanism. Each colour depicts\
    \ its chromatic channel. Straight lines show which portion of V1 signals is pooled\
    \ into V4. The ordinates are shown as log-axes due to the large variations in\
    \ the counts of the different bins."
  Figure 5 Link: articels_figures_by_rev_year\2017\Colour_Constancy_Beyond_the_Classical_Receptive_Field\figure_5.jpg
  Figure 5 caption: Influence of contrast-dependent RF size on illuminant estimation.
  Figure 6 Link: articels_figures_by_rev_year\2017\Colour_Constancy_Beyond_the_Classical_Receptive_Field\figure_6.jpg
  Figure 6 caption: Influence of contrast-dependent surround suppression on illuminant
    estimation.
  Figure 7 Link: articels_figures_by_rev_year\2017\Colour_Constancy_Beyond_the_Classical_Receptive_Field\figure_7.jpg
  Figure 7 caption: "Influence of \u201Cwinners\u201D percentage p on illuminant estimation."
  Figure 8 Link: articels_figures_by_rev_year\2017\Colour_Constancy_Beyond_the_Classical_Receptive_Field\figure_8.jpg
  Figure 8 caption: Colour constancy results of several methods. The recovery angular
    error is indicated on the right bottom corner. The first row shows results for
    a picture from the SFU Lab dataset, the second row from the Grey Ball dataset,
    the third row from the Colour Checker dataset, and the last row from the Multi-illuminant
    dataset.
  Figure 9 Link: articels_figures_by_rev_year\2017\Colour_Constancy_Beyond_the_Classical_Receptive_Field\figure_9.jpg
  Figure 9 caption: Constant versus adaptive V1 and V4 modules. Colour patches on
    the right bottom corner of images in the first row depict the ground truth illuminant
    (in the case of the biased image) and estimated illuminants (for the constant
    and adaptive results).
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Arash Akbarinia
  Name of the last author: C. Alejandro Parraga
  Number of Figures: 9
  Number of Tables: 4
  Number of authors: 2
  Paper title: Colour Constancy Beyond the Classical Receptive Field
  Publication Date: 2017-09-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Angular Error of Several Methods on SFU Lab [63] Benchmark
      Dataset. Lower Figures Indicate Better Performance
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Angular Error of Several Methods on Colour Checker [78] Benchmark
      Dataset. Lower Figures Indicate Better Performance
  Table 3 caption:
    table_text: TABLE 3 Angular Error of Several Methods on Grey Ball [79] Benchmark
      Dataset. Lower Figures Indicate Better Performance
  Table 4 caption:
    table_text: TABLE 4 Recovery Angular Error of Several Methods on Multi-Illuminant
      [80] Benchmark Dataset. Lower Figures Indicate Better Performance
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2753239
- Affiliation of the first author: massachusetts institute of technology, cambridge,
    ma
  Affiliation of the last author: massachusetts institute of technology, cambridge,
    ma
  Figure 1 Link: articels_figures_by_rev_year\2017\CrossModal_Scene_Networks\figure_1.jpg
  Figure 1 caption: Can you recognize scenes across different modalities? Above, we
    show a few examples of our new cross-modal scene dataset. In this paper, we investigate
    how to learn cross-modal scene representations.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\CrossModal_Scene_Networks\figure_2.jpg
  Figure 2 caption: We learn low-level representations specific for each modality
    (white and grays) and a high-level representation that is shared across all modalities
    (red). Above, we also show masks of inputs that activate specific units the most
    [47]. Interestingly, although the network is trained without aligned data, units
    emerge in the shared representation that tend to fire on the same objects independently
    of the modality.
  Figure 3 Link: articels_figures_by_rev_year\2017\CrossModal_Scene_Networks\figure_3.jpg
  Figure 3 caption: 'Scene networks: We use two types of networks. a) For pixel based
    modalities, we use a CNN based off [48] to produce pool5. b) When the input is
    a description, we use an MLP on skip-thought vectors [25] to produce pool5 (as
    text cannot be easily fed into the same CNN).'
  Figure 4 Link: articels_figures_by_rev_year\2017\CrossModal_Scene_Networks\figure_4.jpg
  Figure 4 caption: Statistical regularization. We illustrate this regularization
    with an example. Above, the feature distribution p( x i ) learned from Places
    network is modeled with a GMM, and on incorporated as a prior on x i while optimizing
    the deep model in line drawings modality.
  Figure 5 Link: articels_figures_by_rev_year\2017\CrossModal_Scene_Networks\figure_5.jpg
  Figure 5 caption: 'Cross-modality retrieval: An example of cross-modal retrieval
    given a query from each of the modalities. For each row, the leftmost column depicts
    the query example, while the rest of the columns show the top 2 ranked results
    in each modalitiy.'
  Figure 6 Link: articels_figures_by_rev_year\2017\CrossModal_Scene_Networks\figure_6.jpg
  Figure 6 caption: 'Visualizing unit activations: We visualize pool5 in our cross-modal
    representation above by finding masks of imagesdescriptions that activate a specific
    unit the most [47]. Interestingly, the same unit learns to detect the same concept
    across modalities, suggesting that it may has learned to generalize across these
    modalities.'
  Figure 7 Link: articels_figures_by_rev_year\2017\CrossModal_Scene_Networks\figure_7.jpg
  Figure 7 caption: 't-SNE embedding of cross-modal representation: We visualize the
    embedding for fc7 of representations from different networks using t-SNE [29].
    Colors correspond to the modality. If the representation is agnostic to the modality,
    then the features should not cluster by modality. These visualizations suggest
    that our full method does a better job at discarding modality information than
    baselines.'
  Figure 8 Link: articels_figures_by_rev_year\2017\CrossModal_Scene_Networks\figure_8.jpg
  Figure 8 caption: 'Inverting features across modalities: We visualize some of the
    generated images by our inverting network trained on real images. Top row: Reconstructions
    from real images. These preserve most of the details of the original image but
    are blurry because of the low dimensionality of the pool5 representation. Second
    row: Reconstructions from line drawings, where the network adds colors to the
    reconstructions while preserving the original scene composition. Third row: Inversions
    from the spatial text modality. Reconstructions are less detailed but roughly
    preserve the location, shape and colors of the different parts of the input scene.
    Fourth row: Inversions from the clip-art modality; and inversions from natural
    image to line drawing modality.'
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yusuf Aytar
  Name of the last author: Antonio Torralba
  Number of Figures: 8
  Number of Tables: 5
  Number of authors: 5
  Paper title: Cross-Modal Scene Networks
  Publication Date: 2017-09-18 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Cross-Modal Retrieval mAP: We Report the Mean Average Precision
      (mAP) on Retrieving Images Across Modalities Using fc7 Features'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Cross-Modal Retrieval PR10: We Report the Precision at Top
      10 Retrieval of Images Across Modalities Using fc7 Features'
  Table 3 caption:
    table_text: 'TABLE 3 Mean Cross-Modal Retrieval mAPs Across Layers: Note That
      the Baseline Results Decrease Drastically As We Go Lower Levels (e.g. fc7 to
      fc6) in the Deep Network'
  Table 4 caption:
    table_text: 'TABLE 4 Zero-Shot Scene Classification: We Hold Out 55 Scene Categories
      During Training for the Clip Art, Spatial Text, Line Drawings, and Text Descriptions
      Modalities, and Evaluate the Network''s Ability to Still Classify Them on the
      Validation Set'
  Table 5 caption:
    table_text: 'TABLE 5 Zero-Shot Scene Retrieval: We Hold Out 55 Scene Categories
      During Training for the Clip Art, Spatial Text, Line Drawings, and Text Descriptions
      Modalities, and Evaluate the Network''s Ability to Still Retrieve These Categories'
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2753232
- Affiliation of the first author: "institut f\xFCr informatik, university of bern,\
    \ neubr\xFCckstrasse 10, bern, switzerland"
  Affiliation of the last author: computer science department, university of maryland,
    college park, md
  Figure 1 Link: articels_figures_by_rev_year\2017\StructureAware_Data_Consolidation\figure_1.jpg
  Figure 1 caption: A challenging example with two intertwined clusters, corrupted
    with noise. After projecting the input data (left) onto the underlying structure
    using our structure-aware filtering (SAF) approach (red points in the middle),
    spectral clustering or DBSCAN (right) detect the proper clusters.
  Figure 10 Link: articels_figures_by_rev_year\2017\StructureAware_Data_Consolidation\figure_10.jpg
  Figure 10 caption: Performance under different noise levels and for different datasets.
    We compare the clustering scores of our SAF consolidation to those of direct clustering,
    MD [21] and MFD [26]. SAF performs the best, especially when the noise level is
    high where the structures are better preserved by SAF. See also the supplemental
    material for visual comparisons, available online.
  Figure 2 Link: articels_figures_by_rev_year\2017\StructureAware_Data_Consolidation\figure_2.jpg
  Figure 2 caption: Consolidation process using anisotropic SAF.
  Figure 3 Link: articels_figures_by_rev_year\2017\StructureAware_Data_Consolidation\figure_3.jpg
  Figure 3 caption: Clustering without (SAF) and with anisotropic repulsion (A-SAF).
  Figure 4 Link: articels_figures_by_rev_year\2017\StructureAware_Data_Consolidation\figure_4.jpg
  Figure 4 caption: "We illustrate the iterative consolidation process in a 2D example\
    \ with a data distribution f p = G 0,(\u221E,4) . The user parameters are h=4\
    \ and \u03BC=0.5 , which leads to convergence according to Equation (3). The first\
    \ row shows the actual point movements where the dots are current positions and\
    \ the vectors are pointing to the next locations. The second row shows both empirical\
    \ and theoretical (Equation (6)) update ratios of z \u2032 1 z 1 ."
  Figure 5 Link: articels_figures_by_rev_year\2017\StructureAware_Data_Consolidation\figure_5.jpg
  Figure 5 caption: "Top row: we illustrate changes of point densities during the\
    \ iterative consolidation process with different h values. Bottom row: empirical\
    \ estimate and theoretical prediction of the variances \u03C9 of the intermediate\
    \ point distributions. We show results using mean and median repulsion with blue\
    \ and red crosses, respectively. While theoretical analysis with median repulsion\
    \ is difficult, it empirically follows our prediction."
  Figure 6 Link: articels_figures_by_rev_year\2017\StructureAware_Data_Consolidation\figure_6.jpg
  Figure 6 caption: "Convergence in different dimensions and with median repulsion.\
    \ We add Gaussian noise ( \u03C3=0.15 ) to 2D circles (first row, single circle\
    \ and two concentric ones) and 4D spheres (second row, single sphere and two concentric\
    \ ones). We set the kernel size h=0.1 , and show results with different \u03BC\
    \ values. Equation (3) predicts convergence for \u03BC<0.31 . The third row shows\
    \ histograms of distances to the center of the 4D spheres. The values next to\
    \ the red bell curves are their empirical means and variances, demonstrating convergence\
    \ to thin manifolds as predicted for \u03BC<0.31 ."
  Figure 7 Link: articels_figures_by_rev_year\2017\StructureAware_Data_Consolidation\figure_7.jpg
  Figure 7 caption: Performance of dimensionality reduction without (top) and with
    MD [21] (second row), MFD [26] (third row), and SAF (bottom) consolidation.
  Figure 8 Link: articels_figures_by_rev_year\2017\StructureAware_Data_Consolidation\figure_8.jpg
  Figure 8 caption: "Dimensionality reduction and clustering: the input consists of\
    \ two concentric 2D circles corrupted with 5D noise (the black dots). We elevate\
    \ 2D rings to 5D by appending zeros, and then add standard Gaussian noise. The\
    \ leftmost column shows ground truth labeling (green and blue, outliers black),\
    \ the consolidated points (red) and the input points (gray). We use PCA to project\
    \ the 5D data to 2D. We compare four strategies (from the top row to the bottom),\
    \ \u201Cdirect clustering\u201D, \u201Cconsolidation + clustering\u201D, \u201C\
    projection + clustering\u201D and \u201Cconsolidation + projection + clustering\u201D\
    , with different clustering algorithms (from left to right). The second row reveals\
    \ the sensitivity of most clustering techniques to higher dimensional data, which\
    \ is not clustered well even though the consolidation exposes the structure of\
    \ the data. The third row shows that reducing the dimensionality of the data does\
    \ not solve the problem due to the noisy data. In general, the \u201Cconsolidation\
    \ + projection + clustering\u201D strategy in the bottom row gives the best performance\
    \ (AMI scores in bottom right of subfigures). Some techniques ( k -means, affinity\
    \ propagation, mean shift) are not suitable to cluster this type of data, and\
    \ they do not benefit from consolidation."
  Figure 9 Link: articels_figures_by_rev_year\2017\StructureAware_Data_Consolidation\figure_9.jpg
  Figure 9 caption: "Performance of SAF with increasing dimensionality, compared with\
    \ MD [21] and MFD [26]. The data consists of two concentric hyperspheres with\
    \ different radii, corrupted with Gaussian noise. The rightmost bars \u201CAVG\u201D\
    \ show the average over all clustering methods."
  First author gender probability: 0.75
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Shihao Wu
  Name of the last author: Matthias Zwicker
  Number of Figures: 12
  Number of Tables: 0
  Number of authors: 6
  Paper title: Structure-Aware Data Consolidation
  Publication Date: 2017-09-19 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2754254
- Affiliation of the first author: northwestern polytechnical university, xi'an, shaanxi,
    cn
  Affiliation of the last author: the university of adelaide, adelaide, sa, au
  Figure 1 Link: articels_figures_by_rev_year\2017\FVQA_FactBased_Visual_Question_Answering\figure_1.jpg
  Figure 1 caption: An example visual-based question from our FVQA dataset that requires
    both visual and common-sense knowledge to answer. The answer and mined knowledge
    are generated by our proposed method.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\FVQA_FactBased_Visual_Question_Answering\figure_2.jpg
  Figure 2 caption: The distributions of the collected 5826 questions and the corresponding
    4216 facts over different relationships. The top five relationships are UsedFor,
    Category, IsA , RelatedTo and CapableOf. There are fewer supporting-facts than
    questions because one 'fact' can correspond to multiple 'questions'.
  Figure 3 Link: articels_figures_by_rev_year\2017\FVQA_FactBased_Visual_Question_Answering\figure_3.jpg
  Figure 3 caption: An example of the reasoning process of the proposed VQA approach.
    The visual concepts (objects, scene, attributes) of the input image are extracted
    using trained models, which are further linked to the corresponding semantic entities
    in the knowledge base. The input question is first mapped to one of the query
    types using the LSTM model shown in Section 4.2. The types of key relationships,
    key visual concept and answer source can be determined accordingly. A specific
    query (see Section 4.3) is then performed to find all facts meeting the search
    conditions in KB. These facts are further matched to the keywords extracted from
    the question sentence. The fact with the highest matching score is selected and
    the answer is also obtained accordingly.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Peng Wang
  Name of the last author: Anton van den Hengel
  Number of Figures: 3
  Number of Tables: 13
  Number of authors: 5
  Paper title: 'FVQA: Fact-Based Visual Question Answering'
  Publication Date: 2017-09-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Relationships in Different Knowledge Bases Used for Generating
      Questions
  Table 10 caption:
    table_text: TABLE 10 Accuracies for Different Methods According to Different Answer
      Sources
  Table 2 caption:
    table_text: TABLE 2 Major Datasets for VQA and Their Main Characteristics
  Table 3 caption:
    table_text: TABLE 3 The classification of Questions According to Key Visual Concept,
      KB Source and Answer Source
  Table 4 caption:
    table_text: TABLE 4 Question-Query Mapping Accuracy for Different KB Sources on
      the FVQA Testing Splits
  Table 5 caption:
    table_text: TABLE 5 Overall Accuracy on Our FVQA Testing Splits for Different
      Methods Based on String Matching
  Table 6 caption:
    table_text: TABLE 6 WUPS0.9 on our FVQA Testing Splits for Different Methods
  Table 7 caption:
    table_text: TABLE 7 WUPS0.0 on our FVQA Testing Splits for Different Methods
  Table 8 caption:
    table_text: TABLE 8 Accuracies on the Questions That Asked Based on Different
      Knowledge Base Sources
  Table 9 caption:
    table_text: TABLE 9 Accuracies on Questions That Focus on Three Different Visual
      Concepts
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2754246
- Affiliation of the first author: school of computer science and engineering, nanjing
    university of science and technology, nanjing, china
  Affiliation of the last author: school of engineering, university of california,
    merced, ca
  Figure 1 Link: articels_figures_by_rev_year\2017\Deblurring_Images_via_Dark_Channel_Prior\figure_1.jpg
  Figure 1 caption: Deblurred result on a challenging low-light image. The motion
    blur process makes the dark channel pixels of the blurred image less sparse (c).
    Enforcing sparsity on the dark channel pixels of the recovered image facilitates
    restoring a clean image rather than a blurred one.
  Figure 10 Link: articels_figures_by_rev_year\2017\Deblurring_Images_via_Dark_Channel_Prior\figure_10.jpg
  Figure 10 caption: Deblurring real face images. The proposed algorithm performs
    favorably against a method that uses a face dataset to explore face structures
    for deblurring [13].
  Figure 2 Link: articels_figures_by_rev_year\2017\Deblurring_Images_via_Dark_Channel_Prior\figure_2.jpg
  Figure 2 caption: "Blurred images have less sparse dark channel pixels than clear\
    \ ones. Top: Images. Bottom: Corresponding dark channels computed with an image\
    \ patch of 35\xD735 pixels. The blur process (convolution) results in a weighted\
    \ average of pixels in a neighborhood and tends to increase the minimal pixel\
    \ values."
  Figure 3 Link: articels_figures_by_rev_year\2017\Deblurring_Images_via_Dark_Channel_Prior\figure_3.jpg
  Figure 3 caption: "Intensity histograms for the dark channels of both clear and\
    \ blurred images in a dataset of 3,200 natural images. Blurred images have significantly\
    \ fewer zero dark channel pixels than clear ones, which confirms our analysis.\
    \ The dark channel of each image is computed with an image patch size of 35\xD7\
    35 pixels."
  Figure 4 Link: articels_figures_by_rev_year\2017\Deblurring_Images_via_Dark_Channel_Prior\figure_4.jpg
  Figure 4 caption: "Top: Computing the dark channel D(I) of an image I by the non-linear\
    \ min operator is equivalent to multiplying a binary selection matrix M with the\
    \ vectorized image I . The three squares in the intermediate image denote adjacent\
    \ image patches for computing the dark channel, where the minimum intensity value\
    \ in each patch is marked with different colors. Bottom: The transpose M \u22A4\
    \ enforces identified dark pixels to be consistent with u ."
  Figure 5 Link: articels_figures_by_rev_year\2017\Deblurring_Images_via_Dark_Channel_Prior\figure_5.jpg
  Figure 5 caption: Quantitative evaluations on two benchmark datasets. Our method
    performs competitively against the state-of-the-art methods.
  Figure 6 Link: articels_figures_by_rev_year\2017\Deblurring_Images_via_Dark_Channel_Prior\figure_6.jpg
  Figure 6 caption: Deblurred results using one challenging image from the dataset
    [17]. The deblurred images by other methods are obtained from the reported results
    in [17]. The recovered image by the proposed algorithm with the dark channel prior
    is clearer.
  Figure 7 Link: articels_figures_by_rev_year\2017\Deblurring_Images_via_Dark_Channel_Prior\figure_7.jpg
  Figure 7 caption: Deblurred results on a real natural image. The parts in red boxes
    (b)-(e) contain significant residual blur.
  Figure 8 Link: articels_figures_by_rev_year\2017\Deblurring_Images_via_Dark_Channel_Prior\figure_8.jpg
  Figure 8 caption: On real text images, the proposed generic algorithm generates
    results comparable to methods designed specifically for this scenario.
  Figure 9 Link: articels_figures_by_rev_year\2017\Deblurring_Images_via_Dark_Channel_Prior\figure_9.jpg
  Figure 9 caption: Results on a real saturated image. The deblurring results are
    all generated by the non-blind deconvolution method [16]. Residual blur and ringing
    artifacts exist in the red boxes in (b)-(c).
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jinshan Pan
  Name of the last author: Ming-Hsuan Yang
  Number of Figures: 21
  Number of Tables: 4
  Number of authors: 4
  Paper title: Deblurring Images via Dark Channel Prior
  Publication Date: 2017-09-22 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Success Rates of the Best Three Algorithms on the Dataset
      [18]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Evaluations on the Text Image Dataset [6]
  Table 3 caption:
    table_text: TABLE 3 Run-Time (in second) Performance
  Table 4 caption:
    table_text: TABLE 4 Evaluation of Patch Size on the Dataset [9]
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2753804
