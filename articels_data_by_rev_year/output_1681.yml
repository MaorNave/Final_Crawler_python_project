- Affiliation of the first author: school of electronic information and electrical
    engineering, shanghai jiao tong university, shanghai, china
  Affiliation of the last author: school of electrical and information engineering,
    the university of sydney, nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2020\An_EndtoEnd_Learning_Framework_for_Video_Compression\figure_1.jpg
  Figure 1 caption: "(a): The predictive coding architecture used by the traditional\
    \ video codec H.264 [2] or H.265 [3]. (b): The proposed end-to-end video compression\
    \ network. The modules with green color are not included in the decoder. \u201C\
    MV Encoder Net\u201D and \u201CMV Decoder Net\u201D represent the \u201CMotion\
    \ Vector Encoder Net\u201D and \u201CMotion Vector Decoder Net\u201D."
  Figure 10 Link: articels_figures_by_rev_year\2020\An_EndtoEnd_Learning_Framework_for_Video_Compression\figure_10.jpg
  Figure 10 caption: Performance comparison between the separately trained DVCPro
    models and our newly proposed method DVCProAQ.
  Figure 2 Link: articels_figures_by_rev_year\2020\An_EndtoEnd_Learning_Framework_for_Video_Compression\figure_2.jpg
  Figure 2 caption: Optical flow visualization and statistic analysis.
  Figure 3 Link: articels_figures_by_rev_year\2020\An_EndtoEnd_Learning_Framework_for_Video_Compression\figure_3.jpg
  Figure 3 caption: "Our MV Encoder-decoder network. Conv(3,128,2) represents the\
    \ convolution operation with the kernel size of 3\xD73 , the output channel of\
    \ 128 and the stride of 2. GDNIGDN [5] is the nonlinear transform function. The\
    \ binary feature map is only used for illustration."
  Figure 4 Link: articels_figures_by_rev_year\2020\An_EndtoEnd_Learning_Framework_for_Video_Compression\figure_4.jpg
  Figure 4 caption: Our motion compensation network.
  Figure 5 Link: articels_figures_by_rev_year\2020\An_EndtoEnd_Learning_Framework_for_Video_Compression\figure_5.jpg
  Figure 5 caption: Visual comparison of the predicted frames between our model and
    H.265.
  Figure 6 Link: articels_figures_by_rev_year\2020\An_EndtoEnd_Learning_Framework_for_Video_Compression\figure_6.jpg
  Figure 6 caption: "The proposed motion estimation scheme. \u201CD\u201D and \u201C\
    U\u201D represent the downsampling and upsampling operations, respectively. \u201C\
    C\u201D represents the concatenation operation. \u201CConv\u201D is the convolution\
    \ operation."
  Figure 7 Link: articels_figures_by_rev_year\2020\An_EndtoEnd_Learning_Framework_for_Video_Compression\figure_7.jpg
  Figure 7 caption: Our MV Encoder-decoder network. Conv(3,16,2) represents the convolution
    operation with the kernel size of 3 times 3 , the output channel of 16 and the
    stride of 2.
  Figure 8 Link: articels_figures_by_rev_year\2020\An_EndtoEnd_Learning_Framework_for_Video_Compression\figure_8.jpg
  Figure 8 caption: The network architecture of our motion refinement network.
  Figure 9 Link: articels_figures_by_rev_year\2020\An_EndtoEnd_Learning_Framework_for_Video_Compression\figure_9.jpg
  Figure 9 caption: "An example of the basic compression network after using the adaptive\
    \ quantization layer. \u201CAQL\u201D and \u201CIAQL\u201D represent the adaptive\
    \ quantization layer and inverse adaptive quantization layer. \u201CAE\u201D and\
    \ \u201CAD\u201D represent the arithmetic encoder and arithmetic decoder."
  First author gender probability: 0.95
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Guo Lu
  Name of the last author: Dong Xu
  Number of Figures: 22
  Number of Tables: 5
  Number of authors: 6
  Paper title: An End-to-End Learning Framework for Video Compression
  Publication Date: 2020-04-20 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Network Architecture of the Proposed Adaptive Quantization
      Layer
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 BDBR(%) and BD-PSNR(dB) Performances of H.265 and Our DVCDVCLiteDVCPro
      Methods When Compared With H.264 on the HEVC Standard Test Sequences in Terms
      of PSNR
  Table 3 caption:
    table_text: TABLE 3 BDBR(%) and BD-MSSSIM(dB) Performances of H.265 and our DVCDVCLiteDVCPro
      Methods When Compared With H.264 on the HEVC Standard Test Sequences in Terms
      of MS-SSIM
  Table 4 caption:
    table_text: TABLE 4 The Bit Cost for Encoding Optical Flow Maps and the Corresponding
      PSNR of the Warped Frame
  Table 5 caption:
    table_text: TABLE 5 The Trainable Parameters and FLOPs for Different Models
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2988453
- Affiliation of the first author: department of mechanical engineering, state key
    laboratory of tribology, the beijing key laboratory of precisionultra-precision
    manufacturing equipment control, tsinghua university, beijing, china
  Affiliation of the last author: department of computer science and engineering,
    university of california, san diego, san diego, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\VisibilityAware_PointBased_MultiView_Stereo_Network\figure_1.jpg
  Figure 1 caption: VA-Point-MVSNet performs multi-view stereo reconstruction in a
    coarse-to-fine fashion, learning to predict the 3D flow of each point to the ground
    truth surface based on geometry priors and 2D image appearance cues dynamically
    fetched from multi-view images and regress accurate and dense point clouds iteratively.
  Figure 10 Link: articels_figures_by_rev_year\2020\VisibilityAware_PointBased_MultiView_Stereo_Network\figure_10.jpg
  Figure 10 caption: 'Qualitative results of Scan 9 of DTU dataset. Top: Whole point
    cloud. Bottom: Visualization of normals in zoomed local area. Our VA-Point-MVSNet
    generates detailed point clouds with more high-frequency components than MVSNet.
    For a fair comparison, the depth maps predicted by MVSNet are interpolated to
    the same resolution as our method.'
  Figure 2 Link: articels_figures_by_rev_year\2020\VisibilityAware_PointBased_MultiView_Stereo_Network\figure_2.jpg
  Figure 2 caption: Overview of VA-Point-MVSNet architecture. The visibility-aware
    feature aggregation module aggregates the multi-view image appearance cues to
    generate visibility-robust features for coarse depth prediction and depth refinement
    separately. A coarse depth map is first predicted with low GPU memory and computation
    cost and then unprojected to a point cloud along with hypothesized points. For
    each point, the feature is fetched from the multi-view image feature pyramid dynamically.
    The PointFlow module uses the feature-augmented point cloud for depth residual
    prediction, and the depth map is refined iteratively along with up-sampling.
  Figure 3 Link: articels_figures_by_rev_year\2020\VisibilityAware_PointBased_MultiView_Stereo_Network\figure_3.jpg
  Figure 3 caption: "Different structures of multi-view feature aggregation. (a) avg:\
    \ average operation applied in [43]; (b) max: element-wise max-pooling operation;\
    \ (c) var: variance operation applied in [4]; (d)vis-avg: our visibility-aware\
    \ average operation; (e) vis-max: our visibility-aware max-pooling operation;\
    \ (f)vis-var: our visibility-aware variance operation. \u201CShared MLP\u201D\
    \ stands for multi-layer perceptron with shared weights for all input features.\
    \ Batchnorm and ReLU are used for all layers except the last layer."
  Figure 4 Link: articels_figures_by_rev_year\2020\VisibilityAware_PointBased_MultiView_Stereo_Network\figure_4.jpg
  Figure 4 caption: "Network architecture of the visibility prediction module. The\
    \ module takes as input the feature map of reference view F 0 and that of source\
    \ view F i , and outputs the visibility mask for view i . \u201CShared MLP\u201D\
    \ stands for multi-layer perceptron with shared weights across all pixels, such\
    \ that the visibility of each pixel is predicted independently. The element-wise\
    \ sigmoid function is applied on the last layer to constrain the output in the\
    \ range of [0,1] . Normalized depth is computed by d k \u02DC =( d k \u2212 d\
    \ min )( d max \u2212 d min ) ."
  Figure 5 Link: articels_figures_by_rev_year\2020\VisibilityAware_PointBased_MultiView_Stereo_Network\figure_5.jpg
  Figure 5 caption: 'Illustraion of point hypotheses generation and edge construction:
    For each unprojected point mathbf p , the 2m point hypotheses lbrace mathbf tildepkrbrace
    are generated along the reference camera direction. Directed edges are constructed
    between each hypothesized point and its kNN points for edge convolution.'
  Figure 6 Link: articels_figures_by_rev_year\2020\VisibilityAware_PointBased_MultiView_Stereo_Network\figure_6.jpg
  Figure 6 caption: Network architecture of the proposed PointFlow module.
  Figure 7 Link: articels_figures_by_rev_year\2020\VisibilityAware_PointBased_MultiView_Stereo_Network\figure_7.jpg
  Figure 7 caption: 'Illustration of our synthetic dataset: rendered images for 9
    known camera configurations, image with red border is the reference view, images
    with blue border are the source views used for training.'
  Figure 8 Link: articels_figures_by_rev_year\2020\VisibilityAware_PointBased_MultiView_Stereo_Network\figure_8.jpg
  Figure 8 caption: 'Ground truth visibility mask: (a) reference view; (b) source
    view; (c) ground truth visibility mask computed by checking depth consistency
    between (a) and (b), green represents visible and red represents occluded.'
  Figure 9 Link: articels_figures_by_rev_year\2020\VisibilityAware_PointBased_MultiView_Stereo_Network\figure_9.jpg
  Figure 9 caption: F-score, accuracy, and completeness of different distance thresholds
    on the DTU evaluation dataset (higher is better). For a fair comparison, we upsample
    the depth map predicted by MVSNet to the same resolution as our method before
    depth fusion ( 288times 216 to 640times 480 ). The reconstruction results of Gipuma
    [49] and SurfaceNet [3] are not publicly available.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Rui Chen
  Name of the last author: Hao Su
  Number of Figures: 15
  Number of Tables: 10
  Number of authors: 4
  Paper title: Visibility-Aware Point-Based Multi-View Stereo Network
  Publication Date: 2020-04-22 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison Results of Predicted Depth Accuracy for Different
      Multi-View Feature Aggregation Structures
  Table 10 caption:
    table_text: TABLE 10 Quantitative Results of Published Learning-Based Methods
      on Tanks and Temples Benchmark [9]
  Table 2 caption:
    table_text: TABLE 2 Comparison Results of Supervised Learning and Unsupervised
      Learning of Our vis-var Structure
  Table 3 caption:
    table_text: TABLE 3 Ablation Study on Introduction of Normalized Depth in Our
      Visibility Prediction Module
  Table 4 caption:
    table_text: TABLE 4 Quantitative Results of Reconstruction Quality on the DTU
      Evaluation Dataset (Lower is Better)
  Table 5 caption:
    table_text: TABLE 5 Comparison Result at Different Flow Iterations Measured by
      Reconstruction Quality and Depth Map Resolution on the DTU Evaluation Set
  Table 6 caption:
    table_text: TABLE 6 Ablation Study on Network Architectures on the DTU Evaluation
      Dataset, Which Demonstrates the Effectiveness of Different Components
  Table 7 caption:
    table_text: TABLE 7 Ablation Study of Different Number of Point Hypotheses m m
      on the DTU Evaluation Set [8]
  Table 8 caption:
    table_text: TABLE 8 Comparison Result of Direct Regression and Point Hypotheses
      on the DTU Evaluation Set [8]
  Table 9 caption:
    table_text: TABLE 9 Comparison of Reconstruction Quality on the DTU Evaluation
      Dataset With PU-Net [52]
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2988729
- Affiliation of the first author: department of electrical and computer engineering,
    rice university, houston, usa
  Affiliation of the last author: department of electrical and computer engineering,
    rice university, houston, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\PhlatCam_Designed_PhaseMask_Based_Thin_Lensless_Camera\figure_1.jpg
  Figure 1 caption: '[Top] Non-lensing optics provides a way to achieve thin devices
    at low-cost. Among the various non-lensing optics, phase-masks are versatile in
    their designs and can produce a larger space of Point-spread-functions (PSF).
    [Bottom] PSFs from various optics are shown. Lensing optics have a small PSF support
    while non-lensing optics display large PSFs. The non-lensing optics PSFs were
    experimentally captured.'
  Figure 10 Link: articels_figures_by_rev_year\2020\PhlatCam_Designed_PhaseMask_Based_Thin_Lensless_Camera\figure_10.jpg
  Figure 10 caption: Simulated reconstruction with previously proposed PSFs, random
    binary PSF and our Contour PSF. Random binary PSF satisfies three of the four
    desired characteristics of PSF. However, random binary PSF doesnt satisfy the
    fourth characteristic, that is large contiguous regions of zero intensity. As
    seen from above, contour PSF consistently produces better results.
  Figure 2 Link: articels_figures_by_rev_year\2020\PhlatCam_Designed_PhaseMask_Based_Thin_Lensless_Camera\figure_2.jpg
  Figure 2 caption: Phase-masks are essentially transparent material with different
    heights at different locations. This causes phase modulation of the incoming wavefront,
    and the resultant wave interference produces the PSF at the sensor plane. The
    above image shows the closeup image of the phase-mask using in PhlatCam. The rightmost
    image was taken using a scanning electron microscope (SEM).
  Figure 3 Link: articels_figures_by_rev_year\2020\PhlatCam_Designed_PhaseMask_Based_Thin_Lensless_Camera\figure_3.jpg
  Figure 3 caption: Our proposed phase-mask framework takes the input of target PSF
    and the desired device geometry and outputs an optimized phase-mask design.
  Figure 4 Link: articels_figures_by_rev_year\2020\PhlatCam_Designed_PhaseMask_Based_Thin_Lensless_Camera\figure_4.jpg
  Figure 4 caption: "Conventional imaging and PhlatCam. PhlatCam is 5\u201310\xD7\
    \ thinner and can reconstruct high-fidelity images from multiplexed measurements.\
    \ Additionally, PhlatCam can function in more ways than a conventional camera.\
    \ Specifically, PhlatCam can produce 2D images for any scene distance, refocused\
    \ images at medium distance, and 3D imaging at close distance."
  Figure 5 Link: articels_figures_by_rev_year\2020\PhlatCam_Designed_PhaseMask_Based_Thin_Lensless_Camera\figure_5.jpg
  Figure 5 caption: Illustration of properties of phase mask in a lensless imaging
    setup.
  Figure 6 Link: articels_figures_by_rev_year\2020\PhlatCam_Designed_PhaseMask_Based_Thin_Lensless_Camera\figure_6.jpg
  Figure 6 caption: Our Contour PSF is generated by applying canny edge detection
    on Perlin noise [42].
  Figure 7 Link: articels_figures_by_rev_year\2020\PhlatCam_Designed_PhaseMask_Based_Thin_Lensless_Camera\figure_7.jpg
  Figure 7 caption: 'Modulation Transfer Function (MTF) of lensless point-spread-functions
    (PSFs). The MTF is computed as the radially averaged magnitude spectrum of the
    PSFs. The PSFs compared are: Separable MSEQ [10], Fresnel zone apertures (FZA)
    [32], Tessellated spiral [13], Diffuser [14], Random binary, and our Contour PSF.
    The PSFs are visualized in Fig. 10. The magnitude spectrum of the proposed Contour
    PSF remains large for the entire frequency range indicating better invertibility
    characteristics.'
  Figure 8 Link: articels_figures_by_rev_year\2020\PhlatCam_Designed_PhaseMask_Based_Thin_Lensless_Camera\figure_8.jpg
  Figure 8 caption: Visual illustration of the phase mask design.
  Figure 9 Link: articels_figures_by_rev_year\2020\PhlatCam_Designed_PhaseMask_Based_Thin_Lensless_Camera\figure_9.jpg
  Figure 9 caption: The proposed PSF, designed phase-mask, and the experimentally
    realized PSF of PhlatCam are shown. The experimental PSF closely resembles the
    proposed PSF design, showing the effectiveness of the phase mask design framework.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Vivek Boominathan
  Name of the last author: Ashok Veeraraghavan
  Number of Figures: 17
  Number of Tables: 0
  Number of authors: 4
  Paper title: 'PhlatCam: Designed Phase-Mask Based Thin Lensless Camera'
  Publication Date: 2020-04-23 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2987489
- Affiliation of the first author: zhiyuan college, shanghai jiao tong university,
    shanghai, china
  Affiliation of the last author: department of computer science and engineering,
    and moe key lab of artificial intelligence, ai institute, shanghai jiao tong university,
    shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Unifying_Offline_and_Online_MultiGraph_Matching_via_Finding_Shortest_Paths_on_Su\figure_1.jpg
  Figure 1 caption: Illustration of MGM-Floyd for offline MGM. The green and yellow
    line denotes the matchings used to update and need to be updated, respectively.
    The number in the brackets on each edge denotes the matching score and the composition
    matching is near the number with X ij being the initial matching. In iteration
    i , use G i as internal vertex to update the matchings.
  Figure 10 Link: articels_figures_by_rev_year\2020\Unifying_Offline_and_Online_MultiGraph_Matching_via_Finding_Shortest_Paths_on_Su\figure_10.jpg
  Figure 10 caption: MGM-Floyd (top two) and MGM-SPFA (bottom two) over iterations
    on synthetic outlier data and category car from Willow-ObjectClass.
  Figure 2 Link: articels_figures_by_rev_year\2020\Unifying_Offline_and_Online_MultiGraph_Matching_via_Finding_Shortest_Paths_on_Su\figure_2.jpg
  Figure 2 caption: "Illustration of MGM-SPFA for online MGM. Here G 5 is the arriving\
    \ graph. The green and yellow line denotes the matchings used to update and need\
    \ to be updated, respectively. The number in the brackets on each edge denotes\
    \ the matching score and the composition matching is near the number with X ij\
    \ being the initial matching. In Phase 1, fix all the matchings in H\u2216 G 5\
    \ and update each pair of X iN using other vertices. In Phase 2, fix all X iN\
    \ and use G N as internal vertex to update all the matchings X ij in H\u2216 G\
    \ 5 ."
  Figure 3 Link: articels_figures_by_rev_year\2020\Unifying_Offline_and_Online_MultiGraph_Matching_via_Finding_Shortest_Paths_on_Su\figure_3.jpg
  Figure 3 caption: Matchings (correct in green and otherwise red) of Car, Duck, Motorbike
    from Willow-ObjectClass. We compare [4], [5], [20], [21], [22].
  Figure 4 Link: articels_figures_by_rev_year\2020\Unifying_Offline_and_Online_MultiGraph_Matching_via_Finding_Shortest_Paths_on_Su\figure_4.jpg
  Figure 4 caption: Accuracy and time (in seconds) comparison of RRWM [8] and RRWHM
    [36] as the two-graph matching solver on Willow object dataset.
  Figure 5 Link: articels_figures_by_rev_year\2020\Unifying_Offline_and_Online_MultiGraph_Matching_via_Finding_Shortest_Paths_on_Su\figure_5.jpg
  Figure 5 caption: Offline matching performance comparison on synthetic data (deform,
    outlier, complete) with different versions of MGM-Floyd and CAO [4].
  Figure 6 Link: articels_figures_by_rev_year\2020\Unifying_Offline_and_Online_MultiGraph_Matching_via_Finding_Shortest_Paths_on_Su\figure_6.jpg
  Figure 6 caption: Offline matching performance on objects from Willow-ObjectClass
    and Sub-Rome16K. Different versions of CAO and MGM-Floyd are tested.
  Figure 7 Link: articels_figures_by_rev_year\2020\Unifying_Offline_and_Online_MultiGraph_Matching_via_Finding_Shortest_Paths_on_Su\figure_7.jpg
  Figure 7 caption: Online incremental matching performance on Willow-ObjectClass
    and Sub-Rome16K. Both online and offline MGM methods are evaluated.
  Figure 8 Link: articels_figures_by_rev_year\2020\Unifying_Offline_and_Online_MultiGraph_Matching_via_Finding_Shortest_Paths_on_Su\figure_8.jpg
  Figure 8 caption: Online matching performance on synthetic data (deform, deform+outlier).
    Offline MGM methods are also tested for efficiency comparison.
  Figure 9 Link: articels_figures_by_rev_year\2020\Unifying_Offline_and_Online_MultiGraph_Matching_via_Finding_Shortest_Paths_on_Su\figure_9.jpg
  Figure 9 caption: Distance from S to S uc and S to S pc . On ground truth X , we
    replace part of the matchings with random permutation matrix by varying ratio
    ( x -axis) to generate different S and the corresponding S uc , S pc . It is shown
    S pc is a better delegator to S .
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.85
  Name of the first author: Zetian Jiang
  Name of the last author: Junchi Yan
  Number of Figures: 13
  Number of Tables: 2
  Number of authors: 3
  Paper title: Unifying Offline and Online Multi-Graph Matching via Finding Shortest
    Paths on Supergraph
  Publication Date: 2020-04-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Setting for Table 2(Top), Figs. 4, 5, 6, 7,9, 10, 12(Middle)
      and 11(Base)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Accuracy and Time (in Seconds) of the State-of-the-Art Offline
      MGM Algorithms on Willow-ObjectClass
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2989928
- Affiliation of the first author: school of electrical engineering and computer science,
    university of central florida, orlando, fl, usa
  Affiliation of the last author: school of electrical engineering and computer science,
    university of central florida, orlando, fl, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\NormPreservation_Why_Residual_Networks_Can_Become_Extremely_Deep\figure_1.jpg
  Figure 1 caption: ResNet architecture and its building blocks. Each conv block represents
    a sequence of batch normalization, ReLU, and convolution layers. conv block represents
    the regularized convolution layer.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\NormPreservation_Why_Residual_Networks_Can_Become_Extremely_Deep\figure_2.jpg
  Figure 2 caption: "The ratio of gradient norm at output to gradient norm at input,\
    \ i.e., \u2225 \u2202E \u2202 x l+1 \u2225 2 to \u2225 \u2202E \u2202 x l \u2225\
    \ 2 , of a convolution layer for different number of input and output channels\
    \ at 10th training epoch (a) with, and (b) without the proposed regularization\
    \ on the singular values of the convolution."
  Figure 3 Link: articels_figures_by_rev_year\2020\NormPreservation_Why_Residual_Networks_Can_Become_Extremely_Deep\figure_3.jpg
  Figure 3 caption: Training on CIFAR10. Gradient norm ratio over the first 100 epochs
    for transition blocks (blocks that change the dimension) and non-transition blocks
    (blocks that do not change the dimension). The darker color lines represent the
    transition blocks and the lighter color lines represent the non-transition blocks.
    The proposed regularization enhances the norm-preservation of the transition blocks
    effectively.
  Figure 4 Link: articels_figures_by_rev_year\2020\NormPreservation_Why_Residual_Networks_Can_Become_Extremely_Deep\figure_4.jpg
  Figure 4 caption: Loss (black lines) and error (blue lines) during training procedure
    on CIFAR10. Solid lines represent the test values and dotted lines represent the
    training values. This experiments shows how the residual connections enhance the
    stability of the optimization and how the proposed regularization enhances the
    stability even further.
  Figure 5 Link: articels_figures_by_rev_year\2020\NormPreservation_Why_Residual_Networks_Can_Become_Extremely_Deep\figure_5.jpg
  Figure 5 caption: Comparison of the parameter efficiency on CIFAR10 between ResNet
    and ProcResNet.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Alireza Zaeemzadeh
  Name of the last author: Mubarak Shah
  Number of Figures: 5
  Number of Tables: 3
  Number of authors: 3
  Paper title: 'Norm-Preservation: Why Residual Networks Can Become Extremely Deep?'
  Publication Date: 2020-04-27 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Mean and Maximum Generalization Gap (%) During the First 100
      Epochs of Trainingon CIFA10 for Different Network Architectures, Averaged over
      10 Runs
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance of Different Methods on CIFAR-10 and CIFAR-100
      Using Moderate Data Augmentation (fliptranslation). The modified transition
      blocks in ProcResNet can improve the accuracy of ResNet significantly.
  Table 3 caption:
    table_text: TABLE 3 Ablation Study on ResNet With 164 Layers on CIFAR100
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2990339
- Affiliation of the first author: department of statistics, university of california,
    los angeles, los angeles, ca, usa
  Affiliation of the last author: department of statistics, university of california,
    los angeles, los angeles, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Optimizing_Regularized_Cholesky_Score_for_OrderBased_Learning_of_Bayesian_Networ\figure_1.jpg
  Figure 1 caption: "An example DAG G , its coefficient matrix B 0 , and a permutation\
    \ \u03C0 . B \u03C0 permutes columns and rows of B 0 and is strictly lower triangular."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Optimizing_Regularized_Cholesky_Score_for_OrderBased_Learning_of_Bayesian_Networ\figure_2.jpg
  Figure 2 caption: "A comparison between the MCP (solid line) and the \u2113 1 penalty\
    \ (dashed line)."
  Figure 3 Link: articels_figures_by_rev_year\2020\Optimizing_Regularized_Cholesky_Score_for_OrderBased_Learning_of_Bayesian_Networ\figure_3.jpg
  Figure 3 caption: Test data log-likelihood comparison among BN learning methods.
    Log-likelihoods are shifted by the median of ARCS (the dashed line).
  Figure 4 Link: articels_figures_by_rev_year\2020\Optimizing_Regularized_Cholesky_Score_for_OrderBased_Learning_of_Bayesian_Networ\figure_4.jpg
  Figure 4 caption: "Performance of the BIC selected parameter among a grid of (\u03B3\
    ,\u03BB) given an initial permutation. Tuning parameters that lead to lower SHDs\
    \ than the BIC selection are shown in gray."
  Figure 5 Link: articels_figures_by_rev_year\2020\Optimizing_Regularized_Cholesky_Score_for_OrderBased_Learning_of_Bayesian_Networ\figure_5.jpg
  Figure 5 caption: Distributions of the relative BIC increase. The numbers following
    each DAG report its (p, s 0 ) .
  Figure 6 Link: articels_figures_by_rev_year\2020\Optimizing_Regularized_Cholesky_Score_for_OrderBased_Learning_of_Bayesian_Networ\figure_6.jpg
  Figure 6 caption: Comparison of SHD and reversed edge proportion between experimental
    and observational data with ARCS(CD).
  Figure 7 Link: articels_figures_by_rev_year\2020\Optimizing_Regularized_Cholesky_Score_for_OrderBased_Learning_of_Bayesian_Networ\figure_7.jpg
  Figure 7 caption: A comparison between ARCS(RND) with a high initial temperature
    and ARCS(CD) on experimental data.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.54
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.96
  Name of the first author: Qiaoling Ye
  Name of the last author: Qing Zhou
  Number of Figures: 7
  Number of Tables: 5
  Number of authors: 3
  Paper title: Optimizing Regularized Cholesky Score for Order-Based Learning of Bayesian
    Networks
  Publication Date: 2020-04-27 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison Between ARCS and Initial Estimates on Observational
      Data
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 ARCS Against Other Methods on Observational Data
  Table 3 caption:
    table_text: TABLE 3 ARCS Against Precision Matrix Estimation Methods
  Table 4 caption:
    table_text: TABLE 4 Comparison on SHDs Before (B) and After (A) the Refinement
      Step
  Table 5 caption:
    table_text: TABLE 5 Performance Comparison on Experimental Data
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2990820
- Affiliation of the first author: school of computer science and engineering, university
    of electronic science and technology of china, chengdu, china
  Affiliation of the last author: school of computer science and engineering, university
    of electronic science and technology of china, chengdu, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Maximum_Density_Divergence_for_Domain_Adaptation\figure_1.jpg
  Figure 1 caption: Illustration of domain adaptation and adversarial domain adaptation
    networks (ADAN). In domain adaptation tasks, the source domain and the target
    domain have different data distributions. The goal is to learn a new feature representation
    where the two domains can be well aligned. ADAN leverages the idea of adversarial
    learning [13] and it assumes that the two domains are aligned as long as the domain
    discriminator is confused. However, recent advances reveal that such an assumption
    may be not solid [14]. In this paper, we propose a new method to challenge this
    issue.
  Figure 10 Link: articels_figures_by_rev_year\2020\Maximum_Density_Divergence_for_Domain_Adaptation\figure_10.jpg
  Figure 10 caption: "Results of pseudo labeling on A \u2192 D."
  Figure 2 Link: articels_figures_by_rev_year\2020\Maximum_Density_Divergence_for_Domain_Adaptation\figure_2.jpg
  Figure 2 caption: 'The illustration of our proposed distance loss MDD (Maximum Density
    Divergence). MDD has two motivations: minimizing the inter-domain divergence and
    maximizing the intra-class density.'
  Figure 3 Link: articels_figures_by_rev_year\2020\Maximum_Density_Divergence_for_Domain_Adaptation\figure_3.jpg
  Figure 3 caption: 'The idea illustration of the proposed adversarial tight match
    (ATM) for domain adaptation. Our method simultaneously optimizes the MDD loss
    and the adversarial loss. As a result, it can not only confuse the domain discriminator
    but also guarantee that the two data distributions are well aligned. The feature
    learner F is a deep neural network, e.g., LeNet for the digits recognition and
    ResNet-50 for the object recognition in this paper. The predictor is a softmax
    classifier which has two purposes: generating the classification condition p and
    predicting the pseudo labels y t for target samples.'
  Figure 4 Link: articels_figures_by_rev_year\2020\Maximum_Density_Divergence_for_Domain_Adaptation\figure_4.jpg
  Figure 4 caption: Idea illustration for Eq. (4). Better viewed in color. We use
    orange for source domain and green for target domain. The number in each box denotes
    the class information. A line means that the distance between the two samples
    will be considered into the loss. For clarity, we use 5 samples per domain in
    a batch for this example. To calculate the first term in Eq. (3), we need to calculate
    it in the way shown in (a). In this paper, we simplify it as shown in (b), i.e,
    the first term in Eq. (4). The second and the third terms are calculated as shown
    in (c) and (d), respectively.
  Figure 5 Link: articels_figures_by_rev_year\2020\Maximum_Density_Divergence_for_Domain_Adaptation\figure_5.jpg
  Figure 5 caption: An illustration of the implementation of the proposed method.
    In this paper, we mainly use ResNet-50 as the feature representation network (exceptions
    are stated in the context, e.g., LeNet is used for digits recognition to avoid
    over-fitting). The domain discriminator is implemented by three FC layers. The
    MDD alignment is implemented according to Eq. (5).
  Figure 6 Link: articels_figures_by_rev_year\2020\Maximum_Density_Divergence_for_Domain_Adaptation\figure_6.jpg
  Figure 6 caption: "Model analysis on evaluation SVHN \u2192 MNIST. (a) The value\
    \ of MDD with different epochs. (b) The test error (%) with different epochs.\
    \ (c) The overall loss with different epochs. (d) Parameter sensitivity of \u03B1\
    \ ."
  Figure 7 Link: articels_figures_by_rev_year\2020\Maximum_Density_Divergence_for_Domain_Adaptation\figure_7.jpg
  Figure 7 caption: "Visualization of the learned representations by t-SNE [44]. The\
    \ evaluation SVHN \u2192 MNIST is used as an example. Specifically, figure (a),\
    \ (b) and (c) visualize the original representations (non-adapted), CDAN and our\
    \ representations, respectively. The number near each cluster is the corresponding\
    \ category label. It can be seen that some classes, e.g., 4 and 9, are still confusing\
    \ in CDAN. Our method has good transferability and discriminability with the power\
    \ of MDD."
  Figure 8 Link: articels_figures_by_rev_year\2020\Maximum_Density_Divergence_for_Domain_Adaptation\figure_8.jpg
  Figure 8 caption: "The classification loss (a) and overall loss (b) comparison between\
    \ CDAN (black line) and our method (orange line). The evaluation A \u2192 D is\
    \ used as an example. Better viewed in color. It can be seen that the loss curve\
    \ of our method is generally smoother than the loss curve of CDAN, which verifies\
    \ that the training of our method is more stable than CDAN by further introducing\
    \ the MDD divergence."
  Figure 9 Link: articels_figures_by_rev_year\2020\Maximum_Density_Divergence_for_Domain_Adaptation\figure_9.jpg
  Figure 9 caption: "The effectiveness of the MDD. The left sub-figure reports the\
    \ A -distance of different methods on A \u2192 D. The right sub-figure shows the\
    \ results of different settings, i.e., CDAN, CDAN+MMD and CDAN+MDD, on SVHN \u2192\
    \ MNIST."
  First author gender probability: 0.92
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Jingjing Li
  Name of the last author: Heng Tao Shen
  Number of Figures: 11
  Number of Tables: 6
  Number of authors: 6
  Paper title: Maximum Density Divergence for Domain Adaptation
  Publication Date: 2020-04-28 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Accuracy (%) of Digits Recognition
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Domain Adaptation Results (Accuracy %) on Office-31
  Table 3 caption:
    table_text: TABLE 3 Domain Adaptation Results (Accuracy %) on Office-31
  Table 4 caption:
    table_text: TABLE 4 Domain Adaptation Results (Accuracy %) on ImageCLEF-DA
  Table 5 caption:
    table_text: TABLE 5 Domain Adaptation Results (Accuracy %) on Office-Home Dataset
  Table 6 caption:
    table_text: "TABLE 6 Intra-MDD Ablation Study on Evaluation S \u2192 \u2192M"
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2991050
- Affiliation of the first author: state key laboratory of information security, institute
    of information engineering, chinese academy of sciences, beijing, china
  Affiliation of the last author: state key laboratory of information security, institute
    of information engineering, chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2020\TaskFeature_Collaborative_Learning_with_Application_to_Personalized_Attribute_Pr\figure_1.jpg
  Figure 1 caption: 'Illustration of the Base Model of the Task-Feature Collaborative
    Learning Framework. Left: We form the task-feature relations as an auxiliary bipartite
    graph G BI with tasks and features being the nodes, and L G BI being the Graph
    Laplacian. To separate all the tasks and features into k groups, we expect to
    cut G BI into k connected components. Middle: If we reconsider this requirement
    from the Graph Laplacian, as is shown in Theorem 1, it is equivalent to force
    the smallest k eigenvalues of L G BI to be zero. Since directly doing this is
    intractable, we turn to minimize their sum as a relaxation, which gives birth
    to a novel regularizer based on Theorem 2. Right: Now we shift our attention to
    the model parameters W . The proposed regularizer facilitates a generalized block-diagonal
    structure (up to permutations) toward W , with each block containing a specific
    group of nodes in G BI . In the next section, based on Proposition 1, Theorems
    3, 4, and 5, we will construct an optimization method for TFCL with global convergence
    guarantee. Moreover, we will also show in Theorem 6 that, negative transfer across
    blocks, marked as crosses here, could be effectively suppressed based on the algorithm.'
  Figure 10 Link: articels_figures_by_rev_year\2020\TaskFeature_Collaborative_Learning_with_Application_to_Personalized_Attribute_Pr\figure_10.jpg
  Figure 10 caption: 'Ablation Results (II) The y -axis represents the average AUC
    score on the test set, and the x -axis represents different algorithms: TFCL(Coc)
    shows the performance of TFCL algorithm with our co-grouping regularizer replaced
    by the corresponding regularizer in CocMTL; TFCL(ours) shows the performance of
    our original algorithm.'
  Figure 2 Link: articels_figures_by_rev_year\2020\TaskFeature_Collaborative_Learning_with_Application_to_Personalized_Attribute_Pr\figure_2.jpg
  Figure 2 caption: "Illustration of the Solution in Theorem 3. In this figure, we\
    \ plot the values of l i with respect to the corresponding eigenvalues. We see\
    \ that Theorem 3 considers the multiplicity of \u03BB k ( L G BI ) . This makes\
    \ our algorithm stable even when the eigengap \u03BB k+1 ( L G BI )\u2212 \u03BB\
    \ k ( L G BI ) is zero."
  Figure 3 Link: articels_figures_by_rev_year\2020\TaskFeature_Collaborative_Learning_with_Application_to_Personalized_Attribute_Pr\figure_3.jpg
  Figure 3 caption: "Visualizations of the eigenvector outer-products, which shows\
    \ that V V \u22A4 is not identifiable when we need to pick 2 out of 3 bases from\
    \ the eigenspace of zero."
  Figure 4 Link: articels_figures_by_rev_year\2020\TaskFeature_Collaborative_Learning_with_Application_to_Personalized_Attribute_Pr\figure_4.jpg
  Figure 4 caption: "AUC ( \u2191 ) comparison on the simulated dataset."
  Figure 5 Link: articels_figures_by_rev_year\2020\TaskFeature_Collaborative_Learning_with_Application_to_Personalized_Attribute_Pr\figure_5.jpg
  Figure 5 caption: "Convergence curves for (a) loss function, (b) parameter W in\
    \ terms of the difference between two successive iterations || W t \u2212 W t\u2212\
    1 || , and (c) || U t \u2212 U t\u22121 || ."
  Figure 6 Link: articels_figures_by_rev_year\2020\TaskFeature_Collaborative_Learning_with_Application_to_Personalized_Attribute_Pr\figure_6.jpg
  Figure 6 caption: "Evolution of Spectral Embeddings. We plot the corresponding embeddings\
    \ f 1 \u2026, f d+T in the first five iterations in this group of figures. The\
    \ results suggest that spectral embeddings rapidly form stable and clear clusters\
    \ after the second iteration."
  Figure 7 Link: articels_figures_by_rev_year\2020\TaskFeature_Collaborative_Learning_with_Application_to_Personalized_Attribute_Pr\figure_7.jpg
  Figure 7 caption: Structural Recovery on Simulation Dataset. The x -axis represents
    the users, the y -axis represents the feature. Compared with the competitors,
    TFCL could leverage a clearer block diagonal structure as the ground-truth.
  Figure 8 Link: articels_figures_by_rev_year\2020\TaskFeature_Collaborative_Learning_with_Application_to_Personalized_Attribute_Pr\figure_8.jpg
  Figure 8 caption: Overall AUC comparisons with Boxplot. Here the scatters represent
    all the results coming from all the attributes for Shoes Dataset and Sun Dataset
    each with 15 repetitions. To show the statistical trends, we plot boxplots for
    the two datasets, respectively. Here the scatters are the 15 repetitions over
    the data splits, while the height of the bar represents the mean performance.
  Figure 9 Link: articels_figures_by_rev_year\2020\TaskFeature_Collaborative_Learning_with_Application_to_Personalized_Attribute_Pr\figure_9.jpg
  Figure 9 caption: 'Ablation Results (I) The y -axis represents the average AUC score
    on the test set, and the x -axis represents different algorithms: Org shows the
    performance of our original TFCL algorithm; woAUC shows the performance of our
    algorithm when the AUC loss is replaced with the squared loss; woG shows the performance
    when our proposed co-grouping factor is removed from the model.'
  First author gender probability: 0.66
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.61
  Name of the first author: Zhiyong Yang
  Name of the last author: Qingming Huang
  Number of Figures: 11
  Number of Tables: 2
  Number of authors: 4
  Paper title: Task-Feature Collaborative Learning with Application to Personalized
    Attribute Prediction
  Publication Date: 2020-04-29 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Different Formulations of \u2211 k i=1 \u03BB i ( L G BI\
      \ ) \u2211i=1k\u03BBi(LGBI), Where Ident. Represents the Identifiability of\
      \ U=V V \u22A4 U=VV\u22A4 When the Eigengap \u03BB k+1 ( L G BI )\u2212 \u03BB\
      \ k ( L G BI ) \u03BBk+1(LGBI)-\u03BBk(LGBI) Vanishes"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Ablation Study for Simulated Dataset
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2991344
- Affiliation of the first author: department of computing, imperial college london,
    south kensington campus, london, united kingdom
  Affiliation of the last author: department of computing, imperial college london,
    south kensington campus, london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2020\Towards_a_Complete_D_Morphable_Model_of_the_Human_Head\figure_1.jpg
  Figure 1 caption: A collection of arbitrary complete head reconstructions from unconstrained
    single images. Our work aims to combine the most important attributes of the human
    head (i.e., face, cranium, ears, eyes), in order to synthesize novel and realistic
    3D head models from data deficient sources.
  Figure 10 Link: articels_figures_by_rev_year\2020\Towards_a_Complete_D_Morphable_Model_of_the_Human_Head\figure_10.jpg
  Figure 10 caption: Head texture completion given an unseen facial texture. (a) Input
    facial texture. (b) Recovered completed texture by a pix2pix translation architecture.
    (c) Ground truth completed texture by a graphics artist.
  Figure 2 Link: articels_figures_by_rev_year\2020\Towards_a_Complete_D_Morphable_Model_of_the_Human_Head\figure_2.jpg
  Figure 2 caption: The bespoke combined face & head models. Visualisation of the
    first four shape components along with the mean head shapes. Due to the large
    demographic information of LSFM we are able to construct bespoke combined head
    model for any given age, gender or ethnicity group.
  Figure 3 Link: articels_figures_by_rev_year\2020\Towards_a_Complete_D_Morphable_Model_of_the_Human_Head\figure_3.jpg
  Figure 3 caption: The regression modeling pipeline. 1) The left part illustrates
    the matrix formulation from the original LYHM head model; 2) the central part
    demonstrates how we utilize the MeIn3D database to produce highly-detailed head
    shapes; 3) the final part on the right depicts the registration framework along
    with the per-vertex template weights and the statistical modeling.
  Figure 4 Link: articels_figures_by_rev_year\2020\Towards_a_Complete_D_Morphable_Model_of_the_Human_Head\figure_4.jpg
  Figure 4 caption: A graphical representation of the non-rigid registration of all
    mean meshes along with our head template S t and the calculation of the local
    covariance matrix K i,j U based on the locations of the i th and j th points.
  Figure 5 Link: articels_figures_by_rev_year\2020\Towards_a_Complete_D_Morphable_Model_of_the_Human_Head\figure_5.jpg
  Figure 5 caption: The model refinement pipeline. We start with the GP model defined
    by the universal covariance matrix. For each scan in the MeIn3D dataset we obtain
    full head reconstruction with GP Regression using the sparse landmarks and dense
    ICP algorithm. We then non-rigidly align the face region of the full head reconstruction
    to the scan, and build a new sample covariance matrix to update our model.
  Figure 6 Link: articels_figures_by_rev_year\2020\Towards_a_Complete_D_Morphable_Model_of_the_Human_Head\figure_6.jpg
  Figure 6 caption: "Visualization of the first five principal shape components (with\
    \ \xB13 standard deviations) of our ear model along with the mean ear shape."
  Figure 7 Link: articels_figures_by_rev_year\2020\Towards_a_Complete_D_Morphable_Model_of_the_Human_Head\figure_7.jpg
  Figure 7 caption: 'The bank of iris textures in our model along with our eye mesh
    structure. Our entire eye mesh topology consists of two meshes: The outer one
    is a transparent mesh that forms the lens and the internal mesh is the eyeball,
    depicting the iris texture and the pupil shape. On the bottom right corner, we
    illustrate variations of pupil dilation and contraction with and with out texture.'
  Figure 8 Link: articels_figures_by_rev_year\2020\Towards_a_Complete_D_Morphable_Model_of_the_Human_Head\figure_8.jpg
  Figure 8 caption: Eye 3DMM fitting pipeline for recovering eye region shape, gaze
    direction, and pupil size from single images.
  Figure 9 Link: articels_figures_by_rev_year\2020\Towards_a_Complete_D_Morphable_Model_of_the_Human_Head\figure_9.jpg
  Figure 9 caption: Illustration of the first five components of the eye region shape
    model S el that outlines the eyelid shape along with the peripheral shape around
    the eye. Our model demonstrates large variance and is capable of reconstructing
    any given eyelid and eye region shape across the human population. (i.e., round,
    almond, monolid, hooded, upturned and downturned eyes).
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Stylianos Ploumpis
  Name of the last author: Stefanos Zafeiriou
  Number of Figures: 18
  Number of Tables: 4
  Number of authors: 9
  Paper title: Towards a Complete 3D Morphable Model of the Human Head
  Publication Date: 2020-04-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Head Shape Estimation Accuracy Results for the Fitted Facial
      Meshes of Our Test Set
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Head Shape Estimation Accuracy Results for the Actual Ground
      Truth 3D Facial Meshes of Our Test Set
  Table 3 caption:
    table_text: TABLE 3 Ear Shape Estimation Accuracy Results for the Ground Truth
      3D Meshes of Our Test Set Around the Ear Area
  Table 4 caption:
    table_text: TABLE 4 Our Model Outperforms eye3DMM [60], CNN [62], Random Forests
      (RF) [63], kNN [62], Adaptive Linear Regression (ALR) [64], and Support Vector
      Regression (SVR) [65] in Mean Gaze Estimation Error on the Eyediap Database
      [61]
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2991150
- Affiliation of the first author: school of electronic information and electrical
    engineering, shanghai jiao tong university, shanghai, china
  Affiliation of the last author: school of electronic information and electrical
    engineering, shanghai jiao tong university, shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2020\APLoss_for_Accurate_OneStage_Object_Detection\figure_1.jpg
  Figure 1 caption: Dashed red boxes are the ground truth object boxes. The orange
    filled boxes and other blank boxes are anchors with positive and negative ground
    truth labels, repectively. (a) shows that the detection performance is poor but
    the classification accuracy is still high due to large number of true negatives.
    (b) shows the ranking metric AP can better reflect the actual condition as it
    does not suffer from the large number of true negatives, and is more intrinsically
    consistent with the detection task.
  Figure 10 Link: articels_figures_by_rev_year\2020\APLoss_for_Accurate_OneStage_Object_Detection\figure_10.jpg
  Figure 10 caption: Time cost of AP-loss and complete iteration. Batch size equals
    8. Only 1 GPU is used. Model are based on RetinaNet-ResNet50 and evaluated on
    PASCAL VOC dataset.
  Figure 2 Link: articels_figures_by_rev_year\2020\APLoss_for_Accurate_OneStage_Object_Detection\figure_2.jpg
  Figure 2 caption: Overall framework of the proposed approach. We replace the classification-task
    in one-stage detectors with a ranking task, where the ranking procedure produces
    the primary terms of AP-loss and the corresponding label vector. The optimization
    algorithm is based on an error-driven learning scheme combined with backpropagation.
    The localization-task branch is not shown here due to no modification.
  Figure 3 Link: articels_figures_by_rev_year\2020\APLoss_for_Accurate_OneStage_Object_Detection\figure_3.jpg
  Figure 3 caption: Comparison of label assignments. The dashed red box is the ground
    truth box with class k . (a) In traditional classification task of one-stage detectors,
    the anchor is assigned a foreground label k . (b) In our ranking task framework,
    the anchor replicates K times, and assign the k th anchor to label 1, others 0.
  Figure 4 Link: articels_figures_by_rev_year\2020\APLoss_for_Accurate_OneStage_Object_Detection\figure_4.jpg
  Figure 4 caption: Heaviside step function and piecewise step function. (Best viewed
    in color).
  Figure 5 Link: articels_figures_by_rev_year\2020\APLoss_for_Accurate_OneStage_Object_Detection\figure_5.jpg
  Figure 5 caption: 'Detection results on COCO dataset. Top: Baseline results by RetinaNet
    with Focal-loss. Bottom: Our results by RetinaNet with AP-loss.'
  Figure 6 Link: articels_figures_by_rev_year\2020\APLoss_for_Accurate_OneStage_Object_Detection\figure_6.jpg
  Figure 6 caption: (a) Detection accuracy (mAP) of RetinaNet on VOC2007 test set.
    (b) Detection accuracy (mAP) of SSD on VOC2007 test set. (Best viewed in color).
  Figure 7 Link: articels_figures_by_rev_year\2020\APLoss_for_Accurate_OneStage_Object_Detection\figure_7.jpg
  Figure 7 caption: The correlation between the IoU of anchor with matched ground-truth
    and the classification score.
  Figure 8 Link: articels_figures_by_rev_year\2020\APLoss_for_Accurate_OneStage_Object_Detection\figure_8.jpg
  Figure 8 caption: (a) Convergence curves of different AP-loss optimizations on VOC2007
    trainval set. (b) Final convergence values of AP-loss in different imbalance conditions.
    Dashed line means the loss cannot converge from the ImageNet pre-trained model.
    (Best viewed in color).
  Figure 9 Link: articels_figures_by_rev_year\2020\APLoss_for_Accurate_OneStage_Object_Detection\figure_9.jpg
  Figure 9 caption: Memory cost of different training losses. All models are based
    on RetinaNet-ResNet50 and evaluated on PASCAL VOC dataset.
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.58
  Name of the first author: Kean Chen
  Name of the last author: Junni Zou
  Number of Figures: 12
  Number of Tables: 6
  Number of authors: 6
  Paper title: AP-Loss for Accurate One-Stage Object Detection
  Publication Date: 2020-04-30 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Ablation Experiments on Batch Size, \u03B4 \u03B4 of Piecewise\
      \ Step Function, and Interpolated AP"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison Through Different Training Losses
  Table 3 caption:
    table_text: TABLE 3 Comparison Through Different Training Losses on Robustness
  Table 4 caption:
    table_text: TABLE 4 Detection Results on VOC2007 test Set
  Table 5 caption:
    table_text: TABLE 5 Detection Results on VOC2012 test Set
  Table 6 caption:
    table_text: TABLE 6 Detection Results on COCO test-dev Set
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2991457
