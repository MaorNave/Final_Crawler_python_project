- Affiliation of the first author: school of computing and information systems, university
    of melbourne, parkville, vic, australia
  Affiliation of the last author: tsinghua university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Video_Joint_Modelling_Based_on_Hierarchical_Transformer_for_CoSummarization\figure_1.jpg
  Figure 1 caption: An illustration of co-summarization for two semantically similar
    videos. Cross-video shot-level information aggregation is performed in our method
    to model the pair-wise dependencies between two arbitrary shots in the two videos.
    Then the shot-wise importance score is predicted based on the shot representation
    that contains the cross-video information. Finally, the summary of each video
    is generated according to the scores.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Video_Joint_Modelling_Based_on_Hierarchical_Transformer_for_CoSummarization\figure_2.jpg
  Figure 2 caption: The structure of the first layer of Transformer encoder. For simplicity,
    only two tokens are shown. The positional encodings are added to the input sequence
    at the beginning. In each layer, the self-attention mechanism is applied to the
    sequence, followed by a residual connection and layer normalization. Then, a positional-wise
    feed-forward network is used to transform each feature into a hidden space. At
    last, a residual connection and layer normalization are equipped.
  Figure 3 Link: articels_figures_by_rev_year\2022\Video_Joint_Modelling_Based_on_Hierarchical_Transformer_for_CoSummarization\figure_3.jpg
  Figure 3 caption: The overview of video joint modelling based on hierarchical Transformer
    (VJMHT) for co-summarization. Without loss of generality, only two videos are
    shown, and we assume each video consists of two shots and each shot consists of
    two frames. Two semantically similar videos are sent into VJMHT simultaneously.
    The frames in each shot are encoded by F-Transformer in parallel to obtain the
    shot embedding. The shots are aggregated across videos in S-Transformer. The encoded
    shot embeddings along with the video representation are combined to predict the
    shot-wise importance scores. In addition, video representation reconstruction
    is performed to minimize the distance between the representation of the summary
    and the video. Transformerslinear projections in the same level have the same
    structure and share parameters.
  Figure 4 Link: articels_figures_by_rev_year\2022\Video_Joint_Modelling_Based_on_Hierarchical_Transformer_for_CoSummarization\figure_4.jpg
  Figure 4 caption: The generated summaries and the predicted scores by VJMHT of four
    videos from TVSum. The keyframes in the first row are sampled from the summaries
    generated by our method. As for the curves in the second row, the blue lines depict
    the ground truth scores, while the orange lines depict the predicted ones. The
    video names are on the upper right.
  Figure 5 Link: articels_figures_by_rev_year\2022\Video_Joint_Modelling_Based_on_Hierarchical_Transformer_for_CoSummarization\figure_5.jpg
  Figure 5 caption: The visualization of the predicted important scores by the model
    without video joint modelling (blue line) and the model with video joint modelling
    (red line). The green lines show the ground truth scores.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Haopeng Li
  Name of the last author: Rui Zhang
  Number of Figures: 5
  Number of Tables: 12
  Number of authors: 4
  Paper title: Video Joint Modelling Based on Hierarchical Transformer for Co-Summarization
  Publication Date: 2022-06-27 00:00:00
  Table 1 caption: TABLE 1 A Summary of the Datasets Used in the Experiments
  Table 10 caption: TABLE 10 The Results of Different Numbers of Videos for Joint
    Modelling on TVSum
  Table 2 caption: TABLE 2 The Results (F-Measure) in Different Settings
  Table 3 caption: TABLE 3 The Comparison Results (Rank Correlation Coefficients)
    on SumMe and TVSum
  Table 4 caption: TABLE 4 The Results (F-Measure) of Ablation Studies
  Table 5 caption: TABLE 5 The F-Measure of Different Numbers of Layers in the F-Transformer,
    as Well as the Inference Time on TVSum
  Table 6 caption: TABLE 6 The F-Measure of Different Numbers of Layers in the S-Transformer,
    as Well as the Inference Time on TVSum
  Table 7 caption: TABLE 7 The F-Measure of Different Numbers of Heads in the F-Transformer
  Table 8 caption: TABLE 8 The F-Measure of Different Numbers of Heads in the S-Transformer
  Table 9 caption: TABLE 9 The F-Measure of Different Dimensions of Shot Embeddings
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3186506
- Affiliation of the first author: school of computer science, faculty of engineering,
    the university of sydney, darlington, nsw, australia
  Affiliation of the last author: school of computer science, faculty of engineering,
    the university of sydney, darlington, nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2022\Hierarchical_Prototype_Networks_for_Continual_Graph_Representation_Learning\figure_1.jpg
  Figure 1 caption: The framework of HPNs. On the left, subgraphs of different tasks
    come in sequentially, with the current task denoted as T s . Given a node v ,
    u j k denotes the j th sampled node from k -hop neighbors. In the middle, node
    v and the sampled neighbors ( N sub (v) ) are fed into the selected AFEs ( AFE
    select node or AFE select struct ) to get atomic embeddings ( E select A (v) ),
    which are either matched to existing A-prototypes ( P A ) denoted with black dashed
    arrows or used to establish new A-prototypes (red dashed arrows). The selected
    A-prototypes A(v) are first mapped to node- and class-level embeddings ( E N (v)
    and E C (v) ), which are further matched to a N- and a C-prototypes ( P N and
    P C ) to get the node- and class-level prototype representations ( N(v) and C(v)
    ). Finally, A(v) , N(v) and C(v) form the hierarchical prototype representation
    ( P H (v) ), which is fed into the classifier to perform node classification.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Hierarchical_Prototype_Networks_for_Continual_Graph_Representation_Learning\figure_2.jpg
  Figure 2 caption: 'Left: dynamics of ARS for continual learning tasks on OGB-Arxiv.
    Middle: impact of t A on the number of prototypes in HPNs over Cora. Right: dynamics
    of memory consumption of HPNs on OGB-Products.'
  Figure 3 Link: articels_figures_by_rev_year\2022\Hierarchical_Prototype_Networks_for_Continual_Graph_Representation_Learning\figure_3.jpg
  Figure 3 caption: (a) and (b) are AM and FM of HPNs with different number of AFE
    s and prototype dimensions on OGB-Arxiv. (c) and (d) are AM and FM change with
    when t A varies on Cora.
  Figure 4 Link: articels_figures_by_rev_year\2022\Hierarchical_Prototype_Networks_for_Continual_Graph_Representation_Learning\figure_4.jpg
  Figure 4 caption: "The left two figures show the results of fixing \u03B1=1.0 and\
    \ tuning \u03B2 from 0.0001 to 1000. The right two figures show the results of\
    \ fixing \u03B2=1.0 and tuning \u03B1 . Logarithmic horizontal axis is adopted,\
    \ and the results are obtained on both Cora and Citeseer datasets."
  Figure 5 Link: articels_figures_by_rev_year\2022\Hierarchical_Prototype_Networks_for_Continual_Graph_Representation_Learning\figure_5.jpg
  Figure 5 caption: Visualization of hierarchical prototype representations of nodes
    in the test set of Cora.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xikun Zhang
  Name of the last author: Dacheng Tao
  Number of Figures: 5
  Number of Tables: 7
  Number of authors: 3
  Paper title: Hierarchical Prototype Networks for Continual Graph Representation
    Learning
  Publication Date: 2022-06-28 00:00:00
  Table 1 caption: TABLE 1 The Detailed Statistics of 5 Datasets Used in Our Experiments
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Performance Comparisons Between HPNs and Baselines on 5
    Different Datasets
  Table 3 caption: TABLE 3 Accuracy (%) Changes of GAT and HPNs
  Table 4 caption: TABLE 4 Accuracy (%) Changes of GAT+GEM and GAT+MAS
  Table 5 caption: TABLE 5 Ablation Study on Prototypes of Different Levels of Prototypes
    Over Cora
  Table 6 caption: TABLE 6 Ablation Study on Different Loss Terms Over Cora
  Table 7 caption: TABLE 7 Final Parameter Amount for Models Trained on OGB-Products
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3186909
- Affiliation of the first author: college of intelligence science and technology,
    national university of defense technology, changsha, hunan, china
  Affiliation of the last author: college of computer science, national university
    of defense technology, changsha, hunan, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Learning_to_Detect_D_Symmetry_From_SingleView_RGBD_Images_With_Weak_Supervision\figure_1.jpg
  Figure 1 caption: We propose a deep neural network to estimate 3D symmetry from
    single-view RGB-D images with weak supervision. Given an incomplete shape (a),
    our method predicts symmetry such that the completed shape based on the symmetry
    (b) is similar to existing plausible shapes. This is achieved by learning to embed
    the plausible shapes into a unified distribution (c) in the latent space.
  Figure 10 Link: articels_figures_by_rev_year\2022\Learning_to_Detect_D_Symmetry_From_SingleView_RGBD_Images_With_Weak_Supervision\figure_10.jpg
  Figure 10 caption: 'Quantitative comparisons between the proposed method and the
    baselines in terms of reflectional symmetry detection. The methods are evaluated
    in: (a) ShapeNet holdout view; (b) ShapeNet holdout instance; (c) ScanNet holdout
    view; and (d) ScanNet holdout scene. The numbers alongside each precision-recall
    curve are the corresponding maximum F1-scores (higher is better).'
  Figure 2 Link: articels_figures_by_rev_year\2022\Learning_to_Detect_D_Symmetry_From_SingleView_RGBD_Images_With_Weak_Supervision\figure_2.jpg
  Figure 2 caption: Method overview. The method consists of two main components. First,
    a discriminative variational autoencoder is proposed to distinguish the plausible
    shapes and the implausible shapes via learning two separate distributions for
    each of them in the latent feature space. Second, given an RGB-D image with an
    object in it, the RGB-D transformer aggregates the multi-modality feature and
    estimates symmetry as well as the symmetry-induced object proposal. The network
    training optimizes the predicted symmetry, such that the object proposal is similar
    to the plausible shapes as much as possible. This is achieved by using the plausibility
    loss and the visibility loss while keeping the parameters in the pre-trained encoder
    of the discriminative VAE fixed.
  Figure 3 Link: articels_figures_by_rev_year\2022\Learning_to_Detect_D_Symmetry_From_SingleView_RGBD_Images_With_Weak_Supervision\figure_3.jpg
  Figure 3 caption: The difference between the proposed discriminative VAE (DVAE)
    trained withwithout the reconstruction loss. (a) The DVAE without the reconstruction
    loss tends to distribute the shapes in the latent space according to its completeness.
    (b) In contrast, the reconstruction loss forces the incomplete yet plausible shapes
    to be close to the complete ones in the latent space, such that the shapes are
    distributed in the latent space by the geometry correctness, i.e., shape plausibility.
  Figure 4 Link: articels_figures_by_rev_year\2022\Learning_to_Detect_D_Symmetry_From_SingleView_RGBD_Images_With_Weak_Supervision\figure_4.jpg
  Figure 4 caption: The architecture of the symmetry detection network. The RGB-D
    transformer first feeds the RGB image and the point cloud (converted from the
    depth image) to a 2D CNN and a point convolutional network to extract multi-scale
    appearance and geometric features, respectively. For each 3D point in the object,
    the multi-scale features are fetched, concatenated, and processed by an attention
    module to learn the correlations. This is achieved by the self-attention layers
    and mutual attention layers. The aggregated object feature is fed into a symmetry
    detector that estimates multiple symmetries as well as the corresponding object
    proposals. The estimated symmetries are then optimized by the plausibility loss
    and the visibility loss, simultaneously.
  Figure 5 Link: articels_figures_by_rev_year\2022\Learning_to_Detect_D_Symmetry_From_SingleView_RGBD_Images_With_Weak_Supervision\figure_5.jpg
  Figure 5 caption: Network architecture of self-attention layers and mutual attention
    layers in the RGB-D transformer. (a) The self-attention layers process the point-level
    multi-scale 2D3D features and produce the point-wise feature mathbf Fps . This
    is done on each point individually. (b) The mutual attention layers take all the
    point-wise features as input and output the aggregated global feature of the entire
    object mathbf Fo .
  Figure 6 Link: articels_figures_by_rev_year\2022\Learning_to_Detect_D_Symmetry_From_SingleView_RGBD_Images_With_Weak_Supervision\figure_6.jpg
  Figure 6 caption: Symmetry parametrization. (a) For reflectional symmetry estimation,
    the network outputs a rigid transformation mathbf T=lbrace mathbf R|mathbf trbrace
    that transfers the input point cloud into a canonical coordinate system where
    the x-y plane, y-z plane, and x-z plane are the potential symmetry planes. (b)
    For rotational symmetry estimation, the network outputs the location of the object
    center mathbf c and the axis of the rotational symmetry mathbf u .
  Figure 7 Link: articels_figures_by_rev_year\2022\Learning_to_Detect_D_Symmetry_From_SingleView_RGBD_Images_With_Weak_Supervision\figure_7.jpg
  Figure 7 caption: The visibility loss optimizes the symmetry detection (middle)
    via penalizing the estimated symmetric counterparts (right) that are located in
    the observed region in the camera frustum of the RGB-D image (left).
  Figure 8 Link: articels_figures_by_rev_year\2022\Learning_to_Detect_D_Symmetry_From_SingleView_RGBD_Images_With_Weak_Supervision\figure_8.jpg
  Figure 8 caption: Qualitative symmetry detection results on ShapeNet [5], KITTI
    [18], and ScanNet [11]. We see that our method is able to handle objects with
    complex geometry, heavy occlusion, texture-less surface, sparse points, and poor
    light condition.
  Figure 9 Link: articels_figures_by_rev_year\2022\Learning_to_Detect_D_Symmetry_From_SingleView_RGBD_Images_With_Weak_Supervision\figure_9.jpg
  Figure 9 caption: Qualitative comparisons against existing methods on challenging
    scenarios. Geometric fitting [14] fails on most of the examples due to the data
    incompleteness. Shape completion [17], [34] is unstable on objects with space
    points. SymmetryNet [53] and Pose estimation [6] could detect most of the symmetries
    correctly, but are less accurate on shapes with complex geometry. In contrast,
    our method could accurately detect all the symmetries.
  First author gender probability: 0.55
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.93
  Name of the first author: Yifei Shi
  Name of the last author: Kai Xu
  Number of Figures: 16
  Number of Tables: 4
  Number of authors: 6
  Paper title: Learning to Detect 3D Symmetry From Single-View RGB-D Images With Weak
    Supervision
  Publication Date: 2022-06-28 00:00:00
  Table 1 caption: TABLE 1 Quantitative Comparisons on Symmetry Prediction
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Parameter Study of the Gaussian Distribution of Implausible
    Shapes
  Table 3 caption: TABLE 3 Parameter Study of the Gaussian Distribution of Implausible
    Shapes
  Table 4 caption: TABLE 4 Robustness Evaluation on Objects With Changing Structure
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3186876
- Affiliation of the first author: s-lab, nanyang technological university (ntu),
    singapore
  Affiliation of the last author: s-lab, nanyang technological university (ntu), singapore
  Figure 1 Link: articels_figures_by_rev_year\2022\GLEAN_Generative_Latent_Bank_for_Image_SuperResolution_and_Beyond\figure_1.jpg
  Figure 1 caption: "Example of 16 \xD7 super-resolution (SR). (a) The low-resolution\
    \ input. (b) ESRGAN [1] trains the SR generator from scratch, often produces artifacts\
    \ and unnatural textures. (c) PULSE [2] achieves more realistic textures through\
    \ GAN inversion but fails to recover ground-truth structures. (d) With the proposed\
    \ generative latent bank, GLEAN is able to generate output that not only is close\
    \ to the ground-truth, but also possesses realistic textures. (e) Our lightweight\
    \ model, LightGLEAN, achieves comparable performance while having significantly\
    \ fewer parameters. (f) The ground-truth image."
  Figure 10 Link: articels_figures_by_rev_year\2022\GLEAN_Generative_Latent_Bank_for_Image_SuperResolution_and_Beyond\figure_10.jpg
  Figure 10 caption: Results of Image colorization. In addition to super-resolution,
    GLEAN can be extended to other restoration tasks such as colorization. By employing
    the generative priors in BigGAN, GLEAN tends to produce natural color when compared
    to existing works. GLEAN is also applicable to real-world old photos.
  Figure 2 Link: articels_figures_by_rev_year\2022\GLEAN_Generative_Latent_Bank_for_Image_SuperResolution_and_Beyond\figure_2.jpg
  Figure 2 caption: "Overview of GLEAN. In addition to the latent vectors c i , the\
    \ generator (i.e., the generative latent bank) is also conditioned on the multi-resolution\
    \ features f i . With a pre-trained GAN capturing the natural image prior, this\
    \ encoder-bank-decoder design lifts the burden of learning both fidelity and naturalness\
    \ in the conventional encoder-decoder architecture. E i , S i , and D i denote\
    \ the encoder blocks, latent bank blocks, and decoder blocks, respectively. This\
    \ example corresponds to an input size of 32\xD732 and an output size of 256\xD7\
    256 ."
  Figure 3 Link: articels_figures_by_rev_year\2022\GLEAN_Generative_Latent_Bank_for_Image_SuperResolution_and_Beyond\figure_3.jpg
  Figure 3 caption: "Overview of LightGLEAN. Unlike GLEAN, which generates features\
    \ with resolution down to 4\xD74 , the latent bank in LightGLEAN directly conditions\
    \ on the RRDB feature f 0 , bypassing the style blocks that corresponds to the\
    \ coarse resolutions. In addition, a fixed latent code c is used for all style\
    \ blocks. In this design, LightGLEAN can be devised with much fewer learnable\
    \ parameters."
  Figure 4 Link: articels_figures_by_rev_year\2022\GLEAN_Generative_Latent_Bank_for_Image_SuperResolution_and_Beyond\figure_4.jpg
  Figure 4 caption: Qualitative comparison between GLEAN and LightGLEAN. Despite being
    more lightweight than GLEAN, LightGLEAN provides outputs that are comparable to
    GLEAN. (Zoom in for best view)
  Figure 5 Link: articels_figures_by_rev_year\2022\GLEAN_Generative_Latent_Bank_for_Image_SuperResolution_and_Beyond\figure_5.jpg
  Figure 5 caption: "Comparisons on 16 \xD7 SR on CelebA-HQ [55]. Only GLEAN is able\
    \ to maintain high fidelity while synthesizing realistic textures and details:\
    \ GAN inversion methods fail to preserve the identity, and adversarial loss methods\
    \ struggle to synthesize fine details. ESRGAN + denotes a larger version with\
    \ similar runtime to GLEAN. (Zoom in for best view)"
  Figure 6 Link: articels_figures_by_rev_year\2022\GLEAN_Generative_Latent_Bank_for_Image_SuperResolution_and_Beyond\figure_6.jpg
  Figure 6 caption: "Results of 16 \xD7 SR on other categories. GLEAN can be applied\
    \ to various categories by switching between StyleGANs trained on different categories.\
    \ (Zoom in for best view)"
  Figure 7 Link: articels_figures_by_rev_year\2022\GLEAN_Generative_Latent_Bank_for_Image_SuperResolution_and_Beyond\figure_7.jpg
  Figure 7 caption: "Results on larger scale factors. GLEAN reconstructs realistic\
    \ images highly similar to the GT for up to 64\xD7 upscaling factor. (Zoom in\
    \ for best view)"
  Figure 8 Link: articels_figures_by_rev_year\2022\GLEAN_Generative_Latent_Bank_for_Image_SuperResolution_and_Beyond\figure_8.jpg
  Figure 8 caption: Outputs with diverse poses and contents. Despite GLEAN being trained
    with aligned human faces, it is able to reconstruct faithful images for non-aligned
    and non-human faces. PULSE approximates the GT in low resolution (inlet image
    at the bottom left corner), but its outputs are significantly different from the
    GT when viewed in high resolution.
  Figure 9 Link: articels_figures_by_rev_year\2022\GLEAN_Generative_Latent_Bank_for_Image_SuperResolution_and_Beyond\figure_9.jpg
  Figure 9 caption: Results of Super-Resolution using BigGAN. By employing the multi-class
    prior encapulated in BigGAN [10], GLEAN can be applied to multiple classes using
    a single model. GLEAN outperforms existing works in terms of both fidelity and
    quality. (Zoom in for best view)
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.64
  Name of the first author: Kelvin C.K. Chan
  Name of the last author: Chen Change Loy
  Number of Figures: 17
  Number of Tables: 7
  Number of authors: 5
  Paper title: 'GLEAN: Generative Latent Bank for Image Super-Resolution and Beyond'
  Publication Date: 2022-06-28 00:00:00
  Table 1 caption: TABLE 1 Complexity Comparison Between LightGLEAN and GLEAN
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Datasets Used in Our Experiments
  Table 3 caption: "TABLE 3 Cosine Similarity of ArcFace Features [66] for 16 \xD7\
    \ \xD7 SR"
  Table 4 caption: "TABLE 4 Quantitative (PSNRLPIPS) Comparison on 16 \xD7 \xD7 SR"
  Table 5 caption: TABLE 5 Complexity Comparison
  Table 6 caption: TABLE 6 Quantitative (PSNRLPIPS) Comparison on ImageNet, Using
    BigGAN as the Latent Bank
  Table 7 caption: TABLE 7 Quantitative (NIQEFIDIdentity Similarity) Comparison on
    Real-World Face Image Restoration
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3186715
- Affiliation of the first author: future media center and school of computer science
    and engineering, the university of electronic science and technology of china,
    chengdu, china
  Affiliation of the last author: future media center and school of computer science
    and engineering, the university of electronic science and technology of china,
    chengdu, china
  Figure 1 Link: articels_figures_by_rev_year\2022\LabelGuided_Generative_Adversarial_Network_for_Realistic_Image_Synthesis\figure_1.jpg
  Figure 1 caption: Some synthesized examples of our Lab2Pix-V1 (above the dash) and
    Lab2Pix-V2 (below the dash). Our Lab2Pixs take label maps as the inputs and predicts
    the corresponding realistic images with unpaired-data and paired-data learning.
    The task is extremely hard since the generated samples are supposed to match the
    input label maps and keep realistic in complex scenes at the same time. The generated
    samples from our model are colorful and photo-realistic and contain detailed textures.
  Figure 10 Link: articels_figures_by_rev_year\2022\LabelGuided_Generative_Adversarial_Network_for_Realistic_Image_Synthesis\figure_10.jpg
  Figure 10 caption: Ablation study results on the Cityscapes dataset with paired-data
    training. Lab2Pix-V2 with all components obtains the best results.
  Figure 2 Link: articels_figures_by_rev_year\2022\LabelGuided_Generative_Adversarial_Network_for_Realistic_Image_Synthesis\figure_2.jpg
  Figure 2 caption: Illustration of our proposed unified Lab2Pix (including Lab2Pix-V1
    and Lab2Pix-V2) framework. The generator takes a label map to synthesize multi-scale
    images, and independent discriminators give hierarchical discriminative results
    for each image based on the foreground map extracted from the label map.
  Figure 3 Link: articels_figures_by_rev_year\2022\LabelGuided_Generative_Adversarial_Network_for_Realistic_Image_Synthesis\figure_3.jpg
  Figure 3 caption: The structure of our Adaptive Label Encoder. It separately encodes
    the sketch and semantic label maps according to their characteristics.
  Figure 4 Link: articels_figures_by_rev_year\2022\LabelGuided_Generative_Adversarial_Network_for_Realistic_Image_Synthesis\figure_4.jpg
  Figure 4 caption: The structure of our DG-Norm. The input feature is normalized
    by batch normalization first. Then we use both the label map and global feature
    to predict the new distribution parameters which effect the normalized feature.
  Figure 5 Link: articels_figures_by_rev_year\2022\LabelGuided_Generative_Adversarial_Network_for_Realistic_Image_Synthesis\figure_5.jpg
  Figure 5 caption: The structure of an Label Guided Spatial Co-Attention (LSCA) block.
    Our LSCA fuses features in different layers by an attention map with label guidance.
    C denotes the operation of concatenating.
  Figure 6 Link: articels_figures_by_rev_year\2022\LabelGuided_Generative_Adversarial_Network_for_Realistic_Image_Synthesis\figure_6.jpg
  Figure 6 caption: The proposed hierarchical perceptual discriminator structure,
    which includes two branches. The perceptual branch take the images as the input.
    The concatenation of images and the corresponding label maps are fed to the main
    branch in Lab2Pix-V2, while in Lab2Pix-V1, only images are fed. v 1 , v 2 and
    v 3 indicate Conv 11 - pool 3 , Conv 41 - pool 4 and Conv 51 - pool 5 of pretrained
    VGG16. s represents stride in convolution and pooling. C denotes the operation
    of concatenating.
  Figure 7 Link: articels_figures_by_rev_year\2022\LabelGuided_Generative_Adversarial_Network_for_Realistic_Image_Synthesis\figure_7.jpg
  Figure 7 caption: The proposed unpaired-data Lab2Pix-V1 structure. It takes either
    a sketch label map or a semantic label map as input to produce photo-realistic
    images. The generator use an adaptive label encoder to separately encode the sketch
    and semantic label maps according to their characteristics, and gradually outputs
    higher-resolution (small, medium and large) images in one forward process. The
    structures of different images is guaranteed to be close by the image consistency
    loss, while the correspondence of the output image and input label is verified
    by the cycle segmentation loss.
  Figure 8 Link: articels_figures_by_rev_year\2022\LabelGuided_Generative_Adversarial_Network_for_Realistic_Image_Synthesis\figure_8.jpg
  Figure 8 caption: The proposed paired-data Lab2Pix-V2 structure. The generator outputs
    one high-resolution image in one forward pass and we downsample it to obtain images
    in lower resolutions. The generated samples and real samples are concatenated
    with the label maps respectively before inputted to mathbfDi . Note that, we give
    up LSCA owing to our limited hardware settings.
  Figure 9 Link: articels_figures_by_rev_year\2022\LabelGuided_Generative_Adversarial_Network_for_Realistic_Image_Synthesis\figure_9.jpg
  Figure 9 caption: Ablation study results on the Cityscapes dataset with unpaired-data
    training. Lab2Pix-V1 with all components obtains the best results.
  First author gender probability: 0.61
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Junchen Zhu
  Name of the last author: Heng Tao Shen
  Number of Figures: 17
  Number of Tables: 6
  Number of authors: 5
  Paper title: Label-Guided Generative Adversarial Network for Realistic Image Synthesis
  Publication Date: 2022-06-28 00:00:00
  Table 1 caption: TABLE 1 Ablation Study of the Proposed Unpaired-Data Lab2Pix-V1
    on the Cityscapes Dataset
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Ablation Study of the Proposed Unpaired-Data Lab2Pix-V1
    and Paired-Data Lab2Pix-V2 on Cityscapes Dataset
  Table 3 caption: TABLE 3 Quantitative Results of Different Methods on Cityscapes
    and Facades Datasets
  Table 4 caption: TABLE 4 Quantitative Results of Different Methods on the Edges2shoes
    and Edges2handbags Datasets
  Table 5 caption: TABLE 5 Quantitative Results of Different Methods on Cityscapes,
    COCO-Stuff and ADE20 K Datasets
  Table 6 caption: TABLE 6 Results of the User Study on Cityscapes, COCO-Stuff and
    ADE20 K Datasets. Higher Score Indicates the Results of This Method are Considered
    to Have Higher Quality by Human. We Also Provide the Top1 and Top2 (Chosen as
    the Best and the Best Two Samples) Rates of Each Method in the User Study
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3186752
- Affiliation of the first author: department of computer science, university of pittsburgh,
    pittsburgh, pa, usa
  Affiliation of the last author: google research, zurich, switzerland
  Figure 1 Link: articels_figures_by_rev_year\2022\Learning_to_Overcome_Noise_in_Weak_Caption_Supervision_for_Object_Detection\figure_1.jpg
  Figure 1 caption: We propose two mechanisms to infer pseudo training labels from
    captions. First (top), we determine the potential for strong object supervision
    signal from image-caption pairs (showing one with strong and one with weak signal).
    When supervision is strong, a simple training label extraction technique can be
    used. Second (bottom), we learn a mapping function (a text classifier) from captions
    to labels, which compensates for failures of exact-matching label extraction.
    Finally (right), we train a weakly-supervised object detection model with these
    pseudo image labels.
  Figure 10 Link: articels_figures_by_rev_year\2022\Learning_to_Overcome_Noise_in_Weak_Caption_Supervision_for_Object_Detection\figure_10.jpg
  Figure 10 caption: Vision-language pretraining for weakly-supervised object detection.
    The inputs to the BERT-Tiny model are positional embeddings ( E0, E1, ldots )
    and token embeddings (proposal features ( E[IMG0], E[IMG1], ldots ) and word embeddings
    ( Etyping, Eon, ldots )).
  Figure 2 Link: articels_figures_by_rev_year\2022\Learning_to_Overcome_Noise_in_Weak_Caption_Supervision_for_Object_Detection\figure_2.jpg
  Figure 2 caption: "Image-caption weighting: Green arrows connect neighbors in the\
    \ original semantic space. Blue links show co-occurring images and text. Images\
    \ whose texts are close in semantic space, which are close in the joint space\
    \ (short red links), have high homogeneity scores [ \u03B3 s in Eq. (9)]."
  Figure 3 Link: articels_figures_by_rev_year\2022\Learning_to_Overcome_Noise_in_Weak_Caption_Supervision_for_Object_Detection\figure_3.jpg
  Figure 3 caption: "Harvesting detection models from free-form text. We propose to\
    \ use a pseudo training label inference module (bottom) to amplify signals in\
    \ free-form texts to supervise the learning of the multiple instance detection\
    \ network (top). The detection model is refined by an online refinement module\
    \ (right) to produce the final detection results. Detection ( o det i,j \u2208\
    \ R C ) and classification ( o cls i,j \u2208 R C ) scores and image predictions\
    \ p i \u2208 R C refer to predictions for all classes."
  Figure 4 Link: articels_figures_by_rev_year\2022\Learning_to_Overcome_Noise_in_Weak_Caption_Supervision_for_Object_Detection\figure_4.jpg
  Figure 4 caption: "Example image-caption pairs from MIRFlickr1M and Conceptual Caption\
    \ datasets. For MIRFlickr1M, captions and tags are written by the uploader and\
    \ website users. For ConcCap, captions are parsed from the alt-text HTML attribute\
    \ associated with web images. Both datasets did not crowdsource i.e., pay workers\
    \ to label the images. Note how often \u201Cperson\u201D is mentioned on the right\
    \ but not visible."
  Figure 5 Link: articels_figures_by_rev_year\2022\Learning_to_Overcome_Noise_in_Weak_Caption_Supervision_for_Object_Detection\figure_5.jpg
  Figure 5 caption: Analysis of different text supervision. We compare the pseudo
    labels (Section 3.2) to COCO val ground-truth.
  Figure 6 Link: articels_figures_by_rev_year\2022\Learning_to_Overcome_Noise_in_Weak_Caption_Supervision_for_Object_Detection\figure_6.jpg
  Figure 6 caption: Demonstration of different pseudo labels. Our method fills the
    gap between what is present and what is mentioned, by making inferences on the
    semantic level. Matches to the ground truth are shown in blue.
  Figure 7 Link: articels_figures_by_rev_year\2022\Learning_to_Overcome_Noise_in_Weak_Caption_Supervision_for_Object_Detection\figure_7.jpg
  Figure 7 caption: Recall of PASCAL labels. We evaluate the recall of the COCO-learned
    text classifier, but we show only the overlapped 20 PASCAL VOC classes.
  Figure 8 Link: articels_figures_by_rev_year\2022\Learning_to_Overcome_Noise_in_Weak_Caption_Supervision_for_Object_Detection\figure_8.jpg
  Figure 8 caption: Visualization of our Cap2Det model results on COCO val set. We
    show boxes with confidence scores > 5%. Green boxes denote correct detection results
    ( IoU>0.5 ) while red boxes indicate incorrect ones. Best viewed with 300% zoom-in.
  Figure 9 Link: articels_figures_by_rev_year\2022\Learning_to_Overcome_Noise_in_Weak_Caption_Supervision_for_Object_Detection\figure_9.jpg
  Figure 9 caption: Data versus Performance. Our text classifier learned on COCO generalized
    well on Flickr30K and the noisier Flickr200K data (subset of MIRFlickr1M) formed
    by user-generated content tags.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Mesut Erhan Unal
  Name of the last author: Jesse Berent
  Number of Figures: 12
  Number of Tables: 10
  Number of authors: 8
  Paper title: Learning to Overcome Noise in Weak Caption Supervision for Object Detection
  Publication Date: 2022-06-30 00:00:00
  Table 1 caption: TABLE 1 Average Precision (in %) on the VOC 2007 Test Set (Learning
    From COCO, Flickr30K, MIRFlickr1M, and Conceptual Captions Captions)
  Table 10 caption: TABLE 10 COCO Detection Using Ground-Truth Image Labels, With
    Supervised Models at the Top, Best WSOD in bold
  Table 2 caption: TABLE 2 COCO Test-Dev Results (Learning From COCO Captions), Measured
    by COCO Eval Server
  Table 3 caption: TABLE 3 Evaluation of Our Pseudo Label Inference, Using Ren et
    al. [9] as Our WSOD Method, on COCO2017-Val
  Table 4 caption: TABLE 4 Ranking Images and Object Categories by how Well Captions
    Overlap With the True Image-Level Labels
  Table 5 caption: TABLE 5 Comparing the Filtering Strategies With the Random Sampling
    Baseline, Using AP (in %) on VOC 2007 Test
  Table 6 caption: TABLE 6 Evaluating the Caption Weighting Strategy, Using AP (in
    %) on VOC 2007 Test
  Table 7 caption: TABLE 7 The Impact of Large Corpus Pretraining, Measuring AP (in
    %) on VOC 2007 Test
  Table 8 caption: TABLE 8 Weighting on Conceptual Captions, Using AP (in %) on VOC
    2007 Test Set
  Table 9 caption: TABLE 9 Average Precision (in %) on the Pascal VOC Test Set Using
    Ground-Truth Image-Level Labels
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3187350
- Affiliation of the first author: school of computer and information technology,
    shanxi university, taiyuan, shanxi, china
  Affiliation of the last author: school of computer and information technology, shanxi
    university, taiyuan, shanxi, china
  Figure 1 Link: articels_figures_by_rev_year\2022\SelfConstrained_Spectral_Clustering\figure_1.jpg
  Figure 1 caption: Comparison of the proposed algorithm with semi-supervised spectral
    clustering.
  Figure 10 Link: articels_figures_by_rev_year\2022\SelfConstrained_Spectral_Clustering\figure_10.jpg
  Figure 10 caption: Effect of self-constrained terms on the performance of Self-CSC.
  Figure 2 Link: articels_figures_by_rev_year\2022\SelfConstrained_Spectral_Clustering\figure_2.jpg
  Figure 2 caption: Effect of parameter t on the performance of Self-CSC.
  Figure 3 Link: articels_figures_by_rev_year\2022\SelfConstrained_Spectral_Clustering\figure_3.jpg
  Figure 3 caption: Effect of parameter o on the performance of Self-CSC.
  Figure 4 Link: articels_figures_by_rev_year\2022\SelfConstrained_Spectral_Clustering\figure_4.jpg
  Figure 4 caption: "Effect of parameter \u03B1 on the performance of Self-CSC."
  Figure 5 Link: articels_figures_by_rev_year\2022\SelfConstrained_Spectral_Clustering\figure_5.jpg
  Figure 5 caption: Effect of parameter beta on the performance of Self-CSC.
  Figure 6 Link: articels_figures_by_rev_year\2022\SelfConstrained_Spectral_Clustering\figure_6.jpg
  Figure 6 caption: Effect of parameter lambda on the performance of Self-CSC.
  Figure 7 Link: articels_figures_by_rev_year\2022\SelfConstrained_Spectral_Clustering\figure_7.jpg
  Figure 7 caption: Effect of parameter g on the performance of Self-CSC.
  Figure 8 Link: articels_figures_by_rev_year\2022\SelfConstrained_Spectral_Clustering\figure_8.jpg
  Figure 8 caption: Effect of initialization of Z on the performance of Self-CSC.
  Figure 9 Link: articels_figures_by_rev_year\2022\SelfConstrained_Spectral_Clustering\figure_9.jpg
  Figure 9 caption: Effect of initialization of V on the performance of Self-CSC.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.67
  Name of the first author: Liang Bai
  Name of the last author: Yunxiao Zhao
  Number of Figures: 10
  Number of Tables: 7
  Number of authors: 3
  Paper title: Self-Constrained Spectral Clustering
  Publication Date: 2022-07-04 00:00:00
  Table 1 caption: TABLE 1 Description of Benchmark Data Sets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 ACC Values of Different Algorithms With k k-Means on Benchmark
    Data Sets
  Table 3 caption: TABLE 3 ARI Values of Different Algorithms With k k-Means on Benchmark
    Data Sets
  Table 4 caption: TABLE 4 NMI Values of Different Algorithms With k k-Means on Benchmark
    Data Sets
  Table 5 caption: TABLE 5 ACC Values of Different Algorithms With Linkage on Benchmark
    Data Sets
  Table 6 caption: TABLE 6 ARI Values of Different Algorithms With Linkage on Benchmark
    Data Sets
  Table 7 caption: TABLE 7 NMI Values of Different Algorithms With Linkage on Benchmark
    Data Sets
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3188160
- Affiliation of the first author: state key laboratory of integrated services networks,
    xidian university, xian, china
  Affiliation of the last author: ubtech sydney artificial intelligence centre and
    the school of information technologies, faculty of engineering and information
    technologies, university of sydney, darlington, nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2022\Tensorized_Bipartite_Graph_Learning_for_MultiView_Clustering\figure_1.jpg
  Figure 1 caption: The framework of TBGL, where B (v) is the pre-defined graph of
    the v th view; C (v) is the learned graph of the v th view; E (v) is error.
  Figure 10 Link: articels_figures_by_rev_year\2022\Tensorized_Bipartite_Graph_Learning_for_MultiView_Clustering\figure_10.jpg
  Figure 10 caption: The performances of TBGL with varying the number of anchor points
    on MSRC-v5 and Handwritten4 datasets.
  Figure 2 Link: articels_figures_by_rev_year\2022\Tensorized_Bipartite_Graph_Learning_for_MultiView_Clustering\figure_2.jpg
  Figure 2 caption: "Construction of tensor C\u2208 R n\xD7V\xD7m . \u03A8 (j) denotes\
    \ the jth frontal slice of C ( j\u22081, 2, \u22EF, m )."
  Figure 3 Link: articels_figures_by_rev_year\2022\Tensorized_Bipartite_Graph_Learning_for_MultiView_Clustering\figure_3.jpg
  Figure 3 caption: Illustration of variance-based de-correlation anchor selection
    (VDA).
  Figure 4 Link: articels_figures_by_rev_year\2022\Tensorized_Bipartite_Graph_Learning_for_MultiView_Clustering\figure_4.jpg
  Figure 4 caption: The visualizations of anchor selection results of different methods
    on four toy datasets, where the points with same color represent a cluster; the
    corresponding colored numbers in the first column represent the number of data
    points in the cluster; the red circles represent the selected anchors; the corresponding
    colored numbers from the second to fourth columns represent the number of selected
    anchors in the cluster.
  Figure 5 Link: articels_figures_by_rev_year\2022\Tensorized_Bipartite_Graph_Learning_for_MultiView_Clustering\figure_5.jpg
  Figure 5 caption: The clustering performances of the proposed method with different
    anchor selection methods on four real-world datasets.
  Figure 6 Link: articels_figures_by_rev_year\2022\Tensorized_Bipartite_Graph_Learning_for_MultiView_Clustering\figure_6.jpg
  Figure 6 caption: The clustering performances of our method with the varying value
    of p on MSRC-v5 and Handwritten4 datasets.
  Figure 7 Link: articels_figures_by_rev_year\2022\Tensorized_Bipartite_Graph_Learning_for_MultiView_Clustering\figure_7.jpg
  Figure 7 caption: The visualizations of the learned consensus graph w.w.o. ell text
    1,2 -norm penalty on MSRC dataset.
  Figure 8 Link: articels_figures_by_rev_year\2022\Tensorized_Bipartite_Graph_Learning_for_MultiView_Clustering\figure_8.jpg
  Figure 8 caption: The clustering performances of TBGL w.w.o. self-weighted scheme
    on four real-world datasets.
  Figure 9 Link: articels_figures_by_rev_year\2022\Tensorized_Bipartite_Graph_Learning_for_MultiView_Clustering\figure_9.jpg
  Figure 9 caption: The visualization of the learned bipartite graph via proposed
    method on synthetic dataset.
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Wei Xia
  Name of the last author: Dacheng Tao
  Number of Figures: 11
  Number of Tables: 5
  Number of authors: 6
  Paper title: Tensorized Bipartite Graph Learning for Multi-View Clustering
  Publication Date: 2022-07-04 00:00:00
  Table 1 caption: TABLE 1 Storage Complexity of TBGL, Where V, N, M, K are the Number
    of Views, Samples, Anchors, and Clusters, Respectively
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The Clustering Performances on MSRC-V5 and Handwritten4
    Datasets
  Table 3 caption: TABLE 3 The Clustering Performances on Mnist4 and Caltech101-20
    Datasets
  Table 4 caption: TABLE 4 The Clustering Results and CPU Time (sec.) on Three Large-Scale
    Datasets
  Table 5 caption: "TABLE 5 The Clustering Performances w.w.o. The \u2113 1,2 \u2113\
    1,2-Norm Constraint on Six Datasets, Where the Best Results are Represented by\
    \ Bold Value"
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3187976
- Affiliation of the first author: bytedance ai lab, beijing, china
  Affiliation of the last author: department of computer science, university of california,
    davis, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\An_IntermediateLevel_Attack_Framework_on_the_Basis_of_Linear_Regression\figure_1.jpg
  Figure 1 caption: An overview of the problem setting in [1], [14], and this article,
    in which a baseline attack was performed in advance.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\An_IntermediateLevel_Attack_Framework_on_the_Basis_of_Linear_Regression\figure_2.jpg
  Figure 2 caption: "Illustration of how the intermediate-level discrepancy is calculated\
    \ given an adversarial input x+ \u0394 x and its benign counterpart x . We will\
    \ discuss how 1) the magnitude of the intermediate-level discrepancy (i.e., \u2225\
    g(x+ \u0394 x )\u2212 h adv 0 \u2225 ) and 2) the directional guide for optimizing\
    \ \u0394 x would affect the obtained transferability."
  Figure 3 Link: articels_figures_by_rev_year\2022\An_IntermediateLevel_Attack_Framework_on_the_Basis_of_Linear_Regression\figure_3.jpg
  Figure 3 caption: "On the left is the average success rate of the four methods,\
    \ and on the right the average \u2113 2 magnitude of the intermediate-level discrepancies.\
    \ Apparently, they are positively correlated, and larger intermediate-level distortions\
    \ lead to higher attack success rates on the victim models."
  Figure 4 Link: articels_figures_by_rev_year\2022\An_IntermediateLevel_Attack_Framework_on_the_Basis_of_Linear_Regression\figure_4.jpg
  Figure 4 caption: "How the hyper-parameters of the linear regression methods affect\
    \ the average success rate and middle-layer distortion (i.e., the magnitude of\
    \ intermediate-level discrepancies). On the left subplots, we report the success\
    \ rates, and on the right subplots, we report the magnitudes. \u03F5=8255 ."
  Figure 5 Link: articels_figures_by_rev_year\2022\An_IntermediateLevel_Attack_Framework_on_the_Basis_of_Linear_Regression\figure_5.jpg
  Figure 5 caption: Visualization of the obtained adversarial perturbations using
    I-FGSM. It can be seen that perturbations at later iterations are all very similar.
    epsilon =8255 .
  Figure 6 Link: articels_figures_by_rev_year\2022\An_IntermediateLevel_Attack_Framework_on_the_Basis_of_Linear_Regression\figure_6.jpg
  Figure 6 caption: How the number of PGD runs affects the average attack success
    rate in our framework. epsilon =8255 .
  Figure 7 Link: articels_figures_by_rev_year\2022\An_IntermediateLevel_Attack_Framework_on_the_Basis_of_Linear_Regression\figure_7.jpg
  Figure 7 caption: How the number of LinBP runs affects the average attack success
    rate in our framework. epsilon =8255 .
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Yiwen Guo
  Name of the last author: Hao Chen
  Number of Figures: 7
  Number of Tables: 11
  Number of authors: 4
  Paper title: An Intermediate-Level Attack Framework on the Basis of Linear Regression
  Publication Date: 2022-07-04 00:00:00
  Table 1 caption: TABLE 1 Comparison of ILA++ to ILA and the Baseline Attack, i.e.,
    I-FGSM
  Table 10 caption: TABLE 10 Further Performance Gain Can Be Achieved by Performing
    More than One Run of a Baseline Method and Using a More Powerful Baseline Attack,
    e.g., LinBP
  Table 2 caption: "TABLE 2 Comparison of Different Linear Regression Models in \u2113\
    \ \u221E \u2113\u221E Attack Settings"
  Table 3 caption: "TABLE 3 Comparison of Different Linear Regression Models in \u2113\
    \ 2 \u21132 Attack Settings"
  Table 4 caption: "TABLE 4 Evaluation of Normalized \u2113 \u221E \u2113\u221E Attacks"
  Table 5 caption: "TABLE 5 Evaluation of Normalized \u2113 2 \u21132 Attacks"
  Table 6 caption: "TABLE 6 Average Success Rate of I-FGSM+ElasticNet on Models Excluding\
    \ ResNet-50 in the Untargeted \u2113 \u221E \u2113\u221E Setting"
  Table 7 caption: "TABLE 7 Success Rate of I-FGSM+ElasticNet on the Source Model,\
    \ i.e., ResNet-50, in the Untargeted \u2113 \u221E \u2113\u221E Setting"
  Table 8 caption: TABLE 8 Evaluation of Random Directional Guides in the Input Space
  Table 9 caption: "TABLE 9 Evaluation of Random Directional Guides Generated Directly\
    \ in the Feature Space (Indicated as Rand \u2020 \u2020)"
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3188044
- Affiliation of the first author: engineering research center of digital forensics,
    ministry of education, school of computer and software, nanjing university of
    information science and technology, nanjing, china
  Affiliation of the last author: engineering research center of digital forensics,
    ministry of education, school of computer and software, nanjing university of
    information science and technology, nanjing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Adaptive_MultiView_and_Temporal_Fusing_Transformer_for_D_Human_Pose_Estimation\figure_1.jpg
  Figure 1 caption: 'The architecture of MTF-Transformer. It consists of three successive
    modules: Feature Extractor, Multi-view Fusing Transformer (MFT), and Temporal
    Fusing Transformer (TFT). Feature Extractor predicts 2D pose ( P 2D and C 2D )
    first and then encodes 2D pose into a feature vector for each frame. MFT measures
    the implicit relationship between each pair of views to reconstruct the feature
    adaptively. TFT aggregates the temporal information of the whole sequence and
    predicts the 3D pose of the center frame.'
  Figure 10 Link: articels_figures_by_rev_year\2022\Adaptive_MultiView_and_Temporal_Fusing_Transformer_for_D_Human_Pose_Estimation\figure_10.jpg
  Figure 10 caption: Some predictions under different mask rate.
  Figure 2 Link: articels_figures_by_rev_year\2022\Adaptive_MultiView_and_Temporal_Fusing_Transformer_for_D_Human_Pose_Estimation\figure_2.jpg
  Figure 2 caption: The feature embedding module encodes the 2D prediction into a
    feature vector. It splits the 2D prediction into five partitions and then uses
    five branches to extract features. Finally, the features of five partitions are
    concatenated and mapped to a global feature f .
  Figure 3 Link: articels_figures_by_rev_year\2022\Adaptive_MultiView_and_Temporal_Fusing_Transformer_for_D_Human_Pose_Estimation\figure_3.jpg
  Figure 3 caption: The architecture of multi-view fusing transformer.
  Figure 4 Link: articels_figures_by_rev_year\2022\Adaptive_MultiView_and_Temporal_Fusing_Transformer_for_D_Human_Pose_Estimation\figure_4.jpg
  Figure 4 caption: The architecture of relative relation encoding module.
  Figure 5 Link: articels_figures_by_rev_year\2022\Adaptive_MultiView_and_Temporal_Fusing_Transformer_for_D_Human_Pose_Estimation\figure_5.jpg
  Figure 5 caption: The architecture of temporal fusing transformer. It predicts the
    3D pose of the middle frame.
  Figure 6 Link: articels_figures_by_rev_year\2022\Adaptive_MultiView_and_Temporal_Fusing_Transformer_for_D_Human_Pose_Estimation\figure_6.jpg
  Figure 6 caption: Results of FLEX and MTF-Transformer with different view numbers
    on the Human3.6M.
  Figure 7 Link: articels_figures_by_rev_year\2022\Adaptive_MultiView_and_Temporal_Fusing_Transformer_for_D_Human_Pose_Estimation\figure_7.jpg
  Figure 7 caption: Demonstration of transfer FLEX and MTF-Transformer trained on
    Human3.6M to KTH Multiview Football II.
  Figure 8 Link: articels_figures_by_rev_year\2022\Adaptive_MultiView_and_Temporal_Fusing_Transformer_for_D_Human_Pose_Estimation\figure_8.jpg
  Figure 8 caption: Demonstration of MFT with and without Tij on Human3.6M.
  Figure 9 Link: articels_figures_by_rev_year\2022\Adaptive_MultiView_and_Temporal_Fusing_Transformer_for_D_Human_Pose_Estimation\figure_9.jpg
  Figure 9 caption: The contribution of the 2D pose (17 joint point) from 4 views
    to the 3D prediction (X, Y, Z coordinate) with different kind of Relative-Attention
    module. We measure the gradient of the predicted 3D coordinate to the features
    of each views and consider the maximum value as the contribution ratio. For better
    visualization, the values are normalized to the range of 0 to 1. (a) Transformer
    (b) Point Transformer trained on 4 views (c) MFT trained on 4 views (d) Point
    Transformer trained on 2 views (e) MFT trained on 2 views.
  First author gender probability: 0.96
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Hui Shuai
  Name of the last author: Qingshan Liu
  Number of Figures: 10
  Number of Tables: 12
  Number of authors: 3
  Paper title: Adaptive Multi-View and Temporal Fusing Transformer for 3D Human Pose
    Estimation
  Publication Date: 2022-07-05 00:00:00
  Table 1 caption: TABLE 1 Quantitative Results on Human3.6M
  Table 10 caption: TABLE 10 Results on Human3.6M With Different Number of Added View
  Table 2 caption: TABLE 2 Quantitative Results on TotalCapture
  Table 3 caption: TABLE 3 Results of Different Procedures to Fuse the 2D Pose and
    the Confidence From 2D Detector on Human3.6M
  Table 4 caption: TABLE 4 The Results of Different Design of CAA
  Table 5 caption: TABLE 5 Results of Different Relative Attention Modules on Human3.6M
  Table 6 caption: TABLE 6 Generalization Capability of Different Relative Attention
    Modules
  Table 7 caption: TABLE 7 Results on Human3.6M With Different Setting of D D
  Table 8 caption: TABLE 8 Results of Different Mask Rate M M on Human3.6M
  Table 9 caption: TABLE 9 Results of Different Sequence Length T T on Human3.6M
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3188716
