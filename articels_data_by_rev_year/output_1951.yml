- Affiliation of the first author: university of maryland, college park, md, usa
  Affiliation of the last author: university of maryland, college park, md, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\SfSNet_Learning_Shape_Reflectance_and_Illuminance_of_Faces_in_the_Wild\figure_1.jpg
  Figure 1 caption: Decomposing real world faces into shape, reflectance and illuminance.
    We present SfSNet that learns from a combination of labeled synthetic and unlabeled
    real data to produce an accurate decomposition of an image into surface normals,
    albedo and lighting. Our companion network, SfSMesh, then uses these surface normals
    and the original image to reconstruct a full 3D mesh. (Best viewed in color)
  Figure 10 Link: articels_figures_by_rev_year\2020\SfSNet_Learning_Shape_Reflectance_and_Illuminance_of_Faces_in_the_Wild\figure_10.jpg
  Figure 10 caption: Selected Results From top 5 percent (a,b,c,d) and worst 5 percent
    (e,f,g,h) Reconstructed Images. (Best viewed in color).
  Figure 2 Link: articels_figures_by_rev_year\2020\SfSNet_Learning_Shape_Reflectance_and_Illuminance_of_Faces_in_the_Wild\figure_2.jpg
  Figure 2 caption: Network Architecture. Our SfSNet consists of a novel decomposition
    architecture that uses residual blocks to produce normal and albedo features.
    They are further utilized along with image features to estimate lighting, inspired
    by a physical rendering model. f combines normal and lighting to produce shading.
    (Best viewed in color).
  Figure 3 Link: articels_figures_by_rev_year\2020\SfSNet_Learning_Shape_Reflectance_and_Illuminance_of_Faces_in_the_Wild\figure_3.jpg
  Figure 3 caption: 'Decomposition architectures. We experiment with two architectures:
    (a) skip connection based encoder-decoder; (b) proposed residual block based network.
    Skip connections are shown in red.'
  Figure 4 Link: articels_figures_by_rev_year\2020\SfSNet_Learning_Shape_Reflectance_and_Illuminance_of_Faces_in_the_Wild\figure_4.jpg
  Figure 4 caption: SfSMesh Pipeline. We present SfSMesh, which uses a U-Net architecture
    to generate PNCC and deformation images (Deform) from input image and normals
    predicted by SfSNet. These are combined to produce a surface parameterization
    (Param) of the face. From the surface parametrization normals are calcuated and
    compared with SfS normals. Yellow triangles denote L1 losses used during the first
    phase of training with synthetic data. Both yellow and green triangles indicate
    L1 losses used during the second phase of training with real data.
  Figure 5 Link: articels_figures_by_rev_year\2020\SfSNet_Learning_Shape_Reflectance_and_Illuminance_of_Faces_in_the_Wild\figure_5.jpg
  Figure 5 caption: Inverse Rendering. SfSNet vs Neural Face [10] on the data showcased
    by the authors. (Best viewed in color).
  Figure 6 Link: articels_figures_by_rev_year\2020\SfSNet_Learning_Shape_Reflectance_and_Illuminance_of_Faces_in_the_Wild\figure_6.jpg
  Figure 6 caption: Light Transfer. SfSNet vs Neural Face [10] on the image showcased
    by the authors. We transfer the lighting of the Source image to the Target image
    to produce Transfer image. S denotes shading. Both Target images contain an orangey
    glow, which is not present in the Source image. Ideally in the Transfer image,
    the orangey glow should be removed. Neural Face fails to get rid of the orangey
    lighting effect of the Target image in the Transfer image. (Best viewed in color).
  Figure 7 Link: articels_figures_by_rev_year\2020\SfSNet_Learning_Shape_Reflectance_and_Illuminance_of_Faces_in_the_Wild\figure_7.jpg
  Figure 7 caption: Inverse Rendering. SfSNet vs MoFA [11] on the data provided by
    the authors of the paper. (Best viewed in color).
  Figure 8 Link: articels_figures_by_rev_year\2020\SfSNet_Learning_Shape_Reflectance_and_Illuminance_of_Faces_in_the_Wild\figure_8.jpg
  Figure 8 caption: Inverse Rendering on the Photoface dataset [13] with SfSNet-finetuned.
    The ground-truth albedo is in gray-scale and it encourages our network to also
    output gray-scale albedo.
  Figure 9 Link: articels_figures_by_rev_year\2020\SfSNet_Learning_Shape_Reflectance_and_Illuminance_of_Faces_in_the_Wild\figure_9.jpg
  Figure 9 caption: SfSNet vs Pix2Vertex [7]. Normals produced by SfSNet are significantly
    better than Pix2Vertex, especially for non-ambient illumination and expression.
    Relit images are generated by directional lighting and uniform albedo to highlight
    the quality of the reconstructed normals. Note that (a), (b), and (c) are the
    images showcased by the authors. (Best viewed in color).
  First author gender probability: 0.96
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Soumyadip Sengupta
  Name of the last author: David W. Jacobs
  Number of Figures: 14
  Number of Tables: 5
  Number of authors: 5
  Paper title: 'SfSNet: Learning Shape, Reflectance and Illuminance of Faces in the
    Wild'
  Publication Date: 2020-12-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Normal Reconstruction Error on the Photoface Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Light Classification Accuracy on MultiPIE Dataset
  Table 3 caption:
    table_text: TABLE 3 Reconstruction Error SfSMesh Outperforms ExpNet and Pix2Vertex
      on Bosphorus Dataset. [16]
  Table 4 caption:
    table_text: TABLE 4 Role of SfS-Supervision Training
  Table 5 caption:
    table_text: TABLE 5 SfSNet vs SkipNet+
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3046915
- Affiliation of the first author: cooperative medianet innovation center, shanghai
    jiao tong university, shanghai, china
  Affiliation of the last author: cooperative medianet innovation center, shanghai
    jiao tong university, shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Inferring_Point_Cloud_Quality_via_Graph_Similarity\figure_1.jpg
  Figure 1 caption: "GraphSIM. Objective point cloud quality assessment via GraphSIM\
    \ that consists of the following steps: resampling-based geometric keypoints extraction;\
    \ local graph construction centered at keypoints within clustered neighbors; color\
    \ gradients aggregation for aggregating gradient moments in each graph for a specific\
    \ color channel; and similarity pooling across all color channels and local graphs.\
    \ Resampling is performed using original point cloud geometry information to extract\
    \ keypoints that will be used in both reference and distorted contents as common\
    \ ground. Neighbor clustering assumes a sphere of radius \u03B8 (depicted as orange\
    \ circle). The figure illustrates RGB color space, but other color spaces are\
    \ also applicable."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Inferring_Point_Cloud_Quality_via_Graph_Similarity\figure_2.jpg
  Figure 2 caption: "A Toy Example. a) a local graph as the reference; b) reconstructed\
    \ graph with \u201Cdownsampling\u201D artifacts in which partial points are removed\
    \ from the reference; c) reconstructed graph after performing the clock-wisely\
    \ geometric rotation from the reference; d) reconstructed graph with Compression\
    \ and Reconstruction (C&R) distortions in which additional points are generated\
    \ in processing from the reference. These newly generated points are connected\
    \ to the keypoint using yellow arrows. For the sake of simplicity, we assume all\
    \ neighbors in purple rings have the same distance to the keypoint, and we set\
    \ the local neighbor connection weight as 1."
  Figure 3 Link: articels_figures_by_rev_year\2020\Inferring_Point_Cloud_Quality_via_Graph_Similarity\figure_3.jpg
  Figure 3 caption: Point cloud databases with snapshots of original point clouds.
    (a) SJTU-PCQA [28] and (b) IRPC [29].
  Figure 4 Link: articels_figures_by_rev_year\2020\Inferring_Point_Cloud_Quality_via_Graph_Similarity\figure_4.jpg
  Figure 4 caption: Exemplified point cloud corrupted by GGN and DS. The associated
    MOS and GraphSIM values are also provided.
  Figure 5 Link: articels_figures_by_rev_year\2020\Inferring_Point_Cloud_Quality_via_Graph_Similarity\figure_5.jpg
  Figure 5 caption: "MOS prediction accuracy of objective metrics: subplots (a)-(d)\
    \ are point-wise distance-based metrics used for geometry distortion without color\
    \ attributes (no CN points); subplots (e)(f) consider color attributes (with CN\
    \ points); solid line is \u201C y=x \u201D implying the perfect prediction when\
    \ being overlapped with this line."
  Figure 6 Link: articels_figures_by_rev_year\2020\Inferring_Point_Cloud_Quality_via_Graph_Similarity\figure_6.jpg
  Figure 6 caption: Model performance changes as k increases.
  Figure 7 Link: articels_figures_by_rev_year\2020\Inferring_Point_Cloud_Quality_via_Graph_Similarity\figure_7.jpg
  Figure 7 caption: Model performance changes as k increases.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Qi Yang
  Name of the last author: Jun Sun
  Number of Figures: 7
  Number of Tables: 11
  Number of authors: 5
  Paper title: Inferring Point Cloud Quality via Graph Similarity
  Publication Date: 2020-12-24 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Sample Point Clouds Illustration
  Table 10 caption:
    table_text: TABLE 10 Model Performance in Terms of Various Types of Signal Attributes
  Table 2 caption:
    table_text: TABLE 2 Sample Point Clouds of IPRC Database
  Table 3 caption:
    table_text: TABLE 3 Model Performance (PLCC, SROCC, and RMSE) for Different Point
      Clouds Categories in SJTU-PCQA Database
  Table 4 caption:
    table_text: TABLE 4 Model Performance (PLCC, SROCC and RMSE) for Point Clouds
      Samples in SJTU-PCQA Database in Terms of Different Impairments
  Table 5 caption:
    table_text: TABLE 5 Model Performance (PLCC, SROCC and RMSE) for Different Point
      Clouds Encoded Using G-PCC and V-PCC
  Table 6 caption:
    table_text: TABLE 6 Objective Score (PSNR YUV YUV , GraphSIM GraphSIM) and MOS
      of People Samples in IRPC [29]
  Table 7 caption:
    table_text: TABLE 7 Model Performance With Various Color Spaces
  Table 8 caption:
    table_text: 'TABLE 8 Model Performance With Different Resampling Mechanism: SJTU-PCQA
      People Category is Exemplified With Other Contents Having the Similar Outcomes'
  Table 9 caption:
    table_text: TABLE 9 Model Performance With Various Pooling Methods
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3047083
- Affiliation of the first author: microsoft research asia, beijing, china
  Affiliation of the last author: microsoft research asia, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Global_Context_Networks\figure_1.jpg
  Figure 1 caption: Visualization of attention maps (heatmaps) for different query
    positions (red points) in a non-local block on COCO object detection. The three
    attention maps are all almost the same. More examples are presented in Fig. 2.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Global_Context_Networks\figure_2.jpg
  Figure 2 caption: Visualization of attention maps (heatmaps) for different query
    positions (red points) in a non-local block on COCO object detection. For the
    same image, the attention maps of different query points are almost the same.
    Best viewed in color.
  Figure 3 Link: articels_figures_by_rev_year\2020\Global_Context_Networks\figure_3.jpg
  Figure 3 caption: "Two instantiations of the non-local block: Embedded Gaussian\
    \ and Gaussian. The feature maps are shown by their dimensions, e.g., CxHxW. \u2297\
    \ denotes matrix multiplication, and \u2295 is broadcast element-wise addition.\
    \ For two matrices with different dimensions, broadcast operations first broadcast\
    \ features in each dimension to match the dimensions of the two matrices. The\
    \ feature maps marked in red (e.g., \u2464 att) are statistically analyzed in\
    \ Tables 1, 3 and 2."
  Figure 4 Link: articels_figures_by_rev_year\2020\Global_Context_Networks\figure_4.jpg
  Figure 4 caption: "Architecture of the main blocks. The feature maps are shown as\
    \ feature dimensions, e.g., CxHxW denotes a feature map with channel number C,\
    \ height H and width W. \u2297 denotes matrix multiplication, \u2295 denotes broadcast\
    \ element-wise addition, and \u2299 denotes broadcast element-wise multiplication."
  Figure 5 Link: articels_figures_by_rev_year\2020\Global_Context_Networks\figure_5.jpg
  Figure 5 caption: Visualizations of context modeling attention maps (heatmaps) of
    GCNet and NLNet (red points denote query positions). Their learnt attention maps
    are mostly similar. Also, they learn to focus more on hard cases like relatively
    small size, deformation, occlusion, and blur. Best viewed in color.
  Figure 6 Link: articels_figures_by_rev_year\2020\Global_Context_Networks\figure_6.jpg
  Figure 6 caption: Activation of output of the transform function at different stages
    of GCNet. Best viewed in color.
  Figure 7 Link: articels_figures_by_rev_year\2020\Global_Context_Networks\figure_7.jpg
  Figure 7 caption: Distributions of class selectivity index of ResNet-50 baseline
    and ResNet-50+GCNet on different layers. For deeper layers, GCNet exhibits less
    class selectivity compared to the baseline. Best viewed in color.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.53
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Yue Cao
  Name of the last author: Han Hu
  Number of Figures: 7
  Number of Tables: 13
  Number of authors: 5
  Paper title: Global Context Networks
  Publication Date: 2020-12-24 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Statistical Analysis on Four Instantiations of Non-Local Blocks
  Table 10 caption:
    table_text: TABLE 10 Results of GCNet and NLNet Based on Slow-Only Baseline Using
      R50 as Backbone on Kinetics Validation Set
  Table 2 caption:
    table_text: TABLE 2 Statistical Analysis of Non-Local Block (Embedded Gaussian)
      at Different Stages on Three Tasks
  Table 3 caption:
    table_text: TABLE 3 Fine-Grained Statistical Analysis of Non-Local Block (Embedded
      Gaussian and Gaussian) on Three Tasks
  Table 4 caption:
    table_text: TABLE 4 Statistical Analysis Using Four Instantiations of Non-Local
      Blocks on Cityscape Semantic Segmentation
  Table 5 caption:
    table_text: TABLE 5 Ablation Study Based on Mask R-CNN, Using ResNet-50 as Backbone
      With FPN, for Object Detection and Instance Segmentation on COCO 2017 Validation
      Set
  Table 6 caption:
    table_text: TABLE 6 Ablation Study on Different Normalization and Training Schedules
      for Object Detection and Instance Segmentation on COCO 2017 Validation Set
  Table 7 caption:
    table_text: TABLE 7 Results of GCNet (ratio 4 and 16) With Stronger Backbones
      on COCO 2017 Validation and Test-Dev Sets
  Table 8 caption:
    table_text: TABLE 8 Ablation Study of GCNet with ResNet-50 for Image Classification
      on ImageNet Validation Set
  Table 9 caption:
    table_text: TABLE 9 Comparison of State-of-the-Art Methods With ResNet-50 for
      Image Classification on ImageNet Validation Set
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3047209
- Affiliation of the first author: department of statistics, university of california,
    berkeley, ca, usa
  Affiliation of the last author: electronics and communication sciences unit, indian
    statistical institute, kolkata, india
  Figure 1 Link: articels_figures_by_rev_year\2020\Detecting_Meaningful_Clusters_From_HighDimensional_Data_A_Strongly_Consistent_Sp\figure_1.jpg
  Figure 1 caption: t-SNE plots for Lymphoma dataset, showing the performance of LWK,
    compared to other peer algorithms.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Detecting_Meaningful_Clusters_From_HighDimensional_Data_A_Strongly_Consistent_Sp\figure_2.jpg
  Figure 2 caption: Optimization procedures for W - k -means, sparse k -means, and
    LW - k -means.
  Figure 3 Link: articels_figures_by_rev_year\2020\Detecting_Meaningful_Clusters_From_HighDimensional_Data_A_Strongly_Consistent_Sp\figure_3.jpg
  Figure 3 caption: Ground truth clustering and partitioning by different algorithms
    for data1 dataset.
  Figure 4 Link: articels_figures_by_rev_year\2020\Detecting_Meaningful_Clusters_From_HighDimensional_Data_A_Strongly_Consistent_Sp\figure_4.jpg
  Figure 4 caption: "Mean regularization path and average CER for different values\
    \ of \u03BB for leukemia dataset."
  Figure 5 Link: articels_figures_by_rev_year\2020\Detecting_Meaningful_Clusters_From_HighDimensional_Data_A_Strongly_Consistent_Sp\figure_5.jpg
  Figure 5 caption: Average Weights assigned to different features by the W - k -means,
    sparse k -means and LW - k -means algorithm for lymphoma dataset. LW - k -means
    assigns zero feature weights to many of the features whereas W - k -means and
    sparse k -means does not.
  Figure 6 Link: articels_figures_by_rev_year\2020\Detecting_Meaningful_Clusters_From_HighDimensional_Data_A_Strongly_Consistent_Sp\figure_6.jpg
  Figure 6 caption: t-SNE visualisation of leukemia dataset, showing the performances
    of different clustering algorithms. Here label 0 (denoted by red dots) denotes
    lymphoblastic leukemia (ALL) and label 1 (denoted by with black triangles) denotes
    myeloblastic leukemia (AML).
  Figure 7 Link: articels_figures_by_rev_year\2020\Detecting_Meaningful_Clusters_From_HighDimensional_Data_A_Strongly_Consistent_Sp\figure_7.jpg
  Figure 7 caption: Mean regularization paths for dataset toy1.
  Figure 8 Link: articels_figures_by_rev_year\2020\Detecting_Meaningful_Clusters_From_HighDimensional_Data_A_Strongly_Consistent_Sp\figure_8.jpg
  Figure 8 caption: Boxplot of the average weights assigned by LW - k -means and W
    - k -means to features 1 and 2 for all the 70 datasets. The boxplot shows a lesser
    variability for the LW - k -means weights compared to the W - k -means weights.
  Figure 9 Link: articels_figures_by_rev_year\2020\Detecting_Meaningful_Clusters_From_HighDimensional_Data_A_Strongly_Consistent_Sp\figure_9.jpg
  Figure 9 caption: Boxplot of the average weights assigned by the LW - k -means and
    W - k -means algorithms to features 451 to 500 for all the 70 datasets. The boxplot
    shows that LW - k -means assigns exactly zero feature weights to unimportant features,
    whereas W - k -means fails to do so.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.8
  Name of the first author: Saptarshi Chakraborty
  Name of the last author: Swagatam Das
  Number of Figures: 9
  Number of Tables: 9
  Number of authors: 2
  Paper title: 'Detecting Meaningful Clusters From High-Dimensional Data: A Strongly
    Consistent Sparse Center-Based Clustering Approach'
  Publication Date: 2020-12-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Some Well Known Algorithms on Feature Weighting and Feature
      Selection
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average CER on Lymphoma Dataset
  Table 3 caption:
    table_text: TABLE 3 Symbols and Their Meanings
  Table 4 caption:
    table_text: TABLE 4 Average Performance (in Terms of NMI and CER) for Synthetic
      Datasets
  Table 5 caption:
    table_text: TABLE 5 Average Performance (in Terms of CER and NMI) for Real-Life
      Datasets
  Table 6 caption:
    table_text: TABLE 6 Average CER of MADD and LW LW- k k-Means on Microarray Datasets
  Table 7 caption:
    table_text: TABLE 7 Rand Index Accuracy on UCI Benchmark Data
  Table 8 caption:
    table_text: TABLE 8 Comparison Between LW LW- k k-Means and IF-HCT-PCA
  Table 9 caption:
    table_text: TABLE 9 Matthews Correlation Coefficient For Synthetic Datasets
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3047489
- Affiliation of the first author: state key lab of cad&cg, college of computer science,
    zhejiang university, hangzhou, zhejiang, china
  Affiliation of the last author: state key lab of cad&cg, college of computer science,
    zhejiang university, hangzhou, zhejiang, china
  Figure 1 Link: articels_figures_by_rev_year\2020\PVNet_PixelWise_Voting_Network_for_DoF_Object_Pose_Estimation\figure_1.jpg
  Figure 1 caption: The basic idea of PVNet. Given an input image (a), we predict
    vectors pointing to keypoints for each pixel, as shown in (b), and localize 2D
    keypoints in a RANSAC-based voting scheme, as shown in (c). Based on the correspondences
    between 2D and 3D keypoints, as illustrated in (d) and (e), the 6D object pose
    is recovered by solving a Perspective-n-Point (PnP) problem. The proposed method
    is robust to occlusion (g) and truncation (h), where the green bounding boxes
    represent the ground truth poses and the blue bounding boxes represent our predictions.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\PVNet_PixelWise_Voting_Network_for_DoF_Object_Pose_Estimation\figure_2.jpg
  Figure 2 caption: 'Overview of the keypoint localization using PVNet: (a) An image
    of the Occluded LINEMOD dataset. (b) The architecture of PVNet. (c) Pixel-wise
    vectors pointing to the object keypoints. (d) Semantic labels. (e) Hypotheses
    of the keypoint locations generated by voting. The hypotheses with higher voting
    scores are brighter. (f) Probability distributions of the keypoint locations estimated
    from hypotheses. The mean of a distribution is represented by a red star and the
    covariance matrix is shown by ellipses.'
  Figure 3 Link: articels_figures_by_rev_year\2020\PVNet_PixelWise_Voting_Network_for_DoF_Object_Pose_Estimation\figure_3.jpg
  Figure 3 caption: (a) A 3D object model and its 3D bounding box. (b) Hypotheses
    produced by PVNet for a bounding box corner. (c) Hypotheses produced by PVNet
    for a keypoint selected on the object surface. The smaller variance of the surface
    keypoint shows that it is easier to localize the surface keypoint than the bounding
    box corner in our approach.
  Figure 4 Link: articels_figures_by_rev_year\2020\PVNet_PixelWise_Voting_Network_for_DoF_Object_Pose_Estimation\figure_4.jpg
  Figure 4 caption: Visualization of keypoints of four objects in the LINEMOD dataset.
  Figure 5 Link: articels_figures_by_rev_year\2020\PVNet_PixelWise_Voting_Network_for_DoF_Object_Pose_Estimation\figure_5.jpg
  Figure 5 caption: Using PVNet with a detector for pose estimation of multiple objects.
    With an existing object detector, we detect the objects in the image and crop
    the corresponding image regions into image patches. Then PVNet estimates the 2D
    keypoints for each object, which are used to recover the object poses through
    a PnP solver.
  Figure 6 Link: articels_figures_by_rev_year\2020\PVNet_PixelWise_Voting_Network_for_DoF_Object_Pose_Estimation\figure_6.jpg
  Figure 6 caption: Visualization of results on the Occlusion LINEMOD dataset. Green
    bounding boxes represent the ground-truth poses while blue bounding boxes represent
    our predictions.
  Figure 7 Link: articels_figures_by_rev_year\2020\PVNet_PixelWise_Voting_Network_for_DoF_Object_Pose_Estimation\figure_7.jpg
  Figure 7 caption: Visualization of results on the Truncation LINEMOD dataset are
    shown. The images of the last column are the failure cases, where the visible
    parts are too ambiguous to provide enough information for the pose estimation.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.87
  Name of the first author: Sida Peng
  Name of the last author: Hujun Bao
  Number of Figures: 7
  Number of Tables: 9
  Number of authors: 6
  Paper title: 'PVNet: Pixel-Wise Voting Network for 6DoF Object Pose Estimation'
  Publication Date: 2020-12-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Ablation Studies on Different Configurations for Pose Estimation
      on the Occluded LINEMOD Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Accuracies of Our Method and the Baseline Methods on LINEMOD
      Dataset in Terms of 2D Projection Metric
  Table 3 caption:
    table_text: TABLE 3 The Accuracies of Our Method and the Baseline Methods on the
      LINEMOD Dataset in Terms of the ADD(-S) Metric, Where Glue and Eggbox are Considered
      as Symmetric Objects
  Table 4 caption:
    table_text: TABLE 4 The Accuracies of Our Method and the Baseline Methods on the
      Occluded LINEMOD Dataset in Terms of the ADD(-S) Metric, Where Glue and Eggbox
      are Considered as Symmetric Objects
  Table 5 caption:
    table_text: TABLE 5 The Accuracies of Our Method and the Baseline Methods on the
      Occluded LINEMOD Dataset in Terms of the 2D Projection Metric
  Table 6 caption:
    table_text: TABLE 6 Our results on the Truncated LINEMOD Dataset in Terms of the
      2D Projection and the ADD(-S) Metrics, Where Glue and Eggbox are Considered
      as Symmetric Objects
  Table 7 caption:
    table_text: TABLE 7 The Accuracies of Our Method and the Baseline Methods on the
      YCB-Video Dataset in Terms of the 2D Projection and the ADD(-S) AUC Metrics
  Table 8 caption:
    table_text: TABLE 8 The Accuracies of Our Method and the Baseline Methods on the
      Tless Dataset in Terms of the VSD Metric
  Table 9 caption:
    table_text: TABLE 9 The Accuracies of Methods Based on Different Representations
      on Occluded and Truncated LINEMOD in Terms of the PCK Metric, Which is Evaluated
      Only on Occluded or Truncated Keypoints
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3047388
- Affiliation of the first author: department of biomedical engineering, tsinghua
    university, beijing, china
  Affiliation of the last author: department of biomedical engineering, tsinghua university,
    beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2020\OANet_Learning_TwoView_Correspondences_and_Geometry_Using_OrderAware_Network\figure_1.jpg
  Figure 1 caption: 'The Order-Aware Network to learn two-view correspondences and
    geometry. Inputting correspondences and side information, our network predicts
    the inlier probabilities and EF matrix. PointCN blocks are used to process unordered
    input. Besides, we exploit three operations to exploit the local and global context:
    the DiffPool and DiffUnpool layer to capture local context and the Order-Aware
    Filtering block for global context.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\OANet_Learning_TwoView_Correspondences_and_Geometry_Using_OrderAware_Network\figure_2.jpg
  Figure 2 caption: "Differentiable Pooling layer. DiffPool maps nodes X l to clusters\
    \ X l+1 in a soft assignment manner. The soft assignment matrix is learned by\
    \ h pool (\u22C5) which contains one PointCN block (in dashed red box) and one\
    \ softmax layer."
  Figure 3 Link: articels_figures_by_rev_year\2020\OANet_Learning_TwoView_Correspondences_and_Geometry_Using_OrderAware_Network\figure_3.jpg
  Figure 3 caption: "Designs of Differentiable Unpooling layer, which upsamples X\
    \ \u2032 l+1 to X \u2032 l . (a) Plain DiffUnpool layer. It learns a soft assignment\
    \ matrix using features at level l+1 . (b) Order-Aware DiffUnpool layer. It learns\
    \ a soft assignment matrix using features at level l which can encode the order\
    \ information of nodes at level l ."
  Figure 4 Link: articels_figures_by_rev_year\2020\OANet_Learning_TwoView_Correspondences_and_Geometry_Using_OrderAware_Network\figure_4.jpg
  Figure 4 caption: Order-Aware Filtering block. We insert the Spatial Correlation
    layer to PointCN ResNet block. This layer is complementary to PointCN and can
    help capture the global context effectively. Sizes of feature maps are also marked.
  Figure 5 Link: articels_figures_by_rev_year\2020\OANet_Learning_TwoView_Correspondences_and_Geometry_Using_OrderAware_Network\figure_5.jpg
  Figure 5 caption: Matching results using RANSAC (top), PointCN [16] (middle) and
    our method (bottom). Images are taken from test set of YFCC100M and SUN3D datasets.
    Correpondences are in green if they conform the ground truth essential matrix
    (true positives), and in red otherwise (false positives). Best viewed in color.
  Figure 6 Link: articels_figures_by_rev_year\2020\OANet_Learning_TwoView_Correspondences_and_Geometry_Using_OrderAware_Network\figure_6.jpg
  Figure 6 caption: Training and testing in different numbers of input points. Model
    trained with more points has better generalization ability.
  Figure 7 Link: articels_figures_by_rev_year\2020\OANet_Learning_TwoView_Correspondences_and_Geometry_Using_OrderAware_Network\figure_7.jpg
  Figure 7 caption: DiffUnpool layer visualization. Top 15 responses in different
    clusters are visualized in the same image pair. Different clusters might correspond
    to different motions in different areas. Best viewed in color with 200 percent
    zoom in.
  Figure 8 Link: articels_figures_by_rev_year\2020\OANet_Learning_TwoView_Correspondences_and_Geometry_Using_OrderAware_Network\figure_8.jpg
  Figure 8 caption: DiffUnpool layer visualization. Top 20 responses of one particular
    are visualized in different image pairs. Motions in different pairs are roughly
    consistent. Best viewed in color with 200 percent zoom in.
  Figure 9 Link: articels_figures_by_rev_year\2020\OANet_Learning_TwoView_Correspondences_and_Geometry_Using_OrderAware_Network\figure_9.jpg
  Figure 9 caption: Reconstructions from the Alamo dataset. Traditional method fails
    but our method successes in this challenging scene.
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.55
  Name of the first author: Jiahui Zhang
  Name of the last author: Hongen Liao
  Number of Figures: 9
  Number of Tables: 8
  Number of authors: 10
  Paper title: 'OANet: Learning Two-View Correspondences and Geometry Using Order-Aware
    Network'
  Publication Date: 2020-12-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performances of Baseline Network [16] on YFCC100M Unknown
      Sequences
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Ablation Study on YFCC100M
  Table 3 caption:
    table_text: TABLE 3 Comparison With Other Baselines on Unknown Scenes of YFCC100M
      and SUN3D
  Table 4 caption:
    table_text: TABLE 4 Fundamental Matrix Estimation Results of Different Methods
  Table 5 caption:
    table_text: TABLE 5 Results on FM-Bench [70]
  Table 6 caption:
    table_text: TABLE 6 Part of the Results of the CVPR 2019 Image Matching Challenge
  Table 7 caption:
    table_text: TABLE 7 3D Reconstruction Results on Middle Scale Datasets [71]
  Table 8 caption:
    table_text: TABLE 8 Results of Visual Localization Experiments on the Aachen Day-Night
      Dataset [4], [86]
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3048013
- Affiliation of the first author: key laboratory of intelligent information processing,
    institute of computing technology, chinese academy of sciences, beijing, china
  Affiliation of the last author: school of computer science and technology, university
    of chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Not_All_Samples_are_Trustworthy_Towards_Deep_Robust_SVP_Prediction\figure_1.jpg
  Figure 1 caption: "Overview of our approach. (1) Constructing a comparison graph\
    \ from the crowdsourcing annotations, which is contaminated with outlier labels.\
    \ (2) We propose a generalized deep probabilistic framework, where an outlier\
    \ parameter \u03B3 is learned along with the network parameters \u0398 . Moreover,\
    \ we propose a reformulated framework, which could serve the same purpose even\
    \ without \u03B3 . (3) Our framework will output a clean graph on the training\
    \ set, where contaminated annotations are eliminated. Furthermore, our model could\
    \ predict a rank-preserved score for each unseen instance. Best viewed in color."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Not_All_Samples_are_Trustworthy_Towards_Deep_Robust_SVP_Prediction\figure_2.jpg
  Figure 2 caption: "Comparison of the training strategies over the Human Age Dataset.\
    \ Here \u201CAlt.+ \u03B3 \u201D represents the test set performance from the\
    \ alternative training of the robust learning framework, \u201CJt.+ \u03B3 \u201D\
    \ represents the result from the joint training of the robust learning framework,\
    \ \u201Cvanilla\u201D represents the result from vanilla deep learning model without\
    \ robust learning. We use Model A proposed in Section 5 as an example."
  Figure 3 Link: articels_figures_by_rev_year\2020\Not_All_Samples_are_Trustworthy_Towards_Deep_Robust_SVP_Prediction\figure_3.jpg
  Figure 3 caption: Two implementations of the proposed framework. In Model A, we
    implement a huber-loss-based contraction. In Model B, we implement a rlogit-loss-based-contraction.
  Figure 4 Link: articels_figures_by_rev_year\2020\Not_All_Samples_are_Trustworthy_Towards_Deep_Robust_SVP_Prediction\figure_4.jpg
  Figure 4 caption: Outlier examples detected on Human age dataset.
  Figure 5 Link: articels_figures_by_rev_year\2020\Not_All_Samples_are_Trustworthy_Towards_Deep_Robust_SVP_Prediction\figure_5.jpg
  Figure 5 caption: Average sensitivity analysis on Human age dataset.
  Figure 6 Link: articels_figures_by_rev_year\2020\Not_All_Samples_are_Trustworthy_Towards_Deep_Robust_SVP_Prediction\figure_6.jpg
  Figure 6 caption: Paired comparison matrix with outliers painted on Human age dataset.
  Figure 7 Link: articels_figures_by_rev_year\2020\Not_All_Samples_are_Trustworthy_Towards_Deep_Robust_SVP_Prediction\figure_7.jpg
  Figure 7 caption: Performance comparison under different outlier ratios on Human
    age dataset.
  Figure 8 Link: articels_figures_by_rev_year\2020\Not_All_Samples_are_Trustworthy_Towards_Deep_Robust_SVP_Prediction\figure_8.jpg
  Figure 8 caption: Outlier examples of 4 representative attributes on LFW-10 dataset.
  Figure 9 Link: articels_figures_by_rev_year\2020\Not_All_Samples_are_Trustworthy_Towards_Deep_Robust_SVP_Prediction\figure_9.jpg
  Figure 9 caption: Outlier examples of 4 representative attributes on Shoes dataset.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.89
  Name of the first author: Qianqian Xu
  Name of the last author: Qingming Huang
  Number of Figures: 9
  Number of Tables: 6
  Number of authors: 6
  Paper title: 'Not All Samples are Trustworthy: Towards Deep Robust SVP Prediction'
  Publication Date: 2020-12-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparisons of the Proposed Frameworks
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Notations and Descriptions
  Table 3 caption:
    table_text: TABLE 3 Dataset Summary
  Table 4 caption:
    table_text: TABLE 4 Competitive Results (%) on Human Age Dataset With the Best
      and Second Best Results Highlighted in the Corresponding Color
  Table 5 caption:
    table_text: TABLE 5 Competitive ACC (%) Results on LFW-10 Dataset With the Best
      and Second Best Results Highlighted in the Corresponding Color
  Table 6 caption:
    table_text: TABLE 6 Competitive ACC (%) Results on Shoes Dataset
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3047817
- Affiliation of the first author: reler laboratory, australian artificial intelligence
    institute, university of technology sydney, ultimo, nsw, australia
  Affiliation of the last author: reler laboratory, australian artificial intelligence
    institute, university of technology sydney, ultimo, nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2020\SelfCorrection_for_Human_Parsing\figure_1.jpg
  Figure 1 caption: Different types of label noises in ground-truth annotations. The
    upper row shows the original images. The lower row shows the original ground-truth
    labels. Different types of noisy labels are illustrated from left to right, (a)
    coarse annotation around the boundary area; (b) confused fine-grained categories,
    where the upper-cloth is mislabeled as the coat; (c) confused mirror categories,
    where the right leg is mislabeled as the left leg; (d) multiple-person occlusion.
    Annotation noises are marked in white dashed boxes.
  Figure 10 Link: articels_figures_by_rev_year\2020\SelfCorrection_for_Human_Parsing\figure_10.jpg
  Figure 10 caption: Visualization results on MHP v2.0, CIHP and VIP val sets. All
    our results are depicted on the left part of each pair, while corresponding ground-truth
    labels are shown on the right side.
  Figure 2 Link: articels_figures_by_rev_year\2020\SelfCorrection_for_Human_Parsing\figure_2.jpg
  Figure 2 caption: Overview of the SCHP pipeline. Starting from the warm-up initialization
    by training with inaccurate annotations, we design a cyclically learning scheduler
    to infer more reliable pseudo masks through iteratively aggregating the current
    learned model with the former optimal one in an online manner. Besides, those
    corrected labels can in turn to boost the model performance, simultaneously. In
    this way, the models and the masks get more robust and accurate during the self-correction
    cycles. Label noises are specially marked in white boxes. Details are introduced
    in Section 3.2.
  Figure 3 Link: articels_figures_by_rev_year\2020\SelfCorrection_for_Human_Parsing\figure_3.jpg
  Figure 3 caption: The pipeline for the multiple-person human parsing and video human
    parsing task.
  Figure 4 Link: articels_figures_by_rev_year\2020\SelfCorrection_for_Human_Parsing\figure_4.jpg
  Figure 4 caption: Visualization of SCHP results on LIP val set. The first row shows
    the original input images. The middle row shows the ground-truth labels. Different
    human categories are shown in colors in the third row.
  Figure 5 Link: articels_figures_by_rev_year\2020\SelfCorrection_for_Human_Parsing\figure_5.jpg
  Figure 5 caption: Model-agnostic study. The mIoU performance with different state-of-the-art
    models on LIP val set.
  Figure 6 Link: articels_figures_by_rev_year\2020\SelfCorrection_for_Human_Parsing\figure_6.jpg
  Figure 6 caption: Robustness of SCHP against (a) different backbones and (b) context
    encoding modules. Experiments are conducted on LIP val set.
  Figure 7 Link: articels_figures_by_rev_year\2020\SelfCorrection_for_Human_Parsing\figure_7.jpg
  Figure 7 caption: Examples from LIP train set during our self-correction process.
    Label noises like inaccurate boundary, confused fine-grained categories, confused
    mirror categories, multiple person occlusion are alleviated and resolved during
    the process. The boundaries of our corrected label are prone to be more smooth
    than the ground-truth label. Label noises are highlighted by white dotted boxes.
    Better zoom in to see the details.
  Figure 8 Link: articels_figures_by_rev_year\2020\SelfCorrection_for_Human_Parsing\figure_8.jpg
  Figure 8 caption: Performance curves w.r.t different training cycles. The mIoU,
    pixel accuracy and mean accuracy are depicted in the left, middle and right parts.
    All experiments are conducted on LIP val set.
  Figure 9 Link: articels_figures_by_rev_year\2020\SelfCorrection_for_Human_Parsing\figure_9.jpg
  Figure 9 caption: Performance w.r.t different noise ratios on GTAV dataset.
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.66
  Name of the first author: Peike Li
  Name of the last author: Yi Yang
  Number of Figures: 10
  Number of Tables: 9
  Number of authors: 4
  Paper title: Self-Correction for Human Parsing
  Publication Date: 2020-12-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparisons on the LIP Validation Set
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparisons on the PASCAL-Person-Part Test Set
  Table 3 caption:
    table_text: TABLE 3 Comparison on the ATR Test Set
  Table 4 caption:
    table_text: TABLE 4 The Effect of Our Proposed Model Aggregation (MA) and Label
      Refinement (LR) Strategy is Evaluated on LIP val Set
  Table 5 caption:
    table_text: TABLE 5 The SCHP Performance on Cityscapes & GTA5 Datasets
  Table 6 caption:
    table_text: TABLE 6 Components Analysis on val Set of CIHP
  Table 7 caption:
    table_text: TABLE 7 Comparison With State-of-the-Arts on VIP val Set
  Table 8 caption:
    table_text: TABLE 8 Comparison With State-of-the-Arts on CIHP Dataset
  Table 9 caption:
    table_text: TABLE 9 Comparison With State-of-the-Arts on MHP val Set
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3048039
- Affiliation of the first author: department of computer science, aberystwyth university,
    aberystwyth, u.k
  Affiliation of the last author: lancaster environment centre, lancaster university,
    lancaster, u.k
  Figure 1 Link: articels_figures_by_rev_year\2020\A_SemiSupervised_Deep_RuleBased_Approach_for_Complex_Satellite_Sensor_Image_Anal\figure_1.jpg
  Figure 1 caption: SeRBIA.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\A_SemiSupervised_Deep_RuleBased_Approach_for_Complex_Satellite_Sensor_Image_Anal\figure_2.jpg
  Figure 2 caption: Illustrative example of SeRBIA.
  Figure 3 Link: articels_figures_by_rev_year\2020\A_SemiSupervised_Deep_RuleBased_Approach_for_Complex_Satellite_Sensor_Image_Anal\figure_3.jpg
  Figure 3 caption: "Investigation of influence of \u03C6 , \u03B3 and W on performance\
    \ of SeRBIA."
  Figure 4 Link: articels_figures_by_rev_year\2020\A_SemiSupervised_Deep_RuleBased_Approach_for_Complex_Satellite_Sensor_Image_Anal\figure_4.jpg
  Figure 4 caption: Classification result on a subregion of satellite sensor image
    1 using SeRBIA.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xiaowei Gu
  Name of the last author: Peter M. Atkinson
  Number of Figures: 4
  Number of Tables: 2
  Number of authors: 4
  Paper title: A Semi-Supervised Deep Rule-Based Approach for Complex Satellite Sensor
    Image Analysis
  Publication Date: 2020-12-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Classification Performance Comparison on Benchmark Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Classification Performance Comparison With the State-of-the-Art
      Methods on Benchmark Datasets Under Commonly Used Experimental Protocols
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3048268
- Affiliation of the first author: "university of c\xF3rdoba, c\xF3rdoba, spain"
  Affiliation of the last author: lix, ecole polytechnique, france
  Figure 1 Link: articels_figures_by_rev_year\2020\LAEONet_Revisiting_People_Looking_at_Each_Other_in_Videos\figure_1.jpg
  Figure 1 caption: 'Intimacy or hostility? Head pose, along with body pose and facial
    expressions, is a rich source of information for interpreting human interactions.
    Being able to automatically understand the non-verbal cues provided by the relative
    head orientations of people in a scene enables a new level of human-centric video
    understanding. Green and red pairs of heads represent LAEO and non-LAEO cases,
    respectively. Video source of second row: https:youtu.beB3eFZMvNS1U.'
  Figure 10 Link: articels_figures_by_rev_year\2020\LAEONet_Revisiting_People_Looking_at_Each_Other_in_Videos\figure_10.jpg
  Figure 10 caption: 'Social network using the Average-LAEO (AL) on Friends. We depict
    the %AL between character pairs with the edges in the graph: the thicker the edge,
    the more dominant the relationship. We observe some clear patterns: Ross and Rachel
    or Monica and Julio like each other more than Chandler and Phoebe or Ross and
    Phoebe.'
  Figure 2 Link: articels_figures_by_rev_year\2020\LAEONet_Revisiting_People_Looking_at_Each_Other_in_Videos\figure_2.jpg
  Figure 2 caption: 'Our three branch track LAEO-Net++: It consists of the head branches
    (green), the head-map branch (red) and a fusion block, which concatenates the
    embeddings from the other branches and scores the track sequence as LAEO or not-LAEO
    with a fully connected layer (blue) using softmax loss. In our experiments, we
    use head tracks of length T=10 and head-maps of length M=10 .'
  Figure 3 Link: articels_figures_by_rev_year\2020\LAEONet_Revisiting_People_Looking_at_Each_Other_in_Videos\figure_3.jpg
  Figure 3 caption: (a) AB are not LAEO as C is occluding. (b) AB are LAEO.
  Figure 4 Link: articels_figures_by_rev_year\2020\LAEONet_Revisiting_People_Looking_at_Each_Other_in_Videos\figure_4.jpg
  Figure 4 caption: (top) UCO-LAEO and (bottom) AVA-LAEO datasets. Example of frames
    and LAEO head pair annotations included in our new datasets. Different scenarios,
    people clothing, background clutter and diverse video resolutions, among other
    factors, make them challenging.
  Figure 5 Link: articels_figures_by_rev_year\2020\LAEONet_Revisiting_People_Looking_at_Each_Other_in_Videos\figure_5.jpg
  Figure 5 caption: '(a) Head-maps and (b) augmentation of LAEO samples. (a) We analyse
    all head pairs with a color coding: blue for the left, green for the right and
    red for the remaining heads, such as middle, i.e. not considered for evaluation.
    (b) We generate synthetic LAEO negative training data (red boxes) from positive
    pairs (green box), based on the orientation or the relative position of the heads.'
  Figure 6 Link: articels_figures_by_rev_year\2020\LAEONet_Revisiting_People_Looking_at_Each_Other_in_Videos\figure_6.jpg
  Figure 6 caption: Head embeddings automatically learnt during LAEO training with
    full random initialization after (a) one, (b) ten, (c) twenty epochs. LAEO-Net++
    is paying attention to head orientation to solve the task, and therefore the head
    pose is learnt implicitly by the LAEO task alone (implicit supervision). The set
    of discretized angles obtained from AFLW dataset are shown in the legend. Note
    that the more the network learns the LAEO task, the more the clusters become separate
    (e.g. yellow points that correspond to [-90,45) angles). On the right, we illustrate
    head crops for the discretized set of angles after the training has finished.
    (Best viewed in digital format.).
  Figure 7 Link: articels_figures_by_rev_year\2020\LAEONet_Revisiting_People_Looking_at_Each_Other_in_Videos\figure_7.jpg
  Figure 7 caption: LAEO-Net++ results on UCO-LAEO (top) and AVA-LAEO (bottom). For
    different scenarios, backgrounds, head poses etc., in most cases LAEO-Net++ successfully
    determines if two people are LAEO (green boxes).
  Figure 8 Link: articels_figures_by_rev_year\2020\LAEONet_Revisiting_People_Looking_at_Each_Other_in_Videos\figure_8.jpg
  Figure 8 caption: LAEO-Net++ results on TVHID. (top three rows) correct LAEO results
    when the ground truth is LAEO (green) and not-LAEO (blue). LAEO-Net++ successfully
    detects people LAEO in several situations (illuminations, scales, clutter). (last
    row) failure cases for false positive LAEO detections (first example) and missed
    detections (three last examples). Most failures are missing people LAEO in ambiguous
    scenes; e.g. in the last red frame the characters are LAEO, even though the character
    on the left has closed eyes.
  Figure 9 Link: articels_figures_by_rev_year\2020\LAEONet_Revisiting_People_Looking_at_Each_Other_in_Videos\figure_9.jpg
  Figure 9 caption: 'Interaction prediction with the Average-LAEO versus various Baselines
    on Friends. In addition to the Average-LAEO score (AL), we display four baselines:
    Random Probability (PR), Uniform Probability per Episode (UPE), Shots-Coexistence-Ratio
    (SCR), and Uniform Probability per Shot (UPS). (a) AP performance for AL and various
    baselines for each pair; (more pairs in Section 4 in supplementary material, available
    online) (b) Pair-agnostic precision-recall curves. Some patterns are clear: Ross
    and Rachel or Monica and her workmate interact with each other almost continuously
    when they coexist; albeit their low frequency of co-existence, Joey and Ross,
    Joey and Monica or Ross and Mark interact significantly when they co-exist as
    captured mainly by AL (red). (c) Examples of AL and SCR. We compute the AL of
    each pair and display some examples: true positives (TP), when we correctly predict
    a pair of characters as interacting (green color); true negatives (TN), when we
    correctly predict a pair of characters as not-interacting (blue color); false
    negatives (FN), when we miss pairs that interact (orange color). Note than in
    all examples the SCR results are reversed (see SCR scores): i.e. the green rows
    are wrongly predicted as not-interacting; the blue rows are wrongly predicted
    as interacting; the orange row is correctly predicted as interacting. As expected,
    we observe that the AL fails to determine interactions, where the people are not
    LAEO. In most cases, however, either in real life or in TV-shows a human interaction
    typically involves gazing; hence, the AL is suitable for automatically capturing
    pairs of characters that interact. (Best viewed in digital format.).'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.97
  Name of the first author: "Manuel J. Mar\xEDn-Jim\xE9nez"
  Name of the last author: Vicky Kalogeiton
  Number of Figures: 10
  Number of Tables: 5
  Number of authors: 2
  Paper title: 'LAEO-Net++: Revisiting People Looking at Each Other in Videos'
  Publication Date: 2020-12-31 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of LAEO Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Head-Map and Temporal Window T T
  Table 3 caption:
    table_text: TABLE 3 Head-Map Length M M and Pre-Training Schemes
  Table 4 caption:
    table_text: TABLE 4 Comparison Between LAEO-Net and LAEO-Net++
  Table 5 caption:
    table_text: TABLE 5 LAEO Results on UCO-LAEO, AVA-LAEO and TVHID
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3048482
