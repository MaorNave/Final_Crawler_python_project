- Affiliation of the first author: nvidia, santa clara, ca, usa
  Affiliation of the last author: nvidia, santa clara, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Domain_Stylization_A_Fast_Covariance_Matching_Framework_Towards_Domain_Adaptatio\figure_1.jpg
  Figure 1 caption: The proposed iterative framework estimates the conditional class
    information with semantic segmentation and performs image translation via conditional
    covariance matching. The translated images are used to train the segmentation
    network. This framework allows both tasks to improve each other.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Domain_Stylization_A_Fast_Covariance_Matching_Framework_Towards_Domain_Adaptatio\figure_2.jpg
  Figure 2 caption: Examples of intermediate stylization outputs. First, the synthetic
    image is stylized using the real image ( x R ) without the aid of any segmentation
    map. Then we train a segmentation network ( s 0 ) using these stylized images,
    and use s 0 to segment the real images ( s 0 ( x R ) ). Given the segmentation
    map, we can then better stylize the synthetic image and so on.
  Figure 3 Link: articels_figures_by_rev_year\2020\Domain_Stylization_A_Fast_Covariance_Matching_Framework_Towards_Domain_Adaptatio\figure_3.jpg
  Figure 3 caption: Proposed framework towards joint segmentation and adaptation based
    on conditional covariance matching.
  Figure 4 Link: articels_figures_by_rev_year\2020\Domain_Stylization_A_Fast_Covariance_Matching_Framework_Towards_Domain_Adaptatio\figure_4.jpg
  Figure 4 caption: Additional examples of translation results obtained by domain
    stylization. The same source content image (left images in each row) can be translated
    into multiple stylized ones (the rest three images in each row) given the different
    sampled target images (images in top row).
  Figure 5 Link: articels_figures_by_rev_year\2020\Domain_Stylization_A_Fast_Covariance_Matching_Framework_Towards_Domain_Adaptatio\figure_5.jpg
  Figure 5 caption: "Adaptation results on SYNTHIA \u2192 Cityscapes. Rows correspond\
    \ to sampled images and predictions. Columns correspond to original images, ground\
    \ truths, and results of before adaptation (Synthetic) and after adaptation (CBST\
    \ + DS)."
  Figure 6 Link: articels_figures_by_rev_year\2020\Domain_Stylization_A_Fast_Covariance_Matching_Framework_Towards_Domain_Adaptatio\figure_6.jpg
  Figure 6 caption: 'Examples of image translation results by comparing methods. Top
    row: SYNTHIA images. Bottom row: GTA5 images. Columns from left to right correspond
    to: Original synthetic images, CycleGAN, UNIT, and domain stylization.'
  Figure 7 Link: articels_figures_by_rev_year\2020\Domain_Stylization_A_Fast_Covariance_Matching_Framework_Towards_Domain_Adaptatio\figure_7.jpg
  Figure 7 caption: "Example of image translation results by comparing methods on\
    \ SUNCG \u2192 NYU. Columns from left to right correspond to: Original synthetic\
    \ images, CycleGAN, UNIT, and domain stylization."
  Figure 8 Link: articels_figures_by_rev_year\2020\Domain_Stylization_A_Fast_Covariance_Matching_Framework_Towards_Domain_Adaptatio\figure_8.jpg
  Figure 8 caption: "Example of image translation results by comparing methods on\
    \ GTA5 \u2192 KITTI. Columns from left to right correspond to: Original synthetic\
    \ images, CycleGAN, UNIT, and domain stylization."
  Figure 9 Link: articels_figures_by_rev_year\2020\Domain_Stylization_A_Fast_Covariance_Matching_Framework_Towards_Domain_Adaptatio\figure_9.jpg
  Figure 9 caption: 'Frechet Inception distances (FIDs) between features of Cityscapes
    and different variants of SYNTHIA datasets. Left: the class-wise FIDs on DenseNet-161
    layer-transition2. Right: mean FIDs obtained by different feature extractors.'
  First author gender probability: 0.97
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.91
  Name of the first author: Aysegul Dundar
  Name of the last author: Jan Kautz
  Number of Figures: 9
  Number of Tables: 6
  Number of authors: 6
  Paper title: 'Domain Stylization: A Fast Covariance Matching Framework Towards Domain
    Adaptation'
  Publication Date: 2020-01-24 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Quantitative Results on SYNTHIA \u2192 \u2192Cityscapes"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Quantitative Results on GTA5 \u2192 \u2192Cityscapes"
  Table 3 caption:
    table_text: "TABLE 3 Quantitative Results on SUNCG \u2192 \u2192NYU"
  Table 4 caption:
    table_text: "TABLE 4 Ablation Study on SYNTHIA \u2192 \u2192Cityscapes"
  Table 5 caption:
    table_text: "TABLE 5 Ablation Study 4 on SYNTHIA \u2192 \u2192Cityscapes"
  Table 6 caption:
    table_text: "TABLE 6 Mean AP (mAP) Scores on GTA5 \u2192 \u2192KITTI Under Three\
      \ Protocols: Easy, Moderate (Mdrt), Hard"
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2969421
- Affiliation of the first author: school of computer science and engineering, nanjing
    university of science and technology, nanjing, china
  Affiliation of the last author: school of engineering, university of california,
    merced, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\PhysicsBased_Generative_Adversarial_Models_for_Image_Restoration_and_Beyond\figure_1.jpg
  Figure 1 caption: Two image restoration problems. Our method is motivated by a key
    observation that the restored results should be consistent with the observed inputs
    under the degradation process (i.e., physics model). Enforcing this fundamental
    constraint in the generative adversarial network (GAN) can generate better results
    than those without using the constraint (e.g., (b)). We show that the proposed
    method can be applied to several image restoration and low-level vision problems
    and performs favorably against state-of-the-art algorithms.
  Figure 10 Link: articels_figures_by_rev_year\2020\PhysicsBased_Generative_Adversarial_Models_for_Image_Restoration_and_Beyond\figure_10.jpg
  Figure 10 caption: Results on a real rainy image. Due to the heavy rain, both the
    CNN-based [41], [67] and GAN-based [1], [2], [22] methods are not able to generate
    clear images. The part enclosed in the red box in (c) contains significant color
    distortions (best viewed on high-resolution displays with zoom-in).
  Figure 2 Link: articels_figures_by_rev_year\2020\PhysicsBased_Generative_Adversarial_Models_for_Image_Restoration_and_Beyond\figure_2.jpg
  Figure 2 caption: "Proposed algorithm. The discriminative network D g is used to\
    \ classify whether the distributions of the outputs from the generator G are close\
    \ to those of the ground truth images or not. The discriminative network D h is\
    \ used to classify whether the regenerated result y \u02DC i is consistent with\
    \ the observed image y i or not. All the networks are jointly trained in an end-to-end\
    \ manner."
  Figure 3 Link: articels_figures_by_rev_year\2020\PhysicsBased_Generative_Adversarial_Models_for_Image_Restoration_and_Beyond\figure_3.jpg
  Figure 3 caption: One synthetic blurred image from the text image deblurring dataset
    [32]. The proposed method generates images with much clearer characters.
  Figure 4 Link: articels_figures_by_rev_year\2020\PhysicsBased_Generative_Adversarial_Models_for_Image_Restoration_and_Beyond\figure_4.jpg
  Figure 4 caption: Face image deblurring results. The proposed method generates images
    with fewer artifacts.
  Figure 5 Link: articels_figures_by_rev_year\2020\PhysicsBased_Generative_Adversarial_Models_for_Image_Restoration_and_Beyond\figure_5.jpg
  Figure 5 caption: Natural image deblurring results. The proposed method generates
    images with fewer artifacts.
  Figure 6 Link: articels_figures_by_rev_year\2020\PhysicsBased_Generative_Adversarial_Models_for_Image_Restoration_and_Beyond\figure_6.jpg
  Figure 6 caption: Real text image deblurring results. The proposed method generates
    images with much clearer characters.
  Figure 7 Link: articels_figures_by_rev_year\2020\PhysicsBased_Generative_Adversarial_Models_for_Image_Restoration_and_Beyond\figure_7.jpg
  Figure 7 caption: One synthetic hazy image from the proposed synthetic hazy dataset.
    The structures enclosed in the red box in (f) are not preserved well. The proposed
    method does not need to estimate the transmission map and atmospheric light and
    generates a much clearer image which is visually close to the ground truth image
    (best viewed on high-resolution displays with zoom-in).
  Figure 8 Link: articels_figures_by_rev_year\2020\PhysicsBased_Generative_Adversarial_Models_for_Image_Restoration_and_Beyond\figure_8.jpg
  Figure 8 caption: A hazy image from the dataset [63]. The proposed method generates
    a much clearer image which is visually close to the ground truth.
  Figure 9 Link: articels_figures_by_rev_year\2020\PhysicsBased_Generative_Adversarial_Models_for_Image_Restoration_and_Beyond\figure_9.jpg
  Figure 9 caption: Results on a real hazy image. The proposed method generates a
    clearer image.
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jinshan Pan
  Name of the last author: Ming-Hsuan Yang
  Number of Figures: 13
  Number of Tables: 9
  Number of authors: 8
  Paper title: Physics-Based Generative Adversarial Models for Image Restoration and
    Beyond
  Publication Date: 2020-01-24 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Network Configurations
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Evaluations With the State-of-the-Art Methods
      on the Text Image Deblurring Dataset by [32]
  Table 3 caption:
    table_text: TABLE 3 Quantitative Evaluations With the State-of-the-Art Deblurring
      Methods on Face Images
  Table 4 caption:
    table_text: TABLE 4 Quantitative Evaluations With the State-of-the-Art Deblurring
      Methods on Natural Images
  Table 5 caption:
    table_text: TABLE 5 Quantitative Evaluations With the State-of-the-Art Methods
      on the Proposed Image Dehazing Dataset
  Table 6 caption:
    table_text: TABLE 6 Quantitative Evaluations With the State-of-the-Art Methods
      on the Outdoor Image Dehazing Dataset [63]
  Table 7 caption:
    table_text: "TABLE 7 Quantitative Evaluations With the State-of-the-Art Methods\
      \ on the Image Super-Resolution ( \xD74 \xD74) Problem"
  Table 8 caption:
    table_text: TABLE 8 Effectiveness of the Proposed Algorithm and Loss Functions
      on the Image Dehazing Task
  Table 9 caption:
    table_text: "TABLE 9 Sensitivity Analysis With Respect to the Weight Parameter\
      \ \u03BB \u03BB and the Number of ResBlocks"
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2969348
- Affiliation of the first author: department of electrical engineering, columbia
    university, new york, ny, usa
  Affiliation of the last author: department of electrical engineering, columbia university,
    new york, ny, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\RealTime_Nonparametric_Anomaly_Detection_in_HighDimensional_Settings\figure_1.jpg
  Figure 1 caption: Diagram of the proposed detection schemes.
  Figure 10 Link: articels_figures_by_rev_year\2020\RealTime_Nonparametric_Anomaly_Detection_in_HighDimensional_Settings\figure_10.jpg
  Figure 10 caption: Sample path of the decision statistic where the FDI attack is
    launched at tau =200 .
  Figure 2 Link: articels_figures_by_rev_year\2020\RealTime_Nonparametric_Anomaly_Detection_in_HighDimensional_Settings\figure_2.jpg
  Figure 2 caption: "The lower bound (dashed curve) on the decision threshold h of\
    \ the proposed algorithm for \u03B1<1e such that E \u221E [\u0393]\u2265 10 6\
    \ as N 2 \u2192\u221E ."
  Figure 3 Link: articels_figures_by_rev_year\2020\RealTime_Nonparametric_Anomaly_Detection_in_HighDimensional_Settings\figure_3.jpg
  Figure 3 caption: "Average false alarm period versus the presented lower bound in\
    \ the asymptotic regime where N 2 \u2192\u221E ."
  Figure 4 Link: articels_figures_by_rev_year\2020\RealTime_Nonparametric_Anomaly_Detection_in_HighDimensional_Settings\figure_4.jpg
  Figure 4 caption: "g(\u03B1) versus \u03B1 ."
  Figure 5 Link: articels_figures_by_rev_year\2020\RealTime_Nonparametric_Anomaly_Detection_in_HighDimensional_Settings\figure_5.jpg
  Figure 5 caption: "Average false alarm period, the Walds approximation, and the\
    \ presented lower bound for various \u03B1 and h levels."
  Figure 6 Link: articels_figures_by_rev_year\2020\RealTime_Nonparametric_Anomaly_Detection_in_HighDimensional_Settings\figure_6.jpg
  Figure 6 caption: GEM-based nominal summary statistics for the IEEE-57 bus power
    system.
  Figure 7 Link: articels_figures_by_rev_year\2020\RealTime_Nonparametric_Anomaly_Detection_in_HighDimensional_Settings\figure_7.jpg
  Figure 7 caption: Average false alarm period, approximations, and the lower bound
    for various test thresholds for the smart grid data.
  Figure 8 Link: articels_figures_by_rev_year\2020\RealTime_Nonparametric_Anomaly_Detection_in_HighDimensional_Settings\figure_8.jpg
  Figure 8 caption: Average detection delay versus average false alarm period in detection
    of an FDI attack against the smart grid.
  Figure 9 Link: articels_figures_by_rev_year\2020\RealTime_Nonparametric_Anomaly_Detection_in_HighDimensional_Settings\figure_9.jpg
  Figure 9 caption: ROC curve in detection of an FDI attack against the smart grid.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Mehmet Necip Kurt
  Name of the last author: Xiaodong Wang
  Number of Figures: 20
  Number of Tables: 2
  Number of authors: 3
  Paper title: Real-Time Nonparametric Anomaly Detection in High-Dimensional Settings
  Publication Date: 2020-01-30 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 g(\u03B1) g(\u03B1) Computed Over a Monte Carlo Simulation\
      \ for Some \u03B1 \u03B1 Levels"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Space and Time Complexity of the Proposed Algorithms
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2970410
- Affiliation of the first author: nvidia corporation, helsinki, finland
  Affiliation of the last author: nvidia corporation, helsinki, finland
  Figure 1 Link: articels_figures_by_rev_year\2020\A_StyleBased_Generator_Architecture_for_Generative_Adversarial_Networks\figure_1.jpg
  Figure 1 caption: "While a traditional generator [2] feeds the latent code though\
    \ the input layer only, we first map the input to an intermediate latent space\
    \ W , which then controls the generator through adaptive instance normalization\
    \ (AdaIN) at each convolution layer. Gaussian noise is added after each convolution,\
    \ before evaluating the nonlinearity. Here \u201CA\u201D stands for a learned\
    \ affine transform, and \u201CB\u201D applies learned per-channel scaling factors\
    \ to the noise input. The mapping network f consists of 8 layers and the synthesis\
    \ network g consists of 18 layers\u2014two for each resolution ( 4 2 ,\u2026,\
    \ 1024 2 ). The output of the last layer is converted to RGB using a separate\
    \ 1\xD71 convolution, similar to Karras et al. [2]. Our generator has a total\
    \ of 26.2M trainable parameters, compared to 23.1M in the traditional generator."
  Figure 10 Link: articels_figures_by_rev_year\2020\A_StyleBased_Generator_Architecture_for_Generative_Adversarial_Networks\figure_10.jpg
  Figure 10 caption: The FFHQ dataset offers a lot of variety in terms of age, ethnicity,
    viewpoint, lighting, and image background.
  Figure 2 Link: articels_figures_by_rev_year\2020\A_StyleBased_Generator_Architecture_for_Generative_Adversarial_Networks\figure_2.jpg
  Figure 2 caption: "Uncurated set of images produced by our style-based generator\
    \ (config f) with the FFHQ dataset. Here we used a variation of the truncation\
    \ trick [4], [23], [24] with \u03C8=0.7 for resolutions 4 2 ,\u2026, 32 2 ."
  Figure 3 Link: articels_figures_by_rev_year\2020\A_StyleBased_Generator_Architecture_for_Generative_Adversarial_Networks\figure_3.jpg
  Figure 3 caption: "Two sets of images were generated from their respective latent\
    \ codes (sources A and B); the rest of the images were generated by copying a\
    \ specified subset of styles from source B and taking the rest from source A.\
    \ Copying the styles corresponding to coarse spatial resolutions ( 4 2 , 8 2 )\
    \ brings high-level aspects such as pose, general hair style, face shape, and\
    \ eyeglasses from source B, while all colors (eyes, hair, lighting) and finer\
    \ facial features resemble A. If we instead copy the styles of middle resolutions\
    \ ( 16 2 , 32 2 ) from B, we inherit smaller scale facial features, hair style,\
    \ eyes openclosed from B, while the pose, general face shape, and eyeglasses from\
    \ A are preserved. Finally, copying the fine styles ( 64 2 ,\u2026, 1024 2 ) from\
    \ B brings mainly the color scheme and microstructure."
  Figure 4 Link: articels_figures_by_rev_year\2020\A_StyleBased_Generator_Architecture_for_Generative_Adversarial_Networks\figure_4.jpg
  Figure 4 caption: Examples of stochastic variation. (a) Two generated images. (b)
    Zoom-in with different realizations of input noise. While the overall appearance
    is almost identical, individual hairs are placed very differently. (c) Standard
    deviation of each pixel over 100 different realizations, highlighting which parts
    of the images are affected by the noise. The main areas are the hair, silhouettes,
    and parts of background, but there is also interesting stochastic variation in
    the eye reflections. Global aspects, such as identity and pose, are unaffected
    by stochastic variation.
  Figure 5 Link: articels_figures_by_rev_year\2020\A_StyleBased_Generator_Architecture_for_Generative_Adversarial_Networks\figure_5.jpg
  Figure 5 caption: "Effect of noise inputs at different layers of our generator.\
    \ (a) Noise is applied to all layers. (b) No noise. (c) Noise in fine layers only\
    \ ( 64 2 ,\u2026, 1024 2 ). (d) Noise in coarse layers only ( 4 2 ,\u2026, 32\
    \ 2 ). We can see that the artificial omission of noise leads to featureless \u201C\
    painterly\u201D look. Coarse noise causes large-scale curling of hair and appearance\
    \ of larger background features, while the fine noise brings out the finer curls\
    \ of hair, finer background detail, and skin pores."
  Figure 6 Link: articels_figures_by_rev_year\2020\A_StyleBased_Generator_Architecture_for_Generative_Adversarial_Networks\figure_6.jpg
  Figure 6 caption: "Illustrative example with two factors of variation (image features,\
    \ e.g., masculinity and hair length). (a) An example training set where some combination\
    \ (e.g., long haired males) is missing. (b) This forces the mapping from Z to\
    \ image features to become curved so that the forbidden combination disappears\
    \ in Z to prevent the sampling of invalid combinations. (c) The learned mapping\
    \ from Z to W is able to \u201Cundo\u201D much of the warping."
  Figure 7 Link: articels_figures_by_rev_year\2020\A_StyleBased_Generator_Architecture_for_Generative_Adversarial_Networks\figure_7.jpg
  Figure 7 caption: FID and perceptual path length metrics over the course of training
    in our configurations b and f using the FFHQ dataset. Horizontal axis denotes
    the number of training images seen by the discriminator. The dashed vertical line
    at 8.4M images marks the point when training has progressed to full 1024 2 resolution.
    On the right, we show only one curve for the traditional generators path length
    measurements, because there is no effective difference between full-path and endpoint
    sampling in Z .
  Figure 8 Link: articels_figures_by_rev_year\2020\A_StyleBased_Generator_Architecture_for_Generative_Adversarial_Networks\figure_8.jpg
  Figure 8 caption: Uncurated result images produced by our style-based generator
    in LSUN Bedroom, Car, Cat, and Churchoutdoor datasets.
  Figure 9 Link: articels_figures_by_rev_year\2020\A_StyleBased_Generator_Architecture_for_Generative_Adversarial_Networks\figure_9.jpg
  Figure 9 caption: Uncurated result images produced by our style-based generator
    in LSUN Conferenceroom and Horse datasets.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Tero Karras
  Name of the last author: Timo Aila
  Number of Figures: 11
  Number of Tables: 5
  Number of authors: 3
  Paper title: A Style-Based Generator Architecture for Generative Adversarial Networks
  Publication Date: 2020-01-31 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Fr\xE9chet Inception Distances (FID) for Various Generator\
      \ Designs (Lower is Better)"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 FID With Various Amounts of Mixing Regularization
  Table 3 caption:
    table_text: TABLE 3 Perceptual Path Lengths and Separability Scores (Lower is
      Better)
  Table 4 caption:
    table_text: TABLE 4 Effects of the Mapping Network
  Table 5 caption:
    table_text: TABLE 5 FID for Style-Based versus Traditional Generator in LSUN [59]
      Datasets
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2970919
- Affiliation of the first author: institute of science and engineering, kanazawa
    university, kakuma, kanazawa, ishikawa, japan
  Affiliation of the last author: institute of science and engineering, kanazawa university,
    kakuma, kanazawa, ishikawa, japan
  Figure 1 Link: articels_figures_by_rev_year\2020\A_Bayesian_Formulation_of_Coherent_Point_Drift\figure_1.jpg
  Figure 1 caption: "Notation with respect to corresponding points. The point sets\
    \ colored red and blue represent the source point set Y and the target point set\
    \ X , respectively. The variable c n \u22080,1 indicates whether or not x n is\
    \ a non-outlier, whereas the variable e n \u22081,\u2026,M specifies the index\
    \ of a point in Y that corresponds to x n . Target points x 1 , x 2 , and x 3\
    \ represent the point that corresponds to y m , a non-outlier that does not correspond\
    \ to y m , and an outlier, respectively."
  Figure 10 Link: articels_figures_by_rev_year\2020\A_Bayesian_Formulation_of_Coherent_Point_Drift\figure_10.jpg
  Figure 10 caption: "Registration error versus the numbers of Nystr\xF6m points,\
    \ J and K , used for approximating P and G , respectively. The registration error\
    \ was measured by the root-mean-squared distance (RMSD) using the bunny dataset.\
    \ The symbol sigma represents the square root of the residual variance after the\
    \ optimization, i.e., an estimate of registration error. The second figure is\
    \ an enlarged view of the first figure, and it additionally includes RMSDs computed\
    \ by the BCPD that exactly calculated P and the accelerated CPD."
  Figure 2 Link: articels_figures_by_rev_year\2020\A_Bayesian_Formulation_of_Coherent_Point_Drift\figure_2.jpg
  Figure 2 caption: "BCPD algorithm. The tilde symbol attached to a matrix denotes\
    \ the Kronecker product between the matrix and I D , e.g., \u03A3 ~ =\u03A3\u2297\
    \ I D and P ~ =P\u2297 I D , whereas the tilde symbol attached to a vector denotes\
    \ the Kronecker product between the vector and 1 D , e.g., \u03BD ~ =\u03BD\u2297\
    \ 1 D and \u03BD ~ \u2032 = \u03BD \u2032 \u2297 1 D . The symbol \u03C8 represents\
    \ the digamma function. The m th diagonal element of \u03A3 is denoted by \u03C3\
    \ 2 m . The singular value decomposition is denoted by \u201Csvd.\u201D The determinant\
    \ and trace of a square matrix are denoted by |\u22C5| and Tr(\u22C5) , respectively.\
    \ Unlike CPD, BCPD simultaneously estimates the variables of non-rigid and similarity\
    \ transformations. The acceleration of computations G , \u03A3 , and P is discussed\
    \ in Section 4.6."
  Figure 3 Link: articels_figures_by_rev_year\2020\A_Bayesian_Formulation_of_Coherent_Point_Drift\figure_3.jpg
  Figure 3 caption: "Interpretation of tuning parameters \u03B2 and \u03BB when the\
    \ Gram matrix G is Gaussian. (a) \u03B2 controls the directional correlation between\
    \ displacement vectors. (b) \u03BB controls the expected length of displacement\
    \ vectors."
  Figure 4 Link: articels_figures_by_rev_year\2020\A_Bayesian_Formulation_of_Coherent_Point_Drift\figure_4.jpg
  Figure 4 caption: 'Datasets used for numerical evaluations: bunny, monkey, dragon,
    and armadillo. Each point set colored red was constructed by deforming the point
    set colored blue in a non-linear manner.'
  Figure 5 Link: articels_figures_by_rev_year\2020\A_Bayesian_Formulation_of_Coherent_Point_Drift\figure_5.jpg
  Figure 5 caption: Optimization trajectories for bunny data with rotation and clustered
    outliers. The points in the target and deformed shapes are colored blue and red,
    respectively. The leftmost column shows the initial point sets, and the optimization
    proceeds from left to right.
  Figure 6 Link: articels_figures_by_rev_year\2020\A_Bayesian_Formulation_of_Coherent_Point_Drift\figure_6.jpg
  Figure 6 caption: Optimization trajectories for the armadillo data with rotation
    using the inverse multiquadric kernel. Even if the kernel is non-Gaussian, BCPD
    is scalable to point sets containing more than 100,000 points.
  Figure 7 Link: articels_figures_by_rev_year\2020\A_Bayesian_Formulation_of_Coherent_Point_Drift\figure_7.jpg
  Figure 7 caption: "Evaluation of robustness against the rotations of target point\
    \ sets: bunny (left), monkey (middle), and dragon (right). The non-rigid CPD with\
    \ or without the pre-alignment by the rigid CPD is denoted by \u201CCPD (r+n)\u201D\
    \ or \u201CCPD (n),\u201D respectively."
  Figure 8 Link: articels_figures_by_rev_year\2020\A_Bayesian_Formulation_of_Coherent_Point_Drift\figure_8.jpg
  Figure 8 caption: 'Evaluation of robustness against outliers: bunny (left), monkey
    (middle), and dragon (right). The figures in the bottom are enlarged views of
    the top figures. The registration error of a method for each outliernon-outlier
    ratio was measured by the median of RMSDs among 100 trials.'
  Figure 9 Link: articels_figures_by_rev_year\2020\A_Bayesian_Formulation_of_Coherent_Point_Drift\figure_9.jpg
  Figure 9 caption: 'Evaluation of robustness against clustered outliers: bunny (left),
    monkey (middle), and dragon (right). The figures in the bottom are enlarged views
    of the top figures. The registration error of a method for each outliernon-outlier
    ratio was measured by the median of RMSDs among 100 trials.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Osamu Hirose
  Name of the last author: Osamu Hirose
  Number of Figures: 15
  Number of Tables: 2
  Number of authors: 1
  Paper title: A Bayesian Formulation of Coherent Point Drift
  Publication Date: 2020-02-06 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Computational Costs for the Evaluation of the Bottleneck\
      \ Terms Involving G G, \u03A3 \u03A3, and P P"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Kernel Functions Used for the Evaluation of Computing Times
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2971687
- Affiliation of the first author: department of computer science and engineering,
    university of california, santa cruz, ca, usa
  Affiliation of the last author: microsoft research, redmond, wa, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\VisionLanguage_Navigation_Policy_Learning_and_Adaptation\figure_1.jpg
  Figure 1 caption: Demonstration of the VLN task. The instruction, the local visual
    scene, and the global trajectories in a top-down view is shown. The agent does
    not have access to the top-down view. Path A is the demonstration path following
    the instruction. Path B and C are two different paths executed by the agent.
  Figure 10 Link: articels_figures_by_rev_year\2020\VisionLanguage_Navigation_Policy_Learning_and_Adaptation\figure_10.jpg
  Figure 10 caption: Failure of executing a relatively complicated instruction.
  Figure 2 Link: articels_figures_by_rev_year\2020\VisionLanguage_Navigation_Policy_Learning_and_Adaptation\figure_2.jpg
  Figure 2 caption: Overview of our RCM framework. Please refer to Section 3.1 for
    explanation.
  Figure 3 Link: articels_figures_by_rev_year\2020\VisionLanguage_Navigation_Policy_Learning_and_Adaptation\figure_3.jpg
  Figure 3 caption: Cross-modal reasoning navigator at step t . A language encoder
    encodes the language instruction into a sequence of textual features w i n i=1
    . At each time step t of the decoding stage, the observed panoramic view is split
    and processed into a set of visual features of m image patches, represented as
    v t,j m j=1 . Then a cross-modal attention module is employed to learn the history
    context, textual context, and visual context, which are eventually fed into an
    action predictor for decision making.
  Figure 4 Link: articels_figures_by_rev_year\2020\VisionLanguage_Navigation_Policy_Learning_and_Adaptation\figure_4.jpg
  Figure 4 caption: Cross-modal matching critic that provides the cycle-reconstruction
    intrinsic reward. The matching critic takes visual trajectories as input and outputs
    the conditional probabilities of producing next word given the ground-truth words
    in the instruction. We pretrain the matching critic on the trajactory-instruction
    pairs in the training set and then keep it fixed to provide the intrinsic reward
    during navigation policy training.
  Figure 5 Link: articels_figures_by_rev_year\2020\VisionLanguage_Navigation_Policy_Learning_and_Adaptation\figure_5.jpg
  Figure 5 caption: SIL for exploration on unlabeled data. Given an instruction in
    the unseen environment, the navigator performs a set of trajectories, which are
    then evaluated by our matching critic. The best trajectory with the highest intrinsic
    reward will be saved into the replay buffer for the navigator to imitate. Eventually,
    the navigator approximates a better policy that adapts to the unseen environment.
  Figure 6 Link: articels_figures_by_rev_year\2020\VisionLanguage_Navigation_Policy_Learning_and_Adaptation\figure_6.jpg
  Figure 6 caption: Visualization of the intrinsic reward on seen and unseen validation
    sets. Higher intrinsic rewards lead to better chances to reach the destination.
  Figure 7 Link: articels_figures_by_rev_year\2020\VisionLanguage_Navigation_Policy_Learning_and_Adaptation\figure_7.jpg
  Figure 7 caption: Qualitative examples from the unseen validation set.
  Figure 8 Link: articels_figures_by_rev_year\2020\VisionLanguage_Navigation_Policy_Learning_and_Adaptation\figure_8.jpg
  Figure 8 caption: Misunderstanding of the instruction.
  Figure 9 Link: articels_figures_by_rev_year\2020\VisionLanguage_Navigation_Policy_Learning_and_Adaptation\figure_9.jpg
  Figure 9 caption: Ground errors where objects were not recognized from the visual
    scene.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.55
  Name of the first author: Xin Wang
  Name of the last author: Lei Zhang
  Number of Figures: 10
  Number of Tables: 3
  Number of authors: 8
  Paper title: Vision-Language Navigation Policy Learning and Adaptation
  Publication Date: 2020-02-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison on the R2R Test Set [4]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Ablation Study on Seen and Unseen Validation Sets as Reported
      in [55]
  Table 3 caption:
    table_text: TABLE 3 Updated Results on Seen and Unseen Validation Sets with ML
      + RL Objective [56] Instead of MIXER [57]
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2972281
- Affiliation of the first author: school of computer science, wuhan university, wuhan,
    china
  Affiliation of the last author: school of automation, nanjing university of posts
    and telecommunications, nanjing, china
  Figure 1 Link: articels_figures_by_rev_year\2020\SemiSupervised_MultiView_Deep_Discriminant_Representation_Learning\figure_1.jpg
  Figure 1 caption: Taxonomy of multi-view representation learning literatures. (a)
    Joint representation fuses multiple views through concatenation. (b) Alignment
    representation aligns features by utilizing consensus property. (c) Shared and
    specific representation employs multiple feature extractors for each view and
    inherits the advantages of (a) and (b).
  Figure 10 Link: articels_figures_by_rev_year\2020\SemiSupervised_MultiView_Deep_Discriminant_Representation_Learning\figure_10.jpg
  Figure 10 caption: Explanation of our improvement in Siamese Network from the perspective
    of margin theory. (a) Improved loss pulls samples from the same class to their
    class center, while traditional contrastive loss randomly pulls two samples from
    the same class together. (b) Margin distribution comparison between our improved
    loss and contrastive loss.
  Figure 2 Link: articels_figures_by_rev_year\2020\SemiSupervised_MultiView_Deep_Discriminant_Representation_Learning\figure_2.jpg
  Figure 2 caption: 'Overall architecture and alternative optimization of SMDDRL.
    SMDDRL contains two parts: Multi-view Deep Discriminant Representation Learning
    (MDDRL) network as well as deep metric learning and density clustering based semi-supervised
    learning framework. Building on the representation learning and classification
    backbone, MDDRL adds two components: a) deep metric learning for better discriminability;
    b) orthogonality and adversarial similarity constraints for disentangling shared
    and specific features. Semi-supervised learning framework contains deep metric
    learning (in MDDRL) and density peak clustering, and they are trained alternatively
    (Step1-Step 4).'
  Figure 3 Link: articels_figures_by_rev_year\2020\SemiSupervised_MultiView_Deep_Discriminant_Representation_Learning\figure_3.jpg
  Figure 3 caption: (a) Architecture of MDDRL. (b) Details of multi-view deep representation
    learning with orthogonality and adversarial similarity constraints. (c) Details
    of adversarial similarity constraint and loss.
  Figure 4 Link: articels_figures_by_rev_year\2020\SemiSupervised_MultiView_Deep_Discriminant_Representation_Learning\figure_4.jpg
  Figure 4 caption: Representations learned with and without deep metric learning.
    Coordinates are collected from 525 WebKB samples. (a) is the result with Siamese
    network ( Margin=3.0 ), (b) is the result without it. The results demonstrate
    that deep metric learning makes the decision boundaries clearer.
  Figure 5 Link: articels_figures_by_rev_year\2020\SemiSupervised_MultiView_Deep_Discriminant_Representation_Learning\figure_5.jpg
  Figure 5 caption: Classification accuracy and F1-score on WebKB (a, c) and AD (b,
    d) datasets.
  Figure 6 Link: articels_figures_by_rev_year\2020\SemiSupervised_MultiView_Deep_Discriminant_Representation_Learning\figure_6.jpg
  Figure 6 caption: Image classification results.
  Figure 7 Link: articels_figures_by_rev_year\2020\SemiSupervised_MultiView_Deep_Discriminant_Representation_Learning\figure_7.jpg
  Figure 7 caption: Classification results on BBC dataset.
  Figure 8 Link: articels_figures_by_rev_year\2020\SemiSupervised_MultiView_Deep_Discriminant_Representation_Learning\figure_8.jpg
  Figure 8 caption: Ablation study on WebKB, Noisy MNIST, and BBC.
  Figure 9 Link: articels_figures_by_rev_year\2020\SemiSupervised_MultiView_Deep_Discriminant_Representation_Learning\figure_9.jpg
  Figure 9 caption: Comparison of the representations and losses obtained from the
    original contrastive loss (a, c) and the improved contrastive loss (b, d) on WebKB
    and Noisy MNIST. The mathrm Margin is set as 3.0 for WebKB, and 50.0 for Noisy
    MNIST.
  First author gender probability: 0.94
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Xiaodong Jia
  Name of the last author: Dong Yue
  Number of Figures: 12
  Number of Tables: 6
  Number of authors: 8
  Paper title: Semi-Supervised Multi-View Deep Discriminant Representation Learning
  Publication Date: 2020-02-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Multi-View Representation Learning Methods
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Classification Accuracy on Reuters Datasets
  Table 3 caption:
    table_text: TABLE 3 F1-Score on Reuters Datasets
  Table 4 caption:
    table_text: TABLE 4 Effectiveness of Different Alignment Strategies
  Table 5 caption:
    table_text: TABLE 5 Effectiveness of Different SSL Strategies
  Table 6 caption:
    table_text: TABLE 6 Comparison of Training Time (Seconds)
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2973634
- Affiliation of the first author: school of electrical engineering and computer science,
    washington state university, pullman, wa, usa
  Affiliation of the last author: school of electrical engineering and computer science,
    washington state university, pullman, wa, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\sMRT_MultiResident_Tracking_in_Smart_Homes_With_Sensor_Vectorization\figure_1.jpg
  Figure 1 caption: Floor plan and the locations of sensors deployed in CASAS smart
    homes. (a) Smart home TM004 with 25 motion sensors. (b) Smart home site Kyoto
    with 65 motion sensors, 15 door sensors, and 11 item sensors.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\sMRT_MultiResident_Tracking_in_Smart_Homes_With_Sensor_Vectorization\figure_2.jpg
  Figure 2 caption: A screen shot of the ActViz annotation software used in this research
    to generate the ground truth of the association between sensor events and the
    residents in the smart home.
  Figure 3 Link: articels_figures_by_rev_year\2020\sMRT_MultiResident_Tracking_in_Smart_Homes_With_Sensor_Vectorization\figure_3.jpg
  Figure 3 caption: Multi-resident tracking graph showing the association between
    residents and sensor events and the relationship between sensor messages, sensor
    events and sensor observations. The figure is generated using the sensor messages
    recorded in the Kyoto dataset from the same time period as Tables 1, 2, and 3.
    The arrows in the graph show the movement of all the active residents with respect
    to sensor observations.
  Figure 4 Link: articels_figures_by_rev_year\2020\sMRT_MultiResident_Tracking_in_Smart_Homes_With_Sensor_Vectorization\figure_4.jpg
  Figure 4 caption: The generative model of sensor vectorization.
  Figure 5 Link: articels_figures_by_rev_year\2020\sMRT_MultiResident_Tracking_in_Smart_Homes_With_Sensor_Vectorization\figure_5.jpg
  Figure 5 caption: The sMRT tracking phase.
  Figure 6 Link: articels_figures_by_rev_year\2020\sMRT_MultiResident_Tracking_in_Smart_Homes_With_Sensor_Vectorization\figure_6.jpg
  Figure 6 caption: Accuracy score as a function of number of active residents for
    sMRT, NN-sg, and GNN-sg using the TM004 dataset.
  Figure 7 Link: articels_figures_by_rev_year\2020\sMRT_MultiResident_Tracking_in_Smart_Homes_With_Sensor_Vectorization\figure_7.jpg
  Figure 7 caption: Hamming loss as a function of number of active residents for sMRT,
    NN-sg, and GNN-sg using the TM004 dataset.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Tinghui Wang
  Name of the last author: Diane J. Cook
  Number of Figures: 7
  Number of Tables: 7
  Number of authors: 2
  Paper title: 'sMRT: Multi-Resident Tracking in Smart Homes With Sensor Vectorization'
  Publication Date: 2020-02-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 An Example of Sensor Messages Recorded in the Kyoto Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Sensor Sequence Extracted From Sensor Messages Shown in Table
      1
  Table 3 caption:
    table_text: TABLE 3 Sensor Observations, Recorded Each Time a Sensor is Activated
  Table 4 caption:
    table_text: TABLE 4 Multi-Label Accuracy and Hamming Loss of sMRT, NN-sg, and
      GNN-sg Measured Using the TM004 and Kyoto Datasets
  Table 5 caption:
    table_text: TABLE 5 Performance of sMRT, NN-sg and GNN-sg Using the TM004 Dataset,
      Measured Based on Binary Classification Accuracy on a Per-Resident Basis
  Table 6 caption:
    table_text: TABLE 6 Performance of sMRT, NN-sg and GNN-sg Using the Kyoto Dataset,
      Measured Based on Binary Classification Accuracy on a Per-Resident Basis
  Table 7 caption:
    table_text: TABLE 7 Average Error of sMRT, NN-sg, and GNN-sg in Estimation of
      the Number of Active Residents in the Smart Homes
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2973571
- Affiliation of the first author: department of computer science, university of hong
    kong, hong kong
  Affiliation of the last author: department of computer science, university of hong
    kong, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2020\RelationshipEmbedded_Representation_Learning_for_Grounding_Referring_Expressions\figure_1.jpg
  Figure 1 caption: Cross-modal relationship inference network. Given an expression
    and image, Cross-Modal Relationship Extractor constructs the language-guided visual
    relation graphs (spatial relation graph as an example, the attention scores of
    proposals and edges types are visualized inside green dashed box). The gated graph
    convolutional network capture semantic context and computes the matching scores
    between context of proposals and context of expression (the matching scores of
    proposals are shown inside blue dashed box). Warmer color indicates higher scores
    of pixels and darker blue indicates higher scores of edges types.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\RelationshipEmbedded_Representation_Learning_for_Grounding_Referring_Expressions\figure_2.jpg
  Figure 2 caption: "An overview of our Cross-Modal Relationship Inference Network\
    \ for grounding referring expressions (better view in color). We use color to\
    \ represent semantics, i.e., yellow denotes \u201Cperson\u201D, green denotes\
    \ \u201Cgreen shirt\u201D, blue denotes \u201Cumbrella\u201D, purple means \u201C\
    white T-shirt\u201D, brown means \u201Cwearing\u201D and dark grey refers to \u201C\
    held by\u201D. It includes a Cross-Modal Relationship Extractor (CMRE) and a Gated\
    \ Graph Convolutional Network (GGCN). First, CMRE constructs (a) a spatial relation\
    \ graph from the visual features of object proposals and spatial relationships\
    \ between proposals. Second, CMRE parses the expression into a constituency tree\
    \ and extracts the valid noun phrases. Third, CMRE highlights the vertices (red\
    \ bounding boxes) and edges (solid lines) to generate (b) a language-guided visual\
    \ relation graph using cross-modal attention between wordsphrases in the referring\
    \ expression and the spatial relation graphs vertices and edges. Fourth, GGCN\
    \ fuses the context of words into the language-guided visual relation graph to\
    \ obtain (c) a multimodal (language, visual and spatial information) relation\
    \ graph. Fifth, GGCN captures (d) the multimodal semantic context with first-order\
    \ relationships by performing gated graph convolutional operations in the relation\
    \ graph. By performing gated graph convolutional operations multiple iterations,\
    \ (e) semantic context with multi-order relationships can be computed. Finally,\
    \ CMRIN calculates the matching scores between semantic context of proposals and\
    \ the global context of the referring expression. The triplet loss with online\
    \ hard negative mining is adopted during training and the proposal with the highest\
    \ matching score is chosen."
  Figure 3 Link: articels_figures_by_rev_year\2020\RelationshipEmbedded_Representation_Learning_for_Grounding_Referring_Expressions\figure_3.jpg
  Figure 3 caption: All types of spatial relationships between proposal o i (green
    box) and proposal o j (blue box). The number following the relationship is the
    label.
  Figure 4 Link: articels_figures_by_rev_year\2020\RelationshipEmbedded_Representation_Learning_for_Grounding_Referring_Expressions\figure_4.jpg
  Figure 4 caption: Qualitative results showing initial attention score (gate) maps
    and final matching score maps. We compute the score of a pixel as the maximum
    score of proposals covering it, and normalize the score maps to 0 to 1. Warmer
    color indicates higher score.
  Figure 5 Link: articels_figures_by_rev_year\2020\RelationshipEmbedded_Representation_Learning_for_Grounding_Referring_Expressions\figure_5.jpg
  Figure 5 caption: Qualitative results showing initial attention score (gate) maps
    and final matching score maps. Warmer color indicates higher score.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.76
  Name of the first author: Sibei Yang
  Name of the last author: Yizhou Yu
  Number of Figures: 5
  Number of Tables: 7
  Number of authors: 3
  Paper title: Relationship-Embedded Representation Learning for Grounding Referring
    Expressions
  Publication Date: 2020-02-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison With the State-of-the-Art Methods on RefCOCO, RefCOCO+,
      and RefCOCOg
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Ablation Studies on Variants of Network Architecture of Our
      Proposed CMRIN on RefCOCO, RefCOCO+, and RefCOCOg
  Table 3 caption:
    table_text: TABLE 3 Comparison of Different Schemes for Training Our Proposed
      CMRIN on RefCOCO, RefCOCO+, and RefCOCOg
  Table 4 caption:
    table_text: TABLE 4 Comparison of Different Phrase Designs on RefCOCO, RefCOCO+,
      and RefCOCOg
  Table 5 caption:
    table_text: TABLE 5 Ablation Studies on Variants of Spatial Relation Graph of
      Our Proposed CMRIN on RefCOCO, RefCOCO+, and RefCOCOg
  Table 6 caption:
    table_text: TABLE 6 Experimental Results of SpatialSemantic Relation Graph Branch
      on RefCOCO, RefCOCO+, and RefCOCOg
  Table 7 caption:
    table_text: TABLE 7 Comparison With the State-of-the-Art Methods on RefCOCO, RefCOCO+,
      and RefCOCOg When Using Detected Proposals
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2973983
- Affiliation of the first author: tokyo institute of technology, meguro city, tokyo,
    japan
  Affiliation of the last author: national institute of informatics, chiyoda city,
    tokyo, japan
  Figure 1 Link: articels_figures_by_rev_year\2020\Depth_Sensing_by_NearInfrared_Light_Absorption_in_Water\figure_1.jpg
  Figure 1 caption: 'Shape from water based on bispectral light absorption. (a) and
    (b) show the scene at 905nm and 950nm, after normalizing the illumination and
    camera and filter sensitivity functions. The intensity difference between (a)
    and (b) is due to the difference of water absorption at these two wavelengths
    which we exploit to recover the depth. The color coded depth is shown in (c),
    and the recovered 3D shape is given in (d). (e) shows the target object for this
    example: a textureless ceramic object with strong specularity.'
  Figure 10 Link: articels_figures_by_rev_year\2020\Depth_Sensing_by_NearInfrared_Light_Absorption_in_Water\figure_10.jpg
  Figure 10 caption: Shape recovery of objects by bispectral method with complex geometry,
    texture, and reflection properties. For each row, from left to right, the input
    images at 905nm and 950nm, the depth coded 3D shape, the virtually shaded shape
    and the RGB appearance of the object are shown.
  Figure 2 Link: articels_figures_by_rev_year\2020\Depth_Sensing_by_NearInfrared_Light_Absorption_in_Water\figure_2.jpg
  Figure 2 caption: (a) shows the water absorption curve in the range from 400nm to
    1400nm. (b) shows the setup of the Beer-Lambert law. (c) and (d) illustrate our
    bispectral depth imaging in the coaligned and tilted configuration, respectively.
  Figure 3 Link: articels_figures_by_rev_year\2020\Depth_Sensing_by_NearInfrared_Light_Absorption_in_Water\figure_3.jpg
  Figure 3 caption: (a) shows relative depth error with respect to the reflectance
    spectrum difference, under varying intensity ratios. (b) shows the spectra of
    the 24 patches on the color checker in the range from 400nm to 1400nm. The reflectance
    spectrum difference for spectral pairs of 900nm and 920nm, as well as 900nm and
    950nm for each patch spectrum is shown in (c).
  Figure 4 Link: articels_figures_by_rev_year\2020\Depth_Sensing_by_NearInfrared_Light_Absorption_in_Water\figure_4.jpg
  Figure 4 caption: A reflectance spectra database in the Vis-NIR range from 400nm
    to 1400nm. We empirically find that the spectral reflectance difference for two
    close near-infrared wavelengths is usually negligible.
  Figure 5 Link: articels_figures_by_rev_year\2020\Depth_Sensing_by_NearInfrared_Light_Absorption_in_Water\figure_5.jpg
  Figure 5 caption: Configuration of perspective light and camera.
  Figure 6 Link: articels_figures_by_rev_year\2020\Depth_Sensing_by_NearInfrared_Light_Absorption_in_Water\figure_6.jpg
  Figure 6 caption: (a) shows our co-axial bispectral imaging system, and (b) the
    spectral response functions of the camera and the three filters. (c) is the spectrum
    of the incandescent illuminant. (d) is the calibrated water absorption coefficient.
  Figure 7 Link: articels_figures_by_rev_year\2020\Depth_Sensing_by_NearInfrared_Light_Absorption_in_Water\figure_7.jpg
  Figure 7 caption: Depth estimation error by bispectral method for three planar plates,
    including white target, orange plastic board and white marble.
  Figure 8 Link: articels_figures_by_rev_year\2020\Depth_Sensing_by_NearInfrared_Light_Absorption_in_Water\figure_8.jpg
  Figure 8 caption: Depth estimation error by bispectral method for the white target
    in perspective light source.
  Figure 9 Link: articels_figures_by_rev_year\2020\Depth_Sensing_by_NearInfrared_Light_Absorption_in_Water\figure_9.jpg
  Figure 9 caption: Shape recovery error by bispectral method for a sphere in sectional
    view.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.81
  Name of the first author: Yuta Asano
  Name of the last author: Imari Sato
  Number of Figures: 16
  Number of Tables: 0
  Number of authors: 4
  Paper title: Depth Sensing by Near-Infrared Light Absorption in Water
  Publication Date: 2020-02-14 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2973986
