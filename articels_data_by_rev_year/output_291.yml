- Affiliation of the first author: microsoft research, one microsoft way, redmond,
    wa
  Affiliation of the last author: department of electrical and computer engineering,
    virginia tech., blacksburg, va
  Figure 1 Link: articels_figures_by_rev_year\2014\Adopting_Abstract_Images_for_Semantic_Scene_Understanding\figure_1.jpg
  Figure 1 caption: An example set of semantically similar scenes created by human
    subjects for the same given sentence.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\Adopting_Abstract_Images_for_Semantic_Scene_Understanding\figure_2.jpg
  Figure 2 caption: An illustration of the clip art used to create the children (left)
    and the other available objects (right.)
  Figure 3 Link: articels_figures_by_rev_year\2014\Adopting_Abstract_Images_for_Semantic_Scene_Understanding\figure_3.jpg
  Figure 3 caption: A screenshot of the AMT interface used to create the abstract
    scenes.
  Figure 4 Link: articels_figures_by_rev_year\2014\Adopting_Abstract_Images_for_Semantic_Scene_Understanding\figure_4.jpg
  Figure 4 caption: Example sets of semantically similar scenes. The descriptions
    may be very specific (top) or more generic (second row.) Notice the variety of
    scenes that can convey the same semantic description. The presence and locations
    of objects can change dramatically, while still depicting similar meaning.
  Figure 5 Link: articels_figures_by_rev_year\2014\Adopting_Abstract_Images_for_Semantic_Scene_Understanding\figure_5.jpg
  Figure 5 caption: The mutual information measuring the dependence between classes
    of semantically similar scenes and the (left) occurrence of obejcts, (top) co-occurrence,
    relative depth and position, (middle) person attributes and (bottom) the position
    relative to the head and hand, and absolute position. Some mutual information
    scores are conditioned upon other variables (see text.) The pie chart shows the
    sum of the mutual information or conditional mutual information scores for all
    features. The probability of occurrence of each piece of clip art occurring is
    shown to the left.
  Figure 6 Link: articels_figures_by_rev_year\2014\Adopting_Abstract_Images_for_Semantic_Scene_Understanding\figure_6.jpg
  Figure 6 caption: Retrieval results for various feature types. The retrieval accuracy
    is measured based on the number of correctly retrieved images given a specified
    number of nearest neighbors.
  Figure 7 Link: articels_figures_by_rev_year\2014\Adopting_Abstract_Images_for_Semantic_Scene_Understanding\figure_7.jpg
  Figure 7 caption: The words with the highest total MI and CMI scores across all
    features for different part of speech (left). The words with highest total scores
    across different features types (top-right). Colors indicate the different parts
    of speech. Top non-nouns for several relative spatial features using object orientation
    (bottom-right).
  Figure 8 Link: articels_figures_by_rev_year\2014\Adopting_Abstract_Images_for_Semantic_Scene_Understanding\figure_8.jpg
  Figure 8 caption: The saliency (top) and memorability (bottom) for different types
    of objects. Both are measured using a series of experiments in which subjects
    are asked to identify scenes they have already seen. The saliency is computed
    as the difference between the hit rate of intact scenes minus the hit rate of
    scenes in which an object was removed.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.98
  Name of the first author: C. Lawrence Zitnick
  Name of the last author: Devi Parikh
  Number of Figures: 8
  Number of Tables: 0
  Number of authors: 3
  Paper title: Adopting Abstract Images for Semantic Scene Understanding
  Publication Date: 2014-10-31 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2366143
- Affiliation of the first author: "esiee-paris, universit\xE9 paris-est marne-la-vall\xE9\
    e, ligm, paris, france"
  Affiliation of the last author: "universit\xE9 de reims champagne-ardenne, crestic,\
    \ reims, france"
  Figure 1 Link: articels_figures_by_rev_year\2014\Directed_Connected_Operators_Asymmetric_Hierarchies_for_Image_Filtering_and_Segm\figure_1.jpg
  Figure 1 caption: (a) Neurite image; (b) directed connected filtering.
  Figure 10 Link: articels_figures_by_rev_year\2014\Directed_Connected_Operators_Asymmetric_Hierarchies_for_Image_Filtering_and_Segm\figure_10.jpg
  Figure 10 caption: Non-local adjacency relation. (a) Distal vessels of the 19th
    retinal image of the DRIVE database. (b) Adjacency relation shown on a critical
    threshold of (a). Green links represent symmetric edges while red arrows are asymmetric
    relations. Each S-component is associated to a color printed in a small circle
    inside each pixel.
  Figure 2 Link: articels_figures_by_rev_year\2014\Directed_Connected_Operators_Asymmetric_Hierarchies_for_Image_Filtering_and_Segm\figure_2.jpg
  Figure 2 caption: Undirected and directed graphs (see text).
  Figure 3 Link: articels_figures_by_rev_year\2014\Directed_Connected_Operators_Asymmetric_Hierarchies_for_Image_Filtering_and_Segm\figure_3.jpg
  Figure 3 caption: (a) A directed graph (the vertices and arcs are represented by
    circles and arrows, respectively) whose D-components are X1 =lbrace a,b,c,d,erbrace
    , X2 = lbrace d,erbrace , X3 = lbrace frbrace , X4 = lbrace g,h,irbrace , X5 =
    lbrace h,irbrace and X6 = lbrace irbrace , and whose S-components are Y1 = lbrace
    a,b,crbrace , Y2 = lbrace d,erbrace , Y3 = lbrace frbrace , Y4 = lbrace grbrace
    , Y5 = lbrace hrbrace and Y6 = lbrace irbrace . (b) The DAG mathfrak D(mathfrak
    G) of the S-components of the graph is depicted in (a).
  Figure 4 Link: articels_figures_by_rev_year\2014\Directed_Connected_Operators_Asymmetric_Hierarchies_for_Image_Filtering_and_Segm\figure_4.jpg
  Figure 4 caption: Some elementary graphs.
  Figure 5 Link: articels_figures_by_rev_year\2014\Directed_Connected_Operators_Asymmetric_Hierarchies_for_Image_Filtering_and_Segm\figure_5.jpg
  Figure 5 caption: 'A stack mathcal S = (mathfrak G0, mathfrak G1, mathfrak G2, mathfrak
    G3, mathfrak G4) . First row: Each color represents a S-component. Second row:
    Each color represents a D-component (vertices with more than one color belong
    to all the associated D-components).'
  Figure 6 Link: articels_figures_by_rev_year\2014\Directed_Connected_Operators_Asymmetric_Hierarchies_for_Image_Filtering_and_Segm\figure_6.jpg
  Figure 6 caption: (a) The S-component tree associated to the stack mathcal S of
    Fig. 5, first row. (b) The DAGs shown from top to bottom row are the DAGs of S-components
    of the graphs mathfrak G0, ldots , mathfrak G4 of Fig. 5, first row. (c) The D-component
    hierarchy of the stack mathcal S of Fig. 5, second row. This hierarchy is the
    S-component tree (a) enriched by the relation provided by the DAGs of S-components
    of all level sets of mathcal S (b). The red arrows are the extra links that are
    deduced by transitivity.
  Figure 7 Link: articels_figures_by_rev_year\2014\Directed_Connected_Operators_Asymmetric_Hierarchies_for_Image_Filtering_and_Segm\figure_7.jpg
  Figure 7 caption: 'First row: a graph (DAG) Hi , composed of nodes that are selected
    (in green) or discarded (in red) with respect to some criterion sigma j (see Section
    5.2). Each node corresponds to a S-component ( A , B and C , respectively), and
    models a D-component ( A cup B cup C , B cup C and C , respectively). Second row:
    subgraphs induced by the nodes that satisfy sigma j (green circles). Third row:
    subgraphs induced by the nodes that violate sigma j (red squares).'
  Figure 8 Link: articels_figures_by_rev_year\2014\Directed_Connected_Operators_Asymmetric_Hierarchies_for_Image_Filtering_and_Segm\figure_8.jpg
  Figure 8 caption: "Node selection (regularization) strategies. (a) The DAG Hi of\
    \ the S-components lbrace A, ldots , Irbrace . The components represented as green\
    \ circles (resp. red squares) satisfy (resp. violate) sigma . (b\u2013f) The graphs\
    \ induced by sigma : rm Sel-Minsigma , rm Dis-Maxsigma , Sel-Maxsigma , and Dis-Minsigma\
    \ ."
  Figure 9 Link: articels_figures_by_rev_year\2014\Directed_Connected_Operators_Asymmetric_Hierarchies_for_Image_Filtering_and_Segm\figure_9.jpg
  Figure 9 caption: 'Segmentation results on the DRIVE database. On each row, from
    left to right: pre-processed image, filtering result, and evaluation of the segmentation.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Benjamin Perret
  Name of the last author: Nicolas Passat
  Number of Figures: 11
  Number of Tables: 1
  Number of authors: 5
  Paper title: 'Directed Connected Operators: Asymmetric Hierarchies for Image Filtering
    and Segmentation'
  Publication Date: 2014-10-31 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Result Comparison on the DRIVE Retinal Image Database
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2366145
- Affiliation of the first author: institute of mathematics of the romanian academy
    and computer science department, university of toronto, on, canada
  Affiliation of the last author: department of mathematics, faculty of engineering,
    lund university, sweden and the institute of mathematics of the romanian academy
  Figure 1 Link: articels_figures_by_rev_year\2014\Actions_in_the_Eye_Dynamic_Gaze_Datasets_and_Learnt_Saliency_Models_for_Visual_R\figure_1.jpg
  Figure 1 caption: Heat maps generated from the fixations of 16 human subjects viewing
    six videos selected from the Hollywood-2 and UCF Sports datasets. Fixated locations
    are generally tightly clustered. This suggests a significant degree of consistency
    among human subjects in terms of the spatial distribution of their visual attention.
    See Fig. 3 for quantitative studies.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\Actions_in_the_Eye_Dynamic_Gaze_Datasets_and_Learnt_Saliency_Models_for_Visual_R\figure_2.jpg
  Figure 2 caption: Action recognition performance by humans on the Hollywood-2 database.
    The confusion matrix includes the 12 action labels plus the four most frequent
    combinations of labels in the ground truth.
  Figure 3 Link: articels_figures_by_rev_year\2014\Actions_in_the_Eye_Dynamic_Gaze_Datasets_and_Learnt_Saliency_Models_for_Visual_R\figure_3.jpg
  Figure 3 caption: Static inter-subject agreement. The ROC curves correspond to predicting
    the fixations of one subject from the fixations of the other subjects on the same
    video frame (blue) or on a different video (green) randomly selected from the
    dataset.
  Figure 4 Link: articels_figures_by_rev_year\2014\Actions_in_the_Eye_Dynamic_Gaze_Datasets_and_Learnt_Saliency_Models_for_Visual_R\figure_4.jpg
  Figure 4 caption: Areas of interest are obtained automatically by clustering the
    fixations of subjects. Left. Heat maps illustrating the assignments of fixations
    to AOIs. The colored blobs have been generated by pooling together all fixations
    belonging to the same AOI. Right. Scan path through automatically generated AOIs
    (colored boxes) for three subjects. Arrows illustrate saccades. Semantic labels
    have been manually assigned and illustrates the existance of cognitive routines
    centered at semantically meaningful objects.
  Figure 5 Link: articels_figures_by_rev_year\2014\Actions_in_the_Eye_Dynamic_Gaze_Datasets_and_Learnt_Saliency_Models_for_Visual_R\figure_5.jpg
  Figure 5 caption: Sampled entries from visual vocabularies obtained by clustering
    fixated image regions in the space of HoG descriptors, for several action classes
    from the Hollywood-2 dataset.
  Figure 6 Link: articels_figures_by_rev_year\2014\Actions_in_the_Eye_Dynamic_Gaze_Datasets_and_Learnt_Saliency_Models_for_Visual_R\figure_6.jpg
  Figure 6 caption: Saliency predictions for a video frame (a), both motion-based
    features in isolation (d-h) and combinations (i-l). HoG-MBH detector maps are
    closest to the ground truth (b), consistent with Table 7b.
  Figure 7 Link: articels_figures_by_rev_year\2014\Actions_in_the_Eye_Dynamic_Gaze_Datasets_and_Learnt_Saliency_Models_for_Visual_R\figure_7.jpg
  Figure 7 caption: Note that ground truth saliency maps (cyan) and output of our
    HoG-MBH detector (yellow) are similar to each other, but qualitatively different
    from the central bias map (gray). This gives visual intuition for the significantly
    higher performance of the HoG-MBH detector, over the central bias saliency sampling,
    when used in an end-to-end computer visual action recognition system (Table 5c,
    5d,5e).
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Stefan Mathe
  Name of the last author: Cristian Sminchisescu
  Number of Figures: 7
  Number of Tables: 8
  Number of authors: 2
  Paper title: 'Actions in the Eye: Dynamic Gaze Datasets and Learnt Saliency Models
    for Visual Recognition'
  Publication Date: 2014-10-31 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Contextual Elements that Participants in the Context Recognition
      Group Have Been Asked to Locate, for Each Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Static and Dynamic Consistency Analysis
  Table 3 caption:
    table_text: TABLE 3 Influence of Task on the Spatial Distribution of Fixations
  Table 4 caption:
    table_text: TABLE 4 Harris Spacetime Corners vs. Fixations
  Table 5 caption:
    table_text: TABLE 5 Action Recognition Performance on the Hollywood-2 Data Set
  Table 6 caption:
    table_text: TABLE 6 Action Recognition Performance on the UCF Sports Actions Data
      Set
  Table 7 caption:
    table_text: TABLE 7 Evaluation of Individual Feature Maps and Combinations for
      Human Saliency Prediction
  Table 8 caption:
    table_text: TABLE 8 The Impact of Random Sampling on Action Recognition Performance
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2366154
- Affiliation of the first author: department of engineering, university of cambridge,
    cambridge, england, united kingdom
  Affiliation of the last author: department of engineering, university of cambridge,
    cambridge, england, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2014\GPstruct_Bayesian_Structured_Prediction_Using_Gaussian_Processes\figure_1.jpg
  Figure 1 caption: Schematic representation of structured prediction models. The
    model we are describing here, GPstruct, exhibits all three properties separately
    present in other, existing, models.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\GPstruct_Bayesian_Structured_Prediction_Using_Gaussian_Processes\figure_2.jpg
  Figure 2 caption: Linear-chain factor graph for sequence prediction.
  Figure 3 Link: articels_figures_by_rev_year\2014\GPstruct_Bayesian_Structured_Prediction_Using_Gaussian_Processes\figure_3.jpg
  Figure 3 caption: "left: Effect of sampling hyperparameters every 1,000 steps versus\
    \ fixing h p =1 , over the full history of f samples. f \u2217 MAP scheme, thinning\
    \ at 1:1,000. middle: Effect of thinning, i.e. sampling f \u2217 |f more rarely\
    \ than every f sample. Chunking task, f \u2217 MAP scheme, h p =1 . right: Effect\
    \ of number of f \u2217 |f samples for each f sample. Chunking task, thinning\
    \ at 1:1,000, h p =1 ."
  Figure 4 Link: articels_figures_by_rev_year\2014\GPstruct_Bayesian_Structured_Prediction_Using_Gaussian_Processes\figure_4.jpg
  Figure 4 caption: Error rate cross plot of the 20 gesture video sessions. The axes
    correspond to error rate of GPstruct with SE kernel and CRF, the diagonal line
    shows equal performance. The shadowed stars are those with at least 5 percent
    performance difference.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "S\xE9bastien Brati\xE8res"
  Name of the last author: Zoubin Ghahramani
  Number of Figures: 4
  Number of Tables: 2
  Number of authors: 3
  Paper title: 'GPstruct: Bayesian Structured Prediction Using Gaussian Processes'
  Publication Date: 2014-10-31 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Models, Acronyms and References.
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Experimental Results on Text Processing Task
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2366151
- Affiliation of the first author: department of electronics, electrical and communication
    engineering, university of chinese academy and sciences, beijing, china
  Affiliation of the last author: institute of advanced computer studies, university
    of maryland, college park, md
  Figure 1 Link: articels_figures_by_rev_year\2014\Text_Detection_and_Recognition_in_Imagery_A_Survey\figure_1.jpg
  Figure 1 caption: Text in imagery. (a) Video graphical text. (b) Point-and-shoot
    scene text. (c) Incidental scene text.
  Figure 10 Link: articels_figures_by_rev_year\2014\Text_Detection_and_Recognition_in_Imagery_A_Survey\figure_10.jpg
  Figure 10 caption: Word recognition with a CRF model [195]. (Courtesy of C. Shi).
  Figure 2 Link: articels_figures_by_rev_year\2014\Text_Detection_and_Recognition_in_Imagery_A_Survey\figure_2.jpg
  Figure 2 caption: Frameworks of two commonly used text detection and recognition
    methodologies. (a) Stepwise methodology. (b) Integrated methodology.
  Figure 3 Link: articels_figures_by_rev_year\2014\Text_Detection_and_Recognition_in_Imagery_A_Survey\figure_3.jpg
  Figure 3 caption: Flowchart of the stepwise video text recognition approach with
    detection, tracking, segmentation, recognition and language processing [126].
  Figure 4 Link: articels_figures_by_rev_year\2014\Text_Detection_and_Recognition_in_Imagery_A_Survey\figure_4.jpg
  Figure 4 caption: Illustration of a word spotting approach [148]. Characters are
    recognized with HOG features and Random Ferns classifiers, and words are modeled
    with a pictorial structure model (Courtesy of K. Wang).
  Figure 5 Link: articels_figures_by_rev_year\2014\Text_Detection_and_Recognition_in_Imagery_A_Survey\figure_5.jpg
  Figure 5 caption: The CNN based integrated detection and recognition approach [173].
    (a) CNN for character detection. (b) CNN responses for recognition. (Courtesy
    of T. Wang).
  Figure 6 Link: articels_figures_by_rev_year\2014\Text_Detection_and_Recognition_in_Imagery_A_Survey\figure_6.jpg
  Figure 6 caption: (a) Global text feature extraction [49]. (b) and (c) Sub-regions
    for local feature extraction [174], [182].
  Figure 7 Link: articels_figures_by_rev_year\2014\Text_Detection_and_Recognition_in_Imagery_A_Survey\figure_7.jpg
  Figure 7 caption: Text line segmentation with projection profile (first row) [23],
    and skeleton analysis (second row) (Courtesy of Phan and Tan [141]).
  Figure 8 Link: articels_figures_by_rev_year\2014\Text_Detection_and_Recognition_in_Imagery_A_Survey\figure_8.jpg
  Figure 8 caption: Character segmentation with the projection profile analysis (first
    row) and the path optimization method (second row) [139] (Courtesy of Phan).
  Figure 9 Link: articels_figures_by_rev_year\2014\Text_Detection_and_Recognition_in_Imagery_A_Survey\figure_9.jpg
  Figure 9 caption: Word examples from dataset ICDAR'11 (first row) and IIIT5k (second
    row) ( Table 2).
  First author gender probability: 0.88
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Qixiang Ye
  Name of the last author: David Doermann
  Number of Figures: 14
  Number of Tables: 5
  Number of authors: 2
  Paper title: 'Text Detection and Recognition in Imagery: A Survey'
  Publication Date: 2014-11-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Challenges in Text Detection and Recognition
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Datasets
  Table 3 caption:
    table_text: TABLE 3 Text Detection Performance
  Table 4 caption:
    table_text: TABLE 4 Cropped Word (Localized Text) Recognition Performance
  Table 5 caption:
    table_text: "TABLE 5 \u201CEnd-to-End\u201D Text Recognition Performance"
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2366765
- Affiliation of the first author: department of computer science, university of warwick,
    coventry, cv4 7al, united kingdom
  Affiliation of the last author: department of electrical and electronic engineering,
    university of cagliari, cagliari, italy
  Figure 1 Link: articels_figures_by_rev_year\2014\On_Reducing_the_Effect_of_Covariate_Factors_in_Gait_Recognition_A_Classifier_Ens\figure_1.jpg
  Figure 1 caption: GEIs of the same subject from the USF dataset [5]. The leftmost
    (a) is the gallery GEI in normal condition, while the rest (b)-(g) are probe GEIs
    with covariates (b) viewpoint, (c) walking surface, (d) viewpoint and walking
    surface, (e) carrying condition, (f) carrying condition and viewpoint, (g) elapsed
    time, shoe type, clothing, and walking surface.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\On_Reducing_the_Effect_of_Covariate_Factors_in_Gait_Recognition_A_Classifier_Ens\figure_2.jpg
  Figure 2 caption: An example of modelling the covariate effect by image difference
    between the gallery GEI (i.e., Fig. 1a) and a probe GEI (i.e., Fig. 1 e).
  Figure 3 Link: articels_figures_by_rev_year\2014\On_Reducing_the_Effect_of_Covariate_Factors_in_Gait_Recognition_A_Classifier_Ens\figure_3.jpg
  Figure 3 caption: 'From left to right: difference images between the gallery GEI
    Fig. 1a and the probe GEIs Figs. 1b, 1c, 1d, 1e, 1 f, 1g.'
  Figure 4 Link: articels_figures_by_rev_year\2014\On_Reducing_the_Effect_of_Covariate_Factors_in_Gait_Recognition_A_Classifier_Ens\figure_4.jpg
  Figure 4 caption: Normalized Euclidean distances from gallery sample Fig. 1a to
    probe samples Figs. 1b, 1c, 1d, 1e, 1 f, 1g, based on different projection directions.
  Figure 5 Link: articels_figures_by_rev_year\2014\On_Reducing_the_Effect_of_Covariate_Factors_in_Gait_Recognition_A_Classifier_Ens\figure_5.jpg
  Figure 5 caption: Images from (a) the USF dataset [5], and (b) the OU-ISIR-B dataset
    [16].
  Figure 6 Link: articels_figures_by_rev_year\2014\On_Reducing_the_Effect_of_Covariate_Factors_in_Gait_Recognition_A_Classifier_Ens\figure_6.jpg
  Figure 6 caption: 'On the performance sensitivity to the parameters on the USF dataset:
    (a) N is the dimension of the random subspace; (b) M is the number of projection
    directions of LE1LE2; (c) L is the classifier number.'
  Figure 7 Link: articels_figures_by_rev_year\2014\On_Reducing_the_Effect_of_Covariate_Factors_in_Gait_Recognition_A_Classifier_Ens\figure_7.jpg
  Figure 7 caption: Over the 12 probe sets on the USF dataset, the distribution of
    hatGamma (i.e., the general ratio of false votes to true votes).
  Figure 8 Link: articels_figures_by_rev_year\2014\On_Reducing_the_Effect_of_Covariate_Factors_in_Gait_Recognition_A_Classifier_Ens\figure_8.jpg
  Figure 8 caption: Performance distribution with respect to 31 probe clothes types
    on the OU-ISIR-B dataset.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.53
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yu Guan
  Name of the last author: Fabio Roli
  Number of Figures: 8
  Number of Tables: 8
  Number of authors: 3
  Paper title: 'On Reducing the Effect of Covariate Factors in Gait Recognition: A
    Classifier Ensemble Method'
  Publication Date: 2014-11-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Twelve Pre-Designed Experiments on the USF Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Different Clothing Combinations in the OU-ISIR-B Dataset
  Table 3 caption:
    table_text: TABLE 3 Performance Statistics in Terms of Rank-1 CCRs (Percent) for
      10 Runs on the USF Dataset
  Table 4 caption:
    table_text: TABLE 4 Running Time (Seconds) on the USF Dataset
  Table 5 caption:
    table_text: TABLE 5 Rank-1 CCRs (Percent) of Our Methods on the USF Dataset
  Table 6 caption:
    table_text: TABLE 6 Algorithms Comparison in Terms of Rank-1Rank-5 CCRs (Percent)
      on the USF Dataset
  Table 7 caption:
    table_text: TABLE 7 Performance Statistics in Terms of Rank-1 CCRs (Percent) for
      10 Runs on the OU-ISIR-B Dataset
  Table 8 caption:
    table_text: TABLE 8 Algorithms Comparison in Terms of Rank-1 CCRs (Percent) on
      the OU-ISIR-B Dataset
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2366766
- Affiliation of the first author: research school of computer science, australian
    national university, act, australia
  Affiliation of the last author: research school of computer science, australian
    national university, act, australia
  Figure 1 Link: articels_figures_by_rev_year\2014\Learning_Weighted_Lower_Linear_Envelope_Potentials_in_Binary_Markov_Random_Field\figure_1.jpg
  Figure 1 caption: Example lower linear envelope psi Hc!(yc) (shown solid) with three
    terms (dashed) as a function of W!c(yc) = sum i in c wci yi . When W!c(yc) le
    W1 the first linear function is active, when W1 lt W!c(yc) le W2 the second linear
    function is active, otherwise the third linear function is active.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\Learning_Weighted_Lower_Linear_Envelope_Potentials_in_Binary_Markov_Random_Field\figure_2.jpg
  Figure 2 caption: 'Construction of an st -graph for minimizing energy functions
    with arbitrary weighted lower linear envelope potentials. Every cut corresponds
    to an assignment to the random variables, where variables associated with nodes
    in the cal S set take the value one, and those associated with nodes in the cal
    T set take the value zero. With slight abuse of notation, we use the variables
    to denote nodes in our graph. For each lower linear envelope potential edges are
    added as follows: for each i in c , add an edge from yi to t with weight a1 wci
    ; for each i in c and k = 1, ldots , K-1 , add an edge from zk to yi with weight
    (ak - ak+1) wci ; and for k = 1, ldots , K-1 , add an edge from s to zk with weight
    ak - ak+1 and edge from zk to t with weight bk+1 - bk . Other edges may be required
    to represent unary and pairwise potentials (see [30]).'
  Figure 3 Link: articels_figures_by_rev_year\2014\Learning_Weighted_Lower_Linear_Envelope_Potentials_in_Binary_Markov_Random_Field\figure_3.jpg
  Figure 3 caption: Example piecewise-linear concave function of W!c(yc) = sum i in
    c wci boldsymbol yi . The function can be represented as the minimum over a set
    of linear functions (lower linear envelope) or as a set of sampled points theta
    k with curvature constraint.
  Figure 4 Link: articels_figures_by_rev_year\2014\Learning_Weighted_Lower_Linear_Envelope_Potentials_in_Binary_Markov_Random_Field\figure_4.jpg
  Figure 4 caption: Illustration of how the feature vector phi (boldsymbol y) interpolates
    between samples to produce the correct value for the active linear function.
  Figure 5 Link: articels_figures_by_rev_year\2014\Learning_Weighted_Lower_Linear_Envelope_Potentials_in_Binary_Markov_Random_Field\figure_5.jpg
  Figure 5 caption: Inferred output from our synthetic experiments. Shown are (a)
    the ground-truth labels, (b) noisy inputs, (c) best inferred labels using a pairwise
    model, (d)-(e) inferred output from the model containing higher-order terms after
    three training iterations and at convergence, respectively. Matlab source code
    for reproducing these results is available from the author's homepage.
  Figure 6 Link: articels_figures_by_rev_year\2014\Learning_Weighted_Lower_Linear_Envelope_Potentials_in_Binary_Markov_Random_Field\figure_6.jpg
  Figure 6 caption: Learned linear envelopes (parameters are normalized by the unary
    weight) for synthetic experiments. The first row (a)-(c) shows results with symmetric
    noise ( eta 0 = eta 1 = 0.1 ) while the second row (d)-(f) shows results with
    asymmetric noise ( eta 0 = 0.5 and eta 1 = 0.1 ). Compared are models with unary
    and higher-order potentials with K = 10 linear terms ((a) and (d)), unary, pairwise
    and higher-order potentials ((b) and (e)), and unary and higher-order potentials
    with K = 50 linear terms ((c) and (f)).
  Figure 7 Link: articels_figures_by_rev_year\2014\Learning_Weighted_Lower_Linear_Envelope_Potentials_in_Binary_Markov_Random_Field\figure_7.jpg
  Figure 7 caption: Inferred output from our synthetic experiments with misspecified
    cliques. Shown are inferred outputs from the model at convergence. Rows (i)-(ii)
    correspond to partially covering each grid square with 1, 2 and 5 higher-order
    cliques, respectively. Columns (a)-(d) correspond to the size of each clique (95,
    90, 75, 50 percent grid square coverage, respectively). In addition, 10 percent
    of the cliques were generated to contain random a random mix of pixels.
  Figure 8 Link: articels_figures_by_rev_year\2014\Learning_Weighted_Lower_Linear_Envelope_Potentials_in_Binary_Markov_Random_Field\figure_8.jpg
  Figure 8 caption: 'Example results from our GrabCut experiments. Shown are: (a)
    the image and bounding box, (b) ground-truth segmentation, (c) baseline model
    output, and (d) output from model with higher-order terms.'
  Figure 9 Link: articels_figures_by_rev_year\2014\Learning_Weighted_Lower_Linear_Envelope_Potentials_in_Binary_Markov_Random_Field\figure_9.jpg
  Figure 9 caption: 'Example segmentations produced by our Weizmann Horse experiments.
    Shown are: (a) the image, (b) baseline foreground mask, (c) baseline model foreground
    overlay, (d) higher-order model foreground mask, and (e) higher-order model foreground
    overlay.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Stephen Gould
  Name of the last author: Stephen Gould
  Number of Figures: 9
  Number of Tables: 1
  Number of authors: 1
  Paper title: Learning Weighted Lower Linear Envelope Potentials in Binary Markov
    Random Fields
  Publication Date: 2014-11-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Results from Our Weizmann Horse Experiments
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2366760
- Affiliation of the first author: tecnalia, zamudio, bizkaia, spain
  Affiliation of the last author: microsoft research cambridge, cambridge, united
    kingdom
  Figure 1 Link: articels_figures_by_rev_year\2014\What_Can_Pictures_Tell_Us_About_Web_Pages_Improving_Document_Search_Using_Images\figure_1.jpg
  Figure 1 caption: 'Method overview: the query q is issued (a) to a document search
    engine producing a ranked list mathbf r of Web pages and (b) to a text-based image
    search engine yielding positive training image examples to learn a query-specific
    visual classifier. Finally, (c) the visual classifier is used to rerank the pages
    in the ranking list mathbf r .'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\What_Can_Pictures_Tell_Us_About_Web_Pages_Improving_Document_Search_Using_Images\figure_2.jpg
  Figure 2 caption: "Mean precision 10 using different image features with the GBRT\
    \ reranker based on UDMQ (red) and Indri (yellow). Removing the visual content\
    \ features (\u201Ctext + visual metadata\u201D) or the visualness features from\
    \ our descriptor causes a drop in performance, which suggests that all our features\
    \ contribute to the improvement over the pure text-based system S ."
  Figure 3 Link: articels_figures_by_rev_year\2014\What_Can_Pictures_Tell_Us_About_Web_Pages_Improving_Document_Search_Using_Images\figure_3.jpg
  Figure 3 caption: 'Visualization of a few queries where our system produces (a)
    higher and (b) lower values of prec10 compared to UDMQ. For each query we show
    the five Bing training images that receive the largest classification score by
    the learned visual classifier. The image error is the cross-validation error rate
    of the visual classifier trained on Bing images. Note how the image error tends
    to be lower for the queries in (a) compared to those in (b): indeed, our approach
    tends to do better when the query corresponds to a visual concept (see results
    in Table 3).'
  Figure 4 Link: articels_figures_by_rev_year\2014\What_Can_Pictures_Tell_Us_About_Web_Pages_Improving_Document_Search_Using_Images\figure_4.jpg
  Figure 4 caption: Precision and normalized discounted cumulative gain (NDCG) obtained
    with UDMQ and our approach for a varying number of top documents retrieved (K).
    Here we focus on performance when K is small, as typically in Web search we are
    mostly interested in producing accurate results for the first few documents retrieved.
    Our method outperforms the text-based search engine UDMQ according to both accuracy
    metrics.
  Figure 5 Link: articels_figures_by_rev_year\2014\What_Can_Pictures_Tell_Us_About_Web_Pages_Improving_Document_Search_Using_Images\figure_5.jpg
  Figure 5 caption: Visualization of the five queries where our system yields the
    highest gain in prec10 according to the crowdsourced relevance labels. For each
    query we show the relevant Web pages that are ranked in the top-10 list of our
    system and that are not present in the list of UDMQ. Note that nearly all documents
    contain pictures that are highly representative of the concept expressed by the
    query.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Sergio Rodriguez-Vaamonde
  Name of the last author: Andrew W. Fitzgibbon
  Number of Figures: 5
  Number of Tables: 5
  Number of authors: 3
  Paper title: What Can Pictures Tell Us About Web Pages? Improving Document Search
    Using Images
  Publication Date: 2014-11-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Mean Precision 10 and 30 on the TREC 2009 Million Query Benchmark
      Using Different Ranking Models
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Reranking Using Only One Feature, Rather Than the Full Set
      (Results Based on the Candidate List Given By UDMQ)
  Table 3 caption:
    table_text: "TABLE 3 A Comparison across Queries between the Text-Based Engines\
      \ and the GBRT Image-Based Reranker: The \u201CQueries where S wins\u201D Are\
      \ Those for Which the Text-Based Search Engine Provides Higher Prec10 than Our\
      \ Approach"
  Table 4 caption:
    table_text: TABLE 4 Exhaustive Manual Assessment of the Top-10 Search Results
      for All MQ09 Queries Using Crowdsourcing
  Table 5 caption:
    table_text: TABLE 5 Mean Precision 10 of UDMQ and Our Approach on Visual and Non-Visual
      Queries
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2366761
- Affiliation of the first author: department of electrical and computer engineering,
    university of california, riverside, ca 92521
  Affiliation of the last author: department of electrical and computer engineering,
    university of california, riverside, ca
  Figure 1 Link: articels_figures_by_rev_year\2014\ContextAware_Activity_Modeling_Using_Hierarchical_Conditional_Random_Fields\figure_1.jpg
  Figure 1 caption: An example that demonstrates the importance of context in activity
    recognition. Motion region surrounding the person of interest is located by red
    circle, interacting vehicle is located by blue bounding box.
  Figure 10 Link: articels_figures_by_rev_year\2014\ContextAware_Activity_Modeling_Using_Hierarchical_Conditional_Random_Fields\figure_10.jpg
  Figure 10 caption: Subsets of context attributes used for the development of intra-activity
    context features.
  Figure 2 Link: articels_figures_by_rev_year\2014\ContextAware_Activity_Modeling_Using_Hierarchical_Conditional_Random_Fields\figure_2.jpg
  Figure 2 caption: The left graph shows the video representation of an activity set
    with n motion segments and m candidate activities. The right graph shows the graphical
    representation of our Hierarchical-CRF model. The white nodes are the action variables
    and the gray nodes in the graph are the hidden activity variables. Note that observations
    associated with the model variables are not shown for clear representation.
  Figure 3 Link: articels_figures_by_rev_year\2014\ContextAware_Activity_Modeling_Using_Hierarchical_Conditional_Random_Fields\figure_3.jpg
  Figure 3 caption: 'Illustration of CRF models for activity recognition. (a): Action-based
    Linear-Chain CRF; (b): Action-based higher-order CRF model (with latent activity
    variables); (c): Action-based two-layer Hierarchical-CRF. Note that all the observations
    for the random variables are omitted for compactness; (d): symbols in sub-figures
    (a, b, c); (e): graph representation of the model in [44] for comparison. One
    action segment denotes a random variable in the action layer, whose value is the
    activity label for the action segment. A colored circle denotes a random variable
    in the activity layer, whose value is the label for its connected clique. As shown
    in (a), in the action layer, action segments that belong to the same trajectory
    are modeled as a linear-chain CRF. Then, hidden activity-level variables with
    action-activity edges (in light blue) are added for each action clique to form
    higher-order CRF as shown in (b). An activity and its associated action nodes
    have a same color. Finally, pair-wise activity edges (in red) are added to form
    the proposed two-layer Hierarchical-CRF model.'
  Figure 4 Link: articels_figures_by_rev_year\2014\ContextAware_Activity_Modeling_Using_Hierarchical_Conditional_Random_Fields\figure_4.jpg
  Figure 4 caption: "(a) The image shows one example of inter-activity spatial relationship.\
    \ The red circle indicates the motion region of s at this frame while the purple\
    \ rectangle indicates the activity region of d . Assume SC is defined by quantizing\
    \ and grouping r s (n) into three bins: r s (n)\u22640.5 ( s and d is at the same\
    \ spatial position at the n th frame of s ), 0.5< r s (n)<1.5 ( s is near d at\
    \ the n th frame of s ) and r s (n)\u22651.5 ( s is far away from d at the n th\
    \ frame of s ). In the image, r s (n)>1.5 , so, s c sd (n)=[001] . (b) The image\
    \ shows one example of inter-activity temporal relationship. The n th frame of\
    \ s occurs before d . So, t sd (n)=[100] ."
  Figure 5 Link: articels_figures_by_rev_year\2014\ContextAware_Activity_Modeling_Using_Hierarchical_Conditional_Random_Fields\figure_5.jpg
  Figure 5 caption: Detected objects of interest in the UCLA office scene.
  Figure 6 Link: articels_figures_by_rev_year\2014\ContextAware_Activity_Modeling_Using_Hierarchical_Conditional_Random_Fields\figure_6.jpg
  Figure 6 caption: Subsets of context attributes used for the development of intra-activity
    context features for UCLA Dataset (the superscripts indicates the correspondence
    between the subsets and the objects).
  Figure 7 Link: articels_figures_by_rev_year\2014\ContextAware_Activity_Modeling_Using_Hierarchical_Conditional_Random_Fields\figure_7.jpg
  Figure 7 caption: Examples of agent-object interactions detected from image.
  Figure 8 Link: articels_figures_by_rev_year\2014\ContextAware_Activity_Modeling_Using_Hierarchical_Conditional_Random_Fields\figure_8.jpg
  Figure 8 caption: Precision (a) and recall (b) for the 10 activities in UCLA Office
    Dataset. The activities are defined in Section 5.2. HCRF is the short of Hierarchical-CRF.
  Figure 9 Link: articels_figures_by_rev_year\2014\ContextAware_Activity_Modeling_Using_Hierarchical_Conditional_Random_Fields\figure_9.jpg
  Figure 9 caption: Overall and average per-class accuracy for different methods on
    UCLA Office Dataset. The BOW+SVM method is tested on video clips, while other
    results are in the framework of our proposed action-based CRF models upon automatically
    detected action segments. HCRF is the short of Hierarchical-CRF.
  First author gender probability: 0.98
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yingying Zhu
  Name of the last author: Amit K. Roy-Chowdhury
  Number of Figures: 17
  Number of Tables: 0
  Number of authors: 3
  Paper title: Context-Aware Activity Modeling Using Hierarchical Conditional Random
    Fields
  Publication Date: 2014-11-10 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2369044
- Affiliation of the first author: "sierra team, inria, and \xE9cole normale sup\xE9\
    rieure, paris, france"
  Affiliation of the last author: faculty of computer science, higher school of economics,
    moscow, russia
  Figure 1 Link: articels_figures_by_rev_year\2014\Submodular_Relaxation_for_Inference_in_Markov_Random_Fields\figure_1.jpg
  Figure 1 caption: Subgradient ascent algorithm to maximize the SMR dual (15).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2014\Submodular_Relaxation_for_Inference_in_Markov_Random_Fields\figure_2.jpg
  Figure 2 caption: Coordinate ascent algorithm for maximizing the dual D(bf lambda
    ) (15) in case of pairwise associative potentials.
  Figure 3 Link: articels_figures_by_rev_year\2014\Submodular_Relaxation_for_Inference_in_Markov_Random_Fields\figure_3.jpg
  Figure 3 caption: The three possible cases at the maximum of the SMR dual function.
    The optimal energy value is shown by the horizontal dotted line. Solid lines show
    faces of the hypograph of the dual function D(boldsymbol lambda ) .
  Figure 4 Link: articels_figures_by_rev_year\2014\Submodular_Relaxation_for_Inference_in_Markov_Random_Fields\figure_4.jpg
  Figure 4 caption: The results of experiment 7.1 comparing SMR, DD TRW (both with
    different optimization routines) and TRW-S on segmentation and stereo datasets.
  Figure 5 Link: articels_figures_by_rev_year\2014\Submodular_Relaxation_for_Inference_in_Markov_Random_Fields\figure_5.jpg
  Figure 5 caption: "Energieslower bounds produced by SMR and CWD on the image segmentation\
    \ models. All blue curves correspond to CWD, red curves\u2014to SMR. Plots (a)\
    \ and (b) correspond to the 1-seg and 3-seg models respectively. For each point\
    \ of each plot we report the results aggregated across the standard test set:\
    \ the medians (solid lines), the lower and the upper quartiles (dashed lines).\
    \ This means that for each plot 50 percent of all curves pass between the dashed\
    \ lines."
  Figure 6 Link: articels_figures_by_rev_year\2014\Submodular_Relaxation_for_Inference_in_Markov_Random_Fields\figure_6.jpg
  Figure 6 caption: Comparison of SMR, GTRW and MPF methods. Plot (a) shows the lower
    bounds and the energies obtained by each method. Note that the energies can be
    lower than the lower bounds because the labelings violate the global hard constraints.
    Plot (b) shows the total constraint violation of the primal solution (measured
    in the percentage of pixels that have to be recolored to make the constraints
    consistent). For each curve we report the median, the lower and the upper quartiles.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Anton Osokin
  Name of the last author: Dmitry P. Vetrov
  Number of Figures: 6
  Number of Tables: 2
  Number of authors: 2
  Paper title: Submodular Relaxation for Inference in Markov Random Fields
  Publication Date: 2014-11-10 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Optimal Parameter Values Chosen by the Grid Search in
      Section 7.1
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Comparison of the Energies Obtained by the SMR and the \u03B1\
      \ -Expansion [23] Algorithms on the Two Models with Robust P n -Potts Potentials"
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2369046
