- Affiliation of the first author: futurewei seattle cloud lab, seattle, wa, usa
  Affiliation of the last author: department of computer science, purdue university,
    west lafayette, in, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Learning_Generalized_Transformation_Equivariant_Representations_Via_AutoEncoding\figure_1.jpg
  Figure 1 caption: The figure illustrates a comparison between AED, AET, and AET.
    The AED and AET seek to reconstruct the input data and transformation at the output
    end, respectively. The encoder (E) extracts the representation of input and transformed
    images. The decoder (D) either reconstructs the data in AED, or the transformation
    in AET. The SAT builds a classifier (C) upon the output representation from the
    encoder by capturing the equivariant visual structures under various transformations.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Learning_Generalized_Transformation_Equivariant_Representations_Via_AutoEncoding\figure_2.jpg
  Figure 2 caption: "The figure illustrates the variational approach to unsupervised\
    \ learning and (semi-)supervised learning of autoencoding transformations, namely\
    \ AVT and SAT, respectively. The probability p \u03B8 (z|t,x) acts as the representation\
    \ encoder, while q \u03D5 (t|z, z ~ ) and q \u03D5 (y| z ~ ) play the roles of\
    \ a transformation and label decoder, respectively. By setting the transformation\
    \ to an identity 1 , the corresponding z ~ is the representation of an original\
    \ image."
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Guo-Jun Qi
  Name of the last author: Xiao Wang
  Number of Figures: 2
  Number of Tables: 12
  Number of authors: 4
  Paper title: Learning Generalized Transformation Equivariant Representations Via
    AutoEncoding Transformations
  Publication Date: 2020-10-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison Between Unsupervised Feature Learning Methods on
      CIFAR-10
  Table 10 caption:
    table_text: TABLE 10 Error Rate Percentage of Compared Methods on SVHN Over Ten
      Runs (Four Runs When all Labels are Used)
  Table 2 caption:
    table_text: TABLE 2 Error Rates of Different Classifiers on CIFAR-10
  Table 3 caption:
    table_text: TABLE 3 The Comparison of the KNN Error Rates by Different Models
      With Varying Numbers K K of Neazrest Neighbors on CIFAR-10
  Table 4 caption:
    table_text: TABLE 4 Error Rates on CIFAR-10 When Different Numbers of Samples
      Per Class are Used to Train the Downstream Classifiers
  Table 5 caption:
    table_text: TABLE 5 Top-1 Accuracy With Non-Linear Layers on ImageNet
  Table 6 caption:
    table_text: TABLE 6 Top-1 Accuracy With Linear Layers on ImageNet
  Table 7 caption:
    table_text: TABLE 7 Top-1 Accuracy on the Places Dataset
  Table 8 caption:
    table_text: TABLE 8 Results on PASCAL VOC 2007 Detection Tasks in Terms of the
      Mean Average Precision (mAP)
  Table 9 caption:
    table_text: TABLE 9 Error Rate Percentage of Compared Methods on CIFAR-10 Over
      Ten Runs (Four Runs When all Labels are Used)
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3029801
- Affiliation of the first author: tusimple, beijing, china
  Affiliation of the last author: tusimple, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2020\An_Efficient_Solution_to_NonMinimal_Case_Essential_Matrix_Estimation\figure_1.jpg
  Figure 1 caption: The RANSAC framework contains the collaboration of a minimal solver
    and a non-minimal solver. This framework is known as the gold standard algorithm
    [1]. This paper focuses on relative pose estimation. Here we take line fitting
    for an example due to its convenient visualization.
  Figure 10 Link: articels_figures_by_rev_year\2020\An_Efficient_Solution_to_NonMinimal_Case_Essential_Matrix_Estimation\figure_10.jpg
  Figure 10 caption: Relative pose accuracy with respect to translation length.
  Figure 2 Link: articels_figures_by_rev_year\2020\An_Efficient_Solution_to_NonMinimal_Case_Essential_Matrix_Estimation\figure_2.jpg
  Figure 2 caption: An overview of relations between different formulations.
  Figure 3 Link: articels_figures_by_rev_year\2020\An_Efficient_Solution_to_NonMinimal_Case_Essential_Matrix_Estimation\figure_3.jpg
  Figure 3 caption: The sparsity of our SDP problem. (a) Aggregate sparsity pattern.
    White parts correspond to zeros, and gray parts correspond to nonzeros. There
    are two diagonal blocks in this pattern. (b) Chordal decomposition of the corresponding
    graph. The graph contains 12 nodes and it can be decomposed into 2 distinct maximal
    cliques.
  Figure 4 Link: articels_figures_by_rev_year\2020\An_Efficient_Solution_to_NonMinimal_Case_Essential_Matrix_Estimation\figure_4.jpg
  Figure 4 caption: Robust estimation of essential matrix by solving a line process
    of an M-estimator. In left part, the size of a feature point represents its weight.
  Figure 5 Link: articels_figures_by_rev_year\2020\An_Efficient_Solution_to_NonMinimal_Case_Essential_Matrix_Estimation\figure_5.jpg
  Figure 5 caption: Loss functions. (a) Different loss function. (b) Welsch functions
    [17] with different scale parameters tau .
  Figure 6 Link: articels_figures_by_rev_year\2020\An_Efficient_Solution_to_NonMinimal_Case_Essential_Matrix_Estimation\figure_6.jpg
  Figure 6 caption: Relative pose accuracy with respect to image noise levels.
  Figure 7 Link: articels_figures_by_rev_year\2020\An_Efficient_Solution_to_NonMinimal_Case_Essential_Matrix_Estimation\figure_7.jpg
  Figure 7 caption: Relative pose accuracy with respect to number of point correspondences.
  Figure 8 Link: articels_figures_by_rev_year\2020\An_Efficient_Solution_to_NonMinimal_Case_Essential_Matrix_Estimation\figure_8.jpg
  Figure 8 caption: A sample image pair of EPFL Castle-P19 dataset.
  Figure 9 Link: articels_figures_by_rev_year\2020\An_Efficient_Solution_to_NonMinimal_Case_Essential_Matrix_Estimation\figure_9.jpg
  Figure 9 caption: Relative pose accuracy of the EPFL Castle-P19 dataset.
  First author gender probability: 0.78
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.78
  Name of the first author: Ji Zhao
  Name of the last author: Ji Zhao
  Number of Figures: 14
  Number of Tables: 2
  Number of authors: 1
  Paper title: An Efficient Solution to Non-Minimal Case Essential Matrix Estimation
  Publication Date: 2020-10-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Efficiency Comparison With Other Globally Optimal Methods
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 SDP Formulations Comparison
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3030161
- Affiliation of the first author: state key laboratory of virtual reality technology
    and systems, school of computer science and engineering, beihang university, beijing,
    china
  Affiliation of the last author: state key laboratory of virtual reality technology
    and systems, school of computer science and engineering, beihang university, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2020\First_And_ThirdPerson_Video_CoAnalysis_By_Learning_SpatialTemporal_Joint_Attenti\figure_1.jpg
  Figure 1 caption: The motivation of this work. (a) First- and third-person videos
    are captured simultaneously by a wearable camera and a remote camera. Joint attentions
    are defined as the corresponding attention regions (Shared Regions of Attentions,
    Shared ROAs) across two viewpoints, where ROA-3 indicates the ROA in third-person
    video and ROA-1 indicates that in first-person video. (b) The goal of this work
    is to find the joint attention regions automatically from two views by utilizing
    the spatial-temporal constraints on ROAs. The joint attention helps extract shared
    representations between two views, which benefits a range of firstthirdmixed view-based
    applications.
  Figure 10 Link: articels_figures_by_rev_year\2020\First_And_ThirdPerson_Video_CoAnalysis_By_Learning_SpatialTemporal_Joint_Attenti\figure_10.jpg
  Figure 10 caption: Visualization of the predicted ROAs with T-JANet on unseen 1st-3rd
    dataset [8] (a) and ActivityNet dataset [49] (b). The top row shows the third-person
    view and the bottom row shows the first-person view.
  Figure 2 Link: articels_figures_by_rev_year\2020\First_And_ThirdPerson_Video_CoAnalysis_By_Learning_SpatialTemporal_Joint_Attenti\figure_2.jpg
  Figure 2 caption: Illustration of the proposed joint attention in the form of shared
    Regions Of Attention (ROAs). (a) The shared ROAs can locate the same hand regions
    in both the first- and third-person video frames. This is of great importance
    for the subsequent shared representation learning. (b) The shared ROAs can be
    quite different from traditional ROI (Region Of Interest) since they must be in
    consensus across views.
  Figure 3 Link: articels_figures_by_rev_year\2020\First_And_ThirdPerson_Video_CoAnalysis_By_Learning_SpatialTemporal_Joint_Attenti\figure_3.jpg
  Figure 3 caption: 'Temporal constraints are proposed to solve the following problems.
    (a) ROA misalignment: the ROA of the third frame of the first-person video is
    inconsistent with its adjacent frames. (b) ROA out-of-view: the ROAs of the thirdfourth
    frames of the first-person video are completelypartially missing.'
  Figure 4 Link: articels_figures_by_rev_year\2020\First_And_ThirdPerson_Video_CoAnalysis_By_Learning_SpatialTemporal_Joint_Attenti\figure_4.jpg
  Figure 4 caption: Comparison of different shared representation learning strategies.
    (a) AONet [4] learns the shared representation by extracting features directly
    from CNNs. (b) Our JANet extracts joint attention to guide the shared representation
    learning. The two learning processes benefit each other simultaneously. (c) Our
    T-JANet further adopts RNNs to learn the joint attentionshared representation
    with temporal constraint.
  Figure 5 Link: articels_figures_by_rev_year\2020\First_And_ThirdPerson_Video_CoAnalysis_By_Learning_SpatialTemporal_Joint_Attenti\figure_5.jpg
  Figure 5 caption: "The architecture of our JANet. Given a triple of frames (x,y,z)\
    \ as input, the attention-guided representation module is developed to extract\
    \ channel attention vectors ( M x c , M y c , M z c ) and feature representations\
    \ ( F x c \u2032 , F y c \u2032 , F z c \u2032 ) from the intermediate feature\
    \ maps ( F x c , F y c , F z c ) . The joint attention learning module encourages\
    \ similarity of channel attention vectors between a third-person frame M y c and\
    \ the corresponding first-person frame M x c . The shared representation learning\
    \ module explores the common information between two viewpoints to obtain the\
    \ shared representation. The attention model is represented by \u201CAM\u201D\
    . We conduct the weighted average on feature maps F c based on the channel attention\
    \ vectors M c to generate spatial attention maps. The visualizations are obtained\
    \ by overlapping the spatial attention maps with original frames and are shown\
    \ on the right."
  Figure 6 Link: articels_figures_by_rev_year\2020\First_And_ThirdPerson_Video_CoAnalysis_By_Learning_SpatialTemporal_Joint_Attenti\figure_6.jpg
  Figure 6 caption: Illustration of the temporally extended component in T-JANet.
    (a) We extract channel attention vectors ( M x t c , M y t c ) and feature representations
    following the same setting of JANet. (b) A LSTM network is adopted to capture
    the temporal information of channel attention vectors. Temporal attention loss
    prompts similarity of the LSTM outputs between third-person frame and corresponding
    first-person frame to learn the joint attention temporally. Temporal fusion is
    adopted to incorporate feature representations from previous frame.
  Figure 7 Link: articels_figures_by_rev_year\2020\First_And_ThirdPerson_Video_CoAnalysis_By_Learning_SpatialTemporal_Joint_Attenti\figure_7.jpg
  Figure 7 caption: Examples of video pairs from Charades-Ego dataset. The example
    on the top demonstrates a video pairs which are recorded from both third- and
    first-person viewpoints. The example on the bottom demonstrates a video pairs
    which are recorded from two slightly different third-person viewpoints. Our aim
    is to learn shared representation between first- and third-person viewpoints.
    Thus the example on the bottom is invalid for our research.
  Figure 8 Link: articels_figures_by_rev_year\2020\First_And_ThirdPerson_Video_CoAnalysis_By_Learning_SpatialTemporal_Joint_Attenti\figure_8.jpg
  Figure 8 caption: Visualization of the predicted ROAs from AONet [4], our baseline
    of Without SA and our JANet. We demonstrate four groups of image pairs, each containing
    frames from the first- and third-person viewpoints respectively.
  Figure 9 Link: articels_figures_by_rev_year\2020\First_And_ThirdPerson_Video_CoAnalysis_By_Learning_SpatialTemporal_Joint_Attenti\figure_9.jpg
  Figure 9 caption: 'Visualization of predicted ROAs from our JANet (a) and T-JANet
    (b). We presented video actions with predicted ROAs in both first- and third-person
    viewpoints (from top to bottom: third-person perspective and corresponding first-person
    perspective). ROAs are visualized with heatmaps on input images. The color ranges
    from blue to red, showing low to high attention.'
  First author gender probability: 0.53
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Huangyue Yu
  Name of the last author: Feng Lu
  Number of Figures: 18
  Number of Tables: 5
  Number of authors: 4
  Paper title: First- And Third-Person Video Co-Analysis By Learning Spatial-Temporal
    Joint Attention
  Publication Date: 2020-10-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Results of JANet for Algorithmic Evaluation
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Results of T-JANet for Algorithmic Evaluation
  Table 3 caption:
    table_text: TABLE 3 Quantitative Results of Gaze Prediction and Image Co-Segmentation
  Table 4 caption:
    table_text: TABLE 4 Quantitative Comparison With State-of-the-Art Video Summarization
      Methods on Charades-Ego Dataset
  Table 5 caption:
    table_text: TABLE 5 Quantitative Results of Zero-Shot Action Recognition
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3030048
- Affiliation of the first author: university of modena and reggio emilia, modena,
    italy
  Affiliation of the last author: university of modena and reggio emilia, modena,
    italy
  Figure 1 Link: articels_figures_by_rev_year\2020\Warp_and_Learn_Novel_Views_Generation_for_Vehicles_and_Other_Objects\figure_1.jpg
  Figure 1 caption: We propose a semi-parametric framework to generate realistic novel
    views of a vehicle and or to transfer its appearance to different models.
  Figure 10 Link: articels_figures_by_rev_year\2020\Warp_and_Learn_Novel_Views_Generation_for_Vehicles_and_Other_Objects\figure_10.jpg
  Figure 10 caption: Viewpoints distributions for real (a) and synthetic data (b)
    of the car class in Pascal3D+. Radii have been normalised to unit length for clearness.
    In red viewpoints with elevation lesser or equal than fracpi 8 rad.
  Figure 2 Link: articels_figures_by_rev_year\2020\Warp_and_Learn_Novel_Views_Generation_for_Vehicles_and_Other_Objects\figure_2.jpg
  Figure 2 caption: We model a rigid object with a small of piece-wise planar patches,
    whose vertices are defined by 2D keypoints. We also include a small central crop
    as appearance prior to carry low-frequency information.
  Figure 3 Link: articels_figures_by_rev_year\2020\Warp_and_Learn_Novel_Views_Generation_for_Vehicles_and_Other_Objects\figure_3.jpg
  Figure 3 caption: Model architecture overview. Approximately planar patches are
    extracted from the 2D keypoints locations. The Image Completion Network (ICN)
    uses the synthetic 2.5D sketches as templates to reconstruct objects appearance
    from the patches in a self-supervised fashion. During training, input patches
    are warped forth and back to a randomly sampled viewpoint to enforce resilience
    against homography issues that are likely to be encountered at test time. During
    inference, novel views of the input object are synthesised by providing the ICN
    a novel viewpoint and a (possibly different) rendered 3D model to be used as shape
    guideline.
  Figure 4 Link: articels_figures_by_rev_year\2020\Warp_and_Learn_Novel_Views_Generation_for_Vehicles_and_Other_Objects\figure_4.jpg
  Figure 4 caption: "Results of 360\xB0 rotation. Our output is consistent for the\
    \ whole rotation circle. Best viewed zoomed on screen."
  Figure 5 Link: articels_figures_by_rev_year\2020\Warp_and_Learn_Novel_Views_Generation_for_Vehicles_and_Other_Objects\figure_5.jpg
  Figure 5 caption: Comparison with ablated versions of the proposed method on Pascal3D+
    test set. Better viewed zoomed on screen. Please refer to Section 4.1 for details.
  Figure 6 Link: articels_figures_by_rev_year\2020\Warp_and_Learn_Novel_Views_Generation_for_Vehicles_and_Other_Objects\figure_6.jpg
  Figure 6 caption: Visual results comparison with competitors on Pascal3D+ test set.
    Better viewed zoomed on screen. Please refer to Section 4.1 for details.
  Figure 7 Link: articels_figures_by_rev_year\2020\Warp_and_Learn_Novel_Views_Generation_for_Vehicles_and_Other_Objects\figure_7.jpg
  Figure 7 caption: Predictions of our model from different viewpoints. The geometry-aware
    design of our semi-parametric method allows the model to be resilient to large
    viewpoint variations, including rotation, elevation, and camera distance.
  Figure 8 Link: articels_figures_by_rev_year\2020\Warp_and_Learn_Novel_Views_Generation_for_Vehicles_and_Other_Objects\figure_8.jpg
  Figure 8 caption: Results of time-limited AB preference test against real images.
    Both VON and our method are resilient to human judgement over time. Green line
    denotes random chance. Please refer to Section 4.3 for details.
  Figure 9 Link: articels_figures_by_rev_year\2020\Warp_and_Learn_Novel_Views_Generation_for_Vehicles_and_Other_Objects\figure_9.jpg
  Figure 9 caption: Visual comparison showing the effect of adding synthetic data
    to Pascal3D+ training set. The ICN network trained on the mixture of the two domains
    performs significantly better under extreme viewpoint transformations. Please
    see supplementary material for more examples.
  First author gender probability: 0.82
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Andrea Palazzi
  Name of the last author: Rita Cucchiara
  Number of Figures: 13
  Number of Tables: 3
  Number of authors: 4
  Paper title: 'Warp and Learn: Novel Views Generation for Vehicles and Other Objects'
  Publication Date: 2020-10-13 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Fr\xC3\xA9chet Inception Distances [19] Results for Car"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Blind Randomized AB Test Results
  Table 3 caption:
    table_text: "TABLE 3 Fr\xC3\xA9chet Inception Distances [19] Results for Chair\
      \ Class"
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3030701
- Affiliation of the first author: department of statistics and the department of
    computer science, university of georgia, athens, ga, usa
  Affiliation of the last author: department of computer science and the institute
    for artificial intelligence, university of georgia, athens, ga, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\CoEmbedding_of_Nodes_and_Edges_With_Graph_Neural_Networks\figure_1.jpg
  Figure 1 caption: Illustrative depiction of CensNet for node classification. The
    upper (orange color) components are convolution operations on node adjacency matrix
    and node features, while the lower (green color) components are the corresponding
    line graph convolution. Two types of layers as a combo, (1). Node Layer, update
    the node embedding with node and edge embedding from the previous layer, and (2).
    Edge Layer, update the edge embedding with the edge and node embedded features
    from the preceding layer. The response is n -dimensional categorical variable
    where we assume there are n nodes in the graph.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\CoEmbedding_of_Nodes_and_Edges_With_Graph_Neural_Networks\figure_2.jpg
  Figure 2 caption: Illustrative depiction of CensNet for graph classification and
    regression. Assume there are q graphs, the upper (orange color) components are
    convolution operations on q node adjacency matrix and q node features, while the
    lower (green color) components are the corresponding q line graph convolutions.
    The response is multi-dimensional categorical variables for multi-task graph classification,
    univariate continuous variable for graph regression.
  Figure 3 Link: articels_figures_by_rev_year\2020\CoEmbedding_of_Nodes_and_Edges_With_Graph_Neural_Networks\figure_3.jpg
  Figure 3 caption: Illustration of CensNet-VAE for unsupervised link prediction.
    We use the edge adjacency matrix to update the node feature embedding, which is
    the input for the encoder, and then pass the embedding layer generated from encoder
    to decoder. Finally, embeddings are used to reconstruct the graph.
  Figure 4 Link: articels_figures_by_rev_year\2020\CoEmbedding_of_Nodes_and_Edges_With_Graph_Neural_Networks\figure_4.jpg
  Figure 4 caption: AUCRMSE in validation set for Tox21Lipophilicity. The name of
    each curve is formed with algorithm name and label ratio in training set. For
    example, CensNet90 means the CensNet algorithm with 90 percent data in training
    set.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.94
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.76
  Name of the first author: Xiaodong Jiang
  Name of the last author: Sheng Li
  Number of Figures: 4
  Number of Tables: 3
  Number of authors: 4
  Paper title: Co-Embedding of Nodes and Edges With Graph Neural Networks
  Publication Date: 2020-10-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Node Classification Accuracy (in Percent) on Three Citation
      Graph Data Sets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Experimental Results on Tox21 and Lipophilicity Data Sets
  Table 3 caption:
    table_text: TABLE 3 Link Prediction AUC and AP (in Percent) on Three Citation
      Graph Data Sets
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3029762
- Affiliation of the first author: university of oxford, oxford, u.k.
  Affiliation of the last author: university of oxford, oxford, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2020\Adversarial_Metric_Attack_and_Defense_for_Person_ReIdentification\figure_1.jpg
  Figure 1 caption: The illustration of the adversarial effect in person re-identification.
    Given a probe image, its similarity with the true positive is decreased from 0.829
    to 0.105, and that with the true negative is increased from 0.120 to 0.803 by
    adding human-imperceptible noise to gallery images. The adversarial noise is resized
    to the range [0,1] for visualization.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Adversarial_Metric_Attack_and_Defense_for_Person_ReIdentification\figure_2.jpg
  Figure 2 caption: The failure case (a) of classification attack and the successful
    case (b) of metric attack on two clean images (blue color). The adversarial examples
    (red color) generated by the classification attack cross over the class decision
    boundary, but preserve the pairwise distance between them to a large extent.
  Figure 3 Link: articels_figures_by_rev_year\2020\Adversarial_Metric_Attack_and_Defense_for_Person_ReIdentification\figure_3.jpg
  Figure 3 caption: The ratio of mAP on adversarial examples to that on clean images
    on the DukeMTMC-reID dataset.
  Figure 4 Link: articels_figures_by_rev_year\2020\Adversarial_Metric_Attack_and_Defense_for_Person_ReIdentification\figure_4.jpg
  Figure 4 caption: Two representative ranking lists of two probe images for non-targeted
    attack (a) and targeted attack (b). We mark the ranking position of each gallery
    image on its top and do not elaborately exclude the distractor images and those
    captured by the same camera as the probe. The gallery images with proper ranking
    positions (i.e., true positives and true negatives) are marked in blue, otherwise
    in red.
  Figure 5 Link: articels_figures_by_rev_year\2020\Adversarial_Metric_Attack_and_Defense_for_Person_ReIdentification\figure_5.jpg
  Figure 5 caption: "The mAP comparison of FGSM (a) and I-FGSM (b) by varying the\
    \ maximum magnitude of adversarial perturbation \u03F5 and a selection of distance\
    \ metric. In the legend, the part before symbol \u201C\u201D denotes the metric\
    \ loss used for metric attack and the part after \u201C\u201D denotes the metric\
    \ used to evaluate the performance."
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.67
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Song Bai
  Name of the last author: Philip H.S. Torr
  Number of Figures: 5
  Number of Tables: 5
  Number of authors: 5
  Paper title: Adversarial Metric Attack and Defense for Person Re-Identification
  Publication Date: 2020-10-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance of the Four Base Models and Two State-of-the-Art
      Methods Implemented in This Work
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 The mAP Comparison of White-Box Attack (in shadow) and Black-Box\
      \ Attack (others) When \u03F5=5 \u03B5=5 on the Market-1501 Dataset"
  Table 3 caption:
    table_text: "TABLE 3 The mAP Comparison of White-Box Attack (in shadow) and Black-Box\
      \ Attack (others) When \u03F5=5 \u03B5=5 on the DukeMTMC-reID Dataset"
  Table 4 caption:
    table_text: "TABLE 4 The mAP Comparison of Multi-Model Attack (white-box in shadow)\
      \ When \u03F5=5 \u03B5=5"
  Table 5 caption:
    table_text: TABLE 5 The mAP Comparison Between Normally Trained Models (denoted
      by N) and Metric-Preserving Models (denoted by M) on the Market-1501 Dataset
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3031625
- Affiliation of the first author: "chalmers university of technology, g\xF6teborg,\
    \ sweden"
  Affiliation of the last author: "chalmers university of technology, g\xF6teborg,\
    \ sweden"
  Figure 1 Link: articels_figures_by_rev_year\2020\LongTerm_Visual_Localization_Revisited\figure_1.jpg
  Figure 1 caption: Visual localization in changing urban conditions. We present three
    new datasets, Aachen Day-Night, RobotCar Seasons (shown) and Extended CMU Seasons
    for evaluating 6DOF localization against a prior 3D map (top) using registered
    query images taken from a wide variety of conditions (bottom), including day-night
    variation, weather, and seasonal changes over long periods of time.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\LongTerm_Visual_Localization_Revisited\figure_2.jpg
  Figure 2 caption: Example query images for Aachen Day-Night (top), RobotCar Seasons
    (middle) and the Extended CMU Seasons (bottom) datasets.
  Figure 3 Link: articels_figures_by_rev_year\2020\LongTerm_Visual_Localization_Revisited\figure_3.jpg
  Figure 3 caption: 3D models of the Aachen Day-Night (left, showing database (red),
    day-time query (green), and night-time query images (blue)), RobotCar Seasons
    (middle), and Extended CMU Seasons (right) datasets. For RobotCar and CMU, the
    colors encode the individual submaps.
  Figure 4 Link: articels_figures_by_rev_year\2020\LongTerm_Visual_Localization_Revisited\figure_4.jpg
  Figure 4 caption: Performance on the three datasets of some of the top performing
    methods. Results are shown for the three precision regimes fine, medium and coarse
    (see the individual tables for details on these). The CityScaleLocalization and
    DenseVLAD methods are included in the figures as well, as representatives of the
    structure-based and image-retrieval based methods, respectively.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Carl Toft
  Name of the last author: Torsten Sattler
  Number of Figures: 4
  Number of Tables: 7
  Number of authors: 12
  Paper title: Long-Term Visual Localization Revisited
  Publication Date: 2020-10-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison With Existing Benchmarks for Place Recognition
      and Visual Localization
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Detailed Statistics for the Three Benchmark Datasets Proposed
      in This Paper
  Table 3 caption:
    table_text: TABLE 3 Performance of State-of-the-Art Methods on the Aachen Day-Night
      Dataset
  Table 4 caption:
    table_text: TABLE 4 Performance of State-of-the-Art Methods on the RobotCar Seasons
      Dataset
  Table 5 caption:
    table_text: TABLE 5 Using Multiple Images for Pose Estimation (ActiveSeach+GC)
      on the RobotCar Seasons Dataset
  Table 6 caption:
    table_text: TABLE 6 Using Location Priors to Query Only Submodels Rather Than
      the Full RobotCar Seasons Dataset for Night-Time Queries
  Table 7 caption:
    table_text: TABLE 7 Performance of State-of-the-Art Methods on the Extended CMU
      Seasons Dataset
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3032010
- Affiliation of the first author: futurewei technologies, bellevue, wa, usa
  Affiliation of the last author: department of computer science, university of rochester,
    rochester, ny, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Small_Data_Challenges_in_Big_Data_Era_A_Survey_of_Recent_Progress_on_Unsupervise\figure_1.jpg
  Figure 1 caption: An overview of the landscape of unsupervised and semi-supervised
    methods. This figure shows the relations between different methods, and where
    they intersect with each other. Please refer to Fig. C.3, available in the online
    supplemental material for the categorization of these methods for this survey.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Small_Data_Challenges_in_Big_Data_Era_A_Survey_of_Recent_Progress_on_Unsupervise\figure_2.jpg
  Figure 2 caption: Future directions for (a) unifying transformation and instance
    equivariant representation learning, (b) supervised versus unsupervised data augmentations,
    and (c) self-supervised learning as a regularizer.
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.75
  Name of the first author: Guo-Jun Qi
  Name of the last author: Jiebo Luo
  Number of Figures: 2
  Number of Tables: 1
  Number of authors: 2
  Paper title: 'Small Data Challenges in Big Data Era: A Survey of Recent Progress
    on Unsupervised and Semi-Supervised Methods'
  Publication Date: 2020-10-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Error Rates on CIFAR-10 When Different Numbers of Labeled
      Examples Per Class are Used to Train the Supervised, the Semi-Supervised and
      the Downstream Classifiers for Unsupervised Representations
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3031898
- Affiliation of the first author: school of data and computer science, sun yat-sen
    university, guangzhou, china
  Affiliation of the last author: school of data and computer science, sun yat-sen
    university, guangzhou, china
  Figure 1 Link: articels_figures_by_rev_year\2020\A_Hamiltonian_Monte_Carlo_Method_for_Probabilistic_Adversarial_Attack_and_Learni\figure_1.jpg
  Figure 1 caption: Iterative Deterministic Generator versus Stochastic MCMC-based
    Generator. We choose a natural image to generate 500 adversarial examples and
    visualize these samples by t-SNE [8]. In contrast to two typical iterative deterministic
    methods (PGD [9] with 500 random restarts and MI-FGSM [10] selecting samples at
    the final 500 iterations), MCMC-based method explores the solution space of adversarial
    examples and finds out the decision boundary of target classifier which is easily
    misled to erratic discrimination, then generates multiple diverse adversarial
    examples to attack. It is clear that our method automatically generates all of
    the 500 samples for a certain category in the untargeted attack scenario. When
    gradually increasing the number of MCMC sampling, the generated sequence of adversarial
    examples and their corresponding frequencies collectively depict the true underlying
    distribution of adversarial examples.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\A_Hamiltonian_Monte_Carlo_Method_for_Probabilistic_Adversarial_Attack_and_Learni\figure_2.jpg
  Figure 2 caption: "The success rates of M-PGD (left) and HMCAM (right) on CIFAR10\
    \ over the first 10 iterations, with \u03B5=2255 . Solid lines represent the white-box\
    \ attacks and dashed lines represent the black-box attacks. \u201C A\u2192B \u201D\
    \ means that model B is attacked by adversarial examples generated by model A."
  Figure 3 Link: articels_figures_by_rev_year\2020\A_Hamiltonian_Monte_Carlo_Method_for_Probabilistic_Adversarial_Attack_and_Learni\figure_3.jpg
  Figure 3 caption: "The success rates of M-PGD (left) and HMCAM (right) on CIFAR10\
    \ after 100 iterations, with \u03B5=2255 . Solid lines represent the white-box\
    \ attacks and dashed lines represent the black-box attacks. \u201C A\u2192B \u201D\
    \ means that model B is attacked by adversarial examples generating by model A."
  Figure 4 Link: articels_figures_by_rev_year\2020\A_Hamiltonian_Monte_Carlo_Method_for_Probabilistic_Adversarial_Attack_and_Learni\figure_4.jpg
  Figure 4 caption: "Comparison with different adversarial training methods on CIFAR10.\
    \ We use M-PGD as the attacker and report its success rate, with \u03B5=2255 .\
    \ Our HMCAM can use two orders of magnitude fewer samples than other methods to\
    \ simulate the target distribution."
  Figure 5 Link: articels_figures_by_rev_year\2020\A_Hamiltonian_Monte_Carlo_Method_for_Probabilistic_Adversarial_Attack_and_Learni\figure_5.jpg
  Figure 5 caption: Comparison with different adversarial training methods on both
    clean accuracy and robust accuracy (against PGD-10 with varepsilon =8255 ) of
    Wide ResNet34 on CIFAR10 at every iteration.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Hongjun Wang
  Name of the last author: Liang Lin
  Number of Figures: 5
  Number of Tables: 9
  Number of authors: 4
  Paper title: A Hamiltonian Monte Carlo Method for Probabilistic Adversarial Attack
    and Learning
  Publication Date: 2020-10-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Relationship Between HMC and the Family of Fast Gradient Sign
      Methods
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Success Rates of Several of Non-Targeted Attacks Against
      a Single Network on CIFAR10
  Table 3 caption:
    table_text: TABLE 3 The Success Rates of Several of Non-Targeted Attacks Against
      an Ensemble of Networks on CIFAR10
  Table 4 caption:
    table_text: TABLE 4 Validation Accuracy and Robustness of Preact-ResNet18 on CIFAR10
  Table 5 caption:
    table_text: TABLE 5 Validation Accuracy and Robustness of Wide ResNet34 on CIFAR10
  Table 6 caption:
    table_text: TABLE 6 Validation Accuracy and Robustness of ResNet50 on ImageNet
  Table 7 caption:
    table_text: TABLE 7 Validation Accuracy and Robustness of a small CNN on MNIST
  Table 8 caption:
    table_text: TABLE 8 The Success Rates of Targeted White-Box Attacks on ImageNet
  Table 9 caption:
    table_text: TABLE 9 The Results of Our Targeted Attack on the Real-World Celebrity
      Recognition APIs in Clarifai, AWS and Azure
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3032061
- Affiliation of the first author: department of computer science and engineering,
    dankook university, yongin, republic of korea
  Affiliation of the last author: institute of computer science, heidelberg university,
    heidelberg, germany
  Figure 1 Link: articels_figures_by_rev_year\2020\DeepNCDeepNC_Deep_Generative_Network_Completion\figure_1.jpg
  Figure 1 caption: The schematic overview of our DeepNC method.
  Figure 10 Link: articels_figures_by_rev_year\2020\DeepNCDeepNC_Deep_Generative_Network_Completion\figure_10.jpg
  Figure 10 caption: The computational complexity of sf DeepNC-EM , where the log-log
    plot of the execution time versus |VO| is shown.
  Figure 2 Link: articels_figures_by_rev_year\2020\DeepNCDeepNC_Deep_Generative_Network_Completion\figure_2.jpg
  Figure 2 caption: "An example illustrating the inference process of GraphRNN-S.\
    \ Here, the blue arrows denote the graph-level RNN that encodes the \u201Cgraph\
    \ state\u201D vector h i in its hidden state, and the red and black arrows represent\
    \ the edge generation process whose input is given by the graph-level RNN."
  Figure 3 Link: articels_figures_by_rev_year\2020\DeepNCDeepNC_Deep_Generative_Network_Completion\figure_3.jpg
  Figure 3 caption: "An example illustrating the schematic overview of our DeepNC\
    \ method, where three nodes (i.e., A, B, and C) and two edges with solid lines\
    \ are observable instead of the true graph G T consisting of five nodes and all\
    \ associated edges. Both white and orange entries in S \u03C0 are imputed with\
    \ either 0 or 1 while grey entries in S \u03C0 remain unchanged."
  Figure 4 Link: articels_figures_by_rev_year\2020\DeepNCDeepNC_Deep_Generative_Network_Completion\figure_4.jpg
  Figure 4 caption: The overall architecture of sf DeepNC algorithms.
  Figure 5 Link: articels_figures_by_rev_year\2020\DeepNCDeepNC_Deep_Generative_Network_Completion\figure_5.jpg
  Figure 5 caption: An illustration of the mechanism of sf DeepNC-L . The first three
    steps are shown as an example.
  Figure 6 Link: articels_figures_by_rev_year\2020\DeepNCDeepNC_Deep_Generative_Network_Completion\figure_6.jpg
  Figure 6 caption: An example illustrating the fifth inference step of sf DeepNC-L
    , where nodes M 1 , A, B, and E have been generated sequentially.
  Figure 7 Link: articels_figures_by_rev_year\2020\DeepNCDeepNC_Deep_Generative_Network_Completion\figure_7.jpg
  Figure 7 caption: GED of sf DeepNC-EM over the number of EM iterations. Here, the
    performance of sf DeepNC-L corresponds to the case where the number of EM iterations
    is zero.
  Figure 8 Link: articels_figures_by_rev_year\2020\DeepNCDeepNC_Deep_Generative_Network_Completion\figure_8.jpg
  Figure 8 caption: Performance comparison in terms of GED (the lower the better),
    where the degree of observability in training graphs is set to lbrace 95, 90rbrace
    %.
  Figure 9 Link: articels_figures_by_rev_year\2020\DeepNCDeepNC_Deep_Generative_Network_Completion\figure_9.jpg
  Figure 9 caption: Performance comparison in terms of GED (the lower the better),
    where the degree of missingness in edges between nodes in GO is set to lbrace
    10,15,20rbrace % .
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Cong Tran
  Name of the last author: Michael Gertz
  Number of Figures: 10
  Number of Tables: 5
  Number of authors: 4
  Paper title: 'DeepNC

    DeepNC: Deep Generative Network Completion'
  Publication Date: 2020-10-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of Notations
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Summary of Deep Generative Models of Graphs
  Table 3 caption:
    table_text: TABLE 3 Statistics of 5 Datasets, Where NG and NN Denote the Number
      of Similar Graphs and the Range of the Number of Nodes in Each Dataset, Respectively,
      Including Training Graphs G I GI and a Test Graph G T GT
  Table 4 caption:
    table_text: "TABLE 4 Performance Comparison in Terms of Graph Edit Distance (Average\
      \ \xB1 \xB1 Standard Deviation)"
  Table 5 caption:
    table_text: "TABLE 5 Performance Comparison in Terms of Graph Edit Distance When\
      \ 70 percent of Nodes are Missing (average \xB1 \xB1 standard deviation)"
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3032286
