- Affiliation of the first author: department of computer science, university of york,
    heslington, york, united kingdom
  Affiliation of the last author: department of computer science, university of york,
    heslington, york, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2015\Generative_Graph_Prototypes_from_Information_Theory\figure_1.jpg
  Figure 1 caption: 'First row: example images in the COIL data set. Second row: Delaunay
    graphs associated with the COIL images. Third row: example images in the toys
    data set. Fourth row: Delaunay graphs associated with the toy images.'
  Figure 10 Link: articels_figures_by_rev_year\2015\Generative_Graph_Prototypes_from_Information_Theory\figure_10.jpg
  Figure 10 caption: Comparison of the statistics for the Delaunay graphs and their
    sample graphs.
  Figure 2 Link: articels_figures_by_rev_year\2015\Generative_Graph_Prototypes_from_Information_Theory\figure_2.jpg
  Figure 2 caption: 'COIL data set: (a) variation of the complexity of the supergraph,
    encoded as the approximate Von Neumann entropy, during iterations, (b) variation
    of average log-likelihood of the sample graphs during iterations and (c) variation
    of the overall code-length during iterations.'
  Figure 3 Link: articels_figures_by_rev_year\2015\Generative_Graph_Prototypes_from_Information_Theory\figure_3.jpg
  Figure 3 caption: "\u201CToys\u201D data set: (a) variation of the complexity of\
    \ the supergraph, encoded as the approximate Von Neumann entropy, during iterations,\
    \ (b) variation of the average log-likelihood of the sample graphs during iterations\
    \ and (c) variation of the overall code-length during iterations."
  Figure 4 Link: articels_figures_by_rev_year\2015\Generative_Graph_Prototypes_from_Information_Theory\figure_4.jpg
  Figure 4 caption: "\u201CToys\u201D data set: (a) variation of the complexity of\
    \ the supergraph, encoded as the approximate Von Neumann entropy, during iterations,\
    \ (b) variation of the average log-likelihood of the sample graphs during iterations\
    \ and (c) variation of the overall code-length during iterations."
  Figure 5 Link: articels_figures_by_rev_year\2015\Generative_Graph_Prototypes_from_Information_Theory\figure_5.jpg
  Figure 5 caption: 'Two graphs from the mutagenesis data set: (a) an example of a
    mutagenic compound, (b) an example of a non-mutagenic compound.'
  Figure 6 Link: articels_figures_by_rev_year\2015\Generative_Graph_Prototypes_from_Information_Theory\figure_6.jpg
  Figure 6 caption: The two supergraphs generated from the mutagenesis data set. Edges
    are sampled with a threshold of 1.6, (a) the supergraph for the active set, (b)
    the supergraph for the inactive set.
  Figure 7 Link: articels_figures_by_rev_year\2015\Generative_Graph_Prototypes_from_Information_Theory\figure_7.jpg
  Figure 7 caption: 'Comparison of graph clusterings obtained from Jensen-Shannon
    kernel and edit distance. Row 1: cat (red) and pig (blue). Row 2: bottle1 (black)
    and bottle2 (green). Column 1: edit distance and Column 2: Jensen-Shannon kernel.'
  Figure 8 Link: articels_figures_by_rev_year\2015\Generative_Graph_Prototypes_from_Information_Theory\figure_8.jpg
  Figure 8 caption: Comparison of the statistics for the ER graphs and their sample
    graphs.
  Figure 9 Link: articels_figures_by_rev_year\2015\Generative_Graph_Prototypes_from_Information_Theory\figure_9.jpg
  Figure 9 caption: Comparison of the statistics for the BA scale-free graphs and
    their sample graphs.
  First author gender probability: 0.64
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Lin Han
  Name of the last author: Edwin R. Hancock
  Number of Figures: 11
  Number of Tables: 2
  Number of authors: 3
  Paper title: Generative Graph Prototypes from Information Theory
  Publication Date: 2015-02-05 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of the Classification Results
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Dunn Index for the Clusterings Obtained from the Two Different
      Embeddings
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2400451
- Affiliation of the first author: "perception team, inria grenoble rh\xF4ne-alpes,\
    \ 655, avenue de l'europe, 38330 montbonnot saint-martin, france"
  Affiliation of the last author: "perception team, inria grenoble rh\xF4ne-alpes,\
    \ 655, avenue de l'europe, 38330 montbonnot saint-martin, france"
  Figure 1 Link: articels_figures_by_rev_year\2015\Fusion_of_Range_and_Stereo_Data_for_HighResolution_SceneModeling\figure_1.jpg
  Figure 1 caption: The pipeline of the proposed depth-stereo fusion method. The low-resolution
    depth data are projected onto the color data and refined to yield a high-resolution
    sparse disparity map. Starting from these disparity seeds , an upsampling process
    provides an initial HR dense disparity map. Both the HR seeds and the initial
    dense disparity map are then used by the region-growing depth-stereo fusion to
    produce the final HR depth map. A prominent feature of our method is that fusion
    takes place at several data processing stages.
  Figure 10 Link: articels_figures_by_rev_year\2015\Fusion_of_Range_and_Stereo_Data_for_HighResolution_SceneModeling\figure_10.jpg
  Figure 10 caption: The BMP error of fusion algorithms averaged over the eight test
    images as a function of the error tolerance delta .
  Figure 2 Link: articels_figures_by_rev_year\2015\Fusion_of_Range_and_Stereo_Data_for_HighResolution_SceneModeling\figure_2.jpg
  Figure 2 caption: (Best viewed on screen) (a) The mapping of depth data onto the
    left image causes artifacts in the presence of depth discontinuities. A cascade
    of (b) geometry-consistency and (c) color-consistency filters refines the sparse
    disparity map. Depth values are color-coded from red (close) to blue (far).
  Figure 3 Link: articels_figures_by_rev_year\2015\Fusion_of_Range_and_Stereo_Data_for_HighResolution_SceneModeling\figure_3.jpg
  Figure 3 caption: (Best viewed on screen) Depth upsampling results using (a) triangulation-based
    interpolation [15] after cutting big triangles, (b) joint bilateral filter [34]
    and (c) our method. The depth values are color-coded from red (close) to blue
    (far), while black areas correspond to non-available values. The white edges in
    close-ups show the color edges of the image.
  Figure 4 Link: articels_figures_by_rev_year\2015\Fusion_of_Range_and_Stereo_Data_for_HighResolution_SceneModeling\figure_4.jpg
  Figure 4 caption: "The window split for the color-consistency filter. The pixel\
    \ p is linked with the closest (shaded) sub-window in terms of the color consistency\
    \ (links represent color distances from I p\u2193 to med( I W i ) )."
  Figure 5 Link: articels_figures_by_rev_year\2015\Fusion_of_Range_and_Stereo_Data_for_HighResolution_SceneModeling\figure_5.jpg
  Figure 5 caption: The principal graph that is iteratively considered in our region-growing
    fusion method.
  Figure 6 Link: articels_figures_by_rev_year\2015\Fusion_of_Range_and_Stereo_Data_for_HighResolution_SceneModeling\figure_6.jpg
  Figure 6 caption: An example of the probability distribution of dp constrained by
    different observations.
  Figure 7 Link: articels_figures_by_rev_year\2015\Fusion_of_Range_and_Stereo_Data_for_HighResolution_SceneModeling\figure_7.jpg
  Figure 7 caption: (a) The validity of ordering constraint implies Omega DOsubset
    Omega SO (depth camera is mounted between the color cameras); the shorter the
    baseline is, the more coincident these regions are. (b) Omega DO and Omega SO
    do not overlap when the scene contains very thin foreground objects and the ordering
    constraint is invalid. In (c), an estimation of Omega SO , Omega DO for the example
    of Fig. 2 is shown. Points that have been removed during the refinement step (
    Section 4.1) are also marked as depth-occlusions, while outliers in the initial
    maps can produce false positives for Omega SO , e.g., the red points on the right
    arm of the person.
  Figure 8 Link: articels_figures_by_rev_year\2015\Fusion_of_Range_and_Stereo_Data_for_HighResolution_SceneModeling\figure_8.jpg
  Figure 8 caption: Dependency network for (a) WTA and (b) MRF models in an 1D example.
    Initial and two final candidate graphs for the proposed scheme are shown in (c),
    (d) and (e) respectively.
  Figure 9 Link: articels_figures_by_rev_year\2015\Fusion_of_Range_and_Stereo_Data_for_HighResolution_SceneModeling\figure_9.jpg
  Figure 9 caption: Left image with superimposed depth map (left) and BMP curves (right)
    for the HR stereo pair Lampshade1 . The lower curves show the robustness of the
    proposed schemes to the noise of the initial map.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.95
  Name of the first author: Georgios D. Evangelidis
  Name of the last author: Radu Horaud
  Number of Figures: 13
  Number of Tables: 7
  Number of authors: 3
  Paper title: Fusion of Range and Stereo Data for High-Resolution Scene-Modeling
  Publication Date: 2015-02-05 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Main Mathematical Notations Used in the Paper
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Properties of Inference Algorithms in Stereo andor Depth-Stereo
      Fusion
  Table 3 caption:
    table_text: TABLE 3 Contribution of Various Modules of the Proposed Algorithm
      (BMP Error Averaged over Eight LR Images)
  Table 4 caption:
    table_text: "TABLE 4 BMP for High-Resolution Disparity Maps of the Middlebury\
      \ Dataset with \u03B4=1"
  Table 5 caption:
    table_text: TABLE 5 Average Execution Times of Algorithms for the HR Middlebury
      Data-Set [50] in a 2.6 GHz Machine
  Table 6 caption:
    table_text: TABLE 6 Evaluation on the Dataset of Dal Mutto et al. [18]
  Table 7 caption:
    table_text: TABLE 7 Evaluation on HCIBox Dataset et al. [17]
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2400465
- Affiliation of the first author: school of computer science and engineering, nanjing
    university of science and technology, nanjing, p.r. china
  Affiliation of the last author: national laboratory of pattern recognition, institute
    of automation, chinese academy of sciences, beijing, p.r. china
  Figure 1 Link: articels_figures_by_rev_year\2015\Robust_Structured_Subspace_Learning_for_Data_Representation\figure_1.jpg
  Figure 1 caption: Comparison of different algorithms on MIFlickr and NUS-WIDE learning
    data sets with n=5,000 in terms of MAP.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Robust_Structured_Subspace_Learning_for_Data_Representation\figure_2.jpg
  Figure 2 caption: "The performance in terms of F1 by varying the parameters \u03B2\
    \ and \u03B3 on MIFlickr and NUS-WIDE learning data sets with n=5,000 ."
  Figure 3 Link: articels_figures_by_rev_year\2015\Robust_Structured_Subspace_Learning_for_Data_Representation\figure_3.jpg
  Figure 3 caption: The performance in terms of precision, recall and F1 by varying
    k to compute L on MIFlickr and NUS-WIDE learning data sets with n=5,000 .
  Figure 4 Link: articels_figures_by_rev_year\2015\Robust_Structured_Subspace_Learning_for_Data_Representation\figure_4.jpg
  Figure 4 caption: Convergence curves of the relative error of the objective function
    value of the proposed algorithm.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Zechao Li
  Name of the last author: Hanqing Lu
  Number of Figures: 4
  Number of Tables: 10
  Number of authors: 4
  Paper title: Robust Structured Subspace Learning for Data Representation
  Publication Date: 2015-02-05 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Statistics of the Data Sets with Image and Tag Counts in the
      Format MeanMaximum
  Table 10 caption:
    table_text: "TABLE 10 Classification Results (AC% \xB1 Std) of Different Algorithms\
      \ for Supervised Classification on USPS and tr11 Datasets"
  Table 2 caption:
    table_text: "TABLE 2 Experimental Results (Mean Microauc \xB1 Standard Deviation,\
      \ Mean Macroauc \xB1 Standard Deviation and Mean F1 \xB1 Standard Deviation)\
      \ on the MIRFlickr Data Set"
  Table 3 caption:
    table_text: "TABLE 3 Experimental Results (Mean Microauc \xB1 Standard Deviation,\
      \ Mean Macroauc \xB1 Standard Deviation and Mean F1 \xB1 Standard Deviation)\
      \ on the NUS-WIDE Data Set"
  Table 4 caption:
    table_text: TABLE 4 Dataset Description
  Table 5 caption:
    table_text: "TABLE 5 Clustering Results (ACC% \xB1 Std) of Different Algorithms\
      \ for Clustering on Different Datasets"
  Table 6 caption:
    table_text: "TABLE 6 Clustering Results (NMI% \xB1 Std) of Different Algorithms\
      \ for Clustering on Different Datasets"
  Table 7 caption:
    table_text: "TABLE 7 Semi-Supervised Classification Results (AC% \xB1 Std) of\
      \ Different Algorithms on UMIST and JAFFE Datasets"
  Table 8 caption:
    table_text: "TABLE 8 Semi-Supervised Classification Results (AC% \xB1 Std) of\
      \ Different Algorithms on USPS and tr11 Datasets"
  Table 9 caption:
    table_text: "TABLE 9 Classification Results (AC% \xB1 Std) of Different Algorithms\
      \ for Supervised Classification on UMIST and JAFFE Datasets"
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2400461
- Affiliation of the first author: school of computer science, university of adelaide,
    adelaide, australia, australian research council centre of excellence for robotic
    vision
  Affiliation of the last author: school of computer science, university of adelaide,
    adelaide, australia, australian research council centre of excellence for robotic
    vision
  Figure 1 Link: articels_figures_by_rev_year\2015\Supervised_Hashing_Using_Graph_Cuts_and_Boosted_Decision_Trees\figure_1.jpg
  Figure 1 caption: Some retrieval examples of our method FastHash on CIFAR10. The
    first column shows query images, and the rest are retrieved images in the database.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Supervised_Hashing_Using_Graph_Cuts_and_Boosted_Decision_Trees\figure_2.jpg
  Figure 2 caption: "Comparison of KSH and our FastHash on all datasets. The number\
    \ after \u201CKSH\u201D is the number of support vectors. Both of our FastHash\
    \ and FastHash-Full significantly outperform KSH."
  Figure 3 Link: articels_figures_by_rev_year\2015\Supervised_Hashing_Using_Graph_Cuts_and_Boosted_Decision_Trees\figure_3.jpg
  Figure 3 caption: Results on high-dimensional codebook features. Our FastHash significantly
    outperform others.
  Figure 4 Link: articels_figures_by_rev_year\2015\Supervised_Hashing_Using_Graph_Cuts_and_Boosted_Decision_Trees\figure_4.jpg
  Figure 4 caption: Comparison of combinations of hash functions and binary inference
    methods. Decision tree hash functions perform much better than linear SVM. The
    proposed Block-GC performs much better than the spectral method.
  Figure 5 Link: articels_figures_by_rev_year\2015\Supervised_Hashing_Using_Graph_Cuts_and_Boosted_Decision_Trees\figure_5.jpg
  Figure 5 caption: Comparison of using different loss functions with decision tree
    hash functions. Using the Hinge loss (FastH-Hinge) achieves the best result.
  Figure 6 Link: articels_figures_by_rev_year\2015\Supervised_Hashing_Using_Graph_Cuts_and_Boosted_Decision_Trees\figure_6.jpg
  Figure 6 caption: Comparison with a few unsupervised hashing methods. Unsupervised
    methods perform poorly for preserving label based similarity. Our FastHash performs
    significantly better.
  Figure 7 Link: articels_figures_by_rev_year\2015\Supervised_Hashing_Using_Graph_Cuts_and_Boosted_Decision_Trees\figure_7.jpg
  Figure 7 caption: The top- 2,000 precision curve on large dataset SUN397 ( 1,024
    bits). Our FastHash performs the best.
  Figure 8 Link: articels_figures_by_rev_year\2015\Supervised_Hashing_Using_Graph_Cuts_and_Boosted_Decision_Trees\figure_8.jpg
  Figure 8 caption: The top- 2,000 precision curve on large dataset ILSVRC2012 ( 128
    bits). Our FastHash outperforms others.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Guosheng Lin
  Name of the last author: Anton van den Hengel
  Number of Figures: 8
  Number of Tables: 10
  Number of authors: 3
  Paper title: Supervised Hashing Using Graph Cuts and Boosted Decision Trees
  Publication Date: 2015-02-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of KSH and Our FastHash
  Table 10 caption:
    table_text: TABLE 10 Image Classification Results on Dataset ILSVRC2012
  Table 2 caption:
    table_text: 'TABLE 2 Results Using Two Types of Features: Low-Dimensional GIST
      Features and the High-Dimensional Codebook Features'
  Table 3 caption:
    table_text: TABLE 3 Results of Methods with Dimension Reduction
  Table 4 caption:
    table_text: TABLE 4 Performance of Our FastHash on More Features ( 22,400 Dimensions)
      and More Bits ( 1,024 Bits)
  Table 5 caption:
    table_text: TABLE 5 Comparison of Spectral Method and the Proposed Block GraphCut
      (Block-GC) for Binary Code Inference
  Table 6 caption:
    table_text: TABLE 6 Comparison of Combinations of Hash Functions and Binary Inference
      Methods
  Table 7 caption:
    table_text: TABLE 7 Comparison of Combinations of Different Loss Functions and
      Hash Functions
  Table 8 caption:
    table_text: TABLE 8 Results on SUN397 Dataset
  Table 9 caption:
    table_text: TABLE 9 Results on Two ImageNet Datasets Using CNN Features
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2404776
- Affiliation of the first author: "faculty of electrical engineering, university\
    \ of ljubljana, tr\u017Ea\u0161ka 25, ljubljana, slovenia"
  Affiliation of the last author: "faculty of electrical engineering, university of\
    \ ljubljana, tr\u017Ea\u0161ka 25, ljubljana, slovenia"
  Figure 1 Link: articels_figures_by_rev_year\2015\Robust_Estimation_of_Unbalanced_Mixture_Models_on_Samples_with_Outliers\figure_1.jpg
  Figure 1 caption: An unbalanced three-component normal mixture model with heterogeneous
    scales and the corresponding color-coded normalized ordering ranks obtained by
    the log-likelihood (left) and the proposed confidence level ordering (right).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Robust_Estimation_of_Unbalanced_Mixture_Models_on_Samples_with_Outliers\figure_2.jpg
  Figure 2 caption: "Test sample consisted of inlying observations (gray dots) of\
    \ three-component normal mixture and a fraction of h=0.1 ( 10 percent) of uniformly\
    \ distributed outliers (gray crosses). The ellipses show the 99 percent confidence\
    \ regions of the mixture components for the initial (black) and true (dashed black)\
    \ mixture parameters, while the colored ellipses show the 99 percent confidence\
    \ regions of the estimated mixture components for different values of the trimming\
    \ fraction \u03B1 ."
  Figure 3 Link: articels_figures_by_rev_year\2015\Robust_Estimation_of_Unbalanced_Mixture_Models_on_Samples_with_Outliers\figure_3.jpg
  Figure 3 caption: "Plots of misclassification ratio (MCR) for the mixtures estimated\
    \ by the FAST-TLE (dashed blue) and the proposed methods (red) that at different\
    \ trimming fractions \u03B1 on the test samples in Fig. 2 with uniformly distributed\
    \ outliers."
  Figure 4 Link: articels_figures_by_rev_year\2015\Robust_Estimation_of_Unbalanced_Mixture_Models_on_Samples_with_Outliers\figure_4.jpg
  Figure 4 caption: Median misclassification ratio (mMCR) on test samples with uniformly
    distributed outliers for the mixtures estimated by the FAST-TLE (left) and the
    proposed methods (right).
  Figure 5 Link: articels_figures_by_rev_year\2015\Robust_Estimation_of_Unbalanced_Mixture_Models_on_Samples_with_Outliers\figure_5.jpg
  Figure 5 caption: Median misclassification ratio (mMCR) on test samples of synthetic
    mixtures with different values of the minimal mixture weight. Mixtures were estimated
    by the FAST-TLE (left) and the proposed methods ( right) for different trimming
    fractions alpha .
  Figure 6 Link: articels_figures_by_rev_year\2015\Robust_Estimation_of_Unbalanced_Mixture_Models_on_Samples_with_Outliers\figure_6.jpg
  Figure 6 caption: "Top row from left to right: A set of MR images, i.e., ground\
    \ truth segmentation of the brain structures and tumor, tumor infiltration map,\
    \ T1-, T2-weighted and FLAIR sequences with delineated tumor (red line), T2w\u2013\
    FLAIR distribution of outliers with superimposed ellipses of mixture components\
    \ based on ground truth. The tumor also affected the MR intensities outside its\
    \ boundary (arrows). Bottom row from left to right: Axial cross-sections of ground\
    \ truth segmentations of the brain MR images with increasing tumor volume. The\
    \ cross-sections are shown for different brain phantoms and each of the axial\
    \ cross-sections was taken at the largest axial cross-sectional area of the tumor."
  Figure 7 Link: articels_figures_by_rev_year\2015\Robust_Estimation_of_Unbalanced_Mixture_Models_on_Samples_with_Outliers\figure_7.jpg
  Figure 7 caption: 'Performance of the MR intensity mixture estimation and MR image
    segmentation: (a) median misclassification ratio (mMCR) and (b) median Dice similarity
    coefficient (mDSC), respectively. Low mMCR corresponds to accurate mixture estimation,
    while high mDSC corresponds to accurate segmentation of MR images. Results for
    the FAST-TLE (top ) and the proposed (bottom) method are shown for different mean
    tumor volumes and trimming fractions alpha , and for the white matter (WM), gray
    matter (GM) and cerebrospinal fluid (CSF).'
  Figure 8 Link: articels_figures_by_rev_year\2015\Robust_Estimation_of_Unbalanced_Mixture_Models_on_Samples_with_Outliers\figure_8.jpg
  Figure 8 caption: 'Analysis of MR images shown in Fig. 6 and their cross-sections:
    outlier voxels (top) and segmentation (bottom) of the normal brain structures
    and tumors for FAST-TLE (upper rows) and the proposed (lower rows) methods for
    different trimming fractions alpha . The color-coding of the segmentations is
    the same as in Fig. 6.'
  Figure 9 Link: articels_figures_by_rev_year\2015\Robust_Estimation_of_Unbalanced_Mixture_Models_on_Samples_with_Outliers\figure_9.jpg
  Figure 9 caption: The normalized mean likelihood curve (top), median misclassification
    ratio (mMCR) (middle ) w.r.t. iterations, and the number of iterations required
    to convergence (bottom) of the FAST-TLE and the proposed methods for different
    trimming fractions alpha computed on synthetic samples contaminated by a fraction
    h=0.2 of outliers.
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Alfiia Galimzianova
  Name of the last author: "\u017Diga \u0160piclin"
  Number of Figures: 9
  Number of Tables: 4
  Number of authors: 4
  Paper title: Robust Estimation of Unbalanced Mixture Models on Samples with Outliers
  Publication Date: 2015-02-19 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Median Misclassification Ratio (mMCR, \xD7 10 \u22122 ) and\
      \ Median Absolute Deviation (in parentheses) Computed for the Mixtures Estimated\
      \ by the FAST-TLE and the Proposed Methods on Test Samples with Uniformly Distributed\
      \ Outliers for Six Different Intervals of the Outlier Fraction"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Median Misclassification Ratio (mMCR, \xD7 10 \u22122 ) and\
      \ Median Absolute Deviation (in parentheses) Computed for the Mixtures Estimated\
      \ by the FAST-TLE and the Proposed Methods on Test Samples with Uniformly Distributed\
      \ Outliers for Six Different Intervals of the Outlier Fraction"
  Table 3 caption:
    table_text: "TABLE 3 Median Misclassification Ratio (mMCR, \xD7 10 \u22121 ) and\
      \ Its Median Absolute Deviation (in Parentheses) for the Three Component Normal\
      \ Mixtures Estimated on the MR Image Sets by the FAST-TLE and the Proposed Methods\
      \ for Different Values of Trimming Fractions \u03B1"
  Table 4 caption:
    table_text: TABLE 4 Characteristics of the FAST-TLE and Proposed Method, Which
      Was Tested with Three Implementations of the Confidence Level Ordering
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2404835
- Affiliation of the first author: disney research zurich, zurich, switzerland
  Affiliation of the last author: computer vision and geometry group, eth zurich,
    zurich, switzerland
  Figure 1 Link: articels_figures_by_rev_year\2015\Geometric_Change_Detection_in_Urban_Environments_Using_Images\figure_1.jpg
  Figure 1 caption: Overlay of a Google StreetView image with a cadastral 3D model
    before (left) and after (right) applying our algorithm.
  Figure 10 Link: articels_figures_by_rev_year\2015\Geometric_Change_Detection_in_Urban_Environments_Using_Images\figure_10.jpg
  Figure 10 caption: Overlay of some Google StreetView images with the cadastral 3D
    model before (left) and after (right) running Equation (8).
  Figure 2 Link: articels_figures_by_rev_year\2015\Geometric_Change_Detection_in_Urban_Environments_Using_Images\figure_2.jpg
  Figure 2 caption: "Image formation process of a 2D inconsistency map M t\u2190s\
    \ given a pair of image I s and I t observing the same location."
  Figure 3 Link: articels_figures_by_rev_year\2015\Geometric_Change_Detection_in_Urban_Environments_Using_Images\figure_3.jpg
  Figure 3 caption: "Image formation process of a 2D inconsistency map M t\u2190s\
    \ computed for the scene shown in Fig. 2. Since the new structure in front of\
    \ the building was not modeled by the original geometry, the resulting image M\
    \ t\u2190s reveals some inconsistencies in the corresponding pixels. The red and\
    \ blue circles indicate the evidence of change coming from images I t and I s\
    \ respectively."
  Figure 4 Link: articels_figures_by_rev_year\2015\Geometric_Change_Detection_in_Urban_Environments_Using_Images\figure_4.jpg
  Figure 4 caption: Overview of the registration algorithm.
  Figure 5 Link: articels_figures_by_rev_year\2015\Geometric_Change_Detection_in_Urban_Environments_Using_Images\figure_5.jpg
  Figure 5 caption: "Image formation process for image I t\u2190s . Since the region\
    \ corresponding to the voxel is not a part of the geometry, it generates a second\
    \ evidence of change on image I t in pixels (blue) which are far away from the\
    \ region it should ideally project to (red) if it were a part of the geometry."
  Figure 6 Link: articels_figures_by_rev_year\2015\Geometric_Change_Detection_in_Urban_Environments_Using_Images\figure_6.jpg
  Figure 6 caption: Inconsistency maps corresponding to the pair of images I s and
    I t , obtained using Equation (6) and the one obtained using Equation (17) (below),
    accounting for geometric inaccuracies. False changes due to missing details on
    the building facade disappear in the latter.
  Figure 7 Link: articels_figures_by_rev_year\2015\Geometric_Change_Detection_in_Urban_Environments_Using_Images\figure_7.jpg
  Figure 7 caption: Example scenario where the source image I s was captured more
    than 30 m away from the target image I t . (c) Reprojected image. (d) Inconsistency
    map obtained as a result of Equation (17). (e) Image obtained after filtering
    I t with the spatially varying kernel defined in Section 3.4.2. (f) Inconsistency
    map obtained as result of Equation (19).
  Figure 8 Link: articels_figures_by_rev_year\2015\Geometric_Change_Detection_in_Urban_Environments_Using_Images\figure_8.jpg
  Figure 8 caption: 'Example output of the proposed algorithm. (a) One of the images
    used to recover the initial geometry of the scene, shown in (b). (c) One of the
    images of the same location captured after some time: a new structure was placed.
    (d) Computed volumetric inconsistency map between the new images (c) and the initial
    geometry (b) red indicates inconsistencies.'
  Figure 9 Link: articels_figures_by_rev_year\2015\Geometric_Change_Detection_in_Urban_Environments_Using_Images\figure_9.jpg
  Figure 9 caption: "(a) Image observing a location, whose geometry is shown in (b).\
    \ (c) The volumetric inconsistency map obtained by the algorithm. (d) Graph displaying\
    \ the percentage of inconsistent pixels in the M t\u2190s maps along the entire\
    \ street, a peak indicates a possible change near that location."
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Aparna Taneja
  Name of the last author: Marc Pollefeys
  Number of Figures: 14
  Number of Tables: 0
  Number of authors: 3
  Paper title: Geometric Change Detection in Urban Environments Using Images
  Publication Date: 2015-02-19 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2404834
- Affiliation of the first author: department of electrical engineering, israel institute
    of technology, haifa, israel
  Affiliation of the last author: department of electrical engineering, israel institute
    of technology, haifa, israel
  Figure 1 Link: articels_figures_by_rev_year\2015\The_Perturbed_Variation\figure_1.jpg
  Figure 1 caption: PV score between discrete distributions.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\The_Perturbed_Variation\figure_2.jpg
  Figure 2 caption: "Type-2 error for varying \u03F5 values. Results were averaged\
    \ over 500 repetitions."
  Figure 3 Link: articels_figures_by_rev_year\2015\The_Perturbed_Variation\figure_3.jpg
  Figure 3 caption: 'Precision-recall curve: (a) video clips, (b) female-male gait,
    (c) Auslan dataset.'
  Figure 4 Link: articels_figures_by_rev_year\2015\The_Perturbed_Variation\figure_4.jpg
  Figure 4 caption: Tracking of Shirt (a-c), Lemming (d-f), Skating (h-k), and Human
    (l-k) sequences. The sequences show the PV's ability to track objects with non-rigid
    transformations (Shirt and Skating), partial occlusion (Lemming), out-of-plane-rotations
    (Lemming, Skating), as well as scale changes (Human). Notice that the PV is especially
    suited to track deformations of the object as it permits motion using perturbations.
    (g) shows an example of the object being tracked after extraction from background
    in initial step.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.85
  Name of the first author: Maayan Harel
  Name of the last author: Shie Mannor
  Number of Figures: 4
  Number of Tables: 2
  Number of authors: 2
  Paper title: The Perturbed Variation
  Publication Date: 2015-02-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 MAP for Auslan, Video, and Gait Data Sets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 The Percent of Frames for Which the PASCAL Criterion a 0\
      \ = Area( B predict \u2229 B truth ) Area( B predict \u222A B truth ) >0.5 .\
      \ A Higher Value Indicates Better Tracking1"
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2404836
- Affiliation of the first author: department of computer science, kaist (korea advanced
    institute of science and technology), south korea
  Affiliation of the last author: department of computer science, kaist (korea advanced
    institute of science and technology), south korea
  Figure 1 Link: articels_figures_by_rev_year\2015\Spherical_Hashing_Binary_Code_Embedding_with_Hyperspheres\figure_1.jpg
  Figure 1 caption: The difference between our hypersphere-based binary code embedding
    method and hyperplane-based one. The left and right figures show partitioning
    examples of hypersphere-based and hyperplane-based methods respectively, for 3
    bit binary codes in the 2D space. Each function h i determines the value of i
    th bit of binary codes. The hypersphere-based binary code embedding scheme gives
    a higher number of tightly closed regions compared to hyperplane-based one.
  Figure 10 Link: articels_figures_by_rev_year\2015\Spherical_Hashing_Binary_Code_Embedding_with_Hyperspheres\figure_10.jpg
  Figure 10 caption: Recall curves of different methods when k=100 for the GIST-1M-384D
    dataset. Each hash table is constructed by 64 bits code lengths. The recall (
    y -axis) represents search accuracy and the number of retrieved samples ( x -axis)
    represents search time.
  Figure 2 Link: articels_figures_by_rev_year\2015\Spherical_Hashing_Binary_Code_Embedding_with_Hyperspheres\figure_2.jpg
  Figure 2 caption: "The left figure shows how the avg. of the max. distances among\
    \ points having the same binary code changes with different code lengths based\
    \ on hyperspheres or hyperplanes. We randomly sample 1,000 different binary codes\
    \ to compute avg. of the max. distances. The right figure shows how having more\
    \ common +1 bits in our method effectively forms tighter closed regions. For the\
    \ right curve we randomly sample one million pairs of binary codes. For each pair\
    \ of binary codes ( b i , b j ) we compute the max. distance between pairs of\
    \ points, ( x i , x j ) , where H( x i )= b i and H( x j )= b j . We report the\
    \ avg. of the max. distances as a function of the number of common +1 bits, i.e.\
    \ | b i \u2227 b j | . Both figures are obtained with GIST-1M-960D dataset (Section\
    \ 5.1)."
  Figure 3 Link: articels_figures_by_rev_year\2015\Spherical_Hashing_Binary_Code_Embedding_with_Hyperspheres\figure_3.jpg
  Figure 3 caption: Intuition of the spherical hamming distance. If both two points
    x i and x j are located in one of colored regions then their binary codes b i
    and b j have at least 1, 2, or 3 common +1 bits, respectively. As the number of
    common +1 bits of b i and b j increases, the size of the region containing both
    points x i and x j are expected to become smaller. As a result, the expected distance
    between x i and x j also gets smaller.
  Figure 4 Link: articels_figures_by_rev_year\2015\Spherical_Hashing_Binary_Code_Embedding_with_Hyperspheres\figure_4.jpg
  Figure 4 caption: These two images show how a force between two pivots is computed.
    In the left image a repulsive force is computed since their overlap o i,j is larger
    than the desired amount. On the other hand, the attractive force is computed in
    the right image because their overlap is smaller than the desired amount.
  Figure 5 Link: articels_figures_by_rev_year\2015\Spherical_Hashing_Binary_Code_Embedding_with_Hyperspheres\figure_5.jpg
  Figure 5 caption: 'mAP curves for k -nearest neighbor search with respect to various
    parameters. A pair of values in x -axis are used for two parameters of epsilon
    m and epsilon s , and y -axis represents their corresponding mAP values. Each
    legend consists of four experiment settings ''dataset k binary code length distance
    metric type (HD: Hamming distance, SHD: spherical Hamming distance)''.'
  Figure 6 Link: articels_figures_by_rev_year\2015\Spherical_Hashing_Binary_Code_Embedding_with_Hyperspheres\figure_6.jpg
  Figure 6 caption: Convergence rates of our iterative optimization with three individual
    trials. The optimization processes are finished within 30 iterations when we se
    we set epsilon m as 0.1 and epsilon s as 0.15. This graph also shows that both
    objectives converge to 0 when we increase the number of iterations. This result
    is obtained with the GIST-1M-384D dataset at the 64-bit code length.
  Figure 7 Link: articels_figures_by_rev_year\2015\Spherical_Hashing_Binary_Code_Embedding_with_Hyperspheres\figure_7.jpg
  Figure 7 caption: Visualization of our optimization process with three hyperspheres
    and 500 points in 2D space.
  Figure 8 Link: articels_figures_by_rev_year\2015\Spherical_Hashing_Binary_Code_Embedding_with_Hyperspheres\figure_8.jpg
  Figure 8 caption: Comparison between our method and the-state-of-the-art methods
    with the GIST-1M-384D dataset when k=100 .
  Figure 9 Link: articels_figures_by_rev_year\2015\Spherical_Hashing_Binary_Code_Embedding_with_Hyperspheres\figure_9.jpg
  Figure 9 caption: Comparison between our method and the-state-of-the-art methods
    with the GIST-1M-960D dataset when k=1,000 .
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.56
  Name of the first author: Jae-Pil Heo
  Name of the last author: Sung-Eui Yoon
  Number of Figures: 16
  Number of Tables: 4
  Number of authors: 5
  Paper title: 'Spherical Hashing: Binary Code Embedding with Hyperspheres'
  Publication Date: 2015-02-27 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Experimental Results of Hyperplane Based Methods Combined
      with SHD
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparisons between SHD and SHD-SUB Described in Section 3.2
  Table 3 caption:
    table_text: TABLE 3 The Effect of Our Max-Margin Based Distance Thresholding (Section
      3.5) Indicated by M
  Table 4 caption:
    table_text: TABLE 4 The Effect of SHD with Generalized Spherical Hashing
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2408363
- Affiliation of the first author: institute of computational science, faculty of
    informatics, university of lugano (usi), lugano, switzerland
  Affiliation of the last author: school of electrical engineering, tel-aviv university,
    tel-aviv, israel
  Figure 1 Link: articels_figures_by_rev_year\2015\Multimodal_Manifold_Analysis_by_Simultaneous_Diagonalization_of_Laplacians\figure_1.jpg
  Figure 1 caption: "Top: the first few Laplacian eigenvectors u 1k and u 2k , k=2,\u2026\
    ,5 of two Swiss rolls (first and second rows) with slightly different connectivity\
    \ (shown with lines). The difference in the connectivity results in different\
    \ behavior of the eigenvectors (e.g. the third and the second eigenvectors are\
    \ swapped). Bottom: joint approximate eigenvectors v k computed on the same datasets\
    \ using JADE behave in the same way. (Hot colors represent positive values; cold\
    \ colors represent negative values.)"
  Figure 10 Link: articels_figures_by_rev_year\2015\Multimodal_Manifold_Analysis_by_Simultaneous_Diagonalization_of_Laplacians\figure_10.jpg
  Figure 10 caption: "Comparison of CCO and CD computational complexity, measured\
    \ the mean time (in msec) per iteration for different number of vertices n and\
    \ number of nearest neighbors s used in the definition of the Laplacian. CD is\
    \ shown in two settings: k \u2032 =20 (blue) and k \u2032 =50 (red); CCO is shown\
    \ in green. For a fair comparison, in CD the coupling is performed using all the\
    \ points, hence the complexity grows linearly with n (the complexity of CCO grows\
    \ quadratically with n ). Also, note that CCO depends on the adjacency structure\
    \ (number of nearest neighbors s ) while CD does not."
  Figure 2 Link: articels_figures_by_rev_year\2015\Multimodal_Manifold_Analysis_by_Simultaneous_Diagonalization_of_Laplacians\figure_2.jpg
  Figure 2 caption: Simultaneous two-dimensional embedding of two Swiss rolls with
    slightly different connectivity using JADE, coupled diagonalization (CD) and manifold
    alignment (MA). Ideally, the embedding should 'unroll' the rolls into rectangular
    regions, and the embeddings of the two modalities should coincide. Using the same
    sparse coupling (from 10 to 1 percent of the points, shown in gray lines), CD
    produces a significantly better alignment than MA.
  Figure 3 Link: articels_figures_by_rev_year\2015\Multimodal_Manifold_Analysis_by_Simultaneous_Diagonalization_of_Laplacians\figure_3.jpg
  Figure 3 caption: Alignment of face (green) and statue (blue) manifolds. Each point
    represents an image in the respective dataset; circles represent corresponding
    poses of the statue and face images shown. Crosses denote the data points used
    for coupling. Note some significant misalignment between the manifolds in the
    MA results (marked in red).
  Figure 4 Link: articels_figures_by_rev_year\2015\Multimodal_Manifold_Analysis_by_Simultaneous_Diagonalization_of_Laplacians\figure_4.jpg
  Figure 4 caption: Spectral clustering of the NUS dataset. Shown are a few images
    (randomly sampled) attributed to a single cluster by spectral clustering using
    the Tags modality only (top), the Color modality only (middle) and the Tags+Color
    multimodal clustering using JADE (bottom). Groundtruth clusters are shown in different
    colors. Note the ambiguities in the Tag-based clustering (e.g. swimming tigers
    and underwater scenes) and Color-base clustering (e.g. yellowish tigers and autumn
    scenes).
  Figure 5 Link: articels_figures_by_rev_year\2015\Multimodal_Manifold_Analysis_by_Simultaneous_Diagonalization_of_Laplacians\figure_5.jpg
  Figure 5 caption: Spectral clustering of the Caltech-7 dataset. Shown are a few
    images (randomly sampled) attributed to a single cluster by spectral clustering
    using the Bio-inspired modality only (top), the PHOW modality only (middle) and
    the multimodal clustering using JADE (bottom). Groundtruth clusters are shown
    in different colors. Ideally, a cluster should contain images from a single class
    only.
  Figure 6 Link: articels_figures_by_rev_year\2015\Multimodal_Manifold_Analysis_by_Simultaneous_Diagonalization_of_Laplacians\figure_6.jpg
  Figure 6 caption: Clustering of synthetic multimodal datasets Circles (two modalities
    shown in first and second rows) and Text (third and fourth rows). Marker shape
    represents ground truth clusters; marker color represents the clustering results
    produced by different methods (ideally, all markers of one type should have only
    one color).
  Figure 7 Link: articels_figures_by_rev_year\2015\Multimodal_Manifold_Analysis_by_Simultaneous_Diagonalization_of_Laplacians\figure_7.jpg
  Figure 7 caption: Diffusion distances between objects from the Caltech (top) and
    NUS (bottom) datasets using separate modalities (first and second columns), JADE
    (third column) and CD with coupling (fourth column) and coupling+decoupling (fifth
    column) terms. Note the ambiguities between different classes of objects (marked
    in cyan) when using a single modality.
  Figure 8 Link: articels_figures_by_rev_year\2015\Multimodal_Manifold_Analysis_by_Simultaneous_Diagonalization_of_Laplacians\figure_8.jpg
  Figure 8 caption: Object classification performance on Caltech (left) and NUS (right)
    datasets using diffusion distances computed in each modality separately (Uncoupled),
    a joint eigenspace (JADE), coupled eigenspaces produced by CD with coupling (pos)
    and coupling+decoupling (pos+neg) terms, and the joint eigenspace of the closest
    commuting Laplacians (CCO). Note that CD (pos+neg) performs better than each modality
    on its own and outperforms the other methods.
  Figure 9 Link: articels_figures_by_rev_year\2015\Multimodal_Manifold_Analysis_by_Simultaneous_Diagonalization_of_Laplacians\figure_9.jpg
  Figure 9 caption: Farthest point sampling of NUS (top) and Caltech-7 (bottom) datasets
    using the diffusion distance in the joint eigenspace computed by JADE. First point
    is on the left. Numbers indicate the sampling radius. Note that in both cases,
    the first seven samples cover all the image classes, providing a meaningful subsampling
    of the respective datasets.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Davide Eynard
  Name of the last author: Alexander M. Bronstein
  Number of Figures: 10
  Number of Tables: 1
  Number of authors: 5
  Paper title: Multimodal Manifold Analysis by Simultaneous Diagonalization of Laplacians
  Publication Date: 2015-03-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance of Different Multimodal Clustering Methods of
      different Datasets (AccuracyNormalized Mutual Information in %, the Higher the
      Better)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2408348
- Affiliation of the first author: department of computer science, stony brook university,
    new york, ny, usa
  Affiliation of the last author: department of computer science, stony brook university,
    new york, ny, usa
  Figure 1 Link: articels_figures_by_rev_year\2015\Optimal_Mass_Transport_for_Shape_Matching_and_Comparison\figure_1.jpg
  Figure 1 caption: 'Comparison of geometric mappings for Armadillo surface model
    to a planar unit disk: (a) Front view. (b) Back view. (c) Optimal mass transport
    map result. (d) Conformal mapping result. The results show that conformal mapping
    has much more area distortions on head and hands areas. The normal information
    on the original surfaces is preserved and used for rendering. By the shading information
    on the planar domain ((c) and (d)), the correspondence is illustrated. The hand
    zoom-in image of (d) shows that the conformal map shrinks the fingers to very
    tiny areas which may cause numerical instability, while the hand zoom-in image
    of (c) demonstrates the optimal mass transport method gives a good one-to-one
    mapping result.'
  Figure 10 Link: articels_figures_by_rev_year\2015\Optimal_Mass_Transport_for_Shape_Matching_and_Comparison\figure_10.jpg
  Figure 10 caption: Multidimensional scaling embedding of the Wasserstein distance
    between each pair of face surfaces in the dataset.
  Figure 2 Link: articels_figures_by_rev_year\2015\Optimal_Mass_Transport_for_Shape_Matching_and_Comparison\figure_2.jpg
  Figure 2 caption: (a) Circle-packing texture mappings for conformal parameterization
    (CFP) and (b) area-preserving parameterization (APP) for the model of a human
    head, with the planar unit square parameter domain. The mappings to the parameter
    domain results are also shown in (c) and (d), respectively. (e) to (h) are the
    histograms of angle distortions and area distortions, which demonstrate the accuracy
    of the Optimal Mass Transport map.
  Figure 3 Link: articels_figures_by_rev_year\2015\Optimal_Mass_Transport_for_Shape_Matching_and_Comparison\figure_3.jpg
  Figure 3 caption: Comparison of CFP and area-preserving parameterization (APP) of
    a Bimba sculpture model, shown in (a) and (b), with the spherical parameter domain.
    The normal information on the original surfaces is preserved and used for rendering.(g)
    to (j) are the histograms of angle distortions and area distortions.
  Figure 4 Link: articels_figures_by_rev_year\2015\Optimal_Mass_Transport_for_Shape_Matching_and_Comparison\figure_4.jpg
  Figure 4 caption: Discrete optimal mass transport map with Brenier's approach.
  Figure 5 Link: articels_figures_by_rev_year\2015\Optimal_Mass_Transport_for_Shape_Matching_and_Comparison\figure_5.jpg
  Figure 5 caption: Importance-driven parameterization of a Buddha model.
  Figure 6 Link: articels_figures_by_rev_year\2015\Optimal_Mass_Transport_for_Shape_Matching_and_Comparison\figure_6.jpg
  Figure 6 caption: Seven Armadillo models with isometric deformations, which form
    21 matching pairs in our experiments.
  Figure 7 Link: articels_figures_by_rev_year\2015\Optimal_Mass_Transport_for_Shape_Matching_and_Comparison\figure_7.jpg
  Figure 7 caption: Surface registration results for Armadillo models with isometric
    deformations. Their mapping results are registered using harmonic maps with hard
    constraints (yellow stars). The colored lines connecting color-encoded circular
    dots on (a) and (b) show the registered correspondences by OMT map.
  Figure 8 Link: articels_figures_by_rev_year\2015\Optimal_Mass_Transport_for_Shape_Matching_and_Comparison\figure_8.jpg
  Figure 8 caption: The computation of Wasserstein distance.
  Figure 9 Link: articels_figures_by_rev_year\2015\Optimal_Mass_Transport_for_Shape_Matching_and_Comparison\figure_9.jpg
  Figure 9 caption: "Face surfaces for expression clustering. The first row is \u201C\
    sad\u201D, the second row is \u201Chappy\u201D and the third row is \u201Csurprise\u201D\
    ."
  First author gender probability: 0.79
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.71
  Name of the first author: Zhengyu Su
  Name of the last author: Xianfeng Gu
  Number of Figures: 10
  Number of Tables: 1
  Number of authors: 7
  Paper title: Optimal Mass Transport for Shape Matching and Comparison
  Publication Date: 2015-03-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Geometric Complexities and Running Times
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2408346
