- Affiliation of the first author: tel aviv university, israel
  Affiliation of the last author: tel aviv university, israel
  Figure 1 Link: articels_figures_by_rev_year\2015\Accelerating_Particle_Filter_Using_Randomized_Multiscale_and_Fast_Multipole_Type\figure_1.jpg
  Figure 1 caption: Comparison between the condition number of a Gaussian kernel columns
    selected by different methods.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Accelerating_Particle_Filter_Using_Randomized_Multiscale_and_Fast_Multipole_Type\figure_2.jpg
  Figure 2 caption: Comparison between the condition number of a Gaussian kernel columns
    selected by different methods (zoomed).
  Figure 3 Link: articels_figures_by_rev_year\2015\Accelerating_Particle_Filter_Using_Randomized_Multiscale_and_Fast_Multipole_Type\figure_3.jpg
  Figure 3 caption: A set of representative frames from a basketball tracking sequence.
    The object is tracked using the MSPF Algorithm 5.1 with a direct computation of
    the weights for 10 percent from the total number of particles.
  Figure 4 Link: articels_figures_by_rev_year\2015\Accelerating_Particle_Filter_Using_Randomized_Multiscale_and_Fast_Multipole_Type\figure_4.jpg
  Figure 4 caption: "Comparison between the tracking success rate for a given computational\
    \ budget with standard PF (Algorithm 3.1 which we refer to as the \u201Cnaive\
    \ PF\u201D) and MSPF (Algorithm 5.1)."
  Figure 5 Link: articels_figures_by_rev_year\2015\Accelerating_Particle_Filter_Using_Randomized_Multiscale_and_Fast_Multipole_Type\figure_5.jpg
  Figure 5 caption: 'Comparison between the RMSE for different methods: multiscale
    with ID sampling (Algorithm 4.2), multiscale with WFPS sampling (Algorithm 6.1),
    linear approximation and cubic approximation.'
  Figure 6 Link: articels_figures_by_rev_year\2015\Accelerating_Particle_Filter_Using_Randomized_Multiscale_and_Fast_Multipole_Type\figure_6.jpg
  Figure 6 caption: Computational time of the MSPF with different sampling rates.
    The total number of particles is 1,500 .
  Figure 7 Link: articels_figures_by_rev_year\2015\Accelerating_Particle_Filter_Using_Randomized_Multiscale_and_Fast_Multipole_Type\figure_7.jpg
  Figure 7 caption: A selected set of representative frames from the tennis game that
    demonstrates the tracking performance. The two tennis players were tracked by
    the application of the MSPF Algorithm 5.1 with a direct weights computation for
    10 percent from the total number of particles.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Gil Shabat
  Name of the last author: Amir Averbuch
  Number of Figures: 7
  Number of Tables: 0
  Number of authors: 4
  Paper title: Accelerating Particle Filter Using Randomized Multiscale and Fast Multipole
    Type Methods
  Publication Date: 2015-01-15 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2392754
- Affiliation of the first author: department of eecs, queen mary, university of london,
    london e1 4ns, united kingdom
  Affiliation of the last author: department of eecs, queen mary, university of london,
    london e1 4ns, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2015\Bayesian_Joint_Modelling_for_Object_Localisation_in_Weakly_Labelled_Images\figure_1.jpg
  Figure 1 caption: Different types of objects often co-exist in a single image. Our
    joint learning approach differs from previous approaches which localise each object
    class independently.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Bayesian_Joint_Modelling_for_Object_Localisation_in_Weakly_Labelled_Images\figure_2.jpg
  Figure 2 caption: Graphical model for our WSOL joint topic model. Shaded nodes are
    observed.
  Figure 3 Link: articels_figures_by_rev_year\2015\Bayesian_Joint_Modelling_for_Object_Localisation_in_Weakly_Labelled_Images\figure_3.jpg
  Figure 3 caption: (a) A hierarchical structure of the 20 PASCAL VOC classes using
    WordNet. (b) The class similarity matrix.
  Figure 4 Link: articels_figures_by_rev_year\2015\Bayesian_Joint_Modelling_for_Object_Localisation_in_Weakly_Labelled_Images\figure_4.jpg
  Figure 4 caption: 'Top row in each subfigure: examples of object localisation using
    our-sampling and our-Gaussian. Bottom row: illustration of what is learned by
    the object (foreground) topics via heat map (brighter means object is more likely).
    The first four rows show some examples of PASCAL VOC and last two rows are selected
    from ImageNet.'
  Figure 5 Link: articels_figures_by_rev_year\2015\Bayesian_Joint_Modelling_for_Object_Localisation_in_Weakly_Labelled_Images\figure_5.jpg
  Figure 5 caption: Illustration of the learned background topics.
  Figure 6 Link: articels_figures_by_rev_year\2015\Bayesian_Joint_Modelling_for_Object_Localisation_in_Weakly_Labelled_Images\figure_6.jpg
  Figure 6 caption: Examples of video object localisation.
  Figure 7 Link: articels_figures_by_rev_year\2015\Bayesian_Joint_Modelling_for_Object_Localisation_in_Weakly_Labelled_Images\figure_7.jpg
  Figure 7 caption: Domain adaptation provides more benefit with fewer target domain
    samples.
  Figure 8 Link: articels_figures_by_rev_year\2015\Bayesian_Joint_Modelling_for_Object_Localisation_in_Weakly_Labelled_Images\figure_8.jpg
  Figure 8 caption: Unlabelled data improves foreground heat maps.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.55
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Zhiyuan Shi
  Name of the last author: Tao Xiang
  Number of Figures: 8
  Number of Tables: 7
  Number of authors: 3
  Paper title: Bayesian Joint Modelling for Object Localisation in Weakly Labelled
    Images
  Publication Date: 2015-01-15 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of Model Variables and Parameters
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison with State-of-the-Art Competitors on the Three
      Variations of the PASCAL VOC 2007 Dataset
  Table 3 caption:
    table_text: TABLE 3 Performance of Strong Detectors Trained Using Annotations
      Obtained by Different WSOL Methods
  Table 4 caption:
    table_text: TABLE 4 Initial Annotation Accuracy on ImageNet Dataset
  Table 5 caption:
    table_text: TABLE 5 Performance Comparison on YouTube-Object
  Table 6 caption:
    table_text: TABLE 6 Cross-Domain Transfer Learning Results
  Table 7 caption:
    table_text: TABLE 7 Localisation Performance of Semi-Supervised Learning Using
      Our-Sampling
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2392769
- Affiliation of the first author: department of electrical and computer engineering,
    duke university, durham, nc
  Affiliation of the last author: department of electrical and computer engineering,
    duke university, durham, nc
  Figure 1 Link: articels_figures_by_rev_year\2015\Learning_Efficient_Sparse_and_Low_Rank_Models\figure_1.jpg
  Figure 1 caption: Comparison between RNMF encoders and proximal methods for music
    source separation. Optimality gap as a function of the number of iterationslayers
    with offline trained encoders using the unsupervised regime.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Learning_Efficient_Sparse_and_Low_Rank_Models\figure_2.jpg
  Figure 2 caption: Encoderdecoder architectures for the unstructured Lasso (left),
    and the robust PCANMF (right).
  Figure 3 Link: articels_figures_by_rev_year\2015\Learning_Efficient_Sparse_and_Low_Rank_Models\figure_3.jpg
  Figure 3 caption: "Comparison between RNMF encoders and proximal methods for music\
    \ source separation. The \u2113 2 separation error obtained with offline supervised\
    \ and the unsupervised training. The red dotted line represents the error achieved\
    \ by Algorithm 2 after convergence."
  Figure 4 Link: articels_figures_by_rev_year\2015\Learning_Efficient_Sparse_and_Low_Rank_Models\figure_4.jpg
  Figure 4 caption: Comparison between the learned ISTA-encoders and standard ISTA
    and FISTA on the activity recognition experiment. Depicted is the classification
    performance as a function of the number of iterationslayers with offline trained
    encoders in the unsupervised regime. Vertical lines compare indicate fixed computational
    budget.
  Figure 5 Link: articels_figures_by_rev_year\2015\Learning_Efficient_Sparse_and_Low_Rank_Models\figure_5.jpg
  Figure 5 caption: Performance of CoD sparse encoders, trained under different training
    regimes, measured using the Lasso objective, as a function of sample number in
    the online learning experiment. Shown are the three groups of patches corresponding
    to different texture images from the Brodatz dataset.
  Figure 6 Link: articels_figures_by_rev_year\2015\Learning_Efficient_Sparse_and_Low_Rank_Models\figure_6.jpg
  Figure 6 caption: 'Robust PCA representation of the faces dataset in the presence
    of geometric transformations (misalignment). Left group: original faces; middle
    group: shifted faces; right group: faces optimally realigned during encoding.
    First row: reconstructed face Ds+o ; middle row: its low-rank approximation Ds
    ; and bottom row: sparse outlier o . Value of the RPCA loss for each representation.'
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: P. Sprechmann
  Name of the last author: G. Sapiro
  Number of Figures: 6
  Number of Tables: 3
  Number of authors: 3
  Paper title: Learning Efficient Sparse and Low Rank Models
  Publication Date: 2015-01-15 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Action Classification Rates and Average Computational Costs
      for USC-Sport Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Speaker Identification Misclassification Rates
  Table 3 caption:
    table_text: TABLE 3 Audio Separation Quality ( dB NSDR)
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2392779
- Affiliation of the first author: "escuela de inform\xE1tica y telecomunicaciones,\
    \ universidad diego portales, santiago, chile"
  Affiliation of the last author: department of computer engineering, kyung hee university,
    yongin, south korea
  Figure 1 Link: articels_figures_by_rev_year\2015\Spatiotemporal_Directional_Number_Transitional_Graph_for_Dynamic_Texture_Recogni\figure_1.jpg
  Figure 1 caption: We extract the spatiotemporal directional numbers for each frame,
    and divide the sequence into a 3D grid. For each defined region, we extract a
    DNG. Finally, the graphs are transformed into a one-dimensional vector to create
    the image sequence descriptor.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Spatiotemporal_Directional_Number_Transitional_Graph_for_Dynamic_Texture_Recogni\figure_2.jpg
  Figure 2 caption: (a) A 3D compass mask gives nine spatiotemporal directional responses
    corresponding to each of the symmetry planes of a cube. (b) Approximation of the
    mask that gives the XT-plane response.
  Figure 3 Link: articels_figures_by_rev_year\2015\Spatiotemporal_Directional_Number_Transitional_Graph_for_Dynamic_Texture_Recogni\figure_3.jpg
  Figure 3 caption: (a) (b) (c) Average FER accuracy (Percent) using different grid
    resolutions ( r , c , and t represent row, columns, and time, respectively). (d)
    Illustration of the best grid in a facial image sequence.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.54
  Name of the first author: "Ad\xEDn Ram\xEDrez Rivera"
  Name of the last author: Oksam Chae
  Number of Figures: 3
  Number of Tables: 9
  Number of authors: 2
  Paper title: Spatiotemporal Directional Number Transitional Graph for Dynamic Texture
    Recognition
  Publication Date: 2015-01-15 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Accuracy (Percent) of DNG Using Different SVM Kernels (Inter.,\
      \ \u03C7 2 , and RBF), and a Histogram Descriptor (Hist.) Using SVM (with an\
      \ RBF Kernel) and Nearest Neighbor Classifiers on the Texture Datasets"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Dynamic Texture Classification Accuracy (Percent) for the
      (a) UCLA and (b) DynTex++ Databases
  Table 3 caption:
    table_text: TABLE 3 Confusion Matrix Using SVM (RBF) in the SIR-UCLA
  Table 4 caption:
    table_text: TABLE 4 FER Accuracy (Percent) for (a) CK+ and (b) MMI
  Table 5 caption:
    table_text: TABLE 5 Confusion Matrix of DNG P on MMI
  Table 6 caption:
    table_text: TABLE 6 FER Accuracy (Percent) for the Oulu-CASIA (VIS) Database
  Table 7 caption:
    table_text: TABLE 7 Confusion Matrix of DNG S on Oulu-CASIA (VIS) Dark
  Table 8 caption:
    table_text: TABLE 8 FER Accuracy (Percent) for Oulu-CASIA (NIR) Database
  Table 9 caption:
    table_text: TABLE 9 Confusion Matrix of DNG P on Oulu-CASIA (NIR)
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2392774
- Affiliation of the first author: department of electrical and computer engineering,
    national university of singapore, singapore
  Affiliation of the last author: department of electrical and computer engineering,
    national university of singapore, singapore
  Figure 1 Link: articels_figures_by_rev_year\2015\MultiCamera_Saliency\figure_1.jpg
  Figure 1 caption: Flowchart of the multi-camera saliency framework.
  Figure 10 Link: articels_figures_by_rev_year\2015\MultiCamera_Saliency\figure_10.jpg
  Figure 10 caption: 'Quantitative comparison of various saliency models on MCIE data
    set. TOP: two-view subset, and BOT: three-view subset. The prediction accuracy
    is measured with the shuffled AUC, NSS and CC scores. The reported performance
    is the mean accuracy with 10-fold validations.'
  Figure 2 Link: articels_figures_by_rev_year\2015\MultiCamera_Saliency\figure_2.jpg
  Figure 2 caption: An overview of the LC-KSVD saliency model. In the training phase,
    center-surround and HOG features are first extracted from a Gaussian pyramid of
    each training image. Then, using the ground-truth saliency map generated with
    human fixations, salient and non-salient image patches are sampled, whose features
    are later fed into a dictionary learning algorithm to jointly learn a discriminative
    dictionary and a linear classifier. In the testing phase, the dictionary and weights
    are used to generate multi-scale saliency maps of a test image. These maps are
    finally normalized and combined with a point-wise multiplication into the final
    saliency map.
  Figure 3 Link: articels_figures_by_rev_year\2015\MultiCamera_Saliency\figure_3.jpg
  Figure 3 caption: The conceptual example of the proposed principled framework with
    LC-KSVD saliency model. First, the feature channels obtained from local views
    are transformed and integrated into a common plane with pre-calibrated parameters.
    Then, we compute the global saliency map the learned overcomplete dictionary and
    classification weights. The predicted saliency are then re-projected to the local
    views followed by global normalization step. Node LN, GN and X local normalization,
    global normalization, and point-wise multiplication, respectively.
  Figure 4 Link: articels_figures_by_rev_year\2015\MultiCamera_Saliency\figure_4.jpg
  Figure 4 caption: Example of a transformed image obtained with the pre-calibrated
    parameters.
  Figure 5 Link: articels_figures_by_rev_year\2015\MultiCamera_Saliency\figure_5.jpg
  Figure 5 caption: A conceptual example of projectivity transformation from source
    view to common image plane.
  Figure 6 Link: articels_figures_by_rev_year\2015\MultiCamera_Saliency\figure_6.jpg
  Figure 6 caption: Experimental configurations and comparisons between single-view
    and two-view fixations.
  Figure 7 Link: articels_figures_by_rev_year\2015\MultiCamera_Saliency\figure_7.jpg
  Figure 7 caption: Inter-subject AUC scores (mean and standard deviation) in the
    single-view and two-view experiments. Left and Right indicate the image position
    under two-view experiment.
  Figure 8 Link: articels_figures_by_rev_year\2015\MultiCamera_Saliency\figure_8.jpg
  Figure 8 caption: Experimental configuration and comparison between the Cam1-Cam2
    and Cam2-Cam1 settings. Within-group and between-group AUC scores are computed
    for each setting.
  Figure 9 Link: articels_figures_by_rev_year\2015\MultiCamera_Saliency\figure_9.jpg
  Figure 9 caption: Within-group and between-group AUC scores (means and standard
    deviations) in the two-view experiment.
  First author gender probability: 0.76
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Yan Luo
  Name of the last author: Qi Zhao
  Number of Figures: 12
  Number of Tables: 1
  Number of authors: 4
  Paper title: Multi-Camera Saliency
  Publication Date: 2015-01-15 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Computational Cost of the Proposed Principled Framework with
      the LC-KSVD Saliency Model
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2392783
- Affiliation of the first author: department of electrical and computer engineering,
    university of houston, houston, tx
  Affiliation of the last author: department of electrical and computer engineering,
    university of houston, houston, tx
  Figure 1 Link: articels_figures_by_rev_year\2015\Unsupervised_Discovery_of_Subspace_Trends\figure_1.jpg
  Figure 1 caption: 'Illustrating the impact of feature selection on subspace trend
    discovery. In panels (a-c), cDNA microarray data measuring the expression of 7,288
    genes in 99 breast tumor samples were visualized using three widely used methods
    LLE, ISOMAP, and t-SNE, respectively. Each dot in these visualizations corresponds
    to a tumor sample and is colored by the ER status: ER positive (red) and ER negative
    (blue). These renderings fail to reveal trends in gene expression that are linked
    to the ER status. Panels (d-f) show clear trends in gene expression data from
    ER negative to positive when only 48 of the genes that are specifically selected
    for trend relevance are visualized, eliminating the obscuring effect of the trend-irrelevant
    genes. Without feature selection, the two ER status samples are intermixed with
    each other. With selection, the trend of gradual transition from ER negative to
    positive is revealed.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Unsupervised_Discovery_of_Subspace_Trends\figure_2.jpg
  Figure 2 caption: Illustrating the neighborhood similarity measure for detecting
    associations between two features ( k=4.B=20 ). (a) For a parabolic association
    between x 1 and x 2 , the neighboring data points with respect to feature x 1
    (between the red dashed lines), are neighbors with respect to x 2 (between green
    dashed lines); (b) However for two independent variables x 3 and x 4 , the neighboring
    data points with respect to one variable can be arbitrarily distant with respect
    to the other. (c, d) the distributions derived from the k-NNGs for the associations
    in (a) and (b), respectively.
  Figure 3 Link: articels_figures_by_rev_year\2015\Unsupervised_Discovery_of_Subspace_Trends\figure_3.jpg
  Figure 3 caption: Synthetic data experiments designed to evaluate the neighborhood
    similarity measure's (NS) ability to capture linear and non-linear associations
    with added Gaussian noise, in comparison with four common measures (Pearson Correlation,
    normalized mutual information, distance correlation and maximal information coefficient).
  Figure 4 Link: articels_figures_by_rev_year\2015\Unsupervised_Discovery_of_Subspace_Trends\figure_4.jpg
  Figure 4 caption: 'Projection based visualization of the synthetic 10D dataset without
    trend-relevant feature selection fails to reveal the embedded subspace trends:
    (a) using the top three PCA components; (b) LLE ( k=6 ); (c) ISOMAP ( k=6 ); (d)
    t-SNE (perplexity = 6, smooth measure of k).'
  Figure 5 Link: articels_figures_by_rev_year\2015\Unsupervised_Discovery_of_Subspace_Trends\figure_5.jpg
  Figure 5 caption: Illustrating the main steps to automated subspace trend analysis.
    (a) the 10-dimensional synthetic data matrix displayed as a heatmap; (b) the neighborhood
    similarity matrix showing the main associations in its block structure; (c) illustrating
    the algorithm for identifying the similarity threshold; (d) illustrating the effect
    of thresholding to extract two matrix blocks that correspond clearly to the two
    synthetically embedded subspace trends I & II projected onto 3D visualizations
    in panel (e); and (f) illustrating the MSTs constructed for the extracted subspace
    trends individually and MST ordered heatmap display visually depicting the gradual
    changes of selected features along the trends, and showing the presence of jumps
    indicating the presence of MST branches.
  Figure 6 Link: articels_figures_by_rev_year\2015\Unsupervised_Discovery_of_Subspace_Trends\figure_6.jpg
  Figure 6 caption: "Subspace trend discovery correctly reveals the temporal order\
    \ of cells from cell-cycle time time-series microarray data. (a) Visualization\
    \ using t-SNE with all genes does not reveal the temporal order; (b) automatic\
    \ threshold identification from the distribution of neighborhood similarities;\
    \ (c) visualization of the detected subspace trend with the selected genes successfully\
    \ reveals the temporal order of the cells; (d) MST-ordered heatmap showing the\
    \ selected and unselected genes; (e) variation pattern of selected genes along\
    \ the temporal order; (f), (g) for \u03C3=0.8,0.9and1 and k=2,3,4 , the automatically\
    \ identified thresholds (indicated by the colored arrows) lie in the optimal or\
    \ suboptimal intervals. (h) For varying L values, the identified threshold and\
    \ connection accuracy stay at the same level after L reaches 200."
  Figure 7 Link: articels_figures_by_rev_year\2015\Unsupervised_Discovery_of_Subspace_Trends\figure_7.jpg
  Figure 7 caption: Subspace trend analysis of microarray data correctly reveals B
    cell differentiation stages. (a) Automatic threshold identification from the distribution
    of neighborhood similarities; (b) connection accuracy and clustering accuracy
    for a range of possible similarity thresholds; (c) visualization with all features
    fails to reveal a trend; (d) visualization with the selected gene modules reveals
    a clear subspace trend.
  Figure 8 Link: articels_figures_by_rev_year\2015\Unsupervised_Discovery_of_Subspace_Trends\figure_8.jpg
  Figure 8 caption: 'The trend discovered from HeLa cell mitosis morphology data from
    time-lapse microscopy is consistent with the known cyclical ordering of the six
    stages in the cell division process: interphase, prophase, prometaphase, metaphase,
    anaphase, and telophase. Each node in the visualization represents a cell and
    the color indicates its mitosis stage. (a) Visualization with all features cannot
    reveal the cyclic order in mitosis cell morphology; (b) image patches of cells
    in six mitosis stages from time-lapse microscopy; (c) visualization with selected
    features correctly reveals the cyclic development of mitosis from Interphase to
    telophase and back to Interphase.'
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: Yan Xu
  Name of the last author: Badrinath Roysam
  Number of Figures: 8
  Number of Tables: 2
  Number of authors: 3
  Paper title: Unsupervised Discovery of Subspace Trends
  Publication Date: 2015-01-20 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Glossary of Symbols
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Evaluation of the Neighborhood Similarity on Synthetic Associations
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2394475
- Affiliation of the first author: school of software engineering, chongqing university,
    chongqing, pr china
  Affiliation of the last author: school of software engineering, chongqing university,
    chongqing, pr china
  Figure 1 Link: articels_figures_by_rev_year\2015\Laplacian_ScaleSpace_Behavior_of_Planar_Curve_Corners\figure_1.jpg
  Figure 1 caption: The unified corner model.
  Figure 10 Link: articels_figures_by_rev_year\2015\Laplacian_ScaleSpace_Behavior_of_Planar_Curve_Corners\figure_10.jpg
  Figure 10 caption: "(a) The STAIR model with \u2212\u03C0< \u03B8 l +(\u03C0\u2212\
    \ \u03B8 r )<0 and \u2212\u03C02< \u03B8 l <(\u03C0\u2212 \u03B8 r )<\u03C02 .\
    \ (b) The LSS map. (c) The trajectory produced by \u03C3 2 = F 1 (s) . (d) The\
    \ trajectory produced by \u03C3 2 = F 2 (s) ."
  Figure 2 Link: articels_figures_by_rev_year\2015\Laplacian_ScaleSpace_Behavior_of_Planar_Curve_Corners\figure_2.jpg
  Figure 2 caption: "(a) END model. (b) \u0393 model. (c) STAIR model."
  Figure 3 Link: articels_figures_by_rev_year\2015\Laplacian_ScaleSpace_Behavior_of_Planar_Curve_Corners\figure_3.jpg
  Figure 3 caption: "The LSS map of the \u0393 model."
  Figure 4 Link: articels_figures_by_rev_year\2015\Laplacian_ScaleSpace_Behavior_of_Planar_Curve_Corners\figure_4.jpg
  Figure 4 caption: "(a) The END model with 0\u2264 \u03B8 l + \u03B8 r <\u03C0 and\
    \ \u2212\u03C02< \u03B8 l < \u03B8 r <\u03C02 . (b) The distribution of M \u02D9\
    \ \u03C3 (s) . (c) The LSS map."
  Figure 5 Link: articels_figures_by_rev_year\2015\Laplacian_ScaleSpace_Behavior_of_Planar_Curve_Corners\figure_5.jpg
  Figure 5 caption: "(a) The END model with 0\u2264 \u03B8 l = \u03B8 r <\u03C02 .\
    \ (b) The LSS map."
  Figure 6 Link: articels_figures_by_rev_year\2015\Laplacian_ScaleSpace_Behavior_of_Planar_Curve_Corners\figure_6.jpg
  Figure 6 caption: "(a) The END model with \u2212\u03C0< \u03B8 l + \u03B8 r <0 and\
    \ \u2212\u03C02< \u03B8 l < \u03B8 r <\u03C02 . (b) The LSS map. (c) The trajectory\
    \ produced by \u03C3 2 = F 1 (s) . (d) The trajectory produced by \u03C3 2 = F\
    \ 2 (s) ."
  Figure 7 Link: articels_figures_by_rev_year\2015\Laplacian_ScaleSpace_Behavior_of_Planar_Curve_Corners\figure_7.jpg
  Figure 7 caption: "(a) The END model with \u2212\u03C02< \u03B8 l = \u03B8 r <0\
    \ . (b) the LSS map. (c) The trajectory produced by \u03C3 2 = F 1 (s) . (d) The\
    \ trajectory produced by \u03C3 2 = F 2 (s) ."
  Figure 8 Link: articels_figures_by_rev_year\2015\Laplacian_ScaleSpace_Behavior_of_Planar_Curve_Corners\figure_8.jpg
  Figure 8 caption: "(a) The STAIR model with 0\u2264 \u03B8 l +(\u03C0\u2212 \u03B8\
    \ r )<\u03C0 and \u2212\u03C02< \u03B8 l <(\u03C0\u2212 \u03B8 r )<\u03C02 . (b)\
    \ The LSS map. (c) The trajectory produced by \u03C3 2 = F 1 (s) . (d) The trajectory\
    \ produced by \u03C3 2 = F 2 (s) ."
  Figure 9 Link: articels_figures_by_rev_year\2015\Laplacian_ScaleSpace_Behavior_of_Planar_Curve_Corners\figure_9.jpg
  Figure 9 caption: "(a) The STAIR model with \u2212\u03C02< \u03B8 l =(\u03C0\u2212\
    \ \u03B8 r )<\u03C02 . (b) The LSS map."
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xiaohong Zhang
  Name of the last author: Jeff Kymer
  Number of Figures: 13
  Number of Tables: 0
  Number of authors: 5
  Paper title: Laplacian Scale-Space Behavior of Planar Curve Corners
  Publication Date: 2015-01-23 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2396074
- Affiliation of the first author: school of life science and technology, university
    of electronic science and technology of china, chengdu, china
  Affiliation of the last author: school of life science and technology, university
    of electronic science and technology of china, chengdu
  Figure 1 Link: articels_figures_by_rev_year\2015\Color_Constancy_Using_DoubleOpponency\figure_1.jpg
  Figure 1 caption: The blue and green crosses in the scatter plot (d) denote the
    color distributions of the (a) color-biased and (b) canonical images [33], respectively.
    The red crosses in (d) show the color distribution of the responses of the double-opponent
    cells in our model (i.e., the DT map (c) shown in RGB space for display). The
    true illuminant direction is shown as a black solid line.
  Figure 10 Link: articels_figures_by_rev_year\2015\Color_Constancy_Using_DoubleOpponency\figure_10.jpg
  Figure 10 caption: (a) WST test on the SFU lab dataset and (b) SFU HDR dataset.
  Figure 2 Link: articels_figures_by_rev_year\2015\Color_Constancy_Using_DoubleOpponency\figure_2.jpg
  Figure 2 caption: "The receptive field (RF) of red-green single-opponent Type II\
    \ cells in (a) LGN with color-opponent centre-only RF and (b) Type I cells with\
    \ color-opponent centre-surround RF. (c) The RF of red-green double-opponent cells\
    \ in the primary visual cortex (V1) can be computationally constructed using (d)\
    \ two single-opponent Type II cells with different RF scales and opposite signs.\
    \ In the expression of \u201CA+\u201D or \u201CB-\u201D, the sign \u201C+\u201D\
    \ and \u201C-\u201D denote the excitation and inhibition, respectively. Adapted\
    \ from [1]."
  Figure 3 Link: articels_figures_by_rev_year\2015\Color_Constancy_Using_DoubleOpponency\figure_3.jpg
  Figure 3 caption: The flowchart of our DO based color constancy model. The symbol
    f 1 denotes the transformation from RGB to LMS space. f 2 denotes the transformation
    from DO to LMS space. l , m , s , y , and b denote respectively the components
    of the red, green, blue, yellow, and luminance channels. Pooling denotes the mechanism
    of max or sum .
  Figure 4 Link: articels_figures_by_rev_year\2015\Color_Constancy_Using_DoubleOpponency\figure_4.jpg
  Figure 4 caption: 'More examples illustrating the close fitting of the color distribution
    (red dots) of DO responses (i.e., DT map) in RGB space to the true illuminant
    (black line). From top to bottom: images are from the Gehler-Shi dataset [33],
    SFU lab dataset [69], and SFU HDR dataset [70], respectively.'
  Figure 5 Link: articels_figures_by_rev_year\2015\Color_Constancy_Using_DoubleOpponency\figure_5.jpg
  Figure 5 caption: The responses of model DO cells to a synthetic image rendered
    under greenish illuminant. With the flexible cone weights ( k ), the output of
    DO cells can encode both of the color edges and color regions.
  Figure 6 Link: articels_figures_by_rev_year\2015\Color_Constancy_Using_DoubleOpponency\figure_6.jpg
  Figure 6 caption: Mondrian images with less (first row) or more (second row) color
    blocks. The DT map and the corrected images are also shown (the angular error
    is shown on the lower right corner). k=0.9 and 0.3 are used for the DT map computation
    of the two images. The mondrian images were generated using the surface reflectance
    spectra combined with illuminant spectra from [69].
  Figure 7 Link: articels_figures_by_rev_year\2015\Color_Constancy_Using_DoubleOpponency\figure_7.jpg
  Figure 7 caption: "The influence of receptive field size ( \u03C3 ) and the cone\
    \ weight ( k ) of our model ( DOCC - LMS(max) ) on the measure of median angular\
    \ error. Left: Gehler-Shi dataset, middle: SFU lab dataset, right: SFU HDR dataset.\
    \ For experimental evaluation, in this study we respectively set k=0.1,0.6,0.7\
    \ and \u03C3=3.0,3.0,3.5 for Gehler-Shi dataset, SFU lab dataset, and SFU HDR\
    \ dataset."
  Figure 8 Link: articels_figures_by_rev_year\2015\Color_Constancy_Using_DoubleOpponency\figure_8.jpg
  Figure 8 caption: WST test on the Gehler-Shi dataset.
  Figure 9 Link: articels_figures_by_rev_year\2015\Color_Constancy_Using_DoubleOpponency\figure_9.jpg
  Figure 9 caption: Some examples of indoor and outdoor images from Gehler-Shi dataset
    corrected with multiple methods.
  First author gender probability: 0.95
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.66
  Name of the first author: Shao-Bing Gao
  Name of the last author: Yong-Jie Li
  Number of Figures: 11
  Number of Tables: 5
  Number of authors: 4
  Paper title: Color Constancy Using Double-Opponency
  Publication Date: 2015-01-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance of Various Methods on the Gehler-Shi Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance on the SFU Lab Dataset
  Table 3 caption:
    table_text: TABLE 3 Performance on the SFU HDR Dataset
  Table 4 caption:
    table_text: TABLE 4 Performance of Inter-Dataset Test
  Table 5 caption:
    table_text: TABLE 5 Performance of Two Non-Linear Operations
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2396053
- Affiliation of the first author: center for research in computer vision (crcv),
    university of central florida, orlando, fl
  Affiliation of the last author: center for research in computer vision (crcv), university
    of central florida, orlando, fl
  Figure 1 Link: articels_figures_by_rev_year\2015\Detecting_Humans_in_Dense_Crowds_Using_LocallyConsistent_Scale_Prior_and_Global_\figure_1.jpg
  Figure 1 caption: This figure shows several images from the dataset on which experiments
    were performed. Although there are severe occlusions and pose variations, the
    consistency in scale (size of humans) is evident in all images which can be used
    to restrict the space of detection hypotheses in these images.
  Figure 10 Link: articels_figures_by_rev_year\2015\Detecting_Humans_in_Dense_Crowds_Using_LocallyConsistent_Scale_Prior_and_Global_\figure_10.jpg
  Figure 10 caption: This figure visualizes the improvements using the three aspects
    of the proposed approach. Green boxes show false alarms, black represent false
    positives, while colors in the red to yellow range represent correct detections.
    In the first row, we show the improvement obtained by using combinations-of-parts
    detection. Results in both images are shown at the same precision, thus, a higher
    recall means better performance (shown with yellow arrows). The second row shows
    gain in performance obtained by using priors in addition to CoP detection. For
    clarity, only bounding boxes for heads are drawn. Similarly, the last row shows
    the results of Global Occlusion Reasoning. Yellow arrows indicate improved boxes,
    yellow-in-red arrows highlight false positives that were removed, while red arrow
    shows a box that worsened after GOR.
  Figure 2 Link: articels_figures_by_rev_year\2015\Detecting_Humans_in_Dense_Crowds_Using_LocallyConsistent_Scale_Prior_and_Global_\figure_2.jpg
  Figure 2 caption: "Human detection in DPM [18] is performed using L=67 levels of\
    \ the pyramid. Three pixel locations in given image, shown with different colors,\
    \ have different prior information on scale and confidence. In our approach, the\
    \ scale and confidence priors are discovered automatically which then provide\
    \ a 1 d scoring function at each pixel in the image, as shown on right. By transforming\
    \ the priors to each level of the pyramid, the confidence for detection hypotheses\
    \ is altered based on their consistency with the priors. Increasing the confidence\
    \ of scale-consistent but low-confidence hypotheses allows them to be detected\
    \ without incurring false positives in the rest of the image. Effectively, for\
    \ a 2304\xD73072 image, this amounts to re-scoring all the 3.85 million hypotheses."
  Figure 3 Link: articels_figures_by_rev_year\2015\Detecting_Humans_in_Dense_Crowds_Using_LocallyConsistent_Scale_Prior_and_Global_\figure_3.jpg
  Figure 3 caption: 'Intermediate computations of scale and confidence priors: (a)
    The scales and confidences from detections in an image are transformed into a
    2 d graph. (b) The observed scale prior is obtained using Equation (3), (c) which
    is then smoothed through MRF using Equation (4). The corresponding confidence
    prior is also shown in (d). Heat map is used in (b)-(d) where brighter colors
    indicate larger values.'
  Figure 4 Link: articels_figures_by_rev_year\2015\Detecting_Humans_in_Dense_Crowds_Using_LocallyConsistent_Scale_Prior_and_Global_\figure_4.jpg
  Figure 4 caption: 'Intermediate results on inferred scales after smoothing: Two
    images are shown in (a) and (c) whereas the inferred scale priors are shown in
    (b) and (d), respectively. Truncated quadratic cost in Equation (4), allows us
    to handle sharp discontinuities in the scale field, likely to happen at specific
    viewpoints and due to false positives. The image in (a) has a fountain, where
    there is a gradual change in scale around it (yellow arrow) but a sharp discontinuity
    across it (yellow bar). Similarly, in (c), the initial set of detections had a
    false positive at traffic light larger in size than the immediate neighbors. In
    both cases, the gradual change in scale and discontinuities were preserved by
    MRF.'
  Figure 5 Link: articels_figures_by_rev_year\2015\Detecting_Humans_in_Dense_Crowds_Using_LocallyConsistent_Scale_Prior_and_Global_\figure_5.jpg
  Figure 5 caption: This graph shows the improvement in performance obtained using
    the priors over three iterations. The y and x axes show precision and recall,
    respectively. The curve without the priors is in blue, while the curve with the
    priors after iterations is in red. Also shown are the results of global scale
    priors in greens. Details of experimental setup are in Section 4.
  Figure 6 Link: articels_figures_by_rev_year\2015\Detecting_Humans_in_Dense_Crowds_Using_LocallyConsistent_Scale_Prior_and_Global_\figure_6.jpg
  Figure 6 caption: Results after using scale-consistency and combinations-of-parts
    detection on an image containing almost 3,000 people. The result is 82 percent
    precision at 60 percent recall evaluated only on heads. White bounding boxes signify
    true positives, whereas black represents false positives.
  Figure 7 Link: articels_figures_by_rev_year\2015\Detecting_Humans_in_Dense_Crowds_Using_LocallyConsistent_Scale_Prior_and_Global_\figure_7.jpg
  Figure 7 caption: 'Linear constraints for Binary Integer Programming: Left shows
    the DPM model for a single person and the respective part numbers (from model
    trained on INRIA person dataset). To ensure all parts selected by IP are contiguous,
    we use chain constraints between parts as shown with different colors. Similarly,
    models for two occluding persons are shown on right. The overlap constraints ensure
    that the occluded parts are rejected by the algorithm, thus giving bounding boxes
    consisting of visible parts only.'
  Figure 8 Link: articels_figures_by_rev_year\2015\Detecting_Humans_in_Dense_Crowds_Using_LocallyConsistent_Scale_Prior_and_Global_\figure_8.jpg
  Figure 8 caption: 'Results of Occlusion Reasoning: Two individuals are shown in
    (a) with their bounding boxes for root and deformable parts. (b) After reasoning
    for occlusion, only visible parts are selected, thus resulting in better localization.'
  Figure 9 Link: articels_figures_by_rev_year\2015\Detecting_Humans_in_Dense_Crowds_Using_LocallyConsistent_Scale_Prior_and_Global_\figure_9.jpg
  Figure 9 caption: 'Statistics on the proposed UCF-HDDC dataset: On x-axis is the
    image id, while y -axis shows the number of human annotations in the image. The
    four green bars at the end have counts of 1,276 , 1,852 , 2,816 and 2,845 , respectively.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Haroon Idrees
  Name of the last author: Mubarak Shah
  Number of Figures: 15
  Number of Tables: 0
  Number of authors: 3
  Paper title: Detecting Humans in Dense Crowds Using Locally-Consistent Scale Prior
    and Global Occlusion Reasoning
  Publication Date: 2015-01-23 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2396051
- Affiliation of the first author: department of automation, tsinghua university,
    beijing, china
  Affiliation of the last author: department of automation, tsinghua university, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2015\Detection_and_Rectification_of_Distorted_Fingerprints\figure_1.jpg
  Figure 1 caption: Three impressions of the same finger from FVC2004 DB1. The left
    two are normal fingerprints, while the right one contains severe distortion. The
    match score between the left two according to VeriFinger 6.2 SDK [5], is much
    higher than the match score between the right two. This huge difference is due
    to distortion rather than overlapping area. As shown by red and green rectangles,
    the overlapping area is similar in two cases.
  Figure 10 Link: articels_figures_by_rev_year\2015\Detection_and_Rectification_of_Distorted_Fingerprints\figure_10.jpg
  Figure 10 caption: The Detection ROC curves of our previous algorithm [1] and current
    algorithm on the (a) FVC2004 DB1 and (b) Tsinghua DF database.
  Figure 2 Link: articels_figures_by_rev_year\2015\Detection_and_Rectification_of_Distorted_Fingerprints\figure_2.jpg
  Figure 2 caption: Flowchart of the proposed distortion detection and rectification
    system.
  Figure 3 Link: articels_figures_by_rev_year\2015\Detection_and_Rectification_of_Distorted_Fingerprints\figure_3.jpg
  Figure 3 caption: Flowchart of fingerprint distortion detection. O i represents
    ridge orientation at the i th sampling grid of registered orientation map, while
    P j represents ridge period at the j th sampling grid of registered period map.
    l 1 and l 2 represent the number of sampling points in registered orientation
    map and registered period map, respectively.
  Figure 4 Link: articels_figures_by_rev_year\2015\Detection_and_Rectification_of_Distorted_Fingerprints\figure_4.jpg
  Figure 4 caption: Examples of 10 distortion types in Tsinghua DF database. The blue
    arrows represent the directions of force or torque, and red grids represent the
    distortion grids which are calculated from matched minutiae between the normal
    fingerprint and the distorted fingerprint.
  Figure 5 Link: articels_figures_by_rev_year\2015\Detection_and_Rectification_of_Distorted_Fingerprints\figure_5.jpg
  Figure 5 caption: The center (indicated by red circle) and direction (indicated
    by red arrow) of two fingerprints.
  Figure 6 Link: articels_figures_by_rev_year\2015\Detection_and_Rectification_of_Distorted_Fingerprints\figure_6.jpg
  Figure 6 caption: 'Orientation maps of three patterns: (a) loop, (b) whorl, and
    (c) arch. A fingerprint is separated into two regions by the green separating
    lines crossing the center points. The orientation maps above the lines are similar,
    while orientation maps below the lines are very different.'
  Figure 7 Link: articels_figures_by_rev_year\2015\Detection_and_Rectification_of_Distorted_Fingerprints\figure_7.jpg
  Figure 7 caption: Flowchart of distorted fingerprint rectification.
  Figure 8 Link: articels_figures_by_rev_year\2015\Detection_and_Rectification_of_Distorted_Fingerprints\figure_8.jpg
  Figure 8 caption: First two principal components of distortion fields from the training
    set of Tsinghua DF database.
  Figure 9 Link: articels_figures_by_rev_year\2015\Detection_and_Rectification_of_Distorted_Fingerprints\figure_9.jpg
  Figure 9 caption: Estimating the distortion field of an input fingerprint is equal
    to searching its nearest neighbor in the database of distorted reference fingerprints.
    Here, for visualization purpose, only one reference fingerprint (the fingerprint
    located at the origin of the coordinate system) is used to generate the database
    of distorted reference fingerprints. In practice, multiple reference fingerprints
    are used to achieve better performance.
  First author gender probability: 0.72
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.63
  Name of the first author: Xuanbin Si
  Name of the last author: Yuxuan Luo
  Number of Figures: 19
  Number of Tables: 4
  Number of authors: 4
  Paper title: Detection and Rectification of Distorted Fingerprints
  Publication Date: 2015-02-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Fingerprint Databases Used in This Study1
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Statistics of Detection Error
  Table 3 caption:
    table_text: TABLE 3 Statistics of Rectification Error
  Table 4 caption:
    table_text: TABLE 4 Speed of the Proposed Distortion Detection and Rectification
      Algorithms
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2014.2345403
