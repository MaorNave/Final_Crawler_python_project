- Affiliation of the first author: school of computer science and technology, university
    of chinese academy of sciences, beijing, china
  Affiliation of the last author: school of computer science and technology, university
    of chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\A_Tale_of_HodgeRank_and_Spectral_Method_Target_Attack_Against_Rank_Aggregation_i\figure_1.jpg
  Figure 1 caption: "Comparative results of different attack methods against HodgeRank\
    \ and RankCentrality on simulated data. The target ranking is \u03C0 2 A . The\
    \ box plot illustrates the results of 50 trials with different original data.\
    \ The proposed methods will get the R-Rank to be 1 and Kendall- \u03C4 close to\
    \ 1. The first row (a-f) displays the results of HodgeRank. The second row (g-l)\
    \ is RankCentrality with the reversible stochastic transition matrix. The last\
    \ row (m-r) represents RankCentrality with irreversible stochastic transition\
    \ matrix. The odd columns (column 1, 3, 5) show the R-Rank values and the even\
    \ columns (column 2, 4, 6) are the results of Kendall- \u03C4 . The first two\
    \ columns exhibit the confrontation scenario with complete information and perfect\
    \ feedback. The second two columns reveal the results of adversary with complete\
    \ information and imperfect feedback. The third two columns indicate the incomplete\
    \ information and perfect feedback. The red frames (o & p) are failure cases where\
    \ the proposed methods could not make \u03C0 \u03B8 (1)= \u03C0 2 A (1)= \u03C0\
    \ \u03B8 \xAF (2) ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\A_Tale_of_HodgeRank_and_Spectral_Method_Target_Attack_Against_Rank_Aggregation_i\figure_2.jpg
  Figure 2 caption: Data distribution of poisoned pairwise comparisons on simulated
    data with complete information and perfect feedback. The vertical axis lists all
    possible attack target and the horizontal axis displays all possible pairwise
    comparisons. All results are based on the same observed data. (a) The victim is
    HodgeRank. (b) The victim is RankCentrality and the adversary leverages the reversible
    stochastic transition matrix. (c) The victim is RankCentrality and the adversary
    utilizes the irreversible stochastic transition matrix.
  Figure 3 Link: articels_figures_by_rev_year\2022\A_Tale_of_HodgeRank_and_Spectral_Method_Target_Attack_Against_Rank_Aggregation_i\figure_3.jpg
  Figure 3 caption: "The modification of pairwise comparisons after attack. Here the\
    \ victim is HodgeRank and \u03C0 2 A (1)=9 .the goal of the adversary is 9,10,8,7,6,5,4,3,2,1\
    \ and the original aggregated result is 10,9,8,7,6,5,4,3,2,1 . The horizontal\
    \ axis lists all possible pairwise comparisons. We index them as follows: No.\
    \ (i\u22121)\u22179+1 to i\u22179 are the comparisons (i,j) | j\u2208[10],j\u2260\
    i . The size of red dot stands for the number of pairwise comparisons after attack.\
    \ The larger the radius of the red dot, the more the number of the perturbed pairwise\
    \ comparisons. The vertical axis shows the changes in each comparison. The positivenegative\
    \ vertical coordinate of red dot stands for the incrementdecrement."
  Figure 4 Link: articels_figures_by_rev_year\2022\A_Tale_of_HodgeRank_and_Spectral_Method_Target_Attack_Against_Rank_Aggregation_i\figure_4.jpg
  Figure 4 caption: "Correlations between the original simulated data and different\
    \ modified data, where \u03C0 2 A (1)=9 . The victim is HodgeRank. The scatter\
    \ plots in the lower triangle part show a pair of data whose source is labeled\
    \ in the \u201Cgray\u201D regions. The upper left points of each scatter plot\
    \ belong to the data labeled by the top \u201Cgray\u201D regions; the lower right\
    \ points come from the right gay box. In scatter plot, each point represents one\
    \ type of pairwise comparison and these points are in ascending order of the weight.\
    \ It is noteworthy that the horizontal axis of the scatter plot also shows the\
    \ weight but not the index of pairwise comparison. The Pearson correlations of\
    \ every pair of data are calculated in the upper triangle part and the \u201C\
    \ \u2217\u2217\u2217 \u201D stands for the p -value is smaller than 0.001. The\
    \ diagonal blocks are the data distribution normalized by the total number of\
    \ pairwise comparisons."
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.89
  Name of the first author: Ke Ma
  Name of the last author: Qingming Huang
  Number of Figures: 4
  Number of Tables: 2
  Number of authors: 6
  Paper title: 'A Tale of HodgeRank and Spectral Method: Target Attack Against Rank
    Aggregation is the Fixed Point of Adversarial Game'
  Publication Date: 2022-07-14 00:00:00
  Table 1 caption: TABLE 1 Numeric Results of Different Attack Methods on Dublin Election
    Data
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Numeric Results of Different Attack Methods on Human Age
    Data
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3190939
- Affiliation of the first author: department of computer science and technology,
    tsinghua university, beijing, china
  Affiliation of the last author: department of computer science and technology, tsinghua
    university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\DeepLogic_Joint_Learning_of_Neural_Perception_and_Logical_Reasoning\figure_1.jpg
  Figure 1 caption: "The proposed DeepLogic framework. The forward pass (top) is processed\
    \ sequentially from semantic input x through intermediate symbolic attribute z\
    \ to the final deductive label y . For example, to reason on the relationship\
    \ between, and . First, the system recognize these images into symbols as:\u278A\
    , \u278B and \u278C with the neural perception model. Then, the logic reasoning\
    \ model reasons the relationship between \u278A, \u278B and \u278C, and concludes\
    \ that they meet the logic formula: \u201C\u278A add \u278B equals \u278C\u201D\
    . In the backward pass (bottom left bottom right), the parameters of perception\
    \ model \u03B8 and symbolic system \u03D5 are iteratively optimized with the other\
    \ one as supervision, respectively."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\DeepLogic_Joint_Learning_of_Neural_Perception_and_Logical_Reasoning\figure_2.jpg
  Figure 2 caption: "(a) A single Logic layer as defined in Eq. (10); (b) The illustration\
    \ of deep-logic module (DLM) as is presented in Section 4.2. After the learning\
    \ process, those edges with the biggest weights are preserved, and we obtain the\
    \ formula \u201C Eq(Add( Z 1 , Z 2 ), Z 3 ) \u201D."
  Figure 3 Link: articels_figures_by_rev_year\2022\DeepLogic_Joint_Learning_of_Neural_Perception_and_Logical_Reasoning\figure_3.jpg
  Figure 3 caption: "(a) The forward pass of DeepLogic from p \u03B8 to \u0394 \u03D5\
    \ . (b) The backward pass of DeepLogic with the BPTL algorithm (Algorithm 1).\
    \ (c) Illustration of several cases of deeplogic formulas: (I) the formula for\
    \ \u201C And(Eq( Z 1 , Z 2 ),Eq( Z 2 , Z 3 )) \u201D; (II) A case that two layers\
    \ define the same term \u201C Add( Z 1 , Z 2 ) \u201D (black line and gray line);\
    \ (III) and (IV) ill self-conflict case of formulas. In (III) the equation is\
    \ always \u201CTrue,\u201D and in (IV) the BPTL algorithm will encounter self-confliction\
    \ in the middle node."
  Figure 4 Link: articels_figures_by_rev_year\2022\DeepLogic_Joint_Learning_of_Neural_Perception_and_Logical_Reasoning\figure_4.jpg
  Figure 4 caption: 'Top: test accuracy with different scales of training images on
    MNIST-ADD- alpha , where DL is short for DeepLogic. Bottom: test accuracy with
    different model hidden sizes and different dropout probabilities of RN and DL.'
  Figure 5 Link: articels_figures_by_rev_year\2022\DeepLogic_Joint_Learning_of_Neural_Perception_and_Logical_Reasoning\figure_5.jpg
  Figure 5 caption: 'Top: Perception accuracy when pretraining ptheta on MNIST-ADD-
    alpha dataset; Middle: Training Logic accuracy of DeepLogic- with different batches
    of pretraining data on MNIST-ADD- alpha datasets; Bottom: Training Logic accuracy
    of DeepLogic with different batches of pretraining data on MNIST-ADD- alpha dataset.
    The major findings are: 1) more pretraining batches ensures better accuracy; 2)
    only very few pretraining is required for DeepLogic to finally converge.'
  Figure 6 Link: articels_figures_by_rev_year\2022\DeepLogic_Joint_Learning_of_Neural_Perception_and_Logical_Reasoning\figure_6.jpg
  Figure 6 caption: Average reward of three formulas and accuracy of two attributes
    of DeepLogic on C-MNIST-RULE. Best viewed in color.
  Figure 7 Link: articels_figures_by_rev_year\2022\DeepLogic_Joint_Learning_of_Neural_Perception_and_Logical_Reasoning\figure_7.jpg
  Figure 7 caption: 'Left: data illustrations from C-MNIST-RULE dataset where the
    number attribute follows the ADD rule while the color attribute follows the Mutual
    Exclusion rule. Right: trainingtesting illustration for the number attribute and
    Add rule. (a) and (b) illustrates the learning process of pphi and ptheta resptively
    using Eqs. (22) and (16); (c) and (d) are unsuccessful cases due to perception
    error (non-optimal ptheta ) and logic structure error(non-optimal pphi ), respetively;
    (e) successful cases.'
  Figure 8 Link: articels_figures_by_rev_year\2022\DeepLogic_Joint_Learning_of_Neural_Perception_and_Logical_Reasoning\figure_8.jpg
  Figure 8 caption: Illustration of the DLM module in RPM task. Images are treated
    as inputs and then fed into logic layers, where the logic operation is selected
    from all the possible candidate combinations.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.92
  Name of the first author: Xuguang Duan
  Name of the last author: Wenwu Zhu
  Number of Figures: 8
  Number of Tables: 4
  Number of authors: 5
  Paper title: 'DeepLogic: Joint Learning of Neural Perception and Logical Reasoning'
  Publication Date: 2022-07-15 00:00:00
  Table 1 caption: TABLE 1 Accuracy on the MNIST-ADD Dataset, Where EXTRA SUP Indicates
    Whether the Model Is Trained With Extra Perception Supervision or the Only One-Bit
    Logic Supervision, EXTRA TOOL Indicats Whether the Model Uses Any Extra Tools
    or Not
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Accuracy on C-MNIST-RULE, Where f f Indicates the Model
    is Trained With Extra Symbol Annotations and w w Indicates no Extra Symbol Annotations
    are Involved
  Table 3 caption: TABLE 3 Typical Formulas Learned Under Different Settings in MNIST-ADD
    Dataset
  Table 4 caption: TABLE 4 Testing Accuracy on RAVEN Dataset
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3191093
- Affiliation of the first author: department of electrical and computer engineering,
    centre for computer vision and deep learning, university of windsor, windsor,
    on, canada
  Affiliation of the last author: department of electrical and computer engineering,
    centre for computer vision and deep learning, university of windsor, windsor,
    on, canada
  Figure 1 Link: articels_figures_by_rev_year\2022\A_Review_of_Generalized_ZeroShot_Learning_Methods\figure_1.jpg
  Figure 1 caption: A schematic diagram of ZSL versus GZSL. Assume that the seen class
    contains samples of Otter and Tiger, while the unseen class contains samples of
    Polar bear and Zebra. (a) During the training phase, both GZSL and ZSL methods
    have access to the samples and semantic representations of the seen class. (b)
    During the test phase, ZSL can only recognize samples from the unseen class, while
    (c) GZSL is able to recognize samples from both seen and unseen classes.
  Figure 10 Link: articels_figures_by_rev_year\2022\A_Review_of_Generalized_ZeroShot_Learning_Methods\figure_10.jpg
  Figure 10 caption: A general view of the f-CLSWGAN model [139]). It learns a generative
    model based on the visual features of seen classes conditioned on their corresponding
    semantic representations. It also uses classification loss to generate more discriminative
    visual features.
  Figure 2 Link: articels_figures_by_rev_year\2022\A_Review_of_Generalized_ZeroShot_Learning_Methods\figure_2.jpg
  Figure 2 caption: A schematic view of Transductive and Inductive settings. In inductive
    setting, only the visual features and semantic representations of the seen classes
    ( A and B ) are available. While transductive setting, in addition to the seen
    class information, has access to the unlabelled visual samples of the unseen classes.
  Figure 3 Link: articels_figures_by_rev_year\2022\A_Review_of_Generalized_ZeroShot_Learning_Methods\figure_3.jpg
  Figure 3 caption: A schematic view of different embedding spaces in GZSL (adapted
    from [58]).
  Figure 4 Link: articels_figures_by_rev_year\2022\A_Review_of_Generalized_ZeroShot_Learning_Methods\figure_4.jpg
  Figure 4 caption: A schematic view of the projection domain shift problem (adapted
    from [66]).
  Figure 5 Link: articels_figures_by_rev_year\2022\A_Review_of_Generalized_ZeroShot_Learning_Methods\figure_5.jpg
  Figure 5 caption: A schematic view of the bias concerning seen classes (source)
    in the semantic embedding space (adapted from [72]).
  Figure 6 Link: articels_figures_by_rev_year\2022\A_Review_of_Generalized_ZeroShot_Learning_Methods\figure_6.jpg
  Figure 6 caption: Embedding-based versus generative-based methods. The embedding
    based methods (a) lean an embedding space to project the visual and semantic features
    of seen classes into a common space. Then, the learned embedding space is used
    to perform recognition. In contrast, the generative-based methods (b) learn a
    generative model based on samples of seen classes conditioned on their semantic
    features. Then, the learned model is used to generate visual features for unseen
    classes using the semantic features of unseen classes.
  Figure 7 Link: articels_figures_by_rev_year\2022\A_Review_of_Generalized_ZeroShot_Learning_Methods\figure_7.jpg
  Figure 7 caption: The taxonomy of GZSL models.
  Figure 8 Link: articels_figures_by_rev_year\2022\A_Review_of_Generalized_ZeroShot_Learning_Methods\figure_8.jpg
  Figure 8 caption: A schematic view of SRG [63]. First, the image prototype fk and
    semantic prototype ek of each class are reconstructed using the cluster center
    and (2), respectively. Then, the shared reconstruction coefficients between two
    spaces is learned. Finally, the learned SRG is used to synthesize class prototypes
    for unseen classes to perform prediction.
  Figure 9 Link: articels_figures_by_rev_year\2022\A_Review_of_Generalized_ZeroShot_Learning_Methods\figure_9.jpg
  Figure 9 caption: A schematic of DAZLE [83]. After extracting image features of
    the R regions, the attention features of all attributes are computed using a dense
    attention mechanism. Then, the attention features are aligned with the attribute
    semantic vectors, in order to compute the score of attributes in the image.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Farhad Pourpanah
  Name of the last author: Q. M. Jonathan Wu
  Number of Figures: 12
  Number of Tables: 0
  Number of authors: 8
  Paper title: A Review of Generalized Zero-Shot Learning Methods
  Publication Date: 2022-07-18 00:00:00
  Table 1 caption: Not Available
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3191696
- Affiliation of the first author: school of computer science and technology, beijing
    institute of technology (bit), beijing, china
  Affiliation of the last author: school of artificial intelligence, beijing normal
    university (bnu), beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\MultiOriented_Object_Detection_in_Aerial_Images_With_Double_Horizontal_Rectangle\figure_1.jpg
  Figure 1 caption: Comparison of different representations for multi-oriented objects.
    Both rotated rectangle and quadrilateral representation suffer from discontinuity
    problem, where some similar objects may have very different representations. The
    proposed DHRec does not have such problem.
  Figure 10 Link: articels_figures_by_rev_year\2022\MultiOriented_Object_Detection_in_Aerial_Images_With_Double_Horizontal_Rectangle\figure_10.jpg
  Figure 10 caption: Some qualitative detection results of the proposed method on
    DOTA dataset. The proposed method accurately detects densely distributed and arbitrary-oriented
    objects with different sizes.
  Figure 2 Link: articels_figures_by_rev_year\2022\MultiOriented_Object_Detection_in_Aerial_Images_With_Double_Horizontal_Rectangle\figure_2.jpg
  Figure 2 caption: The proposed multi-oriented object encoding with double horizontal
    rectangles (DHRec). The double horizontal rectangles (blue and green outline)
    are generated from the sorted horizontal and vertical coordinates of the oriented
    object (red outline).
  Figure 3 Link: articels_figures_by_rev_year\2022\MultiOriented_Object_Detection_in_Aerial_Images_With_Double_Horizontal_Rectangle\figure_3.jpg
  Figure 3 caption: All four potential objects in oriented rectangle shapes for a
    given DHRec encoding.
  Figure 4 Link: articels_figures_by_rev_year\2022\MultiOriented_Object_Detection_in_Aerial_Images_With_Double_Horizontal_Rectangle\figure_4.jpg
  Figure 4 caption: "Illustration of defining the two position factors \u0394 t and\
    \ \u0394 l for multi-oriented object decoding. \u0394 t and \u0394 l are mainly\
    \ given by the area ratios between corresponding green filled regions, guiding\
    \ the selection of top and left vertex."
  Figure 5 Link: articels_figures_by_rev_year\2022\MultiOriented_Object_Detection_in_Aerial_Images_With_Double_Horizontal_Rectangle\figure_5.jpg
  Figure 5 caption: "An example of wrongly regarding nearly horizontal object as thin\
    \ oriented one due to regression error of \u0394 ~ t . The revision guided by\
    \ obliquity factor r corrects such mistake."
  Figure 6 Link: articels_figures_by_rev_year\2022\MultiOriented_Object_Detection_in_Aerial_Images_With_Double_Horizontal_Rectangle\figure_6.jpg
  Figure 6 caption: Illustration of the proposed approximated IoU computation using
    shrunk horizontal rectangles from DHRec. This improves the efficiency of the rotated
    NMS required in inference phase.
  Figure 7 Link: articels_figures_by_rev_year\2022\MultiOriented_Object_Detection_in_Aerial_Images_With_Double_Horizontal_Rectangle\figure_7.jpg
  Figure 7 caption: "Distribution of mean IoU for the proposed DHRec and two baselines\
    \ with respect to different ranges of the objects orientation. Both rotated rectangle\
    \ and quadrilateral representation have a large performance drop for objects around\
    \ some orientation (e.g., 0 \u2218 ) close to periods boundary suffering from\
    \ discontinuity issues. The proposed DHRec is able to accurately locate objects\
    \ of arbitrary orientation."
  Figure 8 Link: articels_figures_by_rev_year\2022\MultiOriented_Object_Detection_in_Aerial_Images_With_Double_Horizontal_Rectangle\figure_8.jpg
  Figure 8 caption: "Quantitative analysis (on DOTA) on distribution of position factor\
    \ \u0394 t and obliquity factor r in (a), regression accuracy in terms of mean\
    \ absolute error (MAE) for r and \u0394 t in (b-c), and effect of revision with\
    \ r in (d-e). Nearly horizontal objects occupy a large portion on DOTA. The regression\
    \ of \u0394 t and r is quite accurate. The proposed revision based on r effectively\
    \ corrects the mistakes of regarding nearly horizontal objects as thin oriented\
    \ ones during multi-oriented object decoding from DHRec."
  Figure 9 Link: articels_figures_by_rev_year\2022\MultiOriented_Object_Detection_in_Aerial_Images_With_Double_Horizontal_Rectangle\figure_9.jpg
  Figure 9 caption: Some qualitative results of the proposed method on ICDAR2015 [53],
    ICDAR2017-MLT [54], and SKU110K-R [39].
  First author gender probability: 0.59
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.61
  Name of the first author: Guangtao Nie
  Name of the last author: Hua Huang
  Number of Figures: 11
  Number of Tables: 6
  Number of authors: 2
  Paper title: Multi-Oriented Object Detection in Aerial Images With Double Horizontal
    Rectangles
  Publication Date: 2022-07-18 00:00:00
  Table 1 caption: TABLE 1 Comparisons of the Proposed DHRec Encoding With Rotated
    Rectangle and Quadrilateral Representation on DOTA
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Runtime (In FPS) and Performance (In mAP) Comparison Using
    Different Variants of Rotated NMS During Inference on DOTA
  Table 3 caption: TABLE 3 Quantitative Comparison of the Proposed Method with Two
    Baseline Models Based on ResNet-50 on Scene Text Detection and Packed Oriented
    Object Detection
  Table 4 caption: TABLE 4 Quantitative Evaluation of Applying the Proposed DHRec
    Module to Different Baseline Methods Using the ResNet-50 Backbone on DOTA Dataset
  Table 5 caption: TABLE 5 Quantitative Comparison Between the Proposed Method and
    Some State-of-the-Art Methods Dedicated to Mitigating the Discontinuity Problem
    for Classical Representations on DOTA
  Table 6 caption: TABLE 6 Quantitative Evaluation Using VOC2007 Metric of the Proposed
    DHRec and Some State-of-the-Art Detectors on HRSC2016
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3191753
- Affiliation of the first author: department of automation, tsinghua university,
    beijing, china
  Affiliation of the last author: department of automation, tsinghua university, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2022\Adjacency_Constraint_for_Efficient_Hierarchical_Reinforcement_Learning\figure_1.jpg
  Figure 1 caption: 'A high-level illustration of our motivation: distant subgoals
    g 1 , g 2 , g 3 (blue) can be surrogated by closer subgoals g ~ 1 , g ~ 2 , g
    ~ 3 (yellow) that fall into the k -step adjacent regions.'
  Figure 10 Link: articels_figures_by_rev_year\2022\Adjacency_Constraint_for_Efficient_Hierarchical_Reinforcement_Learning\figure_10.jpg
  Figure 10 caption: Learning curves of HRAC and baselines on locomotion tasks.
  Figure 2 Link: articels_figures_by_rev_year\2022\Adjacency_Constraint_for_Efficient_Hierarchical_Reinforcement_Learning\figure_2.jpg
  Figure 2 caption: "The goal-conditioned HRL framework combined with the proposed\
    \ k -step adjacency constraint, which is implemented by the adjacency network\
    \ \u03C8 (dashed orange box)."
  Figure 3 Link: articels_figures_by_rev_year\2022\Adjacency_Constraint_for_Efficient_Hierarchical_Reinforcement_Learning\figure_3.jpg
  Figure 3 caption: "A high-level illustration of our theoretical result in stochastic\
    \ MDPs. \u0394 g ( G AM ) and \u0394 g ~ ( G AM ) denote the k -step state distribution\
    \ induced by the subgoal g and the subgoal distribution g ~ respectively."
  Figure 4 Link: articels_figures_by_rev_year\2022\Adjacency_Constraint_for_Efficient_Hierarchical_Reinforcement_Learning\figure_4.jpg
  Figure 4 caption: "The functionality of the adjacency network. The k -step adjacent\
    \ region is mapped to an \u03F5 k -ball with euclidean metric in the adjacency\
    \ space, where e g i = \u03C8 \u03D5 ( g i ),i=1,2,3 ."
  Figure 5 Link: articels_figures_by_rev_year\2022\Adjacency_Constraint_for_Efficient_Hierarchical_Reinforcement_Learning\figure_5.jpg
  Figure 5 caption: Discrete control environments used in our experiments. (a) Key-Chest.
    (b) Maze.
  Figure 6 Link: articels_figures_by_rev_year\2022\Adjacency_Constraint_for_Efficient_Hierarchical_Reinforcement_Learning\figure_6.jpg
  Figure 6 caption: Robot arm manipulation environments used in our experiments. (a)
    FetchPush. (b) FetchPickAndPlace.
  Figure 7 Link: articels_figures_by_rev_year\2022\Adjacency_Constraint_for_Efficient_Hierarchical_Reinforcement_Learning\figure_7.jpg
  Figure 7 caption: Quadrupedal robot locomotion environments used in our experiments.
    (a) Ant Gather (adapted from Duan et al. [38]). (b) Ant Maze. (c) Ant Maze Sparse.
    (d) Ant Push.
  Figure 8 Link: articels_figures_by_rev_year\2022\Adjacency_Constraint_for_Efficient_Hierarchical_Reinforcement_Learning\figure_8.jpg
  Figure 8 caption: Learning curves of HRAC and baselines on discrete control tasks.
  Figure 9 Link: articels_figures_by_rev_year\2022\Adjacency_Constraint_for_Efficient_Hierarchical_Reinforcement_Learning\figure_9.jpg
  Figure 9 caption: Learning curves of HRAC and baselines on manipulation tasks.
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Tianren Zhang
  Name of the last author: Feng Chen
  Number of Figures: 15
  Number of Tables: 0
  Number of authors: 5
  Paper title: Adjacency Constraint for Efficient Hierarchical Reinforcement Learning
  Publication Date: 2022-07-19 00:00:00
  Table 1 caption: Not Available
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3192418
- Affiliation of the first author: nation lab of radar signal processing, xidian university,
    xian, shaanxi, china
  Affiliation of the last author: mccombs school of business, the university of texas
    at austin, tx, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\Generative_Text_Convolutional_Neural_Network_for_Hierarchical_Document_Represent\figure_1.jpg
  Figure 1 caption: Overview of the proposed GTCNN (lower part) and its corresponding
    hierarchical Weibull convolutional inference network (upper part).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Generative_Text_Convolutional_Neural_Network_for_Hierarchical_Document_Represent\figure_2.jpg
  Figure 2 caption: Overview of the attention-based convolutional pooling mechanism.
  Figure 3 Link: articels_figures_by_rev_year\2022\Generative_Text_Convolutional_Neural_Network_for_Hierarchical_Document_Represent\figure_3.jpg
  Figure 3 caption: Point log-likelihood of GTCNNs on MR and TREC datasets as a function
    of time with various network structures.
  Figure 4 Link: articels_figures_by_rev_year\2022\Generative_Text_Convolutional_Neural_Network_for_Hierarchical_Document_Represent\figure_4.jpg
  Figure 4 caption: Classification accuracy ( % ) of GTCNNs on MR and TREC datasets
    as a function of the depth with various network structures.
  Figure 5 Link: articels_figures_by_rev_year\2022\Generative_Text_Convolutional_Neural_Network_for_Hierarchical_Document_Represent\figure_5.jpg
  Figure 5 caption: Example of hierarchical phrase-level topics learned from TREC
    dataset by a three-layer GTCNN with the network structure 200-100-50, including
    all the lower-layer topic nodes (directly or indirectly) linked to the 11th node
    at the top layer. The detailed visualization procedure has been described in Section
    5.2.3.
  Figure 6 Link: articels_figures_by_rev_year\2022\Generative_Text_Convolutional_Neural_Network_for_Hierarchical_Document_Represent\figure_6.jpg
  Figure 6 caption: Visualizations of convolutional kernels learned by a single-layer
    GTCNN on TREC dataset, where the underlying score reflects the probability of
    the word appearing in the column of the corresponding kernel.
  Figure 7 Link: articels_figures_by_rev_year\2022\Generative_Text_Convolutional_Neural_Network_for_Hierarchical_Document_Represent\figure_7.jpg
  Figure 7 caption: Supervised comparisons of the classification accuracy as a function
    of training time on (a) IMDB and (b) ELEC.
  Figure 8 Link: articels_figures_by_rev_year\2022\Generative_Text_Convolutional_Neural_Network_for_Hierarchical_Document_Represent\figure_8.jpg
  Figure 8 caption: Semi-supervised classification comparisons with different proportions
    of labeled dataset on (a) IMDB and (b) ELEC.
  Figure 9 Link: articels_figures_by_rev_year\2022\Generative_Text_Convolutional_Neural_Network_for_Hierarchical_Document_Represent\figure_9.jpg
  Figure 9 caption: Examples of the attentions learned by sGTCNN-att on (a) ELEC and
    (b) Dbpeida respectively, where the red regions denotes the attentions on the
    1st hidden layer and the blue ones for the 2nd hidden layer.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Chaojie Wang
  Name of the last author: Mingyuan Zhou
  Number of Figures: 9
  Number of Tables: 7
  Number of authors: 6
  Paper title: Generative Text Convolutional Neural Network for Hierarchical Document
    Representation Learning
  Publication Date: 2022-07-19 00:00:00
  Table 1 caption: TABLE 1 Notations of Variables and Hyperparameters in GTCNN
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Summary Statistics for the Datasets After Tokenization
  Table 3 caption: TABLE 3 Hyperparameter Settings in GTCNN
  Table 4 caption: TABLE 4 Comparisons of Word Perplexity on Three Different Benchmark
    Datasets
  Table 5 caption: TABLE 5 Comparisons of Classification Accuracy on Unsupervisedly
    Extracted Latent Document Representations and Average Training Time (Seconds per
    Gibbs Sampling Iteration Across all Samples) on Three Different Benchmark Datasets
  Table 6 caption: TABLE 6 Comparisons of Classification Accuracy and Number of Model
    Parameters Between Supervised Models
  Table 7 caption: TABLE 7 Comparisons of the Testing Time (Seconds) on RTX 1080 Ti
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3192319
- Affiliation of the first author: department of statistics, indiana university, bloomington,
    in, usa
  Affiliation of the last author: nike, san francisco, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\The_Geometry_of_Nonlinear_Embeddings_in_Kernel_Discriminant_Analysis\figure_1.jpg
  Figure 1 caption: 'Scatterplots of the samples simulated from a mixture of two normal
    distributions with contours of the probability densities for each class overlaid
    in two settings: (a) Scenario 1 and (b) Scenario 2.'
  Figure 10 Link: articels_figures_by_rev_year\2022\The_Geometry_of_Nonlinear_Embeddings_in_Kernel_Discriminant_Analysis\figure_10.jpg
  Figure 10 caption: Contours of the approximate discriminant functions using random
    Fourier features under Scenario 1. The value of D in each panel indicates the
    number of random Fourier features.
  Figure 2 Link: articels_figures_by_rev_year\2022\The_Geometry_of_Nonlinear_Embeddings_in_Kernel_Discriminant_Analysis\figure_2.jpg
  Figure 2 caption: Contours of the population discriminant functions with homogeneous
    polynomial kernels of degree 1 to 4 (upper panels from left to right) and their
    corresponding sample counterparts (lower panels) under Scenario 1. The black dashed
    lines are the optimal classification boundary.
  Figure 3 Link: articels_figures_by_rev_year\2022\The_Geometry_of_Nonlinear_Embeddings_in_Kernel_Discriminant_Analysis\figure_3.jpg
  Figure 3 caption: Contours of the population discriminant functions with inhomogeneous
    polynomial kernels of degree 2 to 4 (upper panels from left to right) and their
    corresponding sample counterparts (lower panels) under Scenario 1. The black dashed
    lines are the optimal classification boundary.
  Figure 4 Link: articels_figures_by_rev_year\2022\The_Geometry_of_Nonlinear_Embeddings_in_Kernel_Discriminant_Analysis\figure_4.jpg
  Figure 4 caption: Contours of the population discriminant functions with homogeneous
    polynomial kernels of degree 1 to 4 (upper panels) and their sample counterparts
    (lower panels) under Scenario 2. The black dashed lines are the optimal classification
    boundaries.
  Figure 5 Link: articels_figures_by_rev_year\2022\The_Geometry_of_Nonlinear_Embeddings_in_Kernel_Discriminant_Analysis\figure_5.jpg
  Figure 5 caption: Contours of the population discriminant functions with inhomogeneous
    polynomial kernels of degree 2 to 4 (upper panels) and their sample counterparts
    (lower panels) under Scenario 2. The black dashed lines are the optimal classification
    boundaries.
  Figure 6 Link: articels_figures_by_rev_year\2022\The_Geometry_of_Nonlinear_Embeddings_in_Kernel_Discriminant_Analysis\figure_6.jpg
  Figure 6 caption: "Contours of the population discriminant functions with inhomogeneous\
    \ polynomial kernels of degree 2 (upper panels) and 4 (lower panels) under Scenario\
    \ 2 with varying class proportions (left panels: \u03C0 1 =0.1 , middle panels:\
    \ \u03C0 1 =0.3 and right panels: \u03C0 1 =0.5 ). The black lines are the optimal\
    \ classification boundaries obtained from the population densities incorporating\
    \ the class proportions."
  Figure 7 Link: articels_figures_by_rev_year\2022\The_Geometry_of_Nonlinear_Embeddings_in_Kernel_Discriminant_Analysis\figure_7.jpg
  Figure 7 caption: "The ratio of between-class variation to within-class variation\
    \ ( \u03BB N ) as a function of the truncation degree N under (a) Scenario 1 and\
    \ (b) Scenario 2."
  Figure 8 Link: articels_figures_by_rev_year\2022\The_Geometry_of_Nonlinear_Embeddings_in_Kernel_Discriminant_Analysis\figure_8.jpg
  Figure 8 caption: Contours of the population Gaussian discriminants approximated
    by polynomials truncated at degree 14 under (a) Scenario 1 and (b) Scenario 2.
    The black dashed lines are the optimal classification boundaries.
  Figure 9 Link: articels_figures_by_rev_year\2022\The_Geometry_of_Nonlinear_Embeddings_in_Kernel_Discriminant_Analysis\figure_9.jpg
  Figure 9 caption: "The ratio of between-class variation to within-class variation\
    \ ( \u03BB D ) as a function of the number of random Fourier features D under\
    \ (a) Scenario 1 and (b) Scenario 2. The red vertical lines indicate the number\
    \ of random features used in Figs. 10 and 11."
  First author gender probability: 0.7
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Jiae Kim
  Name of the last author: Zhiyu Liang
  Number of Figures: 15
  Number of Tables: 3
  Number of authors: 3
  Paper title: The Geometry of Nonlinear Embeddings in Kernel Discriminant Analysis
  Publication Date: 2022-07-20 00:00:00
  Table 1 caption: TABLE 1 Coefficients for the Population Polynomial Discriminants
    Under Scenario 1
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Coefficients for the Population Polynomial Discriminants
    Under Scenario 2
  Table 3 caption: TABLE 3 Test Error Rates of Kernel Discriminant Analysis on the
    Spam Email Data Set With the Inhomogeneous Polynomial Kernels of Varying Degrees
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3192726
- Affiliation of the first author: department of computer science, the graduate center,
    city university of new york, new york, ny, usa
  Affiliation of the last author: department of electrical engineering, the city college,
    and the department of computer science graduate center, city university of new
    york, new york, ny, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\Deep_LearningBased_Action_Detection_in_Untrimmed_Videos_A_Survey\figure_1.jpg
  Figure 1 caption: "Temporal action detection aims to localize action instances in\
    \ time and recognize their categories. The first row demonstrates an example of\
    \ action \u201Clong jump\u201D detected in an untrimmed video from THUMOS14 dataset\
    \ [1]. The second row is an example of an untrimmed video including several action\
    \ instances of interest with various lengths."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Deep_LearningBased_Action_Detection_in_Untrimmed_Videos_A_Survey\figure_2.jpg
  Figure 2 caption: "Task Relations: (a) Temporal detection of action \u201CLong Jump\u201D\
    \ on THUMOS14 [1]. (b) Temporal detection (segmentation) of fine-grained actions\
    \ shown by different colors in a \u201Cmaking pancakes\u201D video on Breakfast\
    \ [11]. (c) and (d) Results from [16] on PASCAL [17]."
  Figure 3 Link: articels_figures_by_rev_year\2022\Deep_LearningBased_Action_Detection_in_Untrimmed_Videos_A_Survey\figure_3.jpg
  Figure 3 caption: Anchor-free versus anchor-based proposal generation.
  Figure 4 Link: articels_figures_by_rev_year\2022\Deep_LearningBased_Action_Detection_in_Untrimmed_Videos_A_Survey\figure_4.jpg
  Figure 4 caption: "Spatio-temporal activity detection: action \u201Dlong jump\u201D\
    \ is localized in time and space. Other than temporal interval of the action,\
    \ bounding box of the person performing the action is detected in each frame."
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.96
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.74
  Name of the first author: Elahe Vahdani
  Name of the last author: Yingli Tian
  Number of Figures: 4
  Number of Tables: 6
  Number of authors: 2
  Paper title: 'Deep Learning-Based Action Detection in Untrimmed Videos: A Survey'
  Publication Date: 2022-07-25 00:00:00
  Table 1 caption: TABLE 1 Main Categories of Temporal Action Detection Task With
    Different Supervision Levels in Training Set
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The Benchmark Datasets for Temporal, Spatio-Temporal, and
    Online Action Detection
  Table 3 caption: TABLE 3 Offline Spatio-Temporal Action Detection Performance on
    AVA 2.1 Validation Set, Measured by mAP (%) for IoU =0.5 =0.5
  Table 4 caption: TABLE 4 Online Temporal Action Detection Performance on THUMOS14
    and TVSeries in Terms of mAP and cAP, Respectively
  Table 5 caption: TABLE 5 Performance of Offline Temporal Action Detection Methods
    on Testing Set of THUMOS14 and Validation Set of ActivityNet (V is the Version)
    Measured by mAP (%) at tIoU Thresholds
  Table 6 caption: TABLE 6 Summary of Temporal Action Detection Methods With Full-Supervision
    (FS) and Limited Supervision (LS)
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3193611
- Affiliation of the first author: school of automation, northwestern polytechnical
    university, xian, china
  Affiliation of the last author: school of automation, northwestern polytechnical
    university, xian, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Holistic_Prototype_Activation_for_FewShot_Segmentation\figure_1.jpg
  Figure 1 caption: "Illustration of the problems in the conventional FSS model. (a)\
    \ Other categories of objects (e.g., potted plants with green rectangles) cause\
    \ serious interference with the segmentation of \u201Cperson,\u201D which is relevant\
    \ to the overfitting of the base category. (b) The network produces inaccurate\
    \ predictions of the boundaries and shapes of objects, indicating a lack of low-level\
    \ details."
  Figure 10 Link: articels_figures_by_rev_year\2022\Holistic_Prototype_Activation_for_FewShot_Segmentation\figure_10.jpg
  Figure 10 caption: "Further qualitative analysis of the activation map (AM). It\
    \ can be observed that there are two typical types of false negatives in AM, i.e.,\
    \ \u03B2 and \u03B3 , as shown in (a). The former is due to the incomplete activation\
    \ of the novel prototype, and the corresponding region (head) remains the same\
    \ after erasing. The latter is due to the incorrect activation of the base prototype\
    \ that is semantically close to the novel one, and the region (leg) to be segmented\
    \ is falsely suppressed. Nevertheless, our HPA still exhibits satisfactory segmentation\
    \ performance (see (f)) since the CRD module tends to capture co-occurrent feature\
    \ representations between support images and query images, which could recover\
    \ such under-segmented regions to a certain extent."
  Figure 2 Link: articels_figures_by_rev_year\2022\Holistic_Prototype_Activation_for_FewShot_Segmentation\figure_2.jpg
  Figure 2 caption: "Overview of the proposed HPA network for the 1-shot segmentation\
    \ task. Given the support-query image pair of the \u201Cbottle\u201D category,\
    \ our model first extracts feature representations through the backbone network,\
    \ then feeds them to the PAM module to filter out the objects of irrelevant categories\
    \ (e.g., \u201Cperson\u201D), and finally leverages the CRD module to further\
    \ refine the segmentation results."
  Figure 3 Link: articels_figures_by_rev_year\2022\Holistic_Prototype_Activation_for_FewShot_Segmentation\figure_3.jpg
  Figure 3 caption: "Flowchart of the proposed Holistic Prototype Activation (HPA)\
    \ network, which is composed of three important components, i.e., training-free\
    \ base prototype acquisition, prototype activation module (PAM), and cross-referenced\
    \ decoder (CRD). We calculate the prototypes of both novel and base categories\
    \ through masked average pooling operation, and then merge them to form the holistic\
    \ prototype set P h . PAM fully activates the query features according to these\
    \ representative vectors. Irrelevant objects with high confidence will be erased\
    \ to generate the activation map M . This probability map, on the other hand,\
    \ provides certain guidance for obtaining well-matched features F \u2032 q . Finally,\
    \ CRD leverages the low-level features of the two branches ( F ~ s and F ~ q )\
    \ to refine the segmentation edges while improving the activation accuracy. Note\
    \ that the losses J seg and J act are evaluated in the meta-training stage, and\
    \ we illustrate them here just for better understanding. More details of the prototype\
    \ matching and max erasing operations can be found in Fig. 4."
  Figure 4 Link: articels_figures_by_rev_year\2022\Holistic_Prototype_Activation_for_FewShot_Segmentation\figure_4.jpg
  Figure 4 caption: "Visual illustration of the prototype activation module (PAM).\
    \ P h denotes the holistic prototype set including both novel prototype P n (colored\
    \ red) and base prototypes P b (colored blue, green, etc.). Given the concatenated\
    \ activation maps, we first perform the max operation to get the Indices-Values\
    \ pairs with the same size H\xD7W\xD7 1. Then, the regions activated by P b (e.g.,\
    \ blue and green regions) in the values map are accordingly erased (i.e., set\
    \ to 0) to derive the final map M . Meanwhile, prototype matching is executed\
    \ in line with the indices map and P h to obtain the well-matched feature F m\
    \ for further segmentation. Best viewed in color."
  Figure 5 Link: articels_figures_by_rev_year\2022\Holistic_Prototype_Activation_for_FewShot_Segmentation\figure_5.jpg
  Figure 5 caption: "Visual illustration of the CRD. Given the low-level features\
    \ F ~ s (support) and F ~ q (query), we first compute the inter- and self- reweighting\
    \ matrixes respectively, and then merge them through a learnable parameter \u03BB\
    \ . After refining the original query feature with the atrous spatial pyramid\
    \ pooling (ASPP) module [48], we concatenate the upsampled high-level feature\
    \ F \u2032 q and the weighted low-level feature F \xAF q along the channel dimension,\
    \ providing discriminative information and object details for further segmentation."
  Figure 6 Link: articels_figures_by_rev_year\2022\Holistic_Prototype_Activation_for_FewShot_Segmentation\figure_6.jpg
  Figure 6 caption: 'Qualitative results of the baseline approach and the variants
    of HPA. From top to bottom: (a) support images, (b) query images, (c) ground-truth,
    (d) predictions of baseline, (e) predictions of the baseline with CRD, (f) predictions
    of the baseline with PAM, (g) predictions of the proposed HPA. The yellow rectangles
    represent the typical regions of segmentation results.'
  Figure 7 Link: articels_figures_by_rev_year\2022\Holistic_Prototype_Activation_for_FewShot_Segmentation\figure_7.jpg
  Figure 7 caption: Comparison of qualitative results between 1-shot and 5-shot segmentation.
    With the discriminative prototype provided by multiple support images, our model
    can yield more accurate predictions.
  Figure 8 Link: articels_figures_by_rev_year\2022\Holistic_Prototype_Activation_for_FewShot_Segmentation\figure_8.jpg
  Figure 8 caption: 'Qualitative results of the proposed HPA on several hard examples
    with various characteristics. From top to bottom: (a) one-to-many, (b) different
    view, (c) occlusion, (d) small object, (e) intra-class variation.'
  Figure 9 Link: articels_figures_by_rev_year\2022\Holistic_Prototype_Activation_for_FewShot_Segmentation\figure_9.jpg
  Figure 9 caption: "Visual illustration of the activation map (AM). We select the\
    \ query samples with more than two semantic classes to better illustrate the effect\
    \ of activation. From left to right: (a) query image with the annotation mask\
    \ (targets of novelbase classes are colored in redblue respectively), (b)&(c):\
    \ AM of the baseline and our approach, (d): AM calculated by the novel prototype,\
    \ (e)&(f): validinvalid AM calculated by the base prototypes. As can be seen,\
    \ the regions in (e) that are activated with high confidence (i.e., valid activation)\
    \ by the base prototypes are correspondingly erased in (d), and then the final\
    \ activation map (c) is obtained. Note that all the activation maps above are\
    \ generated during the meta-testing phase, and \u201Cperson\u201D is served as\
    \ the held-out class. Best viewed in color."
  First author gender probability: 0.8
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: Gong Cheng
  Name of the last author: Junwei Han
  Number of Figures: 16
  Number of Tables: 12
  Number of authors: 3
  Paper title: Holistic Prototype Activation for Few-Shot Segmentation
  Publication Date: 2022-07-25 00:00:00
  Table 1 caption: TABLE 1 Performance Comparison on PASCAL-5 i i in Terms of mIoU
    (%)
  Table 10 caption: TABLE 10 Extended Experiment of Weak-Label Segmentation on PASCAL-5
    i i
  Table 2 caption: TABLE 2 FB-IoU Results on PASCAL-5 i i
  Table 3 caption: TABLE 3 Performance Comparison on COCO-20 i i in Terms of mIoU
    and FB-IoU (%)
  Table 4 caption: TABLE 4 Ablation Studies of the Proposed HPA on PASCAL-5 i i
  Table 5 caption: TABLE 5 Comparison of the Activation Accuracy of Base Class and
    Novel Class Prototypes
  Table 6 caption: TABLE 6 Performance Under Different Types of Activation Loss
  Table 7 caption: "TABLE 7 Ablation Studies on the Fusion Weight \u03BB \u03BB in\
    \ CRD"
  Table 8 caption: TABLE 8 Ablation Studies on the Number of Base Prototypes N b Nb
    in Terms of mIoU, Memory Consumption, Average Training Time (Over 4 Folds), and
    Per-Episode Inference Time
  Table 9 caption: TABLE 9 Extended Experiment of Generalized Few-Shot Segmentation
    on PASCAL-5 i i
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3193587
- Affiliation of the first author: wangxuan institute of computer technology, peking
    university, beijing, china
  Affiliation of the last author: wangxuan institute of computer technology, peking
    university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Imperceptible_Transfer_Attack_and_Defense_on_D_Point_Cloud_Classification\figure_1.jpg
  Figure 1 caption: (a) Existing point perturbation attack is under the white-box
    setting, which shifts each point along the xyz directions with the supervision
    of an adversarial loss function. (b) Perturbed point clouds generated by existing
    white-box attacks generally exhibit outliers or uneven distribution in local regions,
    which are perceptible to humans. Besides, they tend to overfit the target model
    and have low success rates when transferred to attack a new black-box victim model.
    (c) Existing defense methods generally adopt pre-processing to restore the adversarial
    point clouds in the data space, which fails to capture high-level representations
    in the latent space that are crucial to defense.
  Figure 10 Link: articels_figures_by_rev_year\2022\Imperceptible_Transfer_Attack_and_Defense_on_D_Point_Cloud_Classification\figure_10.jpg
  Figure 10 caption: Robustness of our proposed ITA under (a) missing data by dropping
    points and (b) Gaussian noise. The success rates (%) are reported over attacking
    PointNet model. We also sample different numbers of points (1024, 2048, or 4096)
    from the original objects for detailed comparisons.
  Figure 2 Link: articels_figures_by_rev_year\2022\Imperceptible_Transfer_Attack_and_Defense_on_D_Point_Cloud_Classification\figure_2.jpg
  Figure 2 caption: Visualization results of the generated adversarial examples of
    different methods. Compared to the previous most imperceptible GeoA attack, our
    adversarial examples are more imperceptible to humans without outlier or uneven
    local distribution.
  Figure 3 Link: articels_figures_by_rev_year\2022\Imperceptible_Transfer_Attack_and_Defense_on_D_Point_Cloud_Classification\figure_3.jpg
  Figure 3 caption: (a) Illustration of our perturbation strategy, where we only perturb
    each point along the direction of its normal vector within a strictly bounded
    width. (b) Visualization examples of the perturbation generation, where each point
    is perturbed along its normal vector.
  Figure 4 Link: articels_figures_by_rev_year\2022\Imperceptible_Transfer_Attack_and_Defense_on_D_Point_Cloud_Classification\figure_4.jpg
  Figure 4 caption: (a) Illustration of our ITA attack with an adversarial transformation
    model, where we adopt a two-step adversarial learning strategy to train the transformation
    model. (b) To implement our adversarial transformation model, we directly deploy
    two linear layers to imitate natural point-wise transformations. (c) Examples
    of the output point clouds from the learned adversarial transformation models.
  Figure 5 Link: articels_figures_by_rev_year\2022\Imperceptible_Transfer_Attack_and_Defense_on_D_Point_Cloud_Classification\figure_5.jpg
  Figure 5 caption: Illustration of our proposed intra- and inter-class constraints
    in the adversarial training process for point cloud defense. Given the clean point
    cloud data, we first generate corresponding adversarial examples, and then feed
    both of them into the 3D model for discriminative representation learning. The
    intra-class constraint is proposed to reduce the divergence between clean and
    adversarial point clouds of the same object. Meanwhile, the inter-class constraint
    aims to enlarge the difference among point clouds belonging to different objects.
  Figure 6 Link: articels_figures_by_rev_year\2022\Imperceptible_Transfer_Attack_and_Defense_on_D_Point_Cloud_Classification\figure_6.jpg
  Figure 6 caption: The framework of the proposed adversarial-training-based defense
    with similarity constraints in the latent space to improve the robustness of black-box
    models.
  Figure 7 Link: articels_figures_by_rev_year\2022\Imperceptible_Transfer_Attack_and_Defense_on_D_Point_Cloud_Classification\figure_7.jpg
  Figure 7 caption: The generated adversarial point clouds from our proposed ITA attack.
    All the results are organized in a matrix form where the diagonal-entry instances
    present clean data belonging to a certain object class. The other off-diagonal-entry
    instances are the adversarial examples against the PointNet model, where the clean
    instance in each row is the input and the object class in each column is its attack
    target.
  Figure 8 Link: articels_figures_by_rev_year\2022\Imperceptible_Transfer_Attack_and_Defense_on_D_Point_Cloud_Classification\figure_8.jpg
  Figure 8 caption: Adversarial point clouds of challenging cases from both GeoA and
    our proposed ITA attacks. Given a clean instance of each category, we generate
    its adversarial examples under two attacks for comparison.
  Figure 9 Link: articels_figures_by_rev_year\2022\Imperceptible_Transfer_Attack_and_Defense_on_D_Point_Cloud_Classification\figure_9.jpg
  Figure 9 caption: Sensitivity analysis on the bound width B . Experiments are conducted
    on the point clouds of 1024 points using PointNet.
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Daizong Liu
  Name of the last author: Wei Hu
  Number of Figures: 11
  Number of Tables: 13
  Number of authors: 2
  Paper title: Imperceptible Transfer Attack and Defense on 3D Point Cloud Classification
  Publication Date: 2022-07-25 00:00:00
  Table 1 caption: TABLE 1 Comparative Results on the Perturbation Sizes of Different
    Methods Required to Achieve 100% 100% of Attack Success Rate for Adversarial Point
    Clouds
  Table 10 caption: TABLE 10 Both White-Box And Transfer-Based Attack Success Rates
    (%) on PointNet++ by the Proposed ITA Attack With Our Adversarial Transformation
    Model
  Table 2 caption: TABLE 2 Comparative Results on the Attack Success Rate of Different
    Attack Methods
  Table 3 caption: TABLE 3 Comparative Results on the Perturbation Sizes of Different
    Methods Required to Achieve 100% 100% of Attack Success Rate for Adversarial Point
    Clouds Without Budget B B
  Table 4 caption: TABLE 4 The Transfer-Based Attack Success Rates (%) on Five Models
    by Various Attacks Without Our Proposed Adversarial Transformation Model
  Table 5 caption: TABLE 5 The Transfer-Based Attack Success Rates (%) on Five Models
    by Various Attacks With Our Proposed Adversarial Adversarial Transformation Model
  Table 6 caption: TABLE 6 Comparison on Our ITA Attack With Learning-Based (i.e.,
    Linear-Based) and Learning-Free Transformation
  Table 7 caption: TABLE 7 Defense by Adding Different Ratios of Gaussian Noise along
    the Normal Direction
  Table 8 caption: TABLE 8 The Classification Error Rates (%) of the PointNet Model
    Against Different Attacks Under Various Defense Methods
  Table 9 caption: TABLE 9 Both White-Box And Transfer-Based Attack Success Rates
    (%) on PointNet by the Proposed ITA Attack With Our Adversarial Transformation
    Model
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3193449
