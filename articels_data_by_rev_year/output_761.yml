- Affiliation of the first author: computer graphics group, university of siegen,
    hoelderlinstrasse 3, siegen, germany
  Affiliation of the last author: computer graphics group, university of siegen, hoelderlinstrasse
    3, siegen, germany
  Figure 1 Link: articels_figures_by_rev_year\2017\Comprehensive_Use_of_Curvature_for_Robust_and_Accurate_Online_Surface_Reconstruc\figure_1.jpg
  Figure 1 caption: Overview of 3D reconstruction framework.
  Figure 10 Link: articels_figures_by_rev_year\2017\Comprehensive_Use_of_Curvature_for_Robust_and_Accurate_Online_Surface_Reconstruc\figure_10.jpg
  Figure 10 caption: "Comparison of reconstructions for the mit76\u2212417b SL sequence\
    \ by Xiao et al. [46]. Comparison with ElasticFusion shows still frame of https:youtu.be-dzVauPjEU?t=3m52s."
  Figure 2 Link: articels_figures_by_rev_year\2017\Comprehensive_Use_of_Curvature_for_Robust_and_Accurate_Online_Surface_Reconstruc\figure_2.jpg
  Figure 2 caption: "Example curvature estimation for frame 361 of our legoPAMI SL\
    \ data set, using the methods by Zhang et al. [38], and Goldfeather et al. [37]\
    \ , respectively. Left: using the input frame given by the range camera; Right:\
    \ using the current reconstructed surface model. Curvature maps on the top and\
    \ bottom show \u03BA 1 , and \u03BA 2 , respectively; corresponding input normal\
    \ maps convey noise level."
  Figure 3 Link: articels_figures_by_rev_year\2017\Comprehensive_Use_of_Curvature_for_Robust_and_Accurate_Online_Surface_Reconstruc\figure_3.jpg
  Figure 3 caption: Comparing model map quality for two different scenes. The first
    sub-column refers to the simple splatting proposed in [6], the second to our quadratic
    surface intersection based on curvature information (see Section 5.1) and the
    third one is our blending scheme using our quadratic surface intersection (see
    Section 5.2).
  Figure 4 Link: articels_figures_by_rev_year\2017\Comprehensive_Use_of_Curvature_for_Robust_and_Accurate_Online_Surface_Reconstruc\figure_4.jpg
  Figure 4 caption: "Objects used to create each of our scenery. A 1\u20AC coin (\
    \ \u2205=2.325 cm) is used to visualize the object scale."
  Figure 5 Link: articels_figures_by_rev_year\2017\Comprehensive_Use_of_Curvature_for_Robust_and_Accurate_Online_Surface_Reconstruc\figure_5.jpg
  Figure 5 caption: "Comparison of reconstructions for the Lego\u2212PAMI\u2212Free\
    \ sequences."
  Figure 6 Link: articels_figures_by_rev_year\2017\Comprehensive_Use_of_Curvature_for_Robust_and_Accurate_Online_Surface_Reconstruc\figure_6.jpg
  Figure 6 caption: "Comparison of reconstructions for the Brick\u2212Wall ToF (three\
    \ leftmost images) and for the Brick\u2212Wall SL (three rightmost images) sequence."
  Figure 7 Link: articels_figures_by_rev_year\2017\Comprehensive_Use_of_Curvature_for_Robust_and_Accurate_Online_Surface_Reconstruc\figure_7.jpg
  Figure 7 caption: "Comparison of reconstructions for the Stone\u2212Wall SL sequence,\
    \ reconstructed using the offline global optimizer from Zhou and Koltun [13] (top\
    \ row) and our method (bottom row). Images on the left side refer to the right\
    \ column of the stonewall and the images on the right side to the left column\
    \ of the stonewall, where the acquisition starts and ends."
  Figure 8 Link: articels_figures_by_rev_year\2017\Comprehensive_Use_of_Curvature_for_Robust_and_Accurate_Online_Surface_Reconstruc\figure_8.jpg
  Figure 8 caption: "Mean error (top row) and SD (bottom row) for estimated camera\
    \ centers (left) and rotational angles (right) with increasing noise for the Lego\u2212\
    PAMI\u2212TT \xD72 ToF scene. Data points where the camera tracking completely\
    \ failed are omitted. In order to make the results comparable, we include the\
    \ results for a modified version of Sera15 using bilaterally prefiltered depth\
    \ images."
  Figure 9 Link: articels_figures_by_rev_year\2017\Comprehensive_Use_of_Curvature_for_Robust_and_Accurate_Online_Surface_Reconstruc\figure_9.jpg
  Figure 9 caption: "Comparison of reconstructions for the Racing\u2212Car\u2212R3\
    \ ToF sequence by Wasenm\xFCller et al. [45]. For every model point, the absolute\
    \ distance error [m] to the ground truth mesh is visualized using the CloudCompare\
    \ tool [49]."
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Damien Lefloch
  Name of the last author: Andreas Kolb
  Number of Figures: 10
  Number of Tables: 5
  Number of authors: 5
  Paper title: Comprehensive Use of Curvature for Robust and Accurate Online Surface
    Reconstruction
  Publication Date: 2017-01-05 00:00:00
  Table 1 caption:
    table_text: TABLE 1 List of Conventions
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Comparison of the Ego-Motion Robustness for Three Different\
      \ Methods Based on Small (Top Row) and Large (Bottom Row) Scales of Lego\u2212\
      PAMI\u2212T T SL (Left Column) and Lego\u2212PAMI\u2212T T ToF (Right Column)\
      \ Data Sets"
  Table 3 caption:
    table_text: "TABLE 3 Absolute Distance Error [mm] for the Racing\u2212Car\u2212\
      R3 ToF Sequence [45]"
  Table 4 caption:
    table_text: "TABLE 4 Camera Center Error Statistics [cm] for the Robustness Experiment\
      \ Based on the Lego\u2212PAMI\u2212T T \xD71 SL and Applied to Kell13 , Improved\
      \ Version of Kell13 and Our Fully Curvature Enhanced Pipeline"
  Table 5 caption:
    table_text: TABLE 5 Timings of our Complete Reconstruction Pipeline (All Timings
      are in Milliseconds)
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2648803
- Affiliation of the first author: school of eecs, oregon state university, corvallis,
    or
  Affiliation of the last author: school of eecs, oregon state university, corvallis,
    or
  Figure 1 Link: articels_figures_by_rev_year\2017\Dynamic_Programming_for_Instance_Annotation_in_MultiInstance_MultiLabel_Learning\figure_1.jpg
  Figure 1 caption: (a) Graphical model for the proposed ORed-logistic regression
    model for instance annotation. (b) A simplified version of the proposed model.
    Observed variables are shaded. Square nodes denote parameters.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Dynamic_Programming_for_Instance_Annotation_in_MultiInstance_MultiLabel_Learning\figure_2.jpg
  Figure 2 caption: "A high level idea of the proposed solution to compute p( y bi\
    \ =c, Y b | X b ,w) . Nodes that are currently computed on are bolded. (a) The\
    \ original model. (b) The model with new variables. (c) Compute p( Y 1 b | X b\
    \ ,w) . (d) Compute p( Y 2 b | X b ,w) . (e) Compute p( Y k+1 b | X b ,w) with\
    \ 2\u2264k\u2264 n b \u22122 using equation (13). (f) Compute p( Y n b b | X b\
    \ ,w) using equation (13). (g) Compute p( Y \u2216i b | X b ,w) using Proposition\
    \ 2. (h) Compute p( y bi , Y b | X b ,w) using Proposition 3."
  Figure 3 Link: articels_figures_by_rev_year\2017\Dynamic_Programming_for_Instance_Annotation_in_MultiInstance_MultiLabel_Learning\figure_3.jpg
  Figure 3 caption: "Example of the lower triangular matrix A for a bag with three\
    \ labels 1,3,4. Nonzero elements in A are shaded. Sets in the power set of 1,3,4,\
    \ excluding the empty set, are sorted in an ascending order of their cardinality.\
    \ If two sets are equal in cardinality, they are sorted based on the lexicographical\
    \ order. Consequently, the sets are sorted as 1, 3, 4, 1,3, 1,4, 3,4, 1,3,4 which\
    \ are denoted by \u0141 1 , \u0141 2 , \u0141 3 , \u0141 4 , \u0141 5 , \u0141\
    \ 6 , \u0141 7 , respectively. For this order, A is constructed as in Proposition\
    \ 2. For example, from (14a), A(6,2)=p( y bi =4| x bi ,w) since \u0141 2 = 3 and\
    \ \u0141 6 = 3,4 leading to \u0141 2 \u2282 \u0141 6 and \u0141 6 \u2216 \u0141\
    \ 2 = 4. Moreover, from (14b), A(7,7)= \u2211 l\u22081,3,4 p( y bi =l| x bi ,w)\
    \ since \u0141 7 =1,3,4 . Additionally, from (14c), A(2,6)=0 since \u0141 6 \u2288\
    \ \u0141 2 ."
  Figure 4 Link: articels_figures_by_rev_year\2017\Dynamic_Programming_for_Instance_Annotation_in_MultiInstance_MultiLabel_Learning\figure_4.jpg
  Figure 4 caption: Instance annotation accuracy versus percentage of training bags
    for MLR, SIM, LSB, SLR, and Dummy approaches.
  Figure 5 Link: articels_figures_by_rev_year\2017\Dynamic_Programming_for_Instance_Annotation_in_MultiInstance_MultiLabel_Learning\figure_5.jpg
  Figure 5 caption: Histogram of the number of labels per bag | Y b | for Letter Carroll
    and Drosophila in protein datasets.
  Figure 6 Link: articels_figures_by_rev_year\2017\Dynamic_Programming_for_Instance_Annotation_in_MultiInstance_MultiLabel_Learning\figure_6.jpg
  Figure 6 caption: Accuracy versus time when changing the number of iterations. The
    reported runtime is based on an oracle parameter tuning approach in which the
    parameter value yielding the highest accuracy is used. Mfast1 is Mfast with the
    optimal normalization parameter value. Mfast2 is Mfast with the normalization
    parameter value is set to 1.
  Figure 7 Link: articels_figures_by_rev_year\2017\Dynamic_Programming_for_Instance_Annotation_in_MultiInstance_MultiLabel_Learning\figure_7.jpg
  Figure 7 caption: Accuracy and runtime when pruning bags with largest bag label
    cardinality. Dotted lines are estimated using the linear least square from the
    first three points starting from the left in each time plot. The difference between
    the solid and dotted lines at the right of each time plot shows how significant
    bags having a large number of classes contribute to the overall runtime.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Anh T. Pham
  Name of the last author: Xiaoli Z. Fern
  Number of Figures: 7
  Number of Tables: 7
  Number of authors: 3
  Paper title: Dynamic Programming for Instance Annotation in Multi-Instance Multi-Label
    Learning
  Publication Date: 2017-01-05 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Notations Used in This Paper
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Accuracy Results (Percentage) for Instance Label Prediction
      for MLR, Mfast, SIM, LSB, SLR, and Dummy Approaches Using Instance Level Accuracy
      Parameter Tuning
  Table 3 caption:
    table_text: TABLE 3 Accuracy Results (Percentage) for Instance Label Prediction
      for MLR, Mfast, SIM, LSB, SLR, and Dummy Approaches Using Hamming Loss Parameter
      Tuning
  Table 4 caption:
    table_text: TABLE 4 Bag Level Measurements (Percentage) for Methods on Datasets
      with Instance Labels
  Table 5 caption:
    table_text: "TABLE 5 The Average Number of Labels per Bag | Y b | \xAF \xAF \xAF\
      \ \xAF \xAF \xAF \xAF \xAF \xAF , the Total Number of Classes C , and the Maximum\
      \ Number of Labels per Bag | Y b | max for Each MIML Dataset: Sence, Reuters,\
      \ MSRA [19], Corel16k [47], HJA bird, MSCV2, Voc12, Letter Carroll, Letter Frost\
      \ [9], OSU Wrist [48], Alipr [49], LabelMe [50], [51], UIUC Sport [50], [52],\
      \ NUS-WIDE [52], MIR [53], Protein [54]"
  Table 6 caption:
    table_text: TABLE 6 The Runtime (in Seconds) and Accuracy of MLR, SIM, LSB, and
      Mfast on Datasets
  Table 7 caption:
    table_text: TABLE 7 The Runtime (in Seconds) and Accuracy of MLR, SIM, LSB, and
      Mfast on Datasets
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2647944
- Affiliation of the first author: phenomics and bioinformatics research centre, university
    of south australia, adelaide, sa, australia
  Affiliation of the last author: department of statistics, florida state university,
    tallahassee, fl
  Figure 1 Link: articels_figures_by_rev_year\2017\Numerical_Inversion_of_SRNF_Maps_for_Elastic_Shape_Analysis_of_GenusZero_Surface\figure_1.jpg
  Figure 1 caption: Comparison between the proposed multiresolution approach (a) with
    the single resolution version (b). The latter fails to recover complex surfaces.
  Figure 10 Link: articels_figures_by_rev_year\2017\Numerical_Inversion_of_SRNF_Maps_for_Elastic_Shape_Analysis_of_GenusZero_Surface\figure_10.jpg
  Figure 10 caption: Statistical analysis of 3D human body shapes using the SRNF inversion
    proposed in this article.
  Figure 2 Link: articels_figures_by_rev_year\2017\Numerical_Inversion_of_SRNF_Maps_for_Elastic_Shape_Analysis_of_GenusZero_Surface\figure_2.jpg
  Figure 2 caption: Reconstruction of surfaces from their SRNF representations, using
    3,642 SH basis elements. Additional examples are shown in the supplementary file,
    available online.
  Figure 3 Link: articels_figures_by_rev_year\2017\Numerical_Inversion_of_SRNF_Maps_for_Elastic_Shape_Analysis_of_GenusZero_Surface\figure_3.jpg
  Figure 3 caption: 'Deformation transfer: surfaces f 1 , h 1 and f 2 are given. Deformation
    from f 1 to h 1 is learnt and used to deform f 2 to get the new surface h 2 .'
  Figure 4 Link: articels_figures_by_rev_year\2017\Numerical_Inversion_of_SRNF_Maps_for_Elastic_Shape_Analysis_of_GenusZero_Surface\figure_4.jpg
  Figure 4 caption: "Linear interpolation in F versus geodesic path by SRNF inversion.\
    \ The red and blue curves in (c) correspond to the energy path along the geodesic\
    \ and along the linear path, respectively. The energy decreases from 10 \u2212\
    2 to 10 \u22124 ."
  Figure 5 Link: articels_figures_by_rev_year\2017\Numerical_Inversion_of_SRNF_Maps_for_Elastic_Shape_Analysis_of_GenusZero_Surface\figure_5.jpg
  Figure 5 caption: Comparison between (a) linear interpolation in F , (b) geodesic
    path estimated using the pullback metric of Kurtek et al. [18], and (c) geodesic
    path computed using the proposed SRNF inversion.
  Figure 6 Link: articels_figures_by_rev_year\2017\Numerical_Inversion_of_SRNF_Maps_for_Elastic_Shape_Analysis_of_GenusZero_Surface\figure_6.jpg
  Figure 6 caption: Comparison of our approach with different frameworks. Observe
    that the in-between shapes in (b) contain artifacts due to the inaccurate correspondences
    computed using functional maps.
  Figure 7 Link: articels_figures_by_rev_year\2017\Numerical_Inversion_of_SRNF_Maps_for_Elastic_Shape_Analysis_of_GenusZero_Surface\figure_7.jpg
  Figure 7 caption: Comparison between linear interpolations in mathcal F and geodesic
    paths by SRNF inversion.
  Figure 8 Link: articels_figures_by_rev_year\2017\Numerical_Inversion_of_SRNF_Maps_for_Elastic_Shape_Analysis_of_GenusZero_Surface\figure_8.jpg
  Figure 8 caption: Comparison between linear interpolations in mathcal F and geodesic
    paths by SRNF inversion (obtained using 100 PCA basis elements).
  Figure 9 Link: articels_figures_by_rev_year\2017\Numerical_Inversion_of_SRNF_Maps_for_Elastic_Shape_Analysis_of_GenusZero_Surface\figure_9.jpg
  Figure 9 caption: Examples of deformation transfer using geodesic shooting in the
    space of SRNFs. The middle shapes, in gray, are intermediate shapes along the
    deformation paths.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Hamid Laga
  Name of the last author: Anuj Srivastava
  Number of Figures: 10
  Number of Tables: 4
  Number of authors: 4
  Paper title: Numerical Inversion of SRNF Maps for Elastic Shape Analysis of Genus-Zero
    Surfaces
  Publication Date: 2017-01-05 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Computation Time, in Seconds, on Intel i7-3770 CPU 3.40 Ghz
      with 16 Gb of RAM
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Algorithms in Terms of Complexity and Computation
      Time
  Table 3 caption:
    table_text: TABLE 3 Classification Performance, in % , on the Genus-0 Watertight
      3D Models of SHREC07 [20]
  Table 4 caption:
    table_text: TABLE 4 Classification Performance, in % , for Six Different Techniques
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2647596
- Affiliation of the first author: "inria grenoble rh\xF4ne-alpes, montbonnot saint-martin,\
    \ france"
  Affiliation of the last author: "inria grenoble rh\xF4ne-alpes, montbonnot saint-martin,\
    \ france"
  Figure 1 Link: articels_figures_by_rev_year\2017\AudioVisual_Speaker_Diarization_Based_on_Spatiotemporal_Bayesian_Fusion\figure_1.jpg
  Figure 1 caption: The Bayesian spatiotemporal fusion model used for audio-visual
    speaker diarization. Shaded nodes represent the observed variables, while unshaded
    nodes represent latent variables. Note that the visibility-mask variables V t,n
    although observed, they are treated as control variables. This model enables simultaneously
    speaking persons, which is not only a realistic assumption but also very common
    in natural dialogues and applications like for example HRI.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\AudioVisual_Speaker_Diarization_Based_on_Spatiotemporal_Bayesian_Fusion\figure_2.jpg
  Figure 2 caption: The AVDIAR dataset is recorded with a camera-microphone setup.
    (a) To record the training data, a loud-speaker that emits white noise was used.
    A visual marker onto the loud-speaker (circled in green) allows to annotate the
    training data with image locations, each image location corresponds to a loud-speaker
    direction. (b) The image grid of loud-speaker locations used for the training
    data. (c) A typical AVDIAR scenario (the camera-microphone setup is circled in
    green).
  Figure 3 Link: articels_figures_by_rev_year\2017\AudioVisual_Speaker_Diarization_Based_on_Spatiotemporal_Bayesian_Fusion\figure_3.jpg
  Figure 3 caption: Examples of scenarios in the AVDIAR dataset. For the sake of varying
    the acoustic conditions, we used three different rooms to record this dataset.
  Figure 4 Link: articels_figures_by_rev_year\2017\AudioVisual_Speaker_Diarization_Based_on_Spatiotemporal_Bayesian_Fusion\figure_4.jpg
  Figure 4 caption: Example from different datasets. The MVAD dataset (top) contains
    recordings of one to three persons that always face the camera. The AVASM (middle)
    was design to benchmark audio-visual sound-source localization with two simultaneously
    speaking persons or with a moving speaker. The AV16P3 dataset (bottom) contains
    recordings of simultaneously moving and speaking persons.
  Figure 5 Link: articels_figures_by_rev_year\2017\AudioVisual_Speaker_Diarization_Based_on_Spatiotemporal_Bayesian_Fusion\figure_5.jpg
  Figure 5 caption: 'Results obtained on sequence Seq32-4P-S1M1. Visual tracking results
    (first row). The raw audio signal delivered by the left microphone and the speech
    activity region is marked with red rectangles (second row). Speaker diarization
    result (third row) illustrated with a color diagram: each color corresponds to
    the speaking activity of a different person. Annotated ground-truth diarization
    (fourth row).'
  Figure 6 Link: articels_figures_by_rev_year\2017\AudioVisual_Speaker_Diarization_Based_on_Spatiotemporal_Bayesian_Fusion\figure_6.jpg
  Figure 6 caption: Results on sequence Seq12-3P-S2M1.
  Figure 7 Link: articels_figures_by_rev_year\2017\AudioVisual_Speaker_Diarization_Based_on_Spatiotemporal_Bayesian_Fusion\figure_7.jpg
  Figure 7 caption: Results on sequence Seq01-1P-S0M1.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.95
  Name of the first author: Israel D. Gebru
  Name of the last author: Radu Horaud
  Number of Figures: 7
  Number of Tables: 5
  Number of authors: 4
  Paper title: Audio-Visual Speaker Diarization Based on Spatiotemporal Bayesian Fusion
  Publication Date: 2017-01-05 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Scenarios Available with the AVDIAR Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 DER Scores Obtained with MVAD Dataset (%)
  Table 3 caption:
    table_text: TABLE 3 DER Scores Obtained with AVASM Dataset (%)
  Table 4 caption:
    table_text: TABLE 4 DER Scores Obtained with AV16P3 Dataset (%)
  Table 5 caption:
    table_text: TABLE 5 DER Scores Obtained with AVDIAR Dataset (%)
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2648793
- Affiliation of the first author: technical university of denmark (dtu), bygning
    115, kgs. lyngby, denmark
  Affiliation of the last author: massachusetts institute of technology (mit), 77
    massachusetts avenue, cambridge, ma
  Figure 1 Link: articels_figures_by_rev_year\2017\Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds\figure_1.jpg
  Figure 1 caption: Graphical representation of the proposed model for classification.
  Figure 10 Link: articels_figures_by_rev_year\2017\Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds\figure_10.jpg
  Figure 10 caption: True versus estimated confusion matrix (cm) of six different
    workers of the reuters-21,578 dataset.
  Figure 2 Link: articels_figures_by_rev_year\2017\Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds\figure_2.jpg
  Figure 2 caption: Example of four different annotators (represented by different
    colours) with different biases and precisions.
  Figure 3 Link: articels_figures_by_rev_year\2017\Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds\figure_3.jpg
  Figure 3 caption: Graphical representation of the proposed model for regression.
  Figure 4 Link: articels_figures_by_rev_year\2017\Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds\figure_4.jpg
  Figure 4 caption: "Average testset accuracy (over five runs; \xB1 stddev.) of the\
    \ different approaches on the 20-newsgroups data."
  Figure 5 Link: articels_figures_by_rev_year\2017\Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds\figure_5.jpg
  Figure 5 caption: Comparison of the log marginal likelihood between the batch and
    the stochastic variational inference (svi) algorithms on the 20-newsgroups corpus.
  Figure 6 Link: articels_figures_by_rev_year\2017\Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds\figure_6.jpg
  Figure 6 caption: Boxplot of the number of answers per worker (a) and their respective
    accuracies (b) for the reuters dataset.
  Figure 7 Link: articels_figures_by_rev_year\2017\Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds\figure_7.jpg
  Figure 7 caption: Average testset accuracy (over 30 runs; pm stddev.) of the different
    approaches on the reuters data.
  Figure 8 Link: articels_figures_by_rev_year\2017\Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds\figure_8.jpg
  Figure 8 caption: Boxplot of the number of answers per worker (a) and trespective
    accuracies (b) for the LabelMe dataset.
  Figure 9 Link: articels_figures_by_rev_year\2017\Learning_Supervised_Topic_Models_for_Classification_and_Regression_from_Crowds\figure_9.jpg
  Figure 9 caption: Average testset accuracy (over 30 runs; pm stddev.) of the different
    approaches on the LabelMe data.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Filipe Rodrigues
  Name of the last author: Francisco C. Pereira
  Number of Figures: 14
  Number of Tables: 4
  Number of authors: 4
  Paper title: Learning Supervised Topic Models for Classification and Regression
    from Crowds
  Publication Date: 2017-01-05 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Correspondence Between Variational Parameters and the Original
      Parameters
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Overall Statistics of the Classification Datasets Used in
      the Experiments
  Table 3 caption:
    table_text: TABLE 3 Results for Four Example LabelMe Images
  Table 4 caption:
    table_text: TABLE 4 Overall Statistics of the Regression Datasets Used in the
      Experiments
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2648786
- Affiliation of the first author: school of computer science, university of adelaide,
    sa, australia
  Affiliation of the last author: school of computer science and engineering, university
    of electronic science and technology of china, chengdu, china
  Figure 1 Link: articels_figures_by_rev_year\2017\Compositional_Model_Based_Fisher_Vector_Coding_for_Image_Classification\figure_1.jpg
  Figure 1 caption: Comparison of two strategies to increase the modeling accuracy.
    (a) For GMM, d , the average distance (over 500 sampled local features) between
    a local feature and its closest mean vector, increases with the local feature
    dimensionality with the number of GMM is fixed at 100. (b) d is reduced by two
    ideas (1) simply increasing the number of Gaussian mixtures. (2) using the proposed
    generation process. As we see, the latter achieves much lower d even with a small
    number of bases.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Compositional_Model_Based_Fisher_Vector_Coding_for_Image_Classification\figure_2.jpg
  Figure 2 caption: The demonstration of the supervised coding method. In a supervised
    coding method, the supervision information is used to learn the encoder function.
    A supervised coding method is used to guide the decomposition of the discriminative
    part and the residual part of a local feature.
  Figure 3 Link: articels_figures_by_rev_year\2017\Compositional_Model_Based_Fisher_Vector_Coding_for_Image_Classification\figure_3.jpg
  Figure 3 caption: Comparison of GMMFVC and SCFVC with different local feature dimensionality.
    The experiment is conducted on the MIT-67 dataset.
  Figure 4 Link: articels_figures_by_rev_year\2017\Compositional_Model_Based_Fisher_Vector_Coding_for_Image_Classification\figure_4.jpg
  Figure 4 caption: "The impact of the parameter \u03BB in Eq. (15) on the classification\
    \ performance. (a) result on Birds-200 (b) result on Pascal-07 (c) result on MIT-67."
  Figure 5 Link: articels_figures_by_rev_year\2017\Compositional_Model_Based_Fisher_Vector_Coding_for_Image_Classification\figure_5.jpg
  Figure 5 caption: Comparison of with and without the common part Fisher vector.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Lingqiao Liu
  Name of the last author: Heng Tao Shen
  Number of Figures: 5
  Number of Tables: 8
  Number of authors: 7
  Paper title: Compositional Model Based Fisher Vector Coding for Image Classification
  Publication Date: 2017-01-10 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Results on Birds-200
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Results on MIT-67
  Table 3 caption:
    table_text: TABLE 3 Comparison of Results on Pascal VOC 2007 for Each of 20 Classes
  Table 4 caption:
    table_text: TABLE 4 Comparison of Results on Pascal VOC 2012 for Each of 20 Classes
  Table 5 caption:
    table_text: TABLE 5 Comparison of Results on Pascal VOC 2007
  Table 6 caption:
    table_text: 'TABLE 6 Comparison of Results on MIT-67 with Three Different Settings:
      (1) 200-Basis Codebook with 1,000 Dimensional Local Features (2) 500 Gaussian
      Distributions with 400 Dimensional Local Features (3) 1,000 Gaussian Distributions
      with 200 Dimensional Local Features'
  Table 7 caption:
    table_text: 'TABLE 7 Comparison of Results on Birds-200 with Three Different Settings:
      (1) 200-Basis Codebook with 512 Dimensional Local Features (2) 256 Gaussian
      Distributions with 400 Dimensional Local Features (3) 400 Gaussian Distributions
      with 400 Dimensional Local Features'
  Table 8 caption:
    table_text: TABLE 8 The Impact of the Number of Bases on Our Methods
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2651061
- Affiliation of the first author: ming hsieh department of electrical engineering,
    university of southern california, los angeles, ca
  Affiliation of the last author: ming hsieh department of electrical engineering,
    university of southern california, los angeles, ca
  Figure 1 Link: articels_figures_by_rev_year\2017\Measuring_and_Predicting_Tag_Importance_for_Image_Retrieval\figure_1.jpg
  Figure 1 caption: Image-to-image search under the MIR framework. (a) A toy database
    consisting of both tagged and untagged images. (b) Illustration of the image-to-image
    search. Tagged database images are used to learn the common semantic subspace
    between the visual and the textual domains in the training stage. Visual features
    of both the query image and untagged database images will be projected into the
    common semantic subspace to calculate the visual similarity during the query time.
  Figure 10 Link: articels_figures_by_rev_year\2017\Measuring_and_Predicting_Tag_Importance_for_Image_Retrieval\figure_10.jpg
  Figure 10 caption: The NDCG curves for the image-to-image retrieval on the (a) COCO
    and (b) COCO Scene datasets. The dashed lines are upper bounds for importance
    based MIR systems.
  Figure 2 Link: articels_figures_by_rev_year\2017\Measuring_and_Predicting_Tag_Importance_for_Image_Retrieval\figure_2.jpg
  Figure 2 caption: "Two images with the same object tags (\u201Ccar\u201D, \u201C\
    motorbike\u201D, and \u201Cperson\u201D) but substantially different visual content."
  Figure 3 Link: articels_figures_by_rev_year\2017\Measuring_and_Predicting_Tag_Importance_for_Image_Retrieval\figure_3.jpg
  Figure 3 caption: "An overview of the proposed MIRTIP system. Given a query image\
    \ with important \u201Cdog\u201D and \u201Cbicycle\u201D, the MIRTIP system will\
    \ rank the good retrieval examples with important \u201Cdog\u201D and \u201Cbicycle\u201D\
    \ ahead of bad retrieval ones with less important \u201Cdog\u201D and \u201Cbicycle\u201D\
    ."
  Figure 4 Link: articels_figures_by_rev_year\2017\Measuring_and_Predicting_Tag_Importance_for_Image_Retrieval\figure_4.jpg
  Figure 4 caption: "An example of object importance measurement using sentence descriptions,\
    \ where object tags \u201Cperson\u201D and \u201Cmotorbike\u201D appear in sentences\
    \ in synonyms such as \u201Cman\u201D and \u201Cscooter\u201D, respectively."
  Figure 5 Link: articels_figures_by_rev_year\2017\Measuring_and_Predicting_Tag_Importance_for_Image_Retrieval\figure_5.jpg
  Figure 5 caption: "An example for comparison between probability importance and\
    \ discounted probability importance, where the \u201Cbicycle\u201D in both images\
    \ are equally important with probability importance but not with the discounted\
    \ probability importance (left \u201Cbicycle\u201D: 0.6; right \u201Cbicycle\u201D\
    : 1.0)."
  Figure 6 Link: articels_figures_by_rev_year\2017\Measuring_and_Predicting_Tag_Importance_for_Image_Retrieval\figure_6.jpg
  Figure 6 caption: 'The parse trees of two sentences. The acronyms in the trees are:
    S (Sentence), NP (Noun Phrase), VP (Verb Phrase), PP (Preposition Phrase), DT
    (Determiner), JJ (Adjective), NN (Singular Noun), NNS (Plural Noun), VBN (Verb,
    past participle), VBP (Verb, non-3rd person singular present), IN (Preposition
    or subordinating conjunction).'
  Figure 7 Link: articels_figures_by_rev_year\2017\Measuring_and_Predicting_Tag_Importance_for_Image_Retrieval\figure_7.jpg
  Figure 7 caption: Examples of various cues for predicting object and scene tag importance.
    The texts below images give the ground truth tag importance.
  Figure 8 Link: articels_figures_by_rev_year\2017\Measuring_and_Predicting_Tag_Importance_for_Image_Retrieval\figure_8.jpg
  Figure 8 caption: A sample image and its corresponding joint MRF model.
  Figure 9 Link: articels_figures_by_rev_year\2017\Measuring_and_Predicting_Tag_Importance_for_Image_Retrieval\figure_9.jpg
  Figure 9 caption: 'Comparison of continuous-valued tag importance prediction errors
    of seven models: (a) The UIUC dataset, (b) The COCO dataset, and (c) The COCO
    scene dataset.'
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Shangwen Li
  Name of the last author: C.-C. Jay Kuo
  Number of Figures: 15
  Number of Tables: 3
  Number of authors: 5
  Paper title: Measuring and Predicting Tag Importance for Image Retrieval
  Publication Date: 2017-01-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Visual and Textual Features Used for Tag Importance Measurement,
      Prediction, and MIR
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Major Image Datasets
  Table 3 caption:
    table_text: TABLE 3 Performance Comparison of Tag Importance Prediction
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2651818
- Affiliation of the first author: key laboratory of machine perception (moe), peking
    university, beijing, china
  Affiliation of the last author: key laboratory of machine perception (moe), peking
    university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2017\Robust_Matrix_Factorization_by_Majorization_Minimization\figure_1.jpg
  Figure 1 caption: Illustration of MM. The curve of the surrogate function f(x, x
    k ) is above that of the objective function h(x) and equal to h(x) at x k . By
    minimizing the surrogate function f(x, x k ) , we get the next iterate x k+1 .
    The values of objective function h(x) at x k are non-increasing.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Robust_Matrix_Factorization_by_Majorization_Minimization\figure_2.jpg
  Figure 2 caption: "Objective value (8) versus computing time of UNuBi [22] and our\
    \ RMF-MM on large scale synthetic data, where each marker represents one iteration.\
    \ The left part of the curve by UNuBi is not shown as the values are greater than\
    \ 2.1\xD7 10 4 ."
  Figure 3 Link: articels_figures_by_rev_year\2017\Robust_Matrix_Factorization_by_Majorization_Minimization\figure_3.jpg
  Figure 3 caption: Original incomplete and recovered data of the Dinosaur sequence
    with at least 7, 6 and 5 views, respectively. (a) Raw input tracks. (b-g) Full
    tracks reconstructed by ALP [17], CWM [19], LMaFit [20], Reg L 1 [21], UNuBi [22]
    and RMF-MM, respectively. The reconstruction errors Err 1 defined as (42) are
    presented below the tracks.
  Figure 4 Link: articels_figures_by_rev_year\2017\Robust_Matrix_Factorization_by_Majorization_Minimization\figure_4.jpg
  Figure 4 caption: Recovered points for the first (first and third rows) and the
    50th (second and fourth rows) frames of the Giraffe sequence on two levels of
    outliers ( o% =5% and sigma =50 for the first two rows and o% =10% and sigma =100
    for the last two rows). A red dot represents an observed outliers we randomly
    generated. Green is an observed entry, and black is a missing entry. Err 1 and
    Err 2 are presented below the figures (best viewed on screen!).
  Figure 5 Link: articels_figures_by_rev_year\2017\Robust_Matrix_Factorization_by_Majorization_Minimization\figure_5.jpg
  Figure 5 caption: Comparison of image recovery after adding different magnitudes
    of outliers and by choosing different ranks. The first to the fourth rows correspond
    to (sigma =150, r=15) , (sigma =150, r=20) , (sigma =250, r=15) and (sigma =250,
    r=20) , respectively. The PSNRs are presented below each image (best viewed on
    screen!).
  Figure 6 Link: articels_figures_by_rev_year\2017\Robust_Matrix_Factorization_by_Majorization_Minimization\figure_6.jpg
  Figure 6 caption: The description is the same as Fig. 5.
  Figure 7 Link: articels_figures_by_rev_year\2017\Robust_Matrix_Factorization_by_Majorization_Minimization\figure_7.jpg
  Figure 7 caption: Err 1 versus the regularization parameter lambda for all the regularized
    methods, e.g., Reg L1 [21], UNuBi [22] and RMF-MM, on the Dinosaur data set. The
    curves from left to right correspond to data sequence with at least 7, 6 and 5
    views, respectively.
  Figure 8 Link: articels_figures_by_rev_year\2017\Robust_Matrix_Factorization_by_Majorization_Minimization\figure_8.jpg
  Figure 8 caption: PSNR versus the rank r on the image recovery task. The first two
    images correspond to Fig. 5 with outliers magnitude sigma = 150 and 250, respectively.
    The last two correspond to Fig. 6 similarly.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.64
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.62
  Name of the first author: Zhouchen Lin
  Name of the last author: Hongbin Zha
  Number of Figures: 8
  Number of Tables: 2
  Number of authors: 3
  Paper title: Robust Matrix Factorization by Majorization Minimization
  Publication Date: 2017-01-11 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Synthetic Experiments on Two Sizes of Data with Varying Missing\
      \ Data Ratio s Percent, Outliers Ratio o Percent and Outliers Magnitude \u03C3"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average Computing Time (Seconds) of the Competing Algorithms
      on Different Data Sets
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2651816
- Affiliation of the first author: jiangsu province key laboratory of big data analysis
    technology, nanjing university of information science and technology, nanjing,
    china
  Affiliation of the last author: jiangsu province key laboratory of big data analysis
    technology, nanjing university of information science and technology, nanjing,
    china
  Figure 1 Link: "articels_figures_by_rev_year\\2017\\NewtonType_Greedy_Selection_Methods_for\u2113\
    Constrained_Minimization\\figure_1.jpg"
  Figure 1 caption: 'Sparse logistic regression on simulated data: Objective value
    and CPU running time curves of the considered methods.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: "articels_figures_by_rev_year\\2017\\NewtonType_Greedy_Selection_Methods_for\u2113\
    Constrained_Minimization\\figure_2.jpg"
  Figure 2 caption: 'Sparse logistic regression on simulated data: objective value
    versus CPU running time curves of the considered methods. The sample size n=5,000
    is fixed and the sparsity level k is allowed to be varying in 1,000,2,000,4,000,5,000
    .'
  Figure 3 Link: "articels_figures_by_rev_year\\2017\\NewtonType_Greedy_Selection_Methods_for\u2113\
    Constrained_Minimization\\figure_3.jpg"
  Figure 3 caption: 'Sparse logistic regression on real data: Objective value, classification
    error and CPU running time curves of the considered methods.'
  Figure 4 Link: "articels_figures_by_rev_year\\2017\\NewtonType_Greedy_Selection_Methods_for\u2113\
    Constrained_Minimization\\figure_4.jpg"
  Figure 4 caption: 'Sparse L 2 -SVMs on real data: Objective value, classification
    error and CPU running time curves of the considered methods.'
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.61
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.57
  Name of the first author: Xiao-Tong Yuan
  Name of the last author: Qingshan Liu
  Number of Figures: 4
  Number of Tables: 1
  Number of authors: 2
  Paper title: "Newton-Type Greedy Selection Methods for\n\u2113\n0\n-Constrained\
    \ Minimization"
  Publication Date: 2017-01-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Notation Used in This Paper
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2651813
- Affiliation of the first author: department of computer science, ben-gurion university,
    be'er sheva, israel
  Affiliation of the last author: computer science and artificial intelligence lab,
    massachusetts institute of technology, cambridge, ma
  Figure 1 Link: articels_figures_by_rev_year\2017\Transformations_Based_on_Continuous_PiecewiseAffine_Velocity_Fields\figure_1.jpg
  Figure 1 caption: "(a) Integration of sufficiently-nice velocity fields is widely\
    \ used to generate well-behaved nonlinear transformations. The choice of using\
    \ CPA velocity fields, among other benefits, reduces computational costs, increases\
    \ integration accuracy, and simplifies modeling and inference. A CPAB transformation,\
    \ x\u21A6 \u03D5 \u03B8 (x,t) , is one that is based (via integration) on a CPA\
    \ velocity field, v \u03B8 . (b) A 1D example. (c-d) Two 2D examples, where in\
    \ (d) there are also additional constraints. Top row: a continuously-defined v\
    \ \u03B8 in select locations. Middle: Visualizing the horizontal ( v \u03B8 h\
    \ , left) and vertical ( v \u03B8 v , right) components as heat maps highlights\
    \ the CPA property; blue= \u2212\u03BB , green=0, and red= \u03BB where \u03BB\
    \ = max x\u2208\u03A9 max(| v \u03B8 h (x)|,| v \u03B8 v (x)|) . Bottom: I src\
    \ \u2218 \u03D5 \u03B8 (\u22C5,1) ."
  Figure 10 Link: articels_figures_by_rev_year\2017\Transformations_Based_on_Continuous_PiecewiseAffine_Velocity_Fields\figure_10.jpg
  Figure 10 caption: Example registrations of images from the MNIST dataset.
  Figure 2 Link: articels_figures_by_rev_year\2017\Transformations_Based_on_Continuous_PiecewiseAffine_Velocity_Fields\figure_2.jpg
  Figure 2 caption: Several type-I tessellations of a 2D region.
  Figure 3 Link: articels_figures_by_rev_year\2017\Transformations_Based_on_Continuous_PiecewiseAffine_Velocity_Fields\figure_3.jpg
  Figure 3 caption: Several type-II tessellations of a 2D region.
  Figure 4 Link: articels_figures_by_rev_year\2017\Transformations_Based_on_Continuous_PiecewiseAffine_Velocity_Fields\figure_4.jpg
  Figure 4 caption: 'Samples from the prior (Section 4). (a-h) The top 3 rows echo
    those in Fig. 1c and Fig. 1d. The 4mathrmth row shows a deformed grid overlaid
    on the image. The 5mathrmth row shows select trajectories. Note that the trajectories
    and transformations are differentiable (hence continuous) but not piecewise affine.
    The tessellation in (e-h) is a refinement of the one in (a-d). (a) & (e): Ttheta
    in M . (b) & (f): Ttheta in Mmathrmvp . (c) & (g): Ttheta in Mpartial . (d) &
    (h): Ttheta in Mpartial, mathrmvp . See Section 4 for details.'
  Figure 5 Link: articels_figures_by_rev_year\2017\Transformations_Based_on_Continuous_PiecewiseAffine_Velocity_Fields\figure_5.jpg
  Figure 5 caption: Monotonic regression on synthetic data (top) by inferring CPA
    fields (bottom), defined over 100 equal-length cells. We used a squared loss in
    the 4 leftmost columns and a robust one (Geman-McClure) in the 2 others. Col.
    1, 3, and 5 show ML solutions. Col. 2, 4 and 6 show MAP solutions with a smoothness
    prior. In all cases, the function is increasing, as obtained effortlessly from
    the representation.
  Figure 6 Link: articels_figures_by_rev_year\2017\Transformations_Based_on_Continuous_PiecewiseAffine_Velocity_Fields\figure_6.jpg
  Figure 6 caption: 'CDFhistogram representation. 1textst row: Nmathcal P=10 . 2textnd
    row: Nmathcal P=100 . F0 , not shown, is a CDF of a uniform distribution on J=[-3,3]
    . (a) vtheta in mathcal Vpartial ([0,1],mathcal P) . (b) F=Ttheta circ F0 . (c)
    fracdd x F . Note it is not piecewise constant. H0 , not shown, is a cumsum of
    a 20-bin uniform histogram. (e) H=Ttheta circ H0 , a cumsum of a new histogram
    h (f) h .'
  Figure 7 Link: articels_figures_by_rev_year\2017\Transformations_Based_on_Continuous_PiecewiseAffine_Velocity_Fields\figure_7.jpg
  Figure 7 caption: Landmark-based warping. One example in each row. (a) Imathrmsrc
    (and src landmarks). (b) Imathrmsrc (and srcdst landmarks). (c) The inferred vtheta
    . (d) A new image is synthesized by warping Imathrmsrc using Ttheta (and dst &
    Ttheta (mathrmsrc) landmarks). (d) Animation.
  Figure 8 Link: articels_figures_by_rev_year\2017\Transformations_Based_on_Continuous_PiecewiseAffine_Velocity_Fields\figure_8.jpg
  Figure 8 caption: 'Nonlinear time warping in real MoCap Data. Left: a reference
    signal, s0 , 5 other signals, lbrace sirbrace i=15 , and their mean, bars=frac15sum
    i=15 si . The mean is smeared since lbrace sirbrace i=15 are misaligned. Center:
    having inferred the warps, lbrace Ttheta irbrace i=15 , we unwarp the signals,
    setting ui=sicirc T-theta i . Note lbrace uirbrace i=15 are better aligned, as
    is evident by the details preserved in their mean, baru=frac15sum i=15 ui . Right:
    warping baru by Tbartheta +jsigma 1 xi 1 where bartheta =frac15sum i=15theta i
    , jin lbrace -6,-4,-2,0,2,4,6rbrace , xi 1 is the first principal component of
    lbrace theta i-bartheta rbrace i=15 , and sigma 1 is the corresponding standard
    deviation.'
  Figure 9 Link: articels_figures_by_rev_year\2017\Transformations_Based_on_Continuous_PiecewiseAffine_Velocity_Fields\figure_9.jpg
  Figure 9 caption: 'A 3D example. Left: Source shape (taken from the Tosca Dateset:
    http:tosca.cs.technion.ac.il) . Right: Result of applying a volume-preserving
    transformation, sampled from the prior, to the source.'
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Oren Freifeld
  Name of the last author: Jonn W. Fisher
  Number of Figures: 14
  Number of Tables: 2
  Number of authors: 4
  Paper title: Transformations Based on Continuous Piecewise-Affine Velocity Fields
  Publication Date: 2017-01-11 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Values of N P , N v , D=dim( V \u2032 \u03A9,P ) , and d=dim(\
      \ V \u03A9,P ) for the P 's shown in Fig. 2. See also Section 4 ."
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Accuracy: Improvement, in %, of Algorithm 1 ( N steps =10\
      \ , n steps =10 ) Over the Generic Solver (100 Steps). \u03A9=[0,1] Was Tessellated\
      \ into N P Equal-Length Intervals"
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2646685
