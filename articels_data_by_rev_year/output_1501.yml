- Affiliation of the first author: department of information engineering and computer
    science, university of trento, trento, italy
  Affiliation of the last author: department of information engineering and computer
    science, university of trento, trento, italy
  Figure 1 Link: articels_figures_by_rev_year\2019\Progressive_Fusion_for_Unsupervised_Binocular_Depth_Estimation_Using_Cycled_Netw\figure_1.jpg
  Figure 1 caption: Motivation of the proposed unsupervised depth estimation approach
    using cycled generative networks.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Progressive_Fusion_for_Unsupervised_Binocular_Depth_Estimation_Using_Cycled_Netw\figure_2.jpg
  Figure 2 caption: Illustration of the detailed framework of the proposed cycled
    generative networks with Progressive Fusion Network for unsupervised adversarial
    depth estimation. L rec represents the reconstruction loss for different generators;
    L con denotes a consistence loss between the disparity maps generated from the
    two generators.
  Figure 3 Link: articels_figures_by_rev_year\2019\Progressive_Fusion_for_Unsupervised_Binocular_Depth_Estimation_Using_Cycled_Netw\figure_3.jpg
  Figure 3 caption: "Illustration of the proposed Progressive Fusion Network (PFN).\
    \ In the left-to-right stream (in dark red) all tensors are aligned with right\
    \ image. Conversely, in the right-to-left stream (in dark blue) all the tensors\
    \ are aligned with the left image. The estimated left-to-right disparity d (0)\
    \ r is used to align the left image feature map \u03BE (0) l and the right-to-left\
    \ disparity d (0) l with the left-to-right stream. The aligned tensor \u03BE (0)\
    \ r is then concatenated with the right-to-left stream. Skip connections (dotted\
    \ lines) are used to transfer local information from the encoder to the decoder.\
    \ \u2295 denotes the concatenation operator, \u25EFw denotes the warping operator\
    \ introduced in (13), \u25EFUp denotes the 2\xD72 Up-sampling operator."
  Figure 4 Link: articels_figures_by_rev_year\2019\Progressive_Fusion_for_Unsupervised_Binocular_Depth_Estimation_Using_Cycled_Netw\figure_4.jpg
  Figure 4 caption: Qualitative comparison of different baseline models of the proposed
    Half-Cycle approach on KITTI Eigen test split. From left to right our stereo disparity
    progressive fusion, then stereo disparity and features progressive fusion and
    in the fourth column the full Half-Cycle model with adversarial learning. First
    column on the left is the RGB images and right the ground truth depth.
  Figure 5 Link: articels_figures_by_rev_year\2019\Progressive_Fusion_for_Unsupervised_Binocular_Depth_Estimation_Using_Cycled_Netw\figure_5.jpg
  Figure 5 caption: Qualitative comparison of different baseline models of the proposed
    Cycle Stereo approach on KITTI Eigen test split. From left to right RGB images,
    Cycle Stereo with continuous disparity fusion, Cycle Stereo with disparity and
    features fusion, in column four the full model trained with adversarial learning,
    in column five the futher refined full model with SSIM (self-similarity) loss
    and in the right column the ground truth depth maps.
  Figure 6 Link: articels_figures_by_rev_year\2019\Progressive_Fusion_for_Unsupervised_Binocular_Depth_Estimation_Using_Cycled_Netw\figure_6.jpg
  Figure 6 caption: "Accuracy plot with a varying threshold parameter value \u03C4\
    \ for the KITTI, Cityscapes and ApolloScape datasets. The accuracy threshold \u03B1\
    \ is set to \u03B1=1.10 for better visualization."
  Figure 7 Link: articels_figures_by_rev_year\2019\Progressive_Fusion_for_Unsupervised_Binocular_Depth_Estimation_Using_Cycled_Netw\figure_7.jpg
  Figure 7 caption: Qualitative comparison of different Stereo Half-Cycle models on
    the Cityscapes testing dataset. The second column presents progressive disparity
    fusion, they are in general smoother but dont present the level of detail that
    we can find in third and fourth column where we have progressive feature fusion.
    Columns three and four present results from models learned with adversarial loss.
  Figure 8 Link: articels_figures_by_rev_year\2019\Progressive_Fusion_for_Unsupervised_Binocular_Depth_Estimation_Using_Cycled_Netw\figure_8.jpg
  Figure 8 caption: Qualitative comparison of different Stereo Cycle models on the
    Cityscapes testing dataset. Similarly to Fig. 7 in column two we present the results
    with progressive disparity fusion, while columns three, four and five have progressive
    features fusion.
  Figure 9 Link: articels_figures_by_rev_year\2019\Progressive_Fusion_for_Unsupervised_Binocular_Depth_Estimation_Using_Cycled_Netw\figure_9.jpg
  Figure 9 caption: Qualitative comparison with different competitive approaches with
    both supervised and unsupervised settings on the Eigen test set of KITTI dataset.
    The sparse groundtruth depth maps are filled with bilinear interpolation for better
    visualization.
  First author gender probability: 0.82
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Andrea Pilzer
  Name of the last author: Nicu Sebe
  Number of Figures: 9
  Number of Tables: 10
  Number of authors: 6
  Paper title: Progressive Fusion for Unsupervised Binocular Depth Estimation Using
    Cycled Networks
  Publication Date: 2019-09-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Detailed Architecture of the Proposed Network, for Readability
      Reasons We Show the Half-Cycle Structure
  Table 10 caption:
    table_text: TABLE 10 Comparison with the State of the Art on ApolloScape
  Table 2 caption:
    table_text: TABLE 2 Quantitative Evaluation Results of Different Variants of the
      Proposed Approach on the KITTI Dataset for the Ablation Study
  Table 3 caption:
    table_text: TABLE 3 Quantitative Evaluation Results of Different Variants of the
      Proposed Approach on the Cityscapes Dataset for the Ablation Study
  Table 4 caption:
    table_text: TABLE 4 Quantitative Evaluation Results of Different Variants of the
      Proposed Approach on the ApolloScape Dataset
  Table 5 caption:
    table_text: 'TABLE 5 Ablation Study: Exploiting Resynthesized Images'
  Table 6 caption:
    table_text: 'TABLE 6 Ablation Study: Discriminator Usage'
  Table 7 caption:
    table_text: 'TABLE 7 Ablation Study: Discriminator Usage'
  Table 8 caption:
    table_text: 'TABLE 8 Ablation Study: Impact of Feature Alignment: We Compare the
      Results without Alignment of Disparities and Features between the Two Stereo
      Branches (Upper Half) and with Alignment (Bottom Half)'
  Table 9 caption:
    table_text: TABLE 9 Comparison with the State of the Art
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2942928
- Affiliation of the first author: school of intelligent systems engineering, sun
    yat-sen university, guangzhou, china
  Affiliation of the last author: school of data and computer science, sun yat-sen
    university, china
  Figure 1 Link: articels_figures_by_rev_year\2019\Interpretable_Visual_Question_Answering_by_Reasoning_on_Dependency_Trees\figure_1.jpg
  Figure 1 caption: Illustration of our parse-tree-guided reasoning network (PTGRN)
    that sequentially performs reasoning over a dependency tree parsed from the question.
    Conditioned on preceding word nodes, PTGRN alternately mines visual evidence for
    nodes via an attention module and integrates the features of child nodes via a
    gated residual composition module.
  Figure 10 Link: articels_figures_by_rev_year\2019\Interpretable_Visual_Question_Answering_by_Reasoning_on_Dependency_Trees\figure_10.jpg
  Figure 10 caption: Quality comparison of the model with and without GRU and the
    propagation module.
  Figure 2 Link: articels_figures_by_rev_year\2019\Interpretable_Visual_Question_Answering_by_Reasoning_on_Dependency_Trees\figure_2.jpg
  Figure 2 caption: 'PTGRN pipeline. Each PTGRN module is composed of an attention
    module and two gated residual composition modules. Each node receives the encoded
    attention map, the hidden features from its children, and the image feature and
    word encoding. The attention module is employed to generate a new attention map
    conditioned on image features, word encodings and previous attended regions given
    by the child nodes. The gated residual composition module is trained to evolve
    a higher-level representation by dropping and integrating features of its children
    with local visual evidence. The edge modules transform the output attention and
    hidden feature according to the question encoding and the relation types (nmod:
    nominal modifier, dobj: direct object and nsubj: nominal subject). The blue arrows
    indicate the propagation process of the attention map, and the yellow arrows represent
    the process of the visual hidden representation.'
  Figure 3 Link: articels_figures_by_rev_year\2019\Interpretable_Visual_Question_Answering_by_Reasoning_on_Dependency_Trees\figure_3.jpg
  Figure 3 caption: Detailed architecture of the attention module. The image feature,
    previously attended regions and word encoding are projected to 2048-d features.
    Then, they are fused by elementwise multiplication. Finally, the fused feature
    is projected to a 1-d attention map and normalized with softmax.
  Figure 4 Link: articels_figures_by_rev_year\2019\Interpretable_Visual_Question_Answering_by_Reasoning_on_Dependency_Trees\figure_4.jpg
  Figure 4 caption: The gated residual composition module utilizes the architecture
    of a gated recurrent unit to integrate the features of its children with local
    visual evidence or attention map. The sum of children input is considered memory,
    and the local visual evidence or attention map is the input at the current step.
  Figure 5 Link: articels_figures_by_rev_year\2019\Interpretable_Visual_Question_Answering_by_Reasoning_on_Dependency_Trees\figure_5.jpg
  Figure 5 caption: The parse-tree-guided propagation module performs bilinear fusion
    between the hiddenattention map and question encoding. Different edge types have
    the same architecture but different sets of weights.
  Figure 6 Link: articels_figures_by_rev_year\2019\Interpretable_Visual_Question_Answering_by_Reasoning_on_Dependency_Trees\figure_6.jpg
  Figure 6 caption: "Two examples of the dependency trees of questions and corresponding\
    \ regions attended by our model at each step on the CLEVR dataset. The questions\
    \ are shown on the bottom. The input images and dependency parse trees are shown\
    \ on the left and lower parts. The arrows in the dependency tree are drawn from\
    \ the head words to the dependent words. The curved arrows point to pruned leaf\
    \ words that are not nouns. Thus, the word \u201Care\u201D is the root node for\
    \ both examples."
  Figure 7 Link: articels_figures_by_rev_year\2019\Interpretable_Visual_Question_Answering_by_Reasoning_on_Dependency_Trees\figure_7.jpg
  Figure 7 caption: Two examples of the dependency trees of questions and the corresponding
    regions attended by our model at each step on the FigureQA dataset.
  Figure 8 Link: articels_figures_by_rev_year\2019\Interpretable_Visual_Question_Answering_by_Reasoning_on_Dependency_Trees\figure_8.jpg
  Figure 8 caption: Examples of the dependency trees of questions and the corresponding
    regions attended by our model at each step on the VQAv2 dataset.
  Figure 9 Link: articels_figures_by_rev_year\2019\Interpretable_Visual_Question_Answering_by_Reasoning_on_Dependency_Trees\figure_9.jpg
  Figure 9 caption: Two examples of the dependency trees of questions and corresponding
    regions attended by our model at each step on the CLEVR-CoGen test set.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Qingxing Cao
  Name of the last author: Liang Lin
  Number of Figures: 10
  Number of Tables: 8
  Number of authors: 4
  Paper title: Interpretable Visual Question Answering by Reasoning on Dependency
    Trees
  Publication Date: 2019-09-24 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Question Answering Accuracy on the CLEVR Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Question Answering Accuracy on FigureQA Validation
      and Test Sets That Have Alternative Color Schemes
  Table 3 caption:
    table_text: TABLE 3 Question Answering Accuracy on VQAv2 Test-Dev and Test-Std
  Table 4 caption:
    table_text: TABLE 4 Comparisons of Question Answering Accuracy on the CLEVR-CoGenT
      Validation Set
  Table 5 caption:
    table_text: TABLE 5 CLEVR Validation Accuracy for Ablations
  Table 6 caption:
    table_text: TABLE 6 CLEVR Validation Accuracy of PTGRN with Different Word Encoding
      and Provided Layout
  Table 7 caption:
    table_text: TABLE 7 The Accuracy of Attention Maps on CLVER Validation Set
  Table 8 caption:
    table_text: TABLE 8 Accuracy on CLEVR with Randomly Perturbed Dependency Trees
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2943456
- Affiliation of the first author: agency for science technology and research (astar),
    singapore
  Affiliation of the last author: agency for science technology and research (astar),
    singapore
  Figure 1 Link: articels_figures_by_rev_year\2019\Nonlinear_Regression_via_Deep_Negative_Correlation_Learning\figure_1.jpg
  Figure 1 caption: Decision surfaces for classification of artificial spirals dataset
    for both (i) conventional ensemble learning and (ii) NCL learning. The shading
    of the background shows the decision surface for that particular class. The upper
    part of the figure corresponds to NCL learning and lower part stands for conventional
    ensemble learning. (b)-(d) shows the decision surface of individual model and
    (e) shows the ensemble decision surface arising from averaging over its individual
    models. NCL in (b)-(e) leads to much diversified decision surface where errors
    from individual models may cancel out thus resulting in much better generalization
    ability. Best viewed in color.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Nonlinear_Regression_via_Deep_Negative_Correlation_Learning\figure_2.jpg
  Figure 2 caption: Demonstration of the training process of conventional ensemble
    learning and NCL. The central solid gray line represents the ground truth and
    all other lines stand for different base models. Although both conventional ensemble
    learning (left part) and NCL (right part) may lead to correct estimations by simple
    model averaging, NCL results in much diversified individual models which make
    error cancellation possible on testing data.
  Figure 3 Link: articels_figures_by_rev_year\2019\Nonlinear_Regression_via_Deep_Negative_Correlation_Learning\figure_3.jpg
  Figure 3 caption: "Details of the proposed DNCL. Regression is formulated as ensemble\
    \ learning with the same amount of parameter as a single CNN. DNCL processes the\
    \ input by a stack of typical convolutional and pooling layers. Finally, a \u201C\
    divide and conquer\u201D strategy is adapted to learn a pool of regressors to\
    \ regress the output on top of each convolutional feature map at top layers. Each\
    \ regressor is jointly optimized with the CNN by an amended cost function, which\
    \ penalizes correlations with others to make better trade-offs among the bias-variance-covariance\
    \ in the ensemble."
  Figure 4 Link: articels_figures_by_rev_year\2019\Nonlinear_Regression_via_Deep_Negative_Correlation_Learning\figure_4.jpg
  Figure 4 caption: "Visual comparison for 4\xD7 super-resolution of different super-resolution\
    \ results. Fig. 4b shows the ground-truth high resolution image cropped from the\
    \ original image in Fig. 4a."
  Figure 5 Link: articels_figures_by_rev_year\2019\Nonlinear_Regression_via_Deep_Negative_Correlation_Learning\figure_5.jpg
  Figure 5 caption: The Multi-Scale Blob module used in NCL.
  Figure 6 Link: articels_figures_by_rev_year\2019\Nonlinear_Regression_via_Deep_Negative_Correlation_Learning\figure_6.jpg
  Figure 6 caption: Visualization on the diversities with all 64 base models. The
    first row shows the input image and the ground-truth number of people. The second
    row shows the predicted density map from conventional ensemble and NCL, respectively.
    The number in the bracket represents the ground-truth number of people. The third
    row shows the pair-wise euclidean distance between the predictions of individual
    base models in conventional ensemble and NCL, respectively. It can be seen that
    the proposed method leads to much diversified base models which can yield better
    overall performances.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.79
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.67
  Name of the first author: Le Zhang
  Name of the last author: Zeng Zeng
  Number of Figures: 6
  Number of Tables: 12
  Number of authors: 8
  Paper title: Nonlinear Regression via Deep Negative Correlation Learning
  Publication Date: 2019-09-26 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparing Results of Different Methods on the UCFCC50 Dataset
  Table 10 caption:
    table_text: "TABLE 10 Results of Different Ensemble Strategies on the Personality\
      \ and the Urban100 (Scale Factor of \xD74 \xD74) Dataset"
  Table 2 caption:
    table_text: TABLE 2 Comparison of Crowd Counting Methods on the Shanghaitech Dataset
  Table 3 caption:
    table_text: "TABLE 3 Comparison of Mean Absolute Error (MAE \u2193 \u2193) of\
      \ Different Crowd Counting Methods on the WorldExpo10 Dataset"
  Table 4 caption:
    table_text: TABLE 4 Personality Prediction Bench-Marking Using Mean Accuracy A
      A and Coefficient of Determination R 2 R2 Scores
  Table 5 caption:
    table_text: TABLE 5 Comparison of the Properties of the Proposed Method versus
      the Top Teams in the 2016 ChaLearn First Impressions Challenge
  Table 6 caption:
    table_text: TABLE 6 Results of Different Age Estimation Methods on the MORPH [95]
      and FG-NET [96] Datasets
  Table 7 caption:
    table_text: "TABLE 7 Average PSNRSSIMIFC Score for Image Super-Resolution of Scale\
      \ Factor \xD72 \xD72, \xD73 \xD73 and \xD74 \xD74 on Datasets Set5, Set14, BSD100\
      \ and Urban100"
  Table 8 caption:
    table_text: TABLE 8 Comparing the Performance of NCL and Conventional Ensemble
      on the Crowd Counting Task
  Table 9 caption:
    table_text: TABLE 9 Results of Different Ensemble Strategies on the Age and Crowd
      Datasets
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2943860
- Affiliation of the first author: school of information science and technology, shanghaitech
    university, shanghai, china
  Affiliation of the last author: school of information science and technology, shanghaitech
    university, shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2019\Video_Anomaly_Detection_with_Sparse_Coding_Inspired_Deep_Neural_Networks\figure_1.jpg
  Figure 1 caption: "The blue boxes represent the input x t . The green boxes represent\
    \ hidden states h k t or coding vectors \u03B1 k t . The orange circles are similarities\
    \ between neighboring frames."
  Figure 10 Link: articels_figures_by_rev_year\2019\Video_Anomaly_Detection_with_Sparse_Coding_Inspired_Deep_Neural_Networks\figure_10.jpg
  Figure 10 caption: The learned similarity with two different strategies.
  Figure 2 Link: articels_figures_by_rev_year\2019\Video_Anomaly_Detection_with_Sparse_Coding_Inspired_Deep_Neural_Networks\figure_2.jpg
  Figure 2 caption: sRNN-AE with trainable similarity measurement.
  Figure 3 Link: articels_figures_by_rev_year\2019\Video_Anomaly_Detection_with_Sparse_Coding_Inspired_Deep_Neural_Networks\figure_3.jpg
  Figure 3 caption: The whole pipeline of our proposed anomaly detection. It consists
    of a feature extraction module and an anomaly detection module.
  Figure 4 Link: articels_figures_by_rev_year\2019\Video_Anomaly_Detection_with_Sparse_Coding_Inspired_Deep_Neural_Networks\figure_4.jpg
  Figure 4 caption: A sample with an anomaly caused by appearance on the Moving-MNIST
    dataset.
  Figure 5 Link: articels_figures_by_rev_year\2019\Video_Anomaly_Detection_with_Sparse_Coding_Inspired_Deep_Neural_Networks\figure_5.jpg
  Figure 5 caption: Some samples from our new proposed dataset and other datasets.
    The first tow rows represent some samples from the UCSD Ped1, UCSD Ped2, CUHK
    Avenue and Subway Entrance and Subway Exit datasets, respectively. The last two
    rows represent normal and abnormal scenes from our proposed dataset (ShanghaiTech
    Campus).
  Figure 6 Link: articels_figures_by_rev_year\2019\Video_Anomaly_Detection_with_Sparse_Coding_Inspired_Deep_Neural_Networks\figure_6.jpg
  Figure 6 caption: Scores, similarities and distances between neighboring codes of
    two video samples on the Ped2 and ShanghaiTech. We can see that the similarities
    between neighboring frames can be kept for normal events. We highlight the abnormal
    events with red boxes. (Best viewed in color).
  Figure 7 Link: articels_figures_by_rev_year\2019\Video_Anomaly_Detection_with_Sparse_Coding_Inspired_Deep_Neural_Networks\figure_7.jpg
  Figure 7 caption: The change of AUC w.r.t. lambda 1 , lambda 2 and dictionary size.
    (a) and (b) are conducted on Avenue and Ped2 datasets. (c) is conducted on Avenue.
    (Best viewed in color).
  Figure 8 Link: articels_figures_by_rev_year\2019\Video_Anomaly_Detection_with_Sparse_Coding_Inspired_Deep_Neural_Networks\figure_8.jpg
  Figure 8 caption: AUC and sparsity of different numbers of layers in SRNN-AE on
    the Avenue and the ShanghaiTech dataset. (Best viewed in color).
  Figure 9 Link: articels_figures_by_rev_year\2019\Video_Anomaly_Detection_with_Sparse_Coding_Inspired_Deep_Neural_Networks\figure_9.jpg
  Figure 9 caption: The change of training loss with different number of iterations
    on Avenue dataset.
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Weixin Luo
  Name of the last author: Shenghua Gao
  Number of Figures: 11
  Number of Tables: 10
  Number of authors: 7
  Paper title: Video Anomaly Detection with Sparse Coding Inspired Deep Neural Networks
  Publication Date: 2019-09-27 00:00:00
  Table 1 caption:
    table_text: TABLE 1 AUC on Moving-MNIST Dataset
  Table 10 caption:
    table_text: TABLE 10 Running Time of Conv-AE with Feature Extraction, TSC and
      sRNN-AE without Feature Extraction on Avenue Dataset
  Table 2 caption:
    table_text: TABLE 2 Comparision of Our Dataset with Other Released Datasets
  Table 3 caption:
    table_text: TABLE 3 AUC of Different Methods on the Avenue, Ped2, Entrance, Exit
      and our Dataset (ShanghaiTech Campus)
  Table 4 caption:
    table_text: TABLE 4 AUC on the LV Dataset
  Table 5 caption:
    table_text: TABLE 5 AUC of Different RNN Variants on Avenue, Ped2 and ShanghaiTech
      Datasets
  Table 6 caption:
    table_text: TABLE 6 AUC of Different Similarity Measurement on Avenue, Ped2 and
      ShanghaiTech Datasets
  Table 7 caption:
    table_text: TABLE 7 AUC and Average Inference Time of Different Temporal Features
      on Three Datasets
  Table 8 caption:
    table_text: TABLE 8 AUC of Only Spatial and Only Temporal Features on Three Datasets
  Table 9 caption:
    table_text: TABLE 9 AUC of Different Feature Fusions on Three Datasets
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2944377
- Affiliation of the first author: department of computer science, university of houston,
    houston, tx, usa
  Affiliation of the last author: department of computer science, university of houston,
    houston, tx, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\A_GraphBased_Approach_for_Making_ConsensusBased_Decisions_in_Image_Search_and_Pe\figure_1.jpg
  Figure 1 caption: Using Score Distance Graph (SDG) for ranking using similarity
    scores. (a) Probe image, (b) Gallery set with corresponding matching scores using
    3 different algorithms, (c) Score Distance Graph using scores from just 1 algorithm,
    ScoresA (Correct ranking path shown in red), (d) Score Distance Graph using scores
    from all 3 algorithms (applying Mahalanobis Distance), (e) Scores plotted on a
    number line for ScoresA (shortest path connecting all nodes shown in red), (f)
    Correct ranking, (g) Wrong starting node selection leads to wrong ranking (worst
    match chosen as the highest ranked).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\A_GraphBased_Approach_for_Making_ConsensusBased_Decisions_in_Image_Search_and_Pe\figure_2.jpg
  Figure 2 caption: "CMC curves showing the performance of SHaPE vis-\xE0-vis the\
    \ performance of \u2113 -UPPSF and DASF for consensus using (a) VIPeR, (b) CUHK01,\
    \ and (c) CUHK03 datasets."
  Figure 3 Link: articels_figures_by_rev_year\2019\A_GraphBased_Approach_for_Making_ConsensusBased_Decisions_in_Image_Search_and_Pe\figure_3.jpg
  Figure 3 caption: "CMC curves showing the performance of SHaPE vis-\xE0-vis the\
    \ performance of \u2113 -UPPSF and DASF for consensus using QMUL-GRID dataset."
  Figure 4 Link: articels_figures_by_rev_year\2019\A_GraphBased_Approach_for_Making_ConsensusBased_Decisions_in_Image_Search_and_Pe\figure_4.jpg
  Figure 4 caption: An example showing a probe image from CUHK01 dataset and the corresponding
    top-3 matches from the gallery using 5 algorithms and their consensus by SHaPE.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Arko Barman
  Name of the last author: Shishir K. Shah
  Number of Figures: 4
  Number of Tables: 4
  Number of authors: 2
  Paper title: A Graph-Based Approach for Making Consensus-Based Decisions in Image
    Search and Person Re-Identification
  Publication Date: 2019-09-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of the Performances of SHaPE, Graph Fusion, Query-Adaptive
      Late Fusion (QAF), and Global Parameter Tuning for Different Feature Combinations
      Using Holidays, Holidays + 1M, and Ukbench Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance of the 5 Individual Features for Image Search
      Using the 2 Datasets
  Table 3 caption:
    table_text: TABLE 3 Time Required for Computation for a Matlab Implementation
      of SHaPE on a 1.7 GHz Intel Core i3 Processor with 8GB RAM for Different Datasets
      and Gallery Sizes
  Table 4 caption:
    table_text: "TABLE 4 Cumulative Person Re-Identification Rates at Different Ranks\
      \ Using 5 Constituent Algorithms, and Their Combination Using QAF, \u2113 \u2113\
      -UPPSF, and SHaPE"
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2944597
- Affiliation of the first author: department of electrical and electronic engineering,
    university of hong kong, pokfulam, hong kong
  Affiliation of the last author: department of electrical and electronic engineering,
    university of hong kong, pokfulam, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2019\HighDimensional_Dense_Residual_Convolutional_Neural_Network_for_Light_Field_Reco\figure_1.jpg
  Figure 1 caption: Two-plane parameterization of light field imaging.
  Figure 10 Link: articels_figures_by_rev_year\2019\HighDimensional_Dense_Residual_Convolutional_Neural_Network_for_Light_Field_Reco\figure_10.jpg
  Figure 10 caption: Convergence analysis on different types of connections. The curves
    for each connection are based on the Average Mean PSNR on the validation set.
  Figure 2 Link: articels_figures_by_rev_year\2019\HighDimensional_Dense_Residual_Convolutional_Neural_Network_for_Light_Field_Reco\figure_2.jpg
  Figure 2 caption: The details of 4D feedforward convolutions on both spatial and
    angular dimensions.
  Figure 3 Link: articels_figures_by_rev_year\2019\HighDimensional_Dense_Residual_Convolutional_Neural_Network_for_Light_Field_Reco\figure_3.jpg
  Figure 3 caption: "Visualization of the geometric features extracted from the proposed\
    \ 4D framework. (a) The collection of 2D slices through the learned feature maps.\
    \ (b) A certain 2D slice of the 4D geometric feature map shown in (a), and the\
    \ EPIs located at corresponding lines. (c) The spatial reconstruction results.\
    \ (d)\u2013(f) Geometric features extracted from different 4D convolutional layer."
  Figure 4 Link: articels_figures_by_rev_year\2019\HighDimensional_Dense_Residual_Convolutional_Neural_Network_for_Light_Field_Reco\figure_4.jpg
  Figure 4 caption: Illustration of partially occluded region in EPI pattern. The
    positive direction of s denotes the left views.
  Figure 5 Link: articels_figures_by_rev_year\2019\HighDimensional_Dense_Residual_Convolutional_Neural_Network_for_Light_Field_Reco\figure_5.jpg
  Figure 5 caption: The overview of the proposed model. Our model consists of a residual
    network for restoring the local spatio-angular information of light field and
    a refinement network for reconstructing the spatial details of scenes. Blue arrows
    indicate high-dimensional convolution operation, while yellow arrow stands for
    activation operation. Green arrows indicate addition and red arrow denotes upsampling.
  Figure 6 Link: articels_figures_by_rev_year\2019\HighDimensional_Dense_Residual_Convolutional_Neural_Network_for_Light_Field_Reco\figure_6.jpg
  Figure 6 caption: "Upscaling operation used for resolution enhancement. For clarity,\
    \ in this example, we only consider a single feature tensor (batch size is 1)\
    \ with a single channel C=1 . Given the LR input feature tensor with dimension\
    \ C\xD7H\xD7W\xD7S\xD7T ( =1\xD74\xD74\xD73\xD73 ), we first add 2 zero-padding\
    \ frames, and then apply the 4D convolution on the feature tensor. We use four\
    \ 4D convolution kernels to generate the LR feature map with 4 channels (denoted\
    \ by 4 main colors in step \u2461). Subsequently, interpolation is first performed\
    \ on ( S\xD7T ) angular dimensions of LR feature map. For spatial resolution,\
    \ we applied the shuffle operation which enhances the ( H\xD7T ) spatial resolution\
    \ of feature map and reduces the channel resolution. Therefore, at the end we\
    \ have a super-resolved feature tensor of size 1\xD78\xD78\xD75\xD75 ."
  Figure 7 Link: articels_figures_by_rev_year\2019\HighDimensional_Dense_Residual_Convolutional_Neural_Network_for_Light_Field_Reco\figure_7.jpg
  Figure 7 caption: "Illustration of light distribution at the place with two occluders\
    \ (a) the light ray model near occluders. The blue line denotes camera plane and\
    \ x i ( i=0,1,2,3 ) is a point in the background, while s i ( i=0,1,\u2026,8 )\
    \ stands for the viewpoint. The orange square denotes the selected viewpoints\
    \ and pixel in background when occlusions are contributed by only 1 occluder,\
    \ while the red square is used to for places where the occlusions are contributed\
    \ by 2 occluders. (b) illustration of light ray model in the spatial dimensions.\
    \ The solid point represents the pixel without occlusion while hollow point stands\
    \ for the occluded pixel."
  Figure 8 Link: articels_figures_by_rev_year\2019\HighDimensional_Dense_Residual_Convolutional_Neural_Network_for_Light_Field_Reco\figure_8.jpg
  Figure 8 caption: "Finding the angular kernel size. The curves are based on the\
    \ average mean PSNR on a subset of the Stanford Archive scenes with spatial scaling\
    \ factor \xD72 and angular scaling factor \xD72 ."
  Figure 9 Link: articels_figures_by_rev_year\2019\HighDimensional_Dense_Residual_Convolutional_Neural_Network_for_Light_Field_Reco\figure_9.jpg
  Figure 9 caption: Local residual connection. We explore three different ways of
    skip connection in the residual modules for training the proposed models.
  First author gender probability: 0.94
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Nan Meng
  Name of the last author: Edmund Y. Lam
  Number of Figures: 17
  Number of Tables: 3
  Number of authors: 4
  Paper title: High-Dimensional Dense Residual Convolutional Neural Network for Light
    Field Reconstruction
  Publication Date: 2019-10-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Ablation Study of Different Components in the Proposed Model
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Evaluation of State-of-the-Art LFSR Algorithms
  Table 3 caption:
    table_text: TABLE 3 Quantitative Evaluation of State-of-the-Art View Synthesis
      Algorithms
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2945027
- Affiliation of the first author: department of computing, imperial college london,
    london, united kingdom
  Affiliation of the last author: department of computing, imperial college london,
    london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2019\SEWA_DB_A_Rich_Database_for_AudioVisual_Emotion_and_Sentiment_Research_in_the_Wi\figure_1.jpg
  Figure 1 caption: Front page of the online SEWA database and the search filters.
  Figure 10 Link: articels_figures_by_rev_year\2019\SEWA_DB_A_Rich_Database_for_AudioVisual_Emotion_and_Sentiment_Research_in_the_Wi\figure_10.jpg
  Figure 10 caption: Examples of the mimicry episodes.
  Figure 2 Link: articels_figures_by_rev_year\2019\SEWA_DB_A_Rich_Database_for_AudioVisual_Emotion_and_Sentiment_Research_in_the_Wi\figure_2.jpg
  Figure 2 caption: Example of facial landmark annotation. The 49 facial landmarks
    were annotated for all segments included in the basic SEWA dataset.
  Figure 3 Link: articels_figures_by_rev_year\2019\SEWA_DB_A_Rich_Database_for_AudioVisual_Emotion_and_Sentiment_Research_in_the_Wi\figure_3.jpg
  Figure 3 caption: CED curves of the Chehra tracker [54] and the Dlib tracker [57]
    on the manually corrected frames.
  Figure 4 Link: articels_figures_by_rev_year\2019\SEWA_DB_A_Rich_Database_for_AudioVisual_Emotion_and_Sentiment_Research_in_the_Wi\figure_4.jpg
  Figure 4 caption: Examples of hand gesture annotation.
  Figure 5 Link: articels_figures_by_rev_year\2019\SEWA_DB_A_Rich_Database_for_AudioVisual_Emotion_and_Sentiment_Research_in_the_Wi\figure_5.jpg
  Figure 5 caption: Examples of head nod (top row) and head shake (bottom row) sequences.
  Figure 6 Link: articels_figures_by_rev_year\2019\SEWA_DB_A_Rich_Database_for_AudioVisual_Emotion_and_Sentiment_Research_in_the_Wi\figure_6.jpg
  Figure 6 caption: Examples of AU1 (inner brow raiser), AU2 (outer brow raiser),
    AU4 (Brow lowerer), AU12 (Lip corner puller), and AU17 (Chin raiser).
  Figure 7 Link: articels_figures_by_rev_year\2019\SEWA_DB_A_Rich_Database_for_AudioVisual_Emotion_and_Sentiment_Research_in_the_Wi\figure_7.jpg
  Figure 7 caption: An example of the continuously valued annotation results on valence,
    arousal, and liking and disliking.
  Figure 8 Link: articels_figures_by_rev_year\2019\SEWA_DB_A_Rich_Database_for_AudioVisual_Emotion_and_Sentiment_Research_in_the_Wi\figure_8.jpg
  Figure 8 caption: Examples of behaviour templates identified from the basic SEWA
    dataset.
  Figure 9 Link: articels_figures_by_rev_year\2019\SEWA_DB_A_Rich_Database_for_AudioVisual_Emotion_and_Sentiment_Research_in_the_Wi\figure_9.jpg
  Figure 9 caption: Examples of agreement and disagreement episodes.
  First author gender probability: 0.72
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.99
  Name of the first author: Jean Kossaifi
  Name of the last author: Maja Pantic
  Number of Figures: 10
  Number of Tables: 12
  Number of authors: 13
  Paper title: 'SEWA DB: A Rich Database for Audio-Visual Emotion and Sentiment Research
    in the Wild'
  Publication Date: 2019-10-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of Corpus Available for Estimation of Arousal and
      Valence from Audiovisual Data, Featuring Unscripted Interactive Discourse
  Table 10 caption:
    table_text: TABLE 10 Results for Valence and Arousal Estimation Using Deep Convolutional
      Neural Networks
  Table 2 caption:
    table_text: TABLE 2 SEWA Demographics
  Table 3 caption:
    table_text: TABLE 3 Number of Frames with Active AUs in Training (TR), Test (TE),
      and Validation (VA) Set
  Table 4 caption:
    table_text: TABLE 4 Behaviour Templates Identified in the Basic SEWA Dataset
  Table 5 caption:
    table_text: TABLE 5 Agreement Disagreement Episodes Identified in the Video-Chat
      Recordings
  Table 6 caption:
    table_text: TABLE 6 INTERSPEECH 2013 Computational Paralinguistics Challenge Feature
      Set
  Table 7 caption:
    table_text: TABLE 7 Geneva Minimalistic Acoustic Parameter Set
  Table 8 caption:
    table_text: TABLE 8 F1-Score for AU Detection on the Test Partition
  Table 9 caption:
    table_text: TABLE 9 F1-Score for AU Detection on the Development Partition
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2944808
- Affiliation of the first author: beihang university, beijing, china
  Affiliation of the last author: beihang university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2019\MFQE__A_New_Approach_for_MultiFrame_Quality_Enhancement_on_Compressed_Video\figure_1.jpg
  Figure 1 caption: An example for quality fluctuation (top) and quality enhancement
    performance (bottom).
  Figure 10 Link: articels_figures_by_rev_year\2019\MFQE__A_New_Approach_for_MultiFrame_Quality_Enhancement_on_Compressed_Video\figure_10.jpg
  Figure 10 caption: Average results of Delta PSNR (dB) and Delta SSIM for PQFs and
    non-PQFs in all test sequences at different QPs.
  Figure 2 Link: articels_figures_by_rev_year\2019\MFQE__A_New_Approach_for_MultiFrame_Quality_Enhancement_on_Compressed_Video\figure_2.jpg
  Figure 2 caption: Examples of video sequences in our enlarged database.
  Figure 3 Link: articels_figures_by_rev_year\2019\MFQE__A_New_Approach_for_MultiFrame_Quality_Enhancement_on_Compressed_Video\figure_3.jpg
  Figure 3 caption: PSNR (dB) curves of compressed video by various compression standards.
  Figure 4 Link: articels_figures_by_rev_year\2019\MFQE__A_New_Approach_for_MultiFrame_Quality_Enhancement_on_Compressed_Video\figure_4.jpg
  Figure 4 caption: An example of frame-level quality fluctuation in video Football
    compressed by HEVC.
  Figure 5 Link: articels_figures_by_rev_year\2019\MFQE__A_New_Approach_for_MultiFrame_Quality_Enhancement_on_Compressed_Video\figure_5.jpg
  Figure 5 caption: The average CC value of each pair of adjacent frames in HEVC.
  Figure 6 Link: articels_figures_by_rev_year\2019\MFQE__A_New_Approach_for_MultiFrame_Quality_Enhancement_on_Compressed_Video\figure_6.jpg
  Figure 6 caption: The framework of our proposed MFQE approach. Both non-PQFs and
    PQFs are enhanced by MF-CNN with the help of their nearest previous and subsequent
    PQFs. Note that the networks of enhancing PQFs and non-PQFs are trained, respectively.
  Figure 7 Link: articels_figures_by_rev_year\2019\MFQE__A_New_Approach_for_MultiFrame_Quality_Enhancement_on_Compressed_Video\figure_7.jpg
  Figure 7 caption: The architecture of our BiLSTM based PQF detector.
  Figure 8 Link: articels_figures_by_rev_year\2019\MFQE__A_New_Approach_for_MultiFrame_Quality_Enhancement_on_Compressed_Video\figure_8.jpg
  Figure 8 caption: The architecture of our MC-subnet.
  Figure 9 Link: articels_figures_by_rev_year\2019\MFQE__A_New_Approach_for_MultiFrame_Quality_Enhancement_on_Compressed_Video\figure_9.jpg
  Figure 9 caption: The architecture of our QE-subnet. In the multi-scale feature
    extraction component (denoted by C1-C9), the filter sizes of C147, C258 and C369
    are 3 times 3 , 5 times 5 and 7 times 7 , respectively, and the filter number
    is set to 32 for each layer. Note that C1-C9 are directly applied to frames Fp1prime
    , Fnp or Fp2prime . In the densely connected mapping construction (denoted by
    C10-C14), the filter size and number are set to 3 times 3 and 32, respectively.
    The last layer C15 has only one filter with the size of 3 times 3 . In addition,
    the PReLU activation is applied to C1-C14, while BN is applied to C10-C15.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Zhenyu Guan
  Name of the last author: Zulin Wang
  Number of Figures: 15
  Number of Tables: 8
  Number of authors: 6
  Paper title: 'MFQE 2.0: A New Approach for Multi-Frame Quality Enhancement on Compressed
    Video'
  Publication Date: 2019-10-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Averaged SD, PVD and PS Values of Our Database
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Convolutional Layers for Pixel-Wise Motion Estimation
  Table 3 caption:
    table_text: TABLE 3 Performance of Our PQF Detector on Test Sequences
  Table 4 caption:
    table_text: TABLE 4 Performance of Our PQF Detector on Test Sequences at QP =
      37
  Table 5 caption:
    table_text: "TABLE 5 Overall Comparison for \u0394 \u0394PSNR (dB) and \u0394\
      \ \u0394SSIM ( \xD7 10 \u22124 \xD710-4) Over Test Sequences at Five QPs"
  Table 6 caption:
    table_text: TABLE 6 Overall BD-BR Reduction ( % %) of Test Sequences with the
      HEVC Baseline as an Anchor
  Table 7 caption:
    table_text: TABLE 7 Test Speed (fps) and Parameters
  Table 8 caption:
    table_text: "TABLE 8 Overall \u0394 \u0394PSNR (dB) of 10 Test Sequences at QP\
      \ = 37"
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2944806
- Affiliation of the first author: biomedical imaging and bioinformatics lab, machine
    intelligence unit, indian statistical institute, kolkata, west bengal, india
  Affiliation of the last author: biomedical imaging and bioinformatics lab, machine
    intelligence unit, indian statistical institute, kolkata, west bengal, india
  Figure 1 Link: articels_figures_by_rev_year\2019\Approximate_Graph_Laplacians_for_Multimodal_Data_Clustering\figure_1.jpg
  Figure 1 caption: Variation of Silhouette index and F-measure for different values
    of rank parameter r on omics data sets.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Approximate_Graph_Laplacians_for_Multimodal_Data_Clustering\figure_2.jpg
  Figure 2 caption: Variation of difference between full-rank and approximate eigenspaces
    with respect to rank r .
  Figure 3 Link: articels_figures_by_rev_year\2019\Approximate_Graph_Laplacians_for_Multimodal_Data_Clustering\figure_3.jpg
  Figure 3 caption: Scatter plots using first two components of different low-rank
    based approaches on LGG data set.
  Figure 4 Link: articels_figures_by_rev_year\2019\Approximate_Graph_Laplacians_for_Multimodal_Data_Clustering\figure_4.jpg
  Figure 4 caption: Scatter plots using first two components of different low-rank
    based approaches on STAD data set.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.59
  Name of the first author: Aparajita Khan
  Name of the last author: Pradipta Maji
  Number of Figures: 4
  Number of Tables: 6
  Number of authors: 2
  Paper title: Approximate Graph Laplacians for Multimodal Data Clustering
  Publication Date: 2019-10-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparative Performance Analysis of Individual Modalities
      and Proposed Approach on Omics Data
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparative Performance Analysis of Equally and Damped Weighted
      Combination on Omics Data
  Table 3 caption:
    table_text: TABLE 3 Comparative Performance Analysis of Full-Rank and Approximate
      Subspaces of Omics Data
  Table 4 caption:
    table_text: TABLE 4 Effect of Row-normalization on Different Subspaces on Omics
      Data
  Table 5 caption:
    table_text: TABLE 5 Comparative Performance Analysis of Proposed and Existing
      Approaches on Omics Data
  Table 6 caption:
    table_text: TABLE 6 Comparative Performance Analysis of Proposed and Existing
      Approaches on Benchmark Data Sets
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2945574
- Affiliation of the first author: "machine learning group, datalogisk institut, university\
    \ of copenhagen, k\xF8benhavn, denmark"
  Affiliation of the last author: pattern recognition laboratory, tu delft, delft,
    netherlands
  Figure 1 Link: articels_figures_by_rev_year\2019\A_Review_of_Domain_Adaptation_without_Target_Labels\figure_1.jpg
  Figure 1 caption: Example of a domain adaptation problem, in which patients are
    diagnosed with heart disease based on their age and cholesterol. (Left) Data from
    the source domain, a hospital in Budapest. (Right) Data from the target domain,
    a hospital in Long Beach, California.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\A_Review_of_Domain_Adaptation_without_Target_Labels\figure_2.jpg
  Figure 2 caption: Linear classifier (solid black line) trained on source data (left)
    and applied to target data (right).
  Figure 3 Link: articels_figures_by_rev_year\2019\A_Review_of_Domain_Adaptation_without_Target_Labels\figure_3.jpg
  Figure 3 caption: Example of importance-weighting. (Left) The source samples from
    Fig. 1 have been weighted (larger dot size is larger weight) based on their relative
    importance to the target domain, producing the importance-weighted classifier
    (black dashed line). (Right) Predictions made by the adapted classifier.
  Figure 4 Link: articels_figures_by_rev_year\2019\A_Review_of_Domain_Adaptation_without_Target_Labels\figure_4.jpg
  Figure 4 caption: Example of a feature-based method. (Left) The source samples from
    Fig. 1 have been translated to match the data from the target domain. Subsequently,
    a classifier is trained on the mapped source data (black dashed line). The original
    naive classifier (black solid line) is shown for comparison. (Right) The adapted
    classifier is applied to the target samples and predictions are shown.
  Figure 5 Link: articels_figures_by_rev_year\2019\A_Review_of_Domain_Adaptation_without_Target_Labels\figure_5.jpg
  Figure 5 caption: Example of an inference-based method. The target samples are soft-labeled
    through an adversary. The adapted classifier (dashed line) deviates from the unadapted
    source classifier (solid line) such that the empirical risk on the target samples
    (right) is lower than that of the source classifier.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Wouter M. Kouw
  Name of the last author: Marco Loog
  Number of Figures: 5
  Number of Tables: 0
  Number of authors: 2
  Paper title: A Review of Domain Adaptation without Target Labels
  Publication Date: 2019-10-07 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2945942
