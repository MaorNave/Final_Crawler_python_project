- Affiliation of the first author: "faculty of engineering, universidad de la sabana,\
    \ ch\xEDa, colombia"
  Affiliation of the last author: leiden institute of advanced computer science (liacs),
    leiden university, leiden, ca, the netherlands
  Figure 1 Link: articels_figures_by_rev_year\2023\Fast_and_Informative_Model_Selection_Using_Learning_Curve_CrossValidation\figure_1.jpg
  Figure 1 caption: Example of the pruning of LCCV. The black dashed line represents
    threshold value r . The orange dots represent performance observations on the
    validation set per anchor, and the orange curve is an MMF-model fitted through
    these points. The blue dots represent performance observations on the train set
    per anchor, and the blue curve is a fitted MMF model. The dashed green line represents
    the optimistic extrapolation from the last anchors on the validation curve. If
    this optimistic extrapolation does not improve over threshold value r , it is
    unlikely that the learner will actually improve over this value, and the evaluation
    of the learner can be stopped early.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Fast_and_Informative_Model_Selection_Using_Learning_Curve_CrossValidation\figure_2.jpg
  Figure 2 caption: 'Comparison between LCCV, vanilla cross-validation, a racing implementation
    and successive halving on 75 datasets, a superset of the AutoML benchmark. Top
    pane: Comparisons between methods that train the learner using 80% of the data,
    and evaluate the learner on the remaining 20%. Bottom pane: Comparison between
    methods that train using 90% of the data, and evaluate on the remaining 10%.'
  Figure 3 Link: articels_figures_by_rev_year\2023\Fast_and_Informative_Model_Selection_Using_Learning_Curve_CrossValidation\figure_3.jpg
  Figure 3 caption: 'Top: Predicted versus actual improvement when doubling the data.
    Bottom: Cumulative empirical distribution of the gap between these numbers.'
  Figure 4 Link: articels_figures_by_rev_year\2023\Fast_and_Informative_Model_Selection_Using_Learning_Curve_CrossValidation\figure_4.jpg
  Figure 4 caption: Accuracy of the IPL classifier, which predicts whether performance
    will increase by at least beta when doubling the data.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.91
  Name of the first author: Felix Mohr
  Name of the last author: Jan N. van Rijn
  Number of Figures: 4
  Number of Tables: 0
  Number of authors: 2
  Paper title: Fast and Informative Model Selection Using Learning Curve Cross-Validation
  Publication Date: 2023-03-08 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3251957
- Affiliation of the first author: "adana alparslan t\xFCrkes science and technology\
    \ university, adana, turkey"
  Affiliation of the last author: "adana alparslan t\xFCrkes science and technology\
    \ university, adana, turkey"
  Figure 1 Link: articels_figures_by_rev_year\2023\Graph_Theory_Based_LargeScale_Machine_Learning_With_MultiDimensional_Constrained\figure_1.jpg
  Figure 1 caption: ScSpInHItIbRD-VN model architecture [15].
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Graph_Theory_Based_LargeScale_Machine_Learning_With_MultiDimensional_Constrained\figure_2.jpg
  Figure 2 caption: Graph theory-based representation of the ScSpInHItIbRD-VN model.
  Figure 3 Link: articels_figures_by_rev_year\2023\Graph_Theory_Based_LargeScale_Machine_Learning_With_MultiDimensional_Constrained\figure_3.jpg
  Figure 3 caption: Estimated casualties with the constrained optimization algorithms.
  Figure 4 Link: articels_figures_by_rev_year\2023\Graph_Theory_Based_LargeScale_Machine_Learning_With_MultiDimensional_Constrained\figure_4.jpg
  Figure 4 caption: Mean errors and standard deviations of the estimates. The bars
    represent the mean errors and the lines represents the standard deviations. 1,
    2, 3, 4 denote CM-PSO, CM-SHADE, CM-SHADEWO, and CM-RLS, respectively.
  Figure 5 Link: articels_figures_by_rev_year\2023\Graph_Theory_Based_LargeScale_Machine_Learning_With_MultiDimensional_Constrained\figure_5.jpg
  Figure 5 caption: Estimated future casualties under 50000 (dashed red), 100000 (dotted
    black) and 150000 (solid blue) daily vaccinations.
  Figure 6 Link: articels_figures_by_rev_year\2023\Graph_Theory_Based_LargeScale_Machine_Learning_With_MultiDimensional_Constrained\figure_6.jpg
  Figure 6 caption: Estimated future casualties with and without the non-pharmacological
    policies under 150000 daily vaccinations.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Onder Tutsoy
  Name of the last author: Onder Tutsoy
  Number of Figures: 6
  Number of Tables: 3
  Number of authors: 1
  Paper title: Graph Theory Based Large-Scale Machine Learning With Multi-Dimensional
    Constrained Optimization Approaches for Exact Epidemiological Modeling of Pandemic
    Diseases
  Publication Date: 2023-03-13 00:00:00
  Table 1 caption:
    table_text: TABLE I Parameters of the Model
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Priority and Age Specific Vaccination Parameters
  Table 3 caption:
    table_text: TABLE III Weighting Parameters of the Policies
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3256421
- Affiliation of the first author: state key laboratory of multimodal artificial intelligence
    systems, institute of automation, chinese academy of sciences, beijing, china
  Affiliation of the last author: state key laboratory of multimodal artificial intelligence
    systems, institute of automation, chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2023\Affine_Subspace_Robust_LowRank_SelfRepresentation_From_Matrix_to_Tensor\figure_1.jpg
  Figure 1 caption: (a) The samples are approximately drawn from two 1D linear subspace.
    (b) The samples lie in two 1D affine subspace exactly without data errors. (c)
    The data approximately lie in two 1D affine subspace and the affine subspace boldsymboltextaff(mathbf
    X1) spanned by mathbf X1 intersect with boldsymboltextaff(mathbf X2) .
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Affine_Subspace_Robust_LowRank_SelfRepresentation_From_Matrix_to_Tensor\figure_2.jpg
  Figure 2 caption: Edge points are located on the line between two vertices. For
    inside and edge points mathbf xi , the shape of convex hull boldsymboltextconv(mathbf
    Xell -i) keeps consistent with boldsymboltextconv(mathbf Xell ) (see Fig. (a)).
    Whereas for vertices mathbf xi , the shape of boldsymboltextconv(mathbf Xell -i)
    will change (see Fig. (b)).
  Figure 3 Link: articels_figures_by_rev_year\2023\Affine_Subspace_Robust_LowRank_SelfRepresentation_From_Matrix_to_Tensor\figure_3.jpg
  Figure 3 caption: Comparison of the confusion matrices on Scene15 dataset where
    (a)(b)(c) represent the results of ARLRR-M on 3 views respectively and (d) is
    the results of ARLRR-TU on all views.
  Figure 4 Link: articels_figures_by_rev_year\2023\Affine_Subspace_Robust_LowRank_SelfRepresentation_From_Matrix_to_Tensor\figure_4.jpg
  Figure 4 caption: The sensitivity analysis of parameter lambda ((a)(b)(c)) and k
    ((d)(e)(f)) on 100leaves, COIL20, Scene15 and UCI-digits datasets.
  Figure 5 Link: articels_figures_by_rev_year\2023\Affine_Subspace_Robust_LowRank_SelfRepresentation_From_Matrix_to_Tensor\figure_5.jpg
  Figure 5 caption: Convergence curves of our proposals where the first to third rows
    represent ARLRR-M, ARLRR-TU and ARLRR-TS respectively. The blue and purple curves
    are the stop criteria, i.e., Vert mathbf Z(v)-mathbf H(v)Vert infty and Vert mathbf
    P(v)-mathbf I+mathbf Z(v)Vert infty . The red curves denote the performance, i.e.,
    accuracy.
  Figure 6 Link: articels_figures_by_rev_year\2023\Affine_Subspace_Robust_LowRank_SelfRepresentation_From_Matrix_to_Tensor\figure_6.jpg
  Figure 6 caption: The parameter tuning of lambda and k in terms of internal (Silhouette,
    dotted lines) and external (ACC, solid lines) metrics for ARLRR-TU.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yongqiang Tang
  Name of the last author: Wensheng Zhang
  Number of Figures: 6
  Number of Tables: 7
  Number of authors: 3
  Paper title: 'Affine Subspace Robust Low-Rank Self-Representation: From Matrix to
    Tensor'
  Publication Date: 2023-03-15 00:00:00
  Table 1 caption:
    table_text: TABLE I The Description of Experimental Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II The Single-View Subspace Clustering Results on ORL, Notting-Hill,
      COIL-20 and Scene-15 Datasets
  Table 3 caption:
    table_text: TABLE III The Single-View Subspace Clustering Results on 100Leaves,
      Caltech-101, UCI-Digits and Animals-50 Datasets
  Table 4 caption:
    table_text: TABLE IV The Statistical Analysis of Single-View Subspace Clustering
      Results on 24 Views of 8 Datasets
  Table 5 caption:
    table_text: TABLE V The Multi-View Subspace Clustering Results on 8 Datasets
  Table 6 caption:
    table_text: TABLE VI The Multi-View Semi-Supervised Classification Results on
      8 Datasets
  Table 7 caption:
    table_text: TABLE VII The Algorithm Running Time (in Minutes) on All Datasets
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3257407
- Affiliation of the first author: shandong artificial intelligence institute, shandong
    academy of sciences, qilu university of technology, jinan, shandong, china
  Affiliation of the last author: school of information science and technology, northwest
    university, kirkland, wa, usa
  Figure 1 Link: articels_figures_by_rev_year\2023\When_Object_Detection_Meets_Knowledge_Distillation_A_Survey\figure_1.jpg
  Figure 1 caption: Milestones of OD and KD. Taking 2012 as the demarcation line,
    the early OD methods were implemented using artificial features. Later, deep neural
    networks gradually became the mainstream methods for OD. Since RCNN [40] was proposed
    in 2014, various two-stage OD models successively emerged, followed by one-stage
    OD models, such as YOLO [41], DetectNet [42], and so on. At present, most OD models
    have huge numbers of network parameters and high computational complexity. Subsequently,
    model compression and knowledge transfer based on KD were proposed by Hinton in
    2015 [7], [43]. In 2017, KD-based OD models were first proposed; these approaches
    tended to be more lightweight than their predecessors [14], [15]. Later, related
    works increased rapidly, especially in 2021 and 2022. Moreover, KD-based OD models
    are being constantly optimized and applied to various OD tasks, such as KD-based
    conventional OD tasks [20], [44], KD-based 3D OD tasks [45], [46], KD-based incremental
    OD tasks [47], [48], and so on.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\When_Object_Detection_Meets_Knowledge_Distillation_A_Survey\figure_2.jpg
  Figure 2 caption: The basic framework of KD-based OD models. Conventional KD-based
    OD methods are largely applied to 2D images and distill the knowledge using soft
    labels. We represent the framework of these methods using solid lines and boxes.
    Moreover, some improvements to existing methods have been made by introducing
    multi-modal data or feature distillation (marked with dotted linesboxes) to the
    conventional framework.
  Figure 3 Link: articels_figures_by_rev_year\2023\When_Object_Detection_Meets_Knowledge_Distillation_A_Survey\figure_3.jpg
  Figure 3 caption: The structure of KD-based incremental OD.
  Figure 4 Link: articels_figures_by_rev_year\2023\When_Object_Detection_Meets_Knowledge_Distillation_A_Survey\figure_4.jpg
  Figure 4 caption: The utilization of different types of loss in a detailed structure
    of KD-based OD models. Hard label loss and regression loss are commonly used,
    while feature distillation loss and classification distillation loss are selectively
    applied in some methods for specific tasks. We represent these loss functions
    using dotted lines and boxes.
  Figure 5 Link: articels_figures_by_rev_year\2023\When_Object_Detection_Meets_Knowledge_Distillation_A_Survey\figure_5.jpg
  Figure 5 caption: The structure of multiple teacher models guiding one student model
    to learn the knowledge.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.89
  Name of the first author: Zhihui Li
  Name of the last author: Xiaojiang Chen
  Number of Figures: 5
  Number of Tables: 4
  Number of authors: 7
  Paper title: 'When Object Detection Meets Knowledge Distillation: A Survey'
  Publication Date: 2023-03-15 00:00:00
  Table 1 caption:
    table_text: TABLE I The Categorization of Related KD-Based OD Methods Based on
      OD Tasks and KD Strategies
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II The List of Extended OD Tasks Using KD. The Introduction
      of KD Technology Significantly Improves the Performance of These Extended OD
      Methods Via Knowledge Transfer, and Multiple Experiments are Performed on Several
      Related Datasets.
  Table 3 caption:
    table_text: TABLE III Comparison of Models on PASCAL VOC 2007(%). The Items in
      Bold are the mAP 0.5 0.5 Results Obtained by the methodsmodels Achieving Better
      Performance (mAP 0.5 >80.0 0.5>80.0%)
  Table 4 caption:
    table_text: TABLE IV Comparison of Models on MS COCO 2017(%). The Items in Bold
      are the mAP 0.5 0.5 Results With mAP 0.5 >60.0 0.5>60.0%
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3257546
- Affiliation of the first author: department of automation, moe key laboratory of
    system control and information processing, shanghai jiao tong university, shanghai,
    china
  Affiliation of the last author: department of electrical engineering (esat-stadius),
    ku leuven, leuven, belgium
  Figure 1 Link: articels_figures_by_rev_year\2023\Learning_With_Asymmetric_Kernels_Least_Squares_and_Feature_Interpretation\figure_1.jpg
  Figure 1 caption: Simple illustration for asymmetric dissimilarity. In order to
    show the asymmetric information, the dissimilarity between sample C and the other
    samples is shown in sub-figures (b), (c) and (d) as an example. (a) There are
    seven samples on a directed graph where red and green colors indicate two categories.
    The dissimilarity from one sample to another one is defined as the shortest path
    from the first sample to the latter, for example, the dissimilarity from sample
    A to sample E is 2 and if the sample can not be reached, dissimilarity is infty
    . (b) Symmetrization. The directed edges are replaced by undirected edges. (c)
    Source space. The dissimilarity from C to others. (d) Target space. The dissimilarity
    from other samples to C.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Mingzhen He
  Name of the last author: Johan A. K. Suykens
  Number of Figures: 1
  Number of Tables: 4
  Number of authors: 5
  Paper title: 'Learning With Asymmetric Kernels: Least Squares and Feature Interpretation'
  Publication Date: 2023-03-15 00:00:00
  Table 1 caption:
    table_text: "TABLE I Micro-Macro-F1 Scores (mean \xB1 std) of Different Methods\
      \ With the KL Kernel Between GMMs on Several Datasets"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II The Detailed Information of Used Directed Graph Datasets
  Table 3 caption:
    table_text: "TABLE III Micro-Macro-F1 Scores (mean \xB1 std) of Different Algorithms\
      \ on the Nodes Classification Task"
  Table 4 caption:
    table_text: "TABLE IV Classification Accuracy (mean \xB1 std) of LS-SVM and AsK-LS\
      \ With Several Kernels on the UCI Database"
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3257351
- Affiliation of the first author: inria, paris, france
  Affiliation of the last author: inria, paris, france
  Figure 1 Link: articels_figures_by_rev_year\2023\PhysicsInformed_Guided_Disentanglement_in_Generative_Networks\figure_1.jpg
  Figure 1 caption: Guided disentanglement. While naive GANs generate all target scene
    traits at once (Target - Entangled), we learn a disentangled version of the scene
    from guidance of physical model WtextMod(.) with estimated physical parameters
    ( tildew ). Our idea is to combine physical models of well-known phenomena (as
    raindrops) with generative capabilities of GANs, in a complementary manner. We
    combine a physical model for raindrops with wetness learned by the GAN (Target
    - Disentangled), by only training on entangled data (i.e., rainy scene with raindrops
    on the lens). See the unrealistic raindrops in naive GANs. We instead enable the
    generation of target style ( tildew ) or unseen scenarios (here, w1 , w2 ).
  Figure 10 Link: articels_figures_by_rev_year\2023\PhysicsInformed_Guided_Disentanglement_in_Generative_Networks\figure_10.jpg
  Figure 10 caption: Disentanglement user study. We asked 56 users (cf. Section V-A5)
    to judge the lens cleanness (a) on raindrops (r) and dirt (d), or the wetness
    (b) or coloring (c) of textclearmapsto textraintextdrop and textgraymapsto textcolortextdirt
    generated scenes, respectively. Details are in the text. Our system greatly improves
    results following human evaluation metrics.
  Figure 2 Link: articels_figures_by_rev_year\2023\PhysicsInformed_Guided_Disentanglement_in_Generative_Networks\figure_2.jpg
  Figure 2 caption: Model-guided disentanglement. Our unsupervised disentanglement
    process consists of applying a physical model WtextMod(.) to the generated image
    G(x) . Subsequently, the composite image is forwarded to the discriminator and
    the GAN loss ( LG or LD ) is backpropagated (dashed arrows). The model rendering
    depends on the estimated parameters tildew , composed by differentiable ( tildewd
    ) and non-differentiable ones ( tildewnd ). We use a Disentanglement Guidance
    (DG) to avoid interfering with the gradient propagation in the learning process.
    Green stands for real data, red for fake ones.
  Figure 3 Link: articels_figures_by_rev_year\2023\PhysicsInformed_Guided_Disentanglement_in_Generative_Networks\figure_3.jpg
  Figure 3 caption: Model-guided parameters estimation. (a) We exploit a pretrained
    discriminator Dtextent , to calculate an adversarial loss LG on source data augmented
    with the model WtextMod having differentiable parameters wd . In this process,
    the gradient flows only in direction of the differentiable parameters. (b) We
    optimize until convergence differentiable (blue) and non-differentiable (purple)
    parameters, alternatively reaching new minima ( tildewd and tildewnd ) used during
    optimization of the other parameter set. While differentiable parameters are regressed
    (Section III-C), non-differentiable ones require black-box genetic optimization
    (Section III-D), here CMA-ES [88].
  Figure 4 Link: articels_figures_by_rev_year\2023\PhysicsInformed_Guided_Disentanglement_in_Generative_Networks\figure_4.jpg
  Figure 4 caption: Neural-guided disentanglement. We exploit here a separate frozen
    GAN ( WtextGAN ) which renders specific target traits (here, dirt) on generator
    G output images before forwarding them to the discriminator D . We do not show
    gradient propagation for simplicity.
  Figure 5 Link: articels_figures_by_rev_year\2023\PhysicsInformed_Guided_Disentanglement_in_Generative_Networks\figure_5.jpg
  Figure 5 caption: Training pipelines. For model-guided disentanglement, we 1) train
    a naive i2i entangled baseline, 2) use the entangled discriminator feedback to
    estimate optimal parameters tildew and 3) Disentanglement Guidance (DG), and finally
    4) train the guided-GAN with model injection. For neural-guided disentanglement,
    we 1) train a GAN ( WtextGAN ) exploiting additional knowledge as semantics and
    2) use it to inject target traits during our guided-GAN training.
  Figure 6 Link: articels_figures_by_rev_year\2023\PhysicsInformed_Guided_Disentanglement_in_Generative_Networks\figure_6.jpg
  Figure 6 caption: Raindrop disentanglement on textclearmapsto textraintextdrop .
    We compare qualitatively with the state-of-the-art on the textclearmapsto textraintextdrop
    task with rain drops model-guided disentanglement. In the first row, we report
    samples of the target domain. Subsequently, the Source image (2nd row), the translations
    by different baselines (rows 3-7) and our results (rows 8-13). Our model-guided
    network is able to disentangle the generation of peculiar rainy characteristics
    from the drops on the windshield ('Disentangled' rows) and re-injection with estimated
    parameters ('Target-style'). We evaluate both the differentiable-only parameter
    estimation (rows 8-9) and the genetic-based full estimation (rows 10-11). We also
    show injection of other arbitrary parameters w1 , w2 (last 2 rows).
  Figure 7 Link: articels_figures_by_rev_year\2023\PhysicsInformed_Guided_Disentanglement_in_Generative_Networks\figure_7.jpg
  Figure 7 caption: Dirt disentanglement on textgraymapsto textcolortextdirt . We
    compare with MUNIT [49] for the textgraymapsto textcolortextdirt task. Although
    MUNIT successfully mimics the Target style (rows 1,3), our approach lead to a
    more realistic image colorization disentangling the presence of dirt ('Disentangled'
    row mathcal TWtextMod ) We also use the dirt model to reproduce Target images
    ('Target-style' row mathcal TWtextModwd(tildew) ).
  Figure 8 Link: articels_figures_by_rev_year\2023\PhysicsInformed_Guided_Disentanglement_in_Generative_Networks\figure_8.jpg
  Figure 8 caption: Composite disentanglement on textclearmapsto textsnowtextcmp .
    We extend the applicability of our method to composite occlusions, that we validate
    in the textclearmapsto textsnowtextcmp scenario. We add a fence-like occlusion
    (left) and a confidential watermark (right) to syntheticsnow, with random position.
    As expected, we encounter entanglement phenomena for MUNIT, while our model-guided
    network is successful in learning the disentangled appearance ('Disentangled'
    row mathcal TWtextModwd ). In our 'Target-style' row mathcal TWtextModwd(tildewd)
    , we inject the occlusions to mimic the target style.
  Figure 9 Link: articels_figures_by_rev_year\2023\PhysicsInformed_Guided_Disentanglement_in_Generative_Networks\figure_9.jpg
  Figure 9 caption: textsynthmapsto textWCStextfog translations. As visible, MUNIT
    shows entanglement phenomena, leading to artifacts. Our model-guided disentanglement,
    instead, enables to generate a wide range of foggy images, with arbitrary visibility,
    while mantaining realism. Since the fog model WtextMod always blocks the gradient
    propagation in the sky region, the network can not achieve photorealistic disentanglement
    but still improves the generated image quality.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Fabio Pizzati
  Name of the last author: Raoul de Charette
  Number of Figures: 18
  Number of Tables: 3
  Number of authors: 3
  Paper title: Physics-Informed Guided Disentanglement in Generative Networks
  Publication Date: 2023-03-15 00:00:00
  Table 1 caption:
    table_text: TABLE I Disentanglement Tasks. For Each Task, We Indicate the Features
      Entangled in the Target Domain (Also, Shorten as Indices of Task Name), the
      Datasets, and the Model or Neural Guidance Employed for Disentanglement
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Image Quality Evaluation. in (a), We Quantify GAN Metrics
      for All Tasks. While Quality-Aware Metrics are Always Successfully Increased,
      LPIPS Depends on the Visual Complexity of the Model and Presence of Artifacts.
      In (b), We Compare Our Pipeline for Finetuning Semantic Segmentation Network
      Outperforming the State-of-the-Art for Rain Generation
  Table 3 caption:
    table_text: "TABLE III Comparison of the Disentanglement Strategies. Model-Guided\
      \ Strategies do not Require Annotations and Ad-Hoc Generative Networks, but\
      \ They Rely on the Availability of a Somehow Realistic Physics Model. When Using\
      \ Neural-Guided Disentanglement, the Ability to Modify Physical Parameters of\
      \ the Model (\u201CEditable\u201D) is Lost. We Overcome the Need of Cumbersome\
      \ Manual Tuning of w nd wnd With Genetic Optimization in Our Full Strategy.\
      \ However, Results in Section V Advocate That Best Disentanglement Performances\
      \ are Still Obtained by Manually Sizing Each non Differentiable Parameter, at\
      \ the Cost of Intensive Labor and Many Trainings"
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3257486
- Affiliation of the first author: department of computer science and engineering,
    moe key lab of artificial intelligence, ai institute, shanghai jiao tong university,
    shanghai, china
  Affiliation of the last author: department of computer science and engineering,
    moe key lab of artificial intelligence, ai institute, shanghai jiao tong university,
    shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2023\Unsupervised_Learning_of_Graph_Matching_With_Mixture_of_Modes_via_Discrepancy_Mi\figure_1.jpg
  Figure 1 caption: Example of matching with a mixture of two modes. Within mode 1
    or mode 2 (intra-mode) there is full-matching, and between mode 1 and mode 2 (inter-mode)
    the matching is non-valid.
  Figure 10 Link: articels_figures_by_rev_year\2023\Unsupervised_Learning_of_Graph_Matching_With_Mixture_of_Modes_via_Discrepancy_Mi\figure_10.jpg
  Figure 10 caption: Ablation study results of the proposed MGM 3 method on WillowObject
    Class dataset (in line with the right half of Table V).
  Figure 2 Link: articels_figures_by_rev_year\2023\Unsupervised_Learning_of_Graph_Matching_With_Mixture_of_Modes_via_Discrepancy_Mi\figure_2.jpg
  Figure 2 caption: 'Comparison of the four GM settings over multiple graphs (same
    color denotes correspondence): (a) the classical full matching where all node
    pairs have a valid matching, (b) partial matching where some inlier nodes are
    occluded and all nodes partially correspond to a universe (here the universe has
    4 nodes), (c) outlier matching where the noisy outliers do not correspond to any
    nodes in the universe, (d) the mix of both partial and outlier matching, which
    is the most challenge yet the most realistic setting. Most existing papers [9],
    [23], [36], [37] only consider (a), some recent efforts [33], [35] also consider
    (b), and in this article we propose a robust approach considering (a)(b)(d). (c)
    is less common because outliers usually appear together with partial matching.'
  Figure 3 Link: articels_figures_by_rev_year\2023\Unsupervised_Learning_of_Graph_Matching_With_Mixture_of_Modes_via_Discrepancy_Mi\figure_3.jpg
  Figure 3 caption: Overview of the proposed unsupervised (visual) GM learning pipeline.
    Here the GM solver in the white box can be any of the solvers for matching either
    two-graph, multi-graph, or graphs from a mixture of modes. The stop-gradient operation
    prohibits model degeneration [39].
  Figure 4 Link: articels_figures_by_rev_year\2023\Unsupervised_Learning_of_Graph_Matching_With_Mixture_of_Modes_via_Discrepancy_Mi\figure_4.jpg
  Figure 4 caption: Necessity of stop-gradient in our unsupervised learning of graph
    matching, interestingly in line with the conclusions in [39].
  Figure 5 Link: articels_figures_by_rev_year\2023\Unsupervised_Learning_of_Graph_Matching_With_Mixture_of_Modes_via_Discrepancy_Mi\figure_5.jpg
  Figure 5 caption: In the context of visual image matching, it shows a working example
    of the proposed unsupervised MGM 3 learning pipeline with two classes (each contains
    two graphs). As Algorithm IV-C iterates with decreasing annealing parameter beta
    , the graph matching confidence increases (the darker the higher) and so for the
    clustering confidence as indicated by the brightened node color from white to
    greenred. As the other parallel branch, CNN and Sinkhorn net can form a node-wise
    matching network (connected by the blue arrows) whose samples are those within
    each class determined by Algorithm IV-C. The cross-entropy loss is computed to
    minimize the discrepancy between the above matching network and Algorithm IV-C,
    only considering the graphs from the same class. As such, the CNN weights can
    be trained via back-propagation along the flow in black dashed lines.
  Figure 6 Link: articels_figures_by_rev_year\2023\Unsupervised_Learning_of_Graph_Matching_With_Mixture_of_Modes_via_Discrepancy_Mi\figure_6.jpg
  Figure 6 caption: Mean and STD of precision, recall, and f1-score of graph matching
    with a single mode on CUB2011 dataset. MGM statics are computed from all graph
    pairs in each category, and two-graph matching statics are computed from 1000
    random graph pairs. Improvement can be seen from our learning-free methods to
    our unsupervised methods, which are comparative with novel supervised learning
    methods on the testing set.
  Figure 7 Link: articels_figures_by_rev_year\2023\Unsupervised_Learning_of_Graph_Matching_With_Mixture_of_Modes_via_Discrepancy_Mi\figure_7.jpg
  Figure 7 caption: Experiment result on Pascal VOC Keypoint dataset for initializing
    VGG16 weights of NGMv2 [36] and BBGM [35] by the proposed unsupervised learning
    model GANN-MGM. Without modifying any model architectures, the unsupervised pretraining
    by GANN-MGM leads to faster convergence and better performance compared to the
    versions initialized by ImageNet classification weights.
  Figure 8 Link: articels_figures_by_rev_year\2023\Unsupervised_Learning_of_Graph_Matching_With_Mixture_of_Modes_via_Discrepancy_Mi\figure_8.jpg
  Figure 8 caption: Mean and STD results of our learning-free GA-MGM 3 and unsupervised
    learning GANN-MGM 3 on the MGM 3 problem. Since the performance of our learning-free
    GA-MGM 3 is comparative to other learning-free baselines [9], [12] on Willow ObjectClass,
    we compare unsupervised learning-based GANN-MGM 3 with GA-MGM 3 . For the CUB2011
    dataset, both matching and clustering performances can be elevated by unsupervised
    learning. For the Pascal VOC Keypoint dataset, the clustering performance is nearly
    saturated, but the matching accuracy can be improved by unsupervised learning.
  Figure 9 Link: articels_figures_by_rev_year\2023\Unsupervised_Learning_of_Graph_Matching_With_Mixture_of_Modes_via_Discrepancy_Mi\figure_9.jpg
  Figure 9 caption: Our unsupervised learning image matching pipeline. The brown modules
    support gradient back-propagation and are learned by minimizing the discrepancy
    between SuperGlue [1] and our GA-MGM.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.89
  Name of the first author: Runzhong Wang
  Name of the last author: Xiaokang Yang
  Number of Figures: 11
  Number of Tables: 8
  Number of authors: 3
  Paper title: Unsupervised Learning of Graph Matching With Mixture of Modes via Discrepancy
    Minimization
  Publication Date: 2023-03-16 00:00:00
  Table 1 caption:
    table_text: TABLE I The Averaged Inference Time of Learning-Free GA-MGM and GA-MGM
      3 3 w or wo the Compact Matrix Form, on the Willow ObjectClass Dataset in Our
      Experiment (In Line With Table III and the Right Half of Table V)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Hyper-Parameter Configurations Over Different Datasets to
      Reproduce Our Reported Results in This Article
  Table 3 caption:
    table_text: TABLE III Matching Accuracy With Both Learning-Free MGM Methods and
      Supervised Learning Peer Methods on Willow ObjectClass Dataset (Mean and Std
      by 50 Trials). Compared Results are Quoted From the Original Papers Without
      Knowing the Std Results. Our Learning-Free GA-GM and GA-MGM Surpass Previous
      Learning-Free Methods, and Our Unsupervised Learning GANN-MGM Best Performs
      Among Supervised Learning Methods
  Table 4 caption:
    table_text: TABLE IV F1 Scores (%) on Pascal VOC Keypoint Dataset (Without Filtering).
      The Bold Statistics Denote the Best-Performing Method. The Pascal VOC Keypoint
      Dataset Seems to Be Challenging for Unsupervised Models Without Ground Truth
      Labels, and We Develop a New Graph Matching Learning Paradigm for State-of-The-Art
      Models NGMv2 and BBGM in Combination of Unsupervised Pretraining (By GANN-MGM)
      and Supervised Learning
  Table 5 caption:
    table_text: TABLE V Multiple Graph Matching With Mixture of Modes (MGM 3 3) Evaluation
      (With Inference Time in Second) on Willow ObjectClass Dataset. Our Learning-Free
      Version GA-MGM 3 3 is Slightly Inferior to DPMC [12], but Both Matching and
      Clustering Accuracies Can Be Elevated by Unsupervised Learning, and Our Unsupervised
      GANN-MGM 3 3 Surpasses All Peer Methods
  Table 6 caption:
    table_text: "TABLE VI Stereo Reconstruction (SfM) Accuracy and Matching Accuracy\
      \ (%) on the Validation Set of PhotoTourism Dataset. \u201CAUCX\u201D Means\
      \ Area Under Curve if the Pose Error is Smaller Than X Degrees. \u201CPrec\u201D\
      \ Means Matching Precision i.e. Correct Match Predicted Match Correct MatchPredicted\
      \ Match, and \u201CMScore\u201D Means Correct Match Keypoints Correct MatchKeypoints."
  Table 7 caption:
    table_text: TABLE VII Matching Accuracy on Willow Dataset (50 Tests), Where the
      Backbone Net of PIA+GA-MGM is Built by Extending the VGG16 With PIA-GM [23]
  Table 8 caption:
    table_text: TABLE VIII MGM 3 3 on Willow w 40 Cars, 50 Ducks, 40 Motorbikes With
      PIA + GA-MGM 3 3 (Mean and STD by 50 tests)
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3257830
- Affiliation of the first author: school of electronic information engineering and
    shen yuan honors college, beihang university, beijing, china
  Affiliation of the last author: school of electronic information engineering, beihang
    university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2023\DAQE_Enhancing_the_Quality_of_Compressed_Images_by_Exploiting_the_Inherent_Chara\figure_1.jpg
  Figure 1 caption: Motivation of our DAQE approach. There exist regions with different
    defocus values within an image. The defocus values are estimated by the defocus
    map estimation network (DMENet) [25]. The image is compressed by JPEG [2] with
    the quality factor (QF) as 20.
  Figure 10 Link: articels_figures_by_rev_year\2023\DAQE_Enhancing_the_Quality_of_Compressed_Images_by_Exploiting_the_Inherent_Chara\figure_10.jpg
  Figure 10 caption: Efficiency of our DAQE and compared approaches over the DIV2K
    test set compressed by BPG at QP =37 . The number of parameters is marked at the
    center of each circle. A larger circle radius indicates a larger number of parameters.
  Figure 2 Link: articels_figures_by_rev_year\2023\DAQE_Enhancing_the_Quality_of_Compressed_Images_by_Exploiting_the_Inherent_Chara\figure_2.jpg
  Figure 2 caption: Correlation between the patchwise defocus and PSNR (dB) values
    within a single image. Five example images from the DIV2K dataset with different
    contents are presented. The left color bar is for defocus maps, while the right
    one is for PSNR maps. Images are compressed by the BPG codec with the quantization
    parameter (QP) as 37.
  Figure 3 Link: articels_figures_by_rev_year\2023\DAQE_Enhancing_the_Quality_of_Compressed_Images_by_Exploiting_the_Inherent_Chara\figure_3.jpg
  Figure 3 caption: "Correlation between patch quality and features. The \u201Clum\u201D\
    \ and \u201Ccont\u201D are the abbreviations of \u201Cluminance\u201D and \u201C\
    contrast\u201D."
  Figure 4 Link: articels_figures_by_rev_year\2023\DAQE_Enhancing_the_Quality_of_Compressed_Images_by_Exploiting_the_Inherent_Chara\figure_4.jpg
  Figure 4 caption: Average patch quality of three clusters in terms of PSNR (dB)
    and SSIM.
  Figure 5 Link: articels_figures_by_rev_year\2023\DAQE_Enhancing_the_Quality_of_Compressed_Images_by_Exploiting_the_Inherent_Chara\figure_5.jpg
  Figure 5 caption: "Texture difference between three clusters. The \u201Cclus\u201D\
    \ is the abbreviation of \u201Ccluster\u201D. The texture difference is measured\
    \ by the TDIM value ( times 103 )."
  Figure 6 Link: articels_figures_by_rev_year\2023\DAQE_Enhancing_the_Quality_of_Compressed_Images_by_Exploiting_the_Inherent_Chara\figure_6.jpg
  Figure 6 caption: "(a) Framework of the DAQE approach and (b) structure of the defocus\
    \ estimation network (DENet). The notation \u201CConv- N \u201D represents the\
    \ convolution operator with N output feature maps. The notation \u201CMaxPoolRe-\
    \ M \u201D represents the max poolingresampling operator with a factor of M .\
    \ Notations \u201CBN\u201D, \u201CReLU\u201D, and \u201CLeakyReLU\u201D represent\
    \ the batch normalization [29], rectified linear unit [49], and leaky rectified\
    \ linear unit, respectively."
  Figure 7 Link: articels_figures_by_rev_year\2023\DAQE_Enhancing_the_Quality_of_Compressed_Images_by_Exploiting_the_Inherent_Chara\figure_7.jpg
  Figure 7 caption: "Structures of the (a) attention generation network (AGNet) and\
    \ (b) quality enhancement network (QENet). The notation \u201CConv- N \u201D represents\
    \ the convolution operator with N output feature maps. The notation \u201CRe-\
    \ M \u201D represents the resampling operator with a factor of M . The notation\
    \ \u201CReLU\u201D represents the rectified linear unit [49]."
  Figure 8 Link: articels_figures_by_rev_year\2023\DAQE_Enhancing_the_Quality_of_Compressed_Images_by_Exploiting_the_Inherent_Chara\figure_8.jpg
  Figure 8 caption: Rate-distortion curves of our DAQE and compared approaches. The
    rate is measured by the bits per pixel (BPP). The distortion is measured by PSNR
    (dB) and SSIM.
  Figure 9 Link: articels_figures_by_rev_year\2023\DAQE_Enhancing_the_Quality_of_Compressed_Images_by_Exploiting_the_Inherent_Chara\figure_9.jpg
  Figure 9 caption: Qualitative comparison of our DAQE and compared approaches.
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.71
  Name of the first author: Qunliang Xing
  Name of the last author: Yichen Guo
  Number of Figures: 12
  Number of Tables: 6
  Number of authors: 4
  Paper title: 'DAQE: Enhancing the Quality of Compressed Images by Exploiting the
    Inherent Characteristic of Defocus'
  Publication Date: 2023-03-16 00:00:00
  Table 1 caption:
    table_text: TABLE I Variation in the Patchwise Defocus Values Within a Single
      Image
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Datasets Adopted in This Paper. The Maximal Image Resolution
      (res.), Image Usage, and Image Indices of These Datasets are Indicated
  Table 3 caption:
    table_text: TABLE III Quantitative Comparison of Our DAQE and Compared Approaches
      for BPG-Compressed Images. PSNR (dB) and SSIM are Calculated With the BPG Baseline
      as the Anchor. Standard Deviation Values are Presented in Addition to the Results.
      All Results are Calculated on the RGB Channels. The PSNR and SSIM Values are
      Accurate to Two and Three Decimal Places, Respectively.
  Table 4 caption:
    table_text: TABLE IV Quantitative Comparison of Our DAQE and Compared Approaches
      for JPEG-Compressed Images. PSNR (dB) and SSIM are Calculated With the JPEG
      Baseline as the Anchor. Standard Deviation Values are Presented in Addition
      to the Results. All Results are Calculated on the RGB Channels. The PSNR and
      SSIM Values are Accurate to Two and Three Decimal Places, Respectively.
  Table 5 caption:
    table_text: TABLE V Rate-Distortion Performance of Our DAQE and Compared Approaches.
      The Rate-Distortion Performance is Measured by the BD-Rate Reduction (%) With
      the BPGJPEG Baseline as the Anchor. Standard Deviations are Presented in Addition
      to the Results. The Rate is Measured by the Bits Per Pixel (BPP). The Distortion
      is Measured by PSNR (dB) and SSIM.
  Table 6 caption:
    table_text: TABLE VI Ablation Results of Our DAQE Approach in Terms of PSNR (dB)
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3257888
- Affiliation of the first author: bnrist, tsinghua university, beijing, china
  Affiliation of the last author: bnrist, tsinghua university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2023\Random_Cycle_Loss_and_Its_Application_to_Voice_Conversion\figure_1.jpg
  Figure 1 caption: Illustration for IB-based information disentanglement. With the
    IB setting H(Zi)=H(Fi) and appropriate information bias, an AE model that seeks
    for minimum reconstruction loss ||X-hatX||2 will induce disentangled codes.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\Random_Cycle_Loss_and_Its_Application_to_Voice_Conversion\figure_2.jpg
  Figure 2 caption: The IB-based speech disentanglement model with RC loss. f and
    g are encoder and decoder respectively. Solid lines denote the encoding and decoding
    for the original utterance. Dashed lines denote the process of random substitution
    and cyclic decoding-encoding. mathcal Lrec and mathcal Lcyc represent the reconstruction
    loss and the RC loss, respectively.
  Figure 3 Link: articels_figures_by_rev_year\2023\Random_Cycle_Loss_and_Its_Application_to_Voice_Conversion\figure_3.jpg
  Figure 3 caption: Data samples generated by (9). Each curve represents a sample,
    and we choose one sample for each of the 10 classes.
  Figure 4 Link: articels_figures_by_rev_year\2023\Random_Cycle_Loss_and_Its_Application_to_Voice_Conversion\figure_4.jpg
  Figure 4 caption: MI between the content code and the class label, when trained
    with RC loss (RC), adversarial loss (AD) and MI loss (MI) as regularization. In
    the legend, the number appended to the name represents the weight on the regularization.
    The dimensionality of the latent code is 8 (Top) and 2 (Bottom) respectively.
  Figure 5 Link: articels_figures_by_rev_year\2023\Random_Cycle_Loss_and_Its_Application_to_Voice_Conversion\figure_5.jpg
  Figure 5 caption: An example of conversion with the conventional CAE and CAEs with
    adversarial loss (CAE+AD), MI loss (CAE+MI) and RC loss (CAE+RC). The weight for
    the ADMIRC loss is chosen to be 1, though other values show similar results. In
    each figure, the blue solid line shows the sample to provide class code ( S );
    the orange dashed line shows the sample to provide content code ( C ); the green
    dot-dashed line shows the converted sample with class code from S and content
    code from C , denoted by S + C ; the red dotted line is the mean-shift version
    of S + C , denoted by S + C + Delta . If the conversion is perfect, S + C will
    match S in mean value and C in dynamic change, so the curves of S + C + Delta
    and C should overlap. (a) CAE (b) CAE+AD (c) CAE+MI (d) CAE+RC.
  Figure 6 Link: articels_figures_by_rev_year\2023\Random_Cycle_Loss_and_Its_Application_to_Voice_Conversion\figure_6.jpg
  Figure 6 caption: AutoVC diagram in (Left) training phase and (Right) conversion
    phase.
  Figure 7 Link: articels_figures_by_rev_year\2023\Random_Cycle_Loss_and_Its_Application_to_Voice_Conversion\figure_7.jpg
  Figure 7 caption: CycleVC diagram for model training. Only path for the RC loss
    is shown.
  Figure 8 Link: articels_figures_by_rev_year\2023\Random_Cycle_Loss_and_Its_Application_to_Voice_Conversion\figure_8.jpg
  Figure 8 caption: The architecture of CycleFlow. Solid lines denote the path for
    reconstruction loss, and dashed lines denote the path for RC loss. Note that the
    pitch code Z1f is substituted by Z2f in the picture.
  Figure 9 Link: articels_figures_by_rev_year\2023\Random_Cycle_Loss_and_Its_Application_to_Voice_Conversion\figure_9.jpg
  Figure 9 caption: Results of subjective evaluation on 5 listening tasks.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Haoran Sun
  Name of the last author: Thomas F. Zheng
  Number of Figures: 9
  Number of Tables: 6
  Number of authors: 5
  Paper title: Random Cycle Loss and Its Application to Voice Conversion
  Publication Date: &id001 2023-03-16 00:00:00
  Table 1 caption:
    table_text: TABLE I Reconstruction Loss (Rec) and MI Tested on CAE WithWithout
      RC Loss. 'Code Dim' Represents the Dimensionality of the Content Code. Note
      That 1.0 is the Maximum Value of MI Computed by the NormalizedMutualInfoScore
      Function
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE II Comparison Among AutoVC, CycleVC, ADVC and MIVC on Reconstructed
      Speech
  Table 3 caption:
    table_text: TABLE III Comparison Among AutoVC, CycleVC, ADVC and MIVC on Converted
      Speech. 'CP' Denotes to Whom the Converted Speech Will Compare When Computing
      the 'metric'. 's' Denotes Source Speech, 't' Denotes Target Speech
  Table 4 caption:
    table_text: TABLE IV Quality Comparison Among SpeechFlow, CycleFlow and ADFlow
      on Reconstructed Speech
  Table 5 caption:
    table_text: TABLE V MI Between Input Speech and Codes, and Between Pairs of Codes.
      S S Denotes the Original Speech Signal. Z c Zc, Z r Zr, Z f Zf Denote Codes
      for Content, Rhythm and Pitch Respectively
  Table 6 caption:
    table_text: TABLE VI Comparison Among SpeechFlow, CycleFlow and ADFlow on Converted
      Speech. 'CP' Denotes to Whom the Converted Speech Will Compare When Computing
      the 'metric' in the 'conv' Task
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3257839
- Affiliation of the first author: school of computer science, hangzhou dianzi university,
    hangzhou, zhejiang, china
  Affiliation of the last author: school of computer science, hangzhou dianzi university,
    hangzhou, zhejiang, china
  Figure 1 Link: articels_figures_by_rev_year\2023\BrainMachine_Coupled_Learning_Method_for_Facial_Emotion_Recognition\figure_1.jpg
  Figure 1 caption: Overview of feature-mapping-based methods.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2023\BrainMachine_Coupled_Learning_Method_for_Facial_Emotion_Recognition\figure_2.jpg
  Figure 2 caption: Overview of brain-machine coupled learning method for FER. First,
    the preliminary representations of the two domains are extracted by EEGNet [52]
    and CNNNet [76], respectively. The commonalities between EEG signals and visual
    images are found using the common channel. After training, only the model of the
    visual domain is used for testing.
  Figure 3 Link: articels_figures_by_rev_year\2023\BrainMachine_Coupled_Learning_Method_for_Facial_Emotion_Recognition\figure_3.jpg
  Figure 3 caption: Experiment configuration. (a) During the acquisition, the participants
    sit comfortably in a quiet room. Their sitting postures are adjusted to keep their
    eyes fixed in the middle of the screen and a certain distance from the computer
    screen. (b) The acquisition of EEG signals mainly uses the NeuroScan64 lead EEG
    cap, containing 62 scalp electrodes and two reference electrodes.
  Figure 4 Link: articels_figures_by_rev_year\2023\BrainMachine_Coupled_Learning_Method_for_Facial_Emotion_Recognition\figure_4.jpg
  Figure 4 caption: Emotion-induced paradigm for collecting EEG signals. Each facial
    emotion image is displayed for 0.5 seconds. There are 10 seconds of black images
    between different types of images as a buffer in the experiment.
  Figure 5 Link: articels_figures_by_rev_year\2023\BrainMachine_Coupled_Learning_Method_for_Facial_Emotion_Recognition\figure_5.jpg
  Figure 5 caption: Learning framework. The proposed method projects each preliminary
    representation to common and private channels. Later, these two hidden representations
    of each domain are concatenated to make the FER.
  Figure 6 Link: articels_figures_by_rev_year\2023\BrainMachine_Coupled_Learning_Method_for_Facial_Emotion_Recognition\figure_6.jpg
  Figure 6 caption: Structure of the proposed method. The network( C(I,E)(cdot) ,
    PI(cdot) , and PE(cdot) ) in each channel is a three-layer fully connected network.
  Figure 7 Link: articels_figures_by_rev_year\2023\BrainMachine_Coupled_Learning_Method_for_Facial_Emotion_Recognition\figure_7.jpg
  Figure 7 caption: Loss curves of the CFAPS (a), CK+48 (b), and JAFFE (c) datasets
    in the training stage.
  Figure 8 Link: articels_figures_by_rev_year\2023\BrainMachine_Coupled_Learning_Method_for_Facial_Emotion_Recognition\figure_8.jpg
  Figure 8 caption: Confusion matrix. Through the coupled learning, the representation's
    performance is closer to the result in the cognitive domain, indicating that the
    proposed method enables the common channel to learn cognitive knowledge.
  Figure 9 Link: articels_figures_by_rev_year\2023\BrainMachine_Coupled_Learning_Method_for_Facial_Emotion_Recognition\figure_9.jpg
  Figure 9 caption: Visualization of each representation with different loss configuration
    in the testing sets using t-SNE projections [103]. Red indicates the representations
    in the visual domain, and blue indicates the representations in the cognitive
    domain. In the (d), Red circles indicate mathbf Ip , and blue circles indicate
    mathbf Ep . Red crosses indicate mathbf Ic , and blue crosses indicate mathbf
    Ec .
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Dongjun Liu
  Name of the last author: Wanzeng Kong
  Number of Figures: 9
  Number of Tables: 15
  Number of authors: 6
  Paper title: Brain-Machine Coupled Learning Method for Facial Emotion Recognition
  Publication Date: *id001
  Table 1 caption:
    table_text: TABLE I Experimental Paradigm Parameters
  Table 10 caption:
    table_text: TABLE X Results on JAFFE Dataset for Generalization Validation. Best
      Results are in Bold. The Second Best Results are Underlined
  Table 2 caption:
    table_text: TABLE II Comparison of the Proposed Method Using CMD, MMD and KL-Divergence.
      Best Results are in Bold
  Table 3 caption:
    table_text: TABLE III Hyper-Parameter Setting of Training in Each Dataset
  Table 4 caption:
    table_text: TABLE IV EEG Data Format
  Table 5 caption:
    table_text: TABLE V Performance Comparison Among Subjects on CFAPS Dataset. Best
      Performance is in Bold
  Table 6 caption:
    table_text: TABLE VI Image Data Format
  Table 7 caption:
    table_text: TABLE VII Performance Comparison on CFAPS Dataset. The Best Results
      are Shown in Bold
  Table 8 caption:
    table_text: TABLE VIII Performance Comparison of Different Features on CFAPS Dataset.
      The Best Results are Shown in Bold and the Second Best Results are Underlined
      for Each Domain
  Table 9 caption:
    table_text: TABLE IX Results on CK+ Dataset for Generalization Validation. Best
      Results are in Bold. The Second Best Results are Underlined
  paper DOI: https://doi.org/10.1109/TPAMI.2023.3257839
