- Affiliation of the first author: department of computing, the hong kong polytechnic
    university, hong kong
  Affiliation of the last author: department of computing, the hong kong polytechnic
    university, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2020\Grid_Anchor_Based_Image_Cropping_A_New_Benchmark_and_An_Efficient_Model\figure_1.jpg
  Figure 1 caption: "The property of non-uniqueness of image cropping. Given a source\
    \ image, many good crops (labeled with \u201C\u2713\u201D) can be obtained under\
    \ different aspect ratios (e.g., 1:1, 4:3, 16:9). Even under the same aspect ratio,\
    \ there are still multiple acceptable crops. Regarding the three crops with 16:9\
    \ aspect ratio, by taking the middle one as the groundtruth, the bottom one (a\
    \ bad crop, labeled with \u201C\xD7\u201D) will have obviously larger IoU (intersection-over-union)\
    \ than the top one but with worse aesthetic quality. This shows that IoU is not\
    \ a reliable metric to evaluate cropping quality."
  Figure 10 Link: articels_figures_by_rev_year\2020\Grid_Anchor_Based_Image_Cropping_A_New_Benchmark_and_An_Efficient_Model\figure_10.jpg
  Figure 10 caption: "Two examples using \u201Crule of thirds\u201D [50] composition."
  Figure 2 Link: articels_figures_by_rev_year\2020\Grid_Anchor_Based_Image_Cropping_A_New_Benchmark_and_An_Efficient_Model\figure_2.jpg
  Figure 2 caption: The local redundancy of image cropping. Small local changes (e.g.,
    shifting andor scaling) on the cropping window of an acceptable crop (the bottom-right
    one) are very likely to output acceptable crops too.
  Figure 3 Link: articels_figures_by_rev_year\2020\Grid_Anchor_Based_Image_Cropping_A_New_Benchmark_and_An_Efficient_Model\figure_3.jpg
  Figure 3 caption: Illustration of the grid anchor based formulation of image cropping.
    M and N are the numbers of bins for grid partition, while m and n define the adopted
    range of anchors for content preservation.
  Figure 4 Link: articels_figures_by_rev_year\2020\Grid_Anchor_Based_Image_Cropping_A_New_Benchmark_and_An_Efficient_Model\figure_4.jpg
  Figure 4 caption: The content preservation of image cropping. The small crop (b)
    misses the two persons, which are the key objects in the original image although
    itself has a good composition. With content preservation constraint, crop (a)
    will be generated to preserve as much useful information as possible.
  Figure 5 Link: articels_figures_by_rev_year\2020\Grid_Anchor_Based_Image_Cropping_A_New_Benchmark_and_An_Efficient_Model\figure_5.jpg
  Figure 5 caption: Interface of the developed annotation toolbox.
  Figure 6 Link: articels_figures_by_rev_year\2020\Grid_Anchor_Based_Image_Cropping_A_New_Benchmark_and_An_Efficient_Model\figure_6.jpg
  Figure 6 caption: Some sample images from the GAICD dataset.
  Figure 7 Link: articels_figures_by_rev_year\2020\Grid_Anchor_Based_Image_Cropping_A_New_Benchmark_and_An_Efficient_Model\figure_7.jpg
  Figure 7 caption: Histograms of the MOS and standard deviation on the GAICD.
  Figure 8 Link: articels_figures_by_rev_year\2020\Grid_Anchor_Based_Image_Cropping_A_New_Benchmark_and_An_Efficient_Model\figure_8.jpg
  Figure 8 caption: One example source image and several of its annotated crops in
    our GAICD. The MOS is marked under each crop.
  Figure 9 Link: articels_figures_by_rev_year\2020\Grid_Anchor_Based_Image_Cropping_A_New_Benchmark_and_An_Efficient_Model\figure_9.jpg
  Figure 9 caption: "The proposed CNN architecture for image cropping model learning.\
    \ It consists of a multi-scale feature extraction module and a carefully designed\
    \ cropping modeling module. Each convolutional block contains several convolution,\
    \ batch normalization and ReLU layers. Symbols \u201C \xD72 \u201D and \u201C\
    \ 2 \u201D represent bilinear upsampling and downsampling, respectively."
  First author gender probability: 0.96
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.55
  Name of the first author: Hui Zeng
  Name of the last author: Lei Zhang
  Number of Figures: 18
  Number of Tables: 6
  Number of authors: 4
  Paper title: 'Grid Anchor Based Image Cropping: A New Benchmark and An Efficient
    Model'
  Publication Date: 2020-09-15 00:00:00
  Table 1 caption:
    table_text: TABLE 1 IoU Scores of Recent Representative Works and the Developed
      Models in This Work on Two Existing Cropping Benchmarks in Comparison With Two
      Simplest Baselines
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Influence of Different Choices of M,N,m,n M,N,m,n on the Statistics
      of MOS
  Table 3 caption:
    table_text: TABLE 3 Image Cropping Performance by Using Different Feature Extraction
      Modules on the Validation Set
  Table 4 caption:
    table_text: TABLE 4 Ablation Experiments on the Joint Use of RoI, RoD, and Global
      Features
  Table 5 caption:
    table_text: TABLE 5 Image Cropping Performance by Using Different Number and Size
      of Kernels in the Cropping Modeling Module
  Table 6 caption:
    table_text: TABLE 6 Quantitative Comparison Between Different Methods on Testing
      set of GAICD
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3024207
- Affiliation of the first author: fuxi ai lab, netease, hangzhou, zhejiang, china
  Affiliation of the last author: fuxi ai lab, netease, hangzhou, zhejiang, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Neural_Rendering_for_Game_Character_AutoCreation\figure_1.jpg
  Figure 1 caption: We propose a novel method for game character auto-creation under
    a self-supervised learning framework by leveraging differential neural rendering.
    The proposed method converts a single input face photo to a large set of physically
    meaningful facial parameters. Users can further fine-tune the parameters optionally
    according to their needs.
  Figure 10 Link: articels_figures_by_rev_year\2020\Neural_Rendering_for_Game_Character_AutoCreation\figure_10.jpg
  Figure 10 caption: "A close look at the game characters generated by our method\
    \ in two games: \u201CJustice\u201D (first two rows) and \u201CHeaven\u201D (last\
    \ two rows). In each group of the image, the left one shows an input photo (after\
    \ alignment) and the right one shows the generated faces. All the example results\
    \ in this figure are generated by the one-shot method and rendered by the imitators."
  Figure 2 Link: articels_figures_by_rev_year\2020\Neural_Rendering_for_Game_Character_AutoCreation\figure_2.jpg
  Figure 2 caption: "An overview of our method. (a) Our model consists of multiple\
    \ components: an imitator G , a facial parameter translator T , and a facial descriptor\
    \ F . The G , as shown in (b), aims to imitate the behavior of a game engine by\
    \ taking in a set of user-customized facial parameters x and producing a \u201C\
    rendered\u201D facial image y . The T is trained to convert an input face photo\
    \ to facial parameters which maximize the facial similarity between the input\
    \ and the \u201Crendering\u201D result in the feature space produced by the F\
    \ . In (c), we show two groups of faces generated by our imitator and their in-game\
    \ ground truth."
  Figure 3 Link: articels_figures_by_rev_year\2020\Neural_Rendering_for_Game_Character_AutoCreation\figure_3.jpg
  Figure 3 caption: "(a) To effectively measure the cross-domain similarity between\
    \ a generated face and a real one, we design three loss functions: 1) a facial\
    \ identity loss L idt , which computes the distance between two images on top\
    \ of the facial embeddings produced by a pre-trained face recognition network\
    \ F recg , as shown in (b); 2) a facial content loss L ctt , which computes the\
    \ pixel-wise similarity based on the facial semantic maps produced by a face segmentation\
    \ network F seg , as shown in (c); 3) a loopback loss L loop , which ensures the\
    \ facial parameter translator T correctly interprets its own output. (d) shows\
    \ the architecture of our translator T . The key to our method is a self-supervised\
    \ learning framework where a \u201Crecursive consistency\u201D is introduced to\
    \ enforce the facial representation of the rendered image I \u2032 to be similar\
    \ with the input I : I \u2032 =G(T(I))\u2248I ."
  Figure 4 Link: articels_figures_by_rev_year\2020\Neural_Rendering_for_Game_Character_AutoCreation\figure_4.jpg
  Figure 4 caption: The architecture of our imitator G(x) . We train the imitator
    to learn a mapping from a group of facial customization parameters x to a rendered
    facial image y produced by the game engine.
  Figure 5 Link: articels_figures_by_rev_year\2020\Neural_Rendering_for_Game_Character_AutoCreation\figure_5.jpg
  Figure 5 caption: "In our iterative generation mode, the game character auto-creation\
    \ can be considered as a searching process on the manifold of the imitator. We\
    \ aim to find an optimal point y \u2217 =G( x \u2217 ) that minimizes the distance\
    \ between y and the reference face photo y r in their feature space."
  Figure 6 Link: articels_figures_by_rev_year\2020\Neural_Rendering_for_Game_Character_AutoCreation\figure_6.jpg
  Figure 6 caption: Human faces are typically embedded in a low-dimensional subspace
    with facial priors. We investigate the importance of the dimension reduction and
    how it affects the rendering results. (a) Reconstruction energy versus Number
    of principal components in the parameter space of CelebA dataset. We divide the
    facial parameters into seven groups and perform dimension reduction accordingly.
    (b)-(c) Randomly generated characters w and wo dimension reduction.
  Figure 7 Link: articels_figures_by_rev_year\2020\Neural_Rendering_for_Game_Character_AutoCreation\figure_7.jpg
  Figure 7 caption: A visual comparison between our method and a well-known monocular
    3D face reconstruction method 3DMM-CNN [2].
  Figure 8 Link: articels_figures_by_rev_year\2020\Neural_Rendering_for_Game_Character_AutoCreation\figure_8.jpg
  Figure 8 caption: "Four examples of the auto-created game characters in the game\
    \ \u201CJustice\u201D by using our method. All the example results in this figure\
    \ are generated by the iterative method."
  Figure 9 Link: articels_figures_by_rev_year\2020\Neural_Rendering_for_Game_Character_AutoCreation\figure_9.jpg
  Figure 9 caption: "A close look at the game characters generated by our method in\
    \ two games: \u201CJustice\u201D (first two rows) and \u201CHeaven\u201D (last\
    \ two rows). In each group of the image, the left one shows an input photo (after\
    \ alignment) and the right one shows the generated faces. All the example results\
    \ in this figure are generated by the iterative method and rendered by the game\
    \ engines."
  First author gender probability: 0.55
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.54
  Name of the first author: Tianyang Shi
  Name of the last author: Yi Yuan
  Number of Figures: 13
  Number of Tables: 9
  Number of authors: 4
  Paper title: Neural Rendering for Game Character Auto-Creation
  Publication Date: 2020-09-15 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 A Comparison Between \u201CStatistical Shape Basis (SSB)\u201D\
      \ and \u201CManually Defined Shape Basis (MDSB)\u201D on 3D Face Representation"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 A Detailed Configuration of Our Imitator G G
  Table 3 caption:
    table_text: TABLE 3 A Detailed Configuration of Our Face Segmentation Model F
      seg Fseg
  Table 4 caption:
    table_text: TABLE 4 A Detailed Configuration of Our Facial Parameter Translator
      T T
  Table 5 caption:
    table_text: "TABLE 5 A Detailed Configuration of the Facial Customization Parameters\
      \ (Continuous Part) in the Game \u201CJustice\u201D"
  Table 6 caption:
    table_text: TABLE 6 Quantitative Performance Comparison of Different Methods on
      Their Accuracy and Speed
  Table 7 caption:
    table_text: 'TABLE 7 Subjective Evaluation: Selection Ratio [51] of Different
      Methods on Two Datasets'
  Table 8 caption:
    table_text: 'TABLE 8 Ablations Studies on Different Technical Components of Our
      Method: 1) the Facial Content Loss L ctt Lctt, 2) the Facial Identity Loss L
      idt Lidt, 3) the Loopback Loss L loop Lloop, and 4) the Residual Attention block
      (ResAtt) in Our Translator'
  Table 9 caption:
    table_text: TABLE 9 Subjective Evaluation Results of Two Technical Components
      of Our Method 1) Facial Identity Loss L idt Lidt, and 2) Facial Content Loss
      L ctt Lctt
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3024009
- Affiliation of the first author: school of electronic engineering, xidian university,
    xian, shaanxi, china
  Affiliation of the last author: department of electrical & computer engineering,
    university of pittsburgh, pittsburgh, pa, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\LargeScale_Nonlinear_AUC_Maximization_via_Triply_Stochastic_Gradients\figure_1.jpg
  Figure 1 caption: Comparisons of TSAM with other state-of-the-art batch learning
    kernel methods at last run.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\LargeScale_Nonlinear_AUC_Maximization_via_Triply_Stochastic_Gradients\figure_2.jpg
  Figure 2 caption: Comparisons of TSAM with other state-of-the-art online learning
    kernel methods at last run.
  Figure 3 Link: articels_figures_by_rev_year\2020\LargeScale_Nonlinear_AUC_Maximization_via_Triply_Stochastic_Gradients\figure_3.jpg
  Figure 3 caption: Comparisons of TSAM with other state-of-the-art batch learning
    kernel methods at last run.
  Figure 4 Link: articels_figures_by_rev_year\2020\LargeScale_Nonlinear_AUC_Maximization_via_Triply_Stochastic_Gradients\figure_4.jpg
  Figure 4 caption: sigma comparisons of TSAM.
  Figure 5 Link: articels_figures_by_rev_year\2020\LargeScale_Nonlinear_AUC_Maximization_via_Triply_Stochastic_Gradients\figure_5.jpg
  Figure 5 caption: ' c comparisons of TSAM.'
  Figure 6 Link: articels_figures_by_rev_year\2020\LargeScale_Nonlinear_AUC_Maximization_via_Triply_Stochastic_Gradients\figure_6.jpg
  Figure 6 caption: ' D comparisons of TSAM.'
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.92
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Zhiyuan Dang
  Name of the last author: Heng Huang
  Number of Figures: 6
  Number of Tables: 8
  Number of authors: 5
  Paper title: Large-Scale Nonlinear AUC Maximization via Triply Stochastic Gradients
  Publication Date: 2020-09-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Notations
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Time Complexity Comparison Over Kernel AUC Maximization
      Methods
  Table 3 caption:
    table_text: TABLE 3 Several Loss Functions and Their Kernel-Based Gradient
  Table 4 caption:
    table_text: "TABLE 4 Average AUC Performance (mean \xB1 \xB1std) With Batch Learning\
      \ Kernel Methods (\u2013means out of Memory or Over 10,000s per run)"
  Table 5 caption:
    table_text: "TABLE 5 Average AUC Performance (mean \xB1 \xB1std) With Online Learning\
      \ Kernel Methods (\u2013means out of memory or over 10,000s per run)"
  Table 6 caption:
    table_text: "TABLE 6 Average AUC Performance (mean \xB1 \xB1std) With Batch Learning\
      \ Kernel Methods on High Dimension Datasets"
  Table 7 caption:
    table_text: TABLE 7 Descriptions of the Large-Scale Datasets Used in Our Experiments
  Table 8 caption:
    table_text: TABLE 8 Details of High-Dimensional Datasets
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3024987
- Affiliation of the first author: department of electrical and computer engineering,
    university of minnesota, minneapolis, mn, usa
  Affiliation of the last author: department of electrical and computer engineering,
    university of minnesota, minneapolis, mn, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Efficient_and_Stable_Graph_Scattering_Transforms_via_Pruning\figure_1.jpg
  Figure 1 caption: Illustration of the same pGST applied to different graph datasets.
    For the social network (a) most GST branches are pruned, suggesting that most
    information is captured by local interactions.
  Figure 10 Link: articels_figures_by_rev_year\2020\Efficient_and_Stable_Graph_Scattering_Transforms_via_Pruning\figure_10.jpg
  Figure 10 caption: The pGST applied to the Cora dataset with and without structural
    perturbations.
  Figure 2 Link: articels_figures_by_rev_year\2020\Efficient_and_Stable_Graph_Scattering_Transforms_via_Pruning\figure_2.jpg
  Figure 2 caption: "Scattering pattern of a pGST with J=3 and L=3 . Dashed lines\
    \ represent pruned branches. An example x and GFTs of filter banks are depicted\
    \ too. The third filter j=3 at \u2113=1 is pruned because it generates no output\
    \ ( z (3) =0 )."
  Figure 3 Link: articels_figures_by_rev_year\2020\Efficient_and_Stable_Graph_Scattering_Transforms_via_Pruning\figure_3.jpg
  Figure 3 caption: Classification accuracy against number of samples in the authorship
    attribution (a) and SNR in dB for source localization (b). The percentage of features
    after prunning retained in the top- 2|T| most important GST features given by
    the SVM classifier (c).
  Figure 4 Link: articels_figures_by_rev_year\2020\Efficient_and_Stable_Graph_Scattering_Transforms_via_Pruning\figure_4.jpg
  Figure 4 caption: Runtime comparison of the scattering transforms for Fig. 3.
  Figure 5 Link: articels_figures_by_rev_year\2020\Efficient_and_Stable_Graph_Scattering_Transforms_via_Pruning\figure_5.jpg
  Figure 5 caption: Performance of pGSTs for varying tau .
  Figure 6 Link: articels_figures_by_rev_year\2020\Efficient_and_Stable_Graph_Scattering_Transforms_via_Pruning\figure_6.jpg
  Figure 6 caption: Classification accuracy over L .
  Figure 7 Link: articels_figures_by_rev_year\2020\Efficient_and_Stable_Graph_Scattering_Transforms_via_Pruning\figure_7.jpg
  Figure 7 caption: Classification accuracy over J .
  Figure 8 Link: articels_figures_by_rev_year\2020\Efficient_and_Stable_Graph_Scattering_Transforms_via_Pruning\figure_8.jpg
  Figure 8 caption: 3D point cloud classification.
  Figure 9 Link: articels_figures_by_rev_year\2020\Efficient_and_Stable_Graph_Scattering_Transforms_via_Pruning\figure_9.jpg
  Figure 9 caption: The pGST applied to the Cora dataset with and without signal perturbation,
    corroborating the result of Corollary 1.
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Vassilis N. Ioannidis
  Name of the last author: Georgios B. Giannakis
  Number of Figures: 12
  Number of Tables: 3
  Number of authors: 3
  Paper title: Efficient and Stable Graph Scattering Transforms via Pruning
  Publication Date: 2020-09-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Dataset Characteristics
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Graph Classification Accuracy
  Table 3 caption:
    table_text: TABLE 3 SSL Classification Accuracy
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3025258
- Affiliation of the first author: department of information engineering and computer
    science, university of trento, trento, italy
  Affiliation of the last author: facebook, zurich, switzerland
  Figure 1 Link: articels_figures_by_rev_year\2020\Disentangling_Monocular_D_Object_Detection_From_Single_to_MultiClass_Recognition\figure_1.jpg
  Figure 1 caption: "Backbone architecture. Rectangles in the \u201CFPN\u201D block\
    \ represent convolutions followed by iABN sync ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Disentangling_Monocular_D_Object_Detection_From_Single_to_MultiClass_Recognition\figure_2.jpg
  Figure 2 caption: 2D detection module. Rectangles represent convolutions. All convolutions
    but the last per row are followed by iABN sync .
  Figure 3 Link: articels_figures_by_rev_year\2020\Disentangling_Monocular_D_Object_Detection_From_Single_to_MultiClass_Recognition\figure_3.jpg
  Figure 3 caption: "3D detection head. \u201CFC\u201D rectangles represent fully\
    \ connected layers. All FCs except the last of each row are followed by iABN."
  Figure 4 Link: articels_figures_by_rev_year\2020\Disentangling_Monocular_D_Object_Detection_From_Single_to_MultiClass_Recognition\figure_4.jpg
  Figure 4 caption: 'Visualization of the semantics of the outputs of the 2D and 3D
    detection heads. Left: 2D bounding box regression on image plane. Center: 3D bounding
    box regression. Right: allocentric angle from bird-eye view.'
  Figure 5 Link: articels_figures_by_rev_year\2020\Disentangling_Monocular_D_Object_Detection_From_Single_to_MultiClass_Recognition\figure_5.jpg
  Figure 5 caption: 'Trajectories of the optimization process for each group of parameters
    (dimensions, rotation quaternion, projected center, depth), when using the entangled
    (magenta) and disentangled (blue) 3D detection losses. Left-to-right: trajectories
    of dimensions, rotation quaternion (last 3 coordinates), projection of the 3D
    bounding box center on the image and depth of the 3D bounding box center. The
    last plot shows the evolution of the entangled Lmathtt 3Dbb loss for both cases.'
  Figure 6 Link: articels_figures_by_rev_year\2020\Disentangling_Monocular_D_Object_Detection_From_Single_to_MultiClass_Recognition\figure_6.jpg
  Figure 6 caption: Qualitative results on nuScenes (top row) and on KITTI3D (bottom
    row).
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.82
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Andrea Simonelli
  Name of the last author: Peter Kontschieder
  Number of Figures: 6
  Number of Tables: 9
  Number of authors: 5
  Paper title: 'Disentangling Monocular 3D Object Detection: From Single to Multi-Class
    Recognition'
  Publication Date: 2020-09-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 AP | R 40 AP|R40 Validation Set Ablation Results on KITTI3D
      Car Class (0.7 IoU Threshold)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 AP | R 40 AP|R40 Test Set SOTA Results on KITTI3D Car Class
      (0.7 IoU Threshold)
  Table 3 caption:
    table_text: TABLE 3 AP | R 40 AP|R40 Test Set SOTA Results on KITTI3D Pedestrian
      and Cyclist Classes (0.5 IoU Threshold)
  Table 4 caption:
    table_text: 'TABLE 4 AP | R 11 AP|R11 Scores on KITTI3D (0.7 IoU): Val Set Ablation
      Results (Top Part), Val Set SOTA Results (Bottom Part)'
  Table 5 caption:
    table_text: TABLE 5 Ablation Results on KITTI3D With 2D Detection Networks, AP
      | R 40 AP|R40 Scores
  Table 6 caption:
    table_text: TABLE 6 AP | R 40 AP|R40 Results on Pedestrian and Cyclist of Our
      Single and Multi-Class Approach on the KITTI3D Validation Set
  Table 7 caption:
    table_text: TABLE 7 Test Set Results Comparison on All the nuScenes Dataset [58]
      Classes
  Table 8 caption:
    table_text: TABLE 8 Performance Comparison for Results on Category Car in nuScenes
      Dataset [58]
  Table 9 caption:
    table_text: TABLE 9 Detailed Results on Category Car, Pedestrian and Cyclist Obtained
      by Our Model on the nuScenes [58] Test Set
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3025077
- Affiliation of the first author: "department of electrical and computer engineering,\
    \ rutgers university\u2013new brunswick, new brunswick, nj, usa"
  Affiliation of the last author: "department of electrical and computer engineering,\
    \ rutgers university\u2013new brunswick, new brunswick, nj, usa"
  Figure 1 Link: articels_figures_by_rev_year\2020\Differential_Viewpoints_for_Ground_Terrain_Material_Recognition\figure_1.jpg
  Figure 1 caption: (Top) Example from GTOS dataset comprising outdoor measurements
    with multiple viewpoints, illumination conditions and angular differential imaging.
    The example shows scene-surfaces imaged at different illuminationweather conditions.
    (Bottom) Texture encoded angular network (TEAN) for ground terrain material recognition.
  Figure 10 Link: articels_figures_by_rev_year\2020\Differential_Viewpoints_for_Ground_Terrain_Material_Recognition\figure_10.jpg
  Figure 10 caption: The sample images for different material classes. For materials
    with distinct shape or color information like painted asphalt, brick, painted
    turf and stone (top), the recognition performance is similar. But for material
    classes where recognition depends on material reflectance and fine-scale texture
    (like cement, dry grass, limestone and sand), angular gradients are an important
    cue and DAIN significantly outperforms MobileNet V2.
  Figure 2 Link: articels_figures_by_rev_year\2020\Differential_Viewpoints_for_Ground_Terrain_Material_Recognition\figure_2.jpg
  Figure 2 caption: "Differential angular imaging. (Top) Examples of material surface\
    \ images I v . (Bottom) Corresponding differential images I \u03B4 = I v \u2212\
    \ I v+\u03B4 in our GTOS dataset. These sparse images encode angular gradients\
    \ of reflection and 3D relief texture."
  Figure 3 Link: articels_figures_by_rev_year\2020\Differential_Viewpoints_for_Ground_Terrain_Material_Recognition\figure_3.jpg
  Figure 3 caption: 'The measurement equipment for the GTOS database: Mobile Robots
    P3-AT robot, Cyton gamma 300 robot arm, Basler aca2040-90uc camera with Edmund
    Optics 25mmF1.8 lens, DGK 18 percent white balance and color reference card, and
    the 440C Stainless Steel Tight-Tolerance Sphere (McMaster-Carr).'
  Figure 4 Link: articels_figures_by_rev_year\2020\Differential_Viewpoints_for_Ground_Terrain_Material_Recognition\figure_4.jpg
  Figure 4 caption: "(a) The 40 material categories in the GTOS dataset introduced\
    \ in this paper. (b) The material surface observation points. Nine viewpoint angles\
    \ (black spots) separated along an arc spanning 80 \u2218 are measured. For each\
    \ viewpoint, a differential view (green spots) is captured \xB1 5 \u2218 in azimuth\
    \ from the original orientation (the sign is chosen based on robotic arm kinematics.)."
  Figure 5 Link: articels_figures_by_rev_year\2020\Differential_Viewpoints_for_Ground_Terrain_Material_Recognition\figure_5.jpg
  Figure 5 caption: "Methods to combine two image streams, the original image I v\
    \ and the differential angular image I \u03B4 = I v+\u03B4 \u2212 I v . The architecture\
    \ in (c) provides better performance than (a) and (b) and we call it the differential\
    \ angular imaging network (DAIN)."
  Figure 6 Link: articels_figures_by_rev_year\2020\Differential_Viewpoints_for_Ground_Terrain_Material_Recognition\figure_6.jpg
  Figure 6 caption: Multiview DAIN. The 3D filter + pooling method to combine two
    streams (original and differential image) from multiple viewing angles v in [v1,v2,ldots,vN]
    . W , H , and D are the width, height, and depth of corresponding feature maps,
    N is the number of view points.
  Figure 7 Link: articels_figures_by_rev_year\2020\Differential_Viewpoints_for_Ground_Terrain_Material_Recognition\figure_7.jpg
  Figure 7 caption: A deep encoding pooling network (DEP) for material recognition.
    Outputs from convolutional layers are fed into the encoding layer and global average
    pooling layer jointly and their outputs are processed with bilinear model.
  Figure 8 Link: articels_figures_by_rev_year\2020\Differential_Viewpoints_for_Ground_Terrain_Material_Recognition\figure_8.jpg
  Figure 8 caption: A texture encoded angular network (TEAN) for material recognition.
    The input to the reflectance branch is the differential angular image, which captures
    material reflectance information via angular gradients. The input to the texture
    branch is the RGB color image, to provide the ordered and orderless spatial information.
    For the texture branch, we utilize DEP to balance the orderless texture component
    and ordered spatial information. The overall architecture of TEAN enables material
    classification using angular reflectance information, orderless texture and ordered
    spatial structure.
  Figure 9 Link: articels_figures_by_rev_year\2020\Differential_Viewpoints_for_Ground_Terrain_Material_Recognition\figure_9.jpg
  Figure 9 caption: The recognition accuracy of fine-tuned MobileNet V2 and DAIN on
    different observation angles. The recognition accuracy of DAIN outperforms fine-tuned
    MobileNet V2 in different observation angles.
  First author gender probability: 0.65
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Jia Xue
  Name of the last author: Kristin J. Dana
  Number of Figures: 11
  Number of Tables: 9
  Number of authors: 4
  Paper title: Differential Viewpoints for Ground Terrain Material Recognition
  Publication Date: 2020-09-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison Between GTOS Dataset and Some Publicly Available
      BRDF Material Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Architecture of the Deep Encoding Pooling Network Based
      on MobileNet V2 [50]
  Table 3 caption:
    table_text: TABLE 3 Results Comparing Performance of Standard CNN Recognition
      Without Angular Differential Imaging (First Three Rows) to Our Single-View DAIN
      (Middle Three Rows) and Our Multi-View DAIN (Bottom Three Rows)
  Table 4 caption:
    table_text: TABLE 4 Comparison of Accuracy From Different Two Stream Methods as
      Shown in Fig. 5
  Table 5 caption:
    table_text: TABLE 5 Comparison With the State of Art Algorithms on GTOS Dataset
  Table 6 caption:
    table_text: TABLE 6 The Recognition Accuracy of MobileNet V2 Based DAIN and Fine-Tuned
      MobileNet V2 on Different Material Classes
  Table 7 caption:
    table_text: TABLE 7 Comparison Our Deep Encoding Pooling Network (DEP) With MobileNet
      V2 (Left) [50], Bilinear CNN (Mid) [55] and Deep-TEN (Right) [9] on GTOS Dataset
  Table 8 caption:
    table_text: TABLE 8 Comparison With State-of-the-Art Algorithms on Describable
      Textures Dataset (DTD) and Materials in Context Database (MINC)
  Table 9 caption:
    table_text: TABLE 9 Results Comparing Performance of CNN Fine-Tune, DEP, DAIN
      and TEAN Based on MobileNet V2 [50]
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3025121
- Affiliation of the first author: department of electrical and computer science,
    massachusetts institute of technology, cambridge, ma, usa
  Affiliation of the last author: facebook reality labs (oculus research), redmond,
    wa, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\EgoCom_A_MultiPerson_MultiModal_Egocentric_Communications_Dataset\figure_1.jpg
  Figure 1 caption: Synchronized egocentric videos from three people in a conversation.
    Arrows depict the egocentric visual perspective, with each unique color corresponding
    to one perspective.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\EgoCom_A_MultiPerson_MultiModal_Egocentric_Communications_Dataset\figure_2.jpg
  Figure 2 caption: Distribution of train, test, and validation sets for the EgoCom
    dataset.
  Figure 3 Link: articels_figures_by_rev_year\2020\EgoCom_A_MultiPerson_MultiModal_Egocentric_Communications_Dataset\figure_3.jpg
  Figure 3 caption: Gender, dialect, and background noise statistics for the EgoCom
    dataset.
  Figure 4 Link: articels_figures_by_rev_year\2020\EgoCom_A_MultiPerson_MultiModal_Egocentric_Communications_Dataset\figure_4.jpg
  Figure 4 caption: "Probability of turn-taking between host and any participants\
    \ in the EgoCom train dataset. \u2217 Participants includes all (usually two)\
    \ participants, e.g., 72 percent is the probability anyparticipant will be speaking\
    \ in 1s given anyparticipant is currently speaking."
  Figure 5 Link: articels_figures_by_rev_year\2020\EgoCom_A_MultiPerson_MultiModal_Egocentric_Communications_Dataset\figure_5.jpg
  Figure 5 caption: Probability of turn-taking transitions between host and any participants
    in the EgoCom test dataset.
  Figure 6 Link: articels_figures_by_rev_year\2020\EgoCom_A_MultiPerson_MultiModal_Egocentric_Communications_Dataset\figure_6.jpg
  Figure 6 caption: An example of an MLP trained with audio+text features where test
    accuracy increases when all synchronous participants features are used, particularly
    for larger past and future. This figure compares Task 3 versus Task 2.
  Figure 7 Link: articels_figures_by_rev_year\2020\EgoCom_A_MultiPerson_MultiModal_Egocentric_Communications_Dataset\figure_7.jpg
  Figure 7 caption: Human and machine (MLP) baseline performances on EgoCom test set
    for Task 1 across modality of input and duration into the future. The prior (speaker
    label at 0 seconds) is included during MLP training because humans also infer
    this prior. Past history window of 5 seconds is used for both. Raw values for
    human performance are shown in Table 8.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Curtis G. Northcutt
  Name of the last author: Richard Newcombe
  Number of Figures: 7
  Number of Tables: 10
  Number of authors: 4
  Paper title: 'EgoCom: A Multi-Person Multi-Modal Egocentric Communications Dataset'
  Publication Date: 2020-09-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Same-Label Prior Probabilities of EgoCom Train and Test Sets
  Table 10 caption:
    table_text: TABLE 10 Global Transcription Accuracy Across Influencers
  Table 2 caption:
    table_text: TABLE 2 (Task 1) Top-1 EgoCom Test Accuracy for Whether Any Person
      Will be Speaking in 1-10 Seconds Given That Persons Features
  Table 3 caption:
    table_text: TABLE 3 (Task 2) Top-1 EgoCom Test Accuracy Predicting Whether the
      Host Will be Speaking in 1-10 Seconds Given the Hosts Features
  Table 4 caption:
    table_text: TABLE 4 (Task 3) Top-1 EgoCom Test Accuracy Predicting Whether the
      Host Will be Speaking in 1-10 Seconds Given the Concatenation of All Participants
      Features
  Table 5 caption:
    table_text: TABLE 5 (Task 4) Top-1 EgoCom Test Accuracy for Predicting Which of
      Person 1 (Host), 2 (Participant), 3 (Participant), or no One (Label 0) Will
      be Speaking
  Table 6 caption:
    table_text: TABLE 6 Ablation Study of Tasks 1 - 4 With Use Prior = False and Past
      = 4s
  Table 7 caption:
    table_text: TABLE 7 Top Likelihood and Posterior Test Accuracy From Tables 2,
      3, 4, and 5
  Table 8 caption:
    table_text: TABLE 8 Average Human Test Accuracy, Standard Deviation, and Cohens
      Kappa Inter-Rater Reliability for Task 1
  Table 9 caption:
    table_text: TABLE 9 Global Transcription Accuracy Across Demographics
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3025105
- Affiliation of the first author: school of data and computer science, sun yat-sen
    university, guangzhou, guangdong, china
  Affiliation of the last author: sensetime reasearch, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2020\Fashion_Retrieval_via_Graph_Reasoning_Networks_on_a_Similarity_Pyramid\figure_1.jpg
  Figure 1 caption: Comparison between global similarity and a similarity pyramid
    with graph reasoning. The left illustrates the global similarity. The right panel
    shows the similarity pyramid with graph reasoning, where scale 1 computes the
    global similarity, and scales 2 and 3 include local similarities between all possible
    combinations of local patches from each image pair. The dashed gray line indicates
    that the similarity is calculated from two image patches. The pyramid similarities
    (including the global and the local) are reasoned mutually. The blue lines indicate
    interactions between similarities within the same scale while the red dashed lines
    indicate those from two different scales (best viewed in color).
  Figure 10 Link: articels_figures_by_rev_year\2020\Fashion_Retrieval_via_Graph_Reasoning_Networks_on_a_Similarity_Pyramid\figure_10.jpg
  Figure 10 caption: Visualization of the predicted adaptive pooling windows of query
    images at different scales. Each row is a pair of query-gallery images. The first
    four rows are positive pairs while the last row negative pair. In each row, the
    first column shows the gallery image while other columns the query images with
    predicted adaptive pooling windows at different scales superposing on them. The
    blue round points are predicted by vertex prediction unit while the red square
    points are fixed for all scales (best viewed in color).
  Figure 2 Link: articels_figures_by_rev_year\2020\Fashion_Retrieval_via_Graph_Reasoning_Networks_on_a_Similarity_Pyramid\figure_2.jpg
  Figure 2 caption: Comparison with state-of-the-art methods on the DeepFashion consumer-to-shop
    dataset [3]. ImgDrop+GoogleNet and Product+GoogleNet are the two best results
    ever reported [15].
  Figure 3 Link: articels_figures_by_rev_year\2020\Fashion_Retrieval_via_Graph_Reasoning_Networks_on_a_Similarity_Pyramid\figure_3.jpg
  Figure 3 caption: The overall framework of the proposed GRNet. Given one query and
    gallery pair, their features extracted by deep convolutional networks are fed
    into Similarity Computation to build a similarity pyramid graph with all region
    pair similarities being the graph nodes. In the Feature Extraction, the local
    features of query are dynamically extracted via the adaptive window pooling based
    on the features of query and gallery while those of gallery are extracted via
    max-pooling. In the Similarity Computation, x i l is the i th local feature of
    the query at scale l while y j l is the j th one of the galleries, and s ij l
    is their similarity vector. Furthermore, the global and local similarities are
    propagated and updated via Similarity Reasoning. It finally outputs whether the
    input image pair belongs to the same clothing.
  Figure 4 Link: articels_figures_by_rev_year\2020\Fashion_Retrieval_via_Graph_Reasoning_Networks_on_a_Similarity_Pyramid\figure_4.jpg
  Figure 4 caption: "Illustration of adaptive windows. The first and second rows show\
    \ examples with the scale of 2\xD72 and 3\xD73 , respectively. The first column\
    \ shows the fixed spatial windows of the gallery. The second column shows the\
    \ fixed spatial windows of the query in [18], while the thirds shows the proposed\
    \ adaptive windows, which are adjusted based on the content of the gallery."
  Figure 5 Link: articels_figures_by_rev_year\2020\Fashion_Retrieval_via_Graph_Reasoning_Networks_on_a_Similarity_Pyramid\figure_5.jpg
  Figure 5 caption: "Vertices which need to be predicted or are fixed for different\
    \ scales. The blue points are the spatial window vertices on the feature map X\
    \ of the query which need to be predicted and thus can be adjusted adaptively.\
    \ We predict 5, 5, 5, 12, 12, and 12 vertices for scale 1\xD72 , 2\xD71 , 2\xD7\
    2 , 1\xD73 , 3\xD71 , and 3\xD73 respectively. The red points indicate four corners\
    \ of the query which are fixed for all scales."
  Figure 6 Link: articels_figures_by_rev_year\2020\Fashion_Retrieval_via_Graph_Reasoning_Networks_on_a_Similarity_Pyramid\figure_6.jpg
  Figure 6 caption: Architecture of the adaptive window pooling module.
  Figure 7 Link: articels_figures_by_rev_year\2020\Fashion_Retrieval_via_Graph_Reasoning_Networks_on_a_Similarity_Pyramid\figure_7.jpg
  Figure 7 caption: Some examples of query images in the setting of hard-view (a),
    hard-cropping (b) and hard-occlusion (c), respectively.
  Figure 8 Link: articels_figures_by_rev_year\2020\Fashion_Retrieval_via_Graph_Reasoning_Networks_on_a_Similarity_Pyramid\figure_8.jpg
  Figure 8 caption: Visualization of important regions in the query and the gallery
    images. Each 2times 2 images in one rectangle show one query-gallery image pair
    and their corresponding highlights, in which the top-left, the top-right, the
    bottom-left, and the bottom-right are the query, the query highlights, the gallery,
    and the gallery highlights respectively. Query 1 and 3 are occluded by hands;
    query 2 is occluded by trousers; query 4 is side view while its gallery front;
    query 5 is cropped.
  Figure 9 Link: articels_figures_by_rev_year\2020\Fashion_Retrieval_via_Graph_Reasoning_Networks_on_a_Similarity_Pyramid\figure_9.jpg
  Figure 9 caption: Examples of the up-weighted nodes in our similarity pyramid graph.
    Each node represents one similarity of the local patch (indicated by red rectangles)
    pair from the query (the top row) and the gallery (the bottom row). Each 2times
    2 images in one black rectangle show one query-gallery image pair and their up-weighted
    local patch pairs, where the left column shows the most important node before
    the similarity reasoning and the right shows it after the similarity propagation.
    GRNet can up-weight the similarity between aligned salient clothing components
    (e.g., logo) after graph reasoning.
  First author gender probability: 0.95
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yiming Gao
  Name of the last author: Wayne Zhang
  Number of Figures: 13
  Number of Tables: 13
  Number of authors: 7
  Paper title: Fashion Retrieval via Graph Reasoning Networks on a Similarity Pyramid
  Publication Date: 2020-09-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Customer-to-Shop Clothing Retrieval Datasets
  Table 10 caption:
    table_text: TABLE 10 Comparison Between GRNets Trained on Different Training Subsets
      of FindFashion in Terms of Top-1 and Top-20 Accuracy
  Table 2 caption:
    table_text: TABLE 2 Architecture of the Vertex Prediction Unit
  Table 3 caption:
    table_text: TABLE 3 Statistics of Query Pairs of Four Evaluation Setups on FindFashion
  Table 4 caption:
    table_text: TABLE 4 Comparison With State-of-the-Art Methods on DeepFashion Consumer-to-Shop
      Benchmark [3]
  Table 5 caption:
    table_text: TABLE 5 Comparison With State-of-the-Art Methods on Street2Shop [2]
      in Terms of Top-20 Accuracy
  Table 6 caption:
    table_text: TABLE 6 Comparison With State-of-the-Art Fashion Retrieval Methods
      on the DeepFashion2 [17]
  Table 7 caption:
    table_text: TABLE 7 Comparison With State-of-the-Art Methods on FindFashion
  Table 8 caption:
    table_text: TABLE 8 Comparison With State-of-the-Art Methods on FindFashion-Ext
  Table 9 caption:
    table_text: TABLE 9 Ablation Experiments of GRNet on DeepFashion [3]
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3025062
- Affiliation of the first author: school of electronic and information engineering,
    south china university of technology, guangzhou, guangdong, p.r. china
  Affiliation of the last author: faculty of engineering, school of computer science,
    university of sydney, darlington, nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2020\MultiTask_Learning_With_Coarse_Priors_for_Robust_PartAware_Person_ReIdentificati\figure_1.jpg
  Figure 1 caption: The spatial misalignment of body parts across images is common
    in pedestrian images. (a) Misalignment caused by errors in pedestrian detection.
    Both the position and scale of the body parts change, as indicated by the blue
    rectangles. (b) Misalignment due to the movement of flexible body parts, such
    as arms and legs. Both the position and shape of these body parts vary, as indicated
    by the green rectangles. (Best viewed in color.)
  Figure 10 Link: articels_figures_by_rev_year\2020\MultiTask_Learning_With_Coarse_Priors_for_Robust_PartAware_Person_ReIdentificati\figure_10.jpg
  Figure 10 caption: Performance comparison between the hard and soft parameter sharing
    strategies. The horizontal axis stands for the weight of the L2 loss. The red
    and blue dashed lines represent the Rank-1 accuracy and mAP via hard parameter
    sharing, respectively. The solid lines denote the performance of soft parameter
    sharing. (a) Experiments on the first 1times 1 Conv layers of MTs and ATs. (b)
    Experiments on the second 1times 1 Conv layers of MTs and ATs. (Best viewed in
    color.)
  Figure 2 Link: articels_figures_by_rev_year\2020\MultiTask_Learning_With_Coarse_Priors_for_Robust_PartAware_Person_ReIdentificati\figure_2.jpg
  Figure 2 caption: Examples of per-channel responses (heatmaps) on the human body.
    The first column presents the original images. Each of the other five columns
    represents responses on one representative channel, respectively. The channels
    are selected from the last convolutional layer of the ResNet-50 model. Red denotes
    stronger activation. The figure illustrates that there are correspondences between
    each body part and different channels.
  Figure 3 Link: articels_figures_by_rev_year\2020\MultiTask_Learning_With_Coarse_Priors_for_Robust_PartAware_Person_ReIdentificati\figure_3.jpg
  Figure 3 caption: 'Model architecture of MPN in the training stage. Based on the
    ResNet-50 backbone model, MPN builds two tasks for each of the K body parts: one
    main task (MT) and one auxiliary task (AT). For simplicity, only one MT-AT pair
    is shown in this figure. The K ATs are equipped with the coarse prior of body
    part locations for the training images, which provides inductive bias assisting
    the MTs to select and combine the relevant channels for each body part. Inductive
    bias is transferred via two key operations: parameter and feature space alignments
    between each MT-AT pair. The selected part-relevant channels are processed to
    obtain part-level representations. In the inference stage, all ATs are removed
    to leave only the MTs to extract image representations.'
  Figure 4 Link: articels_figures_by_rev_year\2020\MultiTask_Learning_With_Coarse_Priors_for_Robust_PartAware_Person_ReIdentificati\figure_4.jpg
  Figure 4 caption: The three images in each group show the pedestrian image, human
    parsing result by [44], and human segmentation result by [45], respectively. (a)
    The tool in [44] may ignore discriminative accessories, e.g., a backpack. (b)
    It may also neglect undefined body parts, e.g., the neck. (c-d) Each tool may
    fail if the image quality is low. (e) A situation in which both tools have failed
    to work. (f) Segmentation results when there is a severe part missing problem.
  Figure 5 Link: articels_figures_by_rev_year\2020\MultiTask_Learning_With_Coarse_Priors_for_Robust_PartAware_Person_ReIdentificati\figure_5.jpg
  Figure 5 caption: "(a) The pipeline to obtain the coarse prior of body part locations.\
    \ The uniform division between the upper and lower boundaries of the obtained\
    \ mask indicates the coarse location of each body part. (b) The pipeline to obtain\
    \ part-specific feature maps P k (1\u2264k\u2264K) as the input of ATs. This process\
    \ is applied to each channel of F , respectively."
  Figure 6 Link: articels_figures_by_rev_year\2020\MultiTask_Learning_With_Coarse_Priors_for_Robust_PartAware_Person_ReIdentificati\figure_6.jpg
  Figure 6 caption: Structure of the adopted channel attention (CA) module [47]. The
    items in each bracket denote the number of filters, kernel size, and stride, respectively.
    Each Conv layer is followed by a BN layer by default.
  Figure 7 Link: articels_figures_by_rev_year\2020\MultiTask_Learning_With_Coarse_Priors_for_Robust_PartAware_Person_ReIdentificati\figure_7.jpg
  Figure 7 caption: Evaluation on the value of hyper-parameter K for the performance
    of MPN.
  Figure 8 Link: articels_figures_by_rev_year\2020\MultiTask_Learning_With_Coarse_Priors_for_Robust_PartAware_Person_ReIdentificati\figure_8.jpg
  Figure 8 caption: "Visualization of attention maps for each of the K classifiers\
    \ prediction using Grad-CAM. Three representative models in Table 1 are compared:\
    \ (a) baseline model; (b) na\xEFve MTL; (c) MPN. (Best viewed in color.)"
  Figure 9 Link: articels_figures_by_rev_year\2020\MultiTask_Learning_With_Coarse_Priors_for_Robust_PartAware_Person_ReIdentificati\figure_9.jpg
  Figure 9 caption: Visualization of attention maps for each MTs classifier of MPN
    using Grad-CAM. The attention maps of MPN are semantically consistent across images
    in the face of various challenges, e.g., errors in pedestrian detection (a, b),
    dramatic pose variations (c, e), background clutter and occlusion (d). (Best viewed
    in color.).
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Changxing Ding
  Name of the last author: Dacheng Tao
  Number of Figures: 10
  Number of Tables: 9
  Number of authors: 4
  Paper title: Multi-Task Learning With Coarse Priors for Robust Part-Aware Person
    Re-Identification
  Publication Date: 2020-09-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Ablation Study on Each Component of MPN
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance Comparison of Different Types of Prior for Body
      Part Locations in the Training Stage
  Table 3 caption:
    table_text: TABLE 3 Performance Comparison With Variants for FSA (without PSA)
  Table 4 caption:
    table_text: TABLE 4 Performance Comparison With Variants for FSA (with PSA)
  Table 5 caption:
    table_text: TABLE 5 Performance Comparisons on Market-1501
  Table 6 caption:
    table_text: TABLE 6 Performance Comparisons on DukeMTMC-ReID
  Table 7 caption:
    table_text: TABLE 7 Performance Comparisons on CUHK03
  Table 8 caption:
    table_text: TABLE 8 Performance Comparisons on MSMT17
  Table 9 caption:
    table_text: TABLE 9 Comparisons of Model Complexity
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3024900
- Affiliation of the first author: department of computer science and artificial intelligence,
    university of granada, granada, spain
  Affiliation of the last author: department of electrical engineering and computer
    science, northwestern university, evanston, il, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Scalable_Variational_Gaussian_Processes_for_Crowdsourcing_Glitch_Detection_in_LI\figure_1.jpg
  Figure 1 caption: Two examples of glitches observed by the LIGO detector. The fifteen
    types considered in this work will be carefully described in Section 3, see also
    Fig. 3.
  Figure 10 Link: articels_figures_by_rev_year\2020\Scalable_Variational_Gaussian_Processes_for_Crowdsourcing_Glitch_Detection_in_LI\figure_10.jpg
  Figure 10 caption: Elapsed training time as a function of the training set size
    N in the binary LIGO data set. The mean over five independent runs is shown. Above,
    a standard linear scale is used for the x -axis. Notice that SVGPCR exhibits a
    significantly better scalability than classical probabilistic methods, which is
    due to its factorization in mini-batches. Moreover, among previous approaches,
    we can distinguish between those that have already exploded for N=25000 (Yan,
    Rodr14, VGPCR, VFF), and those which have not yet for the full set size (Raykar,
    RFF). In order to better appreciate the differences, a logarithmic scale is used
    for the x -axis in the figure below. This further shows that Rodr14 shoots up
    as early as N=1000 , Yan, VGPCR and VFF do it around N=10000 , and Raykar and
    RFF are starting beyond N=105 .
  Figure 2 Link: articels_figures_by_rev_year\2020\Scalable_Variational_Gaussian_Processes_for_Crowdsourcing_Glitch_Detection_in_LI\figure_2.jpg
  Figure 2 caption: Probabilistic graphical model for SVGPCR. Observed variables are
    depicted in yellow, and those to be estimated in blue. In the latter case, the
    intensity indicates whether the estimation is through a posterior distribution
    (light blue) or a point value (dark).
  Figure 3 Link: articels_figures_by_rev_year\2020\Scalable_Variational_Gaussian_Processes_for_Crowdsourcing_Glitch_Detection_in_LI\figure_3.jpg
  Figure 3 caption: Representative spectrograms for the 15 different types of glitches
    considered in this work. Hanford and Livinsgton refer to the two observatories
    that LIGO comprises, and ER10O1 to two different observation runs. A brief description
    of each glitch is provided in the text. The goal of the GravitySpy project is
    to learn a machine learning system to automatically classify these glitches. The
    labels for the training set are obtained through crowdsourcing.
  Figure 4 Link: articels_figures_by_rev_year\2020\Scalable_Variational_Gaussian_Processes_for_Crowdsourcing_Glitch_Detection_in_LI\figure_4.jpg
  Figure 4 caption: 'Estimation of the expertise degree of the simulated annotators
    in the MNIST problem by SVGPCR. Upper row: true confusion matrices. Lower row:
    mean of the estimated distribution for the confusion matrices. Notice that the
    proposed method perfectly identifies adversarial (fifth) and spammer (fourth)
    annotators. Moreover, not only the structure of the matrices is well identified,
    but also the actual values (the intensity of color is very similar).'
  Figure 5 Link: articels_figures_by_rev_year\2020\Scalable_Variational_Gaussian_Processes_for_Crowdsourcing_Glitch_Detection_in_LI\figure_5.jpg
  Figure 5 caption: 'Upper row: four (out of the 20) examples for which SVGPCR is
    not able to reconstruct the real underlying class in the MNIST problem. Lower
    row: the corresponding probabilities assigned by SVGPCR. In all cases, the proposed
    method assigns the second highest probability to the real class. Notice that some
    of these examples are not easy, and have some features which might lead to confusion.'
  Figure 6 Link: articels_figures_by_rev_year\2020\Scalable_Variational_Gaussian_Processes_for_Crowdsourcing_Glitch_Detection_in_LI\figure_6.jpg
  Figure 6 caption: Some of the inducing point locations learned by SVGPCR in the
    MNIST problem. They have been arranged by columns based on their classification.
    Notice that, for each digit (column), different representative patterns are learned
    in terms of shape, orientation and thickness.
  Figure 7 Link: articels_figures_by_rev_year\2020\Scalable_Variational_Gaussian_Processes_for_Crowdsourcing_Glitch_Detection_in_LI\figure_7.jpg
  Figure 7 caption: Influence of M (number of inducing points) on the test performance
    (accuracy and likelihood) and computational cost (elapsed time at training and
    testing) for SVGPCR in the MNIST problem. As theoretically expected, more inducing
    points lead to better test performance at the expense of a higher computational
    cost.
  Figure 8 Link: articels_figures_by_rev_year\2020\Scalable_Variational_Gaussian_Processes_for_Crowdsourcing_Glitch_Detection_in_LI\figure_8.jpg
  Figure 8 caption: Evolution of test accuracy as a function of the training time
    for different values of M in the MNIST problem. If there is no limit on the available
    training time, then high values of M must be selected (as long as it allows for
    the kernel matrix inversion). However, lower values would be more appropriate
    for a fast training, since the amount of parameters to be trained significantly
    reduces. Moreover, when a certain (problem-dependent) M has been reached, there
    is no a significant benefit by increasing it (observe the difference from M=250
    to M=500 ).
  Figure 9 Link: articels_figures_by_rev_year\2020\Scalable_Variational_Gaussian_Processes_for_Crowdsourcing_Glitch_Detection_in_LI\figure_9.jpg
  Figure 9 caption: Comparison between CPU and GPU implementations of SVGPCR in terms
    of computational cost for the MNIST problem. At training, the time depends on
    the minibatch size, since a greater minibatch implies computations with larger
    matrices and less memory copies, which benefits the GPU. With a minibatch of size
    500 (the default used in this section), the GPU is around two times faster. These
    values are per epoch. At testing (production time), the GPU is over three times
    faster, and logically does not depend on the minibatch size. In this case, the
    shown values are for the whole test set.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "Pablo Morales-\xC1lvarez"
  Name of the last author: Aggelos K. Katsaggelos
  Number of Figures: 15
  Number of Tables: 7
  Number of authors: 5
  Paper title: 'Scalable Variational Gaussian Processes for Crowdsourcing: Glitch
    Detection in LIGO'
  Publication Date: 2020-09-21 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Specifying the Dependence of the ELBO on the Variational Parameters,
      the Kernel Hyperparameters, and the Inducing Locations Through its Five Terms
      in Eq. (15)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Per-Class and Global Test Performance of SVGPCR and SVGP-Gold
      in the MNIST Problem. In Spite of the Corrupted Labels, the Proposed Method
      Almost Recovers the Golden Results
  Table 3 caption:
    table_text: TABLE 3 Per-Class and Global Performance of SVGPCR to Reconstruct
      the Underlying True Label for Training Instances in the MNIST Problem. An Excellent
      Result is Obtained Across All the Classes, With Only 20 (Out of the 60000 Training
      Examples) Not Correctly Predicted
  Table 4 caption:
    table_text: TABLE 4 Per-Class and Global Test Accuracy for the Compared Methods
      in the LIGO Experiment. Mean and Standard Deviation Over Ten Runs are Shown.
      Globally, Svgpcr With Enough Inducing Points Outperforms Dl-Based Methods by
      2 Percent. In Per-Class Results, Notice the Regularity of Svgpcr, Which Performs
      Well Across All Classes Without Standing Out in Many of Them
  Table 5 caption:
    table_text: TABLE 5 Per-Class and Global Test Likelihood for the Compared Methods
      in the LIGO Experiment. Mean and Standard Deviation Over Ten Independent Runs
      are Shown. Globally, Svgpcr With Enough Inducing Points Outperforms Dl-Based
      Methods by Almost 3 Percent. It Also Exhibits a Desirable Regularity Across
      the Different Classes. Moreover, Notice That, Compared to the Accuracy in Table
      4, There Exists Here a Greater Advantage Against Methods That Do Not Quantify
      Uncertainty (i.e., CL-Based Ones)
  Table 6 caption:
    table_text: TABLE 6 Per-Class and Global Test Accuracy for Recent Inference Methods
      on GPs. Mean and Standard Deviation Over Ten Runs are Shown. The Best Performing
      Svgpcr Method is Shown for Comparison. Both NF and GP-Net Slightly Outperform
      Svgpcr. Moreover, Unlike Inducing Points Based Ones, GP-Net Achieves Very Competitive
      Results for M=10 M=10, as Theoretically Expected
  Table 7 caption:
    table_text: TABLE 7 Per-Class and Global Test Likelihood for Recent Inference
      Methods on GPs. Mean and Standard Deviation Over Ten Runs are Shown. The Best
      Performing Svgpcr Method is Shown for Comparison. Both NF And GP-Net Slightly
      Outperform Svgpcr. Moreover, Unlike Inducing Points Based Ones, GP-Net Achieves
      Very Competitive Results for M=10 M=10, as Theoretically Expected
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3025390
