- Affiliation of the first author: czech academy of sciences, institute of information
    theory and automation, praha 8, czech republic
  Affiliation of the last author: czech academy of sciences, institute of information
    theory and automation, praha 8, czech republic
  Figure 1 Link: articels_figures_by_rev_year\2020\BlurInvariant_Similarity_Measurement_of_Images\figure_1.jpg
  Figure 1 caption: The Gopalans distance d G (f, g k ) (solid line) and d G (w, g
    k ) (dash line) as a function of k . According to [3], all solid lines should
    be constant zero.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\BlurInvariant_Similarity_Measurement_of_Images\figure_2.jpg
  Figure 2 caption: "Illustration of two between-image distance measures: Gopalans\
    \ \u201Csubspace to subspace\u201D d G (f,g) and Vageeswarans and L\xE9bls \u201C\
    image to a convex set\u201D I(f,g) . Only the latter one is truly invariant to\
    \ blur."
  Figure 3 Link: articels_figures_by_rev_year\2020\BlurInvariant_Similarity_Measurement_of_Images\figure_3.jpg
  Figure 3 caption: Examples of faces recognized correctly by the projection method
    but misclassified by the Gopalans method.
  Figure 4 Link: articels_figures_by_rev_year\2020\BlurInvariant_Similarity_Measurement_of_Images\figure_4.jpg
  Figure 4 caption: Examples of original and blurred ImageNet pictures recognized
    by both methods (left), by the projection method only (middle) and an image not
    recognized by any method (right).
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.91
  Name of the first author: "Mat\u011Bj L\xE9bl"
  Name of the last author: Jan Flusser
  Number of Figures: 4
  Number of Tables: 3
  Number of authors: 3
  Paper title: Blur-Invariant Similarity Measurement of Images
  Publication Date: 2020-11-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Face Image Recognition
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Animal Image Recognition
  Table 3 caption:
    table_text: TABLE 3 Time in Seconds Needed for a Single Image-to-Image Comparison
      for Gopalans Method (G) and the Projection Algorithm (P) as a Function of the
      Image Size
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3036630
- Affiliation of the first author: computer science and artificial intelligence lab,
    massachusetts institute of technology, cambridge, ma, usa
  Affiliation of the last author: department of computer science, school of engineering,
    the chinese university of hong kong, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2020\Homomorphic_Interpolation_Network_for_Unpaired_ImagetoImage_Translation\figure_1.jpg
  Figure 1 caption: "Rendering intermediate states between (a) \u201Copen-mouth\u201D\
    \ domain and (e) \u201Cclose-mouth\u201D domain. The first-row results are generated\
    \ by StarGAN [4]. Rendering intermediate states is achieved by altering the input\
    \ domain label continuously. (c) and (d) show that abrupt change of expression\
    \ exists. Our results in the second row model intermediate regions and have smooth\
    \ translation effect."
  Figure 10 Link: articels_figures_by_rev_year\2020\Homomorphic_Interpolation_Network_for_Unpaired_ImagetoImage_Translation\figure_10.jpg
  Figure 10 caption: Multi-domain image-to-image translation on RaFD [40].
  Figure 2 Link: articels_figures_by_rev_year\2020\Homomorphic_Interpolation_Network_for_Unpaired_ImagetoImage_Translation\figure_2.jpg
  Figure 2 caption: "Illustration of latent space interpolation along different paths.\
    \ Paths 1 and 2 connect (a) \u201Cnon-smiling male\u201D and (b) \u201C smiling\
    \ female\u201D. They change facial attributes in different orders \u2013 i.e.,\
    \ path 1 changes the smiling expression first while path 2 interpolates gender.\
    \ They naturally serve the multi-domain image-to-image translation task where\
    \ path 1(i) and 2(i) form translation between smiling-to-nonsmiling domains and\
    \ male-to-female domains respectively. Path 3(i) synthesizes smile different from\
    \ path 1(i). Thus, using different target-domain samples, our method can produce\
    \ results required for each domain, termed as multi-modal image-to-image translation.\
    \ Image sequence of the last row illustrates the continuous change of path 3(i)."
  Figure 3 Link: articels_figures_by_rev_year\2020\Homomorphic_Interpolation_Network_for_Unpaired_ImagetoImage_Translation\figure_3.jpg
  Figure 3 caption: Pipeline of the proposed homomorphic interpolation framework.
    (a-c) are the encoder, interpolator and decoder respectively. Given input image
    x i and reference image x j , the encoder E maps them to interpolatable latent
    feature F i and F j . Then the interpolator I performs interpolation between F
    i and F j . Finally, the decoder D inverts the interpolated feature back to the
    image space.
  Figure 4 Link: articels_figures_by_rev_year\2020\Homomorphic_Interpolation_Network_for_Unpaired_ImagetoImage_Translation\figure_4.jpg
  Figure 4 caption: "Illustration of the interpolator. Each conv net branch refers\
    \ to T (i) (\u22C5) , and their weighted sum forms the interpolator."
  Figure 5 Link: articels_figures_by_rev_year\2020\Homomorphic_Interpolation_Network_for_Unpaired_ImagetoImage_Translation\figure_5.jpg
  Figure 5 caption: "Illustration of the homomorphic loss. The blue and yellow curves\
    \ are the interpolation path of the feature space and attribute space respectively.\
    \ The 4 cuboids (upper blue curve) and the 4 attribute vectors (lower yellow curve)\
    \ illustrate sample points in the interpolation paths. Images above the cuboids\
    \ visualize the decoding results of the features. To compute the homomorphic loss,\
    \ we first use the attribute classifier A \u2032 (\u22C5) to map the features\
    \ to the attribute space. The predicted attributes are illustrated as the green\
    \ curve. Finally, the homomorphic loss can be computed by measuring the gap between\
    \ the A \u2032 ( I v ( F i , F j )) (green curve) and I \u2032 v ( z i , z j )\
    \ (yellow curve)."
  Figure 6 Link: articels_figures_by_rev_year\2020\Homomorphic_Interpolation_Network_for_Unpaired_ImagetoImage_Translation\figure_6.jpg
  Figure 6 caption: Effectiveness of each term of the loss functions. (a) is the reference
    image. (b) is the original image. (c-g) are the results without each term. (h)
    is our final result.
  Figure 7 Link: articels_figures_by_rev_year\2020\Homomorphic_Interpolation_Network_for_Unpaired_ImagetoImage_Translation\figure_7.jpg
  Figure 7 caption: "Quantitative results of each terms of our model. The x - and\
    \ Y -axis are the translation accuracy (the larger the better) and FID score (the\
    \ smaller the better). \u201C wo L GA N D 1 \u201D, \u201C wo L style \u201D,\
    \ \u201C wo L GA N D 2 \u201D, \u201C wo L KG \u201D, and \u201C wo L Hom \u201D\
    \ are results by removing L GA N D 1 (Eq. (2)), L style (Eq. (4)), L GA N D 2\
    \ (Eq. (5)) L KG (Eq. (6)), and L Hom (Eq. (11)) respectively. Each curve is plotted\
    \ with different edit strength."
  Figure 8 Link: articels_figures_by_rev_year\2020\Homomorphic_Interpolation_Network_for_Unpaired_ImagetoImage_Translation\figure_8.jpg
  Figure 8 caption: Illustration of the roles of control vector and exemplar. (a-e)
    and (f-j) are the results of CelebA-HQ and RaFD datasets respectively. (a) and
    (f) are the reference images. (b) and (g) are the original images. (c-e) and (h-j)
    are results of setting different control vectors boldsymbolv . Rows 1 and 2 are
    results of using different reference images.
  Figure 9 Link: articels_figures_by_rev_year\2020\Homomorphic_Interpolation_Network_for_Unpaired_ImagetoImage_Translation\figure_9.jpg
  Figure 9 caption: Multi-domain image-to-image translation on CelebA-HQ [22].
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.91
  Name of the first author: Ying-Cong Chen
  Name of the last author: Jiaya Jia
  Number of Figures: 13
  Number of Tables: 7
  Number of authors: 2
  Paper title: Homomorphic Interpolation Network for Unpaired Image-to-Image Translation
  Publication Date: 2020-11-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Architecture of the Homomorphic Interpolation Network
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Grouped Attributes of CelebA-HQ [22]
  Table 3 caption:
    table_text: TABLE 3 Grouped Attributes of RaFD [40]
  Table 4 caption:
    table_text: TABLE 4 Turing Test on CelebA-HQ [22] Dataset
  Table 5 caption:
    table_text: TABLE 5 AB Test on CelebA-HQ [22] Dataset
  Table 6 caption:
    table_text: TABLE 6 Applicability Analysis of Our Model
  Table 7 caption:
    table_text: TABLE 7 The Running Time, GFLOPS, and Model Size of Our Model
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3036543
- Affiliation of the first author: center for machine vision and signal analysis,
    university of oulu, oulu, finland
  Affiliation of the last author: center for machine vision and signal analysis, university
    of oulu, oulu, finland
  Figure 1 Link: articels_figures_by_rev_year\2020\NASFAS_StaticDynamic_Central_Difference_Network_Search_for_Face_AntiSpoofing\figure_1.jpg
  Figure 1 caption: Compared with generic object classification task, the challenge
    of using neural architecture search for the FAS task derives from domain shift
    and unseen spoof attacks.
  Figure 10 Link: articels_figures_by_rev_year\2020\NASFAS_StaticDynamic_Central_Difference_Network_Search_for_Face_AntiSpoofing\figure_10.jpg
  Figure 10 caption: Ablation study of search space components. (a) Baseline search
    space. (b) FAS search space. The meaning of the abbreviations can be referred
    to Section 4.
  Figure 2 Link: articels_figures_by_rev_year\2020\NASFAS_StaticDynamic_Central_Difference_Network_Search_for_Face_AntiSpoofing\figure_2.jpg
  Figure 2 caption: Feature responses of vanilla convolution (VanillaConv) and CDC
    for spoofing faces in shifted domains (illumination & input camera). Consistent
    spoofing pattern (e.g., lattice artifacts) is observed using the CDC.
  Figure 3 Link: articels_figures_by_rev_year\2020\NASFAS_StaticDynamic_Central_Difference_Network_Search_for_Face_AntiSpoofing\figure_3.jpg
  Figure 3 caption: Central difference convolution. If the Conv is replaced by average
    pooling in the aggregation stage, it turns into central difference pooling.
  Figure 4 Link: articels_figures_by_rev_year\2020\NASFAS_StaticDynamic_Central_Difference_Network_Search_for_Face_AntiSpoofing\figure_4.jpg
  Figure 4 caption: Visualization of the static, dynamic and static-dynamic live,
    and spoofing faces.
  Figure 5 Link: articels_figures_by_rev_year\2020\NASFAS_StaticDynamic_Central_Difference_Network_Search_for_Face_AntiSpoofing\figure_5.jpg
  Figure 5 caption: Baseline search space. There are 9 layers to be searched in the
    network space, including six normal cells and three reduction cells. A cell contains
    7 nodes, including two input nodes, four intermediate nodes B1, B2, B3, B4 and
    an output node. The edge between two nodes (except the output node) denotes a
    operation, which is chosen from the vanilla or the CD operation space.
  Figure 6 Link: articels_figures_by_rev_year\2020\NASFAS_StaticDynamic_Central_Difference_Network_Search_for_Face_AntiSpoofing\figure_6.jpg
  Figure 6 caption: FAS search space. There are three cells (one low-level, one mid-level
    and one high-level) to be searched.
  Figure 7 Link: articels_figures_by_rev_year\2020\NASFAS_StaticDynamic_Central_Difference_Network_Search_for_Face_AntiSpoofing\figure_7.jpg
  Figure 7 caption: 'Neural searching on the datasets with multiple domains or attack
    types. We use DARTS based NAS for example. The A, B and C denote the data from
    three respective shifted domains or unseen attack types, which can be easily extended
    to larger ( > 3) cases in practice. (a) NAS: randomly sample the support and query
    sets from entire data, and then search with bi-level optimization strategy. (b)
    DT-NAS: first divide the support set and query set according to the prior knowledge
    of domains or attack types, and then search. (c) DT-Meta-NAS: first meta-train
    the network weights with domaintype-shifted knowledge, and then update the architecture.'
  Figure 8 Link: articels_figures_by_rev_year\2020\NASFAS_StaticDynamic_Central_Difference_Network_Search_for_Face_AntiSpoofing\figure_8.jpg
  Figure 8 caption: Samples of the CASIA-SURF 3DMask dataset. The left four columns
    are indoor while the right two ones are outdoor scenes.
  Figure 9 Link: articels_figures_by_rev_year\2020\NASFAS_StaticDynamic_Central_Difference_Network_Search_for_Face_AntiSpoofing\figure_9.jpg
  Figure 9 caption: Ablation study of CDC, CDP and static-dynamic representation.
    (a) Impact of theta and lambda in CDN. (b) Impact of dynamic representation. (c)
    Comparison among various convolutions and pooling. SD is short for static-dynamic
    representation. Lower ACER indicates better performance.
  First author gender probability: 0.53
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.67
  Name of the first author: Zitong Yu
  Name of the last author: Guoying Zhao
  Number of Figures: 14
  Number of Tables: 8
  Number of authors: 6
  Paper title: 'NAS-FAS: Static-Dynamic Central Difference Network Search for Face
    Anti-Spoofing'
  Publication Date: 2020-11-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Architectures of DepthNet and CDN
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Ablation Study of the Searched Architectures on Different
      Tasks
  Table 3 caption:
    table_text: TABLE 3 The Results of Intra Testing on OULU-NPU [77]
  Table 4 caption:
    table_text: TABLE 4 The Results of Intra Testing on SiW [6]
  Table 5 caption:
    table_text: TABLE 5 Results of Cross-Dataset Intra-Type Testing on OULU-NPU, CASIA-MFSD,
      Replay-Attack, and MSU-MFSD
  Table 6 caption:
    table_text: TABLE 6 AUC (%) of the Model Cross-Type Testing on CASIA-MFSD, Replay-Attack,
      and MSU-MFSD
  Table 7 caption:
    table_text: TABLE 7 Results of the Cross-Type Testing on SiW-M [59]
  Table 8 caption:
    table_text: TABLE 8 Results of Cross-Dataset Cross-Type Testing When Trained on
      OULU-NPU and SiW
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3036338
- Affiliation of the first author: research school of engineering, australian national
    university, canberra, australia
  Affiliation of the last author: school of electronics and information, northwestern
    polytechnical university, xian, china
  Figure 1 Link: articels_figures_by_rev_year\2020\High_Frame_Rate_Video_Reconstruction_Based_on_an_Event_Camera\figure_1.jpg
  Figure 1 caption: "Deblurring and reconstruction results of our method compared\
    \ with the state-of-the-art methods on our real blur event dataset. (a) The input\
    \ blurred image. (b) The corresponding event data. (c) A sharp image for the sweater\
    \ captured as a reference for colour and shape (a real blurred image can hardly\
    \ have its ground truth sharp image). (d) Deblurring result of Tao et al. [1].\
    \ (e) Deblurring result of Jin et al. [2]. Jin uses video as training data to\
    \ train a supervised model to perform deblur, where the video can also be considered\
    \ as similar information as the event data. (f) Reconstruction results of Scheerlinck\
    \ et al. [3] from only events. (g) Reconstruction results of Rebecq et al. [4]\
    \ from only events. Based on their default settings, the time resolution of the\
    \ reconstructed video is around \xD710 times higher than the time resolution of\
    \ the original video. (h) Reconstruction results of Rebecq et al. [4] from only\
    \ events. The time resolution here is around \xD7100 . (i) Reconstruction result\
    \ of Pan et al. [5] from combining events and a single blurred frame. (j)Reconstruction\
    \ results of Scheerlinck et al. [3] from events and images. (k)-(l) Our reconstruction\
    \ result from combining events and multiple blurred frame at different time resolution.\
    \ Our result preserves more abundant and faithful texture and the consistency\
    \ of the natural image. (Best viewed on screen)."
  Figure 10 Link: articels_figures_by_rev_year\2020\High_Frame_Rate_Video_Reconstruction_Based_on_an_Event_Camera\figure_10.jpg
  Figure 10 caption: "An example of our reconstruction result on the color event camera\
    \ dataset CED [62]. (a) The input image. (b) Reconstruction results of Rebecq\
    \ et al. [4] from only events. The temporal resolution of the reconstructed video\
    \ is around \xD712 times higher than the original videos based on their default\
    \ settings. (c) Our mEDI result where the temporal resolution is the same as (b).\
    \ From top to bottom, a scene with a low lighting condition, an outdoor scene,\
    \ a scene with slow-moving objects (static background), and an HDR scene. Our\
    \ mEDI model performs well in the top two rows, while E2VID is able to provide\
    \ vivid color textures in the HDR scene. Note that our method focuses on reconstructing\
    \ high-frame rate videos rather than changing the dynamic range of input videos.\
    \ In order to illustrate our detailed textures in the HDR scene, we employ an\
    \ HDR enhancement method [64]."
  Figure 2 Link: articels_figures_by_rev_year\2020\High_Frame_Rate_Video_Reconstruction_Based_on_an_Event_Camera\figure_2.jpg
  Figure 2 caption: 'The event data and our reconstructed result, where (a) and (b)
    are the input of our method. (a) The intensity image from the DAVIS. (b) Events
    from the event camera plotted in 3D space-time (x,y,t) (blue: positive event;
    red: negative event). (c) The first integral of several events during a small
    time interval. (d) The second integral of events during the exposure time. (e)-(h)
    Samples of reconstructed image with different c . The value is from low (0.10),
    to proper (around 0.23) and high (0.60). Note, c = 0.23 in (g) is the chosen automatically
    by our optimization process.'
  Figure 3 Link: articels_figures_by_rev_year\2020\High_Frame_Rate_Video_Reconstruction_Based_on_an_Event_Camera\figure_3.jpg
  Figure 3 caption: "The examples of our reconstructed results are based on our real\
    \ event dataset. The threshold c is estimated automatically from three blurred\
    \ images and their events based on our mEDI model. (a), (b) Blur image and our\
    \ reconstructed Images L 0 , L 1 and L 2 (c), (d) Reconstruction results of L\
    \ 1 and L 2 by Rebecq et al. [4] from only events. The time resolution here is\
    \ around \xD76 based on their default settings. The time resolution of the reconstructed\
    \ video by E2VID [4] is around \xD78 to 15 times higher than the time resolution\
    \ of the original video. (Best viewed on screen)."
  Figure 4 Link: articels_figures_by_rev_year\2020\High_Frame_Rate_Video_Reconstruction_Based_on_an_Event_Camera\figure_4.jpg
  Figure 4 caption: Deblurring and reconstruction results on our real blur event dataset.
    (a) Input blurred images. (b) Deblurring result of [2]. (c) Baseline 1 for our
    method. We first use the state-of-the-art video-based deblurring method [2] to
    recover a sharp image. Then use the sharp image as input to a state-of-the-art
    reconstruction method [3] to get the intensity image. (d) Baseline 2 for our method.
    We first use method [3] to reconstruct an intensity image. Then use a deblurring
    method [2] to recover a sharp image. (e) Samples from our reconstructed video
    from L(0) to L(150) .
  Figure 5 Link: articels_figures_by_rev_year\2020\High_Frame_Rate_Video_Reconstruction_Based_on_an_Event_Camera\figure_5.jpg
  Figure 5 caption: "Examples of reconstruction results on real event dataset. (a)\
    \ The intensity image from the event camera. (b) Reconstruction result of our\
    \ E2VID et al. [4] from only events. The temporal resolution is around \xD78 based\
    \ on their default settings, while ours are \xD7100 times higher than the original\
    \ videos. (c) Reconstruction result of our EDI model et al. [5] from combining\
    \ events and a single blurred frame. (d) Reconstruction result of our mEDI model\
    \ from combining events and multiple blurred frames. Our method based on multiple\
    \ images gets better results than our previous one based only on one single image,\
    \ especially on large motion scenery and extreme light conditions. (Best viewed\
    \ on screen)."
  Figure 6 Link: articels_figures_by_rev_year\2020\High_Frame_Rate_Video_Reconstruction_Based_on_an_Event_Camera\figure_6.jpg
  Figure 6 caption: "An example of our reconstruction result using different methods\
    \ to estimate c , on a real sequence from the Event-Camera Dataset [58]. (a) The\
    \ blurred image. (b) Deblurring result of [1]. (c) Our result where c is chosen\
    \ by manual inspection. (d) Our result where c is computed automatically by our\
    \ proposed energy minimization Eq. (19). (e) Reconstruction results of Rebecq\
    \ et al. [4] from only events. The temporal resolution of the reconstructed video\
    \ is around \xD78 times higher than the original videos based on their default\
    \ settings. (f) Our mEDI result where the temporal resolution is the same as (e)."
  Figure 7 Link: articels_figures_by_rev_year\2020\High_Frame_Rate_Video_Reconstruction_Based_on_an_Event_Camera\figure_7.jpg
  Figure 7 caption: An example of the reconstructed result on our synthetic event
    dataset based on the GoPro dataset [53]. [53] provides videos to generate blurred
    images and event data. (a) The blurred image. The red close-up frame is for (b)-(e),
    the yellow close-up frame is for (f)-(g). (b) The deblurring result of Jin et
    al. [2]. (c) Our deblurring result. (d) The crop of their reconstructed images
    and the frame number is fixed at 7. Jin et al. [2] uses the GoPro dataset added
    with 20 scenes as training data and their model is supervised by 7 consecutive
    sharp frames. (e) The crop of our reconstructed images. (f) The crop of Reinbacher
    [33] reconstructed images from only events. (g) The crop of Scheerlinck [3] reconstructed
    image, they use both events and the intensity image. For (e)-(g), the shown frames
    are the chosen examples, where the length of the reconstructed video is based
    on the number of events. (Best viewed on screen).
  Figure 8 Link: articels_figures_by_rev_year\2020\High_Frame_Rate_Video_Reconstruction_Based_on_an_Event_Camera\figure_8.jpg
  Figure 8 caption: Deblurring performance plotted against the value of c . The image
    is clearer with higher PSNR value.
  Figure 9 Link: articels_figures_by_rev_year\2020\High_Frame_Rate_Video_Reconstruction_Based_on_an_Event_Camera\figure_9.jpg
  Figure 9 caption: Examples of reconstruction result on our real blur event dataset
    in low lighting and complex dynamic conditions (a) Input blurred images. (b) The
    event information. (c) Deblurring results of [44]. (d) Deblurring results of [45].
    (e) Deblurring results of [1]. (f) Deblurring results of [53]. (g) Deblurring
    results of [2] and they use video as training data. (h) Reconstruction result
    of [5] from combining events and frames. (i) Reconstruction result of [33] from
    only events. (j)-(k) Reconstruction results of [3], (j) from only events, (k)
    from combining events and frames. (l) Our reconstruction result. Results in (c)-(g)
    show that real high dynamic settings and low light conditions are still challenging
    in the deblurring area. Results in (i)-(j) show that while intensity information
    of a scene is still retained with an event camera recording, color, and delicate
    texture information cannot be recovered. (Best viewed on screen).
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.76
  Name of the first author: Liyuan Pan
  Name of the last author: Yuchao Dai
  Number of Figures: 10
  Number of Tables: 1
  Number of authors: 6
  Paper title: High Frame Rate Video Reconstruction Based on an Event Camera
  Publication Date: 2020-11-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Comparisons on the Synthetic Dataset [53]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3036667
- Affiliation of the first author: school of electronic and information engineering,
    south china university of technology, guangzhou, guangdong, china
  Affiliation of the last author: school of electronic and information engineering,
    south china university of technology, guangzhou, guangdong, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Unsupervised_MultiClass_Domain_Adaptation_Theory_Algorithms_and_Practice\figure_1.jpg
  Figure 1 caption: "Plots of various disagreements between two scoring functions\
    \ f \u2032 and f \u2032\u2032 firing on a single instance x in a case of K=3 and\
    \ \u03C1=5 , where the scoring functions satisfy the sum-to-zero constraint. Top\
    \ row: fix f \u2032 (x) to be [10;\u22125;\u22125] and use f \u2032\u2032 (x)=[\
    \ f \u2032\u2032 1 ; f \u2032\u2032 2 ;\u2212( f \u2032\u2032 1 + f \u2032\u2032\
    \ 2 )] as the argument; Bottom row: fix f \u2032\u2032 (x) to be [10;\u22125;\u2212\
    5] and use f \u2032 (x)=[ f \u2032 1 ; f \u2032 2 ;\u2212( f \u2032 1 + f \u2032\
    \ 2 )] as the argument. (a) The MCSD \u02DC (13), which can be considered as an\
    \ absolute margin-based variant of the margin disparity (MD) [18] (cf. terms in\
    \ (4)); (b) the MCSD \u02C6 (14), which is an absolute margin-based equivalent\
    \ of the hypothesis disagreement (HD) [10] (cf. terms in (2)); (c) our proposed\
    \ MCSD (7)."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Unsupervised_MultiClass_Domain_Adaptation_Theory_Algorithms_and_Practice\figure_2.jpg
  Figure 2 caption: An architectural illustration of Multi-class Domain-adversarial
    learning Networks (McDalNets), which is motivated from the theoretically derived
    objective (22). The gradient reversal layer is adopted here to implement the adversarial
    objective; we note that other implementations (e.g., those discussed in [14])
    would apply as well.
  Figure 3 Link: articels_figures_by_rev_year\2020\Unsupervised_MultiClass_Domain_Adaptation_Theory_Algorithms_and_Practice\figure_3.jpg
  Figure 3 caption: "The architecture of our proposed SymmNets, which includes a feature\
    \ extractor \u03C8 and three classifiers of f s , f t , and f st . Note that the\
    \ classifier f st shares its layer neurons with those of f s and f t . Parameters\
    \ of the classifiers (i.e., f s , f t , and f st ) and those of the feature extractor\
    \ \u03C8 are respectively updated by gradients from loss terms in green and yellow\
    \ boxes. Please refer to the main text for how the objectives are defined."
  Figure 4 Link: articels_figures_by_rev_year\2020\Unsupervised_MultiClass_Domain_Adaptation_Theory_Algorithms_and_Practice\figure_4.jpg
  Figure 4 caption: "Convergence plottings on the adaptation task W \u2192 A of the\
    \ Office-31 [54] by the Source Only, McDalNets based on the MCSD \u02C6 surrogate\
    \ (29) (variant of MDD [18]) and MCSD \u02DC surrogate (31) (DANN [13], [62]),\
    \ McDalNets based on the MCSD surrogates L 1 (24) (MCD [16]), KL (25), and CE\
    \ (26), and SymmNets-V2. SymmNets-V2- f s and SymmNets-V2- f t represent the results\
    \ obtained from the source classifier f s and target classifier f t , respectively."
  Figure 5 Link: articels_figures_by_rev_year\2020\Unsupervised_MultiClass_Domain_Adaptation_Theory_Algorithms_and_Practice\figure_5.jpg
  Figure 5 caption: "Histograms of class weight \u03C9 k learned by SymmNets-V2 (with\
    \ active \u03C9 k ) on the task of A \u2192 W under the setting of partial UDA.\
    \ Model is adapted from a 50-layer ResNet."
  Figure 6 Link: articels_figures_by_rev_year\2020\Unsupervised_MultiClass_Domain_Adaptation_Theory_Algorithms_and_Practice\figure_6.jpg
  Figure 6 caption: "Curve plottings for test accuracy of the unknown class (Unknown)\
    \ and the mean accuracy over all classes (OS) and domain-shared classes (OS \u2217\
    \ ), when setting different values of \u03BD in open set UDA. The results are\
    \ reported on the A \u2192 W task of Office-31 dataset [54] based on the SymmNets-V2\
    \ adapted from a 50-layer ResNet."
  Figure 7 Link: articels_figures_by_rev_year\2020\Unsupervised_MultiClass_Domain_Adaptation_Theory_Algorithms_and_Practice\figure_7.jpg
  Figure 7 caption: "The t-SNE visualization of feature representations learned by\
    \ DANN (top row) and SymmNets-V2 (bottom row) under settings of the closed set,\
    \ partial, and open set UDA. Blue and red points are the respective samples from\
    \ the source domain A and target domain W. For partial UDA, we illustrate the\
    \ feature representations learned by SymmNets-V2 (With active \u03C9 k ), where\
    \ we focus on domain-shared classes, and leave the source classes exclusive to\
    \ the target domain as an indistinguishable cluster via the soft class weighting\
    \ scheme, as discussed in Section 4. A visualization with class label information\
    \ is given in the appendices."
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.65
  Name of the first author: Yabin Zhang
  Name of the last author: Kui Jia
  Number of Figures: 7
  Number of Tables: 12
  Number of authors: 5
  Paper title: 'Unsupervised Multi-Class Domain Adaptation: Theory, Algorithms, and
    Practice'
  Publication Date: 2020-11-10 00:00:00
  Table 1 caption:
    table_text: TABLE 1 A Summary of the Used Math Notations
  Table 10 caption:
    table_text: TABLE 10 Accuracy (%) on the Office-Home Dataset [55] for Partial
      UDA
  Table 2 caption:
    table_text: TABLE 2 Summary of Datasets
  Table 3 caption:
    table_text: TABLE 3 Accuracy (%) of Different Instantiations of McDalNets on the
      Datasets of Office-31 [54], ImageCLEF-DA [53], Office-Home [55], Digits [56],
      [57], [58], VisDA-2017 [60], and DomainNet [61] Under the Setting of Closed
      Set UDA
  Table 4 caption:
    table_text: TABLE 4 Ablation Experiments on Components of SymmNets-V2 Using the
      Datasets of Office-31 [54] and VisDA-2017 [60] Under the Setting of Closed Set
      UDA
  Table 5 caption:
    table_text: TABLE 5 Accuracy (%) on the VisDA-2017 Dataset [60] for Closed Set
      UDA
  Table 6 caption:
    table_text: TABLE 6 Accuracy (%) on the Office-31 Dataset [54] for Closed Set
      UDA
  Table 7 caption:
    table_text: TABLE 7 Accuracy (%) on the ImageCLEF-DA Dataset [53] for Closed Set
      UDA
  Table 8 caption:
    table_text: TABLE 8 Accuracy (%) on the Office-Home Dataset [55] for Closed Set
      UDA
  Table 9 caption:
    table_text: TABLE 9 Accuracy (%) on the Office-31 Dataset [54] for Partial UDA
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3036956
- Affiliation of the first author: faculty of engineering, bar-ilan university, ramat
    gan, israel
  Affiliation of the last author: faculty of engineering, bar-ilan university, ramat
    gan, israel
  Figure 1 Link: articels_figures_by_rev_year\2020\A_Unified_Approach_to_Kinship_Verification\figure_1.jpg
  Figure 1 caption: The kinship verification task is to determine whether two face
    images are related by a particular kinship class, such as Brothers (B-B), Sisters
    (S-S), Father-Daughter (F-D), Father-Son (F-S), Mother-Son (M-S), Mother-Daughter
    (M-D), and Siblings (SIBS).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\A_Unified_Approach_to_Kinship_Verification\figure_2.jpg
  Figure 2 caption: "The proposed kin verification system. The logical components\
    \ are color-coded. The face embedding CNN is shared by all kinship classes and\
    \ is initially pretrained on the CASIA-WebFace face image dataset [33]. The embeddings\
    \ \u03C6 \u02C6 i and \u03D5 \u02C6 i of the input face images are fused and classified\
    \ by separate subnetworks, one per kinship class. The weight-sharing FC-2 layer\
    \ is used to refine the face embedding on the RFIW dataset [17]."
  Figure 3 Link: articels_figures_by_rev_year\2020\A_Unified_Approach_to_Kinship_Verification\figure_3.jpg
  Figure 3 caption: "The kinship classification subnetwork. Multiple such subnetworks\
    \ are applied, one per kinship class. (a) The face embeddings \u03C6 \u02C6 i\
    \ and \u03D5 \u02C6 i , are pointwise weighted ( .\u2217) by the learnt weights\
    \ w \u03C6 and w \u03D5 , respectively. The weighted embeddings are processed\
    \ by the fusion subnetwork in Table 2. Average pooling and a Softmax activation\
    \ are applied to the fused embedding, before the BCE loss. (b) The input layers\
    \ of the proposed embedding fusion subnetwork. The embeddings \u03C6 \u02C6 i\
    \ \u2208 R 512 and \u03D5 \u02C6 i \u2208 R 512 are concatenated channel-wise,\
    \ resulting in a w\u2208 R 512\xD72 activation map, that is processed by 512 1\xD7\
    1 transposed (channel-wise) convolutions, resulting in an R 512\xD7512 activation."
  Figure 4 Link: articels_figures_by_rev_year\2020\A_Unified_Approach_to_Kinship_Verification\figure_4.jpg
  Figure 4 caption: Kin verification examples from the RFIW dataset [17], as classified
    by the proposed scheme. The upper row shows a face image, and the succeeding rows
    depict the corresponding kin verification results.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.67
  Name of the first author: Eran Dahan
  Name of the last author: Yosi Keller
  Number of Figures: 4
  Number of Tables: 8
  Number of authors: 2
  Paper title: A Unified Approach to Kinship Verification
  Publication Date: 2020-11-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Face Embedding Network
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Proposed Embedding Fusion Subnetwork
  Table 3 caption:
    table_text: TABLE 3 The RFIW Dataset [17]
  Table 4 caption:
    table_text: TABLE 4 Kin Verification Results of the FG2018 Challenge [38] Using
      the Restricted Protocol
  Table 5 caption:
    table_text: TABLE 5 Kinship Verification Accuracy Percentage Using the RFIW Dataset
      for Methods Applied to the Restricted and Unrestricted Protocols
  Table 6 caption:
    table_text: TABLE 6 Kin Verification Results of the FG2020 Challenge [19] Using
      the Unrestricted Setup
  Table 7 caption:
    table_text: TABLE 7 The Number of Images Per Family and Family Member in the RFIW
      Dataset [17], With and Without Applying the Adaptive Sampling Introduced in
      Section 3.3
  Table 8 caption:
    table_text: TABLE 8 Ablation Study Results
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3036993
- Affiliation of the first author: college of intelligence and computing, tianjin
    university, tianjin, china
  Affiliation of the last author: college of intelligence and computing, tianjin university,
    tianjin, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Deep_Partial_MultiView_Learning\figure_1.jpg
  Figure 1 caption: Illustration of cross partial multi-view networks for classification.
    Given multi-view data with missing views (black blocks), the encoding 1 1. networks
    degrade the complete latent representation into the available views (white blocks).
    Learning multi-view representation according to the distributions of observations
    and classes has the promising potential to encode complementary information, as
    well as provides accurate predictions.
  Figure 10 Link: articels_figures_by_rev_year\2020\Deep_Partial_MultiView_Learning\figure_10.jpg
  Figure 10 caption: Clustering performance comparison with VIGAN.
  Figure 2 Link: articels_figures_by_rev_year\2020\Deep_Partial_MultiView_Learning\figure_2.jpg
  Figure 2 caption: Illustration of CPM-Nets for unsupervised learning. The latent
    representation learning and missing data imputation equipped with adversarial
    strategy (CPM-GAN) are jointly conducted to improve each other.
  Figure 3 Link: articels_figures_by_rev_year\2020\Deep_Partial_MultiView_Learning\figure_3.jpg
  Figure 3 caption: "Classification performance comparison under different missing\
    \ rates ( \u03B7 )."
  Figure 4 Link: articels_figures_by_rev_year\2020\Deep_Partial_MultiView_Learning\figure_4.jpg
  Figure 4 caption: "Classification comparison with view completion using average\
    \ value and cascaded residual autoencoder (CRA) [5] (with missing rate \u03B7\
    =0.5 ). ADNI and 3Sources-partial are multi-modal datasets with naturally modality\
    \ missing."
  Figure 5 Link: articels_figures_by_rev_year\2020\Deep_Partial_MultiView_Learning\figure_5.jpg
  Figure 5 caption: Visualization of representations on Handwritten with missing rate
    eta = 0.5 , where U and S indicate unsupervised and supervised settings in representation
    learning. (Zoom in for best view).
  Figure 6 Link: articels_figures_by_rev_year\2020\Deep_Partial_MultiView_Learning\figure_6.jpg
  Figure 6 caption: Fine-tuning evaluation.
  Figure 7 Link: articels_figures_by_rev_year\2020\Deep_Partial_MultiView_Learning\figure_7.jpg
  Figure 7 caption: Imputation performance comparison with different missing rates
    ( eta ).
  Figure 8 Link: articels_figures_by_rev_year\2020\Deep_Partial_MultiView_Learning\figure_8.jpg
  Figure 8 caption: Clustering performance comparison under different missing rates
    ( eta ).
  Figure 9 Link: articels_figures_by_rev_year\2020\Deep_Partial_MultiView_Learning\figure_9.jpg
  Figure 9 caption: Clustering performance comparison on real-world missing data.
  First author gender probability: 0.94
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Changqing Zhang
  Name of the last author: Qinghua Hu
  Number of Figures: 12
  Number of Tables: 0
  Number of authors: 6
  Paper title: Deep Partial Multi-View Learning
  Publication Date: 2020-11-12 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3037734
- Affiliation of the first author: department of automation, tsinghua university,
    beijing, china
  Affiliation of the last author: department of automation, tsinghua university, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2020\SelfSupervised_Discovering_of_Interpretable_Features_for_Reinforcement_Learning\figure_1.jpg
  Figure 1 caption: Architecture diagram of our self-supervised interpretable framework,
    which includes two stages. In the first stage, we switch S to K 1 and jointly
    pretrain the feature extractor f e and action predictor f a with RL. In the second
    stage, we switch S to K 2 and train the decoder f d while freezing f e and f a
    . The encoder is shared from the feature extractor and also denoted by f e . N
    A is the dimension of action space.
  Figure 10 Link: articels_figures_by_rev_year\2020\SelfSupervised_Discovering_of_Interpretable_Features_for_Reinforcement_Learning\figure_10.jpg
  Figure 10 caption: "Relationship between the average return R \xAF \xAF \xAF \xAF\
    \ and the average mask metrics ( FOR \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF\
    \ \xAF \xAF and BER \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF ).\
    \ FOR \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF and BER \xAF \xAF\
    \ \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF are the average results across\
    \ 10 random states. Each black node in the FOR \xAF \xAF \xAF \xAF \xAF \xAF \xAF\
    \ \xAF \xAF \xAF \xAF \u2212 BER \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF \xAF\
    \ \xAF \xAF plane corresponds to a mask policy."
  Figure 2 Link: articels_figures_by_rev_year\2020\SelfSupervised_Discovering_of_Interpretable_Features_for_Reinforcement_Learning\figure_2.jpg
  Figure 2 caption: Basic attention patterns (time goes from left to right). For each
    task, the top row shows a sequence of state-action pairs generated by the expert
    policy while the bottom row shows a sequence of masked state-action pairs generated
    by the mask policy. Bright areas in heatmaps are the regions used to make decisions
    by the mask policy. Best viewed on a computer monitor.
  Figure 3 Link: articels_figures_by_rev_year\2020\SelfSupervised_Discovering_of_Interpretable_Features_for_Reinforcement_Learning\figure_3.jpg
  Figure 3 caption: Performance comparisons between expert policy and mask policy
    on Lane-following task. Both two policies are evaluated on ten maps. All returns
    are averaged across 15 evaluation episodes, and the errorbars represent the standard
    variations.
  Figure 4 Link: articels_figures_by_rev_year\2020\SelfSupervised_Discovering_of_Interpretable_Features_for_Reinforcement_Learning\figure_4.jpg
  Figure 4 caption: Performance comparisons between expert policy and mask policy
    on Atari games.
  Figure 5 Link: articels_figures_by_rev_year\2020\SelfSupervised_Discovering_of_Interpretable_Features_for_Reinforcement_Learning\figure_5.jpg
  Figure 5 caption: Visualization of the performance on four maps. For each map, three
    rows represent a sequence of states, attention masks and masked states respectively
    (time goes from left to right).
  Figure 6 Link: articels_figures_by_rev_year\2020\SelfSupervised_Discovering_of_Interpretable_Features_for_Reinforcement_Learning\figure_6.jpg
  Figure 6 caption: Comparison between our method and other methods, including Gradient-based
    saliency method (Gradient-Saliency) [18] and Gaussian perturbation-based saliency
    method (Perturbation-Saliency) [21]. For each method, a short sequence of saliency-overlaid
    states is shown.
  Figure 7 Link: articels_figures_by_rev_year\2020\SelfSupervised_Discovering_of_Interpretable_Features_for_Reinforcement_Learning\figure_7.jpg
  Figure 7 caption: "Evolution of the agents attention as the regularization scale\
    \ \u03B1 varies. Bright areas are the \u201Cattended\u201D regions for making\
    \ decisions."
  Figure 8 Link: articels_figures_by_rev_year\2020\SelfSupervised_Discovering_of_Interpretable_Features_for_Reinforcement_Learning\figure_8.jpg
  Figure 8 caption: A problematic situation (time goes from left to right). Three
    rows correspond to original states, generated attention maps and masked states,
    respectively.
  Figure 9 Link: articels_figures_by_rev_year\2020\SelfSupervised_Discovering_of_Interpretable_Features_for_Reinforcement_Learning\figure_9.jpg
  Figure 9 caption: "An example for calculating the mask metrics. (a) is a specific\
    \ state from Duckietown, red area in (b) is the \u201Ctrue\u201D task-relevant\
    \ features that we expect to learn, and unmasked area in (c) is the features extracted\
    \ by our SSINet."
  First author gender probability: 0.75
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.75
  Name of the first author: Wenjie Shi
  Name of the last author: Cheng Wu
  Number of Figures: 14
  Number of Tables: 3
  Number of authors: 6
  Paper title: Self-Supervised Discovering of Interpretable Features for Reinforcement
    Learning
  Publication Date: 2020-11-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of Notation
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Comparison of Average Returns When the Policy is Learned
      With Access to Only the Pixels of Particular Masks Obtained by Different Methods
      During the Training
  Table 3 caption:
    table_text: "TABLE 3 R 2 R2 Correlation Value (Pearson) Between the Average Mask\
      \ Metrics and the Average Return R \xAF \xAF \xAF \xAF R\xAF"
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3037898
- Affiliation of the first author: institute of artificial intelligence and robotics,
    college of artificial intelligence, xian jiaotong university, xian, shannxi, p.
    r. china
  Affiliation of the last author: sensetime computer vision research group (scvrg),
    sydney university, sydney, nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2020\SocialAware_Pedestrian_Trajectory_Prediction_via_States_Refinement_LSTM\figure_1.jpg
  Figure 1 caption: Overview of SR-LSTM. States refinement module is considered as
    an additional subnetwork following the LSTM cells, which aligns all pedestrians
    together and refines current states of them. The refined states are used to predict
    the locations at the next time step. To be concise, this figure shows the model
    of SR-LSTM without any edge LSTMs. See Section 3.2 and Fig. 4 for the full E-SR-LSTM.
  Figure 10 Link: articels_figures_by_rev_year\2020\SocialAware_Pedestrian_Trajectory_Prediction_via_States_Refinement_LSTM\figure_10.jpg
  Figure 10 caption: Qualitative examples of compared methods. Results of SGAN show
    the best predicted sample in 20 sample times. E-SR-LSTM gives the most moderate
    future predictions. Best viewed in color.
  Figure 2 Link: articels_figures_by_rev_year\2020\SocialAware_Pedestrian_Trajectory_Prediction_via_States_Refinement_LSTM\figure_2.jpg
  Figure 2 caption: "Trajectory prediction for the lady at time t (dashed lines) with\
    \ obvious deviation when considering the trajectory of the man on the right up\
    \ to time t (a) or up to time t\u22121 (b)."
  Figure 3 Link: articels_figures_by_rev_year\2020\SocialAware_Pedestrian_Trajectory_Prediction_via_States_Refinement_LSTM\figure_3.jpg
  Figure 3 caption: (a) Activation patterns of hidden neurons in LSTM, where trajectories
    start from the origin. The trajectory patterns marked by the color have top-20
    responses for a certain hidden neuron from database. (b) An example of three interacting
    pedestrians. How will the dyad on the right pay attention to the other pedestrian
    on the left?.
  Figure 4 Link: articels_figures_by_rev_year\2020\SocialAware_Pedestrian_Trajectory_Prediction_via_States_Refinement_LSTM\figure_4.jpg
  Figure 4 caption: Flowchart of the refinement procedure of E-SR-LSTM and SR-LSTM
    exampled by a first-layer refinement, with the subject node i and a neighboring
    node j while other neighboring nodes are omitted. Each colored arrow denotes for
    a certain kind of LSTM state. (a). Refinement of node LSTM state for pedestrian
    i (Eq. (14)). Features from neighboring node j are collected through N-IS and
    W mp , which results in M j . Likewise, M se i,j is produced from the features
    of corresponding edge (i,j) transformed by E-IS and W se . The collected messages
    M j and M se i,j are then passed to c t,l i . (b) States refinement of SR-LSTM
    (Eq. (7)), where only node messages are conveyed. (c) Refinement of the spatial-edge
    LSTM for the spatial edge (i,j) (Eq. (18)). The cell state is updated based on
    node features of h t,l j and h t,l i . (d). Details of the information selection
    module for nodeedge features (NE-IS), where an attention and a gate are generated
    from pairwise features. The illustration of E-IS module is in the similar form
    but replace the location of h t i,x and h t x on the figure.
  Figure 5 Link: articels_figures_by_rev_year\2020\SocialAware_Pedestrian_Trajectory_Prediction_via_States_Refinement_LSTM\figure_5.jpg
  Figure 5 caption: Illustration of the message passing process. The central node
    is the subject node while others are the neighboring nodes. (a) and (b) suggest
    that the central node receives information from surrounding nodes and the corresponding
    edges. (c). The edge receives information from its two vertical nodes.
  Figure 6 Link: articels_figures_by_rev_year\2020\SocialAware_Pedestrian_Trajectory_Prediction_via_States_Refinement_LSTM\figure_6.jpg
  Figure 6 caption: Two kinds of teaching mode. (a) Single-step mode. Current ground
    truth (GT) annotation is given to the next time step as input. (b) Multi-step
    mode, where the current output is used as the input of next time step at inference
    stage.
  Figure 7 Link: articels_figures_by_rev_year\2020\SocialAware_Pedestrian_Trajectory_Prediction_via_States_Refinement_LSTM\figure_7.jpg
  Figure 7 caption: Loss curves of V-LSTM, SR-LSTM and E-SR-LSTM.(a) and (b) denote
    the training loss curve and validation loss curve for the leave-one-out training
    for ETH-univ. (c) and (d) are similar for the UCY-univ.
  Figure 8 Link: articels_figures_by_rev_year\2020\SocialAware_Pedestrian_Trajectory_Prediction_via_States_Refinement_LSTM\figure_8.jpg
  Figure 8 caption: 'Evaluation on MAD ( Y -axis) as a function of the threshold cth
    for nonlinear trajectories ( c>cth ) and linear trajectories ( cleq cth ) on ETH&UCY,
    where c is the nonlinearity degree. (a-b): Comparison with other approaches. (c-d):
    Comparison of the variant models.'
  Figure 9 Link: articels_figures_by_rev_year\2020\SocialAware_Pedestrian_Trajectory_Prediction_via_States_Refinement_LSTM\figure_9.jpg
  Figure 9 caption: Qualitative examples of SR-LSTM, Node w NESRN+Edge, and E-SR-LSTM.
    Best viewed in color.
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.71
  Name of the first author: Pu Zhang
  Name of the last author: Wanli Ouyang
  Number of Figures: 14
  Number of Tables: 8
  Number of authors: 5
  Paper title: Social-Aware Pedestrian Trajectory Prediction via States Refinement
    LSTM
  Publication Date: 2020-11-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance on V-LSTM With Different Data Preprocessings
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Basic Component Analysis for SR-LSTM, Based on the Pretreatment
      Setting of ID4 in Table 1
  Table 3 caption:
    table_text: TABLE 3 Component Analysis for E-SR-LSTM
  Table 4 caption:
    table_text: TABLE 4 Comparison With Existing Methods in Meters
  Table 5 caption:
    table_text: TABLE 5 Comparison With Existing Methods in Normalized Pixels
  Table 6 caption:
    table_text: TABLE 6 Comparison on PWPD and NYGC on Normalized Pixels (nMADnFAD)
  Table 7 caption:
    table_text: TABLE 7 Comparison With CIDNN on PWPD and NYGC
  Table 8 caption:
    table_text: TABLE 8 Comparison of Parameter Amount, Inference Speed on ETH&UCY
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3038217
- Affiliation of the first author: school of mathematics and statistics, the university
    of melbourne, victoria, australia
  Affiliation of the last author: school of computer science and engineering, southeast
    university, nanjing, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Revisiting_Facial_Age_Estimation_With_New_Insights_From_Instance_Space_Analysis\figure_1.jpg
  Figure 1 caption: 'SVM prediction of good (blue) and not-good (orange) performance
    of algorithms for age estimation: our 2007 algorithm [10] (left), the 2002 WAS
    algorithm [12] (middle), SVM (right). Each instance (facial image) is represented
    as a point in the 2-D coordinate system defined by the axes given by equation
    (1).'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Revisiting_Facial_Age_Estimation_With_New_Insights_From_Instance_Space_Analysis\figure_2.jpg
  Figure 2 caption: 'Distribution of six selected features across the instance space,
    with instances colored blue with minimal values of a feature, through to yellow
    for maximal values of a feature. The features are top row: PC3 (left), PC4 (middle),
    PC5 (right); bottom row: PC6 (left), PC7 (middle), PC8 (right).'
  Figure 3 Link: articels_figures_by_rev_year\2020\Revisiting_Facial_Age_Estimation_With_New_Insights_From_Instance_Space_Analysis\figure_3.jpg
  Figure 3 caption: Distribution of feature PC5, with young faces having small (blue)
    PC5 values, mid-age faces having mid-range (green) PC5 values, and older faces
    having larger (yellow) PC5 values. Appended to this plot are several facial image
    instances from the FG-NET database to support the observation that true age grows
    diagonally from left to right across the instance space.
  Figure 4 Link: articels_figures_by_rev_year\2020\Revisiting_Facial_Age_Estimation_With_New_Insights_From_Instance_Space_Analysis\figure_4.jpg
  Figure 4 caption: SVM Prediction of good (blue) and not-good (orange) performance
    of the new algorithm PC5 based on equation (3). Its footprint is unique compared
    to the other algorithms.
  Figure 5 Link: articels_figures_by_rev_year\2020\Revisiting_Facial_Age_Estimation_With_New_Insights_From_Instance_Space_Analysis\figure_5.jpg
  Figure 5 caption: 'Recommended algorithms based on SVM model predictions: our 2007
    algorithm [10] (blue), the 2002 WAS algorithm [12] (orange), SVM (red), PC5 (yellow),
    and no algorithm recommended (grey).'
  Figure 6 Link: articels_figures_by_rev_year\2020\Revisiting_Facial_Age_Estimation_With_New_Insights_From_Instance_Space_Analysis\figure_6.jpg
  Figure 6 caption: Distribution of true age labels in the FG-NET (black) and much
    larger MORPH2 (grey) databases, showing the bias inherent in FG-NET.
  Figure 7 Link: articels_figures_by_rev_year\2020\Revisiting_Facial_Age_Estimation_With_New_Insights_From_Instance_Space_Analysis\figure_7.jpg
  Figure 7 caption: A new instance space based on faces from databases FG-NET (black)
    and MORPH2 (grey), and an initial feature set of 128 deep learning features.
  Figure 8 Link: articels_figures_by_rev_year\2020\Revisiting_Facial_Age_Estimation_With_New_Insights_From_Instance_Space_Analysis\figure_8.jpg
  Figure 8 caption: SVM model prediction for good (blue) and not-good (orange) performance
    of the SVM age estimation algorithm based on retraining of the FG-NET and MORPH2
    datasets using 128 deep learning features.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.6
  Name of the first author: Kate Smith-Miles
  Name of the last author: Xin Geng
  Number of Figures: 8
  Number of Tables: 1
  Number of authors: 2
  Paper title: Revisiting Facial Age Estimation With New Insights From Instance Space
    Analysis
  Publication Date: 2020-11-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Mean Absolute Error (MAE) of Age Prediction (in Years), Averaged
      Across FG-Net, As Reported in [10]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3038760
