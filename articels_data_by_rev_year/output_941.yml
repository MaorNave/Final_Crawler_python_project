- Affiliation of the first author: camera, university of bath, bath, united kingdom
  Affiliation of the last author: cvssp, university of surrey, guildford, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2017\Bayesian_Helmholtz_Stereopsis_with_Integrability_Prior\figure_1.jpg
  Figure 1 caption: 'Left: Reciprocal pair capture; Right: Depth sampling for virtual
    camera pixel p in a 4-connected neighbourhood.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Bayesian_Helmholtz_Stereopsis_with_Integrability_Prior\figure_2.jpg
  Figure 2 caption: Schematic representation of the priors for two laterally neighbouring
    pixels p and q in the xz plane.
  Figure 3 Link: articels_figures_by_rev_year\2017\Bayesian_Helmholtz_Stereopsis_with_Integrability_Prior\figure_3.jpg
  Figure 3 caption: "Depth and normal error for reconstructions from noisy synthetic\
    \ data. Initial sampling [mm] ( \u03B4x , \u03B4y , \u03B4z ) as indicated."
  Figure 4 Link: articels_figures_by_rev_year\2017\Bayesian_Helmholtz_Stereopsis_with_Integrability_Prior\figure_4.jpg
  Figure 4 caption: "Reconstructions and normals from real data; initial sampling\
    \ [mm]: (1, 1, 0.25); \u03B1 (left to right): (1, 0.05, 1, 0.8)."
  Figure 5 Link: articels_figures_by_rev_year\2017\Bayesian_Helmholtz_Stereopsis_with_Integrability_Prior\figure_5.jpg
  Figure 5 caption: Integration using diffusion tensor, energy minimisation, Frankot-Chellappa,
    least squares, M-estimator [43], Poisson [44], Nehab [26] and no explicit integration
    (proposed) of the point clouds obtained by Bayesian HS with dist.DNprior.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Nadejda Roubtsova
  Name of the last author: Jean-Yves Guillemaut
  Number of Figures: 5
  Number of Tables: 1
  Number of authors: 2
  Paper title: Bayesian Helmholtz Stereopsis with Integrability Prior
  Publication Date: 2017-09-22 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Middlebury Accuracy [mm, \u2218 ] and Completeness [%] at\
      \ 90 Percent Threshold in Terms of Depth (D) and Normal (N) Error on Synthetic\
      \ Datasets for ML HS and Bayesian HS with \u03B1 (Sphere, Pear, Bunny) Set as\
      \ Follows per Prior: Nprior (1, 1, 0.1), Dprior (0.3, 0.4, 0.03), corr.DNprior\
      \ (1, 1, 1), dist.DNprior (1, 0.9, 1)"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2749373
- Affiliation of the first author: cas center for excellence in brain science and
    intelligence technology, national laboratory of pattern recognition, institute
    of automation, chinese academy of sciences, university of chinese academy of sciences,
    beijing, china
  Affiliation of the last author: department of computer science and information systems,
    birkbeck college, london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2017\Dual_Sticky_Hierarchical_Dirichlet_Process_Hidden_Markov_Model_and_Its_Applicati\figure_1.jpg
  Figure 1 caption: Points of interest detected in four frames in a video.
  Figure 10 Link: articels_figures_by_rev_year\2017\Dual_Sticky_Hierarchical_Dirichlet_Process_Hidden_Markov_Model_and_Its_Applicati\figure_10.jpg
  Figure 10 caption: 'Two types of generalized atomic activities: (a) moving straight;
    (b) turning.'
  Figure 2 Link: articels_figures_by_rev_year\2017\Dual_Sticky_Hierarchical_Dirichlet_Process_Hidden_Markov_Model_and_Its_Applicati\figure_2.jpg
  Figure 2 caption: A video is represented as a generalized trajectory with multi-
    observations at each frame.
  Figure 3 Link: articels_figures_by_rev_year\2017\Dual_Sticky_Hierarchical_Dirichlet_Process_Hidden_Markov_Model_and_Its_Applicati\figure_3.jpg
  Figure 3 caption: The graphical model of the basic mixture model.
  Figure 4 Link: articels_figures_by_rev_year\2017\Dual_Sticky_Hierarchical_Dirichlet_Process_Hidden_Markov_Model_and_Its_Applicati\figure_4.jpg
  Figure 4 caption: The stick-breaking graphical model for the DP mixture model.
  Figure 5 Link: articels_figures_by_rev_year\2017\Dual_Sticky_Hierarchical_Dirichlet_Process_Hidden_Markov_Model_and_Its_Applicati\figure_5.jpg
  Figure 5 caption: The graphical model of the HDP.
  Figure 6 Link: articels_figures_by_rev_year\2017\Dual_Sticky_Hierarchical_Dirichlet_Process_Hidden_Markov_Model_and_Its_Applicati\figure_6.jpg
  Figure 6 caption: The graphical model for the HDP-HMM.
  Figure 7 Link: articels_figures_by_rev_year\2017\Dual_Sticky_Hierarchical_Dirichlet_Process_Hidden_Markov_Model_and_Its_Applicati\figure_7.jpg
  Figure 7 caption: The graphical model of the sticky HDP-HMM.
  Figure 8 Link: articels_figures_by_rev_year\2017\Dual_Sticky_Hierarchical_Dirichlet_Process_Hidden_Markov_Model_and_Its_Applicati\figure_8.jpg
  Figure 8 caption: The sticky HDP-HMM with multimodal observation models.
  Figure 9 Link: articels_figures_by_rev_year\2017\Dual_Sticky_Hierarchical_Dirichlet_Process_Hidden_Markov_Model_and_Its_Applicati\figure_9.jpg
  Figure 9 caption: The stick-breaking graphical model of the dual sticky HDP-HMM.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Weiming Hu
  Name of the last author: Stephen Maybank
  Number of Figures: 20
  Number of Tables: 6
  Number of authors: 5
  Paper title: Dual Sticky Hierarchical Dirichlet Process Hidden Markov Model and
    Its Application to Natural Language Description of Motions
  Publication Date: 2017-09-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of the Terminologies of Trajectory Modeling and
      the Topic Model
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Clustering Results for the Different Methods on the Hand
      Sign Trajectory Dataset
  Table 3 caption:
    table_text: TABLE 3 Clustering Performance of Different Methods on the Human Action
      Dataset
  Table 4 caption:
    table_text: TABLE 4 Clustering Accuracies on the Synthetic Dataset
  Table 5 caption:
    table_text: TABLE 5 Clustering Accuracies on the Traffic Dataset
  Table 6 caption:
    table_text: TABLE 6 The Comparison Between Our Method and the Traditional CRP-Based
      Method for DP-GMM
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2756039
- Affiliation of the first author: department of electrical and computer engineering,
    university of florida, gainesville, fl
  Affiliation of the last author: precision silver llc, gainesville, fl
  Figure 1 Link: articels_figures_by_rev_year\2017\Discriminative_Multiple_Instance_Hyperspectral_Target_Characterization\figure_1.jpg
  Figure 1 caption: MI-SMF and MI-ACE estimated target concepts in comparison to the
    true target signature used to simulate the data. MI-SMF and MI-ACE do not necessarily
    recover the true target signature as shown by Simulated 2D Data Set 2. Instead,
    the methods estimate the signature that maximizes detection performance.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Discriminative_Multiple_Instance_Hyperspectral_Target_Characterization\figure_2.jpg
  Figure 2 caption: ROC curve analysis using SMF and ACE with the true target signature
    in comparison to MI-SMF and MI-ACE estimated target concepts. MI-SMF and MI-ACE
    do not necessarily recover the true target signature as shown by Simulated 2D
    Data Set 2. Instead, the methods estimate the signature that maximizes detection
    performance.
  Figure 3 Link: articels_figures_by_rev_year\2017\Discriminative_Multiple_Instance_Hyperspectral_Target_Characterization\figure_3.jpg
  Figure 3 caption: Hyperspectral signatures used to generate hyperspectral simulated
    data set. Signatures selected from the ASTER spectral library include Red Slate
    (target), verde antique, phyllite and pyroxenite spectra.
  Figure 4 Link: articels_figures_by_rev_year\2017\Discriminative_Multiple_Instance_Hyperspectral_Target_Characterization\figure_4.jpg
  Figure 4 caption: "MI-SMF, MI-ACE, and e FUMI estimated target concepts and EM-DD\
    \ estimated scaled target concept. MI-SMF and MI-ACE emphasize the distinguishing\
    \ characteristics of the target concept which can be seen by the large value at\
    \ 1 \u03BC m and negative values at around 0.5 \u03BC m which corresponds to wavelength\
    \ regions in which the target concept has relatively large and small magnitude\
    \ in comparison to backgroundnon-target materials, respectively. e FUMI attempts\
    \ to reconstruct the true target signature shape which can be seen by comparing\
    \ its target concept to the true target signature shown in Fig. 3. Finally, for\
    \ EM-DD, the estimated target concepts scaled by the estimated feature scaling\
    \ is shown. The result illustrates that EM-DD is very effective at identifying\
    \ the regions in which the target concept is different from backgroundnon-target\
    \ materials as a result of its estimated scaling parameters. These target concepts\
    \ were estimated using 50 positive and negative bags each containing ten data\
    \ points. Positive bags contained two true target points with an average target\
    \ proportion of 0.9. Zero-mean Gaussian noise was added to the data such that\
    \ the SNR was 40dB."
  Figure 5 Link: articels_figures_by_rev_year\2017\Discriminative_Multiple_Instance_Hyperspectral_Target_Characterization\figure_5.jpg
  Figure 5 caption: "Target concepts estimated for the AR Face \u201CSunglass\u201D\
    \ Target Concept estimated using MI-SMF, MI-ACE, e FUMI, EMDD, EMDD-P and DMIL.\
    \ As can be seen in these figures, MI-SMF and MI-ACE correctly highlight sunglasses\
    \ as the most discriminative features in the imagery to distinguish between the\
    \ target and non-target data points."
  Figure 6 Link: articels_figures_by_rev_year\2017\Discriminative_Multiple_Instance_Hyperspectral_Target_Characterization\figure_6.jpg
  Figure 6 caption: MUUFL Gulfport data set RGB image and the 57 target locations.
  Figure 7 Link: articels_figures_by_rev_year\2017\Discriminative_Multiple_Instance_Hyperspectral_Target_Characterization\figure_7.jpg
  Figure 7 caption: Comparison of target concepts found by MI-SMF, MI-ACE, e FUMI,
    EM-DD-P, EM-DD and DMIL for (a) Brown, (b) Dark Green, (c) Faux Vineyard Green,
    and (d) Pea Green Targets.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Alina Zare
  Name of the last author: Taylor Glenn
  Number of Figures: 7
  Number of Tables: 8
  Number of authors: 3
  Paper title: Discriminative Multiple Instance Hyperspectral Target Characterization
  Publication Date: 2017-09-26 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Simulated Hyperspectral Data Experiments: Varying Proportion
      of Positive versus Negative Bags'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Simulated Hyperspectral Data Experiments: Varying Number
      of True Target Points in Positive Bags'
  Table 3 caption:
    table_text: 'TABLE 3 Simulated Hyperspectral Data Experiments: Varying Mean Target
      Proportion in True Target Points'
  Table 4 caption:
    table_text: "TABLE 4 Normalized Area Under the ROC Curve at False Alarm Rates\
      \ of 0.001 and 1 for the AR Face Dataset \u201CSunglasses\u201D MIL Detection\
      \ Experiment"
  Table 5 caption:
    table_text: TABLE 5 NAUC on MUUFL Gulfport, One Negative Bag
  Table 6 caption:
    table_text: TABLE 6 NAUC on MUUFL Gulfport, Cluster Background into K=15 Negative
      Bags
  Table 7 caption:
    table_text: TABLE 7 NAUC on MUUFL Gulfport, Cluster Background into K=100 Negative
      Bags
  Table 8 caption:
    table_text: TABLE 8 NAUC on MUUFL Gulfport, Each Background Point Is an Individual
      Negative Bag, K=106548
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2756632
- Affiliation of the first author: visual computing department, agency for science
    technology and research, singapore
  Affiliation of the last author: satellite department, agency for science, technology
    and research, singapore
  Figure 1 Link: articels_figures_by_rev_year\2017\SCNN_SubcategoryAware_Convolutional_Networks_for_Object_Detection\figure_1.jpg
  Figure 1 caption: Illustration of the large intra-class variation due to different
    viewpoints, object deformation, occlusion, clutters, etc.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\SCNN_SubcategoryAware_Convolutional_Networks_for_Object_Detection\figure_2.jpg
  Figure 2 caption: Overview of the proposed subcategory-aware CNN learning for object
    detection.
  Figure 3 Link: articels_figures_by_rev_year\2017\SCNN_SubcategoryAware_Convolutional_Networks_for_Object_Detection\figure_3.jpg
  Figure 3 caption: Image sources for both ACF detector and CNN learning.
  Figure 4 Link: articels_figures_by_rev_year\2017\SCNN_SubcategoryAware_Convolutional_Networks_for_Object_Detection\figure_4.jpg
  Figure 4 caption: Experimental results of different clustering methods under different
    subcategory numbers on the INRIA dataset.
  Figure 5 Link: articels_figures_by_rev_year\2017\SCNN_SubcategoryAware_Convolutional_Networks_for_Object_Detection\figure_5.jpg
  Figure 5 caption: Experimental results of different clustering methods under different
    subcategory numbers on the Pascal VoC 2007 dataset.
  Figure 6 Link: articels_figures_by_rev_year\2017\SCNN_SubcategoryAware_Convolutional_Networks_for_Object_Detection\figure_6.jpg
  Figure 6 caption: S-CNN person detection performance for the INRIA person dataset
    under different numbers of learning iterations
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.87
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.9
  Name of the first author: Tao Chen
  Name of the last author: Jiayuan Fan
  Number of Figures: 6
  Number of Tables: 4
  Number of authors: 3
  Paper title: 'S-CNN: Subcategory-Aware Convolutional Networks for Object Detection'
  Publication Date: 2017-09-26 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Different Techniques on the INRIA Person Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average Precision (%) of Different Object Detection Methods
      on the VoC2007 Dataset
  Table 3 caption:
    table_text: TABLE 3 Object Detection Results (%) on the MS COCO Dataset
  Table 4 caption:
    table_text: 'TABLE 4 Investigation of Using Different Data Sources for Multi-Component
      ACF Detector Learning. GT: Ground Truth Rectangles; GTT: Ground Truth Transformations;
      ACFDP: Latent Positives as Detected by ACF Detectors; EdgeBP: Edge Box Detected
      Object Proposals'
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2756936
- Affiliation of the first author: department of computer science, university of northern
    british columbia, bc, canada
  Affiliation of the last author: department of computer science, university of northern
    british columbia (visiting), bc, canada
  Figure 1 Link: Not Available
  Figure 1 caption: Not Available
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Liang Chen
  Name of the last author: Lixin Gao
  Number of Figures: Not Available
  Number of Tables: 3
  Number of authors: 3
  Paper title: Ghost Numbers
  Publication Date: 2017-09-28 00:00:00
  Table 1 caption:
    table_text: TABLE 1 All Possible Accuracy Values Between 69.49 and 100.00 Percent
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 The Table of Performance on HondaUCSD Dataset in [1]Note:
      The table is a copy of the Table 1 in [1]. The color highlights are added by
      the authors of this comment paper.'
  Table 3 caption:
    table_text: TABLE 3 Results on Kinect Dataset in [1]
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2757489
- Affiliation of the first author: school of information and communication engineering,
    beijing university of posts and telecommunications, beijing, china
  Affiliation of the last author: school of information and communication engineering,
    beijing university of posts and telecommunications, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2017\Face_Recognition_via_Collaborative_Representation_Its_Discriminant_Nature_and_Su\figure_1.jpg
  Figure 1 caption: "Two-class examples with identical class separability, i.e., Tr\
    \ S \u2020 T S B , where the points marked by crosses are the test samples to\
    \ be reconstructed. (a) The example with subtle inter-class variance, relative\
    \ to the large total variance. Fortunately, the inter-class variance is perpendicular\
    \ to the principal direction of global variance. (b) The example with large inter-class\
    \ variance, relative to the total variance. (c) The coding coefficients of two\
    \ test samples in (a) and (b) are identical, and very discriminative."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Face_Recognition_via_Collaborative_Representation_Its_Discriminant_Nature_and_Su\figure_2.jpg
  Figure 2 caption: "The illustrative examples of the \u201Cprototype plus variation\u201D\
    \ superposed linear representation model. (a) the randomly selected training images\
    \ from AR database. (b) the first column contains the \u201Cprototypes\u201D derived\
    \ by averaging the images of the same subject, and the rest columns are the \u201C\
    sample-to-centroid\u201D variation images."
  Figure 3 Link: articels_figures_by_rev_year\2017\Face_Recognition_via_Collaborative_Representation_Its_Discriminant_Nature_and_Su\figure_3.jpg
  Figure 3 caption: (a) Percentage of pairs of subspaces whose smallest principal
    angle is smaller than a given value. (b) Average percentage of data points in
    pairs of subspaces that have one or more of their K-nearest neighbors in the other
    subspace.
  Figure 4 Link: articels_figures_by_rev_year\2017\Face_Recognition_via_Collaborative_Representation_Its_Discriminant_Nature_and_Su\figure_4.jpg
  Figure 4 caption: The recognition performance as the dimension of the intermediate
    PCA subspace increases on the AR dataset.
  Figure 5 Link: articels_figures_by_rev_year\2017\Face_Recognition_via_Collaborative_Representation_Its_Discriminant_Nature_and_Su\figure_5.jpg
  Figure 5 caption: Some sample images from the FRGC 2.0 database.
  Figure 6 Link: articels_figures_by_rev_year\2017\Face_Recognition_via_Collaborative_Representation_Its_Discriminant_Nature_and_Su\figure_6.jpg
  Figure 6 caption: The recognition performance as the dimension of the intermediate
    PCA subspace increases on the FRGC dataset using (a) 3 (b) 4 (c) 5 training samples
    per person.
  Figure 7 Link: articels_figures_by_rev_year\2017\Face_Recognition_via_Collaborative_Representation_Its_Discriminant_Nature_and_Su\figure_7.jpg
  Figure 7 caption: (a) The cropped images of some gallery images and corresponding
    probe images in the FERET database. (b) Example images of the differences to the
    class centroid computed from the FRGC version 2 database.
  Figure 8 Link: articels_figures_by_rev_year\2017\Face_Recognition_via_Collaborative_Representation_Its_Discriminant_Nature_and_Su\figure_8.jpg
  Figure 8 caption: The recognition rates of SLRC with ell 1 -regularization (plotted
    by thick symbols) and ell 2 -regularization (plotted by the thin symbols) as a
    function of the value of lambda .
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Weihong Deng
  Name of the last author: Jun Guo
  Number of Figures: 8
  Number of Tables: 5
  Number of authors: 3
  Paper title: 'Face Recognition via Collaborative Representation: Its Discriminant
    Nature and Superposed Representation'
  Publication Date: 2017-09-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Clustering Error (%) of Different Algorithms on the Extended
      Yale B Database
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Recognition Rates (%) on the Extended Yale B Database
  Table 3 caption:
    table_text: TABLE 3 Comparative Recognition Rates of SLRC and Other Recognition
      Methods
  Table 4 caption:
    table_text: TABLE 4 The Face Recognition Rates (%) of Competing Methods on the
      FRGC 2.0 Database with N Training Samples per Person
  Table 5 caption:
    table_text: TABLE 5 Comparative Recognition Rates of SRC and SLRC on FERET Database
      Using Single Training Sample per Person
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2757923
- Affiliation of the first author: graduate school of culture technology, kaist, daejeon,
    republic of korea
  Affiliation of the last author: graduate school of culture technology, kaist, daejeon,
    republic of korea
  Figure 1 Link: articels_figures_by_rev_year\2017\Object_Segmentation_Ensuring_Consistency_Across_MultiViewpoint_Images\figure_1.jpg
  Figure 1 caption: System overview. Our method performs system setup first to obtain
    dense geometric information in each image coordinate by using SFM and JBU. Superpixel-based
    binary segmentation is performed that considers spatial and inter-view correspondences.
    By applying pixel-based boundary refinement, the quality of our segmentation improves
    especially near the segmentation boundaries.
  Figure 10 Link: articels_figures_by_rev_year\2017\Object_Segmentation_Ensuring_Consistency_Across_MultiViewpoint_Images\figure_10.jpg
  Figure 10 caption: Comparison of the results produced after the application of each
    multi-view energy term. (a) Target images. Segmentation results produced (b) using
    only a single-view image term (Section 5.3 ), (c) when omitting the projection
    probability term (Section 5.4.1), (d) when omitting the 3D-color similarity term
    (Section 5.4.2), (e) when omitting the disparity term (Section 5.4.3) and (f)
    when using all of the multi-view image terms.
  Figure 2 Link: articels_figures_by_rev_year\2017\Object_Segmentation_Ensuring_Consistency_Across_MultiViewpoint_Images\figure_2.jpg
  Figure 2 caption: "Camera calibration and depth registration. We use the parameters\
    \ of a customized RGBD camera (\u2460) to obtain a depth image for the reference\
    \ color camera. The application of the SFM algorithm recovers the parameters of\
    \ the target cameras ( \u2461). By processes \u2460 and \u2461 (Eq. (1)), we obtain\
    \ the rotation and the translation matrix between the two 3D coordinates (\u2462\
    )."
  Figure 3 Link: articels_figures_by_rev_year\2017\Object_Segmentation_Ensuring_Consistency_Across_MultiViewpoint_Images\figure_3.jpg
  Figure 3 caption: An example of registered and upsampled depth images represented
    in the image coordinate of a target camera.
  Figure 4 Link: articels_figures_by_rev_year\2017\Object_Segmentation_Ensuring_Consistency_Across_MultiViewpoint_Images\figure_4.jpg
  Figure 4 caption: 'The process of color sample acquisition. The user draws a rectangle
    (red box) near an object of interest in the reference color image. Green lines:
    inter-view correspondence between the reference and the target color images. Foreground
    color samples from the target images are extracted at the locations of inter-view
    correspondences.'
  Figure 5 Link: articels_figures_by_rev_year\2017\Object_Segmentation_Ensuring_Consistency_Across_MultiViewpoint_Images\figure_5.jpg
  Figure 5 caption: 'Single-view graph construction. (a) A target image (b) Probability
    map of the foreground color model. In this image, P[ I p | f p =1] is scaled from
    0 to 255. (c) Edge-aware spatial constraint: The blue superpixels are more consistent
    than the red superpixels. (d) Global color propagation : each superpixel forms
    neighborhood with superpixels that have similar color values.'
  Figure 6 Link: articels_figures_by_rev_year\2017\Object_Segmentation_Ensuring_Consistency_Across_MultiViewpoint_Images\figure_6.jpg
  Figure 6 caption: 'Multi-view graph construction. (a) A target image (b) A projection
    probability map scaled from 0 to 255. (c) Disparity-aware inter-view constraint
    : Inter-view correspondences with similar colors and depth values have a high
    degree of consistency ( Region B ). Inter-view correspondences with different
    colors or depth values are relatively inconsistent ( Region A ). (d) Inter-view
    correspondences in the foreground object region of target image 1. Lines connecting
    two red points denote correspondence pairs that are mismatched as a result of
    incorrect estimation of depth values near the foreground boundary.'
  Figure 7 Link: articels_figures_by_rev_year\2017\Object_Segmentation_Ensuring_Consistency_Across_MultiViewpoint_Images\figure_7.jpg
  Figure 7 caption: 'Multi-view color and geometry propagation. Even if foreground
    (red point) and background (blue point) superpixels have similar color values,
    they are effectively separated using depth information. White lines: edges of
    2D color similarity term ( Section 5.3.3); green lines: 3D color similarity term
    using additional depth cues (Section 5.4.2 ).'
  Figure 8 Link: articels_figures_by_rev_year\2017\Object_Segmentation_Ensuring_Consistency_Across_MultiViewpoint_Images\figure_8.jpg
  Figure 8 caption: Boundary refinement. (a) Source image (b) P F r map in the distance
    term. The white area indicates that the value of P F r is 1. The blue area is
    not part of the computation region. (c) A result of binary segmentation. (d) A
    result of boundary refinement without the distance term. (e) A result of boundary
    refinement with the distance term.
  Figure 9 Link: articels_figures_by_rev_year\2017\Object_Segmentation_Ensuring_Consistency_Across_MultiViewpoint_Images\figure_9.jpg
  Figure 9 caption: Comparison of the results produced after the application of each
    energy term. (a) One of the target images. (b) The ground truth image. (c) and
    (d) Binary segmentation results with and without multi-view energy term, respectively.
    (e) A final result image after refinement. (f) The boundary are for the calculation
    of the boundary accuracy (White area). (g), (h), (i) and (j) are zoomed in images
    of (b), (c), (d) and (e), respectively.
  First author gender probability: 0.88
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.79
  Name of the first author: Seunghwa Jeong
  Name of the last author: Junyong Noh
  Number of Figures: 18
  Number of Tables: 6
  Number of authors: 5
  Paper title: Object Segmentation Ensuring Consistency Across Multi-Viewpoint Images
  Publication Date: 2017-09-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Features of a Superpixel
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Computation Time of Our Approach
  Table 3 caption:
    table_text: TABLE 3 Evaluation of Our Energy Terms
  Table 4 caption:
    table_text: TABLE 4 Quantitative Comparison Using couch Dataset
  Table 5 caption:
    table_text: TABLE 5 Environment Differences According to Methods
  Table 6 caption:
    table_text: TABLE 6 Quantitative Comparison Using Other Standard Datasets
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2757928
- Affiliation of the first author: "mines paristech, cbio \u2013 centre for computational\
    \ biology, fontainebleau, france"
  Affiliation of the last author: "mines paristech, cbio \u2013 centre for computational\
    \ biology, fontainebleau, france"
  Figure 1 Link: articels_figures_by_rev_year\2017\The_Kendall_and_Mallows_Kernels_for_Permutations\figure_1.jpg
  Figure 1 caption: Smooth approximation (in red) of the Heaviside function (in black)
    used to define the mapping (13) for a=1 .
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\The_Kendall_and_Mallows_Kernels_for_Permutations\figure_2.jpg
  Figure 2 caption: Cayley graph of S 4 , generated by the transpositions (1 2) in
    blue, (2 3) in green, and (3 4) in red.
  Figure 3 Link: articels_figures_by_rev_year\2017\The_Kendall_and_Mallows_Kernels_for_Permutations\figure_3.jpg
  Figure 3 caption: 'Across different number of clusters: Left: Computational time
    (in seconds) of k -means algorithms per run. Middle: Average silhouette scores
    of k -means methods. Right : Average silhouette scores of Mallows mixture modeling
    methods.'
  Figure 4 Link: articels_figures_by_rev_year\2017\The_Kendall_and_Mallows_Kernels_for_Permutations\figure_4.jpg
  Figure 4 caption: Clustering results of participating countries to the ESC according
    to their voting behavior illustrated by geographic map (Left) and silhouette plot
    (Right).
  Figure 5 Link: articels_figures_by_rev_year\2017\The_Kendall_and_Mallows_Kernels_for_Permutations\figure_5.jpg
  Figure 5 caption: 'Left: Model performance comparison (ordered by decreasing average
    accuracy across datasets). Middle: Sensitivity of kernel SVMs to C parameter on
    the Breast Cancer 1 dataset. Right: Impact of TSP feature selection on the Prostate
    Cancer 1 dataset. (Special marks on SVM lines denote the parameter returned by
    cross-validation.)'
  Figure 6 Link: articels_figures_by_rev_year\2017\The_Kendall_and_Mallows_Kernels_for_Permutations\figure_6.jpg
  Figure 6 caption: 'Left: Empirical performance of smoothed alternative to Kendall
    kernel on the Medulloblastoma dataset. Right: Empirical convergence of Monte Carlo
    approximate at the fixed window size attaining maximum underlying accuracy from
    the left plot.'
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yunlong Jiao
  Name of the last author: Jean-Philippe Vert
  Number of Figures: 6
  Number of Tables: 2
  Number of authors: 2
  Paper title: The Kendall and Mallows Kernels for Permutations
  Publication Date: 2017-10-05 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of Biomedial Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Prediction Accuracy (%) of Different Methods Across Biomedical
      Datasets
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2719680
- Affiliation of the first author: xlim-sic dept. umr cnrs 7252, university of poitiers,
    poitiers, france
  Affiliation of the last author: xlim-sic dept. umr cnrs 7252, university of poitiers,
    poitiers, france
  Figure 1 Link: articels_figures_by_rev_year\2017\Characterization_of_Color_Images_with_Multiscale_Monogenic_Maxima\figure_1.jpg
  Figure 1 caption: Undecimated monogenic filterbank flowchart for D=2 .
  Figure 10 Link: articels_figures_by_rev_year\2017\Characterization_of_Color_Images_with_Multiscale_Monogenic_Maxima\figure_10.jpg
  Figure 10 caption: Reconstructions with the proposed method on 5 scales.
  Figure 2 Link: articels_figures_by_rev_year\2017\Characterization_of_Color_Images_with_Multiscale_Monogenic_Maxima\figure_2.jpg
  Figure 2 caption: Linewise observation of monogenic features on 3 scales.
  Figure 3 Link: articels_figures_by_rev_year\2017\Characterization_of_Color_Images_with_Multiscale_Monogenic_Maxima\figure_3.jpg
  Figure 3 caption: Amplitude maxima detection and selection at third scale.
  Figure 4 Link: articels_figures_by_rev_year\2017\Characterization_of_Color_Images_with_Multiscale_Monogenic_Maxima\figure_4.jpg
  Figure 4 caption: Elliptical monogenic maxima representation ( C=3 , D=2 ).
  Figure 5 Link: articels_figures_by_rev_year\2017\Characterization_of_Color_Images_with_Multiscale_Monogenic_Maxima\figure_5.jpg
  Figure 5 caption: Maxima jumps through scales.
  Figure 6 Link: articels_figures_by_rev_year\2017\Characterization_of_Color_Images_with_Multiscale_Monogenic_Maxima\figure_6.jpg
  Figure 6 caption: Monogenic amplitude at 1st scale.
  Figure 7 Link: articels_figures_by_rev_year\2017\Characterization_of_Color_Images_with_Multiscale_Monogenic_Maxima\figure_7.jpg
  Figure 7 caption: Segmentation example at 4th scale.
  Figure 8 Link: articels_figures_by_rev_year\2017\Characterization_of_Color_Images_with_Multiscale_Monogenic_Maxima\figure_8.jpg
  Figure 8 caption: Modeling of amplitude and phase for ideal lines and edges.
  Figure 9 Link: articels_figures_by_rev_year\2017\Characterization_of_Color_Images_with_Multiscale_Monogenic_Maxima\figure_9.jpg
  Figure 9 caption: 'Reconstructions based on 5-scale monogenic maxima and 4-scale
    channel-wise Mallat-Zhong method. Lower rows: zoomed areas.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "Rapha\xEBl Soulard"
  Name of the last author: "Philippe Carr\xE9"
  Number of Figures: 13
  Number of Tables: 0
  Number of authors: 2
  Paper title: Characterization of Color Images with Multiscale Monogenic Maxima
  Publication Date: 2017-10-06 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2760303
- Affiliation of the first author: "isit - cnrsuniversit\xE9 d'auvergne, clermont-ferrand,\
    \ france"
  Affiliation of the last author: "isit - cnrsuniversit\xE9 d'auvergne, clermont-ferrand,\
    \ france"
  Figure 1 Link: articels_figures_by_rev_year\2017\Isometric_NonRigid_ShapefromMotion_with_Riemannian_Geometry_Solved_in_Linear_Tim\figure_1.jpg
  Figure 1 caption: The proposed model of NRSfM, where each surface M i is a Riemannian
    manifold defined by embedding the corresponding retinal plane I i .
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Isometric_NonRigid_ShapefromMotion_with_Riemannian_Geometry_Solved_in_Linear_Tim\figure_2.jpg
  Figure 2 caption: Simplified notation for two images.
  Figure 3 Link: articels_figures_by_rev_year\2017\Isometric_NonRigid_ShapefromMotion_with_Riemannian_Geometry_Solved_in_Linear_Tim\figure_3.jpg
  Figure 3 caption: Some images of the rug, table mat, kinect paper and tshirt datasets.
    The five rightmost images of the table mat dataset are zoomed in to improve visibility.
  Figure 4 Link: articels_figures_by_rev_year\2017\Isometric_NonRigid_ShapefromMotion_with_Riemannian_Geometry_Solved_in_Linear_Tim\figure_4.jpg
  Figure 4 caption: Synthetic data experiments. Average normal and depth errors with
    respect to number of views, noise and curvature. Best viewed in colour.
  Figure 5 Link: articels_figures_by_rev_year\2017\Isometric_NonRigid_ShapefromMotion_with_Riemannian_Geometry_Solved_in_Linear_Tim\figure_5.jpg
  Figure 5 caption: Experiments on short sequences. The average normal and depth error
    for each experiment with number of views varying between 3-10 is shown. The views
    of the rug, table mat and kinect paper datasets are selected by uniform sampling
    the long sequences. The views of the tshirt dataset are selected by randomly sampling
    the dataset and the results are averaged over 20 trials. Best viewed in colour.
  Figure 6 Link: articels_figures_by_rev_year\2017\Isometric_NonRigid_ShapefromMotion_with_Riemannian_Geometry_Solved_in_Linear_Tim\figure_6.jpg
  Figure 6 caption: Reconstruction error maps and renderings for the rug, table mat,
    kinect paper and tshirt datasets. We remind that mdhI reconstructs only the visible
    part of the surface. Therefore, the rendering and error map for the kinect paper
    dataset is broken for this method.
  Figure 7 Link: articels_figures_by_rev_year\2017\Isometric_NonRigid_ShapefromMotion_with_Riemannian_Geometry_Solved_in_Linear_Tim\figure_7.jpg
  Figure 7 caption: Experiments on long sequences. The average normal and depth error
    for each frame is shown. The experiment with inextI is only performed for the
    table mat dataset. Best viewed in colour.
  Figure 8 Link: articels_figures_by_rev_year\2017\Isometric_NonRigid_ShapefromMotion_with_Riemannian_Geometry_Solved_in_Linear_Tim\figure_8.jpg
  Figure 8 caption: Images ( 10,20,30,40,50 ) of a partially stretched rubber like
    surface. The first image has the least deformation and the last one has the most.
  Figure 9 Link: articels_figures_by_rev_year\2017\Isometric_NonRigid_ShapefromMotion_with_Riemannian_Geometry_Solved_in_Linear_Tim\figure_9.jpg
  Figure 9 caption: Experiment with an almost stationary object. The first five images
    of the table mat sequence are used. The reconstruction of iso is shown in red.
    The ground truth is indicated with black. E s represents the mean shape error
    (in degrees). E d represents the mean depth error (in mm). The performance of
    iso is almost the same as infP on these five images.
  First author gender probability: 0.97
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Shaifali Parashar
  Name of the last author: Adrien Bartoli
  Number of Figures: 9
  Number of Tables: 4
  Number of authors: 3
  Paper title: Isometric Non-Rigid Shape-from-Motion with Riemannian Geometry Solved
    in Linear Time
  Publication Date: 2017-10-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance of Warps in Noisy Conditions
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Summary of Experiments on Long Sequences
  Table 3 caption:
    table_text: TABLE 3 Mean Shape Error (in degrees) for the Experiment with Partially
      Stretched Surface
  Table 4 caption:
    table_text: TABLE 4 Comparison of Computation Time (in Seconds) for 10, 30 and
      60 Views
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2760301
