- Affiliation of the first author: video and image processing system laboratory, school
    of electronic engineering, xidian university, xi'an, shaanxi, p.r. china
  Affiliation of the last author: video and image processing system laboratory, school
    of electronic engineering, xidian university, xi'an, shaanxi, p.r. china
  Figure 1 Link: articels_figures_by_rev_year\2016\Graphical_Representation_for_Heterogeneous_Face_Recognition\figure_1.jpg
  Figure 1 caption: Overview of the proposed graphical representation based heterogeneous
    face recognition.
  Figure 10 Link: articels_figures_by_rev_year\2016\Graphical_Representation_for_Heterogeneous_Face_Recognition\figure_10.jpg
  Figure 10 caption: Cumulative match score comparison of the baseline methods and
    our method on the USTC-NVIE database.
  Figure 2 Link: articels_figures_by_rev_year\2016\Graphical_Representation_for_Heterogeneous_Face_Recognition\figure_2.jpg
  Figure 2 caption: Examples of the obtained similarity map images. The left two columns
    show three sketch-photo pairs from the CUFSF database. The first two pairs are
    of the same person, and the third pair is of different persons. The corresponding
    similarity map images obtained are shown in the right of the sketch-photo pairs.
    The size of the similarity map image is the same to the face image. We have quantified
    the similarity map images into binary images for better visualization. The bright
    area indicates that the corresponding similarity score is larger than 0.5.
  Figure 3 Link: articels_figures_by_rev_year\2016\Graphical_Representation_for_Heterogeneous_Face_Recognition\figure_3.jpg
  Figure 3 caption: Example images of heterogeneous faces tested in this paper. (a)
    Viewed sketch-photo pair from the CUFSF database. (b) Semi-forensic sketch-photo
    pair from the IIIT-D sketch database. (c) Composite sketch-photo pair from the
    PRIP-VSGC database. (d) Forensic sketch-photo pair from our collected forensic
    sketch database. (e) Near infrared image-photo pair from the CASIA NIR-VIS 2.0
    face database. (f) Thermal infrared image-photo pair from the USTC-NVIE database.
  Figure 4 Link: articels_figures_by_rev_year\2016\Graphical_Representation_for_Heterogeneous_Face_Recognition\figure_4.jpg
  Figure 4 caption: Left top subfigure shows the evaluation for the necessity of spatial
    information; right top subfigure shows the comparison of the proposed CRSM with
    common similarity metrics; left bottom subfigure shows the accuracies of different
    numbers of the nearest neighbors K ; right bottom subfigure shows the accuracies
    by fusion of similarity metrics. All four experiments are conducted on the CUFSF
    database using the SURF feature.
  Figure 5 Link: articels_figures_by_rev_year\2016\Graphical_Representation_for_Heterogeneous_Face_Recognition\figure_5.jpg
  Figure 5 caption: Experiments on using different features and the fusion of them
    on the CUFSF database.
  Figure 6 Link: articels_figures_by_rev_year\2016\Graphical_Representation_for_Heterogeneous_Face_Recognition\figure_6.jpg
  Figure 6 caption: Cumulative match score comparison of the baseline methods, the
    MCWLD method, and our method on the IIIT-D Sketch Database.
  Figure 7 Link: articels_figures_by_rev_year\2016\Graphical_Representation_for_Heterogeneous_Face_Recognition\figure_7.jpg
  Figure 7 caption: Cumulative match score comparison of the baseline methods and
    our method on the PRIP-VSGC Database.
  Figure 8 Link: articels_figures_by_rev_year\2016\Graphical_Representation_for_Heterogeneous_Face_Recognition\figure_8.jpg
  Figure 8 caption: Cumulative match score comparison of the baseline methods, the
    P-RS method, and our method on the forensic sketch database.
  Figure 9 Link: articels_figures_by_rev_year\2016\Graphical_Representation_for_Heterogeneous_Face_Recognition\figure_9.jpg
  Figure 9 caption: Cumulative match score comparison of the baseline methods and
    our method on the CASIA NIR-VIS 2.0 Face Database.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.66
  Name of the first author: Chunlei Peng
  Name of the last author: Jie Li
  Number of Figures: 10
  Number of Tables: 2
  Number of authors: 4
  Paper title: Graphical Representation for Heterogeneous Face Recognition
  Publication Date: 2016-03-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Rank-1 Recognition Accuracies of the State-of-the-Art Approaches
      and Our Method on the CUFSF Database
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Rank-50 Recognition Accuracies of the State-of-the-Art Methods
      and Rank-50 Accuracies with 95% Confidence Intervals of the Proposed G-HFR Method
      on Three Types of Forensic Sketch Databases
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2542816
- Affiliation of the first author: centre for quantum computation & intelligent systems
    and the faculty of engineering and information technology, university of technology
    sydney, 81 broadway street, ultimo, nsw, australia
  Affiliation of the last author: department of computer science and information systems,
    birkbeck college, university of london, london, united kingdom
  Figure 1 Link: Not Available
  Figure 1 caption: Not Available
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Tongliang Liu
  Name of the last author: Stephen J. Maybank
  Number of Figures: Not Available
  Number of Tables: 0
  Number of authors: 4
  Paper title: Algorithm-Dependent Generalization Bounds for Multi-Task Learning
  Publication Date: 2016-03-21 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2544314
- Affiliation of the first author: department of electrical and computer engineering,
    democritus university of thrace, xanthi, greece
  Affiliation of the last author: national and kapodistrian university of athens,
    athens, greece
  Figure 1 Link: articels_figures_by_rev_year\2016\Unsupervised_Spectral_Mesh_Segmentation_Driven_by_Heterogeneous_Graphs\figure_1.jpg
  Figure 1 caption: The distinct steps of the proposed method. (a) A mesh is provided
    as input. (b) The mesh is over-segmented. (c) The heterogeneous graph is constructed
    based on the over-segmentation. The colored dots represent the patches of (b).
    They are connected with edges, which are not shown in the figure. (d) The first
    few eigenvectors are computed. (e) The final segmentation is achieved after analysis
    of the eigenvectors.
  Figure 10 Link: articels_figures_by_rev_year\2016\Unsupervised_Spectral_Mesh_Segmentation_Driven_by_Heterogeneous_Graphs\figure_10.jpg
  Figure 10 caption: Segmentations using a different number of eigenvectors. The eigenvector
    selection criterion can significantly affect the result.
  Figure 2 Link: articels_figures_by_rev_year\2016\Unsupervised_Spectral_Mesh_Segmentation_Driven_by_Heterogeneous_Graphs\figure_2.jpg
  Figure 2 caption: Visualization of an approximation of the Minimum Principal Curvature.
    Concave areas indicate clear segmentation boundaries.
  Figure 3 Link: articels_figures_by_rev_year\2016\Unsupervised_Spectral_Mesh_Segmentation_Driven_by_Heterogeneous_Graphs\figure_3.jpg
  Figure 3 caption: Spectral partitioning stages. Each column illustrates the segmentation
    using one eigenvector from smallest eigenvalue (left) to largest (right). (a)
    Visualization of the eigenvector values. Brown color represents large values gradually
    decreasing to blue. (b) Signs of each eigenvector. Black color indicates negative
    values, while white color indicates positive values. (c) Candidate segments produced
    by the proposed method corresponding to negative values. (d) Candidate segments
    produced by the proposed method corresponding to positive values. (e) The segmentation
    result after processing each eigenvector.
  Figure 4 Link: articels_figures_by_rev_year\2016\Unsupervised_Spectral_Mesh_Segmentation_Driven_by_Heterogeneous_Graphs\figure_4.jpg
  Figure 4 caption: "Notations used during the segment extraction (Section 4.5.1).\
    \ In this example, both maximum and minimum values belong to the same segment\
    \ of S i\u22121 (blue segment of (a)). Therefore the blue segment of (a) constitutes\
    \ both s i\u22121 min and s i\u22121 max . The brown segment of (c) is s i new+\
    \ and the orange segment of (c) is s i new\u2212 (the two produced segments)."
  Figure 5 Link: articels_figures_by_rev_year\2016\Unsupervised_Spectral_Mesh_Segmentation_Driven_by_Heterogeneous_Graphs\figure_5.jpg
  Figure 5 caption: "Segment extraction within an eigenvector. (a) Values of the eigenvector\
    \ U i . (b) Segmentation S i\u22121 . (c) Partitioning using eigenvector U i .\
    \ The red and green dots represent the location of the two seed faces. The red\
    \ dot corresponds to f i min while the green dot corresponds to f i max . The\
    \ isolines U i = t i min and U i = t i max are used to produce new segments. (d)\
    \ S i ."
  Figure 6 Link: articels_figures_by_rev_year\2016\Unsupervised_Spectral_Mesh_Segmentation_Driven_by_Heterogeneous_Graphs\figure_6.jpg
  Figure 6 caption: Illustration of partitioning using the first eigenvector. (a)
    The initial over-segmentation. (b) Visualization of the values of the first eigenvector.
    (c) Visualization of the values |mathbf Ui(k)-mathbf Ui(l)|cdot omega kl which
    are used in (11) to determine the final boundary, with blue color representing
    the smallest observed values. The maximum values form a clear boundary. (d) The
    segmentation as a result of the eigenvector in (b).
  Figure 7 Link: articels_figures_by_rev_year\2016\Unsupervised_Spectral_Mesh_Segmentation_Driven_by_Heterogeneous_Graphs\figure_7.jpg
  Figure 7 caption: An example where a new segment is discarded due to dividing the
    original segment into more than two parts. We show the segmentation as a result
    of the previous eigenvectors in (a) and the current eigenvector in (b). The segment
    which corresponds to negative (blue) values is distinctive with white color in
    (c). This segment lies in the middle of the original segment (in green color in
    (a)). Would it have been added in the segmentation, it would have divided the
    original segment in three parts, thus it is discarded.
  Figure 8 Link: articels_figures_by_rev_year\2016\Unsupervised_Spectral_Mesh_Segmentation_Driven_by_Heterogeneous_Graphs\figure_8.jpg
  Figure 8 caption: (a) An input eigenvector, blue and blown color denote extreme
    values. (b) The signs of the eigenvector in (a). (c) The values |mathbf Ui(k)-mathbf
    Ui(l)|cdot omega kl . (d) k -means clustering with k=3 (e) The result obtained
    with the proposed method.
  Figure 9 Link: articels_figures_by_rev_year\2016\Unsupervised_Spectral_Mesh_Segmentation_Driven_by_Heterogeneous_Graphs\figure_9.jpg
  Figure 9 caption: Two examples of the eigenvector selection. Both figures display
    the second derivative of contiguous eigenvalues ( y-axis) as the rank of eigenvalue
    increases (x-axis). In (a) we select the second local maximum, while in (b) we
    select the first local maximum, since it corresponds to the global maximum.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Panagiotis Theologou
  Name of the last author: Theoharis Theoharis
  Number of Figures: 20
  Number of Tables: 3
  Number of authors: 3
  Paper title: Unsupervised Spectral Mesh Segmentation Driven by Heterogeneous Graphs
  Publication Date: 2016-03-21 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Experimental Results in the Princeton Segmentation Benchmark
      Measured with RI Compared to State-of-Art Algorithms
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Experimental Results in the COSEG Dataset Measured with RI
  Table 3 caption:
    table_text: TABLE 3 Detailed Comparison of the Proposed Method's Performance of
      in the Segmentation Benchmark of [50] Measured with 3DNPRI
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2544311
- Affiliation of the first author: department of electronic engineering, the chinese
    university of hong kong, shatin, hong kong
  Affiliation of the last author: department of electronic engineering, the chinese
    university of hong kong, shatin, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2016\Person_ReIdentification_by_Saliency_Learning\figure_1.jpg
  Figure 1 caption: Salient region could be a body part or a carrying accessory. Some
    salient regions of pedestrians are highlighted with yellow dashed boundaries.
  Figure 10 Link: articels_figures_by_rev_year\2016\Person_ReIdentification_by_Saliency_Learning\figure_10.jpg
  Figure 10 caption: 'Examples of saliency matching in our experiments. It shows four
    types of saliency distributions: saliency in upper body (in blue dashed box),
    saliency of taking bags (in green dashed box), saliency of lower body (in orange
    dashed box), and saliency of stripes on human body (in red dashed box). Best viewed
    in color.'
  Figure 2 Link: articels_figures_by_rev_year\2016\Person_ReIdentification_by_Saliency_Learning\figure_2.jpg
  Figure 2 caption: Illustration of saliency matching with examples. Saliency map
    of each pedestrian image is shown. Best viewed in color.
  Figure 3 Link: articels_figures_by_rev_year\2016\Person_ReIdentification_by_Saliency_Learning\figure_3.jpg
  Figure 3 caption: Diagram of our novel framework of person saliency learning and
    matching for person re-identification.
  Figure 4 Link: articels_figures_by_rev_year\2016\Person_ReIdentification_by_Saliency_Learning\figure_4.jpg
  Figure 4 caption: Interface of user study to obtain person saliency.
  Figure 5 Link: articels_figures_by_rev_year\2016\Person_ReIdentification_by_Saliency_Learning\figure_5.jpg
  Figure 5 caption: Examples of saliency obtained from user study. Each body part
    obtains a saliency value. Saliency map is overlaid on the gray-level image. The
    original color image is on the left.
  Figure 6 Link: articels_figures_by_rev_year\2016\Person_ReIdentification_by_Saliency_Learning\figure_6.jpg
  Figure 6 caption: 'Statistics on saliency user study. Left: Histogram on the numbers
    of trials used to find the targets only based on the most salient parts on query
    images. Right: Histogram on the numbers of trials for all the parts.'
  Figure 7 Link: articels_figures_by_rev_year\2016\Person_ReIdentification_by_Saliency_Learning\figure_7.jpg
  Figure 7 caption: "We take the absolute value of the learned weight vector w , and\
    \ reshape it to a 2-dimensional importance map for different spatial locations.\
    \ Eight importance maps correspond to \u03B1 p i ,k k=1,2,3,4 and \u03B2 p i ,k\
    \ k=1,2,3,4 in Eq. (18)."
  Figure 8 Link: articels_figures_by_rev_year\2016\Person_ReIdentification_by_Saliency_Learning\figure_8.jpg
  Figure 8 caption: Correlation between automatically estimated saliency by different
    approaches (Itti [54], Hou [55], our KNN model and our One-Class SVM (OCSVM) model)
    and estimation from human perception. (a) Scatter plot of correlations over 100
    images. (b) Average correlations.
  Figure 9 Link: articels_figures_by_rev_year\2016\Person_ReIdentification_by_Saliency_Learning\figure_9.jpg
  Figure 9 caption: Examples of estimated saliency map (only body parts are shown).
    (a) Groundtruth saliency for all parts are shown in column (a) . Many parts have
    low saliency scores (in blue color), but a person may have multiple salient parts,
    e.g., in column (a) at bottom right. (b) Pedestrian images. (c) and (d) are general
    image saliency estimated by Itti [54] and Hou [55]. (e) and (f) are person saliency
    estimated by KNN and OCSVM. Number on top of each saliency map indicates the correlation
    with person saliency estimated from user study.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Rui Zhao
  Name of the last author: Xiaogang Wang
  Number of Figures: 14
  Number of Tables: 2
  Number of authors: 3
  Paper title: Person Re-Identification by Saliency Learning
  Publication Date: 2016-03-21 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Description of All the Test Settings in Components Evaluation
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Parameter Settings
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2544310
- Affiliation of the first author: department of statistics, florida state university,
    tallahassee, fl
  Affiliation of the last author: department of statistics, florida state university,
    tallahassee, fl
  Figure 1 Link: articels_figures_by_rev_year\2016\Feature_Selection_with_Annealing_for_Computer_Vision_and_Big_Data_Learning\figure_1.jpg
  Figure 1 caption: "Classification comparison between FSA and Logitboost for equally\
    \ correlated data with M= 10 5 features and N\u2264 10 6 observations, with k=500\
    \ variables selected. Left: training time. Middle: percent of variables correctly\
    \ detected (see sec 3.1.2 ). Right: area under ROC curve."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Feature_Selection_with_Annealing_for_Computer_Vision_and_Big_Data_Learning\figure_2.jpg
  Figure 2 caption: "The value of \u03B2 j ,j= 1,M \xAF \xAF \xAF \xAF \xAF \xAF \xAF\
    \ \xAF \xAF \xAF versus iteration number for simulated data with N=1,000,M=1,000,k=10\
    \ with \u03B7=20,\u03BC=300 ."
  Figure 3 Link: articels_figures_by_rev_year\2016\Feature_Selection_with_Annealing_for_Computer_Vision_and_Big_Data_Learning\figure_3.jpg
  Figure 3 caption: The number of kept features M e versus iteration e for different
    schedules, with M=1,000,k=10, N iter =500 .
  Figure 4 Link: articels_figures_by_rev_year\2016\Feature_Selection_with_Annealing_for_Computer_Vision_and_Big_Data_Learning\figure_4.jpg
  Figure 4 caption: "The loss functions from eqs. (6), (7) and (8). Left: the losses\
    \ on the interval [\u221230,3] . Right: zoom in the interval [\u22124,2] ."
  Figure 5 Link: articels_figures_by_rev_year\2016\Feature_Selection_with_Annealing_for_Computer_Vision_and_Big_Data_Learning\figure_5.jpg
  Figure 5 caption: Piecewise linear response functions fboldsymbolbetaj(xj)=boldsymbolujT(xj)boldsymbolbetaj
    obtained on an eye detection problem using the second order prior (13).
  Figure 6 Link: articels_figures_by_rev_year\2016\Feature_Selection_with_Annealing_for_Computer_Vision_and_Big_Data_Learning\figure_6.jpg
  Figure 6 caption: 'Dependence of the AUC versus algorithm parameters for a linear
    dataset with M=N=1,000, k=k=10 . Left: dependence on eta . Middle: dependence
    on mu when eta =mu 10 . Right: dependence on Niter when eta =20,mu =300 .'
  Figure 7 Link: articels_figures_by_rev_year\2016\Feature_Selection_with_Annealing_for_Computer_Vision_and_Big_Data_Learning\figure_7.jpg
  Figure 7 caption: 'Precision-recall curves for face keypoint detection on the test
    set AFLWMF containing 1,555 images and 3,861 faces. From top to bottom, left to
    right: leftright eye center, leftright nose, leftright mouth corner, leftright
    ear, chin.'
  Figure 8 Link: articels_figures_by_rev_year\2016\Feature_Selection_with_Annealing_for_Computer_Vision_and_Big_Data_Learning\figure_8.jpg
  Figure 8 caption: Sample images from some sequences of three categories in the Hopkins
    155 database with the ground truth labels superimposed.
  Figure 9 Link: articels_figures_by_rev_year\2016\Feature_Selection_with_Annealing_for_Computer_Vision_and_Big_Data_Learning\figure_9.jpg
  Figure 9 caption: The cumulative distribution of the misclassification rate for
    two and three motions in the Hopkins 155 database.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Adrian Barbu
  Name of the last author: Gary Gramajo
  Number of Figures: 9
  Number of Tables: 7
  Number of authors: 4
  Paper title: Feature Selection with Annealing for Computer Vision and Big Data Learning
  Publication Date: 2016-03-21 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Computation Times for Selecting k Variables Using N Observations
      of Dimension M , When N iter =500
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 6 Training Times in Seconds
  Table 3 caption:
    table_text: "TABLE 2 Classification Experiments on Simulated Linearly Separable\
      \ Data with \u03B4=0.9 , Averaged over 100 Runs"
  Table 4 caption:
    table_text: "TABLE 3 Classification Experiments on Simulated Data with Noisy Labels,\
      \ \u03B4=0.9 , Averaged over 100 Runs"
  Table 5 caption:
    table_text: "TABLE 4 Regression Experiments on Simulated Data with Correlation\
      \ \u03B4=0.9 , Averaged over 100 Runs"
  Table 6 caption:
    table_text: TABLE 5 UCI Data Experiments
  Table 7 caption:
    table_text: TABLE 7 Misclassification Rate (in Percent) for Sequences of Full
      Trajectories in the Hopkins 155 Dataset
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2544315
- Affiliation of the first author: "computer vision and geometry group in the department\
    \ of computer science, eth z\xFCrich, switzerland"
  Affiliation of the last author: institute for computer graphics and vision, tu graz,
    austria
  Figure 1 Link: articels_figures_by_rev_year\2016\Homography_Based_Egomotion_Estimation_with_a_Common_Direction\figure_1.jpg
  Figure 1 caption: Comparison of the RANSAC iteration number for 99 percent of success
    probability.
  Figure 10 Link: articels_figures_by_rev_year\2016\Homography_Based_Egomotion_Estimation_with_a_Common_Direction\figure_10.jpg
  Figure 10 caption: 'Evaluation of the 2pt ground plane algorithm: (a) visual odometry
    estimated using the 2pt (red) and the 5pt (black) algorithm. The Vicon ground-truth
    is given in blue. (b) the Relative Pose Error in mm for each individual frame.
    (c) shows the RPE error for the vertical axis (z-axis).'
  Figure 2 Link: articels_figures_by_rev_year\2016\Homography_Based_Egomotion_Estimation_with_a_Common_Direction\figure_2.jpg
  Figure 2 caption: Alignment of the camera with the gravity direction.
  Figure 3 Link: articels_figures_by_rev_year\2016\Homography_Based_Egomotion_Estimation_with_a_Common_Direction\figure_3.jpg
  Figure 3 caption: Evaluation of the 2 point algorithm under sideways and forward
    motion with varying image noise.
  Figure 4 Link: articels_figures_by_rev_year\2016\Homography_Based_Egomotion_Estimation_with_a_Common_Direction\figure_4.jpg
  Figure 4 caption: Evaluation of the 2pt algorithm under different IMU noise and
    constant image noise of 0.5 pixel standard deviation. First row, sideways motion,
    second row forward motion.
  Figure 5 Link: articels_figures_by_rev_year\2016\Homography_Based_Egomotion_Estimation_with_a_Common_Direction\figure_5.jpg
  Figure 5 caption: Evaluation of the 2.5pt algorithm under sideways and forward motion
    with varying image noise.
  Figure 6 Link: articels_figures_by_rev_year\2016\Homography_Based_Egomotion_Estimation_with_a_Common_Direction\figure_6.jpg
  Figure 6 caption: Evaluation of the 2.5pt algorithm under IMU noise and constant
    image noise of 0.5 pixel standard deviation. First row sideways motion, second
    row forward motion.
  Figure 7 Link: articels_figures_by_rev_year\2016\Homography_Based_Egomotion_Estimation_with_a_Common_Direction\figure_7.jpg
  Figure 7 caption: Evaluation of the 3pt-homography algorithm under sideways and
    forward motion with varying image noise.
  Figure 8 Link: articels_figures_by_rev_year\2016\Homography_Based_Egomotion_Estimation_with_a_Common_Direction\figure_8.jpg
  Figure 8 caption: 'Evaluation of the 3pt-homography algorithm under different IMU
    noise and constant image noise of 0.5 pixel standard deviation. First row: sideways
    motion of the camera with varying pitch angle (left) and varying roll angle (right).
    Second row: forward motion of the camera with varying pitch angle (left) and varying
    roll angle (right).'
  Figure 9 Link: articels_figures_by_rev_year\2016\Homography_Based_Egomotion_Estimation_with_a_Common_Direction\figure_9.jpg
  Figure 9 caption: Left, Vicon arena used to record the ground-truth dataset. Center,
    teleoperated Segway mobile robot capturing data. Right, sample image captured
    by the robot.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Olivier Saurer
  Name of the last author: Friedrich Fraundorfer
  Number of Figures: 12
  Number of Tables: 4
  Number of authors: 6
  Paper title: Homography Based Egomotion Estimation with a Common Direction
  Publication Date: 2016-03-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of the Degenerate Conditions (Yes Means Degenerate)
      for the Standard 3pt Method, the Proposed 3pt Homography Method, the 2pt Methods
      and the 2.5pt Method
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Run-Time Comparison of Different Pose Estimation Algorithms
  Table 3 caption:
    table_text: TABLE 3 Root Mean Squared Error Overview
  Table 4 caption:
    table_text: TABLE 4 Root Mean Squared Error Overview for the Wall Sequence
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2545663
- Affiliation of the first author: national laboratory of pattern recognition, institute
    of automation, chinese academy of sciences, beijing, china
  Affiliation of the last author: national laboratory of pattern recognition, center
    for research on intelligent perception and computing, institute of automation,
    chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2016\A_Comprehensive_Study_on_CrossView_Gait_Based_Human_Identification_with_Deep_CNN\figure_1.jpg
  Figure 1 caption: 'Example GEIs of two subjects (S1-S2) in the CASIA-B gait dataset
    [6]. Column a: GEIs in the gallery, with view angle 36 degree . Column b: Probes
    with the same view angle. Columns c-f: Probes with view angle variations. Columns
    g&h: Probes with view angle and clothing variations. Columns i&j: Probes with
    view angle and carrying condition variations. Gait recognition amounts to identifying
    the most similar gallery GEI for each probe.'
  Figure 10 Link: articels_figures_by_rev_year\2016\A_Comprehensive_Study_on_CrossView_Gait_Based_Human_Identification_with_Deep_CNN\figure_10.jpg
  Figure 10 caption: 'Top: The stage where the USF gait sequences are imaged. Bottom:
    The GEIs of two subjects under different conditions. See the text for details.'
  Figure 2 Link: articels_figures_by_rev_year\2016\A_Comprehensive_Study_on_CrossView_Gait_Based_Human_Identification_with_Deep_CNN\figure_2.jpg
  Figure 2 caption: 'Example GEIs of one subject in the CASIA-B gait dataset [6] from
    view 0 degree (left) to 180 degree (right) with an interval of 18 degree. Top
    row: normal walking (NM). Mid row: with a coat (CL). Bottom row: with a bag (BG).'
  Figure 3 Link: articels_figures_by_rev_year\2016\A_Comprehensive_Study_on_CrossView_Gait_Based_Human_Identification_with_Deep_CNN\figure_3.jpg
  Figure 3 caption: Pipeline of a typical GEI-based gait recognition method. The probe
    sample is assigned to S1, which is evaluated as the most similar one.
  Figure 4 Link: articels_figures_by_rev_year\2016\A_Comprehensive_Study_on_CrossView_Gait_Based_Human_Identification_with_Deep_CNN\figure_4.jpg
  Figure 4 caption: 'Three network architectures to be investigated. Rectangles with
    capital letters denote different kinds of layers. Blue with C: Convolution layer.
    Red with N: Normalization layer. Green with P: Spatial pooling layer. Black with
    F: Fully-connected layer. Two adjacent squares filled with zero and one together
    denote a linear two-way classifier. A purple square to the right of a rectangular
    with D indicates that this layer applies the dropout technique. Directly next
    to each of the C and F letters, there is a number denoting the index of that corresponding
    layer in the whole network. The strings following each of them (C1, C2, C3, C1'',
    C2'' and C3'') are formatted as the size of the filters, the stride and the dimensions
    of the feature maps. Likewise, those following the P letters are formatted as
    the size of the pooling cells, the stride and the dimensions of the down-sampled
    feature maps. An integer following F4 or F5 denotes the number of neurons. Better
    viewed in color.'
  Figure 5 Link: articels_figures_by_rev_year\2016\A_Comprehensive_Study_on_CrossView_Gait_Based_Human_Identification_with_Deep_CNN\figure_5.jpg
  Figure 5 caption: 'Top: Learned pair-filters in the first network, i.e., LB in Fig.
    4. Every two horizontally adjacent filters constitute a pair-filter. Bottom: Filters
    of the first convolution stage in the second network, i.e., MT in Fig. 4 . The
    weights are normalized into gray-scale values for illustration. Lighter pixels
    denote positive weights, while dark ones denote negative weights.'
  Figure 6 Link: articels_figures_by_rev_year\2016\A_Comprehensive_Study_on_CrossView_Gait_Based_Human_Identification_with_Deep_CNN\figure_6.jpg
  Figure 6 caption: The small LB and MT networks. Refer to Fig. 4 for more details.
  Figure 7 Link: articels_figures_by_rev_year\2016\A_Comprehensive_Study_on_CrossView_Gait_Based_Human_Identification_with_Deep_CNN\figure_7.jpg
  Figure 7 caption: The large LB and MT networks. Refer to Fig. 4 for more details.
  Figure 8 Link: articels_figures_by_rev_year\2016\A_Comprehensive_Study_on_CrossView_Gait_Based_Human_Identification_with_Deep_CNN\figure_8.jpg
  Figure 8 caption: The two-stream network. Refer to Fig. 4 for more details.
  Figure 9 Link: articels_figures_by_rev_year\2016\A_Comprehensive_Study_on_CrossView_Gait_Based_Human_Identification_with_Deep_CNN\figure_9.jpg
  Figure 9 caption: The 3D CNN network. Refer to Fig. 4 for more details.
  First author gender probability: 0.75
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Zifeng Wu
  Name of the last author: Tieniu Tan
  Number of Figures: 18
  Number of Tables: 5
  Number of authors: 5
  Paper title: A Comprehensive Study on Cross-View Gait Based Human Identification
    with Deep CNNs
  Publication Date: 2016-03-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Our Method with Previous Ones on CASIA-B by
      Average Accuracies (%), Excluding Identical-View Cases
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison with Kusakunniran et al.'s Method [12] and Yu et
      al.'s Baseline [6] under Normal Walking Conditions on CASIA-B by Accuracies
      (%)
  Table 3 caption:
    table_text: TABLE 3 Comparison of Our Method with Previous Ones on CASIA-B by
      Average Accuracies (%), Excluding Identical-View Cases
  Table 4 caption:
    table_text: TABLE 4 Comparison with Hu's Method [37] and Yu et al.'s Baseline
      [6] under Different Walking Conditions on CASIA-B by Accuracies (%)
  Table 5 caption:
    table_text: TABLE 5 Cross-View Gait Recognition Results (%) Obtained on OU-ISIR
      with Our Method and Comparison with the Baseline Reported by the Dataset Authors
      [28]
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2545669
- Affiliation of the first author: centre for vision, speech and signal processing,
    university of surrey, guildford, united kingdom
  Affiliation of the last author: imperial computer vision and learning lab, imperial
    college london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2016\HigherOrder_Occurrence_Pooling_for_BagsofWords_Visual_Concept_Detection\figure_1.jpg
  Figure 1 caption: "Bag-of-Words with Second-order Occurrence Pooling (order r=2\
    \ ). Local descriptors x are extracted from an image and coded by f that operates\
    \ on columns. Circles of various sizes illustrate values of mid-level coefficients.\
    \ Self-tensor product \u2297 r computes co-occurrences of visual words for every\
    \ mid-level feature \u03D5 . Pooling g aggregates visual words from the co-occurrence\
    \ matrices \u03C8 along the direction of stacking."
  Figure 10 Link: articels_figures_by_rev_year\2016\HigherOrder_Occurrence_Pooling_for_BagsofWords_Visual_Concept_Detection\figure_10.jpg
  Figure 10 caption: Comparison of pooling operators on PascalVOC07. Second-order
    Occurrence Pooling ( r=2 , K=600 ), SCC, grey SIFT, and linear kernels were applied
    except for the coder-free case ( r=3 ). For the SC coder, Max-pooling, Gamma Power
    Normalisation, MaxExp , and our n (MaxExpn=5 ) were combined with Second-order
    Occurrence Pooling. Variants of two stage pooling with the first stage eigenvalue
    Power Normalisation or MaxExp (ePN or eMaxExp) followed by second stage coefficient-wise
    Power Normalisation or MaxExp are compared to the matrix logarithm (logm ). Results
    on LLC, LcSA, and SA (Soft Assignment [7], [11]) employ a subset of pooling operators
    to show their varied ability to decorrelate the features.
  Figure 2 Link: articels_figures_by_rev_year\2016\HigherOrder_Occurrence_Pooling_for_BagsofWords_Visual_Concept_Detection\figure_2.jpg
  Figure 2 caption: "Uncertainty in Max-pooling. Mid-level feature coefficients \u03D5\
    \ 1 and \u03D5 2 are produced for descriptors 1\u2264x\u22642 given visual words\
    \ m 1 =1 and m 2 =2 . (a) First-order Occurrence Pooling results in the pooling\
    \ uncertainty u (the grey area). See text for explanations. (b) Second-order statistics\
    \ produce co-occurrence component ( \u03D5 1 \u03D5 2 ) 0.5 that has a maximum\
    \ for x indicated by the dashed stem. This component limits the pooling uncertainty.\
    \ The square root is applied to preserve the linear slopes, e.g., ( \u03D5 1 \u03D5\
    \ 1 ) 0.5 = \u03D5 1 ."
  Figure 3 Link: articels_figures_by_rev_year\2016\HigherOrder_Occurrence_Pooling_for_BagsofWords_Visual_Concept_Detection\figure_3.jpg
  Figure 3 caption: "Bi-modal Bag-of-Words with Second-order Occurrence Pooling. Two\
    \ types of local descriptors x (1) and x (2) are extracted from an image and coded\
    \ by coders f (1) and f (2) . Self-tensor product \u2297 2 computes co-occurrences\
    \ of visual words in every mid-level feature \u03D5 (1) and \u03D5 (2) , respectively.\
    \ Moreover, tensor product \u2297 captures co-occurrences of visual words between\
    \ \u03D5 (1) and \u03D5 (2) (cross-term operation). Pooling g aggregates co-occurring\
    \ visual words."
  Figure 4 Link: articels_figures_by_rev_year\2016\HigherOrder_Occurrence_Pooling_for_BagsofWords_Visual_Concept_Detection\figure_4.jpg
  Figure 4 caption: 'Illustration of residual descriptors. Quantisation loss of the
    descriptors from their original positions x denoted by the grid points, to the
    corresponding reconstructed positions x indicated by the arrows. (a) SC: optimal
    reconstruction (no displacement) within the triangle. ( b) LcSA: poor reconstruction
    within the triangle due to low l=2 .'
  Figure 5 Link: articels_figures_by_rev_year\2016\HigherOrder_Occurrence_Pooling_for_BagsofWords_Visual_Concept_Detection\figure_5.jpg
  Figure 5 caption: "Whitening of the autocorrelation matrix H . (a ) The eigenvalue-\
    \ and (b) coefficient-wise Power Normalisation steps (ePN) and (PN) are shown.\
    \ See H 0.4 , H 0.1 , H \u22170.4 , and H \u22170.1 , ( \u2217 ) is the element-wise\
    \ power."
  Figure 6 Link: articels_figures_by_rev_year\2016\HigherOrder_Occurrence_Pooling_for_BagsofWords_Visual_Concept_Detection\figure_6.jpg
  Figure 6 caption: "Performance of BoW with higher-order occurrence pooling reported\
    \ for several signature lengths K \u2217 . (a) Occurrence Pooling for order r=1,2,3\
    \ with Spatial Coordinate Coding (SCC); () denotes r=2 without SCC. (b , c) BoW\
    \ with order r=2 compared to SPM ( r=1) , DoPM ( r=1) , FV as well as VLAT."
  Figure 7 Link: articels_figures_by_rev_year\2016\HigherOrder_Occurrence_Pooling_for_BagsofWords_Visual_Concept_Detection\figure_7.jpg
  Figure 7 caption: Descriptor fusion with Second-order Occurrence Pooling. (a) baseline
    show results for SC, LLC, and LcSA coders ( r=2 , K=600 ) without fusion. Residual
    Descriptors from Section 3.3 were combined by cross-term (cf. Section 2.1), late
    fusion (cf. Section 3.1), and bi-modal fusion with Second-order Occurrence Pooling.
    FV, VLAT, and DoPM ( r=1 ) use early fusion of grey and colour SIFT.
  Figure 8 Link: articels_figures_by_rev_year\2016\HigherOrder_Occurrence_Pooling_for_BagsofWords_Visual_Concept_Detection\figure_8.jpg
  Figure 8 caption: Evaluation of proposed fusion schemes based on Second-order Occurrence
    Pooling on (a) ImageCLEF11 and (b) MITIndoors67. Plots include late, bi-modal,
    and multi-modal fusions of grey and colour SIFT components as well as Residual
    Descriptors for the linear and chi 2RBF kernels. FV and VLAT use early fusion
    for grey and colour components.
  Figure 9 Link: articels_figures_by_rev_year\2016\HigherOrder_Occurrence_Pooling_for_BagsofWords_Visual_Concept_Detection\figure_9.jpg
  Figure 9 caption: Second- and Third-order Occurrence Pooling for fusion of low-level
    descriptors. Grey SIFT (raw ) with no spatial cues was fused with SCC (raw+SCC),
    SPM (raw+SPM) [31], bi-modal fused SPM (raw+SPM ) and multi-modal fused SPM and
    DoPM (raw+SPM+DoPM). Our Second-order Occurrence Pooling ( r=2 ) of mid-level
    features uses Sparse Coding and Spatial Coordinate Coding (SC+SCC).
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Piotr Koniusz
  Name of the last author: Krystian Mikolajczyk
  Number of Figures: 11
  Number of Tables: 2
  Number of authors: 4
  Paper title: 'Higher-Order Occurrence Pooling for Bags-of-Words: Visual Concept
    Detection'
  Publication Date: 2016-03-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of the Datasets with Corresponding State-of-the-Art
      Results, as Well as Experimental Settings for the Results Presented in This
      Section
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Summary of the Best Results from This Study
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2545667
- Affiliation of the first author: "department of computer science, technion\u2014\
    the israel institute of technology, technion city, haifa, israel"
  Affiliation of the last author: department of information systems, university of
    haifa, carmel mount, rabin building, haifa, israel
  Figure 1 Link: articels_figures_by_rev_year\2016\On_the_Equivalence_of_the_LCKSVD_and_the_DKSVD_Algorithms\figure_1.jpg
  Figure 1 caption: The runtime of LC-KSVD compared to D-KSVD for dictionaries of
    various size.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.82
  Name of the first author: Igor Kviatkovsky
  Name of the last author: Ilan Shimshoni
  Number of Figures: 1
  Number of Tables: 0
  Number of authors: 4
  Paper title: On the Equivalence of the LC-KSVD and the D-KSVD Algorithms
  Publication Date: 2016-03-23 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2545661
- Affiliation of the first author: key laboratory of pattern recognition, beijing
    academy of science and technology, beijing, china
  Affiliation of the last author: school of computer science and engineering, hebei
    university of technology, tianjin, china
  Figure 1 Link: articels_figures_by_rev_year\2016\Probabilistic_Model_for_Robust_Affine_and_NonRigid_Point_Set_Matching\figure_1.jpg
  Figure 1 caption: Illustration of point set matching. Outliers are represented by
    blue and red '+'s for the model (blue 'o') and the scene (red 'o'), respectively.
  Figure 10 Link: articels_figures_by_rev_year\2016\Probabilistic_Model_for_Robust_Affine_and_NonRigid_Point_Set_Matching\figure_10.jpg
  Figure 10 caption: 'Performance of the non-rigid matching algorithms on the CMU
    house data set. Left Column: The number of separation frames is 80 for different
    outlier ratios. Right Column: The number of separation frames is 100 for different
    outlier ratios.'
  Figure 2 Link: articels_figures_by_rev_year\2016\Probabilistic_Model_for_Robust_Affine_and_NonRigid_Point_Set_Matching\figure_2.jpg
  Figure 2 caption: Directed acyclic graph representing the probabilistic point set
    matching. The upper part is a subphase of the regression, and the bottom part
    is a subphase of the clustering.
  Figure 3 Link: articels_figures_by_rev_year\2016\Probabilistic_Model_for_Robust_Affine_and_NonRigid_Point_Set_Matching\figure_3.jpg
  Figure 3 caption: "The influence of the prior on the matching results. The model\
    \ and scene points are blue '+' and red 'o', respectively. q(X) is represented\
    \ by blue ellipses. Upper row: From left to right: Initialization and matching\
    \ results from W 0 =1\xD7I and W 0 =20\xD7I at \u03B8=20 \xB0. Bottom row: From\
    \ left to right: Initialization and matching results from W 0 =1\xD7I and W 0\
    \ =20\xD7I at \u03B8=40 \xB0."
  Figure 4 Link: articels_figures_by_rev_year\2016\Probabilistic_Model_for_Robust_Affine_and_NonRigid_Point_Set_Matching\figure_4.jpg
  Figure 4 caption: 'The influence of isotropic and anisotropic covariance. Top Row:
    Accuracy comparison for GMM-L2, CPD, isotropic and anisotropic VBPSM. Middle Row:
    Matching result of the isotropic VBPSM for 80 percent of missing points in model.
    Bottom Row: Matching result of the anisotropic VBPSM for 80 percent of missing
    points in model. The ellipses and circles with ''+'' are approximate transition
    mixtures q(X) with a 95 percent confidence interval.'
  Figure 5 Link: articels_figures_by_rev_year\2016\Probabilistic_Model_for_Robust_Affine_and_NonRigid_Point_Set_Matching\figure_5.jpg
  Figure 5 caption: "The matching results from the VBPSM on various data sets. Top\
    \ Row: Initialization with model (green '+'), scene (black 'o') and outliers (gray\
    \ 'o'). Bottom Row: Approximate posteriors of p(S|M) with mean and covariance.\
    \ The mixing proportion q(\u03C0) is expressed with colored contours in terms\
    \ of each individual mixture components."
  Figure 6 Link: articels_figures_by_rev_year\2016\Probabilistic_Model_for_Robust_Affine_and_NonRigid_Point_Set_Matching\figure_6.jpg
  Figure 6 caption: "The evolution of VBPSM in terms of point set matching. (a) Initialization.\
    \ The model points are green '+'s. The true shape and outliers in the scene are\
    \ represented by black and gray 'o's, respectively. (b), (c) and (d) The coarse-to-fine\
    \ approximations of p(S|M) in terms of different iterations. The colored ellipses\
    \ are approximate mixtures of X and Y with a 95 percent confidence interval, and\
    \ q(\u03C0) is colored based on the mixture proportion."
  Figure 7 Link: articels_figures_by_rev_year\2016\Probabilistic_Model_for_Robust_Affine_and_NonRigid_Point_Set_Matching\figure_7.jpg
  Figure 7 caption: 'Performance comparison of the GMM-L2 [20], CPD [22] and VBPSM
    methods using data with missing points. Top Row : Initialization with the model
    (green ''+'') and the scene (black ''o''). Row 2-4: Matching results from GMM-L2,
    CPD and VBPSM.'
  Figure 8 Link: articels_figures_by_rev_year\2016\Probabilistic_Model_for_Robust_Affine_and_NonRigid_Point_Set_Matching\figure_8.jpg
  Figure 8 caption: 'Performance comparison of KC, GMM-L2, CPD and VBPSM on data sets
    with multiple clusters of outliers for different standard deviations. Left Column:
    data sets with three outlier clusters and Right Column : data sets with four outlier
    clusters. c is the number of outlier clusters, and w 1 and w 2 are the outlier-to-data
    ratio in the model and scene , respectively.'
  Figure 9 Link: articels_figures_by_rev_year\2016\Probabilistic_Model_for_Robust_Affine_and_NonRigid_Point_Set_Matching\figure_9.jpg
  Figure 9 caption: "Performance comparison for non-rigid matching problems for different\
    \ outlier levels. Top Row: Matching results from VBPSM. The mixing proportion\
    \ q(\u03C0) is colored according to the estimated value. The model and scene points\
    \ are green '+'s and gray 'o's, respectively. Bottom Row: Boxplot comparisons\
    \ in terms of outlier-to-data ratio for uniform and normal outliers."
  First author gender probability: 0.72
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.75
  Name of the first author: Han-Bing Qu
  Name of the last author: Ming Yu
  Number of Figures: 14
  Number of Tables: 1
  Number of authors: 4
  Paper title: Probabilistic Model for Robust Affine and Non-Rigid Point Set Matching
  Publication Date: 2016-03-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Comparison of Computation Times
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2545659
