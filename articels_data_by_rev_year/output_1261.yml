- Affiliation of the first author: microsoft research, cambridge, united kingdom
  Affiliation of the last author: university of california, irvine, irvine, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\Advances_in_Variational_Inference\figure_1.jpg
  Figure 1 caption: A graphical model of the observations boldsymbol x that depend
    on underlying local hidden factors boldsymbol xi and global parameters theta .
    We use boldsymbol z=lbrace theta, boldsymbol xi rbrace to represent all latent
    variables. M is the number of the data points. N is the number of the latent variables.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Advances_in_Variational_Inference\figure_2.jpg
  Figure 2 caption: The graphical model class compatible with stochastic variational
    inference (a), and a graphical visualization of the variational autoencoder (b).
    Dashed lines indicate a variational model that conditions on input data x.
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.75
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Cheng Zhang
  Name of the last author: Stephan Mandt
  Number of Figures: 2
  Number of Tables: 0
  Number of authors: 4
  Paper title: Advances in Variational Inference
  Publication Date: 2018-12-25 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2889774
- Affiliation of the first author: department of engineering science, visual geometry
    group, university of oxford, oxford, united kingdom
  Affiliation of the last author: department of engineering science, visual geometry
    group, university of oxford, oxford, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2018\Automated_Video_Face_Labelling_for_Films_and_TV_Material\figure_1.jpg
  Figure 1 caption: "Results of the automatic character naming using the \u201CCasablanca\u201D\
    \ dataset provided by Bojanowski et al. [2]. In addition to the baseline curve\
    \ of [2] the other curves are: (i) baseline [2] with our explicit background character\
    \ modelling; (ii) using features of [2] with our learning framework and bag formation;\
    \ (iii) adding explicit modelling of background characters, and (iv) changing\
    \ the features to our ConvNet (CNN) based face descriptors and adding liner programming\
    \ label selection."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Automated_Video_Face_Labelling_for_Films_and_TV_Material\figure_2.jpg
  Figure 2 caption: "Label bag formation for the principal characters: Top part: face\
    \ tracks ( t i , Blue Lines) and their ground truth (Names and Pictures) and associated\
    \ speakingnon-speaking label. The boundaries of non-speaking tracks are extended\
    \ to provide better coverage. The aligned subtitles ( S i , Dotted Red Lines)\
    \ are used to form the bags as follows: (a) each subtitle forms a bag containing\
    \ all overlapping tracks (\u201CPositive Subtitle Bags\u201D). (b) These bags\
    \ determine the possible labels for each track (\u201CAcquired labels\u201D).\
    \ (c) Track with given possible labels forms negative example for all other track\
    \ not containing those labels in their bags (\u201CNegative bags\u201D)."
  Figure 3 Link: articels_figures_by_rev_year\2018\Automated_Video_Face_Labelling_for_Films_and_TV_Material\figure_3.jpg
  Figure 3 caption: 'Examples of background characters: Each row shows three frames
    of a track. One can observe that background characters have typical characteristics:
    the scale of the face, the motion through the shot, and their location. These
    characteristics assist in identifying them.'
  Figure 4 Link: articels_figures_by_rev_year\2018\Automated_Video_Face_Labelling_for_Films_and_TV_Material\figure_4.jpg
  Figure 4 caption: 'Background classifier performance: (a) The precision-recall curve
    obtained by training on the Scrubs dataset and testing on the Casablanca dataset,
    and (b) Examples of high scoring detections from the test set.'
  Figure 5 Link: articels_figures_by_rev_year\2018\Automated_Video_Face_Labelling_for_Films_and_TV_Material\figure_5.jpg
  Figure 5 caption: "Per character AP results on Casablanca using our best method\
    \ and [2]. Characters are ordered according to their frequency of occurrence.\
    \ Note the significant improvement for all categories, including \u201CBG\u201D\
    \ (background)."
  Figure 6 Link: articels_figures_by_rev_year\2018\Automated_Video_Face_Labelling_for_Films_and_TV_Material\figure_6.jpg
  Figure 6 caption: Examples of the remaining errors in the Buffy dataset. The first
    row illustrates a false positive face detection included in the dataset. These
    were the most common cause for the errors. The second row shows a case, where
    the actual speaker (Giles) was not included in the face detections resulting in
    a faulty label bag. The third row, illustrates an example of a particularly difficult
    face track, which was misclassified by our approach.
  Figure 7 Link: articels_figures_by_rev_year\2018\Automated_Video_Face_Labelling_for_Films_and_TV_Material\figure_7.jpg
  Figure 7 caption: Examples of the highest scoring misclassified faces in the Sherlock
    episode 1. Each row shows one face track and in all cases the correct labelling
    is background character. The first row illustrates a difficult face track acquired
    in dark conditions. The second and third illustrate cases, where our background
    character does not work well since tracks appear similar to the main character
    tracks. In all cases, the classification certainty is really low.
  Figure 8 Link: articels_figures_by_rev_year\2018\Automated_Video_Face_Labelling_for_Films_and_TV_Material\figure_8.jpg
  Figure 8 caption: Precision-recall curves for three Sherlock episodes and Casablanca.
    The average precision values are indicated in the corresponding legend.
  Figure 9 Link: articels_figures_by_rev_year\2018\Automated_Video_Face_Labelling_for_Films_and_TV_Material\figure_9.jpg
  Figure 9 caption: 'Performance of the face track classifiers: The precision recall
    curve for (a) testing on the Sherlock dataset (RGB track classifier); (b) testing
    on the Casablanca dataset (B&W track classifier).'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.88
  Name of the first author: Omkar M. Parkhi
  Name of the last author: Qiong Cao
  Number of Figures: 9
  Number of Tables: 9
  Number of authors: 3
  Paper title: Automated Video Face Labelling for Films and TV Material
  Publication Date: 2018-12-27 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Dataset Statistics: The Total Number of Tracks in Each Video
      (The Number of Background Character Tracks Shown in Brackets)'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Comparison of Different Bag Formation Strategies: The Average
      Precision Values Obtained using Different Bag Formation Strategies, on the Same
      Set of Tracks and with the Same Features in Each Case'
  Table 3 caption:
    table_text: 'TABLE 3 Evaluation of Bag Properties with Different Formation Strategies:
      For the Positive Bags, We Show the Total Number of Bags Generated over All Characters,
      the Average Size of an Individual Bag, and the Proportion of Bags That Actually
      Contain the Indicated Character'
  Table 4 caption:
    table_text: TABLE 4 Contributions of the Different Components of the Algorithm
      on Casablanca Dataset :The Average Precision Results for the Casablanca Dataset
      Using Different Versions of Our Method and the Baseline [2] with and without
      Our Background Character Modelling
  Table 5 caption:
    table_text: 'TABLE 5 Comparison with State of the Art (TV Series): The Average
      Precision (AP) Values for Each of the Tested Episode in the Buffy and Big Bang
      Theory Datasets'
  Table 6 caption:
    table_text: 'TABLE 6 Comparison with State of the Art (Movies): The Average Precision
      Values for Different Methods on the Casablanca Dataset'
  Table 7 caption:
    table_text: "TABLE 7 Dataset Statistics for Sherlock and Casablanca: \u201CGT\u201D\
      \ Means Face Tracks Provided by the Tracker and Manual Removal of Non-Face Tracks;\
      \ \u201CAuto\u201D Means Face Tracks Provided by the Tracker and Track Classifier"
  Table 8 caption:
    table_text: TABLE 8 Coverage Rates for the New Sherlock and Casablanca Datasets
  Table 9 caption:
    table_text: "TABLE 9 Average Precision Values for the \u201CRAW\u201D Experiment\
      \ Using the New Sherlock Face Tracks"
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2889831
- Affiliation of the first author: department of electrical and computer engineering,
    university of waterloo, waterloo, canada
  Affiliation of the last author: department of computing, the hong kong polytechnic
    university, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2018\Group_Maximum_Differentiation_Competition_Model_Comparison_with_Few_Samples\figure_1.jpg
  Figure 1 caption: A toy example of the gMAD competition. (a) Predictions by Model
    I and Model II. (b) Plot of Model II against Model I. (A, B) and (C, D) are the
    gMAD sample pairs subject to physical measurement.
  Figure 10 Link: articels_figures_by_rev_year\2018\Group_Maximum_Differentiation_Competition_Model_Comparison_with_Few_Samples\figure_10.jpg
  Figure 10 caption: 'Sample frames from the proposed streaming video database for
    the gMAD competition of QoE models. (a) YellowStone: natural, high motion. (b)
    StreetDance: outdoor, high motion. (c) SplitTrailer: human, high motion. (d) CSGO:
    animation, high motion. (e) UCLY: indoor, slow motion. (f) WildAnimal: animal,
    slow motion. (g) Rose: plant, slow motion. (h) Food: still-life, slow motion.'
  Figure 2 Link: articels_figures_by_rev_year\2018\Group_Maximum_Differentiation_Competition_Model_Comparison_with_Few_Samples\figure_2.jpg
  Figure 2 caption: Sample images in [19]. (a) Human. (b) Animal. (c) Plant. (d) Landscape.
    (e) Cityscape. (f) Still-life. (g) Transportation.
  Figure 3 Link: articels_figures_by_rev_year\2018\Group_Maximum_Differentiation_Competition_Model_Comparison_with_Few_Samples\figure_3.jpg
  Figure 3 caption: gMAD image pairs from the Waterloo Exploration Database [19].
    The image pair (A,B) is selected by maximizingminimizing SSIM while holding MS-SSIM
    fixed. Similarly, (C,D) is selected by maximizingminimizing MS-SSIM while holding
    SSIM fixed.
  Figure 4 Link: articels_figures_by_rev_year\2018\Group_Maximum_Differentiation_Competition_Model_Comparison_with_Few_Samples\figure_4.jpg
  Figure 4 caption: gMAD image pairs between SSIM and MS-SSIM. Images (a)-(d) correspond
    to points A - D in Fig. 3. (a) MS-SSIM = 30, SSIM = 53. (b) MS-SSIM = 30, SSIM
    = 13. (c) SSIM = 30, MS-SSIM = 78. (d) SSIM = 30, MS-SSIM = 13.
  Figure 5 Link: articels_figures_by_rev_year\2018\Group_Maximum_Differentiation_Competition_Model_Comparison_with_Few_Samples\figure_5.jpg
  Figure 5 caption: User interface for subjective testing.
  Figure 6 Link: articels_figures_by_rev_year\2018\Group_Maximum_Differentiation_Competition_Model_Comparison_with_Few_Samples\figure_6.jpg
  Figure 6 caption: Pairwise comparison results of the 16 IQA models. (a) Aggressiveness
    matrix. (b) Resistance matrix. Each entry indicates the aggressivenessresistance
    of the row model against the column model. mathbf A -mathbf AT and mathbf R -mathbf
    RT are drawn here for better visibility.
  Figure 7 Link: articels_figures_by_rev_year\2018\Group_Maximum_Differentiation_Competition_Model_Comparison_with_Few_Samples\figure_7.jpg
  Figure 7 caption: Global ranking results of the 16 IQA models.
  Figure 8 Link: articels_figures_by_rev_year\2018\Group_Maximum_Differentiation_Competition_Model_Comparison_with_Few_Samples\figure_8.jpg
  Figure 8 caption: Sample images from ImageNet [45] used for the gMAD competition
    of image aesthetics models. (a)-(h) Images with increasing degrees of perceived
    aesthetics according to our subjective testing.
  Figure 9 Link: articels_figures_by_rev_year\2018\Group_Maximum_Differentiation_Competition_Model_Comparison_with_Few_Samples\figure_9.jpg
  Figure 9 caption: gMAD competition between Jin16 [49] and Kong16 [48] at the high-aesthetics
    level. (a) Best Jin16 for fixed Kong16. (b) Worst Jin16 for fixed Kong16. (c)
    Best Kong16 for fixed Jin16. (d) Worst Kong16 for fixed Jin16.
  First author gender probability: 0.97
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.55
  Name of the first author: Kede Ma
  Name of the last author: Lei Zhang
  Number of Figures: 11
  Number of Tables: 6
  Number of authors: 8
  Paper title: 'Group Maximum Differentiation Competition: Model Comparison with Few
    Samples'
  Publication Date: 2018-12-27 00:00:00
  Table 1 caption:
    table_text: TABLE 1 SRCC Results between K=6 K=6 as the Reference and Other K
      K Values
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Pairwise Comparison Results of Image Aesthetics Models in
      the gMAD Competition
  Table 3 caption:
    table_text: TABLE 3 Global Ranking Results of Image Aesthetics Models in gMAD
  Table 4 caption:
    table_text: 'TABLE 4 Encoding Ladder of Video Clips. kbps: kB per Second'
  Table 5 caption:
    table_text: TABLE 5 Pairwise Comparison Results of QoE Models in the gMAD Competition
  Table 6 caption:
    table_text: TABLE 6 Global Ranking Results of QoE Models in the gMAD Competition
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2889948
- Affiliation of the first author: department of national laboratory of pattern recognition,
    institute of automation, chinese academy of sciences, beijing, china
  Affiliation of the last author: department of national laboratory of pattern recognition,
    institute of automation, chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2018\Deep_SelfEvolution_Clustering\figure_1.jpg
  Figure 1 caption: The motivation of DSEC. In the figure, the green and blue lines
    signify that pairwise patterns are similar or dissimilar, respectively. Intuitively,
    DSEC handles the clustering task by investigating similarities between pairwise
    patterns, i.e., grouping similar patterns into the same clusters and dissimilar
    patterns into different clusters. For this purpose, a group of neurons that represent
    a specific concept with a constant neuron are introduced in the DSEC model to
    indicate clustering labels of patterns directly.
  Figure 10 Link: articels_figures_by_rev_year\2018\Deep_SelfEvolution_Clustering\figure_10.jpg
  Figure 10 caption: Filters learned in the first convolutional layers based on (a)
    the developed DSEC model and (b) the supervised learning.
  Figure 2 Link: articels_figures_by_rev_year\2018\Deep_SelfEvolution_Clustering\figure_2.jpg
  Figure 2 caption: The flowchart of DSEC. Specifically, DSEC consists of two essential
    parts, i.e., learning and clustering. In the learning part, DSEC measures similarities
    between pairwise patterns via dot product between indicator features generated
    by a DNN. Since the ground-truth similarities are unknown in clustering, similar
    and dissimilar pairwise patterns with high likelihood are gradually selected to
    train the DNN. That is, as the learning procedure progresses, more pairwise patterns
    are appended for training until all pairwise patterns are included. In the clustering
    part, conclusively, DSEC clusters patterns by locating the largest activation
    of the indicator features generated by the trained DNN.
  Figure 3 Link: articels_figures_by_rev_year\2018\Deep_SelfEvolution_Clustering\figure_3.jpg
  Figure 3 caption: The search spaces of the indicator features in the 2-dimensional
    space under different clustering constraints. (a) Clustering constraint C . (b)
    Clustering constraints C 0.5 , C 1.0 and C 2.0 .
  Figure 4 Link: articels_figures_by_rev_year\2018\Deep_SelfEvolution_Clustering\figure_4.jpg
  Figure 4 caption: The clustering results of DSEC on the experimental datasets. For
    each dataset, the names of the datasets are written on the upward side, the clustering
    results in the initial, intermediate and final stages are orderly illustrated
    in the top, intermediate and bottom lines. In addition, the clustering accuracies
    at the different stages are listed under the results. Large figures can be found
    in the supplement, available online.
  Figure 5 Link: articels_figures_by_rev_year\2018\Deep_SelfEvolution_Clustering\figure_5.jpg
  Figure 5 caption: The indicator features of the first ten clusters on MNIST, ImageNet-10,
    20NEWS and AudioSet-20. For each dataset, ground-truth labels are written on the
    upward side, indicator features are shown on the right side of examples, followed
    by typical failures at the bottom.
  Figure 6 Link: articels_figures_by_rev_year\2018\Deep_SelfEvolution_Clustering\figure_6.jpg
  Figure 6 caption: The precisions of the selected similar (left) or dissimilar (right)
    pairwise patterns in different stages of clustering.
  Figure 7 Link: articels_figures_by_rev_year\2018\Deep_SelfEvolution_Clustering\figure_7.jpg
  Figure 7 caption: The clustering accuracies of the compared methods with various
    scenarios. The dashed lines represent the results of DSEC-SECT. The comparison
    of the clustering results (a) with the increasing number of clusters on the ILSVRC2012-1K
    dataset (1300 images per cluster), (b) with the increasing number of patterns
    on the CIFAR-10 dataset, and (c) on the imbalanced datasets sampled from MNIST.
  Figure 8 Link: articels_figures_by_rev_year\2018\Deep_SelfEvolution_Clustering\figure_8.jpg
  Figure 8 caption: (a) The distributions of the elements in the generated indicator
    features in the initial and final stages on MNIST and ImageNet-10. (b) The contributions
    of the pre-training techniques on DSEC. Triangles with different colors indicate
    various initial states. Specifically, DSEC+ S(n) represents that n labeled patterns
    are utilized to pre-train the experimental networks in a supervised manner. (c)
    The influence of network configurations on DSEC. Specifically, the three popular
    variants of VGG [34], i.e., VGG-13, VGG-16 and VGG-19, are compared.
  Figure 9 Link: articels_figures_by_rev_year\2018\Deep_SelfEvolution_Clustering\figure_9.jpg
  Figure 9 caption: (a) DSEC with diverse initial values of l and u . (b) DSEC with
    diverse clustering constraints. Specifically, Null1 means that the clustering
    constraint is omitted and our label inference is used for clustering, and Null2
    indicates that the clustering constraint is omitted and K-means is used for clustering.
    (c) The contribution of pre-training. Specifically, L(n) signifies that n labeled
    patterns are used to train networks from randomly initialized status, and hatL(n)
    means that n labeled patterns are utilized to fine-tune the networks pre-trained
    by DSEC.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.78
  Name of the first author: Jianlong Chang
  Name of the last author: Chunhong Pan
  Number of Figures: 10
  Number of Tables: 3
  Number of authors: 4
  Paper title: Deep Self-Evolution Clustering
  Publication Date: 2018-12-27 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Datasets Utilized in the Experiments
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Clustering Results of the Compared Methods on the Experimental
      Datasets
  Table 3 caption:
    table_text: TABLE 3 The Comparison of the Clustering Results on the Experimental
      Datasets with Various Scenarios
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2889949
- Affiliation of the first author: samsung ai center, moscow, russia
  Affiliation of the last author: nizhny novgorod, russia
  Figure 1 Link: articels_figures_by_rev_year\2018\Efficient_and_Robust_Approximate_Nearest_Neighbor_Search_Using_Hierarchical_Navi\figure_1.jpg
  Figure 1 caption: Illustration of the Hierarchical NSW idea. The search starts from
    an element from the top layer (shown red). Red arrows show direction of the greedy
    algorithm from the entry point to the query (shown green).
  Figure 10 Link: articels_figures_by_rev_year\2018\Efficient_and_Robust_Approximate_Nearest_Neighbor_Search_Using_Hierarchical_Navi\figure_10.jpg
  Figure 10 caption: Plots of the query time versus construction time tradeoff for
    Hierarchical NSW on 10M SIFT dataset.
  Figure 2 Link: articels_figures_by_rev_year\2018\Efficient_and_Robust_Approximate_Nearest_Neighbor_Search_Using_Hierarchical_Navi\figure_2.jpg
  Figure 2 caption: Illustration of the heuristic used to select the graph neighbors
    for two isolated clusters. A new element is inserted on the boundary of Cluster
    1. All of the closest neighbors of the element belong to the Cluster 1, thus missing
    the edges of Delaunay graph between the clusters. The heuristic, however, selects
    element e 0 from Cluster 2, thus, maintaining the global connectivity in case
    the inserted element is the closest to e 0 compared to any other element from
    Cluster 1.
  Figure 3 Link: articels_figures_by_rev_year\2018\Efficient_and_Robust_Approximate_Nearest_Neighbor_Search_Using_Hierarchical_Navi\figure_3.jpg
  Figure 3 caption: Plots for query time versus m L parameter for 10M (10 million)
    random vectors with d=4 . The autoselected value 1ln(M) for m L is shown by an
    arrow.
  Figure 4 Link: articels_figures_by_rev_year\2018\Efficient_and_Robust_Approximate_Nearest_Neighbor_Search_Using_Hierarchical_Navi\figure_4.jpg
  Figure 4 caption: Plots for query time versus m L parameter for 100k (100 thousand)
    random vectors with d=1024 . The autoselected value 1ln(M) for m L is shown by
    an arrow.
  Figure 5 Link: articels_figures_by_rev_year\2018\Efficient_and_Robust_Approximate_Nearest_Neighbor_Search_Using_Hierarchical_Navi\figure_5.jpg
  Figure 5 caption: Plots for query time versus m L parameter for 5M SIFT learn dataset.
    The autoselected value 1ln(M) for m L is shown by an arrow.
  Figure 6 Link: articels_figures_by_rev_year\2018\Efficient_and_Robust_Approximate_Nearest_Neighbor_Search_Using_Hierarchical_Navi\figure_6.jpg
  Figure 6 caption: "Plots for query time versus M max0 parameter for 5M SIFT learn\
    \ dataset. The autoselected value 2\u22C5M for M max0 is shown by an arrow."
  Figure 7 Link: articels_figures_by_rev_year\2018\Efficient_and_Robust_Approximate_Nearest_Neighbor_Search_Using_Hierarchical_Navi\figure_7.jpg
  Figure 7 caption: Effect of the method of neighbor selections (baseline corresponds
    to Algorithm 3, heuristic to Algorithm 4) on clustered (100 random isolated clusters)
    and non-clustered d = 10 random vector data.
  Figure 8 Link: articels_figures_by_rev_year\2018\Efficient_and_Robust_Approximate_Nearest_Neighbor_Search_Using_Hierarchical_Navi\figure_8.jpg
  Figure 8 caption: Plots for recall error versus query time for different parameters
    of M for Hierarchical NSW on 5M SIFT learn dataset.
  Figure 9 Link: articels_figures_by_rev_year\2018\Efficient_and_Robust_Approximate_Nearest_Neighbor_Search_Using_Hierarchical_Navi\figure_9.jpg
  Figure 9 caption: Construction time for Hierarchical NSW on 10m SIFT dataset for
    different numbers of threads on two CPU systems.
  First author gender probability: 0.53
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yu A. Malkov
  Name of the last author: D. A. Yashunin
  Number of Figures: 15
  Number of Tables: 3
  Number of authors: 2
  Paper title: Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical
    Navigable Small World Graphs
  Publication Date: 2018-12-28 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Parameters of the Used Datasets on Vector Spaces Benchmark
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Used Datasets for Repetition of the Non-Metric Data Tests
      Subset
  Table 3 caption:
    table_text: TABLE 3 Parameters for Comparison Between Hierarchical NSW and Faiss
      on a 200M Subset of 1B SIFT Dataset
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2889473
- Affiliation of the first author: inria sophia antipolis, valbonne, france
  Affiliation of the last author: inria sophia antipolis, valbonne, france
  Figure 1 Link: articels_figures_by_rev_year\2019\Extracting_Geometric_Structures_in_Images_with_Delaunay_Point_Processes\figure_1.jpg
  Figure 1 caption: Example applications of Delaunay Point Processes to extract planar
    graphs representing blood vessels in retina images (left), and complex polygons
    representing object silhouettes (right). The point distribution creates a dynamic
    Delaunay triangulation while edge and facet labels specify the geometric structure
    (see red edges on close-ups).
  Figure 10 Link: articels_figures_by_rev_year\2019\Extracting_Geometric_Structures_in_Images_with_Delaunay_Point_Processes\figure_10.jpg
  Figure 10 caption: Visual comparisons with existing point processes. Marked point
    process [30] produces configurations of mostly disconnected line-segments. Junction-point
    process [11] better preserves the connectivity of edges but recover badly complex
    junctions of at least four branches (see junctions in tiles). Our Delaunay point
    process exhibits better connectivity and accuracy for both cyclic (bottom) and
    acyclic (top) line-networks.
  Figure 2 Link: articels_figures_by_rev_year\2019\Extracting_Geometric_Structures_in_Images_with_Delaunay_Point_Processes\figure_2.jpg
  Figure 2 caption: Point processes distribute points randomly in a bounded domain.
    While the left example illustrates a uniform distribution, the middle and right
    examples show point processes guided by a non-uniform density h (top left insets).
    In particular, the right example uses the image gradient magnitude as a density
    to distribute points along image contours.
  Figure 3 Link: articels_figures_by_rev_year\2019\Extracting_Geometric_Structures_in_Images_with_Delaunay_Point_Processes\figure_3.jpg
  Figure 3 caption: "Markovian point processes. Traditional point processes exploit\
    \ the Markovian property to define pairs of interacting points, typically a maximal\
    \ euclidean distance \u03F5 between two points (left). Such processes are used\
    \ for detecting objects in images by associating a simple geometric shape, eg\
    \ a rectangle [10], to each point (middle), and for extracting line-networks by\
    \ selecting a subset of pairs of interacting points [11] (right)."
  Figure 4 Link: articels_figures_by_rev_year\2019\Extracting_Geometric_Structures_in_Images_with_Delaunay_Point_Processes\figure_4.jpg
  Figure 4 caption: "Delaunay Point Processes. Contrary to traditional point processes,\
    \ pairs of interacting points are defined more naturally by a Delaunay triangulation\
    \ instead of an arbitrary distance parameter \u03F5 (left). Exploiting such a\
    \ geometric meta-structure allows us to partition the image domain into complex\
    \ polygons by jointly labeling the triangles (middle) or to extract planar graphs\
    \ by jointly labeling the edges (right). We add the four corner points of Domain\
    \ K to p for computing the Delaunay triangulation so that K is entirely partition\
    \ by triangles."
  Figure 5 Link: articels_figures_by_rev_year\2019\Extracting_Geometric_Structures_in_Images_with_Delaunay_Point_Processes\figure_5.jpg
  Figure 5 caption: Birth and Death kernel. A birth inserts a new vertex in the triangulation
    and recomputes the edge connectivity around it by applying edge flips recursively
    until the circumcirle condition is valid everywhere. A death removes a vertex
    and its adjacent edges and reconnect its adjacent vertices so that the circumcirle
    condition is valid.
  Figure 6 Link: articels_figures_by_rev_year\2019\Extracting_Geometric_Structures_in_Images_with_Delaunay_Point_Processes\figure_6.jpg
  Figure 6 caption: Safety domain for point relocation. The blue region (left) corresponds
    to the region where the red vertex can move without entering the circumcirle of
    another triangle, i.e., without flipping the blue edges. The red region (middle)
    corresponds to the region where the red vertex can move without leaving the circumcirle
    of any three successive adjacent vertices, i.e., without flipping the red edges.
    The safety domain, drawn with a black border (right), is the intersection of the
    blue and red regions. In this example, the blue region lies entirely inside the
    red region, although this is not true in the general case.
  Figure 7 Link: articels_figures_by_rev_year\2019\Extracting_Geometric_Structures_in_Images_with_Delaunay_Point_Processes\figure_7.jpg
  Figure 7 caption: Energy decrease with different combinations of kernels. A better
    energy is reached with our combination of three kernels than with just a birth
    and death kernel or with the three kernels with uniform birth.
  Figure 8 Link: articels_figures_by_rev_year\2019\Extracting_Geometric_Structures_in_Images_with_Delaunay_Point_Processes\figure_8.jpg
  Figure 8 caption: Vectorization of line-drawings. Our Delaunay point process recovers
    the center-line of sketchy pen strokes in bitmap line drawings (active edges shown
    in red). The insets shows that our model produces well-connected structures even
    in the presence of multiple overlapping strokes and complex regular patterns.
  Figure 9 Link: articels_figures_by_rev_year\2019\Extracting_Geometric_Structures_in_Images_with_Delaunay_Point_Processes\figure_9.jpg
  Figure 9 caption: Comparison with a two-steps optimization for line drawing vectorization.
    When sampling points independently of the marks (middle), edges of the Delaunay
    triangulation often miss important line junctions, which cannot be recovered by
    a subsequent marking step. Our Delaunay point process samples points and marks
    jointly, which favors the emergence of a well-connected line network (right).
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: Jean-Dominique Favreau
  Name of the last author: Alex Auvolat
  Number of Figures: 17
  Number of Tables: 3
  Number of authors: 4
  Paper title: Extracting Geometric Structures in Images with Delaunay Point Processes
  Publication Date: 2019-01-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Comparisons with Existing Point Processes
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Evaluation on BSDS500
  Table 3 caption:
    table_text: TABLE 3 Quantitative Evaluation on HKU-IS Dataset
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2890586
- Affiliation of the first author: key laboratory of intelligent information processing,
    institute of computing technology, chinese academy of sciences, beijing, china
  Affiliation of the last author: ai institute of qihoo 360, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2019\PerspectiveAdaptive_Convolutions_for_Scene_Parsing\figure_1.jpg
  Figure 1 caption: Illustration of the comparison between standard convolutions and
    perspective-adaptive convolutions. The standard convolutions have fixed-size receptive
    fields (b), which may lead to inconsistent predictions of large objects and invisibility
    of small objects (e). In comparison, the proposed perspective-adaptive convolutions
    learn receptive fields with flexible sizes and shapes (c), which can adaptively
    expand to cover large objects or shrink to focus on small objects with convex
    quadrangles of arbitrary shapes, so as to obtain preferable parsing predictions
    (f).
  Figure 10 Link: articels_figures_by_rev_year\2019\PerspectiveAdaptive_Convolutions_for_Scene_Parsing\figure_10.jpg
  Figure 10 caption: 'Visualization of perspective coefficient maps of PAC-single
    on Cityscapes validation set. From left to right are: image, perspective coefficient
    maps, PAC-single results, groundtruth. Row 1 and 2: the perspective structure
    of perspective coefficient maps. Row 3 and 4: large objects (marked by red boxes)
    have large perspective coefficients to expand the receptive fields, while small
    objects (marked by yellow boxes) have small perspective coefficients to shrink
    the receptive fields. Row 5 and 6: for large objects (marked by yellow boxes),
    the perspective coefficients associated with the center points are slightly larger,
    while the perspective coefficients associated with the points close to boundaries
    are slightly smaller.'
  Figure 2 Link: articels_figures_by_rev_year\2019\PerspectiveAdaptive_Convolutions_for_Scene_Parsing\figure_2.jpg
  Figure 2 caption: Overview of the proposed perspective-adaptive convolutions. We
    apply it to the last layer of CNNs for example. For each position (the red point
    as an example), the associated perspective coefficient vector learned from the
    perspective regression layer is employed to reshape the associated convolutional
    patch, so as to obtain a receptive field with flexible size and shape. Then feature
    vectors are sampled in a bilinear interpolation manner from the convolutional
    patch to perform element-wise multiplication with the kernels (adding with the
    bias is omitted in the figure).
  Figure 3 Link: articels_figures_by_rev_year\2019\PerspectiveAdaptive_Convolutions_for_Scene_Parsing\figure_3.jpg
  Figure 3 caption: "Illustration of the convolutional patches in different convolutions,\
    \ taking a 3 \xD7 3 ( k =1) convolution with dilation d = 2 for example: (a) the\
    \ convolutional patches in standard convolutions are fixed to square; (b) the\
    \ convolutional patches in scale-adaptive convolutions are also squares with flexible\
    \ sizes determined by scale coefficients; (c) the convolutional patches in perspective-adaptive\
    \ convolutions are arbitrary convex quadrangles with flexible sizes and shapes,\
    \ which are determined by perspective coefficient vectors through changing the\
    \ position of their four corners, each corner point is controlled with two components\
    \ of perspective coefficient vectors."
  Figure 4 Link: articels_figures_by_rev_year\2019\PerspectiveAdaptive_Convolutions_for_Scene_Parsing\figure_4.jpg
  Figure 4 caption: "Illustration of receptive fields with flexible sizes and shapes\
    \ in perspective-adaptive convolutions, taking a 3 \xD7 3 convolution with dilation\
    \ =4 for example: (a) when perspective coefficients are equal to 1, the perspective-adaptive\
    \ convolution degenerates to standard convolution; (b) when perspective coefficients\
    \ are smaller than 1, the convolutional patch shrinks so that the receptive field\
    \ is zoomed out; (c) when perspective coefficients are larger than 1, the convolutional\
    \ patch expands so that the receptive field is zoomed in; (d) the receptive field\
    \ is reshaped to rectangle; (e) the receptive field is reshaped to trapezoid;\
    \ (f) the receptive field is reshaped to convex quadrangle of arbitrary shape."
  Figure 5 Link: articels_figures_by_rev_year\2019\PerspectiveAdaptive_Convolutions_for_Scene_Parsing\figure_5.jpg
  Figure 5 caption: Structure of the proposed context adaptive bias. We apply it to
    the last layer of CNNs for example. For each position (the red point as an example),
    local and global contexts are collected from the convolutional results through
    average pooling on local patches and global feature maps. Then the corresponding
    attentive weights learned from the context regression layer are employed to provide
    position-adaptive and context-aware bias through flexible attentive summating.
  Figure 6 Link: articels_figures_by_rev_year\2019\PerspectiveAdaptive_Convolutions_for_Scene_Parsing\figure_6.jpg
  Figure 6 caption: 'Result illustration of perspective-adaptive convolutions on Cityscapes
    validation set. From top to bottom are: image, baseline, PAC-single, PAC-multiple,
    groundtruth. Consistent predictions for large objects and accurate predictions
    for small objects can be obtained from perspective-adaptive convolutions.'
  Figure 7 Link: articels_figures_by_rev_year\2019\PerspectiveAdaptive_Convolutions_for_Scene_Parsing\figure_7.jpg
  Figure 7 caption: 'Result illustration of context adaptive biases on Cityscapes
    validation set. From left to right are: image, before context adaptive biases,
    after context adaptive biases, groundtruth. Mislabeling of some regions can be
    corrected after context adaptive biases.'
  Figure 8 Link: articels_figures_by_rev_year\2019\PerspectiveAdaptive_Convolutions_for_Scene_Parsing\figure_8.jpg
  Figure 8 caption: 'Result illustration of perspective-adaptive convolutions on ADE20K
    validation set. From top to bottom are: image, baseline, PAC-single, PAC-multiple,
    groundtruth. Consistent predictions for large objects and accurate predictions
    for small objects can be obtained from perspective-adaptive convolutions.'
  Figure 9 Link: articels_figures_by_rev_year\2019\PerspectiveAdaptive_Convolutions_for_Scene_Parsing\figure_9.jpg
  Figure 9 caption: 'Result illustration of context adaptive biases on ADE20K validation
    set. From left to right are: image, before context adaptive biases, after context
    adaptive biases, groundtruth. Mislabeling of some regions can be corrected through
    context adaptive biases.'
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Rui Zhang
  Name of the last author: Shuicheng Yan
  Number of Figures: 11
  Number of Tables: 6
  Number of authors: 5
  Paper title: Perspective-Adaptive Convolutions for Scene Parsing
  Publication Date: 2019-01-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Evaluation Results of the Context Adaptive Biases (CAB) with
      Local Patches of Different Sizes m m
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Evaluation Results of the Proposed Methods on Cityscapes Validation
      Set
  Table 3 caption:
    table_text: TABLE 3 Comparison with Other State-of-the-Art Methods on Cityscapes
      Test Set
  Table 4 caption:
    table_text: TABLE 4 Evaluation Results of the Proposed Methods on ADE20K Validation
      Set, Compared with Other State-of-the-Art Methods
  Table 5 caption:
    table_text: "TABLE 5 Evaluation Results of the Proposed Methods Based on the \u201C\
      Model A, 1 Conv\u201D in [55] on Cityscapes Validation Set"
  Table 6 caption:
    table_text: TABLE 6 Evaluation Results of Different Convolutions on Cityscapes
      Validation Set, Evaluated with MAE of Height and Width Between the Bounding-Boxes
      and Receptive Fields of Different Convolutions
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2890637
- Affiliation of the first author: school of engineering, university of california,
    merced, ca, usa
  Affiliation of the last author: school of engineering, university of california,
    merced, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Joint_Image_Filtering_with_Deep_Convolutional_Networks\figure_1.jpg
  Figure 1 caption: Sample applications of joint image filtering. The targetguidance
    pair (top) can be various types of cross-modality visual data. With the help of
    the guidance image, important structures can be transferred to the degraded target
    image to help enhance the spatial resolution or suppress noises (bottom). The
    guidance image can either be high-resolution RGB images, images from different
    sensing modalities, or the target image itself.
  Figure 10 Link: articels_figures_by_rev_year\2019\Joint_Image_Filtering_with_Deep_Convolutional_Networks\figure_10.jpg
  Figure 10 caption: "Qualitative comparisons on depth upsampling. Comparisons against\
    \ existing depth upsampling algorithms for a scaling factor of 8\xD7. The numbers\
    \ (in centimeter) are the RMSE metric comparing against the GT in (b)."
  Figure 2 Link: articels_figures_by_rev_year\2019\Joint_Image_Filtering_with_Deep_Convolutional_Networks\figure_2.jpg
  Figure 2 caption: Network architecture for joint image filter. The proposed deep
    joint image filter model consists of three major components. Each component is
    a three-layer network. The sub-networks CN N T and CN N G aim to extract informative
    feature responses from the target and guidance images, respectively. We then concatenate
    these features responses together and use them as input for the network CN N F
    . In addition, we introduce a skip connection so that the network CN N F learns
    to predict the residuals between the input target image and the desired ground
    truth output. We train the network to selectively transfer main structures while
    suppressing inconsistent structures using an RGBdepth dataset. While we describe
    these sub-networks individually, the parameters of all three sub-networks are
    updated simultaneously during the training stage.
  Figure 3 Link: articels_figures_by_rev_year\2019\Joint_Image_Filtering_with_Deep_Convolutional_Networks\figure_3.jpg
  Figure 3 caption: "Comparison of network design. Joint depth upsampling (8\xD7)\
    \ results of using different network architectures. (a) GT depth map (inset: Guidance\
    \ image). (b) Bicubic upsampling. (c)-(e) Results from the straightforward implementation\
    \ using CN N F and CN N F R . (f) Our results. Note the difference on the bed\
    \ corner and curtain. The numbers are the RMSE metric based on the GT in (a)."
  Figure 4 Link: articels_figures_by_rev_year\2019\Joint_Image_Filtering_with_Deep_Convolutional_Networks\figure_4.jpg
  Figure 4 caption: "Comparison of different types of guidance. Joint depth upsampling\
    \ (8\xD7) results using different types of guidance images. Both (d) and (e) are\
    \ trained using the CN N F network. Our method generates sharper boundary of the\
    \ sculpture (left) and the cone (middle)."
  Figure 5 Link: articels_figures_by_rev_year\2019\Joint_Image_Filtering_with_Deep_Convolutional_Networks\figure_5.jpg
  Figure 5 caption: "Residual prediction. Joint depth upsampling results (8\xD7) of\
    \ using our network with a skip connection. The filtering output (c) is the summation\
    \ of (a) the target input and (b) the predicted output."
  Figure 6 Link: articels_figures_by_rev_year\2019\Joint_Image_Filtering_with_Deep_Convolutional_Networks\figure_6.jpg
  Figure 6 caption: "Effect of training data modalities. (a)-(d) Joint depth map upsampling\
    \ (8\xD7). The model trained with RGBflow data generates similar results when\
    \ compared with the model trained with RGBdepth data. (e)-(h) Joint flow map upsampling\
    \ (8\xD7). (g) The model trained with RGBdepth data and (h) The model trained\
    \ with RGBflow data. The numbers are the RMSE metric comparing against the GT."
  Figure 7 Link: articels_figures_by_rev_year\2019\Joint_Image_Filtering_with_Deep_Convolutional_Networks\figure_7.jpg
  Figure 7 caption: Visualization of feature responses. Sample feature responses of
    the input in Fig. 9(a) at the first layer of CN N T (top) and CN N G (middle),
    and the second layer of CN N F (bottom). For each subnetwork, we select five feature
    channels and visualize the responses through the colormap. The corresponding colorbar
    is shown in the rightmost. Note that with the help of CN N F , inconsistent structures
    (e.g., the window on the wall) are correctly suppressed.
  Figure 8 Link: articels_figures_by_rev_year\2019\Joint_Image_Filtering_with_Deep_Convolutional_Networks\figure_8.jpg
  Figure 8 caption: Selective transfer. Comparisons of different joint upsampling
    methods on handling the texture-copying issue. The carpet on the floor contains
    grid-like texture structures that may be incorrectly transferred to the target
    image. The numbers are the RMSE metric comparing against the GT.
  Figure 9 Link: articels_figures_by_rev_year\2019\Joint_Image_Filtering_with_Deep_Convolutional_Networks\figure_9.jpg
  Figure 9 caption: Visualization of the learned guidance map. Comparison between
    the learned guidance feature maps from CN N G and edge maps from [39]. The network
    CN N G is capable of extracting informative, salient structures from the guidance
    image for content transfer. Furthermore, with the skip connection, the learned
    guidance maps in (c) are cleaner than that in (b) by suppressing inconsistent
    structures (edges on the window and wall) in the targetguidance pair.
  First author gender probability: 0.67
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yijun Li
  Name of the last author: Ming-Hsuan Yang
  Number of Figures: 15
  Number of Tables: 9
  Number of authors: 4
  Paper title: Joint Image Filtering with Deep Convolutional Networks
  Publication Date: 2019-01-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Comparisons on Depth Upsampling
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Run-Time Performance Comparisons
  Table 3 caption:
    table_text: TABLE 3 Quantitative Comparisons of Different Upsampling Methods on
      Difference Solution Maps
  Table 4 caption:
    table_text: "TABLE 4 Quantitative Results (RMSE in Centimeters for 8\xD7) of Using\
      \ Different Filter Numbers in Each Sub-Network"
  Table 5 caption:
    table_text: "TABLE 5 Quantitative Results (RMSE in Centimeters for 8\xD7) of Using\
      \ Different Filter Numbers in the 3rd Layer of CN N T CNN T and CN N G CNN G"
  Table 6 caption:
    table_text: "TABLE 6 Quantitative Results (RMSE in Centimeters for 8\xD7) of Using\
      \ Different Filter Sizes in Each Sub-Network"
  Table 7 caption:
    table_text: "TABLE 7 Quantitative Evaluation (RMSE in Centimeters for 8\xD7) When\
      \ Using Residual-Based CN N F R CNN FR Only Under Different Network Depth d\
      \ d"
  Table 8 caption:
    table_text: TABLE 8 Quantitative Evaluation of Our Model by Increasing the Number
      of Layers (the Depth d d) Used in Each Subnetwork
  Table 9 caption:
    table_text: TABLE 9 Quantitative Evaluation of Different Combinations of Network
      Depth of CN N T CNN T ( CN N G CNN G) and CN N F CNN F
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2890623
- Affiliation of the first author: carnegie mellon university, pittsburgh, pa, usa
  Affiliation of the last author: carnegie mellon university, pittsburgh, pa, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Focal_VisualText_Attention_for_Memex_Question_Answering\figure_1.jpg
  Figure 1 caption: MemexQA examples. The inputs are a question and a sequence of
    a users photos with corresponding metadata. The outputs include a short text answer
    and a few grounding photos to justify the answer.
  Figure 10 Link: articels_figures_by_rev_year\2019\Focal_VisualText_Attention_for_Memex_Question_Answering\figure_10.jpg
  Figure 10 caption: Qualitative analysis of FVTA on the MovieQA dataset. It shows
    the visual justification (movie clip frames) and text justification (subtitles)
    based on the top attention activation. Both justifications provide supporting
    evidence for the system to get the correct answer.
  Figure 2 Link: articels_figures_by_rev_year\2019\Focal_VisualText_Attention_for_Memex_Question_Answering\figure_2.jpg
  Figure 2 caption: Questions and four-choice answer in MemexQA. From the top to bottom
    are album metadata, photos from 2 albums, titles and captions, questions, answer
    choices, and evidential photos. The green choice denotes the correct answer.
  Figure 3 Link: articels_figures_by_rev_year\2019\Focal_VisualText_Attention_for_Memex_Question_Answering\figure_3.jpg
  Figure 3 caption: Example of the annotation web interface. During questions and
    answers collection, annotators are given all the photos and metadata information
    of the assigned Flickr user on the left of the screen. On the right, annotators
    are asked to generate different types of questions and the corresponding answers.
    Meanwhile, they are also required to select evidence photos for each of the question.
    During QA cleanup stage, annotators are asked to select answers for the questions
    given all relevant information. If they select the wrong answers, they are asked
    to provide a reason why they make the mistakes, which will be used for the QA
    cleanup.
  Figure 4 Link: articels_figures_by_rev_year\2019\Focal_VisualText_Attention_for_Memex_Question_Answering\figure_4.jpg
  Figure 4 caption: Question distributions by question types and question albumstopics.
  Figure 5 Link: articels_figures_by_rev_year\2019\Focal_VisualText_Attention_for_Memex_Question_Answering\figure_5.jpg
  Figure 5 caption: An overview of Focal Visual-Text Attention (FVTA) model. For visual-text
    embedding, we use a pre-trained convolutional neural network to embed the photos
    and pre-trained word vectors to embed the words. We use a bi-directional LSTM
    as the sequence encoder. All hidden states from the question and the context are
    used to calculate the FVTA tensor. Based on the FVTA attention, both question
    and the context are summarized into single vectors for the output layer to produce
    final answer. Here the output layer is for open-ended setting while in multiple-choice
    setting, the text embedding of the answer choice is also used as the input, which
    is not shown in this figure.
  Figure 6 Link: articels_figures_by_rev_year\2019\Focal_VisualText_Attention_for_Memex_Question_Answering\figure_6.jpg
  Figure 6 caption: Comparison of our FVTA and classical VQA attention mechanism.
    FVTA considers both visual-text intra-sequence correlations and cross sequence
    interaction, and focuses on a few, small regions. In FVTA, the multi-modal feature
    representation in the sequence data is preserved without losing information.
  Figure 7 Link: articels_figures_by_rev_year\2019\Focal_VisualText_Attention_for_Memex_Question_Answering\figure_7.jpg
  Figure 7 caption: Accuracy versus number of parameters (left) and computational
    demand versus the number of photo input (right) across different models. Computational
    demand is measured in the number of floating-point operation (FLOPS) to process
    the given number of photo input. The FLOPS numbers for other methods are in addition
    to the LSTM baselines FLOPS. We only compare LSTM-based methods. Embedding + LSTM
    + concat has about 3x more FLOPS in general and thus omitted.
  Figure 8 Link: articels_figures_by_rev_year\2019\Focal_VisualText_Attention_for_Memex_Question_Answering\figure_8.jpg
  Figure 8 caption: Example failure cases of FVTA model on MemexQA dataset. Our model
    finds the correct grounding photos in both cases but fails to choose the correct
    answers due to the limitations discussed in Section 6.3.1.
  Figure 9 Link: articels_figures_by_rev_year\2019\Focal_VisualText_Attention_for_Memex_Question_Answering\figure_9.jpg
  Figure 9 caption: Qualitative comparison of FVTA model and other attention models
    on the MemexQA dataset. For each question, we show the answer and the images of
    the highest attention weights. Images are ranked from left to right based on the
    attention weights. The correct images and answers have green border whereas the
    incorrect ones are surrounded by the red border.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Junwei Liang
  Name of the last author: Alexander G. Hauptmann
  Number of Figures: 10
  Number of Tables: 8
  Number of authors: 6
  Paper title: Focal Visual-Text Attention for Memex Question Answering
  Publication Date: 2019-01-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Representative VQA Datasets and Our New Dataset
      Called MemexQA
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Percentage of Answers Found in Text Metadata Excluding Answers\
      \ from \u201CHow Many\u201D Questions"
  Table 3 caption:
    table_text: TABLE 3 Human Performance on MemexQA
  Table 4 caption:
    table_text: TABLE 4 Comparison of Different Methods on MemexQA Multiple-Choice
      Setting by Question Type
  Table 5 caption:
    table_text: TABLE 5 Comparison of Different Methods on MemexQA Open-Ended Setting
      by Question Type
  Table 6 caption:
    table_text: TABLE 6 The Quality Comparison of the Learned FVTA and Classic Attention
  Table 7 caption:
    table_text: TABLE 7 Ablation Studies of the Proposed FVTA Method on the MemexQA
      Dataset
  Table 8 caption:
    table_text: TABLE 8 Accuracy Comparison on the Test and the Validation Set of
      the MovieQA Dataset
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2890628
- Affiliation of the first author: school of biomedical engineering & imaging sciences,
    kings college london, london, united kingdom
  Affiliation of the last author: school of biomedical engineering & imaging sciences,
    kings college london, london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2019\Weighted_Manifold_Alignment_using_Wave_Kernel_Signatures_for_Aligning_Medical_Im\figure_1.jpg
  Figure 1 caption: "Top: Example images of a toy duck, cat and pig from the COIL-20\
    \ dataset. Each object is imaged 72 times under one full rotation and images are\
    \ 128\xD7128 pixels. Bottom: Laplacian eigenmaps embedding for these three datasets\
    \ (from left to right, the duck, cat and pig), with k G =4 . The colour of each\
    \ point corresponds to the viewing angle."
  Figure 10 Link: articels_figures_by_rev_year\2019\Weighted_Manifold_Alignment_using_Wave_Kernel_Signatures_for_Aligning_Medical_Im\figure_10.jpg
  Figure 10 caption: Correlation between low-dimensional coordinates of unlabelled
    PET data and ground truth respiratory signal as a function of manifold weighting
    on the MR data with which the labelled PET sinograms are aligned. Heavily weighting
    the MR images results in an embedding which more closely corresponds with respiratory
    motion state.
  Figure 2 Link: articels_figures_by_rev_year\2019\Weighted_Manifold_Alignment_using_Wave_Kernel_Signatures_for_Aligning_Medical_Im\figure_2.jpg
  Figure 2 caption: 'Top: The same example images from the COIL-20 dataset with significant
    Gaussian noise added (standard deviation of 1 where the original image pixel intensities
    are normalised between 0 and 1). Bottom: Laplacian eigenmap embeddings for these
    datasets, with k G =4 . Some of the structure seen in Fig. 1 is present but much
    has been lost.'
  Figure 3 Link: articels_figures_by_rev_year\2019\Weighted_Manifold_Alignment_using_Wave_Kernel_Signatures_for_Aligning_Medical_Im\figure_3.jpg
  Figure 3 caption: 'Joint embeddings, using weighted MA of the three sets of images,
    where the noiseless dataset is used for the images of the duck, but the noisy
    data used for the cat and pig images. In all cases we show all three datasets
    embedding together. Left: Applying no weighting (standard MA). Centre left: Weighting
    the noisy cat images by a factor of 10. Centre right: Weighting the noisy pig
    images by a factor of 10. Right: Weighting the noiseless duck images by a factor
    of 10. It is clear that only in the final case, where the noiseless dataset is
    weighted, is the original manifold structure cleanly recovered. The idea behind
    our MA scheme is that it produces joint embeddings like that on the right of this
    figure rather than like those on the left.'
  Figure 4 Link: articels_figures_by_rev_year\2019\Weighted_Manifold_Alignment_using_Wave_Kernel_Signatures_for_Aligning_Medical_Im\figure_4.jpg
  Figure 4 caption: The fraction of nearest neighbours correctly predicted by the
    results of the MA plotted against the weighting applied to the noiseless dataset.
    As the weighting C increases the fraction of correct predictions goes to 1. Note
    that the unweighted case, C=1 , highlighted by the red marker performs significantly
    worse than the strongly weighted case highlighted at C=10 .
  Figure 5 Link: articels_figures_by_rev_year\2019\Weighted_Manifold_Alignment_using_Wave_Kernel_Signatures_for_Aligning_Medical_Im\figure_5.jpg
  Figure 5 caption: Normalised motion field magnitude for each sagittal slice in the
    high-resolution dynamic synthetic MR volumes. The two peaks correspond to the
    slices through the left and right lungs, and the right lung shows more significant
    motion due to the heart obscuring the left lung (as in the images in Fig. 9 the
    volunteers right lung corresponds to the left of the image). Using these values
    as the dataset weights c n will place more emphasis on the sagittal slices in
    the centre of the lungs, rather than those around the edge of the torso or around
    the spine. Here the weights are normalised to have a maximal value of C=2 and
    a minimum of 1.
  Figure 6 Link: articels_figures_by_rev_year\2019\Weighted_Manifold_Alignment_using_Wave_Kernel_Signatures_for_Aligning_Medical_Im\figure_6.jpg
  Figure 6 caption: An example of the low-dimensional embedding generated from the
    WKS graph descriptor and weighted MA. All sagittal slices are shown. Colour corresponds
    to a respiratory navigator generated from the gold standard synthetic images.
    The v shape of the manifold is typical, but the strong relation between the position
    in the manifold and the colour of the points illustrates that the embedding places
    close together points which share similar respiratory motion states, as intended.
  Figure 7 Link: articels_figures_by_rev_year\2019\Weighted_Manifold_Alignment_using_Wave_Kernel_Signatures_for_Aligning_Medical_Im\figure_7.jpg
  Figure 7 caption: Mean square difference between reconstructed volumes and ground
    truth volumes using the proposed wave kernel signature (WKS) graph descriptor,
    and the heat kernel signature (HKS) and random walk (RW) feature vector descriptors.
    For each method 250 values are generated, one for each timestep of the synthetic
    4D volumes. Volumes are reconstructed using every sagittal slice and the median
    error taken, which is shown here. The proposed weighted MA scheme reduces the
    reconstruction errors in each case, and the WKS + weighted MA pipeline gives the
    lowest error.
  Figure 8 Link: articels_figures_by_rev_year\2019\Weighted_Manifold_Alignment_using_Wave_Kernel_Signatures_for_Aligning_Medical_Im\figure_8.jpg
  Figure 8 caption: 'Top: Examples from volunteers A (left) and D (right) of a coronal
    slice through the original unaligned volumes. The sagittal slices are not in consistent
    motion states resulting in discontinuities in diaphragm positions. Bottom: Examples
    from a volume reconstructed by stacking sagittal slices aligned by motion state.
    Estimated diaphragm positions for left and right lungs are shown in red.'
  Figure 9 Link: articels_figures_by_rev_year\2019\Weighted_Manifold_Alignment_using_Wave_Kernel_Signatures_for_Aligning_Medical_Im\figure_9.jpg
  Figure 9 caption: 'Top: A coronal and sagittal view of the synthetic MR volumes.
    Bottom: corresponding projections of a PET sinogram, which is visibly noisier
    than the MR image due to the stochastic nature of this imaging modality.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: James R. Clough
  Name of the last author: Andrew P. King
  Number of Figures: 11
  Number of Tables: 1
  Number of authors: 7
  Paper title: Weighted Manifold Alignment using Wave Kernel Signatures for Aligning
    Medical Image Datasets
  Publication Date: 2019-01-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Pearsons Correlation Coefficient between Left and Right Hemidiaphragm
      Positions of Reconstructed Volumes
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2891600
