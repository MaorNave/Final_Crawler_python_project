- Affiliation of the first author: department of electrical and computer engineering,
    carnegie mellon university, pittsburgh, usa
  Affiliation of the last author: department of electrical and computer engineering,
    national university of singapore, singapore
  Figure 1 Link: articels_figures_by_rev_year\2019\Tensor_Robust_Principal_Component_Analysis_with_a_New_Tensor_Nuclear_Norm\figure_1.jpg
  Figure 1 caption: 'Illustrations of RPCA [3] (up row) and our Tensor RPCA (bottom
    row). RPCA: low-rank and sparse matrix decomposition from noisy matrix observations.
    Tensor RPCA: low-rank and sparse tensor decomposition from noisy tensor observations.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Tensor_Robust_Principal_Component_Analysis_with_a_New_Tensor_Nuclear_Norm\figure_2.jpg
  Figure 2 caption: "An illustration of the t-SVD of an n 1 \xD7 n 2 \xD7 n 3 tensor\
    \ [10]."
  Figure 3 Link: articels_figures_by_rev_year\2019\Tensor_Robust_Principal_Component_Analysis_with_a_New_Tensor_Nuclear_Norm\figure_3.jpg
  Figure 3 caption: "Color images can be approximated by low tubal rank tensors. (a)\
    \ A color image can be modeled as a tensor M\u2208 R 512\xD7512\xD73 ; (b) approximation\
    \ by a tensor with tubal rank r=50 ; (c) plot of the singular values of M ."
  Figure 4 Link: articels_figures_by_rev_year\2019\Tensor_Robust_Principal_Component_Analysis_with_a_New_Tensor_Nuclear_Norm\figure_4.jpg
  Figure 4 caption: An illustration of the way to define the tensor nuclear norm and
    the relationship with other tensor concepts. First, the tensor operator norm is
    a special case of the known operator norm performed on the tensors. The tensor
    spectral norm is induced by the tensor operator norm by treating the tensor-tensor
    product as an operator. Then the tensor nuclear norm is defined as the dual norm
    of the tensor spectral norm. We also define the tensor average rank and show that
    its convex envelope is the tensor nuclear norm within the unit ball of the tensor
    spectral norm. As detailed in Section 3, the tensor spectral norm, tensor nuclear
    norm and tensor average rank are also defined on the matricization of the tensor.
  Figure 5 Link: articels_figures_by_rev_year\2019\Tensor_Robust_Principal_Component_Analysis_with_a_New_Tensor_Nuclear_Norm\figure_5.jpg
  Figure 5 caption: "Correct recovery for varying tubal ranks of L 0 and sparsities\
    \ of E 0 . Fraction of correct recoveries across 10 trials, as a function of rank\
    \ t ( L 0 ) ( x -axis) and sparsity of E 0 ( y -axis). Left: sgn( E 0 ) random.\
    \ Right: E 0 = P \u03A9 sgn( L 0 ) ."
  Figure 6 Link: articels_figures_by_rev_year\2019\Tensor_Robust_Principal_Component_Analysis_with_a_New_Tensor_Nuclear_Norm\figure_6.jpg
  Figure 6 caption: Comparison of the PSNR values (top) and running time (bottom)
    obtained by RPCA, SNN and TRPCA on 100 images.
  Figure 7 Link: articels_figures_by_rev_year\2019\Tensor_Robust_Principal_Component_Analysis_with_a_New_Tensor_Nuclear_Norm\figure_7.jpg
  Figure 7 caption: Recovery performance comparison on 6 example images. (a) Original
    image; (b) observed image; (c)-(e) recovered images by RPCA, SNN and TRPCA, respectively;
    (f) and (g) show the comparison of PSNR values and running time (second) on the
    above 6 images.
  Figure 8 Link: articels_figures_by_rev_year\2019\Tensor_Robust_Principal_Component_Analysis_with_a_New_Tensor_Nuclear_Norm\figure_8.jpg
  Figure 8 caption: Background modeling results of four surveillance video sequences.
    (a) Original frames; (b)-(d) low rank and sparse components obtained by RPCA,
    SNN and TRPCA, respectively; (e) running time comparison.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Canyi Lu
  Name of the last author: Shuicheng Yan
  Number of Figures: 8
  Number of Tables: 2
  Number of authors: 6
  Paper title: Tensor Robust Principal Component Analysis with a New Tensor Nuclear
    Norm
  Publication Date: 2019-01-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Parallelism of Sparse Vector, Low-Rank Matrix and Low-Rank
      Tensor
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Correct Recovery for Random Problems of Varying Sizes
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2891760
- Affiliation of the first author: eti, university of siegen, siegen, germany
  Affiliation of the last author: eti, university of siegen, siegen, germany
  Figure 1 Link: articels_figures_by_rev_year\2019\Hierarchical_Bayesian_Inverse_Lighting_of_Portraits_with_a_Virtual_Light_Stage\figure_1.jpg
  Figure 1 caption: This figure shows the input image Fig. 1a, the 3D shape Fig. 1b
    that is estimated by 3DMM fitting from Fig. 1a. The 3DMM, also, provides an average
    human face Fig. 1c. The estimated 3D shape, pose, and average texture are used
    together with a single light source of the VLS to render an image C i . The Fig.
    1d shows three examples of such images.
  Figure 10 Link: articels_figures_by_rev_year\2019\Hierarchical_Bayesian_Inverse_Lighting_of_Portraits_with_a_Virtual_Light_Stage\figure_10.jpg
  Figure 10 caption: Results for 100 and 300 gallery images. First row are input images.
  Figure 2 Link: articels_figures_by_rev_year\2019\Hierarchical_Bayesian_Inverse_Lighting_of_Portraits_with_a_Virtual_Light_Stage\figure_2.jpg
  Figure 2 caption: (a) is a rectangular plot of a sphere of light sources. Each white
    square represents a light source direction. Here, there are 100 light source directions
    distributed on the entire sphere with a random spherically uniform distribution.
    The squares in the middle of the rectangle represent light sources from the frontal
    direction toward the face. Those close to the top and bottom, are light sources
    from up and down, and those close to the middle of left and right edges are light
    sources from behind the head. (b) shows the sphere from the frontal view.
  Figure 3 Link: articels_figures_by_rev_year\2019\Hierarchical_Bayesian_Inverse_Lighting_of_Portraits_with_a_Virtual_Light_Stage\figure_3.jpg
  Figure 3 caption: This is the result of the proposed geometric superpixel segmentation
    Fig. 3a, where the surface normal directions are color coded per pixel, and the
    result of standard photometric superpixel segmentation Fig. 3b on the original
    input image. Zoom in on the PDF if necessary.
  Figure 4 Link: articels_figures_by_rev_year\2019\Hierarchical_Bayesian_Inverse_Lighting_of_Portraits_with_a_Virtual_Light_Stage\figure_4.jpg
  Figure 4 caption: Results for different number of superpixels. First row are input
    images, second row are corresponding results with 10 geometric superpixels, third
    row are results for 20 and last row are corresponding results for 300 superpixels.
  Figure 5 Link: articels_figures_by_rev_year\2019\Hierarchical_Bayesian_Inverse_Lighting_of_Portraits_with_a_Virtual_Light_Stage\figure_5.jpg
  Figure 5 caption: "This image shows the optimized values for parameters \u03B2 \u20D7\
    \ on geometric Fig. 5a and photometric Fig. 5b superpixels, and \u03B8 \u20D7\
    \ on the gallery Fig. 5c. The values are normalized to fit in the [0,1] range\
    \ for visualization. Each C i in a black background has a \u03B8 i \u22600 . Thus,\
    \ the corresponding x i is being regularized and it is more likely to become zero.\
    \ Zoom in PDF or compare with Fig. 1d, if necessary."
  Figure 6 Link: articels_figures_by_rev_year\2019\Hierarchical_Bayesian_Inverse_Lighting_of_Portraits_with_a_Virtual_Light_Stage\figure_6.jpg
  Figure 6 caption: "An input image with a hard cast shadow below the nose toward\
    \ the right cheek of the subject is shown in Fig. 6a. Fig. 6b is an automatically\
    \ marked area using a model-based mask for the area around the nose and the estimated\
    \ 3D model for the input image. Fig. 6c is the albedo image I alb and Fig. 6d\
    \ is (a. I alb ) according to (15), which represents the optimal ambient illumination\
    \ reconstruction for the input image. Fig. 6e is calculated with the equation:\
    \ 1\u2212 (Fig. 6d-Fig. 6a). Note, that the darker areas of the input image will\
    \ lead to negative values, visualized as black pixels, in this image. Finally,\
    \ mark the negative pixels in Fig. 6d only if they are also marked in Fig. 6b,\
    \ to generate the cast shadow mask Fig. 6f. Here, the small spot on the left is\
    \ marked as cast shadow of the nose, which is wrong. However. such errors are\
    \ negligible for the purpose of this paper. The partially masked \u03B2 \u20D7\
    \ is mapped on the image plane, for geometric superpixels Fig. 6g and photometric\
    \ superpixels Fig. 6h. The blue areas are the superpixels for which the \u03B2\
    \ p is fixed to a large value."
  Figure 7 Link: articels_figures_by_rev_year\2019\Hierarchical_Bayesian_Inverse_Lighting_of_Portraits_with_a_Virtual_Light_Stage\figure_7.jpg
  Figure 7 caption: First row are input images of different subjects under similar
    illumination. Second row are rendered spheres with the estimated lighting from
    the respective input images above them. The spheres are rendered with skin BRDF
    and color-corrected. Note that some of the skin properties are wrongly attributed
    to the lighting by our algorithm.
  Figure 8 Link: articels_figures_by_rev_year\2019\Hierarchical_Bayesian_Inverse_Lighting_of_Portraits_with_a_Virtual_Light_Stage\figure_8.jpg
  Figure 8 caption: This figure shows the improvement of the cast shadow reconstruction
    on two input images in separate rows. The column Fig. 8a are the input images
    which contain obvious cast shadows of the nose. The result of 3DMM fitting, rendered
    without the cast shadow Fig. 8b, the result from [1] with the achieved cast shadow
    Fig. 8c, and the cast shadow that is reconstructed with the proposed JMAP Fig.
    8d.
  Figure 9 Link: articels_figures_by_rev_year\2019\Hierarchical_Bayesian_Inverse_Lighting_of_Portraits_with_a_Virtual_Light_Stage\figure_9.jpg
  Figure 9 caption: Transfer the illumination from one image to other. See Section
    7.5 for explanation.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Davoud Shahlaei
  Name of the last author: Volker Blanz
  Number of Figures: 11
  Number of Tables: 1
  Number of authors: 2
  Paper title: Hierarchical Bayesian Inverse Lighting of Portraits with a Virtual
    Light Stage
  Publication Date: 2019-01-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 MISIM and MSIER for Examples from Fig. 11
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2891638
- Affiliation of the first author: department of surgery and cancer, division of computational
    and systems medicine, imperial college london, london, united kingdom
  Affiliation of the last author: school of mathematical sciences, queen mary university
    of london, london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2019\Visibility_Graphs_for_Image_Processing\figure_1.jpg
  Figure 1 caption: Semi-log plot of Similarity measure between the IVGIHVG of Lena
    and Lena polluted with a certain amount of Gaussian white noise, as a function
    of the Noise-to-Signal Ratio (NSR). The benchmark similarity measure computed
    directly on images is based on an appropriately rescaled version of the Normalized
    Mutual Information. Information mapped to the IVGIHVG setting seems to be more
    robust to noise contamination than the raw information stored in the image.
  Figure 10 Link: articels_figures_by_rev_year\2019\Visibility_Graphs_for_Image_Processing\figure_10.jpg
  Figure 10 caption: Performance of the IVGIHVG filters in image pre-processing. a)
    The IVG and IHVG filters are shown together with the LBP (local binary patterns)
    filter and the Texton filter as applied to the Lena face image. b) Distance mathcalD
    between the Lena face image and the same image at different resolution scales
    is measured in the 4-dimensional space of the (normalized) Haralick features,
    and plotted as a function of the standard deviation of the Gaussian 5x5 kernel
    applied to obtain low-resolution images. mathcalD is computed for the original
    unfiltered image as well as for the same image after being pre-processed using
    the filters shown in a). mathcalD is an indicator of the performance of the filters
    as pre-processors (the lowest the better, see the text).
  Figure 2 Link: articels_figures_by_rev_year\2019\Visibility_Graphs_for_Image_Processing\figure_2.jpg
  Figure 2 caption: "Illustration of the process of extracting a topological plot\
    \ from an image. Here we use IHVG for illustration, but a similar process applies\
    \ to IVG. The image is initially mapped into its corresponding IVG or IHVG (here\
    \ an IHVG) where the nodes properties inherit the spatial local information of\
    \ the pixels according to the definition of an IVG or an IHVG. Then, for each\
    \ node a specific topological property is measured \u2013for example its degree\u2013\
    \ and the measured values are mapped back to form a matrix of the same size of\
    \ the original image."
  Figure 3 Link: articels_figures_by_rev_year\2019\Visibility_Graphs_for_Image_Processing\figure_3.jpg
  Figure 3 caption: Illustration of the process of measuring a Visibility Patch of
    order p=3 (in this illustration we use VP 3 , but HVP 3 can be extracted equivalently
    by using the horizontal criterion instead, see the text for definitions).
  Figure 4 Link: articels_figures_by_rev_year\2019\Visibility_Graphs_for_Image_Processing\figure_4.jpg
  Figure 4 caption: Illustration of the labelling of a concrete HVP 3 by making use
    of the sequential visibility graph motifs of order 3 (see Table 1) and a binary
    expansion.
  Figure 5 Link: articels_figures_by_rev_year\2019\Visibility_Graphs_for_Image_Processing\figure_5.jpg
  Figure 5 caption: Running time of the Horizontal Visibility Patch Profile algorithm
    of order 3 as a function of the size of the image. The input of the algorithm
    is the image and the output is the patch profile Z which records the frequency
    distribution of each patch. Feature extraction scales linearly (computation performed
    on a 2.5 GHz IntelCore i7 processor).
  Figure 6 Link: articels_figures_by_rev_year\2019\Visibility_Graphs_for_Image_Processing\figure_6.jpg
  Figure 6 caption: Illustration of the process of extraction an image multiplex visibility
    graph (here applied to a RGB image, hence yielding a multiplex graph with m=3
    layers). Features extracted from each of the layers are combined using a Principal
    Component Analysis to obtain a pseudo-multiplex descriptor, that is fed into a
    classifier.
  Figure 7 Link: articels_figures_by_rev_year\2019\Visibility_Graphs_for_Image_Processing\figure_7.jpg
  Figure 7 caption: Several Visibility Filters mathcalFmathrmHVG applied on Lena (we
    have applied an histogram equalization to balance contrast on the filtered images).
  Figure 8 Link: articels_figures_by_rev_year\2019\Visibility_Graphs_for_Image_Processing\figure_8.jpg
  Figure 8 caption: Set of 512times 512 standard grayscale images often found in literature
    along with their IHVG k -filter.
  Figure 9 Link: articels_figures_by_rev_year\2019\Visibility_Graphs_for_Image_Processing\figure_9.jpg
  Figure 9 caption: Reduction factor (RF) in function of the percentage of saturated
    pixels (SP) obtained with the k -filter on a white noise image (solid blue line)
    and on Lena (brown dashed line). The curve RF-SP is also shown for the case of
    Lena without filter (dotted yellow line) for comparison.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jacopo Iacovacci
  Name of the last author: Lucas Lacasa
  Number of Figures: 15
  Number of Tables: 4
  Number of authors: 2
  Paper title: Visibility Graphs for Image Processing
  Publication Date: 2019-01-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Sequential VGHVG Motifs of Order 3
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Kylberg Texture Dataset: Performance in Classification Accuracy
      Obtained with Different Algorithms'
  Table 3 caption:
    table_text: TABLE 3 2D HeLa Dataset
  Table 4 caption:
    table_text: 'TABLE 4 Natural Color Textures Datasets: Best Classification Accuracy,
      Best Average by Class AUC and Model Average Classification Accuracy MAA Obtained
      with a Quadratic-Kernel Multiclass One-vs-All Support Vector Machine (ova-qSVM)
      Classifier Algorithm over 30 Models Realizations, for Different Sets of Multiplex
      Image Visibility Features'
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2891742
- Affiliation of the first author: key laboratory of intelligent information processing
    of chinese academy of sciences (cas), institute of computing technology, cas,
    beijing, china
  Affiliation of the last author: key laboratory of intelligent information processing
    of chinese academy of sciences (cas), institute of computing technology, cas,
    beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2019\Tattoo_Image_Search_at_Scale_Joint_Detection_and_Compact_Representation_Learning\figure_1.jpg
  Figure 1 caption: 'Tattoo examples: (a) A tattoo on the right hand of a Chiribaya
    mummy in southern Peru who lived from A.D. 900 to 1350, 5 (b) a tattoo on the
    head signifying gang membership association, 6 (c, d) tattoos of a masked ringleader
    of the riots during Euro 2012 qualifier, and the suspect of the masked ringleader
    identified by tattoos. 7'
  Figure 10 Link: articels_figures_by_rev_year\2019\Tattoo_Image_Search_at_Scale_Joint_Detection_and_Compact_Representation_Learning\figure_10.jpg
  Figure 10 caption: Examples of tattoo image search results by the proposed approach
    using (a) tattoo photos and (b) tattoo sketches as queries. For each query tattoo
    image, the top-5 tattoo gallery images in the returned list are given.
  Figure 2 Link: articels_figures_by_rev_year\2019\Tattoo_Image_Search_at_Scale_Joint_Detection_and_Compact_Representation_Learning\figure_2.jpg
  Figure 2 caption: Tattoo images from the Tatt-C dataset [5] suggest that tattoo
    search is challenging because (a) there are various types of tattoos containing
    categories such as humans, animals, plants, flags, objects, abstract, symbols,
    etc., and (b) the intra-class variability in one class of tattoos (i.e., eagle)
    can be very large.
  Figure 3 Link: articels_figures_by_rev_year\2019\Tattoo_Image_Search_at_Scale_Joint_Detection_and_Compact_Representation_Learning\figure_3.jpg
  Figure 3 caption: Overview of the proposed approach for tattoo search at scale via
    joint tattoo detection and compact representation learning. Our approach consists
    of a stem CNN for computing the shared features, an RPN [32] and Fast R-CNN [66]
    for tattoo detection, and a compact representation learning module. The proposed
    approach can be trained end-to-end via stochastic gradient descent (SGD) [30]
    and back-propagation (BP) [67].
  Figure 4 Link: articels_figures_by_rev_year\2019\Tattoo_Image_Search_at_Scale_Joint_Detection_and_Compact_Representation_Learning\figure_4.jpg
  Figure 4 caption: The benefit of using the polarization loss and dispersity loss.
    (a) an original feature vector of real values in the range of [0,1] , (b) the
    learned feature vector after using the polarization loss alone (i.e., each element
    is close to either 0 or 1), (c) the learned feature vector after using the dispersity
    loss alone (i.e., the elements are evenly distributed on the two sides of 0.5),
    and (d) the learned feature vector after jointly using the polarization and dispersity
    losses (i.e., near-binary elements are evenly distributed on the two sides of
    0.5).
  Figure 5 Link: articels_figures_by_rev_year\2019\Tattoo_Image_Search_at_Scale_Joint_Detection_and_Compact_Representation_Learning\figure_5.jpg
  Figure 5 caption: "An example of data augmentation for one tattoo image (referred\
    \ to as \u201Coriginal\u201D in the top row) in the training set to replicate\
    \ various image acquisition conditions, e.g., (a-d) illumination variation, (e-f)\
    \ image blur, (g-l) deformation, and (m-p) perspective distortion. A tattoo sketch\
    \ is also generated for data augmentation (shown under the original tattoo)."
  Figure 6 Link: articels_figures_by_rev_year\2019\Tattoo_Image_Search_at_Scale_Joint_Detection_and_Compact_Representation_Learning\figure_6.jpg
  Figure 6 caption: 'Examples of tattoo images from the four tattoo image databases
    used in our experiments: (a) Tatt-C [5], (b) Flickr [10], (c) DeMSI [75], and
    (d) our WebTattoo dataset.'
  Figure 7 Link: articels_figures_by_rev_year\2019\Tattoo_Image_Search_at_Scale_Joint_Detection_and_Compact_Representation_Learning\figure_7.jpg
  Figure 7 caption: Examples of (a) good tattoo detections, and (b) poor tattoo detections
    by the proposed approach. The green and blue rectangles show the ground-truth
    tattoo bounding boxes and the detected tattoo bounding boxes, respectively. The
    numbers shown above the bounding boxes are the detection confidence scores.
  Figure 8 Link: articels_figures_by_rev_year\2019\Tattoo_Image_Search_at_Scale_Joint_Detection_and_Compact_Representation_Learning\figure_8.jpg
  Figure 8 caption: 'Tattoo search performance (in terms of precision-recall) by the
    proposed approach and the state-of-the-art methods (TattooID [1], SSDH-VGG16 [42],
    Faster R-CNN + SSDH, and OIM-ResNet50 [73]) on the WebTattoo test dataset: (a)
    Without background tattoo images in the gallery set, and (b) with 300K background
    tattoo images in the gallery set; for TattooID, we report its tattoo search performance
    using an extended gallery set with only 100K background images because of its
    long running time.'
  Figure 9 Link: articels_figures_by_rev_year\2019\Tattoo_Image_Search_at_Scale_Joint_Detection_and_Compact_Representation_Learning\figure_9.jpg
  Figure 9 caption: The importance of using accurate tattoo bounding boxes for compact
    feature learning.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.64
  Name of the first author: Hu Han
  Name of the last author: Xilin Chen
  Number of Figures: 14
  Number of Tables: 2
  Number of authors: 5
  Paper title: 'Tattoo Image Search at Scale: Joint Detection and Compact Representation
    Learning'
  Publication Date: 2019-01-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 A Summary of Published Methods on Tattoo Identification and
      Retrieval
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Tattoo Detection (Localization) Performance of the Proposed
      Approach and the State-of-the-Art Methods on the WebTattoo Test and Tatt-C Datasets
      in Terms of Recall versus FPPI
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2891584
- Affiliation of the first author: department of computer science and applied mathematics,
    weizmann institute of science, rehovot, israel
  Affiliation of the last author: department of computer science and applied mathematics,
    weizmann institute of science, rehovot, israel
  Figure 1 Link: articels_figures_by_rev_year\2019\On_Detection_of_Faint_Edges_in_Noisy_Images\figure_1.jpg
  Figure 1 caption: Electron microscopy images depicting cell membranes. Their accurate
    delineation is important to plant biologists.
  Figure 10 Link: articels_figures_by_rev_year\2019\On_Detection_of_Faint_Edges_in_Noisy_Images\figure_10.jpg
  Figure 10 caption: 'Left: Empirical run-time of our straight and curved edge detectors
    compared with their theoretical run-times. Right: Simulation results: F-measures
    of various edge detection algorithms as a function of SNR.'
  Figure 2 Link: articels_figures_by_rev_year\2019\On_Detection_of_Faint_Edges_in_Noisy_Images\figure_2.jpg
  Figure 2 caption: Two adjacent noisy intensity profiles (right) parallel to a long
    edge (left) in an electron microscope image. The noise leads to contrast reversals
    (locations where the red curve exceeds the blue one).
  Figure 3 Link: articels_figures_by_rev_year\2019\On_Detection_of_Faint_Edges_in_Noisy_Images\figure_3.jpg
  Figure 3 caption: "Filters for straight edges of length L=4, width w=4 and spacing\
    \ s=1 : Top left: Design principle. For a straight edge \u03B3 (in black) connecting\
    \ two grid points P 1 , P 2 we design a filter whose response is half of the difference\
    \ between the average of wL2 interpolated pixel values on both sides of \u03B3\
    \ (denoted by \u03B3 \xB1i , i\u22081,2 ). The remaining boxes show four filters\
    \ in clockwise offsets of (from left to right) 0 \u2218 , 11.25 \u2218 , 22.5\
    \ \u2218 , and 45 \u2218 from vertical. The full set of vertical filters includes\
    \ also a filter in orientation offset of 33.75 and all of these filters reflected\
    \ about the vertical axis. The horizontal filters are obtained by transposing\
    \ the vertical ones. Our method computes the responses hierarchically so that\
    \ the filters themselves are never explicitly constructed."
  Figure 4 Link: articels_figures_by_rev_year\2019\On_Detection_of_Faint_Edges_in_Noisy_Images\figure_4.jpg
  Figure 4 caption: "Detection threshold divided by the noise level \u03C3 , measured\
    \ in SNR, as a function of length L . The solid red and blue curves are the empirical\
    \ median of the maximal responses, over 100 pure noise images, for straight lines\
    \ and binary tree beam-curves, respectively. The dashed red and blue lines are\
    \ the corresponding theoretical thresholds of Eq. (5) with \u03B4=0.5. As the\
    \ beam-curves have exponential size, both theoretical and empirical thresholds\
    \ approach a positive constant near 0.5. In contrast, the thresholds for straight\
    \ lines approach zero."
  Figure 5 Link: articels_figures_by_rev_year\2019\On_Detection_of_Faint_Edges_in_Noisy_Images\figure_5.jpg
  Figure 5 caption: 'Left panel: Direct calculation of line integral ( L=4 ): an interpolation
    from the given data points to the red points is followed by the trapezoid rule.
    Right panel: Base level initialization: stencil of four length-1 responses for
    each pixel (left) cover the whole grid (right).'
  Figure 6 Link: articels_figures_by_rev_year\2019\On_Detection_of_Faint_Edges_in_Noisy_Images\figure_6.jpg
  Figure 6 caption: 'Left panel: Integrals of length-2 are constructed from integrals
    of length-1 as follows: i) for existing directions (left), by averaging length-1
    adjacent integrals (dashed lines) in that direction and ii) for a new direction
    (right), by averaging four nearest integrals (dashed lines). Right panel: The
    red lines denote length-4 vertical responses, which are calculated at every 2nd
    row (pink rows), at each pixel in this row. The blue lines denote length-4 horizontal
    responses, which are calculated at every 2nd column (blue columns), at each pixel
    in this column.'
  Figure 7 Link: articels_figures_by_rev_year\2019\On_Detection_of_Faint_Edges_in_Noisy_Images\figure_7.jpg
  Figure 7 caption: 'Left: The three topmost levels of the beam-curve binary tree.
    Right: The ntimes n image at level j=0 is partitioned (dashed line) into two rectangles
    of size n times n2 at level j=1 . Each rectangle is then partitioned (dotted line)
    into two n2 times n2 squares at level j=2 . A curve connecting two boundary pixels
    p1,p2 of level j=0 is a concatenation of up to 2 curves of level j=1 , and up
    to 4 curves of level j=2 .'
  Figure 8 Link: articels_figures_by_rev_year\2019\On_Detection_of_Faint_Edges_in_Noisy_Images\figure_8.jpg
  Figure 8 caption: 'Top row: Straight line filters of width w=4 in a 5 times 5 tile
    forming a parallelogram, and a general quadrangle. The offset curves may exceed
    beyond the boundaries of a tile. Bottom row: Stitching two straight filters at
    level jm to produce a curve at a rectangle of odd level jm-1 . (Right) Stitching
    two sub-curves of level jm-1 to produce curves at a square of even level jm-2
    .'
  Figure 9 Link: articels_figures_by_rev_year\2019\On_Detection_of_Faint_Edges_in_Noisy_Images\figure_9.jpg
  Figure 9 caption: 'Result of various edge detection algorithms to the noisy simulation
    image, at SNR 2 (top), and SNR 3 (bottom). From left to right: Input image, our
    curves O(N1.5) , our lines O(N,log N) , Canny and PMI.'
  First author gender probability: 0.94
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Nati Ofir
  Name of the last author: Ronen Basri
  Number of Figures: 14
  Number of Tables: 1
  Number of authors: 6
  Paper title: On Detection of Faint Edges in Noisy Images
  Publication Date: 2019-01-10 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average F-Measures in Simulations over Images at Three SNR
      Ranges
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2892134
- Affiliation of the first author: college of computer, national university of defense
    technology, changsha, china
  Affiliation of the last author: school of electronics engineering and computer science,
    peking university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2019\Multiple_KernelkkMeans_with_Incomplete_Kernels\figure_1.jpg
  Figure 1 caption: "ACC and NMI comparison with the variation of missing ratios on\
    \ Cornell dataset. For each given missing ratio, the \u201Cincomplete patterns\u201D\
    \ are randomly generated for 10 times and their averaged results are reported.\
    \ The results on other data sets are provided in the appendix, available in the\
    \ online supplemental material due to space limit."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Multiple_KernelkkMeans_with_Incomplete_Kernels\figure_2.jpg
  Figure 2 caption: "ACC and NMI comparison with the variation of missing ratios on\
    \ Caltech101. For each given missing ratio, the \u201Cincomplete patterns\u201D\
    \ are randomly generated for 10 times and their averaged results are reported."
  Figure 3 Link: articels_figures_by_rev_year\2019\Multiple_KernelkkMeans_with_Incomplete_Kernels\figure_3.jpg
  Figure 3 caption: "ACC and NMI comparison with the variation of missing ratios on\
    \ Flower17 and Flower102. For each given missing ratio, the \u201Cincomplete patterns\u201D\
    \ are randomly generated for 10 times and their averaged results are reported."
  Figure 4 Link: articels_figures_by_rev_year\2019\Multiple_KernelkkMeans_with_Incomplete_Kernels\figure_4.jpg
  Figure 4 caption: "ACC and NMI comparison with the variation of missing ratios on\
    \ CCV. For each given missing ratio, the \u201Cincomplete patterns\u201D are randomly\
    \ generated for 10 times and their averaged results are reported."
  Figure 5 Link: articels_figures_by_rev_year\2019\Multiple_KernelkkMeans_with_Incomplete_Kernels\figure_5.jpg
  Figure 5 caption: "Kernel alignment between the original kernels and the imputed\
    \ kernels by different algorithms under different missing ratios. For each given\
    \ missing ratio, the \u201Cincomplete patterns\u201D are randomly generated for\
    \ 10 times and their averaged results are reported. The results on Caltech101-5,\
    \ Caltech101-10 and Caltech101-15 are provided in the appendix, available in the\
    \ online supplemental material due to space limit."
  Figure 6 Link: articels_figures_by_rev_year\2019\Multiple_KernelkkMeans_with_Incomplete_Kernels\figure_6.jpg
  Figure 6 caption: "Clustering accuracy and NMI comparison with the variation of\
    \ missing ratios on Flower17 with an additional noisy kernel. For each given missing\
    \ ratio, the \u201Cincomplete patterns\u201D are randomly generated for 10 times\
    \ and their averaged results are reported."
  Figure 7 Link: articels_figures_by_rev_year\2019\Multiple_KernelkkMeans_with_Incomplete_Kernels\figure_7.jpg
  Figure 7 caption: Kernel coefficients learned by the aforementioned algorithms on
    Flower17 with an additional noisy kernel (with missing ratio=0.1). The base kernel
    indexed by 8 is a noisy one. We also observe that the results with other missing
    ratios are similar.
  Figure 8 Link: articels_figures_by_rev_year\2019\Multiple_KernelkkMeans_with_Incomplete_Kernels\figure_8.jpg
  Figure 8 caption: (a) The objective value of the proposed MKKM-IK at each iteration.
    (b) The effect of lambda on the proposed MKKM-IK-MKC in terms of ACC on Flower17.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Xinwang Liu
  Name of the last author: Wen Gao
  Number of Figures: 8
  Number of Tables: 6
  Number of authors: 10
  Paper title: 'Multiple Kernel

    k

    k-Means with Incomplete Kernels'
  Publication Date: 2019-01-13 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Aggregated ACC and NMI Comparison (mean \xB1 \xB1std) of\
      \ Different Clustering Algorithms on Cornell, Texas, Washington and Wisconsin\
      \ Data Sets"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Datasets Used in Our Experiments
  Table 3 caption:
    table_text: "TABLE 3 Aggregated ACC and NMI Comparison (mean \xB1 \xB1std) of\
      \ Different Clustering Algorithms on Flower17 and Flower102"
  Table 4 caption:
    table_text: "TABLE 4 Aggregated ACC and NMI Comparison (mean \xB1 \xB1std) of\
      \ Different Clustering Algorithms on CCV"
  Table 5 caption:
    table_text: "TABLE 5 Aggregated Alignment between the Original Kernels and the\
      \ Imputed Kernels (mean \xB1 \xB1std) on All Data Sets"
  Table 6 caption:
    table_text: "TABLE 6 Aggregated ACC and NMI Comparison (mean \xB1 \xB1std) of\
      \ Different Clustering Algorithms on Caltech101"
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2892416
- Affiliation of the first author: institute of information engineering, chinese academy
    of sciences, beijing, china
  Affiliation of the last author: national engineering research center for information
    security, and national engineering laboratory for information security technology,
    institute of information engineering, chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2019\Fast_CrossValidation_for_KernelBased_Algorithms\figure_1.jpg
  Figure 1 caption: "The traditional t -CV errors and t -BIF errors for LSSVM with\
    \ t=5,10,20 . The order of the Taylor expansion r=5 and the regularization parameter\
    \ \u03BB=1n ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Fast_CrossValidation_for_KernelBased_Algorithms\figure_2.jpg
  Figure 2 caption: "The mean square discrepancies between t -CV and t -BIF with different\
    \ r , where r is the order of the Taylor expansion. For each training set, we\
    \ choose the \u03C3 and \u03BB by t -fold cross validation on the training set.\
    \ Plotted are the mean square error of the approximate f \u03BA, P S\u2216 S i\
    \ (x) and f \u03BA, P S\u2216 S i (x) for the chosen parameters on the validation\
    \ sample S i ."
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.88
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Yong Liu
  Name of the last author: Weiping Wang
  Number of Figures: 2
  Number of Tables: 12
  Number of authors: 6
  Paper title: Fast Cross-Validation for Kernel-Based Algorithms
  Publication Date: 2019-01-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Average Test Errors ( % %) of t t-BIF and t t-CV on the
      Classification Data Sets for LSSVM with t=5,10,20 t=5,10,20
  Table 10 caption:
    table_text: TABLE 10 The Average Computational Time (in Seconds) of t t-BIF and
      t t-CV for KRR with the Order of the Taylor Expansion r=5 r=5
  Table 2 caption:
    table_text: TABLE 2 The Average Computational Time (in Seconds) of t t-BIF and
      t t-CV for LSSVM with t=5,10,20 t=5,10,20, and the Order of the Taylor Expansion
      r=5 r=5
  Table 3 caption:
    table_text: TABLE 3 The Average Test Errors ( % %) of t t-BIF and t t-CV on the
      Classification Data Sets for LSSVM with Polynomial Kernel, t=5,10,20 t=5,10,20
  Table 4 caption:
    table_text: TABLE 4 The Average Computational Time (in Seconds) of t t-BIF and
      t t-CV for LSSVM with Polynomial Kernel, t=5,10,20 t=5,10,20 and the Order of
      the Taylor Expansion r=5 r=5
  Table 5 caption:
    table_text: TABLE 5 The Average Test Errors ( % %) of t t-BIF and t t-CV for L2-SVM
      with the Order of the Taylor Expansion r=5 r=5
  Table 6 caption:
    table_text: TABLE 6 The Average Computational Time (in Seconds) of t t-BIF and
      t t-CV for L2-SVM with the Order of the Taylor Expansion r=5 r=5
  Table 7 caption:
    table_text: TABLE 7 The Average Test Errors ( % %) of t t-BIF and t t-CV on the
      Classification Data Sets for L1-SVM with the Order of Taylor Expansion r=5 r=5
  Table 8 caption:
    table_text: TABLE 8 The Average Computational Time (in Seconds) of t t-BIF and
      t t-CV for L1-SVM with t=5,10,20 t=5,10,20 and the Order of the Taylor Expansion
      r=5 r=5
  Table 9 caption:
    table_text: TABLE 9 The Testing Mean Square Error of t t-BIF and t t-CV for KRR,
      the Order of Taylor Expansion r=5 r=5
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2892371
- Affiliation of the first author: tusimple, beijing, china
  Affiliation of the last author: electronic information school, wuhan university,
    wuhan, china
  Figure 1 Link: articels_figures_by_rev_year\2019\Minimal_Case_Relative_Pose_Computation_Using_RayPointRay_Features\figure_1.jpg
  Figure 1 caption: "Two views of an RPR-structure with associated measurements. The\
    \ presented methods exploit a known angle between e x and e y to constrain the\
    \ relative pose between C and C \u2032 ."
  Figure 10 Link: articels_figures_by_rev_year\2019\Minimal_Case_Relative_Pose_Computation_Using_RayPointRay_Features\figure_10.jpg
  Figure 10 caption: Estimated visual odometry trajectories. (a)-(c) are results for
    EuRoC V101easy; (d)-(f) are results for EuRoC MH04difficult; (g)-(i) are results
    for MIT Stata Center 2012-01-27-07-37-01.bag dataset. Colorful lines are estimated
    trajectories, and black lines are ground truth trajectories. Best viewed in color.
  Figure 2 Link: articels_figures_by_rev_year\2019\Minimal_Case_Relative_Pose_Computation_Using_RayPointRay_Features\figure_2.jpg
  Figure 2 caption: (a) Relative rotation estimation from RPR correspondences. (b)
    Planarity constraints in two-view geometry.
  Figure 3 Link: articels_figures_by_rev_year\2019\Minimal_Case_Relative_Pose_Computation_Using_RayPointRay_Features\figure_3.jpg
  Figure 3 caption: Principles to determine whether two line segments compose a putative
    junction. Solid lines are detected line segments; dots are their intersections.
    (a) A valid junction; (b) invalid since one of the line segments is too short;
    (c) invalid since the intersection is too far from the segment endpoints; (d)
    invalid since the intersection is outside of the image plane.
  Figure 4 Link: articels_figures_by_rev_year\2019\Minimal_Case_Relative_Pose_Computation_Using_RayPointRay_Features\figure_4.jpg
  Figure 4 caption: Junctions and their support regions. In this scene, there are
    2 co-planar line segments in 3D space. In the image planes, solid lines are detected
    line segments, arrows are the angle bisectors of the detected junctions, and squares
    are the support regions for descriptor extraction.
  Figure 5 Link: articels_figures_by_rev_year\2019\Minimal_Case_Relative_Pose_Computation_Using_RayPointRay_Features\figure_5.jpg
  Figure 5 caption: Probability density functions over maximum algebraic residuals
    for pose estimation results in the 2RPR + 1point configuration.
  Figure 6 Link: articels_figures_by_rev_year\2019\Minimal_Case_Relative_Pose_Computation_Using_RayPointRay_Features\figure_6.jpg
  Figure 6 caption: Test image pair from Castle-P19 dataset [51] with annotated RPR
    features.
  Figure 7 Link: articels_figures_by_rev_year\2019\Minimal_Case_Relative_Pose_Computation_Using_RayPointRay_Features\figure_7.jpg
  Figure 7 caption: Pose estimation accuracy of the 1RPR + 1point method under forward
    and sideways motion with varying image noise.
  Figure 8 Link: articels_figures_by_rev_year\2019\Minimal_Case_Relative_Pose_Computation_Using_RayPointRay_Features\figure_8.jpg
  Figure 8 caption: Pose estimation accuracy of the 1RPR + 1point method under forward
    and sideways motion with varying rotation axis error. The image noise level is
    fixed to 0.5 pixels in standard deviation.
  Figure 9 Link: articels_figures_by_rev_year\2019\Minimal_Case_Relative_Pose_Computation_Using_RayPointRay_Features\figure_9.jpg
  Figure 9 caption: Pose estimation accuracy of the 1RPR method under non-holomic
    motion. (a)(b) with varying image noise; (c)(d) with varying angular deviation
    from the assumed planar motion. The image noise level in the latter experiment
    is fixed to 0.5 pixels in standard deviation.
  First author gender probability: 0.78
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.97
  Name of the first author: Ji Zhao
  Name of the last author: Jiayi Ma
  Number of Figures: 11
  Number of Tables: 3
  Number of authors: 4
  Paper title: Minimal Case Relative Pose Computation Using Ray-Point-Ray Features
  Publication Date: 2019-01-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Unknowns and Constraints for Geometric Primitives
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Configurations for Relative Pose Estimation from Assorted
      Combinations of RPR-Structures and Points
  Table 3 caption:
    table_text: TABLE 3 RMSE for Image Sequences from the EuRoc MAV and the MIT Stata
      Center
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2892372
- Affiliation of the first author: college of system engineering, national university
    of defense technology, changsha, china
  Affiliation of the last author: department of electrical and computer engineering,
    national university of singapore, singapore
  Figure 1 Link: articels_figures_by_rev_year\2019\Online_Meta_Adaptation_for_Fast_Video_Object_Segmentation\figure_1.jpg
  Figure 1 caption: Performance versus runtime comparisons on the DAVIS-16 validation
    dataset [38]. MVOS denotes the proposed method only adapted on the first frame,
    while the numbers 1, 2, or 3 denote the iteration counts for meta adaption. MVOS-OL
    denotes the proposed online adaptation strategy along with MVOS-3. Compared with
    state-of-the-arts, the proposed method is faster and achieves competitive performance.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Online_Meta_Adaptation_for_Fast_Video_Object_Segmentation\figure_2.jpg
  Figure 2 caption: "Overall architecture of the proposed method. Our framework consists\
    \ of training the meta-learner and online adaptation when testing. The meta-learner\
    \ is learned across a series of similar tasks, where each task is to segment two\
    \ frames, k and k+\u03B5 . At testing, the parameters \u03B8 i of the segmentation\
    \ model at frame i are calculated by the trained meta-learner and the gradients\
    \ from frame 1 and i\u22121 . Benefiting from the meta-learner, the proposed method\
    \ can fast and continuously adapt the segmentation network. Best viewed in color."
  Figure 3 Link: articels_figures_by_rev_year\2019\Online_Meta_Adaptation_for_Fast_Video_Object_Segmentation\figure_3.jpg
  Figure 3 caption: Performance (mean G ) variations of different methods over time
    on DAVIS-16 dataset. Best viewed in color.
  Figure 4 Link: articels_figures_by_rev_year\2019\Online_Meta_Adaptation_for_Fast_Video_Object_Segmentation\figure_4.jpg
  Figure 4 caption: Visualization of the intermediate features from the modules of
    Res4, ASSP, and Decoder. The top row is the features from the segmentation network
    directly processing the test frame while the bottom row is the ones with one iteration
    of meta-adaptation on the first frame. Best viewed in color.
  Figure 5 Link: articels_figures_by_rev_year\2019\Online_Meta_Adaptation_for_Fast_Video_Object_Segmentation\figure_5.jpg
  Figure 5 caption: Qualitative comparisons between the proposed MVOS and MVOS-OL.
  Figure 6 Link: articels_figures_by_rev_year\2019\Online_Meta_Adaptation_for_Fast_Video_Object_Segmentation\figure_6.jpg
  Figure 6 caption: Qualitative results of the proposed MVOS-OL on datasets DAVIS
    and Youtube-Objects. The first column is the first frame of a specific sequence
    with its corresponding annotation. The other columns are the segmentation results
    by our MVOS-OL. Best viewed in color.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.57
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Huaxin Xiao
  Name of the last author: Jiashi Feng
  Number of Figures: 6
  Number of Tables: 5
  Number of authors: 5
  Paper title: Online Meta Adaptation for Fast Video Object Segmentation
  Publication Date: 2019-01-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Comparison of Mean Region Similarity J J, Mean
      Contour Accuracy F F, Global Mean G G and Average Per-Frame Runtime on the Benchmark
      Datasets of DAVIS-16 and DAVIS-17
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Comparison of Per-Category Mean Region Similarity
      J J on Youtube-Objects Dataset
  Table 3 caption:
    table_text: TABLE 3 Meta-Learning Ablation Experiments
  Table 4 caption:
    table_text: TABLE 4 Influence of Different Modules in the Segmentation Network
      to Learn
  Table 5 caption:
    table_text: TABLE 5 Online Adaptation Ablation Experiments with the Metric of
      Mean G G
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2890659
- Affiliation of the first author: school of data and computer science, sun yat-sen
    university, guangzhou, china
  Affiliation of the last author: school of data and computer science, sun yat-sen
    university, guangzhou, china
  Figure 1 Link: articels_figures_by_rev_year\2019\D_Human_Pose_Machines_with_SelfSupervised_Learning\figure_1.jpg
  Figure 1 caption: Some visual results of our approach on the Human3.6M benchmark
    [5]. (a) illustrates the intermediate 3D poses estimated by the 2D-to-3D pose
    transformer module, (b) denotes the final 3D poses refined by the 3D-to-2D pose
    projector module, and (c) denotes the ground-truth. The estimated 3D joints are
    reprojected into the images and shown by themselves from the side view (next to
    the images). As shown, the predicted 3D poses in (b) have been significantly corrected,
    compared with (a). Best viewed in color. Note that, red and green indicate left
    and right, respectively.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\D_Human_Pose_Machines_with_SelfSupervised_Learning\figure_2.jpg
  Figure 2 caption: 'An overview of the proposed 3D human pose machine framework.
    Our model predicts the 3D human poses for the given monocular image frames, and
    it progressively refines its predictions with the proposed self-supervised correction.
    Specifically, the estimated 2D pose p 2d t with the corresponding pose representation
    f 2d t for each frame of the input sequence is first obtained and further passed
    into two neural network modules: i) a 2D-to-3D pose transformer module for transforming
    the pose representations from the 2D domain to the 3D domain to intermediately
    predict the human joints p 3d t in the 3D coordinates, and ii) a 3D-to-2D pose
    projector module to obtain the projected 2D pose p 2d t after regressing p 3d
    t into p 3d t . Through minimizing the difference between p 2d t and p 2d t ,
    our model is capable of bidirectionally refining the regressed 3D poses p 3d t
    via the proposed self-supervised correction mechanism. Note that the parameters
    of the 2D-to-3D pose transformer module for all frames are shared to preserve
    the temporal motion coherence. 3K and 2K denotes the dimension of the vector for
    representing the 3D and 2D human pose formed by K skeleton joints, respectively.'
  Figure 3 Link: articels_figures_by_rev_year\2019\D_Human_Pose_Machines_with_SelfSupervised_Learning\figure_3.jpg
  Figure 3 caption: Detailed sub-network architecture of our proposed 3D-to-2D pose
    projector module in the (a) training phase and (b) testing phase. The Fully Connected
    (FC) layers for the regression function are in blue, while those for the projection
    function are in yellow. The black arrows represent the forward data flow, while
    the dashed arrows denote the backward propagation used to update the network parameters
    and perform gradual pose refinement in (a) and (b), respectively.
  Figure 4 Link: articels_figures_by_rev_year\2019\D_Human_Pose_Machines_with_SelfSupervised_Learning\figure_4.jpg
  Figure 4 caption: Qualitative comparisons on the Human3.6M dataset. The 3D poses
    are visualized from the side view, and the cameras are depicted. The results from
    Zhou et al. [14], Pavlakos et al. [19], Lin et al. [21], Zhou et al. [23], Tome
    et al. [20], our model and the ground truth are illustrated from left to right.
    Our model achieves considerably more accurate estimations than all the compared
    methods. Best viewed in color. Red and green indicate left and right, respectively.
  Figure 5 Link: articels_figures_by_rev_year\2019\D_Human_Pose_Machines_with_SelfSupervised_Learning\figure_5.jpg
  Figure 5 caption: Qualitative comparisons of ours and ours wo self-correction on
    the Human3.6M dataset. The input image, estimated 2D pose, ours wo self-correction,
    ours and ground truth are listed from left to right, respectively. With the ground
    truth as reference, one can easily observe that the inaccurately predicted human
    3D joints in ours wo self-correction are effectively corrected in ours. Best viewed
    in color. Red and green indicate left and right, respectively.
  Figure 6 Link: articels_figures_by_rev_year\2019\D_Human_Pose_Machines_with_SelfSupervised_Learning\figure_6.jpg
  Figure 6 caption: Some qualitative comparisons of our model and Zhou et al. [23]
    on two representative datasets in the wild, i.e., KTH Football II [60] (first
    row) and MPII datasets [48] (the remaining rows). For each image, the original
    viewpoint and a better viewpoint are illustrated. Best viewed in color. Red and
    green indicate left and right, respectively.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.73
  Name of the first author: Keze Wang
  Name of the last author: Pengxu Wei
  Number of Figures: 6
  Number of Tables: 5
  Number of authors: 5
  Paper title: 3D Human Pose Machines with Self-Supervised Learning
  Publication Date: 2019-01-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Details of the Convolutional Layers in the 2D Pose Sub-Network
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Comparisons on the Human3.6M Dataset Using 3D
      Pose Errors (in millimeters)
  Table 3 caption:
    table_text: "TABLE 3 Quantitative Comparisons on the HumanEva-I Dataset Using\
      \ 3D Pose Errors (in millimeters) for the \u201Cwalking\u201D, \u201Cjogging\u201D\
      \ and \u201Cboxing\u201D Sequences"
  Table 4 caption:
    table_text: TABLE 4 Comparison of the Average Running Time (milliseconds per image)
      on the Human3.6M Benchmark
  Table 5 caption:
    table_text: TABLE 5 Empirical Comparisons Under Different Settings for Ablation
      Study Using Protocol 1
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2892452
