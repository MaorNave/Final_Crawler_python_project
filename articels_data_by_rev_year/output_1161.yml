- Affiliation of the first author: department of electrical and computer engineering,
    northeastern university, boston, ma, usa
  Affiliation of the last author: department of electrical and computer engineering,
    northeastern university, boston, ma, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\CrossGeneration_Kinship_Verification_with_Sparse_Discriminative_Metric\figure_1.jpg
  Figure 1 caption: Illustration of our cross-generation generative kinship verification
    framework. The towards-young generative model is proposed to first generate young
    parents from its input old images and then the second stage network deal with
    the identity variation for a family pair with age gap mitigated and a newly-designed
    Sparse Discriminative Metric Loss (SDM-Loss), which is exploited to involve the
    positive and negative information.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\CrossGeneration_Kinship_Verification_with_Sparse_Discriminative_Metric\figure_2.jpg
  Figure 2 caption: Example results from aged input to generated younger face on the
    FIW dataset. The first and third rows show the input image, second and fourth
    rows results show each corresponding generated output from our first-stage age
    GAN model.
  Figure 3 Link: articels_figures_by_rev_year\2018\CrossGeneration_Kinship_Verification_with_Sparse_Discriminative_Metric\figure_3.jpg
  Figure 3 caption: Verification results of each fold of father-daughter relationship
    with different threshold for age-to-young face translation. Baseline means no
    face were translated to young.
  Figure 4 Link: articels_figures_by_rev_year\2018\CrossGeneration_Kinship_Verification_with_Sparse_Discriminative_Metric\figure_4.jpg
  Figure 4 caption: Relationship specific ROC curves depicting performance of each
    method. All methods on ResNet feature are reported.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Shuyang Wang
  Name of the last author: Yun Fu
  Number of Figures: 4
  Number of Tables: 1
  Number of authors: 3
  Paper title: Cross-Generation Kinship Verification with Sparse Discriminative Metric
  Publication Date: 2018-08-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Verification Results ( % %) for 5-Fold Experiment on FIW
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2861871
- Affiliation of the first author: school of computer science, tel aviv university,
    tel aviv, israel
  Affiliation of the last author: school of computer science, tel aviv university,
    tel aviv, israel
  Figure 1 Link: articels_figures_by_rev_year\2018\Unsupervised_Generation_of_FreeForm_and_Parameterized_Avatars\figure_1.jpg
  Figure 1 caption: (a) Formal portrait of Albert Einstein. (b) A caricature by Hanoch
    Piven.
  Figure 10 Link: articels_figures_by_rev_year\2018\Unsupervised_Generation_of_FreeForm_and_Parameterized_Avatars\figure_10.jpg
  Figure 10 caption: Shown, side by side are sample images from the CelebA dataset
    and the results obtained by the DANN domain adaptation method [8]. These results
    are not competitive.
  Figure 2 Link: articels_figures_by_rev_year\2018\Unsupervised_Generation_of_FreeForm_and_Parameterized_Avatars\figure_2.jpg
  Figure 2 caption: From the image on the top left, our method computes the parameters
    of the face caricature below it, which can be rendered at multiple views and with
    varying expressions by the computer graphics engine.
  Figure 3 Link: articels_figures_by_rev_year\2018\Unsupervised_Generation_of_FreeForm_and_Parameterized_Avatars\figure_3.jpg
  Figure 3 caption: "The domain shift configurations discussed in Section 3. (a) The\
    \ unsupervised domain adaptation problem. The algorithm minimizes the risk in\
    \ a target domain using training samples ( x j \u223C D S , y S ( x j )) m j=1\
    \ and x i \u223C D T n i=1 . (b) The unsupervised domain transfer problem. In\
    \ this case, the algorithm learns a function G and is being tested on D 1 . The\
    \ algorithm is aided with two datasets: x i \u223C D 1 m i=1 and y( x j )\u223C\
    \ D y 2 n j=1 . For example, in the facial emoji application, D 1 is the distribution\
    \ of facial photos and D 2 is the (unseen) distribution of faces from which the\
    \ observed emoji were generated. (c) The tied output synthesis problem, in which\
    \ we are give a set of samples from one input domain x i \u223C D 1 , and matching\
    \ samples from two tied output domains: (e( c j ), c j )| c j \u223C D Y 2 ."
  Figure 4 Link: articels_figures_by_rev_year\2018\Unsupervised_Generation_of_FreeForm_and_Parameterized_Avatars\figure_4.jpg
  Figure 4 caption: "Illustrations of the domain shift scenarios depicted in Section\
    \ 3. (a) Unsupervised domain adaptation. Each node contains a distribution. The\
    \ horizontal edges denote the mappings between the distributions and the learned\
    \ function is h=g\u2218f . The vertical edges denote the discrepancy between the\
    \ the two distributions f\u2218 D S and f\u2218 D T and the risk between y and\
    \ h on D S . (b) Domain Transfer Network (DTN). The learned function is h=g\u2218\
    f . The horizontal two-sided edges denote the TID and f -constancy risks that\
    \ are used by the algorithm. The vertical two-sided edge stands for the discrepancy\
    \ between D y 2 =y\u2218 D 2 and h\u2218 D 1 . The dashed edges stand for the\
    \ h -constancy risk that is required only in Theorem 1. (c) Tied Output Synthesis.\
    \ The unknown function y is learned by the approximation h=c\u2218g\u2218f . f\
    \ and e are given. D 1 is the distribution of input images at test time. During\
    \ training, we observe tied mappings (y(x),e(y(x))) for unknown samples x\u223C\
    \ D 2 as well as unlabeled samples from the other distribution D 1 . The risks\
    \ that are shared with DTN are omitted for clarity. Figure credit: (a) and (b)\
    \ are borrowed from [7]."
  Figure 5 Link: articels_figures_by_rev_year\2018\Unsupervised_Generation_of_FreeForm_and_Parameterized_Avatars\figure_5.jpg
  Figure 5 caption: "(a) The training constraints for unsupervised domain adaptation\
    \ applied to the problem of recovering the avatar parameters given an input image.\
    \ The learned functions are p,l and d . grad. revesal stands for the gradient\
    \ reversal operation during back propagation. The mapping e is assumed to be known\
    \ a-priori and is used only during inference time to generate an emoji from the\
    \ configurations. (b) The training constraints of the Domain Transfer Network\
    \ method. The learned functions are d and G=g\u2218f , for a given f . The dashed\
    \ lines denote loss terms. (c) The training constraints of the Tied Output Synthesis\
    \ Network. The learned functions are c , d , and G=g\u2218f , for a given f .\
    \ The mapping e is assumed to be known a-priori. The dashed lines denote loss\
    \ terms."
  Figure 6 Link: articels_figures_by_rev_year\2018\Unsupervised_Generation_of_FreeForm_and_Parameterized_Avatars\figure_6.jpg
  Figure 6 caption: Domain transfer from SVHN domain to MNIST domain. Input in odd
    columns; output in even columns.
  Figure 7 Link: articels_figures_by_rev_year\2018\Unsupervised_Generation_of_FreeForm_and_Parameterized_Avatars\figure_7.jpg
  Figure 7 caption: A random subset of the digit 3 from SVHN, transferred to MNIST.
    (a) The input images. (b) The results of our DTN. In all plots, the cases keep
    their respective locations, and are sorted by the probability of 3, as inferred
    by the MNIST classifier on the results of our DTN. (c) The obtained results, in
    which the digit 3 was not shown as part of the set s unlabeled samples from SVNH.
    (d) The obtained results, in which the digit 3 was not shown as part of the set
    t of unlabeled samples in MNIST. (e) The digit 3 was not shown in both s and t
    . (f) The digit 3 was not shown in s , t , and during the training of f .
  Figure 8 Link: articels_figures_by_rev_year\2018\Unsupervised_Generation_of_FreeForm_and_Parameterized_Avatars\figure_8.jpg
  Figure 8 caption: 'Toy problem. (a) Polygon images with three random parameters:
    Number of vertices, radius of enclosing circle and rotation. (b) GAN generated
    images mimicking the class of polygon images. (c) G(x) images created by TOS.
    The TOS is able to benefit from the synthesis engine e and produces images that
    are noticeably more compliant than the GAN.'
  Figure 9 Link: articels_figures_by_rev_year\2018\Unsupervised_Generation_of_FreeForm_and_Parameterized_Avatars\figure_9.jpg
  Figure 9 caption: 'Shown, side by side, are (a) sample images from the CelebA dataset.
    (b) Emoji, from left to right: The images created manually using a web interface
    (for evaluation only), the result of DTN, and the two results of our TOS: G(x)
    and then e(c(G(x))) . (c) VR avatar results: DTN, the two TOS results, and a 3D
    rendering of the resulting configuration file. See Table 5 for retrieval performance.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Adam Polyak
  Name of the last author: Lior Wolf
  Number of Figures: 11
  Number of Tables: 5
  Number of authors: 3
  Paper title: Unsupervised Generation of Free-Form and Parameterized Avatars
  Publication Date: 2018-08-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Architectures of Networks Used in Experiments of Section 6
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Accuracy of the MNIST Classifier on the Sampled Transferred
      by Our DTN Method from SHVN to MNIST
  Table 3 caption:
    table_text: TABLE 3 Domain Adaptation from SVHN to MNIST
  Table 4 caption:
    table_text: TABLE 4 Comparison of Recognition Accuracy of the Digit 3 as Generated
      in MNIST
  Table 5 caption:
    table_text: TABLE 5 A Comparison of Median Rank for Retrieval Out of a Set of
      100,001 Face Images for Either Manually Created Emoji, or Emoji and VR Avatars
      Created by DTN or TOS
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2863282
- Affiliation of the first author: department of computer science and engineering,
    sungkyunkwan university (skku), suwon, south korea
  Affiliation of the last author: department of computer science, the university of
    texas at austin, austin, tx, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\NonExhaustive_Overlapping_Clustering\figure_1.jpg
  Figure 1 caption: (a) Two ground-truth clusters are generated. Green points indicate
    overlap between the clusters, and black points indicate outliers. (b) When we
    only constrain the number of assignments, the solution contains too many false
    positive outliers. (c) The NEO-K-Means objective defined in (2) adds an explicit
    term for non-exhaustiveness that enables it to correctly cluster the data.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\NonExhaustive_Overlapping_Clustering\figure_2.jpg
  Figure 2 caption: The penalty of the edge u, v 1 is zero, the penalty of the edge
    u, v 2 is 2link(u, v 2 ) , and the penalty of the edge u, v 3 is link(u, v 3 )
    .
  Figure 3 Link: articels_figures_by_rev_year\2018\NonExhaustive_Overlapping_Clustering\figure_3.jpg
  Figure 3 caption: The rank of the co-occurrence matrix Z .
  Figure 4 Link: articels_figures_by_rev_year\2018\NonExhaustive_Overlapping_Clustering\figure_4.jpg
  Figure 4 caption: Run times of SDP solvers and the LRSDP solver on synthetic datasets
    with different numbers of data points.
  Figure 5 Link: articels_figures_by_rev_year\2018\NonExhaustive_Overlapping_Clustering\figure_5.jpg
  Figure 5 caption: "Representative images from the clusters produced by the NEO-K-Means\
    \ method. The images on the overlapped region are annotated by both \u201Cwood\u201D\
    \ and \u201Cglass\u201D whereas the images which are not on the overlapped region\
    \ are annotated by either \u201Cwood\u201D or \u201Cglass\u201D."
  Figure 6 Link: articels_figures_by_rev_year\2018\NonExhaustive_Overlapping_Clustering\figure_6.jpg
  Figure 6 caption: "The clustering performance of the iterative NEO-K-Means method\
    \ with different \u03B1 values."
  Figure 7 Link: articels_figures_by_rev_year\2018\NonExhaustive_Overlapping_Clustering\figure_7.jpg
  Figure 7 caption: Output of the multilevel NEO-K-Means algorithm.
  Figure 8 Link: articels_figures_by_rev_year\2018\NonExhaustive_Overlapping_Clustering\figure_8.jpg
  Figure 8 caption: 'A synthetic study of overlapping community detection on a Watts-Strogatz
    cycle graph: (a) & (b) show the normalized cut and the error measure returned
    by the iterative NEO-K-Means algorithm with random initialization, the multilevel
    NEO-K-Means algorithm, and the iterative NEO-K-Means algorithm with the LRSDP
    initialization.'
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Joyce Jiyoung Whang
  Name of the last author: Inderjit S. Dhillon
  Number of Figures: 8
  Number of Tables: 7
  Number of authors: 4
  Paper title: Non-Exhaustive, Overlapping Clustering
  Publication Date: 2018-08-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Possible Relationships between C(u) C(u) and C(v) C(v) in
      a Non-Exhaustive, Overlapping Graph Clustering, and the Total Penalty of an
      Edge u,v u,v According to the Definition of the Normalized Cut
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Objective Values and the Run Time (in Seconds) of SDP
      (Italics) and Our LRSDP (Bold) Solvers on Real-World Data
  Table 3 caption:
    table_text: TABLE 3 Real-World Vector Datasets
  Table 4 caption:
    table_text: TABLE 4 The NEO-K-Means Objective Function Values and Run Times of
      the Iterative NEO-K-Means Algorithm with Four Different Initializations on the
      yeast Dataset
  Table 5 caption:
    table_text: TABLE 5 The Average F1, Pairwise F1, and NMI Scores
  Table 6 caption:
    table_text: TABLE 6 Real-World Graph Datasets
  Table 7 caption:
    table_text: TABLE 7 AUC Scores (%) on Real-World Graphs in Terms of Conductance,
      Weighted Conductance, and Modularity
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2863278
- Affiliation of the first author: school of data and computer science, sun yat-sen
    university, guangzhou, china
  Affiliation of the last author: school of science and engineering (computing), university
    of dundee, dundee, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2018\Early_Action_Prediction_by_Soft_Regression\figure_1.jpg
  Figure 1 caption: "Snapshots from activities calling with a cell-phone (the first\
    \ row) and playing with a cell-phone (the second row). As presented in the first\
    \ row, it is hard to recognize the action when its progress level is less than\
    \ 2. However, if the segment with temporal interval [1, 2] (marked with red box)\
    \ are provided, it becomes clear that the subject is performing the action calling\
    \ with a cell-phone. We also observe that the subsequences at progress level 1\
    \ (temporal interval [0, 1]) in the two activities contain the same action \u201C\
    taking out a cell-phone\u201D."
  Figure 10 Link: articels_figures_by_rev_year\2018\Early_Action_Prediction_by_Soft_Regression\figure_10.jpg
  Figure 10 caption: Illustration of the convergence of our MSRNN model.
  Figure 2 Link: articels_figures_by_rev_year\2018\Early_Action_Prediction_by_Soft_Regression\figure_2.jpg
  Figure 2 caption: "A graphical illustration of the training principle for our soft\
    \ regression based early action prediction framework. In this framework, we develop\
    \ a novel online RGB-D action feature extractor and a soft regression model. The\
    \ early action predictor and soft labels are learned jointly. In the figure, operator\
    \ \u2A02 is a Kronecker product and \u2296 for the loss computation. Each curve\
    \ in the second block (from the left hand side) corresponds to one dimension of\
    \ the computed integral features. For clarity and simplicity, we only present\
    \ a small set of the extracted integral features."
  Figure 3 Link: articels_figures_by_rev_year\2018\Early_Action_Prediction_by_Soft_Regression\figure_3.jpg
  Figure 3 caption: Illustration of the main difference between our soft linear regression
    (SLR) model and soft RNN (SRNN) model. In the SLR model, prediction is achieved
    based on the currently observed subsequences. While in the SRNN model, the dependency
    between all the sequentially arrived subsequences is explicitly explored by a
    temporally connected hidden layer. Hence, the output is obtained based on both
    the currently observing subsequence and historical subsequences. The objectives
    of both the SLR and SRNN models are to minimize the loss between outputs and the
    corresponding soft labels.
  Figure 4 Link: articels_figures_by_rev_year\2018\Early_Action_Prediction_by_Soft_Regression\figure_4.jpg
  Figure 4 caption: Some examples from the ORGBD, SYSU 3DHOI and NTU Large Scale datasets.
    The first, third and fifth row present RGB snapshots from ORGBD, SYSU 3DHOI and
    NTU Large Scale set, respectively. The second, fourth and sixth row present the
    corresponding depth images.
  Figure 5 Link: articels_figures_by_rev_year\2018\Early_Action_Prediction_by_Soft_Regression\figure_5.jpg
  Figure 5 caption: "Comparison results on the ORGBD (a), SYSU 3DHOI (b), and NTU\
    \ Large Scale (c) sets. \u201CMP\u201D, \u201CDCSF\u201D, and \u201CDO\u201D denote\
    \ \u201CMoving Pose [10], [67]\u201D, \u201CDSTIP+DCSF [61], [67]\u201D, and \u201C\
    Discriminative Order-let [67]\u201D, respectively. Best viewed in color."
  Figure 6 Link: articels_figures_by_rev_year\2018\Early_Action_Prediction_by_Soft_Regression\figure_6.jpg
  Figure 6 caption: An example from the SYSU dataset with prediction scores. We uniformly
    selected 10 snapshots from a sequence of calling with a phone for this illustration.
    The top row of the figure shows the confidences of our model correctly predicting
    the underlying subsequences observed until the time of each respected snapshot.
    It could be observed that the actor has completed the action execution in the
    last two snapshots, which is not helpful for prediction and thus becomes redundant.
  Figure 7 Link: articels_figures_by_rev_year\2018\Early_Action_Prediction_by_Soft_Regression\figure_7.jpg
  Figure 7 caption: Prediction scores for eight specific samples from different action
    classes. The vertical axis indicates prediction scores and the horizontal axis
    is the progress level of subsequence.
  Figure 8 Link: articels_figures_by_rev_year\2018\Early_Action_Prediction_by_Soft_Regression\figure_8.jpg
  Figure 8 caption: Example soft labels learned on the SYSU 3D HOI set. The vertical
    axis indicates the values for the soft labels and the horizontal axis is the progress
    level of subsequence.
  Figure 9 Link: articels_figures_by_rev_year\2018\Early_Action_Prediction_by_Soft_Regression\figure_9.jpg
  Figure 9 caption: "More evaluations on the system performance. \u201CAUC\u201D indicates\
    \ the area under all the presented observation ratios. Best viewed in color."
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jian-Fang Hu
  Name of the last author: Jianguo Zhang
  Number of Figures: 11
  Number of Tables: 7
  Number of authors: 6
  Paper title: Early Action Prediction by Soft Regression
  Publication Date: 2018-08-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Prediction (%) on ORGBD Set
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Prediction (%) on the SYSU 3D HOI Set
  Table 3 caption:
    table_text: TABLE 3 Prediction (%) on the NTU Large Scale Set
  Table 4 caption:
    table_text: TABLE 4 Comparison of our RGB-D Early Action Prediction Methods and
      the Conventional RGB Video based Early Action Prediction Approaches on the SYSU
      3D HOI Set
  Table 5 caption:
    table_text: TABLE 5 Evaluation on the Elements in RGB-D Early Action Prediction
      on SYSU 3D HOI Set
  Table 6 caption:
    table_text: TABLE 6 Evaluations of Different Soft Regression Models on SYSU 3D
      HOI Set (%)
  Table 7 caption:
    table_text: TABLE 7 The Comparison of Prediction Speed
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2863279
- Affiliation of the first author: school of computer science, northwestern polytechnical
    university, xian, china
  Affiliation of the last author: shanghaitech university, shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2018\A_Generic_MultiProjectionCenter_Model_and_Calibration_Method_for_Light_Field_Cam\figure_1.jpg
  Figure 1 caption: An illustration of three coordinates in the MPC model.
  Figure 10 Link: articels_figures_by_rev_year\2018\A_Generic_MultiProjectionCenter_Model_and_Calibration_Method_for_Light_Field_Cam\figure_10.jpg
  Figure 10 caption: Pose estimation results of our collected datasets.
  Figure 2 Link: articels_figures_by_rev_year\2018\A_Generic_MultiProjectionCenter_Model_and_Calibration_Method_for_Light_Field_Cam\figure_2.jpg
  Figure 2 caption: "Examples of transformations between 3D structure and 4D light\
    \ field of a Lambertian cube. The leftmost is an original cube and others are\
    \ the projected ones with the changing of parameter f , scaling in the image plane\
    \ k xy ( = k x = k y ) and translation (0,0, x 0 , y 0 ) \u22A4 respectively."
  Figure 3 Link: articels_figures_by_rev_year\2018\A_Generic_MultiProjectionCenter_Model_and_Calibration_Method_for_Light_Field_Cam\figure_3.jpg
  Figure 3 caption: Optical path of a conventional light field camera [1]. There are
    two MPC coordinates inside the camera and in the outer world with linear transformation,
    i.e., L f (i,j,u,v) and L F (I,J,U,V) respectively.
  Figure 4 Link: articels_figures_by_rev_year\2018\A_Generic_MultiProjectionCenter_Model_and_Calibration_Method_for_Light_Field_Cam\figure_4.jpg
  Figure 4 caption: Optical paths of focused light field cameras with different designs
    [2]. There is a conjugate MPC coordinate L F (I,J,U,V) in the outer world with
    the inner one L f (i,j,u,v) .
  Figure 5 Link: articels_figures_by_rev_year\2018\A_Generic_MultiProjectionCenter_Model_and_Calibration_Method_for_Light_Field_Cam\figure_5.jpg
  Figure 5 caption: The indexed pixels L(i,j,u,v) and decoded physical light field
    L(s,t,x,y) of light field cameras in two designs.
  Figure 6 Link: articels_figures_by_rev_year\2018\A_Generic_MultiProjectionCenter_Model_and_Calibration_Method_for_Light_Field_Cam\figure_6.jpg
  Figure 6 caption: Relative errors of intrinsic parameters on the simulated data
    with different number of poses and views.
  Figure 7 Link: articels_figures_by_rev_year\2018\A_Generic_MultiProjectionCenter_Model_and_Calibration_Method_for_Light_Field_Cam\figure_7.jpg
  Figure 7 caption: Standard deviations of relative errors of intrinsic parameters
    on the simulated data with different number of poses and views.
  Figure 8 Link: articels_figures_by_rev_year\2018\A_Generic_MultiProjectionCenter_Model_and_Calibration_Method_for_Light_Field_Cam\figure_8.jpg
  Figure 8 caption: Relative errors of intrinsic parameters on the simulated data
    with different noise levels from 0.1 to 1.5 pixels.
  Figure 9 Link: articels_figures_by_rev_year\2018\A_Generic_MultiProjectionCenter_Model_and_Calibration_Method_for_Light_Field_Cam\figure_9.jpg
  Figure 9 caption: Pose estimation results of datasets captured by [11]. Light fields
    used for calibration in Table 6 are indicated with bold red indexes of corresponding
    camera poses in Figs. (c-f).
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.99
  Name of the first author: Qi Zhang
  Name of the last author: Jingyi Yu
  Number of Figures: 17
  Number of Tables: 15
  Number of authors: 5
  Paper title: A Generic Multi-Projection-Center Model and Calibration Method for
    Light Field Cameras
  Publication Date: 2018-08-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Notation of Symbols in the Paper
  Table 10 caption:
    table_text: TABLE 10 Intrinsic Parameter Estimation Results of Our Collected Datasets
  Table 2 caption:
    table_text: TABLE 2 Intrinsic Parameter Configuration of the Simulated Light Field
      Camera
  Table 3 caption:
    table_text: 'TABLE 3 Min and Max Relative Errors of Intrinsic Parameters (Unit:
      %) on the Simulated Data When the Number of Poses Is Great Than 2'
  Table 4 caption:
    table_text: 'TABLE 4 RMS Ray Re-Projection Errors of Initial Parameter Estimation
      and Optimization with Distortion Rectification (Unit: mm mm )'
  Table 5 caption:
    table_text: 'TABLE 5 Mean Re-Projection Errors of Optimization with Distortion
      Rectification (Unit: pixel pixel )'
  Table 6 caption:
    table_text: TABLE 6 RMS Errors of Optimization with Distortion Rectification Using
      Fewer Poses
  Table 7 caption:
    table_text: TABLE 7 Intrinsic Parameter Estimation Results of Datasets Captured
      by [11]
  Table 8 caption:
    table_text: 'TABLE 8 RMS Re-Projection Error of Sub-Apertures in Dataset B (Uint:
      pixel pixel )'
  Table 9 caption:
    table_text: 'TABLE 9 RMS Ray Re-Projection Errors of Initial Parameter Estimation
      Optimizations without and with Distortion Rectification (Unit: mm mm )'
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2864617
- Affiliation of the first author: institute of physical science and information technology,
    anhui university, hefei, china
  Affiliation of the last author: school of engineering, university of california,
    merced, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\Visual_Tracking_via_Dynamic_Graph_Learning\figure_1.jpg
  Figure 1 caption: Illustration of the original, shrunk and expanded bounding boxes
    on 4 video frames with different challenges, which are represented by the red,
    pink and blue colors, respectively. The optimized patch weights are also shown
    for clarity, in which the hotter color indicates the larger weight. One can see
    that the optimized patch weights are beneficial to suppressing the effects of
    background clutter.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Visual_Tracking_via_Dynamic_Graph_Learning\figure_2.jpg
  Figure 2 caption: Illustration of clutter and noise effects of initial seeds on
    a video frame in the sequence boy [21], please refer to Fig. 1 for the detailed
    descriptions. One can see that the optimized patch weights are robust to clutter
    and noise, but bad if most of initial seeds are clutter and noise.
  Figure 3 Link: articels_figures_by_rev_year\2018\Visual_Tracking_via_Dynamic_Graph_Learning\figure_3.jpg
  Figure 3 caption: Precision and success plots of OPE (one-pass evaluation) [21]
    of the proposed tracker against other state-of-the-art trackers on OTB100. The
    representative score of PR is presented in the legend.
  Figure 4 Link: articels_figures_by_rev_year\2018\Visual_Tracking_via_Dynamic_Graph_Learning\figure_4.jpg
  Figure 4 caption: Sample results of our method against Struck [3], MEEM [5], MUSTer
    [30], DSST [31] and SCM [37].
  Figure 5 Link: articels_figures_by_rev_year\2018\Visual_Tracking_via_Dynamic_Graph_Learning\figure_5.jpg
  Figure 5 caption: PR and SR curves on the Temple Color dataset where ten trackers
    are shown here.
  Figure 6 Link: articels_figures_by_rev_year\2018\Visual_Tracking_via_Dynamic_Graph_Learning\figure_6.jpg
  Figure 6 caption: TRR curves on NUS-PRO, where twenty trackers are shown here.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Chenglong Li
  Name of the last author: Ming-Hsuan Yang
  Number of Figures: 6
  Number of Tables: 4
  Number of authors: 5
  Paper title: Visual Tracking via Dynamic Graph Learning
  Publication Date: 2018-08-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Attribute-based PR Scores on OTB Benchmark Compared with Recent
      Trackers, Where the Best Results of Deep and Non-Deep Trackers Divided by Dash
      Line Are in Red and Green Colors, Respectively
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Attribute-based SR Scores on OTB Benchmark Compared with Recent
      Trackers, Where the Best Results of Deep and Non-Deep Trackers Divided by Dash
      Line Are in Red and Green Colors, Respectively
  Table 3 caption:
    table_text: TABLE 3 Comparison of WPG Against Several Typical Trackers on the
      VOT2016 Challenge Dataset [41], Where the Best Results of Deep and Non-Deep
      Trackers Divided by Dash Line Are in Red and Green Colors, Respectively
  Table 4 caption:
    table_text: TABLE 4 Performance of 4 Variants of the Proposed Method Against the
      SOWP Method [7]
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2864965
- Affiliation of the first author: computer science, johns hopkins university, baltimore,
    md, usa
  Affiliation of the last author: media analytics, nec laboratories america inc, cupertino,
    ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\Deep_Supervision_with_Intermediate_Concepts\figure_1.jpg
  Figure 1 caption: Overview of our approach. We use synthetic training images with
    intermediate shape concepts to deeply supervise the hidden layers of a CNN. At
    test time, given a single real image of an object, we demonstrate accurate localization
    of semantic parts in 2D and 3D, while being robust to intra-class appearance variations
    and occlusions.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Deep_Supervision_with_Intermediate_Concepts\figure_2.jpg
  Figure 2 caption: Illustration of a concept hierarchy with three concepts Y= y 1
    , y 2 , y 3 on 2D input space. Black arrows indicate the finer decomposition within
    the previous concept in the hierarchy. Each color represents one individual class
    defined by the concept.
  Figure 3 Link: articels_figures_by_rev_year\2018\Deep_Supervision_with_Intermediate_Concepts\figure_3.jpg
  Figure 3 caption: Schematic diagram of deep supervision framework.
  Figure 4 Link: articels_figures_by_rev_year\2018\Deep_Supervision_with_Intermediate_Concepts\figure_4.jpg
  Figure 4 caption: Visualization of our rendering pipeline (top-left), DISCO network
    (bottom-left), an example of rendered image and its annotations of 2D keypoints
    (top-right) as well as 3D skeleton (bottom-right).
  Figure 5 Link: articels_figures_by_rev_year\2018\Deep_Supervision_with_Intermediate_Concepts\figure_5.jpg
  Figure 5 caption: Examples of synthesized training images for simulating the multi-car
    occlusion.
  Figure 6 Link: articels_figures_by_rev_year\2018\Deep_Supervision_with_Intermediate_Concepts\figure_6.jpg
  Figure 6 caption: 3D PCK (RMSE[27]) curves of DISCO and 3D-INN on sofa (Fig. 6a),
    chair (Fig. 6b) and bed (Fig. 6c) classes of IKEA dataset. In each figure, X axis
    stands for alpha of PCK and Y axis represents the accuracy.
  Figure 7 Link: articels_figures_by_rev_year\2018\Deep_Supervision_with_Intermediate_Concepts\figure_7.jpg
  Figure 7 caption: Visualization of 2D3D prediction, visibility inference and instance
    segmentation on KITTI-3D (left column) and PASCAL VOC (right column). Last row
    shows failure cases. Circles and lines represent keypoints and their connections.
    Red and green indicate the left and right sides of a car, orange lines connect
    two sides. Dashed lines connect keypoints if one of them is inferred to be occluded.
    Light blue masks present segmentation results.
  Figure 8 Link: articels_figures_by_rev_year\2018\Deep_Supervision_with_Intermediate_Concepts\figure_8.jpg
  Figure 8 caption: Qualitative comparison between 3D-INN and DISCO for 3D stricture
    prediction on IKEA dataset.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Chi Li
  Name of the last author: Manmohan Chandraker
  Number of Figures: 8
  Number of Tables: 7
  Number of authors: 6
  Paper title: Deep Supervision with Intermediate Concepts
  Publication Date: 2018-08-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Notation Table
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Classification Error of Different Methods on CIFAR100
  Table 3 caption:
    table_text: "TABLE 3 PCK[ \u03B1=0.1 \u03B1=0.1] Accuracies (%) of Different Methods\
      \ for 2D and 3D Keypoint Localization on KITTI-3D Dataset"
  Table 4 caption:
    table_text: TABLE 4 Ablative Study of Different Training Data Sources
  Table 5 caption:
    table_text: "TABLE 5 PCK[ \u03B1=0.1 \u03B1=0.1] Accuracies (%) of Different Methods\
      \ for 2D Keypoint Localization on the Car Category of PASCAL VOC"
  Table 6 caption:
    table_text: TABLE 6 Object Segmentation Accuracies (%) of Different Methods on
      PASCAL3D+
  Table 7 caption:
    table_text: "TABLE 7 Average Recall and PCK[ \u03B1=0.1 \u03B1=0.1] Accuracy(%)\
      \ for 3D Structure Prediction on the Sofa and Chair Classes on IKEA Dataset"
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2863285
- Affiliation of the first author: inria, psl research university, paris, france
  Affiliation of the last author: czech institute of informatics, robotics and cybernetics,
    czech technical university in prague, praha 6, czechia
  Figure 1 Link: articels_figures_by_rev_year\2018\Convolutional_Neural_Network_Architecture_for_Geometric_Matching\figure_1.jpg
  Figure 1 caption: 'Top: The proposed model can be trained from synthetic image pairs,
    avoiding the need for manual annotation. Bottom: At evaluation time, the trained
    geometry estimation network automatically aligns two images with substantial appearance
    differences. It is able to estimate large deformable transformations robustly
    in the presence of clutter.'
  Figure 10 Link: articels_figures_by_rev_year\2018\Convolutional_Neural_Network_Architecture_for_Geometric_Matching\figure_10.jpg
  Figure 10 caption: Qualitative results on the TSS dataset. Each row shows one test
    example from the TSS dataset. The last column shows the ground-truth alignment
    used for evaluation.
  Figure 2 Link: articels_figures_by_rev_year\2018\Convolutional_Neural_Network_Architecture_for_Geometric_Matching\figure_2.jpg
  Figure 2 caption: Diagram of the proposed architecture. Images I A and I B are passed
    through feature extraction networks which have tied parameters W , followed by
    a matching network which matches the descriptors. The output of the matching network
    is passed through a regression network which outputs the parameters of the geometric
    transformation.
  Figure 3 Link: articels_figures_by_rev_year\2018\Convolutional_Neural_Network_Architecture_for_Geometric_Matching\figure_3.jpg
  Figure 3 caption: "Correlation map computation with CNN features. The correlation\
    \ map c AB contains all pairwise similarities between individual features f A\
    \ \u2208 f A and f B \u2208 f B . At a particular spatial location (i,j) the correlation\
    \ map output c AB contains all the similarities between f B (i,j) and all f A\
    \ \u2208 f A ."
  Figure 4 Link: articels_figures_by_rev_year\2018\Convolutional_Neural_Network_Architecture_for_Geometric_Matching\figure_4.jpg
  Figure 4 caption: Architecture of the regression network. It is composed of two
    convolutional layers without padding and stride equal to 1, followed by batch
    normalization and ReLU, and a final fully connected layer which regresses to the
    P transformation parameters.
  Figure 5 Link: articels_figures_by_rev_year\2018\Convolutional_Neural_Network_Architecture_for_Geometric_Matching\figure_5.jpg
  Figure 5 caption: "Thin-plate spline control points. Illustration of the 3\xD73\
    \ TPS grid of control points used in the thin-plate spline transformation model."
  Figure 6 Link: articels_figures_by_rev_year\2018\Convolutional_Neural_Network_Architecture_for_Geometric_Matching\figure_6.jpg
  Figure 6 caption: "Estimating progressively more complex geometric transformations.\
    \ Images A and B are passed through a network which estimates an affine transformation\
    \ with parameters \u03B8 AFF (see Fig. 2). Image A is then warped using this transformation\
    \ to roughly align it with B , and passed along with B through a second network\
    \ which estimates a thin-plate spline (TPS) transformation that refines the alignment."
  Figure 7 Link: articels_figures_by_rev_year\2018\Convolutional_Neural_Network_Architecture_for_Geometric_Matching\figure_7.jpg
  Figure 7 caption: "Iterative transformation refinement. In iteration i, image A\
    \ is warped using the cumulative transformation estimate \u03B8 (i\u22121) c obtained\
    \ from the previous iteration ( \u03B8 (0) c is initialized to identity). A fine\
    \ alignment, \u03B8 (i) , between image B and the warped image A is estimated\
    \ and chained onto \u03B8 (i\u22121) c to form the refined cumulative transformation\
    \ estimate \u03B8 (i) c ."
  Figure 8 Link: articels_figures_by_rev_year\2018\Convolutional_Neural_Network_Architecture_for_Geometric_Matching\figure_8.jpg
  Figure 8 caption: Synthetic image generation. Symmetric padding is added to the
    original image to enlarge the sampling region, its central crop is used as image
    A , and image B is created by performing a randomly sampled transformation mathcal
    Ttheta GT .
  Figure 9 Link: articels_figures_by_rev_year\2018\Convolutional_Neural_Network_Architecture_for_Geometric_Matching\figure_9.jpg
  Figure 9 caption: Qualitative results on the PF dataset. Each row shows one test
    example from the Proposal Flow dataset. Ground truth matching keypoints, only
    used for alignment evaluation, are depicted as crosses and circles for images
    A and B , respectively. Keypoints of same color are supposed to match each other
    after image A is aligned to image B . To illustrate the matching error, we also
    overlay keypoints of B onto different alignments of A so that lines that connect
    matching keypoints indicate the keypoint position error vector. Our method manages
    to roughly align the images with an affine transformation (column 2), and then
    perform finer alignment using thin-plate spline (TPS, column 3). The top two examples
    are from the PF-WILLOW dataset while the bottom one is from PF-PASCAL.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Ignacio Rocco
  Name of the last author: Josef Sivic
  Number of Figures: 14
  Number of Tables: 6
  Number of authors: 3
  Paper title: Convolutional Neural Network Architecture for Geometric Matching
  Publication Date: 2018-08-13 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Matching Accuracy on the PF Dataset (PF-PASCALPF-WILLOW)\
      \ Measured in Terms of the PCK ( \u03B1 \u03B1 = 0.1)"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Matching Accuracy on the TSS Dataset in Terms of PCK ( \u03B1\
      =0.05 \u03B1=0.05)"
  Table 3 caption:
    table_text: TABLE 3 Evaluation on the Caltech-101 Dataset
  Table 4 caption:
    table_text: TABLE 4 Evaluation on the Graffiti Benchmark
  Table 5 caption:
    table_text: TABLE 5 Ablation Studies on the Matching Layer and Effect of the Training
      Dataset
  Table 6 caption:
    table_text: TABLE 6 Evaluation of Robustness to Occlusions in Terms of PCK on
      the PF-WILLOW Dataset
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2865351
- Affiliation of the first author: australian centre for robotic vision, the university
    of adelaide, adelaide, australia
  Affiliation of the last author: school of engineering, university of california,
    merced, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\Robust_Visual_Tracking_via_Hierarchical_Convolutional_Features\figure_1.jpg
  Figure 1 caption: Visualization of hierarchical deep features. Convolutional layers
    of a typical CNN model, e.g., AlexNet [17] or VGGNet [25], provide multiple levels
    of abstraction in the feature hierarchies. We visualize a sample frame using the
    VGGNet-19 [25] to extract CNN features from the first to fifth convolutional layers.
    Deep features in the earlier layers retain higher spatial resolution for precise
    localization with low-level visual information similar to the response map of
    Gabor filters [26]. On the other hand, features in the latter layers capture more
    semantic information and less fine-grained spatial details. Our approach aims
    to exploit the semantic information of last layers (right) to handle large appearance
    changes and alleviate drifting by using features of earlier layers (left) for
    precise localization.
  Figure 10 Link: articels_figures_by_rev_year\2018\Robust_Visual_Tracking_via_Hierarchical_Convolutional_Features\figure_10.jpg
  Figure 10 caption: 'Ablation study. Component analysis on the OTB2015 [36] dataset
    with comparisons to the HCFT [76]. SW: soft weight. SP: scale proposals for scale
    estimation. DP: detection proposals for target re-detection. The values at the
    legends with distance precision is based on the threshold of 20 pixels while values
    at the legend of overlap success are based on area under the curve. The proposed
    HCFT incorporates all of these components and achieves performance gains of +3.3
    and +3.6 percent in distance precision and overlap success when compared to the
    HCFT.'
  Figure 2 Link: articels_figures_by_rev_year\2018\Robust_Visual_Tracking_via_Hierarchical_Convolutional_Features\figure_2.jpg
  Figure 2 caption: Spatial resolution of CNN features. Visualization of the CNN features
    of a toy image with a horizontal step edge using VGGNet [25]. We visualize each
    CNN layer by converting its first three principle components of convolutional
    channels to RGB values. Intensities of the dash lines are visualized on the right.
    Note that the conv5-4 layer is less effective in locating the step edge due to
    its low spatial resolution, while the conv3-4 layer is more useful for precise
    localization.
  Figure 3 Link: articels_figures_by_rev_year\2018\Robust_Visual_Tracking_via_Hierarchical_Convolutional_Features\figure_3.jpg
  Figure 3 caption: Main steps of the proposed algorithm. Given an image, we first
    crop the search window centered at the estimated position in the previous frame.
    We use the third, fourth and fifth convolutional layers as our target object representations
    (Section 4.1). Each layer indexed by i is then convolved with the learned linear
    correlation filter w (i) to generate a response map, whose location of the maximum
    value indicates the estimated target position (Section 4.2). We search the multi-level
    response maps to infer the target location in a coarse-to-fine fashion (Section
    4.3). Centered at the estimated target position, we crop an image patch using
    the same scale in the previous frame. We apply the long-term memory filter w L
    to compute the confidence score g of this patch. We detect tracking failures by
    checking if the confidence score is below a given threshold T 0 . We then generate
    region proposals across the whole image and compute their confidence scores using
    w L . We search for the proposal with the highest confidence score as the re-detected
    result (Section 4.4). Note that we conservatively take each re-detected result
    by setting the confidence threshold to 1.5 T 0 . As for scale estimation, we generate
    region proposals with a smaller step size to make them tightly around the estimated
    target position (Section 4.4). We infer the scale change by searching for the
    proposal with the highest confidence score (using the long-term memory filter
    w L to compute confidence scores).
  Figure 4 Link: articels_figures_by_rev_year\2018\Robust_Visual_Tracking_via_Hierarchical_Convolutional_Features\figure_4.jpg
  Figure 4 caption: Visualization of convolutional layers. (a) Four frames from the
    challenging MotorRolling sequence. (b)-(d) Features are from the outputs of the
    convolutional layers conv3-4, conv4-4, and conv5-4 using the VGGNet-19 [25]. The
    yellow bounding boxes indicate the tracking results by our method. Note that although
    the appearance of the target changes significantly, the features using the output
    of the conv5-4 convolution layer (d) is able to discriminate it readily even the
    background has dramatically changed. The conv4-4 (c) and conv3-4 (b) layers encode
    more fine-grained details and are useful to locate target precisely.
  Figure 5 Link: articels_figures_by_rev_year\2018\Robust_Visual_Tracking_via_Hierarchical_Convolutional_Features\figure_5.jpg
  Figure 5 caption: Comparison of different weighting schemes to locate the target
    in the search window (yellow). (a) Sample frame in the KiteSurf [36] sequence.
    (b)-(d) Correlation response maps from single convolutional layers. (e)-(g) Different
    schemes to weight (b), (c) and (d). The maximum values in (e)-(g) are highlighted
    by squares, and their tracking results are with the corresponding colors in (a).
    Note that the soft weight scheme (g) is robust to noisy maximum values in (c)
    and (d) when compared to the soft mean (e) and hard weight (f) schemes.
  Figure 6 Link: articels_figures_by_rev_year\2018\Robust_Visual_Tracking_via_Hierarchical_Convolutional_Features\figure_6.jpg
  Figure 6 caption: Frame-by-frame maximum response values and center location error
    plot on the KiteSurf [36] sequence. In (a)-(c), dash lines indicate maximum values
    of single correlation response maps on different CNN layers. Solid lines show
    maximum values of the combined response map over all CNN layers. Note that the
    soft weight scheme sets the weights to be inversely proportional to the maximum
    values of response maps. The weighted values are more consistent than the soft
    mean and hard weight schemes. The center location error plot shows that the soft
    weight scheme helps track the target over the entire sequence, despite the presence
    of heavy occlusion and abrupt motion in the 38th frame (see Fig. 5), while other
    alternative approaches do not produce satisfactory results.
  Figure 7 Link: articels_figures_by_rev_year\2018\Robust_Visual_Tracking_via_Hierarchical_Convolutional_Features\figure_7.jpg
  Figure 7 caption: Distance precision and overlap success plots on the OTB2015 [36]
    dataset. Quantitative results on the 100 benchmark sequences using OPE, SRE and
    TRE. The legend of distance precision contains threshold scores at 20 pixels,
    while the legend of overlap success contains area-under-the-curve score for each
    tracker. The proposed algorithm, HCFT, performs favorably against the state-of-the-art
    trackers.
  Figure 8 Link: articels_figures_by_rev_year\2018\Robust_Visual_Tracking_via_Hierarchical_Convolutional_Features\figure_8.jpg
  Figure 8 caption: Distance precision and overlap success plots on the OTB2013 [35]
    dataset. Quantitative results on the 50 benchmark sequences using OPE. The legend
    of distance precision contains the threshold scores at 20 pixels while the legend
    of overlap success contains area-under-the-curve score for each tracker. Our tracker
    HCFT performs well against the state-of-the-art algorithms.
  Figure 9 Link: articels_figures_by_rev_year\2018\Robust_Visual_Tracking_via_Hierarchical_Convolutional_Features\figure_9.jpg
  Figure 9 caption: 'Performance evaluation on benchmark attributes: illumination
    variation (IV35), out-of-plane rotation (OR59), scale variation (SV61), occlusion
    (OCC44), deformation (DEF39), motion blur (MB29), fast motion (FM37), in-plane
    rotation (IR51), out-of-view (OV14), background clutter (BC31), and low resolution
    (LR9). The later digits in each acronym mean the number of videos with that attribute.
    The proposed algorithm HCFT performs well against state-of-the-art results.'
  First author gender probability: 0.97
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Chao Ma
  Name of the last author: Ming-Hsuan Yang
  Number of Figures: 15
  Number of Tables: 3
  Number of authors: 4
  Paper title: Robust Visual Tracking via Hierarchical Convolutional Features
  Publication Date: 2018-08-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparisons with State-of-the-Art Trackers on OTB2013 (I)
      [35] and OTB2015 (II) [36] Benchmark Sequences
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average Ranks of Accuracy and Robustness under Baseline and
      Region Noise Experiments on the VOT 2014 Dataset
  Table 3 caption:
    table_text: TABLE 3 Results on VOT2015
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2865311
- Affiliation of the first author: department of augmented vision, german research
    center of artificial intelligence, kaiserslautern, germany
  Affiliation of the last author: university of kaiserslautern, kaiserslautern, germany
  Figure 1 Link: articels_figures_by_rev_year\2018\Flow_Fields_Dense_Correspondence_Fields_for_Highly_Accurate_Large_Displacement_O\figure_1.jpg
  Figure 1 caption: Comparison of state-of-the-art approximate nearest neighbor fields
    (a) and Flow Fields (b) with the same data term. a) and b) are shown with ground
    truth occlusion map (black pixels). c) is after outlier filtering, occluded regions
    are successfully filtered. It can be used as initialization for an optical flow
    method.
  Figure 10 Link: articels_figures_by_rev_year\2018\Flow_Fields_Dense_Correspondence_Fields_for_Highly_Accurate_Large_Displacement_O\figure_10.jpg
  Figure 10 caption: The influence of different parameters of our approach. We plot
    the main measures for each dataset.
  Figure 2 Link: articels_figures_by_rev_year\2018\Flow_Fields_Dense_Correspondence_Fields_for_Highly_Accurate_Large_Displacement_O\figure_2.jpg
  Figure 2 caption: The pipeline of our flow fields approach. For the basic approach
    we only consider the full resolution.
  Figure 3 Link: articels_figures_by_rev_year\2018\Flow_Fields_Dense_Correspondence_Fields_for_Highly_Accurate_Large_Displacement_O\figure_3.jpg
  Figure 3 caption: 'Sketch shows propagation support in gradient descent in 1D space
    ( x -axis: 1D image space, y -axis: 1D flow space). Blue lines: Ground truth.
    Black lines: Current flow. Left: After propagation of initial seeds. Middle: Random
    search (very slow gradient descent). Right: Propagation of noisy random search
    samples leads to much faster gradient decent. In 2D space this effect is even
    more powerful, as propagation is more powerful here (see Fig. 4a).'
  Figure 4 Link: articels_figures_by_rev_year\2018\Flow_Fields_Dense_Correspondence_Fields_for_Highly_Accurate_Large_Displacement_O\figure_4.jpg
  Figure 4 caption: a) Example for the ability of propagation to propagate into different
    directions within a 90 degree angle. Gray pixels reject the flow of the green
    seed pixel. In practice each pixel is a seed. b) Pixel positions of P 1 (green),
    P 2 1 (blue) and P 4 1 (red). The central pixel is in black. c) Our propagation
    directions.
  Figure 5 Link: articels_figures_by_rev_year\2018\Flow_Fields_Dense_Correspondence_Fields_for_Highly_Accurate_Large_Displacement_O\figure_5.jpg
  Figure 5 caption: Illustration of spreading of seeds, based on intuitions underlying
    the proposed method. X axis is image position, y axis optical flow displacement.
    From a seed, spreading (propagation + random search) can distribute the flow far
    in image direction X (propagation) but only in a narrow range in flow direction
    X (due to small random search distance R ). This allows inaccurate matches but
    no real resistant outliers with large EPE (=y axis error) if started from a correct
    seed. An exception are motion discontinuities (yellow ends of green areas) or
    false seeds. Here, outliers with large EPE are possible. However, outliers in
    yellow regions should not propagate well, which keeps these regions small. Propagation
    requires smoothness and in contrast to inliers, outlier regions are usually not
    smooth. If this would not be the case the smoothnesses assumption of optical flow
    [1] would not work.
  Figure 6 Link: articels_figures_by_rev_year\2018\Flow_Fields_Dense_Correspondence_Fields_for_Highly_Accurate_Large_Displacement_O\figure_6.jpg
  Figure 6 caption: Illustration of our multi-scale flow fields approach. Flow offsets
    saved in pixels are propagated in all arrow directions.
  Figure 7 Link: articels_figures_by_rev_year\2018\Flow_Fields_Dense_Correspondence_Fields_for_Highly_Accurate_Large_Displacement_O\figure_7.jpg
  Figure 7 caption: 'Outlier sieve effect. Outliers disappear through propagations
    on different scales. For visualization purposes the valid gray pixels of the scales
    in Fig. 6 are enlarged to fill the whole pixel space. Scales for the numbers are:
    1: n=8 after KD-tree initialization, 2:n=8 after propagation, 3:n=4 after propagation,
    4:n=1 after propagation (we skipped n=2). The full images can be found in our
    supplementary material, which can be found on the Computer Society Digital Library
    at http:doi.ieeecomputersociety.org10.1109TPAMI.2018.2859970.'
  Figure 8 Link: articels_figures_by_rev_year\2018\Flow_Fields_Dense_Correspondence_Fields_for_Highly_Accurate_Large_Displacement_O\figure_8.jpg
  Figure 8 caption: a) Flow field obtained with k = 3 with b) as only initialization
    (black pixels in b) are set to infinity). It shows the powerfulness of our multi-scale
    propagation. c) Like a) but with kd-tree initialization. The 3 marked details
    are preserved due to their presence in the coarsest scale d). e) Like c) but without
    scales (basic approach). Details are not preserved. f) Ground truth. As correspondence
    estimation is impossible in occluded areas and as orientation we blacked such
    areas out.
  Figure 9 Link: articels_figures_by_rev_year\2018\Flow_Fields_Dense_Correspondence_Fields_for_Highly_Accurate_Large_Displacement_O\figure_9.jpg
  Figure 9 caption: 'Besides direct Fourier based low-pass filling an image based
    low-pass can be calculated by successive down and upsampling. If feature extraction
    is used, there are two options: Features can either be calculated before or after
    downsampling. After feature extraction updownsampling is performed not on the
    image but on the feature map.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Christian Bailer
  Name of the last author: Didier Stricker
  Number of Figures: 16
  Number of Tables: 8
  Number of authors: 3
  Paper title: 'Flow Fields: Dense Correspondence Fields for Highly Accurate Large
    Displacement Optical Flow Estimation'
  Publication Date: 2018-08-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Scales and Sub-Scales Used for Our Improved Approach Flow
      Fields+
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Different Correspondence Fields on a Representative
      Subset (2x Every 10th Frame) on Non-Occluded Regions of the MPI-Sintel Training
      Set (Clean and Final)
  Table 3 caption:
    table_text: TABLE 3 Comparison of Our Conference Approach Flow Fields with Different
      Scales on the Middlebury Training Dataset to Demonstrate That the Quality Does
      Not Suffer from Multi-Scale Matching Like in [24]
  Table 4 caption:
    table_text: TABLE 4 Accuracy and Runtime of Our Approaches on the MPI-Sintel Training
      Set
  Table 5 caption:
    table_text: TABLE 5 > > 3px EPE Failure Rate and Runtime of Our Approaches on
      the KITTI 2015 Training Set
  Table 6 caption:
    table_text: TABLE 6 Results on MPI-Sintel
  Table 7 caption:
    table_text: TABLE 7 Results on KITTI 2012 Test Set
  Table 8 caption:
    table_text: TABLE 8 Results on KITTI 2015 Test Set
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2859970
