- Affiliation of the first author: google inc., mountain view, ca, usa
  Affiliation of the last author: department of mathematics, university of washington,
    seattle, wa, usa
  Figure 1 Link: Not Available
  Figure 1 caption: Not Available
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Sameer Agarwal
  Name of the last author: Rekha R. Thomas
  Number of Figures: Not Available
  Number of Tables: 0
  Number of authors: 3
  Paper title: Ideals of the Multiview Variety
  Publication Date: 2019-10-31 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2950631
- Affiliation of the first author: beijing key laboratory of traffic data analysis
    and mining, school of computer and information technology, beijing jiaotong university,
    beijing, china
  Affiliation of the last author: department of computer science and engineering,
    university of south carolina, columbia, sc, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Effects_of_Image_Degradation_and_Degradation_Removal_to_CNNBased_Image_Classific\figure_1.jpg
  Figure 1 caption: Examples of degraded images. From left to right and top to down
    are a hazy image, a motion-blurred image, a fish-eye image, an underwater image,
    a low resolution image, a salt-and-peppered image, an image with white Gaussian
    noise, a Gaussian-blurred image, and an out-of-focus image, respectively.
  Figure 10 Link: articels_figures_by_rev_year\2019\Effects_of_Image_Degradation_and_Degradation_Removal_to_CNNBased_Image_Classific\figure_10.jpg
  Figure 10 caption: Classification accuracy of different sets of dehazed images subjectively
    selected by human.
  Figure 2 Link: articels_figures_by_rev_year\2019\Effects_of_Image_Degradation_and_Degradation_Removal_to_CNNBased_Image_Classific\figure_2.jpg
  Figure 2 caption: Examples of the synthesized degraded images. From top to the bottom
    rows are hazy images, motion-blurred images, fish-eye images, underwater images,
    low resolution images, salt-and-peppered images, images with white Gaussian noise,
    Gaussian-blurred images, and out-of-focus images, respectively. The degradation
    levels are indicated below each image. The first column shows the original clear
    images.
  Figure 3 Link: articels_figures_by_rev_year\2019\Effects_of_Image_Degradation_and_Degradation_Removal_to_CNNBased_Image_Classific\figure_3.jpg
  Figure 3 caption: Sample hazy images in our new Haze-20 dataset.
  Figure 4 Link: articels_figures_by_rev_year\2019\Effects_of_Image_Degradation_and_Degradation_Removal_to_CNNBased_Image_Classific\figure_4.jpg
  Figure 4 caption: The classification accuracy (%) of various degraded images synthesized
    from Caltech-256 dataset. From (a) to (i) are the accuracy (%) of hazy images,
    motion-blurred images, fish-eye images, underwater images, low resolution images,
    salt-and-peppered images, images with white Gaussian noise, Gaussian-blurred images,
    and out-of-focus images, respectively.
  Figure 5 Link: articels_figures_by_rev_year\2019\Effects_of_Image_Degradation_and_Degradation_Removal_to_CNNBased_Image_Classific\figure_5.jpg
  Figure 5 caption: Activations of hidden layers of CNN on image classification. From
    left to right are input images, and the activations at poo l 1 , poo l 2 , poo
    l 3 , poo l 4 , and poo l 5 layers, respectively.
  Figure 6 Link: articels_figures_by_rev_year\2019\Effects_of_Image_Degradation_and_Degradation_Removal_to_CNNBased_Image_Classific\figure_6.jpg
  Figure 6 caption: "The classification accuracy on different hazy images. (a)-(e)\
    \ Classification accuracies on testing synthetic hazy images with \u03B2=1,2,3,4,5\
    \ , respectively. (f) Classification accuracy on the testing images in Haze-20."
  Figure 7 Link: articels_figures_by_rev_year\2019\Effects_of_Image_Degradation_and_Degradation_Removal_to_CNNBased_Image_Classific\figure_7.jpg
  Figure 7 caption: Classification accuracy (%) on synthetic and real-world hazy images
    by using a non-CNN-based image classification method. Here the same kinds of images
    are used for training, i.e., building the basis for sparse coding, and testing,
    just like the case corresponding to the solid curves (AlexNet1, VGGNet1, and ResNet1)
    in Fig. 6.
  Figure 8 Link: articels_figures_by_rev_year\2019\Effects_of_Image_Degradation_and_Degradation_Removal_to_CNNBased_Image_Classific\figure_8.jpg
  Figure 8 caption: "Classification accuracy when training on mixed-level hazy images.\
    \ (a), (b), and (c) Mix all six levels of synthetic images. (d) Mix two levels\
    \ \u03B2=0 and \u03B2=5 . (e) Mix two levels \u03B2=1 and \u03B2=4 . (f) Mix Haze-20\
    \ and HazeClear-20."
  Figure 9 Link: articels_figures_by_rev_year\2019\Effects_of_Image_Degradation_and_Degradation_Removal_to_CNNBased_Image_Classific\figure_9.jpg
  Figure 9 caption: Average PSNR and SSIM values on synthetic image dataset at different
    haze levels.
  First author gender probability: 0.97
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Yanting Pei
  Name of the last author: Song Wang
  Number of Figures: 14
  Number of Tables: 5
  Number of authors: 5
  Paper title: Effects of Image Degradation and Degradation Removal to CNN-Based Image
    Classification
  Publication Date: 2019-11-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Classification Accuracy (%) of Hazy Images, Motion-Blurred
      Images, Fish-Eye Images, and Underwater Images by Using AlexNet and VGGNet-16
      With Different Levels of Degradation, Respectively
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Classification Accuracy (%) When VGGNet-16 CNN Classifiers
      are Trained by Mixing Different-Level Training Images of the Same Degradation
      Kind
  Table 3 caption:
    table_text: TABLE 3 Classification Accuracy (%) on Haze-20 and HazeClear-20 Datasets
      Using VGGNet and AlexNet, Respectively
  Table 4 caption:
    table_text: TABLE 4 The Rank Correlation Between Image-Classification Accuracy
      and PSNRSSIM at Each Haze Level
  Table 5 caption:
    table_text: TABLE 5 The Rank Correlation Between Image-Classification Accuracy
      and PSNRSSIM at Each Motion-Blur Level
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2950923
- Affiliation of the first author: "collective behaviour in biological systems (cobbs)\
    \ lab, uos sapienza, national research council - institute for complex systems\
    \ (cnr\u2013isc), rome, italy"
  Affiliation of the last author: physics department, sapienza university of rome,
    rome, italy
  Figure 1 Link: articels_figures_by_rev_year\2019\SpaRTA_Tracking_Across_Occlusions_via_Partitioning_of_D_Clouds_of_Points\figure_1.jpg
  Figure 1 caption: "Optical occlusions: SIP versus COP representation. Left column:\
    \ 2D occlusions. The blue and the green targets are occluded in the image plane\
    \ of the first camera and well\u2013separated in the other two. Right column:\
    \ 3D occlusion. The blue and the green targets are in 3D proximity, therefore\
    \ they are occluded in all the three cameras. Top row: SIP representation. a)\
    \ The information from the two cameras where the occlusion do not occur make the\
    \ occluded targets to be reconstructed in two different positions (the red circles\
    \ in the right graph of the panel), but with a poor accuracy: both the two 3D\
    \ positions do not correspond to the target centers of mass. b) The two occluded\
    \ targets are associated with one single 3D position (the red circle between the\
    \ blue and the green target), with a consequent loss of the targets identities\
    \ and potential switch of identities after the occlusion occurs. Bottom row: COP\
    \ representation. c) The occluded targets are represented as two well-separated\
    \ dense cloud of 3D points. d) The occluded targets are reconstructed in a single\
    \ cloud of 3D points as big as their total volume. Identities may be retrieved\
    \ by splitting the clusters in the two subsets representing the volumes of the\
    \ two distinct objects, as SpaRTA does using the SDP technique described in Section\
    \ 4.2."
  Figure 10 Link: articels_figures_by_rev_year\2019\SpaRTA_Tracking_Across_Occlusions_via_Partitioning_of_D_Clouds_of_Points\figure_10.jpg
  Figure 10 caption: 'Dataset and results. First row: on the left an image from the
    Davis-08 sparse dataset and on the right a sample of the 3D trajectories, each
    with a different color, of the same dataset reconstructed by SpaRTA. Second row:
    on the left an image from the Davis-08 dense dataset and on the right a sample
    of the 3D trajectories reconstructed by SpaRTA'
  Figure 2 Link: articels_figures_by_rev_year\2019\SpaRTA_Tracking_Across_Occlusions_via_Partitioning_of_D_Clouds_of_Points\figure_2.jpg
  Figure 2 caption: "Clusters graph. Gray circles represent 3D clouds of points dynamically\
    \ linked via black arrows. Distinct connected components are highlighted in different\
    \ colors. The first three components from the left represent trajectories of not\u2013\
    occluded targets, since they are made of only one\u2013to\u2013one linked clusters.\
    \ Conversely the last three components are ambiguous because they have at least\
    \ one node with more than one link from the past or to the future."
  Figure 3 Link: articels_figures_by_rev_year\2019\SpaRTA_Tracking_Across_Occlusions_via_Partitioning_of_D_Clouds_of_Points\figure_3.jpg
  Figure 3 caption: "Cluster graph construction: Dynamic linking. At a generic frame\
    \ t , a point\u2013to\u2013point multi\u2013linking procedure is performed: p\
    \ 0 at time t is connected to p 2 and p 3 at frame (t+1) , while p 1 at frame\
    \ t is connected with p 3 and p 4 . These point\u2013to\u2013point links are then\
    \ used to define cluster\u2013to\u2013cluster links: two clusters are connected\
    \ if there exists at least one point\u2013to\u2013point link between points belonging\
    \ to the two clusters. Therefore, C 0 will be linked to both C 1 and C 2 (the\
    \ two points p 0 and p 1 belong to C 0 and they are both linked to points belonging\
    \ to C 1 and C 2 ). On the opposite, C 3 does not receive any link from the past,\
    \ because none of its points receive a point\u2013to\u2013point link."
  Figure 4 Link: articels_figures_by_rev_year\2019\SpaRTA_Tracking_Across_Occlusions_via_Partitioning_of_D_Clouds_of_Points\figure_4.jpg
  Figure 4 caption: "3D occlusion. A X \u2013shape connected component representing\
    \ a 3D occlusion. The two objects are well\u2013separated for the most part of\
    \ the event, but they share the same cluster during the occlusion. The four branches\
    \ of the X represent the two different trajectories of the two objects before\
    \ and after the occlusion, which is represented as a double grey circle at the\
    \ center of the X . During the SDP procedure the analysis of the ambiguous component\
    \ is restricted to a quite short interval, highlighted in pink."
  Figure 5 Link: articels_figures_by_rev_year\2019\SpaRTA_Tracking_Across_Occlusions_via_Partitioning_of_D_Clouds_of_Points\figure_5.jpg
  Figure 5 caption: '3D occlusion: static linking. a) The static coefficient, wij
    between two points i and j as a function of their mutual distance dij , with r0
    and r1 of the specific 3D occlusion shown in Fig. 6. b), (c), and d) Points belonging
    to the same frame are strongly connected if they are at a very short mutual distance
    (static positive links), while they are strongly disjointed, through a large but
    negative weight, when at a large mutual distance, both within the same target
    (static negative links) or in two different targets (static negative links between
    clusters).'
  Figure 6 Link: articels_figures_by_rev_year\2019\SpaRTA_Tracking_Across_Occlusions_via_Partitioning_of_D_Clouds_of_Points\figure_6.jpg
  Figure 6 caption: "3D occlusion solution. An example of a 3D occlusion from the\
    \ Davis-08 dense dataset [10] used to test the method. On the left: the X \u2013\
    shape component, with the 3D occlusion and the sub\u2013cloud analyzed with the\
    \ SDP technique highlighted in red. On the right: the same component after the\
    \ solution of the 3D occlusion, with the two separated trajectories highlighted\
    \ in blue and green."
  Figure 7 Link: articels_figures_by_rev_year\2019\SpaRTA_Tracking_Across_Occlusions_via_Partitioning_of_D_Clouds_of_Points\figure_7.jpg
  Figure 7 caption: 'Multiple connected component. The three targets A , B , and C
    are involved in the same connected component: target A is occluded at an instant
    of time t with B and at a different instant of time tprime > t with C . We identify
    the two occlusions and we define a minimization problems in a short interval around
    each occlusion. Once the occlusions are solved, we look for the connected components
    involved in the ambiguous original one and we find the three components (the blue,
    the green, and the orange) corresponding to the three different objects.'
  Figure 8 Link: articels_figures_by_rev_year\2019\SpaRTA_Tracking_Across_Occlusions_via_Partitioning_of_D_Clouds_of_Points\figure_8.jpg
  Figure 8 caption: Ghost objects formation. The blue and the green objects, namely,
    Q1 and Q2 are reprojected in the two cameras as q1 and q2 . The pair (q1, q1)
    is a good match because the two pixels are the image of the same 3D point Q1 ,
    and the pair (q2, q2) is a good match as well. The two pairs (q1, q2) and (q2,
    q1) do not represent any object of interest but they are good matches from a epipolar
    perspective, see [37]. The pixel matching procedure will then reconstruct the
    two correct objects Q1 and Q2 and the two ghost objects Q12 and Q21 . The introduction
    of a third camera drastically reduces the creation of ghost points working on
    triplets of pixels, but even in this case ambiguities are not completely solved.
  Figure 9 Link: articels_figures_by_rev_year\2019\SpaRTA_Tracking_Across_Occlusions_via_Partitioning_of_D_Clouds_of_Points\figure_9.jpg
  Figure 9 caption: "Ghost trajectories removal. A Y \u2013shape connected component\
    \ representing a real (blue) and a ghost (red) trajectories occluded at one frame.\
    \ This ambiguous component is due to a wrong dynamic link (the red arrow), which\
    \ can be easily identified from the peculiar Y \u2013shape of the component together\
    \ with the short length of the ghost branch. The wrong dynamic link is detected\
    \ and removed as well as the ghost points."
  First author gender probability: 0.82
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Andrea Cavagna
  Name of the last author: Federico Ricci-Tersenghi
  Number of Figures: 10
  Number of Tables: 1
  Number of authors: 4
  Paper title: SpaRTA Tracking Across Occlusions via Partitioning of 3D Clouds of
    Points
  Publication Date: 2019-11-04 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Comparison of the Quality of the Trajectories Retrieved by
      SpaRTA and the Algorithms: MHT, SDD-MHT [10], CP(LDQD) [2], and GReTA [12] on
      the Public Datasets Labeled Davis-08 sparse and Davis-08 dense Published in
      [10]'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2946796
- Affiliation of the first author: department of electrical engineering and computer
    science, university of michigan, ann arbor, usa
  Affiliation of the last author: department of electrical engineering and computer
    science, university of michigan, ann arbor, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\A_TemporallyAware_Interpolation_Network_for_Video_Frame_Inpainting\figure_1.jpg
  Figure 1 caption: A visual comparison of various video interpolationextrapolation
    tasks. In this article, we explore (e) video frame inpainting. Unlike general
    video inpainting methods, we recover whole, contiguous frames; and unlike frame
    interpolation and video prediction methods, we predict the desired sequence using
    multiple frames that appear both before and after it.
  Figure 10 Link: articels_figures_by_rev_year\2019\A_TemporallyAware_Interpolation_Network_for_Video_Frame_Inpainting\figure_10.jpg
  Figure 10 caption: "Qualitative comparison of the various intermediate predictions\
    \ made by bi-TAI and its ablative variants (Section 4.3). We visualize the fourth\
    \ predicted middle frame (out of five) from a \u201Chandwaving\u201D video clip,\
    \ and zoom in on a specific region (indicated in orange). (a) The ground-truth\
    \ middle frame. (b) Comparison of the forward and backward predictions from the\
    \ Bidirectional Video Prediction Network and the final predictions. (c) Inputs\
    \ and outputs of the interpolation networks of bi-TWI and bi-TAI. The cyan images\
    \ correspond to the forward prediction frame before and after adaptive convolution,\
    \ and the purple images correspond to the backward prediction frame before and\
    \ after adaptive convolution."
  Figure 2 Link: articels_figures_by_rev_year\2019\A_TemporallyAware_Interpolation_Network_for_Video_Frame_Inpainting\figure_2.jpg
  Figure 2 caption: "An overview of our bi-TAI method for video frame inpainting.\
    \ We predict middle frames by blending forward and backward intermediate video\
    \ predictions (generated by \u03D5 pred ) with a Temporally-Aware Interpolation\
    \ (TAI) network ( \u03D5 blend )."
  Figure 3 Link: articels_figures_by_rev_year\2019\A_TemporallyAware_Interpolation_Network_for_Video_Frame_Inpainting\figure_3.jpg
  Figure 3 caption: The Bidirectional Video Prediction Network.
  Figure 4 Link: articels_figures_by_rev_year\2019\A_TemporallyAware_Interpolation_Network_for_Video_Frame_Inpainting\figure_4.jpg
  Figure 4 caption: (a) The architecture of TAIs encoder-decoder network. (b) The
    TAI network applied to the intermediate predictions from the Bidirectional Video
    Prediction Network.
  Figure 5 Link: articels_figures_by_rev_year\2019\A_TemporallyAware_Interpolation_Network_for_Video_Frame_Inpainting\figure_5.jpg
  Figure 5 caption: Performance on the KTH test set for each time step (higher is
    better).
  Figure 6 Link: articels_figures_by_rev_year\2019\A_TemporallyAware_Interpolation_Network_for_Video_Frame_Inpainting\figure_6.jpg
  Figure 6 caption: The distributions of performance on the video clips in the KTH
    test set. Performance per video is computed as the mean score across all predicted
    middle frames (higher is better). Outliers are shown as light gray lines.
  Figure 7 Link: articels_figures_by_rev_year\2019\A_TemporallyAware_Interpolation_Network_for_Video_Frame_Inpainting\figure_7.jpg
  Figure 7 caption: Qualitative results from the KTH dataset for predicting five middle
    frames from five preceding and five following frames (we depict every other frame
    for easier viewing). We indicate preceding and following frames with a green border,
    predicted middle frames with a yellow border, and ground-truth middle frames with
    a green border.
  Figure 8 Link: articels_figures_by_rev_year\2019\A_TemporallyAware_Interpolation_Network_for_Video_Frame_Inpainting\figure_8.jpg
  Figure 8 caption: Additional predictions from bi-TAI on the KTH test set ( m=5 ).
    The yellow frames indicate predictions from bi-TAI, and green frames indicate
    the ground truth. We show the first, third, and fifth middle frame for each video.
  Figure 9 Link: articels_figures_by_rev_year\2019\A_TemporallyAware_Interpolation_Network_for_Video_Frame_Inpainting\figure_9.jpg
  Figure 9 caption: Negative result on the KTH test set ( m=5 ). We show the first,
    third, and fifth middle frames, and zoom in on the area indicated in orange.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ryan Szeto
  Name of the last author: Jason J. Corso
  Number of Figures: 17
  Number of Tables: 5
  Number of authors: 4
  Paper title: A Temporally-Aware Interpolation Network for Video Frame Inpainting
  Publication Date: 2019-11-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of the Training and Testing Sets Used in Our Experiments
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 KTH Test Set Performance
  Table 3 caption:
    table_text: TABLE 3 Ablation Study Performance
  Table 4 caption:
    table_text: TABLE 4 UCF-101 and HMDB-51 Test Set Performance
  Table 5 caption:
    table_text: TABLE 5 ImageNet-VID Test Set Performance
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2951667
- Affiliation of the first author: czech academy of sciences, institute of information
    theory and automation, praha, czech republic
  Affiliation of the last author: czech academy of sciences, institute of information
    theory and automation, praha, czech republic
  Figure 1 Link: articels_figures_by_rev_year\2019\Affine_Invariants_of_Vector_Fields\figure_1.jpg
  Figure 1 caption: Vortex detection in a swirling fluid by template matching. The
    detection method must be invariant to the template deformation.
  Figure 10 Link: articels_figures_by_rev_year\2019\Affine_Invariants_of_Vector_Fields\figure_10.jpg
  Figure 10 caption: The detected vortices in the deformed field when invariants Zi
    up to 7th order were employed. The deformation comprised anisotropic scaling with
    factors 54 (top) and 74 (bottom). The full videos can be found at zoi.utia.cas.czExperiment-with-Karman-Street.
  Figure 2 Link: articels_figures_by_rev_year\2019\Affine_Invariants_of_Vector_Fields\figure_2.jpg
  Figure 2 caption: 'Vector field transformations: (a) An original vector field, (b)
    its inner affine transformation, (c) its outer affine transformation, (d) its
    special total affine transformation. The green arrows in (c) and (d) show the
    vector field without the outer transformation.'
  Figure 3 Link: articels_figures_by_rev_year\2019\Affine_Invariants_of_Vector_Fields\figure_3.jpg
  Figure 3 caption: The graphs representing invariants V a , V b , V c , and V d .
    The edges belonging to E 1 are shown in black, magenta edges belong to E 2 .
  Figure 4 Link: articels_figures_by_rev_year\2019\Affine_Invariants_of_Vector_Fields\figure_4.jpg
  Figure 4 caption: Algorithm for the next graph generation. d=1 for invariants V
    i and d=0 for invariants Z i .
  Figure 5 Link: articels_figures_by_rev_year\2019\Affine_Invariants_of_Vector_Fields\figure_5.jpg
  Figure 5 caption: (a) Gradient field of a grayscale image, which served as a test
    vector field in the synthetic experiments, (b) an example of the vector field
    transformed by a randomly generated TAFT, (c) colormap for gradient visualization,
    where the brightness corresponds to the magnitude and the hue to the direction
    of the gradient.
  Figure 6 Link: articels_figures_by_rev_year\2019\Affine_Invariants_of_Vector_Fields\figure_6.jpg
  Figure 6 caption: The values of the invariants over 100 randomly generated total
    affine transformations. (a) Five selected invariants of the V -type exhibit very
    good invariance (except a few cases when the transformation is close to singular).
    (b) Invariants of the Z -type are not really invariant under these conditions.
    (c) The same invariants of the Z -type when the transformations were constrained
    such that B=A .
  Figure 7 Link: articels_figures_by_rev_year\2019\Affine_Invariants_of_Vector_Fields\figure_7.jpg
  Figure 7 caption: "The relative error of the invariant Z 9 over 100 randomly generated\
    \ total affine transformations and SNR ranging from 30 to -5 dB. The robustness\
    \ is very good for SNR >10 dB. Only the ratio of the \u201Cnoisy\u201D and original\
    \ value is visualized."
  Figure 8 Link: articels_figures_by_rev_year\2019\Affine_Invariants_of_Vector_Fields\figure_8.jpg
  Figure 8 caption: Gradient field with 100 randomly selected templates used in a
    single run of the experiment. The colormap is the same as in Fig. 5.
  Figure 9 Link: articels_figures_by_rev_year\2019\Affine_Invariants_of_Vector_Fields\figure_9.jpg
  Figure 9 caption: "The K\xE1rm\xE1n vortex street with the selected template (the\
    \ first frame of the video)."
  First author gender probability: 0.99
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.91
  Name of the first author: "Jitka Kostkov\xE1"
  Name of the last author: Jan Flusser
  Number of Figures: 12
  Number of Tables: 0
  Number of authors: 3
  Paper title: Affine Invariants of Vector Fields
  Publication Date: 2019-11-06 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2951664
- Affiliation of the first author: college of electrical engineering, sichuan university,
    chengdu, china
  Affiliation of the last author: college of electrical engineering, sichuan university,
    chengdu, china
  Figure 1 Link: articels_figures_by_rev_year\2019\Scalar_Quantization_as_Sparse_Least_Square_Optimization\figure_1.jpg
  Figure 1 caption: Post-quantization accuracy on training and testing data with respect
    to quantization amounts, and their running time. The x -axis for the plots stands
    for quantization amounts(number of quantization), and the y -axis stands for accuracy
    for the first two plots and time in seconds for the third plot.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Scalar_Quantization_as_Sparse_Least_Square_Optimization\figure_2.jpg
  Figure 2 caption: Post-quantization accuracy on training and testing data with respect
    to quantization amounts, focusing on the area where accuracy drops significantly.
    The x -axis stands for number of clusters, the y -axis stands for accuracy.
  Figure 3 Link: articels_figures_by_rev_year\2019\Scalar_Quantization_as_Sparse_Least_Square_Optimization\figure_3.jpg
  Figure 3 caption: "The distribution of \u03B1 weights for neural network last-layer\
    \ quantization."
  Figure 4 Link: articels_figures_by_rev_year\2019\Scalar_Quantization_as_Sparse_Least_Square_Optimization\figure_4.jpg
  Figure 4 caption: "The accuracy with respect to \u03BB 1 values for sole l 1 and\
    \ l 1 + l 2 optimization, respectively. The value of \u03BB 2 is set to | \u03BB\
    \ 2 |=4\u2217 10 \u22123 \u2217 \u03BB 1 ."
  Figure 5 Link: articels_figures_by_rev_year\2019\Scalar_Quantization_as_Sparse_Least_Square_Optimization\figure_5.jpg
  Figure 5 caption: MNIST quantization comparison for l 1 method, l 1 and least square
    method, K-means method, and K-means-based least square method.
  Figure 6 Link: articels_figures_by_rev_year\2019\Scalar_Quantization_as_Sparse_Least_Square_Optimization\figure_6.jpg
  Figure 6 caption: MNIST quantization results for l 0 -based method.
  Figure 7 Link: articels_figures_by_rev_year\2019\Scalar_Quantization_as_Sparse_Least_Square_Optimization\figure_7.jpg
  Figure 7 caption: The distribution of three types of artificially-generated data.
  Figure 8 Link: articels_figures_by_rev_year\2019\Scalar_Quantization_as_Sparse_Least_Square_Optimization\figure_8.jpg
  Figure 8 caption: Quantization results of artificially-generated data. For each
    subplot, the left is the norm loss figures, and the right is the running time.
    The x -axis stands for clusters, and the y -axis stands for l 2 loss for the left
    figures, and for time in seconds for the right ones.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.63
  Name of the first author: Chen Wang
  Name of the last author: Ruisen Luo
  Number of Figures: 8
  Number of Tables: 0
  Number of authors: 7
  Paper title: Scalar Quantization as Sparse Least Square Optimization
  Publication Date: 2019-11-08 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2952096
- Affiliation of the first author: intelligent autonomous systems group, technical
    university darmstadt, darmstadt, germany
  Affiliation of the last author: intelligent autonomous systems group, technical
    university darmstadt, darmstadt, germany
  Figure 1 Link: articels_figures_by_rev_year\2019\Assessing_Transferability_From_Simulation_to_Reality_for_Reinforcement_Learning\figure_1.jpg
  Figure 1 caption: 'Evaluation platforms by Quanser [31]: (left) the 2 DoF Ball-Balancer,
    (right) the linear inverted pendulum, called Cart-Pole. Both systems are under-actuated
    nonlinear balancing problems with continuous state and action spaces.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Assessing_Transferability_From_Simulation_to_Reality_for_Reinforcement_Learning\figure_2.jpg
  Figure 2 caption: "Simulation Optimization Bias (SOB) between the true optimum \u03B8\
    \ \u22C6 and the sample-based optimum \u03B8 \u22C6 n . The shaded region visualizes\
    \ the standard deviation around J(\u03B8) , and J n (\u03B8) is determined by\
    \ a particular set of n sampled domain parameters."
  Figure 3 Link: articels_figures_by_rev_year\2019\Assessing_Transferability_From_Simulation_to_Reality_for_Reinforcement_Learning\figure_3.jpg
  Figure 3 caption: "The estimated expected return evaluated using the optimal solution\
    \ for a set of n domains J n ( \u03B8 \u22C6 n ) , the candidate solution J n\
    \ ( \u03B8 c ) , as well as the true optimal solution J n ( \u03B8 \u22C6 ) .\
    \ Note that J n ( \u03B8 \u22C6 n )> J n ( \u03B8 c ) holds for every instance\
    \ of the 100 random seeds, even if the standard deviation areas overlap. The shaded\
    \ areas show \xB11 standard deviation."
  Figure 4 Link: articels_figures_by_rev_year\2019\Assessing_Transferability_From_Simulation_to_Reality_for_Reinforcement_Learning\figure_4.jpg
  Figure 4 caption: "True optimality gap G( \u03B8 c ) , the approximation from n\
    \ domains G ( \u03B8 c ) , and the simulation optimization bias b[ J n ( \u03B8\
    \ \u22C6 n )] . Note that G n ( \u03B8 c )\u2265G( \u03B8 c ) does not hold for\
    \ every instance of the 100 random seeds, but is true in expectation. The variance\
    \ in G( \u03B8 c ) is only caused by the variance in \u03B8 c . The shaded areas\
    \ show \xB11 standard deviation."
  Figure 5 Link: articels_figures_by_rev_year\2019\Assessing_Transferability_From_Simulation_to_Reality_for_Reinforcement_Learning\figure_5.jpg
  Figure 5 caption: "Upper Confidence Bound on the Optimality Gap (UCBOG) and number\
    \ of candidate solution domain over the iteration count of SPOTA. Every iteration,\
    \ the number and domains (dashed line) and hence the sample size is increased.\
    \ The shaded area visualize \xB11 standard deviation across 9 training runs on\
    \ the simulated Ball-Balancer."
  Figure 6 Link: articels_figures_by_rev_year\2019\Assessing_Transferability_From_Simulation_to_Reality_for_Reinforcement_Learning\figure_6.jpg
  Figure 6 caption: "Evaluation of the learned control policies on the simulated Ball\
    \ Balancer (top row) as well as the simulated Cart-Pole (bottom row), (a) varying\
    \ the ball mass m b , (b) the viscous friction coefficient, (c) the action delay\
    \ \u0394 t a , and (d) the motor pinion radius r mp . Every domain parameter configuration\
    \ has been evaluated on 360 rollouts with different initial states, synchronized\
    \ across all policies. The dashed lines mark the nominal parameter values. The\
    \ solid lines represent the means, and shaded areas show \xB11 standard deviation."
  Figure 7 Link: articels_figures_by_rev_year\2019\Assessing_Transferability_From_Simulation_to_Reality_for_Reinforcement_Learning\figure_7.jpg
  Figure 7 caption: Evaluation of the learned control policies on (a) the real Ball
    Balancer and (b) the real Cart-Pole. The results were obtained from 40 rollouts
    per policy on the Ball Balancer (5 repetitions for 8 different initial ball positions)
    as well as 10 rollouts (1 initial state) on the Cart-Pole. The dashed lines approximately
    mark the threshold where the tasks are solved, i.e., the ball is centered in the
    middle (a) or the pendulum is stabilized upright (b) at the end of the episode.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.91
  Name of the first author: Fabio Muratore
  Name of the last author: Jan Peters
  Number of Figures: 7
  Number of Tables: 1
  Number of authors: 3
  Paper title: Assessing Transferability From Simulation to Reality for Reinforcement
    Learning
  Publication Date: 2019-11-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Definition and of the Expectation of the Expected (Discounted)
      Return, the Simulation Optimization Bias (SOB), the Optimality Gap (OG), and
      Its Estimation
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2952353
- Affiliation of the first author: department of systems and control engineering,
    school of engineering, tokyo institute of technology, tokyo, japan
  Affiliation of the last author: department of systems and control engineering, school
    of engineering, tokyo institute of technology, tokyo, japan
  Figure 1 Link: articels_figures_by_rev_year\2019\InLoc_Indoor_Visual_Localization_with_Dense_Matching_and_View_Synthesis\figure_1.jpg
  Figure 1 caption: "Large-scale indoor visual localization. Given a database of geometrically-registered\
    \ RGBD images, we predict the 6DoF camera pose of a query RGB image by retrieving\
    \ candidate database images, estimating candidate camera poses, and selecting\
    \ the best matching camera pose. To address inherent difficulties in indoor visual\
    \ localization, we introduce the \u201CInLoc\u201D approach that performs a sequence\
    \ of progressively stricter verification steps based on dense information."
  Figure 10 Link: articels_figures_by_rev_year\2019\InLoc_Indoor_Visual_Localization_with_Dense_Matching_and_View_Synthesis\figure_10.jpg
  Figure 10 caption: 'Qualitative comparison between InLoc and NetVLAD+DensePE. From
    top to bottom: Query image, the best matching database image, synthesized view
    rendered from the estimated pose, error map between the query image and the synthesized
    view, localization error (meters, degrees). Warm colors on the error map correspond
    to large errors. Green dots are the inlier matches obtained by P3P-LO-RANSAC.'
  Figure 2 Link: articels_figures_by_rev_year\2019\InLoc_Indoor_Visual_Localization_with_Dense_Matching_and_View_Synthesis\figure_2.jpg
  Figure 2 caption: 'Example images from our InLoc dataset. (Top) Database images.
    (Bottom) Query images. The selected images show the challenges encountered in
    indoor environments: Even small changes in viewpoint lead to large differences
    in appearance; large textureless surfaces (e.g., walls); self-repetitive structures
    (e.g., corridors); significant variation throughout the day due to different illumination
    sources (e.g., active versus indirect illumination).'
  Figure 3 Link: articels_figures_by_rev_year\2019\InLoc_Indoor_Visual_Localization_with_Dense_Matching_and_View_Synthesis\figure_3.jpg
  Figure 3 caption: Examples of query images and verified reference poses. Each column
    show a query image on top and the same image with database edges projected onto
    it in the bottom.
  Figure 4 Link: articels_figures_by_rev_year\2019\InLoc_Indoor_Visual_Localization_with_Dense_Matching_and_View_Synthesis\figure_4.jpg
  Figure 4 caption: Query reference positions in the InLoc dataset. The 329 reference
    poses of query images (blue dots) are plotted on the 3D maps (gray dots) that
    are generated by panoramic 3D scans at 277 distinct positions (red circles).
  Figure 5 Link: articels_figures_by_rev_year\2019\InLoc_Indoor_Visual_Localization_with_Dense_Matching_and_View_Synthesis\figure_5.jpg
  Figure 5 caption: Dense CNN matching. (a) In our coarse-to-fine (c2f) dense matching
    strategy we first perform exhaustive matching on a coarser layer (conv4), then
    search for correspondences in a lower layer (conv3), restricting the search area
    only to a local region around the coarse match. (b) Next, we use the keypoint
    relocalization scheme from [92] to refine the location of the matched features
    (shown by green squares) by relocalizing them in the earlier layers using the
    highest activation in their corresponding receptive fields (orange squares).
  Figure 6 Link: articels_figures_by_rev_year\2019\InLoc_Indoor_Visual_Localization_with_Dense_Matching_and_View_Synthesis\figure_6.jpg
  Figure 6 caption: Matching performance evaluation on the InLoc dataset. The graph
    shows the impact of our coarse-to-fine matching using densely extracted features.
    Each curve shows the average number of matches for each query ( y -axis) which
    are matched within a certain pixels error ( x -axis) from the groundtruth.
  Figure 7 Link: articels_figures_by_rev_year\2019\InLoc_Indoor_Visual_Localization_with_Dense_Matching_and_View_Synthesis\figure_7.jpg
  Figure 7 caption: Impact of different components. The graphs show the impact of
    dense matching (DensePE) and dense pose verification (DensePV and DenseCNNPV)
    on pose estimation quality for (a) the pose candidates retrieved by NetVLAD and
    (b) state-of-the-art baselines. Plots show the fraction of correctly localized
    queries ( y -axis) within a certain distance ( x -axis) whose rotation error is
    at most 10 degree.
  Figure 8 Link: articels_figures_by_rev_year\2019\InLoc_Indoor_Visual_Localization_with_Dense_Matching_and_View_Synthesis\figure_8.jpg
  Figure 8 caption: 'Qualitative comparison of different localization methods (columns).
    From top to bottom: Query image, the best matching database image, synthesized
    view at the estimated pose (without interextra-polation), error map between the
    query image and the synthesized view, localization error (meters, degrees). Warm
    colors on the error map correspond to large errors. Green dots are the inlier
    matches obtained by P3P-LO-RANSAC.'
  Figure 9 Link: articels_figures_by_rev_year\2019\InLoc_Indoor_Visual_Localization_with_Dense_Matching_and_View_Synthesis\figure_9.jpg
  Figure 9 caption: 'Qualitative comparison between InLoc and NetVLAD+SparsePE. From
    top to bottom: Query image, the best matching database image, synthesized view
    rendered from the estimated pose, error map between the query image and the synthesized
    view, localization error (meters, degrees). Warm colors on the error map correspond
    to large errors. Green dots are the inlier matches obtained by P3P-LO-RANSAC.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Hajime Taira
  Name of the last author: Akihiko Torii
  Number of Figures: 10
  Number of Tables: 7
  Number of authors: 8
  Paper title: 'InLoc: Indoor Visual Localization with Dense Matching and View Synthesis'
  Publication Date: 2019-11-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Statistics of the InLoc Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Ablation Study on the InLoc Dataset
  Table 3 caption:
    table_text: TABLE 3 Comparison With State-of-the-Art Localization Methods on the
      InLoc Dataset
  Table 4 caption:
    table_text: TABLE 4 Comparison on Matterport3D [20]
  Table 5 caption:
    table_text: TABLE 5 Evaluation on the 7 Scenes Dataset [30], [71]
  Table 6 caption:
    table_text: TABLE 6 Results on the Extended Dataset, as the Percentage (%) of
      Correctly Localized Queries Within Given Distance (m) and 10 Degree Angular
      Errors
  Table 7 caption:
    table_text: TABLE 7 Computational Time for Each Component of the Proposed Approach
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2952114
- Affiliation of the first author: school of electrical engineering, kaist, daejeon,
    republic of korea
  Affiliation of the last author: school of electrical engineering, kaist, daejeon,
    republic of korea
  Figure 1 Link: articels_figures_by_rev_year\2019\Learning_to_Localize_Sound_Sources_in_Visual_Scenes_Analysis_and_Applications\figure_1.jpg
  Figure 1 caption: Where do these sounds come from? We show an example of interactive
    sound source localization by the proposed algorithm. In this work, we demonstrate
    how to learn to localize sound sources (objects) from the sound signals in visual
    scenes.
  Figure 10 Link: articels_figures_by_rev_year\2019\Learning_to_Localize_Sound_Sources_in_Visual_Scenes_Analysis_and_Applications\figure_10.jpg
  Figure 10 caption: Ambient sound results. We show examples of frames with ambient
    sounds. (a) Sampled input frames. (b) Location responses against object indicating
    sound in Softmax only attention mechanism. (c) Location responses against ambient
    sound in Softmax only attention mechanism. (d) Location responses against object
    indicating sound in ReLU+Softmax attention mechanism. (e) Location responses against
    ambient sound in ReLU+Softmax attention mechanism. The proposed network ouputs
    discernible confidences between object-like and ambient sounds.
  Figure 2 Link: articels_figures_by_rev_year\2019\Learning_to_Localize_Sound_Sources_in_Visual_Scenes_Analysis_and_Applications\figure_2.jpg
  Figure 2 caption: "Network architecture. This architecture is designed to tackle\
    \ the problem of sound source localization with self-supervised learning. The\
    \ network uses video frame and sound pairs to learn to localize the sound sources.\
    \ Each modality is processed in its own network. After integrating (correlating)\
    \ the information from the sound context vector h and the activations of the visual\
    \ network, the localization module (attention mechanism) localizes the sound source.\
    \ By adding the supervised loss component into this architecture, it can be converted\
    \ to a unified architecture which can work as supervised or semi-supervised learning\
    \ as well. In the figure, FC stands for the fully connected layer, and \u2A02\
    \ denotes the weighted sum pooling across spatial dimensions."
  Figure 3 Link: articels_figures_by_rev_year\2019\Learning_to_Localize_Sound_Sources_in_Visual_Scenes_Analysis_and_Applications\figure_3.jpg
  Figure 3 caption: Semantically unmatched results. We show some cases where the proposed
    network with unsupervised learning draws false conclusions. We correct this issue
    by providing prior knowledge.
  Figure 4 Link: articels_figures_by_rev_year\2019\Learning_to_Localize_Sound_Sources_in_Visual_Scenes_Analysis_and_Applications\figure_4.jpg
  Figure 4 caption: Sound source localization dataset. The location and type of the
    sound sources (object versus non-objectambient) are annotated. This dataset is
    used for testing how well our network learns the sound localization and for providing
    supervision to the unified architecture.
  Figure 5 Link: articels_figures_by_rev_year\2019\Learning_to_Localize_Sound_Sources_in_Visual_Scenes_Analysis_and_Applications\figure_5.jpg
  Figure 5 caption: 'Qualitative sound localization results from unsupervised network.
    We feed image and sound pairs through our unsupervised network to localize sound
    sources. Titles of the columns are subjective annotations of contents in the corresponding
    sounds and they are shown only for visualization purpose to give an idea about
    the sound context to readers: We do not use explicit labels.'
  Figure 6 Link: articels_figures_by_rev_year\2019\Learning_to_Localize_Sound_Sources_in_Visual_Scenes_Analysis_and_Applications\figure_6.jpg
  Figure 6 caption: Interactive sound source localization. We show the responses of
    the network to different sounds while keeping the frame same. These results show
    that our network can localize the source of the given sound interactively. Label
    indicates the context of the sound. We do not use explicit labels.
  Figure 7 Link: articels_figures_by_rev_year\2019\Learning_to_Localize_Sound_Sources_in_Visual_Scenes_Analysis_and_Applications\figure_7.jpg
  Figure 7 caption: How well can our network localize the sound sources compare to
    humans? Qualitative comparison of localization between our network and human annotations.
    Human annotations (ground-truth) are represented by bounding boxes, and annotations
    from different subjects are indicated by different colors. The predictions from
    our method are the heat maps on the right panel of each block. We overlay human
    annotated bounding boxes on top of the heat maps for comparisons.
  Figure 8 Link: articels_figures_by_rev_year\2019\Learning_to_Localize_Sound_Sources_in_Visual_Scenes_Analysis_and_Applications\figure_8.jpg
  Figure 8 caption: Qualitative sound localization results from different learning
    methods. We present the sound localization results from different learning methods.
    The supervised method generally localizes the sound source precisely due to the
    guidance of ground truths. Despite using less supervised data, the semi-supervised
    approach also gives comparably accurate localization results.
  Figure 9 Link: articels_figures_by_rev_year\2019\Learning_to_Localize_Sound_Sources_in_Visual_Scenes_Analysis_and_Applications\figure_9.jpg
  Figure 9 caption: Success ratio using varying cIoU threshold. The attention mechanism
    with softmax without ReLU is used.
  First author gender probability: 0.96
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.88
  Name of the first author: Arda Senocak
  Name of the last author: In So Kweon
  Number of Figures: 15
  Number of Tables: 7
  Number of authors: 5
  Paper title: 'Learning to Localize Sound Sources in Visual Scenes: Analysis and
    Applications'
  Publication Date: 2019-11-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance Evaluation With Different Learning Schemes
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance Comparison of Learning Methods With Different
      Amounts of Data
  Table 3 caption:
    table_text: TABLE 3 Performance Measure Against Individual Subjects
  Table 4 caption:
    table_text: TABLE 4 Performance Measure Using Different Test Sets
  Table 5 caption:
    table_text: "TABLE 5 Performance Measure Against Different Loss Balance Weights\
      \ With the \u201CUnsup. 144k + Sup. 0.5k\u201D Data Setup"
  Table 6 caption:
    table_text: TABLE 6 Evaluation of Cross-Modal k-Nearest Neighbor Search With Pseudo
      Labels
  Table 7 caption:
    table_text: TABLE 7 User Study of 360 Degree Video Saliency Maps
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2952095
- Affiliation of the first author: computer science department, national engineering
    laboratory for video technology, key laboratory of machine perception (moe), peking
    university, beijing, p. r. china
  Affiliation of the last author: computer science department, center on frontiers
    of computing studies (cfcs), peking university, beijing, p. r. china
  Figure 1 Link: articels_figures_by_rev_year\2019\ADVAT_An_Asymmetric_Dueling_Mechanism_for_Learning_and_Understanding_Visual_Acti\figure_1.jpg
  Figure 1 caption: "An overview of AD-VAT. The tracker and target are \u201Cdueling.\u201D\
    \ The tracker intends to lockup the target, while the target tries to escape from\
    \ the tracker. They both use a Conv-LSTM network to encode the input and output\
    \ the action. Differently, the target is additionally fed with the trackers observation\
    \ and action, and learns to predict the trackers reward as an auxiliary task.\
    \ Note that TRP is Tracker Reward Predictor. The blue and green represent the\
    \ component of tracker and target, respectively."
  Figure 10 Link: articels_figures_by_rev_year\2019\ADVAT_An_Asymmetric_Dueling_Mechanism_for_Learning_and_Understanding_Visual_Acti\figure_10.jpg
  Figure 10 caption: t-SNE visualization of the latent vectors. The gray dot is the
    feature of the first four frames of each sequence. The colors represents the eight
    directions, respectively. The darkness of the color corresponds to the distance
    between the target and the start point. The instances of each direction are shown
    around the center figure.
  Figure 2 Link: articels_figures_by_rev_year\2019\ADVAT_An_Asymmetric_Dueling_Mechanism_for_Learning_and_Understanding_Visual_Acti\figure_2.jpg
  Figure 2 caption: "Visualizing r 1 + r 2 as a heatmap in x\u2212y plane with the\
    \ tracker in the image center. The yellow area indicates the zone playing zero-sum\
    \ game. Left: The reward adopted in our 2D environment experiment. Right: The\
    \ reward adopted in our 3D environment experiment."
  Figure 3 Link: articels_figures_by_rev_year\2019\ADVAT_An_Asymmetric_Dueling_Mechanism_for_Learning_and_Understanding_Visual_Acti\figure_3.jpg
  Figure 3 caption: "Examples of the six testing environments. The top shows the examples\
    \ of \u201Cblock\u201D map with the historical trajectories of three baselines\
    \ (from left to right: RPF, Ram, Nav) and the trackers observation (on the most\
    \ right). The bottom is about the \u201Cmaze\u201D map."
  Figure 4 Link: articels_figures_by_rev_year\2019\ADVAT_An_Asymmetric_Dueling_Mechanism_for_Learning_and_Understanding_Visual_Acti\figure_4.jpg
  Figure 4 caption: 'The examples of the third-person views of the 3D environments
    used for training and testing. From top to bottom, they are EnvAug Room, EnvAug+Obstacle
    Room, and five realistic testing environments (from left to right: Urban City,
    Snow Village, Parking Lot, Urban Road, and Garden).'
  Figure 5 Link: articels_figures_by_rev_year\2019\ADVAT_An_Asymmetric_Dueling_Mechanism_for_Learning_and_Understanding_Visual_Acti\figure_5.jpg
  Figure 5 caption: The average cumulative reward of the tracker validated in the
    Block-Nav environment, as a function of iterations times. The mean performance
    and the confidence interval are plotted across 5 seeds. On the top, we compare
    AD-VAT to the baselines. On the bottom, we compare AD-VAT to its ablations. Note
    that TAM represents that the target adopts the TAM but not learned with the auxiliary
    task (predicting the trackers reward).
  Figure 6 Link: articels_figures_by_rev_year\2019\ADVAT_An_Asymmetric_Dueling_Mechanism_for_Learning_and_Understanding_Visual_Acti\figure_6.jpg
  Figure 6 caption: 'Target position distributions in a tracker-centric coordinate
    system. Evolution from early training to late training is arranged from left to
    right. From top to bottom: RPF, Ram, Nav, and AD-VAT+. The darker, the bigger
    the counts.'
  Figure 7 Link: articels_figures_by_rev_year\2019\ADVAT_An_Asymmetric_Dueling_Mechanism_for_Learning_and_Understanding_Visual_Acti\figure_7.jpg
  Figure 7 caption: 'The relative position distribution of the AD-VAT+ target while
    adversarial testing the trackers. From left to right: RPF, Ram, Nav, and AD-VAT+.'
  Figure 8 Link: articels_figures_by_rev_year\2019\ADVAT_An_Asymmetric_Dueling_Mechanism_for_Learning_and_Understanding_Visual_Acti\figure_8.jpg
  Figure 8 caption: The adversarial target in AD-VAT attempts to to fool the tracker
    by moving to the wall that is of similar texture to itself.
  Figure 9 Link: articels_figures_by_rev_year\2019\ADVAT_An_Asymmetric_Dueling_Mechanism_for_Learning_and_Understanding_Visual_Acti\figure_9.jpg
  Figure 9 caption: Exemplar cases. The four sequences with yellow number are successful
    cases. The one with the red number is a failure case. The number on the top-left
    corner of each frame is the corresponding frame ID in the episode. The top row
    is a simple map of the first sequence to show the location of the obstacle, target,
    and tracker at each keyframe. The gray circle is the obstacle, the small circle
    is the target, and the small triangle is the tracker. The green, yellow, and orange
    represent the occlusion state of the target, no occlusion, locally occluded, and
    fully occluded, respectively.
  First author gender probability: 0.81
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Fangwei Zhong
  Name of the last author: Yizhou Wang
  Number of Figures: 11
  Number of Tables: 6
  Number of authors: 5
  Paper title: 'AD-VAT+: An Asymmetric Dueling Mechanism for Learning and Understanding
    Visual Active Tracking'
  Publication Date: 2019-11-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Details of Parameters of the Rewards in the 2D and 3D
      Experiments
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Evaluation in the 2D Environments
  Table 3 caption:
    table_text: TABLE 3 Using the Adversarial Target to Attack Each Tracker
  Table 4 caption:
    table_text: TABLE 4 Comparison With Heuristic Target
  Table 5 caption:
    table_text: TABLE 5 Comparison With Standard Two-Step Trackers
  Table 6 caption:
    table_text: TABLE 6 Results on the 3D Environments With Obstacles
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2952590
