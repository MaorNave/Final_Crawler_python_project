- Affiliation of the first author: school of computer and information technology,
    shanxi university, taiyuan, shanxi, china
  Affiliation of the last author: school of computer and information technology, shanxi
    university, taiyuan, shanxi, china
  Figure 1 Link: articels_figures_by_rev_year\2020\SemiSupervised_Clustering_With_Constraints_of_Different_Types_From_Multiple_Info\figure_1.jpg
  Figure 1 caption: Example of a data set with 3 clusters and constraints from 4 experts.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\SemiSupervised_Clustering_With_Constraints_of_Different_Types_From_Multiple_Info\figure_2.jpg
  Figure 2 caption: Comparisons of different algorithms with pairwise constraints.
  Figure 3 Link: articels_figures_by_rev_year\2020\SemiSupervised_Clustering_With_Constraints_of_Different_Types_From_Multiple_Info\figure_3.jpg
  Figure 3 caption: Comparisons of different algorithms with positive labels.
  Figure 4 Link: articels_figures_by_rev_year\2020\SemiSupervised_Clustering_With_Constraints_of_Different_Types_From_Multiple_Info\figure_4.jpg
  Figure 4 caption: Comparisons of different algorithms with positive and negative
    labels.
  Figure 5 Link: articels_figures_by_rev_year\2020\SemiSupervised_Clustering_With_Constraints_of_Different_Types_From_Multiple_Info\figure_5.jpg
  Figure 5 caption: Comparisons of the SC-MPI algorithm with different proportions
    of noisy constraints.
  Figure 6 Link: articels_figures_by_rev_year\2020\SemiSupervised_Clustering_With_Constraints_of_Different_Types_From_Multiple_Info\figure_6.jpg
  Figure 6 caption: Effect of the parameter alpha on the performance of the proposed
    algorithm.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.69
  Name of the first author: Liang Bai
  Name of the last author: Fuyuan Cao
  Number of Figures: 6
  Number of Tables: 1
  Number of authors: 3
  Paper title: Semi-Supervised Clustering With Constraints of Different Types From
    Multiple Information Sources
  Publication Date: 2020-03-10 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Description of Data Sets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2979699
- Affiliation of the first author: state key lab of intelligent technologies and systems,
    beijing national research center for information science and technology (bnrist),
    beijing, china
  Affiliation of the last author: department of automation, tsinghua university, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2020\HardnessAware_Deep_Metric_Learning\figure_1.jpg
  Figure 1 caption: "An illustration of the proposed hardness-aware feature synthesis.\
    \ The upper and lower part of the figure illustrates the embedding space and the\
    \ feature space, respectively. We aim to learn the mapping from the feature space\
    \ to the embedding space so that the euclidean distance can effectively measure\
    \ the similarity. Points with the same color represent the same sample in two\
    \ different spaces and points of the same shape belong to the same class. A curve\
    \ in the feature space represents a manifold near which samples belong to one\
    \ specific class concentrate. The proposed hardness-aware augmentation first modifies\
    \ a sample y \u2212 (or z \u2212 ) to y \u2212 (or z \u2212 ). Then a label-and-hardness-preserving\
    \ generator projects it to y \u02DC \u2212 (or z \u02DC \u2212 ) which is the\
    \ closest point to y \u2212 on the manifold of y \u2212 in the feature space.\
    \ We adaptively control the hardness of the synthetic negative y \u02DC \u2212\
    \ and simultaneously preserve its original label so that the synthetic hardness-aware\
    \ tuple can be favorably exploited for effective training. (Best viewed in color.)"
  Figure 10 Link: articels_figures_by_rev_year\2020\HardnessAware_Deep_Metric_Learning\figure_10.jpg
  Figure 10 caption: Impact of the number of sampled augmented points on the CUB-200-2011
    dataset.
  Figure 2 Link: articels_figures_by_rev_year\2020\HardnessAware_Deep_Metric_Learning\figure_2.jpg
  Figure 2 caption: An illustration of the proposed hardness-aware augmentation. Points
    with the same shape are from the same class. We perform linear interpolation on
    the negative pair in the embedding space to obtain a harder tuple, where the hardness
    level is controlled by the training status of the metric model. As the training
    proceeds, harder and harder tuples are generated to train the metric more efficiently.
    (Best viewed in color.)
  Figure 3 Link: articels_figures_by_rev_year\2020\HardnessAware_Deep_Metric_Learning\figure_3.jpg
  Figure 3 caption: "The overall network architecture of the proposed HDML framework.\
    \ The metric model is a CNN network followed by a fully connected layer. The augmentor\
    \ can sample one or multiple augmented points according to the employed sample\
    \ augmentation strategy, and the generator is composed of two fully connected\
    \ layers with increasing dimensions. The generator takes in the original embedding\
    \ z and the augmented embeddings z separately to obtain y \u2032 and the hardness-aware\
    \ synthetics y \u02DC , respectively, which we use to compute J recon and J soft\
    \ . The red dashed arrow points from the part that the loss is computed on, and\
    \ to the module that the loss directly supervises. For multi-sample augmentation,\
    \ we assess the generated synthetics according to the proposed criteria and select\
    \ k qualified ones from them for further training. Part of the metric and the\
    \ following generator form a similar structure to the well-known autoencoder.\
    \ (Best viewed in color.)"
  Figure 4 Link: articels_figures_by_rev_year\2020\HardnessAware_Deep_Metric_Learning\figure_4.jpg
  Figure 4 caption: An illustration of the relations among the main notations.
  Figure 5 Link: articels_figures_by_rev_year\2020\HardnessAware_Deep_Metric_Learning\figure_5.jpg
  Figure 5 caption: An illustration of the proposed multi-sample augmentation and
    the synthetic selection method. A curve represents a manifold near which samples
    belong to one specific class concentrate. Points of the same shape (circle or
    square) belong to the same class and triangle points denote that they are false
    or from some unknown class. The dashed back circle centered at mathbf y represents
    the distribution of the sampled augmented points. The original triplet is composed
    of three points mathbf y , mathbf y+ , and mathbf y- . We sample four possible
    augmented points hatmathbfy1 , hatmathbf y2 , hatmathbf y3 , and hatmathbf y4
    as demonstration, which are then projected onto tildemathbf y1 , tildemathbf y2
    , tildemathbf y3 , and tildemathbf y4 , respectively, by the hardness-and-label-preserving
    generator. We then assess them based on the proposed criteria and select hatmathbf
    y1 as a qualified synthetic. hatmathbf y-1 is then combined with mathbf y and
    mathbf y+ to construct an effective and adaptive triplet. (Best viewed in color.)
  Figure 6 Link: articels_figures_by_rev_year\2020\HardnessAware_Deep_Metric_Learning\figure_6.jpg
  Figure 6 caption: Comparisons of different settings in the clustering task on the
    Cars196 dataset.
  Figure 7 Link: articels_figures_by_rev_year\2020\HardnessAware_Deep_Metric_Learning\figure_7.jpg
  Figure 7 caption: Comparisons of different settings in the retrieval task on the
    Cars196 dataset.
  Figure 8 Link: articels_figures_by_rev_year\2020\HardnessAware_Deep_Metric_Learning\figure_8.jpg
  Figure 8 caption: Comparisons of converged results using different pulling factors
    in the clustering and retrieval task on the Cars196 dataset.
  Figure 9 Link: articels_figures_by_rev_year\2020\HardnessAware_Deep_Metric_Learning\figure_9.jpg
  Figure 9 caption: Comparisons of using different pulling factors in the retrieval
    task on the Cars196 dataset.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.59
  Name of the first author: Wenzhao Zheng
  Name of the last author: Jie Zhou
  Number of Figures: 12
  Number of Tables: 9
  Number of authors: 3
  Paper title: Hardness-Aware Deep Metric Learning
  Publication Date: 2020-03-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Evaluation of Different Selection Strategies on the CUB-200-2011
      Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Evaluation of Different Selection Criteria on the CUB-200-2011
      Dataset
  Table 3 caption:
    table_text: TABLE 3 P-values of the t-Tests With Respect to Triplet Hard on the
      CUB-200-2011 Dataset
  Table 4 caption:
    table_text: TABLE 4 P-values of the t-Tests With Respect to N-Pair on the CUB-200-2011
      Dataset
  Table 5 caption:
    table_text: TABLE 5 Experimental Results (%) on the CUB-200-2011 Dataset in Comparison
      With Other Methods
  Table 6 caption:
    table_text: TABLE 6 Experimental Results (%) on the Cars196 Dataset in Comparison
      With Other Methods
  Table 7 caption:
    table_text: TABLE 7 Experimental Results (%) on the Stanford Online Products Dataset
      in Comparison With Other Methods
  Table 8 caption:
    table_text: TABLE 8 Experimental Results (%) on the In-Shop Clothes Retrieval
      Dataset in Comparison With Other Methods
  Table 9 caption:
    table_text: TABLE 9 Experimental Results (%) on the VehicleID Dataset in Comparison
      With Other Methods
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2980231
- Affiliation of the first author: "cmap, cnrs, \xE9cole polytechnique, i.p. paris\
    \ and aramis project-team, inria, palaiseau, france"
  Affiliation of the last author: "centre de recherche des cordeliers, universit\xE9\
    \ de paris, inserm, sorbonne universit\xE9, paris, france"
  Figure 1 Link: articels_figures_by_rev_year\2020\Gaussian_Graphical_Model_Exploration_and_Selection_in_High_Dimension_Low_Sample_\figure_1.jpg
  Figure 1 caption: Composite GGM estimation. We respectively identify with green
    or orange bullets the steps adhering to a local or global paradigm. Comments are
    in italics.
  Figure 10 Link: articels_figures_by_rev_year\2020\Gaussian_Graphical_Model_Exploration_and_Selection_in_High_Dimension_Low_Sample_\figure_10.jpg
  Figure 10 caption: Conditional covariance matrix between the 35 variables measured
    on the cohort. The positive correlations are in red and the negative in blue.
    The diagonal coefficients are ignored in this study. GGMselect (left) and Composite
    (middle) share the same colour scale. The rightmost figure corresponds to one
    of the sparsest GLASSO solution.
  Figure 2 Link: articels_figures_by_rev_year\2020\Gaussian_Graphical_Model_Exploration_and_Selection_in_High_Dimension_Low_Sample_\figure_2.jpg
  Figure 2 caption: 'Average performances as a function of the complexity for: the
    MLE from the GGMselect graph (green), the GLASSO solutions (blue) and the MLEs
    from the GLASSO graphs (orange). The average is taken over 100 simulations. In
    each simulation, n=30 data points are simulated from a given true graph, different
    for each subfigure. The two rows of subfigures correspond to two different graph
    sizes, p=30 and p=50 vertices, respectively. The three columns correspond to true
    graphs with different connectivity. At the top of each column, a graph illustrates
    the typical connectivity of the true graphs in said column.'
  Figure 3 Link: articels_figures_by_rev_year\2020\Gaussian_Graphical_Model_Exploration_and_Selection_in_High_Dimension_Low_Sample_\figure_3.jpg
  Figure 3 caption: Graph selection in the presence of a hub. The first figure is
    the true graph. The second and third are the graphs respectively selected by the
    GGMSC and CVCE on the same fixed graph path going from the fully sparse to the
    fully connected, via the GGMselect graph and the true graph.
  Figure 4 Link: articels_figures_by_rev_year\2020\Gaussian_Graphical_Model_Exploration_and_Selection_in_High_Dimension_Low_Sample_\figure_4.jpg
  Figure 4 caption: 'On a single simulation: evolution of and model selected by GGMSC
    (green), CVCE (red) and TCE (blue) along the fixed deterministic path. The true
    graphs position on that path is represented by a vertical grey line. GGMSC stops
    early whereas CVCE selects the true graph (the vertical grey line and the dashed
    red one are the same). Moreover, the CVCE graph is very close to the best graph
    in terms of True Cross Entropy.'
  Figure 5 Link: articels_figures_by_rev_year\2020\Gaussian_Graphical_Model_Exploration_and_Selection_in_High_Dimension_Low_Sample_\figure_5.jpg
  Figure 5 caption: Average KL divergence (y axis) and complexity (x axis) of the
    models selected with GGMSC (green), CVCE (shades of red) and TCE (blue) on synthetic
    data. The sparsity level of the true graph is represented by a black dashed vertical
    line. The second row offers a zoomed in view of the boxed areas to focus on the
    CVCE and TCE models. The graphs selected by the CVCE are much closer to the best
    in True Cross Entropy in terms of performance and edge structure than the GGMSC
    one. Moreover, they are also very close to the true graph used in the simulation,
    even when the sample size is small.
  Figure 6 Link: articels_figures_by_rev_year\2020\Gaussian_Graphical_Model_Exploration_and_Selection_in_High_Dimension_Low_Sample_\figure_6.jpg
  Figure 6 caption: 'Out of sample performances as a function of the complexity for:
    the MLE from the GGMselect graph (green), the GLASSO solutions (blue) and the
    MLEs refitted from the GLASSO graphs (orange).'
  Figure 7 Link: articels_figures_by_rev_year\2020\Gaussian_Graphical_Model_Exploration_and_Selection_in_High_Dimension_Low_Sample_\figure_7.jpg
  Figure 7 caption: Out of Sample Likelihood (y axis) and complexity (x axis) of models
    selected by GGMSC (green), CVCE (shades of red) and OSL (blue) on real data. The
    right picture offers a zoomed out view to include the model selected by OSL on
    the GLASSO path (purple). The left figure corresponds to the boxed area of the
    right figure.
  Figure 8 Link: articels_figures_by_rev_year\2020\Gaussian_Graphical_Model_Exploration_and_Selection_in_High_Dimension_Low_Sample_\figure_8.jpg
  Figure 8 caption: Selected edges by GGMselect (up), our Composite method (mid) and
    the best Out of Sample GLASSO (down) in-between MRI measures. The perspective
    of the sagittal view hides the many edges between symmetrical regions. GLASSO
    proposes too many to allow for interpretation.
  Figure 9 Link: articels_figures_by_rev_year\2020\Gaussian_Graphical_Model_Exploration_and_Selection_in_High_Dimension_Low_Sample_\figure_9.jpg
  Figure 9 caption: Selected edges by GGMselect (up), our Composite method (mid) and
    the best Out of Sample GLASSO (down) between PET (yellow) and MRI (red) measures.
    GGMselect finds no connection in this sub-part of the graph, although one may
    expect some.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Thomas Lartigue
  Name of the last author: "St\xE9phanie Allassonni\xE8re"
  Number of Figures: 10
  Number of Tables: 1
  Number of authors: 6
  Paper title: Gaussian Graphical Model Exploration and Selection in High Dimension
    Low Sample Size Setting
  Publication Date: 2020-03-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average and (Standard Deviation) of the Execution Times of
      Different GGM Methods
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2980542
- Affiliation of the first author: hciiwr, heidelberg university, heidelberg, germany
  Affiliation of the last author: hciiwr, heidelberg university, heidelberg, germany
  Figure 1 Link: articels_figures_by_rev_year\2020\The_Mutex_Watershed_and_its_Objective_Efficient_ParameterFree_Graph_Partitioning\figure_1.jpg
  Figure 1 caption: 'Left: Overlay of raw data from the ISBI 2012 EM segmentation
    challenge and the edges for which attractive (green) or repulsive (red) interactions
    are estimated for each pixel using a CNN. Middle: vertical horizontal repulsive
    interactions at intermediate long range are shown in the top bottom half. Right:
    Active mutual exclusion (mutex) constraints that the proposed algorithm invokes
    during the segmentation process.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\The_Mutex_Watershed_and_its_Objective_Efficient_ParameterFree_Graph_Partitioning\figure_2.jpg
  Figure 2 caption: Two equivalent representations of the seeded watershed clustering
    obtained using (a) a maximum spanning tree computation or (b) Algorithm 1. Both
    graphs share the weighted attractive (green) edges and seeds (hatched nodes).
    The infinitely attractive connections to the auxiliary node (gray) in (a) are
    replaced by infinitely repulsive (red) edges between each pair of seeds in (b).
    The two final clusterings are defined by the active sets (bold edges) and are
    identical. Node colors indicate the clustering result, but are arbitrary.
  Figure 3 Link: articels_figures_by_rev_year\2020\The_Mutex_Watershed_and_its_Objective_Efficient_ParameterFree_Graph_Partitioning\figure_3.jpg
  Figure 3 caption: Some iterations of the Mutex Watershed Algorithm 2 applied to
    a graph with weighted attractive (green) and repulsive (red) edges. Edges accumulated
    in the active set A after a given number of iterations are shown in bold. The
    connectall parameter of the algorithm is set to False, so that only the positive
    edges belonging to the maximum spanning tree of each cluster are added to the
    active set. Once the algorithm terminates, the final active set (f) defines the
    final clustering (indicated using arbitrary node colors). Some edges are not added
    to the active set because they are mutex constrained (yellow highlight) or because
    the associated nodes are already connected and in the same cluster (blue highlight).
  Figure 4 Link: articels_figures_by_rev_year\2020\The_Mutex_Watershed_and_its_Objective_Efficient_ParameterFree_Graph_Partitioning\figure_4.jpg
  Figure 4 caption: Runtime T of Mutex Watershed (without sorting of edges) measured
    on sub-volumes of the ISBI challenge of different sizes (thereby varying the total
    number of edges E ). We plot fracT|E| over |E| in a logarithmic plot, which makes
    T sim |E| log(|E|) appear as straight line. A logarithmic function (blue line)
    is fitted to the measured fracT|E| (blue circles) with ( R2 = 0.9896 ). The good
    fit suggests that empirically T approx mathcal O(E; log E) .
  Figure 5 Link: articels_figures_by_rev_year\2020\The_Mutex_Watershed_and_its_Objective_Efficient_ParameterFree_Graph_Partitioning\figure_5.jpg
  Figure 5 caption: "Consistent and inconsistent active sets \u2013 Two different\
    \ active edge sets A1subseteq E (on the left) and A2subseteq E (on the right)\
    \ on identical toy graphs with six nodes, attractive (green) and repulsive (red)\
    \ edges. The value of the edge indicator xAin lbrace 0,1rbrace |E| defined in\
    \ Eq. (4) is shown for every edge. Members of the active sets are shown as solid\
    \ lines. On the left, the active set A1 is consistent, i.e., does not include\
    \ any conflicted cycle mathcal C-(mathcal G,w) (see Definition 4.1): Therefore,\
    \ it is associated with a clustering (represented by arbitrary node colors). On\
    \ the right, the active set A2 is not consistent and includes at least one conflicted\
    \ cycle (highlighted in yellow), thus it cannot be associated with a node clustering."
  Figure 6 Link: articels_figures_by_rev_year\2020\The_Mutex_Watershed_and_its_Objective_Efficient_ParameterFree_Graph_Partitioning\figure_6.jpg
  Figure 6 caption: Local neighborhood structure of attractive (green) and repulsive
    (red) edges in the Mutex Watershed graph.
  Figure 7 Link: articels_figures_by_rev_year\2020\The_Mutex_Watershed_and_its_Objective_Efficient_ParameterFree_Graph_Partitioning\figure_7.jpg
  Figure 7 caption: Mutex Watershed and baseline segmentation algorithms applied on
    the ISBI Challenge test data. Red arrows point out major errors. Orange arrows
    point to difficult, but correctly segmented regions. All methods share the same
    input maps.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Steffen Wolf
  Name of the last author: Fred A. Hamprecht
  Number of Figures: 7
  Number of Tables: 1
  Number of authors: 7
  Paper title: 'The Mutex Watershed and its Objective: Efficient, Parameter-Free Graph
    Partitioning'
  Publication Date: 2020-03-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Results on the ISBI 2012 EM Segmentation Challenge
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2980827
- Affiliation of the first author: department of automation, state key lab of intelligent
    technologies and systems, beijing national research center for information science
    and technology (bnrist), tsinghua university, beijing, china
  Affiliation of the last author: department of automation, state key lab of intelligent
    technologies and systems, beijing national research center for information science
    and technology (bnrist), tsinghua university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Comprehensive_Instructional_Video_Analysis_The_COIN_Dataset_and_Performance_Eval\figure_1.jpg
  Figure 1 caption: "Visualization of two root-to-leaf branches of the COIN. There\
    \ are three levels of our dataset: domain, task and step. Take the top row as\
    \ an example, in the left box, we show a set of frames of 9 different tasks associated\
    \ with the domain \u201Cvehicles\u201D. In the middle box, we present several\
    \ images of 9 videos belonging to the task \u201Cchange the car tire\u201D. Based\
    \ on this task, in the right box, we display a sequence of frames sampled from\
    \ a specific video, where the indices are presented at the left-top of each frame.\
    \ The intervals in red, blue and yellow indicate the step of \u201Cunscrew the\
    \ screws\u201D, \u201Cjack up the car\u201D and \u201Cput on the tire\u201D, which\
    \ are described with the text in corresponding color at the bottom of the right\
    \ box. All figures are best viewed in color."
  Figure 10 Link: articels_figures_by_rev_year\2020\Comprehensive_Instructional_Video_Analysis_The_COIN_Dataset_and_Performance_Eval\figure_10.jpg
  Figure 10 caption: "Visualized results of step localization. The videos belongs\
    \ to the tasks of \u201Cpaste screen protector on Pad\u201D and \u201Cinstall\
    \ the bicycle rack\u201D."
  Figure 2 Link: articels_figures_by_rev_year\2020\Comprehensive_Instructional_Video_Analysis_The_COIN_Dataset_and_Performance_Eval\figure_2.jpg
  Figure 2 caption: Number of papers related to instructional video analysis published
    on top computer vision conferences (CVPRICCVECCV) over the recent 10 years.
  Figure 3 Link: articels_figures_by_rev_year\2020\Comprehensive_Instructional_Video_Analysis_The_COIN_Dataset_and_Performance_Eval\figure_3.jpg
  Figure 3 caption: The timeline of different tasks for instructional video analysis
    being proposed.
  Figure 4 Link: articels_figures_by_rev_year\2020\Comprehensive_Instructional_Video_Analysis_The_COIN_Dataset_and_Performance_Eval\figure_4.jpg
  Figure 4 caption: "Illustration of the COIN lexicon. The left figure shows the hierarchical\
    \ structure, where the nodes of three different sizes correspond to the domain,\
    \ task and step respectively. For brevity, we do not draw all the tasks and steps\
    \ here. The right figure presents detailed steps of the task \u201Creplace a bulb\u201D\
    , which belongs to the domain \u201Celectrical appliances\u201D."
  Figure 5 Link: articels_figures_by_rev_year\2020\Comprehensive_Instructional_Video_Analysis_The_COIN_Dataset_and_Performance_Eval\figure_5.jpg
  Figure 5 caption: The interface of our new developed annotation tool under the frame
    mode.
  Figure 6 Link: articels_figures_by_rev_year\2020\Comprehensive_Instructional_Video_Analysis_The_COIN_Dataset_and_Performance_Eval\figure_6.jpg
  Figure 6 caption: The duration statistics of the videos (left) and segments (right)
    in the COIN dataset.
  Figure 7 Link: articels_figures_by_rev_year\2020\Comprehensive_Instructional_Video_Analysis_The_COIN_Dataset_and_Performance_Eval\figure_7.jpg
  Figure 7 caption: Flowchart of our proposed task-consistency method. During the
    first bottom-up aggregation stage, the inputs are a series of scores mathbf Sp=lbrace
    mathbf sp1, ldots, mathbf spn, ldots, mathbf spNrbrace of an instructional video,
    which denotes the probabilities of each step appearing in the corresponding proposal.
    We first aggregate them into a video-based score mathbf sv , and map it into another
    score mathbf st to predict the task label Y . At top-down refinement stage, we
    generate a refined mask vector mathbf vr based on the task label. Then we alleviate
    the weights of other bits in mathbf Sp by mathbf vr to ensure the task-consistency.
    The refined scores mathbf Sr are finally utilized to perform NMS process and output
    the final results.
  Figure 8 Link: articels_figures_by_rev_year\2020\Comprehensive_Instructional_Video_Analysis_The_COIN_Dataset_and_Performance_Eval\figure_8.jpg
  Figure 8 caption: Pipeline of our ordering-dependency method. The input of our method
    is a set of proposals Pin and we refine it through the following three stages.
    (1) In order to deal with the overlapped proposals Pin , we first group them into
    a series of segments S . (2) We refine the segments S based on the ordering information
    in the training set. (3) We map the score variation of Delta mathbf ssl into Delta
    mathbf sinn , and finally refine the proposal Pin .
  Figure 9 Link: articels_figures_by_rev_year\2020\Comprehensive_Instructional_Video_Analysis_The_COIN_Dataset_and_Performance_Eval\figure_9.jpg
  Figure 9 caption: "Visualization of auxiliary matrix Omega and transition matrix\
    \ Upsilon of the task \u201CReplace a Bulb\u201D, which contains 4 steps as \u201C\
    remove the light shell\u201D, \u201Ctake out the old bulb\u201D, \u201Cinstall\
    \ the new bulb\u201D and \u201Cinstall the light shell\u201D."
  First author gender probability: 0.93
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.59
  Name of the first author: Yansong Tang
  Name of the last author: Jie Zhou
  Number of Figures: 10
  Number of Tables: 14
  Number of authors: 3
  Paper title: 'Comprehensive Instructional Video Analysis: The COIN Dataset and Performance
    Evaluation'
  Publication Date: 2020-03-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparisons of Existing Datasets Related to Instructional
      Video Analysis
  Table 10 caption:
    table_text: TABLE 10 Comparisons of the Action Segmentation Accuracy (%) on the
      COIN
  Table 2 caption:
    table_text: TABLE 2 Comparisons of the Step Localization Accuracy (%) of the Baselines
      and Our Task-Consistency (TC) Method on the COIN Dataset
  Table 3 caption:
    table_text: TABLE 3 Study of Different Approaches for Calculating the Task Score
      in Task-Consistency Method on the COIN Dataset
  Table 4 caption:
    table_text: TABLE 4 Analysis on Our Proposed TC and OD Methods on the COIN Dataset
  Table 5 caption:
    table_text: "TABLE 5 Study of the Hyper-Parameters \u03BB 1 \u03BB1 and \u03BB\
      \ 2 \u03BB2 for Step Localization on the COIN and Breakfast Dataset"
  Table 6 caption:
    table_text: TABLE 6 Analysis of the OD Method
  Table 7 caption:
    table_text: TABLE 7 Comparisons of the Step Localization Accuracy (%) Over 12
      Domains on the COIN Dataset
  Table 8 caption:
    table_text: TABLE 8 Study of the TC and OD Approaches on Different Basic Models
  Table 9 caption:
    table_text: TABLE 9 Comparisons of the Step Localization Accuracy (%) on the Breakfast
      Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2980824
- Affiliation of the first author: school of computer and information technology,
    liaoning normal university, dalian, china
  Affiliation of the last author: department of automation, beijing national research
    center for information science and technology, institute for brain and cognitive
    sciences, tsinghua university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Model_Study_of_Transient_Imaging_With_MultiFrequency_TimeofFlight_Sensors\figure_1.jpg
  Figure 1 caption: "Frame-based compressive transient imaging: the transient imaging\
    \ system can be modeled with a near-tight frame. Left: Compressive measurements.\
    \ Upper left: Compressive sampling process with the measurement matrix \u03A6\
    \ . Lower left: Recovery process with the discrete Fourier transform matrix \u03A8\
    \ . Right: Wavelet decomposition and reconstruction. Performing the 5-level wavelet\
    \ decomposition of i \u03B2 , s d 1 and s d 3 are the reconstructed detail signals,\
    \ s a 3 is the reconstructed approximation signal, and i is the desired transient\
    \ signal. Upper right: Time slices of the reconstructed transient images."
  Figure 10 Link: articels_figures_by_rev_year\2020\Model_Study_of_Transient_Imaging_With_MultiFrequency_TimeofFlight_Sensors\figure_10.jpg
  Figure 10 caption: "Temporal profiles of two pixels from \u201CB02corner\u201D.\
    \ The blue lines represent the ground truth. The red lines represent the results\
    \ reconstructed by Peters et al.s algorithm. The golden lines represent the results\
    \ reconstructed by Qiao et al.s algorithm. The purple lines represent the results\
    \ reconstructed by Algorithm 3."
  Figure 2 Link: articels_figures_by_rev_year\2020\Model_Study_of_Transient_Imaging_With_MultiFrequency_TimeofFlight_Sensors\figure_2.jpg
  Figure 2 caption: "Operating principle of a multi-frequency ToF sensor. The light\
    \ source is s l (\u03C9t) , the incident light is s r (\u03C9t) , the sensor gain\
    \ is s s (\u03C9t+\u03C6) , and the sensor works in homodyne mode. The captured\
    \ H(\u03C9,\u03C6) is the cross-correlation of the functions s r (\u03C9t) and\
    \ s s (\u03C9t+\u03C6) ."
  Figure 3 Link: articels_figures_by_rev_year\2020\Model_Study_of_Transient_Imaging_With_MultiFrequency_TimeofFlight_Sensors\figure_3.jpg
  Figure 3 caption: "First row: Color visualizations of C , with the vertical axis\
    \ corresponding to modulation frequencies and the horizontal axis corresponding\
    \ to travel times. Second row: Color visualizations of C \u2217 C . Third row:\
    \ The entries of the main diagonal of C \u2217 C . These plots are for phases\
    \ of \u03C6=0 and \u03C02 ."
  Figure 4 Link: articels_figures_by_rev_year\2020\Model_Study_of_Transient_Imaging_With_MultiFrequency_TimeofFlight_Sensors\figure_4.jpg
  Figure 4 caption: "Evaluation of Algorithm 1. The signal at (120,140) is from \u201C\
    Tomato\u201D. The signal at (64,96) is from \u201CCorner\u201D. First row: The\
    \ measurement domain signals (namely, h ) and their amplitude spectra of the Fourier\
    \ transform (namely, |y| ). Second row: The full measurements recovered using\
    \ the Fourier basis as the sparsity basis. Third row: The full measurements recovered\
    \ using the calibrated matrix as the sparsity basis. \u201C k \u201D represents\
    \ the k -sparsity, \u201C L \u201D represents the number of available measurements,\
    \ and \u201C tol \u201D represents the recovery error. Fourth row: The relationship\
    \ between the recovery error and the k -sparsity. Fifth row: The relationship\
    \ between the recovery error and the number of available measurements, where each\
    \ data point is the recovery error when the number of measurements required is\
    \ 55. All of them are for cases with phases of \u03C6=0 and \u03C02 ."
  Figure 5 Link: articels_figures_by_rev_year\2020\Model_Study_of_Transient_Imaging_With_MultiFrequency_TimeofFlight_Sensors\figure_5.jpg
  Figure 5 caption: "Selected frames from \u201CTomato\u201D. First row: Original\
    \ ground-truth frames. Second row: Reconstructions using Heide et al.s algorithm\
    \ with a single i-step. Third row: Reconstructions using Lin et al.s algorithm.\
    \ Fourth row: Reconstructions using Peters et al.s algorithm. Fifth row: Reconstructions\
    \ using Algorithm 2. Sixth row: Reconstructions using Algorithm 3. Each column\
    \ corresponds to the same frame. Temporal profiles of several colored pixels in\
    \ the frame are shown in Fig. 6. Please refer to the supplemental video, available\
    \ online, for more results."
  Figure 6 Link: articels_figures_by_rev_year\2020\Model_Study_of_Transient_Imaging_With_MultiFrequency_TimeofFlight_Sensors\figure_6.jpg
  Figure 6 caption: "Temporal profiles of several pixels from \u201CTomato\u201D.\
    \ The blue lines represent the ground truth. Top: The green lines represent the\
    \ results reconstructed by Heide et al.s algorithm with a single i-step. The golden\
    \ lines represent the results reconstructed by Lin et al.s algorithm. The red\
    \ lines represent the results reconstructed by Peters et al.s algorithm. Bottom:\
    \ The red lines represent the results reconstructed by Algorithm 2. The golden\
    \ lines represent the results reconstructed by Algorithm 3. Each column corresponds\
    \ to the same pixel."
  Figure 7 Link: articels_figures_by_rev_year\2020\Model_Study_of_Transient_Imaging_With_MultiFrequency_TimeofFlight_Sensors\figure_7.jpg
  Figure 7 caption: "Selected frames from \u201CDragon \u201D. First row: Original\
    \ ground-truth frames. Second row: Reconstructions using Heide et al.s algorithm\
    \ with a single i-step. Third row: Reconstructions using Lin et al.s algorithm.\
    \ Fourth row: Reconstructions using Peters et al.s algorithm. Fifth row: Reconstructions\
    \ using Algorithm 2. Sixth row: Reconstructions using Algorithm 3. Each column\
    \ corresponds to the same frame. Temporal profiles of several colored pixels in\
    \ the frames are shown in Fig. 8. Please refer to the supplemental video, available\
    \ online, for more results."
  Figure 8 Link: articels_figures_by_rev_year\2020\Model_Study_of_Transient_Imaging_With_MultiFrequency_TimeofFlight_Sensors\figure_8.jpg
  Figure 8 caption: "Temporal profiles of several pixels from \u201CDragon\u201D.\
    \ The blue lines represent the ground truth. Top: The green lines represent the\
    \ results reconstructed by Heide et al.s algorithm with a single i-step. The golden\
    \ lines represent the results reconstructed by Lin et al.s algorithm. The red\
    \ lines represent the results reconstructed by Peters et al.s algorithm. Bottom:\
    \ The red lines represent the results reconstructed by Algorithm 2. The golden\
    \ lines represent the results reconstructed by Algorithm 3. Each column corresponds\
    \ to the same pixel."
  Figure 9 Link: articels_figures_by_rev_year\2020\Model_Study_of_Transient_Imaging_With_MultiFrequency_TimeofFlight_Sensors\figure_9.jpg
  Figure 9 caption: "Selected frames from \u201CB02corner\u201D. First row: Original\
    \ ground-truth frames. Second row: Reconstructions using Peters et al.s algorithm.\
    \ Third row: Reconstructions using Qiao et al.s algorithm. Fourth row: Reconstructions\
    \ using Algorithm 3. Each column corresponds to the same frame. Temporal profiles\
    \ of two colored pixels in the frame are shown in Fig. 10. Please refer to the\
    \ supplemental video, available online, for more results."
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.76
  Name of the first author: Hongman Wang
  Name of the last author: Qionghai Dai
  Number of Figures: 12
  Number of Tables: 4
  Number of authors: 6
  Paper title: Model Study of Transient Imaging With Multi-Frequency Time-of-Flight
    Sensors
  Publication Date: 2020-03-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Recovery Error of h h on Real Scenes and its Running Time
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Reconstruction Error and Running Time on Synthetic Nonsparse
      Scenes
  Table 3 caption:
    table_text: TABLE 3 Reconstruction Error and Running Time on Synthetic Sparse
      Scene
  Table 4 caption:
    table_text: TABLE 4 Reconstruction Error and Running Time on Real Scenes
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2981574
- Affiliation of the first author: miit key laboratory of pattern analysis and machine
    intelligence, college of computer science and technology, nanjing university of
    aeronautics and astronautics of(nuaa), nanjing, china
  Affiliation of the last author: miit key laboratory of pattern analysis and machine
    intelligence, college of computer science and technology, nanjing university of
    aeronautics and astronautics of(nuaa), nanjing, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Recent_Advances_in_Open_Set_Recognition_A_Survey\figure_1.jpg
  Figure 1 caption: An example of visualizing KKCs, KUCs, and UUCs from the real data
    distribution using t-SNE. Here, 1,3,4,5,9 are randomly selected from PENDIGITS
    as KKCs, while the remaining classes in it as UUCs. Z,I,J,Q,U are randomly selected
    from LETTER as KUCs. This visualization also indicates that the distribution of
    one class may consist of multiple subclasssubclusters, e.g., class 1, 5, U, etc.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Recent_Advances_in_Open_Set_Recognition_A_Survey\figure_2.jpg
  Figure 2 caption: "Comparison of between traditional classification and open set\
    \ recognition. Fig. 2a denotes the distribution of original dataset including\
    \ KKCs 1, 2, 3, 4, and UUCs ?5,?6, where KKCs appear during training and testing\
    \ while UUCs may appear or not during testing. Fig. 2b shows the decision boundary\
    \ of each class obtained by traditional classification methods, and it will obviously\
    \ misclassify when UUCs ?5,?6 appear during testing. Fig. 2c describes open set\
    \ recognition, where the decision boundaries limit the scope of KKCs 1, 2, 3,\
    \ 4, reserving space for UUCs ?5,?6. Via these decision boundaries, the samples\
    \ from some UUCs are labeled as \u201Cunknown\u201D or rejected rather than misclassified\
    \ as KKCs."
  Figure 3 Link: articels_figures_by_rev_year\2020\Recent_Advances_in_Open_Set_Recognition_A_Survey\figure_3.jpg
  Figure 3 caption: A global picture on how the existing OSR methods are linked. SR-based,
    Dis-based, MD-based respectively denote the Sparse Representation-based, Distance-based
    and Margin Distribution-based OSR methods, while +EVT represents the corresponding
    method additionally adopts the statistical Extreme Value Theory. Note that the
    pink dotted line module indicates that there is no related OSR work from the hybrid
    generative and discriminative model perspective at present, which seems also a
    promising research direction in the future.
  Figure 4 Link: articels_figures_by_rev_year\2020\Recent_Advances_in_Open_Set_Recognition_A_Survey\figure_4.jpg
  Figure 4 caption: Data Split. The dataset is first divided into training and testing
    set, then the training set is further divided into a fitting set and a validation
    set containing a closed set simulation and an open set simulation.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.77
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.64
  Name of the first author: Chuanxing Geng
  Name of the last author: Songcan Chen
  Number of Figures: 4
  Number of Tables: 5
  Number of authors: 3
  Paper title: 'Recent Advances in Open Set Recognition: A Survey'
  Publication Date: 2020-03-18 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Differences Between Open Set Recognition and its Related Tasks
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Different Categories of Models for Open Set Recognition
  Table 3 caption:
    table_text: TABLE 3 Available Software Packages
  Table 4 caption:
    table_text: TABLE 4 Comparison Among the Representative OSR Methods Using Non-Depth
      Features
  Table 5 caption:
    table_text: TABLE 5 Comparison Among the Representative OSR Methods Using Depth
      Features
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2981604
- Affiliation of the first author: department of computer engineering, middle east
    technical university (metu), ankara, turkey
  Affiliation of the last author: department of computer engineering, middle east
    technical university (metu), ankara, turkey
  Figure 1 Link: articels_figures_by_rev_year\2020\Imbalance_Problems_in_Object_Detection_A_Review\figure_1.jpg
  Figure 1 caption: (a) The common training pipeline of a generic detection network.
    The pipeline has 3 phases (i.e. feature extraction, detection and BB matching,
    labeling, and sampling) represented by different background colors. (b) Illustration
    of an example imbalance problem from each category for object detection through
    the training pipeline. Background colors specify at which phase an imbalance problem
    occurs.
  Figure 10 Link: articels_figures_by_rev_year\2020\Imbalance_Problems_in_Object_Detection_A_Review\figure_10.jpg
  Figure 10 caption: High-level diagrams of the methods designed for feature-level
    imbalance. (a) Path Aggregation Network. FPN is augmented by an additional bottom-up
    pathway to facilitate a shortcut of the low-level features to the final pyramidal
    feature maps. Red arrows represent the shortcuts. (b) Libra FPN. FPN pyramidal
    features are integrated and refined to learn a residual feature map. We illustrate
    the process originating from P2 feature map. Remaining feature maps ( P3 - P5
    ) follow the same operations. (c) Scale Transferrable Detection Network. Pyramidal
    features are learned via pooling, identity mapping and scale transfer layers depending
    on the layer size. (d) Parallel FPN. Feature maps with difference scales are followed
    by spatial pyramid pooling. These feature maps are fed into the multi-scale context
    aggregation (MSCA) module. We show the input and outputs of MSCA module for P3
    by red arrows. (e) Deep Feature Pyramid Reconfiguration. A set of residual features
    are learned via global attention and local reconfiguration modules. We illustrate
    the process originating from P2 feature map. Remaining feature maps ( P3 - P5
    ) follow the same operations. (f) Zoom Out-And-In Network. A zoom-in phase based
    on deconvolution (shown with red arrows) is adopted before stacking the layers
    of zoom-out and zoom-in phases. The weighting between them is determined by map
    attention decision module. (g) Multi-Level FPN. Backbone features from two different
    levels are fed into Thinned U-Shape Module (TUM) recursively to generate a sequence
    of pyramidal features, which are finally combined into one by acale-wise feature
    aggregation module. (h) NAS-FPN. The layers between backbone features and pyramidal
    features are learned via Neural Architecture Search.
  Figure 2 Link: articels_figures_by_rev_year\2020\Imbalance_Problems_in_Object_Detection_A_Review\figure_2.jpg
  Figure 2 caption: "Problem based categorization of the methods used for imbalance\
    \ problems. Note that a work may appear at multiple locations if it addresses\
    \ multiple imbalance problems \u2013 e.g. Libra R-CNN [29]"
  Figure 3 Link: articels_figures_by_rev_year\2020\Imbalance_Problems_in_Object_Detection_A_Review\figure_3.jpg
  Figure 3 caption: Number of papers per imbalance problem category through years.
  Figure 4 Link: articels_figures_by_rev_year\2020\Imbalance_Problems_in_Object_Detection_A_Review\figure_4.jpg
  Figure 4 caption: Illustration of the class imbalance problems. The numbers of RetinaNet
    [22] anchors on MS-COCO [54] are plotted for foreground-background (a), and foreground
    classes (b). The values are normalized with the total number of images in the
    dataset. The figures depict severe imbalance towards some classes.
  Figure 5 Link: articels_figures_by_rev_year\2020\Imbalance_Problems_in_Object_Detection_A_Review\figure_5.jpg
  Figure 5 caption: Some statistics of common datasets (training sets). For readability,
    the y axes are in logarithmic scale. (a) The total number of examples from each
    class. (b) The number of images vs. the number of examples. (c) The number of
    images vs. the number of classes. (This figure is inspired from Kuznetsova et
    al. [55] but calculated and drawn from scratch).
  Figure 6 Link: articels_figures_by_rev_year\2020\Imbalance_Problems_in_Object_Detection_A_Review\figure_6.jpg
  Figure 6 caption: Illustration of batch-level class imbalance. (a) An example that
    is consistent with the overall dataset (person class has more instances than parking
    meter). (b) An example that has a different distribution from the dataset. Images
    are from the MS COCO dataset.
  Figure 7 Link: articels_figures_by_rev_year\2020\Imbalance_Problems_in_Object_Detection_A_Review\figure_7.jpg
  Figure 7 caption: 'Imbalance in scales of the objects in common datasets: the distributions
    of BB width (a), height (b), and area (c). Values are with respect to the normalized
    image (i.e. relative to the image). For readability, the y axes are in log-scale.'
  Figure 8 Link: articels_figures_by_rev_year\2020\Imbalance_Problems_in_Object_Detection_A_Review\figure_8.jpg
  Figure 8 caption: "An illustration and comparison of the solutions for scale imbalance.\
    \ \u201CPredict\u201D refers to the prediction performed by a detection network.\
    \ The layered boxes correspond to convolutional layers. (a) No scale balancing\
    \ method is employed. (b) Prediction is performed from backbone features at different\
    \ levels (i.e. scales) (e.g. SSD [19]). (c) The intermediate features from different\
    \ scales are combined before making prediction at multiple scales (e.g. Feature\
    \ Pyramid Networks [26]). (d) The input image is scaled first and then processed.\
    \ Each I corresponds to an image pyramidal feature (e.g. Image Pyramids [27]).\
    \ (e) Image and feature pyramids are combined. Rather than applying backbones,\
    \ light networks are used in order to extract features from smaller images."
  Figure 9 Link: articels_figures_by_rev_year\2020\Imbalance_Problems_in_Object_Detection_A_Review\figure_9.jpg
  Figure 9 caption: Feature-level imbalance is illustrated on the FPN architecture.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Kemal Oksuz
  Name of the last author: Emre Akbas
  Number of Figures: 19
  Number of Tables: 3
  Number of authors: 4
  Paper title: 'Imbalance Problems in Object Detection: A Review'
  Publication Date: 2020-03-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Imbalance Problems Reviewed in This Paper
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Frequently Used Notations in the Paper
  Table 3 caption:
    table_text: TABLE 3 A Toy Example Depicting the Selection Methods of Common Hard
      and Soft Sampling Methods
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2981890
- Affiliation of the first author: south china university of technology, guangzhou,
    guangdong province, china
  Affiliation of the last author: salesforce research asia, salesforce.com, singapore
  Figure 1 Link: articels_figures_by_rev_year\2020\Deep_Learning_for_Image_SuperResolution_A_Survey\figure_1.jpg
  Figure 1 caption: Hierarchically-structured taxonomy of this survey.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Deep_Learning_for_Image_SuperResolution_A_Survey\figure_2.jpg
  Figure 2 caption: Super-resolution model frameworks based on deep learning. The
    cube size represents the output size. The gray ones denote predefined upsampling,
    while the green, yellow and blue ones indicate learnable upsampling, downsampling
    and convolutional layers, respectively. And the blocks enclosed by dashed boxes
    represent stackable modules.
  Figure 3 Link: articels_figures_by_rev_year\2020\Deep_Learning_for_Image_SuperResolution_A_Survey\figure_3.jpg
  Figure 3 caption: Interpolation-based upsampling methods. The gray board denotes
    the coordinates of pixels, and the blue, yellow and green points represent the
    initial, intermediate and output pixels, respectively.
  Figure 4 Link: articels_figures_by_rev_year\2020\Deep_Learning_for_Image_SuperResolution_A_Survey\figure_4.jpg
  Figure 4 caption: Transposed convolution layer. The blue boxes denote the input,
    and the green boxes indicate the kernel and the convolution output.
  Figure 5 Link: articels_figures_by_rev_year\2020\Deep_Learning_for_Image_SuperResolution_A_Survey\figure_5.jpg
  Figure 5 caption: Sub-pixel layer. The blue boxes denote the input, and the boxes
    with other colors indicate different convolution operations and different output
    feature maps.
  Figure 6 Link: articels_figures_by_rev_year\2020\Deep_Learning_for_Image_SuperResolution_A_Survey\figure_6.jpg
  Figure 6 caption: Meta upscale module. The blue boxes denote the projection patch,
    and the green boxes and lines indicate the convolution operation with predicted
    weights.
  Figure 7 Link: articels_figures_by_rev_year\2020\Deep_Learning_for_Image_SuperResolution_A_Survey\figure_7.jpg
  Figure 7 caption: Network design strategies.
  Figure 8 Link: articels_figures_by_rev_year\2020\Deep_Learning_for_Image_SuperResolution_A_Survey\figure_8.jpg
  Figure 8 caption: Super-resolution benchmarking. The x -axis and the y -axis denote
    the Multi-Adds and PSNR, respectively, and the circle size represents the number
    of parameters.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Zhihao Wang
  Name of the last author: Steven C. H. Hoi
  Number of Figures: 8
  Number of Tables: 2
  Number of authors: 3
  Paper title: 'Deep Learning for Image Super-Resolution: A Survey'
  Publication Date: 2020-03-23 00:00:00
  Table 1 caption:
    table_text: TABLE 1 List of Public Image Datasets for Super-Resolution Benchmarks
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Super-Resolution Methodology Employed by Some Representative
      Models
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2982166
- Affiliation of the first author: moe key lab of artificial intelligence, ai institute,
    john hopcroft center, shanghai jiao tong university, shanghai, china
  Affiliation of the last author: university of california, los angeles, los angeles,
    ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Interpretable_CNNs_for_Object_Classification\figure_1.jpg
  Figure 1 caption: Comparison of an interpretable filters feature maps with a filters
    feature maps in a traditional CNN.
  Figure 10 Link: articels_figures_by_rev_year\2020\Interpretable_CNNs_for_Object_Classification\figure_10.jpg
  Figure 10 caption: Visualization of neural activations (before the mask layer) of
    interpretable filters learned with and without the filter loss. We visualized
    neural activations of the first interpretable conv-layer before the mask layer
    in the CNN. In comparison, visualization results in Fig. 5 correspond to feature
    maps after the mask layer. Filters trained with the filter loss tended to generate
    more concentrated neural activations and have higher semantic purity that filters
    learned without the filter loss.
  Figure 2 Link: articels_figures_by_rev_year\2020\Interpretable_CNNs_for_Object_Classification\figure_2.jpg
  Figure 2 caption: Architectures of an ordinary conv-layer and an interpretable conv-layer.
    Solid and dashed lines indicate the forward and backward propagations, respectively.
    During the forward propagation, our CNN assigns each interpretable filter with
    a specific mask w.r.t. each input image during the learning process.
  Figure 3 Link: articels_figures_by_rev_year\2020\Interpretable_CNNs_for_Object_Classification\figure_3.jpg
  Figure 3 caption: "Templates of T \u03BC i . We show a toy example of n=3 . Each\
    \ template T \u03BC i matches to a feature map x when the target part mainly triggers\
    \ the i th unit in x . In fact, the algorithm also supports a round template based\
    \ on the L-2 norm distance. Here, we use the L-1 norm distance instead to speed\
    \ up the computation."
  Figure 4 Link: articels_figures_by_rev_year\2020\Interpretable_CNNs_for_Object_Classification\figure_4.jpg
  Figure 4 caption: "Given an input image I , from left to right, we consequently\
    \ show the feature map of a filter after the ReLU layer x , the assigned mask\
    \ T \u03BC , the masked feature map x masked , and the image-resolution RF of\
    \ activations in x masked computed by [50]."
  Figure 5 Link: articels_figures_by_rev_year\2020\Interpretable_CNNs_for_Object_Classification\figure_5.jpg
  Figure 5 caption: Visualization of filters in top conv-layers (top) and quantitative
    contribution of object parts to the prediction (bottom). (top) We used [50] to
    estimate the image-resolution receptive field of activations in a feature map
    to visualize a filters semantics. Each group of four feature maps for a category
    are computed using the same interpretable filter. These images show that each
    interpretable filter is consistently activated by the same object part through
    different images. Nine rows visualize filters in interpretable CNNs, and the last
    two rows correspond to filters in ordinary CNNs. (bottom) The clear disentanglement
    of object-part representations help people to quantify the contribution of different
    object parts to the network prediction. We show the explanation for part contribution,
    which was generated by the method of [3].
  Figure 6 Link: articels_figures_by_rev_year\2020\Interpretable_CNNs_for_Object_Classification\figure_6.jpg
  Figure 6 caption: Heatmaps for distributions of object parts that are encoded in
    interpretable filters. We use all filters in the top conv-layer to compute the
    heatmap. Interpretable filters usually selectively modeled distinct object parts
    of a category and ignored other parts.
  Figure 7 Link: articels_figures_by_rev_year\2020\Interpretable_CNNs_for_Object_Classification\figure_7.jpg
  Figure 7 caption: Grad-CAM visualizations [25] of the traditional conv-layer and
    the interpretable conv-layer. Unlike the traditional conv-layer, the interpretable
    conv-layer usually selectively modeled distinct object parts of a category and
    ignored other parts.
  Figure 8 Link: articels_figures_by_rev_year\2020\Interpretable_CNNs_for_Object_Classification\figure_8.jpg
  Figure 8 caption: Notation for computing the location instability.
  Figure 9 Link: articels_figures_by_rev_year\2020\Interpretable_CNNs_for_Object_Classification\figure_9.jpg
  Figure 9 caption: Examples that were incorrectly classified by the interpretable
    CNN.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.61
  Name of the first author: Quanshi Zhang
  Name of the last author: Song-Chun Zhu
  Number of Figures: 10
  Number of Tables: 15
  Number of authors: 5
  Paper title: Interpretable CNNs for Object Classification
  Publication Date: 2020-03-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Part Interpretability of Filters in CNNs for Binary Classification
      of a Single Category Based on the VOC Part Dataset [4]
  Table 10 caption:
    table_text: TABLE 10 Semantic Purity of Neural Activations of Interpretable Filters
      Learned With and Without the Filter Loss From the VOC Part Dataset
  Table 2 caption:
    table_text: TABLE 2 Part Interpretability of Filters in CNNs That are Trained
      for Multi-Category Classification Based on the VOC Part Dataset [4]
  Table 3 caption:
    table_text: TABLE 3 Location Instability of Filters ( E f,k [ D f,k ] Ef,k[Df,k])
      in CNNs That are Trained for the Binary Classification of a Single Category
      Using the ILSVRC 2013 DET Animal-Part Dataset [43]
  Table 4 caption:
    table_text: TABLE 4 Location Instability of Filters ( E f,k [ D f,k ] Ef,k[Df,k])
      in CNNs That are Trained for Binary Classification of a Single Category Using
      the VOC Part Dataset [4]
  Table 5 caption:
    table_text: TABLE 5 Location Instability of Filters ( E f,k [ D f,k ] Ef,k[Df,k])
      in CNNs for Binary Classification of a Single Category Using the CUB200-2011
      Dataset
  Table 6 caption:
    table_text: TABLE 6 Location Instability of Filters ( E f,k [ D f,k ] Ef,k[Df,k])
      in CNNs That are Trained for Multi-Category Classification
  Table 7 caption:
    table_text: TABLE 7 Classification Accuracy Based on Different Datasets
  Table 8 caption:
    table_text: TABLE 8 Classification Accuracy Based on a Single Filter
  Table 9 caption:
    table_text: TABLE 9 Statistics of Semantic Meanings of Interpretable Filters
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2982882
