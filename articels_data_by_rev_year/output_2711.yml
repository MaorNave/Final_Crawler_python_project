- Affiliation of the first author: tsinghua shenzhen international graduate school,
    tsinghua university, shenzhen, china
  Affiliation of the last author: university of california at merced, merced, ca,
    usa
  Figure 1 Link: articels_figures_by_rev_year\2022\GAN_Inversion_A_Survey\figure_1.jpg
  Figure 1 caption: "Illustration of GAN inversion. Different from the conventional\
    \ sampling and generation process using trained generator G , GAN inversion maps\
    \ a given real image x to the latent space and obtains the latent code z \u2217\
    \ . The reconstructed image x \u2217 is then obtained by x \u2217 =G( z \u2217\
    \ ) . By varying the latent code z \u2217 in different interpretable directions\
    \ e.g., z \u2217 + n 1 and z \u2217 + n 2 where n 1 and n 2 model the age and\
    \ smile in the latent space respectively, we can edit the corresponding attribute\
    \ of the real image. The reconstructed results are from [19]."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\GAN_Inversion_A_Survey\figure_2.jpg
  Figure 2 caption: (a) Architecture of the style-based generator. (b) The latent
    spaces from which the inversion methods are constructed. The synthesis network
    g and AdaIN in (b) are the same as in (a).
  Figure 3 Link: articels_figures_by_rev_year\2022\GAN_Inversion_A_Survey\figure_3.jpg
  Figure 3 caption: "Illustration of GAN inversion methods. (a) Given a well-trained\
    \ GAN model G , photo-realistic images x gen can be generated from randomly sampled\
    \ latent vectors z . GAN inversion aims to obtain the latent code z \u2217 for\
    \ a given image x real . A learning-based inversion method aims to learn an encoder\
    \ network to map an image into the latent space such that the reconstructed image\
    \ based on the latent code look as similar to the original one as possible. An\
    \ optimization-based inversion approach directly solves the objective function\
    \ through back-propagation to find a latent code that minimizes pixel-wise reconstruction\
    \ loss. A hybrid approach first uses an encoder to generate initial latent code\
    \ and then refines it with an optimization algorithm. Depicted by the dotted E\
    \ , the well-trained encoder is included in [19] as a regularizer for optimization.\
    \ Blue blocks represent trainable or iterative modules, and red dashed arrows\
    \ indicate the supervisions."
  Figure 4 Link: articels_figures_by_rev_year\2022\GAN_Inversion_A_Survey\figure_4.jpg
  Figure 4 caption: 'Encoder structure of three learning-based methods: (a) IDinvert
    [19], (b) pSp [28], and (c) Wei et al. [29].'
  Figure 5 Link: articels_figures_by_rev_year\2022\GAN_Inversion_A_Survey\figure_5.jpg
  Figure 5 caption: Illustration of discovering disentangled directions for multiple
    attributes. The projection of mathbf n1 onto mathbf n2 is subtracted from mathbf
    n1, resulting in a new direction mathbf n1-(mathbf n1top mathbf n2) mathbf n2
    . This figure is from [18].
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Weihao Xia
  Name of the last author: Ming-Hsuan Yang
  Number of Figures: 5
  Number of Tables: 1
  Number of authors: 6
  Paper title: 'GAN Inversion: A Survey'
  Publication Date: 2022-06-09 00:00:00
  Table 1 caption: TABLE 1 Properties of GAN Inversion Methods
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3181070
- Affiliation of the first author: jd ai research, beijing, china
  Affiliation of the last author: jd ai research, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\A_Low_Rank_Promoting_Prior_for_Unsupervised_Contrastive_Learning\figure_1.jpg
  Figure 1 caption: 'Probabilistic graphical model on query-key pairs respectively
    for: (a) positive pair dependency based on conventional contrastive learning,
    (b) positive pair dependency based on LORAC.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\A_Low_Rank_Promoting_Prior_for_Unsupervised_Contrastive_Learning\figure_2.jpg
  Figure 2 caption: Conceptual framework of LORAC training procedure.
  Figure 3 Link: articels_figures_by_rev_year\2022\A_Low_Rank_Promoting_Prior_for_Unsupervised_Contrastive_Learning\figure_3.jpg
  Figure 3 caption: "Impact of hyperparameter \u03B2 : (a) Test accuracy versus \u03B2\
    \ , (b) Nuclear norm \u2225 Q \u02C6 \u2225 \u2217 versus \u03B2 , (c) Test accuracy\
    \ versus nuclear norm \u2225 Q \u02C6 \u2225 \u2217 ."
  Figure 4 Link: articels_figures_by_rev_year\2022\A_Low_Rank_Promoting_Prior_for_Unsupervised_Contrastive_Learning\figure_4.jpg
  Figure 4 caption: "Ablation study. (a) Convergence analysis, (b) Number of views\
    \ M versus. top-1 classification accuracy, (c) Histogram of the nuclear norm Q\
    \ \u02C6 on ImageNet100 test data."
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.99
  Name of the first author: Yu Wang
  Name of the last author: Tao Mei
  Number of Figures: 4
  Number of Tables: 6
  Number of authors: 7
  Paper title: A Low Rank Promoting Prior for Unsupervised Contrastive Learning
  Publication Date: 2022-06-09 00:00:00
  Table 1 caption: TABLE 1 PublishedReimplemented Accuracy of Linear Classification
    on Features Pre-Trained on ImageNet1K
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Linear Classification on Other Tasks
  Table 3 caption: 'TABLE 3 Performance on Downstream Tasks: Object Detection [71]
    (Left), Instance Segmentation [72] (Middle) and Keypoint Detection [72] (Right)'
  Table 4 caption: TABLE 4 Semi-Supervised Learning Evaluation as Defined in [53]
  Table 5 caption: TABLE 5 Performance Comparison Under Various h(Q) h(Q) Hypothesis
    Pre-Trained for 100 Epochs on ImageNet100 Dataset
  Table 6 caption: TABLE 6 Performance Comparison Under Different Self-Supervised
    Learning Hypothesis Pre-Trained for 100 Epochs on ImageNet100 Dataset
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3180995
- Affiliation of the first author: pca lab, college of computer science, nankai university,
    tianjin, china
  Affiliation of the last author: pca lab, college of computer science, nankai university,
    tianjin, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Generalized_Focal_Loss_Towards_Efficient_Representation_Learning_for_Dense_Objec\figure_1.jpg
  Figure 1 caption: 'Comparisons between existing separate representation and proposed
    joint representation of classification and localization quality estimation. (a):
    Current practices [20], [42], [47], [50], [55] for the separate usage of the quality
    branch (i.e., IoU or Centerness score) during training and test. (b): Our joint
    representation of classification and localization quality enables high consistency
    between training and inference.'
  Figure 10 Link: articels_figures_by_rev_year\2022\Generalized_Focal_Loss_Towards_Efficient_Representation_Learning_for_Dense_Objec\figure_10.jpg
  Figure 10 caption: The histogram of bounding box regression targets of ATSS. It
    is calculated over all training samples on COCO trainval35k.
  Figure 2 Link: articels_figures_by_rev_year\2022\Generalized_Focal_Loss_Towards_Efficient_Representation_Learning_for_Dense_Objec\figure_2.jpg
  Figure 2 caption: "Unreliable IoU predictions of current dense detector with IoU-branch.\
    \ (a): We demonstrate the (hard) background patch with extremely high predicted\
    \ quality scores (e.g., IoU score \u2265 0.85), based on the optimized IoU-branch\
    \ model in Fig. 1a. The scatter diagram in (b) denotes the randomly sampled instances\
    \ with their predicted scores, where the blue points clearly illustrate the weak\
    \ correlation between predicted classification scores and predicted IoU scores\
    \ for separate representations. The part in red circle contains many possible\
    \ False Positives (FP) with large localization quality predictions, which may\
    \ potentially rank in front of True Positives (TP) and impair the performance\
    \ (see the detection cases in (a)). Instead, our joint representation (green points)\
    \ forces them to be equal and thus alleviates such risks."
  Figure 3 Link: articels_figures_by_rev_year\2022\Generalized_Focal_Loss_Towards_Efficient_Representation_Learning_for_Dense_Objec\figure_3.jpg
  Figure 3 caption: 'Motivation of utilizing the highly relevant statistics of learned
    bounding box distributions to guide the better generation of its estimated localization
    quality. (a): The illustration of General Distribution to represent bounding boxes,
    which models the probability distribution of the predicted edges in a form of
    discrete probability vector. (b): The scatter diagram of the relation between
    Top-1 (mean of four sides) value of General Distribution of predicted boxes and
    their real localization quality (IoU between the prediction and ground-truth),
    calculated over all validation images on COCO [29] dataset. (c) and (d): Two specific
    examples from (b), where the sharp distribution usually corresponds to higher
    quality, whilst the flat one stands for lower quality usually. Green: predicted
    bounding boxes; White: ground-truth bounding boxes.'
  Figure 4 Link: articels_figures_by_rev_year\2022\Generalized_Focal_Loss_Towards_Efficient_Representation_Learning_for_Dense_Objec\figure_4.jpg
  Figure 4 caption: Ambiguity in bounding boxes. Due to occlusion, shadow, blur, etc.,
    the boundaries of many objects are not clear enough, so that the ground-truth
    labels (white boxes) are sometimes not credible and Dirac delta Distribution is
    limited to indicate such issues. Instead, the proposed learned representation
    of General Distribution for bounding boxes can reflect the underlying information
    by its shape, where a flatten distribution denotes the unclear and ambiguous boundaries
    (see red circles) and a sharp one stands for the clear cases. The predicted boxes
    by our model are marked green.
  Figure 5 Link: articels_figures_by_rev_year\2022\Generalized_Focal_Loss_Towards_Efficient_Representation_Learning_for_Dense_Objec\figure_5.jpg
  Figure 5 caption: 'Comparisons of guidance for predicting localization quality between
    existing works (left) and ours (right). Existing works focus on different spatial
    locations of convolutional features, including (a): point [21], [26], [35], [36],
    [37], [42], [47], [50], (b): region [20], (c): border [34] dense points, (d):
    border [34] middle points, (e): border [34] extreme points, (f): regular sampling
    points [49], and (g): deformable sampling points [6], [9]. In contrast, we use
    the statistic of learned box distribution to produce reliable localization quality.'
  Figure 6 Link: articels_figures_by_rev_year\2022\Generalized_Focal_Loss_Towards_Efficient_Representation_Learning_for_Dense_Objec\figure_6.jpg
  Figure 6 caption: The comparisons between conventional methods and our proposed
    GFocal on detection head. We propose improved and efficient representations including
    Classification-IoU Joint Representation, General Distribution and Distribution-Guided
    Quality Predictor for dense object detectors, which are considerably lightweight
    and cost-free in practice.
  Figure 7 Link: articels_figures_by_rev_year\2022\Generalized_Focal_Loss_Towards_Efficient_Representation_Learning_for_Dense_Objec\figure_7.jpg
  Figure 7 caption: The illustration of QFL under quality label y=0.5 . The gradients
    of values near ground-truth label tend to be small.
  Figure 8 Link: articels_figures_by_rev_year\2022\Generalized_Focal_Loss_Towards_Efficient_Representation_Learning_for_Dense_Objec\figure_8.jpg
  Figure 8 caption: textTopkm(cdot) feature. It is robust to object scales.
  Figure 9 Link: articels_figures_by_rev_year\2022\Generalized_Focal_Loss_Towards_Efficient_Representation_Learning_for_Dense_Objec\figure_9.jpg
  Figure 9 caption: Illustrations of modified versions for separateimplicit and joint
    representation. The baseline (i.e., the leftmost figure) is built upon FCOS [42]ATSS
    [50] detector without their quality prediction branch applied.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Xiang Li
  Name of the last author: Jian Yang
  Number of Figures: 16
  Number of Tables: 15
  Number of authors: 6
  Paper title: 'Generalized Focal Loss: Towards Efficient Representation Learning
    for Dense Object Detection'
  Publication Date: 2022-06-09 00:00:00
  Table 1 caption: TABLE 1 Comparisons Between Three Distributions
  Table 10 caption: TABLE 10 Pearson Correlation Coefficients (PCC) for Representative
    Dense Object Detectors
  Table 2 caption: TABLE 2 Comparisons to Additional Quality Branch and Quality Weighting
    Methods
  Table 3 caption: TABLE 3 The Effect of Joint Representations
  Table 4 caption: TABLE 4 Comparisons Between Different Loss Objectives to Optimize
    the Joint Representation
  Table 5 caption: "TABLE 5 Varying \u03B2 \u03B2 for QFL Based on ATSS: \u03B2 \u03B2\
    \ = 2 Performs Best"
  Table 6 caption: 'TABLE 6 Performances Under Different Data Representation of Bounding
    Box Based on FCOS: The Proposed General Distribution Supervised by DFL Improves
    Favorably Over The Competitive Baselines'
  Table 7 caption: "TABLE 7 Performances of Various y n yn and \u0394 \u0394 Based\
    \ on ATSS With GFocal"
  Table 8 caption: TABLE 8 Performances of Various k,d k,d in DGQP Based on ATSS With
    GFocal
  Table 9 caption: TABLE 9 Comparisons Among Different Input Guidance by Fixing the
    Hidden Layer Dimension of DGQP
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3180392
- Affiliation of the first author: faculty of engineering and information technology,
    australian artificial intelligence institute, university of technology sydney,
    ultimo, nsw, australia
  Affiliation of the last author: school of computer science, carnegie mellon university,
    pittsburgh, pa, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\Video_Pivoting_Unsupervised_MultiModal_Machine_Translation\figure_1.jpg
  Figure 1 caption: "People who speak different languages share the same physical\
    \ visual perception. The action \u201Cpetting a cat\u201D is looked similarly\
    \ by an American and a French person."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Video_Pivoting_Unsupervised_MultiModal_Machine_Translation\figure_2.jpg
  Figure 2 caption: 'Pseudo-visual pivoting: (1) multilingual VSE (src-vid-tgt, in
    fact src-vid1, tgt-vid2), and (2) pivoted captioning (src-vid-tgt). The items
    in italic do not exist and are approximated (pseudo). (src, vid, tgt) is colored
    in (green, yellow, blue). Solid red and black lines indicate captioning and translation
    without updates. The encoder and decoder are updated with dashed lines to improve
    the alignments in the multilingual multi-modal embedding space.'
  Figure 3 Link: articels_figures_by_rev_year\2022\Video_Pivoting_Unsupervised_MultiModal_Machine_Translation\figure_3.jpg
  Figure 3 caption: The proposed model structure (English leftrightarrow Chinese).
    We incorporate visual content for unsupervised multi-modal machine translation
    and improve the language latent space alignment with pseudo-visual pivoting. There
    are a total of four objectives during the training procedure.
  Figure 4 Link: articels_figures_by_rev_year\2022\Video_Pivoting_Unsupervised_MultiModal_Machine_Translation\figure_4.jpg
  Figure 4 caption: Detailed structures of the textual encoder and decoder, video
    encoder, and video captioner, where bigoplus denotes the controllable attention.
  Figure 5 Link: articels_figures_by_rev_year\2022\Video_Pivoting_Unsupervised_MultiModal_Machine_Translation\figure_5.jpg
  Figure 5 caption: Overview of visual content extracting. During training, the object
    branch captures the space-time object interaction information via the spatial-temporal
    graph model, while the scene branch provides the global context absent from the
    object branch.
  Figure 6 Link: articels_figures_by_rev_year\2022\Video_Pivoting_Unsupervised_MultiModal_Machine_Translation\figure_6.jpg
  Figure 6 caption: Sample results of several baseline models and our full model.
    BASE refers to model training with text only. We highlight the key words in the
    source, target and our generated sentences in different colors.
  Figure 7 Link: articels_figures_by_rev_year\2022\Video_Pivoting_Unsupervised_MultiModal_Machine_Translation\figure_7.jpg
  Figure 7 caption: Illustrations of retrieval results. The blue and red sentences
    represent the query and ground truth response, respectively. We also report the
    multilingual sentences similarity value between the query and response sentence.
  Figure 8 Link: articels_figures_by_rev_year\2022\Video_Pivoting_Unsupervised_MultiModal_Machine_Translation\figure_8.jpg
  Figure 8 caption: Multilingual sentence representation visualization of t-SNE. Scatters
    in the same color represent a pair.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.86
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: Mingjie Li
  Name of the last author: Alex Hauptmann
  Number of Figures: 8
  Number of Tables: 10
  Number of authors: 6
  Paper title: Video Pivoting Unsupervised Multi-Modal Machine Translation
  Publication Date: 2022-06-09 00:00:00
  Table 1 caption: TABLE 1 The BLEU Scores Between Captioning and Translation Subsets
    in English and Chinese
  Table 10 caption: TABLE 10 Text-To-Video Retrieval on VATEX
  Table 2 caption: TABLE 2 The Sentence-Level Translation Performance Compared With
    Existing UMMT and UMT Models on the VATEX
  Table 3 caption: TABLE 3 The Word-Level Translation Comparison With Existing Text-
    and Text-And-Image-Based Models on the HowToWorld Dictionary Dataset
  Table 4 caption: TABLE 4 POS-Level Performances on the HowToWorld Dataset
  Table 5 caption: TABLE 5 Ablation Studies
  Table 6 caption: TABLE 6 Testing BLEU and CIDER of the Full T+V Model and the Text-Only
    Model Trained With Overlapped Videos or Low Resource Unpaired Corpora
  Table 7 caption: TABLE 7 Captioning Performance on the VATEX Dataset
  Table 8 caption: TABLE 8 Western Languages Translation Performance on the Multi30k
    Testing Set Comparing With Their Benchmarks
  Table 9 caption: TABLE 9 Multilingual Text-to-Text Retrieval on VATEX
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3181116
- Affiliation of the first author: college of engineering and computer science, australian
    national university, canberra, australia
  Affiliation of the last author: department of computing, imperial college london,
    london, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2022\EDFaceCeleb_M_Benchmarking_Face_Hallucination_With_a_MillionScale_Dataset\figure_1.jpg
  Figure 1 caption: The facial pose statistics of the proposed EDFace-Celeb-1 M dataset.
  Figure 10 Link: articels_figures_by_rev_year\2022\EDFaceCeleb_M_Benchmarking_Face_Hallucination_With_a_MillionScale_Dataset\figure_10.jpg
  Figure 10 caption: "Visual results of BD models ( times 4 ) on the EDFace-Celeb-1\
    \ M dataset. From left to right: HR, results of bicubic, DICNet, DICGAN, WaveletSR,\
    \ HiFaceGAN, EDSR, RDN, RCAN, and HAN. \u201CBD\u201D means that the blur artifact\
    \ is applied prior to the downsampling operation."
  Figure 2 Link: articels_figures_by_rev_year\2022\EDFaceCeleb_M_Benchmarking_Face_Hallucination_With_a_MillionScale_Dataset\figure_2.jpg
  Figure 2 caption: Representative face images from the proposed dataset. These face
    images exhibit relatively balanced race distribution., evident pose and appearance
    variations.
  Figure 3 Link: articels_figures_by_rev_year\2022\EDFaceCeleb_M_Benchmarking_Face_Hallucination_With_a_MillionScale_Dataset\figure_3.jpg
  Figure 3 caption: The age statistics of the proposed EDFace-Celeb-1 M dataset.
  Figure 4 Link: articels_figures_by_rev_year\2022\EDFaceCeleb_M_Benchmarking_Face_Hallucination_With_a_MillionScale_Dataset\figure_4.jpg
  Figure 4 caption: The resolution of face images before resizing.
  Figure 5 Link: articels_figures_by_rev_year\2022\EDFaceCeleb_M_Benchmarking_Face_Hallucination_With_a_MillionScale_Dataset\figure_5.jpg
  Figure 5 caption: 'Face hallucination results on the real-world images. From left
    to right: results of bicubic, DICNet, DICGAN, WaveletSR, HiFaceGAN, EDSR, RDN,
    RCAN, and HAN.'
  Figure 6 Link: articels_figures_by_rev_year\2022\EDFaceCeleb_M_Benchmarking_Face_Hallucination_With_a_MillionScale_Dataset\figure_6.jpg
  Figure 6 caption: An increase in similarity across different people after the hallucination.
  Figure 7 Link: articels_figures_by_rev_year\2022\EDFaceCeleb_M_Benchmarking_Face_Hallucination_With_a_MillionScale_Dataset\figure_7.jpg
  Figure 7 caption: Face hallucination results trained on different numbers of training
    samples. From left to right, the columns show results of the DICNet trained using
    text0.3~M , text0.7~M and text1.35~M training samples from the EDFace-Celeb-1
    M dataset.
  Figure 8 Link: articels_figures_by_rev_year\2022\EDFaceCeleb_M_Benchmarking_Face_Hallucination_With_a_MillionScale_Dataset\figure_8.jpg
  Figure 8 caption: "Visual results of BI models ( times 8 ) on the EDFace-Celeb-1\
    \ M dataset. From left to right: HR, results of bicubic, DICNet, DICGAN, WaveletSR,\
    \ HiFaceGAN, EDSR, RDN, RCAN, and HAN. \u201CBI\u201D means the bicubic interpolation."
  Figure 9 Link: articels_figures_by_rev_year\2022\EDFaceCeleb_M_Benchmarking_Face_Hallucination_With_a_MillionScale_Dataset\figure_9.jpg
  Figure 9 caption: "Visual results of BI models ( times 4 ) on the EDFace-Celeb-1\
    \ M dataset. From left to right: HR, results of bicubic, DICNet, DICGAN, WaveletSR,\
    \ HiFaceGAN, EDSR, RDN, RCAN, and HAN. \u201CBI\u201D means the bicubic interpolation."
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Kaihao Zhang
  Name of the last author: Stefanos Zafeiriou
  Number of Figures: 11
  Number of Tables: 4
  Number of authors: 7
  Paper title: 'EDFace-Celeb-1 M: Benchmarking Face Hallucination With a Million-Scale
    Dataset'
  Publication Date: 2022-06-10 00:00:00
  Table 1 caption: TABLE 1 Representative Face Datasets. Most of the Current Public
    Face Datasets do not Consider the Race Problem
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Performance Comparison of Representative Methods for Face
    Hallucination on the EDFace-Celeb-1 M Dataset. Results are Reported in Terms of
    Both PSNR and SSIM
  Table 3 caption: TABLE 3 Performance Comparison of Representative Methods for Face
    Hallucination on the EDFace-Celeb-1 M Dataset
  Table 4 caption: TABLE 4 Performance Comparison of DICNet [33] Trained Using Different
    Numbers of Training Samples From the EDFace-Celeb-1 M Dataset
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3181579
- Affiliation of the first author: school of computer science and engineering, nanyang
    technological university, singapore
  Affiliation of the last author: school of computer science and engineering, nanyang
    technological university, singapore
  Figure 1 Link: articels_figures_by_rev_year\2022\Learning_Structural_Representations_for_Recipe_Generation_and_Food_Retrieval\figure_1.jpg
  Figure 1 caption: The demonstration of our proposed frameworks. In the recipe generation
    model (a), we use the img2tree module to infer the recipe tree structures, which
    are encoded with the graph attention networks to give tree embeddings in the tree2recipe
    module. Then we generate the recipes. In the food cross-modal retrieval framework
    (b), we incorporate the tree structures from the recipe2tree module into the original
    recipe features, which boosts food cross-modal matching performance.
  Figure 10 Link: articels_figures_by_rev_year\2022\Learning_Structural_Representations_for_Recipe_Generation_and_Food_Retrieval\figure_10.jpg
  Figure 10 caption: The visualization of image-to-recipe retrieval results. The top
    row indicates the image query, the bottom row shows the corresponding retrieved
    recipes, which are correctly matched with the ground truth.
  Figure 2 Link: articels_figures_by_rev_year\2022\Learning_Structural_Representations_for_Recipe_Generation_and_Food_Retrieval\figure_2.jpg
  Figure 2 caption: The concise training flow of our proposed models for recipe generation
    and food cross-modal retrieval, which are trained individually. In the shared
    components, we extract the food image features and use the recipe2tree module
    to produce pseudo recipe tree structures. In the recipe generation model, we generate
    trees with the img2tree module that is trained with Ltree . The predicted tree
    structures from the img2tree module are used to generate the recipes in the tree2recipe
    module, which is supervised by Lgen . In the cross-modal retrieval model, we concatenate
    the tree features and recipe features. The concatenated features are trained to
    match with the image features, which is supervised by Ltri .
  Figure 3 Link: articels_figures_by_rev_year\2022\Learning_Structural_Representations_for_Recipe_Generation_and_Food_Retrieval\figure_3.jpg
  Figure 3 caption: Our proposed framework for recipe generation. The ingredients
    and food images are embedded by a pretrained language model and CNN respectively
    to produce the output features mathrmFing and mathrmFimg . Before language generation,
    we first infer the tree structure of target cooking instructions. To do so, we
    utilize the img2tree module, where a RNN produces the nodes and edge links step-by-step
    based on mathrmFimg . Then in tree2recipe module, we adopt graph attention networks
    (GAT) to encode the generated tree adjacency matrix, and get tree embedding mathrmFtree
    . We combine mathrmFing , mathrmFimg and mathrmFtree to construct a final embedding
    for recipe generation, which is performed using a transformer.
  Figure 4 Link: articels_figures_by_rev_year\2022\Learning_Structural_Representations_for_Recipe_Generation_and_Food_Retrieval\figure_4.jpg
  Figure 4 caption: The demonstration of the transformer training for the recipe generation.
    The concatenated features are composed of the image features mathrmFimg , ingredient
    features mathrmFing and tree structure representations mathrmFtree . In the training
    phase, we take the ground truth recipes x as the input. The predicted token and
    the output probability are denoted as hatx and p(hatx=x) respectively. We set
    the transformer layer number N=16 .
  Figure 5 Link: articels_figures_by_rev_year\2022\Learning_Structural_Representations_for_Recipe_Generation_and_Food_Retrieval\figure_5.jpg
  Figure 5 caption: The training flow of food cross-modal retrieval. We first produce
    the sentence-level tree structures from the cooking instructions, where we use
    the sentence features as the node features. The tree structure, cooking instruction
    and ingredient features are denoted as mathrmFtree , mathrmFins and mathrmFing
    respectively. We use the concatenation of mathrmFtree , mathrmFins and mathrmFing
    as the recipe features. The triplet loss is adopted to learn the similarity between
    the recipe features mathrmFrec and image features mathrmFimg .
  Figure 6 Link: articels_figures_by_rev_year\2022\Learning_Structural_Representations_for_Recipe_Generation_and_Food_Retrieval\figure_6.jpg
  Figure 6 caption: The visualization of predicted sentence-level trees for recipes.
    The latent tree structure is obtained from unsupervised learning. The results
    indicate that we can get reasonable parsing tree structures with varying recipe
    length.
  Figure 7 Link: articels_figures_by_rev_year\2022\Learning_Structural_Representations_for_Recipe_Generation_and_Food_Retrieval\figure_7.jpg
  Figure 7 caption: Visualization of recipes from different sources. We show the food
    images and the corresponding recipes, obtained from users and different types
    of models. Words in red indicate the matching parts between recipes uploaded by
    users and that generated by models. Words in yellow background show the redundant
    generated sentences.
  Figure 8 Link: articels_figures_by_rev_year\2022\Learning_Structural_Representations_for_Recipe_Generation_and_Food_Retrieval\figure_8.jpg
  Figure 8 caption: The comparison between the pseudo ground truth trees (produced
    by recipe2tree module) and img2tree generated tree structures.
  Figure 9 Link: articels_figures_by_rev_year\2022\Learning_Structural_Representations_for_Recipe_Generation_and_Food_Retrieval\figure_9.jpg
  Figure 9 caption: The visualization of recipe-to-image retrieval results. For each
    recipe query, we show the top-3 retrieved food images, where the yellow ticks
    suggest the correctly retrieved samples.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.61
  Name of the first author: Hao Wang
  Name of the last author: Chunyan Miao
  Number of Figures: 10
  Number of Tables: 6
  Number of authors: 4
  Paper title: Learning Structural Representations for Recipe Generation and Food
    Retrieval
  Publication Date: 2022-06-10 00:00:00
  Table 1 caption: TABLE 1 Food Cross-Modal Retrieval Evaluation
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Recipe Generation Evaluation
  Table 3 caption: TABLE 3 Generated Recipe Average Length for Recipe Generation
  Table 4 caption: TABLE 4 Ablation Studies on the Composition of Tree Features for
    Cross-Modal Retrieval
  Table 5 caption: TABLE 5 Ablation Studies of Different Recipe Encoders for Cross-Modal
    Retrieval, Where the LSTM and Transformer are Used Respectively as the Backbone
    to Encode the Cooking Recipes
  Table 6 caption: TABLE 6 Ablation Studies of Incorporating the img2tree Module Into
    the Cross-Modal Retrieval Model
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3181294
- Affiliation of the first author: key laboratory of machine perception (moe), school
    of artificial intelligence, peking university, beijing, china
  Affiliation of the last author: key laboratory of machine perception (moe), school
    of artificial intelligence, peking university, beijing, china
  Figure 1 Link: Not Available
  Figure 1 caption: Not Available
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.79
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.64
  Name of the first author: Xingyu Xie
  Name of the last author: Zhouchen Lin
  Number of Figures: 0
  Number of Tables: 10
  Number of authors: 6
  Paper title: 'Optimization Induced Equilibrium Networks: An Explicit Optimization
    Perspective for Understanding Equilibrium Models'
  Publication Date: 2022-06-10 00:00:00
  Table 1 caption: TABLE 1 Examples to Modify the Underlying Optimization Problem
  Table 10 caption: TABLE 10 Evaluation on the Validation Set of Cityscapes Semantic
    Segmentation
  Table 2 caption: TABLE 2 Some Choices for R z Rz and the Corresponding T R z TRz
  Table 3 caption: TABLE 3 (a) The Testing Accuracy (Acc.) of Deep OptEq With Different
    Settings
  Table 4 caption: TABLE 4 Comparisons With Previous Implicit Models
  Table 5 caption: TABLE 5 The Testing Accuracy of Deep OptEq With Different Settings
  Table 6 caption: TABLE 6 The Detailed Archit. of OptEqs
  Table 7 caption: TABLE 7 Evaluation Results on ImageNet Classification With Top-1
    and Top-5 Accuracies
  Table 8 caption: TABLE 8 Comparison Between Unrolling and IFT Based Training ( Params
    199 k)
  Table 9 caption: TABLE 9 Comparison Between FPS on ImageNet
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3181425
- Affiliation of the first author: school of computer science and technology, east
    china normal university, shanghai, china
  Affiliation of the last author: department of computer science, city university
    of hong kong, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2022\Mirror_Detection_With_the_Visual_Chirality_Cue\figure_1.jpg
  Figure 1 caption: Existing single image based mirror detection methods [42] [21],
    which are based on modeling contrastscorrespondences between mirror and non-mirror
    regions, may fail when these relations are not reliable. For example, MirrorNet
    [42] would fail if the contrasts between mirrornon-mirror regions are weak (top
    row) or have multiple degrees (bottom row). PMDNet [21] would fail if correspondences
    do not exist (top row) or are incorrectly detected (bottom row). Our method (Ours)
    leverages the visual chirality cue, which is an intrinsic property of mirrors
    reflecting real-world scenes, to accurately differentiate mirror and non-mirror
    regions.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Mirror_Detection_With_the_Visual_Chirality_Cue\figure_2.jpg
  Figure 2 caption: The pipeline of VCNet. Given an input image, it first extracts
    multi-scale backbone features, and then transforms these features into multi-level
    visual chirality features via the proposed (VCE) modules. These features guide
    the backbone features to produce discriminative mirror features, which are then
    used by the decoders to detect mirrors at different scales. In addition, we also
    use the visual chirality features at different levels, i.e., F 1 m , F 1 c and
    F 4 mc , to learn mirror boundary information via the proposed CED module. Finally,
    we fuse the predicted mirror maps at different scales, the predicted mirror boundary
    map, and the input image using a fusion layer to obtain the final mirror map.
  Figure 3 Link: articels_figures_by_rev_year\2022\Mirror_Detection_With_the_Visual_Chirality_Cue\figure_3.jpg
  Figure 3 caption: The proposed visual chirality embedding (VCE) module. It aims
    to transform the input backbone features F m into visual chirality features F
    vce via a dilated feature extractor, followed by a visual chirality feature extractor.
    The proposed FCF transformation is embedded in the visual chirality feature extractor
    to extract the visual chirality cue.
  Figure 4 Link: articels_figures_by_rev_year\2022\Mirror_Detection_With_the_Visual_Chirality_Cue\figure_4.jpg
  Figure 4 caption: The proposed visual chirality-guided edge detection (CED) module.
    It uses the extracted visual chirality based features (i.e., F 1 c and F 4 mc
    ) to guide the detection of mirror edge features (i.e., F ced ) from the mirror
    features (i.e., F 1 m ).
  Figure 5 Link: articels_figures_by_rev_year\2022\Mirror_Detection_With_the_Visual_Chirality_Cue\figure_5.jpg
  Figure 5 caption: Visual comparison between the proposed VCNet and 11 state-of-the-art
    methods on the MSD and PMD datasets. From left to right are the input images,
    results from DSC [15] and BDRAR [51] for shadow detection, CCNet [16] for semantic
    segmentation, R 3 Net [9], CPDNet [40], BASNet [32], PoolNet [23], EGNet [45]
    and MINet [29] for salient object detection, MirrorNet [42] and PMDNet [21] for
    mirror detection, the proposed VCNet and the ground truth. Our VCNet can detect
    the mirror regions accurately.
  Figure 6 Link: articels_figures_by_rev_year\2022\Mirror_Detection_With_the_Visual_Chirality_Cue\figure_6.jpg
  Figure 6 caption: Visual comparison between the proposed VCNet and PMDNet [21].
    Although PMDNet can correctly detect the correspondences between inside and outside
    of the mirrors in the top two images, it wrongly identifies the outside regions
    as mirrors. In contrast, VCNet uses the visual chirality features to correctly
    identify the mirror regions in both cases. For the bottom image, PMDNet fails
    to detect the whole mirror region, due to the lack of correspondences, while VCNet
    can accurately locate the mirror region.
  Figure 7 Link: articels_figures_by_rev_year\2022\Mirror_Detection_With_the_Visual_Chirality_Cue\figure_7.jpg
  Figure 7 caption: Visual comparison between our model and PDNet [25] on RGBD-Mirror
    dataset. Note that our model does not use the depth information while PDNet uses
    it.
  Figure 8 Link: articels_figures_by_rev_year\2022\Mirror_Detection_With_the_Visual_Chirality_Cue\figure_8.jpg
  Figure 8 caption: Visual comparison of different ablated models on two challenging
    images. (b) is the baseline model, without the VCE and CED modules. (c) and (d)
    add the VCE module to the baseline model, but without and with the FCF Transformation,
    respectively. (e) is our full model, while (f) is the ground truth.
  Figure 9 Link: articels_figures_by_rev_year\2022\Mirror_Detection_With_the_Visual_Chirality_Cue\figure_9.jpg
  Figure 9 caption: Failure cases. From left to right are the input images, ground
    truths and our results. Our method may fail to accurately delineate the mirror
    boundaries if the mirror is partially occluded by a complex object (1st row) or
    by a very small object (2nd row).
  First author gender probability: 0.68
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.76
  Name of the first author: Xin Tan
  Name of the last author: Rynson W.H. Lau
  Number of Figures: 9
  Number of Tables: 5
  Number of authors: 6
  Paper title: Mirror Detection With the Visual Chirality Cue
  Publication Date: 2022-06-10 00:00:00
  Table 1 caption: TABLE 1 Comparing Different Ways of Applying Visual Chirality [22]
    for Mirror Detection, on the MSD Dataset [42] and the PMD Dataset [21]
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Quantitative Comparison Between the Proposed VCNet and
    11 State-of-The-Art Methods, on the MSD Dataset [42] and the PMD Dataset [21]
  Table 3 caption: TABLE 3 Quantitative Comparison Between the Proposed VCNet and
    Other Methods on the RGBD-Mirror Dataset [25]
  Table 4 caption: TABLE 4 Ablation Study of the Proposed FCF Transformation, VCE
    Module and CED Module, on the MSD Dataset
  Table 5 caption: TABLE 5 The Motivation of Using Special Features in Our CED Module
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3181030
- Affiliation of the first author: department of computer science, university of rochester,
    rochester, ny, usa
  Affiliation of the last author: department of computer science, university of rochester,
    rochester, ny, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\Semantic_Layout_Manipulation_With_HighResolution_Sparse_Attention\figure_1.jpg
  Figure 1 caption: "Semantic layout manipulation. Given an input image (1st column)\
    \ and a semantic label map (3rd column) manipulated from an existing semantic\
    \ map (2nd column), our network generates manipulated images (last column) that\
    \ conform to the semantic layout guidance. The images are generated at resolution\
    \ 512\xD7512 . It is worth noting that rather than relying on the ground-truth\
    \ layout for manipulation, our model utilizes a semantic parser [11] to generate\
    \ layouts from input images."
  Figure 10 Link: articels_figures_by_rev_year\2022\Semantic_Layout_Manipulation_With_HighResolution_Sparse_Attention\figure_10.jpg
  Figure 10 caption: Qualitative comparisons on the reconstruction task. We show from
    left to right the input image, target layout, and the results of Deepfillv2 [15],
    MEDFE [65], Profill [64], SESAME [3], CoCosNet [18] and our model, respectively.
    Best viewed by zoom-in on screen.
  Figure 2 Link: articels_figures_by_rev_year\2022\Semantic_Layout_Manipulation_With_HighResolution_Sparse_Attention\figure_2.jpg
  Figure 2 caption: Overall pipeline of our semantic layout manipulation framework.
    In Section 3.2, we first propose a high-resolution sparse attention module (left)
    to generate the initialed warped image with semantically aligned appearances.
    Then, in Section 3.3, a unified semantic layout generator (right) consisting of
    a semantic encoder and a coarse-to-fine decoder is proposed to refine the warped
    image and produce the final result.
  Figure 3 Link: articels_figures_by_rev_year\2022\Semantic_Layout_Manipulation_With_HighResolution_Sparse_Attention\figure_3.jpg
  Figure 3 caption: The High-resolution Sparse Attention Module (HRS-Att) leverages
    cross-domain feature extractors to extract high-resolution features from input
    image and an edited layout. Next, for each query point q from the edited layout,
    a key index sampling step serves to sample sparse key indexes p for sparse attention
    computation. Finally, a sparse attentive warping step is proposed to aggregate
    and warp the input image. More details are described in Section 3.2.
  Figure 4 Link: articels_figures_by_rev_year\2022\Semantic_Layout_Manipulation_With_HighResolution_Sparse_Attention\figure_4.jpg
  Figure 4 caption: The warping and synthesis results of our method against CoCosNet
    [18]. Our HRS-Att can transfer high-resolution details for better semantic layout
    manipulation.
  Figure 5 Link: articels_figures_by_rev_year\2022\Semantic_Layout_Manipulation_With_HighResolution_Sparse_Attention\figure_5.jpg
  Figure 5 caption: Visualization of the correspondence generated by our model. Best
    viewed (e.g., local textures) with zoom-in on screen.
  Figure 6 Link: articels_figures_by_rev_year\2022\Semantic_Layout_Manipulation_With_HighResolution_Sparse_Attention\figure_6.jpg
  Figure 6 caption: Correspondence generated by our sparse attention module. The colored
    points in the left and middle figures represent the query position and the sampled
    sparse keys, respectively. The left figure visualizes the warped image by the
    sparse attention module.
  Figure 7 Link: articels_figures_by_rev_year\2022\Semantic_Layout_Manipulation_With_HighResolution_Sparse_Attention\figure_7.jpg
  Figure 7 caption: Qualitative comparisons to image melding [5].
  Figure 8 Link: articels_figures_by_rev_year\2022\Semantic_Layout_Manipulation_With_HighResolution_Sparse_Attention\figure_8.jpg
  Figure 8 caption: Qualitative comparisons on the manipulation task. We show from
    left to right the input image, target layout, and the results of SESAME [3], CoCosNet
    [18] and our model, respectively. Best viewed by zoom-in on screen.
  Figure 9 Link: articels_figures_by_rev_year\2022\Semantic_Layout_Manipulation_With_HighResolution_Sparse_Attention\figure_9.jpg
  Figure 9 caption: Qualitative comparisons on the real manipulation task. Best viewed
    by zoom-in on screen.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.75
  Name of the first author: Haitian Zheng
  Name of the last author: Jiebo Luo
  Number of Figures: 19
  Number of Tables: 3
  Number of authors: 7
  Paper title: Semantic Layout Manipulation With High-Resolution Sparse Attention
  Publication Date: 2022-06-13 00:00:00
  Table 1 caption: TABLE 1 Quantitative Evaluation on the Reconstruction and Manipulation
    Tasks
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 User Preference for the Results of All Methods
  Table 3 caption: TABLE 3 Quantitative Evaluation of the Ablation Models on the Places365
    Manipulation Task
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3181587
- Affiliation of the first author: "max planck institute for informatics, saarland\
    \ informatics campus, saarbr\xFCcken, germany"
  Affiliation of the last author: "max planck institute for informatics, saarland\
    \ informatics campus, saarbr\xFCcken, germany"
  Figure 1 Link: articels_figures_by_rev_year\2022\Random_and_Adversarial_Bit_Error_Robustness_EnergyEfficient_and_Secure_DNN_Accel\figure_1.jpg
  Figure 1 caption: "Energy and Low-Voltage Operation. Average bit error rate p (blue,\
    \ left y -axis) from 32 14 nm SRAM arrays of size 512\xD764 from [5] and energy\
    \ (red, right y -axis) versus voltage ( x -axis). Voltage is normalized by V min\
    \ , the minimal measured voltage for error-free operation, and the energy per\
    \ SRAM access at V min . SRAM accesses have significant impact on the DNN accelerators\
    \ energy [18]. Reducing voltage leads to exponentially increasing bit error rates.\
    \ ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Random_and_Adversarial_Bit_Error_Robustness_EnergyEfficient_and_Secure_DNN_Accel\figure_2.jpg
  Figure 2 caption: "Robustness to Random Bit Errors. Left: Robust test error RErr\
    \ after injecting random bit errors (lower is better \u2193 , y -axis) plotted\
    \ against bit error rate p ( x -axis). For 8 b, robust quantization (RQuant, red),\
    \ additionally weight clipping (Clipping, dotted blue) or per-layer weight clipping\
    \ (PLClipping, solid blue) and finally adding random bit error training (RandBET,\
    \ violet) robustness improves significantly. Robustness to higher bit error rates\
    \ allows more energy efficient operation, cf. Fig. 1. The Pareto optimal frontier\
    \ is shown for 8 b (black solid) and 4 b (dashed) quantization. Right: RErr against\
    \ up to 320 adversarial bit errors, showing that Clippingcombined with RandBETor\
    \ AdvBETalso allow secure operation."
  Figure 3 Link: articels_figures_by_rev_year\2022\Random_and_Adversarial_Bit_Error_Robustness_EnergyEfficient_and_Secure_DNN_Accel\figure_3.jpg
  Figure 3 caption: "Exemplary SRAM Bit Error Patterns. Measured bit errors from two\
    \ chips with on-chip SRAM (left and right), showing bit flip probability for a\
    \ segment of size 64\xD7128 bits: yellow indicates a bit flip probability of one,\
    \ violet indicates zero probability. We show measurements corresponding to two\
    \ supply voltages. With lower voltage, bit error rate increases. Also, the bit\
    \ errors for higher voltage (= lower bit error rate) are a subset of those for\
    \ lower voltage (= higher rate), cf.Section 3. Our error model randomly distributes\
    \ bit errors across space. However, as example, we also show SRAM chip 2 which\
    \ has a different spatial distribution with bit errors distributed along columns.\
    \ We aim to obtain robustness across different memory arrays, voltages and allowing\
    \ arbitrary DNN weight to memory mappings."
  Figure 4 Link: articels_figures_by_rev_year\2022\Random_and_Adversarial_Bit_Error_Robustness_EnergyEfficient_and_Secure_DNN_Accel\figure_4.jpg
  Figure 4 caption: 'Impact of Random Bit Errors. Original weights ( x -axis) plotted
    against perturbed weights with bit errors ( y -axis), for different fixed-point
    quantization schemes with m=8 bit (left) and p=2.5% . We also show the m=4 bit
    case with Clippingat w max =0.1 , cf.Section 4.2. Color indicates absolute error:
    from zero (violet) to the maximal possible error (yellow) of 1 (left) and 0.1
    (right). Asymmetric per-layer quantization reduces the impact of bit errors compared
    to the symmetric per-layerglobal quantization. Clipping reduces absolute error,
    but the errors relative to w max increase. As discussed in Section 4.1, bit errors
    are substantially more severe than quantization errors.'
  Figure 5 Link: articels_figures_by_rev_year\2022\Random_and_Adversarial_Bit_Error_Robustness_EnergyEfficient_and_Secure_DNN_Accel\figure_5.jpg
  Figure 5 caption: Random Bit Error Training ( RandBET ). We illustrate the data-flow
    for RandBETas in Algorithm 2. Here, textBErr p injects random bit errors in the
    quantized weights v(t) = Q(w(t)) , resulting in tildev(t) , while the forward
    pass is performed on the de-quantized perturbed weights tildewq(t) = Q-1(tildev(t))
    , i.e., fixed-point arithmetic is not emulated. The weight update during training
    is not affected by bit errors and computed in floating point.
  Figure 6 Link: articels_figures_by_rev_year\2022\Random_and_Adversarial_Bit_Error_Robustness_EnergyEfficient_and_Secure_DNN_Accel\figure_6.jpg
  Figure 6 caption: Effect of Weight Clipping. On CIFAR10, weight clipping constraints
    the weights (right), thereby implicitly limiting the possible range for logits
    (left, blue). However, even for wmax =0.1 the DNN is able to produce high confidences
    (middle, blue), suggesting that more weights are used to obtain these logits.
    Furthermore, the impact of random bit errors, p = 1% , on the logitsconfidences
    (red) is reduced significantly. RandBET(trained with p = 1% , wo weight clipping),
    increases the range of weights and is less effective at preserving logitconfidence
    distribution. .
  Figure 7 Link: articels_figures_by_rev_year\2022\Random_and_Adversarial_Bit_Error_Robustness_EnergyEfficient_and_Secure_DNN_Accel\figure_7.jpg
  Figure 7 caption: "Bit Error Robustness on CIFAR10, CIFAR100and TinyImageNet. Average\
    \ RErr plotted against bit error rate p , both in %. We considered various models\
    \ (in \u2022 gray), corresponding to different wmax and p during training. We\
    \ explicitly plot the best model for each bit error rate: for Normal(orange),\
    \ RQuant(red), Clipping(blue) and RandBET(violet). Note that these might correspond\
    \ to different wmax and p (also across datasets). Across all approaches, we plot\
    \ the per-error-rate best model in black: for m = 8,4,3,2 bits, depending on dataset.\
    \ For 8 b and low bit error rates, Clippingis often sufficient. However, for 4\
    \ b or higher bit error rates, RandBETis crucial to keep RErr low."
  Figure 8 Link: articels_figures_by_rev_year\2022\Random_and_Adversarial_Bit_Error_Robustness_EnergyEfficient_and_Secure_DNN_Accel\figure_8.jpg
  Figure 8 caption: Adversarial Bit Error Iterations. We plot loss, cf.Eq. (1), and
    robust error RErr against iterations, both measured on the 100 held-out test examples
    used to find, i.e., train, adversarial bit errors. Clearly, Linfty gradient normalization
    with momentum (in red), targeting first convolutional (yellow) or logit layer
    (violet) is most effective for untargeted attacks. Unfortunately, the attack gets
    easily stuck in bad optima if learning rate is not optimal. Targeted attacks simplify
    optimization and are often more effective (black, right).
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: David Stutz
  Name of the last author: Bernt Schiele
  Number of Figures: 8
  Number of Tables: 9
  Number of authors: 4
  Paper title: 'Random and Adversarial Bit Error Robustness: Energy-Efficient and
    Secure DNN Accelerators'
  Publication Date: 2022-06-13 00:00:00
  Table 1 caption: TABLE 1 Quantization Robustness
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Weight Clipping Robustness
  Table 3 caption: TABLE 3 Fixed Pattern Bit Error Training
  Table 4 caption: TABLE 4 Random Bit Error Training (RandBET)
  Table 5 caption: TABLE 5 Generalization to Profiled Bit Errors
  Table 6 caption: TABLE 6 Bit Errors in Inputs and Activations
  Table 7 caption: TABLE 7 Bit Flip Attack (BFA) [11]
  Table 8 caption: TABLE 8 Adversarial Bit Error Ablation
  Table 9 caption: TABLE 9 Adversarial Bit Error Robustness and AdvBET
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3181972
