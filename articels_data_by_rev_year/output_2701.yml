- Affiliation of the first author: department of computer science, university of hong
    kong, hong kong
  Affiliation of the last author: department of computer science, university of hong
    kong, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2022\Relation_Matters_ForegroundAware_GraphBased_Relational_Reasoning_for_Domain_Adap\figure_1.jpg
  Figure 1 caption: (a) We formulate DAOD as an open-set domain adaptation problem,
    where foreground is the known classes and background is the unknown class. (b)
    Our method reasons the foreground object relationships in both intra- and inter-domain.
    The graph construction process mines the foreground pixels or regions based on
    the extracted features.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Relation_Matters_ForegroundAware_GraphBased_Relational_Reasoning_for_Domain_Adap\figure_2.jpg
  Figure 2 caption: The overall structure of the proposed FGRR, which mainly includes
    the pixel-level and semantic-level relational reasoning modules as well as the
    image-level object-aware reweighting. NC, IOR, and CDA denote the node classification,
    object-aware reweighting, and category-aware domain alignment losses.
  Figure 3 Link: articels_figures_by_rev_year\2022\Relation_Matters_ForegroundAware_GraphBased_Relational_Reasoning_for_Domain_Adap\figure_3.jpg
  Figure 3 caption: "Performance with the variation of IOU thresholds on adaptation\
    \ tasks Cityscapes \u2192 Foggy-Cityscapes and Pascal VOC \u2192 Clipart1k."
  Figure 4 Link: articels_figures_by_rev_year\2022\Relation_Matters_ForegroundAware_GraphBased_Relational_Reasoning_for_Domain_Adap\figure_4.jpg
  Figure 4 caption: "The t-SNE visualization of extracted features on adaptation task\
    \ Pascal VOC \u2192 Clipart1k. Red: source features, Blue: target features. Images\
    \ with orange lines are from the source domain and images with blue lines are\
    \ from target domain."
  Figure 5 Link: articels_figures_by_rev_year\2022\Relation_Matters_ForegroundAware_GraphBased_Relational_Reasoning_for_Domain_Adap\figure_5.jpg
  Figure 5 caption: "Detection results on the target domain. First and second rows:\
    \ Cityscapes \u2192 Foggy-Cityscapes. Third and fourth rows: Sim10K \u2192 Cityscapes."
  Figure 6 Link: articels_figures_by_rev_year\2022\Relation_Matters_ForegroundAware_GraphBased_Relational_Reasoning_for_Domain_Adap\figure_6.jpg
  Figure 6 caption: "Detection results on the target domain. First row: Pascal VOC\
    \ \u2192 Clipart1k. Second row: Pascal VOC \u2192 Watercolor2k. Third row: Pascal\
    \ VOC \u2192 Comic2k."
  Figure 7 Link: articels_figures_by_rev_year\2022\Relation_Matters_ForegroundAware_GraphBased_Relational_Reasoning_for_Domain_Adap\figure_7.jpg
  Figure 7 caption: Detection results on In-house dataset. Bounding boxes with green
    color denote the ground-truth, and with red color denote the prediction results.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.76
  Name of the first author: Chaoqi Chen
  Name of the last author: Yizhou Yu
  Number of Figures: 7
  Number of Tables: 10
  Number of authors: 7
  Paper title: 'Relation Matters: Foreground-Aware Graph-Based Relational Reasoning
    for Domain Adaptive Object Detection'
  Publication Date: 2022-06-01 00:00:00
  Table 1 caption: TABLE 1 Results on Adaptation From PASCAL VOC to Clipart Dataset
    (%)
  Table 10 caption: "TABLE 10 Results on Pascal VOC \u2192 \u2192 Comic2k (%)"
  Table 2 caption: TABLE 2 Results on Adaptation From Cityscapes to Foggy-Cityscapes
    (%)
  Table 3 caption: "TABLE 3 Results on Pascal VOC \u2192 \u2192 Watercolor2k (%)"
  Table 4 caption: "TABLE 4 Results on Pascal VOC \u2192 \u2192 Comic2k (%)"
  Table 5 caption: "TABLE 5 Results on Sim10K \u2192 \u2192 Cityscapes (%)"
  Table 6 caption: "TABLE 6 Results on Public \u2192 \u2192 In-house (%)"
  Table 7 caption: "TABLE 7 Results on PASCAL VOC \u2192 \u2192 Clipart Dataset (%)"
  Table 8 caption: TABLE 8 Ablation of FGRR on Six DAOD Tasks (%)
  Table 9 caption: "TABLE 9 Results on Pascal VOC \u2192 \u2192 Watercolor2k (%)"
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3179445
- Affiliation of the first author: "sorbonne universit\xE9, cnrs, institut des syst\xE8\
    mes intelligents et de robotique, isir, paris, france"
  Affiliation of the last author: "sorbonne universit\xE9, cnrs, institut des syst\xE8\
    mes intelligents et de robotique, isir, paris, france"
  Figure 1 Link: articels_figures_by_rev_year\2022\RED__DataFree_Pruning_of_Deep_Neural_Networks_via_Input_Splitting_and_Output_Mer\figure_1.jpg
  Figure 1 caption: "For each layer l , the hashing algorithm estimates the density\
    \ P l w based on the weights W l using KDE (a). The estimated density d l is then\
    \ evaluated on a discreet grid in order to obtain local minima M \u2212 l and\
    \ maxima M + l (b) to obtain a partition and hashed values for the hashing function\
    \ h l . On the right side, we show examples of weight distributions for real layers\
    \ from ResNet 56 on Cifar10 (c) and ResNet 50 on ImageNet (d)."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\RED__DataFree_Pruning_of_Deep_Neural_Networks_via_Input_Splitting_and_Output_Mer\figure_2.jpg
  Figure 2 caption: "Neuron merging for a fully-connected layer l with weights W ~\
    \ l i,j . Similar colors indicate similar values, e.g., W ~ l 0,0 = W ~ l 1,0\
    \ . The merged weights W \xAF are obtained by merging the first 2 neurons of layer\
    \ l and updating the consecutive layer by adding the corresponding weights."
  Figure 3 Link: articels_figures_by_rev_year\2022\RED__DataFree_Pruning_of_Deep_Neural_Networks_via_Input_Splitting_and_Output_Mer\figure_3.jpg
  Figure 3 caption: "Splitting strategy for a layer with weights W \xAF l \u2208 R\
    \ 4\xD76 where colors indicate weight values. We apply the singleton strategy\
    \ by splitting the layer w.r.t. its input, which replaces multiplications by memory\
    \ accesses referred as duplication operations."
  Figure 4 Link: articels_figures_by_rev_year\2022\RED__DataFree_Pruning_of_Deep_Neural_Networks_via_Input_Splitting_and_Output_Mer\figure_4.jpg
  Figure 4 caption: "Given a layer l with weights W l \u2208 R 3\xD73\xD7 n l\u2212\
    1 \xD7 n l we fix the value of the product n l\u22121 \xD7 n l = 64 2 and we plot\
    \ the empirical values of the pruning ratios E m (nuances of blue) and E s (nuances\
    \ of orange) for different values of n l\u22121 the input dimension. The considered\
    \ priors are the discreet Gaussian, the exponential and uniform distribution.\
    \ We observe the complementarity of merging and splitting."
  Figure 5 Link: articels_figures_by_rev_year\2022\RED__DataFree_Pruning_of_Deep_Neural_Networks_via_Input_Splitting_and_Output_Mer\figure_5.jpg
  Figure 5 caption: Empirical distribution of the pruning ratios from merging (blue)
    and splitting (orange) on several layers of different networks (e.g., ResNets
    and MobileNets) on both ImageNet and Cifar10. These results are the empirical
    counterpart of the theoretical plots on Fig. 4.
  Figure 6 Link: articels_figures_by_rev_year\2022\RED__DataFree_Pruning_of_Deep_Neural_Networks_via_Input_Splitting_and_Output_Mer\figure_6.jpg
  Figure 6 caption: RED++ versus SOTA methods for ResNet 20, 56, and 110 on Cifar10
    in terms of % removed parameters and accuracy preservation. Data-free methods
    are in red while data-driven ones are in blue. Structured methods are plotted
    as triangles, unstructured as squares and ours (structured at the kernel level)
    as a star.
  Figure 7 Link: articels_figures_by_rev_year\2022\RED__DataFree_Pruning_of_Deep_Neural_Networks_via_Input_Splitting_and_Output_Mer\figure_7.jpg
  Figure 7 caption: RED++ versus SOTA methods for EfficientNet B0, ResNet 50 and MobileNet
    V2 on ImageNet. Data-free methods are in red while data-driven ones are in blue.
    Structured methods are plotted as triangles, unstructured as squares and ours
    (structured at the kernel level) as a star.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Edouard Yvinec
  Name of the last author: Kevin Bailly
  Number of Figures: 7
  Number of Tables: 5
  Number of authors: 4
  Paper title: 'RED++ : Data-Free Pruning of Deep Neural Networks via Input Splitting
    and Output Merging'
  Publication Date: 2022-06-02 00:00:00
  Table 1 caption: TABLE 1 Hashing Performance on ResNet 56 on Cifar10 as Compared
    to k-Means Approach
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Empirical Evaluation of the Expected Error From Hashing
  Table 3 caption: TABLE 3 Evaluation of Our Adaptive Hashing in Term of % % Removed
    Weight Values (% Reduction) and Test Accuracy Drop Compared With a Uniform Baseline
  Table 4 caption: TABLE 4 Each Pruning Step of RED++ are Applied in a Sequence
  Table 5 caption: TABLE 5 RED++ Evaluation on Transformers on ImageNet
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3179616
- Affiliation of the first author: national lab of pattern recognition, institute
    of automation, chinese academy of sciences, beijing, china
  Affiliation of the last author: national lab of pattern recognition, institute of
    automation, chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\WeaklySupervised_Video_Object_Grounding_via_Causal_Intervention\figure_1.jpg
  Figure 1 caption: Two settings of video object grounding. They both aim to localize
    the objects described in the sentence to visual regions in a video. (a) Fully-supervised
    video object grounding is explicitly supervised with spatial bounding boxes and
    temporal object occurrences. (b) Weakly-supervised video object grounding only
    has video-sentence annotations during model learning, suffering from the ambiguity
    problem under weak supervision and confounding effect brought by co-occurred objects.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\WeaklySupervised_Video_Object_Grounding_via_Causal_Intervention\figure_2.jpg
  Figure 2 caption: Our proposed unified causal graph. The intervened variations of
    it reflect three types of associations learned in WSVOG task. (a) Existing methods
    mainly learn the ambiguous statistics-based association, suffering from the ambiguity
    problem and confounding effect. (b) We first mitigate the ambiguity problem by
    learning the object-relevant association. (c) We further remove the accompanying
    bad confounding effect by learning the deconfounded object-relevant association.
    The right half part shows the examples for each causal variable in our causal
    graph.
  Figure 3 Link: articels_figures_by_rev_year\2022\WeaklySupervised_Video_Object_Grounding_via_Causal_Intervention\figure_3.jpg
  Figure 3 caption: Overview of the implementations of our derived causal solution
    in WSVOG task. The backbone in blue block learns the ambiguous statistics-based
    association, suffering from the ambiguity problem and confounding effect. We mitigate
    the ambiguity problem via spatial-temporal adversarial contrastive learning, and
    we remove the bad confounding effect via backdoor adjustment.
  Figure 4 Link: articels_figures_by_rev_year\2022\WeaklySupervised_Video_Object_Grounding_via_Causal_Intervention\figure_4.jpg
  Figure 4 caption: Effect of the sizes of content set and style set, which are B
    r and B f , respectively.
  Figure 5 Link: articels_figures_by_rev_year\2022\WeaklySupervised_Video_Object_Grounding_via_Causal_Intervention\figure_5.jpg
  Figure 5 caption: Effect of the sizes of the memory banks in spatial and temporal,
    which are J and W , respectively.
  Figure 6 Link: articels_figures_by_rev_year\2022\WeaklySupervised_Video_Object_Grounding_via_Causal_Intervention\figure_6.jpg
  Figure 6 caption: "Effect of momentum factor \u03B1 in adversarial contrastive learning."
  Figure 7 Link: articels_figures_by_rev_year\2022\WeaklySupervised_Video_Object_Grounding_via_Causal_Intervention\figure_7.jpg
  Figure 7 caption: 'Two qualitative grounding results where: (a) shows the ambiguity
    problem under weak supervision and we mitigate it by learning object-relevant
    association; (b) shows the confounding effect brought by co-occurred objects and
    we remove such effect using backdoor adjustment.'
  Figure 8 Link: articels_figures_by_rev_year\2022\WeaklySupervised_Video_Object_Grounding_via_Causal_Intervention\figure_8.jpg
  Figure 8 caption: More qualitative grounding results for reference, from which we
    can see that our model consistently shows the better grounding performance.
  Figure 9 Link: articels_figures_by_rev_year\2022\WeaklySupervised_Video_Object_Grounding_via_Causal_Intervention\figure_9.jpg
  Figure 9 caption: Failure cases of our method, which reflect the grounding preference
    on salient visual regions, mainly resulted from learning the object-relevant association
    in WSVOG task.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Wei Wang
  Name of the last author: Changsheng Xu
  Number of Figures: 9
  Number of Tables: 5
  Number of authors: 3
  Paper title: Weakly-Supervised Video Object Grounding via Causal Intervention
  Publication Date: 2022-06-03 00:00:00
  Table 1 caption: TABLE 1 Weakly-Supervised Video Object Grounding Results on YouCook2-BB
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 WSVOG Results on ActivityNetEntities
  Table 3 caption: TABLE 3 Out-of-Distribution (OOD) Test on RoboWatch
  Table 4 caption: TABLE 4 Ablation Results on YouCook2-BB and Corresponding OOD Test
    on RoboWatch
  Table 5 caption: TABLE 5 Ablation Results on ActivityNetEntities
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3180025
- Affiliation of the first author: iiai, abu dhabi, uae
  Affiliation of the last author: terminus group, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Salient_Object_Detection_via_Integrity_Learning\figure_1.jpg
  Figure 1 caption: Integrity issues (i.e., (a) macro integrity and (b) micro integrity)
    from SOD. The red and yellow contours represent the ground-truths. Grey regions
    denote the prediction results. The drawing style was inspired by [21].
  Figure 10 Link: articels_figures_by_rev_year\2022\Salient_Object_Detection_via_Integrity_Learning\figure_10.jpg
  Figure 10 caption: Illustration of failure cases. The first and second columns are
    the input images, and the ground-truth masks. The other columns show the prediction
    results from our ICON and the baseline methods.
  Figure 2 Link: articels_figures_by_rev_year\2022\Salient_Object_Detection_via_Integrity_Learning\figure_2.jpg
  Figure 2 caption: "Integrity is a good indicator for saliency prediction. Here are\
    \ some samples from the SOD datasets, with images (left) and ground truth (right)\
    \ listed. To better predict the group of images, e.g., (a) the model needs to\
    \ have the judgment of macro-level integrity, while the model needs to extract\
    \ micro-level integrity efficiently present second group of images [i.e., (b)].On\
    \ the top, several predictions with different integrity qualities are presented.\
    \ Note that: \u25A0 Bad predictions, \u25A0 Comparably high-quality prediction\
    \ results."
  Figure 3 Link: articels_figures_by_rev_year\2022\Salient_Object_Detection_via_Integrity_Learning\figure_3.jpg
  Figure 3 caption: 'Overall architecture of the proposed ICON. Feature extraction
    block: F (1) bkb - F (5) bkb denote different layers from ResNet-50 [57]. Component
    1: the diverse feature aggregation (DFA) module aggregates the features with various
    receptive fields. Component 2: the integrity enhancement (ICE) module aims at
    enhancing the feature channels that highlight the potential integral salient object.
    Component 3: the part-whole verification (PWV) module judges whether the part
    and whole features have strong agreement.'
  Figure 4 Link: articels_figures_by_rev_year\2022\Salient_Object_Detection_via_Integrity_Learning\figure_4.jpg
  Figure 4 caption: "Details of the proposed modules. (a) The diverse feature aggregation\
    \ (DFA) component combines different convolutional kernels to enhance the representational\
    \ ability. (b) The integrity channel enhancement (ICE) component, mines integrity\
    \ information along the channel dimension. (c) The part-whole verification (PWV)\
    \ component is designed for modeling the relation between the parts and the whole\
    \ object. \u201CAsyConv\u201D, \u201CAtrConv\u201D, and \u201CConv\u201D means\
    \ asymmetric block [66], atrous block [67], and conv block, respectively. All\
    \ these blocks include convolution, BatchNorm and ReLU components. \u201CEM Routing\u201D\
    \ means the expectation-maximum routing mechanism [33]. C, H, W denote the channel\
    \ number, height, and width of the feature tensor, respectively."
  Figure 5 Link: articels_figures_by_rev_year\2022\Salient_Object_Detection_via_Integrity_Learning\figure_5.jpg
  Figure 5 caption: Visual comparison of heatmaps before and after using our ICE.
    (a) Input image. (b) Ground truth. Feature heatmaps (c) before and (d) after our
    ICE module. Channel visualizations (e) before and (f) after the ICE module. Our
    ICE module helps the network focus more on the integral salient regions and distinguish
    the foreground regions from the background. The feature heatmap presented in (d)
    clearly demonstrates the ability of our ICE module to capture the integrity representation
    at both macro- and micro levels. The channel visualizations are generated by squeezing
    all channels and then we use Matplotlibs pseudo-color jet for colorization. Zoom
    in for better viewing.
  Figure 6 Link: articels_figures_by_rev_year\2022\Salient_Object_Detection_via_Integrity_Learning\figure_6.jpg
  Figure 6 caption: Visual comparison at different false negative ratios (FNRs). Note
    that the FNRs heavily reflect the integrity scores.
  Figure 7 Link: articels_figures_by_rev_year\2022\Salient_Object_Detection_via_Integrity_Learning\figure_7.jpg
  Figure 7 caption: FNR statistics of 11 methods on four different datasets.
  Figure 8 Link: articels_figures_by_rev_year\2022\Salient_Object_Detection_via_Integrity_Learning\figure_8.jpg
  Figure 8 caption: Precision-recall and F-measure curves of the proposed method and
    other SOTA algorithms on five popular SOD datasets.
  Figure 9 Link: articels_figures_by_rev_year\2022\Salient_Object_Detection_via_Integrity_Learning\figure_9.jpg
  Figure 9 caption: Qualitative comparison of our method with seven SOTA methods.
    Unlike other baseline methods, our method not only accurately locates the salient
    object but also produces sharper edges with fewer background distractors for various
    scenes.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Mingchen Zhuge
  Name of the last author: Ling Shao
  Number of Figures: 10
  Number of Tables: 7
  Number of authors: 6
  Paper title: Salient Object Detection via Integrity Learning
  Publication Date: 2022-06-06 00:00:00
  Table 1 caption: TABLE 1 Quantitative Results on Six Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison Between the Proposed Method and Other SOTA Methods
    on the SOC Test Set
  Table 3 caption: TABLE 3 Ablation Analysis of Our Baseline Gradually Including the
    Newly Proposed Components
  Table 4 caption: TABLE 4 Ablation Analysis of Different Feature Enhancement Methods
    (FEMs) Compared With the DFA Module of Our ICON
  Table 5 caption: TABLE 5 Ablation Analysis of Our ICE and Other Alternative Methods
    Using Various Attention Mechanisms
  Table 6 caption: TABLE 6 Ablation Analysis of Our Method When Using Other Routing
    Mechanisms in Our PWV
  Table 7 caption: TABLE 7 Ablation Analysis of Our Method When Using Different Loss
    Functions
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3179526
- Affiliation of the first author: national laboratory of pattern recognition, casia,
    center for research on intelligent perception and computing, casia, center for
    excellence in brain science and intelligence technology, cas, school of artificial
    intelligence, university of chinese academy of sciences, beijing, china
  Affiliation of the last author: national laboratory of pattern recognition, casia,
    center for research on intelligent perception and computing, casia, center for
    excellence in brain science and intelligence technology, cas, school of artificial
    intelligence, university of chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Memory_Uncertainty_Learning_for_RealWorld_Single_Image_Deraining\figure_1.jpg
  Figure 1 caption: "Stochastic memory network (SMNet). It consists of a convolution\
    \ head, a U-shaped body of memory residual blocks (MRBs), and a convolution-Tanh\
    \ tail. Each MRB is a stack of residual layers (MRLs) augmented optionally with\
    \ memory modules. The 1\xD71 convolution layer in MRL is used only when the input\
    \ and output have different channel numbers. \u03B1 in MRL is a learnable weight."
  Figure 10 Link: articels_figures_by_rev_year\2022\Memory_Uncertainty_Learning_for_RealWorld_Single_Image_Deraining\figure_10.jpg
  Figure 10 caption: Example images of data augmentation.
  Figure 2 Link: articels_figures_by_rev_year\2022\Memory_Uncertainty_Learning_for_RealWorld_Single_Image_Deraining\figure_2.jpg
  Figure 2 caption: "Uncertainty-aware Self-training. It employs a target network\
    \ f \u03BE to produce pseudo labels with uncertain estimates for unlabeled data,\
    \ and then generates noisy data through augmentation from both the synthetic and\
    \ pseudo paired images. \u03B8 of the online network are trained on the synthetic\
    \ data and the augmented noisy data, while \u03BE of the target network are an\
    \ exponential moving average of \u03B8 ."
  Figure 3 Link: articels_figures_by_rev_year\2022\Memory_Uncertainty_Learning_for_RealWorld_Single_Image_Deraining\figure_3.jpg
  Figure 3 caption: Source analysis of the SSID dataset.
  Figure 4 Link: articels_figures_by_rev_year\2022\Memory_Uncertainty_Learning_for_RealWorld_Single_Image_Deraining\figure_4.jpg
  Figure 4 caption: Visual results on the synthetic test set of the DDN-SIRR dataset.
    Best viewed by zooming in the electronic version.
  Figure 5 Link: articels_figures_by_rev_year\2022\Memory_Uncertainty_Learning_for_RealWorld_Single_Image_Deraining\figure_5.jpg
  Figure 5 caption: Visual results on the real-world test set of the SSID dataset.
    The number below every image is the corresponding averaged selection percentage
    in user study.
  Figure 6 Link: articels_figures_by_rev_year\2022\Memory_Uncertainty_Learning_for_RealWorld_Single_Image_Deraining\figure_6.jpg
  Figure 6 caption: Averaged selection percentage of user study.
  Figure 7 Link: articels_figures_by_rev_year\2022\Memory_Uncertainty_Learning_for_RealWorld_Single_Image_Deraining\figure_7.jpg
  Figure 7 caption: Rank-n scores of user study.
  Figure 8 Link: articels_figures_by_rev_year\2022\Memory_Uncertainty_Learning_for_RealWorld_Single_Image_Deraining\figure_8.jpg
  Figure 8 caption: The running time (ms) for a 256times 256 image.
  Figure 9 Link: articels_figures_by_rev_year\2022\Memory_Uncertainty_Learning_for_RealWorld_Single_Image_Deraining\figure_9.jpg
  Figure 9 caption: Qualitative comparisons between MUSS and its soft-attention version.
    The numbers below the derained images are the average selection percentage in
    user study.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.58
  Name of the first author: Huaibo Huang
  Name of the last author: Ran He
  Number of Figures: 13
  Number of Tables: 6
  Number of authors: 3
  Paper title: Memory Uncertainty Learning for Real-World Single Image Deraining
  Publication Date: 2022-06-07 00:00:00
  Table 1 caption: TABLE 1 Summary of Rain Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Quantitative Results on the Synthetic Test Set of the DDN-SIRR
    Dataset
  Table 3 caption: TABLE 3 Quantitative Results on the Real-World Test Set of the
    SSID Dataset
  Table 4 caption: TABLE 4 Ablation Study on the DDN-SIRR Dataset
  Table 5 caption: "TABLE 5 Results of Different Decay Rates \u03C4 \u03C4 on DDN-SIRR"
  Table 6 caption: "TABLE 6 Results of Different Decay Rates \u03C5 \u03C5 on DDN-SIRR"
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3180560
- Affiliation of the first author: school of civil and transportation engineering,
    hebei university of technology, tianjin, china
  Affiliation of the last author: school of civil and transportation engineering,
    hebei university of technology, tianjin, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Fast_Component_Tree_Computation_for_Images_of_Limited_Levels\figure_1.jpg
  Figure 1 caption: "The same gray image with different number \u03BA of levels. The\
    \ 1st row: \u03BA = 256, 128, 64, 32; the 2nd row: \u03BA = 16, 8, 4, 2."
  Figure 10 Link: articels_figures_by_rev_year\2022\Fast_Component_Tree_Computation_for_Images_of_Limited_Levels\figure_10.jpg
  Figure 10 caption: Comparing the time of LLT in 4-8-connectivity and 6-connectivity.
  Figure 2 Link: articels_figures_by_rev_year\2022\Fast_Component_Tree_Computation_for_Images_of_Limited_Levels\figure_2.jpg
  Figure 2 caption: (a) An example from Fig. 5 in [27]. (b) The max-tree. (c) The
    min-tree. (d) The ToS. In all the three trees, the root is the image domain.
  Figure 3 Link: articels_figures_by_rev_year\2022\Fast_Component_Tree_Computation_for_Images_of_Limited_Levels\figure_3.jpg
  Figure 3 caption: (a) An example from Fig. 5 in [27]. (b) The level lines in the
    positive level line tree. (c) The positive level line tree. (d) The level lines
    in the negative level line tree. (e) The negative level line tree. In both trees,
    the root is the image border.
  Figure 4 Link: articels_figures_by_rev_year\2022\Fast_Component_Tree_Computation_for_Images_of_Limited_Levels\figure_4.jpg
  Figure 4 caption: (a) 4-connectivity. (b) 8-connectivity. (c) 6-connectivity of
    type-1. (d) 6-connectivity of type-2. (e) Translation of type-2 6-connectivity
    to hexagonal connectivity. In each case the center pixel has 486 neighbor pixels
    in gray around it.
  Figure 5 Link: articels_figures_by_rev_year\2022\Fast_Component_Tree_Computation_for_Images_of_Limited_Levels\figure_5.jpg
  Figure 5 caption: An example of positive level line (in red) and negative level
    line (in yellow). For the red line, the immediate interior (II) is in light gray,
    and the immediate exterior (IE) in dark gray. In this example, the private region
    of the red line is its immediate interior; the private region of the yellow line
    is its interior (in black).
  Figure 6 Link: articels_figures_by_rev_year\2022\Fast_Component_Tree_Computation_for_Images_of_Limited_Levels\figure_6.jpg
  Figure 6 caption: "(a) A synthetic image. (b) The positive level line tree of the\
    \ image. (c) The intensity function along the dashed yellow line in the image.\
    \ In the figure, the 5 levels, k5- k1, are respectively the private values of\
    \ \u03B1, \u03B2, \u03B4, \u03C3, and \u03B3. The lines \u03B7 and \u03B3 have\
    \ the same private value k1. The unmatched negative lines at \u03B1 are framed\
    \ by the green dashed rectangle. (d) The 4 not-root components. (e) The max-tree,\
    \ where the root component C1 is the image domain with the boundary \u03B7. (f)\
    \ The unmatched forest at the positive line \u03B1."
  Figure 7 Link: articels_figures_by_rev_year\2022\Fast_Component_Tree_Computation_for_Images_of_Limited_Levels\figure_7.jpg
  Figure 7 caption: "(a. upleft) \u03B1 is matched with \u03B2 and \u03B4 at the levels\
    \ in (k4, k5] to get the component C5. (b. upright) \u03B1 is matched with \u03B3\
    \ and \u03B4 at the levels in (k3, k4] to get the component C4. (c. bottomleft)\
    \ \u03B1 is matched with \u03B3 at the levels in (k2, k3] to get the component\
    \ C3. (d. bottomright) \u03C3 is matched with \u03B3 at the levels in (k1, k2]\
    \ to get the component C2; the root level line \u03B7 itself makes the root component\
    \ C1."
  Figure 8 Link: articels_figures_by_rev_year\2022\Fast_Component_Tree_Computation_for_Images_of_Limited_Levels\figure_8.jpg
  Figure 8 caption: The test images. The images of no. 1-12 are listed row by row,
    and in each row from left to right.
  Figure 9 Link: articels_figures_by_rev_year\2022\Fast_Component_Tree_Computation_for_Images_of_Limited_Levels\figure_9.jpg
  Figure 9 caption: Comparing the time of CT in 4-8-connectivity and 6-connectivity.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Rui Tao
  Name of the last author: Jiangang Qiao
  Number of Figures: 20
  Number of Tables: 6
  Number of authors: 2
  Paper title: Fast Component Tree Computation for Images of Limited Levels
  Publication Date: 2022-06-07 00:00:00
  Table 1 caption: TABLE 1 Statistics and Computation Time of the Test Images
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The Run Times of LLT, LLTold, CT, and CTold on the Test
    Images, As Well As the Duplicate Ratios of the Level Lines in the Components and
    in the Level Sets of the Test Images
  Table 3 caption: TABLE 3 The Statistics and Run Times in Milliseconds on G64(I)
    and H64(I)
  Table 4 caption: TABLE 4 The Run Times of the Algorithms on the Original Gray Images
  Table 5 caption: TABLE 5 The Run Times of the Algorithms on the Original Gray Images
  Table 6 caption: TABLE 6 The Average Run-Times in Milliseconds of MSER and TDLST
    on the 32- and 64-level Copies
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3180827
- Affiliation of the first author: key lab of data engineering and kowledge engineering,
    renmin university of china, beijing, china
  Affiliation of the last author: key lab of data engineering and kowledge engineering,
    renmin university of china, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\MVSSNet_MultiView_MultiScale_Supervised_Networks_for_Image_Manipulation_Detectio\figure_1.jpg
  Figure 1 caption: Image manipulation detection by the state-of-the-art. Test images
    in the first three rows are manipulated by splicing, copy-move and inpainting,
    respectively. Test images in the last three rows are authentic (thus with blank
    mask). Our model (MVSS-Net++) strikes a good balance between detection sensitivity
    (lower miss detection on the manipulated) and specificity (lower false alarm on
    the authentic).
  Figure 10 Link: articels_figures_by_rev_year\2022\MVSSNet_MultiView_MultiScale_Supervised_Networks_for_Image_Manipulation_Detectio\figure_10.jpg
  Figure 10 caption: Detection performance on images re-captured by three screenshot
    tools (Snip&Sketch, Chrome and Snipaste) and saved in two different formats (jpg
    and png). Results are sorted by the performance of MVSS-Net++ in descending order.
  Figure 2 Link: articels_figures_by_rev_year\2022\MVSSNet_MultiView_MultiScale_Supervised_Networks_for_Image_Manipulation_Detectio\figure_2.jpg
  Figure 2 caption: "Conceptual diagram of the proposed multi-view multi-scale supervised\
    \ networks for image manipulation detection. We use the edge-supervised branch\
    \ (ESB) and the noise-sensitive branch (NSB) to learn semantic-agnostic features\
    \ for manipulation detection, and multi-scale supervision to strike a proper balance\
    \ between model sensitivity and specificity. The non-trainable sigmoid ( \u03C3\
    \ ) layer is shown in gray. The G clf module is responsible for converting a pixel-level\
    \ segmentation map S(x) to an image-level prediction C(x) . Depending on how the\
    \ module is implemented, we have MVSS-Net which uses global max pooling (GMP)\
    \ and MVSS-Net++ which uses ConvGeM."
  Figure 3 Link: articels_figures_by_rev_year\2022\MVSSNet_MultiView_MultiScale_Supervised_Networks_for_Image_Manipulation_Detectio\figure_3.jpg
  Figure 3 caption: Diagrams of (a) Sobel layer and (b) edge residual block, used
    in ESB for manipulation edge detection.
  Figure 4 Link: articels_figures_by_rev_year\2022\MVSSNet_MultiView_MultiScale_Supervised_Networks_for_Image_Manipulation_Detectio\figure_4.jpg
  Figure 4 caption: Visualization of averaged feature maps of the last ResNet block,
    brighter color indicating higher responses. Manipulation from the top to bottom
    is inpainting, copy-move and splicing. Read from the third column are wo edge
    (standard ResNet with no edge-related block), GSR (ResNet with the GSR-Net alike
    edge branch) and our ESB, which produces more focused responses near tampered
    regions.
  Figure 5 Link: articels_figures_by_rev_year\2022\MVSSNet_MultiView_MultiScale_Supervised_Networks_for_Image_Manipulation_Detectio\figure_5.jpg
  Figure 5 caption: Dual Attention (DA), with its channel attention module shown in
    blue and its position attention module shown in green. While DA was originally
    developed for capturing long-range contextual dependencies of feature maps produced
    by a single-branch network [39], we re-purpose it for fusing feature maps from
    two distinct branches.
  Figure 6 Link: articels_figures_by_rev_year\2022\MVSSNet_MultiView_MultiScale_Supervised_Networks_for_Image_Manipulation_Detectio\figure_6.jpg
  Figure 6 caption: Illustration of the proposed ConvGeM module that converts the
    pixel-level manipulation detection result S(x) to the image-level prediction C(x)
    . The hyper parameter lambda that balances GeM and GeM(Conv) decays w.r.t. the
    training epochs, and is dynamically determined in the training process.
  Figure 7 Link: articels_figures_by_rev_year\2022\MVSSNet_MultiView_MultiScale_Supervised_Networks_for_Image_Manipulation_Detectio\figure_7.jpg
  Figure 7 caption: 'Visualizing pixel-level manipulation detection results of the
    proposed model in varied setups. Data source: DEFACTO [4]. The test images in
    the last three rows are authentic. Setups with multi-scale supervision (Seg+Clf
    and afterwards) improves the detection specificity, yet at the cost of the detection
    sensitivity, which has to be brought back by multi-view feature learning. Among
    all the setups, MVSS-Net++ strikes the best balance between the detection sensitivity
    and specificity.'
  Figure 8 Link: articels_figures_by_rev_year\2022\MVSSNet_MultiView_MultiScale_Supervised_Networks_for_Image_Manipulation_Detectio\figure_8.jpg
  Figure 8 caption: Performance curves w.r.t. the decision threshold. Larger threshold
    means lower sensitivity and higher specificity. Per model, its image-level F1
    score given a specific threshold is obtained by averaging the F1 scores over the
    five testsets, i.e., Columbia, CASIAv1+, COVER, DEF-12k and IMD. The two numbers
    following each model are its optimal threshold and the corresponding F1, as visualized
    in red circles.
  Figure 9 Link: articels_figures_by_rev_year\2022\MVSSNet_MultiView_MultiScale_Supervised_Networks_for_Image_Manipulation_Detectio\figure_9.jpg
  Figure 9 caption: 'Robustness evaluation against two image processing techniques,
    i.e., JPEG compression and Gaussian Blurs. Test set: CASIAv1+. MVSS-Net++ (wo
    aug) indicates training with JPEG compression and Gaussian blur excluded from
    data augmentation. The proposed models are more robust than the baselines.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.66
  Name of the first author: Chengbo Dong
  Name of the last author: Xirong Li
  Number of Figures: 11
  Number of Tables: 8
  Number of authors: 5
  Paper title: 'MVSS-Net: Multi-View Multi-Scale Supervised Networks for Image Manipulation
    Detection'
  Publication Date: 2022-06-07 00:00:00
  Table 1 caption: TABLE 1 A Taxonomy of the State-of-the-Art for Image Manipulation
    Detection
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Two Training Sets and Six Test Sets Used in Our Experiments
  Table 3 caption: TABLE 3 Performance of Different Semantic Segmentation Backbones,
    Trained With the Segmentation Loss Only
  Table 4 caption: TABLE 4 Ablation Study of MVSS-Net
  Table 5 caption: TABLE 5 Performance on Pixel-Level Manipulation Detection
  Table 6 caption: TABLE 6 Performance on Image-Level Manipulation Detection
  Table 7 caption: TABLE 7 Overall Performance Measured by Com-F1, the Harmonic Mean
    of Pixel-Level F1 and Image-Level F1, on Five Test Sets
  Table 8 caption: TABLE 8 Model Inference Speed, Tested on Two NVIDA GPU Cards Respectively
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3180556
- Affiliation of the first author: school of electrical and information engineering,
    tianjin university, tianjin, china
  Affiliation of the last author: terminus group, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\SipMaskv_Enhanced_Fast_Image_and_Video_Instance_Segmentation\figure_1.jpg
  Figure 1 caption: Some instance segmentation results of YOLACT [3] (top) and our
    SipMask (bottom). YOLACT fails to perform an accurate delineation of adjacent
    instances. To address this issue, SipMask introduces a novel spatial preservation
    (SP) module that splits mask prediction into multiple sub-region mask predictions,
    which can preserve spatial information in bounding-box and lead to the improved
    mask prediction.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\SipMaskv_Enhanced_Fast_Image_and_Video_Instance_Segmentation\figure_2.jpg
  Figure 2 caption: "Mask R-CNN [27] in (a) and FCIS [37] in (b) perform mask prediction\
    \ by feature pooling strategies (i.e., RoIAlign and PSRoI) that resize the feature\
    \ map of the proposal to a fixed resolution. Instead of requiring any pooling\
    \ operations, YOLACT [3] in (c) and our SipMask in (d) perform mask prediction\
    \ by a simple linear combination of coefficients and basis masks. Mask R-CNN [27]\
    \ is computationally expensive, including several convolutional and deconvolutional\
    \ operations after the RoIAlign pooling operation, whereas FCIS [37] is memory\
    \ demanding where the PSRoI pooling operation generates position-sensitive maps\
    \ with a large number of channels. YOLACT [3] and our SipMask reduce the computational\
    \ and memory complexity. However, YOLACT predicts a single set of coefficients\
    \ for each detected bounding-box, leading to the loss of spatial information within\
    \ the bounding-box. Compared to YOLACT, our SipMask preserves the instance spatial\
    \ information by predicting a separate set of spatial coefficients for k\xD7k\
    \ sub-regions within the bounding-box."
  Figure 3 Link: articels_figures_by_rev_year\2022\SipMaskv_Enhanced_Fast_Image_and_Video_Instance_Segmentation\figure_3.jpg
  Figure 3 caption: (a) Overall architecture of our SipMask, which is built on anchor-free
    detector FCOS. Our SipMask replaces the original classification and regression
    branches by our mask-specialized classification (Section 3.1) and regression (Section
    3.2) branches. The focus of our design is a novel spatial preservation (SP) module
    in the mask-specialized classification branch. The SP module performs spatial
    coefficients generation and feature alignment. Specifically, the SP module generates
    a separate set of spatial coefficients for multiple sub-regions to preserve the
    spatial information within the bounding-box. Therefore, it can better delineate
    spatially adjacent instances. The mask-specialized regression branch predicts
    the bounding-box offsets and the basis masks. To capture contextual information,
    the basis masks are generated by combining multiple layers of FPN. (b) Spatial
    mask prediction. With the basis masks, spatial coefficients, and the regressed
    bounding-boxes, our spatial mask prediction (SMP) module (Section 3.3) performs
    sub-region mask predictions and final instance mask prediction.
  Figure 4 Link: articels_figures_by_rev_year\2022\SipMaskv_Enhanced_Fast_Image_and_Video_Instance_Segmentation\figure_4.jpg
  Figure 4 caption: Visual comparison between mask generation using (a) a single set
    of coefficients in YOLACT and (b) the separate set of coefficients in our SipMask.
    For simplicity, we only show one detected cat instance and its mask prediction
    procedure. In (a), a linear combination of basis masks and single set of coefficients
    leads to one predicted map M j . After that, the map M j is first pruned and then
    thresholding to produce the final mask M ~ j . Instead, our SipMask in (b) generates
    a separate set of spatial coefficients for each sub-region (quadrant for k=2 )
    within a bounding-box. Then, multiple predicted maps M ij ,i=1,2,3,4 are obtained
    for the bounding-box j . After that, these maps are first pruned and then integrated
    (a simple addition) followed by thresholding to obtain final mask M ~ j . Compared
    to YOLACT, our SipMask can better delineate the adjacent instances (cat), resulting
    in improved mask prediction.
  Figure 5 Link: articels_figures_by_rev_year\2022\SipMaskv_Enhanced_Fast_Image_and_Video_Instance_Segmentation\figure_5.jpg
  Figure 5 caption: A conceptual illustration of center-distance based criteria employed
    in single-stage methods, such as SipMask and CondInst, for mask training. An example
    skateboard is shown with ground-truth (GT) (black rectangle) along with GT center
    (black circle). During training, it utilizes the black circle along with its neighboring
    samples at one pixel distance (red circles) as mask training samples. This implies
    that the box, B1, regressed by one of the red points is used for mask training
    despite likely having a lower overlap with the GT. On the other hand, B2 regressed
    by the blue circle and having a higher overlap with the GT may not be used during
    training, since blue circle is not within one pixel distance from the GT center.
    During inference, boxes like B2 having a higher overlap with the object and fewer
    irrelevant background are likely assigned with higher classification scores. Consequently,
    boxes such as B2 will be selected by the NMS despite such samples not been used
    for mask training. As a result, there is likely to be a mismatch between training
    and inference samples, leading to inferior mask prediction.
  Figure 6 Link: articels_figures_by_rev_year\2022\SipMaskv_Enhanced_Fast_Image_and_Video_Instance_Segmentation\figure_6.jpg
  Figure 6 caption: Overall architecture of our SipMask for single-stage video instance
    segmentation. A tracking branch is added in parallel to mask-specialized classification
    and regression branches, including classification, regression, and segmentation.
    The center features of regressed bounding-boxes are extracted as the tracking
    features.
  Figure 7 Link: articels_figures_by_rev_year\2022\SipMaskv_Enhanced_Fast_Image_and_Video_Instance_Segmentation\figure_7.jpg
  Figure 7 caption: Qualitative results of our SipMaskv2 on MS COCO test-dev set [39].
    Different objects are represented by different colors in each image. Our SipMaskv2
    generates high quality instance segmentation masks in challenging scenarios, such
    as large scale variations and crowded scenes.
  Figure 8 Link: articels_figures_by_rev_year\2022\SipMaskv_Enhanced_Fast_Image_and_Video_Instance_Segmentation\figure_8.jpg
  Figure 8 caption: Qualitative results highlighting the spatial delineation capabilities
    of our spatial preservation (SP) module. Input image with a detected bounding-box
    (red) is shown in column 1 and 4. Mask prediction obtained by the baseline that
    is based on a single set of coefficients is shown in column 2 and 5. Mask prediction
    obtained by our approach that is based on a separate set of spatial coefficients
    in a bounding-box is shown in column 3 and 6. Compared to the baseline, our approach
    is able to better delineate spatially adjacent object instances.
  Figure 9 Link: articels_figures_by_rev_year\2022\SipMaskv_Enhanced_Fast_Image_and_Video_Instance_Segmentation\figure_9.jpg
  Figure 9 caption: Qualitative results on example frames of different videos from
    Youtube-VIS validation set [71]. Across different frames, the object withsame
    predicted identity has same color.
  First author gender probability: 0.79
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Jiale Cao
  Name of the last author: Ling Shao
  Number of Figures: 9
  Number of Tables: 7
  Number of authors: 6
  Paper title: 'SipMaskv2: Enhanced Fast Image and Video Instance Segmentation'
  Publication Date: 2022-06-08 00:00:00
  Table 1 caption: TABLE 1 Comparison With State-of-the-Art Instance Segmentation
    Methods on MS COCO test-dev Set
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Impact of Progressively Integrating (from left to right)
    the Different Components Into the Baseline
  Table 3 caption: TABLE 3 Impact of Individually Integrating the Different Components
    Into the Baseline
  Table 4 caption: TABLE 4 Impact of Varying the Number of Sub-Regions for Spatial
    Coefficients Generation
  Table 5 caption: TABLE 5 Impact of Classification (Class Confidences) and Localization
    (Ground-Truth Overlap) Scores on Our Mask Alignment Weighting Loss
  Table 6 caption: TABLE 6 Integrating Two Modules in Our SipMaskv2, Including Sample
    Selection Scheme (SS) and Instance Refinement Module (IRM), Into the State-of-the-Art
    CondInst [56]
  Table 7 caption: TABLE 7 Comparison With Some State-of-the-Art Video Instance Segmentation
    Methods on YouTube-VIS Validation Set
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3180564
- Affiliation of the first author: trustworthy machine learning lab, school of computer
    science, faculty of engineering, university of sydney, camperdown, nsw, australia
  Affiliation of the last author: trustworthy machine learning lab, school of computer
    science, faculty of engineering, university of sydney, camperdown, nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2022\Extended_T_T_Learning_With_Mixed_ClosedSet_and_OpenSet_Noisy_Labels\figure_1.jpg
  Figure 1 caption: "The illustration of the cluster-dependent extended transition\
    \ matrices. We first conduct clustering on deep representations of training examples\
    \ and obtain different clusters (the top of this figure). Then the proposed method\
    \ learns the transition matrix for different clusters and extends the traditional\
    \ transition matrix to be able to model mixed label noise (the bottom of this\
    \ figure). The transition matrices T i and T \u2218 i are concatenated vertically\
    \ to form the extend transition matrix T \u22C6 i ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Extended_T_T_Learning_With_Mixed_ClosedSet_and_OpenSet_Noisy_Labels\figure_2.jpg
  Figure 2 caption: Illustrations of the transition matrix estimation errors. Figure
    (a) illustrates the estimation error for modeling open-set label noise. Figure
    (b) illustrates the estimation error for modeling mixed label noise. The error
    bar for standard deviation in each figure has been shaded.
  Figure 3 Link: articels_figures_by_rev_year\2022\Extended_T_T_Learning_With_Mixed_ClosedSet_and_OpenSet_Noisy_Labels\figure_3.jpg
  Figure 3 caption: Illustrations of the visualization results on the CIFAR-10+SVHN
    dataset with class-dependent closed-set noise.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xiaobo Xia
  Name of the last author: Tongliang Liu
  Number of Figures: 3
  Number of Tables: 8
  Number of authors: 7
  Paper title: 'Extended T T: Learning With Mixed Closed-Set and Open-Set Noisy Labels'
  Publication Date: 2022-06-08 00:00:00
  Table 1 caption: TABLE 1 Mean and Standard Deviations of Test Accuracies (%) on
    CIFAR-10+SVHN With Class-Dependent Closed-Set Noise
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Mean and Standard Deviations of Test Accuracies (%) on
    CIFAR-10+Gaussian With Class-Dependent Closed-Set Noise
  Table 3 caption: TABLE 3 Mean and Standard Deviations of Test Accuracies (%) on
    CIFAR-10+SVHN With Instance-Dependent Closed-Set Noise
  Table 4 caption: TABLE 4 Mean and Standard Deviations of Test Accuracies (%) on
    CIFAR-10+Gaussian With Instance-Dependent Closed-Set Noise
  Table 5 caption: TABLE 5 Test Accuracies (%) of Different Methods Training on Clothing1M
  Table 6 caption: TABLE 6 Face Datasets for Training and Testing
  Table 7 caption: TABLE 7 Test Accuracies (%) of Different Methods Training on VggFace-2
  Table 8 caption: TABLE 8 Test Accuracies (%) of Different Methods Training on MS1MV0
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3180545
- Affiliation of the first author: school of electronics and communication engineering,
    the shenzhen campus, sun yat-sen university, shenzhen, guangdong, china
  Affiliation of the last author: school of electronics and communication engineering,
    the shenzhen campus, sun yat-sen university, shenzhen, guangdong, china
  Figure 1 Link: articels_figures_by_rev_year\2022\You_Only_Train_Once_Learning_General_and_Distinctive_D_Local_Descriptors\figure_1.jpg
  Figure 1 caption: The Feature Matching Recall (FMR) scores of different approaches
    on the indoor 3DMatch [1] and outdoor ETH [15] datasets. Note that, all methods
    are trained only on the 3DMatch dataset. Our method not only achieves the highest
    score on 3DMatch, but also has the best generalization ability across the unseen
    ETH dataset. .
  Figure 10 Link: articels_figures_by_rev_year\2022\You_Only_Train_Once_Learning_General_and_Distinctive_D_Local_Descriptors\figure_10.jpg
  Figure 10 caption: Qualitative results achieved by the proposed patch-based SpinNet
    and fragment-based SpinNet. The detailed network architecture can be found in
    Table 11 according to the bottom index. Note that, the first and third rows represent
    the descriptors in 3D space with PCA (first three components are RGB color-coded)
    on the 3DMatch and ETH datasets, respectively. The second and fourth rows represent
    the distance errors from the estimated corresponding points (matched by the source
    point cloud in feature space) and the ground-truth corresponding points on the
    3DMatch and ETH datasets, respectively.
  Figure 2 Link: articels_figures_by_rev_year\2022\You_Only_Train_Once_Learning_General_and_Distinctive_D_Local_Descriptors\figure_2.jpg
  Figure 2 caption: The detailed components and processing steps of our Spatial Point
    Transformer.
  Figure 3 Link: articels_figures_by_rev_year\2022\You_Only_Train_Once_Learning_General_and_Distinctive_D_Local_Descriptors\figure_3.jpg
  Figure 3 caption: Illustration of the proposed Neural Feature Extractor.
  Figure 4 Link: articels_figures_by_rev_year\2022\You_Only_Train_Once_Learning_General_and_Distinctive_D_Local_Descriptors\figure_4.jpg
  Figure 4 caption: Visualization of rotation equivariance and invariance while the
    network has not been trained.
  Figure 5 Link: articels_figures_by_rev_year\2022\You_Only_Train_Once_Learning_General_and_Distinctive_D_Local_Descriptors\figure_5.jpg
  Figure 5 caption: Detailed architecture of our 3DCCN. Here, 3DCConv denotes the
    3D cylindrical convolution.
  Figure 6 Link: articels_figures_by_rev_year\2022\You_Only_Train_Once_Learning_General_and_Distinctive_D_Local_Descriptors\figure_6.jpg
  Figure 6 caption: Rotation distributions of different datasets. (a) 3DMatch and
    rotated 3DMatch. (b) rotation distributions of the 3DMatch, KITTI and ETH datasets.
    Note that, for all paired fragments in each dataset, their ground-truth rotations
    are transformed into three Euler angles for plotting.
  Figure 7 Link: articels_figures_by_rev_year\2022\You_Only_Train_Once_Learning_General_and_Distinctive_D_Local_Descriptors\figure_7.jpg
  Figure 7 caption: Feature matching recall on the 3DMatch dataset under different
    inlier distance threshold tau 1 (Left) and inlier ratio threshold tau 2 (Right).
  Figure 8 Link: articels_figures_by_rev_year\2022\You_Only_Train_Once_Learning_General_and_Distinctive_D_Local_Descriptors\figure_8.jpg
  Figure 8 caption: Qualitative results of our method on the cross-dataset generalization
    experiment. The first row is from KITTI to 3DMatch, the second row is from 3DMatch
    to KITTI, and the third row is from 3DMatch to ETH.
  Figure 9 Link: articels_figures_by_rev_year\2022\You_Only_Train_Once_Learning_General_and_Distinctive_D_Local_Descriptors\figure_9.jpg
  Figure 9 caption: The architectures of our patch-based SpinNet and fragment-based
    SpinNet. mathcal C is the number of points within the support radius R . N is
    the number of points in a point cloud fragment.
  First author gender probability: 0.76
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Sheng Ao
  Name of the last author: Zengping Chen
  Number of Figures: 10
  Number of Tables: 11
  Number of authors: 6
  Paper title: 'You Only Train Once: Learning General and Distinctive 3D Local Descriptors'
  Publication Date: 2022-06-09 00:00:00
  Table 1 caption: TABLE 1 The Hyperparameter Settings of Our SpinNet on Different
    Datasets
  Table 10 caption: TABLE 10 Quantitative Results Achieved by Our SpinNet Under Different
    Settings on Three Typical Datasets
  Table 2 caption: TABLE 2 Quantitative Results on the 3DMatch Dataset
  Table 3 caption: TABLE 3 Quantitative Results on the 3DMatch and 3DLoMatch Datasets
    Using Different Numbers of Sampled Keypoints
  Table 4 caption: TABLE 4 Quantitative Results of Different Approaches on the KITTI
    Odometry Dataset
  Table 5 caption: TABLE 5 Quantitative Results on the ETH Dataset
  Table 6 caption: TABLE 6 Quantitative Results on the KITTI Dataset
  Table 7 caption: TABLE 7 Quantitative Results of Different Methods on the Indoor
    3DMatch Dataset
  Table 8 caption: TABLE 8 Quantitative Results on the ETH Dataset
  Table 9 caption: "TABLE 9 The FMR Scores of All Ablated Models on the 3DMatch and\
    \ ETH Datasets With \u03C4 1 \u03C41 = 0.1 cm. Tran. Indicates the Transformation\
    \ on the XY-Plane"
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3180341
