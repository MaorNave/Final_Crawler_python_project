- Affiliation of the first author: department of informatics, technical university
    of munich, munich, germany
  Affiliation of the last author: inria, grenoble, france
  Figure 1 Link: articels_figures_by_rev_year\2019\Generic_Primitive_Detection_in_Point_Clouds_Using_Novel_Minimal_Quadric_Fits\figure_1.jpg
  Figure 1 caption: (a) Number of constraints for a minimal fit in Primal(P) or Dual(D)
    spaces. PD-i refers to ith combination. (b) Number of minimal constraints and
    voting space size for various quadrics.
  Figure 10 Link: articels_figures_by_rev_year\2019\Generic_Primitive_Detection_in_Point_Clouds_Using_Novel_Minimal_Quadric_Fits\figure_10.jpg
  Figure 10 caption: (i) Images captured by an industrial structured light sensor
    and Kinect (last image). (ii) 3D scene. (iii) Detected quadric, shown without
    clipping. (iv) Quadric in (iii) clipped to the points it lies on.
  Figure 2 Link: articels_figures_by_rev_year\2019\Generic_Primitive_Detection_in_Point_Clouds_Using_Novel_Minimal_Quadric_Fits\figure_2.jpg
  Figure 2 caption: 'Illustration of the geometric intuition. (left) Visualizations
    on a sample quadric: the selected basis (three-oriented points); the data-plane;
    the conic of intersection between data-plane and the quadric; the lines on the
    data-plane that are tangent to the quadric. (right) Exemplary drawing that shows
    that the tangent planes to the basis points meet at the pole (see text).'
  Figure 3 Link: articels_figures_by_rev_year\2019\Generic_Primitive_Detection_in_Point_Clouds_Using_Novel_Minimal_Quadric_Fits\figure_3.jpg
  Figure 3 caption: Once a basis is randomly hypothesized, we look for the points
    on the same surface by casting votes on the null-space. The sought pilates ball
    (likely quadric) is marked on the image and below that lies the corresponding
    filled accumulator by KDE [55].
  Figure 4 Link: articels_figures_by_rev_year\2019\Generic_Primitive_Detection_in_Point_Clouds_Using_Novel_Minimal_Quadric_Fits\figure_4.jpg
  Figure 4 caption: "Effect of \u03BB on the surface geometry. We compute null-space\
    \ decomposition for a fixed basis and vary \u03BB from -75 to 75 to generate different\
    \ solutions q along the line in the solution space. The plot presents the transition\
    \ of the surface controlled by \u03BB ."
  Figure 5 Link: articels_figures_by_rev_year\2019\Generic_Primitive_Detection_in_Point_Clouds_Using_Novel_Minimal_Quadric_Fits\figure_5.jpg
  Figure 5 caption: Synthetic evaluations. The plot depicts mean geometric errors
    on points (a) and mean angular errors (b) for different quadric fitting methods.
    The per point error is measured as the average point-to-mesh distance from every
    ground truth vertex to the fitted quadric. The angular error (dashed) is computed
    as the negated dot product between quadric gradient and the ground truth normal.
    Moreover, (c) shows the average error of the gradient norm compared to the ground
    truth and (d) gives speed and detection rate on synthetic data.
  Figure 6 Link: articels_figures_by_rev_year\2019\Generic_Primitive_Detection_in_Point_Clouds_Using_Novel_Minimal_Quadric_Fits\figure_6.jpg
  Figure 6 caption: "a (left). Effect of extended point neighborhood to the fitting.\
    \ b (right). Statistical distribution of the solution-space coefficient and our\
    \ quantization function: PDF (red curve) and inverse CDF (dashed blue-curve) of\
    \ \u03BB over collected data, and ta n \u22121 function (green-line). Note that\
    \ our quantization function is capable of explaining the empirical data."
  Figure 7 Link: articels_figures_by_rev_year\2019\Generic_Primitive_Detection_in_Point_Clouds_Using_Novel_Minimal_Quadric_Fits\figure_7.jpg
  Figure 7 caption: Synthetic tests at various noise levels for different fitting
    methods. Gaussian noise is added to the point coordinates as well as the estimated
    normal. The standard deviation varies from 0.11 percent of the visible quadric
    size to 10 percent.
  Figure 8 Link: articels_figures_by_rev_year\2019\Generic_Primitive_Detection_in_Point_Clouds_Using_Novel_Minimal_Quadric_Fits\figure_8.jpg
  Figure 8 caption: "a (left). Effect of the weight w on the quality of the fit (Algorithm\
    \ 2). b (right). Voting spaces due to different bases selection ( \u03B8 vs votes)."
  Figure 9 Link: articels_figures_by_rev_year\2019\Generic_Primitive_Detection_in_Point_Clouds_Using_Novel_Minimal_Quadric_Fits\figure_9.jpg
  Figure 9 caption: Qualitative evaluation of surface normals. Randomly generated
    quadrics are used as ground truth and fitting is performed. The estimation results
    of gradient magnitude and angle (phase) is color coded on the surface. For color
    selection, we use a jet-like temperature map for the gradient magnitude, where
    blue denotes the lowest and red denotes the highest magnitudes. For the phase,
    an angular map as show in the color-bar is used. The ideal case is given in ground-truth
    against which the methods compete.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Tolga Birdal
  Name of the last author: Peter Sturm
  Number of Figures: 14
  Number of Tables: 2
  Number of authors: 5
  Paper title: Generic Primitive Detection in Point Clouds Using Novel Minimal Quadric
    Fits
  Publication Date: 2019-02-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Detection Accuracy on Real Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Results on ITODD [63] Cylinders: Even without Looking for
      a Cylinder, We Outperform the Model Based [65]'
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2900309
- Affiliation of the first author: school of mechano-electronic engineering, xidian
    university, xian, china
  Affiliation of the last author: school of electrical and information engineering,
    the university of sydney, sydney, australia
  Figure 1 Link: articels_figures_by_rev_year\2019\Synthesizing_Supervision_for_Learning_Deep_Saliency_Network_without_Human_Annota\figure_1.jpg
  Figure 1 caption: Examples of the knowledge cues used for synthesizing supervision
    for training the proposed annotation-free deep salient object detector. Note that
    in each learning stage, we only show one example from the external knowledge source
    and the internal knowledge source, respectively. From this figure, we observe
    that knowledge cues from both knowledge sources can highlight certain parts of
    the object of interest and they can be improved gradually. When compared with
    the internal cues, the external cues might better cover the ground-truth mask
    at early learning stages. When compared with the external cues, the internal cues
    might better cover the ground-truth mask at the later learning stages. The internal
    cues at the first learning stage are less informative as the network is not trained.
  Figure 10 Link: articels_figures_by_rev_year\2019\Synthesizing_Supervision_for_Learning_Deep_Saliency_Network_without_Human_Annota\figure_10.jpg
  Figure 10 caption: Visualization of the semantic segmentation masks generated from
    our annotation-free deep salient object detector and the fully supervised one
    [71]. Our approach can achieve comparable or even better performance for segmenting
    details of objects (see the bicycle tires in the first row) and separating objects
    that are close to each other (see the last row).
  Figure 2 Link: articels_figures_by_rev_year\2019\Synthesizing_Supervision_for_Learning_Deep_Saliency_Network_without_Human_Annota\figure_2.jpg
  Figure 2 caption: "Illustration of the proposed supervision synthesis framework.\
    \ It mainly contains a \u201Cknowledge source transition\u201D mechanism and a\
    \ \u201Csupervision by fusion\u201D mechanism. The former one is used to generate\
    \ informative knowledge cues for synthesizing supervision, while the later one\
    \ is used to synthesize the supervisory signals, i.e., the superpixel-level fusion\
    \ map, the image-level fusion map, and the learning confidence map, to train the\
    \ deep salient object detection network. The whole learning scheme is performed\
    \ iteratively in order to progressively update the extracted knowledge cues and\
    \ improve the synthesized supervisory signals. The pink, blue, gray, purple, and\
    \ green layers in the network indicate the convolutional layer, max-pooling layer,\
    \ fully connected layer, up-sampling layer, and recurrent convolutional layer,\
    \ respectively. Please refer to Section 3.3.1 for more details about the network\
    \ architecture."
  Figure 3 Link: articels_figures_by_rev_year\2019\Synthesizing_Supervision_for_Learning_Deep_Saliency_Network_without_Human_Annota\figure_3.jpg
  Figure 3 caption: Illustration of the intra-image fusion process (a) and the inter-image
    fusion process (b). The basic computational unit of the intra-image fusion process
    is each superpixel region and the fusion process is performed separately within
    each single image. It generates the superpixel-level confidence map and superpixel-level
    fusion map. The basic computational unit of the inter-image fusion process is
    each image and the fusion process is performed on the entire collection of training
    images. It generates the image confidence scores and the image-level fusion map.
  Figure 4 Link: articels_figures_by_rev_year\2019\Synthesizing_Supervision_for_Learning_Deep_Saliency_Network_without_Human_Annota\figure_4.jpg
  Figure 4 caption: Comparison the PR curves between our approach and the conventional
    unsupervised salient object detection networks on four benchmark datasets for
    salient object detection.
  Figure 5 Link: articels_figures_by_rev_year\2019\Synthesizing_Supervision_for_Learning_Deep_Saliency_Network_without_Human_Annota\figure_5.jpg
  Figure 5 caption: Comparison the PR curves between our approach and the state-of-the-art
    fully supervised deep salient object detection networks on four benchmark datasets
    for salient object detection.
  Figure 6 Link: articels_figures_by_rev_year\2019\Synthesizing_Supervision_for_Learning_Deep_Saliency_Network_without_Human_Annota\figure_6.jpg
  Figure 6 caption: Visualization examples of the saliency maps obtained by the proposed
    annotation-free learning framework as well as the other state-of-the-art supervised
    methods.
  Figure 7 Link: articels_figures_by_rev_year\2019\Synthesizing_Supervision_for_Learning_Deep_Saliency_Network_without_Human_Annota\figure_7.jpg
  Figure 7 caption: Evaluation of different knowledge cues and the transition scheme
    used in the proposed knowledge source transition mechanism on four saliency detection
    datasets in terms of the PR curve.
  Figure 8 Link: articels_figures_by_rev_year\2019\Synthesizing_Supervision_for_Learning_Deep_Saliency_Network_without_Human_Annota\figure_8.jpg
  Figure 8 caption: "Evaluation of the supervisory signals generated by the proposed\
    \ \u201Csupervision by fusion\u201D mechanism on four saliency detection datasets\
    \ in terms of the PR curve."
  Figure 9 Link: articels_figures_by_rev_year\2019\Synthesizing_Supervision_for_Learning_Deep_Saliency_Network_without_Human_Annota\figure_9.jpg
  Figure 9 caption: Visualization of the saliency maps generated at different learning
    stages. With more learning stages, our approach can achieve better results for
    localizing the objects of interest and recognizing the detailed boundaries.
  First author gender probability: 0.67
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Dingwen Zhang
  Name of the last author: Dong Xu
  Number of Figures: 10
  Number of Tables: 5
  Number of authors: 4
  Paper title: Synthesizing Supervision for Learning Deep Saliency Network without
    Human Annotation
  Publication Date: 2019-02-20 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Comparison between Our Approach and Other State-of-the-Art\
      \ Methods on Four Saliency Detection Datasets in Terms of AP, F \u03B9 F\u03B9\
      , SOV (Higher Values Indicate Better Results), and MAE (Lower Values Indicate\
      \ Better Results)"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Evaluation of the Knowledge Source Transition Mechanism and\
      \ the \u201CSupervision by Fusion\u201D Mechanism on Four Saliency Detection\
      \ Datasets in Terms of AP, F \u03B9 F\u03B9, SOV (Higher Values Indicate Better\
      \ Results), and MAE (Lower Values Indicate Better Results)"
  Table 3 caption:
    table_text: TABLE 3 Results of Our Proposed Framework by Using Different Unsupervised
      Salient Object Detectors (i.e., MBS [17], HS [29], wCtr [14] and BMS [56]) for
      Extracting the External Knowledge Cues
  Table 4 caption:
    table_text: "TABLE 4 Evaluation of the Iterative Learning Scheme at Different\
      \ Learning Stages on Four Saliency Detection Datasets in Terms of AP, F \u03B9\
      \ F\u03B9, SOV (Higher Values Indicate Better Results), and MAE (Lower Values\
      \ Indicate Better Results)"
  Table 5 caption:
    table_text: TABLE 5 Comparison between Our Approach and the State-of-the-Art Weakly
      Supervised Semantic Segmentation Methods on the Pascal VOC 2012 Datasets (Both
      the Validation and Test Set)
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2900649
- Affiliation of the first author: massachusetts institute of technology, cambridge,
    usa
  Affiliation of the last author: massachusetts institute of technology, cambridge,
    usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Moments_in_Time_Dataset_One_Million_Videos_for_Event_Understanding\figure_1.jpg
  Figure 1 caption: Sample videos. Day-to-day events can happen to many types of actors,
    in different environments, and at different scales. Moments in Time dataset has
    a significant intra-class variation among the categories. Here we illustrate one
    frame for a few video samples and actions. For example, car engines, books, and
    tulips can all open.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Moments_in_Time_Dataset_One_Million_Videos_for_Event_Understanding\figure_2.jpg
  Figure 2 caption: User interface. An example for our binary annotation task for
    the action cooking.
  Figure 3 Link: articels_figures_by_rev_year\2019\Moments_in_Time_Dataset_One_Million_Videos_for_Event_Understanding\figure_3.jpg
  Figure 3 caption: 'Dataset statistics. Left: Distribution of the number of videos
    belonging to each category. Middle: Per class distribution of videos that have
    humans, animals, or objects as agents completing actions. Right: Per class distribution
    of videos that require audio to recognize the class category and videos that can
    be categorized with only visual information.'
  Figure 4 Link: articels_figures_by_rev_year\2019\Moments_in_Time_Dataset_One_Million_Videos_for_Event_Understanding\figure_4.jpg
  Figure 4 caption: 'Comparison to datasets. For each dataset we provide different
    comparisons. Left: The total number of action labels in the training set. Middle:
    The average number of videos per class (some videos can belong to multiple classes).
    Right: The coverage of objects and scenes recognized (top 1) by networks trained
    on Places and Imagenet.'
  Figure 5 Link: articels_figures_by_rev_year\2019\Moments_in_Time_Dataset_One_Million_Videos_for_Event_Understanding\figure_5.jpg
  Figure 5 caption: Overview of top detections for several single stream models. The
    ground truth label and top three model predictions are listed for representative
    frames of videos.
  Figure 6 Link: articels_figures_by_rev_year\2019\Moments_in_Time_Dataset_One_Million_Videos_for_Event_Understanding\figure_6.jpg
  Figure 6 caption: 'Examples of missed detections: We show examples of videos where
    the prediction is not in the top-5. Common failures are often due to background
    clutter or poor generalization across agents (humans, animals, objects).'
  Figure 7 Link: articels_figures_by_rev_year\2019\Moments_in_Time_Dataset_One_Million_Videos_for_Event_Understanding\figure_7.jpg
  Figure 7 caption: 'Predictions and attention: We show some predictions (shown with
    class probability in top left corner) from ResNet50-ImageNet spatial model on
    held-out video data and the heatmaps which highlight the informative regions in
    some frames. For example, for recognizing the action chewing, the network focuses
    on the moving mouth.'
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.99
  Name of the first author: Mathew Monfort
  Name of the last author: Aude Oliva
  Number of Figures: 7
  Number of Tables: 2
  Number of authors: 11
  Paper title: 'Moments in Time Dataset: One Million Videos for Event Understanding'
  Publication Date: 2019-02-24 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Classification Accuracy: We Show Top-1 and Top-5 Accuracy
      of the Baseline Models on the Validation Set'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Dataset Transfer Performance Using ResNet50 I3D Models Pretrained
      on Both Kinetics and Moments in Time
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2901464
- Affiliation of the first author: pillar of information systems technology and design,
    singapore university of technology and design, singapore
  Affiliation of the last author: graduate school of information science and technology,
    osaka university, osaka, japan
  Figure 1 Link: articels_figures_by_rev_year\2019\AmbiguityFree_Radiometric_Calibration_for_Internet_Photo_Collections\figure_1.jpg
  Figure 1 caption: "Pipeline of our method. We estimate the (inverse) radiometric\
    \ response functions for each nonlinear image g ~ 1 , g ~ 2 ,\u2026, g ~ Q by\
    \ rank minimization over the stacks of pixel pairs up to a unified exponential\
    \ ambiguity. The \u2218 operator applies each intermediate inverse response function\
    \ g ~ to both the numerator and denominator of ratio terms in the same row. The\
    \ correct g ~ transforms each row of the matrix to the same vector (up to a scale)\
    \ to make the matrix rank-1. Then, the nonlinear image with highest quality from\
    \ the photo collection is selected to extract patches. Only inlier patches are\
    \ kept to find edge color triplets, among which the triplet with linear distribution\
    \ tells the correct value of \u03B3 for the unified exponential ambiguity above.\
    \ Finally, we obtain inverse radiometric response functions for all photos free\
    \ from ambiguity as g 1 , g 2 ,\u2026, g Q ."
  Figure 10 Link: articels_figures_by_rev_year\2019\AmbiguityFree_Radiometric_Calibration_for_Internet_Photo_Collections\figure_10.jpg
  Figure 10 caption: "Estimated inverse radiometric response functions (denoted with\
    \ \u201CW\u201D, Algorithm 1 and Algorithm 2) using an image collection mixed\
    \ with Internet photos and captured images (in red box). The results compared\
    \ with the intermediate results generated by Algorithm 1 (aligned with ground\
    \ truth and denoted with \u201CP\u201D), \u201CDiaz13\u201D [33] and \u201CLin04\u201D\
    \ [13] on RGB channels of three images and calibrated ground truth are shown.\
    \ The RMSE and Disparity are in the legend of each plot."
  Figure 2 Link: articels_figures_by_rev_year\2019\AmbiguityFree_Radiometric_Calibration_for_Internet_Photo_Collections\figure_2.jpg
  Figure 2 caption: Compression (a raw image compressed at different levels) distorts
    the linear edge color blending property. Close-up views of part of the compressed
    image (in blue box; multiplied by 6 for better visualization) are demonstrated
    in the top row. The image patches below (in red box; multiplied by 6 for better
    visualization) show the distributions of edge color triplets in the RGB space
    (the three red dots are pixels of an edge color triplet with the green line as
    the corresponding edge).
  Figure 3 Link: articels_figures_by_rev_year\2019\AmbiguityFree_Radiometric_Calibration_for_Internet_Photo_Collections\figure_3.jpg
  Figure 3 caption: "Reliable edge color triplet is marked with the blue box and unreliable\
    \ ones are marked with red boxes, and their corresponding distance values (according\
    \ to Eq. (14)) varying with the searching of \u03B3 are plotted in the bottom\
    \ row. The reliable triplet observes a local minimum not appearing at the extreme\
    \ values."
  Figure 4 Link: articels_figures_by_rev_year\2019\AmbiguityFree_Radiometric_Calibration_for_Internet_Photo_Collections\figure_4.jpg
  Figure 4 caption: "The average RMSEdisparity w.r.t. the number of pixel pairs (row-wise)\
    \ and order of polynomials (column-wise). \u201CRed\u201D means larger and \u201C\
    blue\u201D means smaller errors."
  Figure 5 Link: articels_figures_by_rev_year\2019\AmbiguityFree_Radiometric_Calibration_for_Internet_Photo_Collections\figure_5.jpg
  Figure 5 caption: "Evaluation under various noise levels and comparison between\
    \ our method and Diaz13 [33]. The box-and-whisker plot shows the mean (indicated\
    \ as \u201C\xD7\u201D), median, the first and third quartile, and the minimum\
    \ and maximum values for RMSE and disparity for 200 ( 20\xD710 ) estimated response\
    \ functions."
  Figure 6 Link: articels_figures_by_rev_year\2019\AmbiguityFree_Radiometric_Calibration_for_Internet_Photo_Collections\figure_6.jpg
  Figure 6 caption: "Radiometric calibration results using a synthetic dataset. The\
    \ upper row shows the ground truth reflectance, shading images, and the ten selected\
    \ pixel pairs (yellow numbers) with the same normal but different albedo values.\
    \ Two example results of the estimated inverse response functions and the ground\
    \ truth curves are plotted, with the RMSE and disparity values shown in the legend.\
    \ The nonlinear observations (\u201CWith RF\u201D), linearized images (\u201C\
    With IRF\u201D) and their absolute difference maps w.r.t. the \u201COriginal\u201D\
    \ images are shown next to the inverse response curve plots."
  Figure 7 Link: articels_figures_by_rev_year\2019\AmbiguityFree_Radiometric_Calibration_for_Internet_Photo_Collections\figure_7.jpg
  Figure 7 caption: "Comparison of our method and \u201CLin04\u201D [13] with different\
    \ image qualities. Three examples (applied with different response functions in\
    \ R channel) are shown for each image quality level in each column. The RMSE and\
    \ Disparity are in the legend of each plot."
  Figure 8 Link: articels_figures_by_rev_year\2019\AmbiguityFree_Radiometric_Calibration_for_Internet_Photo_Collections\figure_8.jpg
  Figure 8 caption: "Estimated inverse radiometric response functions (denoted with\
    \ \u201CW\u201D, Algorithm 1 and Algorithm 2) using an image collection mixed\
    \ with Internet photos and captured images (in red box). The results compared\
    \ with the intermediate results generated by Algorithm 1 (aligned with ground\
    \ truth and denoted with \u201CP\u201D), \u201CDiaz13\u201D [33] and \u201CLin04\u201D\
    \ [13] on RGB channels of three images and calibrated ground truth are shown.\
    \ The RMSE and Disparity are in the legend of each plot."
  Figure 9 Link: articels_figures_by_rev_year\2019\AmbiguityFree_Radiometric_Calibration_for_Internet_Photo_Collections\figure_9.jpg
  Figure 9 caption: "Estimated inverse radiometric response functions (denoted with\
    \ \u201CW\u201D, Algorithm 1 and Algorithm 2) using an image collection mixed\
    \ with Internet photos and captured images (in red box). The results compared\
    \ with the intermediate results generated by Algorithm 1 (aligned with ground\
    \ truth and denoted with \u201CP\u201D), \u201CDiaz13\u201D [33] and \u201CLin04\u201D\
    \ [13] on RGB channels of three images and calibrated ground truth are shown.\
    \ The RMSE and Disparity are in the legend of each plot."
  First author gender probability: 0.96
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Zhipeng Mo
  Name of the last author: Yasuyuki Matsushita
  Number of Figures: 12
  Number of Tables: 1
  Number of authors: 4
  Paper title: Ambiguity-Free Radiometric Calibration for Internet Photo Collections
  Publication Date: 2019-02-24 00:00:00
  Table 1 caption:
    table_text: TABLE 1 A Unified Summarization of Various Radiometric Calibration
      Methods
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2901458
- Affiliation of the first author: key lab of intelligent perception and systems for
    high-dimensional information of ministry of education, jiangsu key lab of image
    and video understanding for social security, nanjing university of science and
    technology, nanjing, china
  Affiliation of the last author: key lab of intelligent perception and systems for
    high-dimensional information of ministry of education, jiangsu key lab of image
    and video understanding for social security, nanjing university of science and
    technology, nanjing, china
  Figure 1 Link: articels_figures_by_rev_year\2019\Adversarial_Learning_of_StructureAware_Fully_Convolutional_Networks_for_Landmark\figure_1.jpg
  Figure 1 caption: Motivation. We show the importance of strongly enforcing priors
    about the pose structure during training of DCNNs for pose estimation. Learning
    without using such priors generates inaccurate results.
  Figure 10 Link: articels_figures_by_rev_year\2019\Adversarial_Learning_of_StructureAware_Fully_Convolutional_Networks_for_Landmark\figure_10.jpg
  Figure 10 caption: 'Prediction samples on the MPII test set. First row: original
    images. Second row: results by stacked hourglass network (HG) [9]. Third row:
    results by our method. (a)-(c) show three different failure cases of HG.'
  Figure 2 Link: articels_figures_by_rev_year\2019\Adversarial_Learning_of_StructureAware_Fully_Convolutional_Networks_for_Landmark\figure_2.jpg
  Figure 2 caption: "Overview of the proposed Structure-aware Convolutional Network\
    \ for human pose estimation. The sub-network in purple is the stacked multi-task\
    \ network (G) for pose generation. The networks in blue (P) is used to discriminate\
    \ whether the generated pose is \u201Creal\u201D (reasonable as a body shape).\
    \ The loss of G has two parts: Mean Squared Error (MSE) of heatmaps (dashed line\
    \ in purple) and Binary Cross Entropy (BCE) adversarial loss from P (dashed line\
    \ in red). Standalone training of G produces results in the top-right. G and P\
    \ produce results at the bottom-right."
  Figure 3 Link: articels_figures_by_rev_year\2019\Adversarial_Learning_of_StructureAware_Fully_Convolutional_Networks_for_Landmark\figure_3.jpg
  Figure 3 caption: "Architecture of the multi-task generative network G. Black, orange,\
    \ blue and red rectangles indicate convolutional layers, residual blocks, max\
    \ pooling layers and hourglass blocks respectively. \u2295 indicates addition\
    \ of input features. Solid blue and green circles indicate pose and occlusion\
    \ losses in the network. The brief architecture of the hourglass block is shown\
    \ at the right. Stacking of the first and the second networks is displayed and\
    \ more networks can be stacked with the same structure."
  Figure 4 Link: articels_figures_by_rev_year\2019\Adversarial_Learning_of_StructureAware_Fully_Convolutional_Networks_for_Landmark\figure_4.jpg
  Figure 4 caption: Architectures of the 2D pose discriminator networks P and C. On
    the top we show the image for pose estimation, the image with estimated joints
    and heatmaps of right ankle, pelvis and neck (1st, 7th and 9th of all pose heatmaps
    respectively). The expected output for this sample is given at the bottom of the
    dashed box.
  Figure 5 Link: articels_figures_by_rev_year\2019\Adversarial_Learning_of_StructureAware_Fully_Convolutional_Networks_for_Landmark\figure_5.jpg
  Figure 5 caption: Structure of the 3D pose discriminator when a 16-joints pose is
    to be discriminated. In each black rectangle, a few modules are combined.
  Figure 6 Link: articels_figures_by_rev_year\2019\Adversarial_Learning_of_StructureAware_Fully_Convolutional_Networks_for_Landmark\figure_6.jpg
  Figure 6 caption: Quantitative results on the test set of the 300W competition (indoor
    and outdoor) for 68-point prediction. The point-to-point error is normalized by
    the inter-ocular distance.
  Figure 7 Link: articels_figures_by_rev_year\2019\Adversarial_Learning_of_StructureAware_Fully_Convolutional_Networks_for_Landmark\figure_7.jpg
  Figure 7 caption: Results of the discriminator network P for the task of facial
    landmark estimation. The samples are sorted from the highest NRMSE error to the
    lowest one. The discriminator scores of the top 100 samples are marked with the
    gray line. The medium 100 samples are marked with the orange line. The lowest
    100 samples are marked with the blue line.
  Figure 8 Link: articels_figures_by_rev_year\2019\Adversarial_Learning_of_StructureAware_Fully_Convolutional_Networks_for_Landmark\figure_8.jpg
  Figure 8 caption: Samples on the 300W test set. The four rows are results of MDM
    [2], CFSS [77], SDM[50] and our method respectively. After estimation by each
    method, the coordinates are projected to the original image. Then the images are
    cropped to make sure that all the estimated landmarks are within the displayed
    image, which results in different scales of the displayed images.
  Figure 9 Link: articels_figures_by_rev_year\2019\Adversarial_Learning_of_StructureAware_Fully_Convolutional_Networks_for_Landmark\figure_9.jpg
  Figure 9 caption: PCKh comparison on MPII validation set.
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Yu Chen
  Name of the last author: Jian Yang
  Number of Figures: 14
  Number of Tables: 8
  Number of authors: 6
  Paper title: Adversarial Learning of Structure-Aware Fully Convolutional Networks
    for Landmark Localization
  Publication Date: 2019-02-26 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparisons of Mean Error, AUC and Failure Rate (at a Threshold
      of 0.08 of the Normalized Error) on the 300W Test Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results on the AFLW Facial Landmark Detection Test Set
  Table 3 caption:
    table_text: TABLE 3 Comparisons of PCK0.2 Performance on the LSP Dataset
  Table 4 caption:
    table_text: TABLE 4 Results on MPII Human Pose (PCKh0.5)
  Table 5 caption:
    table_text: TABLE 5 Detection Rates of Visible and Invisible Elbows and Wrists
  Table 6 caption:
    table_text: TABLE 6 Results on the MSCOCO Keypoint Detection Test-Dev Set
  Table 7 caption:
    table_text: TABLE 7 Results on Human3.6M under Protocol 1 (No Rigid Alignment
      in Post-Processing)
  Table 8 caption:
    table_text: TABLE 8 Results on Human3.6M under Protocol 2 (Rigid Alignment in
      Post-Processing)
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2901875
- Affiliation of the first author: national key laboratory for novel software technology,
    nanjing university, nanjing, china
  Affiliation of the last author: national key laboratory for novel software technology,
    nanjing university, nanjing, china
  Figure 1 Link: articels_figures_by_rev_year\2019\Learning_Multiple_Local_Metrics_Global_Consideration_Helps\figure_1.jpg
  Figure 1 caption: "Illustration of Lift on a four-class dataset. Existing methods\
    \ learn 4 metrics, one for each class, to cover each locality (dashed ellipsoids\
    \ indicate the contours of the learned metrics). While Lift uses global metric\
    \ M 0 as a bridge to build biases \u0394 M k towards local specialties if needed.\
    \ It is notable that \u0394 M 3 and \u0394 M 4 are automatically discarded when\
    \ the corresponding localities are well expressed by the global metric M 0 , such\
    \ that local metrics in those local areas degenerate to the global one."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Learning_Multiple_Local_Metrics_Global_Consideration_Helps\figure_2.jpg
  Figure 2 caption: Illustration of the matrix diversity regularizer. After solving
    two matrices proximal problem in Eq. 12 with random input, we get diversified
    matrix outputs with different selected rows. Gray degree in the figure is proportional
    to the absolute value of a particular element.
  Figure 3 Link: articels_figures_by_rev_year\2019\Learning_Multiple_Local_Metrics_Global_Consideration_Helps\figure_3.jpg
  Figure 3 caption: Change of the classification performance and the number of output
    metrics when the initial number of local transformations in Lift increases (from
    1 to the double of the class number). The blue histogram and red plot show the
    variations of the average output metrics number and the mean error rates, respectively.
  Figure 4 Link: articels_figures_by_rev_year\2019\Learning_Multiple_Local_Metrics_Global_Consideration_Helps\figure_4.jpg
  Figure 4 caption: "Visualization effects on different datasets, i.e., \u201Caustral\u201D\
    , \u201Cinfant\u201D, \u201CHouse-V\u201D, \u201CHeart\u201D, and two synthetic\
    \ datasets. Different colors represent different classes. The black diamonds in\
    \ results of Lift denote the learned centers. For a certain dataset, Lift dynamically\
    \ outputs one or more projection results."
  Figure 5 Link: articels_figures_by_rev_year\2019\Learning_Multiple_Local_Metrics_Global_Consideration_Helps\figure_5.jpg
  Figure 5 caption: When input examples come from correlated subspaces, we show the
    change of learned number of metrics for Lift and test performance in plots (a)-(b),
    respectively. As the class number increases, the true number of subspaces increases,
    and the dataset becomes more complex. The dotted green line in plot (a) shows
    the number of classes, which is the initial number of local metrics in Lift.
  Figure 6 Link: articels_figures_by_rev_year\2019\Learning_Multiple_Local_Metrics_Global_Consideration_Helps\figure_6.jpg
  Figure 6 caption: (a)-(d) are visualizations of learned local metrics on four datasets.
    Scatter plots on the ceiling are the tSNE results of data. Peaks below show the
    location of learned centers, whose compactness (weights) come from local transformations.
  Figure 7 Link: articels_figures_by_rev_year\2019\Learning_Multiple_Local_Metrics_Global_Consideration_Helps\figure_7.jpg
  Figure 7 caption: (a)-(d) are comprehensibility effects of global and local metrics
    on text corpus. The size of the word is proportional to the feature weights of
    learned transformations.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.54
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.61
  Name of the first author: Han-Jia Ye
  Name of the last author: Yuan Jiang
  Number of Figures: 7
  Number of Tables: 1
  Number of authors: 4
  Paper title: 'Learning Multiple Local Metrics: Global Consideration Helps'
  Publication Date: 2019-02-26 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Comparisons of Classification Performance (Test Errors, mean\
      \ \xB1 \xB1 std.) Based on 3NN"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2901675
- Affiliation of the first author: stanford university, stanford, usa
  Affiliation of the last author: center for health sciences, sri international, menlo
    park, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Logistic_Regression_Confined_by_CardinalityConstrained_Sample_and_Feature_Select\figure_1.jpg
  Figure 1 caption: Our proposed logistic regression simultaneously selects samples
    and features to improve classification accuracy.
  Figure 10 Link: articels_figures_by_rev_year\2019\Logistic_Regression_Confined_by_CardinalityConstrained_Sample_and_Feature_Select\figure_10.jpg
  Figure 10 caption: Balanced accuracy (BAcc) of the proposed method with hybrid regularization
    as functions of the ratio of selected samples ( rho kappa ) and the ratio of selected
    features ( rho tau ).
  Figure 2 Link: articels_figures_by_rev_year\2019\Logistic_Regression_Confined_by_CardinalityConstrained_Sample_and_Feature_Select\figure_2.jpg
  Figure 2 caption: Schematic illustration of synthetic dataset. Six different sets
    are created with 0 to 50 percent of their rows (i.e., samples) and columns (i.e.,
    features) contaminated by noise.
  Figure 3 Link: articels_figures_by_rev_year\2019\Logistic_Regression_Confined_by_CardinalityConstrained_Sample_and_Feature_Select\figure_3.jpg
  Figure 3 caption: Samples images from the COIL20 [27] dataset.
  Figure 4 Link: articels_figures_by_rev_year\2019\Logistic_Regression_Confined_by_CardinalityConstrained_Sample_and_Feature_Select\figure_4.jpg
  Figure 4 caption: Sample images from the CMU PIE [28] dataset.
  Figure 5 Link: articels_figures_by_rev_year\2019\Logistic_Regression_Confined_by_CardinalityConstrained_Sample_and_Feature_Select\figure_5.jpg
  Figure 5 caption: "Balanced accuracy on the synthetic datasets corrupted by different\
    \ ratios of noise. With the gradual increase in noise (in both samples and features),\
    \ the proposed techniques more robustly classify the data, compared to JFSS \u2113\
    \ 1 [53], LR \u2113 0 [62], and FSSVM."
  Figure 6 Link: articels_figures_by_rev_year\2019\Logistic_Regression_Confined_by_CardinalityConstrained_Sample_and_Feature_Select\figure_6.jpg
  Figure 6 caption: "Percentage of the noisy features (top) and samples (bottom) that\
    \ were selected during the 10-fold cross-validation by each method. The methods\
    \ are marked according to the legend in Fig. 5. The bottom plot does not include\
    \ LR \u2113 0 -W and FSSVM-W methods as they omit sample selection. In general,\
    \ proposed methods select less noisy features and samples than the alternatives."
  Figure 7 Link: articels_figures_by_rev_year\2019\Logistic_Regression_Confined_by_CardinalityConstrained_Sample_and_Feature_Select\figure_7.jpg
  Figure 7 caption: Comparisons of F 1 -scores on MNIST dataset with respect to the
    ratio of samples across the two classes in the training set.
  Figure 8 Link: articels_figures_by_rev_year\2019\Logistic_Regression_Confined_by_CardinalityConstrained_Sample_and_Feature_Select\figure_8.jpg
  Figure 8 caption: Example Selected (top) and Not-Selected (bottom) samples in the
    MNIST experiment by our method (i.e., SFS H ).
  Figure 9 Link: articels_figures_by_rev_year\2019\Logistic_Regression_Confined_by_CardinalityConstrained_Sample_and_Feature_Select\figure_9.jpg
  Figure 9 caption: Comparisons of results (i.e., Precision, Recall and F1 -score)
    between the proposed and the baseline methods.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Ehsan Adeli
  Name of the last author: Kilian M. Pohl
  Number of Figures: 12
  Number of Tables: 4
  Number of authors: 5
  Paper title: Logistic Regression Confined by Cardinality-Constrained Sample and
    Feature Selection
  Publication Date: 2019-02-26 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Details of the Medical Image Analysis Datasets Used in This
      Study (svol = supratentorial volume)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Percentages of TPR, TNR, and BAcc of Different Methods for
      Object Verification on COIL20 Dataset
  Table 3 caption:
    table_text: TABLE 3 Percentages of TPR, TNR, and BAcc of Different Methods for
      Face Verification on CMU PIE Dataset
  Table 4 caption:
    table_text: TABLE 4 The Balanced Accuracy (BAcc) of the Proposed Classifier and
      the Baseline Methods on the Three Neuroimaging Datasets
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2901688
- Affiliation of the first author: google-ai, google, inc., san diego, usa
  Affiliation of the last author: google-ai, google, inc., san diego, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Hiding_Images_within_Images\figure_1.jpg
  Figure 1 caption: "Samples from the full-image hiding system. From the left: host\
    \ image, hidden image, the container image (the container holdshides the hidden\
    \ image within it while looking like the host), and the recovered hidden image\
    \ \u2013 this is extracted from only the container. The last two columns are the\
    \ errors for the container versus host and reconstructed versus hidden images\
    \ (enhanced 5\xD7 ). The bottom two rows show examples of larger reconstruction\
    \ errors, likely due to the highly saturated colors that were not well represented\
    \ in the training set."
  Figure 10 Link: articels_figures_by_rev_year\2019\Hiding_Images_within_Images\figure_10.jpg
  Figure 10 caption: What can we see without the unmodified host image? If we blur
    the result image and subtract that from the result image, we see remnants of the
    hidden image (rows A,B,C in particular). Color-constant regions in the host image
    are most susceptible to this. This is a potential vulnerability even without access
    to the original host image.
  Figure 2 Link: articels_figures_by_rev_year\2019\Hiding_Images_within_Images\figure_2.jpg
  Figure 2 caption: 'The three components of the full system. Left: Hidden-Image preparation.
    Center: Hiding the image in the host image. Right: Uncovering the hidden image
    with the Reveal-Network; this is trained simultaneously, but is used by the receiver.'
  Figure 3 Link: articels_figures_by_rev_year\2019\Hiding_Images_within_Images\figure_3.jpg
  Figure 3 caption: 'Transformations made by the preparation network (3 examples shown).
    Left: Original Color Images. Middle: three channels of information extracted by
    the preparation network that are input into the hiding network (other channels
    not shown). Right: zoom of the edge-detectors. In the most easily recognizable
    example, the 2nd channel activates for high frequency regions, e.g., textures
    and edges (shown enlarged (right)).'
  Figure 4 Link: articels_figures_by_rev_year\2019\Hiding_Images_within_Images\figure_4.jpg
  Figure 4 caption: The three component networks are trained as a single network,
    thereby pairing the encoding and decoding portions to work exclusively with each
    other. The first error term 1 affects the Preparation and Hiding component networks.
    Error term 2 affects all 3 components. S is the hidden image, H is the host image.
  Figure 5 Link: articels_figures_by_rev_year\2019\Hiding_Images_within_Images\figure_5.jpg
  Figure 5 caption: 'ROC curves: True Positive Rate versus False Positive Rate for
    StegExpose when detecting images embedded via DNNs.'
  Figure 6 Link: articels_figures_by_rev_year\2019\Hiding_Images_within_Images\figure_6.jpg
  Figure 6 caption: 'Sensitivity to Bits in the Container Image. (Left:) Control case,
    the container image. Changing a bit in the container image obviously has no effect
    on other bits. (Right): However, that same bit flip in the container image has
    effects across all color bands in the recovered hidden image. For example, if
    we want to measure the effect of flipping the 2nd bit in the Blue channel, we
    would look at the marked entries (marked with ).'
  Figure 7 Link: articels_figures_by_rev_year\2019\Hiding_Images_within_Images\figure_7.jpg
  Figure 7 caption: How far does the effect of changing a pixel in the container image
    reach when decoding the hidden image? This figure shows the average effect that
    perturbing a pixel in the container image has on the hidden image reconstruction,
    as a function of distance from the perturbed pixel. After approximately a distance
    of 7 pixels, the effect is negligible.
  Figure 8 Link: articels_figures_by_rev_year\2019\Hiding_Images_within_Images\figure_8.jpg
  Figure 8 caption: The hidden image is embedded within the host image; the result,
    the container image, is shown in the column result. In the hypothetical case of
    an adversary gaining access to the original, unaltered, host image, the adversary
    can magnify the difference between the original host and the container, see column
    diff, diff10, diff20. When magnified 10x and 20x, traces of the hidden image are
    visible, particularly rows A,B,C,G.
  Figure 9 Link: articels_figures_by_rev_year\2019\Hiding_Images_within_Images\figure_9.jpg
  Figure 9 caption: Creating training examples for the discrimination network by generating
    triplets cover,hidden,container from the discarded networks.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Shumeet Baluja
  Name of the last author: Shumeet Baluja
  Number of Figures: 15
  Number of Tables: 5
  Number of authors: 1
  Paper title: Hiding Images within Images
  Publication Date: 2019-02-28 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Network Architectures
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average Error in R,G,B Channels (Out of 256)
  Table 3 caption:
    table_text: "TABLE 3 Average Error in R,G,B Channels (Out of 256) \u2013 When\
      \ Hiding Two Images"
  Table 4 caption:
    table_text: TABLE 4 Retrieval Results for All Reported Experiments from a Database
      of 2,650,000 Images
  Table 5 caption:
    table_text: TABLE 5 PSNR and SSIM Scores
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2901877
- Affiliation of the first author: department of electrical and computer engineering,
    university of waterloo, waterloo, canada
  Affiliation of the last author: department of electrical and computer engineering,
    university of waterloo, waterloo, canada
  Figure 1 Link: articels_figures_by_rev_year\2019\Online_Nearest_Neighbor_Search_Using_Hamming_Weight_Trees\figure_1.jpg
  Figure 1 caption: Hamming weight tree with depth one for 128-bit codes.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Online_Nearest_Neighbor_Search_Using_Hamming_Weight_Trees\figure_2.jpg
  Figure 2 caption: (a) Average radius of search for solving the K NN problem for
    different values of K . (b) Average Hamming weight of 1 billion SIFT vectors that
    are mapped to binary space using hyperplane LSH
  Figure 3 Link: articels_figures_by_rev_year\2019\Online_Nearest_Neighbor_Search_Using_Hamming_Weight_Trees\figure_3.jpg
  Figure 3 caption: A possible configuration of Hamming weight tree with depth 2 for
    128-bit binary codes.
  Figure 4 Link: articels_figures_by_rev_year\2019\Online_Nearest_Neighbor_Search_Using_Hamming_Weight_Trees\figure_4.jpg
  Figure 4 caption: "The paths that must be traversed for finding the codes lying\
    \ at distance r from the query q with \u2225q \u2225 H =64 , \u2225 q (1) 2 \u2225\
    \ H =32 and \u2225 q (2) 2 \u2225 H =32 . Note that to solve the r -neighbor problem,\
    \ we need to check all nodes lying at distance r \u2032 \u2264r from the query.\
    \ For example, to solve the 2-neighbor problem in the above tree, the search algorithm\
    \ must traverse all the red dashed paths."
  Figure 5 Link: articels_figures_by_rev_year\2019\Online_Nearest_Neighbor_Search_Using_Hamming_Weight_Trees\figure_5.jpg
  Figure 5 caption: "Average query time of the nearest neighbor search for \u03C4\
    \ =100, 1000 and 10000 on ANN1B dataset."
  Figure 6 Link: articels_figures_by_rev_year\2019\Online_Nearest_Neighbor_Search_Using_Hamming_Weight_Trees\figure_6.jpg
  Figure 6 caption: Average query time of HWT and linear scan on the ANN1B dataset
    for solving the Hamming NNS.
  Figure 7 Link: articels_figures_by_rev_year\2019\Online_Nearest_Neighbor_Search_Using_Hamming_Weight_Trees\figure_7.jpg
  Figure 7 caption: Average query time of HWT and MIH for the task of Hamming nearest
    neighbor search. The value of m denotes the number of hash tables used in MIH.
  Figure 8 Link: articels_figures_by_rev_year\2019\Online_Nearest_Neighbor_Search_Using_Hamming_Weight_Trees\figure_8.jpg
  Figure 8 caption: Average query time of HWT and AMIH for the task of angular nearest
    neighbor search.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.97
  Name of the first author: Sepehr Eghbali
  Name of the last author: Ladan Tahvildari
  Number of Figures: 8
  Number of Tables: 2
  Number of authors: 3
  Paper title: Online Nearest Neighbor Search Using Hamming Weight Trees
  Publication Date: 2019-03-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average Running Time of Nearest Neighbor Search with HWT and
      Linear Scan (LS) Algorithms on ANN1B and GIST 80M
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance of Different Tree Based Techniques Applied to
      the 256-bit SIFT Dataset with 1 Million Items to Find the Hamming Nearest Neighbor
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2902391
- Affiliation of the first author: department of computing, imperial college london,
    kensington, london, united kingdom
  Affiliation of the last author: center for machine vision and signal analysis, university
    of oulu, oulu, finland
  Figure 1 Link: articels_figures_by_rev_year\2019\Side_Information_for_Face_Completion_A_Robust_PCA_Approach\figure_1.jpg
  Figure 1 caption: The procedure of getting the UV map from an arbitrary 2D image.
  Figure 10 Link: articels_figures_by_rev_year\2019\Side_Information_for_Face_Completion_A_Robust_PCA_Approach\figure_10.jpg
  Figure 10 caption: ROC curves on the CFP dataset.
  Figure 2 Link: articels_figures_by_rev_year\2019\Side_Information_for_Face_Completion_A_Robust_PCA_Approach\figure_2.jpg
  Figure 2 caption: Given an input sequence of incomplete UV maps, we extract the
    shape using 3DMM and perform preliminary completion using GAN. With the left subspace
    and side information provided by GAN, we then carry out PCPSFM to produce more
    refined completion results. After that, we attach the completed UV texture to
    the shape creating images at various poses for face recognition.
  Figure 3 Link: articels_figures_by_rev_year\2019\Side_Information_for_Face_Completion_A_Robust_PCA_Approach\figure_3.jpg
  Figure 3 caption: "Log-scale relative error ( log \u2225L\u2212 L 0 \u2225 F \u2225\
    \ L 0 \u2225 F ) of PCPSM (a) when side information is perfect ( S= L 0 ) and\
    \ (b) when side information is the observation ( S=M )."
  Figure 4 Link: articels_figures_by_rev_year\2019\Side_Information_for_Face_Completion_A_Robust_PCA_Approach\figure_4.jpg
  Figure 4 caption: 'Domains of recovery by various algorithms in the fully observed
    case: (I,III) for random signs and (II,IV) for coherent signs.'
  Figure 5 Link: articels_figures_by_rev_year\2019\Side_Information_for_Face_Completion_A_Robust_PCA_Approach\figure_5.jpg
  Figure 5 caption: 'Domains of recovery by various algorithms in the partially observed
    case: (I,III) for random signs and (II,IV) for coherent signs.'
  Figure 6 Link: articels_figures_by_rev_year\2019\Side_Information_for_Face_Completion_A_Robust_PCA_Approach\figure_6.jpg
  Figure 6 caption: 'Comparison of face denoising ability: (I) Observation; (II) side
    information; (III) PCP; (IV) PCPSM; (V) LRR; (VI) PCPF; (VII) PCPFSM; (VIII) PSSV;
    (IX) RPCAG; and (X) FRPCAG.'
  Figure 7 Link: articels_figures_by_rev_year\2019\Side_Information_for_Face_Completion_A_Robust_PCA_Approach\figure_7.jpg
  Figure 7 caption: (row I) original sequences; (row II) random masks; (row III) sample
    inputs; (row IV) side information; (row V) PCP; (row VI) PCPSM; (row VII) LRR;
    (row VIII) PCPSFM.
  Figure 8 Link: articels_figures_by_rev_year\2019\Side_Information_for_Face_Completion_A_Robust_PCA_Approach\figure_8.jpg
  Figure 8 caption: "2D face synthesis of three views ( \u2212 45 \u2218 , 0 \u2218\
    \ , 45 \u2218 ) from the completed UV maps by various methods."
  Figure 9 Link: articels_figures_by_rev_year\2019\Side_Information_for_Face_Completion_A_Robust_PCA_Approach\figure_9.jpg
  Figure 9 caption: The proposed pipeline for video-based face recognition. The 3DMM
    [48] is fitted on the frames of the video and the incompleted UV maps are estimated.
    The trained GAN [51] is then used to provide an initial estimate of the side information
    and the proposed methodology is applied to generate the completed UV maps. The
    3D model is reused to render the images in the frontal view. Deep neural network
    is used to extract features from all frames and the average of the features is
    used to represent the video.
  First author gender probability: 0.72
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Niannan Xue
  Name of the last author: Stefanos Zafeiriou
  Number of Figures: 13
  Number of Tables: 7
  Number of authors: 5
  Paper title: 'Side Information for Face Completion: A Robust PCA Approach'
  Publication Date: 2019-03-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Measures of UV Completion by Various Algorithms
      on the 4DFAB Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Verification TAR on the CFP Dataset, the Higher TAR the Better
  Table 3 caption:
    table_text: TABLE 3 1:1 Verification TAR on the IJB-B Dataset (Higher Is Better)
  Table 4 caption:
    table_text: TABLE 4 1:1 Verification TAR on the IJB-C Dataset (Higher Is Better)
  Table 5 caption:
    table_text: TABLE 5 Verification Accuracy ( % %) of Different Methods on the YTF
      Dataset
  Table 6 caption:
    table_text: TABLE 6 Verification TAR on the YTF Dataset (Higher Is Better)
  Table 7 caption:
    table_text: TABLE 7 Verification TAR on the PaSC Dataset (Higher Is Better)
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2902556
