- Affiliation of the first author: department of computer science and engineering,
    texas a&m university, college station, tx, usa
  Affiliation of the last author: department of computer science and engineering,
    texas a&m university, college station, tx, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Interpreting_Image_Classifiers_by_Generating_Discrete_Masks\figure_1.jpg
  Figure 1 caption: "Illustrations of the pipeline of our proposed approach. Given\
    \ an image X , we employ a trainable generator to produce a probability mask P\
    \ and sample a mask M . Then we perform element-wise product between X and M to\
    \ obtain a new image X \u02C6 . Finally, we feed X \u02C6 to the discriminator\
    \ to evaluate the quality of P and M ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Interpreting_Image_Classifiers_by_Generating_Discrete_Masks\figure_2.jpg
  Figure 2 caption: "Illustrations of how to reduce the search space with auxiliary\
    \ information. With a 2\xD72 input X , there are 16 possible paths for the generator\
    \ to explore, shown as solid lines. With the guidance from auxiliary information\
    \ (blue dotted lines), the generator is encouraged to search within four states\
    \ (shown in blue) which share the same labels as the auxiliary information and\
    \ tends to ignore the masks in red color. Note that question marks mean unknown\
    \ or unconfident labels. For simplicity, some paths and masks in red are omitted."
  Figure 3 Link: articels_figures_by_rev_year\2020\Interpreting_Image_Classifiers_by_Generating_Discrete_Masks\figure_3.jpg
  Figure 3 caption: An example of incorporating the mask and auxiliary information.
    Darker colors mean 1 while lighter colors denote 0. We employ the gradients to
    build our auxiliary information L g (X) that the obtained gradients are normalized
    to [0,1] and thresholded. Then we combine L g (X) and M by element-wise product
    to obtain the final labels.
  Figure 4 Link: articels_figures_by_rev_year\2020\Interpreting_Image_Classifiers_by_Generating_Discrete_Masks\figure_4.jpg
  Figure 4 caption: Interpretation results for the VGG-16 network using different
    techniques. Different rows show the results for different input images. All saliency
    maps are normalized to range [0,1] and visualized using JET colormap. The darker
    red refers to the higher probability, the green color means the medium probability,
    and the darker blue means the lower probability. Note that the first six rows
    are examples with correct predictions while the predictions of the last three
    rows are wrong.
  Figure 5 Link: articels_figures_by_rev_year\2020\Interpreting_Image_Classifiers_by_Generating_Discrete_Masks\figure_5.jpg
  Figure 5 caption: The interpretation results for model randomization test. In each
    row, from left to right, we show the raw image, the original explanation, the
    explanation for VGG-16 with the final layer randomized, the explanation for VGG-16
    with the final two layers randomized, the explanation for VGG-16 with the final
    three layers randomized, and the explanation for VGG-16 with the final four layers
    randomized.
  Figure 6 Link: articels_figures_by_rev_year\2020\Interpreting_Image_Classifiers_by_Generating_Discrete_Masks\figure_6.jpg
  Figure 6 caption: The bounding boxes generated by our proposed method for six examples.
    The red rectangles show the bounding boxes generated by our method and the green
    ones are annotated ground truth.
  Figure 7 Link: articels_figures_by_rev_year\2020\Interpreting_Image_Classifiers_by_Generating_Discrete_Masks\figure_7.jpg
  Figure 7 caption: The interpretation results for the ablation study. In each row,
    from left to right, we show the raw image, the results of the model with both
    R a and R c reward terms, the results of the model with R a term but without R
    c term, and the results of the model without both terms.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.9
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.78
  Name of the first author: Hao Yuan
  Name of the last author: Shuiwang Ji
  Number of Figures: 7
  Number of Tables: 3
  Number of authors: 5
  Paper title: Interpreting Image Classifiers by Generating Discrete Masks
  Publication Date: 2020-10-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Comparisions Between Different Approaches Using
      Weakly Supervised Object Localization
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Comparisions Between Different Approaches Via
      Saliency Metric
  Table 3 caption:
    table_text: TABLE 3 The Results of the ROAR Evaluation
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3028783
- Affiliation of the first author: department of computer science, boston university,
    boston, ma, usa
  Affiliation of the last author: department of computer science, boston university,
    boston, ma, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Revisiting_ImageLanguage_Networks_for_OpenEnded_Phrase_Detection\figure_1.jpg
  Figure 1 caption: In this paper, we address the task of phrase detection, where
    the goal is to identify any image regions related to a query phrase and remove
    all other candidates. This is a more challenging version of the phrase localization
    task addressed in prior work that assumes ground truth image-phrase pairs are
    given at test time.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Revisiting_ImageLanguage_Networks_for_OpenEnded_Phrase_Detection\figure_2.jpg
  Figure 2 caption: Model Overview. Our phrase detection model follows the Faster
    R-CNN paradigm (shown on the left) consisting of a region proposal network, a
    bounding box regressor, and a region classifier. For the region classifier, which
    separates regions that are relevant to a phrase from irrelevant regions, we benchmark
    several variants on phrase detection based on methods used for the phrase localization
    task. We find careful initialization of the region classification layers are key
    to enable our model to discriminate between related phrases.
  Figure 3 Link: articels_figures_by_rev_year\2020\Revisiting_ImageLanguage_Networks_for_OpenEnded_Phrase_Detection\figure_3.jpg
  Figure 3 caption: Confusion matrix comparing the top 20 most common phrases referring
    to people (in order of number of instances) in the Flickr30K Entities test set
    for different versions of the EmbNet classifier.
  Figure 4 Link: articels_figures_by_rev_year\2020\Revisiting_ImageLanguage_Networks_for_OpenEnded_Phrase_Detection\figure_4.jpg
  Figure 4 caption: Qualitative results comparing an Embedding Network classifier
    with and without CCA initialization. See text for discussion.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Bryan A. Plummer
  Name of the last author: Kate Saenko
  Number of Figures: 4
  Number of Tables: 5
  Number of authors: 7
  Paper title: Revisiting Image-Language Networks for Open-Ended Phrase Detection
  Publication Date: 2020-10-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Phrase Localization Performance
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Phrase Localization Performance Using Augmented Positive Phrases
      (PPA) Discussed in Section 4.2 for Evaluation
  Table 3 caption:
    table_text: TABLE 3 mAP for Phrase Detection Split by Frequency of Training Instances
  Table 4 caption:
    table_text: TABLE 4 mAP for Phrase Detection Split by Frequency of Training Instances
      Where Augmented Positive Phrases (PPA) Discussed in Section 4.2 are Used for
      Evaluation
  Table 5 caption:
    table_text: TABLE 5 Effect the Phrase Filtering Approach Discussed in Section
      6.2 has on the Phrase Detection Task
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3029008
- Affiliation of the first author: baidu vis, beijing, china
  Affiliation of the last author: mit-ibm watson ai lab, cambridge, ma, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Purely_Attention_Based_Local_Feature_Integration_for_Video_Classification\figure_1.jpg
  Figure 1 caption: 'Example of RGB frames sampled from videos. We observe several
    important characteristics of local features of a video: high degree of similarity,
    local identifiability, and multi-component inputs.'
  Figure 10 Link: articels_figures_by_rev_year\2020\Purely_Attention_Based_Local_Feature_Integration_for_Video_Classification\figure_10.jpg
  Figure 10 caption: Visualization of attention weight maps of attention clusters.
    We visualize attention weights by multiplying normalized attention weights to
    corresponding frames. The upper row correspond to original frames. The middle
    eight rows correspond to attention weights with shift operation. The bottom eight
    rows correspond to attention weights without shifting operation.
  Figure 2 Link: articels_figures_by_rev_year\2020\Purely_Attention_Based_Local_Feature_Integration_for_Video_Classification\figure_2.jpg
  Figure 2 caption: Frames sampled from a video in the 20BN-Something-Something dataset.
    The top row treats all of the local features as a single unordered set, while
    the bottom row splits the video into several sub-sequences according to their
    temporal order at first, and treats sub-sequences as different sets.
  Figure 3 Link: articels_figures_by_rev_year\2020\Purely_Attention_Based_Local_Feature_Integration_for_Video_Classification\figure_3.jpg
  Figure 3 caption: Architecture of an attention unit with shifting operation.
  Figure 4 Link: articels_figures_by_rev_year\2020\Purely_Attention_Based_Local_Feature_Integration_for_Video_Classification\figure_4.jpg
  Figure 4 caption: Architecture for basic attention clusters.
  Figure 5 Link: articels_figures_by_rev_year\2020\Purely_Attention_Based_Local_Feature_Integration_for_Video_Classification\figure_5.jpg
  Figure 5 caption: 'Channel pyramid attention with 3 levels: For different levels,
    features are divided into sub-features of different granularities. Attentions
    are applied on each sub-feature sequence separately, and then outputs are concatenated
    to generate a final representation.'
  Figure 6 Link: articels_figures_by_rev_year\2020\Purely_Attention_Based_Local_Feature_Integration_for_Video_Classification\figure_6.jpg
  Figure 6 caption: 'Temporal pyramid attention with 3 levels: The segment-level feature
    sequence is first divided into different length sub-sequences for different levels.
    Channel pyramid attention operations are applied to each sub-sequence separately
    and then the outputs are concatenated together to produce a final descriptor.'
  Figure 7 Link: articels_figures_by_rev_year\2020\Purely_Attention_Based_Local_Feature_Integration_for_Video_Classification\figure_7.jpg
  Figure 7 caption: 'Overall architecture for video classification: Separate attention
    clusters are applied for different feature sets and then the outputs are concatenated
    for classification.'
  Figure 8 Link: articels_figures_by_rev_year\2020\Purely_Attention_Based_Local_Feature_Integration_for_Video_Classification\figure_8.jpg
  Figure 8 caption: The accuracy on Kinetics-400 in each epoch, learned with different
    cluster sizes, with (w) or without (wo) the shifting operation.
  Figure 9 Link: articels_figures_by_rev_year\2020\Purely_Attention_Based_Local_Feature_Integration_for_Video_Classification\figure_9.jpg
  Figure 9 caption: Visualization of attention weight maps of 8-unit attention clusters
    for a sampled video with shifting operation (top), and without (bottom). We show
    attention weights as a heat map with increasingly blue patches indicating a larger
    weight. The 8 rows correspond to the 8 weights of 8 attention units.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.7
  Name of the first author: Xiang Long
  Name of the last author: Chuang Gan
  Number of Figures: 10
  Number of Tables: 11
  Number of authors: 7
  Paper title: Purely Attention Based Local Feature Integration for Video Classification
  Publication Date: 2020-10-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Top-1 Accuracy (%) on Kinetics-400 to Show the Effect of Different
      Cluster Sizes and Training With (w) or Without (wo) Shifting Operation for RGB,
      Flow, and Audio
  Table 10 caption:
    table_text: TABLE 10 Comparisons of Results With Related Schemes Using Different
      Features on Various Datasets
  Table 2 caption:
    table_text: TABLE 2 Accuracy (%) on Kinetics-400 to Show the Effect of Different
      Weighting Functions and Various Cluster Sizes N N
  Table 3 caption:
    table_text: TABLE 3 Top-1 Accuracy (%) on Kinetics to Show the Detailed Effect
      of Linear Transformation and Normalization in the Shifting Operation
  Table 4 caption:
    table_text: TABLE 4 Top-1 Accuracy (%) of Using Channel Pyramid and Temporal Pyramid
      Attention With Different Pyramid Levels on Kinetics-400 Validation Split
  Table 5 caption:
    table_text: TABLE 5 Top-1 Accuracy (%) of Using Channel Pyramid and Temporal Pyramid
      Attention With Different Pyramid Levels on the 20BN-Something-Something Validation
      Split
  Table 6 caption:
    table_text: TABLE 6 Experiments of Class-Wise Accuracy on 20BN-Something-Something
  Table 7 caption:
    table_text: TABLE 7 Top-1 Accuracy (%) of Combining Both Channel Pyramid and Temporal
      Pyramid Attention With Different Pyramid Level on the Kinetics-400 Validation
      Split
  Table 8 caption:
    table_text: "TABLE 8 Top-1 Accuracy (%) of Pyramid\xD7Pyramid Attention With Different\
      \ Pyramid Level on 20BN-Something-Something-v1 Validation Split"
  Table 9 caption:
    table_text: TABLE 9 Mean Classification Accuracy (%) Comparing With State-of-the-Art
      Methods on UCF101 and HMDB51
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3029554
- Affiliation of the first author: google research, kirkland, wa, usa
  Affiliation of the last author: department of electrical and computer engineering,
    rice university, houston, tx, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Uniform_Partitioning_of_Data_Grid_for_Association_Detection\figure_1.jpg
  Figure 1 caption: "Partitioning of D into \u2113 x columns and \u2113 y rows. D\
    \ ij denotes the set of sample points located in the i th row and the j th column.\
    \ Figure is adopted from the conference version of this manuscript [24]."
  Figure 10 Link: articels_figures_by_rev_year\2020\Uniform_Partitioning_of_Data_Grid_for_Association_Detection\figure_10.jpg
  Figure 10 caption: Red circles show developed countries having the largest life
    expectancy but not the largest GDP per capita. Instead their governments health
    expenditure are among the largest ones.
  Figure 2 Link: articels_figures_by_rev_year\2020\Uniform_Partitioning_of_Data_Grid_for_Association_Detection\figure_2.jpg
  Figure 2 caption: OptimizeXAxis [1] considers only consecutive points falling into
    the same row and draw partitions between them. The set of consecutive points falling
    into the same row is called a clump.
  Figure 3 Link: articels_figures_by_rev_year\2020\Uniform_Partitioning_of_Data_Grid_for_Association_Detection\figure_3.jpg
  Figure 3 caption: "Using k-nearest neighbors method to reduce the effect of noise\
    \ in noisy relationships. We replace each point with the average of its neighbors\
    \ in its \u03B4 n -neighborhood."
  Figure 4 Link: articels_figures_by_rev_year\2020\Uniform_Partitioning_of_Data_Grid_for_Association_Detection\figure_4.jpg
  Figure 4 caption: Functional associations we have used to test the MIC and UIC.
    Corresponding results are available in Tables 1 and 2.
  Figure 5 Link: articels_figures_by_rev_year\2020\Uniform_Partitioning_of_Data_Grid_for_Association_Detection\figure_5.jpg
  Figure 5 caption: Non-functional associations we have used to test the MIC and UIC.
    Corresponding results are available in Tables 3 and 4.
  Figure 6 Link: articels_figures_by_rev_year\2020\Uniform_Partitioning_of_Data_Grid_for_Association_Detection\figure_6.jpg
  Figure 6 caption: Power of the MIC and UIC as a function of the level of noise added,
    for eight different types of associations. We have used 500 Monte-Carlo samples
    to compute the power in each plot.
  Figure 7 Link: articels_figures_by_rev_year\2020\Uniform_Partitioning_of_Data_Grid_for_Association_Detection\figure_7.jpg
  Figure 7 caption: Noisy non-functional associations we have used to test the MIC
    and UIC. Corresponding results are available in Tables 5 and 6.
  Figure 8 Link: articels_figures_by_rev_year\2020\Uniform_Partitioning_of_Data_Grid_for_Association_Detection\figure_8.jpg
  Figure 8 caption: Different types of multidimensional associations we have used
    to test the UIC. Corresponding results are available in Tables 7 and 8.
  Figure 9 Link: articels_figures_by_rev_year\2020\Uniform_Partitioning_of_Data_Grid_for_Association_Detection\figure_9.jpg
  Figure 9 caption: Life expectancy versus GDP per capita and government health expenditure
    for developed countries in 2010-2015. Both plots show a positive correlation meaning
    that as GDP per capita and health expenditure grow, life expectancy increases
    as well.
  First author gender probability: 0.94
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ali Mousavi
  Name of the last author: Richard G. Baraniuk
  Number of Figures: 10
  Number of Tables: 10
  Number of authors: 2
  Paper title: Uniform Partitioning of Data Grid for Association Detection
  Publication Date: 2020-10-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Values and Runtime (in sec) of MIC( D D) and UIC( D D) for
      Different Functional Associations in Fig. 4
  Table 10 caption:
    table_text: TABLE 10 Association Between the Life Expectancy and GDP and Health
      Expenditure
  Table 2 caption:
    table_text: TABLE 2 Values and Runtime (in sec) of MIC( D D) and UIC( D D) for
      Different Functional Associations in Fig. 4
  Table 3 caption:
    table_text: TABLE 3 Values and Run Time (in sec) for Calculation of MIC( D D)
      and UIC( D D) for Different Non-Functional Relationships in Fig. 5
  Table 4 caption:
    table_text: TABLE 4 Values and Run Time (in sec) for Calculation of MIC( D D)
      and UIC( D D) for Different Non-Functional Relationships in Fig. 5
  Table 5 caption:
    table_text: TABLE 5 Values and Run Time (in sec) for Calculation of MIC( D D)
      and UIC( D D) for Different Noisy Non-Functional Relationships in Fig. 5
  Table 6 caption:
    table_text: TABLE 6 Values and Run Time (in sec) for the Calculation of MIC( D
      D) and UIC( D D) for Different Noisy Non-Functional Relationships in Fig. 5
  Table 7 caption:
    table_text: TABLE 7 UIC( D D) for Different Functional and Non-Functional Associations
      Denoted in Fig. 8
  Table 8 caption:
    table_text: TABLE 8 UIC( D D), MIC( D D), and ChiMIC( D D) and Their Runtime (in
      sec) for Different Functional and Non-Functional Associations Denoted in Fig.
      8
  Table 9 caption:
    table_text: TABLE 9 UIC( D D) and its Runtime (in sec) for Several Associations
      Where Both x x and y y are Multidimensional
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3029487
- Affiliation of the first author: centre for vision, speech and signal processing
    (cvssp), university of surrey, guildford, u.k.
  Affiliation of the last author: centre for vision, speech and signal processing
    (cvssp), university of surrey, guildford, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2020\Distribution_Cognisant_Loss_for_CrossDatabase_Facial_Age_Estimation_With_Sensiti\figure_1.jpg
  Figure 1 caption: Intra-database versus subjective-exclusive cross-database age
    estimation evaluation protocols. In subjective-exclusive cross-database scenarios,
    the trained models are blind to the characteristics of the test database and the
    subjects in the training database are mutually exclusive. The accuracy of age
    estimation under the intra-database protocol is much higher than that under the
    cross-database one for the same test database.
  Figure 10 Link: articels_figures_by_rev_year\2020\Distribution_Cognisant_Loss_for_CrossDatabase_Facial_Age_Estimation_With_Sensiti\figure_10.jpg
  Figure 10 caption: Average predicted age at each age class for different ethnicity.
    (a) European. (b) African. (c) Indian. (d) Chinese.
  Figure 2 Link: articels_figures_by_rev_year\2020\Distribution_Cognisant_Loss_for_CrossDatabase_Facial_Age_Estimation_With_Sensiti\figure_2.jpg
  Figure 2 caption: Behaviour of the KL and DC loss functions for two Gaussian distributions
    at different points. (a) Two Gaussian distributions. (b) KL loss function. (c)
    DC metric.
  Figure 3 Link: articels_figures_by_rev_year\2020\Distribution_Cognisant_Loss_for_CrossDatabase_Facial_Age_Estimation_With_Sensiti\figure_3.jpg
  Figure 3 caption: Gradient of the proposed DC loss function and KL loss function.
  Figure 4 Link: articels_figures_by_rev_year\2020\Distribution_Cognisant_Loss_for_CrossDatabase_Facial_Age_Estimation_With_Sensiti\figure_4.jpg
  Figure 4 caption: Comparison of the DC, KL, and KL-M loss functions.
  Figure 5 Link: articels_figures_by_rev_year\2020\Distribution_Cognisant_Loss_for_CrossDatabase_Facial_Age_Estimation_With_Sensiti\figure_5.jpg
  Figure 5 caption: "Relationships between different functions G f (\u22C5) ."
  Figure 6 Link: articels_figures_by_rev_year\2020\Distribution_Cognisant_Loss_for_CrossDatabase_Facial_Age_Estimation_With_Sensiti\figure_6.jpg
  Figure 6 caption: Sample images from the evaluation databases.
  Figure 7 Link: articels_figures_by_rev_year\2020\Distribution_Cognisant_Loss_for_CrossDatabase_Facial_Age_Estimation_With_Sensiti\figure_7.jpg
  Figure 7 caption: "The influence of parameter \u03B1 on accuracy."
  Figure 8 Link: articels_figures_by_rev_year\2020\Distribution_Cognisant_Loss_for_CrossDatabase_Facial_Age_Estimation_With_Sensiti\figure_8.jpg
  Figure 8 caption: Validation curves (loss scores versus epoch).
  Figure 9 Link: articels_figures_by_rev_year\2020\Distribution_Cognisant_Loss_for_CrossDatabase_Facial_Age_Estimation_With_Sensiti\figure_9.jpg
  Figure 9 caption: Average predicted age at each age class for each gender. (a) Male.
    (b) Female.
  First author gender probability: 0.94
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Ali Akbari
  Name of the last author: Josef Kittler
  Number of Figures: 14
  Number of Tables: 12
  Number of authors: 5
  Paper title: Distribution Cognisant Loss for Cross-Database Facial Age Estimation
    With Sensitivity Analysis
  Publication Date: 2020-10-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Intra-Database Evaluation on the MORPH Database (RS Protocol)
  Table 10 caption:
    table_text: TABLE 10 MAEs and CS Scores for Images Captured by Cameras With Different
      Quality (Distance 2 = 2.60m)
  Table 2 caption:
    table_text: TABLE 2 Intra-Database Evaluation on the MORPH Database (S1S2+S3 and
      S2S1+S3 Protocols)
  Table 3 caption:
    table_text: 'TABLE 3 Cross-Database Evaluation (MAE & CS) on the Target Databases
      (Training Database: BAG-Full)'
  Table 4 caption:
    table_text: 'TABLE 4 Cross-Database Evaluation on the Target Databases (Training
      Database: BAG-Subset)'
  Table 5 caption:
    table_text: 'TABLE 5 Cross-Dataset Evaluation on the Target Databases (Training
      Database: MORPH)'
  Table 6 caption:
    table_text: TABLE 6 MAEs and CS Scores With Respect to Gender
  Table 7 caption:
    table_text: TABLE 7 MAEs and CS Scores With Respect to Ethnicity
  Table 8 caption:
    table_text: TABLE 8 MAEs and CS Scores for Different Expressions
  Table 9 caption:
    table_text: TABLE 9 MAEs and CS Scores for Images Captured by Cameras With Different
      Quality (Distance 1 = 1m)
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3029486
- Affiliation of the first author: school of computer science, fudan university, shanghai,
    china
  Affiliation of the last author: department of computer science, university of maryland,
    college park, md, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\A_Dynamic_Frame_Selection_Framework_for_Fast_Video_Recognition\figure_1.jpg
  Figure 1 caption: A conceptual overview of our approach. To minimize the overall
    computational cost, AdaFrame learns to choose a small number of frames in order
    to make accurate predictions on a per-video basis.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\A_Dynamic_Frame_Selection_Framework_for_Fast_Video_Recognition\figure_2.jpg
  Figure 2 caption: An overview of AdaFrame. AdaFrame contains a memory-augmented
    LSTM, which serves as an agent to interact with a video sequence. At each time
    step, features from the current step, previous states, and a global context vector
    are used as inputs to the LSTM to produce the current hidden states. Conditioned
    on the hidden states, a prediction is emitted, a policy determining where to look
    next is produced, and a utility estimating expected future rewards is computed.
    See texts for more details.
  Figure 3 Link: articels_figures_by_rev_year\2020\A_Dynamic_Frame_Selection_Framework_for_Fast_Video_Recognition\figure_3.jpg
  Figure 3 caption: "Mean average precision versus computational cost. Results of\
    \ AdaFrame and comparisons with FrameGlimpse [17], FastForward [18], as alternative\
    \ frame selection strategies. For all AdaFrame models, each point in the line\
    \ indicates a different \u03BC (from left to right, \u03BC is 0.2, 0.5 and 0.7)."
  Figure 4 Link: articels_figures_by_rev_year\2020\A_Dynamic_Frame_Selection_Framework_for_Fast_Video_Recognition\figure_4.jpg
  Figure 4 caption: Data flow through AdaFrame over time. Each circle indicates, by
    size, the fraction of samples that exit the model at the corresponding time step.
  Figure 5 Link: articels_figures_by_rev_year\2020\A_Dynamic_Frame_Selection_Framework_for_Fast_Video_Recognition\figure_5.jpg
  Figure 5 caption: Learned inference policies for different classes over time. Each
    square, by density, represents the percentage of samples that are classified at
    the corresponding time step from a certain class in FCVID.
  Figure 6 Link: articels_figures_by_rev_year\2020\A_Dynamic_Frame_Selection_Framework_for_Fast_Video_Recognition\figure_6.jpg
  Figure 6 caption: "Sampled videos from the validation set of FCVID that use different\
    \ number of frames for testing. Frame usage is different not only among different\
    \ categories but also within the same class (e.g., \u201Cmaking cookies\u201D\
    \ and \u201Chiking\u201D)."
  Figure 7 Link: articels_figures_by_rev_year\2020\A_Dynamic_Frame_Selection_Framework_for_Fast_Video_Recognition\figure_7.jpg
  Figure 7 caption: Density estimate function of locations used by AdaFrame at each
    time step. The kernel density estimate function (Y-axis) provides an approximation
    of probability density function.
  Figure 8 Link: articels_figures_by_rev_year\2020\A_Dynamic_Frame_Selection_Framework_for_Fast_Video_Recognition\figure_8.jpg
  Figure 8 caption: Fraction of samples classified over time using utility and entropy.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.75
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Zuxuan Wu
  Name of the last author: Larry S. Davis
  Number of Figures: 8
  Number of Tables: 7
  Number of authors: 5
  Paper title: A Dynamic Frame Selection Framework for Fast Video Recognition
  Publication Date: 2020-10-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Results of Different Frame Selection Strategies on FCVID and
      ActivityNet
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results of Using DPN (Top) and SlowFast (Bottom) as the Backbone
      of AdaFrame on FCVID and ActivityNet
  Table 3 caption:
    table_text: TABLE 3 State-of-the-Art Results With Different Architectures on ActivityNet
  Table 4 caption:
    table_text: TABLE 4 Results of AdaFrame Using Different Global Memories on FCVID
  Table 5 caption:
    table_text: TABLE 5 Results of AdaFrame With Different Reward Functions on FCVID
  Table 6 caption:
    table_text: TABLE 6 Comparisons Between AdaFrame and Alternative Methods in Terms
      of mAP, GFLOPs, Runtime, and Number of Parameters on FCVID
  Table 7 caption:
    table_text: TABLE 7 Comparisons of Different Approaches on Kinetics
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3029425
- Affiliation of the first author: department of computer science, university college
    london, london, uk
  Affiliation of the last author: computer science, carnegie mellon university, pittsburgh,
    pa, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\SelfPose_D_Egocentric_Pose_Estimation_From_a_Headset_Mounted_Camera\figure_1.jpg
  Figure 1 caption: 'Egocentric human pose estimation: driving an avatar from an egocentric
    camera perspective. b) Egocentric perspective of the pose visualized in a) from
    an external point of view; c) 3D joint locations predicted from the input RGB
    only-information shown in b); d) synthetic character driven from the local joint
    rotations estimated alonside the 3D locations.'
  Figure 10 Link: articels_figures_by_rev_year\2020\SelfPose_D_Egocentric_Pose_Estimation_From_a_Headset_Mounted_Camera\figure_10.jpg
  Figure 10 caption: Qualitative results on real images captured in a studio (top)
    and reconstructions of images in the wild from Mo 2 Cap 2 [5] (bottom). Notice
    that the poses are expressed with respect to the camera reference system.
  Figure 2 Link: articels_figures_by_rev_year\2020\SelfPose_D_Egocentric_Pose_Estimation_From_a_Headset_Mounted_Camera\figure_2.jpg
  Figure 2 caption: 'Visualization of different poses with the same character. Top:
    poses rendered from an external camera viewpoint. White represents occlusion,
    which is body parts that would not be visible from the egocentric perspective.
    Bottom: poses rendered from the egocentric camera viewpoint. Color gradient indicates
    the density of image pixels for each area of the body: green is higher pixel density,
    whereas red is lower density. This figure illustrates the challenges faced in
    egocentric human pose estimation: severe self-occlusions, extreme perspective
    effects and lower pixel density for the lower body.'
  Figure 3 Link: articels_figures_by_rev_year\2020\SelfPose_D_Egocentric_Pose_Estimation_From_a_Headset_Mounted_Camera\figure_3.jpg
  Figure 3 caption: Example images from our x R-EgoPose Dataset compared with the
    competitor Mo2Cap2 dataset [5]. The quality of our frames is far superior than
    the randomly sampled frames from mo2cap2, where the characters suffer color matching
    with respect to the background light conditions.
  Figure 4 Link: articels_figures_by_rev_year\2020\SelfPose_D_Egocentric_Pose_Estimation_From_a_Headset_Mounted_Camera\figure_4.jpg
  Figure 4 caption: 'Proposed architecture for egocentric 3D human pose estimation
    consisting of two modules: a) interchangeable 2D pose detector that predicts heatmaps
    from the input RGB image; b) multi-branch auto-encoder that finds a representation
    of poses which includes also a level of uncertainty of predictions per joint.
    Alongside the main branch, for 3D joint location prediction, two auxiliary branches
    as used at training-time to improve latent space distribution. Branch ii) estimates
    local joint rotations, forcing them to be consistent with those rotations extracted
    by the predicted pose from i); branch iii) forces the latent space to include
    a level of uncertainty of the 2D joint locations by reconstructing the given predicted
    heatmaps from the pose embedding. These additional branches have demonstrated
    considerable improvements with respect to a standard AE architecture, as shown
    in Section 5.'
  Figure 5 Link: articels_figures_by_rev_year\2020\SelfPose_D_Egocentric_Pose_Estimation_From_a_Headset_Mounted_Camera\figure_5.jpg
  Figure 5 caption: Reconstructed heatmaps generated by the decoder branch which can
    reproduce the correct uncertainty of the 2D input predictions from the pose embedding.
  Figure 6 Link: articels_figures_by_rev_year\2020\SelfPose_D_Egocentric_Pose_Estimation_From_a_Headset_Mounted_Camera\figure_6.jpg
  Figure 6 caption: Character animation from the joint local rotation predictions
    computed from the input image. Notice how the model is able to retrieve most of
    the desired information even when limbs fall outside the camera field of view.
  Figure 7 Link: articels_figures_by_rev_year\2020\SelfPose_D_Egocentric_Pose_Estimation_From_a_Headset_Mounted_Camera\figure_7.jpg
  Figure 7 caption: Analysis of the angle predictions through time for the Righ Foot
    in sequence of the test-set.
  Figure 8 Link: articels_figures_by_rev_year\2020\SelfPose_D_Egocentric_Pose_Estimation_From_a_Headset_Mounted_Camera\figure_8.jpg
  Figure 8 caption: Performance of our proposed pipeline using different 2D pose estimation
    networks under the influence of white Gaussian noise in the image domain. Networks
    with (p) have been pretrained on ImageNet.
  Figure 9 Link: articels_figures_by_rev_year\2020\SelfPose_D_Egocentric_Pose_Estimation_From_a_Headset_Mounted_Camera\figure_9.jpg
  Figure 9 caption: Qualitative results on synthetic images from our synthetic test-set.
    Notice that the poses are expressed with respect to the camera reference system.
    Blue poses represent ground truth, whereas poses in red correspond to predictions.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Denis Tome
  Name of the last author: Fernando de la Torre
  Number of Figures: 10
  Number of Tables: 10
  Number of authors: 7
  Paper title: 'SelfPose: 3D Egocentric Pose Estimation From a Headset Mounted Camera'
  Publication Date: 2020-10-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Total Number of Frames Per Action and Their Distribution Between
      Train and Test Sets
  Table 10 caption:
    table_text: TABLE 10 Having a Larger Corpus of 2D Annotations can be Leveraged
      to Improve Final 3D Pose Estimation
  Table 2 caption:
    table_text: TABLE 2 Quantitative Evaluation With Martinez et al. [15], a State-of-the-Art
      Approach Developed for Front-Facing Cameras
  Table 3 caption:
    table_text: TABLE 3 Average Reconstruction Error Per Joint Using Eq. (3), Evaluated
      on the Entire Test-Set (see Section 3) With Model Trained Using Only Synthetic
      Data
  Table 4 caption:
    table_text: 'TABLE 4 Performance Analysis: Different Combinations of 2D Pose Detectors
      Combined With the Multi-Branch Lifting Network'
  Table 5 caption:
    table_text: TABLE 5 Average Reconstruction Error Per Joint Using Eq. (3), Evaluated
      on the Entire Test-Set When the Model Architecture Differs Based on the Size
      of the Embedding z z
  Table 6 caption:
    table_text: TABLE 6 Average Reconstruction Error Per Joint Using Eq. (3), Evaluated
      on the Entire Test-Set for Different Heatmap (HM) Reconstruction Sizes
  Table 7 caption:
    table_text: TABLE 7 Model Evaluation Based on Skin Tones
  Table 8 caption:
    table_text: TABLE 8 Quantitative Evaluation on Mo 2 Cap 2 Mo2Cap2 Dataset [5],
      Both Indoor and Outdoor Test-Sets
  Table 9 caption:
    table_text: TABLE 9 Comparison With Other State-of-the-Art Approaches on the Human3.6M
      Dataset (Front-Facing Cameras)
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3029700
- Affiliation of the first author: ministry of education, school of computer science
    and engineering, and the key lab of computer network and information integration,
    southeast university, nanjing, china
  Affiliation of the last author: ministry of education, school of computer science
    and engineering, and the key lab of computer network and information integration,
    southeast university, nanjing, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Head_Pose_Estimation_Based_on_Multivariate_Label_Distribution\figure_1.jpg
  Figure 1 caption: 'The three degrees of freedom (DOF) in head pose: yaw, pitch and
    roll [43].'
  Figure 10 Link: articels_figures_by_rev_year\2020\Head_Pose_Estimation_Based_on_Multivariate_Label_Distribution\figure_10.jpg
  Figure 10 caption: The MAE (Yaw+Pitch) of MLD regwJ with different standard deviations
    which are used to generate the MLDs.
  Figure 2 Link: articels_figures_by_rev_year\2020\Head_Pose_Estimation_Based_on_Multivariate_Label_Distribution\figure_2.jpg
  Figure 2 caption: "Three examples in the Pointing04 database which are labeled with\
    \ the same pose (\u2212 60 \u2218 ,\u2212 60 \u2218 ) ."
  Figure 3 Link: articels_figures_by_rev_year\2020\Head_Pose_Estimation_Based_on_Multivariate_Label_Distribution\figure_3.jpg
  Figure 3 caption: Examples of head-pose label noise in the BU dataset and the Pandora
    dataset.
  Figure 4 Link: articels_figures_by_rev_year\2020\Head_Pose_Estimation_Based_on_Multivariate_Label_Distribution\figure_4.jpg
  Figure 4 caption: Typical head poses from the Pointing04 database together with
    their MLDs generated by Eq. (1).
  Figure 5 Link: articels_figures_by_rev_year\2020\Head_Pose_Estimation_Based_on_Multivariate_Label_Distribution\figure_5.jpg
  Figure 5 caption: The weights between the poses in the weighted Jeffreys divergence.
  Figure 6 Link: articels_figures_by_rev_year\2020\Head_Pose_Estimation_Based_on_Multivariate_Label_Distribution\figure_6.jpg
  Figure 6 caption: HMLD prediction with different strategies.
  Figure 7 Link: articels_figures_by_rev_year\2020\Head_Pose_Estimation_Based_on_Multivariate_Label_Distribution\figure_7.jpg
  Figure 7 caption: Typical examples of the normalized face images in (a) Pointing04
    database, and (b) BJUT-3D database.
  Figure 8 Link: articels_figures_by_rev_year\2020\Head_Pose_Estimation_Based_on_Multivariate_Label_Distribution\figure_8.jpg
  Figure 8 caption: 'Typical examples of the MLD predicted by MLD regwJ : (a) Correct
    prediction; (b) Wrong prediction.'
  Figure 9 Link: articels_figures_by_rev_year\2020\Head_Pose_Estimation_Based_on_Multivariate_Label_Distribution\figure_9.jpg
  Figure 9 caption: The confusion matrices (in % and rounded to the ones place) of
    MLD regwJ on (a) the yaw angles and (b) the pitch angles on the Pointing04 database.
  First author gender probability: 0.6
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.53
  Name of the first author: Xin Geng
  Name of the last author: Yu Zhang
  Number of Figures: 14
  Number of Tables: 8
  Number of authors: 4
  Paper title: Head Pose Estimation Based on Multivariate Label Distribution
  Publication Date: 2020-10-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Head Pose Estimation Results on the Pointing04 Database
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Head Pose Estimation Results on the BJUT-3D Database
  Table 3 caption:
    table_text: TABLE 3 Results of MLD With Different Variances on the Pointing04
      Database
  Table 4 caption:
    table_text: TABLE 4 Label Noise Added into the BJUT-3D Database
  Table 5 caption:
    table_text: "TABLE 5 Head Pose Estimation Results on Biwi Kinect Dataset(mean\
      \ \xB1 \xB1standard Deviation)"
  Table 6 caption:
    table_text: "TABLE 6 Head Pose Estimation Results on AFLW Dataset(mean \xB1 \xB1\
      standard Deviation)"
  Table 7 caption:
    table_text: "TABLE 7 Head Pose Estimation Results on BU Face Tracking Dataset(mean\
      \ \xB1 \xB1standard Deviation)"
  Table 8 caption:
    table_text: TABLE 8 Head Pose Estimation Results on Pandora Dataset
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3029585
- Affiliation of the first author: language technologies institute, carnegie mellon
    university, pittsburgh, pa, usa
  Affiliation of the last author: language technologies institute, carnegie mellon
    university, pittsburgh, pa, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Contrastive_Adaptation_Network_for_Single_and_MultiSource_Domain_Adaptation\figure_1.jpg
  Figure 1 caption: 'Comparison between previous domain-discrepancy minimization methods
    and ours. Left: The domain shift exists between the source and target data before
    adaptation. Middle: Class-agnostic adaptation aligns source and target data at
    the domain-level, neglecting the class label of the sample, and hence may lead
    to suboptimal solutions. Consequently, the target samples of one label may be
    misaligned with source samples of a different label. Right: Our method performs
    class-aware alignment across domains. To avoid the misalignment, only the intra-class
    domain discrepancy is minimized. The inter-class domain discrepancy is maximized
    to enhance the models generalization ability.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Contrastive_Adaptation_Network_for_Single_and_MultiSource_Domain_Adaptation\figure_2.jpg
  Figure 2 caption: The training process of CAN. To minimize CDD, we perform alternative
    optimization between updating the target label hypotheses through clustering and
    adapting feature representations through back-propagation. For the clustering,
    we apply spherical K-means clustering of target samples based on their current
    feature representations. The number of clusters equals to that of underlying classes
    and the initial center of each class cluster is set to the center of source data
    within the same class. Then ambiguous data (i.e. far from the affiliated cluster
    centers) and ambiguous classes (i.e. containing few target samples around affiliated
    cluster centers) are discarded. For the feature adaptation, the labeled target
    samples provided by the clustering stage, together with the labeled source samples,
    pass through the network to achieve their multi-layer feature representations.
    The features of domain-specific FC layers are adopted to estimate CDD [equation
    (5)]. Besides, we apply cross-entropy loss on independently sampled source data.
    Back-propagating with minimizing CDD and cross-entropy loss [equation (15)] adapts
    the features and provides class-aware alignment. Detailed descriptions can be
    found in Section 3.4.
  Figure 3 Link: articels_figures_by_rev_year\2020\Contrastive_Adaptation_Network_for_Single_and_MultiSource_Domain_Adaptation\figure_3.jpg
  Figure 3 caption: The framework of multi-source CAN. The feature extractor and the
    classifier are shared across all the source domains and the target domain. Same
    as the general CAN framework, the feature update and the clustering are alternatively
    performed. For the clustering, we first independently cluster target samples based
    on the centers of each source domain. Then the clustering ensemble is performed
    to generate more accurate and robust target label estimations. Compared to the
    general CAN framework shown in Fig. 2, we further introduce the Boundary-Sensitive
    Alignment (BSA) module to aggregate the CDD losses between target and different
    sources.
  Figure 4 Link: articels_figures_by_rev_year\2020\Contrastive_Adaptation_Network_for_Single_and_MultiSource_Domain_Adaptation\figure_4.jpg
  Figure 4 caption: 'Visualization with t-SNE for different adaptation methods (best
    viewed in color). Left: t-SNE of JAN. Right: CAN. The input activations of the
    last FC layer are used for the computation of t-SNE. The results are on Office-31
    task mathbf W rightarrow mathbf A .'
  Figure 5 Link: articels_figures_by_rev_year\2020\Contrastive_Adaptation_Network_for_Single_and_MultiSource_Domain_Adaptation\figure_5.jpg
  Figure 5 caption: "(a-b) The curve of CDD and accuracy during training on task mathbf\
    \ A rightarrow mathbf D of the Office-31 dataset. The \u201CCDD-G\u201D denotes\
    \ the contrastive domain discrepancy computed with ground-truth target labels.\
    \ (c-d) The sensitivity of accuracy of CAN to beta . The results for mathbf A\
    \ rightarrow mathbf D (Left) and mathbf D rightarrow mathbf A (Right) are illustrated\
    \ as examples. The trends for other tasks are similar."
  Figure 6 Link: articels_figures_by_rev_year\2020\Contrastive_Adaptation_Network_for_Single_and_MultiSource_Domain_Adaptation\figure_6.jpg
  Figure 6 caption: The illustration of false target predictions. The ground-truth
    target labels are shown in the leftmost. The label below each image sample is
    predicted by the CAN. The failure cases can be roughly split into two groups,
    i.e. the reasonable failure and the systematic failure. The examples shown come
    from the VisDA-2017 dataset. And similar patterns can be found in other datasets
    (e.g. Office-31 and DomainNet).
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Guoliang Kang
  Name of the last author: Alexander Hauptmann
  Number of Figures: 6
  Number of Tables: 6
  Number of authors: 5
  Paper title: Contrastive Adaptation Network for Single- and Multi-Source Domain
    Adaptation
  Publication Date: 2020-10-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Classification Accuracy (%) for All the Six Tasks of Office-31
      Dataset Based on ResNet-50 [44], [45]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Classification Accuracy (%) on the VisDA-2017 Validation Set
      Based on ResNet-101 [44], [45]
  Table 3 caption:
    table_text: TABLE 3 Classification Accuracy (%) for the Six Tasks of DomainNet
      Dataset Based on ResNet-101 [44], [45]
  Table 4 caption:
    table_text: TABLE 4 The Effect of Alternative Optimization (AO) and CAS
  Table 5 caption:
    table_text: TABLE 5 Comparison With Different Ways of Utilizing Pseudo Target
      Labels
  Table 6 caption:
    table_text: TABLE 6 Comparison Among Different Clustering Ensemble Methods Described
      in Section 3.5
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3029948
- Affiliation of the first author: department of electrical engineering and computer
    science, massachusetts institute of technology, cambridge, ma, usa
  Affiliation of the last author: department of electrical engineering and computer
    science, massachusetts institute of technology, cambridge, ma, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\TSM_Temporal_Shift_Module_for_Efficient_and_Scalable_Video_Understanding_on_Edge\figure_1.jpg
  Figure 1 caption: Temporal Shift Module (TSM) performs efficient temporal modeling
    by moving the feature map along the temporal dimension. It is computationally
    free on top of a 2D convolution, but achieves strong temporal modeling ability.
    TSM efficiently supports both offline and online video recognition. Bi-directional
    TSM mingles both past and future frames with the current frame, which is suitable
    for high-throughput offline video recognition. Uni-directional TSM mingles only
    the past frame with the current frame, which is suitable for low-latency online
    video recognition.
  Figure 10 Link: articels_figures_by_rev_year\2020\TSM_Temporal_Shift_Module_for_Efficient_and_Scalable_Video_Understanding_on_Edge\figure_10.jpg
  Figure 10 caption: Two kinds of video backbone design. Straight-up backbone does
    not perform temporal pooling and is more data efficient. Pooled-up version requires
    many input frames and drains IO.
  Figure 2 Link: articels_figures_by_rev_year\2020\TSM_Temporal_Shift_Module_for_Efficient_and_Scalable_Video_Understanding_on_Edge\figure_2.jpg
  Figure 2 caption: (a) Latency overhead of TSM due to data movement. (b) Residual
    TSM achieve better performance than in-place shift. We choose 14 proportion residual
    shift as our default setting. It achieves higher accuracy with a negligible overhead.
  Figure 3 Link: articels_figures_by_rev_year\2020\TSM_Temporal_Shift_Module_for_Efficient_and_Scalable_Video_Understanding_on_Edge\figure_3.jpg
  Figure 3 caption: Residual shift is better than in-place shift. In-place shift happens
    before a convolution layer (or a residual block). Residual shift fuses temporal
    information inside a residual branch.
  Figure 4 Link: articels_figures_by_rev_year\2020\TSM_Temporal_Shift_Module_for_Efficient_and_Scalable_Video_Understanding_on_Edge\figure_4.jpg
  Figure 4 caption: Uni-directional TSM for online video recognition.
  Figure 5 Link: articels_figures_by_rev_year\2020\TSM_Temporal_Shift_Module_for_Efficient_and_Scalable_Video_Understanding_on_Edge\figure_5.jpg
  Figure 5 caption: TSM enjoys better accuracy-cost trade-off than I3D family and
    ECO family on Something-Something-V1 [18] dataset. (GCN includes the cost of ResNet-50
    RPN to generate region proposals.)
  Figure 6 Link: articels_figures_by_rev_year\2020\TSM_Temporal_Shift_Module_for_Efficient_and_Scalable_Video_Understanding_on_Edge\figure_6.jpg
  Figure 6 caption: TSM improves detection results with the help of temporal cues.
    For the left video, 2D baseline R-FCN generates false positive due to the glare
    of car headlight on frame 234, while TSM does not have such issue by considering
    the temporal information. For the right video, R-FCN generates false positive
    surrounding the bus due to occlusion by the traffic sign on frame 234. Also, it
    fails to detect motorcycle on frame 4 due to occlusion. TSM model addresses such
    issues with the help of temporal information.
  Figure 7 Link: articels_figures_by_rev_year\2020\TSM_Temporal_Shift_Module_for_Efficient_and_Scalable_Video_Understanding_on_Edge\figure_7.jpg
  Figure 7 caption: Early recognition on UCF101. TSM gives high prediction accuracy
    after only observing a small portion of the video.
  Figure 8 Link: articels_figures_by_rev_year\2020\TSM_Temporal_Shift_Module_for_Efficient_and_Scalable_Video_Understanding_on_Edge\figure_8.jpg
  Figure 8 caption: Early recognition on Something-Something and Jester datasets.
    TSM consistently outperforms TRN [68] at various portions by a large margin.
  Figure 9 Link: articels_figures_by_rev_year\2020\TSM_Temporal_Shift_Module_for_Efficient_and_Scalable_Video_Understanding_on_Edge\figure_9.jpg
  Figure 9 caption: 'Analyzing how different design aspects influence the distributed
    training scalability of video recognition models: (a) Computation efficiency;
    (b) data loading efficiency; (c) networking efficiency.'
  First author gender probability: 0.78
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.73
  Name of the first author: Ji Lin
  Name of the last author: Song Han
  Number of Figures: 14
  Number of Tables: 12
  Number of authors: 4
  Paper title: 'TSM: Temporal Shift Module for Efficient and Scalable Video Understanding
    on Edge Devices'
  Publication Date: 2020-10-09 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Our Method Consistently Outperforms 2D Counterparts on Multiple
      Datasets at Zero Extra Computation (Protocol: ResNet-50 8f input, 10 Clips for
      Kinetics, 2 for Others, Full-Resolution)'
  Table 10 caption:
    table_text: TABLE 10 Efficiency Statistics of Different Models
  Table 2 caption:
    table_text: TABLE 2 Comparing TSM Against Other Methods on Something-Something
      Dataset (Center Crop, 1 ClipVideo Unless Otherwise Specified)
  Table 3 caption:
    table_text: TABLE 3 TSM Can Consistently Improve the Performance Over Different
      Backbones on Kinetics Dataset
  Table 4 caption:
    table_text: TABLE 4 Results on Something-Something-V2
  Table 5 caption:
    table_text: TABLE 5 Compare to State-of-the-Art Methods on Kinetics
  Table 6 caption:
    table_text: TABLE 6 TSM Enjoys Low GPU Inference Latency and High Throughput
  Table 7 caption:
    table_text: TABLE 7 Comparing the Accuracy of Offline TSM and Online TSM on Different
      Datasets
  Table 8 caption:
    table_text: TABLE 8 TSM Efficiently Runs on Edge Devices With Low Latency
  Table 9 caption:
    table_text: TABLE 9 Video Detection Results on ImageNet-VID
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3029799
