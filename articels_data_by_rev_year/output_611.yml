- Affiliation of the first author: "institut f\xFCr informationsverarbeitung, leibniz\
    \ university hannover, hannover, germany"
  Affiliation of the last author: "institut f\xFCr informationsverarbeitung, leibniz\
    \ university hannover, hannover, germany"
  Figure 1 Link: articels_figures_by_rev_year\2016\D_Reconstruction_of_Human_Motion_from_Monocular_Image_Sequences\figure_1.jpg
  Figure 1 caption: 'Real world scenario of KTH database [1]. Left: frames 115, 136
    and 143 of Sequence 1 from Football Dataset II. Right: 3D reconstruction using
    our proposed method.'
  Figure 10 Link: articels_figures_by_rev_year\2016\D_Reconstruction_of_Human_Motion_from_Monocular_Image_Sequences\figure_10.jpg
  Figure 10 caption: Influence of the regularization parameter beta on the normalized
    3D error. In a wide range, the reconstruction improves (left of dotted line) if
    the regularizer is used as compared to optimization without it ( beta =0 ). Computed
    on CMU MoCap (subject7walk1run1, subject13jump2).
  Figure 2 Link: articels_figures_by_rev_year\2016\D_Reconstruction_of_Human_Motion_from_Monocular_Image_Sequences\figure_2.jpg
  Figure 2 caption: '3D reconstruction (green circles, blue lines) and ground truth
    data (red crosses). Top: Using approach of Gotardo and Martinez [4]. Most non-rigid
    structure from motion approaches with no rotation and unknown base poses fail,
    although they produce a small reprojection error (left). From other perspectives
    (right) a wrong reconstruction can be observed. Bottom: Our approach. Correct
    reconstruction in all views.'
  Figure 3 Link: articels_figures_by_rev_year\2016\D_Reconstruction_of_Human_Motion_from_Monocular_Image_Sequences\figure_3.jpg
  Figure 3 caption: Our method. (1) First we are learning 3D base poses from training
    data. (2) Input sequence. (3) Cameras are recovered from estimated 3D poses and
    2D poses. (4) Weights for base poses are calculated by minimizing the reprojection
    error. Steps (3) and (4) are alternated until the algorithm converges.
  Figure 4 Link: articels_figures_by_rev_year\2016\D_Reconstruction_of_Human_Motion_from_Monocular_Image_Sequences\figure_4.jpg
  Figure 4 caption: "Influence of the camera path regularization on the reconstruction\
    \ result. A low value for the regularization parameter \u03B3 avoids flips while\
    \ a high value enforces a static camera. The best results are obtained for values\
    \ between 1 and 5 ."
  Figure 5 Link: articels_figures_by_rev_year\2016\D_Reconstruction_of_Human_Motion_from_Monocular_Image_Sequences\figure_5.jpg
  Figure 5 caption: Comparison of ground truth coefficients of the first four base
    poses (top) with fitted periodic function (bottom) using the data set of N. Troje
    [27].
  Figure 6 Link: articels_figures_by_rev_year\2016\D_Reconstruction_of_Human_Motion_from_Monocular_Image_Sequences\figure_6.jpg
  Figure 6 caption: Temporal behavior of bone lengths obtained by unconstrained optimization.
    The maximal variation is about 40 mm. Computed on CMU MoCap (subject7walk1).
  Figure 7 Link: articels_figures_by_rev_year\2016\D_Reconstruction_of_Human_Motion_from_Monocular_Image_Sequences\figure_7.jpg
  Figure 7 caption: 2D reprojection error and 3D reconstruction error with different
    regularization parameter beta . While the 2D error is not changing much or getting
    worse, the 3D error gets significantly better at most parameter values. Computed
    on CMU MoCap (subject35walk1). Qualitatively there is no difference between different
    motion categories.
  Figure 8 Link: articels_figures_by_rev_year\2016\D_Reconstruction_of_Human_Motion_from_Monocular_Image_Sequences\figure_8.jpg
  Figure 8 caption: 'Influence on the number of used base poses on the 3D error using
    the non-periodic reconstruction (labels: walk, run, jump) and the periodic reconstruction
    (label: periodic). The number of used base poses is crucial for a good 3D reconstruction.
    Using more than 10 base poses for each motion category worsens the reconstruction
    error. For better visibility, the errors are normalized on the 3D error when using
    two base poses. The periodic reconstruction is done on the same walking sequence
    as the non-periodic reconstruction. Computed on CMU MoCap (subject35walk2run1,
    subject13jump1).'
  Figure 9 Link: articels_figures_by_rev_year\2016\D_Reconstruction_of_Human_Motion_from_Monocular_Image_Sequences\figure_9.jpg
  Figure 9 caption: 'Influence on the number of used base poses on the 2D error using
    the non-periodic reconstruction (labels: walk, run, jump) and the periodic reconstruction
    (label: periodic). The 2D error decreases when more base poses are used. For better
    visibility, the errors are normalized on the 2D error when using two base poses.
    The periodic reconstruction is done on the same walking sequence as the non-periodic
    reconstruction. Computed on CMU MoCap (subject35walk2run1, subject13jump1).'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.91
  Name of the first author: Bastian Wandt
  Name of the last author: Bodo Rosenhahn
  Number of Figures: 24
  Number of Tables: 1
  Number of authors: 3
  Paper title: 3D Reconstruction of Human Motion from Monocular Image Sequences
  Publication Date: 2016-04-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average 3D Reconstruction Error in cm on the CMU Dataset (Walk,
      Run, Jump), HumanEva Walking Dataset (HE) and KTH Football Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2553028
- Affiliation of the first author: eecs department at the university of california,
    berkeley, berkeley, ca
  Affiliation of the last author: cse department at the university of california,
    san diego, la jolla, ca
  Figure 1 Link: articels_figures_by_rev_year\2016\Shape_Estimation_from_Shading_Defocus_and_Correspondence_Using_LightField_Angula\figure_1.jpg
  Figure 1 caption: Light-field Depth Estimation Using Shading, Defocus, and Correspondence
    Cues. In this work, we present a novel algorithm that estimates shading to improve
    depth recovery using light-field angular coherence. Here we have an input of a
    real scene with a shell surface and a camera tilted slightly toward the right
    of the image (a). We obtain improved defocus (b) and correspondence (c) depth
    cues for depth estimation (d,e). However, because local depth estimation is only
    accurate at edges or textured regions, depth estimation of the shell appears regularized
    and planar. We use the depth estimation to estimate shading, which is S (f), the
    component in I=AS , where I is the observed image and A is the albedo (g). With
    the depth and shading estimations, we can refine our depth to better represent
    the surface of the shell (h,i). Throughout this paper, we use the scale on the
    right to represent depth.
  Figure 10 Link: articels_figures_by_rev_year\2016\Shape_Estimation_from_Shading_Defocus_and_Correspondence_Using_LightField_Angula\figure_10.jpg
  Figure 10 caption: Uniform Albedo Comparisons We compare qualitative and quantitative
    measures with two different examples against Lytro Illum Software, Wanner and
    Goldluecke [49], Jeon et al. [54], contrast-based defocus and correspondence from
    Section 3, and angular coherence based defocus and correspondence from Section
    5.1. On the top, we have an example of a cupcake, where our algorithm is able
    to estimate the contours of the cupcake decorations. On the bottom, we have an
    image of a flat cat figurine. We can see that our algorithm is able to recover
    the curvature of the body and face. For comparison against ground truth, we use
    the NextEngine 3D scanner to obtain the ground truth and align each of the resulting
    depth maps using the iterative closest point (ICP) algorithm. The color diagram
    shows the Euclidean distance of each ground truth point to the closest point after
    the ICP transformation for each algorithm. We can see that we align closely with
    the ground truth with the lowest RMSE.
  Figure 2 Link: articels_figures_by_rev_year\2016\Shape_Estimation_from_Shading_Defocus_and_Correspondence_Using_LightField_Angula\figure_2.jpg
  Figure 2 caption: Defocus and Correspondence Strengths and Weaknesses. Each cue
    has its benefits and limitations. Most previous works use one cue or another,
    as it is hard to acquire and combine both in the same framework. In our paper,
    we exploit the strengths of both cues. Additionally, we provide further refinement,
    using the shading cue.
  Figure 3 Link: articels_figures_by_rev_year\2016\Shape_Estimation_from_Shading_Defocus_and_Correspondence_Using_LightField_Angula\figure_3.jpg
  Figure 3 caption: Defocus and Correspondence Framework. This setup shows three different
    poles at different depths with a top view (a) and camera view (b). The light-field
    camera captures an image (c) with its epipolar image. By processing each row's
    EPI (d), we shear the EPI to perform refocusing. Our contribution lies in computing
    both defocus analysis (e), which integrates along angle u (vertically) and computes
    the spatial x (horizontal) gradient, and correspondence (f), which computes the
    angular u (vertical) variance. The response to each shear value is shown in (g)
    and (h). By combining the two cues through regularization, the algorithm produces
    high quality depth estimation (i). In Sections 4 and 5, we refine the defocus
    and correspondence measure and incorporate shading information to our regularization
    to produce better shape and normal estimation results by using angular coherence.
    With angular coherence, our defocus and correspondence measures are more robust
    in scenes with less texture and edges.
  Figure 4 Link: articels_figures_by_rev_year\2016\Shape_Estimation_from_Shading_Defocus_and_Correspondence_Using_LightField_Angula\figure_4.jpg
  Figure 4 caption: Contrast-Based Defocus and Correspondence Results. Defocus consistently
    shows better results at noisy regions and repeating patterns, while correspondence
    provides sharper results. By combining both cues, our method provides more consistent
    results in real-world examples. The two low light images on the top show how our
    algorithm is able to estimate depth even at high ISO settings. The flowers (bottom
    left and right) show how we recover complicated shapes and scenes. By combining
    both cues, our algorithm still produces reasonable results. However, we can see
    that the contrast-based defocus and correspondence measures perform poorly in
    scenes where textures and edges are absent ( Figs. 10, 11, and 13). Therefore,
    we develop more robust cue measurements with angular coherence in Section 5.
  Figure 5 Link: articels_figures_by_rev_year\2016\Shape_Estimation_from_Shading_Defocus_and_Correspondence_Using_LightField_Angula\figure_5.jpg
  Figure 5 caption: "Angular Coherence and Refocusing. In a scene where the main lens\
    \ is focused to point X with a distance \u03B1 \u2217 from the camera, the micro-lenses\
    \ enable the sensor to capture different viewpoints represented as angular pixels\
    \ as shown on the bottom. As noted by Seitz and Dyer [6], the angular pixels exhibit\
    \ angular coherence, which gives us photo, depth, and shading consistency. In\
    \ our paper, we extend this analysis by finding a relationship between angular\
    \ coherence and refocusing, as described in Section 4 . In captured data, pixels\
    \ are not guaranteed to focus at \u03B1 (shown on the top). Therefore, we cannot\
    \ enforce angular coherence on the initial captured light-field image. We need\
    \ to shear the initial light-field image using Eq. (1) from Section 4, use the\
    \ angular coherence constraints from Section 4, and remap the constraints back\
    \ to the original coordinates using Eq. (6) from Section 4.1 ."
  Figure 6 Link: articels_figures_by_rev_year\2016\Shape_Estimation_from_Shading_Defocus_and_Correspondence_Using_LightField_Angula\figure_6.jpg
  Figure 6 caption: Pipeline. The pipeline of our algorithm contains multiple steps
    to estimate the depth of our input light-field image (a). The first is to locally
    estimate the depth (line 2), which provides us both confidence (b) and local depth
    estimation (c). We use these two to regularize depth without shading cues (d)
    (line 3). The depth is planar, which motivates us to use shading information to
    refine our depth. We first estimate shading (e) (line 4), which is used to estimate
    lighting (f) (line 5). We then use the lighting, shading, initial depth, and confidence
    to regularize into our final depth (g) (line 6).
  Figure 7 Link: articels_figures_by_rev_year\2016\Shape_Estimation_from_Shading_Defocus_and_Correspondence_Using_LightField_Angula\figure_7.jpg
  Figure 7 caption: 'Depth estimation using angular coherence. On the top, we have
    a scene with a dinosaur. Even refocused to a non-optimal depth, not equal to alpha
    , high contrast still exists. By using a contrast based defocus measure, the optimal
    response is hard to distinguish. On the bottom, we have a scene with a black dot
    in the center. When refocused at a non-optimal depth, the angular pixels may exhibit
    the same color as the neighboring pixels. Both the optimal and non-optimal alpha
    measures would have low variance. However, by using angular coherence to compute
    the measures, we can see that, in both cases, the resulting measure better differentiates
    alpha from the rest, giving us better depth estimation and confidence (also in
    Fig. 10). Note: For defocus measurement, we inverted the contrast-based defocus
    response for clearer visualization.'
  Figure 8 Link: articels_figures_by_rev_year\2016\Shape_Estimation_from_Shading_Defocus_and_Correspondence_Using_LightField_Angula\figure_8.jpg
  Figure 8 caption: Angular Coherence and Robust Shading. From the shading image we
    generate (a), without angular coherency causes noise and unwanted artifacts (b).
    With angular coherence, the noise reduces. Quantitatively, we can see these effects
    in Fig. 9.
  Figure 9 Link: articels_figures_by_rev_year\2016\Shape_Estimation_from_Shading_Defocus_and_Correspondence_Using_LightField_Angula\figure_9.jpg
  Figure 9 caption: Qualitative and quantitative synthetic measurement. We have a
    simple diffuse ball lit by a distant point light-source (a). With just regularization
    without shading information, our depth estimation does not represent the shape
    (b,c). With our shading image (d), our depth estimation recovers the ball's surface
    (e,f). We added a Gaussian noise with a variable variance. Without the shading
    constraint, the RMSE against ground truth shading and depth are high. Angular
    coherence results lower RMSE for both shading and depth.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Michael W. Tao
  Name of the last author: Ravi Ramamoorthi
  Number of Figures: 13
  Number of Tables: 0
  Number of authors: 6
  Paper title: Shape Estimation from Shading, Defocus, and Correspondence Using Light-Field
    Angular Coherence
  Publication Date: 2016-04-14 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2554121
- Affiliation of the first author: electrical and computer engineering department
    and cylab biometrics center, carnegie mellon university, pittsburgh, pa
  Affiliation of the last author: concordia university, montreal, qc, canada
  Figure 1 Link: articels_figures_by_rev_year\2016\Compressed_Submanifold_Multifactor_Analysis\figure_1.jpg
  Figure 1 caption: 'Multi-factors in Extended Yale-B DB [9]: (A) Distribution of
    the first two principal components trained on first 3 subjects with 9 poses and
    64 lighting conditions, (B) Distribution of 64 lighting conditions of the first
    subject, (C) Face images of the first subject across 11 lighting conditions and
    9 different poses.'
  Figure 10 Link: articels_figures_by_rev_year\2016\Compressed_Submanifold_Multifactor_Analysis\figure_10.jpg
  Figure 10 caption: Examples of RP for dimensionality reduction. (A) the first two
    eigenvectors trained from all images of the first subject in Yale-B database,
    (B) the first two eigenvectors trained from the images as in (A) projected on
    RP subspace at 50 percent, (C) the first two eigenvectors trained from those images
    projected on RP subspace at only 10 percent.
  Figure 2 Link: articels_figures_by_rev_year\2016\Compressed_Submanifold_Multifactor_Analysis\figure_2.jpg
  Figure 2 caption: Multifactor data presented in Tensor (left) and the corresponding
    elementary Tensor projection (right).
  Figure 3 Link: articels_figures_by_rev_year\2016\Compressed_Submanifold_Multifactor_Analysis\figure_3.jpg
  Figure 3 caption: (A) Tensor with missing values, and (B) the tensor and multifactor
    flattening process.
  Figure 4 Link: articels_figures_by_rev_year\2016\Compressed_Submanifold_Multifactor_Analysis\figure_4.jpg
  Figure 4 caption: "In HOSVD, the three-order tensor X is decomposed into a core\
    \ tensor Z and three orthogonal matrices: U (pixels), V 1 (subjects) and V 2 (lightings).\
    \ In MPCA, HOSVD is reformulated in terms of matrices, instead of tensors, using\
    \ the Kronecker product. The 1st three images of subject 1 are presented in the\
    \ 1st column V 1 1 of V \u22A4 1 , while the next three images of subject 2 are\
    \ in the 2nd column V 2 1 of V \u22A4 1 . Similar ideas are used in three lighting\
    \ conditions V \u22A4 2 ."
  Figure 5 Link: articels_figures_by_rev_year\2016\Compressed_Submanifold_Multifactor_Analysis\figure_5.jpg
  Figure 5 caption: "The illustration to averaging process with 6 training images\
    \ of 3 subjects and 2 lighting conditions. The 6\xD76 matrix is the Gram matrix\
    \ of the reordered images with an appropriate permutation matrix. In (A), two\
    \ 3\xD73 blocks in grey are the Gram matrices of 2 lightings. Each of two subsets\
    \ consists of 3 subjects' faces under each of 2 lightings. The averaging of the\
    \ Gram matrix G 1 of these 3\xD73 block matrices in grey presents the average\
    \ dot products among 3 subjects across 2 lightings. This process is applied similarly\
    \ for the subject factor in (B)."
  Figure 6 Link: articels_figures_by_rev_year\2016\Compressed_Submanifold_Multifactor_Analysis\figure_6.jpg
  Figure 6 caption: "Comparison between the subspaces built via SVD- \u2113 2 and\
    \ SVD- \u2113 1 . It is easy to see that SVD- \u2113 2 is distorted by two outliers.\
    \ Meanwhile SVD- \u2113 1 still produces a robust subspace."
  Figure 7 Link: articels_figures_by_rev_year\2016\Compressed_Submanifold_Multifactor_Analysis\figure_7.jpg
  Figure 7 caption: Basis eigenvectors produced from CMU-PIE DB. (A) The first six
    eigenvectors trained by SVD- ell 2 on three subjects at frontal pose and 21 different
    lighting conditions, (B) The corresponding eigenvectors trained by our proposed
    SVD- ell 1 method.
  Figure 8 Link: articels_figures_by_rev_year\2016\Compressed_Submanifold_Multifactor_Analysis\figure_8.jpg
  Figure 8 caption: 'Eigenvectors using SVD- ell 1 on CMU-PIE: (A) Subject variations,
    (B) Pose variations, and (C) Lighting variations.'
  Figure 9 Link: articels_figures_by_rev_year\2016\Compressed_Submanifold_Multifactor_Analysis\figure_9.jpg
  Figure 9 caption: A comparison between MPCA and CSMA. (A) contains three submanifolds
    with different structures under 30 lighting conditions at three different poses
    from Extended Yale-B. (B) In MPCA decomposition, the global geometry is preserved
    by averaging all three submanifolds to the same structure. In other words, PCA
    preserves the distances between all pairs of samples regardless of the presence
    of multiple factors. (C) In CSMA decomposition, it aims to preserve all of the
    blue and red curves, not merely their averages. Thus, the reconstruction obtained
    by CSMA more reliably represents the original structure than that obtained by
    MPCA.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.55
  Name of the first author: Khoa Luu
  Name of the last author: Ching Y. Suen
  Number of Figures: 17
  Number of Tables: 4
  Number of authors: 4
  Paper title: Compressed Submanifold Multifactor Analysis
  Publication Date: 2016-04-14 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Reconstruction Errors (Mean \xB1 SD) on Tensors with Missing\
      \ Values"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 CSMA Reconstruction Errors (PSNR) on Tensors with Noisy Values\
      \ (Mean \xB1 SD)"
  Table 3 caption:
    table_text: TABLE 3 Image Inpainting Comparison between Our CSMA Method and Liu
      et al. Method [1]
  Table 4 caption:
    table_text: TABLE 4 Computational Times of the Tensor Based Method in Face Recognition
      on CMU-MPIE Database [44]
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2554107
- Affiliation of the first author: "epita research and development laboratory (lrde),\
    \ 14-16 rue voltaire, fr-94270 le kremlin-bic\xEAtre"
  Affiliation of the last author: "laboratoire d'informatique gaspard-monge, universit\xE9\
    \ paris-est, \xE9quipe a3si, esiee paris, noisy-le-grand, france"
  Figure 1 Link: articels_figures_by_rev_year\2016\Hierarchical_Segmentation_Using_TreeBased_Shape_Spaces\figure_1.jpg
  Figure 1 caption: 'Shape-space filtering framework: we replace the black path with
    the red path, thus extending the classical connected operator framework (black
    path).'
  Figure 10 Link: articels_figures_by_rev_year\2016\Hierarchical_Segmentation_Using_TreeBased_Shape_Spaces\figure_10.jpg
  Figure 10 caption: 'Hierarchical segmentation results on the BSDS500. From left
    to right: Image, inverted saliency map, and segmentations at the optimal dataset
    scale (ODS) and optimal image scale (OIS).'
  Figure 2 Link: articels_figures_by_rev_year\2016\Hierarchical_Segmentation_Using_TreeBased_Shape_Spaces\figure_2.jpg
  Figure 2 caption: "A synthetic example of a hierarchy transformation through M E\
    \ . Blue numbers are the attribute values (e.g., inverse of average of gradient's\
    \ magnitude along the contour). For visualization purpose, the inverse of M E\
    \ is shown. A cut (the red dashed line) of this new hierarchy H \u2032 is different\
    \ from any cut of H (e.g., the red cut or the green cut in (b))."
  Figure 3 Link: articels_figures_by_rev_year\2016\Hierarchical_Segmentation_Using_TreeBased_Shape_Spaces\figure_3.jpg
  Figure 3 caption: A synthetic image (a), and its associated dendrogram (b) representing
    a hierarchy of image segmentations H . The red dashed curve in (b) is a cut of
    the dendrogram (i.e., hierarchy), the partition given by this cut is illustrated
    with red boundaries in (a).
  Figure 4 Link: articels_figures_by_rev_year\2016\Hierarchical_Segmentation_Using_TreeBased_Shape_Spaces\figure_4.jpg
  Figure 4 caption: 'General overview of the proposed scheme: shape space construction
    by a hierarchical representation of the image, followed by the computation of
    extinction values E via a Min-tree TT constructed on the shape space. Finally,
    the saliency map M E is obtained by weighing the boundaries of candidate regions
    by the extinction values of the corresponding local minima.'
  Figure 5 Link: articels_figures_by_rev_year\2016\Hierarchical_Segmentation_Using_TreeBased_Shape_Spaces\figure_5.jpg
  Figure 5 caption: "Object spotting by repeating the process of selecting the \u201C\
    most likely\u201D node Ri and discarding the ancestors and descendants of node\
    \ Ri . Blue values are corresponding attribute values mathcal A . The three nodes\
    \ represented by red circles are the spotted objects, where R1 is spotted firstly,\
    \ then R2 and R3 . Note that the green node R13 is more meaningful than R11 ,\
    \ but it is not spotted."
  Figure 6 Link: articels_figures_by_rev_year\2016\Hierarchical_Segmentation_Using_TreeBased_Shape_Spaces\figure_6.jpg
  Figure 6 caption: 'An example of the object spotting scheme by selecting important
    local minima of an attribute mathcal A as meaningful objects. The shape space
    is built from the tree of shapes, and mathcal A is the context-based energy estimator
    [35]. Left: Tree weighted by mathcal A (top) and filtered mathcal Aprime (bottom);
    Filled circles: local minima; Colorized filled circles: resistant local minima
    after connected filtering in the shape space. Middle: Evolution of mathcal A (top)
    and filtered mathcal Aprime (bottom) along the branch surrounded by dashed contours
    in the tree. Right: extinction-based saliency map mathcal M mathcal E (top) and
    spotted meaningful objects surrounded by the colorized contour.'
  Figure 7 Link: articels_figures_by_rev_year\2016\Hierarchical_Segmentation_Using_TreeBased_Shape_Spaces\figure_7.jpg
  Figure 7 caption: Illustration of extinction values.
  Figure 8 Link: articels_figures_by_rev_year\2016\Hierarchical_Segmentation_Using_TreeBased_Shape_Spaces\figure_8.jpg
  Figure 8 caption: Materialization of pixels with 0-faces (blue disks), 1-faces (green
    strips), and 2-faces (red squares). The original pixels are the 2-faces, the boundaries
    are materialized with 0-faces and 1-faces. The contour of the cyan region is composed
    of black 1-faces and 0-faces.
  Figure 9 Link: articels_figures_by_rev_year\2016\Hierarchical_Segmentation_Using_TreeBased_Shape_Spaces\figure_9.jpg
  Figure 9 caption: An example showing the scheme of the saliency map computation.
    The saliency maps are inverted for better visualization.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Yongchao Xu
  Name of the last author: Laurent Najman
  Number of Figures: 15
  Number of Tables: 3
  Number of authors: 4
  Paper title: Hierarchical Segmentation Using Tree-Based Shape Spaces
  Publication Date: 2016-04-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Region Benchmarks on the BSDS300 [39] and BSDS500 [1]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Evaluation on GTSDB Test Dataset [40]
  Table 3 caption:
    table_text: TABLE 3 Quantitative Results on the Dataset of SmartDoc Competition
      [41]
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2554550
- Affiliation of the first author: department of computing, imperial college london,
    london, united kingdom
  Affiliation of the last author: department of computing, imperial college london,
    london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2016\A_Deep_Matrix_Factorization_Method_for_Learning_Attribute_Representations\figure_1.jpg
  Figure 1 caption: (a) A Semi-NMF model results in a linear transformation of the
    initial input space. (b) Deep Semi-NMF learns a hierarchy of hidden representations
    that aid in uncovering the final lower-dimensional representation of the data.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\A_Deep_Matrix_Factorization_Method_for_Learning_Attribute_Representations\figure_2.jpg
  Figure 2 caption: A Deep Semi-NMF model learns a hierarchical structure of features,
    with each layer learning a representation suitable for clustering according to
    the different attributes of our data. In this simplified, for demonstration purposes,
    example from the CMU Multi-PIE database, a Deep Semi-NMF model is able to simultaneously
    learn features for pose clustering ( H 1 ), for expression clustering ( H 2 ),
    and for identity clustering ( H 3 ). Each of the images in X has an associated
    colour coding that indicates its memberships according to each of these attributes
    (poseexpressionidentity).
  Figure 3 Link: articels_figures_by_rev_year\2016\A_Deep_Matrix_Factorization_Method_for_Learning_Attribute_Representations\figure_3.jpg
  Figure 3 caption: A weakly-supervised Deep Semi-NMF model uses prior knowledge we
    have about the attributes of our model to improve the final representation of
    our data. In this illustration we incorporate information from pose, expression,
    and identity attributes into the three feature layers of our model H pose , H
    expression , and H identity respectively.
  Figure 4 Link: articels_figures_by_rev_year\2016\A_Deep_Matrix_Factorization_Method_for_Learning_Attribute_Representations\figure_4.jpg
  Figure 4 caption: 'XM2VTS-Pixel Intensities: Accuracy for clustering based on the
    representations learned by each model with respect to identities. The deep architectures
    are comprised of two representation layers (1260-625- a ) and the representations
    used were from the top layer. In parentheses we show the AUC scores.'
  Figure 5 Link: articels_figures_by_rev_year\2016\A_Deep_Matrix_Factorization_Method_for_Learning_Attribute_Representations\figure_5.jpg
  Figure 5 caption: "CMU PIE\u2013Pixel Intensities: Accuracy for clustering based\
    \ on the representations learned by each model with respect to identities. The\
    \ deep architectures are comprised of two representation layers (1024-625- a )\
    \ and the representations used were from the top layer. In parenthesese we show\
    \ the AUC scores."
  Figure 6 Link: articels_figures_by_rev_year\2016\A_Deep_Matrix_Factorization_Method_for_Learning_Attribute_Representations\figure_6.jpg
  Figure 6 caption: "CASIA WebFace\u2013Pixel Intensities: Accuracy for clustering\
    \ based on the representations learned by each model with respect to identities.\
    \ The deep architectures are comprised of two representation layers (10000-625-\
    \ a ) and the representations used were from the top layer. In parentheses we\
    \ show the AUC scores."
  Figure 7 Link: articels_figures_by_rev_year\2016\A_Deep_Matrix_Factorization_Method_for_Learning_Attribute_Representations\figure_7.jpg
  Figure 7 caption: 'Supervised pre-training: Clustering accuracy on the CMU PIE dataset,
    after supervised training on the XM2VTS dataset using a priori Deep Semi-NMF.
    In parentheses we show the AUC scores.'
  Figure 8 Link: articels_figures_by_rev_year\2016\A_Deep_Matrix_Factorization_Method_for_Learning_Attribute_Representations\figure_8.jpg
  Figure 8 caption: A three layer Deep WSF model trained on CMU MultiPIE with only
    frontal illumination (camera 5). The bars depict the accuracy levels for the pose
    (), emotion (), and identity ( ) respectively, for each layer, with a linear SVM
    classifier.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: George Trigeorgis
  Name of the last author: "Bj\xF6rn W. Schuller"
  Number of Figures: 8
  Number of Tables: 2
  Number of authors: 4
  Paper title: A Deep Matrix Factorization Method for Learning Attribute Representations
  Publication Date: 2016-04-15 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 The Reconstruction Error ( \u2225X\u2212 X ~ \u2225 2 F )\
      \ for Each of the Algorithms on the CMU PIE Dataset, for a Variable Number of\
      \ Components"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Performance in Classification Accuracy on the CMU Multi-PIE
      Dataset Using an SVM Classifier on Top of the Learned Features
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2554555
- Affiliation of the first author: department of computer science and automation,
    indian institute of science, bangalore, karnataka, india
  Affiliation of the last author: department of computer science and automation, indian
    institute of science, bangalore, karnataka, india
  Figure 1 Link: articels_figures_by_rev_year\2016\Bayesian_Modeling_of_Temporal_Coherence_in_Videos_for_Entity_Discovery_and_Summa\figure_1.jpg
  Figure 1 caption: 'Top: a window consisting of frames 20000,20001,20002, Bottom:
    another window- with frames 21000,21001,21002. The detections are linked on spatio-temporal
    basis to form tracklets. One person (marked with red) occurs in both windows,
    the other character (marked with blue) occurs only in the second. The two red
    tracklets should be associated though they are from non-contiguous windows.'
  Figure 10 Link: articels_figures_by_rev_year\2016\Bayesian_Modeling_of_Temporal_Coherence_in_Videos_for_Entity_Discovery_and_Summa\figure_10.jpg
  Figure 10 caption: Entity-based summarization of Mahabharata Episode 22 using TC-CRF.
    Each image is a reshaped cluster mean.
  Figure 2 Link: articels_figures_by_rev_year\2016\Bayesian_Modeling_of_Temporal_Coherence_in_Videos_for_Entity_Discovery_and_Summa\figure_2.jpg
  Figure 2 caption: A flow-chart illustrating the overall approach, especially the
    pre-processing.
  Figure 3 Link: articels_figures_by_rev_year\2016\Bayesian_Modeling_of_Temporal_Coherence_in_Videos_for_Entity_Discovery_and_Summa\figure_3.jpg
  Figure 3 caption: 'TC at Detection level: Detections in successive frames (linked
    to form a tracklet) are almost identical in appearance, i.e., have nearly identical
    visual features.'
  Figure 4 Link: articels_figures_by_rev_year\2016\Bayesian_Modeling_of_Temporal_Coherence_in_Videos_for_Entity_Discovery_and_Summa\figure_4.jpg
  Figure 4 caption: 'TC at Tracklet level: Blue tracklets 1,3 are spatio-temporally
    close (connected by broken lines), and belong to the same person. Similarly red
    tracklets 2 and 4.'
  Figure 5 Link: articels_figures_by_rev_year\2016\Bayesian_Modeling_of_Temporal_Coherence_in_Videos_for_Entity_Discovery_and_Summa\figure_5.jpg
  Figure 5 caption: Face detections (top), and the corresponding atoms (reshaped to
    square images) found by TC-CRP (bottom).
  Figure 6 Link: articels_figures_by_rev_year\2016\Bayesian_Modeling_of_Temporal_Coherence_in_Videos_for_Entity_Discovery_and_Summa\figure_6.jpg
  Figure 6 caption: Different atoms for different poses of same person.
  Figure 7 Link: articels_figures_by_rev_year\2016\Bayesian_Modeling_of_Temporal_Coherence_in_Videos_for_Entity_Discovery_and_Summa\figure_7.jpg
  Figure 7 caption: Non-face tracklet vectors (reshaped) recovered by TC-CRP. Note
    that one face tracklet has been wrongly reported as non-face.
  Figure 8 Link: articels_figures_by_rev_year\2016\Bayesian_Modeling_of_Temporal_Coherence_in_Videos_for_Entity_Discovery_and_Summa\figure_8.jpg
  Figure 8 caption: Car detections (top), and the corresponding atoms (reshaped to
    square images) found by TC-CRP (bottom).
  Figure 9 Link: articels_figures_by_rev_year\2016\Bayesian_Modeling_of_Temporal_Coherence_in_Videos_for_Entity_Discovery_and_Summa\figure_9.jpg
  Figure 9 caption: Entity-based summarization of Big Bang Theory episode 4 using
    TC-CRF.
  First author gender probability: 0.84
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Adway Mitra
  Name of the last author: Chiranjib Bhattacharyya
  Number of Figures: 13
  Number of Tables: 14
  Number of authors: 3
  Paper title: Bayesian Modeling of Temporal Coherence in Videos for Entity Discovery
    and Summarization
  Publication Date: 2016-04-21 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Details of Datasets
  Table 10 caption:
    table_text: TABLE 10 Purity, Entity Coverage and Tracklet Coverage Results for
      Different Methods on Cars and Aeroplanes Videos
  Table 2 caption:
    table_text: TABLE 2 Comparison of TCCRP and HMRF by Clustering Accuracy on the
      Benchmark Dataset
  Table 3 caption:
    table_text: TABLE 3 Purity Results for Different Methods
  Table 4 caption:
    table_text: TABLE 4 Entity Coverage Results for Different Methods
  Table 5 caption:
    table_text: TABLE 5 Tracklet Coverage Results for Different Methods
  Table 6 caption:
    table_text: TABLE 6 Cluster Purity Results for Different Methods When Number of
      Significant Clusters Is Same (Shown in Brackets)
  Table 7 caption:
    table_text: TABLE 7 Online (Single-Pass) Analysis on four Videos
  Table 8 caption:
    table_text: TABLE 8 Discovery of Non-Face Tracklets
  Table 9 caption:
    table_text: TABLE 9 Fraction of Ground Truth Tracks That Are Fully Linked
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2557785
- Affiliation of the first author: university of kentucky, lexington, ky
  Affiliation of the last author: university of kentucky, lexington, ky
  Figure 1 Link: articels_figures_by_rev_year\2016\RealTime_Simultaneous_Pose_and_Shape_Estimation_for_Articulated_Objects_Using_a_\figure_1.jpg
  Figure 1 caption: Our novel algorithm effectively estimates the pose of articulated
    objects using one single depth camera, such as human and dogs, even with challenging
    cases.
  Figure 10 Link: articels_figures_by_rev_year\2016\RealTime_Simultaneous_Pose_and_Shape_Estimation_for_Articulated_Objects_Using_a_\figure_10.jpg
  Figure 10 caption: Visual comparison of our results (second row) with KinectSDK
    V1 (first row) on some relatively complex poses.
  Figure 2 Link: articels_figures_by_rev_year\2016\RealTime_Simultaneous_Pose_and_Shape_Estimation_for_Articulated_Objects_Using_a_\figure_2.jpg
  Figure 2 caption: "The differential bone coordinates [39]. g k is the location of\
    \ joint k and d k is the vector from parent of joint k to itself. Each vector\
    \ \u03B7 m,k encodes the relative position of the vertex v m with respect to bone\
    \ k . The differential bone coordinate defined in Eq. (27) accumulates such relative\
    \ positions along all controlling bones of this vertex and encodes the information\
    \ of surface geometry."
  Figure 3 Link: articels_figures_by_rev_year\2016\RealTime_Simultaneous_Pose_and_Shape_Estimation_for_Articulated_Objects_Using_a_\figure_3.jpg
  Figure 3 caption: An example of automatic approximation of the our original skinned
    template model (left) with a set of 100 spheres (right) using the algorithm in
    [6].
  Figure 4 Link: articels_figures_by_rev_year\2016\RealTime_Simultaneous_Pose_and_Shape_Estimation_for_Articulated_Objects_Using_a_\figure_4.jpg
  Figure 4 caption: "Comparison of the two different approaches to define the Gaussian\
    \ mixture centroids ( a \u0398 mn ) in Eq. (39). Here c is the sphere center and\
    \ x is an input point with normal n . The closest point on the sphere to x is\
    \ the point a \u2032 . However, the desired corresponding point for x should actually\
    \ be on the opposite side of the sphere, which would favor the moving of input\
    \ points (blue area) to align with the sphere. Therefore our normal-based intersection\
    \ point a is a better candidate."
  Figure 5 Link: articels_figures_by_rev_year\2016\RealTime_Simultaneous_Pose_and_Shape_Estimation_for_Articulated_Objects_Using_a_\figure_5.jpg
  Figure 5 caption: The distance transform map computed from the segmentation mask.
    See Section 5.2.1 for details.
  Figure 6 Link: articels_figures_by_rev_year\2016\RealTime_Simultaneous_Pose_and_Shape_Estimation_for_Articulated_Objects_Using_a_\figure_6.jpg
  Figure 6 caption: The overall system block diagram. The input are depth videos and
    a prior generic model for the subject to be tracked. The output are a personalized
    shape model and the pose for each frame.
  Figure 7 Link: articels_figures_by_rev_year\2016\RealTime_Simultaneous_Pose_and_Shape_Estimation_for_Articulated_Objects_Using_a_\figure_7.jpg
  Figure 7 caption: Quantitative evaluation of our tracker on the SMMC-10 dataset
    [19] with two error metrics. Notice that in (b), although the method by Ye et
    al. [46] achieves comparative accuracy, their reported computation time is much
    higher.
  Figure 8 Link: articels_figures_by_rev_year\2016\RealTime_Simultaneous_Pose_and_Shape_Estimation_for_Articulated_Objects_Using_a_\figure_8.jpg
  Figure 8 caption: Quantitative evaluations of our tracker's accuracy with comparisons
    with the state of the arts.
  Figure 9 Link: articels_figures_by_rev_year\2016\RealTime_Simultaneous_Pose_and_Shape_Estimation_for_Articulated_Objects_Using_a_\figure_9.jpg
  Figure 9 caption: Quantitative evaluations of the effectiveness of the free space
    and the self-collision constraints.
  First author gender probability: 0.77
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Mao Ye
  Name of the last author: Ruigang Yang
  Number of Figures: 18
  Number of Tables: 2
  Number of authors: 5
  Paper title: Real-Time Simultaneous Pose and Shape Estimation for Articulated Objects
    Using a Single Depth Camera
  Publication Date: 2016-04-21 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Three Datasets We Use for Quantitative Evaluations
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Parameter Settings for Our Experiments
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2557783
- Affiliation of the first author: mpi for informatics, saarland, germany
  Affiliation of the last author: mpi for informatics, saarland, germany
  Figure 1 Link: articels_figures_by_rev_year\2016\MARCOnIConvNetBased_MARkerLess_Motion_Capture_in_Outdoor_and_Indoor_Scenes\figure_1.jpg
  Figure 1 caption: 'Our ConvNet-based marker-less motion capture algorithm reconstructs
    joint angles of multiple people performing complex motions in outdoor settings,
    such as in this scene recorded with only three mobile phones: (left) 3D pose overlaid
    with one camera view, (right) 3D visualization of captured skeletons.'
  Figure 10 Link: articels_figures_by_rev_year\2016\MARCOnIConvNetBased_MARkerLess_Motion_Capture_in_Outdoor_and_Indoor_Scenes\figure_10.jpg
  Figure 10 caption: (top) Plot showing the average 3D joint position reconstruction
    error for sequence Marker1 using two input cameras only. (bottom) Visual results
    for variants gen+disc , disc and gen, respectively. Note that the correct reconstruction
    of the pose (e.g., hands and feet) is only possible with the combined terms in
    the energy function (gen+disc ).
  Figure 2 Link: articels_figures_by_rev_year\2016\MARCOnIConvNetBased_MARkerLess_Motion_Capture_in_Outdoor_and_Indoor_Scenes\figure_2.jpg
  Figure 2 caption: 'SoG model overview. Left: Body model generated from example input
    images. Middle Default skeleton with ConvNet detections (blue joints). Right:
    Image SoG approximation generated from a quad-tree (each cell represents one Gaussian).'
  Figure 3 Link: articels_figures_by_rev_year\2016\MARCOnIConvNetBased_MARkerLess_Motion_Capture_in_Outdoor_and_Indoor_Scenes\figure_3.jpg
  Figure 3 caption: Input image overlaid with the detection likelihood heat map of
    the right knee (intensity of red channel mapped to [0,1] detector output).
  Figure 4 Link: articels_figures_by_rev_year\2016\MARCOnIConvNetBased_MARkerLess_Motion_Capture_in_Outdoor_and_Indoor_Scenes\figure_4.jpg
  Figure 4 caption: "Convolution Network Architecture: The nominal input image sizes\
    \ are 256 \xD7 256, 128 \xD7 128 and 64 \xD7 64. LCN is performed in each resolution\
    \ bank separately. The output convolution features are pairwise added (across\
    \ feature maps) then sent through more Convolution+ReLU stages. There are five\
    \ convolution stages in each bank, including the first fully connected layer."
  Figure 5 Link: articels_figures_by_rev_year\2016\MARCOnIConvNetBased_MARkerLess_Motion_Capture_in_Outdoor_and_Indoor_Scenes\figure_5.jpg
  Figure 5 caption: 'Refinement of the Body Part Detections using the pose posterior.
    Left: Overlay of the heat-map for the right ankle joint over the input image.
    Middle: sampling from pose posterior around the rough 2D position p init j,c (black
    dots). Right: The final refined location of the body part d j,c (blue dot).'
  Figure 6 Link: articels_figures_by_rev_year\2016\MARCOnIConvNetBased_MARkerLess_Motion_Capture_in_Outdoor_and_Indoor_Scenes\figure_6.jpg
  Figure 6 caption: 'Left: Joint detection likelihoods for the right ankle in the
    heat-maps H j,c exhibit notable positional uncertainty, and there are many false
    positives and close-by multiple detections. Right: Even though two body parts
    for the same class (i.e., lower wrist) are close to each other in the images,
    our approach is able to correctly estimate their individual locations.'
  Figure 7 Link: articels_figures_by_rev_year\2016\MARCOnIConvNetBased_MARkerLess_Motion_Capture_in_Outdoor_and_Indoor_Scenes\figure_7.jpg
  Figure 7 caption: 'Qualitative results: From top to bottom particular frames for
    the SBoard , Soccer2 and Volleyball sequences recorded with only 3 cameras. For
    each sequence, from left to right, 3D pose overlaid with an input view and 3D
    visualizations of the captured skeletons.'
  Figure 8 Link: articels_figures_by_rev_year\2016\MARCOnIConvNetBased_MARkerLess_Motion_Capture_in_Outdoor_and_Indoor_Scenes\figure_8.jpg
  Figure 8 caption: 'Qualitative results: From left to right particular frames for
    the Juggling , Soccer , Run2 , Walk2 and Walk1 sequences recorded with only 2-3
    cameras. For each sequence, from top to bottom, 3D pose overlaid with the input
    camera views for two frames and 3D visualizations of the captured skeletons.'
  Figure 9 Link: articels_figures_by_rev_year\2016\MARCOnIConvNetBased_MARkerLess_Motion_Capture_in_Outdoor_and_Indoor_Scenes\figure_9.jpg
  Figure 9 caption: Frame from the Juggling sequence. From left to right, comparison
    between gen+disc, disc and gen, respectively. Our combined approach gen+disc estimates
    limb poses more accurately.
  First author gender probability: 0.0
  Gender of the first author: Not Available
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: A. Elhayek
  Name of the last author: C. Theobalt
  Number of Figures: 14
  Number of Tables: 5
  Number of authors: 9
  Paper title: "MARCOnI\u2014ConvNet-Based MARker-Less Motion Capture in Outdoor and\
    \ Indoor Scenes"
  Publication Date: 2016-04-21 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Specification for Each Sequence in Our Multi-Model Data Set;
      Reference Data: MP= Marker-Based, A3D=Annotated, DMC=Results of [9]'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average Overlap of the Projected 3D SoG with the Person in
      an Input View Not Used for Tracking
  Table 3 caption:
    table_text: TABLE 3 Average 3D Joint Position Error [cm] Relative to the Results
      Computed from Manual Joint Annotations
  Table 4 caption:
    table_text: TABLE 4 Average 3D Joint Position Error [cm] Based on Reference Results
      Computed with Three Different Modalities
  Table 5 caption:
    table_text: TABLE 5 Average 3D Joint Position Error on HumanEva
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2557779
- Affiliation of the first author: department of electrical and electronic engineering,
    university of cagliari, piazza d'armi, cagliari, italy
  Affiliation of the last author: department of electrical and electronic engineering,
    university of cagliari, piazza d'armi, cagliari, italy
  Figure 1 Link: articels_figures_by_rev_year\2016\Statistical_MetaAnalysis_of_Presentation_Attacks_for_Secure_Multibiometric_Syste\figure_1.jpg
  Figure 1 caption: A bimodal system combining face and fingerprint, that can potentially
    incur presentation attacks against either biometric, or both.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Statistical_MetaAnalysis_of_Presentation_Attacks_for_Secure_Multibiometric_Syste\figure_2.jpg
  Figure 2 caption: "Matching score distributions of fake fingerprints (top row) and\
    \ faces (bottom row) for LivDet09-Silicone ( Sensor: Biometrika, Matcher: Veryfinger),\
    \ LivDet11-Alginate (S: Biometrika, M: Bozorth3), LivDet11-Gelatin (S: Italdata,\
    \ M: Bozorth3), LivDet11-Latex (S: Italdata, M: Bozorth3), Personal Photo Attack\
    \ (S : commercial webcam, M: EBGM), 3D Mask Attack (S: Microsoft Kinect, M : ISV),\
    \ Print Attack (S: commercial webcam, M: EBGM), Photo Attack (S : commercial webcam,\
    \ M: PCA). Simulated fake distributions according to our model are also shown\
    \ for comparison, at low risk (LivDet09-Silicone, LivDet11-Alginate, 3D Mask Attack,\
    \ and Personal Photo Attack), medium risk (LivDet11-Gelatin, and Print Attack),\
    \ and high risk (LivDet11-Latex, and Photo Attack). The values of ( \u03BC \u03B1\
    \ , \u03C3 \u03B1 ) used to simulate each attack scenario are reported in Table\
    \ 3."
  Figure 3 Link: articels_figures_by_rev_year\2016\Statistical_MetaAnalysis_of_Presentation_Attacks_for_Secure_Multibiometric_Syste\figure_3.jpg
  Figure 3 caption: "Results of fitting our model to the considered datasets [17],\
    \ [18], [29]. Each real fake distribution is represented as a point with coordinates\
    \ ( \u03BC \u03B1 , \u03C3 \u03B1 ) . A red (green) 'x' ('+') denotes a fake fingerprint\
    \ distribution obtained by the Bozorth3 (Veryfinger) matcher and the Biometrika\
    \ (Italdata) sensor. A blue (black) 'x' denotes a fake face distribution obtained\
    \ by the EBGM (PCA) matcher and a commercial webcam. The black '+' denotes the\
    \ distribution of fake faces for the 3D Mask Attack database, obtained by the\
    \ ISV matcher and the Microsoft Kinect sensor. The area under the dashed black\
    \ curve corresponds to \u03C3 \u03B1 \u2264 \u03BC \u03B1 (1\u2212 \u03BC \u03B1\
    \ ) \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u221A , and\
    \ delimits the family of possible fake distributions generated by our meta-model.\
    \ Low-, medium-, and high-impact presentation attacks clustered to form the corresponding\
    \ attack scenarios are highlighted respectively as blue, orange, and red shaded\
    \ areas."
  Figure 4 Link: articels_figures_by_rev_year\2016\Statistical_MetaAnalysis_of_Presentation_Attacks_for_Secure_Multibiometric_Syste\figure_4.jpg
  Figure 4 caption: Attack impact for each attack scenario of our meta-model (Eq.
    (5) ).
  Figure 5 Link: articels_figures_by_rev_year\2016\Statistical_MetaAnalysis_of_Presentation_Attacks_for_Secure_Multibiometric_Syste\figure_5.jpg
  Figure 5 caption: Beta distributions and attack impact (in parentheses) for the
    three fingerprint and face presentation attack scenarios identified in Fig. 3.
  Figure 6 Link: articels_figures_by_rev_year\2016\Statistical_MetaAnalysis_of_Presentation_Attacks_for_Secure_Multibiometric_Syste\figure_6.jpg
  Figure 6 caption: A Bayesian network equivalent to Eq. (6).
  Figure 7 Link: articels_figures_by_rev_year\2016\Statistical_MetaAnalysis_of_Presentation_Attacks_for_Secure_Multibiometric_Syste\figure_7.jpg
  Figure 7 caption: Results for the considered bimodal system. Plots in the first
    and third column report the average DET curves attained by each fusion rule, when
    no attack is considered ('no spoof'), and under presentation attacks from LivDet15
    ('fingerprint spoofing') and CASIA ('face spoofing'). The yellow and purple shaded
    areas represent the confidence bands for the mathrm SFAR predicted by our approach,
    over the family of fake score distributions represented by our meta-model, for
    face and fingerprint, respectively. The background color of plots in the second
    and fourth column represents the value of the fused score f(mathbf s) for each
    rule, in the space of matching scores. The black solid line represents its decision
    function at mathrm FRR=2 percent. We also report points corresponding to genuine
    users, impostors and presentation attacks, to compare the different decision functions.
  Figure 8 Link: articels_figures_by_rev_year\2016\Statistical_MetaAnalysis_of_Presentation_Attacks_for_Secure_Multibiometric_Syste\figure_8.jpg
  Figure 8 caption: Matching score distributions for CASIA and LivDet15. Fake score
    distributions fitted with our meta-model are shown for comparison. The values
    of (mu alpha,sigma alpha) found to simulate them are (0.33,0.15) for CASIA (attack
    impact= 14 percent), and (0.32,0.23) for LivDet15 (attack impact= 23 percent).
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Battista Biggio
  Name of the last author: Fabio Roli
  Number of Figures: 8
  Number of Tables: 4
  Number of authors: 4
  Paper title: Statistical Meta-Analysis of Presentation Attacks for Secure Multibiometric
    Systems
  Publication Date: 2016-04-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 ISO Standard Nomenclature (Under Development) for Biometric
      Systems and Presentation Attacks [5], [6], and Commonly-Used Alternatives
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Dataset Characteristics: Number of Clients ( Clients), and
      Number of Spoof ( Spoofs) and Live ( live) Images per Client'
  Table 3 caption:
    table_text: TABLE 3 Attack Scenarios for Fingerprint and Faces, and Their Parameters
  Table 4 caption:
    table_text: TABLE 4 Average Percent Performance (and Standard Deviation) Attained
      by Each Rule At FRR=2 Percent, in Terms of FAR , SFAR under the LivDet15 ( SFAR
      Fing.) and CASIA ( SFAR face) Presentation Attacks, SFAR under the Fingerprint
      ( SFAR H1 ) and Face ( SFAR H2 ) High-Impact Simulated Attacks, and the Corresponding
      GFAR Values (Denoted with GFAR for the Livdet15 and CASIA Attacks, and GFAR
      H1,H2 for the Simulated Attacks)
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2558154
- Affiliation of the first author: esat-psi, australia national university, iminds,
    belgium
  Affiliation of the last author: ku leuven, esat-psi, iminds, belgium
  Figure 1 Link: articels_figures_by_rev_year\2016\Rank_Pooling_for_Action_Recognition\figure_1.jpg
  Figure 1 caption: Illustration of how rank pooling works. In this video, as Emma
    moved out from the house, the appearance of the frames evolves with time. A ranking
    machine learns this evolution of the appearance over time and returns a ranking
    function. We use the parameters of this ranking function as a new video representation
    which captures vital information about the action.
  Figure 10 Link: articels_figures_by_rev_year\2016\Rank_Pooling_for_Action_Recognition\figure_10.jpg
  Figure 10 caption: Hollywood2 action recognition performance with respect to the
    length of the video using our rank pooling method.
  Figure 2 Link: articels_figures_by_rev_year\2016\Rank_Pooling_for_Action_Recognition\figure_2.jpg
  Figure 2 caption: "Various pooling operations given data plotted on a 2d feature\
    \ space (gray circles stands for data, red circles for average pooling and yellow\
    \ circles for max pooling, whereas the green dashed lines stand for rank pooling).\
    \ The green dashed hyperplanes returned by our rank pooling not only describe\
    \ nicely the latent data structure, but also are little affected by the random\
    \ data noise. In contrast the average and max pooling represenations are notably\
    \ disturbed. In fact, max pooling even creates \u201Cghost\u201D circles in areas\
    \ of the feature space where no data exist."
  Figure 3 Link: articels_figures_by_rev_year\2016\Rank_Pooling_for_Action_Recognition\figure_3.jpg
  Figure 3 caption: "Using ranking machines for modeling the video temporal evolution\
    \ of appearances, or alternatively, the video dynamics. We see in (a) the original\
    \ signal of independent frame representation, (b) the signal obtained by moving\
    \ average, (c) the signal obtained by time varying mean vector (different colors\
    \ refer to different dimensions in the signal v t ). In (d), (e) and (f) we plot\
    \ the predicted ranking score of each frame obtained from signal (a), (b) and\
    \ (c) respectively after applying the ranking function (predicted ranking value\
    \ at t, s t = u T \u22C5 v t )."
  Figure 4 Link: articels_figures_by_rev_year\2016\Rank_Pooling_for_Action_Recognition\figure_4.jpg
  Figure 4 caption: "Processing steps of rank pooling for action recognition. First,\
    \ we extract frames x 1 \u22EF x n from each video. Then we generate feature v\
    \ t for frame t by processing frames from x 1 to x t as explained in Section 4.\
    \ Afterwards, using ranking machines we learn the video representation u for each\
    \ video. Finally, video specific u vectors are used as a representation for action\
    \ classification."
  Figure 5 Link: articels_figures_by_rev_year\2016\Rank_Pooling_for_Action_Recognition\figure_5.jpg
  Figure 5 caption: Examples from the six action categories in the KTH action recognition
    dataset [31]. From left to right you see the actions boxing, clapping, waving
    , walking, jogging and running. From top to bottom you see an example frame from
    a random video, the forward rank pooling, the reverse rank pooling and the result
    after the standard mean pooling. The rank pooling as well as the mean representations
    are computed on the image pixels. We observe that the forward and reverse rank
    pooling indeed capture some of the crisp, temporal changes of the actions, whereas
    the mean representations lose the details.
  Figure 6 Link: articels_figures_by_rev_year\2016\Rank_Pooling_for_Action_Recognition\figure_6.jpg
  Figure 6 caption: Some example frames from the top performing categories of the
    HMDB51, Hollywood2, and ChaLearn-Gestures dataset, respectively.
  Figure 7 Link: articels_figures_by_rev_year\2016\Rank_Pooling_for_Action_Recognition\figure_7.jpg
  Figure 7 caption: "Per class AP in the Hollywood2 dataset. The AP is improved significantly\
    \ for all classes, with an exception of \u201CDrive car\u201D, where context already\
    \ provides useful information."
  Figure 8 Link: articels_figures_by_rev_year\2016\Rank_Pooling_for_Action_Recognition\figure_8.jpg
  Figure 8 caption: Mean class similarity obtained with (left) max-pooling and (right)
    rank pooling on MPII Cooking activities dataset using BOW-based MBH features extracted
    on dense trajectories. Non-linear forward rank pooling are used for our method.
  Figure 9 Link: articels_figures_by_rev_year\2016\Rank_Pooling_for_Action_Recognition\figure_9.jpg
  Figure 9 caption: Comparison of action recognition performance after removing some
    frames from each video randomly on Hollywood2. rank pooling appears to be stable
    even when up to 20 percent of the frames are missing.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.96
  Name of the first author: Basura Fernando
  Name of the last author: Tinne Tuytelaars
  Number of Figures: 10
  Number of Tables: 10
  Number of authors: 5
  Paper title: Rank Pooling for Action Recognition
  Publication Date: 2016-04-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Different Video Representations for Rank Pooling
  Table 10 caption:
    table_text: TABLE 10 Comparison of the Proposed Approach with the State-of-the-Art
      Methods on Chalearn Gesture Recognition Dataset Sorted by Reverse Chronological
      Order
  Table 2 caption:
    table_text: TABLE 2 One-vs-All Accuracy on HMDB51 Dataset [28]
  Table 3 caption:
    table_text: TABLE 3 Results in mAP on Hollywood2 Dataset [38]
  Table 4 caption:
    table_text: TABLE 4 Results in mAP on MPII Cooking Fine Grained Action Dataset
      [48]
  Table 5 caption:
    table_text: TABLE 5 Detailed Analysis of Precision and Recall on the Chalearn
      Gesture Recognition Dataset [9]
  Table 6 caption:
    table_text: TABLE 6 Comparison of Different Features Maps for Ranking and Classification
  Table 7 caption:
    table_text: TABLE 7 Pooling Parameters as Representations from Different Parametric
      Models
  Table 8 caption:
    table_text: TABLE 8 Results Obtained on Hollywood2 Dataset Using CNN (vgg-16 Network
      [53]) Features
  Table 9 caption:
    table_text: TABLE 9 Comparison of the Proposed Approach with the State-of-the-Art
      Methods Sorted by Reverse Chronological Order
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2558148
