- Affiliation of the first author: department of computer science, university of california,
    los angeles, ca
  Affiliation of the last author: department of statistics and computer science, university
    of california, los angeles, ca
  Figure 1 Link: articels_figures_by_rev_year\2017\Attribute_AndOr_Grammar_for_Joint_Parsing_of_Human_Pose_Parts_and_Attributes\figure_1.jpg
  Figure 1 caption: 'An attributed parse graph for a human image includes three components
    in colored edges: (i) The hierarchical whole-part decomposition (blue); (ii) the
    articulation of body parts (green); and (iii) the attributes associated with each
    node in the hierarchy (red). The parse graph also includes the probabilities and
    thus uncertainty at each node for attributes.'
  Figure 10 Link: articels_figures_by_rev_year\2017\Attribute_AndOr_Grammar_for_Joint_Parsing_of_Human_Pose_Parts_and_Attributes\figure_10.jpg
  Figure 10 caption: Part-attribute relation. We compute mutual information between
    atomic parts and attribute to find the parts that contribute the most to each
    attribute. If the mutual information value is higher than the mean value (the
    yellow lines), we consider the part to be associated with the attribute (the red
    bars). We only compute for 14 terminal parts. The mid-level and root parts will
    synthesize the attribute relations from child nodes. Please see text for details
    and part indexes.
  Figure 2 Link: articels_figures_by_rev_year\2017\Attribute_AndOr_Grammar_for_Joint_Parsing_of_Human_Pose_Parts_and_Attributes\figure_2.jpg
  Figure 2 caption: Examples of our pedestrian attribute dataset. Each image includes
    one target person and keypoint annotations. This dataset consists of many kinds
    of variations in pose and attributes. The attribute categories are shown on Table
    1.
  Figure 3 Link: articels_figures_by_rev_year\2017\Attribute_AndOr_Grammar_for_Joint_Parsing_of_Human_Pose_Parts_and_Attributes\figure_3.jpg
  Figure 3 caption: "Attribute constraints. We infer the pose p g \u2217 either using\
    \ Equations (18) or (20). When we do not have attribute constraints (i.e., using\
    \ Equation (18)) the model selects the part that maximize the scores in local,\
    \ and the part could come from different person, when there are multiple people\
    \ close. However, by having the attribute as global constraints (i.e., using Equation\
    \ (20)), we can enforce model to have consistent attributes which subsequently\
    \ results in better pose-estimation."
  Figure 4 Link: articels_figures_by_rev_year\2017\Attribute_AndOr_Grammar_for_Joint_Parsing_of_Human_Pose_Parts_and_Attributes\figure_4.jpg
  Figure 4 caption: Phrase structure grammar defines coarse-to-fine representation.
    Each grammar rule decomposes a part into smaller constituent parts.
  Figure 5 Link: articels_figures_by_rev_year\2017\Attribute_AndOr_Grammar_for_Joint_Parsing_of_Human_Pose_Parts_and_Attributes\figure_5.jpg
  Figure 5 caption: Dependency grammar defines adjacency relations that connects the
    geometry of a part to its dependent parts. It is well suited for representing
    objects that exhibit large articulated deformations.
  Figure 6 Link: articels_figures_by_rev_year\2017\Attribute_AndOr_Grammar_for_Joint_Parsing_of_Human_Pose_Parts_and_Attributes\figure_6.jpg
  Figure 6 caption: An example of the parse graph following the A-AOG, which includes
    a parse graph for human body detection and pose and a parse graph for human attributes.
  Figure 7 Link: articels_figures_by_rev_year\2017\Attribute_AndOr_Grammar_for_Joint_Parsing_of_Human_Pose_Parts_and_Attributes\figure_7.jpg
  Figure 7 caption: We define 14 atomic parts, 2 mid-level parts, and root part. We
    define nine part types for each part. Each attribute has corresponding parts,
    and connected with part types through attribute relations. In this figure, we
    only show attributes of head and full body, and draw phrase structure grammar
    relation for better illustration.
  Figure 8 Link: articels_figures_by_rev_year\2017\Attribute_AndOr_Grammar_for_Joint_Parsing_of_Human_Pose_Parts_and_Attributes\figure_8.jpg
  Figure 8 caption: We generate part proposals from input image and find positive
    and negative proposals using keypoint annotation information.
  Figure 9 Link: articels_figures_by_rev_year\2017\Attribute_AndOr_Grammar_for_Joint_Parsing_of_Human_Pose_Parts_and_Attributes\figure_9.jpg
  Figure 9 caption: In previous approaches, (left) parts are defined by drawing square
    bounding box around keypoint [13], [38] or by annotating precise bounding box
    [6]. (right) We define parts based on our part proposal process, and it handles
    geometric and scale variation of parts efficiently and effectively.
  First author gender probability: 0.94
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.61
  Name of the first author: Seyoung Park
  Name of the last author: Song-Chun Zhu
  Number of Figures: 14
  Number of Tables: 5
  Number of authors: 3
  Paper title: Attribute And-Or Grammar for Joint Parsing of Human Pose, Parts and
    Attributes
  Publication Date: 2017-07-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Attribute List in Our Pedestrian Attribute Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Attribute Prediction Performance on Attributes of People Dataset
  Table 3 caption:
    table_text: TABLE 3 Results for Attribute Classification on the Proposed Pedestrian
      Attribute Dataset
  Table 4 caption:
    table_text: TABLE 4 Pose Estimation Result on the Attributes of People Dataset
  Table 5 caption:
    table_text: TABLE 5 Comparison of Strict PCP Results on the FLIC Dataset
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2731842
- Affiliation of the first author: department of computing, the hong kong polytechnic
    university, hung hom, hong kong
  Affiliation of the last author: department of computing, the hong kong polytechnic
    university, hung hom, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2017\Robust_Online_Matrix_Factorization_for_Dynamic_Background_Subtraction\figure_1.jpg
  Figure 1 caption: 'Background subtraction results by the proposed OMoGMF method
    on ( a ) bootstrap sequence; ( b ) campus sequence. First row (from left to right):
    Original frame, noise and background extracted by the proposed method. Second
    row: Three noise components (after scale processing) extracted by the method,
    corresponding to the moving object, the shadow along the object, weak camera noise
    (for (a)) and the moving object, leaves shaking variance, weak camera noise (for
    (b)), respectively.'
  Figure 10 Link: articels_figures_by_rev_year\2017\Robust_Online_Matrix_Factorization_for_Dynamic_Background_Subtraction\figure_10.jpg
  Figure 10 caption: 'From upper to lower: Original frames in airport video; aligned
    frames, backgrounds and foregrounds obtained by t-OMoGMF.'
  Figure 2 Link: articels_figures_by_rev_year\2017\Robust_Online_Matrix_Factorization_for_Dynamic_Background_Subtraction\figure_2.jpg
  Figure 2 caption: 'From left to right: Original frames in Camera Jitter videos,
    backgrounds extracted by RASL, t-GRASTA and t-OMoGMF.'
  Figure 3 Link: articels_figures_by_rev_year\2017\Robust_Online_Matrix_Factorization_for_Dynamic_Background_Subtraction\figure_3.jpg
  Figure 3 caption: The graphical model for OMoGMF.
  Figure 4 Link: articels_figures_by_rev_year\2017\Robust_Online_Matrix_Factorization_for_Dynamic_Background_Subtraction\figure_4.jpg
  Figure 4 caption: "Tendency curves of the largest variance ( \u03C3 t 1 ) 2 (with\
    \ scale 10 \u22122 ) and || U t | | F (with scale 10 3 ) along time under different\
    \ values of N t\u22121 and \u03C1 , respectively, for curtain sequence. Typical\
    \ video frames and some foregrounds extracted by our method along time are also\
    \ depicted."
  Figure 5 Link: articels_figures_by_rev_year\2017\Robust_Online_Matrix_Factorization_for_Dynamic_Background_Subtraction\figure_5.jpg
  Figure 5 caption: "Residuals in different iterations of t-OMoGMF on a transformed\
    \ frame of \u201Cairport\u201D (first row)\u201Ccurtain\u201D (second row) video\
    \ in Li dataset."
  Figure 6 Link: articels_figures_by_rev_year\2017\Robust_Online_Matrix_Factorization_for_Dynamic_Background_Subtraction\figure_6.jpg
  Figure 6 caption: Typical frames in nine video sequences of the original and transformed
    Li dataset.
  Figure 7 Link: articels_figures_by_rev_year\2017\Robust_Online_Matrix_Factorization_for_Dynamic_Background_Subtraction\figure_7.jpg
  Figure 7 caption: 'From left to right: Typical frames from curtain and watersurface
    sequences, groundtruth foreground objects, foregrounds detected by all competing
    methods.'
  Figure 8 Link: articels_figures_by_rev_year\2017\Robust_Online_Matrix_Factorization_for_Dynamic_Background_Subtraction\figure_8.jpg
  Figure 8 caption: 'Performance of OMoGMF on video sequences with (a) sudden illumination
    change and (b) missing entries. From upper to lower: (a) Original video frames,
    extracted backgrounds and residuals; (b) Original video frames, 15 percent subsampling
    frames, extracted backgrounds.'
  Figure 9 Link: articels_figures_by_rev_year\2017\Robust_Online_Matrix_Factorization_for_Dynamic_Background_Subtraction\figure_9.jpg
  Figure 9 caption: 'From upper to lower: Original frames in the shoppingmall video,
    backgrounds and residuals (brightening 0.5 gray scales for all pixels for better
    visualization) obtained by OMoGMF with 1 percent sub-sampling rate.'
  First author gender probability: 0.92
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.55
  Name of the first author: Hongwei Yong
  Name of the last author: Lei Zhang
  Number of Figures: 13
  Number of Tables: 7
  Number of authors: 4
  Paper title: Robust Online Matrix Factorization for Dynamic Background Subtraction
  Publication Date: 2017-07-27 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Model Comparison of Typical Subspace-Based Background Subtraction
      Methods
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Model Comparison of Typical Transformed Background Subtraction
      Methods
  Table 3 caption:
    table_text: TABLE 3 F-measure(%) Comparison of All Competing Methods on All Video
      Sequences in Li Dataset
  Table 4 caption:
    table_text: TABLE 4 FPS of Online Competing Methods on Three Videos, Each with
      200 Frames While with Different Frame Size, in Li Dataset
  Table 5 caption:
    table_text: TABLE 5 F-Measure(%) and FPS of OMoGMF and GRASTA Under Different
      Sub-Sampling Rates on Three Videos, Each with 1,000 Frames and Different Frame
      Size, in Li Dataset
  Table 6 caption:
    table_text: TABLE 6 F-Measure(%) of Different Competing Methods on Li Dataset
      Added with Artificial Transformations
  Table 7 caption:
    table_text: TABLE 7 F-Measure(%) Comparison of Transformed Methods on Real Data
      with Camera Jitter
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2732350
- Affiliation of the first author: college of electronic and information engineering,
    south china university of technology, guangzhou, china
  Affiliation of the last author: mathematical institute, university of oxford, oxford,
    united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2017\Learning_SpatialSemantic_Context_with_Fully_Convolutional_Recurrent_Network_for_\figure_1.jpg
  Figure 1 caption: Overview of the proposed method. Variable-length pen-tip trajectories
    are first translated into offline signature feature maps that preserve the essential
    online information. Then, a multi-spatial-context fully convolutional recurrent
    network (MC-FCRN) takes input of the signature feature maps with receptive fields
    of different scales in a sliding window manner and generates a predicting sequence.
    Finally, an implicit LM is proposed to derive the final label sequence by exploiting
    the semantic context of embedding vectors that are transformed from the predicting
    sequence.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Learning_SpatialSemantic_Context_with_Fully_Convolutional_Recurrent_Network_for_\figure_2.jpg
  Figure 2 caption: '(a) Illustration of feature extraction of path signature. (b)
    A simple example of calculation of path signature features. (c) Path signature
    of one typical online handwritten text example. (d) Left: Path signature of the
    original pen-tip trajectories; Right: Path signature of the pen-tip trajectories
    with randomly added connections between adjacent strokes. It is notable that excepting
    for the additional connections, the original part of the sequential data has the
    same path signature (same color).'
  Figure 3 Link: articels_figures_by_rev_year\2017\Learning_SpatialSemantic_Context_with_Fully_Convolutional_Recurrent_Network_for_\figure_3.jpg
  Figure 3 caption: Illustration of multiple spatial contexts. Different receptive
    fields in the same time step have the same center position, and their region sizes
    should satisfy Eq. (14).
  Figure 4 Link: articels_figures_by_rev_year\2017\Learning_SpatialSemantic_Context_with_Fully_Convolutional_Recurrent_Network_for_\figure_4.jpg
  Figure 4 caption: t-SNE visualization of the predicting feature vectors of some
    typical characters extracted from the output sequence of MC-FCRN (left) and implicit
    LM (right). After applying implicit LM, the system manages to rectify most of
    the unreasonably distributed feature vector by incorporating semantic context
    information.
  Figure 5 Link: articels_figures_by_rev_year\2017\Learning_SpatialSemantic_Context_with_Fully_Convolutional_Recurrent_Network_for_\figure_5.jpg
  Figure 5 caption: Illustration of network architecture of MC-FCRN.
  Figure 6 Link: articels_figures_by_rev_year\2017\Learning_SpatialSemantic_Context_with_Fully_Convolutional_Recurrent_Network_for_\figure_6.jpg
  Figure 6 caption: Curves of correct rate of (residual) recurrent network on validation
    set for the first training stage (refer to Section 6.2).
  Figure 7 Link: articels_figures_by_rev_year\2017\Learning_SpatialSemantic_Context_with_Fully_Convolutional_Recurrent_Network_for_\figure_7.jpg
  Figure 7 caption: Unconstrained handwritten Chinese text lines with their corresponding
    labels and the result predicted by the systems.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.79
  Name of the first author: Zecheng Xie
  Name of the last author: Terry Lyons
  Number of Figures: 7
  Number of Tables: 5
  Number of authors: 5
  Paper title: Learning Spatial-Semantic Context with Fully Convolutional Recurrent
    Network for Online Handwritten Chinese Text Recognition
  Publication Date: 2017-07-28 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Effect of Path Signature (Percent)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Effect of Residual Recurrent Network (Percent)
  Table 3 caption:
    table_text: TABLE 3 Effect of Spatial Context (Percent)
  Table 4 caption:
    table_text: TABLE 4 Effect of Semantic Context (Percent)
  Table 5 caption:
    table_text: TABLE 5 Comparison with Previous Methods Based on Correct Rate and
      Accuracy Rate (Percent) for Dataset-ICDAR and Dataset-CASIA
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2732978
- Affiliation of the first author: department of automation, state key lab of intelligent
    technologies and systems and tsinghua national laboratory for information science
    and technology (tnlist), beijing, china
  Affiliation of the last author: department of automation, state key lab of intelligent
    technologies and systems and tsinghua national laboratory for information science
    and technology (tnlist), beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2017\TwoStream_Transformer_Networks_for_VideoBased_Face_Alignment\figure_1.jpg
  Figure 1 caption: 'The pipeline of the proposed two-stream transformer networks
    (TSTN). The input to our TSTN consists of both color (RGB) channels of still images
    and a video clip that contains a sequence of face images. The main objectives
    of our TSTN are two-fold: 1) the RGB channels are exploited as the appearance
    information in the spatial stream, and 2) the multiple frames (a video clip) are
    directly encoded as active appearance codes by exploiting the consistency information
    in the temporal stream. The final positions of facial landmarks are determined
    by a weighted fusion of both the spatial and temporal streams. The parameters
    of the designed networks are jointly optimized by back-propagation. During the
    testing phase, we feed a face sequence as the input to the learned two-stream
    networks and then predict a series of landmark positions for the face video clip.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\TwoStream_Transformer_Networks_for_VideoBased_Face_Alignment\figure_2.jpg
  Figure 2 caption: The work flow of the temporal stream. The proposed temporal stream
    network consists of an encoder-decoder module to encode the spatial information
    as active appearance codes U , and a two-layer RNN module to memorize the temporal
    information to flow across frames. Having obtained the decoded feature maps V
    , we sample a set of patches and transform these spatial-temporal patches to facial
    landmark locations by a convolutional neural network.
  Figure 3 Link: articels_figures_by_rev_year\2017\TwoStream_Transformer_Networks_for_VideoBased_Face_Alignment\figure_3.jpg
  Figure 3 caption: CED curves of our TSTN compared to the state-of-the-arts on three
    categories in 300-VW [34] separately. In contrast to the state-of-the-art methods,
    our TSTN achieves comparable results in category two and superior performance
    in category one and the most difficult category three.
  Figure 4 Link: articels_figures_by_rev_year\2017\TwoStream_Transformer_Networks_for_VideoBased_Face_Alignment\figure_4.jpg
  Figure 4 caption: Resulting examples of our TSTN on the 557th video clip in 300-VW
    [34] Category Three, where the selected tracked subject undergoes severe poses
    over time. The bottom subfigure shows that our TSTN exhibits robustness to difficult
    cases like large variations of facial aspect ratios.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.9
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.59
  Name of the first author: Hao Liu
  Name of the last author: Jie Zhou
  Number of Figures: 4
  Number of Tables: 4
  Number of authors: 4
  Paper title: Two-Stream Transformer Networks for Video-Based Face Alignment
  Publication Date: 2017-08-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Averaged Error Comparisons of Our Proposed TSTN with the State-of-the-Art
      Face Alignment Approaches Including Both Conventional Hand-Crafted Approaches
      (SDM [46], CFSS [50], PIEFA [26] and iCCR [32]) and Deep-Learning Based Approaches
      (TCDCN [49], TSCN [35] and REDN [25]) on 300-VW Dataset [34]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Comparisons of CED Values Where The RMSEs Are Less Than 0.05\
      \ and 0.1 with Different Specifications of \u03B2 1 , \u03B2 2 on the 300-VW\
      \ Fullset [34]"
  Table 3 caption:
    table_text: TABLE 3 Comparisons of CED Values Where The RMSEs Are Less Than 0.05
      and 0.1 with Different Network Decisions on the 300-VW Fullset [34]
  Table 4 caption:
    table_text: TABLE 4 Averaged Error Comparisons of Our Model with the State-of-the-Arts
      on the TF Dataset [1], Where 7 Landmarks Were Employed for Evaluation
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2734779
- Affiliation of the first author: "computer vision laboratory, eth z\xFCrich, switzerland"
  Affiliation of the last author: "computer vision laboratory, eth z\xFCrich, switzerland"
  Figure 1 Link: articels_figures_by_rev_year\2017\Visual_Recognition_in_RGB_Images_and_Videos_by_Learning_from_RGBD_Data\figure_1.jpg
  Figure 1 caption: 'Image recognition in RGB images by learning from RGB-D data:
    we have both RGB images and depth images in the source domain, and only RGB images
    in the target domain.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Wen Li
  Name of the last author: Luc Van Gool
  Number of Figures: 1
  Number of Tables: 3
  Number of authors: 4
  Paper title: Visual Recognition in RGB Images and Videos by Learning from RGB-D
    Data
  Publication Date: 2017-08-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of the Objective Functions from Our Three DAM2S Algorithms
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Accuracies (%) for Objection Recognition (OR),
      Cross-Dataset Human Action Recognition (CD-HAR), and Cross-View Human Action
      Recognition (CV-HAR)
  Table 3 caption:
    table_text: TABLE 3 Comparison of Training Time (Seconds) for Our Three DAM2S
      Methods for the Cross-Dataset Human Action Recognition Task
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2734890
- Affiliation of the first author: school of mathematics and statistics and ministry
    of education key lab of intelligent networks and network security, xi'an jiaotong
    university, shaanxi, p.r. china
  Affiliation of the last author: school of mathematics and statistics and ministry
    of education key lab of intelligent networks and network security, xi'an jiaotong
    university, shaanxi, p.r. china
  Figure 1 Link: articels_figures_by_rev_year\2017\KroneckerBasisRepresentation_Based_Tensor_Sparsity_and_Its_Applications_to_Tenso\figure_1.jpg
  Figure 1 caption: "Illustration of correlation priors on each mode of an MSI. (a)\
    \ A real MSI of size 80\xD780\xD730 . (b) Singular value curves of the matrices\
    \ unfolded along three tensor modes. The low-rank property of the subspace along\
    \ each mode can be easily observed from the dramatic decreasing effect of these\
    \ curves."
  Figure 10 Link: articels_figures_by_rev_year\2017\KroneckerBasisRepresentation_Based_Tensor_Sparsity_and_Its_Applications_to_Tenso\figure_10.jpg
  Figure 10 caption: Performance variation of the proposed method in terms of the
    LRRE on different t and different sampling rate. (a) Ranks along each mode is
    (10, 30, 30) . (b) Ranks along each mode is (25,25,25) .
  Figure 2 Link: articels_figures_by_rev_year\2017\KroneckerBasisRepresentation_Based_Tensor_Sparsity_and_Its_Applications_to_Tenso\figure_2.jpg
  Figure 2 caption: "Illustration of (a) Tucker decomposition and (b) CP decomposition\
    \ of X\u2208 R I 1 \xD7 I 2 \xD7 I 3 . (c) Vector case representation. (d) Matrix\
    \ case decomposition."
  Figure 3 Link: articels_figures_by_rev_year\2017\KroneckerBasisRepresentation_Based_Tensor_Sparsity_and_Its_Applications_to_Tenso\figure_3.jpg
  Figure 3 caption: "(a) An MSI X 0 \u2208 R 80\xD780\xD726 (upper) and it's Tucker\
    \ decomposition. (b) Core tensor S\u2208 R 80\xD780\xD726 of X . Note that the\
    \ size of the nonzero-block is 69\xD771\xD717 , and 78.4 percent of its elements\
    \ are zeroes. (c) Typical slices of S , where deeper color of the element represents\
    \ a larger value of it. (d) 6 Kronecker bases of X , which relate to the largest\
    \ 6 elements in core tensor S . (e) Noised MSI (lack of tensor sparsty) and it's\
    \ core tensor (the size of the nonzero-block is 80\xD780\xD726 and most of its\
    \ elements are nonzero)."
  Figure 4 Link: articels_figures_by_rev_year\2017\KroneckerBasisRepresentation_Based_Tensor_Sparsity_and_Its_Applications_to_Tenso\figure_4.jpg
  Figure 4 caption: A visual display of the KBR measure.
  Figure 5 Link: articels_figures_by_rev_year\2017\KroneckerBasisRepresentation_Based_Tensor_Sparsity_and_Its_Applications_to_Tenso\figure_5.jpg
  Figure 5 caption: Flowchart of the proposed MSI denoising method.
  Figure 6 Link: articels_figures_by_rev_year\2017\KroneckerBasisRepresentation_Based_Tensor_Sparsity_and_Its_Applications_to_Tenso\figure_6.jpg
  Figure 6 caption: (a) The images at two bands (400 and 700 nm) of chart and staffed
    toy; (b) The corresponding images corrupted by Gaussian noise with variance v
    = 0.2 , (c)-(m) The restored images obtained by the 10 utilized MSI denoising
    methods. Two demarcated areas in each image are amplified at a 4 time larger scale
    and with the same degree of contrast for easy observation of details.
  Figure 7 Link: articels_figures_by_rev_year\2017\KroneckerBasisRepresentation_Based_Tensor_Sparsity_and_Its_Applications_to_Tenso\figure_7.jpg
  Figure 7 caption: (a) The Original image located at the first band in HYDICE urban
    data set; (b)-(l) The restored image obtained by the 11 utilized methods.
  Figure 8 Link: articels_figures_by_rev_year\2017\KroneckerBasisRepresentation_Based_Tensor_Sparsity_and_Its_Applications_to_Tenso\figure_8.jpg
  Figure 8 caption: (a) Original images located at the band centered at 700 nm of
    fake and real lemons; (b) The corresponding sampled images with sampling rate
    10 percent; (c)-(j) Restored images obtained by 8 competing methods; (k) Combination
    results of the first 10, 100 and 1,000 bases obtained by KBR-TC, respectively.
  Figure 9 Link: articels_figures_by_rev_year\2017\KroneckerBasisRepresentation_Based_Tensor_Sparsity_and_Its_Applications_to_Tenso\figure_9.jpg
  Figure 9 caption: 'From left to right: Original video frames, background and foreground
    extracted by all competing methods.'
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Qi Xie
  Name of the last author: Zongben Xu
  Number of Figures: 10
  Number of Tables: 4
  Number of authors: 4
  Paper title: Kronecker-Basis-Representation Based Tensor Sparsity and Its Applications
    to Tensor Recovery
  Publication Date: 2017-08-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance Comparison (Mean + Variance) of 11 Competing Methods
      with Respect to 4 PQIs Averaged over All 32 Scenes and All Extents of Noises
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance Comparison of 7 Competing TC Methods with Respect
      to RRE on Synthetic Data with Rank Setting (30,30,30) (Upper) and (10, 35, 40)
      (Lower)
  Table 3 caption:
    table_text: TABLE 3 The Average Performance Comparison of 8 Competing TC Methods
      with Different Sampling Rates on 32 MSI
  Table 4 caption:
    table_text: TABLE 4 Quantitative Performance Comparison of All Competing Methods
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2734888
- Affiliation of the first author: school of electronic engineering and computer science,
    queen mary university of london, london, united kingdom
  Affiliation of the last author: school of electronic engineering and computer science,
    queen mary university of london, london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2017\ZeroShot_Learning_on_Semantic_Class_Prototype_Graph\figure_1.jpg
  Figure 1 caption: A semantic manifold distance model unifies Semantic Embedding
    (SE) and Semantic Relatedness (SR) based methods for ZSL. Given an unseen class
    image, x and P x are the visual feature vector and its projection in the embedding
    space respectively. The seen and unseen class prototypes are denoted as y and
    z respectively.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\ZeroShot_Learning_on_Semantic_Class_Prototype_Graph\figure_2.jpg
  Figure 2 caption: 'An example of semantic manifold: The class prototypes of object
    classes from the ImageNet 2012 1K dataset are grouped into eight superclasses
    (food, invertebrate, canine, bird, instrument, vehicle, structure and covering)
    according to [25] and visualised by the 1,000D word2vec embedding [26] in a 2D
    space using t-SNE [27] .'
  Figure 3 Link: articels_figures_by_rev_year\2017\ZeroShot_Learning_on_Semantic_Class_Prototype_Graph\figure_3.jpg
  Figure 3 caption: 'The advantage of semantic manifold distance: Without considering
    the distribution of class prototypes captured by the semantic manifold structure,
    a test image x is classified as an unseen class z 1 using the euclidean distance.
    When considering the prototype distribution, x is classified to z 2 by being on
    the same class manifold in the semantic embedding space.'
  Figure 4 Link: articels_figures_by_rev_year\2017\ZeroShot_Learning_on_Semantic_Class_Prototype_Graph\figure_4.jpg
  Figure 4 caption: (a) The conventional ranking loss objective will force the projected
    visual feature vectors (yellow circle) from the same seen class to be tightly
    around their corresponding prototype in the semantic embedding space. (b) By considering
    the unseen class prototypes, our UPR-SE model will generalise better from seen
    to unseen classes.
  Figure 5 Link: articels_figures_by_rev_year\2017\ZeroShot_Learning_on_Semantic_Class_Prototype_Graph\figure_5.jpg
  Figure 5 caption: After incorporating a test image into a semantic class prototype
    graph, zero-shot learning can be viewed as an extended absorbing Markov chain
    process (AMP) on the graph.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Zhenyong Fu
  Name of the last author: Shaogang Gong
  Number of Figures: 5
  Number of Tables: 9
  Number of authors: 4
  Paper title: Zero-Shot Learning on Semantic Class Prototype Graph
  Publication Date: 2017-08-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 A Summary of the Four Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Evaluation on AwA in Classification Accuracy (%)
  Table 3 caption:
    table_text: TABLE 3 Evaluation on CUB in Classification Accuracy (%)
  Table 4 caption:
    table_text: TABLE 4 Evaluation on aP&Y in Classification Accuracy (%)
  Table 5 caption:
    table_text: TABLE 5 The hit5 Classification Accuracy (%) of Compared Methods on
      ImageNet 2010 1K
  Table 6 caption:
    table_text: TABLE 6 Evaluating Different Manifold-Based Distances for ZSL (%)
  Table 7 caption:
    table_text: TABLE 7 ZSL Results (%) Obtained Using AwA 40 Seen Classes, ImageNet
      1K Classes and AwA 40 Plus ImageNet 1K Classes to Construct the Semantic Graph
      on AwA
  Table 8 caption:
    table_text: TABLE 8 Evaluating the Unseen Prototype Regularisation (UPR) (%)
  Table 9 caption:
    table_text: TABLE 9 Comparative Evaluation Measured in AUSUC (the Higher the Better)
      for Generalised Zero-Shot Learning on AwA
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2737007
- Affiliation of the first author: department of information engineering, chinese
    university of hong kong, shatin, hong kong sar
  Affiliation of the last author: department of information engineering, chinese university
    of hong kong, shatin, hong kong sar
  Figure 1 Link: articels_figures_by_rev_year\2017\Deep_Learning_Markov_Random_Field_for_Semantic_Segmentation\figure_1.jpg
  Figure 1 caption: (a) The network architecture of a deep parsing network (DPN).
    (b) DPN extends a contemporary CNN architecture to model unary terms and additional
    layers are carefully devised to approximate the mean field algorithm (MF) for
    pairwise terms. (c) DPN enables dynamic linking of nodes in Markov Random Field
    (MRF) by incorporating domain knowledge.
  Figure 10 Link: articels_figures_by_rev_year\2017\Deep_Learning_Markov_Random_Field_for_Semantic_Segmentation\figure_10.jpg
  Figure 10 caption: Ablation study of (a) training strategy (b) required MF iterations.
    (Best viewed in color).
  Figure 2 Link: articels_figures_by_rev_year\2017\Deep_Learning_Markov_Random_Field_for_Semantic_Segmentation\figure_2.jpg
  Figure 2 caption: (a) Illustration of the pairwise terms in DPN. (b) Explains the
    label contexts. (c) and (d) show that the mean field update of DPN corresponds
    to convolutions.
  Figure 3 Link: articels_figures_by_rev_year\2017\Deep_Learning_Markov_Random_Field_for_Semantic_Segmentation\figure_3.jpg
  Figure 3 caption: Illustrative depiction of (a) triple penalty term and (b) mixture
    of local label contexts term.
  Figure 4 Link: articels_figures_by_rev_year\2017\Deep_Learning_Markov_Random_Field_for_Semantic_Segmentation\figure_4.jpg
  Figure 4 caption: (a) and (b) show the padding of the filters. (c) Illustrates local
    convolution of b12.
  Figure 5 Link: articels_figures_by_rev_year\2017\Deep_Learning_Markov_Random_Field_for_Semantic_Segmentation\figure_5.jpg
  Figure 5 caption: (a) and (b) illustrates the convolutions of b13 and the poolings
    in b14.
  Figure 6 Link: articels_figures_by_rev_year\2017\Deep_Learning_Markov_Random_Field_for_Semantic_Segmentation\figure_6.jpg
  Figure 6 caption: Visualization of (a) learned label compatibility (b) learned contextual
    information. (Best viewed in color) .
  Figure 7 Link: articels_figures_by_rev_year\2017\Deep_Learning_Markov_Random_Field_for_Semantic_Segmentation\figure_7.jpg
  Figure 7 caption: Visualization of (a) 3D learned label compatibility (b) learned
    spatial-temporal contextual information. (Best viewed in color).
  Figure 8 Link: articels_figures_by_rev_year\2017\Deep_Learning_Markov_Random_Field_for_Semantic_Segmentation\figure_8.jpg
  Figure 8 caption: (a) and (b) are neighboring video frames from CamVid dataset.
    When the vehicle moved forward, the 'sky' (blue) shift upward and the 'pedestrian'
    (orange) shift downward. (Best viewed in color).
  Figure 9 Link: articels_figures_by_rev_year\2017\Deep_Learning_Markov_Random_Field_for_Semantic_Segmentation\figure_9.jpg
  Figure 9 caption: Step-by-step visualization of DPN. (Best viewed in color).
  First author gender probability: 0.7
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.93
  Name of the first author: Ziwei Liu
  Name of the last author: Xiaoou Tang
  Number of Figures: 14
  Number of Tables: 7
  Number of authors: 5
  Paper title: Deep Learning Markov Random Field for Semantic Segmentation
  Publication Date: 2017-08-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 A Comparison Between the Network Architectures of VGG 16 and
      DPN
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Summary of Datasets
  Table 3 caption:
    table_text: TABLE 3 Ablation Study of Hyper-Parameters
  Table 4 caption:
    table_text: TABLE 4 The Effectiveness of Spatial-Temporal DPN
  Table 5 caption:
    table_text: TABLE 5 Per-Class Results on VOC12
  Table 6 caption:
    table_text: TABLE 6 Per-Class Results on Cityscapes
  Table 7 caption:
    table_text: TABLE 7 Per-Class Results on CamVid
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2737535
- Affiliation of the first author: department of automation, tsinghua university,
    beijing, china
  Affiliation of the last author: department of automation, tsinghua university, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2017\Simultaneous_Local_Binary_Feature_Learning_and_Encoding_for_Homogeneous_and_Hete\figure_1.jpg
  Figure 1 caption: The basic idea of the proposed SLBFLE approach for face representation.
    For each training face image, we extract pixel difference vectors (PDVs) and jointly
    learn a discriminative mapping W and a dictionary D for feature extraction. The
    mapping is to project each PDV into a low-dimensional binary vector, and the dictionary
    is used as the codebook for feature local encoding. For each test image, the PDVs
    are first extracted and encoded into binary codes using the learned feature mapping,
    and then converted as a histogram feature with the learned dictionary.
  Figure 10 Link: articels_figures_by_rev_year\2017\Simultaneous_Local_Binary_Feature_Learning_and_Encoding_for_Homogeneous_and_Hete\figure_10.jpg
  Figure 10 caption: ROC curves of different methods on the CASIA NIR-VIS 2.0 dataset.
  Figure 2 Link: articels_figures_by_rev_year\2017\Simultaneous_Local_Binary_Feature_Learning_and_Encoding_for_Homogeneous_and_Hete\figure_2.jpg
  Figure 2 caption: 'The basic idea of the LBP method, where a two-stage procedure
    is used for local feature extraction: feature mapping and feature encoding. For
    the feature mapping stage, the difference between the central pixel and the neighboring
    pixels are computed and binarized with a fixed threshold. For the feature encoding
    stage, the mapped binary codes are encoded as a real value by using a hand-crafted
    pattern coding strategy.'
  Figure 3 Link: articels_figures_by_rev_year\2017\Simultaneous_Local_Binary_Feature_Learning_and_Encoding_for_Homogeneous_and_Hete\figure_3.jpg
  Figure 3 caption: "An illustration to show how to extract pixel difference vectors\
    \ (PDV) from the original face image. Given a face patch whose size is (2R+1)\xD7\
    (2R+1) , we first compute the difference between the central pixel and the neighboring\
    \ pixels. Then, these differences are considered as a PDV. In this figure, R is\
    \ selected as 2, so that there are 24 neighboring pixels selected and the PDV\
    \ is a 24-dimensional feature vector."
  Figure 4 Link: articels_figures_by_rev_year\2017\Simultaneous_Local_Binary_Feature_Learning_and_Encoding_for_Homogeneous_and_Hete\figure_4.jpg
  Figure 4 caption: The flow-chart of the SLBFLE-based face representation approach.
    For each training face, we first divide it into several non-overlapped regions
    and learn the feature mapping W and dictionary D for each region. Then, we applied
    the learned filter and dictionary to extract histogram feature for each block
    and concatenated into a longer feature vector for face representation. Finally,
    the cosine similarity measure is used to measure face similarity for verification.
  Figure 5 Link: articels_figures_by_rev_year\2017\Simultaneous_Local_Binary_Feature_Learning_and_Encoding_for_Homogeneous_and_Hete\figure_5.jpg
  Figure 5 caption: The flow-chart of the proposed C-SLBFLE method. We first extract
    the coupled-PDVs from a pair of aligned face images captured from different modalities.
    These coupled-PDVs are then used to jointly learn the feature mapping W p and
    W q for the modality P and Q , respectively, and codebook D . Each feature mapping
    matrix composes of a common projection matrix, W c , and modal-specific projection
    matrices W (p) s and W (q) s . The final codebook and projection matrices are
    then used to extract the face representations during testing stage.
  Figure 6 Link: articels_figures_by_rev_year\2017\Simultaneous_Local_Binary_Feature_Learning_and_Encoding_for_Homogeneous_and_Hete\figure_6.jpg
  Figure 6 caption: ROC curve of different face descriptors on LFW with the unsupervised
    setting.
  Figure 7 Link: articels_figures_by_rev_year\2017\Simultaneous_Local_Binary_Feature_Learning_and_Encoding_for_Homogeneous_and_Hete\figure_7.jpg
  Figure 7 caption: ROC curve of different face verification methods on LFW with the
    image-restricted with label-free outside data setting.
  Figure 8 Link: articels_figures_by_rev_year\2017\Simultaneous_Local_Binary_Feature_Learning_and_Encoding_for_Homogeneous_and_Hete\figure_8.jpg
  Figure 8 caption: ROC curve of different face verification methods on YTF under
    the image-restricted setting.
  Figure 9 Link: articels_figures_by_rev_year\2017\Simultaneous_Local_Binary_Feature_Learning_and_Encoding_for_Homogeneous_and_Hete\figure_9.jpg
  Figure 9 caption: ROC curves of different feature descriptors on the PaSC dataset
    in the unsupervised setting.
  First author gender probability: 0.95
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.59
  Name of the first author: Jiwen Lu
  Name of the last author: Jie Zhou
  Number of Figures: 10
  Number of Tables: 12
  Number of authors: 3
  Paper title: Simultaneous Local Binary Feature Learning and Encoding for Homogeneous
    and Heterogeneous Face Recognition
  Publication Date: 2017-08-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Mean Verification Rate (VR) (%) and Area Under ROC (AUC) Comparison
      with State-of-the-Art Face Descriptors on LFW with the Unsupervised Setting
  Table 10 caption:
    table_text: TABLE 10 Comparisons of the Recognition Performance of Different Variations
      of Our Methods, Where the Mean AUC Is Used for LFW (Unsupervised) and for PaSC,
      and the Mean Rank-1 Identification of All Four Subjects for FERET, Respectively
  Table 2 caption:
    table_text: TABLE 2 Mean Verification Rate and the Standard Error (%) Comparison
      with State-of-the-Art Face Verification Methods on LFW with the Image-Restricted
      with Label-Free Outside Data Setting
  Table 3 caption:
    table_text: TABLE 3 Comparisons of the Mean Verification Rate and Standard Error
      (%) with State-of-the-Art Learning-Based Face Descriptors on YTF Under the Image-Restricted
      Setting
  Table 4 caption:
    table_text: TABLE 4 Comparisons of the Mean Verification Rate and Standard Error
      (%) with the State-of-the-Art Face Verification Methods on YTF Under the Image-Restricted
      Setting
  Table 5 caption:
    table_text: TABLE 5 Rank-One Recognition Rates (%) Comparison with State-of-the-Art
      Feature Descriptors with the Standard FERET Evaluation Protocol
  Table 6 caption:
    table_text: TABLE 6 The Verification Rate at the 1.0 Percent FAR of Different
      Methods on the PaSC Dataset Still-Video Face Recognition Experiments
  Table 7 caption:
    table_text: TABLE 7 Performance Comparison on the CASIA NIR-VIS 2.0 Dataset, Where
      VR Is (twice) the Mean Verification Rate When the FAR Is Set to 0.1 and 1.0
      Percent
  Table 8 caption:
    table_text: "TABLE 8 Performance Comparison of C-SLBFLE( R=4 ) the CASIA NIR-VIS\
      \ 2.0 Dataset at Varying \u03B7 , Where VR Is (twice) the Mean Verification\
      \ Rate When the FAR Is Set to 0.1 and 1.0 Percent"
  Table 9 caption:
    table_text: TABLE 9 The Recognition Rate (%) of Different Heterogeneous Face Recognition
      Methods on the MultiPIE Dataset at Varying Poses with Neutral Expression and
      Frontal Illumination
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2737538
- Affiliation of the first author: institute of systems and robotics, university of
    coimbra, coimbra, portugal
  Affiliation of the last author: institute of systems and robotics, university of
    coimbra, coimbra, portugal
  Figure 1 Link: articels_figures_by_rev_year\2017\PiecewisePlanar_StereoScan_Sequential_Structure_and_Motion_Using_Plane_Primitive\figure_1.jpg
  Figure 1 caption: Experiment in an urban sequence acquired by a moving vehicle with
    a forward-looking stereo pair. The sequence has 1370 frames covering 1100m. The
    errors in loop closing were 1.8 percent in translation and 0.5 degree in rotation
    with PPSS, which compares against 3.6 percent in translation and 26 degree in
    rotation with LIBVISO2 [1] and 2.0 percent in translation and 4.3 degree in rotation
    with VisualSFM [2], [3]. In addition, PPSS provides a visually pleasant PPR that
    can be viewed in the following link https:youtu.beIhELZ3-wPU0.
  Figure 10 Link: articels_figures_by_rev_year\2017\PiecewisePlanar_StereoScan_Sequential_Structure_and_Motion_Using_Plane_Primitive\figure_10.jpg
  Figure 10 caption: (a) Information about image resolution, stereo camera baseline,
    acquisition rate and maximum range for accurate depth estimation for the 3 acquisition
    setups. The maximum range is computed by considering a depth error of 2 percent
    and a disparity error of 0.6 pixels. (b) Vehicle setups S2 and S3 with a corresponding
    acquired stereo pair enclosed in a black and red box, respectively.
  Figure 2 Link: articels_figures_by_rev_year\2017\PiecewisePlanar_StereoScan_Sequential_Structure_and_Motion_Using_Plane_Primitive\figure_2.jpg
  Figure 2 caption: 'Segmentation and reconstruction results obtained in different
    scenarios: (a) Planes detected in each view independently, where a standard MRF
    is used only for visualization purposes. (b) Result obtained when applying a regular
    MRF after the motion initialization. (c) PEARL refinement followed by a regular
    MRF. (d) Result obtained when applying the proposed pipeline with the new MRF
    formulation. In all cases, lines with colors corresponding to the detected vertical
    and nearly vertical planes are shown in the top view of the 3D models.'
  Figure 3 Link: articels_figures_by_rev_year\2017\PiecewisePlanar_StereoScan_Sequential_Structure_and_Motion_Using_Plane_Primitive\figure_3.jpg
  Figure 3 caption: Different stages of the pipeline. The planes detected in each
    view are used for pose estimation. The pose and the plane hypotheses are refined
    in a discrete-continuous optimization step and a final MRF is used for dense labelling.
    Scene planes are identified with colors.
  Figure 4 Link: articels_figures_by_rev_year\2017\PiecewisePlanar_StereoScan_Sequential_Structure_and_Motion_Using_Plane_Primitive\figure_4.jpg
  Figure 4 caption: "(a) The relative pose estimation can be cast as a point registration\
    \ problem in the dual projective space P 3\u2217 . (b) A descriptor is computed\
    \ for the plane triplets and used in a nearest-neighbours approach for finding\
    \ putative matches between the planes. Similarities between angles in the descriptor\
    \ give rise to different hypotheses, depicted by the points near planes \u03A9\
    \ 1 and \u03A9 2 and line L ."
  Figure 5 Link: articels_figures_by_rev_year\2017\PiecewisePlanar_StereoScan_Sequential_Structure_and_Motion_Using_Plane_Primitive\figure_5.jpg
  Figure 5 caption: 'Back-propagation across stereo pairs: A closer view of the door
    plane allows its correct detection and propagation to previous pairs.'
  Figure 6 Link: articels_figures_by_rev_year\2017\PiecewisePlanar_StereoScan_Sequential_Structure_and_Motion_Using_Plane_Primitive\figure_6.jpg
  Figure 6 caption: (a) Due to the lack of texture, the white wall is not fully reconstructed
    when using the standard MRF formulation. (b) Data cost of each pixel, for two
    different labels corresponding to a scene plane, obtained with a standard MRF
    (left column) and new the MRF formulation (right column). The color corresponding
    to each plane in the semi-dense labelling (top image) is indicated next to the
    data cost matrices. The color bar corresponds to the matching cost. (c) The new
    MRF assigns the correct label to the textureless pixels.
  Figure 7 Link: articels_figures_by_rev_year\2017\PiecewisePlanar_StereoScan_Sequential_Structure_and_Motion_Using_Plane_Primitive\figure_7.jpg
  Figure 7 caption: Example of a case where a legitimate occlusion originates label
    inconsistency. The proposed approach detects that this occlusion is a legitimate
    one and does not change the labels of the corresponding pixels.
  Figure 8 Link: articels_figures_by_rev_year\2017\PiecewisePlanar_StereoScan_Sequential_Structure_and_Motion_Using_Plane_Primitive\figure_8.jpg
  Figure 8 caption: New post-processing step applied after the MRF labelling to ensure
    label consistency across frames. The initial dense labelling of each of the two
    frames (images on the left) is modified by finding the areas of disagreement between
    labels (images in the middle) and changing them so that corresponding areas in
    the scene have the same label (images on the right). This allows to correct problems
    of (a) occlusion and (b) the reconstruction of non-planar objects such as vegetation.
  Figure 9 Link: articels_figures_by_rev_year\2017\PiecewisePlanar_StereoScan_Sequential_Structure_and_Motion_Using_Plane_Primitive\figure_9.jpg
  Figure 9 caption: Reconstruction results obtained when (a) not using the proposed
    post-processing step, (b) using the post-processing step as it is proposed, and
    (c) using the post-processing step with the alternative of considering the discard
    label as a neutral element.
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Carolina Raposo
  Name of the last author: "Jo\xE3o P. Barreto"
  Number of Figures: 14
  Number of Tables: 0
  Number of authors: 3
  Paper title: 'Piecewise-Planar StereoScan: Sequential Structure and Motion Using
    Plane Primitives'
  Publication Date: 2017-08-09 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2737425
