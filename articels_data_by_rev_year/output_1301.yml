- Affiliation of the first author: school of computing, informatics, and decision
    systems engineering, arizona state university, tempe, usa
  Affiliation of the last author: school of computing, informatics, and decision systems
    engineering, arizona state university, tempe, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Hyperbolic_Wasserstein_Distance_for_Shape_Indexing\figure_1.jpg
  Figure 1 caption: (a) A multiply connected human face surface; (b) the fundamental
    group of surface (a); (c) the fundamental domain of surface (a); (d) another fundamental
    domain of surface (a), which is deformed from (c) with a Fuchsian transformation
    of surface (a).
  Figure 10 Link: articels_figures_by_rev_year\2019\Hyperbolic_Wasserstein_Distance_for_Shape_Indexing\figure_10.jpg
  Figure 10 caption: Experimental results of human facial expression analysis with
    hyperbolic Wasserstein distance.
  Figure 2 Link: articels_figures_by_rev_year\2019\Hyperbolic_Wasserstein_Distance_for_Shape_Indexing\figure_2.jpg
  Figure 2 caption: "Illustrations of hyperbolic geometry: (a) Poincar\xE9 disk model;\
    \ (b) geodesics in Poincar\xE9 disk; (c) Klein model."
  Figure 3 Link: articels_figures_by_rev_year\2019\Hyperbolic_Wasserstein_Distance_for_Shape_Indexing\figure_3.jpg
  Figure 3 caption: The overall computational pipeline of the proposed hyperbolic
    Wasserstein distance.
  Figure 4 Link: articels_figures_by_rev_year\2019\Hyperbolic_Wasserstein_Distance_for_Shape_Indexing\figure_4.jpg
  Figure 4 caption: (a) A hyperbolic triangle; (b) the circle packing metric on a
    hyperbolic triangle.
  Figure 5 Link: articels_figures_by_rev_year\2019\Hyperbolic_Wasserstein_Distance_for_Shape_Indexing\figure_5.jpg
  Figure 5 caption: "Front (a) and back (b) views of a left cortical surface with\
    \ six landmark curves and the fundamental group of paths; (c) embedding of the\
    \ cortical surface onto the Poincar\xE9 disk."
  Figure 6 Link: articels_figures_by_rev_year\2019\Hyperbolic_Wasserstein_Distance_for_Shape_Indexing\figure_6.jpg
  Figure 6 caption: "Geodesic curve lifting: (a) Fuchsian transformations that map\
    \ \u03C4 + i and \u03C4 \u2212 i to the u axis; (b) Fuchsian transformation that\
    \ maps \u03C4 + i to \u03C4 \u2212 i ; (c) a finite portion of the universal covering\
    \ space; (d) recomputation of the fundamental group of paths as geodesics in the\
    \ Poincar\xE9 disk."
  Figure 7 Link: articels_figures_by_rev_year\2019\Hyperbolic_Wasserstein_Distance_for_Shape_Indexing\figure_7.jpg
  Figure 7 caption: When the recomputed fundamental group of paths are lifted to 3D
    surfaces, they are consistent across different surfaces (a) and (b).
  Figure 8 Link: articels_figures_by_rev_year\2019\Hyperbolic_Wasserstein_Distance_for_Shape_Indexing\figure_8.jpg
  Figure 8 caption: Canonical fundamental domain decomposition for the initial map
    construction between surfaces.
  Figure 9 Link: articels_figures_by_rev_year\2019\Hyperbolic_Wasserstein_Distance_for_Shape_Indexing\figure_9.jpg
  Figure 9 caption: "Illustration of the power Voronoi diagram on the Poincar\xE9\
    \ disk, where each point (black dot) is associated with a Voronoi cell (green\
    \ boundary) and the radius of each circle (blue) centered at each point."
  First author gender probability: 0.59
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Jie Shi
  Name of the last author: Yalin Wang
  Number of Figures: 14
  Number of Tables: 3
  Number of authors: 2
  Paper title: Hyperbolic Wasserstein Distance for Shape Indexing
  Publication Date: 2019-02-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Classification Results of Our Method and Three Other Surface
      Shape Features, the Surface Area, Mean Curvature and Spectral l 2 l2 Distance
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Running Time Comparison of Hyperbolic Wasserstein Distance
      and Spectral l 2 l2 Distance
  Table 3 caption:
    table_text: TABLE 3 Comparison of Hyperbolic Wasserstein Distance with Other Shape
      Measures in Robustness to Noise
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2898400
- Affiliation of the first author: mathematical institute, serbian academy of sciences,
    belgrade, serbia
  Affiliation of the last author: school of computer science and informatics, cardiff
    university, cardiff, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2019\Measuring_Shapes_with_Desired_Convex_Polygons\figure_1.jpg
  Figure 1 caption: Some random shapes; these are intuitively judged as shapes whose
    circularity, elongation and convexity differ.
  Figure 10 Link: articels_figures_by_rev_year\2019\Measuring_Shapes_with_Desired_Convex_Polygons\figure_10.jpg
  Figure 10 caption: A noise free triangle is on the left in the first row. The remaining
    five shapes correspond to the same triangle with added different levels of noise.
    The graphs of cal Glambda, mathbf P(S), lambda in [0,5], are below these shapes,
    as well as their computed values for lambda =1 and lambda =5 .
  Figure 2 Link: articels_figures_by_rev_year\2019\Measuring_Shapes_with_Desired_Convex_Polygons\figure_2.jpg
  Figure 2 caption: Different circularity measures tend to rank shapes differently.
  Figure 3 Link: articels_figures_by_rev_year\2019\Measuring_Shapes_with_Desired_Convex_Polygons\figure_3.jpg
  Figure 3 caption: "The reference polygon P is on the left. The second, third, and\
    \ fourth shapes correspond to a kangaroo image at an initial resolution, and 8\
    \ and 16 times lower resolution, respectively. The first table provides values\
    \ of G \u03BB=1,P (S) computed numerically for the full resolution image at different\
    \ rotation angle increments. The second table shows G \u03BB=1,P (S) values obtained\
    \ for different image resolutions of the kangaroo shape."
  Figure 4 Link: articels_figures_by_rev_year\2019\Measuring_Shapes_with_Desired_Convex_Polygons\figure_4.jpg
  Figure 4 caption: 'First row: Two sub-figures on the left show the behavior of Psi
    lambda =1, mathbf P(x,y) and Psi lambda =5, mathbf P(x,y) , respectively, for
    the points (x,y) inside the reference polygon mathbf P (on the right). Second
    row: Sub-figures show the the behavior of Psi lambda =1, mathbf P(x,y) and Psi
    lambda =5, mathbf P(x,y) for (x,y) notin mathbf P.'
  Figure 5 Link: articels_figures_by_rev_year\2019\Measuring_Shapes_with_Desired_Convex_Polygons\figure_5.jpg
  Figure 5 caption: In all the rows, the polygon on the left is used to define the
    cal Gmathbf P(S) measure. The remaining 4 shapes (in each row) are listed in accordance
    with their decreasing cal Gmathbf P(S) value. The computed cal Gmathbf P(S) values
    are given next to their corresponded shapes.
  Figure 6 Link: articels_figures_by_rev_year\2019\Measuring_Shapes_with_Desired_Convex_Polygons\figure_6.jpg
  Figure 6 caption: Three shapes, ranked by cal Gmathbf P(S) measure (for different
    choices of the polygon mathbf P ) are in the first row. The selected polygons,
    used for the computation of cal Gmathbf P(S) , are displayed on the left, in the
    rows (a), (b), and (c). The shapes observed are displayed in the second, third,
    and fourth row, according to their computed cal Gmathbf P(S) values.
  Figure 7 Link: articels_figures_by_rev_year\2019\Measuring_Shapes_with_Desired_Convex_Polygons\figure_7.jpg
  Figure 7 caption: S1-S4 circle shapes, affected by noise, are in the first row.
    The polygonal shape S5, in the second row, is noise-free, while the remaining
    three shapes S6-S8 are affected by noise. cal Gmathbf P(S) values, are below the
    corresponding shapes. cal Gmathbf P(S) values for a noise-free circle are 0.6909,
    0.8645, 0.5652, and 0.9986, respectively, if the polygons mathbf P1 , mathbf P2
    , mathbf P3 , and mathbf P4 were used.
  Figure 8 Link: articels_figures_by_rev_year\2019\Measuring_Shapes_with_Desired_Convex_Polygons\figure_8.jpg
  Figure 8 caption: Polygons mathbf P1 and mathbf P2 used to define cal Glambda, mathbf
    P1(S) and cal Glambda, mathbf P2(S) are in the left column. The shapes measured
    are in the top row. Their corresponded graphs cal Glambda, mathbf P1(S) and cal
    Glambda, mathbf P2(S) for lambda in [0,5], are below them. The values cal Glambda
    =1, mathbf P1(S) and cal Glambda =5, mathbf P1(S) , are also included.
  Figure 9 Link: articels_figures_by_rev_year\2019\Measuring_Shapes_with_Desired_Convex_Polygons\figure_9.jpg
  Figure 9 caption: The graphs of the difference and ratio of cal Glambda, mathbf
    P1(S1) and cal Glambda, mathbf P1(S2) , for lambda in [0,5] , are on the left.
    The related minimum and maximum values are also given. The related graphs for
    the shapes S3 and S4 , are third and fourth graph in the top row. In this case
    the difference minimum is 0 while the ratio minimum is equal to 1, both are reached
    for lambda =0 .
  First author gender probability: 0
  Gender of the first author: Not Available
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "Jovi\u0161a \u017Duni\u0107"
  Name of the last author: Paul L. Rosin
  Number of Figures: 16
  Number of Tables: 1
  Number of authors: 2
  Paper title: Measuring Shapes with Desired Convex Polygons
  Publication Date: 2019-02-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Classification Accuracies for Diatoms
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2898830
- Affiliation of the first author: school of electrical and electronic engineering,
    nanyang technological university, singapore
  Affiliation of the last author: school of electrical and electronic engineering,
    nanyang technological university, singapore
  Figure 1 Link: articels_figures_by_rev_year\2019\SkeletonBased_Online_Action_Prediction_Using_Scale_Selection_Network\figure_1.jpg
  Figure 1 caption: Figure (a) illustrates an untrimmed streaming sequence that contains
    multiple action instances. We need to recognize the current ongoing action at
    each time step when only a part (e.g., 10 percent) of it is performed. Figure
    (b) depicts our SSNet approach for online action prediction. At time t , only
    a part of the action waving hand is observed. Our SSNet selects the convolutional
    layer 2 rather than 3 for prediction, as the perception window of 2 mainly covers
    the performed part of current action, while 3 involves too many frames from the
    previous action which can interfere the prediction at time step t .
  Figure 10 Link: articels_figures_by_rev_year\2019\SkeletonBased_Online_Action_Prediction_Using_Scale_Selection_Network\figure_10.jpg
  Figure 10 caption: Action prediction results on the G3D dataset.
  Figure 2 Link: articels_figures_by_rev_year\2019\SkeletonBased_Online_Action_Prediction_Using_Scale_Selection_Network\figure_2.jpg
  Figure 2 caption: Illustration of the proposed SSNet for action prediction over
    the temporal axis. The solid lines denote the SSNet links activated at current
    step t , and the dashed lines indicate the links activated at other time steps.
    Our SSNet has 14 1-D convolutional layers. Here we only show 3 layers for clarity.
    At each time step, SSNet predicts the class ( c t ) of the ongoing action, and
    also estimates the temporal distance ( s t ) to current actions start point. Calculation
    details of c t and s t are shown in Fig. 3. Convolutional filters are shared at
    each layer, yet different across layers. (Best viewed in color)
  Figure 3 Link: articels_figures_by_rev_year\2019\SkeletonBased_Online_Action_Prediction_Using_Scale_Selection_Network\figure_3.jpg
  Figure 3 caption: "Details of our SSNet that jointly predicts the class label c\
    \ t and regresses the start points distance s t for the current ongoing action\
    \ at time t. If the regressed result s t\u22121 at the previous time step ( t\u2212\
    1 ) indicates that layer 3 corresponds to the most proper window scale (i.e.,\
    \ l p t =3 ), then our network will use layers 1-3 for class prediction, while\
    \ the activations from the layers above 3 are dropped (marked with cross in the\
    \ figure). In this figure, we only show a subset of convolutional nodes of our\
    \ SSNet, and other ones in the hierarchical structure (depicted as the solid lines\
    \ in Fig. 2) are omitted for clarity. The parameters of the convolutional layers\
    \ and FC (fully connected) layers in our SSNet are trained jointly in an end-to-end\
    \ fashion."
  Figure 4 Link: articels_figures_by_rev_year\2019\SkeletonBased_Online_Action_Prediction_Using_Scale_Selection_Network\figure_4.jpg
  Figure 4 caption: Illustration of the dilated convolution layer used in our network.
    At each position, the 1-D convolutional filter covers a time range (labeled as
    a red box), and only the two boundary nodes (corresponding to two time steps)
    in the covered range are used, while the other nodes between these two nodes are
    not used by the dilated convolutional operation for this position.
  Figure 5 Link: articels_figures_by_rev_year\2019\SkeletonBased_Online_Action_Prediction_Using_Scale_Selection_Network\figure_5.jpg
  Figure 5 caption: (a) The skeleton joints of the human body form a tree structure.
    We set the head joint (joint 1) as the root node, and the height of the tree in
    this figure is 8. (b) Illustration of the convolution with triangular filters
    sliding over the tree structure. The green and the blue triangles indicate the
    convolutions with two different filter sizes.
  Figure 6 Link: articels_figures_by_rev_year\2019\SkeletonBased_Online_Action_Prediction_Using_Scale_Selection_Network\figure_6.jpg
  Figure 6 caption: Illustration of the hierarchy of dilated tree convolutions that
    learns the multi-level structured representations over the input skeleton joints
    (labeled in red) at each frame. The solid arrows denote the dilated tree convolutions
    with triangular filters. In our method, 3 dilated tree convolutional layers are
    used to cover the input skeleton tree with height 8, while in this figure, we
    only show 2 layers that cover the tree with height 4 for clarity. Note that the
    bottom of this figure shows a full binary tree, while the human skeleton only
    has a subset of the nodes of a full binary tree. Therefore, in implementation,
    the convolutional operations only need to be performed on a subset of the nodes.
    The channel number of the input skeleton is 3, namely, the 3D coordinates (x,y,z)
    of each joint. (Best viewed in color)
  Figure 7 Link: articels_figures_by_rev_year\2019\SkeletonBased_Online_Action_Prediction_Using_Scale_Selection_Network\figure_7.jpg
  Figure 7 caption: Action prediction results on the OAD dataset.
  Figure 8 Link: articels_figures_by_rev_year\2019\SkeletonBased_Online_Action_Prediction_Using_Scale_Selection_Network\figure_8.jpg
  Figure 8 caption: Action prediction results on the ChaLearn Gesutre dataset.
  Figure 9 Link: articels_figures_by_rev_year\2019\SkeletonBased_Online_Action_Prediction_Using_Scale_Selection_Network\figure_9.jpg
  Figure 9 caption: Action prediction results on the PKUMMD dataset.
  First author gender probability: 0.9
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: Jun Liu
  Name of the last author: Alex C. Kot
  Number of Figures: 13
  Number of Tables: 12
  Number of authors: 5
  Paper title: Skeleton-Based Online Action Prediction Using Scale Selection Network
  Publication Date: 2019-02-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Details of the Main Structure of SSNet
  Table 10 caption:
    table_text: TABLE 10 Start Point Regression Errors
  Table 2 caption:
    table_text: TABLE 2 Details of the Hierarchy of Dilated Tree Convolutions (Corresponding
      to Fig. 6)
  Table 3 caption:
    table_text: TABLE 3 Number of Parameters and Computational Efficiency of Our SSNet
      When Using Different Skeleton Representations within It
  Table 4 caption:
    table_text: TABLE 4 Action Prediction Accuracies on the OAD Dataset
  Table 5 caption:
    table_text: TABLE 5 Action Prediction Accuracies on the ChaLearn Gesture Dataset
  Table 6 caption:
    table_text: TABLE 6 Action Prediction Accuracies on the PKUMMD Dataset
  Table 7 caption:
    table_text: TABLE 7 Action Prediction Accuracies on the G3D Dataset
  Table 8 caption:
    table_text: TABLE 8 Action Prediction Accuracies (%) of SSNet with Different Skeleton
      Representations
  Table 9 caption:
    table_text: TABLE 9 Start Point Regression Performance ( SL SL- Score Score)
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2898954
- Affiliation of the first author: school of electronic, electrical and communication
    engineering, university of chinese academy of sciences (ucas), beijing, china
  Affiliation of the last author: school of electronic, electrical and communication
    engineering, university of chinese academy of sciences (ucas), beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2019\MinEntropy_Latent_Model_for_Weakly_Supervised_Object_Detection\figure_1.jpg
  Figure 1 caption: Evolution of object locations during learning. Blue boxes denote
    proposals of high object probability and white ones detected objects. It can be
    seen that our approach reduces localization randomness and learns object extent.
    (Best viewed in color.)
  Figure 10 Link: articels_figures_by_rev_year\2019\MinEntropy_Latent_Model_for_Weakly_Supervised_Object_Detection\figure_10.jpg
  Figure 10 caption: Object detection examples on the PASCAL VOC 2012 and MS COCO
    2014 datasets. Yellow bounding boxes denote ground-truth annotations, green boxes
    correct detection results and red boxes false detection results. (Best viewed
    in color).
  Figure 2 Link: articels_figures_by_rev_year\2019\MinEntropy_Latent_Model_for_Weakly_Supervised_Object_Detection\figure_2.jpg
  Figure 2 caption: Illustration of the min-entropy latent model (MELM). A clique
    partition module is proposed to collect objectsparts from redundant proposals;
    Based on the cliques, a global min-entropy model is defined for object clique
    discovery; Within discovered cliques, a local min-entropy model is proposed to
    suppress object parts and select true objects. The three components are iteratively
    performed.
  Figure 3 Link: articels_figures_by_rev_year\2019\MinEntropy_Latent_Model_for_Weakly_Supervised_Object_Detection\figure_3.jpg
  Figure 3 caption: The proposals of high scores are selected and dynamically partitioned
    into same cliques if they are spatially related (i.e., overlapping with each other)
    and class related (i.e., having similar object class scores). Clique partition
    targets at collecting objectobject parts and activating true object extent.
  Figure 4 Link: articels_figures_by_rev_year\2019\MinEntropy_Latent_Model_for_Weakly_Supervised_Object_Detection\figure_4.jpg
  Figure 4 caption: "MELM is deployed as a clique partition module and two network\
    \ branches for object clique discovery and object localization. These two network\
    \ branches are unified with feature learning and optimized with a recurrent learning\
    \ algorithm. \u201CM1\u201D, \u201CM2\u201D and \u201CM3\u201D are heatmaps about\
    \ proposal scores without min-entropy, with global min-entropy, and with local\
    \ min-entropy, respectively. N is the number of object categories."
  Figure 5 Link: articels_figures_by_rev_year\2019\MinEntropy_Latent_Model_for_Weakly_Supervised_Object_Detection\figure_5.jpg
  Figure 5 caption: Flowchart of (a) the recurrent learning algorithm and (b) unfolded
    accumulated recurrent learning algorithm. The solid lines denote network connections
    and dotted lines denote forward-only connections.
  Figure 6 Link: articels_figures_by_rev_year\2019\MinEntropy_Latent_Model_for_Weakly_Supervised_Object_Detection\figure_6.jpg
  Figure 6 caption: Visualization of the clique partition, object clique discovery,
    and object localization results. (a) Bounding boxes of different colors denote
    proposals from different cliques. (b) Score maps of cliques and objects. (Best
    viewed in color).
  Figure 7 Link: articels_figures_by_rev_year\2019\MinEntropy_Latent_Model_for_Weakly_Supervised_Object_Detection\figure_7.jpg
  Figure 7 caption: Evolution of cliques. (Best viewed in color).
  Figure 8 Link: articels_figures_by_rev_year\2019\MinEntropy_Latent_Model_for_Weakly_Supervised_Object_Detection\figure_8.jpg
  Figure 8 caption: Gradient, entropy, and localization on the PASCAL VOC 2007 trainval
    set. (a) The evolution of entropy. (b) The evolution of gradient. (c) Localization
    accuracy. (d) Localization variance.
  Figure 9 Link: articels_figures_by_rev_year\2019\MinEntropy_Latent_Model_for_Weakly_Supervised_Object_Detection\figure_9.jpg
  Figure 9 caption: Comparison of the learned object locations by WSDDN [22] and MELM.
    The yellow boxes in the first column denote ground-truth objects. The white boxes
    denote the learned object locations and the blue boxes denote the high-scored
    proposals. It can be seen that for WSDDN the learned object locations evolved
    with large randomness, i.e., switch among the proposals around the objects. In
    contrast the object locations learned by MELM are consistent and have small randomness,
    which is quantified by the localization variance curves in the last column. (Best
    viewed in color).
  First author gender probability: 0.72
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.88
  Name of the first author: Fang Wan
  Name of the last author: Qixiang Ye
  Number of Figures: 10
  Number of Tables: 7
  Number of authors: 5
  Paper title: Min-Entropy Latent Model for Weakly Supervised Object Detection
  Publication Date: 2019-02-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Detection Mean Average Precision (%) on the PASCAL VOC 2007
      Test Set
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Detection Mean Average Precision (%) on the PASCAL VOC 2007
      val val Set
  Table 3 caption:
    table_text: TABLE 3 Detection Mean Average Precision (%) on the PASCAL VOC 2007
      Test Set
  Table 4 caption:
    table_text: TABLE 4 Correct Localization Rate (%) on the PASCAL VOC 2007 trainval
      trainval Set
  Table 5 caption:
    table_text: TABLE 5 Detection Mean Average Precision (%) on the PASCAL VOC 2010,
      2012, and the ILSVRC 2013 Datasets
  Table 6 caption:
    table_text: TABLE 6 Detection and Localization Performance (%) on MSCOCO 2014
  Table 7 caption:
    table_text: TABLE 7 Image Classification mAP (%) on the PASCAL VOC 2007 test test
      Set
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2898858
- Affiliation of the first author: department of electrical engineering and computer
    science, university of california at berkeley, berkeley, ca, usa
  Affiliation of the last author: department of electrical engineering and computer
    science, university of california at berkeley, berkeley, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\MultiView_Supervision_for_SingleView_Reconstruction_via_Differentiable_Ray_Consi\figure_1.jpg
  Figure 1 caption: We learn to predict 3D shape from a single input view. Our framework
    can leverage training data of the form of multi-view observations, and learn 3D
    reconstruction despite the lack of any direct supervision.
  Figure 10 Link: articels_figures_by_rev_year\2019\MultiView_Supervision_for_SingleView_Reconstruction_via_Differentiable_Ray_Consi\figure_10.jpg
  Figure 10 caption: Visualization of predictions using the Stanford Online Product
    Dataset. (Top) Input image. (Middle) Predicted shape in the emergent canonical
    pose. (Bottom) Predicted shape rotated according to the predicted pose.
  Figure 2 Link: articels_figures_by_rev_year\2019\MultiView_Supervision_for_SingleView_Reconstruction_via_Differentiable_Ray_Consi\figure_2.jpg
  Figure 2 caption: "Visualization of various aspects of our Differentiable Ray Consistency\
    \ formulation. a) Predicted 3D shape represented as probabilistic occupancies\
    \ and the observation image where we consider consistency between the predicted\
    \ shape and the ray corresponding to the highlighted pixel. b) Ray termination\
    \ events (Section 3.2) \u2013 the random variable z r =i corresponds to the event\
    \ where the ray terminates at the i th voxel on its path, z r = N r +1 represents\
    \ the scenario where the ray escapes the grid. c) Depiction of event probabilities\
    \ (Section 3.2) where red indicates a high probability of the ray terminating\
    \ at the corresponding voxel. d) Given the ray observation, we define event costs\
    \ (Section 3.3). In the example shown, the costs are low (white color) for events\
    \ where ray terminates in voxels near the observed termination point and high\
    \ (red color) otherwise. e) The ray consistency loss (Section 3.4) is defined\
    \ as the expected event cost and our formulation allows us to obtain gradients\
    \ for occupancies (red indicates that loss decreases if occupancy value increases,\
    \ blue indicates the opposite). While in this example we consider a depth observation,\
    \ our formulation allows incorporating diverse kinds of observations by defining\
    \ the corresponding event cost function as discussed in Sections 3.3 and 3.5.\
    \ Best viewed in color."
  Figure 3 Link: articels_figures_by_rev_year\2019\MultiView_Supervision_for_SingleView_Reconstruction_via_Differentiable_Ray_Consi\figure_3.jpg
  Figure 3 caption: 'Reconstructions on the ShapeNet dataset visualized using two
    representative views. Left to Right: Input, Ground-truth, 3D Training, Ours (Mask),
    Fusion (Depth), DRC (Depth), Fusion (Noisy Depth), and DRC (Noisy Depth).'
  Figure 4 Link: articels_figures_by_rev_year\2019\MultiView_Supervision_for_SingleView_Reconstruction_via_Differentiable_Ray_Consi\figure_4.jpg
  Figure 4 caption: Analysis of the per-category reconstruction performance. a) As
    we increase the number of views available per instance for training, the performance
    initially increases and saturates after few available views. b) As the amount
    of noise in depth observations used for training increases, the performance of
    our approach remains relatively consistent.
  Figure 5 Link: articels_figures_by_rev_year\2019\MultiView_Supervision_for_SingleView_Reconstruction_via_Differentiable_Ray_Consi\figure_5.jpg
  Figure 5 caption: 'PASCAL VOC reconstructions visualized using two representative
    views. Left to Right: Input, Ground-truth (as annotated in PASCAL 3D), Deformable
    Models [5], DRC (Pascal), Shapenet 3D, DRC (Joint).'
  Figure 6 Link: articels_figures_by_rev_year\2019\MultiView_Supervision_for_SingleView_Reconstruction_via_Differentiable_Ray_Consi\figure_6.jpg
  Figure 6 caption: Sample results on Cityscapes using ego-motion sequences for learning
    single image 3D reconstruction. Given a single input image (left), our model predicts
    voxel occupancy probabilities and per-voxel semantic class distribution. We use
    this prediction to render, in the top row, estimated disparity and semantics for
    a camera moving forward by 3, 6, 9, 12 metres, respectively. The bottom row renders
    similar output but using a 2.5D representation of ground-truth pixel-wise disparity
    and pixel-wise semantic labels inferred by [45].
  Figure 7 Link: articels_figures_by_rev_year\2019\MultiView_Supervision_for_SingleView_Reconstruction_via_Differentiable_Ray_Consi\figure_7.jpg
  Figure 7 caption: Sample results on ShapeNet dataset using multiple RGB images as
    supervision for training. We show the input image (left) and the visualize 3D
    shape predicted using our learned model from two novel views. Best viewed in color.
  Figure 8 Link: articels_figures_by_rev_year\2019\MultiView_Supervision_for_SingleView_Reconstruction_via_Differentiable_Ray_Consi\figure_8.jpg
  Figure 8 caption: 'Shape predictions on the validation set using a single RGB input
    image. We visualize the voxel occupancies by rendering the corresponding mesh
    (obtained via marching cubes) from a canonical pose. Left to Right: a) Input Image,
    b) Ground-truth, c) 3D Supervised Prediction, d,e) Multi-view & Pose Supervision
    (Mask, Depth), f,g) Mult-view wo Rotation Supervision (Mask, Depth), and h,i)
    Mult-view wo Rotation and Translation Supervision (Mask, Depth).'
  Figure 9 Link: articels_figures_by_rev_year\2019\MultiView_Supervision_for_SingleView_Reconstruction_via_Differentiable_Ray_Consi\figure_9.jpg
  Figure 9 caption: 'Rotation predictions on a random subset of the validation images.
    For visualization, we render the ground-truth voxel occupancies using the corresponding
    rotation. Left to Right: a) Input Image, b) Ground-truth Rotation, c) GT Supervised
    Prediction, d,e) Multi-view wo Rot Supervision (Mask, Depth), and f,g) Multi-view
    wo Rot and Trans Supervision (Mask, Depth).'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Shubham Tulsiani
  Name of the last author: Jitendra Malik
  Number of Figures: 10
  Number of Tables: 3
  Number of authors: 4
  Paper title: Multi-View Supervision for Single-View Reconstruction via Differentiable
    Ray Consistency
  Publication Date: 2019-02-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Analysis of Our Method Using Mean IoU on ShapeNet
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Mean IoU on PASCAL VOC
  Table 3 caption:
    table_text: TABLE 3 Analysis of the Performance for Single-View Shape (Left) and
      Pose (Right) Prediction
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2898859
- Affiliation of the first author: tencent ai lab, shenzhen, p.r. china
  Affiliation of the last author: national engineering laboratory for video technology,
    key laboratory of machine perception (moe), peking university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2019\EndtoEnd_Active_Object_Tracking_and_Its_RealWorld_Deployment_via_Reinforcement_L\figure_1.jpg
  Figure 1 caption: 'The pipeline of active tracking. Left: end-to-end approach. Right:
    passive tracking plus other modules.'
  Figure 10 Link: articels_figures_by_rev_year\2019\EndtoEnd_Active_Object_Tracking_and_Its_RealWorld_Deployment_via_Reinforcement_L\figure_10.jpg
  Figure 10 caption: Visual results of individual actions of the proposed active tracker
    on the video clip of Sphere. From left to right, they are actions of Forward (move-forward),
    Left (turn-leftturn-left-and-move-forward), Right (turn-rightturn-right-and-move-forward),
    and Stop (no-op).
  Figure 2 Link: articels_figures_by_rev_year\2019\EndtoEnd_Active_Object_Tracking_and_Its_RealWorld_Deployment_via_Reinforcement_L\figure_2.jpg
  Figure 2 caption: An overview of the network architecture of our active tracker.
    The observation encoder contains three layers, two convolutional layers and one
    fully-connected layer. The sequence encoder is a single-layer LSTM, which encodes
    the image features over time. The actor-critic network corresponds to two branches
    of fully-connected layers.
  Figure 3 Link: articels_figures_by_rev_year\2019\EndtoEnd_Active_Object_Tracking_and_Its_RealWorld_Deployment_via_Reinforcement_L\figure_3.jpg
  Figure 3 caption: A top view of a map with the local coordinate system. The green
    dot indicates the agent (tracker). The gray dot indicates the initial position
    and orientation of an object to be tracked. Three gray dots mean three possible
    initial configurations. Arrow indicates the orientation of an object. Dashed gray
    lines are parallel to the y -axis. The outer thick black rectangle represents
    the boundary. Best viewed in color.
  Figure 4 Link: articels_figures_by_rev_year\2019\EndtoEnd_Active_Object_Tracking_and_Its_RealWorld_Deployment_via_Reinforcement_L\figure_4.jpg
  Figure 4 caption: Maps and screenshots of ViZDoom environments. In all maps, the
    green dot (with white arrow indicating orientation) represents the agent. The
    gray dot indicates the object. Blue lines are planned paths and black lines are
    walls. Best viewed in color.
  Figure 5 Link: articels_figures_by_rev_year\2019\EndtoEnd_Active_Object_Tracking_and_Its_RealWorld_Deployment_via_Reinforcement_L\figure_5.jpg
  Figure 5 caption: Screenshots of UE environments. From left to right, there are
    Stefani, Malcom, Path1, Path2, Square1, and Square2. Best viewed in color.
  Figure 6 Link: articels_figures_by_rev_year\2019\EndtoEnd_Active_Object_Tracking_and_Its_RealWorld_Deployment_via_Reinforcement_L\figure_6.jpg
  Figure 6 caption: Recovering tracking when the target disappears in the SharpTurn
    environment.
  Figure 7 Link: articels_figures_by_rev_year\2019\EndtoEnd_Active_Object_Tracking_and_Its_RealWorld_Deployment_via_Reinforcement_L\figure_7.jpg
  Figure 7 caption: Saliency maps learned by the tracker. The top row shows input
    observations, and the bottom row shows their corresponding saliency maps. The
    corresponding actions of these input images are turn-right-and-move-forward, turn-left-and-move-forward,
    turn-left-and-move-forward, turn-left-and-move-forward, and move-forward, respectively.
    These saliency maps clearly show the focus of the tracker.
  Figure 8 Link: articels_figures_by_rev_year\2019\EndtoEnd_Active_Object_Tracking_and_Its_RealWorld_Deployment_via_Reinforcement_L\figure_8.jpg
  Figure 8 caption: The output actions of the proposed active tracker in the Woman
    (top) and Sphere (bottom) sequences from VOT dataset.
  Figure 9 Link: articels_figures_by_rev_year\2019\EndtoEnd_Active_Object_Tracking_and_Its_RealWorld_Deployment_via_Reinforcement_L\figure_9.jpg
  Figure 9 caption: Visual results of individual actions of the proposed active tracker
    on the video clip of Woman. From left to right, they are actions of Forward (move-forward),
    Left (turn-leftturn-left-and-move-forward), Right (turn-rightturn-right-and-move-forward),
    and Stop (no-op).
  First author gender probability: 0.81
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Wenhan Luo
  Name of the last author: Yizhou Wang
  Number of Figures: 14
  Number of Tables: 8
  Number of authors: 6
  Paper title: End-to-End Active Object Tracking and Its Real-World Deployment via
    Reinforcement Learning
  Publication Date: 2019-02-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance under Different Protocols in the Standard Testing
      Environment
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance of the Proposed Active Tracker in Different Testing
      Environments
  Table 3 caption:
    table_text: TABLE 3 Comparison with Traditional Trackers
  Table 4 caption:
    table_text: TABLE 4 Performance under Different Protocols in S2MP2
  Table 5 caption:
    table_text: TABLE 5 Comparison with Traditional Trackers
  Table 6 caption:
    table_text: TABLE 6 Mapping Discrete Actions from Virtual to Real
  Table 7 caption:
    table_text: TABLE 7 Mapping Continuous Actions from Virtual to Real
  Table 8 caption:
    table_text: TABLE 8 Comparison between Active Trackers with Different Types of
      Action Space
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2899570
- Affiliation of the first author: "computer vision center, universitat aut\xF2noma\
    \ de barcelona, barcelona, spain"
  Affiliation of the last author: media integration and communication center, university
    of florence, firenze, fi, italy
  Figure 1 Link: articels_figures_by_rev_year\2019\Exploiting_Unlabeled_Data_in_CNNs_by_SelfSupervised_Learning_to_Rank\figure_1.jpg
  Figure 1 caption: Self-supervised learning to rank. Our architecture is based on
    a shared CNN backbone (in white) to which we add problem-specific layers (in blue)
    that end in an output that solves the primary regression task, and layers (also
    in blue) that end in an output that solves a self-supervised ranking task (see
    Section 3). Self supervision is provided by a pair generation module that is able
    to generate pairs with known relative ranks.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Exploiting_Unlabeled_Data_in_CNNs_by_SelfSupervised_Learning_to_Rank\figure_2.jpg
  Figure 2 caption: 'Network architecture and ranked pair generation for IQA. Top:
    our network for no-reference IQA uses a VGG16 network pretrained on ImageNet.
    We decapitate the network and replace the original head with a new fully-connected
    layer generating a single output. Bottom: pairs with known ranking are generated
    by distorting images with standard, parametric distortions. Increasing the distortion
    level guarantees that the images are of progressively worse quality.'
  Figure 3 Link: articels_figures_by_rev_year\2019\Exploiting_Unlabeled_Data_in_CNNs_by_SelfSupervised_Learning_to_Rank\figure_3.jpg
  Figure 3 caption: Example images from the retrieved crowd scene dataset. (top) Representative
    images using key words as query. (bottom) Representative images using training
    image as query image (the query image is depicted on the left).
  Figure 4 Link: articels_figures_by_rev_year\2019\Exploiting_Unlabeled_Data_in_CNNs_by_SelfSupervised_Learning_to_Rank\figure_4.jpg
  Figure 4 caption: "Network architecture and ranked pair generation for crowd counting.\
    \ Top: our counting network uses a VGG16 network truncated at the fifth convolutional\
    \ layer (before maxpooling). To this network we add a 3\xD73\xD71 convolutional\
    \ layer with stride 1 which should estimate local crowd density. A sum pooling\
    \ layer is added to the ranking channel of the network to arrive at a scalar value\
    \ whose relative rank is known. Bottom: image pairs with known relative ranks\
    \ are generated by selectively cropping unlabeled crowd images so that successive\
    \ crops are entirely contained in previous ones."
  Figure 5 Link: articels_figures_by_rev_year\2019\Exploiting_Unlabeled_Data_in_CNNs_by_SelfSupervised_Learning_to_Rank\figure_5.jpg
  Figure 5 caption: Active learning results on TID 2013. We plot LCC as a function
    of the percentage of labeled data used from the training set.
  Figure 6 Link: articels_figures_by_rev_year\2019\Exploiting_Unlabeled_Data_in_CNNs_by_SelfSupervised_Learning_to_Rank\figure_6.jpg
  Figure 6 caption: 'Examples of predicted density maps for the UCFCC50 (Top row,
    true count: 3406 prediction: 3052) and ShanghaiTech datasets (Bottom row, true
    count: 361 prediction: 365). Left column: crowd image. Middle column: ground truth.
    Right column: prediction.'
  Figure 7 Link: articels_figures_by_rev_year\2019\Exploiting_Unlabeled_Data_in_CNNs_by_SelfSupervised_Learning_to_Rank\figure_7.jpg
  Figure 7 caption: Active learning results on Shanghai A. MAE is plotted as a function
    of the percentage of labeled data from the training set.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xialei Liu
  Name of the last author: Andrew D. Bagdanov
  Number of Figures: 7
  Number of Tables: 11
  Number of authors: 3
  Paper title: Exploiting Unlabeled Data in CNNs by Self-Supervised Learning to Rank
  Publication Date: 2019-02-15 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Ablation Study on the Entire TID2013 Database
  Table 10 caption:
    table_text: TABLE 10 MAE and MSE Error on the UCF-QNRF Dataset
  Table 2 caption:
    table_text: TABLE 2 Performance Evaluation (SROCC) on the Entire TID2013 Database
  Table 3 caption:
    table_text: TABLE 3 Generalization to Unseen Distortions
  Table 4 caption:
    table_text: TABLE 4 LCC (above) and SROCC (below) Evaluation on LIVE Dataset
  Table 5 caption:
    table_text: TABLE 5 Ablation Study on UCFCC50 with Five-Fold Cross Validation
  Table 6 caption:
    table_text: TABLE 6 MAE and MSE Error on the UCFCC50 Dataset
  Table 7 caption:
    table_text: TABLE 7 MAE and MSE Error on ShanghaiTech
  Table 8 caption:
    table_text: TABLE 8 MAE Results on the WorldExpo10 Dataset
  Table 9 caption:
    table_text: TABLE 9 MAE and MSE Error on the UCSD Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2899857
- Affiliation of the first author: department of electronic engineering, tsinghua
    university, beijing, china
  Affiliation of the last author: school of engineering, university of california,
    merced, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Progressive_Representation_Adaptation_for_Weakly_Supervised_Object_Localization\figure_1.jpg
  Figure 1 caption: Weakly supervised object localization. Given a collection of training
    images with image-level annotations, our goal is to train object detectors for
    localizing objects in unseen images.
  Figure 10 Link: articels_figures_by_rev_year\2019\Progressive_Representation_Adaptation_for_Weakly_Supervised_Object_Localization\figure_10.jpg
  Figure 10 caption: Sample results of detection errors. Green boxes indicate ground-truth
    instance annotation. Red boxes indicate false positives.
  Figure 2 Link: articels_figures_by_rev_year\2019\Progressive_Representation_Adaptation_for_Weakly_Supervised_Object_Localization\figure_2.jpg
  Figure 2 caption: 'Progressive adaptation. Standard supervised methods use instance-level
    annotations (e.g., tight bounding boxes) to train object detectors. Most weakly
    supervised methods use mine object proposals to select object instances from a
    large and noisy candidate pool in one single step. We propose a two-step progressive
    adaptation approach: classification adaptation (Section 3) and detection adaptation
    (Section 4). Our approach effectively filter out the noisy object proposal collection
    and thus can mine confident candidates for learning discriminative object detectors.'
  Figure 3 Link: articels_figures_by_rev_year\2019\Progressive_Representation_Adaptation_for_Weakly_Supervised_Object_Localization\figure_3.jpg
  Figure 3 caption: Classification adaptation. We set the number of output nodes in
    the last fully-connected layer to 2C , where C is number of object categories.
    These 2C entries are grouped into C pairs for indicating the presence and absence
    of each object category. See Section 3 for details.
  Figure 4 Link: articels_figures_by_rev_year\2019\Progressive_Representation_Adaptation_for_Weakly_Supervised_Object_Localization\figure_4.jpg
  Figure 4 caption: Detection adaptation. We start with generating a collection of
    class-independent proposal using an off-the-shelf object proposal algorithm [22].
    We leverage two scoring strategies to collect class-specific object proposals
    from this set of class-independent proposals (Section 4.1). The contrast scores
    (4) measure the class prediction confidence drop between an object proposal and
    its mask-out image using adapted classification network. The activation scores
    (7) are computed based on class-specific feature maps [36]. After ranking the
    proposals using the two scores, we then apply multiple instance learning to select
    confident candidates for each class (Section 4.2). We further refine the selected
    proposals using segmentation cues to obtain bounding boxes with tight spatial
    support (Section 4.3). Finally, we use the refined object proposals to fine-tune
    all the layers (marked magenta), resulting in a network that is fully adapted
    for detection (Section 4.4).
  Figure 5 Link: articels_figures_by_rev_year\2019\Progressive_Representation_Adaptation_for_Weakly_Supervised_Object_Localization\figure_5.jpg
  Figure 5 caption: Examples of class-specific object proposals. We show top 10 proposals
    for each category (different colors indicate mined proposals for different categories).
  Figure 6 Link: articels_figures_by_rev_year\2019\Progressive_Representation_Adaptation_for_Weakly_Supervised_Object_Localization\figure_6.jpg
  Figure 6 caption: Examples of the refined bounding boxes. The first column shows
    the selected proposals by MIL. The second column shows the generated segments
    using segmentation cues [58]. The last column shows the obtained tight bounding
    boxes. Using segmentation cues helps obtain improved localization accuracy.
  Figure 7 Link: articels_figures_by_rev_year\2019\Progressive_Representation_Adaptation_for_Weakly_Supervised_Object_Localization\figure_7.jpg
  Figure 7 caption: Effect of the minimum bounding box overlap for mining class-specific
    proposals.
  Figure 8 Link: articels_figures_by_rev_year\2019\Progressive_Representation_Adaptation_for_Weakly_Supervised_Object_Localization\figure_8.jpg
  Figure 8 caption: Sample detection results on the PASCAL VOC 2007 test set. Green
    boxes indicate ground-truth instance annotation. Yellow boxes indicate correction
    detections (with IoU geq 0.5 ). For all the testing results, we set threshold
    of detection as 0.8 and use NMS to remove duplicate detections.
  Figure 9 Link: articels_figures_by_rev_year\2019\Progressive_Representation_Adaptation_for_Weakly_Supervised_Object_Localization\figure_9.jpg
  Figure 9 caption: "Detector error analysis. The detections are categorized into\
    \ five types of correct detection (Cor), false positives due to poor localization\
    \ (Loc), confusion with similar objects (Sim), confusion with other VOC objects\
    \ (Oth), and confusion with background (BG). Each plot shows types of detection\
    \ as top detections increase. Line plots show recall as function of the number\
    \ of objects by IoU \u22650.5 (solid) and IoU \u22650.1 (dash). The VGGNet is\
    \ used as the base network for training object detectors."
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Dong Li
  Name of the last author: Ming-Hsuan Yang
  Number of Figures: 10
  Number of Tables: 7
  Number of authors: 5
  Paper title: Progressive Representation Adaptation for Weakly Supervised Object
    Localization
  Publication Date: 2019-02-15 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Comparisons in Terms of Correct Localization
      on the PASCAL VOC 2007 Trainval Set
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Comparisons in Terms of Average Precision on
      the PASCAL VOC 2007 Test Set
  Table 3 caption:
    table_text: TABLE 3 Quantitative Comparisons in Terms of Average Precision on
      the PASCAL VOC 2010 Test Set
  Table 4 caption:
    table_text: TABLE 4 Quantitative Comparisons in Terms of Average Precision on
      the PASCAL VOC 2012 Test Set
  Table 5 caption:
    table_text: TABLE 5 Object Detection on the ILSVRC 2013 Dataset
  Table 6 caption:
    table_text: TABLE 6 Different Mask-Out Strategies in Terms of Average Correct
      Localization from Top M M Proposals
  Table 7 caption:
    table_text: TABLE 7 Effect of the Ratio between Contrast and Activation Scores
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2899839
- Affiliation of the first author: shenzhen graduate school, peking university, shenzhen,
    china
  Affiliation of the last author: shenzhen graduate school, peking university, shenzhen,
    china
  Figure 1 Link: articels_figures_by_rev_year\2019\A_Disocclusion_Inpainting_Framework_for_DepthBased_View_Synthesis\figure_1.jpg
  Figure 1 caption: Example of foreground penetration problem in image or video inpainting
    methods. (a) The hole regions. (b) Criminisis method [16]. (c) PatchMatch [29].
    (d) Image Melding [33]. (e) Huangs method [34]. (f) Wexlers method [28]. (g) Newsons
    method [44]. (h) Ground truth.
  Figure 10 Link: articels_figures_by_rev_year\2019\A_Disocclusion_Inpainting_Framework_for_DepthBased_View_Synthesis\figure_10.jpg
  Figure 10 caption: "Disocclusions filling for \u201CBallet\u201D. (a) Rendered video.\
    \ (b) Rendered BG video. (c) The the blending of (a) and (b) . (d) Criminisis\
    \ inpainting method [16] is used to fill the holes of (c). (e) Criminisis inpainting\
    \ method [16] is used to fill the holes of (b). (f) Priority-BP inpainting method\
    \ [22] is used to fill the holes of (b). (g) (e) is used to fill the holes of\
    \ (a). (h) (f) is used to fill the holes of (a)."
  Figure 2 Link: articels_figures_by_rev_year\2019\A_Disocclusion_Inpainting_Framework_for_DepthBased_View_Synthesis\figure_2.jpg
  Figure 2 caption: Example of disocclusion filling. (a) The hole regions. (b) Image
    inpainting based method. (c) Background reconstruction based method. (d) Video
    completion method. (e) Ground truth.
  Figure 3 Link: articels_figures_by_rev_year\2019\A_Disocclusion_Inpainting_Framework_for_DepthBased_View_Synthesis\figure_3.jpg
  Figure 3 caption: Examples of the problem in traditional background models. (a)
    The scenario consists of reciprocal motion and stationary foreground. (b) Moving
    camera scenario.
  Figure 4 Link: articels_figures_by_rev_year\2019\A_Disocclusion_Inpainting_Framework_for_DepthBased_View_Synthesis\figure_4.jpg
  Figure 4 caption: Block diagram of the proposed framework with BG reconstruction
    work in virtual view.
  Figure 5 Link: articels_figures_by_rev_year\2019\A_Disocclusion_Inpainting_Framework_for_DepthBased_View_Synthesis\figure_5.jpg
  Figure 5 caption: Depth value prediction. (a) Warped depth map with holes. (b) Predicted
    depth map.
  Figure 6 Link: articels_figures_by_rev_year\2019\A_Disocclusion_Inpainting_Framework_for_DepthBased_View_Synthesis\figure_6.jpg
  Figure 6 caption: "Separation of foreground and background for \u201CBallet\u201D\
    . (a) Initial seeds on the depth map, foreground label (red line) and the background\
    \ label (green line). (b) Background regions. (c) Foreground regions."
  Figure 7 Link: articels_figures_by_rev_year\2019\A_Disocclusion_Inpainting_Framework_for_DepthBased_View_Synthesis\figure_7.jpg
  Figure 7 caption: Performance comparison of (a) background reconstruction without
    using foreground removal. (b) background reconstruction using foreground removal.
  Figure 8 Link: articels_figures_by_rev_year\2019\A_Disocclusion_Inpainting_Framework_for_DepthBased_View_Synthesis\figure_8.jpg
  Figure 8 caption: Performance comparison of (a) background reconstruction without
    using depth information. (b) background reconstruction using depth information.
  Figure 9 Link: articels_figures_by_rev_year\2019\A_Disocclusion_Inpainting_Framework_for_DepthBased_View_Synthesis\figure_9.jpg
  Figure 9 caption: "Background reconstruction comparison in \u201CDancer\u201D sequence.\
    \ (a) without using foreground removal. (b) using foreground removal."
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.66
  Name of the first author: Guibo Luo
  Name of the last author: Zhaotian Li
  Number of Figures: 17
  Number of Tables: 6
  Number of authors: 4
  Paper title: A Disocclusion Inpainting Framework for Depth-Based View Synthesis
  Publication Date: 2019-02-15 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Characteristics of the Test Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Evaluate of the Foreground Detection Performance
  Table 3 caption:
    table_text: TABLE 3 The Objective Evaluations of Different Combination of Methods
      Using PSNR
  Table 4 caption:
    table_text: TABLE 4 The Objective Evaluations of the Proposed and Other Methods
      Using PSNR and SSIM
  Table 5 caption:
    table_text: TABLE 5 Flicker Value of the Synthesized Videos
  Table 6 caption:
    table_text: 'TABLE 6 Comparison of Running Time (Time Unit: s)'
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2899837
- Affiliation of the first author: department of biomedical engineering, mathematical
    institute for data science, johns hopkins university, baltimore, usa
  Affiliation of the last author: department of biomedical engineering, mathematical
    institute for data science, johns hopkins university, baltimore, usa
  Figure 1 Link: articels_figures_by_rev_year\2019\Structured_LowRank_Matrix_Factorization_Global_Optimality_Algorithms_and_Applica\figure_1.jpg
  Figure 1 caption: 'Recovered spatial segmentations from phantom dataset. Left: True
    spatial labels. Middle: Spatial labels recovered with sparse + low-rank + TV regularization,
    with U initialized as an identity matrix. Right: Same as the middle panel but
    with U initialized as 50 columns of random values uniformly distributed between
    [0,1].'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2019\Structured_LowRank_Matrix_Factorization_Global_Optimality_Algorithms_and_Applica\figure_2.jpg
  Figure 2 caption: 'Example reconstructed calcium signal from phantom dataset. The
    two rows correspond to two different example image frames. From left to right:
    Raw data. True calcium signal. Reconstruction with sparse + low-rank regularization.
    Reconstruction with sparse + low-rank + TV regularization with U initialized as
    the identity. Reconstruction with sparse + low-rank + TV regularization with U
    initialized as 50 columns of random values uniformly distributed in [0,1].'
  Figure 3 Link: articels_figures_by_rev_year\2019\Structured_LowRank_Matrix_Factorization_Global_Optimality_Algorithms_and_Applica\figure_3.jpg
  Figure 3 caption: 'Example recovered spatial components from phantom dataset. Top
    Left: First 9 most significant spatial components recovered via Principal Component
    Analysis (PCA). Bottom Left: First 9 most significant spatial components recovered
    with sparse and low-rank regularization. Top Right: First 9 most significant spatial
    components recovered using sparse, low-rank, and TV regularization, with U initialized
    as the identity. Bottom Right: Same as the top right panel but with U initialized
    as 50 columns of random values uniformly distributed in [0,1].'
  Figure 4 Link: articels_figures_by_rev_year\2019\Structured_LowRank_Matrix_Factorization_Global_Optimality_Algorithms_and_Applica\figure_4.jpg
  Figure 4 caption: 'Reconstructed spike trains from phantom dataset with sparse +
    low-rank + TV for the components shown in Fig. 3. Blue lines are the temporal
    components estimated by our algorithm, while the red dots correspond to the true
    temporal spike times. Left Panel: Reconstruction with U initialized as the identity.
    Right Panel: Reconstruction with U initialized as 50 columns of random values
    uniformly distributed in [0,1].'
  Figure 5 Link: articels_figures_by_rev_year\2019\Structured_LowRank_Matrix_Factorization_Global_Optimality_Algorithms_and_Applica\figure_5.jpg
  Figure 5 caption: 'Results from an in vivo calcium imaging dataset. Left: Spatial
    features for 5 example regions. (Top Row) Manually segmented regions. (Bottom
    3 Rows) Corresponding spatial feature recovered by our method with various regularizations.
    Note that regions 1 and 2 are different parts of the same neurons - see discussion
    in the text. Right: Example frames from the dataset corresponding to time points
    where the example regions display a significant calcium signal. (Top Row) Actual
    Data. (Bottom 3 Rows) Estimated signal for the example frame with various regularizations.'
  Figure 6 Link: articels_figures_by_rev_year\2019\Structured_LowRank_Matrix_Factorization_Global_Optimality_Algorithms_and_Applica\figure_6.jpg
  Figure 6 caption: 'Results of PCA applied to an in vivo calcium imaging dataset.
    Left: The first 5 most significant spatial components from PCA analysis. Right:
    Example image frames reconstructed from the first 20 most significant Principal
    Components. The example frames are the same is in Fig. 5.'
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Benjamin D. Haeffele
  Name of the last author: "Ren\xE9 Vidal"
  Number of Figures: 6
  Number of Tables: 3
  Number of authors: 2
  Paper title: 'Structured Low-Rank Matrix Factorization: Global Optimality, Algorithms,
    and Applications'
  Publication Date: 2019-02-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Typical Properties of Optimization Problems in the Product
      Space ( X X) versus Factorized Space (U,V) (U,V) (Items in Bold Are Desirable)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Regularization Parameters for in vivo Calcium Imaging Experiments
  Table 3 caption:
    table_text: TABLE 3 Hyperspectral Imaging Compressed Recovery Error Rates
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2900306
