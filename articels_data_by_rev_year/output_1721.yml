- Affiliation of the first author: mathematical institute for data science (minds),
    johns hopkins university, baltimore, md, usa
  Affiliation of the last author: department of biomedical engineering and institute
    for computational medicine, center for imaging science, johns hopkins university,
    baltimore, md, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\Kernel_kGroups_via_Hartigans_Method\figure_1.jpg
  Figure 1 caption: "We report the mean accuracy (error bars are too small to be visible)\
    \ over 100 Monte Carlo runs. (a) x\u223C 1 2 N(0,1.5)+ 1 2 N(1.5,0.3) ; (b) x\u223C\
    \ 1 2 e N(0,1.5) + 1 2 e N(1.5,0.3) ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Kernel_kGroups_via_Hartigans_Method\figure_2.jpg
  Figure 2 caption: 'We consider 2000 points sampled from the same respective distributions
    of Fig. 1. We perform a kernel density estimation for kernel k-groups, while for
    k-means and GMM we show the estimated Gaussian fit. The respective accuracies
    for k-means, GMM, and kernel k-groups are as follows: (a) 0.764,0.870,0.800 ;
    (b) 0.516,0.533,0.851 . Note that in the latter case only kernel k-groups was
    able to distinguish the two classes.'
  Figure 3 Link: articels_figures_by_rev_year\2020\Kernel_kGroups_via_Hartigans_Method\figure_3.jpg
  Figure 3 caption: "Comparing kernel k-groups to other methods; see legend in plot\
    \ (f). For each experiment, except for (e), we perform 100 Monte Carlo runs and\
    \ show the mean accuracy (error bars are standard deviation but are too small\
    \ to be visible). (a) Higher dimensional Gaussian mixture (56). Dashed line is\
    \ Bayes accuracy \u22480.86 . We use the standard energy statistics metric \u03C1\
    \ 1 in (52). (b) Higher dimensional Gaussian mixture (57). Bayes accuracy is \u2248\
    0.95 . We again use \u03C1 1 . (c) Gaussian mixture with parameters (58). We increase\
    \ the number of data points, and compare different metrics (see (52)\u2013(54)).\
    \ The best results for kernel k-groups and spectral clustering use \u03C1 \u02DC\
    \ 1 . (d) Same as in (c) but for lognormal mixture with (58). The plot suggests\
    \ that neither of these methods are consistent in this case since Bayes accuracy\
    \ is \u22480.90 . (e) Effect of large number of clusters on a two-dimensional\
    \ grid. All clusters have 10 points sampled from N(\u03BC,0.1I) where \u03BC assume\
    \ coordinates on the grid (separated by one unit). We increase the number of clusters\
    \ and use the standard \u03C1 1 . (f) Unbalanced clusters. The data is normally\
    \ distributed (59) where we vary m\u2208[0,240] . We use the standard metric \u03C1\
    \ 1 ."
  Figure 4 Link: articels_figures_by_rev_year\2020\Kernel_kGroups_via_Hartigans_Method\figure_4.jpg
  Figure 4 caption: "Girvan-Newman benchmark [45] where n=128 , k=4 , and d=16 . (a)\
    \ \u03BB=1.1 , (b) \u03BB=2.0 , and (c) \u03BB=3.5 . Note how the clusters become\
    \ more well-defined as \u03BB>1 increases. See Table 1 for clustering results."
  Figure 5 Link: articels_figures_by_rev_year\2020\Kernel_kGroups_via_Hartigans_Method\figure_5.jpg
  Figure 5 caption: Comparison of kernel k-groups (Algorithm 2) to Bethe Hessian [44]
    (see also [42]). We choose a stochastic block model with n=1000 vertices and average
    degree d=3 . We plot the overlap (63) against the signal-to-noise ratio (62).
    We sample 100 graphs (dots are the mean and error bars one standard deviation).
    (a) Both methods perform closely, but with a minor improvement of kernel k-groups.
    (b, c) Bethe Hessian degenerates performance as k increases, contrary to kernel
    k-groups which remains accurate; we include the result with k=2 (dashed black
    line) for reference.
  Figure 6 Link: articels_figures_by_rev_year\2020\Kernel_kGroups_via_Hartigans_Method\figure_6.jpg
  Figure 6 caption: Clustering the dermatology dataset [46], [47] with kernel k-groups
    using rho 12 (see (52)). We show a heatmap of the class membership. This should
    be compared with Table 2 of [5] (our results are more accurate). See also Table
    2.
  Figure 7 Link: articels_figures_by_rev_year\2020\Kernel_kGroups_via_Hartigans_Method\figure_7.jpg
  Figure 7 caption: 'Violin plots showing the distribution of the results obtained
    in the experiments of Table 3: kernel k-groups (red), kernel k-means (blue) and
    spectral clustering (green). We consider 100 Monte Carlo runs for each experiment.
    In most cases kernel k-groups has a smaller variance compared to kernel k-means
    (both are initialized with k-means++).'
  Figure 8 Link: articels_figures_by_rev_year\2020\Kernel_kGroups_via_Hartigans_Method\figure_8.jpg
  Figure 8 caption: (a) Drosophila left hemisphere [52] after clustered into k=2 communities
    with kernel k-groups (see Table 5). A very similar picture is obtained for the
    right hemisphere, and thus omitted. For the left hemisphere one class has 116
    nodes and the other 93. For the right hemisphere one class has 112 nodes and the
    other 101. (b) Graph obtained after clustering the arXiv GR-QC network [53] (see
    Table 6). Note that there is a large community (purple) that is highly connected,
    and many other smaller communities. The number of nodes in the top 5 communities
    are lbrace 1955, 200, 123, 101, 79, ldots rbrace .
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "Guilherme Fran\xE7a"
  Name of the last author: Joshua T. Vogelstein
  Number of Figures: 8
  Number of Tables: 6
  Number of authors: 3
  Paper title: "Kernel k-Groups via Hartigan\u2019s Method"
  Publication Date: 2020-05-28 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Clustering Graphs From the Model Shown in Fig. 4
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Dermatology Dataset [46], [47] (See Also Fig. 6)
  Table 3 caption:
    table_text: TABLE 3 Kernel k-Groups (Groups), Kernel k-Means (Means) and Spectral
      Clustering (Spectral) on Several Public Datasets From UCI Repository [46]
  Table 4 caption:
    table_text: TABLE 4 Community Detection in Real World Networks With Ground Truth
  Table 5 caption:
    table_text: TABLE 5 Clustering the Drosophila Connectome [52]
  Table 6 caption:
    table_text: TABLE 6 Community Detection in arXiv GR-QC Network [53]
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2998120
- Affiliation of the first author: department of computer science and engineering,
    texas a&m university, college station, tx, usa
  Affiliation of the last author: department of computer science and engineering,
    texas a&m university, college station, tx, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\SecondOrder_Pooling_for_Graph_Neural_Networks\figure_1.jpg
  Figure 1 caption: 'Illustrations of our proposed graph pooling methods: bilinear
    mapping second-order pooling (SOPool bimap ) in Section 3.3 and attentional second-order
    pooling (SOPool attn ) in Section 3.4. This is an example for a graph G with n=8
    nodes. GNNs can learn representations for each node and graph pooling processes
    node representations into a graph representation vector h G .'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\SecondOrder_Pooling_for_Graph_Neural_Networks\figure_2.jpg
  Figure 2 caption: "Examples of graphs that pooling methods discussed in Section\
    \ 3.5 fail to distinguish, i.e., producing the same graph representation for different\
    \ graphs G 1 and G 2 . The same color denotes the same node representation. (a)\
    \ Covariance pooling (CovPool) and attentional pooling (AttnPool) both fail. CovPool\
    \ fails because subtracting the mean results in h G 1 = h G 2 =0 . AttnPool computes\
    \ the mean of node representations, leading to h G 1 = h G 2 as well. (b) AttnPool\
    \ fails in this example with the same \u03BC ."
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.78
  Name of the first author: Zhengyang Wang
  Name of the last author: Shuiwang Ji
  Number of Figures: 2
  Number of Tables: 5
  Number of authors: 2
  Paper title: Second-Order Pooling for Graph Neural Networks
  Publication Date: 2020-06-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison Results Between Our Proposed Methods and Baselines
      Described in Section 4.1
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison Results Between Our Proposed Methods and Other
      Graph Pooling Methods by Fixing the GNN Before Graph Pooling to GIN-0, as Described
      in Section 4.3
  Table 3 caption:
    table_text: TABLE 3 Results of Our Proposed Methods With Different GNNs Before
      Graph Pooling, as Described in Section 4.4
  Table 4 caption:
    table_text: TABLE 4 Comparison Results Between Different Hierarchical Graph Pooling
      Methods
  Table 5 caption:
    table_text: TABLE 5 Comparison Results of SOPool mattn mattn With Different Hierarchical
      GNNs
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2999032
- Affiliation of the first author: clova ai research, naver corp, seongnam-si, gyeonggi-do,
    south korea
  Affiliation of the last author: school of integrated technology, yonsei university,
    seoul, south korea
  Figure 1 Link: articels_figures_by_rev_year\2020\AttentionBased_Dropout_Layer_for_Weakly_Supervised_Single_Object_Localization_an\figure_1.jpg
  Figure 1 caption: Attention-based dropout layer (ADL) block diagram. The attention
    map is generated by channel-wise average pooling of the input feature map. Based
    on the attention map, we produce a drop mask and importance map using thresholding
    and sigmoid activation, respectively. The drop mask or the importance map is selected
    stochastically at each iteration and applied to the input feature map. Please
    note that this figure illustrates the case where the importance map is selected.
  Figure 10 Link: articels_figures_by_rev_year\2020\AttentionBased_Dropout_Layer_for_Weakly_Supervised_Single_Object_Localization_an\figure_10.jpg
  Figure 10 caption: Qualitative evaluation results on the COCO dataset. Each column
    represents the ground truth, input image, segmentation map generated by seed,
    expand, and constrain (SEC) technique, and the segmentation map generated by our
    attention-based dropout layer (ADL).
  Figure 2 Link: articels_figures_by_rev_year\2020\AttentionBased_Dropout_Layer_for_Weakly_Supervised_Single_Object_Localization_an\figure_2.jpg
  Figure 2 caption: Drop mask and attention map at each layer of VGG-GAP [1]. At lower-level
    layers, the attention maps include general features, whereas class-specific features
    are included in the attention maps at higher-level layers. The drop masks also
    erase the most discriminative part more effectively at higher-level layers. Please
    note that the drop mask is overlaid with the input image for better visualization.
    Because the importance map has a distribution very similar to that of the attention
    map, we do not visualize it.
  Figure 3 Link: articels_figures_by_rev_year\2020\AttentionBased_Dropout_Layer_for_Weakly_Supervised_Single_Object_Localization_an\figure_3.jpg
  Figure 3 caption: Qualitative evaluation results corresponding to the upper part
    of Table 1. It can be seen that, as the drop rate decreases, the model focuses
    only on the most discriminative part of the object.
  Figure 4 Link: articels_figures_by_rev_year\2020\AttentionBased_Dropout_Layer_for_Weakly_Supervised_Single_Object_Localization_an\figure_4.jpg
  Figure 4 caption: Qualitative evaluation results corresponding to the lower part
    of Table 1. When only the importance map is applied, the model focuses only on
    the most discriminative part of the object. In contrast, when the drop mask is
    applied, the model can localize the entire extent of the object. We note that
    the best localization result can be obtained by using both the drop mask and importance
    map.
  Figure 5 Link: articels_figures_by_rev_year\2020\AttentionBased_Dropout_Layer_for_Weakly_Supervised_Single_Object_Localization_an\figure_5.jpg
  Figure 5 caption: Qualitative evaluation results corresponding to Table 2. We observe
    that the localization accuracy increases when the attention-based dropout layers
    (ADLs) are applied to multiple feature maps.
  Figure 6 Link: articels_figures_by_rev_year\2020\AttentionBased_Dropout_Layer_for_Weakly_Supervised_Single_Object_Localization_an\figure_6.jpg
  Figure 6 caption: Qualitative evaluation results of VGG-GAP [1] on the CUB-200-2011
    and ImageNet-1k datasets. The left image in each figure is the input image. The
    red bounding box represents the ground truth, and the green bounding box represents
    the estimates. The middle image is the heatmap, and the right image depicts the
    overlap between the input image and the heatmap. We also compared our method with
    the vanilla model side by side.
  Figure 7 Link: articels_figures_by_rev_year\2020\AttentionBased_Dropout_Layer_for_Weakly_Supervised_Single_Object_Localization_an\figure_7.jpg
  Figure 7 caption: Qualitative evaluation results on Pascal VOC 2012 dataset. Each
    column represents the ground truth, input image, segmentation map generated by
    seed, expand, and constrain (SEC) technique, and segmentation map generated by
    attention-based dropout layer (ADL).
  Figure 8 Link: articels_figures_by_rev_year\2020\AttentionBased_Dropout_Layer_for_Weakly_Supervised_Single_Object_Localization_an\figure_8.jpg
  Figure 8 caption: Failure case in ImageNet-1k experiments. The target class is snowmobile.
    The model with attention-based dropout layer (ADL) learns the less discriminative
    region, which is not included in the object. Specifically, the model captures
    not only the snowmobile, but also the snow and tree.
  Figure 9 Link: articels_figures_by_rev_year\2020\AttentionBased_Dropout_Layer_for_Weakly_Supervised_Single_Object_Localization_an\figure_9.jpg
  Figure 9 caption: Pseudo masks on the COCO dataset. The foreground of each category
    is visualized in a different color. Each column represents the ground truth, input
    image, pseudo mask generated by seed, expand, and constrain (SEC) technique, and
    the pseudo mask generated by our attention-based dropout layer (ADL).
  First author gender probability: 0.96
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.87
  Name of the first author: Junsuk Choe
  Name of the last author: Hyunjung Shim
  Number of Figures: 10
  Number of Tables: 10
  Number of authors: 3
  Paper title: Attention-Based Dropout Layer for Weakly Supervised Single Object Localization
    and Semantic Segmentation
  Publication Date: 2020-06-01 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Top: Accuracy According to droprate'
  Table 10 caption:
    table_text: TABLE 10 Mean IOU (mIOU) Between the Pseudo Masks and the Segmentation
      Results
  Table 2 caption:
    table_text: TABLE 2 Effect on Accuracy as a Result of the Choice of Feature Maps
      Applied for Attention-Based Dropout Layer (ADL)
  Table 3 caption:
    table_text: TABLE 3 Effect on the Type of Activation Layers
  Table 4 caption:
    table_text: TABLE 4 Quantitative Evaluation Results on the CUB-200-2011 and ImageNet-1k
      Datasets
  Table 5 caption:
    table_text: TABLE 5 Compatibility With Standard Dropout
  Table 6 caption:
    table_text: TABLE 6 Comparison of Attention-Based Dropout Layer (ADL) With SpatialDropout
      and MaxDrop
  Table 7 caption:
    table_text: TABLE 7 Quantitative Evaluation Results on the Pascal VOC 2012 Dataset
  Table 8 caption:
    table_text: TABLE 8 Quantitative Evaluation Results on the COCO Dataset
  Table 9 caption:
    table_text: TABLE 9 Segmentation Performance Gain by Class on the COCO Dataset
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2999099
- Affiliation of the first author: department of cse, washington university in st.
    louis, st. louis, usa
  Affiliation of the last author: department of cs, university of illinois, urbana,
    usa
  Figure 1 Link: Not Available
  Figure 1 caption: Not Available
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.62
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ayan Chakrabarti
  Name of the last author: David A. Forsyth
  Number of Figures: Not Available
  Number of Tables: 0
  Number of authors: 3
  Paper title: Guest Editors' Introduction to the Special Section on Computational
    Photography
  Publication Date: 2020-06-04 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2993888
- Affiliation of the first author: key laboratory of intelligent perception and image
    understanding of ministry of education, school of artificial intelligence, xidian
    university, xian, shaanxi, china
  Affiliation of the last author: key laboratory of machine perception (moe), school
    of eecs, peking university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Accelerated_Variance_Reduction_Stochastic_ADMM_for_LargeScale_Machine_Learning\figure_1.jpg
  Figure 1 caption: 'Comparison of the linearized proximal SVRG-ADMM and our ASVRG-ADMM
    algorithms for both SC and non-SC problems on the two data sets: a9a (top) and
    epsilon (bottom).'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Accelerated_Variance_Reduction_Stochastic_ADMM_for_LargeScale_Machine_Learning\figure_2.jpg
  Figure 2 caption: Comparison of different stochastic ADMM methods for non-SC graph-guided
    fused Lasso problems on the four data sets. The y -axis represents the objective
    value minus the minimum value (top) or test loss (bottom), and the x -axis corresponds
    to the running time (seconds).
  Figure 3 Link: articels_figures_by_rev_year\2020\Accelerated_Variance_Reduction_Stochastic_ADMM_for_LargeScale_Machine_Learning\figure_3.jpg
  Figure 3 caption: Comparison of the stochastic ADMM methods for SC graph-guided
    logistic regression problems on a9a (top) and w8a (bottom).
  Figure 4 Link: articels_figures_by_rev_year\2020\Accelerated_Variance_Reduction_Stochastic_ADMM_for_LargeScale_Machine_Learning\figure_4.jpg
  Figure 4 caption: 'Accuracy comparison of multi-class classification on 20newsgroups:
    accuracy versus running time (left) or number of epochs (right).'
  Figure 5 Link: articels_figures_by_rev_year\2020\Accelerated_Variance_Reduction_Stochastic_ADMM_for_LargeScale_Machine_Learning\figure_5.jpg
  Figure 5 caption: Comparison of all the methods for generalized graph-guided fused
    Lasso on a9a, where regularization parameters lambda 1 = lambda 2 = 10-5 .
  Figure 6 Link: articels_figures_by_rev_year\2020\Accelerated_Variance_Reduction_Stochastic_ADMM_for_LargeScale_Machine_Learning\figure_6.jpg
  Figure 6 caption: Comparison of all the methods for multi-task learning problems
    on 20newsgroups, where the regularization parameter lambda 1 = 10-4 .
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.64
  Name of the first author: Yuanyuan Liu
  Name of the last author: Zhouchen Lin
  Number of Figures: 6
  Number of Tables: 2
  Number of authors: 6
  Paper title: Accelerated Variance Reduction Stochastic ADMM for Large-Scale Machine
    Learning
  Publication Date: 2020-06-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Convergence Rates and Memory Requirements of
      Various Stochastic ADMM Algorithms, Including Stochastic ADMM (STOC-ADMM) [9],
      Stochastic Average Gradient ADMM (SAG-ADMM) [19], Stochastic Dual Coordinate
      Ascent ADMM (SDCA-ADMM) [20], Scalable Stochastic ADMM (SCAS-ADMM) [21], Stochastic
      Variance Reduced Gradient ADMM (SVRG-ADMMM) [22], and Our ASVRG-ADMM
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Summary of All the Real-World Data Sets Used in Our Experiments
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3000512
- Affiliation of the first author: school of computer science and technology, east
    china normal university, shanghai, china
  Affiliation of the last author: school of computer science and technology, east
    china normal university, shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2020\MultiView_Representation_Learning_With_Deep_Gaussian_Processes\figure_1.jpg
  Figure 1 caption: The graphical model for the DGPs.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\MultiView_Representation_Learning_With_Deep_Gaussian_Processes\figure_2.jpg
  Figure 2 caption: The graphical model for two-layer DGPs on multi-view data.
  Figure 3 Link: articels_figures_by_rev_year\2020\MultiView_Representation_Learning_With_Deep_Gaussian_Processes\figure_3.jpg
  Figure 3 caption: The graphical model for the MvDGPs.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.89
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.57
  Name of the first author: Shiliang Sun
  Name of the last author: Qiuyang Liu
  Number of Figures: 3
  Number of Tables: 14
  Number of authors: 3
  Paper title: Multi-View Representation Learning With Deep Gaussian Processes
  Publication Date: 2020-06-10 00:00:00
  Table 1 caption:
    table_text: TABLE 1 WebKB Data Set
  Table 10 caption:
    table_text: TABLE 10 Influence of Representation Dimension on Algorithm Performance
  Table 2 caption:
    table_text: TABLE 2 Comparison of Representation Ability of Different Methods
      on WebKB
  Table 3 caption:
    table_text: TABLE 3 Impact of Classifier Selection on Data Representation
  Table 4 caption:
    table_text: TABLE 4 Comparison of Different Classifiers on WebKB
  Table 5 caption:
    table_text: TABLE 5 Internet Advertisements Data Set
  Table 6 caption:
    table_text: TABLE 6 Comparison of Representation Ability of Different Methods
      on Internet Advertisements
  Table 7 caption:
    table_text: TABLE 7 Comparison of Different Classifiers on Internet Advertisements
  Table 8 caption:
    table_text: TABLE 8 Comparison of Representation Ability of Different Methods
      on Multiple Features
  Table 9 caption:
    table_text: TABLE 9 Comparison of Different Classifiers on Multiple Features
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3001433
- Affiliation of the first author: department of computer, control, and management
    engineering, sapienza university of rome, rome, italy
  Affiliation of the last author: mapillary research, graz, austria
  Figure 1 Link: articels_figures_by_rev_year\2020\MultiDIAL_Domain_Alignment_Layers_for_Multisource_Unsupervised_Domain_Adaptation\figure_1.jpg
  Figure 1 caption: MultiDIAL as applied on AlexNet [27]. Source and target images
    are provided as input to the network. After passing through the first layers,
    they enter our DA-layer where source and target distributions are aligned. The
    DA-Layer is shown in detail in Fig. 2. After flowing through the whole network,
    source samples contribute to a Softmax loss, while target samples contribute to
    an Entropy loss, which promotes classification models which maximally separate
    unlabeled data. Note that we use multiple DA-layers to align learned feature representations
    at different levels.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\MultiDIAL_Domain_Alignment_Layers_for_Multisource_Unsupervised_Domain_Adaptation\figure_2.jpg
  Figure 2 caption: "The DA-layer learns the global statistics of all domains and\
    \ normalizes the source and target mini-batches according to the computed mean\
    \ and variance, different for each domain (see Section 3.1). The amount by which\
    \ each distribution is influenced by the global one and therefore the degree of\
    \ domain alignment, depends on a parameter, \u03B1\u2208[0.0,1.0] , which is also\
    \ automatically learned."
  Figure 3 Link: articels_figures_by_rev_year\2020\MultiDIAL_Domain_Alignment_Layers_for_Multisource_Unsupervised_Domain_Adaptation\figure_3.jpg
  Figure 3 caption: "Distributions of randomly sampled sourcetarget features from\
    \ different layers of MultiDIAL \u2013 Inception-BN learned on the Amazon\u2013\
    DSLR task of the Office 31 dataset (best viewed on screen)."
  Figure 4 Link: articels_figures_by_rev_year\2020\MultiDIAL_Domain_Alignment_Layers_for_Multisource_Unsupervised_Domain_Adaptation\figure_4.jpg
  Figure 4 caption: alpha parameters learned on the Office31 dataset, plotted as a
    function of layer depth (left and center) and training iteration (right).
  Figure 5 Link: articels_figures_by_rev_year\2020\MultiDIAL_Domain_Alignment_Layers_for_Multisource_Unsupervised_Domain_Adaptation\figure_5.jpg
  Figure 5 caption: Accuracy on the Office31 dataset when considering different architectures
    based on AlexNet.
  Figure 6 Link: articels_figures_by_rev_year\2020\MultiDIAL_Domain_Alignment_Layers_for_Multisource_Unsupervised_Domain_Adaptation\figure_6.jpg
  Figure 6 caption: alpha parameters learned on the SVHN-Mnist dataset, plotted as
    a function of layer depth.
  Figure 7 Link: articels_figures_by_rev_year\2020\MultiDIAL_Domain_Alignment_Layers_for_Multisource_Unsupervised_Domain_Adaptation\figure_7.jpg
  Figure 7 caption: Sensitivity analysis on the PACS multisource setting, using a
    ResNet18. The chart on the left shows the average dataset accuracy when the batch
    size is varied, while on the right we can see the effect of varying the entropy
    loss weight.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Fabio Maria Carlucci
  Name of the last author: "Samuel Rota Bul\xF3"
  Number of Figures: 7
  Number of Tables: 10
  Number of authors: 5
  Paper title: 'MultiDIAL: Domain Alignment Layers for (Multisource) Unsupervised
    Domain Adaptation'
  Publication Date: 2020-06-10 00:00:00
  Table 1 caption:
    table_text: TABLE 1 AlexNet-Based Approaches on Office31Full Sampling Protocol
  Table 10 caption:
    table_text: TABLE 10 MultiDIAL Results on the Multi-Source PACS Setting
  Table 2 caption:
    table_text: TABLE 2 VGGf-Based Approaches on Office31Full Sampling Protocol
  Table 3 caption:
    table_text: TABLE 3 VGGf-Based Approaches on Office-Home [52]. Art (Ar), Clipart
      (Cl), Product (Pr), Real-World (Rw)
  Table 4 caption:
    table_text: TABLE 4 Inception-Based Approaches on Office31Full Sampling Protocol
  Table 5 caption:
    table_text: TABLE 5 Office-Caltech Results Using the Full Protocol
  Table 6 caption:
    table_text: TABLE 6 Results on the SVHN to MNIST Benchmark
  Table 7 caption:
    table_text: TABLE 7 Cross-Dataset Testbed Results Using the Protocol in [49]
  Table 8 caption:
    table_text: TABLE 8 Cross-Dataset Testbed Results Using the Protocol in [45]
  Table 9 caption:
    table_text: TABLE 9 MultiDIAL Results on the Multi-Source Office31 Setting
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3001338
- Affiliation of the first author: school of computer science and engineering, south
    china university of technology, guangzhou, china
  Affiliation of the last author: school of computer science and engineering, south
    china university of technology, guangzhou, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Video_Snapshot_Single_Image_Motion_Expansion_via_Invertible_Motion_Embedding\figure_1.jpg
  Figure 1 caption: Our proposed method encodes spatio-temporal information of the
    frame sequence into an embedded image (a), called video snapshot. Later, this
    video snapshot can be restored back to the video. In (b), images from the first
    row to the third row of each case are the groundtruth frames, restored frames
    from the video snapshot and the difference map of MAE. Note that, since we have
    to compare the restored frames with the groundtruth frames, the results in (b)
    are the direct output from the decoder without interpolation. Our complete outputs
    with interpolation in GIF format can be found in the supplementary materials,
    which can be found on the Computer Society Digital Library at http:doi.ieeecomputersociety.org10.1109TPAMI.2020.3001644.
    The PSNR, SSIM and MAE values are shown at the bottom right corner in each corresponding
    image.
  Figure 10 Link: articels_figures_by_rev_year\2020\Video_Snapshot_Single_Image_Motion_Expansion_via_Invertible_Motion_Embedding\figure_10.jpg
  Figure 10 caption: Qualitative comparison with existing interpolation methods. Our
    method produces a smooth result with less artifacts.
  Figure 2 Link: articels_figures_by_rev_year\2020\Video_Snapshot_Single_Image_Motion_Expansion_via_Invertible_Motion_Embedding\figure_2.jpg
  Figure 2 caption: Overall architecture of our proposed method. The Motion Embedding
    Network aims to encode a sequence of consecutive frames I into a video snapshot
    by a motion attention module and an encoder. The Motion Expansion Network then
    tries to make the video snapshot alive and generates the output sequence O by
    a decoder and an interpolation network. Note that, with the interpolation network,
    the number of output frames O will be much larger than the original input sequence
    I . This leads to a much longer and smoother video than the original one.
  Figure 3 Link: articels_figures_by_rev_year\2020\Video_Snapshot_Single_Image_Motion_Expansion_via_Invertible_Motion_Embedding\figure_3.jpg
  Figure 3 caption: Network structure of Motion Attention Module.
  Figure 4 Link: articels_figures_by_rev_year\2020\Video_Snapshot_Single_Image_Motion_Expansion_via_Invertible_Motion_Embedding\figure_4.jpg
  Figure 4 caption: Motion attention maps on two consecutive frames. We visualize
    the motion attention maps (mapped to color space) by directly drawing it onto
    their reference frames.
  Figure 5 Link: articels_figures_by_rev_year\2020\Video_Snapshot_Single_Image_Motion_Expansion_via_Invertible_Motion_Embedding\figure_5.jpg
  Figure 5 caption: Visualization of video snapshot. (a) is the groundtruth and (b)
    is video snapshot generated by Motion Embedding Network. When we zoom in the images,
    motion textures can be observed at the bottom row of (b). We can easily observe
    that the patterns in static and moving areas are different, like the motion textures
    around the girl.
  Figure 6 Link: articels_figures_by_rev_year\2020\Video_Snapshot_Single_Image_Motion_Expansion_via_Invertible_Motion_Embedding\figure_6.jpg
  Figure 6 caption: 'Demonstration of the decoded image output by the decoder. Here,
    we only select one frame from the output sequence for a simple illustration. The
    difference between the groundtruth image (a) and decoded image (b) is visually
    imperceptible. The SSIM and PSNR values between (a) and (b) are: SSIM:0.9933 PSNR:40.166.'
  Figure 7 Link: articels_figures_by_rev_year\2020\Video_Snapshot_Single_Image_Motion_Expansion_via_Invertible_Motion_Embedding\figure_7.jpg
  Figure 7 caption: Network structure of interpolation network. The PWC-Net [39] is
    used to calculate the optical flow. The second network is the proposed interpolation
    network. The interpolated frame can be obtained by warping according to the intermediate
    flow predicted by the interpolation network.
  Figure 8 Link: articels_figures_by_rev_year\2020\Video_Snapshot_Single_Image_Motion_Expansion_via_Invertible_Motion_Embedding\figure_8.jpg
  Figure 8 caption: Multiple frames interpolation. Given two input images (a) and
    (b), our proposed interpolation module can predict inter-frame at arbitrary time
    t . (c) and (d) are the results at timesteps t=1.33 and t=1.66 respectively. (e)-(h)
    are bi-directional optical flow generated by our interpolation network at timesteps
    t=1.33 and t=1.66 , respectively.
  Figure 9 Link: articels_figures_by_rev_year\2020\Video_Snapshot_Single_Image_Motion_Expansion_via_Invertible_Motion_Embedding\figure_9.jpg
  Figure 9 caption: Visualization of the embedded image using two embedding related
    losses. Artifacts can be found at the blow up areas of (b) and (c). The embedded
    image generated from the complete model with full losses in (d) has the least
    artifacts and are more similar with the groundtruth image (a).
  First author gender probability: 0.61
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Qianshu Zhu
  Name of the last author: Shengfeng He
  Number of Figures: 18
  Number of Tables: 5
  Number of authors: 5
  Paper title: 'Video Snapshot: Single Image Motion Expansion via Invertible Motion
    Embedding'
  Publication Date: 2020-06-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Statistics of Restored Frames on Different Network Configurations
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Statistics of Restored Frames on Different Embedding Ranges
  Table 3 caption:
    table_text: TABLE 3 Statistics of Restored Frames on Different Frame Intervals
  Table 4 caption:
    table_text: TABLE 4 Interpolation Comparison on the UCF101 Dataset
  Table 5 caption:
    table_text: TABLE 5 Timing Statistics in Testing Phase (in Seconds)
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3001644
- Affiliation of the first author: university of adelaide, adelaide, sa, australia
  Affiliation of the last author: microsoft research, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Structured_Knowledge_Distillation_for_Dense_Prediction\figure_1.jpg
  Figure 1 caption: "An example on the semantic segmentation task shows comparisons\
    \ in terms of computation complexity, number of parameters and mIoU for different\
    \ networks on the Cityscapes test set. The FLOPs is calculated with the input\
    \ resolution of 512\xD71024 pixels. Red triangles are the results of our distillation\
    \ method while others are without distillation. Blue circles are collected from\
    \ FCN [1], RefineNet [4], SegNet [13], ENet [6], PSPNet [3], ERFNet [14], ESPNet\
    \ [15], MobileNetV2Plus [16], and OCNet [17]. With our proposed distillation method,\
    \ we can achieve a higher mIoU, with no extra FLOPs and parameters."
  Figure 10 Link: articels_figures_by_rev_year\2020\Structured_Knowledge_Distillation_for_Dense_Prediction\figure_10.jpg
  Figure 10 caption: 'The effect of structured distillation on CamVid. This figure
    shows that distillation can improve the results in two cases: trained over only
    the labeled data and over both the labeled and extra unlabeled data.'
  Figure 2 Link: articels_figures_by_rev_year\2020\Structured_Knowledge_Distillation_for_Dense_Prediction\figure_2.jpg
  Figure 2 caption: Our distillation framework with the semantic segmentation task
    as an example. (a) Pair-wise distillation; (b) Pixel-wise distillation; (c) Holistic
    distillation. In the training process, we keep the cumbersome network fixed as
    our teacher net, and only the student net and the discriminator net are optimized.
    The student net with a compact architecture is trained with the three distillation
    terms and a task-specific loss, e.g., the cross entropy loss for semantic segmentation.
  Figure 3 Link: articels_figures_by_rev_year\2020\Structured_Knowledge_Distillation_for_Dense_Prediction\figure_3.jpg
  Figure 3 caption: "Illustrations of the connection range \u03B1 and the granularity\
    \ \u03B2 of each node."
  Figure 4 Link: articels_figures_by_rev_year\2020\Structured_Knowledge_Distillation_for_Dense_Prediction\figure_4.jpg
  Figure 4 caption: We show 7 different architectures of the discriminator. The red
    arrow represents a self-attention layer. The orange block denotes a convolution
    block with stride 2. We insert an average pooling layer to the output block to
    obtain the final score.
  Figure 5 Link: articels_figures_by_rev_year\2020\Structured_Knowledge_Distillation_for_Dense_Prediction\figure_5.jpg
  Figure 5 caption: The score distribution of segmentation maps generated by different
    student nets evaluated by a well-trained discriminator. With adversarial training,
    score distributions of segmentation maps become closer to the teacher (the orange
    one); and our method (the red one) is the closest one to the teacher.
  Figure 6 Link: articels_figures_by_rev_year\2020\Structured_Knowledge_Distillation_for_Dense_Prediction\figure_6.jpg
  Figure 6 caption: Segmentation results for structured objects with ResNet18 (1.0)
    trained with different discriminators. (a) Wo holistic distillation, (b) W Dshallow,
    (c) W Dnoattention, (d) Our method, (e) Teacher net, (f) Ground truth, (g) Image.
    One can see a strong discriminator can help the student learn structure objects
    better. With the attention layers, labels of the objects are more consistent.
  Figure 7 Link: articels_figures_by_rev_year\2020\Structured_Knowledge_Distillation_for_Dense_Prediction\figure_7.jpg
  Figure 7 caption: Illustrations of the effectiveness of structured distillation
    schemes in terms of class IoU scores using the network MobileNetV2Plus [16] on
    the Cityscapes test set. Both pixel-level and structured distillation are helpful
    for improving the performance especially for the hard classes with low IoU scores.
    The improvement from structured distillation is more significant for structured
    objects, such as bus and truck.
  Figure 8 Link: articels_figures_by_rev_year\2020\Structured_Knowledge_Distillation_for_Dense_Prediction\figure_8.jpg
  Figure 8 caption: 'Qualitative results on the Cityscapes testing set produced from
    MobileNetV2Plus: (a) initial images, (b) wo distillation, (c) only w pixel-wise
    distillation, (d) Our distillation schemes: both pixel-wise and structured distillation
    schemes. The segmentation map in the red box about four structured objects: trunk,
    person, bus and traffic sign are zoomed in. One can see that the structured distillation
    method (ours) produces more consistent labels.'
  Figure 9 Link: articels_figures_by_rev_year\2020\Structured_Knowledge_Distillation_for_Dense_Prediction\figure_9.jpg
  Figure 9 caption: Qualitative results on the CamVid test set produced from ESPNet.
    Wo dis. represents for the baseline student network trained without distillation.
  First author gender probability: 0.8
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Yifan Liu
  Name of the last author: Jingdong Wang
  Number of Figures: 12
  Number of Tables: 14
  Number of authors: 3
  Paper title: Structured Knowledge Distillation for Dense Prediction
  Publication Date: 2020-06-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Effect of Different Components of the Loss in the Proposed
      Method
  Table 10 caption:
    table_text: TABLE 10 Depth Estimation
  Table 2 caption:
    table_text: TABLE 2 The Impact of the Connection Range and Node Granularity
  Table 3 caption:
    table_text: TABLE 3 We Choose ResNet18 (1.0) as the Example for the Student Net
  Table 4 caption:
    table_text: TABLE 4 We Choose ResNet18 (1.0) as the Example for the Student Net
  Table 5 caption:
    table_text: TABLE 5 We Choose ResNet18 (1.0) as the Example for the Student Net
  Table 6 caption:
    table_text: TABLE 6 Comparison of Feature Transfer MIMIC [11], [19], Attention
      Transfer [46], and Local Pair-Wise Distillation [12] Against Our Pair-Wise Distillation
  Table 7 caption:
    table_text: TABLE 7 The Segmentation Results on the Testing, Validation (Val.)
      Set of Cityscapes
  Table 8 caption:
    table_text: TABLE 8 The Segmentation Performance on the Test Set of CamVid
  Table 9 caption:
    table_text: TABLE 9 The mIoU and Pixel Accuracy on Validation Set of ADE20K
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3001940
- Affiliation of the first author: school of automation, northwestern polytechnical
    university, xian, china
  Affiliation of the last author: school of automation, northwestern polytechnical
    university, xian, china
  Figure 1 Link: articels_figures_by_rev_year\2020\Evaluation_of_Saccadic_Scanpath_Prediction_Subjective_Assessment_Database_and_Re\figure_1.jpg
  Figure 1 caption: Comparison of different research directions on visual attention
    modeling. Based on eye-tracking raw fixations, Gaussian smoothing is applied to
    generate the ground-truth for saliency estimation. In contrast, the scanpath of
    each subject is extracted for the evaluation of saccadic models.
  Figure 10 Link: articels_figures_by_rev_year\2020\Evaluation_of_Saccadic_Scanpath_Prediction_Subjective_Assessment_Database_and_Re\figure_10.jpg
  Figure 10 caption: Architecture of the deep autoencoder for extracting the encoding
    of semantic hashing.
  Figure 2 Link: articels_figures_by_rev_year\2020\Evaluation_of_Saccadic_Scanpath_Prediction_Subjective_Assessment_Database_and_Re\figure_2.jpg
  Figure 2 caption: "Different rankings under distinct metrics. The symbols \u201C\
    \ > ,\u201D \u201C < ,\u201D and \u201C = \u201D are used to show the rankings\
    \ of two predicted scanpaths according to the reference scanpath. In both groups\
    \ of evaluation, the proposed metric gives a better score to the predicted scanpath\
    \ 1, which is consistent with human subjective assessment."
  Figure 3 Link: articels_figures_by_rev_year\2020\Evaluation_of_Saccadic_Scanpath_Prediction_Subjective_Assessment_Database_and_Re\figure_3.jpg
  Figure 3 caption: Graphical user interface (GUI) used for obtaining subjective assessment.
    Each subject was required to determine which predicted scanpath shows a better
    prediction by referring to the human scanpath. The number is the index of the
    fixation. The reference scanpath was also presented on the images of predicted
    scanpaths to facilitate comparison.
  Figure 4 Link: articels_figures_by_rev_year\2020\Evaluation_of_Saccadic_Scanpath_Prediction_Subjective_Assessment_Database_and_Re\figure_4.jpg
  Figure 4 caption: Calculation of TDE. First, the predicted scanpath and human scanpaths
    are divided into saccadic pieces with k fixations (e.g., k=2 in the figure). Then,
    for each piece on the predicted scanpath, its minimal distance to the set of divided
    human scanpaths is computed. Finally, based on the minimal distance, the total
    similarity between the predicted scanpath and human fixation data can be estimated.
  Figure 5 Link: articels_figures_by_rev_year\2020\Evaluation_of_Saccadic_Scanpath_Prediction_Subjective_Assessment_Database_and_Re\figure_5.jpg
  Figure 5 caption: Calculation of SS. First, the mean-shift is utilized to cluster
    all human fixations. Then, each scanpath is encoded into a string based on the
    clusters. Finally, the Needleman-Wunsch string matching scores are computed to
    measure the average similarity between the predicted scanpath and human scanpaths.
  Figure 6 Link: articels_figures_by_rev_year\2020\Evaluation_of_Saccadic_Scanpath_Prediction_Subjective_Assessment_Database_and_Re\figure_6.jpg
  Figure 6 caption: Predicted scanpaths of distinct saccadic models for subjective
    assessment.
  Figure 7 Link: articels_figures_by_rev_year\2020\Evaluation_of_Saccadic_Scanpath_Prediction_Subjective_Assessment_Database_and_Re\figure_7.jpg
  Figure 7 caption: 'Consistent and inconsistent assessments for the pairs of scanpaths.
    Left: consistent subjective rankings among all subjects. Right: inconsistent judgments
    among subjects.'
  Figure 8 Link: articels_figures_by_rev_year\2020\Evaluation_of_Saccadic_Scanpath_Prediction_Subjective_Assessment_Database_and_Re\figure_8.jpg
  Figure 8 caption: The histogram of human annotations over all of the questions.
  Figure 9 Link: articels_figures_by_rev_year\2020\Evaluation_of_Saccadic_Scanpath_Prediction_Subjective_Assessment_Database_and_Re\figure_9.jpg
  Figure 9 caption: "Diagram of the proposed data-driven metric. The metric takes\
    \ a pair of predicted scanpath (\u201CPredicted Scanpath 1\u201D and \u201CPredicted\
    \ Scanpath 2\u201D) and a reference scanpath as input. First, three scanpaths\
    \ are encoded by extracting the features of fixations on scanpaths. Then, the\
    \ gap in the distance to the reference scanpath of the two predicted scanpaths\
    \ is computed and fed into the LSTM for training. Finally, the LSTM outputs a\
    \ binary judgment of the predicted scanpath that is more similar to the reference\
    \ scanpath (\u201C1\u201D: the predicted scanpath 1 is better; \u201C2\u201D:\
    \ the predicted scanpath 2 is better)."
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.67
  Name of the first author: Chen Xia
  Name of the last author: Dingwen Zhang
  Number of Figures: 16
  Number of Tables: 6
  Number of authors: 3
  Paper title: 'Evaluation of Saccadic Scanpath Prediction: Subjective Assessment
    Database and Recurrent Neural Network Based Metric'
  Publication Date: 2020-06-15 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Existing Evaluation Metrics for Saccade Prediction
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Characteristics of Eye-Tracking Data Sets
  Table 3 caption:
    table_text: TABLE 3 Performance and Ranking of 15 Saccadic Models Under Distinct
      Categories Of Metrics
  Table 4 caption:
    table_text: TABLE 4 Comparing the Performance of Prediction Models and Inter-Observer
      Model on Distinct Databases
  Table 5 caption:
    table_text: TABLE 5 Accuracy of Correct Judgment on the Ranking of Predicted Scanpaths
      Based on Reference Scanpath (A1) and Accuracy of Correctly Discriminating a
      Human Scanpath from Generated Scanpaths (A2)
  Table 6 caption:
    table_text: TABLE 6 Ablation Results and Effect of Different Components of the
      Metric
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.3002168
