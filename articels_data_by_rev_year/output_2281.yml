- Affiliation of the first author: institute of high performance computing (ihpc),
    astar, singapore, singapore
  Affiliation of the last author: shanghai engineering research center of intelligent
    vision and imaging, and shanghai engineering research center of energy efficient
    and custom ai ic, shanghaitech university, shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Spherical_DNNs_and_Their_Applications_in____Images_and_Videos\figure_1.jpg
  Figure 1 caption: "Distortion introduced by an equirectangular projection. Left:\
    \ a 360 \u2218 image on a sphere; Right: the corresponding 360 \u2218 image of\
    \ the left image on an equirectangular panorama."
  Figure 10 Link: articels_figures_by_rev_year\2021\Spherical_DNNs_and_Their_Applications_in____Images_and_Videos\figure_10.jpg
  Figure 10 caption: The performance comparison between our spherical U-Net and the
    state-of-the-art methods.
  Figure 2 Link: articels_figures_by_rev_year\2021\Spherical_DNNs_and_Their_Applications_in____Images_and_Videos\figure_2.jpg
  Figure 2 caption: An illustration of the existing work for sphere data.
  Figure 3 Link: articels_figures_by_rev_year\2021\Spherical_DNNs_and_Their_Applications_in____Images_and_Videos\figure_3.jpg
  Figure 3 caption: "Parameter sharing. This figure indicates how a spherical crown\
    \ kernel changes on a sphere and its projected panorama from north pole to south\
    \ pole with an angle interval equalling \u03C04 . The first row is the region\
    \ of the spherical crown kernel in sphere. The second row shows the region of\
    \ the spherical crown kernel in the projected panorama. The third row shows sampling\
    \ grid corresponding to each kernel location. The red curve represents the sampling\
    \ grid along the \u03B8 direction and the blue curve represents the sampling grid\
    \ along the \u03D5 direction."
  Figure 4 Link: articels_figures_by_rev_year\2021\Spherical_DNNs_and_Their_Applications_in____Images_and_Videos\figure_4.jpg
  Figure 4 caption: 'Network architecture: (a) Spherical U-Net; (b) Spherical ConvLSTM.'
  Figure 5 Link: articels_figures_by_rev_year\2021\Spherical_DNNs_and_Their_Applications_in____Images_and_Videos\figure_5.jpg
  Figure 5 caption: "The network architecture of our method for the gaze prediction\
    \ in 360 \u2218 videos."
  Figure 6 Link: articels_figures_by_rev_year\2021\Spherical_DNNs_and_Their_Applications_in____Images_and_Videos\figure_6.jpg
  Figure 6 caption: From left to right are the whole panorama, the FoV region and
    the small image patch around the attention point.
  Figure 7 Link: articels_figures_by_rev_year\2021\Spherical_DNNs_and_Their_Applications_in____Images_and_Videos\figure_7.jpg
  Figure 7 caption: 'Dataset Analysis: (a) the examples of five sports in our 360
    circ video dataset. (b) the distribution of the five sports based on the number
    of videos; (c) the distribution of eye fixations on equirectangular panorama.
    (Best viewed in color).'
  Figure 8 Link: articels_figures_by_rev_year\2021\Spherical_DNNs_and_Their_Applications_in____Images_and_Videos\figure_8.jpg
  Figure 8 caption: Grad-CAM visualizations for the classification decision. The first
    three rows show the visualizations for three sport panoramic frames. The last
    three rows show the visualizations for a sequence of panoramic images sampled
    from a skateboarding video.
  Figure 9 Link: articels_figures_by_rev_year\2021\Spherical_DNNs_and_Their_Applications_in____Images_and_Videos\figure_9.jpg
  Figure 9 caption: 'The rotation invariance along the theta and phi direction: The
    first and third columns are rotated frames and the second and forth columns are
    our predictions.'
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Yanyu Xu
  Name of the last author: Shenghua Gao
  Number of Figures: 11
  Number of Tables: 7
  Number of authors: 3
  Paper title: "Spherical DNNs and Their Applications in 360 \u2218 \u2218 Images\
    \ and Videos"
  Publication Date: 2021-07-27 00:00:00
  Table 1 caption: TABLE 1 The Architecture of the Spherical U-Net
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 The Classification Performance Comparison on the Spherical\
    \ MNIST [10] and Our ShanghaiTech 360 \u2218 \u2218 Video Dataset"
  Table 3 caption: TABLE 3 The Performance Comparison Between Our Spherical U-Net
    and Spherical LSTM U-Net and the State-of-the-Art Methods on Our Video Saliency
    Dataset
  Table 4 caption: TABLE 4 The Performance Comparison Between Our Spherical U-Net
    and the State-of-the-Art Methods on the Salient360! [62] Dataset
  Table 5 caption: 'TABLE 5 Ablation Study: The Performance Comparison Among Different
    Configurations of Our Method on Our Video Saliency Dataset'
  Table 6 caption: "TABLE 6 Rotation Invariance: The Performance Comparison on Testing\
    \ Set With Different the \u0398 \u0398 and \u03A6 \u03A6"
  Table 7 caption: TABLE 7 The Performance Comparison on the Trade-Off Between Time
    & Memory Costs and Performance
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3100259
- Affiliation of the first author: department of mathematics, nanjing university of
    science and technology, nanjing, china
  Affiliation of the last author: department of mathematics, nanjing university, nanjing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2021\AbdomenCTK_Is_Abdominal_Organ_Segmentation_a_Solved_Problem\figure_1.jpg
  Figure 1 caption: Examples of abdominal organs in CT scans, including multi-center,
    multi-phase, multi-vendor, and multi-disease cases.
  Figure 10 Link: articels_figures_by_rev_year\2021\AbdomenCTK_Is_Abdominal_Organ_Segmentation_a_Solved_Problem\figure_10.jpg
  Figure 10 caption: Challenging examples from testing sets in continual learning
    multi-organ segmentation benchmark.
  Figure 2 Link: articels_figures_by_rev_year\2021\AbdomenCTK_Is_Abdominal_Organ_Segmentation_a_Solved_Problem\figure_2.jpg
  Figure 2 caption: Task overview and the associated features.
  Figure 3 Link: articels_figures_by_rev_year\2021\AbdomenCTK_Is_Abdominal_Organ_Segmentation_a_Solved_Problem\figure_3.jpg
  Figure 3 caption: Overview of the existing abdominal CT datasets and our augmented
    (plus) abdominal datasets. Red, green, blue, and yellow regions denote liver,
    kidney, spleen, and pancreas, respectively.
  Figure 4 Link: articels_figures_by_rev_year\2021\AbdomenCTK_Is_Abdominal_Organ_Segmentation_a_Solved_Problem\figure_4.jpg
  Figure 4 caption: Organ volume and contrast phase distributions in AbdomenCT-1K.
  Figure 5 Link: articels_figures_by_rev_year\2021\AbdomenCTK_Is_Abdominal_Organ_Segmentation_a_Solved_Problem\figure_5.jpg
  Figure 5 caption: Comparison of Dice similarity coefficient (DSC) and normalized
    surface Dice (NSD).
  Figure 6 Link: articels_figures_by_rev_year\2021\AbdomenCTK_Is_Abdominal_Organ_Segmentation_a_Solved_Problem\figure_6.jpg
  Figure 6 caption: Violin plots of the segmentation performances (DSC and NSD) of
    different organs in single organ segmentation tasks.
  Figure 7 Link: articels_figures_by_rev_year\2021\AbdomenCTK_Is_Abdominal_Organ_Segmentation_a_Solved_Problem\figure_7.jpg
  Figure 7 caption: Well-segmented and challenging examples from testing sets in the
    large-scale fully supervised multi-organ segmentation study.
  Figure 8 Link: articels_figures_by_rev_year\2021\AbdomenCTK_Is_Abdominal_Organ_Segmentation_a_Solved_Problem\figure_8.jpg
  Figure 8 caption: Challenging examples from testing sets in fully supervised segmentation
    benchmark.
  Figure 9 Link: articels_figures_by_rev_year\2021\AbdomenCTK_Is_Abdominal_Organ_Segmentation_a_Solved_Problem\figure_9.jpg
  Figure 9 caption: Challenging examples from testing sets in semi-supervised segmentation
    benchmark.
  First author gender probability: 0.97
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.66
  Name of the first author: Jun Ma
  Name of the last author: Xiaoping Yang
  Number of Figures: 10
  Number of Tables: 12
  Number of authors: 17
  Paper title: 'AbdomenCT-1K: Is Abdominal Organ Segmentation a Solved Problem?'
  Publication Date: 2021-07-27 00:00:00
  Table 1 caption: TABLE 1 Overview of the Popular Abdominal CT Benchmark Datasets
  Table 10 caption: TABLE 10 Task Settings and Quantitative Baseline Results of Continual
    Learning
  Table 2 caption: TABLE 2 Quantitative Analysis of Inter-Rater Variability Between
    Two Radiologists
  Table 3 caption: TABLE 3 Quantitative Results of Single Organ Segmentation
  Table 4 caption: TABLE 4 Quantitative Results of Fully Supervised Multi-Organ Segmentation
    in Terms of Average DSC and NSD
  Table 5 caption: TABLE 5 Task Settings and Quantitative Baseline Results of Fully
    Supervised Multi-Organ Segmentation Benchmark
  Table 6 caption: TABLE 6 Task Settings of Semi-Supervised Multi-Organ Segmentation
  Table 7 caption: TABLE 7 Quantitative Multi-Organ Segmentation Results in Semi-Supervised
    Benchmark
  Table 8 caption: TABLE 8 Task Settings and Quantitative Baseline Results of Weakly
    Supervised Abdominal Organ Segmentation
  Table 9 caption: TABLE 9 Quantitative Multi-Organ Segmentation Results in Weakly
    Supervised Benchmark
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3100536
- Affiliation of the first author: national laboratory of pattern recognition, institute
    of automation, chinese academy of sciences, beijing, china
  Affiliation of the last author: department of computer science and information systems,
    birkbeck college, university of london, london, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2021\InteractionAware_SpatioTemporal_Pyramid_Attention_Networks_for_Action_Classifica\figure_1.jpg
  Figure 1 caption: 'Our interaction-aware spatial pyramid attention layer: The feature
    maps of different sizes in different layers are used to construct a multi-scale
    attention layer to obtain more accurate spatial attention.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\InteractionAware_SpatioTemporal_Pyramid_Attention_Networks_for_Action_Classifica\figure_2.jpg
  Figure 2 caption: "The process of aggregating K groups of feature maps I i \u2208\
    \ R K\xD7 W i \xD7 H i \xD7 C i into one group of feature maps M \u2032 i \u2208\
    \ R W i \xD7 H i \xD7 C i ."
  Figure 3 Link: articels_figures_by_rev_year\2021\InteractionAware_SpatioTemporal_Pyramid_Attention_Networks_for_Action_Classifica\figure_3.jpg
  Figure 3 caption: Our interaction-aware spatio-temporal attention-based framework
    for one stream for action recognition.
  Figure 4 Link: articels_figures_by_rev_year\2021\InteractionAware_SpatioTemporal_Pyramid_Attention_Networks_for_Action_Classifica\figure_4.jpg
  Figure 4 caption: 'Comparisons of the results between different numbers of sampled
    frames per video for training (A) and testing (B) respectively on the UCF101 split1
    dataset: (A) K = 1, 3 frames for training and (B) K = 1, 5, 10, 15, 20, 25, 30
    frames for testing.'
  Figure 5 Link: articels_figures_by_rev_year\2021\InteractionAware_SpatioTemporal_Pyramid_Attention_Networks_for_Action_Classifica\figure_5.jpg
  Figure 5 caption: 'Difference of confusion matrices for the RGB stream on the UCF101
    split1 dataset: (A) between 1 scale and no attention layer; (B) between 2 scales
    and 1 scale; (C) between 3 scales and 2 scales; (D) between 3 scales and no attention
    layer: The larger negative changes on the diagonal and the larger positive changes
    on the off-diagonal denote the higher improvement; It is expected that the color
    denoting negative changes is on the diagonal and the color denoting positive changes
    is on the off-diagonal.'
  Figure 6 Link: articels_figures_by_rev_year\2021\InteractionAware_SpatioTemporal_Pyramid_Attention_Networks_for_Action_Classifica\figure_6.jpg
  Figure 6 caption: 'Visualization of salient receptive fields in different frames
    from the appearance (RGB) and motion (Flow) streams: Each row shows 5 frames from
    videos. Blue, green, and red regions correspond to the centers of salient receptive
    fields obtained by using 1 scale, 2 scales, and 3 scales respectively; a), b)
    and c) and g), h) and i) show the results of action ApplyEyeMakeup from the RGB
    and Flow streams respectively; d), e) and f) and j), k) and l) show the results
    of action iceDancing from the RGB and Flow streams respectively.'
  Figure 7 Link: articels_figures_by_rev_year\2021\InteractionAware_SpatioTemporal_Pyramid_Attention_Networks_for_Action_Classifica\figure_7.jpg
  Figure 7 caption: 'Visualization of salient receptive fields for different positions
    in the attentional feature maps from the appearance (RGB) stream: a) PlayingGuitar,
    b) PlayingFlute, c) ParallelBars, d) Skijet. Three scales of attention were used;
    Each image shows the results from different positions (wm, hm).'
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Weiming Hu
  Name of the last author: Stephen Maybank
  Number of Figures: 7
  Number of Tables: 15
  Number of authors: 6
  Paper title: Interaction-Aware Spatio-Temporal Pyramid Attention Networks for Action
    Classification
  Publication Date: 2021-07-27 00:00:00
  Table 1 caption: 'TABLE 1 Evaluation of Positions of the Interaction-Aware Attention
    Layer on the UCF101 Split 1 Dataset: K = 3 for Training and K = 25 for Testing'
  Table 10 caption: 'TABLE 10 The Numbers (in millions) of the Parameters in the Networks
    With or Without the Proposed Interaction-Aware Attention Layer: 1 Scale, 2 Scales,
    and 3 Scales in a Spatio-Temporal Pyramid Were Considered'
  Table 2 caption: 'TABLE 2 Evaluation of Different Numbers of Scales With Inception-ResNet-V2
    on the UCF101 Split 1 Dataset: K = 3 for Training and K = 25 for Testing'
  Table 3 caption: 'TABLE 3 Performance of Different Fusion Functions With 3 Scales
    on the UCF101 Split 1 Dataset: K = 3 for Training and K = 25 for Testing'
  Table 4 caption: 'TABLE 4 Evaluation of the Components of the Attention Model With
    Inception-ResNet-V2 and the Attention Layer: K = 3 for Training and K = 25 for
    Testing on the UCF101 Split 1 Dataset'
  Table 5 caption: TABLE 5 Comparison With Using 1x1 Convolutions Shared Across All
    the Spatial Locations on the UCF101 Split 1 Dataset
  Table 6 caption: TABLE 6 Recognition Rate (top-1) of the Interaction-Aware Pyramid
    Attention Layer and the Baseline Network on the CIFAR 100+ Image Dataset
  Table 7 caption: TABLE 7 The Five Action Classes for Which the Attention Mechanism
    Increases the Classification Accuracy By the Largest Amount on the RGB Branch
  Table 8 caption: TABLE 8 The Five Action Classes for Which the Attention Mechanism
    Reduces the Classification Accuracy By the Largest Amount on the RGB Branch
  Table 9 caption: 'TABLE 9 Performance of the Proposed Attention Layer on Popular
    Networks, VGGNet-16, BN-Inception, and Inception-ResNet-V2 on the UCF101 Split
    1 Dataset: K = 3 for Training and K = 25 for Testing; Both L Linteractive and
    L Lattn Were Used'
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3100277
- Affiliation of the first author: school of remote sensing and information engineering,
    wuhan university, wuhan, china
  Affiliation of the last author: school of remote sensing and information engineering,
    wuhan university, wuhan, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Multiple_Video_Frame_Interpolation_via_Enhanced_Deformable_Separable_Convolution\figure_1.jpg
  Figure 1 caption: Illustration of the sampling locations (pink point) in a 7times
    7 checkerboard, in which the black square represents a convolution patch. In the
    center of the blue rectangle lattice, the sampling location is integer. (a) Baseline
    kernel-based methods with a 3times 3 convolution patch. (b) Our method with a
    3times 3 convolution patch. (c) Conventional flow-based methods. (d) Adaptive
    warping based methods with a 2times 2 convolution patch.
  Figure 10 Link: articels_figures_by_rev_year\2021\Multiple_Video_Frame_Interpolation_via_Enhanced_Deformable_Separable_Convolution\figure_10.jpg
  Figure 10 caption: PSNR at each time step when generating times 6 slow motion frames
    on the Vimeo90K-Septuplet test set [22].
  Figure 2 Link: articels_figures_by_rev_year\2021\Multiple_Video_Frame_Interpolation_via_Enhanced_Deformable_Separable_Convolution\figure_2.jpg
  Figure 2 caption: Modeling arbitrary time interpolation from networks trained for
    t=0.5 . Despite correct pixels are chosen for synthesis, the occlusion is mistakenly
    solved, making the balls incomplete.
  Figure 3 Link: articels_figures_by_rev_year\2021\Multiple_Video_Frame_Interpolation_via_Enhanced_Deformable_Separable_Convolution\figure_3.jpg
  Figure 3 caption: 'Illustration of the architecture of our proposed EDSC network,
    which includes five major components: an encoder-decoder architecture and a set
    of kernel, mask, offset, and bias estimators.'
  Figure 4 Link: articels_figures_by_rev_year\2021\Multiple_Video_Frame_Interpolation_via_Enhanced_Deformable_Separable_Convolution\figure_4.jpg
  Figure 4 caption: 'Architecture of the sub-network of kernel estimator: (a) for
    single in-between frame generation and (b) multiple frame generation.'
  Figure 5 Link: articels_figures_by_rev_year\2021\Multiple_Video_Frame_Interpolation_via_Enhanced_Deformable_Separable_Convolution\figure_5.jpg
  Figure 5 caption: Interpolation error comparisons among kernel-based methods on
    the Middlebury Evaluation set [12]. Lower bars represent better performances.
  Figure 6 Link: articels_figures_by_rev_year\2021\Multiple_Video_Frame_Interpolation_via_Enhanced_Deformable_Separable_Convolution\figure_6.jpg
  Figure 6 caption: Qualitative evaluation on a video with significant object motion.
  Figure 7 Link: articels_figures_by_rev_year\2021\Multiple_Video_Frame_Interpolation_via_Enhanced_Deformable_Separable_Convolution\figure_7.jpg
  Figure 7 caption: Qualitative evaluation with respect to large motion (blue rectangle)
    and occlusion (yellow rectangle).
  Figure 8 Link: articels_figures_by_rev_year\2021\Multiple_Video_Frame_Interpolation_via_Enhanced_Deformable_Separable_Convolution\figure_8.jpg
  Figure 8 caption: Qualitative evaluation on a video with explicit camera motion.
  Figure 9 Link: articels_figures_by_rev_year\2021\Multiple_Video_Frame_Interpolation_via_Enhanced_Deformable_Separable_Convolution\figure_9.jpg
  Figure 9 caption: Qualitative evaluation on a video with discontinuous motion.
  First author gender probability: 0.75
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.95
  Name of the first author: Xianhang Cheng
  Name of the last author: Zhenzhong Chen
  Number of Figures: 17
  Number of Tables: 13
  Number of authors: 2
  Paper title: Multiple Video Frame Interpolation via Enhanced Deformable Separable
    Convolution
  Publication Date: 2021-07-29 00:00:00
  Table 1 caption: TABLE 1 A List of Notations Mainly Used in This Paper
  Table 10 caption: "TABLE 10 Runtime Comparisons in Seconds With Existing Methods\
    \ on a 640\xD7480 640\xD7480 Sequence"
  Table 2 caption: TABLE 2 A List of Conditions in Which Our Method can be Equivalent
    or Similar to the Other Kinds of Algorithms in Terms of Pixel Reference
  Table 3 caption: TABLE 3 Network Setting Comparison and Analysis on Different Frame
    Interpolation Algorithms
  Table 4 caption: TABLE 4 Quantitative Comparisons Against Methods Using Adaptive
    Convolutional Kernels
  Table 5 caption: TABLE 5 Quantitative Comparisons Against Methods Without Using
    Adaptive Convolutional Kernels
  Table 6 caption: TABLE 6 Quantitative Comparisons Against Kernel-Based Methods on
    SNU-FILM [42] Dataset (abbreviated by S.F.)
  Table 7 caption: TABLE 7 Quantitative Comparisons Against Methods Using Warping
    Operation Guided by Given Optical Flow
  Table 8 caption: TABLE 8 Quantitative Comparisons Against the Effect of Interpolation
    Coefficient
  Table 9 caption: TABLE 9 Runtime of the Proposed Method (seconds)
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3100714
- Affiliation of the first author: school of computer science and technology, university
    of chinese academy of sciences, beijing, china
  Affiliation of the last author: key laboratory of big data mining and knowledge
    management (bdkm), school of computer science and technology, university of chinese
    academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Learning_With_Multiclass_AUC_Theory_and_Algorithms\figure_1.jpg
  Figure 1 caption: Coarse-grained Performance Comparison. We plot the overall mathsf
    MAUCuparrow over the 15 splits against different algorithms. Here each bar presents
    the performance of a specific algorithm. The bar itself captures the mean performance,
    and the scatters distributed along a bar are the results over 15 splits.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Learning_With_Multiclass_AUC_Theory_and_Algorithms\figure_2.jpg
  Figure 2 caption: 'Fine-grained Comparison Over the Minority Class Pairs (Traditional
    Datasets). The x -axis gives the frequency rank of the class pairs (i,j) , i.e.,
    p i p j , where a larger rank represents a larger frequency. The y -axis represents
    the AUC i|j of the corresponding class pairs. Each line in a plot then captures
    the minority class pair performance of a given algorithm. To have a clearer look
    at tendency, we carry out two filtering processes before we visualize the plot:
    (a) For those datasets which has more than 5 class pairs, we only visualize the
    bottom-5 pairs in terms of the frequency to turn our focus to the minority pairs
    in the dataset. (b) To have a clearer look at the difference of the top competitors,
    we filter out the pairs with a smaller AUC i|j than 0.7.'
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.89
  Name of the first author: Zhiyong Yang
  Name of the last author: Qingming Huang
  Number of Figures: 2
  Number of Tables: 5
  Number of authors: 5
  Paper title: 'Learning With Multiclass AUC: Theory and Algorithms'
  Publication Date: 2021-07-30 00:00:00
  Table 1 caption: "TABLE 1 Acceleration for Three Losses, Where N \xAF = \u2211 N\
    \ C i=1 n i log n i +(N\u2212 n i )log(N\u2212 n i ) N\xAF=\u2211i=1NCnilogni+(N-ni)log(N-ni)"
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 Performance Comparison Based on MAUC \u2191 MAUC\u2191\
    \ With Deep Learning"
  Table 3 caption: TABLE 3 Fine-Grained Comparison Over the Minority Class Pairs (Deep
    Learning Datasets)
  Table 4 caption: TABLE 4 Peformance Comparison on iNaturalist 2017 Dataset
  Table 5 caption: TABLE 5 Running Time Comparison
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3101125
- Affiliation of the first author: institute of science and engineering, kanazawa
    university, kanazawa, japan
  Affiliation of the last author: institute of science and engineering, kanazawa university,
    kanazawa, japan
  Figure 1 Link: articels_figures_by_rev_year\2021\Erratum_to_A_Bayesian_Formulation_of_Coherent_Point_Drift\figure_1.jpg
  Figure 1 caption: Comparisons of rigid registrations using the ASL data (left) and
    the UWA data (right). The y-axis represents the rate of correct registrations
    using 40 pairs of point sets. A trial is defined as being a success if the angular
    difference from the ground truth is less than a value specified by the x-axis.
    For the UWA dataset, the ground truth was defined based on the results of Go-ICP,
    owing to its almost perfect performance finding partial overlaps in the dataset.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Osamu Hirose
  Name of the last author: Osamu Hirose
  Number of Figures: 1
  Number of Tables: 0
  Number of authors: 1
  Paper title: "Erratum to \u201CA Bayesian Formulation of Coherent Point Drift\u201D"
  Publication Date: 2021-08-04 00:00:00
  Table 1 caption: Not Available
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3092384
- Affiliation of the first author: "instituto nacional de astrof\xEDsica, \xF3ptica\
    \ y eectr\xF3nica, puebla, mexico"
  Affiliation of the last author: google research, brain team, zurich, switzerland
  Figure 1 Link: Not Available
  Figure 1 caption: Not Available
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Hugo Jair Escalante
  Name of the last author: Neil Houlsby
  Number of Figures: 0
  Number of Tables: 1
  Number of authors: 7
  Paper title: 'Guest Editorial: Automated Machine Learning'
  Publication Date: 2021-08-04 00:00:00
  Table 1 caption: TABLE 1 Overview of Articles Included in the AutoML Special Section
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3077106
- Affiliation of the first author: nanjing university, nanjing, jiangsu, china
  Affiliation of the last author: university of kentucky, lexington, ky, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\Detailed_Avatar_Recovery_From_Single_Image\figure_1.jpg
  Figure 1 caption: Our method takes a single image of a person in the wild as input
    and predicts detailed human body shape with texture, namely human avatar. Our
    method recovers body shapes with surface details that fit the input image well,
    and also hallucinates the complete texture from the visible region.
  Figure 10 Link: articels_figures_by_rev_year\2021\Detailed_Avatar_Recovery_From_Single_Image\figure_10.jpg
  Figure 10 caption: Qualitative ablation comparison of texture synthesis.
  Figure 2 Link: articels_figures_by_rev_year\2021\Detailed_Avatar_Recovery_From_Single_Image\figure_2.jpg
  Figure 2 caption: 'The flow of our shape-recovering method goes from the bottom
    left to the top right. The mesh deformation architecture consists of three levels:
    joint, anchor and per-vertex. In each level, the 3D mesh is projected to 2D space
    and sent together with the source image to the prediction network. The 3D mesh
    gets deformed by the predicted results to produce refined human body shapes.'
  Figure 3 Link: articels_figures_by_rev_year\2021\Detailed_Avatar_Recovery_From_Single_Image\figure_3.jpg
  Figure 3 caption: The handles definition in different levels for mesh deformation.
  Figure 4 Link: articels_figures_by_rev_year\2021\Detailed_Avatar_Recovery_From_Single_Image\figure_4.jpg
  Figure 4 caption: The pipeline for texture completion.
  Figure 5 Link: articels_figures_by_rev_year\2021\Detailed_Avatar_Recovery_From_Single_Image\figure_5.jpg
  Figure 5 caption: 'We compare our method on 3D human model reconstruction with previous
    approaches. The results of our method in different stages are shown: joint deformed,
    anchor deformed and vertex deformed (final result). Comparing to other methods,
    our method recovers more accurate joints and the body with shape details. The
    human body shape fits better to the input image, especially in body limbs. The
    rightmost column shows we can get more accurate recovered shapes when ground truth
    human silhouette is enforced (labeled as HMD (+Sil.)). Note that the images are
    cropped for the compact layout.'
  Figure 6 Link: articels_figures_by_rev_year\2021\Detailed_Avatar_Recovery_From_Single_Image\figure_6.jpg
  Figure 6 caption: We compare our method with DeepHuman [73], PIFu [53], and Tex2Shape
    [5]. These three methods are all trained using ground-truth 3D human shapes. It
    is worth noting that the input image to PIFu has been segmented using the ground-truth
    silhouette, while the inputs of the other methods are original images.
  Figure 7 Link: articels_figures_by_rev_year\2021\Detailed_Avatar_Recovery_From_Single_Image\figure_7.jpg
  Figure 7 caption: The results selected according to the percentage of the silhouette
    IoU.
  Figure 8 Link: articels_figures_by_rev_year\2021\Detailed_Avatar_Recovery_From_Single_Image\figure_8.jpg
  Figure 8 caption: "We show some recovered meshes and the ground truth meshes on\
    \ the RECON (left) and SYN dataset (right). The meshes are rendered in the side\
    \ view by rotating the mesh 90 \u2218 around the vertical axis."
  Figure 9 Link: articels_figures_by_rev_year\2021\Detailed_Avatar_Recovery_From_Single_Image\figure_9.jpg
  Figure 9 caption: We show some texture synthesis results. For each group, we show
    the source image and render the recovered mesh with predicted texture in the front
    view and back view. Though there is slight color distortion comparing to the source
    image, our method is able to predict plausible texture, and can even hallucinate
    the completely invisible face from the back.
  First author gender probability: 0.9
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Hao Zhu
  Name of the last author: Ruigang Yang
  Number of Figures: 13
  Number of Tables: 7
  Number of authors: 6
  Paper title: Detailed Avatar Recovery From Single Image
  Publication Date: 2021-08-04 00:00:00
  Table 1 caption: TABLE 1 Quantitative Evaluation
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 WILD Dataset Components
  Table 3 caption: TABLE 3 Ablation Experiments
  Table 4 caption: TABLE 4 Ablation Study of Texture Synthesis
  Table 5 caption: TABLE 5 Ablation Study of Photometric Loss
  Table 6 caption: TABLE 6 Joint Error Evaluation on H36M Dataset
  Table 7 caption: TABLE 7 Quantitative Comparison With 3D Supervised Methods
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3102128
- Affiliation of the first author: school of computer science and technology and the
    moe key laboratory for advanced theory and application in statistics and data
    science, east china normal university, shanghai, china
  Affiliation of the last author: school of data science and airs, chinese university
    of hong kong, shenzhen, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Structured_Cooperative_Reinforcement_Learning_With_TimeVarying_Composite_Action_\figure_1.jpg
  Figure 1 caption: 'The conceptual diagram of time-varying composite action space
    and structured cooperative reinforcement learning algorithm (SCORE). Left: The
    single-agent problem with time-varying composite sub-action space. Sub-action
    spaces with different functions are generally heterogeneous, with different semantics
    and data structures. For example, the four functional sub-action spaces in the
    figure are one-dimensional continuous space, one-dimensional discrete space, two-dimensional
    continuous space, and a mixed space of one-dimensional continuous space and one-dimensional
    discrete space. At each time step, the agent needs to determine all sub-actions
    based on the policy jointly. Right: The homogeneous multi-agent problem with the
    variable number of agents after decomposition. Different sub-action spaces are
    considered independently and regarded as agents. All agents have the same local
    observations and shared team rewards. This paper proposes the SCORE algorithm
    based on graph attention network (GAT) and hierarchical variational autoencoder
    (HVAE) to solve this multi-agent problem. After decomposing the composite actions,
    HVAE maps the sub-action spaces with different functions to the common latent
    space into a more straightforward homogeneity problem. At the same time, GAT is
    used to deal with the variable number of agents and to capture the dependencies
    between sub-action spaces with different functions.'
  Figure 10 Link: articels_figures_by_rev_year\2021\Structured_Cooperative_Reinforcement_Learning_With_TimeVarying_Composite_Action_\figure_10.jpg
  Figure 10 caption: The pairwise relationships of sub-actions for carbon dioxide
    and temperature in tomato simulator. These 2 sub-action spaces are both 24-dimensional,
    representing the hourly temperature and carbon dioxide concentration in 24 hours
    of a simulated day. We select the temperature and carbon dioxide values at 0:00
    (upper left), 06:00 (upper right), 12:00 (lower left) and 18:00 (lower right)
    in a whole sub-action trajectory. This action trajectory is sampled in the test
    environment after the algorithm training converges in the fully sub-action space
    of the tomato simulator.
  Figure 2 Link: articels_figures_by_rev_year\2021\Structured_Cooperative_Reinforcement_Learning_With_TimeVarying_Composite_Action_\figure_2.jpg
  Figure 2 caption: 'We choose the tomato planting task to show the dependence between
    different sub-action spaces in the precision agriculture scene. Left: The tomato
    planting task contains four sub-action spaces with different functions, from top
    to bottom: temperature, light, irrigation, and carbon dioxide. Middle: As the
    carbon dioxide concentration and temperature increase, the activity of crop enzymes
    will increase, which will increase crop yields. If the temperature drops while
    the carbon dioxide concentration increases, it will reduce the enzyme activity
    and reduce the crop yield. Right: To increase crop yield, the correlation constraint
    between the carbon dioxide sub-action space and the illumination sub-action space
    should be positive.'
  Figure 3 Link: articels_figures_by_rev_year\2021\Structured_Cooperative_Reinforcement_Learning_With_TimeVarying_Composite_Action_\figure_3.jpg
  Figure 3 caption: The action space representation learning stage based on hierarchical
    variational autoencoder. For the sake of clarity, only the training process of
    the embedding of one sub-action space is shown here. For each sub-action space,
    SCORE will first fix the others sub-actions and then sample a batch of local observations
    from the environment by randomly sample a batch of current sub-action. Then, the
    local observation set processed by the state encoder is mean-pooled to obtain
    the sub-action space embedding distribution. The latter action encoder and action
    decoder constitute a standard conditional variational autoencoder. The input is
    the sub-action space embedding (sample from the embedding distribution), the local
    observation, and the sub-action at a certain timestep. The output is the reconstructed
    local observation. Besides enhancing the representation ability of the sub-action
    space embedding, we introduce a self-supervised auxiliary task to predict the
    local observation sequence in the future.
  Figure 4 Link: articels_figures_by_rev_year\2021\Structured_Cooperative_Reinforcement_Learning_With_TimeVarying_Composite_Action_\figure_4.jpg
  Figure 4 caption: The graph-attention-network-based centralized critic and decentralized
    actor architecture for robust and transferable policy learning. At each timestep,
    each sub-action output by different agents will be input into the action encoder
    together with each sub-action space embedding, and the projected sub-action representation
    is obtained. After passing the state-action pairing interaction function and state
    embedding, the obtained agent representation will pass through the graph attention
    network. By fully capturing the correlation between different sub-actions, the
    joint Q value function is finally obtained. The decentralized actor and centralized
    critic based on hypernetwork form an implicit credit allocation structure similar
    to Zhou et al. [57].
  Figure 5 Link: articels_figures_by_rev_year\2021\Structured_Cooperative_Reinforcement_Learning_With_TimeVarying_Composite_Action_\figure_5.jpg
  Figure 5 caption: How SCORE transfers to the unseen sub-action spaces. Since the
    centralized critic based on the graph attention network can handle any number
    of agents, we only need to represent the unseen sub-action spaces through the
    action space representation learning module and then fine-tune the centralized
    critic. Of course, we also need to train the actor-networks corresponding to the
    unseen sub-action spaces from scratch.
  Figure 6 Link: articels_figures_by_rev_year\2021\Structured_Cooperative_Reinforcement_Learning_With_TimeVarying_Composite_Action_\figure_6.jpg
  Figure 6 caption: Performance comparison of fully sub-action spaces and unseen sub-action
    spaces in Spider environment. The vertical axis represents the average episode
    return in 10 different test environments. The horizontal axis represents the total
    number of times the training phase interacts with the environment in millions
    of units. All algorithms are tested under three different random seeds, and the
    shaded area represents the standard deviation.
  Figure 7 Link: articels_figures_by_rev_year\2021\Structured_Cooperative_Reinforcement_Learning_With_TimeVarying_Composite_Action_\figure_7.jpg
  Figure 7 caption: 'Comparison of the robustness of different algorithms in the Spider
    environment. The above results are obtained by conducting 10 experiments with
    different random seeds in the test environment after the algorithm training converges.
    The dot represents the average episode return, the thick vertical line represents
    the standard deviation, and the thin vertical line represents the maximum and
    minimum values. The horizontal axis indicates the number of temporarily invalid
    sub-actions, from 1 to 7. The specific failure mechanism is as follows: at each
    timestep in an episode, a certain number of sub-actions will be randomly taken
    to invalidate. There are three failure modes, default value completion (black),
    random value completion (blue), and sub-action value completion of the previous
    timestep (red).'
  Figure 8 Link: articels_figures_by_rev_year\2021\Structured_Cooperative_Reinforcement_Learning_With_TimeVarying_Composite_Action_\figure_8.jpg
  Figure 8 caption: Performance comparison of fully sub-action spaces and unseen sub-action
    spaces in tomato simulator. The vertical axis represents the average episode return
    in 10 different test environments. The horizontal axis represents the total number
    of times the training phase interacts with the environment in millions of units.
    All algorithms are tested under 3 different random seeds, and the shaded area
    represents the standard deviation.
  Figure 9 Link: articels_figures_by_rev_year\2021\Structured_Cooperative_Reinforcement_Learning_With_TimeVarying_Composite_Action_\figure_9.jpg
  Figure 9 caption: 'Comparison of the robustness of different algorithms in tomato
    simulator. The above results are obtained by conducting 10 experiments with different
    random seeds in the test environment after the algorithm training converges. The
    dot represents the average episode return, the thick vertical line represents
    the standard deviation, and the thin vertical line represents the maximum and
    minimum values. The horizontal axis indicates the number of temporarily invalid
    sub-action spaces, from 1 to 3. The specific failure mechanism is as follows:
    at each timestep in an episode, a certain number of sub-action spaces will be
    randomly taken to invalidate. There are three failure modes, default value completion
    (black), random value completion (blue), and action value completion of the previous
    timestep (red).'
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.83
  Name of the first author: Wenhao Li
  Name of the last author: Hongyuan Zha
  Number of Figures: 12
  Number of Tables: 3
  Number of authors: 5
  Paper title: Structured Cooperative Reinforcement Learning With Time-Varying Composite
    Action Space
  Publication Date: 2021-08-04 00:00:00
  Table 1 caption: TABLE 1 The Detailed Information of Tomato Simulator
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The Ablation Studies of the Variants of SCORE Algorithm
  Table 3 caption: TABLE 3 Different Baselines Use Different Methods to Transfer to
    the Unseen Sub-Action Spaces
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3102140
- Affiliation of the first author: department of electrical and computer engineering,
    automation and systems research institute (asri), seoul national university, seoul,
    south korea
  Affiliation of the last author: department of electrical and computer engineering,
    automation and systems research institute (asri), seoul national university, seoul,
    south korea
  Figure 1 Link: articels_figures_by_rev_year\2021\Learning_to_Forget_for_MetaLearning_via_TaskandLayerWise_Attenuation\figure_1.jpg
  Figure 1 caption: '(a) A small degree of conflict: When the desired initialization
    location for each task is close to each other, the overall meta-update gradient
    direction points towards the location that aligns well with the desired initialization
    for each task. (b) A large degree of conflict, on the other hand, occurs when
    the direction of the desired initialization update for each task does not align.
    This scenario results in the overall meta-update gradient pointing towards the
    location desired by neither of tasks. A large degree of conflict is observed in
    MAML, especially at the higher layers of neural networks. Such undesired (hence
    compromised) initialization location can hinder learning during fast adaptation
    to each task, as illustrated in Fig. 2.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\Learning_to_Forget_for_MetaLearning_via_TaskandLayerWise_Attenuation\figure_2.jpg
  Figure 2 caption: "Visualization of optimization landscape. In [5], the stability\
    \ and smoothness of the optimization landscape are analyzed by measuring Lipschitzness\
    \ and the \u201Ceffective\u201D \u03B2 -smoothness of loss. We use these measurements\
    \ to analyze the learning dynamics of both MAML and our proposed method. Both\
    \ approaches are trained on 5-way 5-shot miniImageNet classification tasks, i.e.,\
    \ they are investigated for their inner-loop optimization. At each inner-loop\
    \ update step, we measure the variations in loss (a), the l 2 difference in gradients\
    \ (b), and the maximum difference in gradient over the distance (c) as we move\
    \ to different points along the computed gradient for that gradient descent. We\
    \ take an average of these values over the number of inner-loop updates and plot\
    \ them against the training iterations. The thinner shade in plots (a) and (b)\
    \ and the lower the values in plot (c) correspond to the smoother loss landscape,\
    \ suggesting less training difficulty [5]."
  Figure 3 Link: articels_figures_by_rev_year\2021\Learning_to_Forget_for_MetaLearning_via_TaskandLayerWise_Attenuation\figure_3.jpg
  Figure 3 caption: "Analysis on the degree of conflict and attenuation. (a) The degree\
    \ of conflict is measured (in radian) and has been observed to vary among layers.\
    \ For MAML, deeper layers exhibit a greater degree of conflict, which aligns with\
    \ the observation that deeper layers encode more task specific features [7]. After\
    \ applying L2F to MAML, the degree of conflict has decreased greatly. (b) The\
    \ manual attenuation of an initialization by different levels (i.e., the lower\
    \ \u03B3 is, the stronger is the attenuation) for each layer affects the classification\
    \ accuracy of a 4-layer CNN on miniImageNet. Deeper layers seem to prefer stronger\
    \ attenuation. This observation supports the argument that the initialization\
    \ quality is more compromised in the larger degree of conflict and the compromised\
    \ part needs to be minimized. (c) The degree of conflict between each meta-train\
    \ task and MAML initialization varies. This observation indicates that the amount\
    \ of irrelevant prior knowledge differs for each task. (d) Different attenuation\
    \ parameters \u03B3 are generated by L2F for each meta-test task, especially for\
    \ middle-level layers. This differentiation suggests that the degree of conflict\
    \ and thus the preferred amount of attenuation vary for each task, especially\
    \ in the middle-level layers."
  Figure 4 Link: articels_figures_by_rev_year\2021\Learning_to_Forget_for_MetaLearning_via_TaskandLayerWise_Attenuation\figure_4.jpg
  Figure 4 caption: 5-shot regression. The task distributions for training and evaluation
    are either (a) identical or (b) disjoint, with no overlap. In both cases, MAML+L2F
    (our model) is more fitted to the true function. The small dotted lines represent
    the regression results with initial weights before inner-loop optimization (but
    excluded from the legends for clear visualization).
  Figure 5 Link: articels_figures_by_rev_year\2021\Learning_to_Forget_for_MetaLearning_via_TaskandLayerWise_Attenuation\figure_5.jpg
  Figure 5 caption: Reinforcement learning results in three different environments.
    MAML+L2F outperforms MAML.
  Figure 6 Link: articels_figures_by_rev_year\2021\Learning_to_Forget_for_MetaLearning_via_TaskandLayerWise_Attenuation\figure_6.jpg
  Figure 6 caption: 2D navigation reinforcement learning task. The task distribution
    during evaluation is (a) the same as or (b) different from training. In both cases,
    MAML+L2F reaches the ending point, whereas MAML often fails to reach the destination.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.83
  Name of the first author: Sungyong Baik
  Name of the last author: Kyoung Mu Lee
  Number of Figures: 6
  Number of Tables: 12
  Number of authors: 4
  Paper title: Learning to Forget for Meta-Learning via Task-and-Layer-Wise Attenuation
  Publication Date: 2021-08-04 00:00:00
  Table 1 caption: TABLE 1 5-Way Classification Test Accuracy on miniImageNet and
    tieredImageNet
  Table 10 caption: TABLE 10 Average Reward Reported for 2D Navigation Task
  Table 2 caption: TABLE 2 5-Way Classification Test Accuracy on FC100 and CIFAR-FS
  Table 3 caption: TABLE 3 Test Accuracy on Meta-Dataset, in Which Models are Trained
    on ILSVRC-2012 Only (Top) or All Datasets (Bottom)
  Table 4 caption: TABLE 4 Ablation Studies on Inner-Loop Update Steps
  Table 5 caption: TABLE 5 Ablation Studies on Attenuation Scope
  Table 6 caption: TABLE 6 Ablation Studies on the Types of Representation for Task
    Embedding
  Table 7 caption: TABLE 7 Ablation Studies on Task-Adaptive Transformation Depicting
    the Effectiveness of Attenuation
  Table 8 caption: TABLE 8 Test Accuracy on 5-Way 5-Shot Cross-Domain Few-Shot Classification
  Table 9 caption: TABLE 9 MSE Averaged Over 100 Sampled Points With 95 % %Confidence
    Intervals on 5-Shot Regression
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3102098
