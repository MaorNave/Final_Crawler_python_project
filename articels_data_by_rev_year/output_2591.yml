- Affiliation of the first author: unsw udash, school of mathematics & statistics,
    university of new south wales sydney, sydney, nsw, australia
  Affiliation of the last author: unsw udash, school of mathematics & statistics,
    university of new south wales sydney, sydney, nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2022\Hawkes_Processes_With_Stochastic_Exogenous_Effects_for_ContinuousTime_Interactio\figure_1.jpg
  Figure 1 caption: "Left and middle panels: membership distribution functions \u03C0\
    \ ik (t) (red solid lines) for node 1 and node 2. Right panel: exogenous rate\
    \ function for the interactions from node 1 to node 2. All these functions are\
    \ left-continuous, piecewise-constant functions."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Hawkes_Processes_With_Stochastic_Exogenous_Effects_for_ContinuousTime_Interactio\figure_2.jpg
  Figure 2 caption: "Posterior concentration performance in the synthetic data case.\
    \ Top row displays the behaviours for parameter \u03B1 and bottom row displays\
    \ \u03C4 s behaviours. Columns 1,2,3 denote the numbers of interactions in each\
    \ testing case are 1948,5894,14259 accordingly. The x -axis value of the green\
    \ line denotes the initialization of the parameters and that of the red line denotes\
    \ the ground-truth value of the parameter. We use histograms of the converged\
    \ posterior samples to approximate the parameters true posterior distributions."
  Figure 3 Link: articels_figures_by_rev_year\2022\Hawkes_Processes_With_Stochastic_Exogenous_Effects_for_ContinuousTime_Interactio\figure_3.jpg
  Figure 3 caption: 'Left: training log-likelihood traceplot on College for SE-HP,
    using both MCMC and SG-MCMC. Right: AUC for each dataset under the SE-HP, under
    different modelling assumptions. Left: Different numbers of communities K . Right:
    Different scenarios for each nodes changepoints.'
  Figure 4 Link: articels_figures_by_rev_year\2022\Hawkes_Processes_With_Stochastic_Exogenous_Effects_for_ContinuousTime_Interactio\figure_4.jpg
  Figure 4 caption: 'Visualisation examples of the SE-HP on the Email (left panel)
    and Overflow (right panel) datasets in the first two rows, and (bottom row) average
    exogenous rate functions sum ine j[boldsymbol Ci(t)]top boldsymbol Lambda boldsymbol
    Cj(t)(H2N(N-1)) on all datasets. Panel top-left: nodes membership distributions,
    with columns describing the membership distribution at different times. Colors
    denote communities and for each column, the length of coloured line is proportional
    to the probability of the node belonging to the community at that time point.
    Panel top-right: Community compatibility matrix boldsymbol Lambda . Rows and columns
    represent different communities and each entry represents the community-to-community
    compatibility value, ranging from zero (dark blue) to highest (yellow). Second
    row: The time-evolving exogenous rate functions (blue and orange curves), with
    time on the x -axis. The interactions between these two nodes are shown as red
    and blue vertical lines along the time lines.'
  Figure 5 Link: articels_figures_by_rev_year\2022\Hawkes_Processes_With_Stochastic_Exogenous_Effects_for_ContinuousTime_Interactio\figure_5.jpg
  Figure 5 caption: Visualisations on the relationships between nodes degrees ( Di
    ), the number of changepoints ( Ji ), and the activity parameter ( Hi ), for each
    dataset. The green, yellow and red bars in each panels represent groups of nodes
    with low, medium and high valued degrees, respectively. Different thresholds of
    groupings for Di were chosen to ensure the groups have similar sizes across datasets.
  Figure 6 Link: articels_figures_by_rev_year\2022\Hawkes_Processes_With_Stochastic_Exogenous_Effects_for_ContinuousTime_Interactio\figure_6.jpg
  Figure 6 caption: 'Visualization on the membership distributions. Top row: one nodes
    membership distribution visualisation for the Self-Convolution strategy; bottom
    row: one nodes membership distribution visualisation for the Neighbour-Convolution
    strategy. Columns describe the membership distribution at different times. Colors
    denote communities and for each column, the length of coloured line is proportional
    to the probability of the node belonging to the community at that time point.'
  Figure 7 Link: articels_figures_by_rev_year\2022\Hawkes_Processes_With_Stochastic_Exogenous_Effects_for_ContinuousTime_Interactio\figure_7.jpg
  Figure 7 caption: Average exogenous rate functions sum ine j[boldsymbol Ci(t)]top
    boldsymbol Lambda boldsymbol Cj(t)(H2N(N-1)) on all the four real datasets for
    the Self-Convolution strategy.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.9
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xuhui Fan
  Name of the last author: Scott A. Sisson
  Number of Figures: 7
  Number of Tables: 6
  Number of authors: 5
  Paper title: Hawkes Processes With Stochastic Exogenous Effects for Continuous-Time
    Interaction Modelling
  Publication Date: 2022-03-23 00:00:00
  Table 1 caption: "TABLE 1 Testing Log-Likelihood (TLL), AUC (Area Under ROC Curve)\
    \ and MAE ( \xD7 10 \u22122 \xD710-2) Values"
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 Excitation Function Parameters \u03B1,\u03C4 \u03B1,\u03C4\
    \ for the SE-HP-MSE"
  Table 3 caption: TABLE 3 Running Time of SE-HP Evaluations (Seconds Per Iteration)
  Table 4 caption: "TABLE 4 Excitation Function Parameters \u03B1,\u03C4 \u03B1,\u03C4\
    \ for the SE-HP and CONST"
  Table 5 caption: TABLE 5 Link Prediction Performance (AUC) for SR-PP, SE-HP and
    SE-HP-MSE
  Table 6 caption: "TABLE 6 Excitation Function Parameters \u03B1,\u03C4 \u03B1,\u03C4\
    \ for the SE-HP-MSE"
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3161649
- Affiliation of the first author: school of computing, national university of singapore,
    singapore
  Affiliation of the last author: school of computing, national university of singapore,
    singapore
  Figure 1 Link: articels_figures_by_rev_year\2022\Point_SpatioTemporal_Transformer_Networks_for_Point_Cloud_Video_Modeling\figure_1.jpg
  Figure 1 caption: "Illustration of different point-based methods for spatio-temporal\
    \ modeling in point cloud videos. a) MeteorNet [27] employs point tracking to\
    \ preserve spatio-temporal structure. b) PSTNet [12] limits temporal modeling\
    \ range (e.g, 1 neighboring frame) to preserve spatio-temporal structure. c) P4Transformer\
    \ [11] is able to adaptively search related points across entire videos based\
    \ on self-attention and global weighted sum pooling. However, it does not encode\
    \ the spatio-temporal structure like MeteorNet or PSTNet. d) The proposed PST-Transformer\
    \ is equipped with an ability to encode spatio-temporal structure based on the\
    \ spatial displacement ( \u03B4 x , \u03B4 y , \u03B4 z ) and temporal difference\
    \ \u0394 t ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Point_SpatioTemporal_Transformer_Networks_for_Point_Cloud_Video_Modeling\figure_2.jpg
  Figure 2 caption: "Illustration of the proposed PST-Transformer. The input contains\
    \ L=3 frames, with N=3 points per frame. PST-Transformer is updating the feature\
    \ of point a in the 1st frame based on all the points in the video. When modeling\
    \ the spatio-temporal structure from point a to point i in the 3rd frame, PST-Transformer\
    \ first employs a video-level self-attention to achieve the attention weight \u03B1\
    \ ai , which indicates how much point i is similar or related to point a . Then,\
    \ to encode the spatio-temporal structure from point a to point i , PST-Transformer\
    \ applies a spatial structure encoding function S \u03B4s (\u22C5) and a temporal\
    \ structure encoding function T 2 (\u22C5) on the feature of point i based on\
    \ the spatial displacement \u03B4s and temporal difference \u0394 t =2 , respectively.\
    \ Finally, the encoded feature of point i is added into the new feature of point\
    \ a based on the attention weight \u03B1 ai ."
  Figure 3 Link: articels_figures_by_rev_year\2022\Point_SpatioTemporal_Transformer_Networks_for_Point_Cloud_Video_Modeling\figure_3.jpg
  Figure 3 caption: Illustration of PST-Transformer with multi-head attention, ReLUs,
    LayerNorms and residual connections, where h is the number of heads.
  Figure 4 Link: articels_figures_by_rev_year\2022\Point_SpatioTemporal_Transformer_Networks_for_Point_Cloud_Video_Modeling\figure_4.jpg
  Figure 4 caption: "Illustration of PST-Transformer networks. a) 3D action recognition.\
    \ The input contains 5 frames, with N points per frame. (1) Based on temporal\
    \ radius r t =1 and temporal stride s t =2 , the t 2 and t 4 frames are selected.\
    \ Suppose the spatial subsampling rate s s =N3 . Then, N s s =3 points are sampled\
    \ for each selected frame. Based on the sampled points, spatial radius r s and\
    \ temporal radius r t , spatio-temporal local areas are constructed. (2) Each\
    \ spatio-temporal local area is encoded to a vector by point 4D convolution [11]\
    \ as the region feature. (3) Our PST-Transformer updates local features by collecting\
    \ more information from similar or related areas. (4) A max pooling merges local\
    \ features to a global feature, which is then mapped to action predictions by\
    \ a multilayer perceptron (MLP). b) 4D semantic segmentation. Because points are\
    \ subsampled, after PST-Transformer, we employ a feature propagation operation\
    \ [34] to recover the sampled N \u2032 points to the original N points by interpolating\
    \ features. Finally, an MLP maps interpolated features to point predictions."
  Figure 5 Link: articels_figures_by_rev_year\2022\Point_SpatioTemporal_Transformer_Networks_for_Point_Cloud_Video_Modeling\figure_5.jpg
  Figure 5 caption: Influence of spatial subsampling rate and temporal stride on the
    accuracy and running memory of the PST-Transformer (small) model. We conduct 3D
    action recognition on the MSR-Action3D [20] dataset with 24 frames.
  Figure 6 Link: articels_figures_by_rev_year\2022\Point_SpatioTemporal_Transformer_Networks_for_Point_Cloud_Video_Modeling\figure_6.jpg
  Figure 6 caption: Visualization of the attention and output of PST-Transformer.
    Brighter color indicates higher attention or activation. Our PST-Transformer pays
    correct attention across the frames. Moreover, PST-Transformer outputs high activations
    to salient moving areas, demonstrating the ability to capture the most informative
    clues in action reasoning.
  Figure 7 Link: articels_figures_by_rev_year\2022\Point_SpatioTemporal_Transformer_Networks_for_Point_Cloud_Video_Modeling\figure_7.jpg
  Figure 7 caption: Visualization of the learned parameters of temporal functions
    mathcal TDelta t . Closer frames affect more on the current moment. Moreover,
    parameters appear to follow Gaussian distributions with different standard deviations,
    like different heads capturing the temporal structure at different scales.
  Figure 8 Link: articels_figures_by_rev_year\2022\Point_SpatioTemporal_Transformer_Networks_for_Point_Cloud_Video_Modeling\figure_8.jpg
  Figure 8 caption: 4D semantic segmentation visualization on the Synthia 4D [5] dataset.
    Incorrect predictions are marked with circles. All the methods achieve satisfactory
    results. However, our PST-Transformer makes fewer incorrect predictions than other
    methods.
  Figure 9 Link: articels_figures_by_rev_year\2022\Point_SpatioTemporal_Transformer_Networks_for_Point_Cloud_Video_Modeling\figure_9.jpg
  Figure 9 caption: Influence of the number of PST-Transformer layers and the number
    of heads on NTU RGB+D 60 [37] with the cross-subject evaluation protocol. Note
    that, when we change the number of heads, the total feature dimension of each
    layer is fixed.
  First author gender probability: 0.9
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Hehe Fan
  Name of the last author: Mohan Kankanhalli
  Number of Figures: 9
  Number of Tables: 7
  Number of authors: 3
  Paper title: Point Spatio-Temporal Transformer Networks for Point Cloud Video Modeling
  Publication Date: 2022-03-23 00:00:00
  Table 1 caption: TABLE 1 Efficiency and Accuracy Comparison on the MSR-Action3D
    Dataset [20] With 24 Frames
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison Between Convolution-Based and Concatenation-Based
    Spatial Structure Encoding Methods on the PST-Transformer (small) Model
  Table 3 caption: TABLE 3 3D Action Recognition on MSR-Action3D [20] With Different
    Numbers of Frames
  Table 4 caption: TABLE 4 3D Action Recognition Accuracy (%) on NTU RGB+D 60 [37]
    and NTU RGB+D 120 [21]
  Table 5 caption: TABLE 5 4D Semantic Segmentation Results (mIoU %) on the Synthia
    4D Dataset [5]
  Table 6 caption: TABLE 6 Influence of Spatio-Temporal Encoding and Decomposition
    on NTU RGB+D 60 [37] (Cross-Subject)
  Table 7 caption: TABLE 7 Comparison of P4Transformer and PST-Transformer
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3161735
- Affiliation of the first author: national laboratory of radar signal processing,
    xidian university, xian, china
  Affiliation of the last author: westlake university, hangzhou, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Recurrent_Neural_Networks_for_Snapshot_Compressive_Imaging\figure_1.jpg
  Figure 1 caption: Principle of video SCI (top-left), spectral SCI (bottom-left),
    and the proposed BIRNAT for reconstruction (middle). For video SCI system, a dynamic
    scene, shown as a sequence of images (either grayscale or color) at different
    timestamps ([ t 1 , t 2 , ..., t B ], top-left), passes through a dynamic aperture
    (bottom-left), which imposes individual coding patterns. The coded frames after
    the aperture are then integrated overtime on a grayscale or color camera, forming
    a single-frame compressed measurement (middle). Different from the video SCI system,
    the spectral SCI system detects measurements including tens of spectral channels
    coded by the mask and dispersed by a disperser. These measurements along with
    the dynamic masks are fed into our BIRNAT to reconstruct the series (right) of
    the 3D scene, which can be grayscale, color video, or hyperspectral images.
  Figure 10 Link: articels_figures_by_rev_year\2022\Recurrent_Neural_Networks_for_Snapshot_Compressive_Imaging\figure_10.jpg
  Figure 10 caption: 'Real data Wheel: results of GAP-TV, DeSCI, E2E-CNN, PnP-FFDNet
    and BIRNAT.'
  Figure 2 Link: articels_figures_by_rev_year\2022\Recurrent_Neural_Networks_for_Snapshot_Compressive_Imaging\figure_2.jpg
  Figure 2 caption: "Left: the proposed preprocessing approach to normalizing the\
    \ measurement. We feed the concatenation of normalization measurement Y \xAF \xAF\
    \ \xAF \xAF and Y \xAF \xAF \xAF \xAF \u2299 C k B k=1 into the proposed BIRNAT.\
    \ Middle: the specific structure of BIRNAT including i) the attention based CNN\
    \ (AttRes-CNN) to reconstruct the first frame X \u02C6 f 1 ; ii) forward RNN to\
    \ recurrently reconstruct the following frames X \u02C6 f k B k=2 ; iii) backward\
    \ RNN to perform reverse-order reconstruction X \u02C6 b k 1 k=B\u22121 . Right:\
    \ details of AttRes-CNN and RNN cell. C denotes concatenation along the channel\
    \ dimension. F \u2219 (where \u2022 can be x , r , y or h ) denotes a non-linear\
    \ module composed of several convolutional layers."
  Figure 3 Link: articels_figures_by_rev_year\2022\Recurrent_Neural_Networks_for_Snapshot_Compressive_Imaging\figure_3.jpg
  Figure 3 caption: Self-attention module.
  Figure 4 Link: articels_figures_by_rev_year\2022\Recurrent_Neural_Networks_for_Snapshot_Compressive_Imaging\figure_4.jpg
  Figure 4 caption: "Recurrent reconstruction module for BIRNAT-color. For forward\
    \ reconstruction process, the previous RGB frame X \u02C6 f t\u22121 will be divided\
    \ into four sub-frames X \u02C6 r,f t\u22121 , X \u02C6 g1,f t\u22121 , X \u02C6\
    \ g2,f t\u22121 and X \u02C6 b,f t\u22121 , corresponding to the Bayer filters,\
    \ and then four RNNs are applied to reconstruct the next frame."
  Figure 5 Link: articels_figures_by_rev_year\2022\Recurrent_Neural_Networks_for_Snapshot_Compressive_Imaging\figure_5.jpg
  Figure 5 caption: Reconstructed frames of GAP-TV, DeSCI, PnP-FFDNet, E2E-CNN, 3D-Unet
    and BIRNAT on six simulated video SCI datasets. Please see the full results in
    the supplementary material for details, which can be found on the Computer Society
    Digital Library at http:doi.ieeecomputersociety.org10.1109TPAMI.2022.3161934.
  Figure 6 Link: articels_figures_by_rev_year\2022\Recurrent_Neural_Networks_for_Snapshot_Compressive_Imaging\figure_6.jpg
  Figure 6 caption: Selected attention maps of the first frame. Yellow points denote
    the pixels randomly selected from each image, and red areas denote the active
    places.
  Figure 7 Link: articels_figures_by_rev_year\2022\Recurrent_Neural_Networks_for_Snapshot_Compressive_Imaging\figure_7.jpg
  Figure 7 caption: Frame-wise reconstruction quality on BIRNAT and other algorithms
    on Traffic (a-b) and Vehicle (c-d).
  Figure 8 Link: articels_figures_by_rev_year\2022\Recurrent_Neural_Networks_for_Snapshot_Compressive_Imaging\figure_8.jpg
  Figure 8 caption: Reconstructed frames of GAP-TV, DeSCI, PnP-FFDNet, PnP-FastDVDnet
    and BIRNAT-color on six color simulated video SCI datasets.
  Figure 9 Link: articels_figures_by_rev_year\2022\Recurrent_Neural_Networks_for_Snapshot_Compressive_Imaging\figure_9.jpg
  Figure 9 caption: Reconstructed frames of GAP-TV, DeSCI, HSSP, lambda -net, TSA-net,
    Deep-GSM, GAP-net and BIRNAT on simulated spectral SCI datasets. Please look over
    the full results in the supplementary material for details, available online.
  First author gender probability: 0.75
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.6
  Name of the first author: Ziheng Cheng
  Name of the last author: Xin Yuan
  Number of Figures: 15
  Number of Tables: 6
  Number of authors: 7
  Paper title: Recurrent Neural Networks for Snapshot Compressive Imaging
  Publication Date: 2022-03-24 00:00:00
  Table 1 caption: TABLE 1 The Average Results of PSNR in dB (Left Entry) and SSIM
    (Right Entry) and Running Time per MeasurementShot in Seconds by Different Algorithms
    on 6 Grayscale Simulation Datasets and the GPU Memory Footprint (GB) During Training
    (Left Entry) and Testing (Right Entry) for one Sample
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The Average Results of PSNR in dB (Left Entry) and SSIM
    (Right Entry) and Running Time per MeasurementShot in Seconds by Different Algorithms
    on 6 Color Simulation Datasets
  Table 3 caption: TABLE 3 The Average Results of PSNR in dB (Left Entry) and SSIM
    (Right Entry) by Different Algorithms on 10 Spectral Images Simulation Datasets
  Table 4 caption: TABLE 4 Ablation Study of BIRNAT for the 6 Gray-Scale and 10 Spectral
    Images Simulation Datasets, the Average PSNR and SSIM is Shown
  Table 5 caption: TABLE 5 The Comparison of Grayscale Videos Reconstruction Quality
    (Average PSNR of six Gray-Scale Simulation Datasets) of the Four Methods at Different
    Measurement Noise Levels
  Table 6 caption: TABLE 6 The Comparison of Grayscale Videos Reconstruction Quality
    (Average PSNR of six Gray-Scale Simulation Datasets) of the Four Methods at Different
    Mask Errors Levels
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3161934
- Affiliation of the first author: wangxuan institute of computer technology, peking
    university, beijing, china
  Affiliation of the last author: wangxuan institute of computer technology, peking
    university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Controllable_Image_Synthesis_With_AttributeDecomposed_GAN\figure_1.jpg
  Figure 1 caption: 'Controllable image synthesis with desired component attributes
    provided by multiple source images. Left (ADGAN): When applying our ADGAN in the
    task of pose guided person image synthesis (PIS), human attributes including pose
    and component attributes are embedded into the latent space as the pose code and
    the decomposed style code. Various target person images can be generated in user
    control via the editable style code. Right (ADGAN++): Our ADGAN++ handles the
    task of semantic image synthesis (SIS) in two stages. First, we extract the decomposed
    semantic code and style code for each separated component, which are then utilized
    to synthesize styled component images. Then, we can replace any semantic region
    in the target image, which is desired to have the same component attribute as
    the source image, with the corresponding styled component image generated in the
    first stage. The final synthesis result is obtained by implementing the refinement
    operation on the modified target image. Notice that by directly feeding semantic
    maps into ADGAN as input instead of poses, the SIS task with a small number of
    component attributes can also be properly handled by ADGAN. Moreover, by using
    a simple pose2sem module to convert the input pose into the corresponding semantic
    map, our ADGAN++ can handle the pose-guided PIS task with satisfactory performance
    as well.'
  Figure 10 Link: articels_figures_by_rev_year\2022\Controllable_Image_Synthesis_With_AttributeDecomposed_GAN\figure_10.jpg
  Figure 10 caption: Our pose2sem module bridges the gap between the PIS task and
    the SIS task by converting a target pose to the target semantic map according
    to the input source persons image, pose, and semantic map. Cooperating with the
    pre-trained pose2sem module, ADGAN++ is able to handle the part-controllable PIS
    task with comparable performance as ADGAN.
  Figure 2 Link: articels_figures_by_rev_year\2022\Controllable_Image_Synthesis_With_AttributeDecomposed_GAN\figure_2.jpg
  Figure 2 caption: An overview of the network architecture of our ADGAN generator.
    Here we take the person image synthesis task as an example. The target pose and
    source person are embedded into the latent space via two independent pathways,
    called pose encoding and decomposed component encoding, respectively. For the
    latter, we employ a human parser to separate component attributes and encode them
    via a global texture encoder. A series of style blocks equipped with a fusion
    module are introduced to inject the texture style of source person into the pose
    code by controlling the affine transformation parameters in AdaIN layers. Finally,
    the desired image is reconstructed via a decoder.
  Figure 3 Link: articels_figures_by_rev_year\2022\Controllable_Image_Synthesis_With_AttributeDecomposed_GAN\figure_3.jpg
  Figure 3 caption: Details of the texture encoder used in the generator of ADGANADGAN++.
    A global texture encoding is introduced by concatenating the outputs of the learnable
    encoder and the fixed VGG encoder.
  Figure 4 Link: articels_figures_by_rev_year\2022\Controllable_Image_Synthesis_With_AttributeDecomposed_GAN\figure_4.jpg
  Figure 4 caption: Effects of the proposed DCE and GTE. (a) A source person and (b)
    a target pose for inputs. (c) The result generated without DCE and GTE, but using
    a common convolution encoder to encode the style image without segmenting parts.
    (d) The result generated without only DCE, meaning that it uses GTE to encode
    the style image without segmenting parts. (e) The result generated with both two
    modules.
  Figure 5 Link: articels_figures_by_rev_year\2022\Controllable_Image_Synthesis_With_AttributeDecomposed_GAN\figure_5.jpg
  Figure 5 caption: Loss curves for the effectiveness of our DCE module in the PIS
    training process.
  Figure 6 Link: articels_figures_by_rev_year\2022\Controllable_Image_Synthesis_With_AttributeDecomposed_GAN\figure_6.jpg
  Figure 6 caption: Auxiliary effects of the fusion module (FM) for DCE. (a) A source
    person and (b) a target pose for inputs. (c) The result generated without DCE.
    (d) The result generated with DCE introduced but no FM contained in style blocks,
    namely using a linear layer to reshape the style code to the AdaIN parameters.
    (e) The result generated with both DCE and FM.
  Figure 7 Link: articels_figures_by_rev_year\2022\Controllable_Image_Synthesis_With_AttributeDecomposed_GAN\figure_7.jpg
  Figure 7 caption: Examples of results obtained by ADGAN for synthesizing person
    images in arbitrary poses.
  Figure 8 Link: articels_figures_by_rev_year\2022\Controllable_Image_Synthesis_With_AttributeDecomposed_GAN\figure_8.jpg
  Figure 8 caption: Effects of the contextual loss.
  Figure 9 Link: articels_figures_by_rev_year\2022\Controllable_Image_Synthesis_With_AttributeDecomposed_GAN\figure_9.jpg
  Figure 9 caption: An overview of the network architecture of our ADGAN++ generator
    which contains two stages. In the first stage, we synthesize images for all semantic
    regions one by one according to the target semantic map and the given source style
    images. We can specify which parts in the target image should be styled as those
    in the corresponding source images. Then we stitch those chosen synthesized component
    images into a complete image. In the second stage, we encode the stitched image
    into a feature map which then goes through a few residual convolution blocks equipped
    with our SegIN module to generate the final target image. Note that the fusion
    module consisting of several fully-connected layers adopted in ADGAN is kept because
    we empirically found that it improves the training stability of our model and
    helps to obtain more visually-pleasing synthesis results.
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.64
  Name of the first author: Guo Pu
  Name of the last author: Zhouhui Lian
  Number of Figures: 23
  Number of Tables: 8
  Number of authors: 6
  Paper title: Controllable Image Synthesis With Attribute-Decomposed GAN
  Publication Date: 2022-03-24 00:00:00
  Table 1 caption: TABLE 1 Results of the Parameter Study Conducted for the Fusion
    Module in our Methods
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Quantitative Comparison of Our Methods (i.e., ADGAN and
    ADGAN++) and Other Existing Approaches on DeepFashion
  Table 3 caption: TABLE 3 Results of the User Study (%) (%) for Different Methods
    in PIS
  Table 4 caption: TABLE 4 Quantitative Comparison of the Reconstruction Quality of
    our Methods and Other State-of-the-art Approaches on CelebAMask-HQ
  Table 5 caption: TABLE 5 Quantitative Comparison of the Reconstruction Quality of
    ADGAN++ With Other State-of-the-art Approaches on ADE20K
  Table 6 caption: TABLE 6 Quantitative Comparison of Part-Level Style Transfer Results
    Generated by our Methods and Other State-of-the-art Approaches Evaluated on CelebAMask-HQ
    Using the FID Measure
  Table 7 caption: TABLE 7 Results of the User Study Conducted for Part-Level Style
    Transfer Results Generated by our Methods and Other State-of-the-art Approaches
    Evaluated on CelebAMask-HQ
  Table 8 caption: TABLE 8 Comparison of Average Runtimes (Seconds Per Image) of Different
    Methods
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3161985
- Affiliation of the first author: cas key laboratory of technology in gipas, university
    of science and technology of china, hefei, anhui, china
  Affiliation of the last author: cas key laboratory of technology in gipas, university
    of science and technology of china, hefei, anhui, china
  Figure 1 Link: articels_figures_by_rev_year\2022\DualityInduced_Regularizer_for_Semantic_Matching_Knowledge_Graph_Embeddings\figure_1.jpg
  Figure 1 caption: "An illustration of why basic DURA can improve the performance\
    \ of SM models when the embedding dimensions are 2. Suppose that triples ( u i\
    \ , r j , v k ) ( k=1,2,\u2026,n ) are valid. (a) Fig. 1a demonstrates that tail\
    \ entities connected to a head entity through the same relation should have similar\
    \ embeddings. (b) Fig. 1b shows that SM models without regularization can get\
    \ the same score even though the embeddings of v k are dissimilar. (c) Fig. 1c\
    \ shows that with DURA, embeddings of v k are encouraged to locate in a small\
    \ region."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\DualityInduced_Regularizer_for_Semantic_Matching_Knowledge_Graph_Embeddings\figure_2.jpg
  Figure 2 caption: "An illustration of how temporal DURA influence the performance\
    \ of SM models when the embedding dimensions are 2. Suppose that triples ( u i\
    \ , r j , v k , t l ) ( k=1,2,\u2026,n ) are valid. Figs. 2b and 2d show that\
    \ SM models without regularization can get the same score even though and the\
    \ embeddings of original entities v k and time-dependent entities v k T l are\
    \ dissimilar, respectively. Fig. 2c shows that with DURA1, embeddings of v k are\
    \ encouraged to locate in a small region. Fig. 2e shows that with DURA2, embeddings\
    \ of time-dependent entities v k T l are encouraged to locate in a small region,\
    \ while the corresponding embeddings of original entities may be dissimilar."
  Figure 3 Link: articels_figures_by_rev_year\2022\DualityInduced_Regularizer_for_Semantic_Matching_Knowledge_Graph_Embeddings\figure_3.jpg
  Figure 3 caption: "The effect of entity embeddings \u03BB -sparsity on MRR. The\
    \ experiments are conducted on FB15k-237."
  Figure 4 Link: articels_figures_by_rev_year\2022\DualityInduced_Regularizer_for_Semantic_Matching_Knowledge_Graph_Embeddings\figure_4.jpg
  Figure 4 caption: Sensitivity to hyper-parameters of DURA. The x -axis is the logarithm
    of each hyper-parameter based on 10 and The y -axis is mean reciprocal rank (MRR)
    on test data.
  Figure 5 Link: articels_figures_by_rev_year\2022\DualityInduced_Regularizer_for_Semantic_Matching_Knowledge_Graph_Embeddings\figure_5.jpg
  Figure 5 caption: Average MRR of RESCAL with different regularizers on different
    types of relations. The dataset is FB15k-237.
  Figure 6 Link: articels_figures_by_rev_year\2022\DualityInduced_Regularizer_for_Semantic_Matching_Knowledge_Graph_Embeddings\figure_6.jpg
  Figure 6 caption: Visualization of tail entity embeddings using T-SNE. A point represents
    a tail entity. Points in the same color represent tail entities that have the
    same ( h r , r j ) context.
  Figure 7 Link: articels_figures_by_rev_year\2022\DualityInduced_Regularizer_for_Semantic_Matching_Knowledge_Graph_Embeddings\figure_7.jpg
  Figure 7 caption: Sensitivity to hyper-parameters of DURA1 and DURA2. The x -axis
    is the logarithm of each hyper-parameter based on 10 and the y -axis is mean reciprocal
    rank (MRR) on test data.
  Figure 8 Link: articels_figures_by_rev_year\2022\DualityInduced_Regularizer_for_Semantic_Matching_Knowledge_Graph_Embeddings\figure_8.jpg
  Figure 8 caption: Sensitivity to hyper-parameters of DURA. The x -axis is the logarithm
    of each hyper-parameter based on 10 and the y -axis is mean reciprocal rank (MRR)
    on test data.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Jie Wang
  Name of the last author: Feng Wu
  Number of Figures: 8
  Number of Tables: 6
  Number of authors: 6
  Paper title: Duality-Induced Regularizer for Semantic Matching Knowledge Graph Embeddings
  Publication Date: 2022-03-24 00:00:00
  Table 1 caption: TABLE 1 Hyper-Parameters Found by Grid Search
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Evaluation Results on WN18RR, FB15k-237 and YAGO3-10 Datasets
  Table 3 caption: TABLE 3 Comparison Between DURA, Squared Frobenius Norm (FRO),
    and Nuclear 3-Norm (N3) Regularizers
  Table 4 caption: TABLE 4 Hyper-Parameters Found by Grid Search
  Table 5 caption: TABLE 5 Evaluation Results on ICEWS14, ICEWS05-15, and YAGO15k
    Datasets
  Table 6 caption: TABLE 6 Ablation Results on the Combined and Uncombined Regularizers.
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3161804
- Affiliation of the first author: key laboratory of intelligent interaction and applications,
    ministry of industry and information technology, school of computer science, school
    of artificial intelligence, optics and electronics (iopen), northwestern polytechnical
    university, xian, shaanxi, china
  Affiliation of the last author: key laboratory of intelligent interaction and applications,
    ministry of industry and information technology, school of artificial intelligence,
    optics and electronics (iopen), northwestern polytechnical university, xian, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Fast_Locality_Discriminant_Analysis_With_Adaptive_Manifold_Embedding\figure_1.jpg
  Figure 1 caption: Illustration of the graph constructed by two types of ways. v
    i is the i th data point and m i refers to the i th anchor point. Data points
    and anchor points of the same color belong to the same class.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Fast_Locality_Discriminant_Analysis_With_Adaptive_Manifold_Embedding\figure_2.jpg
  Figure 2 caption: "2-D scatter points visualization on a multimodal data set. (a)-(d)\
    \ are the learned results of FLDA corresponding to different parameters, respectively.\
    \ The positions of anchor points are marked by \u201C\u25CA \u201D."
  Figure 3 Link: articels_figures_by_rev_year\2022\Fast_Locality_Discriminant_Analysis_With_Adaptive_Manifold_Embedding\figure_3.jpg
  Figure 3 caption: 2-D scatter points visualization of 6 feature extraction methods
    on the three-ring toy data set. The first row figures show the results on the
    three-ring toy data with 3-dimensional noises and noise variance = 0.25. The second
    row figures show the results on the three-ring toy data with 8-dimensional noises
    and noise variance = 5. (g) and (n) show the anchor points learned by FLDA under
    different noise conditions.
  Figure 4 Link: articels_figures_by_rev_year\2022\Fast_Locality_Discriminant_Analysis_With_Adaptive_Manifold_Embedding\figure_4.jpg
  Figure 4 caption: 2-D scatter points visualization of 6 feature extraction methods
    on the X-type toy data set. The first row figures show the results on the X-type
    toy data with 10-dimensional noises and noise variance = 16. The second row figures
    show the results on the X-type toy data with 100-dimensional noises and noise
    variance = 32. (g) and (n) show the anchor points learned by FLDA under different
    noise conditions.
  Figure 5 Link: articels_figures_by_rev_year\2022\Fast_Locality_Discriminant_Analysis_With_Adaptive_Manifold_Embedding\figure_5.jpg
  Figure 5 caption: t-SNE visualization on the Wine data set after dimension reduction
    by RDR, LFDA, LSDA, LADA, SPDA and FLDA.
  Figure 6 Link: articels_figures_by_rev_year\2022\Fast_Locality_Discriminant_Analysis_With_Adaptive_Manifold_Embedding\figure_6.jpg
  Figure 6 caption: 2-D visualization on the Wine data set when the dimension is reduced
    to 2 by RDR, LFDA, LSDA, LADA, SPDA and FLDA.
  Figure 7 Link: articels_figures_by_rev_year\2022\Fast_Locality_Discriminant_Analysis_With_Adaptive_Manifold_Embedding\figure_7.jpg
  Figure 7 caption: Average NNC accuracy of 8 feature extraction methods on six benchmark
    data sets when the dimension of the learned subspace varies. x -axis is the number
    of reduced dimensions and y -axis is the classification accuracy.
  Figure 8 Link: articels_figures_by_rev_year\2022\Fast_Locality_Discriminant_Analysis_With_Adaptive_Manifold_Embedding\figure_8.jpg
  Figure 8 caption: Performance variation of FLDA with different regularization parameter
    gamma and number of anchor points p on four data sets.
  Figure 9 Link: articels_figures_by_rev_year\2022\Fast_Locality_Discriminant_Analysis_With_Adaptive_Manifold_Embedding\figure_9.jpg
  Figure 9 caption: The convergence curves of FLDA on four data sets.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Feiping Nie
  Name of the last author: Rong Wang
  Number of Figures: 9
  Number of Tables: 10
  Number of authors: 3
  Paper title: Fast Locality Discriminant Analysis With Adaptive Manifold Embedding
  Publication Date: 2022-03-25 00:00:00
  Table 1 caption: TABLE 1 Notations
  Table 10 caption: TABLE 10 Performance Measures Based on Confusion Matrix
  Table 2 caption: TABLE 2 Brief Statistics of the State-of-the-Art Local Discriminant
    Analysis Methods
  Table 3 caption: TABLE 3 Brief Description of Data Sets
  Table 4 caption: "TABLE 4 Classification Results (Best Average Accuracy% \xB1 \xB1\
    \ std) Comparison Among Different Global and Local LDA Algorithms on Seven Data\
    \ Sets With the 1-Nearest Neighbor Classifier (NNC) and the Linear SVM Classifier\
    \ (SVM)"
  Table 5 caption: "TABLE 5 Classification Results (Best Average Accuracy% \xB1 \xB1\
    \ std) Comparison Among Different Global and Local LDA Algorithms on Seven Data\
    \ Sets With the 1-Nearest Neighbor Classifier (NNC) and the Linear SVM Classifier\
    \ (SVM)"
  Table 6 caption: "TABLE 6 The Minimum Computational Time (mean \xB1 \xB1 std) of\
    \ Local Discriminant Analysis Methods on Five Data Sets"
  Table 7 caption: "TABLE 7 Performance Comparison (Best Average Results% \xB1 \xB1\
    \ std) of Different Dimensionality Reduction Methods on Five Imbalanced Data Sets"
  Table 8 caption: TABLE 8 Details of Imbalanced Data Sets, Where IR is the Imbalance
    Ratio Value Between the Large Class and the Small Class
  Table 9 caption: TABLE 9 Confusion Matrix
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3162498
- Affiliation of the first author: university of maryland, college park, md, usa
  Affiliation of the last author: university of maryland, college park, md, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\Dataset_Security_for_Machine_Learning_Data_Poisoning_Backdoor_Attacks_and_Defens\figure_1.jpg
  Figure 1 caption: 'Backdoor attacks with different triggers: (a) a square pattern
    flips the identity of a stop-sign [75]; (b) dead code as the trigger for source
    code modeling [77].'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.88
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Micah Goldblum
  Name of the last author: Tom Goldstein
  Number of Figures: 1
  Number of Tables: 0
  Number of authors: 9
  Paper title: 'Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks,
    and Defenses'
  Publication Date: 2022-03-25 00:00:00
  Table 1 caption: Not Available
  Table 10 caption: Not Available
  Table 2 caption: Not Available
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3162397
- Affiliation of the first author: department of biomedical engineering, georgia institute
    of technology, atlanta, ga, usa
  Affiliation of the last author: department of biomedical engineering, georgia institute
    of technology, atlanta, ga, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\BodyPressure__Inferring_Body_Pose_and_Contact_Pressure_From_a_Depth_Image\figure_1.jpg
  Figure 1 caption: We use fast physics simulations to generate BodyPressureSD, a
    large synthetic human resting pose dataset, and then train a deep model, BodyPressureWnet,
    to infer pose and pressure from a depth image. (a) Our simulation method rests
    bodies on a soft bed and covers them with blankets. We then render depth images
    from the perspective of an overhead camera, and generate pressure images from
    a pressure mat underneath the person. (b) Using an augmented dataset with a mix
    of this synthetic data combined with real data captured by a depth camera, we
    learn a mapping from depth and gender to pose and contact pressure. (c) This enables
    a camera to infer the pressure distribution of the person and potentially detect
    pressure injuries in the real world.
  Figure 10 Link: articels_figures_by_rev_year\2022\BodyPressure__Inferring_Body_Pose_and_Contact_Pressure_From_a_Depth_Image\figure_10.jpg
  Figure 10 caption: Behavior comparison of BPBnet and BPWnet. BPBnet has lower contact
    pressure error, but its projection onto the inferred mesh contains an artefact.
    BPWnet plausibly infers a high pressure on the foot, while BPBnet incorrectly
    assigns a high pressure to the underside of the upper leg.
  Figure 2 Link: articels_figures_by_rev_year\2022\BodyPressure__Inferring_Body_Pose_and_Contact_Pressure_From_a_Depth_Image\figure_2.jpg
  Figure 2 caption: We fit 3D SMPL bodies to the SLP dataset [3], which we use for
    initializing the physics simulator and for training and testing our deep models.
    Our method resolves depth ambiguity using a loss between the SMPL mesh and 3D
    points from the depth image. Examples are shown without the depth loss term, resulting
    in poses with depth error. Examples are also shown without BetaNet, resulting
    in bodies with unreasonable shapes.
  Figure 3 Link: articels_figures_by_rev_year\2022\BodyPressure__Inferring_Body_Pose_and_Contact_Pressure_From_a_Depth_Image\figure_3.jpg
  Figure 3 caption: 'Our synthetic data generation method involves two processes:
    The first process is similar to our previous work [7]; it involves (I.) sampling
    random human poses, (II.) resting dynamic capsulized bodies on a soft bed to find
    a resting pose, (III.) resting a finer body representation on the bed to improve
    human body shape detail, and (IV.) using a simulated pressure sensing mat underneath
    the person to compute a pressure image. The second process involves (V.) covering
    the body with a blanket, (VI.) pulling the top of the blanket down to uncover
    the persons head, (VII.) extracting deformed meshes, (VIII.) creating a solid
    mesh, and (IX.) simulating a depth image from a pinhole camera positioned above
    the bed.'
  Figure 4 Link: articels_figures_by_rev_year\2022\BodyPressure__Inferring_Body_Pose_and_Contact_Pressure_From_a_Depth_Image\figure_4.jpg
  Figure 4 caption: BodyPressureSD synthetic data samples created by resting bodies
    on a mattress and covering them with a blanket.
  Figure 5 Link: articels_figures_by_rev_year\2022\BodyPressure__Inferring_Body_Pose_and_Contact_Pressure_From_a_Depth_Image\figure_5.jpg
  Figure 5 caption: Example of resting pose diversity. Left blue pose shows a SLP-3Dfits
    example. Right black examples shows BodyPressureSD resting poses and body shapes
    that were initialized in the simulator by adding noise to the left blue pose (Eq.
    4) and dropped on the mattress.
  Figure 6 Link: articels_figures_by_rev_year\2022\BodyPressure__Inferring_Body_Pose_and_Contact_Pressure_From_a_Depth_Image\figure_6.jpg
  Figure 6 caption: BodyPressureWnet (BPWnet), a deep network that learns a mapping
    from depth and gender to pose and contact pressure. Depth, mathcal D is encoded
    with a black-box CNN and outputs SMPL [2] human model parameters widehatboldsymbolPsi
    , which is used to reconstruct a SMPL human mesh mathcal widehatMH . Using white-box
    image reconstruction components DMR and PMR, it refines the pose estimate and
    outputs a pressure map. The pressure map features are calibrated with CAL to produce
    a contact pressure estimate mathcal widehatP . The two module design refine estimates
    with initial and residual stages; subscripts 1 and 2 indicate estimates from each
    module, respectively.
  Figure 7 Link: articels_figures_by_rev_year\2022\BodyPressure__Inferring_Body_Pose_and_Contact_Pressure_From_a_Depth_Image\figure_7.jpg
  Figure 7 caption: Differentiable white-box depth and pressure map reconstruction.
    DMR computes a linear depth map mathcal D+ between the height of the camera and
    the top surface of the human mesh (left). PMR computes a linear pressure map mathcal
    P+ between the undeformed height of the surface of the bed and the human mesh
    (right). Variable b is the distance between the camera and the bed.
  Figure 8 Link: articels_figures_by_rev_year\2022\BodyPressure__Inferring_Body_Pose_and_Contact_Pressure_From_a_Depth_Image\figure_8.jpg
  Figure 8 caption: 'Results: Inferring pose, contact pressure, and localized pressure
    distribution from depth using BPWnet. Showing real data with people occluded by
    blankets in the depth images. All cases are from the last 22 test subjects in
    the SLP dataset; white coverings indicate cover 1 and black indicate cover 2.
    The far right renderings in each group are a mirror flip because they show the
    pressure distribution underneath the body; the top shows an inferred pose while
    the bottom shows a pose from the SLP-3Dfits annotations.'
  Figure 9 Link: articels_figures_by_rev_year\2022\BodyPressure__Inferring_Body_Pose_and_Contact_Pressure_From_a_Depth_Image\figure_9.jpg
  Figure 9 caption: SMPL segmentation into pressure injury risk areas.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Henry M. Clever
  Name of the last author: Charles C. Kemp
  Number of Figures: 11
  Number of Tables: 5
  Number of authors: 4
  Paper title: BodyPressure - Inferring Body Pose and Contact Pressure From a Depth
    Image
  Publication Date: 2022-03-28 00:00:00
  Table 1 caption: TABLE 1 Human Pose and Body Shape Dataset Partitions
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Human Pose Estimation (Pose), Contact Pressure (P. Img.),
    and Pressure Distribution (v2vP) Error Results When Evaluating on the 22 Subject
    Test Set
  Table 3 caption: TABLE 3 Pressure Distribution - v2vP, MSE (kPa 2 2) Error Comparison
    Across Common Pressure Injury Risk Regions [19], [20]
  Table 4 caption: TABLE 4 Ablation Study - Evaluated on the 22 Subject Test Set
  Table 5 caption: TABLE 5 CAL Feature Calibration Test - Evaluated on the 22 Subject
    Test Set
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3158902
- Affiliation of the first author: department of electrical and computer engineering,
    national yang ming chiao tung university, hsinchu, taiwan
  Affiliation of the last author: department of electrical and computer engineering,
    national yang ming chiao tung university, hsinchu, taiwan
  Figure 1 Link: articels_figures_by_rev_year\2022\Learning_ContinuousTime_Dynamics_With_Attention\figure_1.jpg
  Figure 1 caption: ODE solver for implementation of continuous-time dynamics.
  Figure 10 Link: articels_figures_by_rev_year\2022\Learning_ContinuousTime_Dynamics_With_Attention\figure_10.jpg
  Figure 10 caption: Comparison of the continuous-time attention scores and the predicted
    classes (the first two rows) and true classes (the third row) of a different irregularly-sampled
    sequence by using ODE-RNN and ADN under three conditions. Classification results
    are divided into two halves due to the long sequence. Class labels are defined
    by 0 for walking, 1 for falling, 2 for lying, 3 for sitting, 4 for standing up,
    5 for on all fours, and 6 for sitting on the ground. HA dataset is used.
  Figure 2 Link: articels_figures_by_rev_year\2022\Learning_ContinuousTime_Dynamics_With_Attention\figure_2.jpg
  Figure 2 caption: Dynamic of hidden state z(t) for ODE solver in ODE-RNN.
  Figure 3 Link: articels_figures_by_rev_year\2022\Learning_ContinuousTime_Dynamics_With_Attention\figure_3.jpg
  Figure 3 caption: Continuous-time state trajectory z(t) for x t n using ODE-RNN.
  Figure 4 Link: articels_figures_by_rev_year\2022\Learning_ContinuousTime_Dynamics_With_Attention\figure_4.jpg
  Figure 4 caption: Calculation of continuous-time attention where the dot-product
    of continuous-time query vector mathbf q (widetildet) and state trajectory mathbf
    z (t) is calculated to find attention score alpha (widetildet,t) and then integrated
    it with mathbf z (t) via integral to calculate the context vector mathbf c (widetildet)
    . widetildet is the time for query mathbf q (widetildet) .
  Figure 5 Link: articels_figures_by_rev_year\2022\Learning_ContinuousTime_Dynamics_With_Attention\figure_5.jpg
  Figure 5 caption: (a) Discrete-time attention alpha tn tm is computed by taking
    summation of dot-products between query and states at discrete time instants lbrace
    t1,t2,t3rbrace . (b) Continuous-time attention alpha (widetildet,t) is computed
    by taking integral and interpolating via ODE solver for all possible seen and
    unseen query instants tildet along continuous time t .
  Figure 6 Link: articels_figures_by_rev_year\2022\Learning_ContinuousTime_Dynamics_With_Attention\figure_6.jpg
  Figure 6 caption: ODE solver for implementation of continuous-time attention.
  Figure 7 Link: articels_figures_by_rev_year\2022\Learning_ContinuousTime_Dynamics_With_Attention\figure_7.jpg
  Figure 7 caption: Dynamics for ODE solver in attentive differential network.
  Figure 8 Link: articels_figures_by_rev_year\2022\Learning_ContinuousTime_Dynamics_With_Attention\figure_8.jpg
  Figure 8 caption: Dynamics for ODE solver in causal attentive differential network.
  Figure 9 Link: articels_figures_by_rev_year\2022\Learning_ContinuousTime_Dynamics_With_Attention\figure_9.jpg
  Figure 9 caption: "Comparison of continuous-time attention scores of an irregularly-sampled\
    \ sequence, shown by darkness of red, under three conditions. \xD7 reveals the\
    \ samples at irregular time points. HA dataset is used."
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.64
  Name of the first author: Jen-Tzung Chien
  Name of the last author: Yi-Hsiang Chen
  Number of Figures: 14
  Number of Tables: 4
  Number of authors: 2
  Paper title: Learning Continuous-Time Dynamics With Attention
  Publication Date: 2022-03-28 00:00:00
  Table 1 caption: 'TABLE 1 Comparison for the Properties of Different Differential
    Networks (Gen: Generative Model, Seq: Sequential Learning, E-D: Encoder-Decoder
    Structure, RS: Recurrent Structure, Att: Continuous-Time Attention, Dyn: No. of
    Dynamics (or Differential Equations), Sv: No. of Solvers).'
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Comparison of Accuracy (%), Number of Parameters, and Memory
    and Computation Costs Relative to ODE-RNN for Action Recognition Using HA
  Table 3 caption: TABLE 3 Comparison of Accuracy (%), F1-Score and mF1-Score for
    Emotion Recognition Using MELD
  Table 4 caption: "TABLE 4 Comparison of AUC and AUPRC for Morality Prediction and\
    \ Test MSE ( \xD7 10 \u22123 \xD710-3) for Interpolation Task Using PhysioNet"
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3162711
- Affiliation of the first author: school of electronics, wuhan university, wuhan,
    china
  Affiliation of the last author: school of computer science, wuhan university, wuhan,
    china
  Figure 1 Link: articels_figures_by_rev_year\2022\Learning_to_Extract_Building_Footprints_From_OffNadir_Aerial_Images\figure_1.jpg
  Figure 1 caption: An illustration of the BFE problem in off-nadir images. In contrast
    with those in near-nadir images (a), for a building in off-nadir images (b), there
    are often non-negligible offsets between the projections of the roof and the footprint,
    and the boundary of the building footprint is usually partially visible due to
    the heavy occlusion by its facade. In this paper, we present a building instance
    model (c) by relating the occluded footprint to the visible roof of the building
    with a learned offset vector.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Learning_to_Extract_Building_Footprints_From_OffNadir_Aerial_Images\figure_2.jpg
  Figure 2 caption: The architecture of the proposed LOFT scheme (based on Mask R-CNN).
    The backbone is the ResNet with Feature Pyramid Network (FPN), and RPN stands
    for Region Proposal Network. The RoI Align layers produce the feature maps F b
    , F r , and F o to generate the building bounding box (B-Bbox), the roof mask
    (R-Mask), and the offset vector (Offset). The footprint mask (F-Mask) is finally
    computed with the predicted R-Mask and Offset in the inference stage.
  Figure 3 Link: articels_figures_by_rev_year\2022\Learning_to_Extract_Building_Footprints_From_OffNadir_Aerial_Images\figure_3.jpg
  Figure 3 caption: The architecture of the FOA module. Conv and FC denote the convolution
    (Conv) layer and fully connected (FC) layer, respectively. There are four branches,
    and the parameters of FC layers are shared. In the training stage, each branch
    rotates the input feature map F o by the given rotation angle and regresses the
    corresponding offset vector. In the inference stage, the offset vectors regressed
    by four branches are rotated inversely and fused to form the final offset vector.
  Figure 4 Link: articels_figures_by_rev_year\2022\Learning_to_Extract_Building_Footprints_From_OffNadir_Aerial_Images\figure_4.jpg
  Figure 4 caption: Several annotated samples in BONAI dataset geo-located in different
    cities. For every building, its roof (blue) and footprint (yellow) are labeled
    with polygonal masks, and the offset vector (red) records the translation between
    them.
  Figure 5 Link: articels_figures_by_rev_year\2022\Learning_to_Extract_Building_Footprints_From_OffNadir_Aerial_Images\figure_5.jpg
  Figure 5 caption: Qualitative comparison between the prior arts of BFE and our method
    on BONAI dataset. Best viewed in color and zoomed-in view.
  Figure 6 Link: articels_figures_by_rev_year\2022\Learning_to_Extract_Building_Footprints_From_OffNadir_Aerial_Images\figure_6.jpg
  Figure 6 caption: Qualitative comparison between LOFT and LOFT w FOA. Best viewed
    in color and zoomed-in view.
  Figure 7 Link: articels_figures_by_rev_year\2022\Learning_to_Extract_Building_Footprints_From_OffNadir_Aerial_Images\figure_7.jpg
  Figure 7 caption: Some typical failure cases of LOFT on BONAI dataset. Blue and
    red mean prediction and ground truth, respectively.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.97
  Name of the first author: Jinwang Wang
  Name of the last author: Gui-Song Xia
  Number of Figures: 7
  Number of Tables: 9
  Number of authors: 6
  Paper title: Learning to Extract Building Footprints From Off-Nadir Aerial Images
  Publication Date: 2022-03-28 00:00:00
  Table 1 caption: TABLE 1 The Quantity of Images and Building Instances Contained
    by BONAI
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Quantitative Comparison of the Baselines and Our Methods
    on the Test Set of BONAI Dataset
  Table 3 caption: TABLE 3 The Influence of Offset Head on BONAI Dataset (%)
  Table 4 caption: TABLE 4 The Influence of the FOA Module on BONAI Dataset (%)
  Table 5 caption: TABLE 5 F1-Scores of Different Rotation Angle Sets in the FOA Module
  Table 6 caption: TABLE 6 Results of Different Network Design Choices of the FOA
  Table 7 caption: TABLE 7 Comparison of Image-Level Rotation Augmentation (IRA) With
    the FOA Module
  Table 8 caption: TABLE 8 The End-Point Error Comparison of LOFT With or Without
    FOA
  Table 9 caption: TABLE 9 The F1-Scores of Roof and Footprint Extraction by Using
    LOFT or Mask R-CNN
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3162583
