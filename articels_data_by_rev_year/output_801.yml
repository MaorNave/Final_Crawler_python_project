- Affiliation of the first author: Not Available
  Affiliation of the last author: Not Available
  Figure 1 Link: Not Available
  Figure 1 caption: Not Available
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Kristen Grauman
  Name of the last author: Antonio Torralba
  Number of Figures: Not Available
  Number of Tables: 0
  Number of authors: 3
  Paper title: 'Guest Editorial: Best of CVPR 2015'
  Publication Date: 2017-03-03 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2663859
- Affiliation of the first author: center of imaging science, johns hopkins university,
    baltimore, md
  Affiliation of the last author: center of imaging science, johns hopkins university,
    baltimore, md
  Figure 1 Link: Not Available
  Figure 1 caption: Not Available
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Manolis C. Tsakiris
  Name of the last author: "Ren\xE9 Vidal"
  Number of Figures: Not Available
  Number of Tables: 0
  Number of authors: 2
  Paper title: Algebraic Clustering of Affine Subspaces
  Publication Date: 2017-03-06 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2678477
- Affiliation of the first author: institute of intelligent machines, chinese academy
    of sciences, hefei, people's republic of china
  Affiliation of the last author: center for research on intelligent perception and
    computing national laboratory of pattern recognition institute of automation,
    chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2017\Fast_Supervised_Discrete_Hashing\figure_1.jpg
  Figure 1 caption: Precisionsample=500 as functions of the number of hashing bits
    (16, 32, 64, 96, 128) on the CIFAR-10 database.
  Figure 10 Link: articels_figures_by_rev_year\2017\Fast_Supervised_Discrete_Hashing\figure_10.jpg
  Figure 10 caption: MAP as functions of the number of hashing bits (16, 32, 64, 128,
    256) on the FRGC database.
  Figure 2 Link: articels_figures_by_rev_year\2017\Fast_Supervised_Discrete_Hashing\figure_2.jpg
  Figure 2 caption: Precision of Hamming radius 2 as functions of the number of hashing
    bits (16, 32, 64, 96, 128) on the CIFAR-10 database.
  Figure 3 Link: articels_figures_by_rev_year\2017\Fast_Supervised_Discrete_Hashing\figure_3.jpg
  Figure 3 caption: Accuracy as functions of the number of hashing bits (16, 32, 64,
    96, 128) on the CIFAR-10 database.
  Figure 4 Link: articels_figures_by_rev_year\2017\Fast_Supervised_Discrete_Hashing\figure_4.jpg
  Figure 4 caption: Precisionsample=500 as functions of the number of hashing bits
    (16, 32, 64, 96, 128) on the MNIST database.
  Figure 5 Link: articels_figures_by_rev_year\2017\Fast_Supervised_Discrete_Hashing\figure_5.jpg
  Figure 5 caption: Precision of Hamming radius 2 as functions of the number of hashing
    bits (16, 32, 64, 96, 128) on the MNIST database.
  Figure 6 Link: articels_figures_by_rev_year\2017\Fast_Supervised_Discrete_Hashing\figure_6.jpg
  Figure 6 caption: Recall of Hamming radius 2 as functions of the number of hashing
    bits (16, 32, 64, 96, 128) on the MNIST database.
  Figure 7 Link: articels_figures_by_rev_year\2017\Fast_Supervised_Discrete_Hashing\figure_7.jpg
  Figure 7 caption: F-measure of Hamming radius 2 as functions of the number of hashing
    bits (16, 32, 64, 96, 128) on the MNIST database.
  Figure 8 Link: articels_figures_by_rev_year\2017\Fast_Supervised_Discrete_Hashing\figure_8.jpg
  Figure 8 caption: MAP as functions of the number of hashing bits (16, 32, 64, 96,
    128) on the MNIST database.
  Figure 9 Link: articels_figures_by_rev_year\2017\Fast_Supervised_Discrete_Hashing\figure_9.jpg
  Figure 9 caption: Accuracy as functions of the number of hashing bits (16, 32, 64,
    96, 128) on the MNIST database.
  First author gender probability: 0.88
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jie Gui
  Name of the last author: Tieniu Tan
  Number of Figures: 11
  Number of Tables: 3
  Number of authors: 5
  Paper title: Fast Supervised Discrete Hashing
  Publication Date: 2017-03-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Experimental Results on the CIFAR-10 Database When the Number
      of Hashing Bits Is 128
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Experimental Results on the MNIST Database When the Number
      of Hashing Bits Is 64
  Table 3 caption:
    table_text: TABLE 3 Experimental Results on the FRGC Face Database When the Number
      of Hashing Bits Is 32
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2678475
- Affiliation of the first author: peking university, beijing, china
  Affiliation of the last author: peking university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2017\MultiTask_Learning_with_Low_Rank_Attribute_Embedding_for_MultiCamera_Person_ReId\figure_1.jpg
  Figure 1 caption: Illustration of low rank attribute embedding with three attribute
    vectors from task T 1 as examples. With the learned transformation matrix, the
    original binary attributes are converted to continuous attributes. Semantically
    related attributes are recovered even though they are absent in the original attribute
    vectors, i.e., the attribute female is non-zero in the embedded attribute vector
    due to the presence of both skirt and handbag, even though its value is 0 in the
    original attribute vector a 1 3 .
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\MultiTask_Learning_with_Low_Rank_Attribute_Embedding_for_MultiCamera_Person_ReId\figure_2.jpg
  Figure 2 caption: Illustration of our MTL-LORAE framework.
  Figure 3 Link: articels_figures_by_rev_year\2017\MultiTask_Learning_with_Low_Rank_Attribute_Embedding_for_MultiCamera_Person_ReId\figure_3.jpg
  Figure 3 caption: CMC curves of our approach and state-of-the-art approaches on
    the iLIDS-VID dataset (left) and PRID dataset (right).
  Figure 4 Link: articels_figures_by_rev_year\2017\MultiTask_Learning_with_Low_Rank_Attribute_Embedding_for_MultiCamera_Person_ReId\figure_4.jpg
  Figure 4 caption: CMC curves of our approach and 5 state-of-the-art approaches with
    attributes added on the PRID dataset.
  Figure 5 Link: articels_figures_by_rev_year\2017\MultiTask_Learning_with_Low_Rank_Attribute_Embedding_for_MultiCamera_Person_ReId\figure_5.jpg
  Figure 5 caption: The values of objective function during optimization on the iLIDS-VID
    dataset (top) and PRID dataset (bottom).
  Figure 6 Link: articels_figures_by_rev_year\2017\MultiTask_Learning_with_Low_Rank_Attribute_Embedding_for_MultiCamera_Person_ReId\figure_6.jpg
  Figure 6 caption: Examples of attribute correlations learned on the PRID dataset.
    The averaged correlation and mean absolute error across different persons are
    shown in each example.
  Figure 7 Link: articels_figures_by_rev_year\2017\MultiTask_Learning_with_Low_Rank_Attribute_Embedding_for_MultiCamera_Person_ReId\figure_7.jpg
  Figure 7 caption: Examples of attribute correlations learned on the VIPeR dataset.
    The averaged correlation and mean absolute error across different persons are
    shown in each example.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.79
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Chi Su
  Name of the last author: Wen Gao
  Number of Figures: 7
  Number of Tables: 12
  Number of authors: 6
  Paper title: Multi-Task Learning with Low Rank Attribute Embedding for Multi-Camera
    Person Re-Identification
  Publication Date: 2017-03-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 CMC Scores of Ranks from 1 to 50 on the iLIDS-VID Dataset
  Table 10 caption:
    table_text: "TABLE 10 CMC Scores at Ranks 1, 5 and 10 by MTL-LOREA with Varying\
      \ \u03B3 (Importance of Attribute Embedding Error Term) and \u03BB (Sparse Regularization)\
      \ on iLIDS-VID Dataset"
  Table 2 caption:
    table_text: TABLE 2 CMC Scores of Ranks from 1 to 50 on the PRID Dataset
  Table 3 caption:
    table_text: TABLE 3 CMC Scores of Our Approach and Five State-of-the-Art Approaches
      with Attributes Added at Ranks from 1 to 50 on the PRID Dataset
  Table 4 caption:
    table_text: TABLE 4 CMC Scores of Ranks from 1 to 20 on the VIPeR Dataset
  Table 5 caption:
    table_text: TABLE 5 Comparison of Precision, Recall and F 1 -Score (in %) by Existing
      Methods and Our Approach on SAIVT-SoftBio Dataset
  Table 6 caption:
    table_text: TABLE 6 Comparison of Precision, Recall and F 1 -Score (in %) Regarding
      All Camera Pairs by Existing Methods and Our Approach on SAIVT-SoftBio Dataset
  Table 7 caption:
    table_text: TABLE 7 CMC Scores of MTL-LOREA with Deep Features, i.e., Percentage
      (%) of Correct Matches, of Ranks 1, Rank 5, Rank 10 and Rank 20 on the iLIDS-VID
      Dataset, PRID Dataset and VIPeR Dataset
  Table 8 caption:
    table_text: TABLE 8 Comparison of Precision, Recall and F 1 -Score (in %) Regarding
      All Camera Pairs by Our Approach with Deep Features on SAIVT-SoftBio Dataset
  Table 9 caption:
    table_text: TABLE 9 CMC Scores of Ranks from 1 to 50 on the iLIDS-VID, PRID and
      VIPeR Datasets by STL, MTL-Att, MTL-FR and the Complete MTL-LOREA
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2679002
- Affiliation of the first author: department of computer science, cornell university,
    ithaca, ny
  Affiliation of the last author: brain of things inc., redwood city, ca
  Figure 1 Link: articels_figures_by_rev_year\2017\WatchnPatch_Unsupervised_Learning_of_Actions_and_Relations\figure_1.jpg
  Figure 1 caption: Our Watch-Bot understands what human is currently doing by automatically
    segmenting the composite activity into basic level actions. We propose a completely
    unsupervised approach to modeling the human skeleton and object features to the
    actions, as well as the pairwise action co-occurrence and temporal relations.
    Using the learned model, our robot detects humans' forgotten actions and reminds
    them by pointing out the related object using the laser spot.
  Figure 10 Link: articels_figures_by_rev_year\2017\WatchnPatch_Unsupervised_Learning_of_Actions_and_Relations\figure_10.jpg
  Figure 10 caption: Forgotten actionobject detection accuracy varied with the number
    of action-topics in the 'office' dataset.
  Figure 2 Link: articels_figures_by_rev_year\2017\WatchnPatch_Unsupervised_Learning_of_Actions_and_Relations\figure_2.jpg
  Figure 2 caption: Video representation. (1) A video frames ( f i ) is first decomposed
    into a sequence of overlapping fixed-length temporal clips. (2) The human-skeleton-trajectoriesinteractive-object-trajectories
    are extracted from each clip, and we cluster them to form the human-dictionaryobject-dictionary.
    (3) Then the video is represented as a sequence of human-word and object-word
    indices by mapping its human-skeleton-trajectoriesinteractive-object-trajectories
    to the nearest human-wordsobject-words in the dictionary. (4) An activity video
    is about a set of action-topicsobject-topics indicating which actions are present
    and which types of objects are interacting with. (5) We learn the mapping of action-wordsobject-words
    to the action-topicsobject-topics, as well as the co-occurrence and the temporal
    relations between the topics. (6) We assign the topics to clips using the learned
    model.
  Figure 3 Link: articels_figures_by_rev_year\2017\WatchnPatch_Unsupervised_Learning_of_Actions_and_Relations\figure_3.jpg
  Figure 3 caption: 'Examples of the human skeletons (red line) and the extracted
    interacting objects (green mask, left: fridge, right: book).'
  Figure 4 Link: articels_figures_by_rev_year\2017\WatchnPatch_Unsupervised_Learning_of_Actions_and_Relations\figure_4.jpg
  Figure 4 caption: The graphic model of our causal topic model.
  Figure 5 Link: articels_figures_by_rev_year\2017\WatchnPatch_Unsupervised_Learning_of_Actions_and_Relations\figure_5.jpg
  Figure 5 caption: The relative time distributions learned by our model on training
    set (the blue dashed line) and the ground-truth histogram of the relative time
    over the whole dataset (the green solid line).
  Figure 6 Link: articels_figures_by_rev_year\2017\WatchnPatch_Unsupervised_Learning_of_Actions_and_Relations\figure_6.jpg
  Figure 6 caption: Examples of composite activity videos. Our model is able to consider
    complex action relations using pairwise action co-occurrence and temporal modeling.
  Figure 7 Link: articels_figures_by_rev_year\2017\WatchnPatch_Unsupervised_Learning_of_Actions_and_Relations\figure_7.jpg
  Figure 7 caption: (a). Our Watch-Bot system. It consists of a Kinect v2 sensor that
    inputs RGB-D frames of human actions, a laptop that infers the forgotten action
    and the related object, a pantilt camera that localizes the object, mounted with
    a fixed laser pointer that points out the object. (b). The system pipeline. The
    robot first uses the learned model to infer the forgotten action and the related
    object based on the Kinect's input. Then it maps the view from the Kinect to the
    pantilt camera so that the bounding box of the object is mapped in the camera's
    view. Finally, the camera moves until the laser spot lies in the bounding box
    of the target object.
  Figure 8 Link: articels_figures_by_rev_year\2017\WatchnPatch_Unsupervised_Learning_of_Actions_and_Relations\figure_8.jpg
  Figure 8 caption: Illustration of patched action and object inference using our
    model. Given a test video, we infer the forgotten action-topic and object-topic
    in each segmentation point ( t 1 , t 2 as above). Then we select the top segment
    from the inferred action-topic's segment cluster with the inferred object-topic
    with the maximum patchscore .
  Figure 9 Link: articels_figures_by_rev_year\2017\WatchnPatch_Unsupervised_Learning_of_Actions_and_Relations\figure_9.jpg
  Figure 9 caption: Segmentation AccAP varied with the number of topics in the 'office'
    test dataset.
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Chenxia Wu
  Name of the last author: Ashutosh Saxena
  Number of Figures: 12
  Number of Tables: 3
  Number of authors: 6
  Paper title: 'Watch-n-Patch: Unsupervised Learning of Actions and Relations'
  Publication Date: 2017-03-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Notations in Our Model
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results Using the Same Number of Topics as the Ground-Truth
      Action Classes
  Table 3 caption:
    table_text: TABLE 3 Robotic Experiment Results
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2679054
- Affiliation of the first author: computer science & engineering, michigan state
    university, east lansing, mi
  Affiliation of the last author: computer science & engineering, michigan state university,
    east lansing, mi
  Figure 1 Link: articels_figures_by_rev_year\2017\Clustering_Millions_of_Faces_by_Identity\figure_1.jpg
  Figure 1 caption: Diagram of a possible clustering configuration, used to illustrate
    evaluation metrics. Six samples are partitioned into two clusters; A1, A2, and
    A3 are labeled with the same identity, sample B1 is labeled with a different identity,
    and samples U1 and U2 are unlabeled.
  Figure 10 Link: articels_figures_by_rev_year\2017\Clustering_Millions_of_Faces_by_Identity\figure_10.jpg
  Figure 10 caption: Pairwise precision versus the proposed internal quality measure,
    for all clusters generated by the proposed Approximate Rank-Order clustering algorithm
    on the full LFW dataset. Points in blue are clusters of size 3 or larger, points
    in red are of size 2. The highlighted set of points on the left edge of the figure
    are all of size 2, with zero pairwise precision. Since we can't reliably distinguish
    between good and bad two-item clusters, we discard them from consideration.
  Figure 2 Link: articels_figures_by_rev_year\2017\Clustering_Millions_of_Faces_by_Identity\figure_2.jpg
  Figure 2 caption: Face representation. An RGB image is input (a), keypoints are
    detected (b), the image is normalized following the procedure described in [11]
    (c), the normalized image is input to a convolutional neural network (d), and
    the 320-dimensional output of the final average-pooling layer is used as the face
    representation (e). An N-way softmax classification layer (f) is used during training
    only.
  Figure 3 Link: articels_figures_by_rev_year\2017\Clustering_Millions_of_Faces_by_Identity\figure_3.jpg
  Figure 3 caption: Approximate rank-order clustering. Given a set of unlabeled face
    images (a), nearest neighbor lists are computed for each image (b); nearest neighbor
    lists are then used to compute distances between faces (c). (b) shows the nearest
    neighbor lists of only five faces in (a). d m (a,b) (Eq. (5)) is the asymmetric
    distance between faces a and b whereas D m (a,b) (Eq. (6)) is the symmetric distance
    between faces a and b .
  Figure 4 Link: articels_figures_by_rev_year\2017\Clustering_Millions_of_Faces_by_Identity\figure_4.jpg
  Figure 4 caption: Example face images from the a) LFW, b) Youtube Faces, c) Webfaces,
    and d) CASIA-webface datasets.
  Figure 5 Link: articels_figures_by_rev_year\2017\Clustering_Millions_of_Faces_by_Identity\figure_5.jpg
  Figure 5 caption: "Examples of \u201Cpure\u201D (single individual) clusters (a,\
    \ b), and \u201Cimpure\u201D (multiple individuals) clusters (c,d) generated by\
    \ the proposed Approximate Rank-Order clustering on the entire LFW dataset. Faces\
    \ not belonging to the majority identity in each cluster are outlined in red."
  Figure 6 Link: articels_figures_by_rev_year\2017\Clustering_Millions_of_Faces_by_Identity\figure_6.jpg
  Figure 6 caption: F-measures attained by approximate rank-order clustering at different
    effective numbers of clusters (generated by varying the distance threshold).
  Figure 7 Link: articels_figures_by_rev_year\2017\Clustering_Millions_of_Faces_by_Identity\figure_7.jpg
  Figure 7 caption: Numbers of times each face in the LFW database appeared in any
    other face's top-200 nearest neighbor list for a) the exact nearest neighbors,
    and b) the nearest neighbors computed via randomized k-d tree approximation.
  Figure 8 Link: articels_figures_by_rev_year\2017\Clustering_Millions_of_Faces_by_Identity\figure_8.jpg
  Figure 8 caption: Example image pairs flagged at five std. dev. below the mean genuine
    score on LFW. (a) a genuine LFW match, (b), an impostor LFW match, (c) a near
    duplicate in the background dataset, and (d) poor quality images in the background
    dataset.
  Figure 9 Link: articels_figures_by_rev_year\2017\Clustering_Millions_of_Faces_by_Identity\figure_9.jpg
  Figure 9 caption: Example images from clusters generated from the YTF dataset. a)
    two clusters, each containing frames from one video of the same subject, b) a
    cluster containing frames from two videos of the same subject, where the background
    for the video is apparently identical, c) a subset of a cluster containing 28
    different identities; many of these images are poorly lit.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Charles Otto
  Name of the last author: Anil K. Jain
  Number of Figures: 13
  Number of Tables: 7
  Number of authors: 3
  Paper title: Clustering Millions of Faces by Identity
  Publication Date: 2017-03-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 A Summary of Related Studies on Face Clustering
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Clustering Results on the Complete LFW Dataset
  Table 3 caption:
    table_text: TABLE 3 Clustering Results on the LFW Dataset, with Approximate Rank-Order
      Clustering, and LFW with Additional 1 Million Web-Downloaded Face Images
  Table 4 caption:
    table_text: TABLE 4 Clustering Results Using Approximate Rank-Order Clustering
      on the LFW Dataset with Increasing Amounts of Augmented Data, and Different
      Search Size Strategies for the Approximate Nearest Neighbor Calculations
  Table 5 caption:
    table_text: TABLE 5 Large-Scale Clustering Results Using the Proposed Approximate
      Rank-Order Clustering, with Randomized K-D Tree Nearest Neighbor Approximation
  Table 6 caption:
    table_text: TABLE 6 YTF Clustering Results Using the Proposed Approximate Rank-Order
      Clustering for Clustering All Frames in the Dataset, and for Randomly Sampling
      Three Frames per Video
  Table 7 caption:
    table_text: TABLE 7 Correlation of Internal Cluster Quality Measures with Precision,
      for the LFW Dataset
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2679100
- Affiliation of the first author: intelligent systems lab amsterdam, university of
    amsterdam, science park 904, amsterdam, xh, the netherlands
  Affiliation of the last author: "computer vision center, universitat aut\xF2noma\
    \ de barcelona, barcelona, spain"
  Figure 1 Link: articels_figures_by_rev_year\2017\ExpressionInvariant_Age_Estimation_Using_Structured_Learning\figure_1.jpg
  Figure 1 caption: Our graphical model to jointly learn the age and the expression.
    x represents the feature vector, h denotes the latent variables, y a and y e are
    the corresponding age and expression respectively. Note that, while all x i are
    connected with y a and y e , we do not show these connections in this figure for
    the sake of clarity.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\ExpressionInvariant_Age_Estimation_Using_Structured_Learning\figure_2.jpg
  Figure 2 caption: 'Example faces and age distributions of FACES datasets. The expression
    distribution for FACES dataset is uniform where each subject shows six basic expressions:
    Neutrality, happiness, anger, fear, disgust, and sadness.'
  Figure 3 Link: articels_figures_by_rev_year\2017\ExpressionInvariant_Age_Estimation_Using_Structured_Learning\figure_3.jpg
  Figure 3 caption: Example faces and age distributions of Lifespan datasets. The
    Lifespan dataset contains neutral (580) and happy (258) faces.
  Figure 4 Link: articels_figures_by_rev_year\2017\ExpressionInvariant_Age_Estimation_Using_Structured_Learning\figure_4.jpg
  Figure 4 caption: Example faces and age distributions of NEMO datasets. The NEMO
    dataset contains neutral (995) and happy (1,090) faces.
  Figure 5 Link: articels_figures_by_rev_year\2017\ExpressionInvariant_Age_Estimation_Using_Structured_Learning\figure_5.jpg
  Figure 5 caption: Average face regions corresponding to different hidden states
    (from left to right) for the bottom and top face regions.
  Figure 6 Link: articels_figures_by_rev_year\2017\ExpressionInvariant_Age_Estimation_Using_Structured_Learning\figure_6.jpg
  Figure 6 caption: Result with different cardinality of hidden states on FACES (left),
    Lifespan (middle) and NEMO (right) datasets.
  Figure 7 Link: articels_figures_by_rev_year\2017\ExpressionInvariant_Age_Estimation_Using_Structured_Learning\figure_7.jpg
  Figure 7 caption: Results for varying R on the FACES, Lifespan and NEMO datasets
    for age estimation (left) and expression recognition (right).
  Figure 8 Link: articels_figures_by_rev_year\2017\ExpressionInvariant_Age_Estimation_Using_Structured_Learning\figure_8.jpg
  Figure 8 caption: 'The structures we evaluated in the experiments. From left to
    right: 1times 3 , 2times 3 , 2times 2 .'
  Figure 9 Link: articels_figures_by_rev_year\2017\ExpressionInvariant_Age_Estimation_Using_Structured_Learning\figure_9.jpg
  Figure 9 caption: The graphical model to jointly learn the age, expression, and
    gender. mathbf x represents the feature vector, mathbf h denotes the latent variables,
    ya , ye and yg are the corresponding age, expression and gender respectively.
    Note that, while all xi are connected with ya , ye and yg , these connections
    are not shown for the sake of clarity.
  First author gender probability: 0.89
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Zhongyu Lou
  Name of the last author: Theo Gevers
  Number of Figures: 9
  Number of Tables: 12
  Number of authors: 5
  Paper title: Expression-Invariant Age Estimation Using Structured Learning
  Publication Date: 2017-03-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Expression-Independent and Expression-Joint Learning Are Evaluated
      on FACES and Lifespan Datasets
  Table 10 caption:
    table_text: TABLE 10 Computational Complexity (in Seconds) of the Proposed Algorithm
      (Joint-Learn) and Independent Learning (Indep-Learn)
  Table 2 caption:
    table_text: TABLE 2 Age Estimation Error for Each Expression Subset on FACES Dataset
  Table 3 caption:
    table_text: TABLE 3 Age Estimation Error for Each Expressions Subset on Lifespan
      Dataset
  Table 4 caption:
    table_text: TABLE 4 Age Estimation Error for Each Expressions Subset on NEMO Dataset
  Table 5 caption:
    table_text: TABLE 5 Expression Recognition Using Age-Joint and Age-Independent
      Learning Evaluated on FACES, Lifespan and NEMO Dataset
  Table 6 caption:
    table_text: TABLE 6 Expression Recognition Accuracy for Each Expression Subset
      on FACES Dataset
  Table 7 caption:
    table_text: TABLE 7 Expression Recognition Accuracy for Each Expression Subset
      on Lifespan Dataset
  Table 8 caption:
    table_text: TABLE 8 Expression Recognition Accuracy for Each Expression Subset
      on NEMO Dataset
  Table 9 caption:
    table_text: TABLE 9 Age and Expression Estimation Using Different Structures on
      FACES, Lifespan and NEMO Datasets
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2679739
- Affiliation of the first author: eecs department, university of california, berkeley,
    berkeley, ca
  Affiliation of the last author: cse department, university of california, san diego,
    la jolla, ca
  Figure 1 Link: articels_figures_by_rev_year\2017\SVBRDFInvariant_Shape_and_Reflectance_Estimation_from_a_LightField_Camera\figure_1.jpg
  Figure 1 caption: Comparison of depth estimation results of different algorithms.
    We texture map the input image onto the depth maps, so we can clearly see where
    each method fails. It can be seen that our method correctly handles the glossy
    surface, while other methods generate visible artifacts, especially around the
    specular parts.
  Figure 10 Link: articels_figures_by_rev_year\2017\SVBRDFInvariant_Shape_and_Reflectance_Estimation_from_a_LightField_Camera\figure_10.jpg
  Figure 10 caption: Shape and reflectance estimation results on a more complicated
    shape for example materials in the MERL dataset. For shape estimation, the upper-left
    shows the recovered depth, while the lower-right shows the error percentage (hotter
    color means larger error). For reflectance estimation, we show the recovered BRDF
    compared to ground truth curves.
  Figure 2 Link: articels_figures_by_rev_year\2017\SVBRDFInvariant_Shape_and_Reflectance_Estimation_from_a_LightField_Camera\figure_2.jpg
  Figure 2 caption: Optical flow for glossy surfaces. (a) The difference between two
    images at the same pixel position, is (b) the view change plus (c) the spatial
    change. (d) Summing these two changes gives the overall change.
  Figure 3 Link: articels_figures_by_rev_year\2017\SVBRDFInvariant_Shape_and_Reflectance_Estimation_from_a_LightField_Camera\figure_3.jpg
  Figure 3 caption: Comparison between cameras focused at infinity and focused at
    some finite depth.
  Figure 4 Link: articels_figures_by_rev_year\2017\SVBRDFInvariant_Shape_and_Reflectance_Estimation_from_a_LightField_Camera\figure_4.jpg
  Figure 4 caption: (a) The BRDF invariant (20) result and the depth result when the
    cameras are focused at some finite distance but the formula for infinity focus
    (39) is used. Note that the BRDF invariant differs from zero by a large amount,
    and the depth reconstruction is far from accurate. (b) The results when the formula
    for finite focus (41) is used.
  Figure 5 Link: articels_figures_by_rev_year\2017\SVBRDFInvariant_Shape_and_Reflectance_Estimation_from_a_LightField_Camera\figure_5.jpg
  Figure 5 caption: (a) Depth error versus number of cameras (virtual viewpoints)
    used. We add Gaussian noise of variance 10-4 on a synthetic sphere and test the
    performance when different numbers of cameras are used, from three to 81 ( 9 times
    9 array). Although theoretically three cameras are enough, depth recovery is very
    sensitive to noise. As the number of cameras increases, the system becomes more
    robust. (b) Depth error versus camera baseline. Our method performs the best when
    the baseline is between 0.01 cm to about 0.5 cm.
  Figure 6 Link: articels_figures_by_rev_year\2017\SVBRDFInvariant_Shape_and_Reflectance_Estimation_from_a_LightField_Camera\figure_6.jpg
  Figure 6 caption: Depth errors on different material types. Our method achieves
    good results on all materials except fabric. For all material types, we outperform
    the other methods.
  Figure 7 Link: articels_figures_by_rev_year\2017\SVBRDFInvariant_Shape_and_Reflectance_Estimation_from_a_LightField_Camera\figure_7.jpg
  Figure 7 caption: Shape and reflectance estimation results on a spatially-varying
    example in the MERL dataset. (a) Two materials, alum bronze and green metal, are
    blended linearly from left to right. We reconstruct (b) the depth and (c)(d) the
    BRDFs for each column, where two examples are shown at the red and green points
    specified in (a). Finally, we show a relighting example in (e). The error percentage
    compared to (f) the ground truth is 3.20 percent.
  Figure 8 Link: articels_figures_by_rev_year\2017\SVBRDFInvariant_Shape_and_Reflectance_Estimation_from_a_LightField_Camera\figure_8.jpg
  Figure 8 caption: (a) We apply Gaussians with different standard deviations ( sigma
    ) on the frequency domain to generate shapes with different smoothnesses, then
    plot the corresponding depth errors. (b) Example shapes for sigma =0.5 and sigma
    =1.5 .
  Figure 9 Link: articels_figures_by_rev_year\2017\SVBRDFInvariant_Shape_and_Reflectance_Estimation_from_a_LightField_Camera\figure_9.jpg
  Figure 9 caption: Shape and reflectance estimation results on a sphere for example
    materials in the MERL dataset. For shape estimation, the upper-left shows the
    recovered depth, while the lower-right shows the error percentage (hotter color
    means larger error). For reflectance estimation, we show the recovered BRDF compared
    to ground truth curves.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ting-Chun Wang
  Name of the last author: Ravi Ramamoorthi
  Number of Figures: 12
  Number of Tables: 0
  Number of authors: 4
  Paper title: SVBRDF-Invariant Shape and Reflectance Estimation from a Light-Field
    Camera
  Publication Date: 2017-03-09 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2680442
