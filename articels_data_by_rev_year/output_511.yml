- Affiliation of the first author: department of computing, imperial college london,
    united kingdom
  Affiliation of the last author: department of computing, imperial college london,
    united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2015\Robust_Correlated_and_Individual_Component_Analysis\figure_1.jpg
  Figure 1 caption: Application of RCITW and compared techinques to synthetic data.
    (a) Result visualisation, where the input spirals have been corrupted by sparse
    spike noise. The original sequences corrupted by noise are shown in (i), while
    in (ii) the time-warped latent space obtained via each method is shown. (b) Mean
    alignment error obtained by the CTW, the GTW, the RCTW and the RCITW, as a function
    of the percentage of corrupted samples on synthetic data.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Robust_Correlated_and_Individual_Component_Analysis\figure_2.jpg
  Figure 2 caption: Example data included in the CASIA HFB [4] (male and female subject,
    visual, infra-red and 3d) and CUHK [5] (female and male subject, visual and sketch)
    databases.
  Figure 3 Link: articels_figures_by_rev_year\2015\Robust_Correlated_and_Individual_Component_Analysis\figure_3.jpg
  Figure 3 caption: Recognition error obtained by the compared methods on the CASIA
    HFB and CUHK databases.
  Figure 4 Link: articels_figures_by_rev_year\2015\Robust_Correlated_and_Individual_Component_Analysis\figure_4.jpg
  Figure 4 caption: Action Unit alignment results comparing the RCITW, the RCTW, the
    CTW, and the GTW. (a) Error for apex phase. (b) Average error.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.99
  Name of the first author: Yannis Panagakis
  Name of the last author: Maja Pantic
  Number of Figures: 4
  Number of Tables: 4
  Number of authors: 4
  Paper title: Robust Correlated and Individual Component Analysis
  Publication Date: 2015-11-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison among the RCICA, the RCCA, the JIVE, the COBE,
      and the CCA on the Synthetic Data
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results for Predicting Interest from Emotion Dimensions in
      the SEMAINE Database, Using Facial Trackings (Face), Audio Cues (Audio), Feature-Level
      Fusion ( F l ), CCA-Based Fusion (CCA f ), RCICA Fusion (RCICA f ) and Other
      Compared Techniques
  Table 3 caption:
    table_text: TABLE 3 Detection Accuracy of Conflict on the Political Debate Data,
      Utilising Facial Trackings (Face), Audio Cues (Audio), Feature-Level Fusion
      ( F l ), CCA-Based Fusion (CCA f ), Robust CCA Fusion (R CICA f ) and Other
      Compared Techniques
  Table 4 caption:
    table_text: TABLE 4 Performance Comparison of Various Methods in Face Clustering
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2497700
- Affiliation of the first author: department of statistics, university of california,
    la
  Affiliation of the last author: department of statistics and computer science, university
    of california, la
  Figure 1 Link: articels_figures_by_rev_year\2015\Learning_AndOr_Model_to_Represent_Context_and_Occlusion_for_Car_Detection_and_Vi\figure_1.jpg
  Figure 1 caption: Illustration of varying complexities in car detection from four
    datasets. (a) The PASCAL VOC2007 car dataset [2] consists of single cars under
    different viewpoints but with less occlusion as pointed out in [5]. (b) The KITTI
    car benchmark [1] includes on-road cars captured by a camera mounted upon a driving
    car which have more occlusions but restricted viewpoints. (c) The Street-Parking
    car dataset [6] includes cars with heavy occlusions but less multi-car context
    and (d) The Parking-Lot car dataset [7] consists of cars with heavy occlusions
    and rich multi-car context. The proposed And-Or model is learned for car detection
    in all four datasets.
  Figure 10 Link: articels_figures_by_rev_year\2015\Learning_AndOr_Model_to_Represent_Context_and_Occlusion_for_Car_Detection_and_Vi\figure_10.jpg
  Figure 10 caption: 'Left and Middle: Training and testing samples from the synthetic
    dataset. Right: detection results of DPM and And-Or Structure.'
  Figure 2 Link: articels_figures_by_rev_year\2015\Learning_AndOr_Model_to_Represent_Context_and_Occlusion_for_Car_Detection_and_Vi\figure_2.jpg
  Figure 2 caption: Illustration of the statistical regularities of car occlusions
    and multi-car contextual patterns by CAD simulation. We represent car-to-car occlusion
    at semantic part level (left) and generate a large number of synthetic occlusion
    configurations (middle) w.r.t. four factors (car type, orientation, relative position
    and camera view). We represent the regularities of different combinations of part
    visibilities (i.e., occlusion configurations) by a hierarchical And-Or model.
    This model also represents multi-car contextual patterns (right) based on the
    geometric configurations of single cars.
  Figure 3 Link: articels_figures_by_rev_year\2015\Learning_AndOr_Model_to_Represent_Context_and_Occlusion_for_Car_Detection_and_Vi\figure_3.jpg
  Figure 3 caption: Illustration of our And-Or model for car detection. It represents
    multi-car contextual patterns and occlusion configurations jointly by modeling
    spatially-aligned multi-cars together and composing visible parts explicitly for
    single cars. (Best viewed in color).
  Figure 4 Link: articels_figures_by_rev_year\2015\Learning_AndOr_Model_to_Represent_Context_and_Occlusion_for_Car_Detection_and_Vi\figure_4.jpg
  Figure 4 caption: Illustration of generating multi-car positive samples.
  Figure 5 Link: articels_figures_by_rev_year\2015\Learning_AndOr_Model_to_Represent_Context_and_Occlusion_for_Car_Detection_and_Vi\figure_5.jpg
  Figure 5 caption: 'Left-Top: 2 -car context patterns on the KITTI dataset [1] and
    self-collected Parking Lot dataset. Each context pattern is represented by a specific
    color set, and each circle stands for the center of each cluster. Left-Bottom
    : Overlap ratio histograms of the KITTI dataset and the Parking Lot dataset (we
    show the occluded cases only). Right: some cropped examples with different occlusions.
    The two bounding boxes in a car pair are shown in red and blue respectively. (Best
    viewed in color).'
  Figure 6 Link: articels_figures_by_rev_year\2015\Learning_AndOr_Model_to_Represent_Context_and_Occlusion_for_Car_Detection_and_Vi\figure_6.jpg
  Figure 6 caption: 'Illustration of learning occlusion configurations. It consists
    of three components: (i) Generating occlusion configurations using CAD simulations
    with 17 semantic parts in total; (ii) Learning the initial And-Or structure based
    on the data matrix constructed from the simulated occlusion configurations. Each
    row of the data matrix represents an example and the columns represent the visibility
    of the 17 semantic parts (a whitegray entry denotes a part is visibleinvisible.
    Each example is represented by an And-node as one child of the root Or-node; (iii)
    Refining the initial And-Or structure using graph compression algorithm [57] to
    seek the consistently visible parts (e.g., X ) and optional part clusters (e.g.,
    Y and Z ).'
  Figure 7 Link: articels_figures_by_rev_year\2015\Learning_AndOr_Model_to_Represent_Context_and_Occlusion_for_Car_Detection_and_Vi\figure_7.jpg
  Figure 7 caption: 'Top: The distribution of overlap ratio and cars per image on
    the Street-Parking dataset. Bottom: Comparison of the average number of cars per
    image.'
  Figure 8 Link: articels_figures_by_rev_year\2015\Learning_AndOr_Model_to_Represent_Context_and_Occlusion_for_Car_Detection_and_Vi\figure_8.jpg
  Figure 8 caption: Precision-recall curves on the test subset splitted from the KITTI
    trainset (Left) and the Parking Lot dataset (Right).
  Figure 9 Link: articels_figures_by_rev_year\2015\Learning_AndOr_Model_to_Represent_Context_and_Occlusion_for_Car_Detection_and_Vi\figure_9.jpg
  Figure 9 caption: Examples of successful and failure cases by our model on the KITTI
    dataset (first three rows), the Parking Lot dataset (the fourth and fifth rows)
    and the Street Parking dataset (the last two rows). Best viewed in color and magnification.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.61
  Name of the first author: Tianfu Wu
  Name of the last author: Song-Chun Zhu
  Number of Figures: 11
  Number of Tables: 5
  Number of authors: 3
  Paper title: Learning And-Or Model to Represent Context and Occlusion for Car Detection
    and Viewpoint Estimation
  Publication Date: 2015-11-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance Comparison (in AP) on the KITTI Benchmark [1]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance Comparison (in AP) on the Street Parking Dataset
      [6]
  Table 3 caption:
    table_text: TABLE 3 Performance Comparison (in AP) on the PASCAL VOC 2007[2]
  Table 4 caption:
    table_text: TABLE 4 View Estimation on Pascal VOC 2006 Car Dataset [2] and 3D
      Car Dataset [3]
  Table 5 caption:
    table_text: TABLE 5 The Results of VDPM, DPM-VOC+VP and And-Or Structure on the
      PASCAL3D+ Car Dataset [4]
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2497699
- Affiliation of the first author: department of signal processing and communications,
    university carlos iii in madrid, spain
  Affiliation of the last author: technical staff at bell labs (alcatel-lucent), nj,
  Figure 1 Link: articels_figures_by_rev_year\2015\Infinite_Factorial_UnboundedState_Hidden_Markov_Model\figure_1.jpg
  Figure 1 caption: Graphical model of the nonbinary finite FHMM.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Infinite_Factorial_UnboundedState_Hidden_Markov_Model\figure_2.jpg
  Figure 2 caption: Graphical observation model for the nonbinary infinite factorial
    HMM.
  Figure 3 Link: articels_figures_by_rev_year\2015\Infinite_Factorial_UnboundedState_Hidden_Markov_Model\figure_3.jpg
  Figure 3 caption: Small scale experiment. (a) Autocorrelation plot for the IFUHMM.
    (b) Number of samples for the autocorrelation to fall below 0.1 .
  Figure 4 Link: articels_figures_by_rev_year\2015\Infinite_Factorial_UnboundedState_Hidden_Markov_Model\figure_4.jpg
  Figure 4 caption: Evolution of the log-likelihood.
  Figure 5 Link: articels_figures_by_rev_year\2015\Infinite_Factorial_UnboundedState_Hidden_Markov_Model\figure_5.jpg
  Figure 5 caption: Small scale experiment. Histograms of (a) M+ and (b) Q under the
    IFUHMM.
  Figure 6 Link: articels_figures_by_rev_year\2015\Infinite_Factorial_UnboundedState_Hidden_Markov_Model\figure_6.jpg
  Figure 6 caption: Histogram of Q under the IFUHMM.
  Figure 7 Link: articels_figures_by_rev_year\2015\Infinite_Factorial_UnboundedState_Hidden_Markov_Model\figure_7.jpg
  Figure 7 caption: Histogram of M+ under the IFUHMM.
  Figure 8 Link: articels_figures_by_rev_year\2015\Infinite_Factorial_UnboundedState_Hidden_Markov_Model\figure_8.jpg
  Figure 8 caption: REDD database. Percentage of total power consumed by each device.
  Figure 9 Link: articels_figures_by_rev_year\2015\Infinite_Factorial_UnboundedState_Hidden_Markov_Model\figure_9.jpg
  Figure 9 caption: AMP database. Percentage of total power consumed by each device.
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Isabel Valera
  Name of the last author: Fernando Perez-Cruz
  Number of Figures: 9
  Number of Tables: 3
  Number of authors: 3
  Paper title: Infinite Factorial Unbounded-State Hidden Markov Model
  Publication Date: 2015-11-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Accuracy for the Small Scale Experiment
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 REDD Database
  Table 3 caption:
    table_text: TABLE 3 AMP Database
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2498931
- Affiliation of the first author: school of electrical engineering, communication
    theory lab., kth royal institute of technology, stockholm, sweden
  Affiliation of the last author: school of electrical engineering, communication
    theory lab., kth royal institute of technology, stockholm, sweden
  Figure 1 Link: articels_figures_by_rev_year\2015\Variational_Inference_for_Watson_Mixture_Model\figure_1.jpg
  Figure 1 caption: "Scatter plot of samples from four different Watson distributions\
    \ on the sphere for positive and negative concentration parameters, \u03BB . Samples\
    \ generated from Watson distribution with \u03BB=\xB14 are shown with blue dots\
    \ and those with \u03BB=\xB140 are shown with red dots."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Variational_Inference_for_Watson_Mixture_Model\figure_2.jpg
  Figure 2 caption: Directed acyclic graph representing the Bayesian mixture model
    of Watson distributions. Nodes denote random variables, edges denote possible
    dependence, and plates denote replication.
  Figure 3 Link: articels_figures_by_rev_year\2015\Variational_Inference_for_Watson_Mixture_Model\figure_3.jpg
  Figure 3 caption: "Comparison of the posterior distribution estimated by the HMC\
    \ inference (first row) with the approximate posterior distribution estimated\
    \ by the variational approach (second row) for Example 4 .1: (left) the posterior\
    \ distribution of the mean direction p(( \u03BC 1 , \u03BC 2 )\u2223\u03BB) ,\
    \ q(( \u03BC 1 , \u03BC 2 )\u2223\u03BB) ; (middle) the posterior distribution\
    \ of the concentration parameter p(\u03BB) , q(\u03BB) ; (right) the posterior\
    \ distribution of the mean direction and the concentration parameter p(( \u03BC\
    \ 1 , \u03BC 2 ),\u03BB) , q(( \u03BC 1 , \u03BC 2 ),\u03BB) ."
  Figure 4 Link: articels_figures_by_rev_year\2015\Variational_Inference_for_Watson_Mixture_Model\figure_4.jpg
  Figure 4 caption: Average convergence time per dimension for variational approach
    (left) and HAIS (right) in Example 4.3.
  Figure 5 Link: articels_figures_by_rev_year\2015\Variational_Inference_for_Watson_Mixture_Model\figure_5.jpg
  Figure 5 caption: 'Model pruning of the mixture components, as evaluated in Example
    4.4: (left) the effective number of components remained after training versus
    the number of components K ; (right) the value of the lower bound after convergence
    versus K . The algorithm is initialized with Kin lbrace 5,8,12rbrace which are
    indicated respectively with red, blue, and black colors. Cases where results exhibit
    over-pruning are marked with medstar , whereas cases where there are no indication
    of over-pruning, the result are marked with +. This figure shows the result of
    20 trials where there are random horizontal perturbations for sake of demonstration.'
  Figure 6 Link: articels_figures_by_rev_year\2015\Variational_Inference_for_Watson_Mixture_Model\figure_6.jpg
  Figure 6 caption: Average SDR for the given reverberation time. The methods with
    the ideal permutation alignment procedure are shown by Ideal EM-GMM and Ideal
    VI-VMM.
  Figure 7 Link: articels_figures_by_rev_year\2015\Variational_Inference_for_Watson_Mixture_Model\figure_7.jpg
  Figure 7 caption: 'Measures of cluster quality on Human Fibroblasts data (Data C):
    (left) the average homogeneity mathrmHmathrmave ; (right) the average separation
    mathrmSmathrmave , across 10 trials. For mathrmHmathrmave higher values and for
    mathrmSmathrmave lower values are preferable.'
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Jalil Taghia
  Name of the last author: Arne Leijon
  Number of Figures: 7
  Number of Tables: 6
  Number of authors: 2
  Paper title: Variational Inference for Watson Mixture Model
  Publication Date: 2015-11-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Systematic Bias Introduced by Variational Approach for
      Example 4.2
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Average Mean Square Error Introduced by Variational Approach
      for Example 4 .2
  Table 3 caption:
    table_text: TABLE 3 The Average Log Marginal Likelihood Given by Variational Approach
      and HAIS for Example 4.3
  Table 4 caption:
    table_text: TABLE 4 Comparison of the Modeling Capability of Model A and Model
      B in Modeling the Time Frequency Observations for a Given Mixture in Various
      Conditions ( N=240 )
  Table 5 caption:
    table_text: TABLE 5 Separation Results in Terms of the Average Output Signal-to-Distortion
      Ratio of All Sources
  Table 6 caption:
    table_text: TABLE 6 Clustering Accuracy on Gene-Expression Data Set Over 10 Trials
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2498935
- Affiliation of the first author: computer vision and active perception lab (cvap),
    royal institute of technology (kth), stockholm, sweden
  Affiliation of the last author: computer vision and active perception lab (cvap),
    royal institute of technology (kth), stockholm, sweden
  Figure 1 Link: articels_figures_by_rev_year\2015\Factors_of_Transferability_for_a_Generic_ConvNet_Representation\figure_1.jpg
  Figure 1 caption: The improvements achieved by optimizing the transferability factors
    are significant. In this work, we systematically identify and list many factors
    that can significantly affect the transferability of a ConvNet representation
    from a source visual recognition task to a target one. We provide exhaustive experimental
    evidence showing how to optimize the performance based on these factors. This
    optimization boosted the performance of the transferred ConvNet representation
    on 17 challenging visual recognition tasks with up to a 50 percent reduction in
    the relative error rates. The violet bars show the performance of non-ConvNet
    state of the art systems on different datasets. The pink stacked bars show the
    improvement when using off-the-shelf ConvNet features with standard settings and
    a linear SVM classifier. The burnt orange stacked bars show the gains made by
    optimizing the transferability factors for each task. Detailed results are given
    in Table 9 . The accuracy is measured using the standard evaluation criteria of
    each task, see the references in Table 2.
  Figure 10 Link: articels_figures_by_rev_year\2015\Factors_of_Transferability_for_a_Generic_ConvNet_Representation\figure_10.jpg
  Figure 10 caption: 'Dimensionality Reduction: We use Principal Component Analysis,
    applied separately for each task, to linearly transform the original ConvNet representation
    obtained from first fully connected layer (4,096 dimensional) to a lower dimensional
    representation.'
  Figure 2 Link: articels_figures_by_rev_year\2015\Factors_of_Transferability_for_a_Generic_ConvNet_Representation\figure_2.jpg
  Figure 2 caption: Transferring a ConvNet Representation. ConvNet representations
    are effective for visual recognition. The picture above shows the pipeline of
    transferring a source ConvNet representation to a target task of interest. We
    define several factors which control the transferability of such representations
    to different tasks (questions with blue arrow). These factors come into play at
    different stages of the transfer process. Optimizing these factors is crucial
    if one wants to maximize the performance of the transferred representation (see
    Fig. 1).
  Figure 3 Link: articels_figures_by_rev_year\2015\Factors_of_Transferability_for_a_Generic_ConvNet_Representation\figure_3.jpg
  Figure 3 caption: 'Network Width: Over-parametrized networks (OverFeat) can be effective
    when the target task is close to the labelled data. The performance on less similar
    tasks can suffer from over-specialization when the number of network parameters
    is increased. Overall, under-parametrized networks (Tiny) are unable to generalize
    as well. The Tiny network, though it has 10 times fewer parameters than OverFeat
    still preserves much of OverFeat''s performance, could be useful for scenarios
    where real-time computation is a priority or the system memory capacity is limited.'
  Figure 4 Link: articels_figures_by_rev_year\2015\Factors_of_Transferability_for_a_Generic_ConvNet_Representation\figure_4.jpg
  Figure 4 caption: 'Network Depth: Over-parametrizing networks by increasing their
    number of convolutional layers is effective for nearly all the target tasks. Performance
    does saturate for some tasks, but there is no significant performance drop with
    increasing depth as opposed to the trend observed when over-parametrizing by increasing
    a network''s width. The horizontal axis indicates the network''s number of convolutional
    layers. The representation is taken from the first fully connected layer right
    after the last convolutional layer.'
  Figure 5 Link: articels_figures_by_rev_year\2015\Factors_of_Transferability_for_a_Generic_ConvNet_Representation\figure_5.jpg
  Figure 5 caption: 'Depth versus Width: Over-parametrization of a network can be
    achieved by increasing either its width, depth or both. This figure compares the
    effect of increasing a network''s depth as opposed to its width on the final performance
    of its resulting representation. A solid (dashed) line indicates the change of
    performance when the depth (width) of the network is increased. Circles indicate
    networks of depth 8 (e.g., AlexNet) and squares indicate networks of depth > 8.
    The results show that increasing the depth is more efficient in the number of
    parameters per unit of performance gain (the solid lines have higher slopes than
    the dashed ones). Refer to Tables 4 and 3 for the exact architecture of the networks
    used in this experiment. The representation is taken from the first fully connected
    layer right after the last convolutional layer. The tree on the right depicts
    the relationship between the different networks.'
  Figure 6 Link: articels_figures_by_rev_year\2015\Factors_of_Transferability_for_a_Generic_ConvNet_Representation\figure_6.jpg
  Figure 6 caption: 'Early Stopping: Plotted above is the performance of the representation
    extracted from layer 6 of the AlexNet ConvNet versus the number of iterations
    of SGD used to train the initial network. Early stopping, which can act as a regularizer,
    does not help to produce a more transferable representation in our scenarios.
    Every 5,000 iterations is an epoch over 1.3 M images in ILSVRC.'
  Figure 7 Link: articels_figures_by_rev_year\2015\Factors_of_Transferability_for_a_Generic_ConvNet_Representation\figure_7.jpg
  Figure 7 caption: 'Density versus Diversity of Training Data: Changing the number
    of training images for the source task by altering the number of images per class
    versus the number of classes changes the final performances on the target tasks.
    The results using lower diversity are consistently inferior to those obtained
    using a lower density (in a point-to-point comparison). This indicates the diversity
    of the source training data is more important than its density when transferring
    the learnt representation. The trend is observed regardless of the similarity
    of the target task to the source task.'
  Figure 8 Link: articels_figures_by_rev_year\2015\Factors_of_Transferability_for_a_Generic_ConvNet_Representation\figure_8.jpg
  Figure 8 caption: 'Representation Layer: The graph plots the performance of representations
    extracted from different layers of AlexNet for different recognition tasks. A
    distinct pattern emerges: as the task moves further from object image classification
    the earlier fully connected layers are more effective. For instance, layer 8 works
    best for PASCAL VOC image classification (very similar to ImageNet), but the best
    performance for all retrieval tasks is at layer 6.'
  Figure 9 Link: articels_figures_by_rev_year\2015\Factors_of_Transferability_for_a_Generic_ConvNet_Representation\figure_9.jpg
  Figure 9 caption: 'Spatial Pooling: In order to obtain meaningful results for retrieval
    with representations from convolutional layers, we applied spatial pooling over
    regular spatial grids of different sizes for different tasks. Objects with more
    complex structures, such as sculptures and buildings, need more spatial resolution
    for optimal performance.'
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Hossein Azizpour
  Name of the last author: Stefan Carlsson
  Number of Figures: 11
  Number of Tables: 9
  Number of authors: 5
  Paper title: Factors of Transferability for a Generic ConvNet Representation
  Publication Date: 2015-11-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Best Practices to Transfer a ConvNet Representation Trained
      for the Source Task of ImageNet to a Target Task
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 15 Visual Recognition Tasks Sorted Categorically by their
      Similarity to the Object Image Classification Task Defined by ILSVRC12
  Table 3 caption:
    table_text: 'TABLE 3 Wider Networks: Size Details of the ConvNets with Different
      Widths Used in Our Experiments'
  Table 4 caption:
    table_text: 'TABLE 4 Deeper Networks: Size Details of the Different Deep ConvNets
      Used in Our Experiments'
  Table 5 caption:
    table_text: 'TABLE 5 Source Task: Results on all Target Tasks Using Representations
      Optimized for Different Source Tasks'
  Table 6 caption:
    table_text: TABLE 6 Fine-Tuning
  Table 7 caption:
    table_text: 'TABLE 7 Additional Data (Fine-Tuning): The Table Presents the mAP
      Accuracy of a Sliding Window Detector Based on Different ConvNet Representations
      for Three Object Classes from VOC 2007'
  Table 8 caption:
    table_text: 'TABLE 8 Classifier: The Effect on the Performance of Using Different
      Classifiers'
  Table 9 caption:
    table_text: 'TABLE 9 Final Results: The Table Presents the Final Results of the
      ConvNet Representations (+ Linear SVM) with the Transfer Factors Optimized and
      Compares them to Non-ConvNet State of the Art Results'
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2500224
- Affiliation of the first author: department of electronic engineering, the chinese
    university of hong kong
  Affiliation of the last author: department of electronic engineering, the chinese
    university of hong kong
  Figure 1 Link: articels_figures_by_rev_year\2015\RealTime_Head_Pose_Tracking_with_Online_Face_Template_Reconstruction\figure_1.jpg
  Figure 1 caption: General framework of the proposed head pose tracking method.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\RealTime_Head_Pose_Tracking_with_Online_Face_Template_Reconstruction\figure_2.jpg
  Figure 2 caption: Fitting zones for frame selection.
  Figure 3 Link: articels_figures_by_rev_year\2015\RealTime_Head_Pose_Tracking_with_Online_Face_Template_Reconstruction\figure_3.jpg
  Figure 3 caption: An 2D illustration for the face model fitting process formulated
    by eq. (2).
  Figure 4 Link: articels_figures_by_rev_year\2015\RealTime_Head_Pose_Tracking_with_Online_Face_Template_Reconstruction\figure_4.jpg
  Figure 4 caption: Success ratio depending on the angular error thresholds for (a)
    BIWI and (b) ICT-3DHP. The red lines represent the performance of the random forests
    [1]. The green lines represent the performance of our method. The blue dashed
    line in (a) represents the performance of our method which replaces the face detection
    [16] with [1] for head pose initialization.
  Figure 5 Link: articels_figures_by_rev_year\2015\RealTime_Head_Pose_Tracking_with_Online_Face_Template_Reconstruction\figure_5.jpg
  Figure 5 caption: "(a) A plot of initial head pose errors versus estimation head\
    \ pose errors on BIWI. Frames with head rotation angles under a specified threshold\
    \ value are used to plot the corresponding lines. (b) Estimation error computed\
    \ for local angle ranges (15 \u2218 pitch \xD7 15 \u2218 yaw) on BIWI. The color\
    \ encodes the number of images (side bar) falling into each region."
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Songnan Li
  Name of the last author: Lu Sheng
  Number of Figures: 5
  Number of Tables: 3
  Number of authors: 4
  Paper title: Real-Time Head Pose Tracking with Online Face Template Reconstruction
  Publication Date: 2015-11-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 BIWI and ICT-3DPH Head Pose Databases
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Mean Rotation Errors (in Unit of Degree) for Yaw, Pitch and
      Roll on the BIWI Database
  Table 3 caption:
    table_text: TABLE 3 Mean Rotation Errors (in Unit of Degree) for Yaw, Pitch and
      Roll on the ICT-3DHP Database
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2500221
- Affiliation of the first author: j. crayton pruitt family department of biomedical
    engineering, university of florida, gainesville, fl
  Affiliation of the last author: centre for quantum computation & intelligent systems
  Figure 1 Link: articels_figures_by_rev_year\2015\TwoDimensional_Whitening_Reconstruction_for_Enhancing_Robustness_of_Principal_Co\figure_1.jpg
  Figure 1 caption: Visual block dragram of TWR in different steps ( m = 30). (a)
    Original face, (b) the face after subtracting the mean vector u , (c) whitened
    face.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\TwoDimensional_Whitening_Reconstruction_for_Enhancing_Robustness_of_Principal_Co\figure_2.jpg
  Figure 2 caption: 'Face representation with different methods. (a) Using original
    face images as the input (From top to bottom: original faces, the faces represented
    by PCA, the faces represented by PCA II); (b) using the whitened face images as
    the input (From top to bottom: whitened faces, the faces represented by PCA, the
    faces represented by PCA II).'
  Figure 3 Link: articels_figures_by_rev_year\2015\TwoDimensional_Whitening_Reconstruction_for_Enhancing_Robustness_of_Principal_Co\figure_3.jpg
  Figure 3 caption: A sample of pixel value distribution of the original face and
    its whitened face, the width of each bar is 0.05. (a) Original face (from Extended
    Yale-B database), (b) pixel value distribution of the original face, (c) whitened
    face, (d) pixel value distribution of the whitened face.
  Figure 4 Link: articels_figures_by_rev_year\2015\TwoDimensional_Whitening_Reconstruction_for_Enhancing_Robustness_of_Principal_Co\figure_4.jpg
  Figure 4 caption: A comparison of original faces and whitened faces using PCA and
    PCA II for a three-class problem, the labels of the three human are 7, 23, and
    33 in Extended Yale-B database, respectively. (a) PCA with original faces, (b)
    PCA II with original faces, (c) PCA with whitened faces, (d) PCA II with whitened
    faces.
  Figure 5 Link: articels_figures_by_rev_year\2015\TwoDimensional_Whitening_Reconstruction_for_Enhancing_Robustness_of_Principal_Co\figure_5.jpg
  Figure 5 caption: The sample faces and corresponding whitened faces from different
    databases. (a) Original faces from CMU PIE database and corresponding whitened
    faces, (b) original faces from AR database and corresponding whitened faces.
  Figure 6 Link: articels_figures_by_rev_year\2015\TwoDimensional_Whitening_Reconstruction_for_Enhancing_Robustness_of_Principal_Co\figure_6.jpg
  Figure 6 caption: From top to bottom, each row shows the sample images of subsets
    1, 2, 3, 4, and 5, respectively.
  Figure 7 Link: articels_figures_by_rev_year\2015\TwoDimensional_Whitening_Reconstruction_for_Enhancing_Robustness_of_Principal_Co\figure_7.jpg
  Figure 7 caption: Accuracy with different m on two face databases when each image
    is resized to 32times 32 pixels. (a) Extended Yale-B ( q = 2), (b) CMU PIE ( q
    = 1).
  Figure 8 Link: articels_figures_by_rev_year\2015\TwoDimensional_Whitening_Reconstruction_for_Enhancing_Robustness_of_Principal_Co\figure_8.jpg
  Figure 8 caption: The pixel value distribution of one whitened face with different
    m , the width of each bar is 0.05. (a) Original face, (b) m = 10, (c) m = 20,
    (d) m = 30.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.55
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xiaoshuang Shi
  Name of the last author: Dacheng Tao
  Number of Figures: 8
  Number of Tables: 6
  Number of authors: 6
  Paper title: Two-Dimensional Whitening Reconstruction for Enhancing Robustness of
    Principal Component Analysis
  Publication Date: 2015-11-18 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Classification Results (Mean Accuracy \xB1 Standard Derivation\
      \ ( % )) on Extended Yale-B Database under Lighting Variability"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Classification Results (Mean Accuracy \xB1 Standard Derivation\
      \ ( % )) on CMU PIE Database under Variations with Lighting"
  Table 3 caption:
    table_text: TABLE 3 Classification Results ( % ) on Faces under Illumination Changes
      from Slight to Severe
  Table 4 caption:
    table_text: TABLE 4 Clustering Results on Extended Yale-B Database with Faces
      under Lighting Variation
  Table 5 caption:
    table_text: TABLE 5 Clustering Results on CMU PIE Database with Faces under Variations
      with Lighting and Facial Expression
  Table 6 caption:
    table_text: TABLE 6 Clustering Results on AR Database with Faces under Variations
      with Lighting, Facial Expression and Occlusion
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2501810
- Affiliation of the first author: department of computing, imperial college london,
    180 queens gate, london, united kingdom
  Affiliation of the last author: department of computing, imperial college london,
    180 queens gate, london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2015\Doubly_Sparse_Relevance_Vector_Machine_for_Continuous_Facial_Behavior_Estimation\figure_1.jpg
  Figure 1 caption: Plates diagram of the model.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Doubly_Sparse_Relevance_Vector_Machine_for_Continuous_Facial_Behavior_Estimation\figure_2.jpg
  Figure 2 caption: Kernel weights learned by DSRVM (left) and SMKL (right) on the
    Sonnenburg artificial data [46] for the varying frequency of the target function.
  Figure 3 Link: articels_figures_by_rev_year\2015\Doubly_Sparse_Relevance_Vector_Machine_for_Continuous_Facial_Behavior_Estimation\figure_3.jpg
  Figure 3 caption: 'Results on the artificial data (left column) for a varying frequency
    of the target function and results on the ShoulderPain data (right column) for
    the pain targets and a varying number of training examples: CORR (first row),
    MSE (second row), the number of selected relevance vectors (RV) (third row), and
    the number of selected relevant kernels (RK) with non-zero kernel weights (fourth
    row). Note that RV is shown on the logarithmic scale.'
  Figure 4 Link: articels_figures_by_rev_year\2015\Doubly_Sparse_Relevance_Vector_Machine_for_Continuous_Facial_Behavior_Estimation\figure_4.jpg
  Figure 4 caption: 'ShoulderPain dataset: The values of kernel weights v learned
    by DSRVM (left) for various spatial (S) and temporal (T) scales are indicated
    by the intensity of color red of the corresponding patches. Each patch corresponds
    to one kernel, and the larger the kernel weight the redder the patch. The reddest
    patches correspond well with the FAU definitions presented in [8] . Additionally
    we show the learned kernel weights by SMKL (right) for selected scales.'
  Figure 5 Link: articels_figures_by_rev_year\2015\Doubly_Sparse_Relevance_Vector_Machine_for_Continuous_Facial_Behavior_Estimation\figure_5.jpg
  Figure 5 caption: 'DISFA data: The values of kernel weights v learned by DSRVM (left)
    and SMKL (right) for different FAUs. See the caption of Fig. 4 . Note: the facial
    image serves for region identification, it does not show activation of target
    FAUs.'
  Figure 6 Link: articels_figures_by_rev_year\2015\Doubly_Sparse_Relevance_Vector_Machine_for_Continuous_Facial_Behavior_Estimation\figure_6.jpg
  Figure 6 caption: DSRVM results measured by CORR for different numbers of facial
    patches. Shown is FAU12 from DISFA as example. Additionally, the learned kernel
    weights v are shown for 2, 9 and 28 patches.
  Figure 7 Link: articels_figures_by_rev_year\2015\Doubly_Sparse_Relevance_Vector_Machine_for_Continuous_Facial_Behavior_Estimation\figure_7.jpg
  Figure 7 caption: A sample video frame from the SEMAINE dataset. See the caption
    of Fig. 4 . Additionally, we show the SMKL results for arousal, which show that
    DSRVM learns sparser kernel weights.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.99
  Name of the first author: Sebastian Kaltwang
  Name of the last author: Maja Pantic
  Number of Figures: 7
  Number of Tables: 4
  Number of authors: 3
  Paper title: Doubly Sparse Relevance Vector Machine for Continuous Facial Behavior
    Estimation
  Publication Date: 2015-11-19 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Results on the ShoulderPain Data for the Pain Targets (Left
      Column) and on the SEMAINE Data for the Arousal (Ar.) and Valence (Val.) Targets
      (Right Column)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results on DISFA for FAU and S=6, T=1
  Table 3 caption:
    table_text: TABLE 3 Model Statistics on the DISFA Data Averaged over All FAU Targets
  Table 4 caption:
    table_text: TABLE 4 Comparison with Previous Work on the SEMAINE Dataset
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2501824
- Affiliation of the first author: robotics institute, carnegie mellon university
  Affiliation of the last author: robotics institute, carnegie mellon university
  Figure 1 Link: articels_figures_by_rev_year\2015\Factorized_Graph_Matching\figure_1.jpg
  Figure 1 caption: "Matching two human bodies with five and four features using FGM.\
    \ FGM simultaneously estimates the correspondence and a smooth non-rigid transformation\
    \ between shapes. FGM is able to factorize the 20\xD720 pairwise affinity matrix\
    \ as a Kronecker product of six smaller matrices. The first two groups of matrices\
    \ of size 5\xD716 and 4\xD710 encode the structure of each of the graphs. The\
    \ last two matrices encode the affinities for nodes ( 5\xD74 ) and edges ( 16\xD7\
    10 )."
  Figure 10 Link: articels_figures_by_rev_year\2015\Factorized_Graph_Matching\figure_10.jpg
  Figure 10 caption: Memory cost for GM problems. (a) An example of pair of graphs
    with 100 nodes. The edge connection is computed using Delaunay triangulation.
    (b) Number of elements to represent the affinity matrix.
  Figure 2 Link: articels_figures_by_rev_year\2015\Factorized_Graph_Matching\figure_2.jpg
  Figure 2 caption: An example GM problem. (a) Two synthetic graphs. (b) The 1 st
    graph's incidence matrices G 1 and H 1 , where the non-zero elements in each column
    of G 1 and H 1 indicate the starting and ending nodes in the corresponding directed
    edge, respectively. (c) The 2 nd graph's incidence matrices G 2 and H 2 . (d)
    The node affinity matrix K p and the edge affinity matrix K q . (e) The node correspondence
    matrix X and the edge correspondence matrix Y . (f) The global affinity matrix
    K .
  Figure 3 Link: articels_figures_by_rev_year\2015\Factorized_Graph_Matching\figure_3.jpg
  Figure 3 caption: Different relaxations on the constraints for GM.
  Figure 4 Link: articels_figures_by_rev_year\2015\Factorized_Graph_Matching\figure_4.jpg
  Figure 4 caption: "Parameterization of three geometrical transformations in 2 -D\
    \ space, where T , \u03C4(\u22C5) , \u03A8 and \u03C8(\u22C5) denote the parameter\
    \ set, transformation function, constraint set and penalization function respectively."
  Figure 5 Link: articels_figures_by_rev_year\2015\Factorized_Graph_Matching\figure_5.jpg
  Figure 5 caption: Statistics of K computed for the graphs extracted from three real
    image datasets used in the experiments. The histograms in the top row illustrate
    the ratio between the rank and the dimension of K . The histograms in the bottom
    row show the ratio between the minimum and the maximum eigenvalues of the K .
    For the last two datasets (columns), we test K with zero values on the diagonal
    as well. The top and bottom rows indicate that all the computed K are full-rank
    and indefinite respectively.
  Figure 6 Link: articels_figures_by_rev_year\2015\Factorized_Graph_Matching\figure_6.jpg
  Figure 6 caption: "An example of using the path-following algorithm for optimizing\
    \ the GM problem. (a) The comparison of the objectives during the optimization\
    \ with respect to the change of \u03B1 . (b) The comparison of using FW and MFW\
    \ for optimizing J \u03B1 (X) at two \u03B1 s. (c) The change of x ij (each curve)\
    \ as \u03B1\u21921 ."
  Figure 7 Link: articels_figures_by_rev_year\2015\Factorized_Graph_Matching\figure_7.jpg
  Figure 7 caption: Eight possible shapes of the parabola f(eta) = a eta 2 + b eta
    and the corresponding optimal step sizes eta = mathrm arg,maxeta in [0, 1] f(eta)
    .
  Figure 8 Link: articels_figures_by_rev_year\2015\Factorized_Graph_Matching\figure_8.jpg
  Figure 8 caption: Comparison between ICP and DGM for aligning shapes for several
    initial values of rotation and scale parameters. (a) Examples of initializations
    for different angles and scales. (b) Objective surfaces obtained by ICP. (c) Objective
    surfaces obtained by DGM.
  Figure 9 Link: articels_figures_by_rev_year\2015\Factorized_Graph_Matching\figure_9.jpg
  Figure 9 caption: Comparison of GM methods on synthetic datasets. (a) Performance
    as a function of the outlier number ( nout ). (b) Performance as a function of
    the edge noise ( sigma ). (c) Performance as a function of the density of edges
    ( rho ).
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Feng Zhou
  Name of the last author: Fernando De la Torre
  Number of Figures: 16
  Number of Tables: 0
  Number of authors: 2
  Paper title: Factorized Graph Matching
  Publication Date: 2015-11-19 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2501802
- Affiliation of the first author: institute of computer science, foundation for research
    and technology - hellas (forth) and the department of computer science, university
    of crete, heraklion, crete, greece
  Affiliation of the last author: institute of computer science, foundation for research
    and technology - hellas (forth) and the department of computer science, university
    of crete, heraklion, crete, greece
  Figure 1 Link: articels_figures_by_rev_year\2015\FullBody_Pose_TrackingThe_Top_View_Reprojection_Approach\figure_1.jpg
  Figure 1 caption: '(a) 11-part body model: head, upper and lower torso, upper and
    forearms and upper and lower legs. Body parts are modeled either as elliptical
    (upper and lower torso) or circular (rest 9 parts) cylinders. (b) Joints of the
    human body: Red spheres represent spherical joints while yellow spheres represent
    hinge joints. (c) Anthropometric measures of an adult human, expressed proportionally
    to the human''s height H .'
  Figure 10 Link: articels_figures_by_rev_year\2015\FullBody_Pose_TrackingThe_Top_View_Reprojection_Approach\figure_10.jpg
  Figure 10 caption: Average error in body pose estimation as a function of the number
    of hypotheses tracked for each body part, along with the corresponding frame rate.
    Blue graph refers to the complete data set; red graph refers to the subset that
    included cases with significant occlusions.
  Figure 2 Link: articels_figures_by_rev_year\2015\FullBody_Pose_TrackingThe_Top_View_Reprojection_Approach\figure_2.jpg
  Figure 2 caption: Different views of a simulated 3D point cloud of an elliptic cylinder
    which rotates around the x -axis without occlusions (top row) and while occluded
    by another object (bottom row) and the respective plot of the re-projection ratio.
    In both cases, the re-projection ratio becomes minimum at the cylinder's top and
    bottom view, as shown in the middle column.
  Figure 3 Link: articels_figures_by_rev_year\2015\FullBody_Pose_TrackingThe_Top_View_Reprojection_Approach\figure_3.jpg
  Figure 3 caption: Illustrative generated virtual views of the observed user. Reprojection
    ratio is minimized at the Top View , denoted with the green rectangle. The small
    images depict the reprojected points from the corresponding virtual view.
  Figure 4 Link: articels_figures_by_rev_year\2015\FullBody_Pose_TrackingThe_Top_View_Reprojection_Approach\figure_4.jpg
  Figure 4 caption: (a) Elliptic cylinder model of the human torso. (b) Reprojections
    of the point cloud from three indicative hypothetical views.
  Figure 5 Link: articels_figures_by_rev_year\2015\FullBody_Pose_TrackingThe_Top_View_Reprojection_Approach\figure_5.jpg
  Figure 5 caption: Computation of f reproj for two different upper torso pose hypotheses
    (top) and their corresponding top-views with the reprojected points (bottom).
    (a) indicates the hypothesis with the minimum score and (b) an erroneous hypothesis.
  Figure 6 Link: articels_figures_by_rev_year\2015\FullBody_Pose_TrackingThe_Top_View_Reprojection_Approach\figure_6.jpg
  Figure 6 caption: 'Indicative graphs of fTVR score under different percentages of
    occlusion compared to ground truth data: (a) no occlusion, (b) 30 percent and
    (c) 60 percent occlusion. The two bottom axes are the pitch and roll angles, namely
    the hypothesis configuration, and the vertical axis is the corresponding fTVR
    . Pitch and roll angles are given as angular distance from the ground truth, which
    lies at (0,0), the center of the bottom grid. While additional local minima appear,
    fTVR presents a strong, well defined, global minimum, at -or close to- the actual
    pose configuration.'
  Figure 7 Link: articels_figures_by_rev_year\2015\FullBody_Pose_TrackingThe_Top_View_Reprojection_Approach\figure_7.jpg
  Figure 7 caption: Overview of the pose recovery methodology. The first step (top)
    concerns the human body segmentation and depth-based ordering and the second step
    (bottom) the pose recovery and tracking.
  Figure 8 Link: articels_figures_by_rev_year\2015\FullBody_Pose_TrackingThe_Top_View_Reprojection_Approach\figure_8.jpg
  Figure 8 caption: (a) Indicative input from the RGB-D sensor. RGB image is depicted
    in the left part and the corresponding organized 3D point cloud of the depth channel
    is depicted in the right part. (b) Detected faces by skin-color classification
    on the RGB image. (c) Approximated reprojection of scene's overview, rotated by
    90 degrees around the horizontal axis of the camera system. Users are ordered
    based on the detected face depth. (d) Point cloud of each human-body is segmented
    based on the scene overview. Grey area depicts the 3D points shared by users 1
    and 2.
  Figure 9 Link: articels_figures_by_rev_year\2015\FullBody_Pose_TrackingThe_Top_View_Reprojection_Approach\figure_9.jpg
  Figure 9 caption: Methodology pipeline and information flow between the subsystems.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Markos Sigalas
  Name of the last author: Panos Trahanias
  Number of Figures: 17
  Number of Tables: 3
  Number of authors: 3
  Paper title: "Full-Body Pose Tracking\u2014The Top View Reprojection Approach"
  Publication Date: 2015-11-20 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Joint Angular Limits of the Employed Model
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparative Analysis Using MHAD, with Respect to Different
      Users and Different Activities
  Table 3 caption:
    table_text: TABLE 3 Comparative Assessment Results
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2502582
