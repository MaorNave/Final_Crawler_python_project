- Affiliation of the first author: presto, japan science and technology agency, japan
  Affiliation of the last author: department of computer science, aalto university,
    finland
  Figure 1 Link: articels_figures_by_rev_year\2016\Generalized_Sparse_Learning_of_Linear_Models_Over_the_Complete_Subgraph_Feature_\figure_1.jpg
  Figure 1 caption: "Boolean vectors I G n (x) associated with x\u2208T( G n ) . In\
    \ this example, x\u2286 x \u2032 . Hence v i =0 for u i =0 . Only coordinates\
    \ v i for u i =1 can be either 0 or 1 , as stated in Lemma 2."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Generalized_Sparse_Learning_of_Linear_Models_Over_the_Complete_Subgraph_Feature_\figure_2.jpg
  Figure 2 caption: The proposed algorithm for solving Problem (2).
  Figure 3 Link: articels_figures_by_rev_year\2016\Generalized_Sparse_Learning_of_Linear_Models_Over_the_Complete_Subgraph_Feature_\figure_3.jpg
  Figure 3 caption: Learning curves for RAND (average over 100 trials).
  Figure 4 Link: articels_figures_by_rev_year\2016\Generalized_Sparse_Learning_of_Linear_Models_Over_the_Complete_Subgraph_Feature_\figure_4.jpg
  Figure 4 caption: Distribution of the size of selected subgraph features (average
    over 100 trials).
  Figure 5 Link: articels_figures_by_rev_year\2016\Generalized_Sparse_Learning_of_Linear_Models_Over_the_Complete_Subgraph_Feature_\figure_5.jpg
  Figure 5 caption: The number of isomorphic subgraph features (average over 100 trials).
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ichigaku Takigawa
  Name of the last author: Hiroshi Mamitsuka
  Number of Figures: 5
  Number of Tables: 3
  Number of authors: 2
  Paper title: Generalized Sparse Learning of Linear Models Over the Complete Subgraph
    Feature Set
  Publication Date: 2016-05-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Statistics of Carefully Chosen Parameters for the Same Number
      of Selected Features
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Classification Accuracy for the CPDB Dataset (10-Fold CV)
  Table 3 caption:
    table_text: TABLE 3 Performance and Search-Space Size for the CPDB Dataset (10-Fold
      CV)
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2567399
- Affiliation of the first author: department of electrical and computer engineering,
    duke university, durham, nc
  Affiliation of the last author: department of electrical and computer engineering,
    duke university, durham, nc
  Figure 1 Link: articels_figures_by_rev_year\2016\InformationTheoretic_Compressive_Measurement_Design\figure_1.jpg
  Figure 1 caption: "Classification accuracy and fractional error for signal recovery\
    \ on the USPS datasets under the energy constaint. (a) The classification accuracy\
    \ under various values of \u03B2 . (b) The fractional error of signal recovery\
    \ under various values of \u03B2 . In (b), the same symbol identifications are\
    \ used as in (a)."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\InformationTheoretic_Compressive_Measurement_Design\figure_2.jpg
  Figure 2 caption: "Classification accuracy and fractional error for signal recovery\
    \ on the USPS datasets under the orthonormality constraint. (a) The classification\
    \ accuracy under various values of \u03B2 . (b) The fractional error of signal\
    \ recovery under various values of \u03B2 . In (b), the same symbol identifications\
    \ are used as in (a)."
  Figure 3 Link: articels_figures_by_rev_year\2016\InformationTheoretic_Compressive_Measurement_Design\figure_3.jpg
  Figure 3 caption: "Classification accuracy and fractional error for signal recovery\
    \ on the chemical sensing dataset. (a) The classification accuracy under various\
    \ values of \u03B2 . (b) The fractional error of signal recovery under various\
    \ values of \u03B2 . In (b), the same symbol identifications are used as in (a)."
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Liming Wang
  Name of the last author: Lawrence Carin
  Number of Figures: 3
  Number of Tables: 9
  Number of authors: 5
  Paper title: Information-Theoretic Compressive Measurement Design
  Publication Date: 2016-05-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Classification Accuracy on the Satellite Datasets under the
      Energy Constraint with L (i) =5
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Fractional Error of the Signal Recovery on the Satellite\
      \ Datasets with Various Values of \u03B2 under the Energy Constraint L (i) =5"
  Table 3 caption:
    table_text: TABLE 3 Classification Accuracy on the Satellite Datasets under the
      Orthonormality Constraint with L (i) =5
  Table 4 caption:
    table_text: "TABLE 4 Fractional Error of Signal Recovery on the Satellite Datasets\
      \ with Various Values of \u03B2 under the Orthonormality Constraint with L (i)\
      \ =5"
  Table 5 caption:
    table_text: TABLE 5 Classification Accuracy on the Satellite Datasets under the
      Energy Constraint with L (i) =1
  Table 6 caption:
    table_text: "TABLE 6 Fractional Error of the Signal Recovery on the Satellite\
      \ Datasets with Various Values of \u03B2 under the Energy Constraint with L\
      \ (i) =1"
  Table 7 caption:
    table_text: "TABLE 7 Classification and Rank under Various Values of \u03B2 on\
      \ the Satellite Datasets"
  Table 8 caption:
    table_text: "TABLE 8 Classification and Rank under Various Values of \u03B2 on\
      \ the USPS Datasets"
  Table 9 caption:
    table_text: "TABLE 9 Classification and Rank under Various Values of \u03B2 on\
      \ the Chemical-Sensing Dataset"
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2568189
- Affiliation of the first author: iiit-delhi, new delhi, india
  Affiliation of the last author: iiit-delhi, new delhi, india
  Figure 1 Link: articels_figures_by_rev_year\2016\Face_Verification_via_Class_Sparsity_Based_Supervised_Encoding\figure_1.jpg
  Figure 1 caption: Illustrating the steps involved in the proposed face verification
    algorithm based on the class sparsity based supervised encoder.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Face_Verification_via_Class_Sparsity_Based_Supervised_Encoding\figure_2.jpg
  Figure 2 caption: Sample images from the (a) LFW database and (b) PaSC database-still
    to still challenge.
  Figure 3 Link: articels_figures_by_rev_year\2016\Face_Verification_via_Class_Sparsity_Based_Supervised_Encoding\figure_3.jpg
  Figure 3 caption: ROC curves on the (a) restricted protocol of the LFW database
    and (b) both the protocols of the PaSC database.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Angshul Majumdar
  Name of the last author: Mayank Vatsa
  Number of Figures: 3
  Number of Tables: 6
  Number of authors: 3
  Paper title: Face Verification via Class Sparsity Based Supervised Encoding
  Publication Date: 2016-05-17 00:00:00
  Table 1 caption:
    table_text: TABLE 1 A Literature Review of Recent Deep Learning Based Face Recognition
      Algorithms
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparing Verification Rates at EER on the LFW Database Using
      Two Protocols
  Table 3 caption:
    table_text: TABLE 3 Verification Rates of Existing and Proposed Algorithms on
      the PaSC Still-to-Still Matching Database
  Table 4 caption:
    table_text: TABLE 4 Comparing the Performance of Supervised and Unsupervised Encoding
  Table 5 caption:
    table_text: TABLE 5 Verification Rate of the Proposed L-CSSE Based Feature Representation
      with Different Classifiers
  Table 6 caption:
    table_text: TABLE 6 Verification Rates of Existing and Proposed Algorithms on
      LFW (Unrestricted Labeled Outside Data) and PaSC-Full (Still-to-Still Matching)
      Databases
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2569436
- Affiliation of the first author: department of electrical engineering and computer
    science (cs division), university of california, berkeley, ca, usa
  Affiliation of the last author: department of electrical engineering and computer
    science (cs division), university of california, berkeley, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2016\Fully_Convolutional_Networks_for_Semantic_Segmentation\figure_1.jpg
  Figure 1 caption: Fully convolutional networks can efficiently learn to make dense
    predictions for per-pixel tasks like semantic segmentation.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Fully_Convolutional_Networks_for_Semantic_Segmentation\figure_2.jpg
  Figure 2 caption: 'Our DAG nets learn to combine coarse, high layer information
    with fine, low layer information. Pooling and prediction layers are shown as grids
    that reveal relative spatial coarseness, while intermediate layers are shown as
    vertical lines. First row (FCN-32s): Our single-stream net, described in Section
    4.1 , upsamples stride 32 predictions back to pixels in a single step. Second
    row (FCN-16s): Combining predictions from both the final layer and the pool4 layer,
    at stride 16, lets our net predict finer details, while retaining high-level semantic
    information. Third row (FCN-8s): Additional predictions from pool3, at stride
    8, provide further precision.'
  Figure 3 Link: articels_figures_by_rev_year\2016\Fully_Convolutional_Networks_for_Semantic_Segmentation\figure_3.jpg
  Figure 3 caption: Transforming fully connected layers into convolution layers enables
    a classification net to output a spatial map. Adding differentiable interpolation
    layers and a spatial loss (as in Fig. 1 ) produces an efficient machine for end-to-end
    pixelwise learning.
  Figure 4 Link: articels_figures_by_rev_year\2016\Fully_Convolutional_Networks_for_Semantic_Segmentation\figure_4.jpg
  Figure 4 caption: Refining fully convolutional networks by fusing information from
    layers with different strides improves spatial detail. The first three images
    show the output from our 32, 16, and 8 pixel stride nets (see Fig. 3).
  Figure 5 Link: articels_figures_by_rev_year\2016\Fully_Convolutional_Networks_for_Semantic_Segmentation\figure_5.jpg
  Figure 5 caption: Training on whole images is just as effective as sampling patches,
    but results in faster (wall clock time) convergence by making more efficient use
    of data. Left shows the effect of sampling on convergence rate for a fixed expected
    batch size, while right plots the same by relative wall clock time.
  Figure 6 Link: articels_figures_by_rev_year\2016\Fully_Convolutional_Networks_for_Semantic_Segmentation\figure_6.jpg
  Figure 6 caption: 'Fully convolutional networks improve performance on PASCAL. The
    left column shows the output of our most accurate net, FCN-8s. The second shows
    the output of the previous best method by Hariharan et al. [14]. Notice the fine
    structures recovered (first row), ability to separate closely interacting objects
    (second row), and robustness to occluders (third row). The fifth and sixth rows
    show failure cases: the net sees lifejackets in a boat as people and confuses
    human hair with a dog.'
  Figure 7 Link: articels_figures_by_rev_year\2016\Fully_Convolutional_Networks_for_Semantic_Segmentation\figure_7.jpg
  Figure 7 caption: 'FCNs learn to recognize by shape when deprived of other input
    detail. From left to right: regular image (not seen by network), ground truth,
    output, mask input.'
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Evan Shelhamer
  Name of the last author: Trevor Darrell
  Number of Figures: 7
  Number of Tables: 8
  Number of authors: 3
  Paper title: Fully Convolutional Networks for Semantic Segmentation
  Publication Date: 2016-05-24 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Adapting ILSVRC Classifiers to FCNs
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Image-to-Image Optimization Methods
  Table 3 caption:
    table_text: TABLE 3 Comparison of FCN Variations
  Table 4 caption:
    table_text: TABLE 4 Results on PASCAL VOC
  Table 5 caption:
    table_text: TABLE 5 Results on NYUDv2
  Table 6 caption:
    table_text: TABLE 6 Results on SIFT Flow
  Table 7 caption:
    table_text: TABLE 7 Results on PASCAL-Context for the 59 Class Task
  Table 8 caption:
    table_text: TABLE 8 The Role of Foreground, Background, and Shape Cues
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2572683
- Affiliation of the first author: google switzerland, zurich, switzerland
  Affiliation of the last author: google switzerland, zurich, switzerland
  Figure 1 Link: articels_figures_by_rev_year\2016\MultiLanguage_Online_Handwriting_Recognition\figure_1.jpg
  Figure 1 caption: Example inputs for online handwriting recognition in different
    languages. See text for details.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\MultiLanguage_Online_Handwriting_Recognition\figure_2.jpg
  Figure 2 caption: 'Examples of ambiguities: a selection of symbols that look like
    a circle with their Unicode code-point number and short descriptions.'
  Figure 3 Link: articels_figures_by_rev_year\2016\MultiLanguage_Online_Handwriting_Recognition\figure_3.jpg
  Figure 3 caption: 'Lattice creation: on the input ink (a), we determine cut points
    and create three segments (b). From the segments, we create a segmentation lattice
    (c) by grouping adjacent segments in character hypotheses. The character hypotheses
    are classified (d) and the lattice is labeled (e).'
  Figure 4 Link: articels_figures_by_rev_year\2016\MultiLanguage_Online_Handwriting_Recognition\figure_4.jpg
  Figure 4 caption: "Additional features in the lattice (Section 5.4) and beam search\
    \ on this lattice with all weights set to 1.0, except for the garbage weight set\
    \ to \u2212 1. The beam search uses a beam size of 4 which means that in each\
    \ step it expands the top-4 entries of the beam. The edge costs in the beam correspond\
    \ to the newly added costs from traversing an edge and the newly added language\
    \ model cost. The total cost contains the sum of these and the total cost of the\
    \ path before this last step. The gray entries in the beam are those that are\
    \ not expanded in the next step."
  Figure 5 Link: articels_figures_by_rev_year\2016\MultiLanguage_Online_Handwriting_Recognition\figure_5.jpg
  Figure 5 caption: Illustrations of script specific properties.
  Figure 6 Link: articels_figures_by_rev_year\2016\MultiLanguage_Online_Handwriting_Recognition\figure_6.jpg
  Figure 6 caption: "Character error rates for our system with respect to the number\
    \ of training samples used. Each point on the lines corresponds to doubling the\
    \ training set. The point at the position \u201C11 + unsup\u201D corresponds to\
    \ using the full set of labeled data along with a similar amount of unsupervised\
    \ (self-labeled, unlabeled) data."
  Figure 7 Link: articels_figures_by_rev_year\2016\MultiLanguage_Online_Handwriting_Recognition\figure_7.jpg
  Figure 7 caption: Example errors. See text for details.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Daniel Keysers
  Name of the last author: Victor Carbune
  Number of Figures: 7
  Number of Tables: 6
  Number of authors: 5
  Paper title: Multi-Language Online Handwriting Recognition
  Publication Date: 2016-05-25 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Terminology Used in this Paper
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Summary of the Characteristics of the Different Languages
      and Scripts and How We Adjust Our System to Handle Them
  Table 3 caption:
    table_text: TABLE 3 Error Rates on UNIPEN-1 Data in Comparison to the State of
      the Art
  Table 4 caption:
    table_text: TABLE 4 Error Rates on IAM-OnDB Test Set in Comparison to the State
      of the Art. A '' in the 'system' Column Indicates the Use of an Open Training
      Set
  Table 5 caption:
    table_text: TABLE 5 Character Error Rates on the Validation Data Using Successively
      More of the System Components Described Above for English (en), Spanish (es),
      German (de), Arabic (ar), Korean (ko), Thai (th), and Hindi (hi) Along with
      the Respective Number of Items and Characters in the Sets
  Table 6 caption:
    table_text: TABLE 6 Languages by Usage (Cloud Recognizer) over a Week in January
      2016
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2572693
- Affiliation of the first author: institute of artificial intelligence and robotics,
    xi'an jiaotong university, shaanxi, china
  Affiliation of the last author: department of statistics, university of california,
    los angeles, ca
  Figure 1 Link: articels_figures_by_rev_year\2016\Modeling_D_HumanObject_Interactions_for_Joint_Event_Segmentation_Recognition_and\figure_1.jpg
  Figure 1 caption: Examples of objects in video frames. These objects can hardly
    be recognized by appearance features inside the bounding boxes due to heavy occlusion
    and large appearance variation, but they can be recognized in the context of actions.
  Figure 10 Link: articels_figures_by_rev_year\2016\Modeling_D_HumanObject_Interactions_for_Joint_Event_Segmentation_Recognition_and\figure_10.jpg
  Figure 10 caption: Object search. (a) Point cloud. (b) Object prediction probability.
    (c) Potential locations. (d) Non-void locations. (e) Refined locations. (f) Final
    locations on 2D image.
  Figure 2 Link: articels_figures_by_rev_year\2016\Modeling_D_HumanObject_Interactions_for_Joint_Event_Segmentation_Recognition_and\figure_2.jpg
  Figure 2 caption: The 4DHOI model. (a) The framework of the model. The input is
    an RGB-D video with human skeletons. The outputs are the hierarchical interpretations
    of the video sequence, including event recognition, segmentation, and object localization.
    (b) Object recognition and localization in the 3D point cloud (upper) and the
    RGB image (lower) through the 4DHOI model after analyzing the video events.
  Figure 3 Link: articels_figures_by_rev_year\2016\Modeling_D_HumanObject_Interactions_for_Joint_Event_Segmentation_Recognition_and\figure_3.jpg
  Figure 3 caption: A hierarchical graph of the 4D human-object interactions for an
    example event fetch water from dispenser .
  Figure 4 Link: articels_figures_by_rev_year\2016\Modeling_D_HumanObject_Interactions_for_Joint_Event_Segmentation_Recognition_and\figure_4.jpg
  Figure 4 caption: Human-object geometric relations in 3D space.
  Figure 5 Link: articels_figures_by_rev_year\2016\Modeling_D_HumanObject_Interactions_for_Joint_Event_Segmentation_Recognition_and\figure_5.jpg
  Figure 5 caption: Examples of the learned geometric relations in atomic events.
    The odd-number columns are about the instances of the learned atomic events. The
    indices denote the atomic event number in each event. The even-number columns
    are about the probability maps of object prediction, where warmer colors indicate
    higher probabilities of locations where objects appear.
  Figure 6 Link: articels_figures_by_rev_year\2016\Modeling_D_HumanObject_Interactions_for_Joint_Event_Segmentation_Recognition_and\figure_6.jpg
  Figure 6 caption: The atomic event transition probability. (a) Duration-dependent
    transition. (b) Duration-independent transition.
  Figure 7 Link: articels_figures_by_rev_year\2016\Modeling_D_HumanObject_Interactions_for_Joint_Event_Segmentation_Recognition_and\figure_7.jpg
  Figure 7 caption: "The dynamic programming beam search inference algorithm. (a)\
    \ Toy examples of the given graph set. The goal is to interpret the input video\
    \ sequence with the graphs in this set. (b) Dynamic programming process to interpret\
    \ each frame. Each path denotes one possible interpretation of the video. (c)\
    \ Interpreting the frame f t based on the j th interpretation G j t\u22121 of\
    \ the video in the interval [1,t\u22121] . (d) Three types of interpretations\
    \ of the frame f t based on G j t\u22121 ."
  Figure 8 Link: articels_figures_by_rev_year\2016\Modeling_D_HumanObject_Interactions_for_Joint_Event_Segmentation_Recognition_and\figure_8.jpg
  Figure 8 caption: Comparison between OEM and EM. (a) Our OEM. Each color denotes
    an atomic event. (b) Conventional EM.
  Figure 9 Link: articels_figures_by_rev_year\2016\Modeling_D_HumanObject_Interactions_for_Joint_Event_Segmentation_Recognition_and\figure_9.jpg
  Figure 9 caption: Scene alignment. (a) Original scene. (b) Delaunay triangles surfaces.
    (c) Norm clustering. (d) Transformed scene. (e) Aligned scene. (f) Reference scene.
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.61
  Name of the first author: Ping Wei
  Name of the last author: Song-Chun Zhu
  Number of Figures: 15
  Number of Tables: 7
  Number of authors: 4
  Paper title: Modeling 4D Human-Object Interactions for Joint Event Segmentation,
    Recognition, and Object Localization
  Publication Date: 2016-06-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Dataset Comparison. MV, Multiview; TN, Total Video Number;
      AN, Average Number of the Videos of Each Event Category; AL, Average Length
      (Frame) of Each Video
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Event Recognition Accuracy Comparison on Multiview RGB-D Event
      Dataset
  Table 3 caption:
    table_text: TABLE 3 Event Recognition Accuracy Comparison on DailyActivity3D Dataset
  Table 4 caption:
    table_text: TABLE 4 Event Recognition Accuracy Comparison on MSR-Action3D Dataset
  Table 5 caption:
    table_text: TABLE 5 Comparison of Sequence Segmentation Algorithms with Two Accuracy
      Metrics
  Table 6 caption:
    table_text: TABLE 6 Average Precision (AP) Comparison on Multiview RGB-D Event
      Dataset
  Table 7 caption:
    table_text: TABLE 7 The Overall Average Precision (AP) Comparison on DailyActivity3D
      Dataset
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2574712
- Affiliation of the first author: department of electrical engineering and computer
    science, university of california at berkeley, ca
  Affiliation of the last author: department of electrical engineering and computer
    science, university of california at berkeley, ca
  Figure 1 Link: articels_figures_by_rev_year\2016\Learning_CategorySpecific_Deformable_D_Models_for_Object_Reconstruction\figure_1.jpg
  Figure 1 caption: Example outputs of our system, given a single image of a scene
    having chairs, a class that the system was exposed to during training. The coloring
    on the right image signals object-centric depth (we do not aim for globally consistent
    depths across multiple objects). Blue means close to the camera, red means far
    from the camera.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Learning_CategorySpecific_Deformable_D_Models_for_Object_Reconstruction\figure_2.jpg
  Figure 2 caption: Overview of our full reconstruction method. We leverage estimated
    instance segmentations and predicted viewpoints to generate a full 3D mesh and
    a high frequency 2.5D depth map for each object in the image.
  Figure 3 Link: articels_figures_by_rev_year\2016\Learning_CategorySpecific_Deformable_D_Models_for_Object_Reconstruction\figure_3.jpg
  Figure 3 caption: Overview of our training pipeline. We use an annotated image collection
    to estimate camera projection parameters which we then use along with object silhouettes
    to learn 3D shape models. Our learnt shape models, as illustrated in the rightmost
    figure are capable of deforming to capture intra-class shape variation.
  Figure 4 Link: articels_figures_by_rev_year\2016\Learning_CategorySpecific_Deformable_D_Models_for_Object_Reconstruction\figure_4.jpg
  Figure 4 caption: 'NRSfM camera estimation: Estimated cameras visualized using a
    3D car wireframe.'
  Figure 5 Link: articels_figures_by_rev_year\2016\Learning_CategorySpecific_Deformable_D_Models_for_Object_Reconstruction\figure_5.jpg
  Figure 5 caption: Mean shapes learnt for rigid classes in PASCAL VOC obtained using
    our basis shape formulation. Color encodes depth when viewed frontally.
  Figure 6 Link: articels_figures_by_rev_year\2016\Learning_CategorySpecific_Deformable_D_Models_for_Object_Reconstruction\figure_6.jpg
  Figure 6 caption: Viewpoint predictions for unoccluded groundtruth instances using
    our algorithm. The columns show 15th, 30th, 45th, 60th, 75th and 90th percentile
    instances respectively in terms of the error. We visualize the predictions by
    rendering a 3D model using our predicted viewpoint.
  Figure 7 Link: articels_figures_by_rev_year\2016\Learning_CategorySpecific_Deformable_D_Models_for_Object_Reconstruction\figure_7.jpg
  Figure 7 caption: Fully automatic reconstructions on detected instances (0.5 IoU
    with ground truth) using our models on rigid categories in PASCAL VOC. We show
    our instance segmentation input, the inferred shape overlaid on the image, a 2.5D
    depth map (after the bottom-up refinement stage), the mesh in the image viewpoint
    and two other views. It can be seen that our method produces plausible reconstructions
    which is a remarkable achievement given just a single image and noisy instance
    segmentations. Color encodes depth in the image coordinate frame (blue is closer).
    More results can be found at https:goo.glMgVQzZ.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Shubham Tulsiani
  Name of the last author: Jitendra Malik
  Number of Figures: 7
  Number of Tables: 4
  Number of authors: 4
  Paper title: Learning Category-Specific Deformable 3D Models for Object Reconstruction
  Publication Date: 2016-06-01 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Studying the Quality of Our Learnt 3D Models: Comparison
      between Our Method and [23], [63] Using Ground Truth Keypoints and Masks on
      PASCAL VOC'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Viewpoint Estimation with Ground Truth Box
  Table 3 caption:
    table_text: TABLE 3 Mean Performance of Our Approach for Various Metrics
  Table 4 caption:
    table_text: TABLE 4 Ablation Study for Our Method AssumingRelaxing Various Annotations
      at Test Time on Objects in PASCAL VOC
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2574713
- Affiliation of the first author: "department of mathematics and computer science,\
    \ saarland university, saarbr\xFCcken, saarland, germany"
  Affiliation of the last author: "department of mathematics and computer science,\
    \ saarland university, saarbr\xFCcken, saarland, germany"
  Figure 1 Link: articels_figures_by_rev_year\2016\An_Efficient_Multilinear_Optimization_Framework_for_Hypergraph_Matching\figure_1.jpg
  Figure 1 caption: Illustration of constructing the affinity tensor. Each entry is
    computed by comparing the angles of two triangles formed by three candidate correspondences
    ( i 1 , i 2 ),( j 1 , j 2 ) and ( k 1 , k 2 ) .
  Figure 10 Link: articels_figures_by_rev_year\2016\An_Efficient_Multilinear_Optimization_Framework_for_Hypergraph_Matching\figure_10.jpg
  Figure 10 caption: Demo of matching results on CMU House dataset with large baseline
    ( mathrm baseline=80 ). a) Input images. Yellow dots denote inlier points, blue
    dots denote outlier points. b) c) Matching results of previous second-order methods.
    d) e) Matching results of previous higher-order methods. f) g) h) i) Matching
    results of our higher-order approaches. The yellowred lines indicate correctincorrect
    matches. Matching score is reported for each method. (Best viewed in color.)
  Figure 2 Link: articels_figures_by_rev_year\2016\An_Efficient_Multilinear_Optimization_Framework_for_Hypergraph_Matching\figure_2.jpg
  Figure 2 caption: Scatter plots showing matching score of adaptive versus non-adaptive
    methods over different datasets.
  Figure 3 Link: articels_figures_by_rev_year\2016\An_Efficient_Multilinear_Optimization_Framework_for_Hypergraph_Matching\figure_3.jpg
  Figure 3 caption: Matching point sets in R 2 . The number of outliers is varied
    from 0 to 200 . See Fig. 4 for the legend. (Best viewed in color.)
  Figure 4 Link: articels_figures_by_rev_year\2016\An_Efficient_Multilinear_Optimization_Framework_for_Hypergraph_Matching\figure_4.jpg
  Figure 4 caption: Performance of all algorithms with varying deformation noise.
    The number of inlier points is fixed as n in =20. (Best viewed in color.)
  Figure 5 Link: articels_figures_by_rev_year\2016\An_Efficient_Multilinear_Optimization_Framework_for_Hypergraph_Matching\figure_5.jpg
  Figure 5 caption: CMU house dataset with different number of points in two images.
    (Best viewed in color.)
  Figure 6 Link: articels_figures_by_rev_year\2016\An_Efficient_Multilinear_Optimization_Framework_for_Hypergraph_Matching\figure_6.jpg
  Figure 6 caption: Running time of higher-order methods. (Best viewed in color.)
  Figure 7 Link: articels_figures_by_rev_year\2016\An_Efficient_Multilinear_Optimization_Framework_for_Hypergraph_Matching\figure_7.jpg
  Figure 7 caption: Matching score and accuracy of higher-order algorithms on Willow
    Object Dataset [9] . (Best viewed in color.)
  Figure 8 Link: articels_figures_by_rev_year\2016\An_Efficient_Multilinear_Optimization_Framework_for_Hypergraph_Matching\figure_8.jpg
  Figure 8 caption: Demo of matching results. Matching score is reported for each
    method. (Best viewed in color.)
  Figure 9 Link: articels_figures_by_rev_year\2016\An_Efficient_Multilinear_Optimization_Framework_for_Hypergraph_Matching\figure_9.jpg
  Figure 9 caption: Demo of matching results on CMU House dataset with small baseline
    ( mathrm baseline=50 ). a) Input images. Yellow dots denote inlier points, blue
    dots denote outlier points. b) c) Matching results of previous second-order methods.
    d) e) Matching results of previous higher-order methods. f) g) h) i) Matching
    results of our higher-order approaches. The yellowred lines indicate correctincorrect
    matches. Matching score is reported for each method. (Best viewed in color.)
  First author gender probability: 0.99
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Quynh Nguyen
  Name of the last author: Matthias Hein
  Number of Figures: 12
  Number of Tables: 8
  Number of authors: 4
  Paper title: An Efficient Multilinear Optimization Framework for Hypergraph Matching
  Publication Date: 2016-06-01 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Comparison between the Standard Third-Order Methods (BCAGM3,\
      \ BCAGM3+\u03A8 ) and the Standard Fourth-Order Methods (BCAGM, BCAGM+\u03A8\
      \ [36]) on Different Datasets"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Comparison between the Adaptive Third-Order Methods (Adapt-BCAGM3,\
      \ Adapt\u2212BCAGM3+\u03A8 ) and the Adaptive Fourth-Order Methods (Adapt-BCAGM,\
      \ Adapt\u2212BCAGM+\u03A8 ) on Different Datasets"
  Table 3 caption:
    table_text: "TABLE 3 Adaptive Third-Order Methods (Adapt-BCAGM3, Adapt\u2212BCAGM3+\u03A8\
      \ ) versus Non-Adaptive Third-Order Methods (BCAGM3, BCAGM3+\u03A8 )"
  Table 4 caption:
    table_text: "TABLE 4 Adaptive Fourth-Order Methods (Adapt-BCAGM, Adapt\u2212BCAGM+\u03A8\
      \ ) versus Non-Adaptive Fourth-Order Methods (BCAGM, BCAGM+\u03A8 [36])"
  Table 5 caption:
    table_text: "TABLE 5 Adaptive Third-Order Methods (Adapt-BCAGM3, Adapt\u2212BCAGM3+\u03A8\
      \ ) versus Non-Adaptive Fourth-Order Methods (BCAGM, BCAGM+\u03A8 [36])"
  Table 6 caption:
    table_text: TABLE 6 Average Running Time (in Seconds) of Higher-Order Algorithms
      on the Synthetic Outlier Settings
  Table 7 caption:
    table_text: TABLE 7 Average Running Time (in Seconds) of Higher-Order Algorithms
      on the CMU House Dataset
  Table 8 caption:
    table_text: TABLE 8 Average Running Time (in Seconds) of Higher-Order Algorithms
      on Willow Dataset
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2574706
- Affiliation of the first author: Not Available
  Affiliation of the last author: Not Available
  Figure 1 Link: Not Available
  Figure 1 caption: Not Available
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: Not Available
  Figure 2 caption: Not Available
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Ronen Basri
  Name of the last author: "Ren\xE9 Vidal"
  Number of Figures: Not Available
  Number of Tables: 0
  Number of authors: 4
  Paper title: 'Guest Editorial: Special Section on CVPR 2014'
  Publication Date: 2016-06-02 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2560278
- Affiliation of the first author: university of science and technology of china,
    hefei, anhui, china
  Affiliation of the last author: visual computing group, microsoft research, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2016\Faster_RCNN_Towards_RealTime_Object_Detection_with_Region_Proposal_Networks\figure_1.jpg
  Figure 1 caption: Different schemes for addressing multiple scales and sizes. (a)
    Pyramids of images and feature maps are built, and the classifier is run at all
    scales. (b) Pyramids of filters with multiple scalessizes are run on the feature
    map. (c) We use pyramids of reference boxes in the regression functions.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Faster_RCNN_Towards_RealTime_Object_Detection_with_Region_Proposal_Networks\figure_2.jpg
  Figure 2 caption: Faster R-CNN is a single, unified network for object detection.
    The RPN module serves as the 'attention' of this unified network.
  Figure 3 Link: articels_figures_by_rev_year\2016\Faster_RCNN_Towards_RealTime_Object_Detection_with_Region_Proposal_Networks\figure_3.jpg
  Figure 3 caption: 'Left: Region Proposal Network (RPN). Right: Example detections
    using RPN proposals on PASCAL VOC 2007 test. Our method detects objects in a wide
    range of scales and aspect ratios.'
  Figure 4 Link: articels_figures_by_rev_year\2016\Faster_RCNN_Towards_RealTime_Object_Detection_with_Region_Proposal_Networks\figure_4.jpg
  Figure 4 caption: Recall versus IoU overlap ratio on the PASCAL VOC 2007 test set.
  Figure 5 Link: articels_figures_by_rev_year\2016\Faster_RCNN_Towards_RealTime_Object_Detection_with_Region_Proposal_Networks\figure_5.jpg
  Figure 5 caption: Selected examples of object detection results on the PASCAL VOC
    2007 test set using the Faster R-CNN system. The model is VGG-16 and the training
    data is 07+12 trainval (73.2 percent mAP on the 2007 test set). Our method detects
    objects of a wide range of scales and aspect ratios. Each output box is associated
    with a category label and a softmax score in [0,1] . A score threshold of 0.6
    is used to display these images. The running time for obtaining these results
    is 198 ms per image, including all steps.
  Figure 6 Link: articels_figures_by_rev_year\2016\Faster_RCNN_Towards_RealTime_Object_Detection_with_Region_Proposal_Networks\figure_6.jpg
  Figure 6 caption: Selected examples of object detection results on the MS COCO test-dev
    set using the Faster R-CNN system. The model is VGG-16 and the training data is
    COCO trainval (42.7 percent mAP0.5 on the test-dev set). Each output box is associated
    with a category label and a softmax score in [0,1] . A score threshold of 0.6
    is used to display these images. For each image, one color represents one object
    category in that image.
  Figure 7 Link: articels_figures_by_rev_year\2016\Faster_RCNN_Towards_RealTime_Object_Detection_with_Region_Proposal_Networks\figure_7.jpg
  Figure 7 caption: Error analyses on models trained with and without MS COCO data.
    The test set is PASCAL VOC 2007 test. Distribution of top-ranked Cor (correct),
    Loc (false due to poor localization), Sim (confusion with a similar category),
    Oth (confusion with a dissimlar category), BG (fired on background) is shown,
    which is generated by the published diagnosis code of [40].
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.95
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Shaoqing Ren
  Name of the last author: Jian Sun
  Number of Figures: 7
  Number of Tables: 13
  Number of authors: 4
  Paper title: 'Faster R-CNN: Towards Real-Time Object Detection with Region Proposal
    Networks'
  Publication Date: 2016-06-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Learned Average Proposal Size for Each Anchor Using the
      ZF Net (Numbers for s=600 )
  Table 10 caption:
    table_text: TABLE 10 Detection Results of Faster R-CNN on PASCAL VOC 2007 Test
      Set Using Different Numbers of Proposals in Testing
  Table 2 caption:
    table_text: TABLE 2 Detection Results on PASCAL VOC 2007 Test Set (Trained on
      VOC 2007 Trainval)
  Table 3 caption:
    table_text: TABLE 3 Detection Results on PASCAL VOC 2007 Test Set
  Table 4 caption:
    table_text: TABLE 4 Detection Results on PASCAL VOC 2012 Test Set
  Table 5 caption:
    table_text: TABLE 5 Timing (ms) on a K40 GPU, Except SS Proposal Is Evaluated
      in a CPU
  Table 6 caption:
    table_text: TABLE 6 Results on PASCAL VOC 2007 Test Set with Fast R-CNN Detectors
      and VGG-16
  Table 7 caption:
    table_text: TABLE 7 Results on PASCAL VOC 2012 Test Set with Fast R-CNN Detectors
      and VGG-16
  Table 8 caption:
    table_text: TABLE 8 Detection Results of Faster R-CNN on PASCAL VOC 2007 Test
      Set Using Different Settings of Anchors
  Table 9 caption:
    table_text: "TABLE 9 Detection Results of Faster R-CNN on PASCAL VOC 2007 Test\
      \ Set Using Different Values of \u03BB in Equation (1)"
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2577031
