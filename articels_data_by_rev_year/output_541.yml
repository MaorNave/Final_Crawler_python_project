- Affiliation of the first author: national laboratory of pattern recognition, institute
    of automation, chinese academy of sciences, beijing, china
  Affiliation of the last author: national laboratory of pattern recognition, institute
    of automation, chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2015\Exploring_Local_and_Overall_Ordinal_Information_for_Robust_Feature_Description\figure_1.jpg
  Figure 1 caption: The workflow of our methods. For a region extracted by an affine
    covariant region detector, we firstly normalized it into a circular region of
    a fixed radius. Next, the overall intensity order is used to divide the patch
    into several subregions called ordinal bins. Then, the LIOP (OIOP) code is computed
    for each pixel in the patch. Finally, the LIOP (OIOP) descriptor is constructed
    by accumulating the LIOP (OIOP) codes of pixels in each ordinal bin respectively
    and then by concatenating them together.
  Figure 10 Link: articels_figures_by_rev_year\2015\Exploring_Local_and_Overall_Ordinal_Information_for_Robust_Feature_Description\figure_10.jpg
  Figure 10 caption: Average results by using Hessian-Affine, MSER, EBR and IBR regions
    in Oxford dataset.
  Figure 2 Link: articels_figures_by_rev_year\2015\Exploring_Local_and_Overall_Ordinal_Information_for_Robust_Feature_Description\figure_2.jpg
  Figure 2 caption: "The computation of LIOP. (a) and (b) illustrate the rotationally\
    \ invariant sampling strategy. (c) gives an example of computing the LIOP code\
    \ for a pixel according to the index table of \u03A0 4 in (d)."
  Figure 3 Link: articels_figures_by_rev_year\2015\Exploring_Local_and_Overall_Ordinal_Information_for_Robust_Feature_Description\figure_3.jpg
  Figure 3 caption: "Visualization of the computation of OIOP. Here we use 3 sampling\
    \ points ( p 1 , p 2 , p 3 ) and 4 quantization levels. (a) shows a normalized\
    \ region (referred to as a patch in this paper). (b) visualizes the quantization\
    \ values of sampling location p 1 , p 2 and p 3 respectively. For the image corresponding\
    \ to p i , each pixel x is assigned a specific color according to the quantization\
    \ value \u03B7( p i ;T) . Note that these distributions are quite different for\
    \ different sampling points, implying that each distribution could contribute\
    \ discriminative information to OIOP. (c) gives an example for the computation\
    \ of OIOP. For a pixel x , we first compute the quantization values of its local\
    \ sampling points as \u03B7( p 1 ;T) , \u03B7( p 2 ;T) and \u03B7( p 3 ;T) , then\
    \ convert them into OIOP code according to Eq. (7)."
  Figure 4 Link: articels_figures_by_rev_year\2015\Exploring_Local_and_Overall_Ordinal_Information_for_Robust_Feature_Description\figure_4.jpg
  Figure 4 caption: The quantization value distribution of different ordinal bins.
  Figure 5 Link: articels_figures_by_rev_year\2015\Exploring_Local_and_Overall_Ordinal_Information_for_Robust_Feature_Description\figure_5.jpg
  Figure 5 caption: The overall intensity order distribution for sampling points of
    pixels in the each ordinal bin when the number of ordinal bins is 4 . The red
    lines denote the learned positions according to Eq. (8) when the quantization
    level is 4 . Note that in the horizontal axis of these figures, the positions
    are normalized by the number of the pixels in the patch for illustration purpose.
  Figure 6 Link: articels_figures_by_rev_year\2015\Exploring_Local_and_Overall_Ordinal_Information_for_Robust_Feature_Description\figure_6.jpg
  Figure 6 caption: The average performance of the proposed descriptors on Harris-Affine
    region under different parameter settings.
  Figure 7 Link: articels_figures_by_rev_year\2015\Exploring_Local_and_Overall_Ordinal_Information_for_Robust_Feature_Description\figure_7.jpg
  Figure 7 caption: The average performance of OIOP (with standard quantization) and
    OIOP (with learning based quantization) across different parameter settings.
  Figure 8 Link: articels_figures_by_rev_year\2015\Exploring_Local_and_Overall_Ordinal_Information_for_Robust_Feature_Description\figure_8.jpg
  Figure 8 caption: Average results by using different number of support regions in
    Oxford dataset.
  Figure 9 Link: articels_figures_by_rev_year\2015\Exploring_Local_and_Overall_Ordinal_Information_for_Robust_Feature_Description\figure_9.jpg
  Figure 9 caption: Detailed results under various image transformations in Oxford
    dataset for Harris-Affine region. From left to the right, the results are of 1-2,
    1-3, 1-4 and 1-5 image pairs in each sequence.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Zhenhua Wang
  Name of the last author: Fuchao Wu
  Number of Figures: 16
  Number of Tables: 4
  Number of authors: 4
  Paper title: Exploring Local and Overall Ordinal Information for Robust Feature
    Description
  Publication Date: 2015-12-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Parameters of Our Descriptors
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 The Error Rates at 0.95 Recall on the Liberty, Notre Dame,
      and Yosemite Dataset (20,000 Matches and Nonmatches Are Used)
  Table 3 caption:
    table_text: TABLE 3 Object Recognition Accuracy on the ZuBuD and Kentucky Datasets
      with Different Local Descriptors
  Table 4 caption:
    table_text: TABLE 4 The Average Time Required for Describing One Feature for Each
      Method
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2513396
- Affiliation of the first author: "laboratoire hubert curien, universit\xE9 jean\
    \ monnet, saint etienne, france"
  Affiliation of the last author: "laboratoire hubert curien, universit\xE9 jean monnet,\
    \ saint etienne, france"
  Figure 1 Link: articels_figures_by_rev_year\2015\Joint_ColorSpatialDirectional_Clustering_and_Region_Merging_JCSDRM_for_Unsupervi\figure_1.jpg
  Figure 1 caption: Illustration of the proposed image generation model. The first
    row shows the color and depth image. The second row shows the model features in
    their respective spaces.
  Figure 10 Link: articels_figures_by_rev_year\2015\Joint_ColorSpatialDirectional_Clustering_and_Region_Merging_JCSDRM_for_Unsupervi\figure_10.jpg
  Figure 10 caption: Histogram of kappa for planar and non-planar surfaces.
  Figure 2 Link: articels_figures_by_rev_year\2015\Joint_ColorSpatialDirectional_Clustering_and_Region_Merging_JCSDRM_for_Unsupervi\figure_2.jpg
  Figure 2 caption: Block diagram of the proposed segmentation method.
  Figure 3 Link: articels_figures_by_rev_year\2015\Joint_ColorSpatialDirectional_Clustering_and_Region_Merging_JCSDRM_for_Unsupervi\figure_3.jpg
  Figure 3 caption: Illustration with an example of the proposed RGB-D segmentation
    method.
  Figure 4 Link: articels_figures_by_rev_year\2015\Joint_ColorSpatialDirectional_Clustering_and_Region_Merging_JCSDRM_for_Unsupervi\figure_4.jpg
  Figure 4 caption: "Illustration of a 3d view of the Region Adjacency Graph constructed\
    \ from JCSD clustered regions shown in Fig. 3. For each circle (node), its radius\
    \ represents the concentration ( \u03BA ) and its orientation represents the mean\
    \ direction ( \u03BC ) of the image normals of the associated region. Each edge\
    \ represents the weight ( w d or w b ) among two nodes. In this picture several\
    \ circles resemble ellipses because of 3D to 2D projection. The 2D view of this\
    \ graph overlaid on the original image is illustrated in Fig. 3."
  Figure 5 Link: articels_figures_by_rev_year\2015\Joint_ColorSpatialDirectional_Clustering_and_Region_Merging_JCSDRM_for_Unsupervi\figure_5.jpg
  Figure 5 caption: Illustration of the region merging predicate with different examples.
    The left column shows the RoI under process (surrounded by a black boundary) in
    the original image. The middle column shows the RoI (labeled as C) and the neighboring
    regions (labeled with numbers). The last column shows the magnitude of RGB-D image
    gradient.
  Figure 6 Link: articels_figures_by_rev_year\2015\Joint_ColorSpatialDirectional_Clustering_and_Region_Merging_JCSDRM_for_Unsupervi\figure_6.jpg
  Figure 6 caption: Illustration of the region merging strategy for a single regionnode.
    (a) shows the RoI under process (surrounded by a black boundary) in the original
    image. (b) shows the RoI (labeled as C) and the neighboring regions (labeled with
    numbers). (c) provides the computed values (NA means not necessary to compute)
    w.r.t. the RoI. (d) shows the regions after merging operation is completed for
    all RoIs.
  Figure 7 Link: articels_figures_by_rev_year\2015\Joint_ColorSpatialDirectional_Clustering_and_Region_Merging_JCSDRM_for_Unsupervi\figure_7.jpg
  Figure 7 caption: Histogram of GTRC [35] scores of different methods.
  Figure 8 Link: articels_figures_by_rev_year\2015\Joint_ColorSpatialDirectional_Clustering_and_Region_Merging_JCSDRM_for_Unsupervi\figure_8.jpg
  Figure 8 caption: Segmentation examples (from top to bottom) on NYU RGB-D database
    (NYUD2). (a) Input Color image (b) Input Depth image (c) Ground truth (d) JCSD-RM
    (our proposed) (e) UCM-RGBD [9] (f) GBS-CDN [42] (g) SP [7] and (h) GCF [10].
  Figure 9 Link: articels_figures_by_rev_year\2015\Joint_ColorSpatialDirectional_Clustering_and_Region_Merging_JCSDRM_for_Unsupervi\figure_9.jpg
  Figure 9 caption: Segmentation examples with lower GTRC scores (less than 0.4).
    (a) Input Color Image (b) Ground Truth Segmentation (c) Segmentation with the
    JCSD-RM method and (d) GTRC score.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Md. Abul Hasnat
  Name of the last author: "Alain Tr\xE9meau"
  Number of Figures: 10
  Number of Tables: 4
  Number of authors: 3
  Paper title: Joint Color-Spatial-Directional Clustering and Region Merging (JCSD-RM)
    for Unsupervised RGB-D Image Segmentation
  Publication Date: 2015-12-30 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Sensitivity of JCSD-RM with Respect to the Parameters k,\
      \ \u03BA p ,t h b ,t h d ,t h r "
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison with the State of the Art
  Table 3 caption:
    table_text: TABLE 3 Computation Time of JCSD-RM w.r.t. Different Image Scales
  Table 4 caption:
    table_text: TABLE 4 Comparison among Different Image Models and the Clustering
      Results withwithout the Region Merging Method
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2513407
- Affiliation of the first author: "computer vision laboratory, ic faculty, \xE9cole\
    \ polytechnique f\xE9d\xE9rale de lausanne, lausanne, switzerland"
  Affiliation of the last author: "computer vision laboratory, ic faculty, \xE9cole\
    \ polytechnique f\xE9d\xE9rale de lausanne, lausanne, switzerland"
  Figure 1 Link: articels_figures_by_rev_year\2015\Tracking_Interacting_Objects_Using_Intertwined_Flows\figure_1.jpg
  Figure 1 caption: 'Motivation for our approach. (a) Thresholding the probability
    occupancy map detector [2] scores for cars and people produces only one strong
    detection in this specific frame of a complete video sequence. (b) Linking people
    detections across frames using the K-shortest paths algorithm [5] reveals the
    presence of an additional person. (c) This additional person constitutes evidence
    for the presence of a car he will get in. This allows our algorithm to find the
    car as well in spite of the car detection failure. Because we treat people and
    cars symmetrically, the situation could have been reversed: The car could have
    been unambiguously detected and have served as evidence for the appearance of
    a person stepping out of it. This would not be the case if we tracked cars first
    and people potentially coming out of them next.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2015\Tracking_Interacting_Objects_Using_Intertwined_Flows\figure_2.jpg
  Figure 2 caption: A graph representing three spatial locations at three consecutive
    times. (a) Each ellipse denotes a spatial vertex, representing a spatial location
    at a time instant. Some are connected to a source and a sink to allow entrances
    and exits. (b) Each circle inside an ellipse denotes a pose vertex, representing
    a pose on a spatial location or a state of an object. In this case, there are
    four possible poses on each spatial location.
  Figure 3 Link: articels_figures_by_rev_year\2015\Tracking_Interacting_Objects_Using_Intertwined_Flows\figure_3.jpg
  Figure 3 caption: 'Flow constraints in a two-pose case. In each of the eight examples,
    the two circles represent two pose nodes at the same spatial location. The solid
    and the dotted arrows represent respectively non-zero flows g kj and f kj of the
    container and of the visible containee objects. Top Row: Forbidden configurations,
    which are all cases where a containee and a container coexist at the same location
    and at the same time instant without interacting with each other. For example,
    the configuration on the left could be interpreted as someone jumping in and out
    of the car at the same time. Bottom Row: Feasible configurations.'
  Figure 4 Link: articels_figures_by_rev_year\2015\Tracking_Interacting_Objects_Using_Intertwined_Flows\figure_4.jpg
  Figure 4 caption: "Construction of the tracklet graph. (a) We obtain a set of paths\
    \ by the graph pruning approach as described in Section 4.1. We use the solid\
    \ lines to denote the spatial trajectories obtained by the KSP approach described\
    \ in Step 1, and the dotted lines to denote the paths obtained by dynamic programming\
    \ in Step 2. We use the yellow circles to denote the joint spatial locations.\
    \ For each spatial tracklet \u03C4 q , there is a unique predecessor spatial vertex\
    \ and a unique successor, denoted by v t ( \u03C4 q ) and v T ( \u03C4 q ) respectively.\
    \ (b) We use t q and T q to denote the first and last time instant of \u03C4 q\
    \ respectively. We compute pose tracklets on \u03C4 q using dynamic programming.\
    \ The black arrows denote the pose transitions with the lowest total cost among\
    \ all transitions that connect the pose 0 at time t q to the pose 0 at time T\
    \ q . Therefore, we treat the shortest path 0-1-0 as a pose tracklet of \u03C4\
    \ q and collapse its vertices into a single vertex. (c) We use \u03B3 i q to denote\
    \ a pose tracklet of \u03C4 q . The graph can be further simplified by keeping\
    \ only those edges along the shortest path connecting a pose vertex at time t\
    \ q \u22121 to a pose vertex at time T q +1 . The black arrows highlight the two\
    \ edges along the shortest path from pose 0 at time t q \u22121 to pose 0 at time\
    \ T q +1 ."
  Figure 5 Link: articels_figures_by_rev_year\2015\Tracking_Interacting_Objects_Using_Intertwined_Flows\figure_5.jpg
  Figure 5 caption: 'Tracking results on five representative subsequences taken from
    our datasets. Top row. Sample frames with the detected container objects highlighted
    with circles and containee ones with dots. Bottom Row. Corresponding color-coded
    top-view trajectories for interacting objects in the scene. The arrows indicate
    the traversal direction and the numbers on the top-right corners are the frame
    indices. Note that, in the FIBA case and the ISSIA case, even though there are
    many players in the field, we plot only two trajectories: one for the ball and
    the other one for the player in possession of the ball.'
  Figure 6 Link: articels_figures_by_rev_year\2015\Tracking_Interacting_Objects_Using_Intertwined_Flows\figure_6.jpg
  Figure 6 caption: Comparing our proposed approach (TIF-MIP) against the baselines
    in terms of the MOTA scores. Our tracker yields a significant improvement on all
    datasets, thanks to the joint-global optimization on both container and containee
    objects. (a)-(g) We plot the MOTA curve w.r.t a range of overlap thresholds on
    the image plane. (h)-(i) We plot the MOTA curve w.r.t a range of distances between
    the detections and the ground truths on the ground plane.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Xinchao Wang
  Name of the last author: Pascal Fua
  Number of Figures: 6
  Number of Tables: 4
  Number of authors: 4
  Paper title: Tracking Interacting Objects Using Intertwined Flows
  Publication Date: 2015-12-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Notations
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Validation Sequences
  Table 3 caption:
    table_text: TABLE 3 Comparison of False Positive (FP) Rate, False Negative (FN)
      Rate and Identity Switches (IDS) Rate between All the Methods at Overlap Ratio
      of 0.5
  Table 4 caption:
    table_text: TABLE 4 Comparison of Computational Costs for PIF, TIF-LP and TIF-MIP
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2513406
- Affiliation of the first author: australian national university, canberra, act,
    australia
  Affiliation of the last author: beijing lab of intelligent information technology,
    beijing institute of technology, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2015\GoICP_A_Globally_Optimal_Solution_to_D_ICP_PointSet_Registration\figure_1.jpg
  Figure 1 caption: "Nonconvexity of the registration problem. Top: two 1D point-sets\
    \ x 1 , x 2 and y 1 , y 2 , y 3 . Bottom-left: residual error (closest-point distance)\
    \ for x 1 as a function of translation t ; the three dashed curves are \u2225\
    \ x 1 +t\u2212 y j \u2225 with j=1,2,3 respectively. Bottom-right: the overall\
    \ L 2 registration error; the two dashed curves are e i (t ) 2 with i=1,2 respectively.\
    \ The residual error functions are nonconvex, thus the L 2 error function is also\
    \ nonconvex."
  Figure 10 Link: articels_figures_by_rev_year\2015\GoICP_A_Globally_Optimal_Solution_to_D_ICP_PointSet_Registration\figure_10.jpg
  Figure 10 caption: 'Evolution of Go-ICP registration for the bunny dataset. The
    model point-set and data point-set are shown in red and green respectively. BnB
    and ICP collaboratively update the registration: ICP refines the solution found
    by BnB and BnB guides ICP into the convergence basins of multiple local minima
    with increasingly lower registration errors.'
  Figure 2 Link: articels_figures_by_rev_year\2015\GoICP_A_Globally_Optimal_Solution_to_D_ICP_PointSet_Registration\figure_2.jpg
  Figure 2 caption: "SE(3) space parameterization for BnB. Left: the rotation space\
    \ SO(3) is parameterized in a solid radius- \u03C0 ball with the angle-axis representation.\
    \ Right: the translation is assumed to be within a 3D cube [\u2212\u03BE,\u03BE\
    \ ] 3 where \u03BE can be readily set. The octree data-structure is used to divide\
    \ (branch) the domains and the yellow box in each diagram represents a sub-cube."
  Figure 3 Link: articels_figures_by_rev_year\2015\GoICP_A_Globally_Optimal_Solution_to_D_ICP_PointSet_Registration\figure_3.jpg
  Figure 3 caption: Distance computation from R r x to R r 0 x used in the derivation
    of the rotation uncertainty radius.
  Figure 4 Link: articels_figures_by_rev_year\2015\GoICP_A_Globally_Optimal_Solution_to_D_ICP_PointSet_Registration\figure_4.jpg
  Figure 4 caption: "Uncertainty radii at a point. Left: rotation uncertainty ball\
    \ for C r (in red) with center R r 0 x (blue dot) and radius \u03B3 r . Right:\
    \ translation uncertainty ball for C t (in red) with center x+ t 0 (blue dot)\
    \ and radius \u03B3 t . In both diagrams, the uncertainty balls enclose the range\
    \ of R r x or x+t (in green)."
  Figure 5 Link: articels_figures_by_rev_year\2015\GoICP_A_Globally_Optimal_Solution_to_D_ICP_PointSet_Registration\figure_5.jpg
  Figure 5 caption: "Deriving the lower bound. Any transformed data point R r x+t\
    \ lies within the uncertainty ball (in yellow) centered at R r 0 x+ t 0 with radius\
    \ \u03B3= \u03B3 r + \u03B3 t . Model points y j \u2217 and y j \u2217 0 are closest\
    \ to R r x+t and R r 0 x+ t 0 respectively. It is clear that a\u2264b\u2264c where\
    \ a= e i \u2013 \u2013 and c= e i ( R r ,t) . See text for more details."
  Figure 6 Link: articels_figures_by_rev_year\2015\GoICP_A_Globally_Optimal_Solution_to_D_ICP_PointSet_Registration\figure_6.jpg
  Figure 6 caption: 'Collaboration of BnB and ICP. Left: BnB and ICP collaboratively
    update the upper bounds during the search process. Right: with the guidance of
    BnB, ICP only explores un-discarded, promising cubes with small lower bounds marked
    up by BnB.'
  Figure 7 Link: articels_figures_by_rev_year\2015\GoICP_A_Globally_Optimal_Solution_to_D_ICP_PointSet_Registration\figure_7.jpg
  Figure 7 caption: "Remaining cubes of the BnBs. The first five figures show the\
    \ remaining cubes in the rotation \u03C0 -ball of the rotation BnBs, for an irregular\
    \ tetrahedron, a cuboid with three different side-lengths, a regular tetrahedron,\
    \ a regular cube, and a regular octahedron respectively. The last figure shows\
    \ a typical example of remaining cubes of a translation BnB, for the irregular\
    \ tetrahedron. (Best viewed when zoomed in.)"
  Figure 8 Link: articels_figures_by_rev_year\2015\GoICP_A_Globally_Optimal_Solution_to_D_ICP_PointSet_Registration\figure_8.jpg
  Figure 8 caption: A clustered scene (black circles) and the registration results
    of Go-ICP for the five shapes.
  Figure 9 Link: articels_figures_by_rev_year\2015\GoICP_A_Globally_Optimal_Solution_to_D_ICP_PointSet_Registration\figure_9.jpg
  Figure 9 caption: "Remaining rotation domains of the outer rotation BnB on 2D slices\
    \ of the \u03C0 -ball, for the synthetic points. Results using the DT and the\
    \ kd-tree are within magenta and green polygons, respectively. The white dots\
    \ denote optimal rotations. From left to right: a cuboid, a regular tetrahedron\
    \ and a regular cube. The colors on the slices indicate registration errors evaluated\
    \ via inner translation BnB: red for high error and blue for low error. (Best\
    \ viewed when zoomed in)"
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.65
  Name of the first author: Jiaolong Yang
  Name of the last author: Yunde Jia
  Number of Figures: 20
  Number of Tables: 1
  Number of authors: 4
  Paper title: 'Go-ICP: A Globally Optimal Solution to 3D ICP Point-Set Registration'
  Publication Date: 2015-12-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Running Time (in Seconds) of Go-ICP with DTs for the Registration
      of the Partially Overlapping Point-Sets in Fig. 16
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2015.2513405
- Affiliation of the first author: department of applied mathematics, university of
    barcelona, barcelona, spain
  Affiliation of the last author: department of applied mathematics, university of
    barcelona, barcelona, spain
  Figure 1 Link: articels_figures_by_rev_year\2016\Survey_on_RGB_D_Thermal_and_Multimodal_Approaches_for_Facial_Expression_Recognit\figure_1.jpg
  Figure 1 caption: In the 19th century, Duchenne de Boulogne conducted experiments
    on how FEs are produced. From [4].
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Survey_on_RGB_D_Thermal_and_Multimodal_Approaches_for_Facial_Expression_Recognit\figure_2.jpg
  Figure 2 caption: 'Primary emotions expressed on the face. From left to right: disgust,
    fear, joy, surprise, sadness, anger. From [14].'
  Figure 3 Link: articels_figures_by_rev_year\2016\Survey_on_RGB_D_Thermal_and_Multimodal_Approaches_for_Facial_Expression_Recognit\figure_3.jpg
  Figure 3 caption: Taxonomy for AFER in Computer Vision. Red corresponds to RGB,
    green to 3D, and purple to thermal.
  Figure 4 Link: articels_figures_by_rev_year\2016\Survey_on_RGB_D_Thermal_and_Multimodal_Approaches_for_Facial_Expression_Recognit\figure_4.jpg
  Figure 4 caption: Examples of lower and upper face AUs in FACS. Reprinted from [14].
  Figure 5 Link: articels_figures_by_rev_year\2016\Survey_on_RGB_D_Thermal_and_Multimodal_Approaches_for_Facial_Expression_Recognit\figure_5.jpg
  Figure 5 caption: Sample images from the LFPW dataset aligned with the SDM. Obtained
    from [81] .
  Figure 6 Link: articels_figures_by_rev_year\2016\Survey_on_RGB_D_Thermal_and_Multimodal_Approaches_for_Facial_Expression_Recognit\figure_6.jpg
  Figure 6 caption: 'General execution pipeline for the different modality fusion
    approaches. The tensor product symbols represent the modality fusion strategy.
    Approach-specific components of the pipeline are represented with different line
    types: dotted corresponds to early fusion, dashed to late fusion, dashed-dotted
    to direct data fusion and gray to sequential fusion.'
  Figure 7 Link: articels_figures_by_rev_year\2016\Survey_on_RGB_D_Thermal_and_Multimodal_Approaches_for_Facial_Expression_Recognit\figure_7.jpg
  Figure 7 caption: FE datasets. (a) The CK [190] dataset (top) contains posed exaggerated
    expressions. The CK+ [191] (bottom) extends CK by introducing spontaneous expressions.
    (b) MMI [192], the first dataset to contain profile views. (c) MultiPIE [193]
    has multiview samples under varying illumination conditions. (d) SFEW [194], an
    in the wild dataset. (e) Primary FEs in Bosphorus [195], a 3D dataset. (f) KTFE
    [196] dataset, thermal images of primary spontaneous FEs.
  Figure 8 Link: articels_figures_by_rev_year\2016\Survey_on_RGB_D_Thermal_and_Multimodal_Approaches_for_Facial_Expression_Recognit\figure_8.jpg
  Figure 8 caption: Historical evolution of AFER.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ciprian Adrian Corneanu
  Name of the last author: Sergio Escalera Guerrero
  Number of Figures: 8
  Number of Tables: 2
  Number of authors: 4
  Paper title: 'Survey on RGB, 3D, Thermal, and Multimodal Approaches for Facial Expression
    Recognition: History, Trends, and Affect-Related Applications'
  Publication Date: 2016-01-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 A Non-Comprehensive List of RGB FE Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 A Non-Comprehensive List of 3D and Thermal FE Datasets
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2515606
- Affiliation of the first author: eecs department at the university of california,
    berkeley, ca
  Affiliation of the last author: cse department at the university of california,
    san diego, la jolla, ca
  Figure 1 Link: articels_figures_by_rev_year\2016\Depth_Estimation_with_Occlusion_Modeling_Using_LightField_Cameras\figure_1.jpg
  Figure 1 caption: Comparison of depth estimation results of different algorithms
    from a light field input image. Darker represents closer and lighter represents
    farther. It can be seen that only our occlusion-aware algorithm successfully captures
    most of the holes in the basket, while other methods either smooth over them,
    or have artifacts as a result.
  Figure 10 Link: articels_figures_by_rev_year\2016\Depth_Estimation_with_Occlusion_Modeling_Using_LightField_Cameras\figure_10.jpg
  Figure 10 caption: Depth estimation results on our synthetic dataset. Some intensities
    in the insets are adjusted for better contrast. In the first example, our method
    successfully captures the shapes of the leaves, while all other methods generate
    smoothed results. In the second example, our method captures the holes in the
    chair as well as the thin structure of the lamp, while other methods obtain smoothed
    or thicker structures. In the last example, our method captures the thin structure
    of the lamp and the chandelier, while other methods fail or generate thickened
    results.
  Figure 2 Link: articels_figures_by_rev_year\2016\Depth_Estimation_with_Occlusion_Modeling_Using_LightField_Cameras\figure_2.jpg
  Figure 2 caption: Non-occluded versus occluded pixels. (a) At non-occluded pixels,
    all view rays converge to the same point in the scene if refocused to the correct
    depth. (b) However, photo-consistency fails to hold at occluded pixels, where
    some view rays will hit the occluder.
  Figure 3 Link: articels_figures_by_rev_year\2016\Depth_Estimation_with_Occlusion_Modeling_Using_LightField_Cameras\figure_3.jpg
  Figure 3 caption: "Light field occlusion model. (a) Pinhole model for central camera\
    \ image formation. An occlusion edge on the imaging plane corresponds to an occluding\
    \ plane in the 3D space. (b) The \u201Creversed\u201D pinhole model for light\
    \ field formation. It can be seen that when we refocus to the occluded plane,\
    \ we get a projection of the occluder on the camera plane, forming a reversed\
    \ pinhole camera model."
  Figure 4 Link: articels_figures_by_rev_year\2016\Depth_Estimation_with_Occlusion_Modeling_Using_LightField_Cameras\figure_4.jpg
  Figure 4 caption: Occlusions in different views. The insets are the angular patches
    of the red pixels when refocused to the correct depth. At the occlusion edge in
    the central view, the angular patch can be divided evenly into two regions, one
    with photo-consistency and one without. However, for pixels around the occlusion
    edge, although the central view is not occluded, some other views will still get
    occluded. Hence, the angular patch will not be photo-consistent, and will be unevenly
    divided into occluded and visible regions.
  Figure 5 Link: articels_figures_by_rev_year\2016\Depth_Estimation_with_Occlusion_Modeling_Using_LightField_Cameras\figure_5.jpg
  Figure 5 caption: "Color consistency constraint. (b)(e) We can see that when we\
    \ refocus to the correct depth, we get low variance in half the angular patch.\
    \ However, in (c)(f) although we refocused to an incorrect depth, it still gives\
    \ low variance response since the occluded plane is very textureless, so we get\
    \ a \u201Creversed\u201D angular patch. To address this, we add another constraint\
    \ that p 1 and p 2 should be similar to the averages of R 1 and R 2 in (d), respectively."
  Figure 6 Link: articels_figures_by_rev_year\2016\Depth_Estimation_with_Occlusion_Modeling_Using_LightField_Cameras\figure_6.jpg
  Figure 6 caption: Occlusion Predictor (Synthetic Scene). The intensities are adjusted
    for better contrast. F-measure is the harmonic mean of precision and recall compared
    to the ground truth. By combining three cues from depth, correspondence and refocus,
    we can obtain a better prediction of occlusions.
  Figure 7 Link: articels_figures_by_rev_year\2016\Depth_Estimation_with_Occlusion_Modeling_Using_LightField_Cameras\figure_7.jpg
  Figure 7 caption: Real-world results of different stages of our algorithm. We first
    apply edge detection on the central input, run our depth estimation algorithm
    on the light-field image to get an initial depth and an occlusion response prediction,
    and finally use the occlusion to regularize the initial depth to get a final depth
    map. We can then run the occlusion predictor on this final depth again to get
    a refined occlusion.
  Figure 8 Link: articels_figures_by_rev_year\2016\Depth_Estimation_with_Occlusion_Modeling_Using_LightField_Cameras\figure_8.jpg
  Figure 8 caption: (a) PR-curve of occlusion boundaries on dataset of Wanner et al.
    [27] (b) PR-curve on our dataset. (c) F-measure versus noise level. Our method
    achieves better results than current state-of-the-art methods, and is robust to
    noise.
  Figure 9 Link: articels_figures_by_rev_year\2016\Depth_Estimation_with_Occlusion_Modeling_Using_LightField_Cameras\figure_9.jpg
  Figure 9 caption: Depth estimation results on synthetic data by Wanner et al. [27].
    Some intensities in the insets are adjusted for better contrast. In the first
    example, note that our method correctly captures the shape of the doorwindow,
    while all other algorithms fail and produce smooth transitions. Similarly, in
    the second example our method reproduces accurate boundaries along the twigleaf,
    while other algorithms generate smoothed results or fail to capture the details,
    and have artifacts. Finally, in the last example, our method is the only one which
    can capture the antennas of the butterfly, and preserve the boundary of the wings,
    while other methods fail or generate smoothed results.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ting-Chun Wang
  Name of the last author: Ravi Ramamoorthi
  Number of Figures: 13
  Number of Tables: 1
  Number of authors: 3
  Paper title: Depth Estimation with Occlusion Modeling Using Light-Field Cameras
  Publication Date: 2016-01-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Depth RMSE on Synthetic Scenes
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2515615
- Affiliation of the first author: university of washington, seattle, wa
  Affiliation of the last author: vatic labs, san francisco, ca
  Figure 1 Link: articels_figures_by_rev_year\2016\Bayesian_NonParametric_Clustering_of_Ranking_Data\figure_1.jpg
  Figure 1 caption: "Sample- \u03C3 -Stagewise algorithm for exactly sampling \u03C3\
    \ from the conjugate posterior given \u03B8 ."
  Figure 10 Link: articels_figures_by_rev_year\2016\Bayesian_NonParametric_Clustering_of_Ranking_Data\figure_10.jpg
  Figure 10 caption: Test set log-likelihood of DPM and KDE on the Jester data, n=70,,t=5
    and training sample size N=100, 1,000, 3,000 , averaged over 10 replicates. The
    test set size is 3,000.
  Figure 2 Link: articels_figures_by_rev_year\2016\Bayesian_NonParametric_Clustering_of_Ranking_Data\figure_2.jpg
  Figure 2 caption: "Sample- \u03B8 -Slice algorithm for slice sampling \u03B8 given\
    \ \u03C3 ."
  Figure 3 Link: articels_figures_by_rev_year\2016\Bayesian_NonParametric_Clustering_of_Ranking_Data\figure_3.jpg
  Figure 3 caption: Slice-Gibbs algorithm for estimating a DPM of GM models.
  Figure 4 Link: articels_figures_by_rev_year\2016\Bayesian_NonParametric_Clustering_of_Ranking_Data\figure_4.jpg
  Figure 4 caption: "Sample- \u03C3 -N1 algorithm for sampling \u03C3 from the conjugate\
    \ posterior when N=1 ."
  Figure 5 Link: articels_figures_by_rev_year\2016\Bayesian_NonParametric_Clustering_of_Ranking_Data\figure_5.jpg
  Figure 5 caption: "Beta-Gibbs algorithm for estimating a DPM of GM models; \u25EF\
    \u2217 marks places where B ~ eta was approximated by Beta ."
  Figure 6 Link: articels_figures_by_rev_year\2016\Bayesian_NonParametric_Clustering_of_Ranking_Data\figure_6.jpg
  Figure 6 caption: "Relative error Beta B ~ eta\u22121 versus a the equivalent number\
    \ of inversions for various values of b and for n=10,20,50 . Note that b= N c\
    \ +\u03BD+2 is the smallest b for which we will resort to approximation; a is\
    \ lower bounded by \u03BD r j ."
  Figure 7 Link: articels_figures_by_rev_year\2016\Bayesian_NonParametric_Clustering_of_Ranking_Data\figure_7.jpg
  Figure 7 caption: Performance of Slice-Gibbs and Beta-Gibbs on four artificial datasets,
    averaged over 10 replicates. Each plot displays VI distance to the true data labeling.
  Figure 8 Link: articels_figures_by_rev_year\2016\Bayesian_NonParametric_Clustering_of_Ranking_Data\figure_8.jpg
  Figure 8 caption: 'Performance of Exact-Beta-Gibbs, Beta-Gibbs and Slice-Gibbs on
    artificial datasets 2 and 4, averaged over 10 replicates. Left: VI distance to
    the true data labeling. Right: posterior mean theta c,j versus rank j for the
    10 clusters found, averaged over the last 150 sampling steps of one replicate.'
  Figure 9 Link: articels_figures_by_rev_year\2016\Bayesian_NonParametric_Clustering_of_Ranking_Data\figure_9.jpg
  Figure 9 caption: 'Performance of DPM, EBMS, and KDE on a mixture of K Mallows models,
    with n=12,,t=5 and training sample size N=100,ldots 10,000 , averaged over 10
    replicates. The test set size is 3,000. Top: test set log-likelihood; bottom:
    VI distance to true data labeling. EBMS was too slow for the larger N ''s.'
  First author gender probability: 0.99
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.93
  Name of the first author: "Marina Meil\u0103"
  Name of the last author: Harr Chen
  Number of Figures: 11
  Number of Tables: 2
  Number of authors: 2
  Paper title: Bayesian Non-Parametric Clustering of Ranking Data
  Publication Date: 2016-01-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average Running Time of Exact-Beta-Gibbs, Beta-Gibbs and Slice-Gibbs
      on Dataset 2 and 4, over 10 Replicates, with Standard Deviation
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Top Courses of the Five Largest Clusters Found in a Representative
      Run of the DPM over College Admissions Data
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2515599
- Affiliation of the first author: department of computer science, university of north
    carolina, chapel hill, nc
  Affiliation of the last author: department of computer science, university of north
    carolina, chapel hill, nc
  Figure 1 Link: articels_figures_by_rev_year\2016\Parametric_Regression_on_the_Grassmannian\figure_1.jpg
  Figure 1 caption: "Illustration of parametric regression and inference. At the point\
    \ marked \u2297 , the objective for (1) traffic videos is to predict the independent\
    \ variable r \u2217 (e.g., speed), whereas for (2) corpus callosum shapes we seek\
    \ the manifold-valued Y \u2217 at specific values of the independent variable\
    \ (e.g., age). Elements on the Grassmannian are visualized as lines through the\
    \ origin, i.e., Y i \u2208G(1,2) ."
  Figure 10 Link: articels_figures_by_rev_year\2016\Parametric_Regression_on_the_Grassmannian\figure_10.jpg
  Figure 10 caption: Estimated time-warp functions for TW-GGR.
  Figure 2 Link: articels_figures_by_rev_year\2016\Parametric_Regression_on_the_Grassmannian\figure_2.jpg
  Figure 2 caption: Illustration of time-warped regression in R . The dashed straight-line
    (middle) shows the fitting result in the warped time coordinates, and the solid
    curve (right) demonstrates the fitting result to the original data points (left).
  Figure 3 Link: articels_figures_by_rev_year\2016\Parametric_Regression_on_the_Grassmannian\figure_3.jpg
  Figure 3 caption: Cubic spline regression in R . The left side shows the regression
    result, and the remaining plots show the other states.
  Figure 4 Link: articels_figures_by_rev_year\2016\Parametric_Regression_on_the_Grassmannian\figure_4.jpg
  Figure 4 caption: "CS-GGR ( 1 control point) versus Su et al. [16] ( \u03BB 1 \u03BB\
    \ 2 =10 ) in terms of the largest eigenvalue of the state-transition matrix A\
    \ of Eq. (18) (reconstructed from the observability matrices that we obtain along\
    \ each path) to the ground truth."
  Figure 5 Link: articels_figures_by_rev_year\2016\Parametric_Regression_on_the_Grassmannian\figure_5.jpg
  Figure 5 caption: Corpora callosa (with the subject's age) [29].
  Figure 6 Link: articels_figures_by_rev_year\2016\Parametric_Regression_on_the_Grassmannian\figure_6.jpg
  Figure 6 caption: Examples of the UCSD traffic dataset [44].
  Figure 7 Link: articels_figures_by_rev_year\2016\Parametric_Regression_on_the_Grassmannian\figure_7.jpg
  Figure 7 caption: 'Top: Example frames from the UCSD pedestrian dataset [45]. Bottom:
    Total crowd count over all frames (left), and average people count over a 400
    -frame sliding window (right).'
  Figure 8 Link: articels_figures_by_rev_year\2016\Parametric_Regression_on_the_Grassmannian\figure_8.jpg
  Figure 8 caption: Comparison between Std-GGR, TW-GGR and CS-GGR (with one control
    point) on the corpus callosum data [29]. The shapes are generated along the fitted
    curves and are colored by age (best viewed in color).
  Figure 9 Link: articels_figures_by_rev_year\2016\Parametric_Regression_on_the_Grassmannian\figure_9.jpg
  Figure 9 caption: Comparison between Std-GGR, TW-GGR and CS-GGR (with one control
    point) on the rat calvarium data (38 landmarks shown) [46]. The shapes are generated
    along the fitted curves and the landmarks are colored by age in days (best-viewed
    in color).
  First author gender probability: 0.61
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yi Hong
  Name of the last author: Marc Niethammer
  Number of Figures: 12
  Number of Tables: 3
  Number of authors: 5
  Paper title: Parametric Regression on the Grassmannian
  Publication Date: 2016-01-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of the Regression Results on Synthetic Data
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Comparison of Std-GGR, TW-GGR and CS-GGR with One (1) and\
      \ Two (2) Control Points to the Approaches of Rentmeesters [28] and Su et al.\
      \ [16] (for \u03BB 1 \u03BB 2 =110 )"
  Table 3 caption:
    table_text: "TABLE 3 Mean Energy and Mean Absolute Errors over All CV-Folds \xB1\
      1\u03C3 on Training and Testing Data"
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2516533
- Affiliation of the first author: faculty of computer and information science, university
    of ljubljana, ljubljana, slovenia
  Affiliation of the last author: faculty of computer and information science, university
    of ljubljana, ljubljana, slovenia
  Figure 1 Link: articels_figures_by_rev_year\2016\A_Novel_Performance_Evaluation_Methodology_for_SingleTarget_Trackers\figure_1.jpg
  Figure 1 caption: Examples of bounding boxes (red) at 0.5 overlap with the ground
    truth (green). Notice that the rectangles still fit the objects quite well.
  Figure 10 Link: articels_figures_by_rev_year\2016\A_Novel_Performance_Evaluation_Methodology_for_SingleTarget_Trackers\figure_10.jpg
  Figure 10 caption: The scatter plot for the woman sequence shows the failures for
    each tracker w.r.t. frame number.
  Figure 2 Link: articels_figures_by_rev_year\2016\A_Novel_Performance_Evaluation_Methodology_for_SingleTarget_Trackers\figure_2.jpg
  Figure 2 caption: Effects of re-initialization in performance estimators. The expected
    values and standard deviations of the estimators are shown in solid and dashed
    lines, respectively.
  Figure 3 Link: articels_figures_by_rev_year\2016\A_Novel_Performance_Evaluation_Methodology_for_SingleTarget_Trackers\figure_3.jpg
  Figure 3 caption: 'Summary of the VOT2014 dataset properties: frames sizes per sequence
    (a), ground truth bounding box sizes per sequence (b), number of frames per sequence
    (c), percentage of frames per visual attribute with number of frames per attribute
    in parentheses (d). The abbreviations CM, IC, OC, SC, MC and NE stand for camera
    motion, illumination change, occlusion, scale change, motion change and neutral
    attributes, respectively.'
  Figure 4 Link: articels_figures_by_rev_year\2016\A_Novel_Performance_Evaluation_Methodology_for_SingleTarget_Trackers\figure_4.jpg
  Figure 4 caption: Examples of the diversity of bounding box annotations for different
    images (top) and box plots of per-sequence distribution of ground truth overlaps.
  Figure 5 Link: articels_figures_by_rev_year\2016\A_Novel_Performance_Evaluation_Methodology_for_SingleTarget_Trackers\figure_5.jpg
  Figure 5 caption: Overlaps after re-initialization averaged over a large number
    of trackers and many re-initializations (top) and the derivative of this graph
    with respect to time (bottom). The derivative becomes negligible after 10 frames.
  Figure 6 Link: articels_figures_by_rev_year\2016\A_Novel_Performance_Evaluation_Methodology_for_SingleTarget_Trackers\figure_6.jpg
  Figure 6 caption: The mean and variance of estimators that apply per-frame (green)
    and per-sequence (red) visual attribute annotation. The dashed lines show average
    performance on the dataset. The abbreviations CM, IC, OC, SC, and MC are used
    for camera motion, illumination change, occlusion, scale change and motion change,
    respectively.
  Figure 7 Link: articels_figures_by_rev_year\2016\A_Novel_Performance_Evaluation_Methodology_for_SingleTarget_Trackers\figure_7.jpg
  Figure 7 caption: The AR ranking and raw plots for the baseline and bounding box
    perturbation experiments calculated by sequence-pooled ranking. A tracker is among
    top-performing if it resides close to the top-right corner of the plot.
  Figure 8 Link: articels_figures_by_rev_year\2016\A_Novel_Performance_Evaluation_Methodology_for_SingleTarget_Trackers\figure_8.jpg
  Figure 8 caption: The AR-rank plots of the baseline experiment with respect to the
    six sequence attributes. A tracker is among top-performing if it resides close
    to the top-right corner of the plot.
  Figure 9 Link: articels_figures_by_rev_year\2016\A_Novel_Performance_Evaluation_Methodology_for_SingleTarget_Trackers\figure_9.jpg
  Figure 9 caption: The AR-rank and raw plots for the baseline experiment with per-attribute
    normalization (upper row) and per-sequence normalization (bottom row).
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.95
  Name of the first author: Matej Kristan
  Name of the last author: "Luka \u010Cehovin"
  Number of Figures: 11
  Number of Tables: 7
  Number of authors: 9
  Paper title: A Novel Performance Evaluation Methodology for Single-Target Trackers
  Publication Date: 2016-01-12 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Influence of Different Burn-In Values on Raw Accuracy
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Robustness Raw and Rank Values for Different Values of Frames
      Skipped N skip
  Table 3 caption:
    table_text: TABLE 3 Rank Variance (var.) with (T) and without (N) Difference Tests
      for Accuracy and Robustness Computed for Sequence-Pooled (Seq. pool.) and Attribute-Normalized
      (Att. norm.) Setting
  Table 4 caption:
    table_text: TABLE 4 Performance of Estimators with Re-Initialization, Y(WIR) ,
      and without Re-Initialization, N(NOR) Indicated in the Column Denoted by R
  Table 5 caption:
    table_text: TABLE 5 Ranking Results of the Baseline and Bounding Box Perturbation
      Experiments without Rank Normalization (Sequence-Pooled) and the Baseline Experiment
      with Per-Attribute Normalization
  Table 6 caption:
    table_text: 'TABLE 6 Tracking Difficulty for the Six Visual Attributes: Camera
      Motion (CM), Illumination Change (IC), Occlusion (OC), Object Size Change (SC),
      Object Motion Change (MC) and Neutral (NE)'
  Table 7 caption:
    table_text: TABLE 7 Sequence Difficulty from Tracking Perspective
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2516982
