- Affiliation of the first author: school of electrical and electronic engineering,
    yonsei university, seoul, south korea
  Affiliation of the last author: school of electrical and electronic engineering,
    yonsei university, seoul, south korea
  Figure 1 Link: articels_figures_by_rev_year\2018\DiscreteContinuous_Transformation_Matching_for_Dense_Semantic_Correspondence\figure_1.jpg
  Figure 1 caption: 'Visualization of our DCTM results: (a) Source image, (b) target
    image, (c), (d) ground truth correspondences, (e), (f), (g), (h) warped images
    and correspondences after discrete and continuous optimization, respectively.
    For semantically similar images undergoing non-rigid deformations, our DCTM estimates
    reliable correspondences by iteratively optimizing the discrete label space via
    continuous regularization.'
  Figure 10 Link: articels_figures_by_rev_year\2018\DiscreteContinuous_Transformation_Matching_for_Dense_Semantic_Correspondence\figure_10.jpg
  Figure 10 caption: Qualitative results on the PF-PASCAL benchmark [31]. The source
    images were warped to the target images using correspondences.
  Figure 2 Link: articels_figures_by_rev_year\2018\DiscreteContinuous_Transformation_Matching_for_Dense_Semantic_Correspondence\figure_2.jpg
  Figure 2 caption: Illustration of (a) FCSS descriptor [11] and (b) affine-FCSS descriptor.
    Within a support window, sampling patterns W l s and W l t are transformed according
    to affine transformation T i .
  Figure 3 Link: articels_figures_by_rev_year\2018\DiscreteContinuous_Transformation_Matching_for_Dense_Semantic_Correspondence\figure_3.jpg
  Figure 3 caption: Our DCTM method consists of discrete optimization and continuous
    optimization. Our DCTM method differs from the conventional PMF [24] by alternately
    optimizing the discrete label space and updating the discrete labels through continuous
    regularization.
  Figure 4 Link: articels_figures_by_rev_year\2018\DiscreteContinuous_Transformation_Matching_for_Dense_Semantic_Correspondence\figure_4.jpg
  Figure 4 caption: 'DCTM convergence: (a) Source image; (b) Target image; Iterative
    evolution of warped images (c), (e), (g) after discrete optimization and (d),
    (f), (h) after continuous optimization after iteration 1, 2, and 3. Our DCTM optimizes
    the discrete label space with continuous regularization during the iterations,
    which facilitates convergence and boosts matching performance.'
  Figure 5 Link: articels_figures_by_rev_year\2018\DiscreteContinuous_Transformation_Matching_for_Dense_Semantic_Correspondence\figure_5.jpg
  Figure 5 caption: 'Comparison of DCTM and CC-DCTM: (a), (b) Source and target images,
    warped images with estimated correspondences after discrete and continuous optimization
    in (c), (d) DCTM, and (e), (g) CC-DCTM, and (f), (h) their corresponding confidence
    maps. The estimated confidences in CC-DCTM effectively reduce the effects of outliers
    during an iteration (top) and alleviate the ambiguity between image structures
    and correspondence fields (bottom), which greatly improves matching quality and
    convergence.'
  Figure 6 Link: articels_figures_by_rev_year\2018\DiscreteContinuous_Transformation_Matching_for_Dense_Semantic_Correspondence\figure_6.jpg
  Figure 6 caption: Convergence analysis of DCTM and CC-DCTM on the TSS benchmark
    [29].
  Figure 7 Link: articels_figures_by_rev_year\2018\DiscreteContinuous_Transformation_Matching_for_Dense_Semantic_Correspondence\figure_7.jpg
  Figure 7 caption: Qualitative results on the TSS benchmark [29]. The source images
    were warped to the target images using correspondences.
  Figure 8 Link: articels_figures_by_rev_year\2018\DiscreteContinuous_Transformation_Matching_for_Dense_Semantic_Correspondence\figure_8.jpg
  Figure 8 caption: Average matching accuracy with respect to endpoint error threshold
    on the TSS benchmark [29].
  Figure 9 Link: articels_figures_by_rev_year\2018\DiscreteContinuous_Transformation_Matching_for_Dense_Semantic_Correspondence\figure_9.jpg
  Figure 9 caption: Qualitative results on the PF-WILLOW benchmark [30]. The source
    images were warped to the target images using correspondences.
  First author gender probability: 0.88
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.95
  Name of the first author: Seungryong Kim
  Name of the last author: Kwanghoon Sohn
  Number of Figures: 15
  Number of Tables: 7
  Number of authors: 4
  Paper title: Discrete-Continuous Transformation Matching for Dense Semantic Correspondence
  Publication Date: 2018-10-26 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Matching Accuracy Compared to State-of-the-Art Correspondence
      Techniques with Different Features and Matching Methods on the TSS Benchmark
      [29]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Matching Accuracy Compared to State-of-the-Art Correspondence
      Techniques on the TSS Benchmark [29]
  Table 3 caption:
    table_text: TABLE 3 Matching Accuracy Compared to State-of-the-Art Correspondence
      Techniques on the PF-WILLOW Benchmark [30]
  Table 4 caption:
    table_text: TABLE 4 Matching Accuracy Compared to State-of-the-Art Correspondence
      Techniques on the PF-PASCAL Benchmark [31]
  Table 5 caption:
    table_text: TABLE 5 Matching Accuracy Compared to State-of-the-Art Correspondence
      Techniques on the CUB-200-2011 Benchmark [32]
  Table 6 caption:
    table_text: TABLE 6 Matching Accuracy on the PASCAL-VOC Dataset [33]
  Table 7 caption:
    table_text: TABLE 7 Matching Accuracy on the Caltech-101 Dataset [34]
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2878240
- Affiliation of the first author: state key lab of intelligent technologies and systems,
    beijing national research center for information science and technology (bnrist),
    tsinghua university, beijing, china
  Affiliation of the last author: state key lab of intelligent technologies and systems,
    beijing national research center for information science and technology (bnrist),
    tsinghua university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2018\Runtime_Network_Routing_for_Efficient_Image_Classification\figure_1.jpg
  Figure 1 caption: Four different types of models. (a) is the full neural network,
    where all the parameters are preserved. (b) is a static efficient model obtained
    by pruning some of the connections or directly reducing the parameter size. (c)
    is a runtime incremental routing network. It has the same number of parameters
    as the full model, while only part of the network is routed according to the input
    image in an incremental way. (d) is a multi-path network, where there are several
    smaller branches. The network routes between one of the branches to obtain higher
    accuracy-calculation tradeoff. (c) and (d) are the models used in our RNR framework.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Runtime_Network_Routing_for_Efficient_Image_Classification\figure_2.jpg
  Figure 2 caption: 'The overall framework of our RNP. RNP consists of two sub-networks:
    the backbone CNN network and the decision network. The convolution kernels of
    backbone CNN network are dynamically pruned according to the output Q-value of
    the decision network, conditioned on the state forming from the last calculated
    feature maps.'
  Figure 3 Link: articels_figures_by_rev_year\2018\Runtime_Network_Routing_for_Efficient_Image_Classification\figure_3.jpg
  Figure 3 caption: Overall framework of our runtime routing for multi-path network.
    A decision network is designed to decide which path to take in a multi-path CNN
    conditioned on the output of previous layer.
  Figure 4 Link: articels_figures_by_rev_year\2018\Runtime_Network_Routing_for_Efficient_Image_Classification\figure_4.jpg
  Figure 4 caption: Different phases of multi-path CNN. (a) is the training phase
    and (b) is the testing phase.
  Figure 5 Link: articels_figures_by_rev_year\2018\Runtime_Network_Routing_for_Efficient_Image_Classification\figure_5.jpg
  Figure 5 caption: The average multiplication numbers of different classes in our
    intuitive experiment. We show the computation numbers for both the whole network
    (on the left) and the fully pruned convolutional layer conv3 (on the right). The
    results show that RNP succeeds to focus more on faces images by preserving more
    convolutional channels while prunes the network more when dealing with background
    images, reaching a good tradeoff between accuracy and speed.
  Figure 6 Link: articels_figures_by_rev_year\2018\Runtime_Network_Routing_for_Efficient_Image_Classification\figure_6.jpg
  Figure 6 caption: The results on CIFAR-10 (on the left) and CIFAR-100 (on the right).
    For vanilla curve, the rightmost point is the full model and the leftmost is the
    frac14 model. RNP outperforms naive channel reduction models consistently by a
    very large margin.
  Figure 7 Link: articels_figures_by_rev_year\2018\Runtime_Network_Routing_for_Efficient_Image_Classification\figure_7.jpg
  Figure 7 caption: Visualization of the original images and the feature maps of four
    convolutional groups, respectively. The presented feature maps are the average
    of corresponding convolutional groups.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.59
  Name of the first author: Yongming Rao
  Name of the last author: Jie Zhou
  Number of Figures: 7
  Number of Tables: 7
  Number of authors: 4
  Paper title: Runtime Network Routing for Efficient Image Classification
  Publication Date: 2018-10-26 00:00:00
  Table 1 caption:
    table_text: TABLE 1 The Architecture of Models Used in Our Experiments
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparisons of Increase of Top-5 Error on ILSVRC2012-val (%)
      for VGG-16 with Recent State-of-the-Art Methods, Where We Used 10.1% 10.1% Top-5
      Error Baseline as the Reference
  Table 3 caption:
    table_text: TABLE 3 The Increase of Top-1top-5 Error (%) and GPU Inference Time
      (ms) under Different Theoretical Speed-Up Ratios on the ILSVRC2012-val Set
  Table 4 caption:
    table_text: TABLE 4 Experimental Results on the CIFAR-10 and CIFAR-100 Dataset
      with ResNet-18 for Our Proposed RNR Method
  Table 5 caption:
    table_text: TABLE 5 Comparisons of the Image Classification Accuracy (%) on the
      CIFAR-10 and CIFAR-100 with Vanilla Speed-Up Solutions for ResNet-18 and ResNet-34
  Table 6 caption:
    table_text: TABLE 6 The GPU Inference Time under Different Theoretical Speed-Up
      Ratios on the CIFAR-100 Dataset
  Table 7 caption:
    table_text: TABLE 7 The Increase of Top-1Top-5 Error (%) and GPU Inference Time
      (ms) under Different Theoretical Speed-Up Ratios on the ILSVRC2012-val Set
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2878258
- Affiliation of the first author: department of computer science and engineering,
    university of california, riverside, usa
  Affiliation of the last author: department of electrical and computer engineering,
    university of california, riverside, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\ContextAware_Query_Selection_for_Active_Learning_in_Event_Recognition\figure_1.jpg
  Figure 1 caption: A sequence of a video stream [2] shows three new unlabeled activities
    - person getting out of a car ( a 1 ) at T+0s , person opening a car trunk ( a
    2 ) at T+7s , and person carrying an object ( a 3 ) at T+12s . These activities
    are spatio-temporally correlated, and this information can provide context. Conventional
    approaches to active learning for activity recognition do not exploit these relationships
    in order to select the most informative instances. However, our approach exploits
    context and actively selects instances (in this case a 2 ) that provide maximum
    information about other neighbors.
  Figure 10 Link: articels_figures_by_rev_year\2018\ContextAware_Query_Selection_for_Active_Learning_in_Event_Recognition\figure_10.jpg
  Figure 10 caption: Comparison of the effect of different action segmentation methods
    on 50Salads dataset. X-axis is the number of video sequences in the train set
    and Y-axis is the mean average precision. AutoSeg and GTSeg are automatic segmentation
    using TCN and truth segmentation respectively.
  Figure 2 Link: articels_figures_by_rev_year\2018\ContextAware_Query_Selection_for_Active_Learning_in_Event_Recognition\figure_2.jpg
  Figure 2 caption: Our proposed framework for learning activity models continuously.
    Please see the text in Section 1.1 for details.
  Figure 3 Link: articels_figures_by_rev_year\2018\ContextAware_Query_Selection_for_Active_Learning_in_Event_Recognition\figure_3.jpg
  Figure 3 caption: Illustration of a CRF for encoding the contextual information.
    Please see the text in Section 3 for details.
  Figure 4 Link: articels_figures_by_rev_year\2018\ContextAware_Query_Selection_for_Active_Learning_in_Event_Recognition\figure_4.jpg
  Figure 4 caption: This figure illustrates the partitioning of the graph G a of activity
    nodes into two subgraphs, (i) the graph G L a , whose nodes should be queried
    for labeling and (ii) the graph G NL a which is not labeled. The green dotted
    lines denote the links between the two subgraphs. As in Eqn. 15, the joint entropy
    (J.E.) of the entire graph can be expressed as summation of the J.E. of two the
    subgraphs minus the mutual information of the links in between them.
  Figure 5 Link: articels_figures_by_rev_year\2018\ContextAware_Query_Selection_for_Active_Learning_in_Event_Recognition\figure_5.jpg
  Figure 5 caption: An example run of our proposed active learning framework on a
    part of activity sequence from VIRAT dataset. Circles are activity nodes along
    with their class probability distribution. Edges have different thickness based
    on the pairwise mutual information. The node labels are - getting out of the vehicle
    (GOV), opening vehicle trunk (OVT), unloading from vehicle (UOV), closing vehicle
    trunk (CVT), and getting into the vehicle (GIV). Inference on the CRF (top) gives
    us marginal probability distribution of the nodes and edges. We use these distributions
    to compute entropy and mutual information. Relative mutual information is shown
    by the thickness of the edges, whereas entropy of the nodes are plotted below
    the top CRF. Equation 14 exploits entropy and mutual information criteria in order
    to select the most informative nodes (2-OVT, 3-UOV, and 7-OVT). We condition upon
    these nodes (filled) and perform inference again, which provides us more accurate
    recognition and a system with lower entropy (bottom plot).
  Figure 6 Link: articels_figures_by_rev_year\2018\ContextAware_Query_Selection_for_Active_Learning_in_Event_Recognition\figure_6.jpg
  Figure 6 caption: Performance comparison against other competitive active learning
    methods on four datasets such as (a) UCF50, (b) VIRAT, and (c) MPII-Cooking. The
    X-axis represents the number of manually labeled training instances, whereas the
    Y-axis represents correct recognition accuracy on a set of unseen test instances.
    Please see the text in Section 5.3 for detailed explanation. Best viewed in color.
  Figure 7 Link: articels_figures_by_rev_year\2018\ContextAware_Query_Selection_for_Active_Learning_in_Event_Recognition\figure_7.jpg
  Figure 7 caption: Performance comparison against other state-of-the-art batch and
    incremental methods. The X-axis represents the number of manually labeled training
    instances, whereas the Y-axis represents correct recognition accuracy on a set
    of unseen test instances. Please see the text in Section 5.4 for detailed explanation.
    Best viewed in color.
  Figure 8 Link: articels_figures_by_rev_year\2018\ContextAware_Query_Selection_for_Active_Learning_in_Event_Recognition\figure_8.jpg
  Figure 8 caption: Performance comparison among the four different variants of our
    proposed method. The X-axis represents the number of manually labeled training
    instances, whereas the Y-axis represents correct recognition accuracy on a set
    of unseen test instances. For a given value of X, all the method use same amount
    of manually labeled data, but the amount of labeled data can be different. Please
    see the text in Section 5.5 for detailed explanation. Best viewed in color.
  Figure 9 Link: articels_figures_by_rev_year\2018\ContextAware_Query_Selection_for_Active_Learning_in_Event_Recognition\figure_9.jpg
  Figure 9 caption: (a) Comparison with other state-of-the-art active learning methods.
    (b) Effect of number of query samples on the accuracy. (c) Performance comparison
    among the four variants. Please see the text in Section 5.6 for detailed explanation.
    The X-axis is the sequence number of training videos. At each step, we use twenty
    training sequences to update the model and evaluate on the test set. In this experiment,
    for each sequence, we select forty percent of the most informative instances to
    be labeled manually. The Y-axis is the accuracy of the prediction. Best viewed
    in color.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Mahmudul Hasan
  Name of the last author: Amit K. Roy-Chowdhury
  Number of Figures: 12
  Number of Tables: 1
  Number of authors: 4
  Paper title: Context-Aware Query Selection for Active Learning in Event Recognition
  Publication Date: 2018-10-30 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Our Results Against State-of-the-Art Batch and
      Incremental Methods
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2878696
- Affiliation of the first author: college of computer science, nankai university,
    tianjin, china
  Affiliation of the last author: school of computer sience and engineering, nanjing
    university of science and technology, nanjing, china
  Figure 1 Link: articels_figures_by_rev_year\2018\Richer_Convolutional_Features_for_Edge_Detection\figure_1.jpg
  Figure 1 caption: We build a simple network based on VGG16 [11] to produce side
    outputs (c-h). One can see that convolutional features become coarser gradually,
    and the intermediate layers (c,d,f,g) contain essential fine details that do not
    appear in other layers.
  Figure 10 Link: articels_figures_by_rev_year\2018\Richer_Convolutional_Features_for_Edge_Detection\figure_10.jpg
  Figure 10 caption: The precision-recall curves for the evaluation of boundary measure
    ( Fb [8]) and region measure ( Fop [41]) of classical image segmentation on the
    BSDS500 test set [9].
  Figure 2 Link: articels_figures_by_rev_year\2018\Richer_Convolutional_Features_for_Edge_Detection\figure_2.jpg
  Figure 2 caption: Our RCF network architecture. The input is an image with arbitrary
    sizes, and our network outputs an edge possibility map in the same size.
  Figure 3 Link: articels_figures_by_rev_year\2018\Richer_Convolutional_Features_for_Edge_Detection\figure_3.jpg
  Figure 3 caption: "The comparison with some competitors on the BSDS500 [9] dataset.\
    \ \u2020 means GPU time."
  Figure 4 Link: articels_figures_by_rev_year\2018\Richer_Convolutional_Features_for_Edge_Detection\figure_4.jpg
  Figure 4 caption: The evaluation results on the standard BSDS500 [9] and NYUD [36]
    datasets. The multiscale version of RCF is only evaluated on the BSDS500 dataset.
    Here, the solid lines represent CNN based methods, while the dotted lines represent
    non-deep algorithms.
  Figure 5 Link: articels_figures_by_rev_year\2018\Richer_Convolutional_Features_for_Edge_Detection\figure_5.jpg
  Figure 5 caption: "The comparison with some competitors on the NYUD dataset [36].\
    \ \u2020means GPU time."
  Figure 6 Link: articels_figures_by_rev_year\2018\Richer_Convolutional_Features_for_Edge_Detection\figure_6.jpg
  Figure 6 caption: 'Some examples of RCF. Top two rows: BSDS500 [9]. Bottom two rows:
    NYUD [36]. From Left to Right: origin image, ground truth, RCF edge map, RCF UCM
    map.'
  Figure 7 Link: articels_figures_by_rev_year\2018\Richer_Convolutional_Features_for_Edge_Detection\figure_7.jpg
  Figure 7 caption: The comparisons on the Multicue dataset [39]. The numbers in the
    parentheses mean standard deviations.
  Figure 8 Link: articels_figures_by_rev_year\2018\Richer_Convolutional_Features_for_Edge_Detection\figure_8.jpg
  Figure 8 caption: Results of some thought networks.
  Figure 9 Link: articels_figures_by_rev_year\2018\Richer_Convolutional_Features_for_Edge_Detection\figure_9.jpg
  Figure 9 caption: Evaluation results of boundaries ( Fb [8]) and regions ( Fop [41])
    on the BSDS500 test set [9].
  First author gender probability: 0.57
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.93
  Name of the first author: Yun Liu
  Name of the last author: Jinhui Tang
  Number of Figures: 12
  Number of Tables: 0
  Number of authors: 7
  Paper title: Richer Convolutional Features for Edge Detection
  Publication Date: 2018-10-31 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2878849
- Affiliation of the first author: school of computer, national university of defense
    technology, changsha, china
  Affiliation of the last author: school of electronics engineering and computer science,
    peking university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2018\Late_Fusion_Incomplete_MultiView_Clustering\figure_1.jpg
  Figure 1 caption: "Clustering accuracy, NMI and purity comparison with various missing\
    \ ratios on Flower17 and Flower102. The results of MVEC [14] on Flower102 are\
    \ not reported due to the \u201Cout of memory\u201D error."
  Figure 10 Link: articels_figures_by_rev_year\2018\Late_Fusion_Incomplete_MultiView_Clustering\figure_10.jpg
  Figure 10 caption: Clustering accuracy, NMI and purity comparison of MKKM-IK [30]
    and LF-MVC+ZF with various missing ratios on Flower17.
  Figure 2 Link: articels_figures_by_rev_year\2018\Late_Fusion_Incomplete_MultiView_Clustering\figure_2.jpg
  Figure 2 caption: "Clustering accuracy, NMI and purity comparison with various missing\
    \ ratios on Caltech102. The results of MVEC [14] on Caltech102-20, Caltech102-25\
    \ and Caltech102-30 are not reported due to the \u201Cout of memory\u201D error."
  Figure 3 Link: articels_figures_by_rev_year\2018\Late_Fusion_Incomplete_MultiView_Clustering\figure_3.jpg
  Figure 3 caption: Clustering accuracy, NMI and purity comparison with various missing
    ratios on UCI-Digital.
  Figure 4 Link: articels_figures_by_rev_year\2018\Late_Fusion_Incomplete_MultiView_Clustering\figure_4.jpg
  Figure 4 caption: Clustering accuracy, NMI and purity comparison with various missing
    ratios on Protein Fold.
  Figure 5 Link: articels_figures_by_rev_year\2018\Late_Fusion_Incomplete_MultiView_Clustering\figure_5.jpg
  Figure 5 caption: "Clustering accuracy, NMI and purity comparison with various missing\
    \ ratios on CCV. The results of MVEC [14] on CCV are not reported due to the \u201C\
    out of memory\u201D error."
  Figure 6 Link: articels_figures_by_rev_year\2018\Late_Fusion_Incomplete_MultiView_Clustering\figure_6.jpg
  Figure 6 caption: Running time comparison of different algorithms with various number
    of samples on Flower102, CCV and Caltech102-30 datasets.
  Figure 7 Link: articels_figures_by_rev_year\2018\Late_Fusion_Incomplete_MultiView_Clustering\figure_7.jpg
  Figure 7 caption: The clustering results by the learned mathbf H of LF-IMVC with
    iterations, where lambda is set as 2-3 on Flower17, Flower102, CCV, UCI-digtal
    and Caltech102-30 datasets in this experiment. The results in terms of ACC and
    purity with other missing ratios are similar and omitted due to space limit.
  Figure 8 Link: articels_figures_by_rev_year\2018\Late_Fusion_Incomplete_MultiView_Clustering\figure_8.jpg
  Figure 8 caption: The sensitivity of LF-IMVC with the variation of lambda on Flower17,
    Flower102, CCV, UCI-digtal and Caltech102-30 datasets. The results in terms of
    ACC and purity with other missing ratios are similar and omitted due to space
    limit.
  Figure 9 Link: articels_figures_by_rev_year\2018\Late_Fusion_Incomplete_MultiView_Clustering\figure_9.jpg
  Figure 9 caption: The objective value of LF-IMVC with iterations on Flower17, Flower102,
    CCV, UCI-digtal and Caltech102-30 datasets. The curves with other missing ratios
    are similar and omitted due to space limit.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.95
  Name of the first author: Xinwang Liu
  Name of the last author: Wen Gao
  Number of Figures: 10
  Number of Tables: 7
  Number of authors: 9
  Paper title: Late Fusion Incomplete Multi-View Clustering
  Publication Date: 2018-11-01 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Aggregated ACC, NMI and Purity Comparison (mean \xB1 \xB1\
      \ std) of Different Clustering Algorithms on Flower17 and Flower102"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Aggregated ACC, NMI and Purity Comparison (mean \xB1 \xB1\
      \ std) of Different Clustering Algorithms on Caltech102"
  Table 3 caption:
    table_text: "TABLE 3 Aggregated ACC, NMI and Purity Comparison (mean \xB1 \xB1\
      \ std) of Different Clustering Algorithms on UCI-Digital"
  Table 4 caption:
    table_text: TABLE 4 Datasets Used in Our Experiments
  Table 5 caption:
    table_text: "TABLE 5 Aggregated ACC, NMI and Purity Comparison (mean \xB1 \xB1\
      \ std) of Different Clustering Algorithms on Protein Fold"
  Table 6 caption:
    table_text: "TABLE 6 Aggregated ACC, NMI and Purity Comparison (mean \xB1 \xB1\
      \ std) of Different Clustering Algorithms on CCV"
  Table 7 caption:
    table_text: "TABLE 7 Aggregated Running Time Comparison (mean \xB1 \xB1 std) of\
      \ Different Clustering Algorithms on Benchmark Datasets (in Seconds)"
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2879108
- Affiliation of the first author: hikvision research institute, santa clara, usa
  Affiliation of the last author: department of statistics, university of california,
    los angeles, los angeles, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\Cooperative_Training_of_Descriptor_and_Generator_Networks\figure_1.jpg
  Figure 1 caption: The flow chart of Algorithm D for training the descriptor network.
    The updating in Step D2 is based on the difference between the observed examples
    and the synthesized examples. The Langevin sampling of the synthesized examples
    from the current model in Step D1 can be time consuming.
  Figure 10 Link: articels_figures_by_rev_year\2018\Cooperative_Training_of_Descriptor_and_Generator_Networks\figure_10.jpg
  Figure 10 caption: Interpolation between latent vectors of the scene images on the
    two ends.
  Figure 2 Link: articels_figures_by_rev_year\2018\Cooperative_Training_of_Descriptor_and_Generator_Networks\figure_2.jpg
  Figure 2 caption: The flow chart of Algorithm G for training the generator network.
    The updating in Step G2 is based on the observed examples and their inferred latent
    factors. The Langevin sampling of the latent factors from the current posterior
    distribution in Step G1 can be time consuming.
  Figure 3 Link: articels_figures_by_rev_year\2018\Cooperative_Training_of_Descriptor_and_Generator_Networks\figure_3.jpg
  Figure 3 caption: The flow chart of the cooperative algorithm. The part of the flow
    chart for training the descriptor is similar to Algorithm D in Fig. 1, except
    that the D1 Langevin sampling is initialized from the initial synthesized examples
    supplied by the generator. The part of the flow chart for training the generator
    can also be mapped to Algorithm G in Fig. 2, except that the revised synthesized
    examples play the role of the observed examples, and the known generated latent
    factors can be used as inferred latent factors (or be used to initialize the G1
    Langevin sampling of the latent factors).
  Figure 4 Link: articels_figures_by_rev_year\2018\Cooperative_Training_of_Descriptor_and_Generator_Networks\figure_4.jpg
  Figure 4 caption: The MCMC teaching of the generator alternates between Markov transition
    and projection. The family of the generator models G is illustrated by the black
    curve. Each distribution is illustrated by a point.
  Figure 5 Link: articels_figures_by_rev_year\2018\Cooperative_Training_of_Descriptor_and_Generator_Networks\figure_5.jpg
  Figure 5 caption: "Generating texture patterns. Each row displays one texture experiment,\
    \ where the first image is the training image, and the rest are 3 of the images\
    \ generated by the CoopNets algorithm. The observed and synthesized images are\
    \ of size 224 \xD7 224 pixels."
  Figure 6 Link: articels_figures_by_rev_year\2018\Cooperative_Training_of_Descriptor_and_Generator_Networks\figure_6.jpg
  Figure 6 caption: "Generating object patterns. Each row displays one object experiment,\
    \ where the first 3 images are 3 of the training images, and the rest are 6 of\
    \ the synthesized images generated by the CoopNets algorithm. The observed and\
    \ synthesized images are of size 64\xD764 pixels."
  Figure 7 Link: articels_figures_by_rev_year\2018\Cooperative_Training_of_Descriptor_and_Generator_Networks\figure_7.jpg
  Figure 7 caption: 'Left: Average softmax class probability on single Imagenet-1k
    category versus the number of training images. Middle: Top 5 classification error.
    Right: Average pairwise structural similarity.'
  Figure 8 Link: articels_figures_by_rev_year\2018\Cooperative_Training_of_Descriptor_and_Generator_Networks\figure_8.jpg
  Figure 8 caption: "Generating scene patterns. Both observed and synthesized scene\
    \ images are shown for each category. The image size is 64\xD764 pixels. The categories\
    \ are from MIT places205 dataset. (a) volcano. (b) desert. (c) rock. (d) apartment\
    \ building."
  Figure 9 Link: articels_figures_by_rev_year\2018\Cooperative_Training_of_Descriptor_and_Generator_Networks\figure_9.jpg
  Figure 9 caption: "Generating scene patterns. (a) observed images randomly selected\
    \ from 10 Imagenet-1k scene categories. (b) synthesized images generated by CoopNets\
    \ learned from 10 Imagenet-1k scene categories. The training set consists of 1,100\
    \ images randomly sampled from each category. The observed and synthesized images\
    \ are of size 64 \xD7 64 pixels."
  First author gender probability: 0.63
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.86
  Name of the first author: Jianwen Xie
  Name of the last author: Ying Nian Wu
  Number of Figures: 17
  Number of Tables: 6
  Number of authors: 5
  Paper title: Cooperative Training of Descriptor and Generator Networks
  Publication Date: 2018-11-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Inception Scores of Different Methods on Learning from 10
      ImageNet-1k Scene Categories
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 A Comparison of Parzen Window-Based Log-Likelihood Estimates
      for MNIST Dataset
  Table 3 caption:
    table_text: "TABLE 3 The Performance of CoopNets, DCGAN, W-GAN, and VAE on LSUN\
      \ Bedrooms, CelebA and Cifar-10 Datasets with Respect to the Fr\xE9chet Inception\
      \ Distance (FID)"
  Table 4 caption:
    table_text: TABLE 4 Human Perceptual Study for Comparing Synthesis Qualities of
      Different Generative Models
  Table 5 caption:
    table_text: TABLE 5 A Comparison of Recovery Performances of Different Methods
      in 3 Experiments
  Table 6 caption:
    table_text: TABLE 6 A Comparison of Models for Synthesizing Dynamic Textures
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2879081
- Affiliation of the first author: "computer vision and multimodal computing group,\
    \ max planck institute for informatics, saarbr\xFCcken, germany"
  Affiliation of the last author: "computer vision and multimodal computing group,\
    \ max planck institute for informatics, saarbr\xFCcken, germany"
  Figure 1 Link: articels_figures_by_rev_year\2018\Person_Recognition_in_Personal_Photo_Collections\figure_1.jpg
  Figure 1 caption: In social media photos, depending on face occlusion or pose, different
    cues may be effective. For example, the surfer in the third column is not recognised
    using only head and body cues due to unusual pose. However, she is successfully
    recognised when additional attribute cues are considered.
  Figure 10 Link: articels_figures_by_rev_year\2018\Person_Recognition_in_Personal_Photo_Collections\figure_10.jpg
  Figure 10 caption: 'PIPA test set accuracy of methods on the frontal ( FR ), non-frontal
    ( NFR ), and no face detected ( NFD ) subsets. Left: Original split, right: Day
    split.'
  Figure 2 Link: articels_figures_by_rev_year\2018\Person_Recognition_in_Personal_Photo_Collections\figure_2.jpg
  Figure 2 caption: Face detections and head annotations in PIPA. The matches are
    determined by overlap (intersection over union). For matched faces (heads), the
    detector DPM component gives the orientation information (frontal versus non-frontal).
  Figure 3 Link: articels_figures_by_rev_year\2018\Person_Recognition_in_Personal_Photo_Collections\figure_3.jpg
  Figure 3 caption: Visualisation of Original, Album, Time and Day splits for three
    identities (rows 1-3). Greater appearance gap is observed from Original to Day
    splits.
  Figure 4 Link: articels_figures_by_rev_year\2018\Person_Recognition_in_Personal_Photo_Collections\figure_4.jpg
  Figure 4 caption: 'Regions considered for feature extraction: face f , head h ,
    upper body u , full body b , and scene s . More than one cue can be extracted
    per region (e.g., h 1 , h 2 ).'
  Figure 5 Link: articels_figures_by_rev_year\2018\Person_Recognition_in_Personal_Photo_Collections\figure_5.jpg
  Figure 5 caption: PIPA val set performance of different cues versus the SGD iterations
    in fine-tuning.
  Figure 6 Link: articels_figures_by_rev_year\2018\Person_Recognition_in_Personal_Photo_Collections\figure_6.jpg
  Figure 6 caption: Regions considered for analysis. We consider cues from head (
    h ) as well as upper body ( u ) sized patches that are either chosen randomly
    (r-patch, column 1) or in sliding window manner (sw, column 2). Recognition results
    for sw are visualised in Original (column 3) and Day (column 4) splits.
  Figure 7 Link: articels_figures_by_rev_year\2018\Person_Recognition_in_Personal_Photo_Collections\figure_7.jpg
  Figure 7 caption: Comparison of our region choice fhubs against three types of baseline
    region types, random patch (r-patch), sliding window (sw), and random initialisation
    (r-init), based on head ( h ) and upper body ( u ) sized regions. We report the
    val set accuracy against the number of cues used. The combination orders for r-patch
    and r-init are random; for sw, we combine regions close to the head first.
  Figure 8 Link: articels_figures_by_rev_year\2018\Person_Recognition_in_Personal_Photo_Collections\figure_8.jpg
  Figure 8 caption: "PIPA val set accuracy of naeil \u2295 \u03BB h deepid for varying\
    \ values of \u03BB . Round dots denote the maximal val accuracy."
  Figure 9 Link: articels_figures_by_rev_year\2018\Person_Recognition_in_Personal_Photo_Collections\figure_9.jpg
  Figure 9 caption: PIPA test set relative accuracy of various methods in the four
    splits, against the final system naeil2.
  First author gender probability: 0.9
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Seong Joon Oh
  Name of the last author: Bernt Schiele
  Number of Figures: 20
  Number of Tables: 7
  Number of authors: 4
  Paper title: Person Recognition in Personal Photo Collections
  Publication Date: 2018-11-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Split Statistics for val and test Sets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 PIPA val set Accuracy of Cues Based on Different Image Regions
      and Their Concatenations ( + + means Concatenation)
  Table 3 caption:
    table_text: TABLE 3 PIPA val set Accuracy of Different Scene Cues
  Table 4 caption:
    table_text: TABLE 4 PIPA val set Accuracy of Different Cues Based on Extended
      Data
  Table 5 caption:
    table_text: TABLE 5 PIPA Attributes Details
  Table 6 caption:
    table_text: TABLE 6 PIPA val set Accuracy of Methods Involving h deepid hdeepid
  Table 7 caption:
    table_text: TABLE 7 PIPA test Set Accuracy (%) of the Proposed Method and Prior
      Arts on the Four Splits
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2877588
- Affiliation of the first author: "instituto universitario para el desarrollo tecnol\xF3\
    gico y la innovaci\xF3n en comunicaciones, universidad de las palmas de gran canaria\
    \ campus de tafira, las palmas de gran canaria, spain"
  Affiliation of the last author: "polytechnique montr\xE9al, montr\xE9al, canada"
  Figure 1 Link: articels_figures_by_rev_year\2018\iDeLog_Iterative_Dual_Spatial_and_Kinematic_Extraction_of_SigmaLognormal_Paramet\figure_1.jpg
  Figure 1 caption: Block diagram of iDeLog framework to calculate the Sigma-Lognormal
    parameters of an on-line signature. Solid blue lines represent observed movement,
    while dotted red lines represent reconstructed movement.
  Figure 10 Link: articels_figures_by_rev_year\2018\iDeLog_Iterative_Dual_Spatial_and_Kinematic_Extraction_of_SigmaLognormal_Paramet\figure_10.jpg
  Figure 10 caption: DET curves of the Manhattan distance automatic signature verifier
    with the original and the reconstructed databases with ScripStudio, iDeLog with
    smoothing, and with iDeLog without smoothing. The databases are BiosecureID, MCYT100
    and SUSIGVisual.
  Figure 2 Link: articels_figures_by_rev_year\2018\iDeLog_Iterative_Dual_Spatial_and_Kinematic_Extraction_of_SigmaLognormal_Paramet\figure_2.jpg
  Figure 2 caption: 'Four lobes of an observed velocity profile with the points p
    2 , p 3 and p 4 used to calculate the lognormal of the third lobe. Red circles:
    maximum and minimum from the observed velocity profile. Dotted line: different
    lognormals approaching the third velocity lobe calculated from different t o .
    All the lognormals are so similar that the figure cannot tell them apart.'
  Figure 3 Link: articels_figures_by_rev_year\2018\iDeLog_Iterative_Dual_Spatial_and_Kinematic_Extraction_of_SigmaLognormal_Paramet\figure_3.jpg
  Figure 3 caption: Estimation of the target point t p 2 from the salient points s
    p 1 , s p 2 , and s p 3 .
  Figure 4 Link: articels_figures_by_rev_year\2018\iDeLog_Iterative_Dual_Spatial_and_Kinematic_Extraction_of_SigmaLognormal_Paramet\figure_4.jpg
  Figure 4 caption: "Estimation of values \u03B8 s 2 and \u03B8 e 2 for the second\
    \ stroke of the trajectory."
  Figure 5 Link: articels_figures_by_rev_year\2018\iDeLog_Iterative_Dual_Spatial_and_Kinematic_Extraction_of_SigmaLognormal_Paramet\figure_5.jpg
  Figure 5 caption: Procedure to calculate Dj , given the target points tpj - 1 ,
    tpj , and their start and end angles theta sj and theta ej .
  Figure 6 Link: articels_figures_by_rev_year\2018\iDeLog_Iterative_Dual_Spatial_and_Kinematic_Extraction_of_SigmaLognormal_Paramet\figure_6.jpg
  Figure 6 caption: Observed and reconstructed trajectory and velocity of a signature
    with initial parameters.
  Figure 7 Link: articels_figures_by_rev_year\2018\iDeLog_Iterative_Dual_Spatial_and_Kinematic_Extraction_of_SigmaLognormal_Paramet\figure_7.jpg
  Figure 7 caption: 'Left: Observed and reconstructed trajectory, salient points,
    virtual target points and arcs of circumference obtained with the initial parameters.
    Right: Observed and reconstructed signature after virtual target point displacement.'
  Figure 8 Link: articels_figures_by_rev_year\2018\iDeLog_Iterative_Dual_Spatial_and_Kinematic_Extraction_of_SigmaLognormal_Paramet\figure_8.jpg
  Figure 8 caption: Observed and reconstructed trajectory and velocity after optimization.
  Figure 9 Link: articels_figures_by_rev_year\2018\iDeLog_Iterative_Dual_Spatial_and_Kinematic_Extraction_of_SigmaLognormal_Paramet\figure_9.jpg
  Figure 9 caption: DET curves of the DTW automatic signature verifier with the original
    databases and the reconstructed databases with ScripStudio, iDeLog with smoothing,
    and with iDeLog without smoothing. The databases are BiosecureID, MCYT100 and
    SUSIGVisual.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Miguel A. Ferrer
  Name of the last author: "R\xE9jean Plamondon"
  Number of Figures: 10
  Number of Tables: 5
  Number of authors: 4
  Paper title: 'iDeLog: Iterative Dual Spatial and Kinematic Extraction of Sigma-Lognormal
    Parameters'
  Publication Date: 2018-11-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Mathematical Notations
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results of ScriptStudio and iDeLog
  Table 3 caption:
    table_text: TABLE 3 Mean and Standard Deviation of the Number of Lognormals Detected
      by ScriptStudio and iDeLog
  Table 4 caption:
    table_text: TABLE 4 Results of iDeLog with and without Smoothing
  Table 5 caption:
    table_text: TABLE 5 Area Between the DET Curves of the Original and Reconstructed
      Signatures Databases
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2879312
- Affiliation of the first author: bosch center for artificial intelligence, renningen,
    germany
  Affiliation of the last author: intelligent autonomous systems lab, tu darmstadt,
    darmstadt, germany
  Figure 1 Link: articels_figures_by_rev_year\2018\Numerical_Quadrature_for_Probabilistic_Policy_Search\figure_1.jpg
  Figure 1 caption: "A closed-loop control structure with controller \u03C0 \u03B8\
    \ , system dynamics f and target state x d . We model the system dynamics f as\
    \ a Gaussian process and assume that the policy \u03C0 \u03B8 is parameterized\
    \ by \u03B8 . The proposed algorithm learns f and \u03B8 from interactions with\
    \ the real system."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Numerical_Quadrature_for_Probabilistic_Policy_Search\figure_2.jpg
  Figure 2 caption: This figure illustrates multi-step-ahead prediction when the input
    is a distribution ( (a)-(c) are taken from [35]). Starting with a normally distributed
    state centered around the inflection point of the right slope in the mountain
    car domain (see Section 3), the state distribution is approximated at T=30 . The
    plots show the approximate state distribution obtained with numerical quadrature
    (a), moment matching (c) and the reference Monte Carlo sampling result, computed
    with 10 5 samples (b). As can be seen, the state distribution significantly differs
    from a Gaussian. Furthermore, moment matching does not match the first two moments
    of the state distribution. The distribution obtained with our NQ based approach
    (a) closely matches the distribution resulting from MC sampling (b). In case of
    MM (c), the iterative approximation as Gaussian amplifies the initial error. As
    a result, the distribution obtained by MM (a) is far off the sampled distribution.
    The KL-divergence between the Monte Carlo result and numerical quadrature (blue)
    MC and moment matching (red) as a function of system time are shown in plot (d).
  Figure 3 Link: articels_figures_by_rev_year\2018\Numerical_Quadrature_for_Probabilistic_Policy_Search\figure_3.jpg
  Figure 3 caption: Construction of suitable quadrature rules. Plot (a) shows the
    nodes of a Gaussian product quadrature rule on the unit square. Here, the quadrature
    rule for the unit square was constructed as an outer product of the Gaussian quadrature
    rule with 5 nodes. A state space partition with approximately 4000 rectangles
    for the mountain car system obtained with Algorithm 2 is shown in (b). For each
    rectangle in this partition, a Gaussian product quadrature such as (a) is employed.
  Figure 4 Link: articels_figures_by_rev_year\2018\Numerical_Quadrature_for_Probabilistic_Policy_Search\figure_4.jpg
  Figure 4 caption: Comparison of the numerical quadrature and moment matching approximations
    for policy search. Given a pre-trained GP dynamics model for the mountain car
    domain, a squashed linear policy was learned starting in each cell. Policy values
    and gradients were computed with numerical quadrature (plots (a) and (c)) and
    moment matching (plots (b) and (d)). With the learned policy, rollouts from the
    corresponding cell were computed with time horizons T=30 and T=90 . The cells
    are colored according to the average distance of the rollout final state to the
    target state (darker is better).
  Figure 5 Link: articels_figures_by_rev_year\2018\Numerical_Quadrature_for_Probabilistic_Policy_Search\figure_5.jpg
  Figure 5 caption: Average reward with standard deviation as a function of system
    interaction time for the cart-pole swing up and hold and the cart-double-pole
    balancing tasks. One episode equals 4 seconds of system interaction time. For
    both tasks, squashed linear policies were learned with our nuQuPS approach (blue)
    and Pilco (red). All results were averaged over five trials with different initial
    policies.
  Figure 6 Link: articels_figures_by_rev_year\2018\Numerical_Quadrature_for_Probabilistic_Policy_Search\figure_6.jpg
  Figure 6 caption: Average reward with standard deviation as a function of system
    interaction time for the mountain car task. One Episode equals 3 seconds of system
    interaction time. The task was to learn a squashed linear policy for all car starting
    positions in [-1,1]times [-0.1,0.1] . For nuQuPS, a uniform distribution over
    this region was chosen as the starting state. For Pilco, we chose 10 starting
    points on a regular grid in the starting region. PVI obtained rollouts with the
    starting point sampled from the uniform starting distribution at the end of each
    episode. After learning, a squashed, linear policy was fitted with a least squares
    approach.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.91
  Name of the first author: Julia Vinogradska
  Name of the last author: Jan Peters
  Number of Figures: 6
  Number of Tables: 0
  Number of authors: 5
  Paper title: Numerical Quadrature for Probabilistic Policy Search
  Publication Date: 2018-11-02 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2879335
- Affiliation of the first author: school of automation, southeast university, nanjing,
    china
  Affiliation of the last author: school of automation, southeast university, nanjing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2018\Inverse_Visual_Question_Answering_A_New_Benchmark_and_VQA_Diagnosis_Tool\figure_1.jpg
  Figure 1 caption: 'Illustration of iVQA: Input answers and images along with the
    top questions generated by our model.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Inverse_Visual_Question_Answering_A_New_Benchmark_and_VQA_Diagnosis_Tool\figure_2.jpg
  Figure 2 caption: Examples of belief sets of two VQA models (VQA1 and VQA2) on image
    I . Candidates are generated by corresponding RL-trained iVQA models (iVQA1 and
    iVQA2), and then verified by VQA models to build belief sets.
  Figure 3 Link: articels_figures_by_rev_year\2018\Inverse_Visual_Question_Answering_A_New_Benchmark_and_VQA_Diagnosis_Tool\figure_3.jpg
  Figure 3 caption: "Schematic of the proposed question generation model. The question\
    \ encoder encodes the image and question to the mean and variance of a Gaussian\
    \ distribution. Then the decoder takes an image feature vector f I , an answer\
    \ encoding a , and a noise vector z as inputs, and generates visual questions.\
    \ The noise vector z is sampled from N(\u03BC, \u03C3 2 \u22C51) during training,\
    \ and N(0,1) for sampling."
  Figure 4 Link: articels_figures_by_rev_year\2018\Inverse_Visual_Question_Answering_A_New_Benchmark_and_VQA_Diagnosis_Tool\figure_4.jpg
  Figure 4 caption: Qualitative results of iVQA. Larger numbers in brackets mean higher
    confidence.
  Figure 5 Link: articels_figures_by_rev_year\2018\Inverse_Visual_Question_Answering_A_New_Benchmark_and_VQA_Diagnosis_Tool\figure_5.jpg
  Figure 5 caption: Comparison of effects of different distractors on different models
    on the test set.
  Figure 6 Link: articels_figures_by_rev_year\2018\Inverse_Visual_Question_Answering_A_New_Benchmark_and_VQA_Diagnosis_Tool\figure_6.jpg
  Figure 6 caption: 'Schematic of the proposed belief set generation framework. There
    are three modules: (a) Sampling questions. Multiple questions are sampled from
    an iVQA model by varying z , which is drawn from a normal distribution. (b) Reward
    computation. The module takes image I , sample questions q i n i=1 , log probabilities
    for each path s i n i=1 as inputs, and returns the reward r( q i ) for each candidate
    q i using a given VQA model under diagnosis. (c) Policy gradient based update.
    The gradient is passed to the encoder and the decoder of the iVQA model to update
    the parameters of both.'
  Figure 7 Link: articels_figures_by_rev_year\2018\Inverse_Visual_Question_Answering_A_New_Benchmark_and_VQA_Diagnosis_Tool\figure_7.jpg
  Figure 7 caption: Examples of belief set categorisation. Note that GT stands for
    ground truth question; BS denotes the question in the belif set; and AN is the
    answer given the image and GT.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.51
  Name of the first author: Feng Liu
  Name of the last author: Changyin Sun
  Number of Figures: 7
  Number of Tables: 5
  Number of authors: 5
  Paper title: 'Inverse Visual Question Answering: A New Benchmark and VQA Diagnosis
    Tool'
  Publication Date: 2018-11-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Overall Question Generation Performance on the Testing Set
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 VQA versus iVQA: Bias and Multimodal Inputs'
  Table 3 caption:
    table_text: TABLE 3 Belief Set Compositions of Different VQA Models
  Table 4 caption:
    table_text: TABLE 4 Visualisation of the Top Beliefs of Different VQA Models
  Table 5 caption:
    table_text: "TABLE 5 VQA Accuracy on the Proposed Extra-Challenging VQA Dataset,\
      \ Where \u2020 \u2020 Indicates the Model Is Not Contribute to the Set of Hard\
      \ Examples"
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2880185
