- Affiliation of the first author: imperial college london, london, united kingdom
  Affiliation of the last author: university of california, santa cruz, ca, usa
  Figure 1 Link: "articels_figures_by_rev_year\\2018\\P\xF3lya_Urn_Latent_Dirichlet_Allocation_A_Doubly_Sparse_Massively_Parallel_Sampler\\\
    figure_1.jpg"
  Figure 1 caption: Directed acyclic graph for LDA.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: "articels_figures_by_rev_year\\2018\\P\xF3lya_Urn_Latent_Dirichlet_Allocation_A_Doubly_Sparse_Massively_Parallel_Sampler\\\
    figure_2.jpg"
  Figure 2 caption: "Log-posterior trace plots for standard partially collapsed LDA\
    \ and P\xF3lya Urn LDA, on a runtime scale (hours). Dashed line indicates completion\
    \ of 1,000 iterations."
  Figure 3 Link: "articels_figures_by_rev_year\\2018\\P\xF3lya_Urn_Latent_Dirichlet_Allocation_A_Doubly_Sparse_Massively_Parallel_Sampler\\\
    figure_3.jpg"
  Figure 3 caption: "Log-posterior trace plots for standard partially collapsed LDA\
    \ and P\xF3lya Urn LDA, on a per iteration scale."
  Figure 4 Link: "articels_figures_by_rev_year\\2018\\P\xF3lya_Urn_Latent_Dirichlet_Allocation_A_Doubly_Sparse_Massively_Parallel_Sampler\\\
    figure_4.jpg"
  Figure 4 caption: "Left: runtime for \u03A6 and z for P\xF3lya Urn LDA, as a percentage\
    \ of standard partially collapsed LDA \u2013 lower values are faster. Right: percentage\
    \ of runtime taken by \u03A6 and z for P\xF3lya Urn LDA."
  Figure 5 Link: "articels_figures_by_rev_year\\2018\\P\xF3lya_Urn_Latent_Dirichlet_Allocation_A_Doubly_Sparse_Massively_Parallel_Sampler\\\
    figure_5.jpg"
  Figure 5 caption: "Runtime and convergence for P\xF3lya Urn LDA, partially Collapsed\
    \ LDA, Fully Collapsed Sparse LDA, and Fully Collapsed Light LDA, on the NYT corpora."
  Figure 6 Link: "articels_figures_by_rev_year\\2018\\P\xF3lya_Urn_Latent_Dirichlet_Allocation_A_Doubly_Sparse_Massively_Parallel_Sampler\\\
    figure_6.jpg"
  Figure 6 caption: "Left: test set log-likelihood for P\xF3lya Urn LDA and Partially\
    \ Collapsed LDA. Right: Test set topic coherence for P\xF3lya Urn LDA and Partially\
    \ Collapsed LDA."
  Figure 7 Link: "articels_figures_by_rev_year\\2018\\P\xF3lya_Urn_Latent_Dirichlet_Allocation_A_Doubly_Sparse_Massively_Parallel_Sampler\\\
    figure_7.jpg"
  Figure 7 caption: "Left-most three plots: Runtime for P\xF3lya Urn LDA, Partially\
    \ Collapsed LDA, and Fully Collapsed Sparse LDA on a single core. Right-most two\
    \ plots: runtime for P\xF3lya Urn LDA versus number of available CPU cores."
  Figure 8 Link: "articels_figures_by_rev_year\\2018\\P\xF3lya_Urn_Latent_Dirichlet_Allocation_A_Doubly_Sparse_Massively_Parallel_Sampler\\\
    figure_8.jpg"
  Figure 8 caption: "Runtime for P\xF3lya Urn LDA for various rare word thresholds,\
    \ vocabulary sizes, and data sizes."
  Figure 9 Link: "articels_figures_by_rev_year\\2018\\P\xF3lya_Urn_Latent_Dirichlet_Allocation_A_Doubly_Sparse_Massively_Parallel_Sampler\\\
    figure_9.jpg"
  Figure 9 caption: Trace plots for the collapsed and uncollapsed Gibbs samplers for
    a textT distribution on mathbb R2 with rho in lbrace 0.9, 0.99, 0.999rbrace ,
    together with target distributions in grayscale. In 25 iterations, the uncollapsed
    Gibbs sampler has traversed the entire distribution multiple times, whereas the
    collapsed Gibbs sampler has not done so even once, covering increasingly less
    distance for larger rho .
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Alexander Terenin
  Name of the last author: David Draper
  Number of Figures: 9
  Number of Tables: 2
  Number of authors: 4
  Paper title: "P\xF3lya Urn Latent Dirichlet Allocation: A Doubly Sparse Massively\
    \ Parallel Sampler"
  Publication Date: 2018-06-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Notation for LDA
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Corpora Used in Experiments
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2832641
- Affiliation of the first author: university of chinese academy of sciences, beijing,
    china
  Affiliation of the last author: university of chinese academy of sciences, beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2018\Unifying_Visual_Attribute_Learning_with_Object_Recognition_in_a_Multiplicative_F\figure_1.jpg
  Figure 1 caption: Models for attribute learning. (a) Direct attribute prediction
    model; (b) indirect attribute prediction model; (c) category-sensitive attribute
    learning model; (d) our proposed multiplicative model. x,a and y denote image,
    attribute and label vectors respectively. W,U,V are model parameters. Bold lines
    mean known relationship.
  Figure 10 Link: articels_figures_by_rev_year\2018\Unifying_Visual_Attribute_Learning_with_Object_Recognition_in_a_Multiplicative_F\figure_10.jpg
  Figure 10 caption: Quality of individual attribute predictors trained with UMF-H
    by leveraging auxiliary data.
  Figure 2 Link: articels_figures_by_rev_year\2018\Unifying_Visual_Attribute_Learning_with_Object_Recognition_in_a_Multiplicative_F\figure_2.jpg
  Figure 2 caption: "Illustration of our method for predicting attribute \u201Cblue\u201D\
    . Category information helps to address the negative correlation problem. 0.5\
    \ denotes the decision boundary for attribute prediction."
  Figure 3 Link: articels_figures_by_rev_year\2018\Unifying_Visual_Attribute_Learning_with_Object_Recognition_in_a_Multiplicative_F\figure_3.jpg
  Figure 3 caption: Pipeline of attribute prediction for unseen categories.
  Figure 4 Link: articels_figures_by_rev_year\2018\Unifying_Visual_Attribute_Learning_with_Object_Recognition_in_a_Multiplicative_F\figure_4.jpg
  Figure 4 caption: UMF-Deep model integrating AlexNet with UMF-H. The black arrow
    indicates fully connection. The yellow circles are hidden units which are used
    for both attribute prediction and categorization.
  Figure 5 Link: articels_figures_by_rev_year\2018\Unifying_Visual_Attribute_Learning_with_Object_Recognition_in_a_Multiplicative_F\figure_5.jpg
  Figure 5 caption: Corrected attribute prediction by leveraging category information.
    The yellow box below the image shows the category label of the object.
  Figure 6 Link: articels_figures_by_rev_year\2018\Unifying_Visual_Attribute_Learning_with_Object_Recognition_in_a_Multiplicative_F\figure_6.jpg
  Figure 6 caption: Pairwise comparison of the relevant methods. Each cell counts
    the number of attributes which are predicted better by the topmost method than
    the rightmost method. For each attribute, we choose area under ROC curve (AUC)
    to evaluate the comparison methods.
  Figure 7 Link: articels_figures_by_rev_year\2018\Unifying_Visual_Attribute_Learning_with_Object_Recognition_in_a_Multiplicative_F\figure_7.jpg
  Figure 7 caption: Deep attribute prediction on aPascal-aYahoo and Caltech-UCSD birds
    datasets.
  Figure 8 Link: articels_figures_by_rev_year\2018\Unifying_Visual_Attribute_Learning_with_Object_Recognition_in_a_Multiplicative_F\figure_8.jpg
  Figure 8 caption: Attention area of UMF-Deep attribute predictor on aPaY. The yellow
    box indicates the attribute for visualization.
  Figure 9 Link: articels_figures_by_rev_year\2018\Unifying_Visual_Attribute_Learning_with_Object_Recognition_in_a_Multiplicative_F\figure_9.jpg
  Figure 9 caption: Enhanced attribute prediction on aPascal-aYahoo dataset by leveraging
    auxiliary data.
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.64
  Name of the first author: Kongming Liang
  Name of the last author: Xilin Chen
  Number of Figures: 11
  Number of Tables: 5
  Number of authors: 5
  Paper title: Unifying Visual Attribute Learning with Object Recognition in a Multiplicative
    Framework
  Publication Date: 2018-06-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Attribute Prediction (mAUC) on aPaY and CUB Datasets
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Attribute Prediction on aPaY Dataset with Auxiliary data
  Table 3 caption:
    table_text: TABLE 3 Attribute Prediction on Category-Attribute Pairs
  Table 4 caption:
    table_text: TABLE 4 Comparison of Different Zero-Shot Learning Methods
  Table 5 caption:
    table_text: TABLE 5 Performances on HICO Dataset
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2836461
- Affiliation of the first author: facebook ai research, menlo park, usa
  Affiliation of the last author: facebook ai research, menlo park, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\Mask_RCNN\figure_1.jpg
  Figure 1 caption: The Mask R-CNN framework for instance segmentation.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Mask_RCNN\figure_2.jpg
  Figure 2 caption: Mask R-CNN results on the COCO test set. These results are based
    on ResNet-101 [4], achieving a mask AP of 35.7 and running at 5 fps. Masks are
    shown in color, and bounding box, category, and confidences are also shown.
  Figure 3 Link: articels_figures_by_rev_year\2018\Mask_RCNN\figure_3.jpg
  Figure 3 caption: "Implementation of RoIAlign. The dashed grid is a feature map\
    \ on which RoIAlign is performed, the solid lines illustrate an RoI (with 2\xD7\
    2 bins in this example), and the dots denote the 4 sampling points within each\
    \ bin. The value of each sampling point is computed by bilinear interpolation\
    \ from the nearby grid points on the feature map. No quantization is performed\
    \ on any coordinates involved in the RoI, its bins, or the sampling points."
  Figure 4 Link: articels_figures_by_rev_year\2018\Mask_RCNN\figure_4.jpg
  Figure 4 caption: "Head Architecture: We extend two existing Faster R-CNN heads\
    \ [4], [14]. LeftRight panels show the heads for the ResNet C4 and FPN backbones,\
    \ from [4] and [14], respectively, to which a mask branch is added. Numbers denote\
    \ spatial resolution and channels. Arrows denote either conv, deconv, or fc layers\
    \ as can be inferred from context (conv preserves spatial dimension while deconv\
    \ increases it). All convs are 3\xD73, except the output conv which is 1\xD71,\
    \ deconvs are 2\xD72 with stride 2, and we use ReLU [30] in hidden layers. Left:\
    \ 'res5' denotes ResNet's fifth stage, which for simplicity we altered so that\
    \ the first conv operates on a 7\xD77 RoI with stride 1 (instead of 14\xD714 stride\
    \ 2 as in [4]). Right: '\xD74' denotes a stack of four consecutive convs."
  Figure 5 Link: articels_figures_by_rev_year\2018\Mask_RCNN\figure_5.jpg
  Figure 5 caption: More results of Mask R-CNN on COCO test images, using ResNet-101-FPN
    and running at 5 fps, with 35.7 mask AP (Table 1).
  Figure 6 Link: articels_figures_by_rev_year\2018\Mask_RCNN\figure_6.jpg
  Figure 6 caption: FCIS+++ [24] (top) versus Mask R-CNN (bottom, ResNet-101-FPN).
    FCIS exhibits systematic artifacts on overlapping objects.
  Figure 7 Link: articels_figures_by_rev_year\2018\Mask_RCNN\figure_7.jpg
  Figure 7 caption: Keypoint detection results on COCO test using Mask R-CNN (ResNet-50-FPN),
    with person segmentation masks predicted from the same model. This model has a
    keypoint AP of 63.1 and runs at 5 fps.
  Figure 8 Link: articels_figures_by_rev_year\2018\Mask_RCNN\figure_8.jpg
  Figure 8 caption: Mask R-CNN results on Cityscapes test (32.0 AP). The bottom-right
    image shows a failure prediction.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.96
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Kaiming He
  Name of the last author: Ross Girshick
  Number of Figures: 8
  Number of Tables: 9
  Number of authors: 4
  Paper title: Mask R-CNN
  Publication Date: 2018-06-05 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Instance Segmentation Mask AP on COCO test-dev
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Ablations
  Table 3 caption:
    table_text: TABLE 3 Object Detection Single-Model Results (Bounding Box AP), versus
      State-of-the-Art on test-dev
  Table 4 caption:
    table_text: TABLE 4 Advanced Results of Mask R-CNN on COCO minival, Showing Both
      Mask AP and Bounding Box AP
  Table 5 caption:
    table_text: TABLE 5 Keypoint Detection AP on COCO test-dev
  Table 6 caption:
    table_text: TABLE 6 Multi-Task Learning of Box, Mask, and Keypoint About the Person
      Category, Evaluated on minival
  Table 7 caption:
    table_text: TABLE 7 RoIAlign versus RoIPool for Keypoint Detection on minival
  Table 8 caption:
    table_text: TABLE 8 Advanced Results of Mask R-CNN for Keypoint Detection on minival
  Table 9 caption:
    table_text: TABLE 9 Results on Cityscapes val ('AP [val]' Column) and test (Remaining
      Columns) Sets
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2844175
- Affiliation of the first author: centre for artificial intelligence, university
    of technology sydney, ultimo, nsw, australia
  Affiliation of the last author: school of mathematics and statistics, ministry of
    education key lab of intelligent networks and network security, xi'an jiaotong
    university, xian shi, shaanxi, p.r. china
  Figure 1 Link: articels_figures_by_rev_year\2018\FewExample_Object_Detection_with_Model_Communication\figure_1.jpg
  Figure 1 caption: "A simplified version of MSPLD without multi-modal learning. The\
    \ blue boxes in the top row contain the training images where the few labeled\
    \ and the many unlabeled images are in the gray and yellow areas, respectively.\
    \ The gray solid box represents our detector, R-FCN. We train the detector using\
    \ the few annotated images. The detector generates reliable pseudo instance-level\
    \ labels and then gets improved with these pseudo-labeled bounding boxes, as shown\
    \ round 1. In the following rounds (iterations), the improved detector can generate\
    \ larger numbers of reliable pseudo-labels that further update the detector. When\
    \ the label generation and detector updating steps work iteratively, more pseudo\
    \ boxes are obtained from \u201Ceasy\u201D to \u201Chard\u201D, and the detector\
    \ becomes more robust."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\FewExample_Object_Detection_with_Model_Communication\figure_2.jpg
  Figure 2 caption: The working flow of MSPLD when multi-modal learning is integrated
    with Fig. 1. An example with three models is shown. The three discs with different
    colors indicate the basic detectors. The images in the middle are the training
    data. The three detectors complement each other in validating the selected training
    samples. For example, as shown in the bottom row, the 1st model only detects two
    objects and misalignment exists with the detected plant. The 2nd model detects
    three other objects. When considering the detections of the 1st model, the misaligned
    plant is corrected, and the car with the blue box is also used to train the 2nd
    model. So more training data with reliable labels are used to improve the performance
    of model 2. Similarly, the 3rd model obtains more pseudo boxes and gets updated
    in turn. The whole procedure iterates until convergence.
  Figure 3 Link: articels_figures_by_rev_year\2018\FewExample_Object_Detection_with_Model_Communication\figure_3.jpg
  Figure 3 caption: "The change of \u03BB , precision, recall and mAP for the first\
    \ four training iterations of MSPLD. \u201Cmv\u201D and \u201Cno\u201D denote\
    \ using and not using multi-modal learning, respectively. \u201CImgR\u201D and\
    \ \u201CInsR\u201D indicate the image-level and instance-level recall, respectively.\
    \ \u201CImgP\u201D and \u201CInsP\u201D indicate the image-level and instance-level\
    \ precision, respectively."
  Figure 4 Link: articels_figures_by_rev_year\2018\FewExample_Object_Detection_with_Model_Communication\figure_4.jpg
  Figure 4 caption: Some poorly located or missed training samples. The yellow rectangles
    are the generated labeled boxes, and the discs denote the ground-truth objects.
    In image 2, the green and purple circles indicate people and sofa, respectively.
    We observe that the sofa is missed due to occlusions and different people are
    not well separated.
  Figure 5 Link: articels_figures_by_rev_year\2018\FewExample_Object_Detection_with_Model_Communication\figure_5.jpg
  Figure 5 caption: "Performance comparison of MSPLD on PASCAL VOC 2007 using different\
    \ selection numbers for the initial labeled images. In \u201Cw image label\u201D\
    , we simply leverage the image label to filter the undesired pseudo boxes."
  Figure 6 Link: articels_figures_by_rev_year\2018\FewExample_Object_Detection_with_Model_Communication\figure_6.jpg
  Figure 6 caption: Qualitative results of MSPLD over the training iterations. The
    boxes with different colors indicate the generated pseudo boxes by our method
    for different classes.
  Figure 7 Link: articels_figures_by_rev_year\2018\FewExample_Object_Detection_with_Model_Communication\figure_7.jpg
  Figure 7 caption: Qualitative results of the inaccurate pseudo instance-level labels
    generated by MSPLD during the training procedure. The green boxes indicate the
    ground-truth object annotation. The yellow boxes indicate the generated pseudo
    boxes by MSPLD. The white blocks show the object classes.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.68
  Name of the first author: Xuanyi Dong
  Name of the last author: Deyu Meng
  Number of Figures: 7
  Number of Tables: 8
  Number of authors: 5
  Paper title: Few-Example Object Detection with Model Communication
  Publication Date: 2018-06-07 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Different Supervision Information Used in Weakly
      (Semi-) Supervised and Few-Example Object Detection Algorithms
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Method Comparisons in Average Precision (AP) on the PASCAL
      VOC 2007 Test Set
  Table 3 caption:
    table_text: TABLE 3 Method Comparisons in Correct Localization (CorLoc [64]) on
      the PASCAL VOC 2007 Trainval Set
  Table 4 caption:
    table_text: TABLE 4 Performance Comparison on PASCAL VOC 2007 of Different Proposal
      Generation Methods
  Table 5 caption:
    table_text: TABLE 5 Performance Comparison on the PASCAL VOC 2012, MS COCO 2014,
      and ILSVRC 2013 Datasets
  Table 6 caption:
    table_text: TABLE 6 The Performance of Each Detector Employed in MSPLD
  Table 7 caption:
    table_text: TABLE 7 Ablation Studies
  Table 8 caption:
    table_text: TABLE 8 Performance Comparison of MSPLD on PASCAL VOC 2007 Using Different
      Numbers of Noisy Images for the MSPLD Model with k=3 k=3 for Initialization
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2844853
- Affiliation of the first author: "department of engineering \u201Cenzo ferrari\u201D\
    , university of modena and reggio emilia, modena, mo, italy"
  Affiliation of the last author: "department of engineering \u201Cenzo ferrari\u201D\
    , university of modena and reggio emilia, modena, mo, italy"
  Figure 1 Link: articels_figures_by_rev_year\2018\Predicting_the_Drivers_Focus_of_Attention_The_DReyeVE_Project\figure_1.jpg
  Figure 1 caption: An example of visual attention while driving (d), estimated from
    our deep model using (a) raw video, (b) optical flow and (c) semantic segmentation.
  Figure 10 Link: articels_figures_by_rev_year\2018\Predicting_the_Drivers_Focus_of_Attention_The_DReyeVE_Project\figure_10.jpg
  Figure 10 caption: A single FoA branch of our prediction architecture. The COARSE
    module (see Fig. 9) is applied to both a cropped and a resized version of the
    input tensor, which is a videoclip of 16 consecutive frames. The cropped input
    is used during training to augment the data and the variety of ground truth fixation
    maps. The prediction of the resized input is stacked with the last frame of the
    videoclip and fed to a stack of convolutional layers (refinement module) with
    the aim of refining the prediction. Training is performed end-to-end and weights
    between COARSE modules are shared. At test time, only the refined predictions
    are used. Note that the complete model is composed of three of these branches
    (see Fig. 11), each of which predicting visual attention for different inputs
    (namely image, optical flow and semantic segmentation). All activations in the
    refinement module are LeakyReLU with alpha =10-3 , except for the last single
    channel convolution that features ReLUs. Crop and resize streams are highlighted
    by light blue and orange arrows respectively.
  Figure 2 Link: articels_figures_by_rev_year\2018\Predicting_the_Drivers_Focus_of_Attention_The_DReyeVE_Project\figure_2.jpg
  Figure 2 caption: 'Examples taken from a random sequence of DR(eye)VE. From left
    to right: frames from the eye tracking glasses with gaze data, from the roof-mounted
    camera, temporal aggregated fixation maps (as defined in Section 3) and overlays
    between frames and fixation maps.'
  Figure 3 Link: articels_figures_by_rev_year\2018\Predicting_the_Drivers_Focus_of_Attention_The_DReyeVE_Project\figure_3.jpg
  Figure 3 caption: Registration between the egocentric and roof-mounted camera views
    by means of SIFT descriptor matching.
  Figure 4 Link: articels_figures_by_rev_year\2018\Predicting_the_Drivers_Focus_of_Attention_The_DReyeVE_Project\figure_4.jpg
  Figure 4 caption: Resulting fixation map from a 1 second integration (25 frames).
    The adoption of the max aggregation of Eq. (1) allows to account in the final
    map two brief glances towards traffic lights.
  Figure 5 Link: articels_figures_by_rev_year\2018\Predicting_the_Drivers_Focus_of_Attention_The_DReyeVE_Project\figure_5.jpg
  Figure 5 caption: "Examples of the categorization of frames where gaze is far from\
    \ the mean. Overall, 108 060 frames ( \u223C 20 percent of DR(eye)VE) were extended\
    \ with this type of information."
  Figure 6 Link: articels_figures_by_rev_year\2018\Predicting_the_Drivers_Focus_of_Attention_The_DReyeVE_Project\figure_6.jpg
  Figure 6 caption: Mean frame (a) and fixation map (b) averaged across the whole
    sequence 02, highlighting the link between driver's focus and the vanishing point
    of the road.
  Figure 7 Link: articels_figures_by_rev_year\2018\Predicting_the_Drivers_Focus_of_Attention_The_DReyeVE_Project\figure_7.jpg
  Figure 7 caption: As speed gradually increases, driver's attention converges towards
    the vanishing point of the road. (a) When the car is approximately stationary,
    the driver is distracted by many objects in the scene. (b-e) As the speed increases,
    the driver's gaze deviates less and less from the vanishing point of the road.
    To measure this effect quantitatively, a two-dimensional Gaussian is fitted to
    approximate the mean map for each speed range, and the determinant of the covariance
    matrix Sigma is reported as an indication of its spread (the determinant equals
    the product of eigenvalues, each of which measures the spread along a different
    data dimension). The bar plots illustrate the amount of downtown (red), countryside
    (green) and highway (blue) frames that concurred to generate the average gaze
    position for a specific speed range. Best viewed on screen.
  Figure 8 Link: articels_figures_by_rev_year\2018\Predicting_the_Drivers_Focus_of_Attention_The_DReyeVE_Project\figure_8.jpg
  Figure 8 caption: Proportion of semantic categories that fall within the driver's
    fixation map when thresholded at increasing values (from left to right). Categories
    exhibiting a positive trend (e.g., road and vehicles) suggest a real attention
    focus, while a negative trend advocates for an awareness of the object which is
    only circumstantial. See Section 3.1 for details.
  Figure 9 Link: articels_figures_by_rev_year\2018\Predicting_the_Drivers_Focus_of_Attention_The_DReyeVE_Project\figure_9.jpg
  Figure 9 caption: The COARSE module is made of an encoder based on C3D network [65]
    followed by a bilinear upsampling (bringing representations back to the resolution
    of the input image) and a final 2D convolution. During feature extraction, the
    temporal axis is lost due to 3D pooling. All convolutional layers are preceded
    by zero paddings in order keep borders, and all kernels have size 3 along all
    dimensions. Pooling layers have size and stride of (1, 2, 2, 4) and (2, 2, 2,
    1) along temporal and spatial dimensions respectively. All activations are ReLUs.
  First author gender probability: 0.82
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Andrea Palazzi
  Name of the last author: Rita Cucchiara
  Number of Figures: 17
  Number of Tables: 4
  Number of authors: 5
  Paper title: 'Predicting the Driver''s Focus of Attention: The DR(eye)VE Project'
  Publication Date: 2018-06-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of the DR(eye)VE Dataset Characteristics
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 A Comparison Between DR(eye)VE and Other Datasets
  Table 3 caption:
    table_text: TABLE 3 Experiments Illustrating the Superior Performance of the multi-branch
      Model Over Several Baselines and Competitors
  Table 4 caption:
    table_text: TABLE 4 The Ablation Study Performed on Our multi-branch Model
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2845370
- Affiliation of the first author: department of computer science, city university
    of hong kong, kowloon tong, hong kong
  Affiliation of the last author: department of computer science, city university
    of hong kong, kowloon tong, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2018\DensityPreserving_Hierarchical_EM_Algorithm_Simplifying_Gaussian_Mixture_Models_\figure_1.jpg
  Figure 1 caption: (a) A typical framework for a first-order Markov model, where
    x t is the state at time t (e.g., object location) and y t is the observation
    at time t (e.g., image frame). (b). An illustration of model size growing with
    time. Simplification is applied to prevent it from growing too large.
  Figure 10 Link: articels_figures_by_rev_year\2018\DensityPreserving_Hierarchical_EM_Algorithm_Simplifying_Gaussian_Mixture_Models_\figure_10.jpg
  Figure 10 caption: Pipeline for visual tracking using GMM posteriors and density
    simplification.
  Figure 2 Link: articels_figures_by_rev_year\2018\DensityPreserving_Hierarchical_EM_Algorithm_Simplifying_Gaussian_Mixture_Models_\figure_2.jpg
  Figure 2 caption: Examples of reduced mixture models using different approaches.
    Each row compares DPHEM with a baseline method and shows a typical difference.
    The base mixture model contains K b =2500 components, and the reduced mixture
    contains K r components. KL is the KL divergence between the base mixture and
    the reduced mixture.
  Figure 3 Link: articels_figures_by_rev_year\2018\DensityPreserving_Hierarchical_EM_Algorithm_Simplifying_Gaussian_Mixture_Models_\figure_3.jpg
  Figure 3 caption: Recursive Bayesian filtering using GMMs. DPHEM is applied to simplify
    the posterior GMM.
  Figure 4 Link: articels_figures_by_rev_year\2018\DensityPreserving_Hierarchical_EM_Algorithm_Simplifying_Gaussian_Mixture_Models_\figure_4.jpg
  Figure 4 caption: "A 1D example of likelihood approximation using sum of scaled\
    \ Gaussians. \u0394 indicates calculation of the new residuals, r i = r i \u2212\
    \ f (k) ( x i ) ."
  Figure 5 Link: articels_figures_by_rev_year\2018\DensityPreserving_Hierarchical_EM_Algorithm_Simplifying_Gaussian_Mixture_Models_\figure_5.jpg
  Figure 5 caption: 'A 1D example of simplifying a KDE mixture for human location
    modelling. The top row shows KDEs representing spatial distributions of check-in
    locations ( x -axis represents location) at different hierarchical levels: 10
    check-ins of an individual ( p 1 ); 100 check-ins in a local region ( p 2 ); 10,000
    check-ins for the whole population ( p 3 ). In the bottom row, an individual''s
    spatial distribution is a KDE mixture, which is a weighted combination of the
    KDEs, and is simplified using DPHEM. k is the number of components in each KDE.'
  Figure 6 Link: articels_figures_by_rev_year\2018\DensityPreserving_Hierarchical_EM_Algorithm_Simplifying_Gaussian_Mixture_Models_\figure_6.jpg
  Figure 6 caption: Experiments on reducing 2D GMMs with 2500 components. (a) Average
    KL-divergence between the original and reduced models for different number of
    reduced components K r . (b) Processing time versus number of reduced components.
  Figure 7 Link: articels_figures_by_rev_year\2018\DensityPreserving_Hierarchical_EM_Algorithm_Simplifying_Gaussian_Mixture_Models_\figure_7.jpg
  Figure 7 caption: Scatter plots of the log-likelihoods of test events for approximate
    models with K r =90 versus the original KDE mixture.
  Figure 8 Link: articels_figures_by_rev_year\2018\DensityPreserving_Hierarchical_EM_Algorithm_Simplifying_Gaussian_Mixture_Models_\figure_8.jpg
  Figure 8 caption: 'KDE mixture reduction: (a-b) MSE and MAE of test-event log-likelihoods
    and (c) mean L2 distance of test-event likelihoods between mixture KDE and approximate
    models; (d) Average computing time per event; (e) Average learning time per iteration
    for each method. K r is the number of components in the approximate models.'
  Figure 9 Link: articels_figures_by_rev_year\2018\DensityPreserving_Hierarchical_EM_Algorithm_Simplifying_Gaussian_Mixture_Models_\figure_9.jpg
  Figure 9 caption: "Example of human location modeling: (a1) All check-in records\
    \ used for training and testing on map; (a2) All training and testing of one selected\
    \ individual on a 9\xD79 grid; (a3) The individual's training and testing data.\
    \ (b) Log-likelihood map of the individual's spatial distribution estimated from\
    \ check-in records with mixture KDE model (MixKDE) and the simplified models with\
    \ K r =90 . Red dots are the test events of this individual. meanLL is the average\
    \ log-likelihood of the test events. MSE and MAE are the mean squared error and\
    \ mean absolute error between the test event log-likelihoods predicted by the\
    \ original MixKDE and the approximate model, while MeanL2 is the mean L2 distance\
    \ between test event likelihoods. For better visualization, log-likelihood values\
    \ are truncated at -30."
  First author gender probability: 0.55
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Lei Yu
  Name of the last author: Antoni B. Chan
  Number of Figures: 14
  Number of Tables: 2
  Number of authors: 3
  Paper title: 'Density-Preserving Hierarchical EM Algorithm: Simplifying Gaussian
    Mixture Models for Approximate Inference'
  Publication Date: 2018-06-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of DPHEM with Related Works
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average Position and Angle Error of the First 10 Localized
      Frames for Each Test Video Sequence Using Stereo Visual Odometry
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2845371
- Affiliation of the first author: state key laboratory of synthetical automation
    for process industries, northeastern university, shenyang, p. r. china
  Affiliation of the last author: state key laboratory of synthetical automation for
    process industries, northeastern university, shenyang, p. r. china
  Figure 1 Link: articels_figures_by_rev_year\2018\Light_Field_Reconstruction_Using_Convolutional_Network_on_EPI_and_Extended_Appli\figure_1.jpg
  Figure 1 caption: "Comparison of light field reconstruction results on Stanford\
    \ microscope light field data Neurons 20\xD7 [14] using 3\xD73 input views. The\
    \ proposed learning-based EPI reconstruction produces better results in this challenging\
    \ case."
  Figure 10 Link: articels_figures_by_rev_year\2018\Light_Field_Reconstruction_Using_Convolutional_Network_on_EPI_and_Extended_Appli\figure_10.jpg
  Figure 10 caption: Depth estimation results using the reconstructed light fields.
    The arrows in the third row indicate the depth errors caused by the artifacts
    of the reconstructed light fields.
  Figure 2 Link: articels_figures_by_rev_year\2018\Light_Field_Reconstruction_Using_Convolutional_Network_on_EPI_and_Extended_Appli\figure_2.jpg
  Figure 2 caption: An illustration of EPI upsampling results. (a) The input low-angular-resolution
    EPI, where d=4 pixels is the disparity between the neighboring views. See the
    angular aliasing effects in the low-angular-resolution EPI; (b) Straightforward
    CNN-based angular super-resolution will cause aliasing in the EPI, leading to
    ghosting effects in the reconstructed light field; (c) The result after using
    EPI blur (on the spatial dimension) and bicubic interpolation (on the angular
    dimension); (d) The final high-angular-resolution EPI produced by the proposed
    framework; (e) The ground truth high-angular-resolution EPI.
  Figure 3 Link: articels_figures_by_rev_year\2018\Light_Field_Reconstruction_Using_Convolutional_Network_on_EPI_and_Extended_Appli\figure_3.jpg
  Figure 3 caption: "The proposed \u201Cblur-restoration-deblur\u201D framework for\
    \ light field reconstruction on an EPI."
  Figure 4 Link: articels_figures_by_rev_year\2018\Light_Field_Reconstruction_Using_Convolutional_Network_on_EPI_and_Extended_Appli\figure_4.jpg
  Figure 4 caption: Hierarchical reconstruction of the full light field. (a) The input
    light field is composed of the images marked in red; (b) In Step 1, the EPIs from
    the horizontal views (in the left dashed boxes) are used to generate the novel
    views marked in green, and the EPIs from the vertical views (in the right dashed
    boxes) are used to generate the novel views marked in blue. (c) In Step 2, the
    views generated from the Step 1 (in the dashed boxes) are used to produce the
    rest of views (marked in yellow).
  Figure 5 Link: articels_figures_by_rev_year\2018\Light_Field_Reconstruction_Using_Convolutional_Network_on_EPI_and_Extended_Appli\figure_5.jpg
  Figure 5 caption: The proposed detail restoration network is composed of three layers.
    The first and second layers are followed by a rectified linear unit (ReLU). The
    final output of the network is the sum of the predicted residual (detail) and
    the input.
  Figure 6 Link: articels_figures_by_rev_year\2018\Light_Field_Reconstruction_Using_Convolutional_Network_on_EPI_and_Extended_Appli\figure_6.jpg
  Figure 6 caption: "(a) A densely sampled EPI that contains four lines of different\
    \ slopes. The disparity is no greater than 1 pixel. (b) The Fourier spectrum of\
    \ the EPI in (a), where the lines in (a) are marked with arrows in their corresponding\
    \ colors. Note that the Fourier spectrum of the green line is occluded by the\
    \ \u03A9 u -axis. (c) The light field is downsampled in the angular dimensions,\
    \ producing an angularly undersampled EPI. The undersampling generates copies\
    \ of the Fourier spectrum, where one copy is indicated by a black arrow. (d) Direct\
    \ CNN-based super-resolution causes high-frequency leakage from the copies. The\
    \ band-limited filter shown in the black dashed box can only reconstruct light\
    \ field to a certain depth. The color bar on the right side of (b) shows the power\
    \ range of the Fourier spectrum after taking the logarithm."
  Figure 7 Link: articels_figures_by_rev_year\2018\Light_Field_Reconstruction_Using_Convolutional_Network_on_EPI_and_Extended_Appli\figure_7.jpg
  Figure 7 caption: "(a) The Fourier spectrum of the undersampled EPI after the \u201C\
    blur\u201D step, where the high-frequency copies are efficiently suppressed. (b)\
    \ The Fourier spectrum of the EPI after the \u201Crestoration\u201D step. Compared\
    \ with the Fourier spectrum in (a), the high-frequency components are restored,\
    \ while the high copies that lead to aliasing are remained unchanged. (c) The\
    \ Fourier spectrum of the EPI after being processed by the entire framework. (d)\
    \ The super-resolved EPI produced by the proposed framework."
  Figure 8 Link: articels_figures_by_rev_year\2018\Light_Field_Reconstruction_Using_Convolutional_Network_on_EPI_and_Extended_Appli\figure_8.jpg
  Figure 8 caption: Comparison of the proposed approach against other methods on the
    real-world scenes. The results show the ground truth images, error maps of the
    synthesized results in the Y channel, close-up versions of the image portions
    in the blue and yellow boxes, and the EPIs located at the red line shown in the
    ground truth view. The EPIs are upsampled to an appropriate scale in the angular
    dimension for better viewing. The lowest image in each block shows a close-up
    of the portion of the EPIs in the red box.
  Figure 9 Link: articels_figures_by_rev_year\2018\Light_Field_Reconstruction_Using_Convolutional_Network_on_EPI_and_Extended_Appli\figure_9.jpg
  Figure 9 caption: Comparison of the proposed approach against other methods on the
    microscope light field datasets. The results show the ground truth ( reference
    images), synthesized results, close-up versions, and the EPIs located at the red
    line shown in the ground truth view.
  First author gender probability: 0.75
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.89
  Name of the first author: Gaochang Wu
  Name of the last author: Tianyou Chai
  Number of Figures: 14
  Number of Tables: 5
  Number of authors: 5
  Paper title: Light Field Reconstruction Using Convolutional Network on EPI and Extended
    Applications
  Publication Date: 2018-06-08 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Quantitative Results (PSNR MS-SSIM) of Reconstructed Light
      Fields on the Real-World Scenes
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Quantitative Results (PSNR MS-SSIM) of Reconstructed Light
      Fields on the Microscope Light Field Datasets
  Table 3 caption:
    table_text: TABLE 3 Quantitative Results (PSNR MS-SSIM) of the Reconstructed Light
      Fields on the Synthetic Scenes of the HCI Datasets
  Table 4 caption:
    table_text: TABLE 4 RMSE Values of the Estimated Depth Using the Approach by Wang
      et al. [39] on HCI Datasets
  Table 5 caption:
    table_text: TABLE 5 Quantitative Results of the Disparity Map Range (pixel) and
      Error (RMSE) and Rendered Views (PSNR MS-SSIM)
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2845393
- Affiliation of the first author: visual computing center, king abdullah university
    of science and technology, thuwal, saudi arabia
  Affiliation of the last author: visual computing center, king abdullah university
    of science and technology, thuwal, saudi arabia
  Figure 1 Link: "articels_figures_by_rev_year\\2018\\\u2113p\u2113pBox_ADMM_A_Versatile_Framework_for_Integer_Programming\\\
    figure_1.jpg"
  Figure 1 caption: "Geometric illustration of the equivalence between \u2113 p -box\
    \ intersection and the set of binary points in R 2 . For clarity, we just show\
    \ the cases when p\u22081,2,5 ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: "articels_figures_by_rev_year\\2018\\\u2113p\u2113pBox_ADMM_A_Versatile_Framework_for_Integer_Programming\\\
    figure_2.jpg"
  Figure 2 caption: A hierarchical organization of widely used integer programming
    (IP) methods.
  Figure 3 Link: "articels_figures_by_rev_year\\2018\\\u2113p\u2113pBox_ADMM_A_Versatile_Framework_for_Integer_Programming\\\
    figure_3.jpg"
  Figure 3 caption: "Geometric illustration of the z 2 update process using the \u2113\
    \ p projection. Left: The moving path of z 2 through the \u2113 1 and \u2113 2\
    \ projection. Middle: Plots of the normalized moving distance d(\u03B8)d( \u03C0\
    \ 4 ) w.r.t. the normalized moving angle \u03B8 \u03C0 4 for different p values.\
    \ Right: The moving speed of z 2 at different places. See the text for a detailed\
    \ description."
  Figure 4 Link: "articels_figures_by_rev_year\\2018\\\u2113p\u2113pBox_ADMM_A_Versatile_Framework_for_Integer_Programming\\\
    figure_4.jpg"
  Figure 4 caption: "The optimization procedure of \u2113 2 -box ADMM on the segmentation\
    \ task of the cameraman image (resized to 100\xD7100 , i.e., n=1e4 ). Notice how\
    \ the continuous solution becomes more binary and how the energy decreases, getting\
    \ closer to the global minimum (min-cut result). At convergence (iteration 120),\
    \ the final solution is binary and its energy is only 0.4 percent away from the\
    \ global minimum."
  Figure 5 Link: "articels_figures_by_rev_year\\2018\\\u2113p\u2113pBox_ADMM_A_Versatile_Framework_for_Integer_Programming\\\
    figure_5.jpg"
  Figure 5 caption: "MRF-based segmentation results. The user can determine the unary\
    \ costs using free-hand strokes (1st column) or a simple rectangle similar to\
    \ [14] (2nd column). The user can add hard linear constraints (3rd column) by\
    \ identifying pixels belonging to the foreground (green) or background (red).\
    \ In the 4th and 5th columns, we show some multi-class results with K=3,4 . Note\
    \ how our \u2113 p -box method converges to discrete solutions that are very similar\
    \ to those of min-cut, a state-of-the-art application-specific algorithm."
  Figure 6 Link: "articels_figures_by_rev_year\\2018\\\u2113p\u2113pBox_ADMM_A_Versatile_Framework_for_Integer_Programming\\\
    figure_6.jpg"
  Figure 6 caption: "Graph matching results. For visualization purposes, the error\
    \ bars of the objective and accuracy (std) are made smaller ( 1 5 ) for all methods.\
    \ The runtime of the penalty method ( 311\xB123 seconds) is much higher than other\
    \ methods, thus it is not shown here. Since \u2113 p -box ADMM with different\
    \ p values (we use p=0.5,1,2,5,10 ) converge to the same objective and accuracy,\
    \ with slight differences in runtime, we only show the results of \u2113 2 -box\
    \ ADMM for simplicity."
  Figure 7 Link: "articels_figures_by_rev_year\\2018\\\u2113p\u2113pBox_ADMM_A_Versatile_Framework_for_Integer_Programming\\\
    figure_7.jpg"
  Figure 7 caption: "Four examples of generating the set of binary vectors in R 2\
    \ by intersecting of a pair of continuous sets using different \u2113 p norm spheres\
    \ and balls. For example, the first plot on the left is the intersection of a\
    \ shifted \u2113 2 -ball x:\u2225x\u2212 1 2 \u2225 2 2 \u2264 1 2 and the shifted\
    \ \u2113 1 -sphere x:\u2225x\u2212 1 2 \u2225 1 =1 ."
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.75
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Baoyuan Wu
  Name of the last author: Bernard Ghanem
  Number of Figures: 7
  Number of Tables: 2
  Number of authors: 2
  Paper title: "\u2113\np\n\u2113p-Box ADMM: A Versatile Framework for Integer Programming"
  Publication Date: 2018-06-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Energy Minimization Results on Cameraman Showing Mean(std)
      Values of the Energy of Eq. (24) and Runtime (Seconds) for All Methods Over
      5 Runs
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Clustering Results Showing Mean(std) Values of Three Measures:
      RI Score ( % %), Objective of Eq. (27), and Runtime (Seconds), Over 10 Random
      Runs of All Methods'
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2845842
- Affiliation of the first author: school of electrical and computer engineering,
    national technical university of athens, athens, greece
  Affiliation of the last author: "computational intelligence laboratory, institute\
    \ of informatics and telecommunications, national center for scientific research\
    \ \u201Cdemokritos\u201D, athens, greece"
  Figure 1 Link: articels_figures_by_rev_year\2018\Efficient_LearningFree_Keyword_Spotting\figure_1.jpg
  Figure 1 caption: 'Contrast-normalization examples: (a),(d) initial grayscale images,
    (b),(e) Sauvola''s binarization, (c),(f) proposed contrast normalization.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2018\Efficient_LearningFree_Keyword_Spotting\figure_2.jpg
  Figure 2 caption: "Different instances of the words \u201Cobject\u201D and \u201C\
    should\u201D, containing vertical size variation, before (a),(b),(c),(d) and after\
    \ (e),(f),(g),(h)."
  Figure 3 Link: articels_figures_by_rev_year\2018\Efficient_LearningFree_Keyword_Spotting\figure_3.jpg
  Figure 3 caption: Overview of the proposed mPOG descriptor extraction applied on
    an image part together with the corresponding reconstruction. The reconstructed
    orientation images visualizehighlight the stored information in each orientation.
  Figure 4 Link: articels_figures_by_rev_year\2018\Efficient_LearningFree_Keyword_Spotting\figure_4.jpg
  Figure 4 caption: Visualization of the matching procedure between a query and a
    word image using different zoning schemes ( nw = 4 & nd = 5 ).
  Figure 5 Link: articels_figures_by_rev_year\2018\Efficient_LearningFree_Keyword_Spotting\figure_5.jpg
  Figure 5 caption: Overview of the multi-instance selective matching for the case
    of multiple main zone detection.
  Figure 6 Link: articels_figures_by_rev_year\2018\Efficient_LearningFree_Keyword_Spotting\figure_6.jpg
  Figure 6 caption: 'Two indicative examples of the same word in each dataset: (a)
    George Washington, (b) IAM, (c) Bentham, (d) Modern, (e) Botany and (f) Konzilsprotokolle.'
  Figure 7 Link: articels_figures_by_rev_year\2018\Efficient_LearningFree_Keyword_Spotting\figure_7.jpg
  Figure 7 caption: Performance evaluation of the mPOG descriptor and the selective
    matching procedure on GW1 for different zoning parameters.
  Figure 8 Link: articels_figures_by_rev_year\2018\Efficient_LearningFree_Keyword_Spotting\figure_8.jpg
  Figure 8 caption: 'Two cases of erroneous retrieval: (Top row) 80 percent P5 (b)
    60 percent P5. Query is separated from the top 5 retrieved words with a dotted
    line. As it can be observed, PSeq-mPOG+MISM retrieves a set of words that have
    almost identical prefix or suffix.'
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: George Retsinas
  Name of the last author: Basilis Gatos
  Number of Figures: 8
  Number of Tables: 8
  Number of authors: 4
  Paper title: Efficient Learning-Free Keyword Spotting
  Publication Date: 2018-06-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Properties of the DatasetsSetups Used for Evaluation
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Evaluation Metrics on GW1 Dataset
  Table 3 caption:
    table_text: TABLE 3 Average Retrieval Time (Per Query) - Performance Trade-Off
      for the Proposed Re-Ranking Procedure (Using PSeq-mPOG+SM on GW1)
  Table 4 caption:
    table_text: TABLE 4 Resource Requirements for Sequential mPOG Features on GW1
  Table 5 caption:
    table_text: TABLE 5 MAP Evaluation on (a) GW2 and (b) GW3 & IAM Datasets
  Table 6 caption:
    table_text: TABLE 6 Evaluation Metrics for Bentham and Modern Datasets on ICFHR14
      Competition
  Table 7 caption:
    table_text: TABLE 7 Evaluation Metrics for Bentham Dataset on ICDAR15 Competition
  Table 8 caption:
    table_text: TABLE 8 MAP Evaluation on ICFHR16 Competition Datasets
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2845880
- Affiliation of the first author: brown university, providence, ri, usa
  Affiliation of the last author: sri international, menlo park, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2018\Differential_Geometry_in_Edge_Detection_Accurate_Estimation_of_Position_Orientat\figure_1.jpg
  Figure 1 caption: 'Gradient direction is not a good estimate of edge orientation.
    (a) This synthetically generated image illustrates how shading along a curve affects
    the gradient direction: The variation of intensity along the contour is increased
    from left to right. The first-order gradient direction (shown in red) is adversely
    affected while the Third-Order (TO) estimate (shown in green) is not. (b) Examples
    of the misalignment of edges with curves extracted from real images.'
  Figure 10 Link: articels_figures_by_rev_year\2018\Differential_Geometry_in_Edge_Detection_Accurate_Estimation_of_Position_Orientat\figure_10.jpg
  Figure 10 caption: A curve bundle spanning the size of the edge uncertainty is initialized
    at every edge. (a) The initial bundle is shown for the reference edge. (b)-(g)
    show how the bundle shrinks with additional constraints from intersecting the
    curve bundles of neighboring edges. The corresponding representations in the parameter
    space are shown on the right. Note that while this intersection takes place for
    all k0 , only slices through k0=-0.1 are shown to highlight the process of intersection.
  Figure 2 Link: articels_figures_by_rev_year\2018\Differential_Geometry_in_Edge_Detection_Accurate_Estimation_of_Position_Orientat\figure_2.jpg
  Figure 2 caption: 'How consistent is edge orientation with the underlying curve
    orientation? (a) Original image. Edges of three typical edge detectors, shown
    in red are compared with Third-Order edge orientation estimate of the same detection
    shown in green and the underlying curves shown in blue: (b) gradient-based (GB)
    detector, (c) gPb, (d) SE. In the second row, the difference between edge orientation
    and the underlying curve orientation is quantified as a histogram and shown as
    the dashed graph for GB, gPb, SE, and compared to our Third-Order versions of
    each detector shown as the solid graph. The cumulative histograms are shown at
    last. Similarly in the third row, the histogram of localization error is shown,
    comparing localization with that using our Third-Order estimates for GB, gPb and
    SE edge detectors. The cumulative histograms are shown at last.'
  Figure 3 Link: articels_figures_by_rev_year\2018\Differential_Geometry_in_Edge_Detection_Accurate_Estimation_of_Position_Orientat\figure_3.jpg
  Figure 3 caption: "The additional computation to complete the Third-Order process\
    \ is relatively high for very fast edge detectors (GB, SE) but is only a modest\
    \ 1-2 percent increase in the gPb process. Running times are shown for a typical\
    \ BSDS image (481\xD7361) running on a single core of Intel(R) Core(TM) i7-3970X\
    \ CPU 3.50G Hz. All the methods were implemented in matlab with core functions\
    \ implemented in C."
  Figure 4 Link: articels_figures_by_rev_year\2018\Differential_Geometry_in_Edge_Detection_Accurate_Estimation_of_Position_Orientat\figure_4.jpg
  Figure 4 caption: "(a) The curve passes exactly through legal perturbations of edges,\
    \ namely, perturbing edge location \u03B4x and edge orientation \u03B4\u03B8 ,\
    \ as shown in (b), within fixed bounds (\u03B4 x max ,\u03B4 \u03B8 max ) ."
  Figure 5 Link: articels_figures_by_rev_year\2018\Differential_Geometry_in_Edge_Detection_Accurate_Estimation_of_Position_Orientat\figure_5.jpg
  Figure 5 caption: 'The set of smooth curves passing through an oriented edge constrained
    by a model is called a curve bundle. The perturbations in location and orientation
    of the reference edge within the specified tolerance, (delta xmax, delta theta
    max) enlarge the bundle: (a) linear, (b) circular arc, and (c) Euler Spiral models.'
  Figure 6 Link: articels_figures_by_rev_year\2018\Differential_Geometry_in_Edge_Detection_Accurate_Estimation_of_Position_Orientat\figure_6.jpg
  Figure 6 caption: 'The curve bundle transport problem: Given a curve mathcal C ,
    in the curve bundle of edge e1 , defined by (delta x1, delta theta 1, k0) , what
    is the representation of mathcal C at edge e2 ( delta x2, delta theta 2, k0) ?'
  Figure 7 Link: articels_figures_by_rev_year\2018\Differential_Geometry_in_Edge_Detection_Accurate_Estimation_of_Position_Orientat\figure_7.jpg
  Figure 7 caption: (a) The curve bundle at edge P1 (shown in red) is transported
    to edge P2 and intersected with the bundle there leading to a significantly reduced
    space of curves.
  Figure 8 Link: articels_figures_by_rev_year\2018\Differential_Geometry_in_Edge_Detection_Accurate_Estimation_of_Position_Orientat\figure_8.jpg
  Figure 8 caption: A curve bundle volume can be transported by transporting each
    cross section (top), which in turn can be more efficiently done by transporting
    only the boundary vertices of each cross section (bottom).
  Figure 9 Link: articels_figures_by_rev_year\2018\Differential_Geometry_in_Edge_Detection_Accurate_Estimation_of_Position_Orientat\figure_9.jpg
  Figure 9 caption: (a) The space of curves spanned by two or more edges is obtained
    by intersecting the curve bundle at each edge, after properly transporting it
    to a common edge at different levels of k . (b) The overall curve space can be
    represented by a volume in (delta x, delta theta, k0) space. (c) A volume in (delta
    x, delta theta, k0) can be more efficiently represented by the two extreme values
    kmin(delta x, delta theta) and kmax(delta x, delta theta) at each discrete state
    in (delta x, delta theta) , which can be visualized as two height functions shown
    in (d).
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Benjamin B. Kimia
  Name of the last author: Amir Tamrakar
  Number of Figures: 24
  Number of Tables: 0
  Number of authors: 4
  Paper title: 'Differential Geometry in Edge Detection: Accurate Estimation of Position,
    Orientation and Curvature'
  Publication Date: 2018-06-12 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2018.2846268
