- Affiliation of the first author: department of electrical and computer engineering,
    new york university abu dhabi, abu dhabi, uae
  Affiliation of the last author: department of electrical and computer engineering,
    new york university abu dhabi, abu dhabi, uae
  Figure 1 Link: articels_figures_by_rev_year\2016\DeepShape_DeepLearned_Shape_Descriptor_for_D_Shape_Retrieval\figure_1.jpg
  Figure 1 caption: The framework of the proposed discriminative auto-encoder based
    shape descriptor.
  Figure 10 Link: articels_figures_by_rev_year\2016\DeepShape_DeepLearned_Shape_Descriptor_for_D_Shape_Retrieval\figure_10.jpg
  Figure 10 caption: The proposed descriptors for the clean and noisy models of crab
    and hand. In (a) and (c), the green and red shapes were generated with noise of
    R=0.01 and R=0.04 , respectively. In (b) and (d), the descriptors for the shapes
    are represented by the yellow, green and red curves, corresponding to the clean
    model, the noisy model with noise of R=0.01 , and the noisy model with noise of
    R=0.04 , respectively.
  Figure 2 Link: articels_figures_by_rev_year\2016\DeepShape_DeepLearned_Shape_Descriptor_for_D_Shape_Retrieval\figure_2.jpg
  Figure 2 caption: The multiscale shape distributions of the centaur and human models.
    The left two columns show the centaur models with isometric transformations and
    the corresponding multiscale shape distributions. The right two columns show the
    human models with non-isometric structural variations and the corresponding multiscale
    shape distributions, respectively.
  Figure 3 Link: articels_figures_by_rev_year\2016\DeepShape_DeepLearned_Shape_Descriptor_for_D_Shape_Retrieval\figure_3.jpg
  Figure 3 caption: Example shapes in the McGill dataset. The left three figures show
    the crab shapes while the right three figures show the Hand shapes with non-rigid
    transformations.
  Figure 4 Link: articels_figures_by_rev_year\2016\DeepShape_DeepLearned_Shape_Descriptor_for_D_Shape_Retrieval\figure_4.jpg
  Figure 4 caption: Example shapes with different transformations in the SHREC'10
    ShapeGoogle dataset. From left to right, the centaur shapes with isometry, isometry+topology,
    topology, partiality and triangulation transformations are shown.
  Figure 5 Link: articels_figures_by_rev_year\2016\DeepShape_DeepLearned_Shape_Descriptor_for_D_Shape_Retrieval\figure_5.jpg
  Figure 5 caption: Two human shapes with different poses in the SHREC'14 human dataset.
    The left three figures show shapes with pose changes from one person while the
    right three figures show shapes with different poses from another person.
  Figure 6 Link: articels_figures_by_rev_year\2016\DeepShape_DeepLearned_Shape_Descriptor_for_D_Shape_Retrieval\figure_6.jpg
  Figure 6 caption: Different kinds of armchair shapes in the SHREC'14 LSCRTB dataset.
  Figure 7 Link: articels_figures_by_rev_year\2016\DeepShape_DeepLearned_Shape_Descriptor_for_D_Shape_Retrieval\figure_7.jpg
  Figure 7 caption: The precision-recall curves for the multiscale shape distribution
    descriptor and the proposed shape descriptor on the McGill shape dataset.
  Figure 8 Link: articels_figures_by_rev_year\2016\DeepShape_DeepLearned_Shape_Descriptor_for_D_Shape_Retrieval\figure_8.jpg
  Figure 8 caption: The precision-recall curves for the shape descriptor using standard
    auto-encoder (without the Fisher discrimination criterion) and the proposed shape
    descriptor on the McGill dataset.
  Figure 9 Link: articels_figures_by_rev_year\2016\DeepShape_DeepLearned_Shape_Descriptor_for_D_Shape_Retrieval\figure_9.jpg
  Figure 9 caption: The proposed descriptors for the teddy-bear model and the human
    model.
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.54
  Name of the first author: Jin Xie
  Name of the last author: Yi Fang
  Number of Figures: 10
  Number of Tables: 4
  Number of authors: 5
  Paper title: 'DeepShape: Deep-Learned Shape Descriptor for 3D Shape Retrieval'
  Publication Date: 2016-07-29 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Retrieval Results on the McGill Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Retrieval Results (Mean Average Precision) on the SHREC'10
      ShapeGoogle Dataset
  Table 3 caption:
    table_text: TABLE 3 Retrieval Results (Mean Average Precision) on the SHREC'14
      Human Dataset
  Table 4 caption:
    table_text: TABLE 4 Retrieval Results on the SHREC'14 LSCRTB Dataset
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2596722
- Affiliation of the first author: institute for computer graphics and vision, graz
    university of technology, graz, austria
  Affiliation of the last author: institute for computer graphics and vision, graz
    university of technology, graz, austria
  Figure 1 Link: articels_figures_by_rev_year\2016\Trainable_Nonlinear_Reaction_Diffusion_A_Flexible_Framework_for_Fast_and_Effecti\figure_1.jpg
  Figure 1 caption: "The architecture of our proposed diffusion model with a reaction\
    \ force \u03C8 t = \u03BB t A \u22A4 (Au\u2212f) and G=0 . It is represented as\
    \ a feed-forward network. Note that the additional convolution step with the rotated\
    \ kernels k \xAF i (cf. Equ. (15)) does not appear in conventional feed-forward\
    \ CNs."
  Figure 10 Link: articels_figures_by_rev_year\2016\Trainable_Nonlinear_Reaction_Diffusion_A_Flexible_Framework_for_Fast_and_Effecti\figure_10.jpg
  Figure 10 caption: "Trained filters (in the first and last stage) of the TNRD 5\
    \ 7\xD77 model for the noise level \u03C3=25 . We can find first, second and higher-order\
    \ derivative filters, as well as rotated derivative filters along different directions.\
    \ These filters are effective for image structure detection, such as image edge\
    \ and texture."
  Figure 2 Link: articels_figures_by_rev_year\2016\Trainable_Nonlinear_Reaction_Diffusion_A_Flexible_Framework_for_Fast_and_Effecti\figure_2.jpg
  Figure 2 caption: Our diffusion network can also be interpreted as a CN with a feedback
    step, which makes it different from conventional feed-forward networks. Due to
    the feedback step, it can be categorized into recurrent networks [25].
  Figure 3 Link: articels_figures_by_rev_year\2016\Trainable_Nonlinear_Reaction_Diffusion_A_Flexible_Framework_for_Fast_and_Effecti\figure_3.jpg
  Figure 3 caption: "Function approximation via Gaussian \u03C6 g (z) or triangular-shaped\
    \ \u03C6 t (z) radial basis function, respectively for the function \u03D5(z)=\
    \ 2sz 1+ s 2 z 2 with s= 1 20 . Both approximation methods use 63 basis functions\
    \ equidistantly centered at [\u2212310:10:310] ."
  Figure 4 Link: articels_figures_by_rev_year\2016\Trainable_Nonlinear_Reaction_Diffusion_A_Flexible_Framework_for_Fast_and_Effecti\figure_4.jpg
  Figure 4 caption: "An image denoising example for noise level \u03C3=25 to illustrate\
    \ how our learned TNRD 5 5\xD75 works. (b)-(e) are intermediate results at stages\
    \ 1\u20134, and (f) is the output of stage 5, i.e., the final denoising result."
  Figure 5 Link: articels_figures_by_rev_year\2016\Trainable_Nonlinear_Reaction_Diffusion_A_Flexible_Framework_for_Fast_and_Effecti\figure_5.jpg
  Figure 5 caption: "The figure shows four characteristic influence functions (left\
    \ plot in each subfigure) together with their corresponding penalty functions\
    \ (right plot in each subfigure), learned by our proposed method in the TNRD 5\
    \ 5\xD75 model. A major finding in this paper is that our learned penalty functions\
    \ significantly differ from the usual penalty functions adopted in partial differential\
    \ equations and energy minimization methods. In contrast to their usual robust\
    \ smoothing properties which is caused by a single minimum around zero, most of\
    \ our learned functions have multiple minima different from zero and hence are\
    \ able to enhance certain image structures. See Section 4.3 for more information."
  Figure 6 Link: articels_figures_by_rev_year\2016\Trainable_Nonlinear_Reaction_Diffusion_A_Flexible_Framework_for_Fast_and_Effecti\figure_6.jpg
  Figure 6 caption: "Well distributed gradients \u2202L \u2202\u0398 over stages for\
    \ the TNRD 5 5\xD75 model at the initialization point \u0398 0 with a plain setting.\
    \ One can see that the \u201Cvanishing gradient\u201D phenomenon [6] in the back-propagation\
    \ phase of a conventional deep model does not appear in our training model."
  Figure 7 Link: articels_figures_by_rev_year\2016\Trainable_Nonlinear_Reaction_Diffusion_A_Flexible_Framework_for_Fast_and_Effecti\figure_7.jpg
  Figure 7 caption: "Patterns synthesized from uniform noise using our learned diffusion\
    \ models. (a) is generated by (16) using the parameters (linear filters and influence\
    \ functions) in a stage of our learned TNRD 5 5\xD75 for image denoising, (b)\
    \ is generated by (16) using the parameters in a stage of our learned TNRD 5 7\xD7\
    7 for image super resolution, and (c) is also from a stage of our learned TNRD\
    \ 5 7\xD77 for image super resolution."
  Figure 8 Link: articels_figures_by_rev_year\2016\Trainable_Nonlinear_Reaction_Diffusion_A_Flexible_Framework_for_Fast_and_Effecti\figure_8.jpg
  Figure 8 caption: "Influence of the number of training examples for the training\
    \ model TNRD 5 5\xD75 ."
  Figure 9 Link: articels_figures_by_rev_year\2016\Trainable_Nonlinear_Reaction_Diffusion_A_Flexible_Framework_for_Fast_and_Effecti\figure_9.jpg
  Figure 9 caption: "Influence of the filter size (based on a relatively small training\
    \ data set of 400 images of size 180\xD7180 )."
  First author gender probability: 0.8
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yunjin Chen
  Name of the last author: Thomas Pock
  Number of Figures: 13
  Number of Tables: 5
  Number of authors: 2
  Paper title: 'Trainable Nonlinear Reaction Diffusion: A Flexible Framework for Fast
    and Effective Image Restoration'
  Publication Date: 2016-08-01 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Average PSNR (dB) on 68 Images from [48] for Image Denoising\
      \ with \u03C3=15,25 and 50"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Run Time Comparison for Image Denoising (in Seconds) with
      Different Implementations
  Table 3 caption:
    table_text: "TABLE 3 PSNR (dB) and Run Time (s) Performance for Upscaling Factors\
      \ \xD72 , \xD73 and \xD74 on the Set5 Dataset"
  Table 4 caption:
    table_text: "TABLE 4 Upscaling Factor \xD73 Performance in Terms of PSNR(dB) and\
      \ Runtime (s) per Image on the Set14 Dataset"
  Table 5 caption:
    table_text: TABLE 5 JPEG Deblocking Results for Natural Images, Reported with
      Average PSNR Values
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2596743
- Affiliation of the first author: division of electrical engineering, hanyang university,
    ansan, kyeonggi-do, south korea
  Affiliation of the last author: department of electrical and computer engineering,
    seoul national university, seoul, south korea
  Figure 1 Link: articels_figures_by_rev_year\2016\Procrustean_Normal_Distribution_for_NonRigid_Structure_from_Motion\figure_1.jpg
  Figure 1 caption: "Linearizing the distribution of X \u2032 ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Procrustean_Normal_Distribution_for_NonRigid_Structure_from_Motion\figure_2.jpg
  Figure 2 caption: "The conceptual diagram of the PND. The PND is a Gaussian distribution\
    \ orthogonal to P N ( Y \xAF \xAF \xAF \xAF ) , which is closely related to the\
    \ similarity transform of the mean shape Y \xAF \xAF \xAF \xAF ."
  Figure 3 Link: articels_figures_by_rev_year\2016\Procrustean_Normal_Distribution_for_NonRigid_Structure_from_Motion\figure_3.jpg
  Figure 3 caption: "Average reconstruction errors for different shape distributions.\
    \ \u201C+\u201D, \u201C \u25A1 \u201D, and \u201C \u25EF \u201D indicate the Gaussian,\
    \ uniform, and Laplace distributions, respectively; the solid, dashed, and dotted\
    \ lines represent the number of bases used (20, 50, and 100 bases, respectively)."
  Figure 4 Link: articels_figures_by_rev_year\2016\Procrustean_Normal_Distribution_for_NonRigid_Structure_from_Motion\figure_4.jpg
  Figure 4 caption: "Reconstructed results of benchmark datasets (top row: EM-PND,\
    \ middle row: CSF2, bottom row: SPM, \u2218 : ground truth, + : reconstructed\
    \ points)."
  Figure 5 Link: articels_figures_by_rev_year\2016\Procrustean_Normal_Distribution_for_NonRigid_Structure_from_Motion\figure_5.jpg
  Figure 5 caption: Experiments on randomly generated missing entries.
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.9
  Name of the first author: Minsik Lee
  Name of the last author: Songhwai Oh
  Number of Figures: 5
  Number of Tables: 5
  Number of authors: 3
  Paper title: Procrustean Normal Distribution for Non-Rigid Structure from Motion
  Publication Date: 2016-08-01 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Average Reconstruction Errors wo Noise
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Average Reconstruction Errors wNoise
  Table 3 caption:
    table_text: TABLE 3 Average Camera Motion Errors (Degree)
  Table 4 caption:
    table_text: TABLE 4 Errors for Different Rotations (the Default Solution of Rotations
      and the Solution of EM-PND; the Numbers in Parentheses Are the Number of Shape
      Basis K )
  Table 5 caption:
    table_text: TABLE 5 Average Reconstruction Errors for the Face Sequence with Structured
      Missing Points
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2596720
- Affiliation of the first author: departments of radiology and medical informatics,
    erasmus mc, biomedical imaging group rotterdam, rotterdam, ca, the netherlands
  Affiliation of the last author: biomedical imaging group rotterdam, departments
    of radiology and medical informatics, erasmus mc, rotterdam, ca, the netherlands
  Figure 1 Link: articels_figures_by_rev_year\2016\Randomly_Perturbed_BSplines_for_Nonrigid_Image_Registration\figure_1.jpg
  Figure 1 caption: "One-dimensional B-splines with four different orders: (a) \u03B2\
    \ 0 (x) ; (b) \u03B2 1 (x) ; (c) \u03B2 2 (x) ; (d) \u03B2 3 (x) ."
  Figure 10 Link: articels_figures_by_rev_year\2016\Randomly_Perturbed_BSplines_for_Nonrigid_Image_Registration\figure_10.jpg
  Figure 10 caption: 'Performance comparison of different approaches on brain data.
    An asterisk () above the label indicates the result is significantly different
    (paired Wilcoxon signed rank test, p<0.05 ) from Dirac-3-3: (a) registration accuracy
    using eta 4=5 mm (overall mean overlap, higher values are better); (b) registration
    accuracy using eta 4=3 mm; (c) transformation smoothness using eta 4=5 mm (standard
    deviation of DSJ , lower values are better); (d) transformation smoothness using
    eta 4=3 mm.'
  Figure 2 Link: articels_figures_by_rev_year\2016\Randomly_Perturbed_BSplines_for_Nonrigid_Image_Registration\figure_2.jpg
  Figure 2 caption: 'An example case on artificial motion: (a) fixed image; (b) randomly
    deformed moving image with additional noise; (c) difference image.'
  Figure 3 Link: articels_figures_by_rev_year\2016\Randomly_Perturbed_BSplines_for_Nonrigid_Image_Registration\figure_3.jpg
  Figure 3 caption: "Results on artificial motion. An asterisk () above the label\
    \ indicates the result is significantly different (paired Wilcoxon signed rank\
    \ test, p<0.05 ) from Dirac-3-3: (a) boxplot of Residual( T \u03BC ( T init ),Id)\
    \ on 120 test cases by each approach; (b) averaged rank in Residual( T \u03BC\
    \ ( T init ),Id) . Lower ranks indicate better performance."
  Figure 4 Link: articels_figures_by_rev_year\2016\Randomly_Perturbed_BSplines_for_Nonrigid_Image_Registration\figure_4.jpg
  Figure 4 caption: "Comparison of the results obtained using the proposed stochastic\
    \ approach and using a brute-force deterministic optimization. Boxplots show the\
    \ distribution of Residual( T \u03BC ( T init ),Id) on the 120 test cases with\
    \ artificial motion."
  Figure 5 Link: articels_figures_by_rev_year\2016\Randomly_Perturbed_BSplines_for_Nonrigid_Image_Registration\figure_5.jpg
  Figure 5 caption: Convergence curves of four registration methods on one representative
    test case with artificial motion, including Dirac-3-3, Dirac-1-3, Uniform-1-3
    and Gaussian-1-3 at the first resolution level.
  Figure 6 Link: articels_figures_by_rev_year\2016\Randomly_Perturbed_BSplines_for_Nonrigid_Image_Registration\figure_6.jpg
  Figure 6 caption: Variability of the registration accuracy achieved by stochastic
    approaches. Boxplots show the standard deviations of Residual(boldsymbolTboldsymbolhatmu(boldsymbolTinit),boldsymbolId)
    over 10 random seeds on the 120 test cases with artifical motions.
  Figure 7 Link: articels_figures_by_rev_year\2016\Randomly_Perturbed_BSplines_for_Nonrigid_Image_Registration\figure_7.jpg
  Figure 7 caption: Two-dimensional averaged ranks of different approaches using eta
    4=10 mm on lung data. Lower ranks indicate better performance.
  Figure 8 Link: articels_figures_by_rev_year\2016\Randomly_Perturbed_BSplines_for_Nonrigid_Image_Registration\figure_8.jpg
  Figure 8 caption: Convergence curves of the FFD and RPFFD approaches on one representative
    test case of lung data, including Dirac-3-3, Dirac-1-3, Uniform-1-3 and Gaussian-1-3
    at the first resolution level.
  Figure 9 Link: articels_figures_by_rev_year\2016\Randomly_Perturbed_BSplines_for_Nonrigid_Image_Registration\figure_9.jpg
  Figure 9 caption: Computation time (mean and standard deviation) of B-spline orders
    m=3, 2, 1 at each resolution level on lung data.
  First author gender probability: 0.73
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Wei Sun
  Name of the last author: Stefan Klein
  Number of Figures: 13
  Number of Tables: 3
  Number of authors: 3
  Paper title: Randomly Perturbed B-Splines for Nonrigid Image Registration
  Publication Date: 2016-08-04 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Description of DIR-Lab Lung Data
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 TREs (in mm) of Different Approaches Using Final B-Spline
      Order n=3 , for 10 Lung CT Pairs
  Table 3 caption:
    table_text: TABLE 3 Smoothness (Standard Deviation of D SJ ) of Different Approaches
      Using Final B-Spline Order n=3 , for 10 Lung CT Pairs
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2598344
- Affiliation of the first author: computer science department, stanford university,
    stanford, ca
  Affiliation of the last author: computer science department, stanford university,
    stanford, ca
  Figure 1 Link: articels_figures_by_rev_year\2016\Deep_VisualSemantic_Alignments_for_Generating_Image_Descriptions\figure_1.jpg
  Figure 1 caption: 'MotivationConcept figure: Our model treats language as a rich
    label space and generates descriptions of image regions.'
  Figure 10 Link: articels_figures_by_rev_year\2016\Deep_VisualSemantic_Alignments_for_Generating_Image_Descriptions\figure_10.jpg
  Figure 10 caption: RNN generated captions on test images, using regions provided
    by AMT worker annotations in the VG data. Each region is described by our RNN
    model in isolation, and we sort resulting captions by the conditional log probability
    of the caption given the region, which can be interpreted as a measure of confidence.
    This visualization tries to show more generated captions without cluttering the
    images with correspondence lines; the colors indicate which caption matches which
    region, but the specific colors are arbitrary.
  Figure 2 Link: articels_figures_by_rev_year\2016\Deep_VisualSemantic_Alignments_for_Generating_Image_Descriptions\figure_2.jpg
  Figure 2 caption: Overview of our approach. A dataset of images and their sentence
    descriptions is the input to our model (left). Our model first infers the correspondences
    (middle, Section 3.1) and then learns to generate novel descriptions (right, Section
    3.2).
  Figure 3 Link: articels_figures_by_rev_year\2016\Deep_VisualSemantic_Alignments_for_Generating_Image_Descriptions\figure_3.jpg
  Figure 3 caption: Diagram for evaluating the image-sentence score S kl . Object
    regions are embedded with a CNN (left). Words (enriched by their context) are
    embedded in the same multimodal space with a BRNN (right). Pairwise similarities
    are computed with inner products (magnitudes shown in grayscale) and finally reduced
    to image-sentence score with Equation (8).
  Figure 4 Link: articels_figures_by_rev_year\2016\Deep_VisualSemantic_Alignments_for_Generating_Image_Descriptions\figure_4.jpg
  Figure 4 caption: Diagram of our multimodal Recurrent Neural Network generative
    model. The RNN takes a word, the context from previous time steps and defines
    a distribution over the next word in the sentence. The RNN is conditioned on the
    image information at the first time step. START and END are special tokens.
  Figure 5 Link: articels_figures_by_rev_year\2016\Deep_VisualSemantic_Alignments_for_Generating_Image_Descriptions\figure_5.jpg
  Figure 5 caption: Example alignments predicted by our model. For every test image
    above, we retrieve the most compatible test sentence and visualize the highest-scoring
    region for each word (before MRF smoothing described in Section 3.1.4) and the
    associated scores ( v T i s t ). We hide the alignments of low-scoring words to
    reduce clutter. We assign each region an arbitrary color.
  Figure 6 Link: articels_figures_by_rev_year\2016\Deep_VisualSemantic_Alignments_for_Generating_Image_Descriptions\figure_6.jpg
  Figure 6 caption: Flickr30K test set regions with high vector magnitude, indicating
    a strong influence on the image-sentence score.
  Figure 7 Link: articels_figures_by_rev_year\2016\Deep_VisualSemantic_Alignments_for_Generating_Image_Descriptions\figure_7.jpg
  Figure 7 caption: Examples of highest scoring regions for queried snippets of text,
    on 5,000 images of our MSCOCO test set.
  Figure 8 Link: articels_figures_by_rev_year\2016\Deep_VisualSemantic_Alignments_for_Generating_Image_Descriptions\figure_8.jpg
  Figure 8 caption: Example sentences generated by the multimodal RNN for test images.
  Figure 9 Link: articels_figures_by_rev_year\2016\Deep_VisualSemantic_Alignments_for_Generating_Image_Descriptions\figure_9.jpg
  Figure 9 caption: Example region predictions on MSCOCO. We use our region-level
    multimodal RNN to generate text (shown on the right of each image) for some of
    the bounding boxes in each image. The lines are grounded to centers of bounding
    boxes and the colors are chosen arbitrarily.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.66
  Name of the first author: Andrej Karpathy
  Name of the last author: Li Fei-Fei
  Number of Figures: 10
  Number of Tables: 5
  Number of authors: 2
  Paper title: Deep Visual-Semantic Alignments for Generating Image Descriptions
  Publication Date: 2016-08-05 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Image-Sentence Ranking Experiment Results
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 This Table Shows the Top Magnitudes of Vectors ( \u2225 s\
      \ t \u2225 ) for Words in Flickr30K"
  Table 3 caption:
    table_text: TABLE 3 Evaluation of Full Image Predictions on 1,000 Test Images
  Table 4 caption:
    table_text: TABLE 4 BLEU Score Evaluation of Image Region Annotations
  Table 5 caption:
    table_text: TABLE 5 Captioning Evaluation on VG Test Image Regions for a Fullframe
      Model Trained on MSCOCO and a Region-Level Model Trained on VG Regions
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2598339
- Affiliation of the first author: department of biomedical engineering, johns hopkins
    university, baltimore, md
  Affiliation of the last author: department of biomedical engineering, johns hopkins
    university, baltimore, md
  Figure 1 Link: articels_figures_by_rev_year\2016\A_Novel_Nonparametric_Maximum_Likelihood_Estimator_for_Probability_Density_Funct\figure_1.jpg
  Figure 1 caption: "Comparison of BLMLTrivial and BLMLBQP using surrogate data\u2014\
    Illustration of the results of BLMLTrivial and BLMLBQP algorithms using a non-strictly\
    \ positive true pdf f(x)=0.4 sinc 2 (0.4x) , (A,C) and a strictly positive pdf\
    \ f(x)=0.078( sinc 2 (0.2x)+ sinc 2 (0.2x+0.2) ) 2 ( B,D). The cut-off frequency\
    \ was assumed to be f c = f true c . The p -values were calculated using a paired\
    \ t -test at n=81 . Note in (B), the red line is beneath the blue line."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\A_Novel_Nonparametric_Maximum_Likelihood_Estimator_for_Probability_Density_Funct\figure_2.jpg
  Figure 2 caption: "Comparison of BLML and KD estimation using surrogate data\u2014\
    Comparison of the results of the BLMLTrivial and BLMLQuick estimators to the KDE2nd,\
    \ KDE6th and sinc KD estimators. MISE as a function of n for (A) a strictly positive\
    \ band-limited true pdf (the one used in Fig. 1B) and ( B) an infinite band standard\
    \ normal pdf. For the BLML estimators the cut-off frequencies are chosen as f\
    \ c =2 f true c for the true band-limited pdf and f c =2 for the normal true pdf.\
    \ For the KDE2nd and KDE6th, the optimal bandwidths are chosen as q= 0.4 f c n\
    \ \u22120.2 and 0.8 f c n \u2212113 , respectively and also to match the MISE\
    \ for the BLML estimator for n=1 . For the KDEsinc, the f c is kept the same as\
    \ the f c for BLML estimators. (C ) MISE as a function of the cut-off frequency\
    \ f c f true c for a true band-limited pdf with cut-off frequency f true c . n=\
    \ 10 4 is used for creating this plot. (D) Computation time as a function of n\
    \ . The p -values are calculated between the BLMLTrivial estimator and other estimators\
    \ using paired t-test for either log 10 (n)=5 (A,B,D) or log 10 ( f c f true c\
    \ )=1.6 ( C) and are color coded. Note that the dark blue line in (A,B,C) is beneath\
    \ the light blue line."
  Figure 3 Link: articels_figures_by_rev_year\2016\A_Novel_Nonparametric_Maximum_Likelihood_Estimator_for_Probability_Density_Funct\figure_3.jpg
  Figure 3 caption: "Comparison of BLML, KDE and PML using standard normal data\u2014\
    (A) MISE and (B) M \u0394 LogL as a function of number of sample points. All p\
    \ -values are calculated using paired t -test and are color coded. Note that the\
    \ missing value for PML Gauss method for logn=0 since the variance can not be\
    \ estimated from one sample, and that the dark blue line is beneath the lighter\
    \ blue line in panel (B)."
  Figure 4 Link: articels_figures_by_rev_year\2016\A_Novel_Nonparametric_Maximum_Likelihood_Estimator_for_Probability_Density_Funct\figure_4.jpg
  Figure 4 caption: "Estimation of f true c \u2014The MNLL and dMNLL d f c curves\
    \ as a function of f c . The cons is an arbitrary constant that is added to MNLL\
    \ so that the logarithm of sum could exist."
  Figure 5 Link: articels_figures_by_rev_year\2016\A_Novel_Nonparametric_Maximum_Likelihood_Estimator_for_Probability_Density_Funct\figure_5.jpg
  Figure 5 caption: "Comparison of M \u0394 LogL obtained using BLMLQuick and KDE2nd\
    \ using surrogate data where cut-off frequency and bandwidths are obtained by\
    \ cross validation\u2014(A) an infinite band standard normal pdf and ( B) a strictly\
    \ positive band-limited true pdf (the one used in Fig. 1 B). All p -values are\
    \ calculated using paired t -test and are color coded. Note that values for KDE2nd\
    \ are missing in (B) for all n because its M \u0394 LogL came out be very large\
    \ (beyond machine limit) see text for detailed explanation."
  Figure 6 Link: articels_figures_by_rev_year\2016\A_Novel_Nonparametric_Maximum_Likelihood_Estimator_for_Probability_Density_Funct\figure_6.jpg
  Figure 6 caption: "Estimation of tail probabilities\u2014(A,B,C) Top row plots the\
    \ estimated and true logarithm of the Normal, Gumbel and Student-t pdf, respectively.\
    \ The bottom row plots p estimators using the BLML, KDE2nd and Bernoulli methods\
    \ for data generated from the three pdfs, respectively. '' denotes p<0.05 between\
    \ the BLML and the indicated method. p -values are are calculated using paired\
    \ t -test."
  Figure 7 Link: articels_figures_by_rev_year\2016\A_Novel_Nonparametric_Maximum_Likelihood_Estimator_for_Probability_Density_Funct\figure_7.jpg
  Figure 7 caption: "Spiking activity of grid and place cells\u2014 (A) A schematic\
    \ of the circular arena where the rat was freely foraging. (B,C) Spiking activity\
    \ of a \u201Csimple\u201D (unimodal) place cell and a \u201Ccomplex\u201D (multimodal)\
    \ place cell respectively. The black dots mark the (x,y) coordinates of a rat's\
    \ position when the cells spiked."
  Figure 8 Link: articels_figures_by_rev_year\2016\A_Novel_Nonparametric_Maximum_Likelihood_Estimator_for_Probability_Density_Funct\figure_8.jpg
  Figure 8 caption: "Performance comparison of PML, Histograms, KDE2nd and BLMLQuick\
    \ methods on neuroscience data\u2014(A,B) Comparison for simple and complex place\
    \ cell, respectively. The top row plots the estimates for f(x,y|textspike) . The\
    \ bottom row plots normalized log-likelihood computed on the test data. indicates\
    \ p<0.01 between the BLMLQuick and the indicated method."
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 1.0
  Name of the first author: Rahul Agarwal
  Name of the last author: Sridevi V. Sarma
  Number of Figures: 8
  Number of Tables: 0
  Number of authors: 3
  Paper title: A Novel Nonparametric Maximum Likelihood Estimator for Probability
    Density Functions
  Publication Date: 2016-08-05 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2598333
- Affiliation of the first author: centre for mathematical sciences, lund university,
    lund, sweden
  Affiliation of the last author: centre for mathematical sciences, lund university,
    lund, sweden
  Figure 1 Link: articels_figures_by_rev_year\2016\CityScale_Localization_for_Cameras_with_Known_Vertical_Direction\figure_1.jpg
  Figure 1 caption: "The registration problem for points lying on cones: Find a 3D\
    \ translation t and a planar rotation R so that the 3D points U \u2032 i lie on\
    \ or within the cones C i ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\CityScale_Localization_for_Cameras_with_Known_Vertical_Direction\figure_2.jpg
  Figure 2 caption: "The reprojection errors for two example points are propagated\
    \ as cones in 3D. The points in 3D are located at different heights. The error\
    \ cones intersected with planes parallel to z=0 give rise to error conics. Cutting\
    \ a cone with a number of planes between two heights z=h\u2212\u0394 and z=h+\u0394\
    \ and projecting them onto the ground plane results in a shape (black) that can\
    \ be approximated with a quadrilateral Q (green)."
  Figure 3 Link: articels_figures_by_rev_year\2016\CityScale_Localization_for_Cameras_with_Known_Vertical_Direction\figure_3.jpg
  Figure 3 caption: Propagating the errors from point u K to u i can be done using
    the Minkowski difference of the error conics. The upper and lower bounds on the
    rotation angle for a point u i are also shown. Note that if the error shapes are
    ellipses centred at the origin, the Minkowski difference equals the Minkowski
    sum.
  Figure 4 Link: articels_figures_by_rev_year\2016\CityScale_Localization_for_Cameras_with_Known_Vertical_Direction\figure_4.jpg
  Figure 4 caption: Error propagation from point uK to ui (for points in a height
    interval), using the Minkowski difference of error quadrilaterals. The upper and
    lower bounds on the rotation angle are also shown.
  Figure 5 Link: articels_figures_by_rev_year\2016\CityScale_Localization_for_Cameras_with_Known_Vertical_Direction\figure_5.jpg
  Figure 5 caption: 'Top: One of the SLR images to build the model (left) and one
    of the iPhone query images (right). Note the illumination difference. Bottom:
    Rate of inliers for the 101 query images in the shopping street experiment.'
  Figure 6 Link: articels_figures_by_rev_year\2016\CityScale_Localization_for_Cameras_with_Known_Vertical_Direction\figure_6.jpg
  Figure 6 caption: 'Top: Histogram over the localization error with respect to ground
    truth of the proposed method (green) compared to exhaustive 2-point RANSAC (blue)
    for the semi-synthetic experiment. Bottom: Execution times for 2-point RANSAC
    schemes (dashed lines) and the proposed outlier removal step (green line). The
    x-axis shows the number of outliers. The number of inliers was fixed to 10.'
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: "Linus Sv\xE4rm"
  Name of the last author: Magnus Oskarsson
  Number of Figures: 6
  Number of Tables: 2
  Number of authors: 4
  Paper title: City-Scale Localization for Cameras with Known Vertical Direction
  Publication Date: 2016-08-05 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Results on the Dubrovnik Dataset; see [11]
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Results on the San Francisco dataset SF-0 at Precision 95
      Percent; see [34]
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2598331
- Affiliation of the first author: engineering research center for advanced computing
    engineering software of ministry of education, china
  Affiliation of the last author: 360 ai institute and national university of singapore,
    singapore
  Figure 1 Link: articels_figures_by_rev_year\2016\Learning_to_Segment_Human_by_Watching_YouTube\figure_1.jpg
  Figure 1 caption: An overview of our framework. Given raw YouTube videos, the proposed
    framework learns a good segmentation network by iteratively inferring video-context
    derived human masks and updating the network. For each video, we first extract
    mid-level supervoxels (unique colors represent unique supervoxels, and same colors
    across frames denote the same supervoxels) in the key frames. For each key frame,
    we use a pre-trained imperfect human detector to initialize the location of each
    instance of interest. The spatio-temporal graph optimization is then performed
    to extract video-context derived human masks, which are then utilized to update
    the segmentation network. As a feedback, the updated network will provide refined
    confidence maps to help generate better human masks.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Learning_to_Segment_Human_by_Watching_YouTube\figure_2.jpg
  Figure 2 caption: Illustration of the spatio-temporal graph. Nodes are spatial superpixels
    in every key frame. For legibility, only a small subset of nodes and connections
    are depicted. Best viewed in color.
  Figure 3 Link: articels_figures_by_rev_year\2016\Learning_to_Segment_Human_by_Watching_YouTube\figure_3.jpg
  Figure 3 caption: Exemplars of the generated video-context derived human masks.
    We display four videos with various background, views and appearances.
  Figure 4 Link: articels_figures_by_rev_year\2016\Learning_to_Segment_Human_by_Watching_YouTube\figure_4.jpg
  Figure 4 caption: Example human segmentation results on PASCAL VOC 2012 validation
    using our method. For each image, we show the results of our version and its variant
    using image-based segmentation.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xiaodan Liang
  Name of the last author: Shuicheng Yan
  Number of Figures: 4
  Number of Tables: 3
  Number of authors: 7
  Paper title: Learning to Segment Human by Watching YouTube
  Publication Date: 2016-08-05 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Comparison of Our Models with Video-Context Guided Inference\
      \ (\u201COurs\u201D), Image-Based Segmentation (\u201COurs (Image-Based)\u201D\
      ), the Version without Using Pairwise Potentials (\u201COurs (wo Pair)\u201D\
      ) and the Version without Using Higher-Order Potentials (\u201COurs (wo Higher)\u201D\
      ) in Terms of IoU (%) on PASCAL VOC 2012 Validation Set"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Different Network Training Strategies in Terms
      of IoU (%) on PASCAL VOC 2012 Validation Set
  Table 3 caption:
    table_text: TABLE 3 Results on the PASCAL VOC 2012 Test Set
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2598340
- Affiliation of the first author: perceptiveio inc., san francisco, ca
  Affiliation of the last author: imperial college london, london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2016\Latent_Regression_Forest_Structured_Estimation_of_D_Hand_Poses\figure_1.jpg
  Figure 1 caption: Our method can be viewed as a coarse-to-fine search process for
    locating hand parts, guided by a binary latent tree model (LTM); starting from
    root of the LTM, we minimise the offset to its children at each level until reaching
    a leaf node which corresponds to a skeletal joint position. For simplicity, we
    only show the searching process for one joint. All figures are best viewed in
    colour and high-definition.
  Figure 10 Link: articels_figures_by_rev_year\2016\Latent_Regression_Forest_Structured_Estimation_of_D_Hand_Poses\figure_10.jpg
  Figure 10 caption: Effect of different LTMs.
  Figure 2 Link: articels_figures_by_rev_year\2016\Latent_Regression_Forest_Structured_Estimation_of_D_Hand_Poses\figure_2.jpg
  Figure 2 caption: Colour coding of the 16 skeletal parts in this paper.
  Figure 3 Link: articels_figures_by_rev_year\2016\Latent_Regression_Forest_Structured_Estimation_of_D_Hand_Poses\figure_3.jpg
  Figure 3 caption: Comparing holistic and patch-based regression trees. (a) A holistic
    regression tree takes the whole hand as input and gives one pose result; (b) a
    patch-based regression tree takes each patch as input and aggregates results.
  Figure 4 Link: articels_figures_by_rev_year\2016\Latent_Regression_Forest_Structured_Estimation_of_D_Hand_Poses\figure_4.jpg
  Figure 4 caption: 'The distance matrix (left) and latent tree model (right) generated
    with the information distances defined in [16].The cross symbol indicates the
    centroid. (P: proximal phalanx, I: intermediate phalanx, D: distal phalanx, the
    same in other figures.)'
  Figure 5 Link: articels_figures_by_rev_year\2016\Latent_Regression_Forest_Structured_Estimation_of_D_Hand_Poses\figure_5.jpg
  Figure 5 caption: 'Comparison between using Euclidean and geodesic distance: (a)
    Euclidean distance between two joints, (b) geodesic distance between two joints,
    (c) distance matrix generated with Euclidean distance, (d) distance matrix generated
    with geodesic distance.'
  Figure 6 Link: articels_figures_by_rev_year\2016\Latent_Regression_Forest_Structured_Estimation_of_D_Hand_Poses\figure_6.jpg
  Figure 6 caption: 'Learnt latent tree models. Top: using Euclidean distance. Bottom:
    using geodesic distance. Solid circles represent observable vertices and dashed
    ones latent vertices. Yellow dash lines encircle the three parts of middle finger.
    They are separated In the case of euclidean distance and grouped together in the
    case of geodesic distance.'
  Figure 7 Link: articels_figures_by_rev_year\2016\Latent_Regression_Forest_Structured_Estimation_of_D_Hand_Poses\figure_7.jpg
  Figure 7 caption: A comparison between (a) a traditional Regression Tree, where
    each patch sample is extracted from a depth image, propagated down the tree and
    ends up at one leaf node; and (b) a latent regression tree, where the whole point
    cloud is propagated down the tree and keep dividing until ending up at 16 leaf
    nodes.
  Figure 8 Link: articels_figures_by_rev_year\2016\Latent_Regression_Forest_Structured_Estimation_of_D_Hand_Poses\figure_8.jpg
  Figure 8 caption: Examples from our ICVL dataset. These images are in-plane rotated
    to cover the roll degree. Depth images are segmented, cropped and normalised for
    visualisation.
  Figure 9 Link: articels_figures_by_rev_year\2016\Latent_Regression_Forest_Structured_Estimation_of_D_Hand_Poses\figure_9.jpg
  Figure 9 caption: Effect of different LTMs.
  First author gender probability: 0.93
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.7
  Name of the first author: Danhang Tang
  Name of the last author: Tae-Kyun Kim
  Number of Figures: 19
  Number of Tables: 1
  Number of authors: 4
  Paper title: 'Latent Regression Forest: Structured Estimation of 3D Hand Poses'
  Publication Date: 2016-08-10 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Efficiency Comparison
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2599170
- Affiliation of the first author: school of engineering and computer science, victoria
    university wellington, wellington, new zealand
  Affiliation of the last author: school of engineering and computer science, victoria
    university wellington, wellington, new zealand
  Figure 1 Link: articels_figures_by_rev_year\2016\Scatter_Component_Analysis_A_Unified_Framework_for_Domain_Adaptation_and_Domain_\figure_1.jpg
  Figure 1 caption: Visualization. Projections of synthetic data onto the first two
    leading eigenvectors. Numbers in brackets indicate the classification accuracy
    on the target using 1-nearest neighbor (1NN). The top and bottom rows show the
    domains and classes respectively.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2016\Scatter_Component_Analysis_A_Unified_Framework_for_Domain_Adaptation_and_Domain_\figure_2.jpg
  Figure 2 caption: L-SVM and DAM average performance accuracy (%) relative to the
    performance on Raw features. The numbers on the top or bottom of the bars show
    the absolute accuracy. The red line indicates the Raw baseline performance, see
    Table 4 for the exact numbers.
  Figure 3 Link: Not Available
  Figure 3 caption: Not Available
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.63
  Name of the first author: Muhammad Ghifary
  Name of the last author: Mengjie Zhang
  Number of Figures: 2
  Number of Tables: 10
  Number of authors: 4
  Paper title: 'Scatter Component Analysis: A Unified Framework for Domain Adaptation
    and Domain Generalization'
  Publication Date: 2016-08-11 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Accuracy % on the USPS+MNIST and MSRC+VOC2007 Datasets
  Table 10 caption:
    table_text: TABLE 10 Average Domain Generalization Runtime (Seconds) over All
      Cross-Domain Recognition Tasks in Each Dataset
  Table 2 caption:
    table_text: TABLE 2 Accuracy % on the Office+Caltech Images with SURF-BoW Features
  Table 3 caption:
    table_text: TABLE 3 Accuracy % on the Office+Caltech Images with DeCAF 6 Features
  Table 4 caption:
    table_text: TABLE 4 Average Accuracy (%) on Raw Features
  Table 5 caption:
    table_text: TABLE 5 Average Runtime (Seconds) over All Cross-Domain Tasks in Each
      Dataset
  Table 6 caption:
    table_text: TABLE 6 The Groundtruth 1NN Accuracy % of Five-Class Classification
      When Training on One Dataset (the Left-Most Column) and Testing on Another (the
      Upper-Most Row)
  Table 7 caption:
    table_text: TABLE 7 Domain Generalization Performance Accuracy ( % ) on the VLCS
      Dataset with DeCAF 6 Features as Inputs
  Table 8 caption:
    table_text: TABLE 8 Domain Generalization Performance Accuracy ( % ) on the Office+Caltech
      Dataset with DeCAF 6 Features as Inputs
  Table 9 caption:
    table_text: TABLE 9 Domain Generalization Performance Accuracy ( % ) on the IXMAS
      Dataset with Dense Trajectory-Based Features
  paper DOI: https://doi.org/10.1109/TPAMI.2016.2599532
