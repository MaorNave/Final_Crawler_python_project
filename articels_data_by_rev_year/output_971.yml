- Affiliation of the first author: information technologies institutecentre for research
    and technology hellas (certh), thermi, greece
  Affiliation of the last author: school of electronic engineering and computer science,
    queen mary university of london, london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2017\Linear_Maximum_Margin_Classifier_for_Learning_from_Uncertain_Data\figure_1.jpg
  Figure 1 caption: Linear SVM with Gaussian Sample Uncertainty (LSVM-GSU). The solid
    line depicts the decision boundary of the proposed algorithm, and the dashed line
    depicts the decision boundary of the standard linear SVM (LSVM).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Linear_Maximum_Margin_Classifier_for_Learning_from_Uncertain_Data\figure_2.jpg
  Figure 2 caption: Illustrative example of calculating (a) the standard linear SVM's
    hinge loss, and (b) the proposed linear SVM-GSU's loss. In (c), the hinge loss
    is compared with the proposed linear SVM-GSU's loss for various quantities of
    uncertainty.
  Figure 3 Link: articels_figures_by_rev_year\2017\Linear_Maximum_Margin_Classifier_for_Learning_from_Uncertain_Data\figure_3.jpg
  Figure 3 caption: Toy example illustrating on 2D data, (a) the proposed LSVM-GSU
    (red solid line) in comparison with the standard LSVM (blue dashed line), and
    (b)-(d) with the standard SVM that learns by sampling from the input Gaussians
    (LSVM-sampling), where N is the sampling size.
  Figure 4 Link: articels_figures_by_rev_year\2017\Linear_Maximum_Margin_Classifier_for_Learning_from_Uncertain_Data\figure_4.jpg
  Figure 4 caption: Difference between the separating hyperplanes of LSVM-GSU and
    the standard LSVM with sampling (angle theta ), when varying the number of samples
    used in the standard SVM, for the 2D and 3D toy datasets.
  Figure 5 Link: articels_figures_by_rev_year\2017\Linear_Maximum_Margin_Classifier_for_Learning_from_Uncertain_Data\figure_5.jpg
  Figure 5 caption: Comparisons between the proposed LSVM-GSU, the baseline LSVM,
    and the LSVM with isotropic noise in (a) the original MNIST dataset ( D0 ), and
    (b)-(f) the noisy generated datasets D1 - D5 .
  Figure 6 Link: articels_figures_by_rev_year\2017\Linear_Maximum_Margin_Classifier_for_Learning_from_Uncertain_Data\figure_6.jpg
  Figure 6 caption: "MNIST \u201C1\u201D versus \u201C7\u201D experimental results\
    \ using 25, 50, 100, 500, 1000, 3000, 6000 positive examples per digit. The proposed\
    \ LSVM-GSU using learning linear subspaces ( mathrmLSVMtext-GSUtext-Smathrmp )\
    \ is compared to the baseline linear SVM (LSVM), Power SVM (PSVM) [14], and a\
    \ linear SVM extension which handles the uncertainty isotropically (LSVM-iso),\
    \ as in [18], [27]. The fraction of variance preserved for the proposed method\
    \ is (a) p=0.85 (dataset D3 ), (b) p=0.95 (dataset D4 ). Very similar results\
    \ are observed for all other datasets."
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Christos Tzelepis
  Name of the last author: Ioannis Patras
  Number of Figures: 6
  Number of Tables: 6
  Number of authors: 3
  Paper title: Linear Maximum Margin Classifier for Learning from Uncertain Data
  Publication Date: 2017-11-10 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 MNIST \u201C1\u201D versus \u201C7\u201D Experimental Results\
      \ in Terms of Testing Accuracy"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison Between the Proposed LSVM-GSU, the Baseline LSVM,
      Power SVM, and LSVM-iso
  Table 3 caption:
    table_text: TABLE 3 Comparisons Between the Proposed LSVM-GSU, the Baseline NB,
      LSVM, Power SVM, and the LSVM with Isotropic Noise
  Table 4 caption:
    table_text: TABLE 4 Comparisons Between the Proposed LSVM-GSU and the Baseline
      LSVM, Similarly to [47]
  Table 5 caption:
    table_text: TABLE 5 Event Detection Performance (AP and MAP) of the Linear SVM-GSU
      Compared to the Baseline Linear SVM, Power SVM [14], and a LSVM Extension for
      Handling Isotropic Uncertainty (as in [18], [27]) Using the MED15 (for Training)
      and MED14Test (for Testing) Datasets
  Table 6 caption:
    table_text: TABLE 6 Mean Vector and Covariance Matrix of the i th Example for
      Feature Configurations 1 and 2 of the Video Event Detection Experiments
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2772235
- Affiliation of the first author: department of electrical and computer engineering,
    ohio state university, columbus, oh
  Affiliation of the last author: department of electrical and computer engineering,
    ohio state university, columbus, oh
  Figure 1 Link: articels_figures_by_rev_year\2017\A_Simple_Fast_and_HighlyAccurate_Algorithm_to_Recover_D_Shape_from_D_Landmarks_o\figure_1.jpg
  Figure 1 caption: Framework of the proposed algorithm to estimate the 3D shape of
    a set of 2D landmark points on a single image. The resulting 3D shape is estimated
    in real-time ( >1,000 framess on an i7 desktop).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\A_Simple_Fast_and_HighlyAccurate_Algorithm_to_Recover_D_Shape_from_D_Landmarks_o\figure_2.jpg
  Figure 2 caption: "Deep network for dealing with missing data. d (0) is the input\
    \ to the network, \u201CA\u201D is the recurrent layer with \u03C4 steps for estimating\
    \ missing inputs. \u201CB\u201D is the multi-layer neural network defined in Section\
    \ 3.2.1 and summarized in Fig. 1 . \u201CC\u201D combines the results of \u201C\
    A\u201D and \u201CB\u201D to yield the final output of the network."
  Figure 3 Link: articels_figures_by_rev_year\2017\A_Simple_Fast_and_HighlyAccurate_Algorithm_to_Recover_D_Shape_from_D_Landmarks_o\figure_3.jpg
  Figure 3 caption: Results of our algorithm on images of the humans in-the-wild dataset
    [34]. The first and fifth columns are the single 2D images and 2D landmarks (in
    yellow) used by our algorithm. The reconstructed 3D shapes are shown on the second,
    third, fourth, sixth, seventh and eight columns; these show the 3D reconstruction
    from multiple views to more clearly demonstrate the quality of the recovered 3D
    shape.
  Figure 4 Link: articels_figures_by_rev_year\2017\A_Simple_Fast_and_HighlyAccurate_Algorithm_to_Recover_D_Shape_from_D_Landmarks_o\figure_4.jpg
  Figure 4 caption: "Illustration of our 3D estimation results on the Helen database\
    \ [37]. The model is trained on the BU \u2212 3DFE face database and tested on\
    \ Helen. The first and fifth columns show the input 2D images with landmarks (in\
    \ yellow); the other columns show the estimated 3D shapes."
  Figure 5 Link: articels_figures_by_rev_year\2017\A_Simple_Fast_and_HighlyAccurate_Algorithm_to_Recover_D_Shape_from_D_Landmarks_o\figure_5.jpg
  Figure 5 caption: Illustration of our 3D estimation results on the FG3DCar database.
    The first and fifth columns corresponds to the input 2D images and landmark points
    (in yellow), with the other columns showing the recovered 3D shapes.
  Figure 6 Link: articels_figures_by_rev_year\2017\A_Simple_Fast_and_HighlyAccurate_Algorithm_to_Recover_D_Shape_from_D_Landmarks_o\figure_6.jpg
  Figure 6 caption: 3D shape reconstruction results on the flag flapping in the wind
    sequence. Comparisons of the reconstructed 3D shape (in red) with its 3D ground-truth
    (in green).
  Figure 7 Link: articels_figures_by_rev_year\2017\A_Simple_Fast_and_HighlyAccurate_Algorithm_to_Recover_D_Shape_from_D_Landmarks_o\figure_7.jpg
  Figure 7 caption: "Additive random Gaussian noise is used to test the robustness\
    \ of the proposed algorithm to 2D inaccurate detections on CMU motion capture\
    \ database (a), BU \u2212 3DFE database (b), FG3DCar database (c) and flag flapping\
    \ in the wind database (d). Reconstruction error is shown on the y axis and \u03C3\
    \ on the x axis. Note the tinny recosntruction errors even for large values of\
    \ \u03C3 . Sensitivity of reconstruction to each landmark when \u03C3 increases\
    \ on CMU motion capture database (e), BU \u2212 3DFE database (f), FG3DCar database\
    \ (g) and flag flapping in the wind database (h). The radius of the circle indicates\
    \ the relative reconstruction error for each landmarks."
  Figure 8 Link: articels_figures_by_rev_year\2017\A_Simple_Fast_and_HighlyAccurate_Algorithm_to_Recover_D_Shape_from_D_Landmarks_o\figure_8.jpg
  Figure 8 caption: 'Qualitative results. Top row: Missing landmarks are identified
    with a dotted line in the images. Bottom row: Missing landmarks are marked in
    red in the images.'
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Ruiqi Zhao
  Name of the last author: Aleix M. Martinez
  Number of Figures: 8
  Number of Tables: 5
  Number of authors: 3
  Paper title: A Simple, Fast and Highly-Accurate Algorithm to Recover 3D Shape from
    2D Landmarks on a Single Image
  Publication Date: 2017-11-13 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparative 3D Mean Reconstruction Errors of Our Method and
      Zhou et al. [35] on the Human 3.6M Database
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparisons of the Reconstruction Error on CMU Motion Capture
      Database
  Table 3 caption:
    table_text: "TABLE 3 Comparative Results of Our Appriach and Zhou et al. [2] on\
      \ BU \u2212 3DFE, 3DFAW and FG3DCar"
  Table 4 caption:
    table_text: TABLE 4 Results with Different Number of Training Samples, with and
      without Data Augmentation on BU-3DFE
  Table 5 caption:
    table_text: TABLE 5 Comparisons of the Reconstruction Error on Five Databases
      with Different Proportions of Missing Data Entries
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2772922
- Affiliation of the first author: department of computer science, university of illinois,
    urbana champaign, il
  Affiliation of the last author: department of computer science, university of illinois,
    urbana champaign, il
  Figure 1 Link: articels_figures_by_rev_year\2017\Learning_without_Forgetting\figure_1.jpg
  Figure 1 caption: We wish to add new prediction tasks to an existing CNN vision
    system without requiring access to the training data for existing tasks. This
    table shows relative advantages of our method compared to commonly used methods.
    Limitations of our method include requiring knowing discrete task ID for each
    sample, requiring all new task data in advance, and the performance influenced
    by task similarity. (Section 5.1).
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Learning_without_Forgetting\figure_2.jpg
  Figure 2 caption: Illustration for our method (f) and methods we compare to (b-e).
    Images and labels used in training are shown. Data for different tasks are used
    in alternation in joint training.
  Figure 3 Link: articels_figures_by_rev_year\2017\Learning_without_Forgetting\figure_3.jpg
  Figure 3 caption: Procedure for Learning without Forgetting.
  Figure 4 Link: articels_figures_by_rev_year\2017\Learning_without_Forgetting\figure_4.jpg
  Figure 4 caption: "Performance of each task when gradually adding new tasks to a\
    \ pre-trained network. Different tasks are shown in different sub-graphs. The\
    \ x -axis labels indicate the new task added to the network each time. Error bars\
    \ shows \xB12 standard deviations for 3 runs with different \u03B8 n random initializations.\
    \ Markers are jittered horizontally for visualization, but line plots are not\
    \ jittered to facilitate comparison. For all tasks, our method degrades slower\
    \ over time than fine-tuning and outperforms feature extraction in most scenarios.\
    \ For Places2 \u2192 VOC, our method performs comparably to joint training."
  Figure 5 Link: articels_figures_by_rev_year\2017\Learning_without_Forgetting\figure_5.jpg
  Figure 5 caption: "Influence of subsampling new task training set on compared methods.\
    \ The x -axis indicates diminishing training set size. Three runs of our experiments\
    \ with different random \u03B8 n initialization and dataset subsampling are shown.\
    \ Scatter points are jittered horizontally for visualization, but line plots are\
    \ not jittered to facilitate comparison. Differences between LwF and compared\
    \ methods on both the old task and the new task decrease with less data, but the\
    \ observations remain the same. LwF outperforms fine-tuning despite the change\
    \ in training set size."
  Figure 6 Link: articels_figures_by_rev_year\2017\Learning_without_Forgetting\figure_6.jpg
  Figure 6 caption: Illustration for alternative network modification methods. In
    (a), more fully connected layers are task-specific, rather than shared. In (b),
    nodes for multiple old tasks (not shown) are connected in the same way. LwF can
    also be applied to Network Expansion by unfreezing all nodes and matching output
    responses on the old tasks.
  Figure 7 Link: articels_figures_by_rev_year\2017\Learning_without_Forgetting\figure_7.jpg
  Figure 7 caption: 'Visualization of both new and old task performance for compared
    methods, some with different weights of losses. (a)(b): comparing methods; (c)(d):
    comparing losses. Larger symbols signifies larger lambda o , i.e., heavier weight
    towards response-preserving loss.'
  Figure 8 Link: articels_figures_by_rev_year\2017\Learning_without_Forgetting\figure_8.jpg
  Figure 8 caption: Influence of new-old task similarity on old task performance preservation,
    related to the original old task performance. As the tasks becomes further irrelevant,
    the old task preservation drops.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Zhizhong Li
  Name of the last author: Derek Hoiem
  Number of Figures: 8
  Number of Tables: 2
  Number of authors: 2
  Paper title: Learning without Forgetting
  Publication Date: 2017-11-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance for the Single New Task Scenario
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Performance of Our Method Versus Various Alternative Design
      Choices
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2773081
- Affiliation of the first author: school of computer science, the university of adelaide,
    adelaide, sa, australia
  Affiliation of the last author: school of computer science, the university of adelaide,
    adelaide, sa, australia
  Figure 1 Link: articels_figures_by_rev_year\2017\Guaranteed_Outlier_Removal_for_Point_Cloud_Registration_with_Correspondences\figure_1.jpg
  Figure 1 caption: "Illustrating the concept of GORE for robust point cloud registration\
    \ (1). In the above diagrams, green indicates true inliers I \u2217 and red indicates\
    \ true outliers H\u2216 I \u2217 . In this instance, the input H contains N=2000\
    \ correspondences with outlier rate \u03B7=0.98 . In 4.62 seconds, GORE reduced\
    \ the input to a smaller correspondence set H \u2032 of size 60 with \u03B7=0.28\
    \ . Using \u03C1=0.99 in the stopping criterion (2), executing RANSAC on H \u2032\
    \ terminated in 0.005 seconds. Executing RANSAC on the original input H , however,\
    \ required 132 seconds to converge."
  Figure 10 Link: articels_figures_by_rev_year\2017\Guaranteed_Outlier_Removal_for_Point_Cloud_Registration_with_Correspondences\figure_10.jpg
  Figure 10 caption: A failure case for bunny with N = 73 and eta = 0.23 .
  Figure 2 Link: articels_figures_by_rev_year\2017\Guaranteed_Outlier_Removal_for_Point_Cloud_Registration_with_Correspondences\figure_2.jpg
  Figure 2 caption: Relating the Euclidean and the angular error. (a) A solid ball
    intersects the surface of a sphere at a spherical patch, which has a circular
    outline in the sphere (highlighted in red). (b) Side view of the circular outline.
  Figure 3 Link: articels_figures_by_rev_year\2017\Guaranteed_Outlier_Removal_for_Point_Cloud_Registration_with_Correspondences\figure_3.jpg
  Figure 3 caption: "(a) Interpreting rotation R k according to (11). (b) The uncertainty\
    \ region L k ( x i ) (21). (c) This figure shows S \u03B4(\u03B8) ( A \u03B8,\
    \ y k B x i ) intersecting with S \u03F5 i ( y i ) for a particular \u03B8 . (d)\
    \ We wish to find a bounding interval \u0398 i =[ \u03B8 a , \u03B8 b ]\u2282\
    [\u2212\u03C0,\u03C0] on \u03B8 for which the intersection is non-empty."
  Figure 4 Link: articels_figures_by_rev_year\2017\Guaranteed_Outlier_Removal_for_Point_Cloud_Registration_with_Correspondences\figure_4.jpg
  Figure 4 caption: 'Results of rotation search on synthetic data. Top row: Runtimes
    of different optimization pipelines under different data size N and outlier rates.
    Bottom row: Angular errors of estimated rotations w.r.t. the globally optimal
    rotation.'
  Figure 5 Link: articels_figures_by_rev_year\2017\Guaranteed_Outlier_Removal_for_Point_Cloud_Registration_with_Correspondences\figure_5.jpg
  Figure 5 caption: 'Qualitative results of GORE for 6 DoF Euclidean registration
    (30) with N=500 . Column 1: Input correspondences (true inliers are represented
    by green lines, and true outliers by red lines). Column 2: Data remaining after
    GORE. Column 3: Registration using approximate solution T ~ produced by GORE.'
  Figure 6 Link: articels_figures_by_rev_year\2017\Guaranteed_Outlier_Removal_for_Point_Cloud_Registration_with_Correspondences\figure_6.jpg
  Figure 6 caption: Time (top row) and RMSE (bottom row) per every ( N , dataset)
    combination for 6 DoF Euclidean registration. Time is plotted in log scale.
  Figure 7 Link: articels_figures_by_rev_year\2017\Guaranteed_Outlier_Removal_for_Point_Cloud_Registration_with_Correspondences\figure_7.jpg
  Figure 7 caption: 'Qualitative results of GORE for 6 DoF Euclidean registration
    (30) with N=500 on RGB-D data. Column 1: Input correspondences (true inliers are
    represented by green lines, and true outliers by red lines). Column 2: Data remaining
    after GORE. Column 3: Registration using approximate solution T ~ produced by
    GORE. For localization, the localized object is colored in red.'
  Figure 8 Link: articels_figures_by_rev_year\2017\Guaranteed_Outlier_Removal_for_Point_Cloud_Registration_with_Correspondences\figure_8.jpg
  Figure 8 caption: Time (top row) and RMSE (bottom row) per N for stitching and localization
    problems on RGB-D data. Time is plotted in log scale.
  Figure 9 Link: articels_figures_by_rev_year\2017\Guaranteed_Outlier_Removal_for_Point_Cloud_Registration_with_Correspondences\figure_9.jpg
  Figure 9 caption: Effect of BnB on GORE for synthetic data. (a) Pruning rate of
    GORE with and without BnB is compared for data with 50 to 98 percent outlier ratios.
    (b) Time comparison for GORE with and without BnB.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.94
  Name of the first author: "\xC1lvaro Parra Bustos"
  Name of the last author: Tat-Jun Chin
  Number of Figures: 10
  Number of Tables: 4
  Number of authors: 2
  Paper title: Guaranteed Outlier Removal for Point Cloud Registration with Correspondences
  Publication Date: 2017-11-14 00:00:00
  Table 1 caption:
    table_text: TABLE 1 6 DoF Euclidean Registration Results
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 RGB-D Registration Results
  Table 3 caption:
    table_text: TABLE 3 Effect of BnB on GORE for Real Data
  Table 4 caption:
    table_text: TABLE 4 Comparison Against to Albarelli's Variants
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2773482
- Affiliation of the first author: draper laboratory, cambridge, ma
  Affiliation of the last author: department of electrical engineering and computer
    science, syracuse university, syracuse, ny
  Figure 1 Link: articels_figures_by_rev_year\2017\Copula_Based_Classifier_Fusion_Under_Statistical_Dependence\figure_1.jpg
  Figure 1 caption: Scatter plots of (x,y) with the same marginals but different copulas.
  Figure 10 Link: articels_figures_by_rev_year\2017\Copula_Based_Classifier_Fusion_Under_Statistical_Dependence\figure_10.jpg
  Figure 10 caption: Copula classifier fusion with Twitter data.
  Figure 2 Link: articels_figures_by_rev_year\2017\Copula_Based_Classifier_Fusion_Under_Statistical_Dependence\figure_2.jpg
  Figure 2 caption: "Clayton copula density scatter plot under two different \u03B8\
    \ values."
  Figure 3 Link: articels_figures_by_rev_year\2017\Copula_Based_Classifier_Fusion_Under_Statistical_Dependence\figure_3.jpg
  Figure 3 caption: "Brier scores with 95 percent confidence intervals for \u03C1\
    =0.9 ."
  Figure 4 Link: articels_figures_by_rev_year\2017\Copula_Based_Classifier_Fusion_Under_Statistical_Dependence\figure_4.jpg
  Figure 4 caption: "Brier scores with 95 percent confidence intervals for \u03C1\
    =0.6 ."
  Figure 5 Link: articels_figures_by_rev_year\2017\Copula_Based_Classifier_Fusion_Under_Statistical_Dependence\figure_5.jpg
  Figure 5 caption: "Comparison of logit-normal and copula based fusion models with\
    \ varying training data sizes ( \u03C1=0.9 )."
  Figure 6 Link: articels_figures_by_rev_year\2017\Copula_Based_Classifier_Fusion_Under_Statistical_Dependence\figure_6.jpg
  Figure 6 caption: Histograms of single sensor versus logit-Normal and copula models.
    Copula based fusion provides sharper outputs.
  Figure 7 Link: articels_figures_by_rev_year\2017\Copula_Based_Classifier_Fusion_Under_Statistical_Dependence\figure_7.jpg
  Figure 7 caption: Fusion framework for Twitter bot classification.
  Figure 8 Link: articels_figures_by_rev_year\2017\Copula_Based_Classifier_Fusion_Under_Statistical_Dependence\figure_8.jpg
  Figure 8 caption: Copula density scatter plots of classifier outputs for bot and
    human Twitter users. There is asymmetric positive dependence that we can exploit
    for classifier fusion.
  Figure 9 Link: articels_figures_by_rev_year\2017\Copula_Based_Classifier_Fusion_Under_Statistical_Dependence\figure_9.jpg
  Figure 9 caption: Histograms of single classifier outputs versus those after copula
    based fusion.
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Onur Ozdemir
  Name of the last author: Pramod K. Varshney
  Number of Figures: 10
  Number of Tables: 2
  Number of authors: 5
  Paper title: Copula Based Classifier Fusion Under Statistical Dependence
  Publication Date: 2017-11-16 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Some Commonly Used Copula Functions
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Brier Scores
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2774300
- Affiliation of the first author: visics lab of the katholieke universiteit (ku),
    leuven, belgium
  Affiliation of the last author: key lab of intelligent information processing of
    chinese academy of sciences (cas), institute of computing technology (ict), beijing,
    china
  Figure 1 Link: articels_figures_by_rev_year\2017\Cross_EuclideantoRiemannian_Metric_Learning_with_Application_to_Face_Recognition\figure_1.jpg
  Figure 1 caption: "Conceptual illustration of traditional Euclidean metric learning\
    \ (a), Euclidean-to-Euclidean metric learning (b), Riemannian metric learning\
    \ (c), Riemannian-to-Riemannian metric learning (d) and the proposed cross Euclidean-to-Riemannian\
    \ metric learning (e). R D 1 R D 2 , M M 1 M 2 and R d indicate a Euclidean space,\
    \ a Riemannian manifold and a common subspace, respectively. fg,\u03C6\u03C8 denote\
    \ linear and non-linear transformations, and different shapes (i.e., circles and\
    \ rectangles) represent classes."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Cross_EuclideantoRiemannian_Metric_Learning_with_Application_to_Face_Recognition\figure_2.jpg
  Figure 2 caption: "Overview of the proposed Cross Euclidean-to-Riemannian Metric\
    \ Learning (CERML) framework. R D , M , H x H y , R d represent a Euclidean space,\
    \ a Riemannian manifold, a Hilbert space and a common subspace respectively. f\
    \ x f y , \u03C8 x \u03C8 y denote linear and nonlinear transformation functions,\
    \ and different shapes represent classes."
  Figure 3 Link: articels_figures_by_rev_year\2017\Cross_EuclideantoRiemannian_Metric_Learning_with_Application_to_Face_Recognition\figure_3.jpg
  Figure 3 caption: Example still images and video frames from internet face datasets
    YTC (a), YTF (b) and surveillance-like face datasets PaSC (c), COX (d).
  Figure 4 Link: articels_figures_by_rev_year\2017\Cross_EuclideantoRiemannian_Metric_Learning_with_Application_to_Face_Recognition\figure_4.jpg
  Figure 4 caption: V2SS2V face recognition results (%) of the proposed CEMRL dealing
    with different representations of videos for PaSC (deep feature) and COX (gray
    feature). Here, CERML-E, CERML-G, CERML-A, CERML-S, CERML-EG, CERML-EA, CERML-ES
    respectively indicate videos are represented by mean, subspace, affine subspace,
    SPD matrix, mean+subspace, mean+affine subspace, mean+SPD matrix. Note that CERML-GAS
    is the proposed method in our prior work [21].
  Figure 5 Link: articels_figures_by_rev_year\2017\Cross_EuclideantoRiemannian_Metric_Learning_with_Application_to_Face_Recognition\figure_5.jpg
  Figure 5 caption: Convergence characteristics of the optimization algorithm of the
    proposed CERML-ES in the task of V2S face recognition on COX. Here, the 10 lines
    indicate the results of the 10 random V1-S testings on COX. The value '1' in x
    -axis is the case of the initialization.
  Figure 6 Link: articels_figures_by_rev_year\2017\Cross_EuclideantoRiemannian_Metric_Learning_with_Application_to_Face_Recognition\figure_6.jpg
  Figure 6 caption: V2V face recognition results (%) of the proposed CEMRL dealing
    with different representations of videos using gray features for YTC, YTF, COX
    and deep features for PaSC. Here, CERML-E, CERML-G, CERML-A, CERML-S, CERML-EG,
    CERML-EA, CERML-ES respectively indicate videos are represented by mean, subspace,
    affine subspace, SPD matrix, mean+subspace, mean+affine subspace, mean+SPD matrix.
  Figure 7 Link: articels_figures_by_rev_year\2017\Cross_EuclideantoRiemannian_Metric_Learning_with_Application_to_Face_Recognition\figure_7.jpg
  Figure 7 caption: Convergence behaviors of the optimization algorithm of the proposed
    CERML-ES in the task of V2V face recognition for COX. Here, the 10 lines respectively
    indicate the results of the 10 random V2-V1 tests on COX. The value '1' along
    the x -axis is the case of the initialization.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.64
  Name of the first author: Zhiwu Huang
  Name of the last author: Xilin Chen
  Number of Figures: 7
  Number of Tables: 3
  Number of authors: 5
  Paper title: Cross Euclidean-to-Riemannian Metric Learning with Application to Face
    Recognition from Video
  Publication Date: 2017-11-22 00:00:00
  Table 1 caption:
    table_text: TABLE 1 V2SS2V Face Recognition Results (%) on PaSC and COX Using
      GrayDeep Features
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 V2V Face Recognition Results (%) on YTC and YTF Using GrayDeep
      Features
  Table 3 caption:
    table_text: TABLE 3 V2V Face Recognition Results (%) on PaSC and COX Using GrayDeep
      Features
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2776154
- Affiliation of the first author: department of electrical and computer engineering,
    texas a&m university, college station, tx
  Affiliation of the last author: department of electrical and computer engineering,
    texas a&m university, college station, tx
  Figure 1 Link: articels_figures_by_rev_year\2017\Safe_Feature_Screening_for_Generalized_LASSO\figure_1.jpg
  Figure 1 caption: Schematic illustration of bound propagation algorithm. In the
    figures l 1 and l 2 are two lines corresponding two inequalities of u 1 and u
    2 . (A) Initial context for u 1 and u 2 as the illustrated box. (B) Upper bound
    for u 1 is updated to 0.7 based on the intersection of l 1 and the upper bound
    of u 2 . (C) Upper bound for u 2 is updated to 0.8 due to the intersection of
    l 2 and the upper bound of u 1 .
  Figure 10 Link: articels_figures_by_rev_year\2017\Safe_Feature_Screening_for_Generalized_LASSO\figure_10.jpg
  Figure 10 caption: Dynamic screening. The left figure presents the number of reduced
    features with the increasing number of iterations. The right figure compares the
    running time for ADMM and ADMM with dynamic screening at different duality gap
    values.
  Figure 2 Link: articels_figures_by_rev_year\2017\Safe_Feature_Screening_for_Generalized_LASSO\figure_2.jpg
  Figure 2 caption: Regularization graph examples. (A) Tree graph; (B) Graph with
    loops. Each node corresponds to one entry in D T u , with several entries in the
    vector u .
  Figure 3 Link: articels_figures_by_rev_year\2017\Safe_Feature_Screening_for_Generalized_LASSO\figure_3.jpg
  Figure 3 caption: Algorithm flow for sequential GL screening.
  Figure 4 Link: articels_figures_by_rev_year\2017\Safe_Feature_Screening_for_Generalized_LASSO\figure_4.jpg
  Figure 4 caption: "Rejection rates with and without transformation. The two plots\
    \ in the first row are for GFL Linear Regression based on data from Section 7.1.1\
    \ with the edge number at p\u22121 and 1.2p ; the two plots in the second row\
    \ are for GFL Logistic Regression based on data from Section 7.1.2 with the edge\
    \ number at p\u22121 and 1.2p . In the figures, \u201CBP\u201D stands for bound\
    \ propagation, and \u201CTransf+BP\u201D is bound propagation with transformation."
  Figure 5 Link: articels_figures_by_rev_year\2017\Safe_Feature_Screening_for_Generalized_LASSO\figure_5.jpg
  Figure 5 caption: Rejection rate for GFL screening when the edge number is p-1 .
    The upper left figure is for the synthetic data in Section 7.1.1; the upper right
    figure is for the FDG-PET data set in Section 7.4.1. The lower left figure is
    for the synthetic data in Section 7.1.2; and the lower right figure is for the
    breast cancer data in Section 7.4.2. For these four data sets, we use 50 or 100
    lambda 's ranging from 0.05 times lambda max to lambda max .
  Figure 6 Link: articels_figures_by_rev_year\2017\Safe_Feature_Screening_for_Generalized_LASSO\figure_6.jpg
  Figure 6 caption: Rejection rates for SGFL Linear Regression on simulation data
    with different lambda 1lambda 2 ratios.
  Figure 7 Link: articels_figures_by_rev_year\2017\Safe_Feature_Screening_for_Generalized_LASSO\figure_7.jpg
  Figure 7 caption: Rejection rates for CPLEX and Bound Propagation on GFL with the
    edge density |E|=1.2p (left) and |E|=1.3p (right) ( n=50 and p=500 ).
  Figure 8 Link: articels_figures_by_rev_year\2017\Safe_Feature_Screening_for_Generalized_LASSO\figure_8.jpg
  Figure 8 caption: Average upper and lower bounds by bound propagation and CPLEX
    on GFL with the edge density |E|=1.2p (left) and |E|=1.3p (right) ( n=50 and p=500
    ).
  Figure 9 Link: articels_figures_by_rev_year\2017\Safe_Feature_Screening_for_Generalized_LASSO\figure_9.jpg
  Figure 9 caption: Mean and variance values for bound difference between CPLEX and
    bound propagation on GFL with the edge density |E|=1.2p (first two figures) and
    |E|=1.3p (third and forth figures) ( n=50 and p=500 ).
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.95
  Name of the first author: Shaogang Ren
  Name of the last author: Xiaoning Qian
  Number of Figures: 12
  Number of Tables: 9
  Number of authors: 4
  Paper title: Safe Feature Screening for Generalized LASSO
  Publication Date: 2017-11-22 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Dual Forms of Different Loss Functions in (2)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 Results for GFL Linear Regression on Synthetic Data with\
      \ Different Edge Density ( \u03C3=1.0 )"
  Table 3 caption:
    table_text: TABLE 3 Results for GFL Linear Regression on Synthetic Data with Different
      Noise Level ( |E|=1.1p )
  Table 4 caption:
    table_text: "TABLE 4 Results for GFL Logistic Regression on Synthetic Data with\
      \ Different Edge Density ( \u03C3=1.0 )"
  Table 5 caption:
    table_text: "TABLE 5 Results on Synthetic Data for SGFL Linear Regression with\
      \ Different Edge Densities ( \u03C3=1.0 )"
  Table 6 caption:
    table_text: TABLE 6 Running Time (in seconds) for CPLEX and Bound Propagation
  Table 7 caption:
    table_text: TABLE 7 Results on FDG-PET Data Set
  Table 8 caption:
    table_text: TABLE 8 Results for GFL-LogR on Breast Cancer Data Set
  Table 9 caption:
    table_text: TABLE 9 Results on Breast Cancer Data for SGFL Logistic Regression
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2776267
- Affiliation of the first author: school of data and computer science, sun yat-sen
    university, guangzhou, guangdong province, china
  Affiliation of the last author: department of electrical and computer engineering,
    national university of singapore, singapore
  Figure 1 Link: articels_figures_by_rev_year\2017\ProposalFree_Network_for_InstanceLevel_Object_Segmentation\figure_1.jpg
  Figure 1 caption: Exemplar instance-level object segmentation results. Different
    colors indicate the different object instances for each category. To better show
    the predicted locations of each instance, we plot velocity vectors starting from
    each pixel to its corresponding predicted instance center as shown by the arrow.
    Note that the pixels predicting similar object centers can be directly collected
    as one instance region. Best viewed in color.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\ProposalFree_Network_for_InstanceLevel_Object_Segmentation\figure_2.jpg
  Figure 2 caption: An overview of our proposal-free network. The targets of our PFN
    include the number of instances of all categories, category-level confidences
    for each pixel and the coordinates of the instance bounding box that each pixel
    belongs to. The off-the-self clustering method can be utilized to generate final
    instance-level segmentation results.
  Figure 3 Link: articels_figures_by_rev_year\2017\ProposalFree_Network_for_InstanceLevel_Object_Segmentation\figure_3.jpg
  Figure 3 caption: The detailed network architecture and parameter setting of PFN.
    First, the category-level segmentation network is fine-tuned based on the pre-trained
    VGG-16 classification network. The cross-entropy loss is used for optimization.
    Second, the instance-level segmentation network that simultaneously predicts the
    number of instances of all categories and the instance location vector for each
    pixel is further fine-tuned. The multi-scale prediction streams (with different
    resolution and reception fields) are appended to the intermediate convolutional
    layers, and are then fused to generate ultimate instance location predictions.
    The regression loss is used during training. To predict the number of instances,
    the convolutional feature maps and the instance location maps are concatenated
    together for inference, and the euclidean loss is used. The two losses from two
    targets are jointly optimized for the whole network training.
  Figure 4 Link: articels_figures_by_rev_year\2017\ProposalFree_Network_for_InstanceLevel_Object_Segmentation\figure_4.jpg
  Figure 4 caption: The exemplar segmentation results by refining the category-level
    segmentation with the predicted numbers of instances. For each image, we show
    their classification results inferred from category-level segmentation and the
    predicted numbers of instances in the left. In the first row, the refining strategy
    is to convert the inconsistent predicted labels into background. In the second
    row, the refining strategy is to convert the wrongly predicted labels in category-level
    segmentation to the ones predicted in the number vector of instances. Different
    colors indicate different object instances. Better viewed in zoomed-in color pdf
    file.
  Figure 5 Link: articels_figures_by_rev_year\2017\ProposalFree_Network_for_InstanceLevel_Object_Segmentation\figure_5.jpg
  Figure 5 caption: Comparison of segmentation results by constraining the number
    of pixels of each clustered object instance. The version without size constraints
    wrongly clusters two neighboring instances as one instance while our full version
    can precisely separate them.
  Figure 6 Link: articels_figures_by_rev_year\2017\ProposalFree_Network_for_InstanceLevel_Object_Segmentation\figure_6.jpg
  Figure 6 caption: Illustration of instance-level object segmentation results by
    the proposed PFN. We also compare our results with that of SDS [39]. Since SDS
    [39] predicts the scores for each proposal of each category, the top 10 predictions
    for each image of the person category are shown for the visual comparison. Note
    that for instance-level segmentation results, different colors only indicate different
    object instances and do not represent the semantic categories. In terms of category-level
    segmentation, different colors are used to denote different semantic labels. Best
    viewed in color.
  Figure 7 Link: articels_figures_by_rev_year\2017\ProposalFree_Network_for_InstanceLevel_Object_Segmentation\figure_7.jpg
  Figure 7 caption: Illustration of failure cases. Our PFN may fail to segment object
    instances with heavy occlusion (in first row) and small size (in second row).
    The instance-level segmentation of our PFN is also limited by the accuracy of
    category-level segmentation prediction (in third and fourth row).
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Xiaodan Liang
  Name of the last author: Shuicheng Yan
  Number of Figures: 7
  Number of Tables: 5
  Number of authors: 6
  Paper title: Proposal-Free Network for Instance-Level Object Segmentation
  Publication Date: 2017-11-22 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Comparison of Instance-Level Segmentation Performance with
      Four State-of-the-Arts Using Mean A P r Metric over 20 Classes at 0.5 and 0.7
      IoU Scores, When Evaluating with the Ground-Truth Annotations from SBD Dataset,
      Including 5,623 Images for Training and 5,732 for Testing
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Comparison of Instance-Level Segmentation Performance with
      Two State-of-the-Arts Using A P r Metric over 20 Classes at 0.5 IoU on the PASCAL
      VOC 2012 Validation Set, Including 9,906 Images for Training and 1,449 Images
      for Testing
  Table 3 caption:
    table_text: TABLE 3 Comparison of Instance-Level Segmentation Performance with
      Different Architecture Variants of Our PFN Using A P r Metric over 20 Classes
      at 0.5 IoU on the Randomly Selected 1,449 Images from PASCAL VOC 2012 Train
      Set for the Validation
  Table 4 caption:
    table_text: TABLE 4 Comparison of Instance-Level Segmentation Performance with
      Different Architecture Variants of Our PFN Using A P r vol Metric over 20 Classes
      That Averages All A P r Performance from 0.1 to 0.9 IoU Scores on the Randomly
      Selected 1,449 Images from PASCAL VOC 2012 Train Set for the Validation
  Table 5 caption:
    table_text: TABLE 5 Per-Class Instance-Level Segmentation Results Using A P r
      Metric over 20 Classes at 0.6 to 0.9 (with a Step Size of 0.1) IoU Scores on
      the VOC PASCAL 2012 Validation Set
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2775623
- Affiliation of the first author: department of electrical and electronic engineering,
    imperial college london, london, united kingdom
  Affiliation of the last author: department of electrical and electronic engineering,
    imperial college london, london, united kingdom
  Figure 1 Link: articels_figures_by_rev_year\2017\Learning_Kinematic_Structure_Correspondences_Using_MultiOrder_Similarities\figure_1.jpg
  Figure 1 caption: The proposed framework reliably builds up kinematic structure
    correspondence matches across heterogeneous objects captured with different sensors.
    Our method can for example find correspondences between a upper-body dancing human
    in a 2D grey image sequence, the iCub and NAO humanoid robots in 2D RGB videos,
    and a dancing human in depth image sequences.
  Figure 10 Link: articels_figures_by_rev_year\2017\Learning_Kinematic_Structure_Correspondences_Using_MultiOrder_Similarities\figure_10.jpg
  Figure 10 caption: Various kinematic structure correspondence matching results using
    the proposed method (best viewed in colour). The bottom-left correspondence is
    analysed in more detail in Fig. 12.
  Figure 2 Link: articels_figures_by_rev_year\2017\Learning_Kinematic_Structure_Correspondences_Using_MultiOrder_Similarities\figure_2.jpg
  Figure 2 caption: Overall pipeline of the proposed method. First, features are extracted
    from the two image sequences, and the motion segments are estimated. Based on
    this, we generate the kinematic structures of the object within each image sequence.
    We use hypergraph matching with normalised weight terms to simultaneously consider
    structural topology, kinematic correlation and combinatorial motion. The resulting
    correspondences are robust to noise and outliers, and can be used in various computer
    vision and robotics applications.
  Figure 3 Link: articels_figures_by_rev_year\2017\Learning_Kinematic_Structure_Correspondences_Using_MultiOrder_Similarities\figure_3.jpg
  Figure 3 caption: "Conceptual illustration of hypergraph edges showing the three\
    \ different orders \u03BA(e)=1 , \u03BA(e)=2 and \u03BA(e)=3 ."
  Figure 4 Link: articels_figures_by_rev_year\2017\Learning_Kinematic_Structure_Correspondences_Using_MultiOrder_Similarities\figure_4.jpg
  Figure 4 caption: "Example of the consecutive subgraph isomorphisms of G into G\
    \ \u2032 . The subgraph isomorphisms of G into a perfect (i.e., noise free, without\
    \ node removal) G \u2032 are used to produce the correspondence matrix M (illustrated\
    \ in a cyan background region). Using a node removal process, we generate the\
    \ subset G \u2032 composed of all valid graph variations G \u2032 k . Note that\
    \ the variation G \u2032 \u2216 v \u2032 2 \u2032 is not added to G \u2032 as\
    \ deg( v \u2032 2 \u2032 )=3 , and thus v \u2032 2 \u2032 cannot be removed (illustrated\
    \ in a yellow background region). Top right: Example of a possible subgraph isomorphism\
    \ X l of G into G \u2032 . Note that A \u2217 l (1, 1 \u2032 )=|deg( v 1 )\u2212\
    deg( v \u2032 1 \u2032 )|=|1\u22124|=3 triggers the local constraint \u03B8 .\
    \ The global constraint \u03C4 is checked against \u2211 i, i \u2032 A \u2217\
    \ l (i, i \u2032 )=3+0+0+0+1=4 . The subgraph isomorphism is only kept if both\
    \ the local and the global constraint are validated."
  Figure 5 Link: articels_figures_by_rev_year\2017\Learning_Kinematic_Structure_Correspondences_Using_MultiOrder_Similarities\figure_5.jpg
  Figure 5 caption: "Visualization of the joint estimates and motion range descriptor.\
    \ The left half shows the segment centre positions y , and the neighbouring points\
    \ N to find the joint positions J (Eq. (10)). The right half visualises the vectors\
    \ v which are used to calculate the angles \u03B1 (Eq. (11)). Best viewed in colour."
  Figure 6 Link: articels_figures_by_rev_year\2017\Learning_Kinematic_Structure_Correspondences_Using_MultiOrder_Similarities\figure_6.jpg
  Figure 6 caption: Performance according to outlier tests (left), kinematic deformation
    ratio (centre) and symmetry order changes (right). The kinematic structures in
    the top row are generated in 2D and the bottom row's results are from 3D structures.
    It can be observed that our method achieves the best performance over all other
    algorithms in most cases.
  Figure 7 Link: articels_figures_by_rev_year\2017\Learning_Kinematic_Structure_Correspondences_Using_MultiOrder_Similarities\figure_7.jpg
  Figure 7 caption: 'Experiments on real image datasets: dancing human versus dancing
    iCub. Two static images are used for the appearance based methods (best viewed
    in colour).'
  Figure 8 Link: articels_figures_by_rev_year\2017\Learning_Kinematic_Structure_Correspondences_Using_MultiOrder_Similarities\figure_8.jpg
  Figure 8 caption: 'Experiments on real image datasets: Baxter versus OWI-535 Robotic
    Arm Edge (best viewed in colour).'
  Figure 9 Link: articels_figures_by_rev_year\2017\Learning_Kinematic_Structure_Correspondences_Using_MultiOrder_Similarities\figure_9.jpg
  Figure 9 caption: 'Experiments on real image datasets: Yellow crane versus digging
    arm (best viewed in colour).'
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Hyung Jin Chang
  Name of the last author: Yiannis Demiris
  Number of Figures: 15
  Number of Tables: 2
  Number of authors: 5
  Paper title: Learning Kinematic Structure Correspondences Using Multi-Order Similarities
  Publication Date: 2017-11-24 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance on the Real Kinematic Structure Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Validations on the Threshold Parameters
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2777486
- Affiliation of the first author: human longevity inc., mountain view, ca
  Affiliation of the last author: human longevity inc., mountain view, ca
  Figure 1 Link: articels_figures_by_rev_year\2017\Ensembles_of_Lasso_Screening_Rules\figure_1.jpg
  Figure 1 caption: "Graphical illustration for three different cases, where z is\
    \ maximized in x T j v=z . (a): the case where x T j v=z is tangent to the sphere\
    \ at its contact, and meets the half-space constraint; (b): the case where b k\
    \ \u2212 a T k o\u22650 , and the hyperplane x T j v=z meets the intersection\
    \ between the sphere and the half-space constraint; (c): the case where b k \u2212\
    \ a T k o<0 , and the hyperplane x T j v=z meets the intersection between the\
    \ sphere and the half-space constraint."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2017\Ensembles_of_Lasso_Screening_Rules\figure_2.jpg
  Figure 2 caption: "Rejection ratio and runtime of various screening methods on simulated\
    \ datasets with the feature correlations of 0.1 (first column), 0.5 (second column),\
    \ and 0.9 (third column) (see text for details) given a sequence of \u03BB parameters.\
    \ Two AdaScreen instances with 100 dual lasso half-space constraints outperformed\
    \ the other methods in terms of both rejection ratio and runtime."
  Figure 3 Link: articels_figures_by_rev_year\2017\Ensembles_of_Lasso_Screening_Rules\figure_3.jpg
  Figure 3 caption: "Rejection ratio (first row) and runtime (second row) for comparison\
    \ between AdaScreen and BagScreen (first column); comparison between AdaScreen\
    \ and strong rule (second column); and demonstration of the effects of local half-spaces\
    \ on screening (third column) on simulated datasets with the feature correlation\
    \ of 0.5 given a sequence of \u03BB parameters."
  Figure 4 Link: articels_figures_by_rev_year\2017\Ensembles_of_Lasso_Screening_Rules\figure_4.jpg
  Figure 4 caption: Rejection ratio and runtime on PEMS (first column), Alzheimer's
    disease (second column), and PIE image (third column) datasets by three instances
    of AdaScreen, Sasvi, EDPP, DPP, DOME, and SAFE rules given a sequence of lambda
    parameters.
  Figure 5 Link: articels_figures_by_rev_year\2017\Ensembles_of_Lasso_Screening_Rules\figure_5.jpg
  Figure 5 caption: Speed-up comparison of a variety of screening algorithms on PEMS
    (left), Alzheimer's disease (center), and PIE image (right) dataset.
  Figure 6 Link: articels_figures_by_rev_year\2017\Ensembles_of_Lasso_Screening_Rules\figure_6.jpg
  Figure 6 caption: Accuracy in mean squared error (MSE) for varying regularization
    parameter lambda on PEMS (left), Alzheimer's disease (center), and PIE image (right)
    dataset.
  Figure 7 Link: articels_figures_by_rev_year\2017\Ensembles_of_Lasso_Screening_Rules\figure_7.jpg
  Figure 7 caption: The speed-up for various solvers using AdaScreen when compared
    against the corresponding solver without screening.
  Figure 8 Link: articels_figures_by_rev_year\2017\Ensembles_of_Lasso_Screening_Rules\figure_8.jpg
  Figure 8 caption: 'UML diagram of our screening implementation: The modular design
    allows us to easily implement screening rules by integrating any sphere and any
    multiple half-space constraints.'
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.83
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Seunghak Lee
  Name of the last author: Christoph Lippert
  Number of Figures: 8
  Number of Tables: 2
  Number of authors: 5
  Paper title: Ensembles of Lasso Screening Rules
  Publication Date: 2017-11-24 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Summary of Lasso Screening Algorithms with Their Sphere and
      Half-Space Constraints (Seq. Column Shows Whether Sequential Screening Is Supported
      or Not)
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 List of Screening Rules and Their Properties Implemented in
      Our Screening Software Package
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2017.2765321
