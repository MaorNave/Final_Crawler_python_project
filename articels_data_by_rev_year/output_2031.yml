- Affiliation of the first author: national engineering research center for multimedia
    software, school of computer science, institute of artificial intelligence and
    the hubei key laboratory of multimedia and network communication engineering,
    wuhan university, wuhan, china
  Affiliation of the last author: riken center for advanced intelligence project,
    and the department of complexity science and engineering, graduate school of frontier
    sciences, university of tokyo, tokyo, japan
  Figure 1 Link: articels_figures_by_rev_year\2021\LocalDrop_A_Hybrid_Regularization_for_Deep_Neural_Networks\figure_1.jpg
  Figure 1 caption: 'The y -axis of three figures all represent classification error
    (%). Left: (a) The performance of LocalDrop and adaptive dropout with the Rademacher
    complexity (i.e., AD with RC) [36] to prevent overfitting influenced by the size
    of two fully-connected layers on CIFAR-10. Middle: (b) The performance of LocalDrop
    and adaptive dropout with the Rademacher complexity [36] influenced by epoch number
    on CIFAR-10. Right: (c) The performance of LocalDrop and adaptive dropout with
    the Rademacher complexity [36] influenced by epoch number on CIFAR-100.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\LocalDrop_A_Hybrid_Regularization_for_Deep_Neural_Networks\figure_2.jpg
  Figure 2 caption: "The comparison of the derived upper bound with the classification\
    \ error (%) in the optimization process on CIFAR-10 dataset in FCNs. We set \u03B4\
    \ as 1 for simplicity. Two y-axes are applied for better visualization of the\
    \ comparison between the derived upper bound and the actual classification error\
    \ (%)."
  Figure 3 Link: articels_figures_by_rev_year\2021\LocalDrop_A_Hybrid_Regularization_for_Deep_Neural_Networks\figure_3.jpg
  Figure 3 caption: The basic network structure of LocalDrop with DropBlock in ResNet-50.
  Figure 4 Link: Not Available
  Figure 4 caption: Not Available
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.95
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ziqing Lu
  Name of the last author: Masashi Sugiyama
  Number of Figures: 3
  Number of Tables: 7
  Number of authors: 6
  Paper title: 'LocalDrop: A Hybrid Regularization for Deep Neural Networks'
  Publication Date: 2021-02-23 00:00:00
  Table 1 caption: TABLE 1 Classification Error (%) on CIFAR Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Classification Error (%) on CIFAR-10, CIFAR-100 and ILSVRC2012
    Datasets
  Table 3 caption: "TABLE 3 The Influences of \u03BB \u03BB on Classification Error(%)\
    \ of CIFAR-10"
  Table 4 caption: "TABLE 4 The Influences of h h and m m on Classification Error(%)\
    \ on CIFAR-10 When \u03BB=0.1 \u03BB=0.1"
  Table 5 caption: TABLE 5 Time Comsumption (Minutes) on CIFAR Datasets Every 100
    Epochs
  Table 6 caption: TABLE 6 The Influences of Block Size on Classification Error(%)
    of Three Datasets
  Table 7 caption: TABLE 7 Time Comsumption (Minutes) on Three Datasets Every 100
    Epochs
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3061463
- Affiliation of the first author: saudi electronic university, jeddah, saudi arabia
  Affiliation of the last author: united imaging intelligence (uii) america, inc.,
    cambridge, ma, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\ZeroShot_Deep_Domain_Adaptation_With_Common_Representation_Learning\figure_1.jpg
  Figure 1 caption: "We propose zero-shot deep domain adaptation (ZDDA) for domain\
    \ adaptation and sensor fusion. ZDDA learns from the task-irrelevant dual-domain\
    \ pairs when the task-relevant target-domain training data is unavailable. In\
    \ this example domain adaptation task (MNIST [33] \u2192 MNIST-M [14]), the task-irrelevant\
    \ gray-RGB pairs are from the Fashion-MNIST [60] dataset and the Fashion-MNIST-M\
    \ dataset (the colored version of the Fashion-MNIST [60] dataset with the details\
    \ in Section 4.1)."
  Figure 10 Link: articels_figures_by_rev_year\2021\ZeroShot_Deep_Domain_Adaptation_With_Common_Representation_Learning\figure_10.jpg
  Figure 10 caption: The experimental results of changing the number of classes used
    in the task-irrelevant training data for the experiment IDs 2 and 5 of the 10-class
    classification in Table 8.
  Figure 2 Link: articels_figures_by_rev_year\2021\ZeroShot_Deep_Domain_Adaptation_With_Common_Representation_Learning\figure_2.jpg
  Figure 2 caption: An overview of ZDDA template of training procedure which has three
    steps or phases. In step 1, we choose to train s 1 and fix t , but we can also
    train t and fix s 1 to simulate the target-domain representation. In step 2, t
    can also be trainable instead of being fixed, but we choose to fix it to make
    the number of trainable parameters manageable. The details are explained in Section
    3.
  Figure 3 Link: articels_figures_by_rev_year\2021\ZeroShot_Deep_Domain_Adaptation_With_Common_Representation_Learning\figure_3.jpg
  Figure 3 caption: An overview of the ZDDA-C training procedure for classification
    task. We use the images from the SUN RGB-D [49] dataset for illustration. ZDDA-C
    simulates the target-domain representation using the source-domain data, builds
    a joint network with the supervision from the source domain, and trains a sensor
    fusion network.
  Figure 4 Link: articels_figures_by_rev_year\2021\ZeroShot_Deep_Domain_Adaptation_With_Common_Representation_Learning\figure_4.jpg
  Figure 4 caption: An overview of the ZDDA testing procedure. We use the SUN RGB-D
    [49] images for illustration. Different from the color coding in Fig. 3, the colors
    here are purely used to distinguish different CNNsclassifierspredictions.
  Figure 5 Link: articels_figures_by_rev_year\2021\ZeroShot_Deep_Domain_Adaptation_With_Common_Representation_Learning\figure_5.jpg
  Figure 5 caption: An overview of the ZDDA-ML training procedure which has three
    steps or phases.
  Figure 6 Link: articels_figures_by_rev_year\2021\ZeroShot_Deep_Domain_Adaptation_With_Common_Representation_Learning\figure_6.jpg
  Figure 6 caption: An overview of the ZDDA-ML testing procedures for domain adaptation
    and sensor fusion.
  Figure 7 Link: articels_figures_by_rev_year\2021\ZeroShot_Deep_Domain_Adaptation_With_Common_Representation_Learning\figure_7.jpg
  Figure 7 caption: A triplet contains a three pairs of RGB-D images.
  Figure 8 Link: articels_figures_by_rev_year\2021\ZeroShot_Deep_Domain_Adaptation_With_Common_Representation_Learning\figure_8.jpg
  Figure 8 caption: The t-SNE visualization results for the representations generated
    by the baseline and ours. (a), (b) and (c) are the t-SNE visualization results
    of the feature representations produced by naive source only baseline, ZDDA-C
    2 and target only baseline, respectively. All of them are trained for the task
    MNIST rightarrow MNIST-M.
  Figure 9 Link: articels_figures_by_rev_year\2021\ZeroShot_Deep_Domain_Adaptation_With_Common_Representation_Learning\figure_9.jpg
  Figure 9 caption: Performance comparison between the two sensor fusion methods using
    (top two) 1 random black rectangle as the testing noise model. The testing noisy
    image is created by adding a black rectangle with a random location and size to
    the clean image. We compare the classification accuracy of (a) naive fusion and
    (b) ZDDA-C 3 under different noise levels in both RGB and depth testing data.
    (c) shows that ZDDA-C 3 outperforms the naive fusion under all the noise levels.
    The bottom row uses black images as the noisy images. We compare the classification
    accuracy (%) of (d) naive fusion and (e) ZDDA-C 3 under different noise levels
    in both RGB and depth testing data. (f) shows that ZDDA-C 3 outperforms the naive
    fusion under most conditions.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.75
  Name of the first author: Mohammed Kutbi
  Name of the last author: Ziyan Wu
  Number of Figures: 12
  Number of Tables: 17
  Number of authors: 3
  Paper title: Zero-Shot Deep Domain Adaptation With Common Representation Learning
  Publication Date: 2021-02-23 00:00:00
  Table 1 caption: TABLE 1 Working Condition Comparison Between ZDDA and Other Existing
    Methods
  Table 10 caption: TABLE 10 10-Fold Cross Validation With Different Selections of
    Classes for 9-Scene Classification
  Table 2 caption: TABLE 2 Problem Setting Comparison Between ZDDA, Unsupervised Domain
    Adaptation (UDA), Multi-View Learning (MVL), and Domain Generalization (DG)
  Table 3 caption: TABLE 3 The Statistics of the Datasets we Use
  Table 4 caption: TABLE 4 The Number of the RGB-D Pairs of Our Selected 10 Scenes
    in the SUN RGB-D [49] Dataset
  Table 5 caption: TABLE 5 The Base Network Architecture (BNA) we Use in Our Experiments
  Table 6 caption: "TABLE 6 The Overall Average per Class Accuracy (%) of the Domain\
    \ Adaptation Tasks (Gray Scale Images \u2192 \u2192 RGB Images) Formed by the\
    \ Datasets in Table 3, Where we Introduce the Dataset IDs and Use Them to Refer\
    \ to the Datasets Here"
  Table 7 caption: "TABLE 7 The Performance Comparison of the Domain Adaptation Task\
    \ MNIST \u2192 \u2192MNIST-M"
  Table 8 caption: TABLE 8 Performance Comparison With Different Numbers of Classes
    in Scene Classification
  Table 9 caption: TABLE 9 5-Fold Cross Validation With Different Training-Testing
    Splits for 10-Scene Classification
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3061204
- Affiliation of the first author: department of information and computing sciences,
    utrecht university, utrecht, the netherlands
  Affiliation of the last author: department of information and computing sciences,
    utrecht university, utrecht, the netherlands
  Figure 1 Link: articels_figures_by_rev_year\2021\MultiDataset_Multitask_Learning_of_Egocentric_Vision_Tasks\figure_1.jpg
  Figure 1 caption: We adapt the MTL network structure from [24] to accommodate the
    tasks from a range of datasets within a single network. In (a) the network combines
    task-specific output layers by aggregating the gradients from each output. In
    (b) we extend the structure by further attaching task-specific layers for the
    additional tasks in the new datasets.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\MultiDataset_Multitask_Learning_of_Egocentric_Vision_Tasks\figure_2.jpg
  Figure 2 caption: Mixed-batch loss approximation. A batch is subsampled for each
    task. The loss from each task layer is averaged over its datasets samples. Task-specific
    losses are passed through their respective layers.
  Figure 3 Link: articels_figures_by_rev_year\2021\MultiDataset_Multitask_Learning_of_Egocentric_Vision_Tasks\figure_3.jpg
  Figure 3 caption: Correlations for classification weights across tasks in multi-dataset
    model E ALL + G ALL (Zoom in for best view).
  Figure 4 Link: articels_figures_by_rev_year\2021\MultiDataset_Multitask_Learning_of_Egocentric_Vision_Tasks\figure_4.jpg
  Figure 4 caption: Confusion matrices for mapped verbs (a) and mapped nouns (b) from
    the EGTEA tasks on the EPIC validation split.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Georgios Kapidis
  Name of the last author: Remco C. Veltkamp
  Number of Figures: 4
  Number of Tables: 9
  Number of authors: 3
  Paper title: Multi-Dataset, Multitask Learning of Egocentric Vision Tasks
  Publication Date: 2021-02-23 00:00:00
  Table 1 caption: TABLE 1 List of Datasets and Their Characteristics
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 SD-MTL Top1Top5 Accuracy (%) for Actions (A), Verbs (V)
    and Nouns (N) for EPIC and EGTEA (Reported From [24]) and for Activities (A) and
    Locations (L) for ADL for the Best Performing Weights on (A)
  Table 3 caption: TABLE 3 EPIC, EGTEA, and ADL MD-MTL Task Combinations
  Table 4 caption: TABLE 4 Mapping EGTEA Verb-Noun Tasks on EPIC
  Table 5 caption: TABLE 5 Comparison Between SD and MD-MTL on the Mapped Verbs and
    Nouns
  Table 6 caption: TABLE 6 Action Recognition Performance on Charego1 and Charego3,
    the First- and Third-Person Splits of Charades-EGO, Respectively
  Table 7 caption: TABLE 7 Comparison Across Batch Formation Strategies Mixed (MB),
    Interleaved Batches (IB), and Interleaved Datasets (ID) on the Task Combinations
    of EPIC and EGTEA
  Table 8 caption: TABLE 8 State-of-the-Art Comparison on EPIC-Kitchens
  Table 9 caption: TABLE 9 Action Recognition Accuracy on EGTEA Gaze+
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3061479
- Affiliation of the first author: institute of high performance computing, astar,
    singapore
  Affiliation of the last author: institute of high performance computing, astar,
    singapore
  Figure 1 Link: articels_figures_by_rev_year\2021\Natural_Language_Video_Localization_A_Revisit_in_SpanBased_Question_Answering_Fr\figure_1.jpg
  Figure 1 caption: An illustration of localizing a temporal moment in an untrimmed
    video by a given language query, and the general procedures of natural language
    video localization.
  Figure 10 Link: articels_figures_by_rev_year\2021\Natural_Language_Video_Localization_A_Revisit_in_SpanBased_Question_Answering_Fr\figure_10.jpg
  Figure 10 caption: Statistic of normalized moment lengths in videos for both TACoS
    textorg and TACoS tan .
  Figure 2 Link: articels_figures_by_rev_year\2021\Natural_Language_Video_Localization_A_Revisit_in_SpanBased_Question_Answering_Fr\figure_2.jpg
  Figure 2 caption: An overview of the proposed architectures for NLVL. The feature
    extractors, i.e., GloVe and 3D ConvNet, are fixed during training. (a) depicts
    the structure of VSLNet. (b) shows the architecture of VSLNet-L. Note the standard
    span-based QA framework (VSLBase) is similar to VSLNet by removing the Self-Attention
    and Query-Guided Highlighting modules.
  Figure 3 Link: articels_figures_by_rev_year\2021\Natural_Language_Video_Localization_A_Revisit_in_SpanBased_Question_Answering_Fr\figure_3.jpg
  Figure 3 caption: The structure of feature encoder.
  Figure 4 Link: articels_figures_by_rev_year\2021\Natural_Language_Video_Localization_A_Revisit_in_SpanBased_Question_Answering_Fr\figure_4.jpg
  Figure 4 caption: "An illustration of foreground and background of visual features.\
    \ \u03B1 is the ratio of foreground extension."
  Figure 5 Link: articels_figures_by_rev_year\2021\Natural_Language_Video_Localization_A_Revisit_in_SpanBased_Question_Answering_Fr\figure_5.jpg
  Figure 5 caption: The structure of query-guided highlighting (QGH).
  Figure 6 Link: articels_figures_by_rev_year\2021\Natural_Language_Video_Localization_A_Revisit_in_SpanBased_Question_Answering_Fr\figure_6.jpg
  Figure 6 caption: Illustration of splitting video into clip segments.
  Figure 7 Link: articels_figures_by_rev_year\2021\Natural_Language_Video_Localization_A_Revisit_in_SpanBased_Question_Answering_Fr\figure_7.jpg
  Figure 7 caption: The structure of nil prediction module (NPM).
  Figure 8 Link: articels_figures_by_rev_year\2021\Natural_Language_Video_Localization_A_Revisit_in_SpanBased_Question_Answering_Fr\figure_8.jpg
  Figure 8 caption: The Mean IoU (%) performance of CTRL [25], SCDM [32], 2D-TAN Pool
    [42] and VSLNet on the TACoS dataset.
  Figure 9 Link: articels_figures_by_rev_year\2021\Natural_Language_Video_Localization_A_Revisit_in_SpanBased_Question_Answering_Fr\figure_9.jpg
  Figure 9 caption: Mean IoU ( % ) results of VSLNet on the TACoS tan dataset under
    different maximal visual representation lengths n .
  First author gender probability: 0.9
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Hao Zhang
  Name of the last author: Rick Siow Mong Goh
  Number of Figures: 18
  Number of Tables: 10
  Number of authors: 6
  Paper title: 'Natural Language Video Localization: A Revisit in Span-Based Question
    Answering Framework'
  Publication Date: 2021-02-23 00:00:00
  Table 1 caption: TABLE 1 Statistics of the Natural Language Video Localization (NLVL)
    Datasets
  Table 10 caption: TABLE 10 Results ( % %) of VSLNet-L on TACoS Using Different Split
    Scales With n=600 n=600
  Table 2 caption: "TABLE 2 Results ( % %) of \u201C Rank1,IoU=\u03BC Rank 1, IoU\
    \ =\u03BC\u201D and \u201CmIoU\u201D Compared With the State-of-the-Art on Charades-STA"
  Table 3 caption: "TABLE 3 Results ( % %) of \u201C Rank1,IoU=\u03BC Rank1,IoU=\u03BC\
    \u201D and \u201CmIoU\u201D Compared With the State-of-the-Art on TACoS"
  Table 4 caption: "TABLE 4 Results ( % %) of \u201C Rank1,IoU=\u03BC Rank1,IoU=\u03BC\
    \u201D and \u201CmIoU\u201D Compared With State-of-the-Arts on ActivityNet Captions"
  Table 5 caption: TABLE 5 Statistics of Videos and Annotations w.r.t. Different Video
    Lengths Over NLVL Datasets
  Table 6 caption: TABLE 6 Comparison of mIoU ( % %) Between VSLNet and VSLNet-L on
    TACoS Dataset w.r.t. Different Video Lengths
  Table 7 caption: TABLE 7 Comparison of mIoU ( % %) Between VSLNet and VSLNet-L on
    ActivityNet Captions w.r.t. Different Video Lengths
  Table 8 caption: TABLE 8 Comparison Between Models With Alternative Modules in VSLBase
    on Charades-STA
  Table 9 caption: "TABLE 9 Performance Gains ( % %) of Different Modules Over \u201C\
    \ Rank1,IoU=0.7 Rank 1, IoU =0.7\u201D on Charades-STA"
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3060449
- Affiliation of the first author: school of automation, southeast university, nanjing,
    china
  Affiliation of the last author: australian institute of artificial intelligence,
    university of technology sydney, ultimo, nsw, australia
  Figure 1 Link: articels_figures_by_rev_year\2021\Recursive_Copy_and_Paste_GAN_Face_Hallucination_From_Shaded_Thumbnails\figure_1.jpg
  Figure 1 caption: Motivation of Re-CPGAN. Internal and recursive external CPnets
    are introduced to mimic the Clone Stamp Tool. Internal CPnet copies well-illuminated
    facial details and then paste them onto shadow regions. Recursive external CPnet
    further retouches the face using an external guided face from the UI-HR face database
    during the upsampling process to compensate for uneven illumination in the final
    HR face.
  Figure 10 Link: articels_figures_by_rev_year\2021\Recursive_Copy_and_Paste_GAN_Face_Hallucination_From_Shaded_Thumbnails\figure_10.jpg
  Figure 10 caption: Illustration of the generated NI faces. (a) The UI face (original
    UI-HR face in Multi-PIE). (b) The generated NI face samples of (a). (c) The spatially
    transformed and downsampled versions of (b).
  Figure 2 Link: articels_figures_by_rev_year\2021\Recursive_Copy_and_Paste_GAN_Face_Hallucination_From_Shaded_Thumbnails\figure_2.jpg
  Figure 2 caption: "Comparison of our proposed Re-CPGAN with the state-of-the-art\
    \ methods ( 16\xD716 , 8\xD7 ). (( 16\xD716 , 8\xD7 ): 16\xD716 represents the\
    \ resolution of the original input NI-LR face; 8\xD7 represents the magnification\
    \ factor). (a) Interpolated unaligned NI-LR image. (b) Guided UI-HR image ( 128\xD7\
    128 pixels). (c) Ground-truth UI-HR image ( 128\xD7128 pixels, not available in\
    \ training). (d) Illumination normalization result of (a) by applying [3]. (e)\
    \ Face hallucination result of (d) by applying [4]. (f) Result of face hallucination\
    \ followed by illumination normalization by applying [4] and then [3] to the NI-LR\
    \ face. (g) Result of our previous method CPGAN [5]. (h) Result of Re-CPGAN (\
    \ 128\xD7128 pixels)."
  Figure 3 Link: articels_figures_by_rev_year\2021\Recursive_Copy_and_Paste_GAN_Face_Hallucination_From_Shaded_Thumbnails\figure_3.jpg
  Figure 3 caption: The pipeline of the copy and paste upsampling network (CPUN) in
    our proposed Re-CPGAN.
  Figure 4 Link: articels_figures_by_rev_year\2021\Recursive_Copy_and_Paste_GAN_Face_Hallucination_From_Shaded_Thumbnails\figure_4.jpg
  Figure 4 caption: The architecture of the internal CPnet. Copy block here treats
    the output features of CA module as the both content features and guided features.
    Paste block here represents the additive operation functionally. Here, we conduct
    the channel-wise average operation on the feature maps.
  Figure 5 Link: articels_figures_by_rev_year\2021\Recursive_Copy_and_Paste_GAN_Face_Hallucination_From_Shaded_Thumbnails\figure_5.jpg
  Figure 5 caption: The architecture of the recursive external CPnet. Here, STN, RB
    and HEM represent the spatial transformer network, the residual block, and the
    heatmap estimation module, respectively.
  Figure 6 Link: articels_figures_by_rev_year\2021\Recursive_Copy_and_Paste_GAN_Face_Hallucination_From_Shaded_Thumbnails\figure_6.jpg
  Figure 6 caption: "The diagram of the copy block. The \u201Ccopied\u201D features\
    \ FCGN represent the output features of the copy block in the Re-CPGAN variant\
    \ with N EX-CP units. Here, we conduct the channel-wise average operation on the\
    \ feature maps."
  Figure 7 Link: articels_figures_by_rev_year\2021\Recursive_Copy_and_Paste_GAN_Face_Hallucination_From_Shaded_Thumbnails\figure_7.jpg
  Figure 7 caption: Impacts of different components and losses on face super-resolution
    ( 16times 16 , 8times ). (a) Interpolated unaligned NI-LR image. (b) Ground-truth
    UI-HR image ( 128times 128 pixels). (c) Result without using the internal CPnet
    but a simple input convolutional layer instead. (d) Result without using the recursive
    external CPnet. (e) Result of Re-CPGAN without adopting Lic . Note that specular
    appears in the left side of the forehead. (f) Result of Re-CPGAN trained by Lmse
    . (g) Result of Re-CPGAN trained by Lmse and Lid . (h) Result of CPUN (Re-CPGAN
    without employing Ladv ). (i) Result without using an external guided face. Content
    features ( FC ) replace guided features ( FG ) in the copy block. (j) Result without
    data augmentation. (k) Result of Re-CPGAN with 8 Ex-CP units. (l) Result of Re-CPGAN.
    Note that, in this experiment 16 Ex-CP units are used for all the cases except
    (k).
  Figure 8 Link: articels_figures_by_rev_year\2021\Recursive_Copy_and_Paste_GAN_Face_Hallucination_From_Shaded_Thumbnails\figure_8.jpg
  Figure 8 caption: The framework of our proposed RaIN model.
  Figure 9 Link: articels_figures_by_rev_year\2021\Recursive_Copy_and_Paste_GAN_Face_Hallucination_From_Shaded_Thumbnails\figure_9.jpg
  Figure 9 caption: The testing stage of our RaIN model. RAIN enables us to generate
    sufficient NI faces with random illumination conditions from some sampled vectors
    z .
  First author gender probability: 0.66
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.6
  Name of the first author: Yang Zhang
  Name of the last author: Xin Yu
  Number of Figures: 20
  Number of Tables: 5
  Number of authors: 6
  Paper title: 'Recursive Copy and Paste GAN: Face Hallucination From Shaded Thumbnails'
  Publication Date: 2021-02-23 00:00:00
  Table 1 caption: "TABLE 1 Average PSNR [dB], SSIM and FLLE Results of Compared Methods\
    \ on the Testing Sets ( 16\xD716 16\xD716, 8\xD7 8\xD7)"
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 Ablation Study of Different Sub-Networks on CelebA and\
    \ Multi-PIE Databases ( 16\xD716 16\xD716, 8\xD7 8\xD7)"
  Table 3 caption: "TABLE 3 Ablation Study of Different Losses ( 16\xD716 16\xD716,\
    \ 8\xD7 8\xD7)"
  Table 4 caption: TABLE 4 Face Recognition Performance Comparison on the Multi-PIE
    Database
  Table 5 caption: TABLE 5 Face Expression Classification Results for Different Methods
    on the Multi-PIE Expression Dataset
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3061312
- Affiliation of the first author: university of rochester, rochester, ny, usa
  Affiliation of the last author: kwai inc., bellevue, wa, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\PINE_Universal_Deep_Embedding_for_Graph_Nodes_via_Partial_Permutation_Invariant_\figure_1.jpg
  Figure 1 caption: "Illustration of f(\u22C5) for \u201CHomogeneous\u201D Graph Node\
    \ Embedding Learning using Theorem 3.3. (There is only 1 type of nodes.) For an\
    \ arbitrary node v , the neighborhood information X v is aggregated via the function\
    \ h and g to compute the embedding of node v . Since there is only 1-type node\
    \ neighbors, the function g are shared for all neighbors."
  Figure 10 Link: articels_figures_by_rev_year\2021\PINE_Universal_Deep_Embedding_for_Graph_Nodes_via_Partial_Permutation_Invariant_\figure_10.jpg
  Figure 10 caption: F1-score (macro, micro) (%) of multi-label classification in
    heterogeneous graphs.
  Figure 2 Link: articels_figures_by_rev_year\2021\PINE_Universal_Deep_Embedding_for_Graph_Nodes_via_Partial_Permutation_Invariant_\figure_2.jpg
  Figure 2 caption: "Illustration of f(\u22C5) for \u201CHeterogeneous\u201D Graph\
    \ Node Embedding Learning using Theorem 3.3. (There are K types of nodes). g 1\
    \ ,\u2026, g K are dealing with K -type node neighborhood respectively. The outputs\
    \ of g 1 ,\u2026, g K are concatenated and h extracts the heterogeneous neighborhood\
    \ information to formulate the embedding of node v ."
  Figure 3 Link: articels_figures_by_rev_year\2021\PINE_Universal_Deep_Embedding_for_Graph_Nodes_via_Partial_Permutation_Invariant_\figure_3.jpg
  Figure 3 caption: Accuracy (%) of multi-class classification in Cora, Citeseer,
    Pubmed, Wikipedia, and Email-eu dataset.
  Figure 4 Link: articels_figures_by_rev_year\2021\PINE_Universal_Deep_Embedding_for_Graph_Nodes_via_Partial_Permutation_Invariant_\figure_4.jpg
  Figure 4 caption: The t-SNE of Cora dataset.
  Figure 5 Link: articels_figures_by_rev_year\2021\PINE_Universal_Deep_Embedding_for_Graph_Nodes_via_Partial_Permutation_Invariant_\figure_5.jpg
  Figure 5 caption: The t-SNE of Citeseer dataset.
  Figure 6 Link: articels_figures_by_rev_year\2021\PINE_Universal_Deep_Embedding_for_Graph_Nodes_via_Partial_Permutation_Invariant_\figure_6.jpg
  Figure 6 caption: The t-SNE of Email-eu dataset.
  Figure 7 Link: articels_figures_by_rev_year\2021\PINE_Universal_Deep_Embedding_for_Graph_Nodes_via_Partial_Permutation_Invariant_\figure_7.jpg
  Figure 7 caption: The t-SNE of Pubmed dataset.
  Figure 8 Link: articels_figures_by_rev_year\2021\PINE_Universal_Deep_Embedding_for_Graph_Nodes_via_Partial_Permutation_Invariant_\figure_8.jpg
  Figure 8 caption: The t-SNE of Wikipedia dataset.
  Figure 9 Link: articels_figures_by_rev_year\2021\PINE_Universal_Deep_Embedding_for_Graph_Nodes_via_Partial_Permutation_Invariant_\figure_9.jpg
  Figure 9 caption: Dimension sensitivity illustration for PINE on Cora and Wikipedia.
  First author gender probability: 0.88
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.78
  Name of the first author: Shupeng Gui
  Name of the last author: Ji Liu
  Number of Figures: 10
  Number of Tables: 4
  Number of authors: 8
  Paper title: 'PINE: Universal Deep Embedding for Graph Nodes via Partial Permutation
    Invariant Set Functions'
  Publication Date: 2021-02-23 00:00:00
  Table 1 caption: TABLE 1 Summary of Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Inductive Prediction Results for the PPI Dataset (Micro-Averaged
    F1 Scores)
  Table 3 caption: TABLE 3 Prediction Results for the Reddit Dataset (Micro-Averaged
    F1 Scores)
  Table 4 caption: TABLE 4 Prediction Results on the Heterogeneous Graph Node Classification
    Task (F1 Scores)
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3061162
- Affiliation of the first author: school of mathematics and statistics, xian jiaotong
    university, xian, shaanxi, china
  Affiliation of the last author: school of mathematics and statistics, xian jiaotong
    university, xian, shaanxi, china
  Figure 1 Link: articels_figures_by_rev_year\2021\Variational_HyperAdam_A_MetaLearning_Approach_to_Network_Training\figure_1.jpg
  Figure 1 caption: "The probabilistic modeling of network optimization using learning-based\
    \ optimizer. O represents the learned optimizer. g t is the gradient of the optimizee\
    \ at the t th training step, and D t is the mini-batch of training data. d t is\
    \ the parameter update vector with approximate posterior distribution q \u0398\
    \ ( d t | D t , w t\u22121 ) , from which d ~ t is sampled to update network parameter.\
    \ H t is the historical information of optimization trajectory."
  Figure 10 Link: articels_figures_by_rev_year\2021\Variational_HyperAdam_A_MetaLearning_Approach_to_Network_Training\figure_10.jpg
  Figure 10 caption: Comparisons of different optimizers for training CNN-1 and CNN-2
    on MNISTCIFAR10 with batch normalization and dropout for longer horizons.
  Figure 2 Link: articels_figures_by_rev_year\2021\Variational_HyperAdam_A_MetaLearning_Approach_to_Network_Training\figure_2.jpg
  Figure 2 caption: "Diagram of StateBlock. Normalization refers to normalizing the\
    \ gradient g t with its euclidean norm. \u201CFC + ELU\u201D means fully-connected\
    \ layer with ELU as nonlinear activation function."
  Figure 3 Link: articels_figures_by_rev_year\2021\Variational_HyperAdam_A_MetaLearning_Approach_to_Network_Training\figure_3.jpg
  Figure 3 caption: "The architecture of VR-HyperAdam. As shown on the left sub-figure,\
    \ from bottom to top, the gradient vector of network parameters is first processed\
    \ by StateBlock to output current state at step t , then taken as inputs to VarBlock\
    \ and ExpBlock for estimating variance and expectation for q \u0398 ( d t | D\
    \ t , w t\u22121 ) . After that, by reparameterization, a parameter update vector\
    \ d ~ t is sampled from q \u0398 ( d t | D t , w t\u22121 ) . The ExpBlock on\
    \ the right sub-figure is based on HyperAdam in our conference version [24]."
  Figure 4 Link: articels_figures_by_rev_year\2021\Variational_HyperAdam_A_MetaLearning_Approach_to_Network_Training\figure_4.jpg
  Figure 4 caption: "An illustration of parameter optimization by VR-HyperAdam algorithm.\
    \ d ~ t is sampled from approximate posterior distribution q \u0398 ( d t | D\
    \ t , w t\u22121 ) with expectation \u03BC t as an ensemble of several moments\
    \ generated by Adams with different adaptively learned exponential decay rates."
  Figure 5 Link: articels_figures_by_rev_year\2021\Variational_HyperAdam_A_MetaLearning_Approach_to_Network_Training\figure_5.jpg
  Figure 5 caption: "Diagram of AdamCell where C t =[ m t , v t , \u03B2 t , \u03B3\
    \ t ] , \u039B t =[ \u03B2 t , \u03B3 t , \u03B2 t , \u03B3 t ] , and C ~ t =[\
    \ G t , G 2 t ,1,1] ( 1\u2208 R N w \xD7J )."
  Figure 6 Link: articels_figures_by_rev_year\2021\Variational_HyperAdam_A_MetaLearning_Approach_to_Network_Training\figure_6.jpg
  Figure 6 caption: Comparisons of different optimizers for training single-layer
    MLPs with different activation functions.
  Figure 7 Link: articels_figures_by_rev_year\2021\Variational_HyperAdam_A_MetaLearning_Approach_to_Network_Training\figure_7.jpg
  Figure 7 caption: Comparisons of different optimizers for training deep MLP.
  Figure 8 Link: articels_figures_by_rev_year\2021\Variational_HyperAdam_A_MetaLearning_Approach_to_Network_Training\figure_8.jpg
  Figure 8 caption: Comparisons of different optimizers for training LSTM.
  Figure 9 Link: articels_figures_by_rev_year\2021\Variational_HyperAdam_A_MetaLearning_Approach_to_Network_Training\figure_9.jpg
  Figure 9 caption: Comparisons of different optimizers for training 9-hidden-layer
    MLP, 2-layer LSTM and TSCModel for longer training steps.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Shipeng Wang
  Name of the last author: Zongben Xu
  Number of Figures: 18
  Number of Tables: 3
  Number of authors: 4
  Paper title: 'Variational HyperAdam: A Meta-Learning Approach to Network Training'
  Publication Date: 2021-02-23 00:00:00
  Table 1 caption: TABLE 1 Final Losses for Training Single-Layer MLP After 100 Steps
    With Different Activation Functions
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Top-1 Accuracy of ResNet-20, ResNet-32 and TSCModel on
    Validation Sets
  Table 3 caption: TABLE 3 Comparisons of Runtime
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3061581
- Affiliation of the first author: pca lab, key laboratory of intelligent perception
    and systems for high-dimensional information of ministry of education, school
    of computer science and engineering, nanjing university of science and technology,
    nanjing, china
  Affiliation of the last author: jd explore academy, jd.com, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\InstanceDependent_Positive_and_Unlabeled_Learning_With_Labeling_Bias_Estimation\figure_1.jpg
  Figure 1 caption: Setting comparison of our PU method and existing methods. (a)
    shows the existing instance-independent models, and (b) illustrates our instance-dependent
    setting.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2021\InstanceDependent_Positive_and_Unlabeled_Learning_With_Labeling_Bias_Estimation\figure_2.jpg
  Figure 2 caption: "The performances of various methods on the synthetic dataset\
    \ under Strategy 1. (a) shows the real positive and negative data; (c) shows the\
    \ unlabeled and biased positive data for model training; (b) and (d) present the\
    \ true \u03B7(x) and the estimated \u03B7(x) of LBE; (e) \u223C (j) display the\
    \ classification results generated by uPU, nnPU, LDCE, PUSB, PWE, and LBE. The\
    \ classification accuracy of every method is presented above the corresponding\
    \ subfigure."
  Figure 3 Link: articels_figures_by_rev_year\2021\InstanceDependent_Positive_and_Unlabeled_Learning_With_Labeling_Bias_Estimation\figure_3.jpg
  Figure 3 caption: "The performances of various methods on the synthetic dataset\
    \ under Strategy 2. (a) shows the real positive and negative data; (c) shows the\
    \ unlabeled and biased positive data for model training; (b) and (d) present the\
    \ true \u03B7(x) and the estimated \u03B7(x) of LBE; (e) \u223C (j) display the\
    \ classification results generated by uPU, nnPU, LDCE, PUSB, PWE, and LBE. The\
    \ classification accuracy of every method is presented above the corresponding\
    \ subfigure."
  Figure 4 Link: articels_figures_by_rev_year\2021\InstanceDependent_Positive_and_Unlabeled_Learning_With_Labeling_Bias_Estimation\figure_4.jpg
  Figure 4 caption: "The performances of LBE-MLP on the synthetic dataset under Strategies\
    \ 1 and 2. The upper panel refers to Strategy 1, where (a) \u223C (d) show the\
    \ unlabeled and biased positive data for model training, true \u03B7(x) , the\
    \ estimated \u03B7(x) by LBE-MLP, and the classification result of LBE-MLP, respectively.\
    \ The lower panel refers to Strategy 2, where (e) \u223C (h) have the same meanings\
    \ as (a) \u223C (d)."
  Figure 5 Link: articels_figures_by_rev_year\2021\InstanceDependent_Positive_and_Unlabeled_Learning_With_Labeling_Bias_Estimation\figure_5.jpg
  Figure 5 caption: Examples of even and odd images in the adopted USPS dataset.
  Figure 6 Link: articels_figures_by_rev_year\2021\InstanceDependent_Positive_and_Unlabeled_Learning_With_Labeling_Bias_Estimation\figure_6.jpg
  Figure 6 caption: Examples of fighting and non-fighting video frames in HockeyFight
    dataset.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.64
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Chen Gong
  Name of the last author: Dacheng Tao
  Number of Figures: 6
  Number of Tables: 3
  Number of authors: 7
  Paper title: Instance-Dependent Positive and Unlabeled Learning With Labeling Bias
    Estimation
  Publication Date: 2021-02-23 00:00:00
  Table 1 caption: TABLE 1 The Characteristics of Six UCI Datasets
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 Comparison of Test Accuracies (Mean \xB1 \xB1std) for\
    \ Our Proposed Method and the Baselines on Six UCI Datasets Under Different Sampling\
    \ Strategies and \u03C0 \u03C0s"
  Table 3 caption: TABLE 3 Comparison of Test Accuracies for Our Proposed Method and
    the Baselines on Three Real-World Datasets Including HockeyFight, USPS, and SwiffProt
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3061456
- Affiliation of the first author: state key laboratory of information security, institute
    of information engineering, chinese academy of sciences, beijing, china
  Affiliation of the last author: university of california, merced and yonsei university,
    merced, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2021\Deblurring_Dynamic_Scenes_via_Spatially_Varying_Recurrent_Neural_Networks\figure_1.jpg
  Figure 1 caption: A challenging dynamic scene example. As the blur in the image
    is spatially varying, conventional CNN-based methods (which usually adopt convolution
    and non-linear activation operations, e.g., Nah et al. [4] and Tao et al. [5],
    to approximate this problem) do not handle this problem well. The results by [4]
    and [5] still have some remaining blurs in the deblurred results. Our method is
    based on a spatially varying RNN, which is able to model the spatially varying
    property, capture a larger receptive field, and thus generates a sharper image.
  Figure 10 Link: articels_figures_by_rev_year\2021\Deblurring_Dynamic_Scenes_via_Spatially_Varying_Recurrent_Neural_Networks\figure_10.jpg
  Figure 10 caption: "Effectiveness of the proposed RNNs, where \u201CAF\u201D is\
    \ the abbreviation of after. (a) and (l) are the blurry input and its ground-truth.\
    \ (b) is estimated by the proposed network without RNNs. (c)-(f) are selected\
    \ feature maps before the 1D RNNs, after the first, third, and fourth 1D RNNs,\
    \ respectively. (g) is estimated by the proposed 1D RNN based network. (h)-(j)\
    \ are selected feature maps before the 2D RNNs, after the first and second 2D\
    \ RNNs, respectively. (k) is estimated by the proposed network with 2D RNN. Without\
    \ RNNs, the outputs are still blurry with significant artifacts and RNNs can help\
    \ remove this degradation as well as recover the clean image."
  Figure 2 Link: articels_figures_by_rev_year\2021\Deblurring_Dynamic_Scenes_via_Spatially_Varying_Recurrent_Neural_Networks\figure_2.jpg
  Figure 2 caption: The deconvolution process needs to consider a large image region.
    (a) is a clean image. (c) is obtained by blurring (a) with the motion kernel from
    [56] as shown in (b). (d) shows a regularized inverse filter from Wiener filtering
    [57], which can remove the corresponding motion blur in (c). (e) is the deblurred
    result by the inverse kernel. The non-zero region of the inverse filter in (d)
    is much larger than the blur kernel in (b).
  Figure 3 Link: articels_figures_by_rev_year\2021\Deblurring_Dynamic_Scenes_via_Spatially_Varying_Recurrent_Neural_Networks\figure_3.jpg
  Figure 3 caption: "A toy example of fusing the information of the spatially varying\
    \ RNN from different directions by the CNN, where \u201CRF\u201D is the abbreviation\
    \ of receptive field. The non-black region is the receptive field of the center\
    \ red pixel. (a) Four receptive fields of the RNN from different directions, and\
    \ each RNN only considers 1D information. Without adding the CNN between RNNs,\
    \ this formulation not effectively take 2D information into account. (b) Adding\
    \ a 1\xD71 CNN can fuse the information from the RNNs. (c) Adding another RNN\
    \ facilitates the proposed model to consider 2D information with a large receptive\
    \ field."
  Figure 4 Link: articels_figures_by_rev_year\2021\Deblurring_Dynamic_Scenes_via_Spatially_Varying_Recurrent_Neural_Networks\figure_4.jpg
  Figure 4 caption: Receptive fields of spatially varying RNNs with different connections
    for the center red pixel. (a) 1D RNN with one-way connection and (b) 2D spatial
    propagation RNN with three-way connection. The propagation of one-way connections
    is restricted to a single row, while the three-way connections can expand the
    region to a triangular 2D plane with respect to each direction.
  Figure 5 Link: articels_figures_by_rev_year\2021\Deblurring_Dynamic_Scenes_via_Spatially_Varying_Recurrent_Neural_Networks\figure_5.jpg
  Figure 5 caption: Proposed network model. The reddish-brown blocks denote shared
    layers in both the 1D and 2D RNN-based deblurring networks; the blue blocks in
    (b) denote 1D RNN configuration; the red blocks in (c) denote 2D RNN configuration.
    The proposed network uses two CNNs to extract features and generate pixel-wise
    weights for the spatially varying RNN. For RNN deconvolution, four 1D RNNs or
    two 2D RNNs are applied to the feature maps to remove blurs and each RNN considers
    four directions. A convolution layer is added after every RNN to fuse the information.
    Four skip links are added between feature extraction and image reconstruction
    as well as in the weight generation network. One CNN is used in image reconstruction
    to estimate the final deblurred image. The non-linear function ReLU or Leaky ReLU
    is used after each CNN. Table 1 reports detailed CNN configurations of the proposed
    network.
  Figure 6 Link: articels_figures_by_rev_year\2021\Deblurring_Dynamic_Scenes_via_Spatially_Varying_Recurrent_Neural_Networks\figure_6.jpg
  Figure 6 caption: An illustrative example of the proposed model for image deblurring
    with a 1D RNN. We use the left-to-right recurrent propagation in 1D RNN as an
    example. The deep CNN generates a weight map that guides the propagation of the
    RNN. For the adjacent pixels (depicted in blue) in a sharp region, the estimated
    RNN weights are close to zero, which indicates that there is no interaction between
    these two pixels. For the adjacent pixels (depicted in green) in a blurred region,
    the estimated weights reflect the motion cues that are propagated and used for
    deblurring.
  Figure 7 Link: articels_figures_by_rev_year\2021\Deblurring_Dynamic_Scenes_via_Spatially_Varying_Recurrent_Neural_Networks\figure_7.jpg
  Figure 7 caption: Quantitative evaluations on the dynamic scene deblurring dataset
    [4]. The proposed method generates much clearer images with higher PSNR and SSIM
    values.
  Figure 8 Link: articels_figures_by_rev_year\2021\Deblurring_Dynamic_Scenes_via_Spatially_Varying_Recurrent_Neural_Networks\figure_8.jpg
  Figure 8 caption: Qualitative evaluation for the video deblurring dataset [64].
    Our algorithm performs favorably against the state-of-the-art deblurring methods.
  Figure 9 Link: articels_figures_by_rev_year\2021\Deblurring_Dynamic_Scenes_via_Spatially_Varying_Recurrent_Neural_Networks\figure_9.jpg
  Figure 9 caption: Qualitative evaluation on the real blurry dataset [7]. The proposed
    method generates much sharper images with clearer structures and characters.
  First author gender probability: 0.95
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Wenqi Ren
  Name of the last author: Ming-Hsuan Yang
  Number of Figures: 12
  Number of Tables: 4
  Number of authors: 8
  Paper title: Deblurring Dynamic Scenes via Spatially Varying Recurrent Neural Networks
  Publication Date: 2021-02-23 00:00:00
  Table 1 caption: TABLE 1 Network Configurations
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Quantitative Evaluation on the Dynamic Scene Deblurring
    Dataset [4] in Terms of PSNR and SSIM
  Table 3 caption: "TABLE 3 Run-Time and Model Size for an Image With the Size of\
    \ 720\xD71280 Pixels"
  Table 4 caption: TABLE 4 Ablation Study on the Dynamic Scene Dataset [4]
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3061604
- Affiliation of the first author: institute for artificial intelligence, tsinghua
    university (thuai), beijing, china
  Affiliation of the last author: institute for artificial intelligence, tsinghua
    university (thuai), beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2021\SelfReinforcing_Unsupervised_Matching\figure_1.jpg
  Figure 1 caption: Forming process of feature matrices. Feature encoder is trained
    on the augmented data of seen modality in advance. Here the Chinese character
    images are taken for example. Font of Song is treated as the seen modality and
    Lishu as the emerging modality to be matched.
  Figure 10 Link: articels_figures_by_rev_year\2021\SelfReinforcing_Unsupervised_Matching\figure_10.jpg
  Figure 10 caption: Match results on the one-template traffic sign matching problem.
    For one in-the-field traffic sign (top row of each subfigure), its matched clean
    signs ranking from one to five inferred by SUM are presented below. Images in
    green boxes denote the ground-truth matched signs.
  Figure 2 Link: articels_figures_by_rev_year\2021\SelfReinforcing_Unsupervised_Matching\figure_2.jpg
  Figure 2 caption: "A toy example of DPW matching. Grayscale values (black numbers)\
    \ are given inside pixels. Pixel indices of S are marked using numbers in red\
    \ boxes. Element correspondence between S and E by DPW are depicted with the same\
    \ pixel indices in red boxes. If using euclidean distance as the distance measurement\
    \ for two pixels, the DPW distance D dpw (S,E)=654 , but the naive pixel-wise\
    \ distance ||S\u2212E| | 1 =1118 ."
  Figure 3 Link: articels_figures_by_rev_year\2021\SelfReinforcing_Unsupervised_Matching\figure_3.jpg
  Figure 3 caption: 'Intuitive function of the LoFA: adapt the local feature of emerging
    image to render it compatible with the paired local feature of seen image.'
  Figure 4 Link: articels_figures_by_rev_year\2021\SelfReinforcing_Unsupervised_Matching\figure_4.jpg
  Figure 4 caption: "Diagram for (a) \u201Coptimize\u201D and (b) \u201Cadapt\u201D\
    \ in SFM. In (a), the strips of E and S in same color represent matched feature\
    \ elements evaluated by DPW. In (b), the strips also represent feature elements\
    \ and the adapted feature elements in E (t+1) maintain their original positions\
    \ in E ."
  Figure 5 Link: articels_figures_by_rev_year\2021\SelfReinforcing_Unsupervised_Matching\figure_5.jpg
  Figure 5 caption: "Schematic of two-tier self-reinforcing learning mechanism. SFM\
    \ loop works inside SIM loop. Subsets S k i n T i=1 and E l i n T i=1 are formed\
    \ after the T th SIM loop by absorbing the potentially matched cross-modality\
    \ feature matrices from S i N i=1 and E i N i=1 according to the statistics of\
    \ the DPW distances between S i N i=1 and the adapted feature matrices of E T\
    \ i N i=1 , where E T i =LoFA( E i | w \u2217 T ) (see Algorithm 3)."
  Figure 6 Link: articels_figures_by_rev_year\2021\SelfReinforcing_Unsupervised_Matching\figure_6.jpg
  Figure 6 caption: A general processing pipeline that humans deal with the cross-modality
    visual matching problems at the level of cognitive psychology.
  Figure 7 Link: articels_figures_by_rev_year\2021\SelfReinforcing_Unsupervised_Matching\figure_7.jpg
  Figure 7 caption: Model architecture for the deep domain confusion method.
  Figure 8 Link: articels_figures_by_rev_year\2021\SelfReinforcing_Unsupervised_Matching\figure_8.jpg
  Figure 8 caption: Examples of matching falsely with high frequency under metric
    of top-1 match accuracy. In each subfigure, the emerging character to be matched
    is presented on left, and several most frequently matched characters of seen modality
    inferred by SUM are presented on right. Bottom frequency is computed based on
    the statistics over the 100 experimental results. Images in green boxes denote
    ground-truth matched characters.
  Figure 9 Link: articels_figures_by_rev_year\2021\SelfReinforcing_Unsupervised_Matching\figure_9.jpg
  Figure 9 caption: Match accuracies on one-template traiffic sign matching problem.
  First author gender probability: 0.95
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.63
  Name of the first author: Jiang Lu
  Name of the last author: Changshui Zhang
  Number of Figures: 13
  Number of Tables: 2
  Number of authors: 3
  Paper title: Self-Reinforcing Unsupervised Matching
  Publication Date: 2021-02-24 00:00:00
  Table 1 caption: TABLE 1 Architectures of the CNN Trained on Seen Modalities
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 Top-1 and Top-5 Match Accuracies on the One-Template Chinese\
    \ Character Matching Problem (Mean \xB1 \xB1 Std % %)"
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2021.3061945
