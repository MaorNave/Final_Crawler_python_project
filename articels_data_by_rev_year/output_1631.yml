- Affiliation of the first author: idiap research institute, martigny, switzerland
  Affiliation of the last author: idiap research institute, martigny, switzerland
  Figure 1 Link: articels_figures_by_rev_year\2020\A_Bayesian_Approach_to_Recurrence_in_Neural_Networks\figure_1.jpg
  Figure 1 caption: "The long short term memory of [8]. Non-linearities \u03C8 are\
    \ taken to be tanh ."
  Figure 10 Link: articels_figures_by_rev_year\2020\A_Bayesian_Approach_to_Recurrence_in_Neural_Networks\figure_10.jpg
  Figure 10 caption: Word Error Rate (%) on AMI for various RNN architectures.
  Figure 2 Link: articels_figures_by_rev_year\2020\A_Bayesian_Approach_to_Recurrence_in_Neural_Networks\figure_2.jpg
  Figure 2 caption: "The gated recurrent unit of [11]. As in the LSTM, the non-linearity\
    \ \u03C8 is usually tanh ."
  Figure 3 Link: articels_figures_by_rev_year\2020\A_Bayesian_Approach_to_Recurrence_in_Neural_Networks\figure_3.jpg
  Figure 3 caption: Logit and odds curves.
  Figure 4 Link: articels_figures_by_rev_year\2020\A_Bayesian_Approach_to_Recurrence_in_Neural_Networks\figure_4.jpg
  Figure 4 caption: A Bayesian recurrent unit incorporating a probabilistic forget
    gate.
  Figure 5 Link: articels_figures_by_rev_year\2020\A_Bayesian_Approach_to_Recurrence_in_Neural_Networks\figure_5.jpg
  Figure 5 caption: The layer-wise recursion with a forget gate.
  Figure 6 Link: articels_figures_by_rev_year\2020\A_Bayesian_Approach_to_Recurrence_in_Neural_Networks\figure_6.jpg
  Figure 6 caption: The layer-wise recursion with a forget gate and an input gate.
  Figure 7 Link: articels_figures_by_rev_year\2020\A_Bayesian_Approach_to_Recurrence_in_Neural_Networks\figure_7.jpg
  Figure 7 caption: Phoneme Error Rate (%) on TIMIT for various RNN architectures.
  Figure 8 Link: articels_figures_by_rev_year\2020\A_Bayesian_Approach_to_Recurrence_in_Neural_Networks\figure_8.jpg
  Figure 8 caption: Phoneme Error Rate (%) on TIMIT for various RNN architectures.
  Figure 9 Link: articels_figures_by_rev_year\2020\A_Bayesian_Approach_to_Recurrence_in_Neural_Networks\figure_9.jpg
  Figure 9 caption: Word Error Rate (%) on WSJ for various RNN architectures.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.96
  Name of the first author: Philip N. Garner
  Name of the last author: Sibo Tong
  Number of Figures: 10
  Number of Tables: 1
  Number of authors: 2
  Paper title: A Bayesian Approach to Recurrence in Neural Networks
  Publication Date: 2020-02-28 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Statistics of Datasets Used in This Work: Speakers and Sentences
      are Counts, the Amounts of Speech Data for Training and Evaluation Sets are
      in Hours'
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2976978
- Affiliation of the first author: "institut pascal, universit\xE9 clermont auvergne\
    \ cnrs, clermont-ferrand, france"
  Affiliation of the last author: "institut pascal, universit\xE9 clermont auvergne\
    \ cnrs, clermont-ferrand, france"
  Figure 1 Link: articels_figures_by_rev_year\2020\Rolling_Shutter_Homography_and_its_Applications\figure_1.jpg
  Figure 1 caption: (a) Video-based (frame-by-frame processing) methods assume smooth
    [4], [21] or even constant velocity [16] between each two consecutive frames.
    However, for a general unordered set of images (b), it is hard or impossible to
    enforce the relative poses and their instantaneous-motions basing on these assumptions.
  Figure 10 Link: articels_figures_by_rev_year\2020\Rolling_Shutter_Homography_and_its_Applications\figure_10.jpg
  Figure 10 caption: Mapping and 3D reconstruction errors by using GSH and RSH on
    sequence trans and rot respectively.
  Figure 2 Link: articels_figures_by_rev_year\2020\Rolling_Shutter_Homography_and_its_Applications\figure_2.jpg
  Figure 2 caption: RS images (a). Stitching results obtained with well-known commercial
    stitching applications such as AutoStitch [22] (b) Microsoft Image Composite Editor
    (ICE) [23] (c) Adobe Photoshop [24] (d) state-of-the-art multiple homographies
    stitching method APAP [25] (e) and AANAP [26] (f). The stitching results and the
    correction of the RS effects obtained with the the proposed method are shown in
    (g) and (h).
  Figure 3 Link: articels_figures_by_rev_year\2020\Rolling_Shutter_Homography_and_its_Applications\figure_3.jpg
  Figure 3 caption: Examples illustrating the general RS Homography case (a) and the
    rotate-only case (b).
  Figure 4 Link: articels_figures_by_rev_year\2020\Rolling_Shutter_Homography_and_its_Applications\figure_4.jpg
  Figure 4 caption: Comparison of GS homography mapping and RS homography mapping.
  Figure 5 Link: articels_figures_by_rev_year\2020\Rolling_Shutter_Homography_and_its_Applications\figure_5.jpg
  Figure 5 caption: Errors of relative pose estimation with increasing image noise.
  Figure 6 Link: articels_figures_by_rev_year\2020\Rolling_Shutter_Homography_and_its_Applications\figure_6.jpg
  Figure 6 caption: Errors of relative pose estimation with increasing outlier rate.
  Figure 7 Link: articels_figures_by_rev_year\2020\Rolling_Shutter_Homography_and_its_Applications\figure_7.jpg
  Figure 7 caption: Errors of relative pose estimation with increasing rotational
    speed (a) and translational speed (b).
  Figure 8 Link: articels_figures_by_rev_year\2020\Rolling_Shutter_Homography_and_its_Applications\figure_8.jpg
  Figure 8 caption: Rate of inliers with increasing rotational speed and translational
    speed.
  Figure 9 Link: articels_figures_by_rev_year\2020\Rolling_Shutter_Homography_and_its_Applications\figure_9.jpg
  Figure 9 caption: Comparison of trajectory estimation (right sides) by using GSH
    and RSH on two RS image sequences (examples of input RS images are shown on the
    left side).
  First author gender probability: 0.94
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Yizhen Lao
  Name of the last author: Omar Ait-Aider
  Number of Figures: 16
  Number of Tables: 2
  Number of authors: 2
  Paper title: Rolling Shutter Homography and its Applications
  Publication Date: 2020-03-02 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Error of the Poses Estimated From Rolling Shutter Sequences
      Against to Ground Truth
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Errors of the Trajectory Estimation, 2D Transformation and
      3D Reconstruction
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2977644
- Affiliation of the first author: school of electrical engineering, tel aviv university,
    tel aviv, israel
  Affiliation of the last author: hatter department of marine technologies, charney
    school of marine sciences, university of haifa, haifa, israel
  Figure 1 Link: articels_figures_by_rev_year\2020\Underwater_Single_Image_Color_Restoration_Using_HazeLines_and_a_New_Quantitative\figure_1.jpg
  Figure 1 caption: 'The proposed color restoration and transmission estimation method:
    First, the veiling-light (the ambient light that is scattered into the line of
    sight) is estimated. The pixels whose color is the veiling light are shown in
    color. Then, the transmission estimation and color restoration are repeated for
    multiple water types that have different optical characteristics, as described
    in Section 3.2. Finally, the best result is selected automatically based on the
    gray-world assumption. Photo from [3].'
  Figure 10 Link: articels_figures_by_rev_year\2020\Underwater_Single_Image_Color_Restoration_Using_HazeLines_and_a_New_Quantitative\figure_10.jpg
  Figure 10 caption: "Average angular reproduction error (in degrees) of gray-scale\
    \ patches, measured for different color restoration techniques (shown in Figs.\
    \ 6 and 8). The color charts are numbered in increasing order, starting with the\
    \ closest to the camera, and are annotated on the images on the left column. The\
    \ right column shows the average angular reproduction error \u03C8 \xAF for each\
    \ chart and each method. Lower is better."
  Figure 2 Link: articels_figures_by_rev_year\2020\Underwater_Single_Image_Color_Restoration_Using_HazeLines_and_a_New_Quantitative\figure_2.jpg
  Figure 2 caption: "Left: Approximate attenuation coefficients ( \u03B2 ) of Jerlov\
    \ water types. Data taken from [1], based on measurements in [39]. Solid lines\
    \ mark open ocean water types while dashed lines mark coastal water types. Middle:\
    \ \u03B2 ratios of R,G,B channels for each water type, based the wavelengths:\
    \ 475 nm, 525 nm, and 600 nm, respectively. Right: simulation of the appearance\
    \ of a perfect white surface viewed at depth of 1-20 m in different water types\
    \ (reproduced from [40])."
  Figure 3 Link: articels_figures_by_rev_year\2020\Underwater_Single_Image_Color_Restoration_Using_HazeLines_and_a_New_Quantitative\figure_3.jpg
  Figure 3 caption: "Quantitative evaluations in the literature. (a) Diver holding\
    \ a board with six colored patches, from left to right: before diving, diving\
    \ at a depth of 5m , and diving at a depth of 15m . (b-c) Water tank experiments:\
    \ due to the tanks size ( 90cm\xD745cm\xD745cm ), the objects are 30 cm deep and\
    \ the distance between the objects and the camera is approximately 60cm . Deep\
    \ sea soil is added to the seawater to increase scattering, at a concentration\
    \ of 20mgliter (b) and 100mgliter (c). (d) Example of an underwater taken in a\
    \ swimming pool [23], where similar photos were taken with different cameras to\
    \ evaluate white balancing techniques."
  Figure 4 Link: articels_figures_by_rev_year\2020\Underwater_Single_Image_Color_Restoration_Using_HazeLines_and_a_New_Quantitative\figure_4.jpg
  Figure 4 caption: "An example scene from the dataset. [Left] The image from the\
    \ left camera. The sizes of two identical color chart were measured based on the\
    \ 3D reconstruction, and are marked on the enlarged areas on the left. The true\
    \ size of the chart is: 12cm\xD718.5cm . At a distance of 6m from the camera,\
    \ the reconstruction accuracy is about \u223C1.5cm , which is quite accurate considering\
    \ the precision of existing datasets [49]. [Center] The distances of the objects\
    \ from the left camera. [Right] The same scene with the color cards masked."
  Figure 5 Link: articels_figures_by_rev_year\2020\Underwater_Single_Image_Color_Restoration_Using_HazeLines_and_a_New_Quantitative\figure_5.jpg
  Figure 5 caption: Creating the dataset. [Top left] The stereo rig used for acquisition.
    [Bottom left] The color charts and calibration checkerboard. [Right] Dive sites
    location.
  Figure 6 Link: articels_figures_by_rev_year\2020\Underwater_Single_Image_Color_Restoration_Using_HazeLines_and_a_New_Quantitative\figure_6.jpg
  Figure 6 caption: 'Comparison of single-image underwater enhancement techniques.
    For Ancuti [24] the reference image used for all results is shown as an insert
    in R4376. In [24], [26] and ours the insert in R3008 provides a zoom-in on the
    furthest chart. Order of dive sites from left to right: Med 1, Red 2, Med 2, Red
    1.'
  Figure 7 Link: articels_figures_by_rev_year\2020\Underwater_Single_Image_Color_Restoration_Using_HazeLines_and_a_New_Quantitative\figure_7.jpg
  Figure 7 caption: "True distances and estimated transmission maps for images shown\
    \ in Fig. 6, shown only for methods that estimate a transmission map. The quality\
    \ of the transmission is measured by the Pearson correlation coefficient \u03C1\
    \ , which is calculated between the negative logarithm of the estimated transmission\
    \ and the true distance."
  Figure 8 Link: articels_figures_by_rev_year\2020\Underwater_Single_Image_Color_Restoration_Using_HazeLines_and_a_New_Quantitative\figure_8.jpg
  Figure 8 caption: 'Comparison of single-image underwater enhancement techniques.
    Order of dive sites from left to right: Med 2, Red 1, Med 2, Med 1.'
  Figure 9 Link: articels_figures_by_rev_year\2020\Underwater_Single_Image_Color_Restoration_Using_HazeLines_and_a_New_Quantitative\figure_9.jpg
  Figure 9 caption: "True distances and estimated transmission maps for images shown\
    \ in Fig. 8, shown only for methods that estimate a transmission map. The quality\
    \ of the transmission is measured by the Pearson correlation coefficient \u03C1\
    \ , which is calculated between the negative logarithm of the estimated transmission\
    \ and the true distance."
  First author gender probability: 0.94
  Gender of the first author: female
  Gender of the last author: female
  Last author gender probability: 0.66
  Name of the first author: Dana Berman
  Name of the last author: Tali Treibitz
  Number of Figures: 14
  Number of Tables: 2
  Number of authors: 4
  Paper title: Underwater Single Image Color Restoration Using Haze-Lines and a New
    Quantitative Dataset
  Publication Date: 2020-03-02 00:00:00
  Table 1 caption:
    table_text: "TABLE 1 Each Column Shows the Pearson Correlation Coefficient \u03C1\
      \ \u03C1 Between the Estimated Transmission Maps and the Ground Truth Distance\
      \ for a Different Image, as Shown in Figs. 7, 9, and 12"
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: "TABLE 2 The Average Reproduction Angular Error \u03C8 \xAF \u03C8\
      \xAF in RGB Space Between the Gray-Scale Patches and a Pure Gray Color, For\
      \ All Each Chart in Each Image, and All Methods, Including the Input and Global\
      \ Contrast Stretch (labeled Cont.)"
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2977624
- Affiliation of the first author: department of electronic engineering, national
    taipei university of technology, taipei, taiwan
  Affiliation of the last author: department of electrical engineering, national taiwan
    university, taipei, taiwan
  Figure 1 Link: articels_figures_by_rev_year\2020\DSNet_Joint_Semantic_Learning_for_Object_Detection_in_Inclement_Weather_Conditio\figure_1.jpg
  Figure 1 caption: 'Architecture of the proposed DSNet. The framework consists of
    two subnetworks: the Restoration subnetwork and the Detection subnetwork. The
    Feature Recovery module is only activated during the training phase, and the Restored
    image is identical to the Foggy input image in size.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\DSNet_Joint_Semantic_Learning_for_Object_Detection_in_Inclement_Weather_Conditio\figure_2.jpg
  Figure 2 caption: Flowchart of the Restoration subnetwork. Notably, because the
    framework only focuses on the object detection task, the Feature Recovery Module
    is only activated during training to produce restored images for the proposed
    multi-task learning procedure.
  Figure 3 Link: articels_figures_by_rev_year\2020\DSNet_Joint_Semantic_Learning_for_Object_Detection_in_Inclement_Weather_Conditio\figure_3.jpg
  Figure 3 caption: Example images and annotations from the FOD dataset. The purple
    and yellow boxes denote the ground-truth bounding boxes of the car and person
    categories, respectively.
  Figure 4 Link: articels_figures_by_rev_year\2020\DSNet_Joint_Semantic_Learning_for_Object_Detection_in_Inclement_Weather_Conditio\figure_4.jpg
  Figure 4 caption: Effects of the Restoration subnetwork in the proposed model on
    the FOD test set. DSNet is our model which contains both the Detection and Restoration
    subnetworks. DSNet-1 denotes the proposed model without using the Restoration
    subnetwork.
  Figure 5 Link: articels_figures_by_rev_year\2020\DSNet_Joint_Semantic_Learning_for_Object_Detection_in_Inclement_Weather_Conditio\figure_5.jpg
  Figure 5 caption: Detection results of our DSNet and different object detectors
    on synthetic foggy images (left column), natural foggy image (right column). The
    red and cyan boxes demonstrate the predicted bounding boxes of the car and person,
    respectively. Bounding boxes with a score greater than or equal to 0.5 will be
    drawn.
  Figure 6 Link: articels_figures_by_rev_year\2020\DSNet_Joint_Semantic_Learning_for_Object_Detection_in_Inclement_Weather_Conditio\figure_6.jpg
  Figure 6 caption: Detection results of our DSNet and different combination models
    on synthetic foggy images (left column), natural foggy image (right column). The
    red and cyan boxes demonstrate the predicted bounding boxes of the car and person,
    respectively. Bounding boxes with a score greater than or equal to 0.5 will be
    drawn.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.61
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Shih-Chia Huang
  Name of the last author: Da-Wei Jaw
  Number of Figures: 6
  Number of Tables: 4
  Number of authors: 3
  Paper title: 'DSNet: Joint Semantic Learning for Object Detection in Inclement Weather
    Conditions'
  Publication Date: 2020-03-03 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Statistic of the FOD Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Effects of the CB Module and Upsampling Submodule in the DSNet
      Model on the FOD Test Set
  Table 3 caption:
    table_text: TABLE 3 The mAP of the Proposed DSNet and Four State-of-the-Art Object
      Detection Methods on the FOD Test Set and the Foggy Driving Dataset
  Table 4 caption:
    table_text: TABLE 4 The mAP and Speed of the Proposed DSNet, RetinaNet, and Three
      Concatenation Models on the FOD Test Set and Foggy Driving Dataset
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2977911
- Affiliation of the first author: university of cambridge, cambridge, united kingdom
  Affiliation of the last author: inception institute of artificial intelligence,
    abu dhabi, uae
  Figure 1 Link: articels_figures_by_rev_year\2020\Deeply_Supervised_Discriminative_Learning_for_Adversarial_Defense\figure_1.jpg
  Figure 1 caption: 2-D penultimate layer activations of a clean image and its adversarial
    counterpart (PGD attack) for standard softmax trained model and our method on
    MNIST (top row) and CIFAR-10 (bottom row) datasets. Note that our method correctly
    maps the attacked image to its true-class feature space.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Deeply_Supervised_Discriminative_Learning_for_Adversarial_Defense\figure_2.jpg
  Figure 2 caption: "Comparison between different training methods. The red circle\
    \ encompasses the adversarial sample space within a perturbation budget \u2225\
    \u03B4 \u2225 p <\u03F5 ."
  Figure 3 Link: articels_figures_by_rev_year\2020\Deeply_Supervised_Discriminative_Learning_for_Adversarial_Defense\figure_3.jpg
  Figure 3 caption: "An illustration of our training with deep supervision of distance-based\
    \ loss functions L Dist . G \u03D5 (\u22C5) is an auxiliary branch to map features\
    \ to a low-dimensional output, which is then used as the loss in Eq. (9)."
  Figure 4 Link: articels_figures_by_rev_year\2020\Deeply_Supervised_Discriminative_Learning_for_Adversarial_Defense\figure_4.jpg
  Figure 4 caption: t-SNE visualization of 10 classes for the last layer feature embeddings
    from model trained using Prototype Conformity Loss (left) and Triplet Loss (right)
    for CIFAR-100 dataset. The class-wise feature space embeddings for our loss are
    better separated than for other distance-based metrics for hard datasets like
    CIFAR-100.
  Figure 5 Link: articels_figures_by_rev_year\2020\Deeply_Supervised_Discriminative_Learning_for_Adversarial_Defense\figure_5.jpg
  Figure 5 caption: t-SNE plots of features at different layers, where mathcal LtextPC
    is applied, on MNIST (top), CIFAR-10 (middle) and SVHN (bottom) datasets. The
    inter-class distances and intra-class compactness (denoted by red arrows and brown
    circles, respectively) increase along the depth of the network. Best viewed in
    color.
  Figure 6 Link: articels_figures_by_rev_year\2020\Deeply_Supervised_Discriminative_Learning_for_Adversarial_Defense\figure_6.jpg
  Figure 6 caption: Class-wise distribution of penultimate layer features (2 neurons
    only) on CIFAR-10 dataset. As the training progresses, the class prototypes move
    farther away from each other, resulting in a guaranteed robustness against adversarial
    attacks. Best viewed in color.
  Figure 7 Link: articels_figures_by_rev_year\2020\Deeply_Supervised_Discriminative_Learning_for_Adversarial_Defense\figure_7.jpg
  Figure 7 caption: Robustness of our model (without adversarial training) against
    white-box attacks for various perturbation budgets.
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.99
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Aamir Mustafa
  Name of the last author: Ling Shao
  Number of Figures: 7
  Number of Tables: 13
  Number of authors: 6
  Paper title: Deeply Supervised Discriminative Learning for Adversarial Defense
  Publication Date: 2020-03-05 00:00:00
  Table 1 caption:
    table_text: 'TABLE 1 Two Network Architectures: CNN-6 (MNIST, FMNIST) and ResNet-110
      (CIFAR-10,100 and SVHN)'
  Table 10 caption:
    table_text: TABLE 10 ResNet-110 Network Architecture Used for Deep Softmax Supervision
      ( L CE ) (LCE) and Deep Center Loss ( L CL ) (LCL), Contrastive Loss ( L Co
      ) (LCo) or Triplet Loss ( L T ) (LT) Supervision for CIFAR-10 Dataset
  Table 2 caption:
    table_text: "TABLE 2 Adversarial Settings of Our Experiments. \u03B1 \u03B1, \u03BC\
      \ \u03BC, i i, Respectively Denote the Step-Size, the Decay-Factor and the Number\
      \ of Attack Steps for a Perturbation Budge \u03F5 \u03B5"
  Table 3 caption:
    table_text: TABLE 3 Robustness of Our Model in White-Box and Black-Box Settings
  Table 4 caption:
    table_text: TABLE 4 Robustness in Adaptive White-Box Attack Settings
  Table 5 caption:
    table_text: TABLE 5 Comparison on CIFAR-10 Dataset for White-Box Adversarial Attacks
      (Numbers Shows Robustness, Higher is Better)
  Table 6 caption:
    table_text: TABLE 6 Comparison on MNIST Dataset for White-Box Adversarial Attacks
      (Numbers Shows Robustness, Higher is Better)
  Table 7 caption:
    table_text: TABLE 7 Comparison on CIFAR-100 Dataset for White-Box Adversarial
      Attacks (Numbers Shows Robustness, Higher is Better)
  Table 8 caption:
    table_text: TABLE 8 Comparison of Our Approach With [46] on 4 Datasets
  Table 9 caption:
    table_text: TABLE 9 Comparison of Adversarial Robustness for Various Loss Functions
      (Center Loss, Contrastive Loss, and Triplet Loss) on the CIFAR-10 & 100 Datasets
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2978474
- Affiliation of the first author: "centre for applied autonomous sensor systems (aass),\
    \ \xF6rebro university, \xF6rebro, sweden"
  Affiliation of the last author: "centre for applied autonomous sensor systems (aass),\
    \ \xF6rebro university, \xF6rebro, sweden"
  Figure 1 Link: articels_figures_by_rev_year\2020\Point_Set_Registration_for_D_Range_Scans_Using_Fuzzy_ClusterBased_Metric_and_Eff\figure_1.jpg
  Figure 1 caption: "The meanscenters of NDT, GMM and fuzzy clusters (denoted by \u201C\
    x\u201Ds, \u201Co\u201Ds and \u201C \u2217 \u201Ds, respectively) for the full\
    \ bunny model with and without noisy outliers. This model has 35947 points, and\
    \ the number of components of each representation is 64. The number of noisy outliers\
    \ is 20 percent that of the original points, and they are uniformly distributed\
    \ in a cuboid centered by the original model and with length, width and height\
    \ 1.5 times those of the original bounding box."
  Figure 10 Link: articels_figures_by_rev_year\2020\Point_Set_Registration_for_D_Range_Scans_Using_Fuzzy_ClusterBased_Metric_and_Eff\figure_10.jpg
  Figure 10 caption: The registration errors of our method for the partially overlapping
    bunny and chicken scans in Fig. 9 with different trimming ratios.
  Figure 2 Link: articels_figures_by_rev_year\2020\Point_Set_Registration_for_D_Range_Scans_Using_Fuzzy_ClusterBased_Metric_and_Eff\figure_2.jpg
  Figure 2 caption: "Fuzzy clustering and noisy outlier pruning results for the partial\
    \ bunny scan \u201Cbun090\u201D with different degrees of noise. The first to\
    \ third rows show the cases with the number of noisy outliers to be 20, 30 and\
    \ 50 percent, respectively, of that of the real points (the number of real points\
    \ is 30379). The noisy outliers are distributed in the same way as in Fig. 1.\
    \ For each case, N C =60 , and each \u201C \u2217 \u201D denotes a fuzzy cluster\
    \ center. (a.1), (b.1) and (c.1) show the clustering results using fuzzy c-means\
    \ [40], and (c.2) shows the result using robust fuzzy c-means [41]. The rest shows\
    \ the results of the pruning method introduced in Section 4.2, where (a.2), (b.2)\
    \ and (c.3) are the pruning results of the first step and (a.3), (b.3) and (c.4)\
    \ are the pruning results of the second step by removing 15 percent of the points\
    \ with the largest distance losses. \u201CAFPCD\u201D is an index related to the\
    \ registration quality assessment defined in Section 4.3."
  Figure 3 Link: articels_figures_by_rev_year\2020\Point_Set_Registration_for_D_Range_Scans_Using_Fuzzy_ClusterBased_Metric_and_Eff\figure_3.jpg
  Figure 3 caption: "Some examples of point set registration. The fixed sets and the\
    \ moving sets are in yellow and blue, respectively. In the rightmost column, the\
    \ yellow \u201C \u2217 \u201Ds and the blue \u201C+\u201Ds are their fuzzy cluster\
    \ centers."
  Figure 4 Link: articels_figures_by_rev_year\2020\Point_Set_Registration_for_D_Range_Scans_Using_Fuzzy_ClusterBased_Metric_and_Eff\figure_4.jpg
  Figure 4 caption: "Examples of AFPCD using bunny point sets. Each \u2217 denotes\
    \ a fuzzy cluster center. The first to the third rows are \u201Cbunzipper\u201D\
    \ (full model), \u201Cbun090\u201D, and \u201Cearback\u201D, respectively."
  Figure 5 Link: articels_figures_by_rev_year\2020\Point_Set_Registration_for_D_Range_Scans_Using_Fuzzy_ClusterBased_Metric_and_Eff\figure_5.jpg
  Figure 5 caption: "The AFPCDs of three full bunny models with different resolutions.\
    \ Each \u201C \u2217 \u201D denotes a fuzzy cluster center. (a). \u201Cbunzipperres2\u201D\
    ; (b). \u201Cbunzipperres3\u201D; (c). \u201Cbunzipperres4\u201D."
  Figure 6 Link: articels_figures_by_rev_year\2020\Point_Set_Registration_for_D_Range_Scans_Using_Fuzzy_ClusterBased_Metric_and_Eff\figure_6.jpg
  Figure 6 caption: "Initial SE(3) space for BnB, which is same as the domain parameterization\
    \ of Go-ICP [29]. Left: the initial rotation cube C r , [\u2212\u03C0,\u03C0 ]\
    \ 3 . Right: the initial translation cube C t , [\u2212L,L ] 3 , which is supposed\
    \ to include the optimal solution of translation. The small pink boxes give the\
    \ examples of octant subcubes for rotation and translation."
  Figure 7 Link: articels_figures_by_rev_year\2020\Point_Set_Registration_for_D_Range_Scans_Using_Fuzzy_ClusterBased_Metric_and_Eff\figure_7.jpg
  Figure 7 caption: Local convergence tests for LM-ICP [2], NDT [15], JRMPC [1] and
    our method. The first row shows the two pairs of partially overlapping scans used
    in the tests, where an example of their initial poses is given in each left part.
    The second row shows the errors epsilon boldsymbollambda . The third row shows
    the values of rho boldsymbollambda to validate the proposed registration quality
    assessment. The last row presents the time costs.
  Figure 8 Link: articels_figures_by_rev_year\2020\Point_Set_Registration_for_D_Range_Scans_Using_Fuzzy_ClusterBased_Metric_and_Eff\figure_8.jpg
  Figure 8 caption: "Registration results of NDT [15], GMM [12] and our method for\
    \ two full bunny models with noisy outliers. The fixed set (red) is \u201Cbunzipper\u201D\
    , and the moving set (blue) is \u201Cbunzipperres2\u201D. The noisy outliers are\
    \ same as those in Fig. 1. The angle difference of the initial poses is 20circ\
    \ . The time costs of the optimization of NDT, GMM and our method are 0.9098 s,\
    \ 1.1913 s, and 0.3730 s, respectively."
  Figure 9 Link: articels_figures_by_rev_year\2020\Point_Set_Registration_for_D_Range_Scans_Using_Fuzzy_ClusterBased_Metric_and_Eff\figure_9.jpg
  Figure 9 caption: Registration results of sparse ICP [9] and our method for noisy
    point sets. The first row shows a partial-to-full registration, and the remaining
    rows show two partially overlapping registrations. For each point set, the number
    of noisy outliers is 20 percent that of the real points, and the noisy outliers
    are distributed in the same way as that in Fig. 1. For the noisy horse scan pair,
    NPF = 58182 and NPM = 30521 . For the noisy bunny scan pair, NPF = 48308 and NPM
    = 48117 . For the noisy chicken scan pair, NPF = 35422 and NPM = 36198 .
  First author gender probability: 0.55
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Qianfang Liao
  Name of the last author: Henrik Andreasson
  Number of Figures: 18
  Number of Tables: 3
  Number of authors: 3
  Paper title: Point Set Registration for 3D Range Scans Using Fuzzy Cluster-Based
    Metric and Efficient Global Optimization
  Publication Date: 2020-03-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Registration Results of the Optimality Tests
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Time Costs of the Global Optimization in the Ablation Study
      in Fig. 11
  Table 3 caption:
    table_text: "TABLE 3 The MeanLargest Reductions in \u03F5 \u03BB \u03B5\u03BB\
      s and Time Costs Using Our Method Compared to Results Using Go-ICP in Fig. 14"
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2978477
- Affiliation of the first author: national engineering laboratory for visual information
    processing and application, xian jiaotong university, xian, shaanxi, china
  Affiliation of the last author: "\xE9cole polytechnique f\xE9d\xE9rale de lausanne,\
    \ computer vision laboratory, lausanne, switzerland"
  Figure 1 Link: articels_figures_by_rev_year\2020\EigendecompositionFree_Training_of_Deep_Networks_for_Linear_LeastSquare_Problems\figure_1.jpg
  Figure 1 caption: Eigenvector switching. (a) 3D points lying on a plane in black
    and distant outlier in red. (b) When the weights assigned to all the points are
    one, the eigenvector corresponding to the smallest eigenvalue is e sub , the vector
    shown in blue in (a), and on the right in the top portion of (b), where we sort
    the eigenvectors by decreasing eigenvalue. As the optimization progresses and
    the weight assigned to the outlier decreases, the eigenvector corresponding to
    the smallest eigenvalue switches to e noise , the vector shown in green in (a),
    which introduces a sharp change in the gradient values.
  Figure 10 Link: articels_figures_by_rev_year\2020\EigendecompositionFree_Training_of_Deep_Networks_for_Linear_LeastSquare_Problems\figure_10.jpg
  Figure 10 caption: Quantitative PnP results for denoising. (a) Rotation and (b)
    translation errors for our method and several baselines with varying amount of
    additive noise on the 2D coordinates. Our method gives the best results in terms
    of rotation, and performs similarly to the best performing method, OPnP, for translation.
    Note that for translation, most methods, including ours, give extremely low error.
  Figure 2 Link: articels_figures_by_rev_year\2020\EigendecompositionFree_Training_of_Deep_Networks_for_Linear_LeastSquare_Problems\figure_2.jpg
  Figure 2 caption: 'Qualitative PnP results for outlier removal. Left two columns:
    Pairs of images between which we transfer the 2D points through the 2D-3D correspondences
    for Top: Florence and Bottom: Prague. From the third column to the fifth column:
    For each pair, we show in gray the reprojection of the 3D point cloud after applying
    the rotation and translation predicted by our model, OPnP, and EPnP+RANSAC, respectively.
    The red dots correspond to the ground-truth locations. Note that our models predictions
    cover the ground truth much more closely than the compared methods.'
  Figure 3 Link: articels_figures_by_rev_year\2020\EigendecompositionFree_Training_of_Deep_Networks_for_Linear_LeastSquare_Problems\figure_3.jpg
  Figure 3 caption: "Qualitative comparison of our results with those of RANSAC. From\
    \ left to right, in each column, (a) our results and (b) RANSAC results on the\
    \ \u201Cfountain-P11\u201D of [37], (c) our results and (d) RANSAC results on\
    \ the \u201Cbrown-bm-3-05\u201D of SUN3D. First two rows: We display the correspondences\
    \ that each algorithm labeled as inliers. The true positives are shown in blue\
    \ and the false ones in red. The false positives of our approach are still close\
    \ to being correct, while those of RANSAC are truly wrong. Third row: With the\
    \ estimated essential matrices, we plot the epipolar lines and their corresponding\
    \ points for the false positives for each method. Note how, although the false\
    \ positives look senseless for RANSAC, they are indeed following an epipolar geometry\
    \ that captures both true and false positives. Fourth row: Same as in the third\
    \ row, but with the ground-truth essential matrix. Our false positives are still\
    \ close to the true epipolar line, whereas the ones from RANSAC are not."
  Figure 4 Link: articels_figures_by_rev_year\2020\EigendecompositionFree_Training_of_Deep_Networks_for_Linear_LeastSquare_Problems\figure_4.jpg
  Figure 4 caption: "Loss evolution for different training samples. This example was\
    \ obtained using synthetic data for the wide-baseline stereo task. As in the plane\
    \ fitting experiment, no network is involved and we directly optimize the matches\
    \ weights. We set the total number of matches to be 100 and initialize all the\
    \ weights to be one in the three data samples, represented as different colors:\
    \ one sample with 10 outliers (blue), one with 30 outliers (magenta), and one\
    \ with 50 outliers (cyan). We show, from top to bottom, our main loss term, our\
    \ regularizer and the corresponding trace value tr( A \xAF \u22A4 A \xAF ) . In\
    \ each case, we plot the evolution of the term as optimization progresses. As\
    \ the weights applied to the matches remove the outliers, L eig approaches zero\
    \ for all cases. Note, however, that, while tr( A \xAF \u22A4 A \xAF ) also decreases,\
    \ it remains far from zero, and more importantly, reaches final values that differ\
    \ significantly for the different samples. By contrast, our regularizer L aux\
    \ has much lower values that are of similar magnitudes for all samples, as indicated\
    \ by the fact that the three curves overlap. This facilitates setting the weights\
    \ of this regularizer."
  Figure 5 Link: articels_figures_by_rev_year\2020\EigendecompositionFree_Training_of_Deep_Networks_for_Linear_LeastSquare_Problems\figure_5.jpg
  Figure 5 caption: Plane fitting in the presence of a single outlier. We report the
    results for Single Value Decomposition(SVD), self-adjoint Eigendecomposition(Eigh),
    and for our loss function in Eq. (16). (a) Results using vanilla gradient-descent.
    To visualize all results in a single graph, we plot the results in log scale.
    (b) Results using Adam [38]. Regardless of the choice of optimizers, SVD and Eigh
    slowly decrease in the beginning, followed by a sudden drop, denoted with the
    red dot. By contrast, our method converges gradually and smoothly
  Figure 6 Link: articels_figures_by_rev_year\2020\EigendecompositionFree_Training_of_Deep_Networks_for_Linear_LeastSquare_Problems\figure_6.jpg
  Figure 6 caption: "Plane fitting in the presence of multiple outliers. We report\
    \ results for Singular Value Decomposition (SVD), self-adjoint Eigendecomposition\
    \ (Eigh), and our loss function. (a) Loss evolution for all methods. While our\
    \ loss converges smoothly, SVD and Eigh remain stagnant until the shear drop at\
    \ iteration 352. (b) Inlier selection of SVD. The inliers chosen by SVD are shown\
    \ in red, and the outliers in blue. Positions 1 to 100 are true inliers. With\
    \ multiple outliers, SVD discards many inliers, as well as estimating outliers\
    \ as inliers until iteration 352, denoted with the yellow vertical line, where\
    \ it no longer makes mistakes. By contrast, as shown in (c), our approach correctly\
    \ rejects the outliers and accepts all inliers, starting from iteration 10, again\
    \ marked with the yellow vertical line. Our method also does not show the sporadic\
    \ change in the selection of inliers visible in the case of SVD in (b). (d) Illustration\
    \ of switching. We draw the three eigenvectors of the ground-truth plane as the\
    \ three horizontal lines, and the euclidean distance to them from the first, second,\
    \ and third eigenvectors estimated by SVD as yellow, blue, and red lines, respectively.\
    \ Note the abrupt transition at iteration 352, due to the first eigenvector correctly\
    \ following the largest ground-truth eigenvector starting from this iteration,\
    \ whereas it was following the smallest until this iteration. This is clearly\
    \ harmful for optimization. (e) Magnitude of the gradient \u2202L \u2202X for\
    \ each iteration with SVD, in log scale. Note the drastic change in iteration\
    \ 352."
  Figure 7 Link: articels_figures_by_rev_year\2020\EigendecompositionFree_Training_of_Deep_Networks_for_Linear_LeastSquare_Problems\figure_7.jpg
  Figure 7 caption: Qualitative ellipse fitting results. (a) Fitting results for various
    methods in the presence of outliers and minor noise. (b) Results in the presence
    of noise only. Our method gives the most accurate results. Note that in (a), our
    method is the only method that fits the true ellipse in the center well, due to
    the abundance of outliers in this setup.
  Figure 8 Link: articels_figures_by_rev_year\2020\EigendecompositionFree_Training_of_Deep_Networks_for_Linear_LeastSquare_Problems\figure_8.jpg
  Figure 8 caption: Quantitative ellipse fitting results. We report the center point
    error as in [20]. (a) Outlier removal results. (b) Denoising results. (c) Simultaneous
    outlier removal and denoising results with moderate noise of 2 times 10-2 . (d)
    Simultaneous outlier removal and denoising results with 10 outliers. Our method
    performs best in all cases.
  Figure 9 Link: articels_figures_by_rev_year\2020\EigendecompositionFree_Training_of_Deep_Networks_for_Linear_LeastSquare_Problems\figure_9.jpg
  Figure 9 caption: Quantitative PnP results for outlier removal. (a) Rotation and
    (b) translation errors for our method and several baselines. Our method gives
    extremely stable results despite the abundance of outliers, whereas all compared
    methods perform significantly worse as the number of outliers increases. Even
    when these methods perform well on either rotation or translation, they do not
    perform well on both. By contrast, Ours yields near zero errors for both measures
    up to 130 outliers (i.e., 65 percent).
  First author gender probability: 0.98
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: Zheng Dang
  Name of the last author: Mathieu Salzmann
  Number of Figures: 13
  Number of Tables: 0
  Number of authors: 6
  Paper title: Eigendecomposition-Free Training of Deep Networks for Linear Least-Square
    Problems
  Publication Date: 2020-03-06 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2978812
- Affiliation of the first author: department of computer science and engineering,
    university at buffalo, buffalo, ny, usa
  Affiliation of the last author: department of computer science and engineering,
    university at buffalo, buffalo, ny, usa
  Figure 1 Link: articels_figures_by_rev_year\2020\SibNet_Sibling_Convolutional_Encoder_for_Video_Captioning\figure_1.jpg
  Figure 1 caption: 'Overview of the proposed SibNet, which employs a dual-branch
    architecture to collaboratively encode videos. The proposed loss function contains
    three components: content loss L c , semantic loss L s , and decoder loss L d
    . We leverage autoencoder and visual-semantic joint embedding to impose fine-grained
    regularization that pushes content branch to capture visual contents and pushes
    semantic branch to encode video semantics.'
  Figure 10 Link: articels_figures_by_rev_year\2020\SibNet_Sibling_Convolutional_Encoder_for_Video_Captioning\figure_10.jpg
  Figure 10 caption: "Qualitative comparison between variants of our model, i.e.,\
    \ SibNet. \u201CSingle\u201D,\u201CDL\u201D and \u201COurs\u201D denote captions\
    \ generated by variants named Single (DL-3), Ours (DL), and Ours (Full) which\
    \ are introduced in Section 4.3.2, respectively. \u201CGT\u201D denotes ground\
    \ truth captions. We highlight both incorrect (shown in blue) and correct (shown\
    \ in red) words or phrases in the generated captions."
  Figure 2 Link: articels_figures_by_rev_year\2020\SibNet_Sibling_Convolutional_Encoder_for_Video_Captioning\figure_2.jpg
  Figure 2 caption: Illustration of the proposed Sibling Convolutional Encoder (SibNet),
    which is composed of the content branch and the semantic branch, denoted as CNN
    c and CNN s , respectively. We construct both branches by stacking three and six
    identical temporal convolutional blocks (TCBs) (we will introduce TCB in Section
    3.1.3). Soft-attention mechanism is utilized in our RNN decoder.
  Figure 3 Link: articels_figures_by_rev_year\2020\SibNet_Sibling_Convolutional_Encoder_for_Video_Captioning\figure_3.jpg
  Figure 3 caption: Illustration of the content branch CNN c implemented via an autoencoder.
    Note that the content loss of the autoencoder is one component of our final training
    loss.
  Figure 4 Link: articels_figures_by_rev_year\2020\SibNet_Sibling_Convolutional_Encoder_for_Video_Captioning\figure_4.jpg
  Figure 4 caption: Illustration of the semantic branch CNN s implemented via visual-semantic
    joint embedding. Note that the semantic loss of visual-semantic embedding is one
    component of our final training loss.
  Figure 5 Link: articels_figures_by_rev_year\2020\SibNet_Sibling_Convolutional_Encoder_for_Video_Captioning\figure_5.jpg
  Figure 5 caption: Illustration of our temporal convolutional block (TCB), which
    is the basic component of both the content branch and the semantic branch.
  Figure 6 Link: articels_figures_by_rev_year\2020\SibNet_Sibling_Convolutional_Encoder_for_Video_Captioning\figure_6.jpg
  Figure 6 caption: "Variants of SibNet, which we compare in Table 5. Here, CNN c\
    \ , CNN s , and RNN d represent the content branch, the semantic branch, and the\
    \ decoder, respectively. \u201CAE\u201D and \u201CJE\u201D denote the remaining\
    \ modules in the autoencoder and the visual-semantic joint embedding model. Lc\
    \ , Ls , and Ld represent the content loss, the semantic loss, and the decoder\
    \ loss, which are defined in Equations (4), (1), and (6)."
  Figure 7 Link: articels_figures_by_rev_year\2020\SibNet_Sibling_Convolutional_Encoder_for_Video_Captioning\figure_7.jpg
  Figure 7 caption: 'Evaluation of the impact of both branches depths on the performance
    of our model. First row: impact of the TCB block number in content branch, where
    the TCB number in semantic branch is fixed to 6. Second row: impact of the TCB
    block number in semantic branch, where the TCB number in content branch is fixed
    to 3.'
  Figure 8 Link: articels_figures_by_rev_year\2020\SibNet_Sibling_Convolutional_Encoder_for_Video_Captioning\figure_8.jpg
  Figure 8 caption: A t-SNE visualization of video embeddings generated by our visual-semantic
    joint embedding model. Videos belonging to the same category have the same kind
    of maker. The category label information is provided by MSR-VTT dataset.
  Figure 9 Link: articels_figures_by_rev_year\2020\SibNet_Sibling_Convolutional_Encoder_for_Video_Captioning\figure_9.jpg
  Figure 9 caption: "Qualitative comparison of our model, i.e., SibNet, and previous\
    \ state-of-the-art methods, including S2VT [4] and Temporal Attention [48]. \u201C\
    S2VT\u201D,\u201CTA\u201D and \u201COurs\u201D denote captions generated by S2VT\
    \ [4], Temporal Attention [48] and our model, respectively. \u201CGT\u201D denotes\
    \ ground truth captions. We highlight both incorrect (shown in blue) and correct\
    \ (shown in red) words or phrases in the generated captions."
  First author gender probability: 0.76
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.83
  Name of the first author: Sheng Liu
  Name of the last author: Junsong Yuan
  Number of Figures: 10
  Number of Tables: 6
  Number of authors: 3
  Paper title: 'SibNet: Sibling Convolutional Encoder for Video Captioning'
  Publication Date: 2020-03-06 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Performance Comparisons on YouTube2Text (MSVD) Dataset
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: 'TABLE 2 Performance Comparisons on the Test Set of MSR-VTT: Comparisons
      with State-of-the-Art Methods and Methods that Rank Top-4 on the Leaderboard
      of MSR-VTT Challenge'
  Table 3 caption:
    table_text: TABLE 3 A Comparison of the Performance of TCB-Based Network and its
      RNN-Based Counterparts, Including Networks Based on GRU, Bidirectional GRU (BiGRU),
      LSTM, and Bidirectional LSTM (BiLSTM)
  Table 4 caption:
    table_text: TABLE 4 Performance of Different Variants of the Proposed SibNet,
      whose Content Branch and Semantic Branch are Formed by a Stack of Different
      Variants of TCB, on YouTube2Text Dataset
  Table 5 caption:
    table_text: TABLE 5 Performance of Different Variants of the Proposed SibNet on
      YouTube2Text and MSR-VTT Datasets
  Table 6 caption:
    table_text: TABLE 6 The Number of Parameters of SibNet and Previous State-of-the-Art
      Methods
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2019.2940007
- Affiliation of the first author: department of computer science, university of bonn,
    bonn, germany
  Affiliation of the last author: department of medical biometry, informatics and
    epidemiology, medical faculty, university of bonn, bonn, germany
  Figure 1 Link: articels_figures_by_rev_year\2020\Bias_in_CrossEntropyBased_Training_of_Deep_Survival_Networks\figure_1.jpg
  Figure 1 caption: "Comparison of cross-entropy-based and log-likelihood-based optimization.\
    \ The hazard estimates presented in the plot were obtained from a simulated data\
    \ set with 100,000 instances. The simulation design was based on a data-generating\
    \ model with q=11 time intervals and a constant hazard rate of h(t|X)=0.4 that\
    \ was estimated by numerical optimization using the Nelder-Mead method. The values\
    \ of the censoring indicators c i , i=1,\u2026,100,000 , were generated according\
    \ to the equation c i =I( T i > C i ) , where T i and C i , i=1,\u2026,100,000\
    \ , were the values of the true event times and the censoring times, respectively.\
    \ The latter were generated from the probability model P( C i =t)= b q+1\u2212\
    t \u2211 q s=1 b q+1\u2212s , where b>0 was a tuning parameter that was adjusted\
    \ such that the censoring rate took values between 5 and 95 percent. The program\
    \ code to reproduce the results of the simulation study is available online at\
    \ https:imbie.meb.uni-bonn.de schmidDeepSurvivalAnalysis.zip."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2020\Bias_in_CrossEntropyBased_Training_of_Deep_Survival_Networks\figure_2.jpg
  Figure 2 caption: "Calibration plots obtained from the data sets in Table 1. The\
    \ plots show the Kaplan-Meier curves (red lines) and the average predicted survival\
    \ probabilities, as obtained from training the DRSA and DeepHit networks with\
    \ either the cross-entropy loss (black lines) or the negative log-likelihood loss\
    \ (blue lines). The dashed lines refer to unregularized predictions ( \u03B1=0\
    \ ) whereas the solid black and blue lines refer to regularized predictions that\
    \ were obtained by adding the term L z to the optimization problems in (13) and\
    \ (15). The weighting parameter \u03B1 was optimized on the validation data."
  Figure 3 Link: articels_figures_by_rev_year\2020\Bias_in_CrossEntropyBased_Training_of_Deep_Survival_Networks\figure_3.jpg
  Figure 3 caption: "Calibration plots obtained from the modified SEER breast cancer\
    \ data with age- and tumor-size-dependent censoring. The plots show the Kaplan-Meier\
    \ curves (red lines) and the average predicted survival probabilities, as obtained\
    \ from training the DRSA and DeepHit networks with either the cross-entropy loss\
    \ (black lines) or the negative log-likelihood loss (blue lines). The dashed lines\
    \ refer to unregularized predictions ( \u03B1=0 ) whereas the solid lines refer\
    \ to regularized predictions that were obtained by adding the term L z to the\
    \ optimization problems in (13) and (15)."
  Figure 4 Link: articels_figures_by_rev_year\2020\Bias_in_CrossEntropyBased_Training_of_Deep_Survival_Networks\figure_4.jpg
  Figure 4 caption: Brier Score values obtained from the DRSA analysis of the data
    sets in Table 1. The black and blue lines refer to the average prediction error
    curves obtained from cross-entropy-based and likelihood-based network training,
    respectively. The red lines refer to the average prediction error curves obtained
    from the Kaplan-Meier reference model. The plots in the left column refer to the
    unregularized predictions whereas the plots in the right column refer to the regularized
    predictions that incorporated the term L z in the optimization problems (13) and
    (15). Note that the Brier Score values should not be compared across rows, as
    the underlying training and test data sets were different in size.
  Figure 5 Link: articels_figures_by_rev_year\2020\Bias_in_CrossEntropyBased_Training_of_Deep_Survival_Networks\figure_5.jpg
  Figure 5 caption: Brier Score values obtained from the DeepHit analysis of the data
    sets in Table 1. The black and blue lines refer to the average prediction error
    curves obtained from cross-entropy-based and likelihood-based network training,
    respectively. The red lines refer to the average prediction error curves obtained
    from the Kaplan-Meier reference model. The plots in the left column refer to the
    unregularized predictions whereas the plots in the right column refer to the regularized
    predictions that incorporated the term Lz in the optimization problems (13) and
    (15). Note that the Brier Score values should not be compared across rows, as
    the underlying training and test data sets were different in size.
  Figure 6 Link: articels_figures_by_rev_year\2020\Bias_in_CrossEntropyBased_Training_of_Deep_Survival_Networks\figure_6.jpg
  Figure 6 caption: Partial dependence plots for age at diagnosis, as obtained from
    training the DeepHit architecture on the SEER breast cancer data (33 percent censoring,
    alpha > 0 ). The point estimates represent the average predicted five-year survival
    rates, as obtained from modified test data sets that kept age at diagnosis at
    a constant value while leaving the other data unchanged. The smooth curves were
    obtained from P-spline estimates using the default implementation in the R package
    mgcv. Note the different ranges of the y-axes of the two panels.
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Shekoufeh Gorgi Zadeh
  Name of the last author: Matthias Schmid
  Number of Figures: 6
  Number of Tables: 3
  Number of authors: 2
  Paper title: Bias in Cross-Entropy-Based Training of Deep Survival Networks
  Publication Date: 2020-03-09 00:00:00
  Table 1 caption:
    table_text: TABLE 1 Experimental Design for the Analysis of the SEER Breast Cancer
      Data and the CRASH-2 Data
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: TABLE 2 Greenwood-Nam-DAgostino (GND) Test Statistics Obtained From
      Cross-Entropy-Based and Likelihood-Based Training of the DRSA and DeepHit Networks
  Table 3 caption:
    table_text: TABLE 3 Estimated C C-Indices Obtained From Cross-Entropy-Based and
      Likelihood-Based Training of the DRSA and DeepHit Networks
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2979450
- Affiliation of the first author: department of computer science and technology,
    bnrist, moe-key laboratory of pervasive computing, tsinghua university, beijing,
    china
  Affiliation of the last author: department of computer science and technology, bnrist,
    moe-key laboratory of pervasive computing, tsinghua university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2020\FeatureAware_Uniform_Tessellations_on_Video_Manifold_for_ContentSensitive_Superv\figure_1.jpg
  Figure 1 caption: "Superpixels (induced by clipping supervoxels on frames 21, 41\
    \ and 61) obtained by GB [9], GBH [13], SWA [7], [32], [33], MeanShift [28], TSP\
    \ [5], TS-PPM [16], CSS [43] and our FCSS. All methods generate approximately\
    \ 1,500 supervoxels. TSP, TS-PPM, CSS and FCSS produce regular supervoxels (and\
    \ accordingly regular clipped superpixels), while other methods produce highly\
    \ irregular supervoxels. As shown in Section 6, these four methods are insensitive\
    \ to supervoxel relabeling and achieve a good balance among commonly used quality\
    \ metrics pertaining to supervoxels, including UE3D, SA3D, BRD and EV, while FCSS\
    \ runs 5\xD7 to 10\xD7 faster than TSP, and the peak memory required by FCSS is\
    \ 22\xD7 smaller than TSP and 7\xD7 to 15\xD7 smaller than TS-PPM. Both FCSS and\
    \ CSS generate more supervoxels in content-rich areas (e.g., bushes on the lake\
    \ shore) and fewer supervoxels in content-sparse areas (e.g., lake surface), while\
    \ FCSS better captures low-level video features (e.g., local objectregionmotion\
    \ boundaries) than CSS, leading to better performance in two video applications\
    \ (Section 7). See Appendix, which can be found on the Computer Society Digital\
    \ Library at http:doi.ieeecomputersociety.org10.1109TPAMI.2020.2979714, for more\
    \ visual comparison and accompanying demo video for more dynamic details."
  Figure 10 Link: articels_figures_by_rev_year\2020\FeatureAware_Uniform_Tessellations_on_Video_Manifold_for_ContentSensitive_Superv\figure_10.jpg
  Figure 10 caption: Average F measures in the spatiotemporal closure application.
    The results are averaged on Stein et al. [35] dataset. Our FCSS method achieves
    the best average F measure among all ten methods.
  Figure 2 Link: articels_figures_by_rev_year\2020\FeatureAware_Uniform_Tessellations_on_Video_Manifold_for_ContentSensitive_Superv\figure_2.jpg
  Figure 2 caption: "Left: regular 3D lattices of voxels in R 3 . Middle and right:\
    \ the map \u03A6:\u03A5\u2192M\u2282 R 6 stretches the unit cube \u22A1 v (red\
    \ box) centered at the voxel v(x,y,t)\u2208\u03A5 into a 3-manifold M . Each corner\
    \ a i of \u22A1 v , i=1,2,\u2026,8 , is the center of its eight neighboring voxels."
  Figure 3 Link: articels_figures_by_rev_year\2020\FeatureAware_Uniform_Tessellations_on_Video_Manifold_for_ContentSensitive_Superv\figure_3.jpg
  Figure 3 caption: "Comparison of FCSS and CSS generation on a synthetic, degenerate\
    \ gray video \u03A5 , for easy illustration. In \u03A5 , each image frame at time\
    \ t is a degenerate 1D gray line image I t . Supervoxels are generated by a tessellation\
    \ in \u03A5 . Left: \u03A5 is mapped to a video manifold M=\u03A6(I,t)\u2282 R\
    \ 3 , whose area elements give a good measure of content density in \u03A5 . Middle\
    \ and right: two local minimums of the tessellation energy specified in Eq.(6).\
    \ The generating points are shown in dots on M and their inverse images by \u03A6\
    \ \u22121 are shown in red crosses + in \u03A5 . In this toy example, the local\
    \ boundaries in \u03A5 can be characterized by the zero crossing of the second\
    \ derivative of \u03A5(I,t) (shown in red circles in left). Note that the tessellations\
    \ in middle and right are generated without this information. The generating points\
    \ in FCSS are farther away from local boundaries [indicating by a larger average\
    \ boundary distance proposed in Eq.(8)], and the cell boundaries of the corresponding\
    \ tessellation better capture these local boundaries than that of CSS."
  Figure 4 Link: articels_figures_by_rev_year\2020\FeatureAware_Uniform_Tessellations_on_Video_Manifold_for_ContentSensitive_Superv\figure_4.jpg
  Figure 4 caption: "The diameter d m =\u2225 p m1 \u2212 p m2 \u2225 2 of an RVT\
    \ cell C M ( s m ) (shaded area) is the maximum euclidean distance between pairs\
    \ of points in this cell. The splitting operation \u2227: s m \u2192( s \u2032\
    \ p , s \u2032 q ) splits an RVT cell C M ( s m ) (shaded area) into two arbitrary\
    \ new cells C \u2032 1 (orange shaded area) and C \u2032 2 (green shaded area),\
    \ satisfying p m1 \u2208 C \u2032 1 and p m2 \u2208 C \u2032 2 , C \u2032 1 \u2229\
    \ C \u2032 2 =\u2205 and C \u2032 1 \u222A C \u2032 2 = C M ( s m ) . The mass\
    \ centroids of cells C M ( s m ) , C \u2032 1 and C \u2032 2 are s \u2032 m ,\
    \ s \u2032 p , s \u2032 q , respectively. Lemma 2 proves that s \u2032 m lies\
    \ on the line segment connecting s \u2032 p and s \u2032 q ."
  Figure 5 Link: articels_figures_by_rev_year\2020\FeatureAware_Uniform_Tessellations_on_Video_Manifold_for_ContentSensitive_Superv\figure_5.jpg
  Figure 5 caption: Evaluation of ten representative methods and our methods (FCSS
    and streaming FCSS) on the SegTrack v2 dataset. Superpixels in all methods are
    unrelabeled. Due to its high computational cost, NCut is run at a fixed frame
    resolution of 240times 160 downsampled from original videos and is not present
    in (h). Only FCSS achieves good performance on all seven metrics of UE3D, SA3D,
    BRD, EV, compactness, running time and peak memory. In particular, FCSS is 5times
    to 10times faster than TSP. The peak memory of FCSS is 22times smaller than TSP
    and 7times to 15times smaller than TS-PPM. Similar performances are observed on
    the other three video datasets (BuffaloXiph, BVDS and CamVid), which are reported
    in Appendix, available in the online supplemental material.
  Figure 6 Link: articels_figures_by_rev_year\2020\FeatureAware_Uniform_Tessellations_on_Video_Manifold_for_ContentSensitive_Superv\figure_6.jpg
  Figure 6 caption: For easy illustration, we present a superpixel example on a 2D
    image. Assume 8-connectivity. For an arbitrary image with arbitrary ground-truth
    segmentation, four unrelabeled superpixels are sufficient to achieve a perfect
    performance on the BRD metric, i.e., textBRD=0 . These four superpixels are characterized
    by the parity of the coordinates (x,y) of image pixels; i.e., the green, yellow,
    red and blue superpixels consist of pixels with coordinates (even, even), (even,
    odd), (odd, even), and (odd, odd), respectively.
  Figure 7 Link: articels_figures_by_rev_year\2020\FeatureAware_Uniform_Tessellations_on_Video_Manifold_for_ContentSensitive_Superv\figure_7.jpg
  Figure 7 caption: Evaluation of relabeled supervoxels on the SegTrack v2 dataset.
    Only FCSS achieves good performance on all five metrics of UE3D, SA3D, BRD, EV,
    and compactness.
  Figure 8 Link: articels_figures_by_rev_year\2020\FeatureAware_Uniform_Tessellations_on_Video_Manifold_for_ContentSensitive_Superv\figure_8.jpg
  Figure 8 caption: The average F measures of different supervoxel results on Youtube-Objects
    Dataset. The results are plotted per object class and each object class contains
    several video sequences. Larger F measure values mean better foreground propagation
    results. The results show that FCSS, TS-PPM and CSS are the top three methods
    overall in the whole dataset. FCSS has better results than TS-PPM in six classes
    (car, horse, motorbike, cow, boat, dog) and has better results than CSS in nine
    classes (except for the train class).
  Figure 9 Link: articels_figures_by_rev_year\2020\FeatureAware_Uniform_Tessellations_on_Video_Manifold_for_ContentSensitive_Superv\figure_9.jpg
  Figure 9 caption: Foreground propagation results of seven supervoxel methods on
    one example in Youtube-objects dataset [29]. Three representative frames are selected.
    The foreground masks are shown in green. The incorrectly labeled areas are circled
    in red. The average F measure for each example video is shown in the bracket below
    three frames. The value of the F measure ranges in [0,1], and larger values mean
    better results.
  First author gender probability: 0.58
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Ran Yi
  Name of the last author: Yong-Jin Liu
  Number of Figures: 11
  Number of Tables: 0
  Number of authors: 6
  Paper title: Feature-Aware Uniform Tessellations on Video Manifold for Content-Sensitive
    Supervoxels
  Publication Date: 2020-03-10 00:00:00
  Table 1 caption:
    table_text: Not Available
  Table 10 caption:
    table_text: Not Available
  Table 2 caption:
    table_text: Not Available
  Table 3 caption:
    table_text: Not Available
  Table 4 caption:
    table_text: Not Available
  Table 5 caption:
    table_text: Not Available
  Table 6 caption:
    table_text: Not Available
  Table 7 caption:
    table_text: Not Available
  Table 8 caption:
    table_text: Not Available
  Table 9 caption:
    table_text: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2020.2979714
