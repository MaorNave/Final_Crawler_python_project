- Affiliation of the first author: department of mathematics, hong kong baptist university,
    kowloon tong, hong kong
  Affiliation of the last author: department of mathematics, hong kong baptist university,
    kowloon tong, hong kong
  Figure 1 Link: articels_figures_by_rev_year\2022\A_Progressive_Hierarchical_Alternating_Least_Squares_Method_for_Symmetric_Nonneg\figure_1.jpg
  Figure 1 caption: "Illustrations of g \u2032 (\u03BB) and h(\u03BB) for cases (I)-(III)\
    \ with different conditions (i)-(iii) satisfied. The red point indicates the \u03BB\
    \ \xAF ."
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\A_Progressive_Hierarchical_Alternating_Least_Squares_Method_for_Symmetric_Nonneg\figure_2.jpg
  Figure 2 caption: Relative error and optimality gap for synthetical data sets.
  Figure 3 Link: articels_figures_by_rev_year\2022\A_Progressive_Hierarchical_Alternating_Least_Squares_Method_for_Symmetric_Nonneg\figure_3.jpg
  Figure 3 caption: Relative error and optimality gap for real data sets.
  Figure 4 Link: articels_figures_by_rev_year\2022\A_Progressive_Hierarchical_Alternating_Least_Squares_Method_for_Symmetric_Nonneg\figure_4.jpg
  Figure 4 caption: Clustering accuracy for real data sets.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 0.75
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.55
  Name of the first author: Liangshao Hou
  Name of the last author: Li-Zhi Liao
  Number of Figures: 4
  Number of Tables: 2
  Number of authors: 3
  Paper title: A Progressive Hierarchical Alternating Least Squares Method for Symmetric
    Nonnegative Matrix Factorization
  Publication Date: 2022-09-14 00:00:00
  Table 1 caption: TABLE 1 Real Data Sets Used in Experiments
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Average Clustering Accuracy for Different Data Sets
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3206465
- Affiliation of the first author: "department of engineering \u201Cenzo ferrari\u201D\
    , the university of modena and reggio emilia, modena, italy"
  Affiliation of the last author: "department of engineering \u201Cenzo ferrari\u201D\
    , the university of modena and reggio emilia, modena, italy"
  Figure 1 Link: articels_figures_by_rev_year\2022\ClassIncremental_Continual_Learning_Into_the_eXtended_DERVerse\figure_1.jpg
  Figure 1 caption: A view of CiCL timelines. T c is the task that is currently being
    learned by the model; T c ~ indicates the task at which an example entered the
    memory buffer.
  Figure 10 Link: articels_figures_by_rev_year\2022\ClassIncremental_Continual_Learning_Into_the_eXtended_DERVerse\figure_10.jpg
  Figure 10 caption: On Split CIFAR-100 and Split miniImageNet, analysis evaluating
    how the number of heads pre-allocated in preparation of future tasks affects the
    final average accuracy.
  Figure 2 Link: articels_figures_by_rev_year\2022\ClassIncremental_Continual_Learning_Into_the_eXtended_DERVerse\figure_2.jpg
  Figure 2 caption: Three visuals that depict some facets of bias accumulation inherent
    Dark Experience Replay++.
  Figure 3 Link: articels_figures_by_rev_year\2022\ClassIncremental_Continual_Learning_Into_the_eXtended_DERVerse\figure_3.jpg
  Figure 3 caption: "X-DERreckons on distinct objectives for different partitions\
    \ of the output space: i) it applies the Cross-Entropy loss in a way that isolates\
    \ the head of the current task; ii) it relieves forgetting by applying Knowledge\
    \ Distillation on examples from M ; iii) it warms (future) logits tied to unseen\
    \ classes; Meanwhile, the predictions stored in M are regularly updated to deal\
    \ with secondary information (\u201Cfuture past\u201D) relating old examples with\
    \ the classes emerging later on the stream."
  Figure 4 Link: articels_figures_by_rev_year\2022\ClassIncremental_Continual_Learning_Into_the_eXtended_DERVerse\figure_4.jpg
  Figure 4 caption: Unlike standard approaches, X-DERintroduces pretext tasks for
    anticipating unseen classes.
  Figure 5 Link: articels_figures_by_rev_year\2022\ClassIncremental_Continual_Learning_Into_the_eXtended_DERVerse\figure_5.jpg
  Figure 5 caption: Visual summary of which loss terms cover each classification heads.
  Figure 6 Link: articels_figures_by_rev_year\2022\ClassIncremental_Continual_Learning_Into_the_eXtended_DERVerse\figure_6.jpg
  Figure 6 caption: For the experimental settings reported in Table 1, the trend of
    the average test-set accuracy on the observed tasks.
  Figure 7 Link: articels_figures_by_rev_year\2022\ClassIncremental_Continual_Learning_Into_the_eXtended_DERVerse\figure_7.jpg
  Figure 7 caption: Accuracy of models trained from scratch on memory buffers of ER
    (Labels), DER++ (Logits, Both), and X-DER(Logits, Both). The resulting accuracy
    represents a proxy of the informativeness of the memory buffer.
  Figure 8 Link: articels_figures_by_rev_year\2022\ClassIncremental_Continual_Learning_Into_the_eXtended_DERVerse\figure_8.jpg
  Figure 8 caption: Effect of several regularization methods on net calibration (Split
    CIFAR-100). While most of them degrade with lower memory size (left), X-DERyields
    robust performance.
  Figure 9 Link: articels_figures_by_rev_year\2022\ClassIncremental_Continual_Learning_Into_the_eXtended_DERVerse\figure_9.jpg
  Figure 9 caption: Analysis of generalization to unseen classes. (a) For each of
    the four remaining tasks of CIFAR-100, the performance versus training-set size
    trend for different CL methods; (b) The curves describing the forward transfer
    at the end of every task.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: female
  Last author gender probability: 0.77
  Name of the first author: Matteo Boschini
  Name of the last author: Simone Calderara
  Number of Figures: 16
  Number of Tables: 2
  Number of authors: 5
  Paper title: Class-Incremental Continual Learning Into the eXtended DER-Verse
  Publication Date: 2022-09-14 00:00:00
  Table 1 caption: TABLE 1 Class-Incremental Continual Learning Final Average Accuracy
    (FAA) and Final Forgetting (FF) (In Parentheses)
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Secondary Information Metrics (Lower is Better)
  Table 3 caption: Not Available
  Table 4 caption: Not Available
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3206549
- Affiliation of the first author: department of automation, key laboratory of system
    control and information processing of ministry of education, key laboratory of
    marine intelligent equipment and system of ministry of education, shanghai engineering
    research center of intelligent control and management, shanghai jiao tong university,
    shanghai, china
  Affiliation of the last author: department of automation, key laboratory of system
    control and information processing of ministry of education, key laboratory of
    marine intelligent equipment and system of ministry of education, shanghai engineering
    research center of intelligent control and management, shanghai jiao tong university,
    shanghai, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Efficient_D_Deep_LiDAR_Odometry\figure_1.jpg
  Figure 1 caption: The Point Feature Pyramid, Pose Warping, and Attentive Cost Volume
    (PWC) structure in our proposed EfficientLO-Net. The pose is refined layer by
    layer through iterative pose warp-refinement. The whole process is realized end-to-end
    by making all modules differentiable. In the LiDAR point clouds, the small black
    points are the whole point cloud. The big black points are the sampled points
    in P C 1 . Distinctive colors of big points in embedding mask measure the contribution
    of sampled points to the pose estimation.
  Figure 10 Link: articels_figures_by_rev_year\2022\Efficient_D_Deep_LiDAR_Odometry\figure_10.jpg
  Figure 10 caption: 3D and 2D trajectory results on KITTI validation sequence 09
    with ground truth. Our method obtained the similar accurate trajectory with full
    A-LOAM.
  Figure 2 Link: articels_figures_by_rev_year\2022\Efficient_D_Deep_LiDAR_Odometry\figure_2.jpg
  Figure 2 caption: The details of the proposed EfficientLO-Net architecture. The
    network is composed of four set conv layers in point feature pyramid, one attentive
    cost volume, one initial embedding mask and pose generation module, and three
    pose warp-refinement modules. The network outputs the predicted poses from four
    levels for supervised training.
  Figure 3 Link: articels_figures_by_rev_year\2022\Efficient_D_Deep_LiDAR_Odometry\figure_3.jpg
  Figure 3 caption: Visualization of the projection-aware representation of point
    clouds. During the projection process, the 3D point cloud will be projected onto
    a cylindrical plane according to the parameters of the LiDAR. The raw XYZ coordinates
    will be recorded in the cylindrical plane. For subsequent operations, the left
    and right sides of the XYZ coordinate map obtained from cylindrical projection
    are connected.
  Figure 4 Link: articels_figures_by_rev_year\2022\Efficient_D_Deep_LiDAR_Odometry\figure_4.jpg
  Figure 4 caption: 'The details of projection-aware set-conv modules. The module
    is composed of four key parts: stride-based sampling, projection-aware grouping,
    3D distance-based filtering, and feature aggregation. The output is the sampled
    points with aggregated feature on the corresponding projected 2D plane.'
  Figure 5 Link: articels_figures_by_rev_year\2022\Efficient_D_Deep_LiDAR_Odometry\figure_5.jpg
  Figure 5 caption: Attention Cost-volume. This module takes two frames of point clouds
    with their local features as input and associates the two point clouds. Finally,
    the module outputs the embedding features located in PC1 .
  Figure 6 Link: articels_figures_by_rev_year\2022\Efficient_D_Deep_LiDAR_Odometry\figure_6.jpg
  Figure 6 caption: The details of projection-aware cost volume modules. The module
    is composed of two projection-aware grouping parts, and two feature aggregation
    parts. The output is PC1 with aggregated feature from PC2 on the corresponding
    projected 2D plane.
  Figure 7 Link: articels_figures_by_rev_year\2022\Efficient_D_Deep_LiDAR_Odometry\figure_7.jpg
  Figure 7 caption: The details of the proposed Pose Warp-Refinement module at the
    l th level.
  Figure 8 Link: articels_figures_by_rev_year\2022\Efficient_D_Deep_LiDAR_Odometry\figure_8.jpg
  Figure 8 caption: 'The details of projection-aware set-upconv modules. The module
    is composed of four key parts: center points specification, projection-aware grouping,
    3D distance-based filtering, and feature aggregation. The output is the points
    in dense layer with aggregated feature on their corresponding projected 2D plane.'
  Figure 9 Link: articels_figures_by_rev_year\2022\Efficient_D_Deep_LiDAR_Odometry\figure_9.jpg
  Figure 9 caption: Trajectory results of A-LOAM and ours on KITTI training sequences
    with ground truth. Ours is much better than the A-LOAM without mapping. And ours
    also has similar performance on the two sequences with full A-LOAM, though ours
    is for odometry and A-LOAM has the mapping optimization.
  First author gender probability: 0.97
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.97
  Name of the first author: Guangming Wang
  Name of the last author: Hesheng Wang
  Number of Figures: 13
  Number of Tables: 12
  Number of authors: 5
  Paper title: Efficient 3D Deep LiDAR Odometry
  Publication Date: 2022-09-15 00:00:00
  Table 1 caption: TABLE 1 Detailed Network Parameters in EfficientLO-Net
  Table 10 caption: TABLE 10 The Ablation Study Results of LiDAR Odometry for the
    Projection-Aware 3D Feature Learning on KITTI Odometry Dataset [32]
  Table 2 caption: TABLE 2 The LiDAR Odometry Experiment Results on KITTI Odometry
    Dataset [32]
  Table 3 caption: TABLE 3 The LiDAR Odometry Experiment Results on KITTI Odometry
    Dataset [32] Compared With [22]
  Table 4 caption: TABLE 4 The LiDAR Odometry Experiment Results on KITTI Odometry
    Dataset [32] Compared With [20]
  Table 5 caption: TABLE 5 The LiDAR Odometry Results on Sequences 04 and 10 of KITTI
    Odometry Dataset [32]
  Table 6 caption: TABLE 6 The LiDAR Odometry Results on Sequences 09 and 10 of KITTI
    Odometry Dataset [32]
  Table 7 caption: TABLE 7 The LiDAR Odometry Results on Official Test Sequences 11-21
    of KITTI Odometry Dataset [32]
  Table 8 caption: TABLE 8 The Runtime Comparison Between Ours and LO-Net
  Table 9 caption: TABLE 9 The Ablation Study Results of LiDAR Odometry for the Network
    Structure on KITTI Odometry Dataset [32]
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3207015
- Affiliation of the first author: school of artificial intelligence, beijing normal
    university, beijing, china
  Affiliation of the last author: school of artificial intelligence, beijing normal
    university, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\Transitional_Learning_Exploring_the_Transition_States_of_Degradation_for_Blind_S\figure_1.jpg
  Figure 1 caption: Overall flow of transitional learning for blind SR.
  Figure 10 Link: articels_figures_by_rev_year\2022\Transitional_Learning_Exploring_the_Transition_States_of_Degradation_for_Blind_S\figure_10.jpg
  Figure 10 caption: SR results on images with mixed synthetic or unknown real-world
    degradations for times 4 upscaling. Our TLSR textReal and TLSR dagger textReal
    reconstruct more distinct and realistic details, and has fewer distortions than
    other methods.
  Figure 2 Link: articels_figures_by_rev_year\2022\Transitional_Learning_Exploring_the_Transition_States_of_Degradation_for_Blind_S\figure_2.jpg
  Figure 2 caption: For both convolutive degradations and additive degradations, arbitrary
    degradation (blur kernel or noisy map) can be approximated by the transition state
    of two primary degradations.
  Figure 3 Link: articels_figures_by_rev_year\2022\Transitional_Learning_Exploring_the_Transition_States_of_Degradation_for_Blind_S\figure_3.jpg
  Figure 3 caption: "Architecture of the proposed TLSR, which consists of three modules:\
    \ (1) homogeneous feature extraction network, which is parameter-shared for arbitrary\
    \ transitional degradations; (2) DoT estimation network for estimating \u03C4\
    \ from the randomly cropped local patches; (3) transitional learning module stacks\
    \ a group of transitional transformation function \u03A5 to rebuild an adaptive\
    \ network."
  Figure 4 Link: articels_figures_by_rev_year\2022\Transitional_Learning_Exploring_the_Transition_States_of_Degradation_for_Blind_S\figure_4.jpg
  Figure 4 caption: "Network interpolation: a feasible implementation of the transitional\
    \ transformation function \u03A5 . Unlike [52], since each sample in a batch has\
    \ different DoT \u03C4 b , we apply each specific \u03C4 b to rebuild the corresponding\
    \ adaptive network and use the group convolutions for acceleration."
  Figure 5 Link: articels_figures_by_rev_year\2022\Transitional_Learning_Exploring_the_Transition_States_of_Degradation_for_Blind_S\figure_5.jpg
  Figure 5 caption: "Ablation studies of the transitional learning, the experiments\
    \ are conducted on BSD100 dataset for \xD74 upscaling."
  Figure 6 Link: articels_figures_by_rev_year\2022\Transitional_Learning_Exploring_the_Transition_States_of_Degradation_for_Blind_S\figure_6.jpg
  Figure 6 caption: "Quantitative analysis of the transitional transformation function:\
    \ (a) effects of the non-linear activation for additive degradation (left) and\
    \ convolutive degradation (right); (b) effects of the group convolution. Note\
    \ that \u03C4=0.5 ."
  Figure 7 Link: articels_figures_by_rev_year\2022\Transitional_Learning_Exploring_the_Transition_States_of_Degradation_for_Blind_S\figure_7.jpg
  Figure 7 caption: Statistical distribution and visual explanation of DoT estimation.
  Figure 8 Link: articels_figures_by_rev_year\2022\Transitional_Learning_Exploring_the_Transition_States_of_Degradation_for_Blind_S\figure_8.jpg
  Figure 8 caption: "Qualitative visual comparisons of the proposed TLSR and state-of-the-arts\
    \ blind SR methods for times 4 upscaling on \u201Cimg004\u201D and \u201Cimg099\u201D\
    \ from Urban100 dataset with additive degradations. Our TLSR reconstructs the\
    \ edges and structures better than other methods."
  Figure 9 Link: articels_figures_by_rev_year\2022\Transitional_Learning_Exploring_the_Transition_States_of_Degradation_for_Blind_S\figure_9.jpg
  Figure 9 caption: "Qualitative comparisons of the proposed TLSR and state-of-the-arts\
    \ blind SR methods for times 4 upscaling on \u201Cimg079\u201D and \u201Cimg096\u201D\
    \ from Urban100 dataset with convolutive degradations. Our TLSR has fewer distortions\
    \ and better structuresedges than other methods."
  First author gender probability: 0.61
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.61
  Name of the first author: Yuanfei Huang
  Name of the last author: Hua Huang
  Number of Figures: 12
  Number of Tables: 6
  Number of authors: 5
  Paper title: 'Transitional Learning: Exploring the Transition States of Degradation
    for Blind Super-resolution'
  Publication Date: 2022-09-15 00:00:00
  Table 1 caption: "TABLE 1 Quantitative Comparisons of the Proposed TLSR and SOTAs\
    \ on Blind SR With Additive Degradations for \xD74 \xD74 Upscaling"
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 Quantitative Comparisons of the Proposed TLSR and SOTAs\
    \ on Blind SR With Convolutive Degradations for \xD74 \xD74 Upscaling"
  Table 3 caption: "TABLE 3 Quantitative Comparisons of the Proposed TLSR and State-of-the-art\
    \ SR Methods on Computational Complexities for \u2191 \xD72 \u2191\xD72 Upscaling"
  Table 4 caption: "TABLE 4 Quantitative Comparisons (PSNR \u2191 \u2191) of the Proposed\
    \ TLSR Real Real and SOTAs on Blind SR With Mixed Synthetic and Unknown Real-World\
    \ Degradations for \xD74 \xD74 Upscaling"
  Table 5 caption: "TABLE 5 Quantitative Comparisons (PSNR \u2191 \u2191) of the Proposed\
    \ TLSR and SOTAs on Blind SR With Anisotropic Gaussian Blur Degradations for \xD7\
    4 \xD74 Upscaling on Urban100 Dataset"
  Table 6 caption: "TABLE 6 Quantitative Comparisons (PSNR \u2191 \u2191) of the Proposed\
    \ TLSR and SOTAs on Blind JPEG Compession Artifacts Removal"
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3206870
- Affiliation of the first author: department of computer science and engineering,
    michigan state university, east lansing, mi, usa
  Affiliation of the last author: department of computer science and engineering,
    michigan state university, east lansing, mi, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\PrintsGAN_Synthetic_Fingerprint_Generator\figure_1.jpg
  Figure 1 caption: Example of a rolled fingerprint synthesized by PrintsGAN and overlaid
    with its minutiae representation. Minutiae are automatically annotated with the
    Verifinger v12 SDK. The fingerprint here qualitatively shows the realism of the
    fingerprints generated by PrintsGAN.
  Figure 10 Link: articels_figures_by_rev_year\2022\PrintsGAN_Synthetic_Fingerprint_Generator\figure_10.jpg
  Figure 10 caption: Histograms (a) and CDFs (b) for imposter score distributions.
  Figure 2 Link: articels_figures_by_rev_year\2022\PrintsGAN_Synthetic_Fingerprint_Generator\figure_2.jpg
  Figure 2 caption: Examples of real fingerprints taken from an operational forensic
    database (a, b, c) [12] and the publicly available NIST SD 302 database (d, e)
    [13]. These fingerprints provide a reference point for qualitatively determining
    the realism of the synthetic fingerprints shown throughout the paper.
  Figure 3 Link: articels_figures_by_rev_year\2022\PrintsGAN_Synthetic_Fingerprint_Generator\figure_3.jpg
  Figure 3 caption: Example images taken from prior fingerprint synthesis algorithms;
    (a) [20], (b) [21], (c) [22], (d) [23], (e) [24], (f) [25], (g) [26], (h) [27],
    (i) [28], (j) [29], (k) [30], (l) [31]. Existing synthesis algorithms are limited
    by a lack of realism (domain gap between real and synthetic fingerprints), e.g.,
    (a-i). GAN based synthesis methods generate more realistic fingerprints e.g.,
    (j-l), however, all but [30] are not able to generate multiple impressions for
    a given fingerprint (they only generate unique fingerprints). Although [30] generates
    multiple impressions per finger, it is limited to generating fingerprint patches
    as opposed to full rolled fingerprints. Our proposed PrintsGAN generates more
    realistic rolled fingerprints than the baselines (via a crowd-source evaluation)
    and is also capable of generating multiple impressions per finger. This enables
    us to train a CNN on top of our synthetically generated fingerprints to learn
    a discriminative fingerprint representation for fingerprint matching.
  Figure 4 Link: articels_figures_by_rev_year\2022\PrintsGAN_Synthetic_Fingerprint_Generator\figure_4.jpg
  Figure 4 caption: Schematic of PrintsGAN. PrintsGAN operates in two stages. In the
    first stage, a Master-Print, or a new identity is generated. A Master-Print is
    a binarized friction ridge pattern at 250 ppi. After synthesizing a Master-Print,
    it is passed to a non-linear warping and cropping module to simulate the effects
    of pressing the finger against a fingerprint reader platen at different roll,
    pitch, yaw, and degree of pressure. Finally, this warped and cropped Master-Print
    is passed to the second stage of the synthesis process where it is rendered with
    realistic textural details at 500 ppi. By passing different identity noise z ID
    , distortion noise z distort , and texture noise ( z texture ), PrintsGAN is able
    to generate many fingerprint identities as well as impressions per identity. In
    this manner, PrintsGAN models both the inter-class and intra-class variance of
    a large fingerprint database, making it useful for training deep networks to extract
    representations for matching.
  Figure 5 Link: articels_figures_by_rev_year\2022\PrintsGAN_Synthetic_Fingerprint_Generator\figure_5.jpg
  Figure 5 caption: An example of a synthetic fingerprint identity, with five different
    impressions, generated by PrintsGAN. The top row shows the binary Master-Print
    with various warpings and croppings. The bottom row shows each of those Master-Print
    warps after a textural rendering.
  Figure 6 Link: articels_figures_by_rev_year\2022\PrintsGAN_Synthetic_Fingerprint_Generator\figure_6.jpg
  Figure 6 caption: A rolled fingerprint from [12] is binarized via our trained grayscale
    fingerprint-to-binary auto-encoder.
  Figure 7 Link: articels_figures_by_rev_year\2022\PrintsGAN_Synthetic_Fingerprint_Generator\figure_7.jpg
  Figure 7 caption: Examples of genuine pairs (top row) and imposter pairs (bottom
    row) synthesized by PrintsGAN. The minutiae matching score of Verifinger v12 SDK
    are displayed (below each pair) to show that i) PrintsGAN can generate unique
    fingerprints (low imposter pair scores) and ii) PrintsGAN can generate multiple
    impressions per finger (high genuine pair scores). Note, the matching threshold
    for Verifinger v12 for a False Acceptance Rate of 0.01% is a match score of 48.
  Figure 8 Link: articels_figures_by_rev_year\2022\PrintsGAN_Synthetic_Fingerprint_Generator\figure_8.jpg
  Figure 8 caption: Example prompt from experiment 1 of the expert survey on synthetic
    fingerprint realism. Participants were asked to rank the six synthesis methods
    in order of realism on a scale from 1-6, 1 being the most realistic to 6 being
    the least realistic.
  Figure 9 Link: articels_figures_by_rev_year\2022\PrintsGAN_Synthetic_Fingerprint_Generator\figure_9.jpg
  Figure 9 caption: Example prompt for experiment 2 of the expert survey on synthetic
    fingerprint realism. Experts were asked to chose between real or fake when presented
    with a fingerprint image.
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Joshua James Engelsma
  Name of the last author: Anil K. Jain
  Number of Figures: 12
  Number of Tables: 6
  Number of authors: 3
  Paper title: 'PrintsGAN: Synthetic Fingerprint Generator'
  Publication Date: 2022-09-15 00:00:00
  Table 1 caption: TABLE 1 Examples of Publicly Available Fingerprint Datasets
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Examples of Publicly Available Face Datasets
  Table 3 caption: 'TABLE 3 Results for Expert Crowdsourcing Experiment 1: Rate Each
    Fingerprint from Most Realistic to Least Realistic (1 = Most Realistic, 6 = Least
    Realistic)'
  Table 4 caption: 'TABLE 4 Results for Expert Crowdsourcing Experiment 2: Rate Each
    Fingerprints as Real or Fake'
  Table 5 caption: TABLE 5 Fingerprint Metrics for Real (DB-1) and PrintsGAN (DB-2)
    Fingerprints
  Table 6 caption: TABLE 6 Authentication Accuracy
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3204591
- Affiliation of the first author: korea aerospace university, goyang, south korea
  Affiliation of the last author: yonsei university, seoul, south korea
  Figure 1 Link: articels_figures_by_rev_year\2022\Stereo_Confidence_Estimation_via_Locally_Adaptive_Fusion_and_Knowledge_Distillat\figure_1.jpg
  Figure 1 caption: Network configuration. LAF-Net consists of four sub-networks,
    including feature extraction networks, attention inference networks, scale inference
    network, and recursive refinement networks. Given matching cost, disparity, and
    image as input, it outputs confidence of the disparity.
  Figure 10 Link: articels_figures_by_rev_year\2022\Stereo_Confidence_Estimation_via_Locally_Adaptive_Fusion_and_Knowledge_Distillat\figure_10.jpg
  Figure 10 caption: 'Effects of specification according to confidence: (a) color
    image, initial disparity map, and ground-truth confidence, initial disparity maps
    by gradually removing a percentage of pixels with lowest confidence by (top to
    bottom) CCNN [18], DLAF-Net, and LAF-Net, with (b) 5%, (c) 10%, (d) 20%, and (e)
    estimated confidence maps.'
  Figure 2 Link: articels_figures_by_rev_year\2022\Stereo_Confidence_Estimation_via_Locally_Adaptive_Fusion_and_Knowledge_Distillat\figure_2.jpg
  Figure 2 caption: 'Visualization of learned attention maps: top-1 matching cost,
    disparity, left color image, and the attention maps for matching cost, disparity,
    and color, respectively.'
  Figure 3 Link: articels_figures_by_rev_year\2022\Stereo_Confidence_Estimation_via_Locally_Adaptive_Fusion_and_Knowledge_Distillat\figure_3.jpg
  Figure 3 caption: Illustration of bilinear sampler in scale inference networks.
    For each pixel i , the feature Y can be warped as enlarged size feature Y S .
    With the stride, the neighbors j S are convolved as Z .
  Figure 4 Link: articels_figures_by_rev_year\2022\Stereo_Confidence_Estimation_via_Locally_Adaptive_Fusion_and_Knowledge_Distillat\figure_4.jpg
  Figure 4 caption: 'Effectiveness of recursive refinement networks: (a) left color
    image, (b) initial disparity, (c) estimated confidence map without recursive module,
    (d) thresholded disparity with (c), (e) estimated confidence map with recursive
    module, (f) thresholded disparity with (e). The mismatched pixels in the red boxes
    are reliably detected with the proposed recursive refinement networks.'
  Figure 5 Link: articels_figures_by_rev_year\2022\Stereo_Confidence_Estimation_via_Locally_Adaptive_Fusion_and_Knowledge_Distillat\figure_5.jpg
  Figure 5 caption: Network configuration in knowledge distilation framework. The
    teacher networks (i.e., LAF-Net) take tri-modal input, while the student networks
    (i.e., DLAF-Net) take as input a disparity only. To train the student networks
    by transferring the knowledge of the teacher, we use the knowledge distilation
    framework with a learned temperature to boost the performance.
  Figure 6 Link: articels_figures_by_rev_year\2022\Stereo_Confidence_Estimation_via_Locally_Adaptive_Fusion_and_Knowledge_Distillat\figure_6.jpg
  Figure 6 caption: 'Effectiveness of temperature inference networks: (a) ground truth
    confidence, (b) estimated confidence with teacher networks, namely LAF-Net, (c)
    difference between ground truth confidence (a) and estimated confidence (b), (d)
    temperature T inferred by the temperature inference networks, (e) soft label with
    fixed T=1 , and (f) soft label with T in (d). By using locally-varying learned
    temperatures, the effects of erroneous outputs of teach can be reduced, which
    boosts the distillation performance.'
  Figure 7 Link: articels_figures_by_rev_year\2022\Stereo_Confidence_Estimation_via_Locally_Adaptive_Fusion_and_Knowledge_Distillat\figure_7.jpg
  Figure 7 caption: Visualization of geometric and photometric consistency. The depth
    map obtained by reference image and different views should be consistent as in
    (a). The warped image using depth map should be consistent as in (b).
  Figure 8 Link: articels_figures_by_rev_year\2022\Stereo_Confidence_Estimation_via_Locally_Adaptive_Fusion_and_Knowledge_Distillat\figure_8.jpg
  Figure 8 caption: Illustration of M-LAF-Net. Compared to LAF-Net, which takes the
    left disparity and image as inputs, we use the concatenation of multiple depth
    maps and warped images as inputs in M-LAF-Net.
  Figure 9 Link: articels_figures_by_rev_year\2022\Stereo_Confidence_Estimation_via_Locally_Adaptive_Fusion_and_Knowledge_Distillat\figure_9.jpg
  Figure 9 caption: Sparsification curves of selected images for MID 2006 [33], MID
    2014 [34], and KITTI 2015 dataset [35] using (a), (c), (e) census-SGM and (b),
    (d), (f) MC-CNN. The sparsification curve for the ground-truth confidence map
    is described as optimal.
  First author gender probability: 0.7
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 0.95
  Name of the first author: Sunok Kim
  Name of the last author: Kwanghoon Sohn
  Number of Figures: 16
  Number of Tables: 8
  Number of authors: 5
  Paper title: Stereo Confidence Estimation via Locally Adaptive Fusion and Knowledge
    Distillation
  Publication Date: 2022-09-16 00:00:00
  Table 1 caption: TABLE 1 The Average AUC Values for MID 2006 [33], MID 2014 [34],
    and KITTI 2015 [35] Dataset
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 The Average AUC Values for KITTI 2015 Dataset [35] With
    GA-Net [43]
  Table 3 caption: TABLE 3 The Average AUC Values for SUN3D Dataset [36] With MC-CNN
    [5]
  Table 4 caption: TABLE 4 The Execution Time for the DLAF-Net, CCNN, LAF-Net, and
    M-LAF-Net and the Number of Parameters of Each Network
  Table 5 caption: TABLE 5 Ablation Study for the Various Combination of Input Modalities
    in LAF-Net on MID 2006 [33], MID 2014 [34], and KITTI 2015 [35] Dataset, When
    the Raw Matching Cost is Obtained Using M C-CNN [5]
  Table 6 caption: TABLE 6 Ablation Study for the Effectivness of Each Sub-Network
    in LAF-Net on MID 2006 [33], MID 2014 [34], and KITTI 2015 [35] Dataset, When
    the Raw Matching Cost is Obtained Using MC-CNN [5]
  Table 7 caption: TABLE 7 The Average AUC Values of DLAF-Net With MC-CNN With Fixed
    Temperature T=0.1,1,10 T=0.1,1,10 and Locally Varying T i Ti
  Table 8 caption: TABLE 8 Ablation Study for the Geometric and Photometric Consistency
    in M-LAF-Net on SUN3D Dataset [36]
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3207286
- Affiliation of the first author: school of data science, fudan university, shanghai,
    china
  Affiliation of the last author: department of engineering science, university of
    oxford, oxford, u.k.
  Figure 1 Link: articels_figures_by_rev_year\2022\Dynamic_Graph_Message_Passing_Networks\figure_1.jpg
  Figure 1 caption: "Contextual information is crucial for complex scene understanding\
    \ tasks. To recognise the \u201Cboathouse\u201D, one needs to consider the \u201C\
    boat\u201D and the \u201Cwater\u201D next to it. Fully-connected message passing\
    \ models (a) are able to obtain this information, but are prohibitively expensive.\
    \ Furthermore, they capture a lot of redundant information (i.e.\u201Ctrees\u201D\
    \ and \u201Csky\u201D). Locally-connected models (b) are more efficient, but miss\
    \ out on important context. Our proposed approach (c), dynamically samples a small\
    \ subset of relevant feature nodes based on a learned dynamic sampling scheme,\
    \ i.e. the learned position-specific random walk (indicated by the white dashed\
    \ arrow lines), and also dynamically predicts filter weights and affinities (indicated\
    \ by unique edge and square colors.), which are both conditioned on the sampled\
    \ feature nodes."
  Figure 10 Link: articels_figures_by_rev_year\2022\Dynamic_Graph_Message_Passing_Networks\figure_10.jpg
  Figure 10 caption: More qualitative examples of the instance segmentation task on
    the COCO validation dataset. The odd rows are the results from the Mask R-CNN
    baseline [19], [41]. The even rows are the detection results from our DGMN approach.
    Note how our approach often produces better segmentations and fewer false-positive
    and false-negative detections.
  Figure 2 Link: articels_figures_by_rev_year\2022\Dynamic_Graph_Message_Passing_Networks\figure_2.jpg
  Figure 2 caption: "Overview of our proposed dynamic graph message passing network\
    \ (DGMN). The neighbourhood used to update the feature representation of each\
    \ node (we show a single node with a red square) is predicted dynamically conditioned\
    \ on each input. This is done by first uniformly sampling (denoted by \u201CUS\u201D\
    ) a set of S neighbourhoods around each node. Each neighbourhood contains K (e.g.\
    \ 3\xD73 ) sampled nodes. Here, the blue nodes were sampled with a low sampling\
    \ rate, and the green ones with a high sampling rate. Walks are predicted (conditioned\
    \ on the input) from these uniformly sampled nodes, denoted by the \u25EFs symbol\
    \ representing the random walk sampling operation described in Section 3.3. DMC\
    \ 1 ,\u22EF, DMC S and \u03B2 1 ,\u22EF, \u03B2 S denotes S dynamic message calculation\
    \ operations and S message scaling parameters, respectively. The DMC module is\
    \ detailed in Figure 3. The symbol \u2295 indicates an element-wise addition operation."
  Figure 3 Link: articels_figures_by_rev_year\2022\Dynamic_Graph_Message_Passing_Networks\figure_3.jpg
  Figure 3 caption: "Schematic illustration of the proposed dynamic message passing\
    \ calculation (DMC) module. The small red square indicates the receiving node\
    \ whose message is calculated from its neighbourhood, i.e. the sampled K (e.g.\
    \ 3\xD73 ) features nodes. The module accepts a feature map as input and produces\
    \ its corresponding message map. The symbol \u2217 denotes group convolution operation\
    \ using the dynamically predicted and position specific group kernels and affinities."
  Figure 4 Link: articels_figures_by_rev_year\2022\Dynamic_Graph_Message_Passing_Networks\figure_4.jpg
  Figure 4 caption: "The architecture of DGMN2 (DGMN2-Tiny). \u201C d \u2032 \u201D\
    \ denotes the embedding dimension of tokens in each attention head. \u201CRelPos\u201D\
    \ denotes the relative position encoding. \u2A01 and \u2A02 denotes element-wise\
    \ sum and matrix multiplication respectively."
  Figure 5 Link: articels_figures_by_rev_year\2022\Dynamic_Graph_Message_Passing_Networks\figure_5.jpg
  Figure 5 caption: Visualisation of the nodes sampled via learning the random walks
    with our network (trained for instance segmentation on COCO). The red point indicates
    a receiving node i . Different colour families (i.e. yellow and blue) indicate
    the learned position specific weights and affinities of the sampled nodes. Different
    colours in the same family show the sampled nodes with different sampling rates
    for the same receiving node.
  Figure 6 Link: articels_figures_by_rev_year\2022\Dynamic_Graph_Message_Passing_Networks\figure_6.jpg
  Figure 6 caption: Validation curves of mathrmAPbox and mathrmAPmask on COCO for
    Mask-RCNN baseline, Non-local and the proposed DGMN. The number of training epochs
    is 90K.
  Figure 7 Link: articels_figures_by_rev_year\2022\Dynamic_Graph_Message_Passing_Networks\figure_7.jpg
  Figure 7 caption: Qualitative examples of our results for semantic segmentation
    on Cityscapes (first row), and object detection and instance segmentation on COCO
    (second row).
  Figure 8 Link: articels_figures_by_rev_year\2022\Dynamic_Graph_Message_Passing_Networks\figure_8.jpg
  Figure 8 caption: Qualitative results of the Dilated FCN baseline [65] and our proposed
    DGMN model on the Cityscapes dataset.
  Figure 9 Link: articels_figures_by_rev_year\2022\Dynamic_Graph_Message_Passing_Networks\figure_9.jpg
  Figure 9 caption: Qualitative examples of the instance segmentation task on the
    COCO validation dataset. The odd rows are the results from the Mask R-CNN baseline
    [19], [41]. The even rows are the results from our DGMN approach. Note how our
    approach often produces better segmentations and fewer false-positive and false-negative
    detections.
  First author gender probability: 0.66
  Gender of the first author: female
  Gender of the last author: male
  Last author gender probability: 1.0
  Name of the first author: Li Zhang
  Name of the last author: Philip H. S. Torr
  Number of Figures: 11
  Number of Tables: 12
  Number of authors: 5
  Paper title: Dynamic Graph Message Passing Networks
  Publication Date: 2022-09-19 00:00:00
  Table 1 caption: TABLE 1 Architecture Specifications of DGMN2 Variants
  Table 10 caption: TABLE 10 Object Detection Performance Using Our DGMN2 With Sparse
    R-CNN [51] on COCO Validation Set
  Table 2 caption: TABLE 2 Ablation Study on the Cityscapes Validation Set for Semantic
    Segmentation
  Table 3 caption: TABLE 3 Quantitative Results of Different Models on the COCO 2017
    Validation Set for Object Detection ( A P b AP b) and Instance Segmentation (
    A P m AP m)
  Table 4 caption: TABLE 4 Comparison to State-of-the-Art for Semantic Segmentation
    on Cityscapes
  Table 5 caption: TABLE 5 Quantitative Results via Plugging Our DGMN Module on Different
    Backbones on the COCO 2017 test-dev Set for Object Detection ( A P box AP box
    ) and Instance Segmentation ( A P mask AP mask )
  Table 6 caption: TABLE 6 Evaluation Results on ILSVRC-2012 ImageNet-1K [45] Validation
    Set
  Table 7 caption: TABLE 7 Object Detection Performance Using Our DGMN2 With RetinaNet
    [35] on COCO Validation Set
  Table 8 caption: TABLE 8 Instance Segmentation Performance Using Our DGMN2 With
    Mask R-CNN [19] on COCO Validation Set
  Table 9 caption: TABLE 9 Object Detection Performance Using Our DGMN2 With Deformable
    DETR [74] on COCO Validation Set
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3207500
- Affiliation of the first author: department of electrical and computer engineering,
    necotis, sherbrooke university, sherbrooke, qc, canada
  Affiliation of the last author: department of electronic systems, norwegian university
    of science and technology, trondheim, norway
  Figure 1 Link: articels_figures_by_rev_year\2022\NAAQA_A_Neural_Architecture_for_Acoustic_Question_Answering\figure_1.jpg
  Figure 1 caption: 'Overview of the CLEAR dataset generation process. Highlighted
    in red: ten randomly sampled sounds from the elementary sounds bank, are assembled
    to create an acoustic scene. The attributes of each elementary sound are depicted
    in blue. The question template (orange) and the elementary sounds attributes are
    combined to instantiate a question. The answer is generated by applying each steps
    of the question functional program (purple) on the acoustic scene definition (blue).
    The impact of the reverberations can be seen in the changes of the signals envelops.'
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\NAAQA_A_Neural_Architecture_for_Acoustic_Question_Answering\figure_2.jpg
  Figure 2 caption: "Common Architecture. Two inputs: a spectro-temporal representation\
    \ of an acoustic scene and a textual question. The spectro-temporal representation\
    \ goes through a feature extractor (Parallel and Interleaved feature extractor\
    \ detailed in Section 4.2.1 for NAAQA and Resnet101 pretrained on ImageNet for\
    \ Visual FiLM) and then a serie of J Resblocks that are linearly modulated by\
    \ \u03B2 j and \u03B3 j (learned from the question input) via FiLM layers. Coordinate\
    \ maps are inserted before convolution blocks that are illustrated with a pink\
    \ border. The output is a probability distribution of the possible answers."
  Figure 3 Link: articels_figures_by_rev_year\2022\NAAQA_A_Neural_Architecture_for_Acoustic_Question_Answering\figure_3.jpg
  Figure 3 caption: Acoustic feature extraction.
  Figure 4 Link: articels_figures_by_rev_year\2022\NAAQA_A_Neural_Architecture_for_Acoustic_Question_Answering\figure_4.jpg
  Figure 4 caption: Test accuracy by question type and the number of relation in the
    question for Optimized NAAQA Parallel . The overall accuracy for this configuration
    is 79.1%. The presence of before or after in a question constitutes a temporal
    relation. The accuracy is NA for relative position and count compare since these
    types of question do no include relations. The hyper-parameters are described
    in the end of Section 6.1.3.
  Figure 5 Link: Not Available
  Figure 5 caption: Not Available
  Figure 6 Link: Not Available
  Figure 6 caption: Not Available
  Figure 7 Link: Not Available
  Figure 7 caption: Not Available
  Figure 8 Link: Not Available
  Figure 8 caption: Not Available
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.99
  Name of the first author: "J\xE9r\xF4me Abdelnour"
  Name of the last author: Giampiero Salvi
  Number of Figures: 4
  Number of Tables: 5
  Number of authors: 3
  Paper title: 'NAAQA: A Neural Architecture for Acoustic Question Answering'
  Publication Date: 2022-09-19 00:00:00
  Table 1 caption: TABLE 1 Types of Questions with Examples and Possible Answers
  Table 10 caption: Not Available
  Table 2 caption: TABLE 2 Datasets Statistics
  Table 3 caption: TABLE 3 Results on CLEAR2
  Table 4 caption: "TABLE 4 Results on DAQA \u2032 the Table Presents Number of Parameters,\
    \ Average Training, Validation and Test Accuracy (%) With Standard Deviation Over\
    \ Five Repetitions of the Training as Well as Average Training Time"
  Table 5 caption: TABLE 5 Impact of the Placement of Time and Frequency Coordinate
    Maps
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3194311
- Affiliation of the first author: masdar, mohamed bin zayed university of artificial
    intelligence, abu dhabi, uae
  Affiliation of the last author: qualcomm, san diego, ca, usa
  Figure 1 Link: articels_figures_by_rev_year\2022\Stylized_Adversarial_Defense\figure_1.jpg
  Figure 1 caption: A robust model trained with our proposed Stylized Adversarial
    Training (SAT) framework generalizes not only to adversarial noise but also handles
    naturally occurring distributional shifts (e.g., contrast change in the above
    example), common corruptions (e.g., sensor noise) and performs better stylization
    compared to a Naturally Trained (NT) model.
  Figure 10 Link: Not Available
  Figure 10 caption: Not Available
  Figure 2 Link: articels_figures_by_rev_year\2022\Stylized_Adversarial_Defense\figure_2.jpg
  Figure 2 caption: Stylized Adversarial Training (SAT). Our style transfer module
    (left) crafts perturbations based on three complimentary cues, that include content
    ( L c ) and style ( L s ) of the target image as well as the classifier boundary
    information ( L ce ). Based on the generated perturbations, our adversarial training
    approach seeks to minimize the distance between clean and adversarial examples
    of the same class and maximize the inter-class distances (right).
  Figure 3 Link: articels_figures_by_rev_year\2022\Stylized_Adversarial_Defense\figure_3.jpg
  Figure 3 caption: (Left to right) Latent space t-SNE visualization of the logits
    extracted from Trades [7], our SAT ( n=1 ) and our SAT ( n=10 ) on CIFAR10 test
    set. Compared to Trades [7], our SAT models form distinct class-wise clusters
    which help retain clean accuracy and improves robustness.
  Figure 4 Link: articels_figures_by_rev_year\2022\Stylized_Adversarial_Defense\figure_4.jpg
  Figure 4 caption: 'White-Box analysis (%): Our proposed SAT handles the data distributional
    shifts significantly better than Trades [7]. Models are evaluated on CINIC-10
    [44] test set. PGD attack ran for 10 iterations. SATs state-of-the-art robustness
    to such distributional shifts complements the strength of our proposed approach
    to enhance the generalizability of deep neural networks.'
  Figure 5 Link: articels_figures_by_rev_year\2022\Stylized_Adversarial_Defense\figure_5.jpg
  Figure 5 caption: 'White-box analysis: Our approach with non-adversarial transformation
    is compared against GCE [30]. Robustness is measured on CIFAR-10 test set against
    PGD with 20 iterations and random restarts. Models trained with our approach are
    significantly robust compared to GCE method [30].'
  Figure 6 Link: articels_figures_by_rev_year\2022\Stylized_Adversarial_Defense\figure_6.jpg
  Figure 6 caption: Black-Box analysis:Our approach trained with non-adversarial perturbations
    is compared against GCE [30]. Adversaries are generated using MIFGSM with 10 iterations
    on CIFAR-10 test set. Our trained models show high resistance to transferable
    attack as compared to GCE [30].
  Figure 7 Link: articels_figures_by_rev_year\2022\Stylized_Adversarial_Defense\figure_7.jpg
  Figure 7 caption: White-box robustness analysis shows the effectiveness of different
    losses introduced in Eq. (3). Results are reported for WideResNet on CIFAR10 dataset
    for single-step SAT (Algorithm 1).
  Figure 8 Link: articels_figures_by_rev_year\2022\Stylized_Adversarial_Defense\figure_8.jpg
  Figure 8 caption: Style transfer using features of ResNet18 trained on CIFAR10 dataset.
    Robust features obtained using SAT produced more perceptually appealing style
    transfer as compared to naturally trained (NT) feature space of ResNet18. Comparison
    is made under the same hyper-parameters and number of iterations (100). (Top to
    bottom) 1st and 2nd rows show style and target images while 3rd and 4th rows show
    style transfer using NT and SAT models, respectively.
  Figure 9 Link: Not Available
  Figure 9 caption: Not Available
  First author gender probability: 1.0
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.98
  Name of the first author: Muzammal Naseer
  Name of the last author: Fatih Porikli
  Number of Figures: 8
  Number of Tables: 13
  Number of authors: 5
  Paper title: Stylized Adversarial Defense
  Publication Date: 2022-09-19 00:00:00
  Table 1 caption: TABLE 1 White-box Attack Scenario
  Table 10 caption: TABLE 10 Convergence Analysis of Single-Step SAT
  Table 2 caption: TABLE 2 White-box Attack Scenario
  Table 3 caption: TABLE 3 White-Box Attack Scenario
  Table 4 caption: TABLE 4 Adversarial Robustness Against Unconstrained Adversarial
    Attack, ROA [41] at Different Window Sizes
  Table 5 caption: TABLE 5 White-Box Target-Attack Scenario
  Table 6 caption: 'TABLE 6 White-Box: Comparison Between PCL and Our Proposed Defense
    (SAT) Under the Threat Model of PCL'
  Table 7 caption: TABLE 7 Comparative Analysis of Robustness (%) to Common Corruptions
    is Shown
  Table 8 caption: TABLE 8 The Use of Data Augmentation Improves the Robustness of
    Our SAT (WRN-28-10 [48]) by a Large Margin
  Table 9 caption: TABLE 9 Hyper-Parameter Search
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3207917
- Affiliation of the first author: state key laboratory of information security, institute
    of information engineering, chinese academy of sciences, beijing, china
  Affiliation of the last author: school of computer science and technology, university
    of chinese academy of sciences, beijing, china
  Figure 1 Link: articels_figures_by_rev_year\2022\MaxMatch_SemiSupervised_Learning_With_WorstCase_Consistency\figure_1.jpg
  Figure 1 caption: Overview of our proposed framework. Different from existing models,
    we consider the worst-case consistency over a class-invariant semantic uncertainty
    set B sem (x) around the original unlabeled data point x .
  Figure 10 Link: articels_figures_by_rev_year\2022\MaxMatch_SemiSupervised_Learning_With_WorstCase_Consistency\figure_10.jpg
  Figure 10 caption: Change of top-1 test error rates (%) on ImageNet-1k for MaxMatch
    and FixMatch with different learning rates. Results of MaxMatch are in shades
    of red, while those of FixMatch are in shades of blue.
  Figure 2 Link: articels_figures_by_rev_year\2022\MaxMatch_SemiSupervised_Learning_With_WorstCase_Consistency\figure_2.jpg
  Figure 2 caption: Images convey the numerical proximity but without semantic proximity.
  Figure 3 Link: articels_figures_by_rev_year\2022\MaxMatch_SemiSupervised_Learning_With_WorstCase_Consistency\figure_3.jpg
  Figure 3 caption: Error rate (%) over 5 different folds with varying labeled set
    size on CIFAR-10.
  Figure 4 Link: articels_figures_by_rev_year\2022\MaxMatch_SemiSupervised_Learning_With_WorstCase_Consistency\figure_4.jpg
  Figure 4 caption: Error rate (%) over 5 different folds with varying labeled set
    size on CIFAR-100.
  Figure 5 Link: articels_figures_by_rev_year\2022\MaxMatch_SemiSupervised_Learning_With_WorstCase_Consistency\figure_5.jpg
  Figure 5 caption: Error rate (%) over 5 different folds with varying labeled set
    size on SVHN.
  Figure 6 Link: articels_figures_by_rev_year\2022\MaxMatch_SemiSupervised_Learning_With_WorstCase_Consistency\figure_6.jpg
  Figure 6 caption: Error rate (%) over 5 different folds with 1000 labeled samples
    on STL-10.
  Figure 7 Link: articels_figures_by_rev_year\2022\MaxMatch_SemiSupervised_Learning_With_WorstCase_Consistency\figure_7.jpg
  Figure 7 caption: Values of loss terms on labeled and unlabeled data of MaxMatch
    and FixMatch using 4000 labels on CIFAR-10. The loss on labeled samples is denoted
    with the suffix L, and that on unlabeled samples is with the suffix U. Moreover,
    in contrast to the worst-case loss in MaxMatch, we also plot the best-case loss
    on unlabeled samples as MaxMatch-U-min.
  Figure 8 Link: articels_figures_by_rev_year\2022\MaxMatch_SemiSupervised_Learning_With_WorstCase_Consistency\figure_8.jpg
  Figure 8 caption: t-SNE visualization for feature embeddings of MaxMatch using 250
    labels on CIFAR-10.
  Figure 9 Link: articels_figures_by_rev_year\2022\MaxMatch_SemiSupervised_Learning_With_WorstCase_Consistency\figure_9.jpg
  Figure 9 caption: Change of error rates (%) on unlabeled data of ImageNet-1k for
    MaxMatch and FixMatch trained from scratch.
  First author gender probability: 0.71
  Gender of the first author: male
  Gender of the last author: male
  Last author gender probability: 0.89
  Name of the first author: Yangbangyan Jiang
  Name of the last author: Qingming Huang
  Number of Figures: 12
  Number of Tables: 4
  Number of authors: 8
  Paper title: 'MaxMatch: Semi-Supervised Learning With Worst-Case Consistency'
  Publication Date: 2022-09-21 00:00:00
  Table 1 caption: TABLE 1 Error Rate (%) on ImageNet-1k
  Table 10 caption: Not Available
  Table 2 caption: "TABLE 2 Error Rate (%) With Different Initial Learning Rates (\
    \ \u03B1 \u03B1)"
  Table 3 caption: TABLE 3 Performance Comparison Between Adopting RandAugment (RA)
    and CTAugment (CTA) on SVHN
  Table 4 caption: TABLE 4 GPU Hours per Epoch During the Training
  Table 5 caption: Not Available
  Table 6 caption: Not Available
  Table 7 caption: Not Available
  Table 8 caption: Not Available
  Table 9 caption: Not Available
  paper DOI: https://doi.org/10.1109/TPAMI.2022.3208419
